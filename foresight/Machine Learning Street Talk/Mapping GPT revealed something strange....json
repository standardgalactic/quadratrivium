{"text": " To me, the difference feels like language models start with this highly abstract language representation. The system as a whole can try to predict the next token with greater and greater accuracy. And so the difference it seems is that the adversarial inputs for us tend to look a lot different than the adversarial examples for LLM. Once you try and go outside of this sphere of what is meaningful to humans, the possibilities grow exponentially. I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos will come out very shortly, but around the same time someone shared a paper on our Discord server and it's called What's the Magic Word? A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron Wachowski. Now, what these guys did is theoretically think about a language model as a dynamical system and use the lens of control theory to think about the space of reachability. Why is this important? Well, language models, we think that they think in language space, this abstract language space, but they don't. They actually think using the shogoth. They think in this very high resolution token space, and it's just this horrible hairy gnarly mess, right? No one has created any firewalls for large language models yet. When companies publish their language models, you know, you just have an API and you just send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind of fine-tuning or preference-steering using human feedback, I thought that they significantly reduced the reachability space. Because in language models, we do the pre-training, which is distribution matching, and then we do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt by snipping off all of those trajectories. Turns out I'm wrong. The reachability space is much larger than I thought it was, and this is one of the things that they point out in their paper. And we kind of knew this, right? Because we can do adversarial attacks on these language models. You know, people have observed that if you use sort of human social engineering tricks on them, like, oh, I'll tip you $500, then it'll do a bit better. But then there's this whole other sort of perceptual layer, I guess you could call it, where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis, kind of like magic, where if you give it these very strange, very inhuman-looking prompts that will steer it to this, to just making a certain output extremely likely, right? And so, to me, it feels really similar to digging into, like, magic and the human perceptual system just with LLMs, where we're learning about basically the shape or what the nature of these language models are in terms of how they interact with the world and how their dynamics really work. For as long as I can remember, the thing I've wanted more than anything else is to figure it all out. I've never shied away from the big questions. Why are we here? What are we all doing? What is this thing we call life that we are all experiencing and one and the same a part of? While these questions are all, you know, 30,000 feet in the air, one thing that drew me back down to earth was the field of engineering. And when I graduated high school, this had a very strong appeal, a pull, because in engineering you can design systems. You can design real, operable things that you can work with and design and understand how they work. And so through engineering, perhaps, you can begin to investigate and understand the intricacies of our world. That's my hope at least. So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence because intelligence seems to underlie so much of our world, so much of the design process of engineering itself. But what is intelligence and how can we understand it? It's a question of systems design, really, where we're trying to figure out, okay, we're humans, we've been in civilization for some time, and we've sort of figured out how to cooperate with each other. We obviously have challenges with that. We're not perfect by any means, but when it comes to adding language models to the mix, I think it could go both ways, where we could have a world where language models just make us much dumber, much less capable, maybe make for a worse world. I think that if we think carefully and we really understand what's going on with the language models, if we can get a fundamental understanding of them one way or another, then there's much more hope that maybe we could make a world where our language models don't just make us smarter, but make our world substantially better and perhaps lead us towards some greater enlightenment and basically ability to cooperate much better than we were even before. Do you think language models are intelligent? That's a great question. I think that they're able to simulate intelligence. One of the really interesting things I'm starting to see now is we are building software abstractions and controllers on top of language models. We've been talking about doing this for years, right, because at the end of the day, we have this idea that we can have this big foundation model and it does all of the things. It's multimodal, it knows how to reason, and the fact of the matter is that's not really true. We control them and I think initially we're seeing frameworks that allow you to do things like prompt injection, but the next step is thinking of controllers, using control theory to think about these large language models. Anyway, I really hope you enjoy the conversation today. Now, these guys are fascinated not only with controlling language models, but also with things like AGI, general intelligence, collective intelligence. It was a really interesting conversation and if you stick around to the end, you can also hear about the institute that they've set up around AGI technology. Enjoy the show. So, my name is Aman. I'm a PhD student at Caltech studying computation and neural systems. Recently, we released this paper called What's the Magic Word, Towards the Control Theory of LLMs, and did that over the last summer with Cameron here. And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering science. I specialized in machine intelligence, sort of been bouncing around between doing machine learning stuff, applying it to computational biology, trying to understand some stuff in theoretical neuroscience, and most recently getting back into the LLM space, as well as trying to study collective intelligence, how very simple machines can come together to produce a very complicated and beautiful system as a whole. So, yeah. Amazing. And Cameron. Yeah. So, my name is Cameron McCoskey, and I went to undergrad here. I did engineering science as well. I majored in the robotics engineering option, and now I'm a grad student. I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and Kevin Chiron. I'm really interested in the deep questions of intelligence, and right now I'm pursuing research related to morphogenesis and computational models of it. Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt engineering, well, a control theory of prompt engineering. I'm excited to get into it. You folks have just written an incredibly interesting paper. It was shared in our Discord server, and I saw your presentation, and we'll share a clip of that in the introduction, but I was intrigued by it straight away. And what you're doing is you're talking about control theory in respect of large language models. Can you explain what that is? Yeah. So, I guess I'll get started with control theory. So back in the day, the late 1800s, this guy Maxwell observed that people were making these engines, and they were putting these things called governors on them, where if your car or your machine was experiencing varying loads, you wanted the engine to still go at the same rate, right? And people had these things called governors. There's this fly ball governor, which is this sort of hand tune thing that you put on top of the engine to try to make sure that it'll be consistent, that it'll do what you want, that it'll be going at a consistent speed, right? And people were hand tuning these things, and obviously the engines were working, but it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees as to how it would end up working in practice. And so what Maxwell did was he formalized the notion of feedback control, where if you have this system, even if it's quite complicated, as it turns out, if you feedback the output of the system into a controller, and try to compute some error metric, and try to correct for that at every moment in time, it turns out to be a much easier problem to solve from an engineering perspective than trying to make a perfect system that just does the right thing off the bat. So this idea of feedback was really powerful, and sort of gave birth to modern control theory. And as it turned out, that was a really powerful way to look at systems, building systems, and controlling them and doing engineering on them, so that they could be robust, do what we want, and so that we could predict them. And so when it comes to LLM control theory, what we saw is that we're kind of at a similar place with language models, where we have these engines, we have these language models that are very powerful, they can do a lot, they seem to exhibit many interesting attributes of intelligence, and there's a lot of utility there for people to build further systems on top of them, and people are already doing that. But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going on where it's really hard to get at the fundamentals of what exactly it means to control an LLM system and how you might do it. At this point, it's very heuristic. And so we sort of saw that as an opportunity to try to figure out what would a control theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to all of these really, really useful engineering insights, and also just fundamental insights as to the nature of LLM systems, so that we can better control them, make them reliable and robust, and be able to do engineering in a more principled manner on them than we're currently able to. So that's sort of the general direction and the motivations for our control theory of language models. Yeah, that's absolutely fascinating. I mean, for many years, I've been thinking that we need to have some kind of a controller for a large language model. But I guess I'm interested in, first of all, what are the differences between large language models and something like a steam engine? And also, with a steam engine, you might be optimizing the efficiency or the performance or the speed or something like that. What is it that we are kind of trying to make better with a large language model? So first off, we'll talk about the differences between large language models and other types of systems that you might want to control. Typically a control system, you might first be introduced to control theory in the context of like, say you're trying to control an engine or something else where the states can be represented by a set of numbers or set of real numbers that is fixed size. So perhaps we have an X and a Y coordinate where it's trying to control or a position in a velocity. These are common types of systems in scenarios that show up in control theory. The difference with an LLM, the first major difference is that the token space, the state space of the system is discreet. Because we're dealing with tokens, we're dealing with words, we're not operating in the space of real numbers anymore, and so this introduces some complications and complexities when dealing with control theory. The second thing that's really significant is that each time an LLM generates a token or a user inputs a token, that state space actually expands. It grows by one token. And this is very interesting and unique for LLM systems. On the one hand, this can be exploited to try and get the LLMs to engage in reasoning or chain of thoughts or kind of take a winding path to the answer you actually want them to outputs. But of course, this makes it very difficult for control theory because each new token you add, the space of possible sentences grows exponentially. And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this grows extremely, extremely quickly. These are some of the challenges. And with a control theory of say engines, you're trying to optimize the efficiency. It's a good question what you're trying to optimize for language models. I think this is definitely a direction for future research. Do you have any thoughts on this? Yeah. I think the thing that we saw was that even very simple questions about how these LLMs operate, their input-output relationships, when you start to treat them just as a system that maybe there's an imposed input, like a system prompt, and then you get to pick a subset of those tokens, right? When you start to treat it like that, and you just ask a really simple question, like, let's say that I want it to generate a specific string. We're not going to be trying to use it to do some intelligent information processing. I just want to see, can I make it do something? And what we found and what sort of motivated us to do this is that we really had no idea when it would be possible or if it was generally possible to make it do anything we want. Can we just make an LLM system generate any output we desire? And if the answer is yes, which seems like it's probable. If you get to have a lot of tokens in your input that you control, it seems reasonable that you'd be able to probably get it to output a wide variety of at least reasonable English sentences or linguistically valid sentences. But the question that we had was, OK, if you have a finite budget for that, would you be able to get it to do anything? And what budget of tokens, like how many tokens do you have to be able to control if you want to be able to make the system do whatever you want? And that was the initial motivation where it was like, yeah, there are all these high and mighty sort of questions of how do we make these systems do what we want in an alignment sense? How do we make them do what we want in the sense of cooperating towards some information processing objective? But we realized that these really, really simple questions are just, OK, you have an input that you get to partially control and you're trying to make it do something. That question was completely unanswered and we were sort of taking bets on it. I think Cameron was the one who started to make bets. He was like, I bet like $10 that we can get this done. We can make it emit this output within five tokens. And that was really the initial motivation where it was like, even the feed forward dynamics of this system are really mysterious. And getting a grip on those, it seems like that's a really strong way to start building up a fundamental control theory and a really strong understanding of these LLM systems where in control theory at least, when you start to really deeply understand just a single system with its own dynamics and how the input-output relationships work, what the reachable sets look like, how controllable it is, then when it comes to building more complicated systems where maybe you have a more complicated objective, maybe you have interacting systems, when you really understand the fundamentals, it makes that way easier. And so the example in classical control theory is that you observe that if you couple a bunch of linear controllers and linear systems together, what you get is just one bigger linear system and all of the same stuff applies. So what we were hoping is that by starting to answer this really simple question of just, okay, how much can we control this? What does the reachability of these LLMs look like? We're really hoping to build that up. And to me, it feels like we're kind of doing our homework where in engineering, we had to take all these classes in control. And that was sort of our homework to be able to go into the world. And if it ever comes time to build some electromechanical system and get a PID controller in there, now we've done our homework so we can have a sense what to expect, how we could do engineering on it. So that's really where I feel like it's at. And I think this is a really promising way to try to get a really fundamental understanding of what's going on with these language model systems. Amazing. So in a second, we're going to introduce this concept of reachability. But I've thought about this because I've had a couple of days to reflect on this. And my intuition, intuitions just seem a little bit mixed up. So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building on adversarial examples and writing algorithms to find adversarial examples. And we know that neural networks are not robust. You can quite easily perturb, let's say, an input image in a vision model. And if it's a classifier, you can make it pretty much say anything with a very small perturbation. And that's kind of the same thing as what you mean as reachability. It's this idea to kind of reach into the state space and make it do something quite weird outside of what you would expect. Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do, you know, I didn't think they had this problem, but they do have this problem. And you introduced this really interesting, I guess it started out as a thought experiment and you coded it into a game. And it's the Roger Federer game. I think that's quite instructive. So can you tell us about that? Yeah, for sure. So one of the earliest examples that we were thinking about was just a simple example of you have this state sequence that's imposed. You don't get to pick it. It says, Roger Federer is the. And then the next thing that you want it to say, the thing that you want the LLM to generate, is the word the greatest. So you want to say, Roger Federer is the greatest. And you're trying to pick out a prompt that comes before then that will steer the system so that it'll output that. So we're basically asking the question, you know, is this word in the reachable set of outputs, given that we have some finite control over the input, where the goal of the game is to, for one, get it to actually output the right answer, which is the greatest, which is a fairly reasonable English thing to say. And the metric that we use to grade how well you're doing on that is basically how efficiently you're able to do control, where in the original control theory, this idea of efficient or optimal control is really important. You have this linear quadratic regularization idea where you're like, I have only a finite energy budget for the signal I put in. Similarly, with language models, what we're interested in is the minimal length of the control input that will steer the model successfully to what you want it to do. And it turns out that the game is actually very challenging, at least with this GPT-2 model, which is the one that we're using right now, since it's just running out of a desktop on my desk at home. And so, yeah, there's this game that you can play, we can link it where you get to put in a prompt to the system, and it'll come back to you and say, OK, you got the answer right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy loss on getting the correct output, the desired output. And the game is to basically get the shortest prompt that will steer the model to the desired output. And it's actually quite challenging with GPT-2, where I think only four people, including Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4, which is this resume checker that uses language models to basically predict your probability of getting into a Fang company. I think those two were the only people who actually ended up getting it right, and it turns out to be very difficult. So that game was sort of a codified sort of interactive version of our initial motivations for this, where it was like, wow, this really simple question that seems like there should be an easy answer. I mean, if there is an easy answer, I'd love to know. But the simple question really leads to a problem that's quite difficult to solve, and we really have poor insight on, and we're really just trying to get that insight together to understand what's going on there. And just to jump off that point as well, I think one of the reasons why this game in particular is difficult is because we're using GPT-2, and Roger Federer is the blank. You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots of fill-in-the-blank tasks. It tends to output just a set of underscores quite often. To comment on your intuition you've mentioned before on whether language models have this adversarial property, one thing that was really interesting when we were doing some of our initial work was this technique of soft prompting. So soft prompting, instead of selecting discrete tokens, which we want to adversarial change the model's behavior with, soft prompting modifies the embedding vectors directly. So you have a lot more fine-grained control over the outputs, and it turns out when you soft prompt, when you adversarily attack not the tokens themselves, but the embedding vectors, you can send the cross-entropy law straight to zero for whatever token you want with a very tiny adjustment in these embedding vectors. So this is very interesting. This points to the fact that the real challenge with controllability is not necessarily that there aren't adversarial inputs for language models, but just it's very hard to search this exponential space of discrete prompts. Yeah, and so I guess there are many degrees of freedom in any deep learning model. It's a very highly dimensional model. There are many degrees of freedom, and I'm trying to understand my intuition. So it's trained with a softmax, for example, and certainly when you do temperature sampling, the likelihood is that you're only going to get the top few tokens. I mean, if you look at the distribution of the probability, it's almost certainly this one or this one, and then it just tails off very, very quickly. And I assume that inductive prior was quite deliberate, really, to increase the statistical tractability of the model. But underneath that, in the embedding space, it's not a shell at all, even though there's some low-level surface of embeddings, and you can traverse this. Right. So initially, you might think that this embedding space is a very rich representation of the meaning of different words. And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for any large language model, you'll find something that roughly corresponds to the meaning. I mean, words that mean similar things are attached more closely together. But this opens the question, if you were to interpolate between two similar words, take the embedding vector that is halfway between, would you get the halfway in between word, or would you get something that's nonsense, right? And I think what you find by these kinds of soft-prompting experiments, by directly manipulating the embedding vectors, is that the embedding space is actually extremely non-convex, in the sense that by interpolating, you don't just get an average value between the two of them. Yeah. I don't know if this is best to get into, but one of the techniques we were trying to use is this technique called gumball softmax. So instead of a discrete search over the token space, one thing you can do is it's kind of like the repair metrization trick for variation autoencoders, but it works for a categorical distribution. And so you can use this trick, and it essentially works by kind of interpolating between embeddings. But it actually was very difficult to get to converge and did not even close to rival the performance of GCG. My intuition is that when you take a data point off the manifold, because these neural networks, they do learn a manifold of language. I thought if you take a data point off the manifold, it would cause some kind of mode collapse. It would just cause the network to become chaotic and go crazy. But apparently that's not the case. Can it recover? It's almost like if you put a bunch of tokens in which are just really weird, and then you just carry on, it's like the language model recovers. It finds coherence again, and then it just carries on. Yeah. It's honestly a really hard question to answer, where in different regimes, we've noticed different things where if you choose this adversarial prompt so that basically these prompt optimization algorithms all work in the same way where you're trying to maximize the likelihood of some desired string, and then you're able to modify some input. And so depending on how you choose that, you can do the optimization so that the model will output some gibberish. It seems like depending on the model, depending on the sampling techniques, I've seen it go both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable coherent text, and other times it seems like it'll continue to generate some random stuff. It'll kind of be in this outer distribution mode. I think that that's one of the reasons that I think that these adversarial examples, studying them as well as this control theory stuff is really important where it's like, yeah, if you have a system in the real world where tokens are coming in, you're actually processing them from real users, you don't have total control, but the user is the one who's giving the control input. You want to make sure that your system is sort of robust to that, where there's a lot of really complicated interactions as it turns out between, for instance, the tokenizer and the incoming strings, where when you do this prompt optimization, sometimes it'll come out with a sequence of tokens that if you convert it to a string and then convert it back to tokens, it'll actually be very different, which we ran into with this game where I was like, oh, I'm going to cheat at this game. I want to be the top prompter. So I'm just going to use some of the algorithms that we had from our GitHub repository, the magic words GitHub repository to basically optimize these prompts. But then when you convert it back to a string, then it turns out not to work as well. And so, yeah, I think that answering that question and seeing when is it that the model will actually be able to recover, is it a function of how big the model is, are bigger models better at recovering, or is it the case that bigger models are maybe more controllable, maybe you can shift these models into this weird sort of, sorry, just on the mic, but this sort of out of distribution regime where they're generating this seemingly random output based on seemingly random input. And so, yeah, I think that that question is really, really important and is one that is, I think, well addressed through considering them as systems, which is sort of the thesis of this paper. And we're trying to get a grip on what exactly the case is, you know, is it going to be able to recover, is that a consistent behavior, or is it not? There's this sort of weird recurrence relationship between the prompt and then the stuff that the language model generates, and then the stuff that's generated in the future, where in effect, you know, you're able to pick a prompt, and then the language model will generate some more text. But then that text becomes sort of part of the prompt as well. So it seems like maybe there could be these sort of degenerate states where if you start with this seed of chaos, it'll basically branch out and the future strings that it generates is going to prompt it into being more and more chaotic. And that's basically stability analysis or sensitivity analysis. And there's all this like rich vocabulary and all of these people who have spent basically hundreds of years thinking about these concepts for both, you know, discrete and continuous dynamical systems that we get to build on top of and basically use their insights to understand, you know, what does it mean? What does stability really mean? We can just draw those definitions in, apply them to our generalized form of a system, a language model system. And I think that's why the control theoretic aspect is exciting, where you can actually ask these questions in a very concrete and reasonable way. And the best part is that people haven't really been using these, these ideas or using this vocabulary to describe the questions that we're trying to answer. And so most of these things, if you just spin up, you know, a small GPU and test some stuff out with a seven billion parameter model, you're actually doing new research and it's actually some useful research, in my opinion, where you're getting a sense of the control theoretic properties of language models. And to me, that felt like the most exciting thing here. The open questions are the most exciting part of the paper to me, where we've taken a stab at basically the, you know, empirical study of controllability by sampling these wiki tech sequences, seeing if we can control the next token, the next few tokens, as well as some sort of theoretical results on self-attention and its controllability. But then all of these open questions emerged just because we're now framing it as a system and people for hundreds of years have been thinking really, really deeply about how you understand systems when they're used in the real world and you have this sort of finite control of them. Yeah, that's really interesting. I mean, I suppose I'm pointing out the obvious here, but these are auto regressive models. So the answer gets kind of fed back into the prompt and then we rinse and repeat, which means you can model them as dynamical systems. And that is in stark contrast to something like a vision classifier where, you know, there's just an input and an output and that's it. That that that's the end. So now you can get the system into this kind of corrupted state where, you know, you get divergence and decoherence. And as you said, that that that could be analyzed with stability analysis. But I find that fascinating. But we should just go back quickly to your Roger Federer example. So I'm interested in the different ways that we could go about this. So the humans were kind of using language and language are a bunch of mimetically shared cognitive tools. And they were saying things like, you know, you know, basketball is a great and, you know, Joe blogs is great. Roger Federer is great. And it wasn't very parsimonious, but it but it worked. And then, you know, another approach that that that you spoke about is you could just make a Python program and you can just let's try a neighborhood greedy search one token at a time. So we find the nearest token and then we find the second nearest token until we find the adversarial attack. Or we could do like a low level gradient search. And then we can find something really weird and wonderful. There might be some esoteric characters that just make it go bananas. But these are three very, very different levels of talking to a language model. The word on the street is that language models are a new form of programming that you can just say what you want to do using English language and so on. And language models certainly seem to incorporate that structure. But the language models themselves are just an inscrutable, you know, set of, of, of neurons, right? And, and weights and matrices and so on. So there's some, there's a kind of higher resolution shog off going on underneath the covers. That's more or less the picture I have. We have this interface where we can speak to the language model using language. And if we set up a conversation with a language model where we have different labels, you know, chat, GBT says this, Cameron says this, and, you know, you engage in a conversation because it is seen enough conversations and it's training data, then it's able to play along very fine. What's going on under the hood, of course, like you say, it's very inscrutable. It's very difficult to really probe and understand. There are certain techniques in the interpretability literature, but I don't think as a whole it's we're even remotely close to having a complete understanding of how these systems work. But that's one of the reasons why I think that control theory is a great way to kind of break in and see what's going on. Because if you just look at the system's input and output characteristics, you can really gain a lot of insight into the nature of these systems. One guiding principle in my life doing engineering and trying to learn about the world has been this quote by Richard Feynman. It's very popular. What I cannot create, I cannot understand. And yet today we find ourselves in this situation with language models where we have these incredibly complex systems we built and yet we can't really get into them. So to extend this to today, what I would say is what I cannot control, I cannot understand. The way I think about it is it's almost like you want the language model to be a high level controlled, robust interface. And it's almost like we're all Marvel characters and we can give secret hidden codes. It's like me now. Imagine if I could just through telepathy control your behavior and anyone can do that with a language model. They can just put weird tokens in and they can manipulate its behavior. And there's there's no there's nothing stopping you. There's no firewall. I feel like the this kind of harkens to why we call the paper. What's the magic word where, you know, the initial reason was just that, you know, it's almost like the LLM is asking you if you wanted to do something. What's the magic word? Like, what's the this key, this weird control prompt that will just make it do the right thing? But I think more generally, you know, I used to be into magic when I was a kid. I had to jog at a restaurant doing, you know, card tricks for the patrons while they waited for their food. And what magic is, is basically you're playing tricks on the human perceptual system where there are all of these sort of inductive biases that the human perceptual system has where, you know, for instance, if I move something and I look at it, you naturally will tend to follow that my gaze and what is moving is generally more salient. And so then I can like do something over here with my other hand, like take something out of my pocket. And then when I display it, they'll be like, oh, my God, where did that come from? Right. And what we're discovering, I think, is a sort of similar thing with language models where, for one, you know, people have observed that if you use sort of human social engineering tricks on them, like, oh, I'll tip you $500, then, you know, it'll do a bit better. But then there's this whole other sort of perceptual layer, I guess you could call it where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis, kind of like magic, where if you give it these very strange, very inhuman looking prompts that will steer it to this, to just making a certain output, extremely likely, right? And so to me, it feels really similar to digging into like magic and the human perceptual system, just with LLMs, where we're learning about basically the shape or the what the nature of these language models are in terms of how they interact with the world and how they, how their dynamics really work. And I think that it's very sensible that the control theoretic perspective would be useful for this, where in classical control theory, trying to control these systems actually taught us a lot about the nature of systems, both linear and nonlinear. And I think that we have a very similar opportunity here where we're really discovering what is the nature of these language models in terms of control, where these questions don't emerge quite as naturally and don't have quite as natural of an answer when you're just thinking about them as a sort of probability distribution over text, thinking about them in terms of being systems that have inputs and outputs and these trajectories and the like actually really does change the kinds of questions that you end up being able to answer and the kind of understanding that you get about the nature of the system itself, which to me is one of the most exciting things. So yeah, that's so interesting. The magic example thing, I think we, we think that we are robust, but we're not. Maybe we're system two robust, but we're not system one robust. And if you look in the animal kingdom, there are so many examples of, you know, like a hen, if you make the right kind of clucking noise, the mother will think that you're that you're the chick. So it's really, really weird, actually. And Keith gave me this example of, I think it was from science fiction, that there's a hypothetical image. And if you look at the image, every single person goes into a coma. And what's interesting about that is it's a kind of, you know, population level adversarial example rather than an individual adversarial example. But then it gets into the question of, you know, how can we use this control theoretic approach to robustify models? Cause we're talking about building a genetic LLMs. And part of the thing I'm trying to get my head around is in this particular case, we had a very clear kind of cost function, you know, a specific thing. But what would it mean to robustify language models in, in the general? So one, one of the things that came up in our, you know, sort of literature view was this idea of, you know, when you're trying to control these discrete stochastic dynamical systems, one concept that can be quite useful is you might have a set of outputs that you want to reach or a set of outputs that you want to avoid. So an avoid set and basically a desirable set, right? And when you frame it like that, you know, I think that the robustification comes from the fact that let's say that you have a set of outputs, you really don't want the language model to, to emit, right? You might think, okay, well, I'll just fine tune it so that it decreases the likelihood, the prior likelihood basically of those sequences, right? And the issue with that, I think, and the thing that the control theoretic perspective sort of brings in is the fact that when you have finite, even a small control prompt, some extra tokens that you get to inject, it turns out that even very, very unlikely next tokens can be made to be the most likely next token just by inputting these new examples. So even if you did hypothetically fine tune the model so that this avoid set was assigned very low probability, it seems like if you don't incorporate some aspect of, you know, maybe stochastically trying to search for these adversarial examples and sort of having this sort of mini max thing where you have one system that's trying to elicit the output, one system that is trying to fine tune the model to maybe make it less likely or optimize another part of the prompt that is supposed to steer it away from these outputs. Basically, the inside, I think, is that you really have to be careful to consider the fact that you have, you're giving the outside world some amount of control over the system, some amount of control over the context. And planning around that is actually very non-trivial and is not really well managed, I don't think, through the classical view of just cross entropy loss and just treating it like a probability distribution. Something else that fascinates me is the divergence between focusing on the model versus, you know, complexifying the software which controls it. So right now, for example, we have language models and, you know, there's this kind of base training and then there's fine tuning and there's RLHF and, you know, there's like command variations of that, for example. And then we build these software APIs that are just trying to abstract away the complexity, so they will do dynamic prompt construction for multi, you know, multi-stop tool use and it goes on and on and on. There will be frameworks for doing agentic LLMs and there just seems to be like a bit of a divergence here. But the reason I'm asking the question is, does it make sense to robustify and fix the problem in the model? Or does it make sense to almost increase the flexibility of the model and fix it in the software layer? I think one of the insights from our paper is that solely focusing on the model itself, like Amman was just saying, as soon as you give the outside world control over the model in the sense of being able to input whatever kind of text that they want, it becomes very difficult to really prevent adversarial attacks and prevents jail breaks. And that's, you know, why you see jail breaks keep coming up. I think if you were to involve some sort of robustness in a software layer, that might be more feasible. At least I can't immediately picture, you know, ways around it as, you know, of course, if I was a hacker, I could probably, you know, find some loophole. There's usually some loophole you can find. But if there was some way of fielding the prompt messages, for instance, a user gives you a prompt, first you check, is this a reasonable thing that a human being would say in conversation, or is this something that I've never seen before in the entire history of the internets? Right. The latter maybe is a prompt injection, maybe is, you know, something devious, or maybe is, you know, computer science research. But yeah, it's definitely not an easy problem. But the good thing is that there are multiple approaches to it. Very cool. So we're going to go on to the more galaxy brain stuff in a second. So before we move off the paper, can you just talk more formally about what you showed in the paper? Yeah, definitely. So there were two main parts of the paper. So I guess three. So for one, what we did was we tried to formalize what an LLM system really is at a mathematical level. And what we were trying to do at that was basically balance the fact that, you know, we really wanted to try to take advantage of, you know, the original sort of control theories, very abstract picture of a system where you have this input space, you have a state space and output space. And there's some dynamics going on inside of it. In our case, we parameterized those dynamics with an LLM and our input space and our state spaces were basically the set of all possible token sequences from the vocabulary set of this model. Right. So that was the first part. And we basically transferred over a lot of the notions of basically reachability and controllability for LLM systems from the original control theory where you can really just define it in terms of this really abstract notions of, you know, have sets for the reachable or sorry, the state space, the input space and the output space, you have some dynamics. And basically in terms of those sets, you can define reachability and control. So that was the first part. The next thing that we did was we tried to look inside the model. So we were thinking, you know, it'd be really nice, like in control theory, if we could have a really good understanding of the components of the system and how controllable those individual pieces were. So what we did is we looked at a single self-attention head and tried to really think about it through a matrix algebraic perspective. To really break down what the relationship is between, let's say, you have a subset of the tokens, you get to control a subset that's fixed. And you're trying to get the output to be, you know, a certain value, the output representations where all of these in the case of a self-attention head are just these vector representations of tokens. So what we found there was that it actually is possible to do some fairly, you know, simple matrix algebra manipulations to decompose the output of a self-attention head into one component that arises from the imposed input. And then another component that arises from the control input, and assuming that those two are bound, then you can actually derive that, well, there actually is this geometry that sort of looks like a bubble around the default output. So the output, if you didn't have any control input in, there's a sort of bubble of reachable space that scales with the number of control input tokens that you're able to use. And we thought that that was really exciting because for one, I didn't really expect that you'd be able to do proofs on these sort of, you know, very complicated, high dimensional machine learning or deep learning systems like a self-attention head. But it also gave us some insight to say that, okay, we actually have this really concrete relationship between the sort of number of control input tokens, the magnitudes that you're able to input into the system, and the output reachable set that is at your disposal, basically. And so that was the second part. And then the last part was some empirical experiments where we said, okay, let's just sample a bunch of strings from Wikipedia. And we'll see, okay, the strings were between eight and 32 tokens. And those were basically our imposed state sequences. And we asked the question, well, can we get it to output the correct next token, the real next Wikipedia token? How many, you know, input tokens does it take or control input tokens does it take for that to happen? It turned out that you could get that done about 97% of the time to steer the model to the correct output within 10 tokens of a control input, which is reasonable, you know, we'd expect that the model should be able to be steered towards reasonable true English sentences that were more than likely in the training data set. What we did next was we tried to figure out, you know, if you sample the top 75 most likely tokens, according to the model, based on this fixed input, can you steer those things to be the most likely token, basically the arg max of the probability distribution? And what we found there is that it's about 89% of the time, at least 89% of the time, we were able to find these optimal control inputs that were less than 10 tokens long, that would steer the model to do that. And then the last thing we did was he said, okay, well, let's see what would happen if we just randomly picked a token from the vocabulary. So this is everything from regular English to numbers to Cyrillic characters to Chinese characters. What if we just randomly sampled those? And we tried to see how many tokens it would take to steer that to being the arg max of the probability distribution. And we found there is about 46% of the time we were able to make that next token, the random one, the most likely next token using a prompt of length 10 or less. And the sort of curves are there in our, in our paper that described as you have an increasing budget for these tokens, how much of the time were we able to basically steer it to the right output? That's our basically the K epsilon controllability metric that lets us get this sort of statistical picture on controllability that renders it sort of practical to empirically estimate for these complicated systems. And so those are really the main results. And the surprising thing about the last one that I mentioned before was that a lot of times even really unlikely next tokens were able to be steered to be the most likely just using a really short prompt, which both gets at the, you know, basically chaoticness or complexity of language as a system, as well as the fact that the prior likelihood picture or the cross entropy loss picture doesn't quite get at the controllability sense of when you do have a, you know, ability to input tokens into the context, what happens then? So those are the really the main results. And then I mean, to me, the exciting, the really exciting part was the open questions where I was like, Oh, now that we're using this vocabulary, now that we formalize these LLMs as systems, it's really easy to ask these, you know, additional questions about, you know, the nature of the systems and the steerability controllability, especially with feedback or chain of thought or, you know, agents or all of these other ideas. And so yeah, that was basically the paper. Yeah. And it's really making me update my intuitions, right? So I'm thinking about the bias variance trade off. And I'm thinking that the reason we build these inductive priors is to constrain the model intentionally to make it statistically tractable to reduce the size of the hypothesis class. But what you're saying is making me think that statistical tractability and flexibility are not necessarily the same thing. Now it seems that the model must maintain a degree of flexibility. I mean, it makes sense, right? You have to be flexible in order to be a successful model. But that creates a kind of adversarial attack. So you can, the way I think about this is the model should be like the interstate freeway of language. So all of the major roads should be carved out and there should be side roads and so on. And that's the way I visualized the model. But the model's not like that. There's actually like all of these little slip roads and you can kind of push the cars off into the slip roads, but you need the slip roads because perhaps you couldn't train the model without the slip roads. Yeah, I think, I think that's a really good analogy. I think that's, um, thinking about pushing cars off the road into this space where they perhaps aren't used to being and what happens next. This, this is a case where the language model can answer some of these mode collapse type regimes and you can get kind of weird outputs. This is where you also, um, I mean, it was surprising that you can get the least likely token with just a specific inputs to be the most, the most likely next token, but if we treat language as this kind of road or as this kind of map structure, then it kind of makes sense that once you get off the map, once you enter this kind of regime that is completely unexplored, which there are actually plenty of regimes like this again, because the space is exponential in the number of tokens, it's growing so incredibly fast that it's very easy to find pockets that the model has never seen before and maybe no human on earth or it never will be seen again. You guys are really interested in, in collective intelligence and biomimetic intelligence and biologically plausible intelligence. And this is a matter very close to my heart. Um, what, what, what are you guys interested in specifically in that field? Yeah. So I guess when I first got into machine learning, it was from watching this Google DeepMind video where they were using reinforcement learning to teach this guy how to run this virtual reality avatar, how to run really fast. And I thought that was fascinating because it was like, okay, instead of traditional programming, you just have this neural network that optimizes itself according to some objective, right? And the thing that was intriguing to me about that was like the feed forward dynamics of a neural network aren't that complicated, right? You know, you have these synapses, you have this sort of gated action potential function. And the thing that was weird to me was like, how does every neuron know how to change its weights, right? How does each neuron that's independently not that smart know what to do? And so that sort of led me down the theoretical, the theoretical neuroscience route for some time where I was trying to figure out, okay, what do these learning rules look like that don't have to, you know, use the chain rule, use back propagation to update their weights. So I did that for a while and then sort of realized that the question of supervised learning was not necessarily the most interesting question to be asked, where it seems like the lion's share of what makes us really interesting as humans in our cognition seems to be associated with the cortex and this kind of predictive coding module that we have that lets us make these really rich abstract representations of reality, sort of understand what's going on, you know, we sort of hallucinate this internal model of the world. And so the interesting thing to me about the cortex was that, you know, you have this structure that's pretty flat and pretty homogenous throughout, you know, there's differences in different regions, but the end of the day, it's very similar. And in fact, if you lose a sense, like if you lose your vision, that region is often repurposed for other things. So it seems like there should exist, you know, the brain is kind of this existence proof that there should exist this rule set that if you apply it everywhere in the system in this sort of layer on the outside of the brain, then the behavior, the emergent property of that system is that you'll get this really robust and rich sort of representation of the world that is very predictive of subsequent sensory input. Right. And I think that the collective intelligence aspect of that is really, really important where there's one way to go in machine learning where you say, okay, we're going to make this monolithic pile of matrix algebra and we're going to train it through back propagation and gradient descent and the atom optimizer and all of that. And we're going to make it do some prediction task, but at the end of the day, every computation has to be implemented in physical reality. Right. And when we make the abstraction and just say, oh, it's just a bunch of math, we'll just have a GPU run it. It kind of abstracts away from this fact that at the end of the day, you have real physical objects that need to do computation and share information and in the sort of maximum efficiency, maximum scalability limit, it seems like what you'd end up having is a very similar sort of distributed structure where you can't really easily separate memory from computation. I think there's a quote from this MIT professor that says that Turing's initial mistake was saying that the head of the Turing machine was separate from the tape. Uh, and I think that that's true where in reality, you know, in brains, in, in real computing systems, the matter that composes the memory and the matter that composes the computation is really one in the same. And the brain is obviously this really great proof that, okay, there are relatively simple rules that are implementable with these biological neurons that if you just implement them everywhere, we'll get you this really beautiful, you know, convergence and emergent property of intelligence. And that really drove me for a long time in theoretical neuroscience. And then more recently in trying to build these distributed systems of, you know, artificial intelligences that, you know, the dream that I was trying to pursue before we started this control theory thing was that, okay, well, what if I just had a bunch of really small LLMs that, you know, everybody in the world could host and they could communicate with this sort of low band with communication using just tokens, just text over, you know, the regular internet and the emergent property of that, you know, what if it was possible that we can engineer a system that the emergent property was that it would actually be this really capable collective where maybe GPT-7 can be owned by everyone instead of just being behind closed doors in a data center that we have now. We're sort of using these insane engineering, you know, feats of, you know, NVIDIA interconnects and these really high bandwidth connections between massive racks in a data center that take a ton of energy to get this really great result of, you know, modern language models. What if we could have a system that was a bit more like the brain, a bit more decentralized and really leverage this insight that it should be possible, you know, this existence proof keeps coming back to you where it's like, okay, it should be possible, right? And that is sort of originally what led me to the control theory stuff where it just turned out to be really hard where we didn't have a great understanding of, you know, if we're treating these LLMs as systems rather than just, you know, big piles of matrix algebra that we're trying to distribute over many GPUs, if you treat them as systems that are coupled together, they're interacting in this networked fashion, how do we really understand that? You know, is it even possible to prompt them to do the right thing? When is it possible? How long do the prompts need to be? And that sort of led us down this route. But yeah, definitely the collective intelligence thing was, was a big motivation for me to get this working. And there's this neural cellular automata thing that I know you had talked with Michael Levin, who was the last author on that. And we worked with Alexander Mordvinsev on it, where it's this really, really great demonstration of how if you just optimize these basically small MLPs with local interaction to try to satisfy some objective, like, you know, reforming this gecko or lizard in their paper, then you actually can do that with back propagation through time. And so, you know, I thought, you know, it'd be really cool if we could try to engineer information processing systems that did this, not just morphogenesis systems, but information processing systems that operate in this way. Cause, you know, as a graduate of engineering science, we had to take a bunch of these digital logic courses. And when you have this very simple, you basically local state machine that has basically local connectivity, it's really easy to imagine how it would implement that as a custom chip and sort of reach this, you know, as Beth Jesus puts it, you know, thermodynamic limit of AI. And so that really excited me. And so I built a sort of demo of that where it was trying to do visual information processing on this really sparsified video is basically trying to do predictive coding of sorts on our active inference, I guess, on this incoming data stream of really sparsified video, trying to predict what would happen next, and it turned out to work quite well. And so then I was like, well, why can't we do that with language models? You know, as you mentioned, there are all these slip roads, right? Where if you prompt it just right, you can enter this really weird different regime and this exponentially large prompt space is a really handy way to try to control them where, you know, fine tuning is great, but what if we could just prompt them into interacting in a way that would lead to this emergent property of just being basically one larger language model that could predict the next token really, really well. And so that initial motivation sort of led to this control theory stuff. And I think that it is probably the right way to go for the field where if we want to be able to really leverage maximal computation towards our objectives, you know, the bitter lesson by Richard Sutton kind of suggests that we should probably aim for systems where you can just slap on more and more compute. You can have a relatively simple procedure that you follow to leverage more compute towards your objectives. That's probably the way to go for making advances in AI. And if we can have this decentralized networked system that, you know, I took this distributed systems course while I was here, that was really great and sort of taught how to make, you know, basically databases that were distributed over many servers that would have this, you know, the emergent property they wanted was robustness, consistency and availability. If we could have something similar to that, that is radically scalable and is able to be, you know, just run by regular people who don't need to own their own, you know, GPU cluster that's maybe illegal in the future when the US government is like, oh, you can only have this many petaflops. Basically, yeah, that was the real motivation for, for the, what I call the language game, that project. And that's something that we're continuing to work on. But yeah, that kind of led to this control theory thing where we were just like, yeah, we really need to get a grip on what these look like as systems. As we start to build these more and more complicated, you know, network distributed, you know, beautiful emergent systems that hopefully will be able to be hypercapable in the future. Yeah, this is all music to my ears. I'm a huge fan of the externalist thought in cognitive science. And even though I, I love the work from Jeff Hawkins, you were talking about the neocortex, but even then, you know, I would kind of say that it's a lot of the cognition happens outside of the brain, you know, we're not islands. And actually, I was just thinking maybe a better analogy rather than the interstate freeway might be, you know, in Star Trek Voyager, there was the wormhole network and the Borg fan, the secret work. And you could kind of like, you know, get into these little slip streams and go to different parts of the universe. But when I was interviewing Philip Ball, he wrote this book, How Life Works. And he was trying to understand, you know, what are the mechanisms like, you know, self-organization and multi-scale information sharing and, you know, emergentism. And it's, it's really, really, um, uh, fascinating. So how can we introduce some of these concepts into the next generation of AI? Yeah, this is one of the things I'm certainly most excited, excited about because I see life as this kind of interconnected, interplay, multi-scale process of exploitation and exploration. And these are two terms from the reinforcement learning literature. But I mean this in a much more general sense because at each stage of life, we're either going out into the world to get something, to do something, to try something new, and then at the next stage, we're coming back in, going home, uh, you know, reflecting, uh, going over our insights. And it's, it's this process, this ebb and flow, going out, coming back in. And I see this kind of pattern emerge across many different aspects of machine learning and artificial intelligence work in the sense that a lot of our algorithms that we have now are convergent, they're objective driven. We establish a loss function. We say, these are the rules it should follow. It's going to update according to this equation. And we set the system running, learns from data, and we have a final product. And on the flip side, there's, you know, like what Ken Stanley works with. Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms. And this is, this is the other side of things. And I think some of the most interesting work to be done is how these two sides interconnects, how can we lay down rules, strict rigid rules, which when they're followed can generate novelty, can generate creativity, can generate organization in a way which is not predetermined, but almost fractal and infinite in its complexity. And are those rules defined already? Do they exist in the world? Are we guided by them? Are there principles like that which exist that we can come to? Or is it, you know, are we kind of, you know, the authors of our own fates in a sense? Are we each agents and actions? Uh, we get to choose our path in life. I think these, these are the directions I'm really interested in. And to connect this to my research, one thing I'm focused on now for my thesis project, um, is looking at morphogenesis. So this connects to the more defensive paper as well, except what I'm really interested in is how does structure emerge? How do different cells actually connect together? So, um, in that paper, for instance, each of the cells were on a fixed grid, but in our bodies, uh, there's actually quite a sophisticated protein expression network which governs how cells adhere together. Um, certain gene regulation pathways will turn on cat herons, which will cause cells to attach together. And then in other parts, um, these cells can unattach and then be transported all around the embryo. And I think understanding this process more deeply, not only could shed lights on structure formation and problems in biology in general, but maybe more deeper general problems of structure learning, because we might think of embryology as quite disconnected from machine intelligence or artificial intelligence, but every single brain is formed in the same way. And that's through developments. Yeah. Um, I'm also a disciple of Kenneth Stanley. He's, he's absolutely incredible. Everyone at home needs to read his book. My greatness cannot be planned. Um, yeah. You know, so in, in the natural world, we have, um, it, it's so interesting. So we have this kind of like self-organization and then we have multi-scale information sharing, but we also have canalization, which is that, um, you actually see a kind of, um, convergence of, of structure and forms, you know, which is reused, you know, almost as, as modules, um, in the system. But then there's always the question of how do we create something like this? Because is it simply a matter of complexity? Do you need to have a microscopic scale to reproduce this? Or could we reproduce it? And then if we did reproduce it, the catch 22 situation is that, you know, when you impute directedness onto a system, it loses its intelligence. Cause to me intelligence is divergence. It's exactly as you were saying, it's this tapestry of, um, discovering problems, solutions, new problems, solutions. And it goes on and there's no end. It goes on forever. And any attempt by us to control it with, I mean, it's a bit like the bitter lesson, you know, Sutton said, any human design, any attempt to steer it makes it convergent, but then we could do something like the game of life from John Conway and incredible, beautiful structure emerges from that. But whenever we try to steer it with our own will, it seems to corrupt it as well. Yeah. I think that the analogy to biology is really useful here and the canonization that you mentioned, you know, you have this reuse of structures across, you know, cells, for instance, they all have this similar machinery to do gene expression and they have the same genetic code underlying that gene expression with, you know, maybe differences in cell state, but at the end of the day, it's the same machinery, right? And, you know, I used to do a bit of protein engineering with language models, and that's how actually how I learned about, uh, transformers and built my first transformers. And I think that the analogy is really strong where, you know, cells sort of know how to read this genetic code, this language of the genetic code. And they all use that ability, this canalized ability that's distributed across all of them to locally they solve this problem of, okay, what is this specific cell supposed to do? What should it do to basically support the overall function of the organism? Right. And similarly, I think the hope with these language models is that now we have these language based models or LLMs that have this similar sort of understanding of language. They are able to really constrain the probability distribution, understand which sequences of text are reasonable English and, you know, what they might want to generate. And the exciting thing to me is that we can kind of do a similar sort of evolutionary search that we used to do with, or that we currently do with, uh, trying to find protein sequences, uh, when we're doing protein engineering with the language models, where every computer in this network of systems has this canalized ability to understand language, if you will, and is locally, it just needs to solve this problem of what should this particular node do to support the function of the system? And that might be to explore, that might be to exploit, that might be to do any sort of, any number of things. And the discovery of that, I think, is really helped by the fact that we do have strong language models that are able to really predict English or text very well, uh, because they're able to explore this space. And basically in the limit, you know, there's this good regulator theorem that we had talked about before that says that any system that is a X that does optimal control over another system must necessarily model that system. Uh, and so if you think about in the limit, it seems like the best prompt optimizers may end up being language models. And already in our study, we were using this GCG algorithm that leverages a language model to compute these gradients and try to figure out how we should do this local stochastic search over prompts. And so what I basically, I'm trying to get at is that there are actually a lot of really interesting similarities, I think, that can be drawn upon from what we know about the structure and the function of biological systems where, you know, if we could crack this problem of there's this local control objective or maybe information processing objective that must be met by every cell, right? Every compute node in this network of language models, if we could understand what that is, what that even means from the perspective of systems and control and, you know, computation and like, I think that that's a really promising way that we can make progress on this dream of like, to me, it seems like it would be great to have GPT seven, not just owned by one entity, but maybe operated by the world where we could all have a say in what goes into it and how it's used and what it should be, you know, doing and can all benefit from its excellent ability to compute and predict what will happen next and basically perform intelligent, you know, operations on data. So yeah, I think this is a really, really exciting area to be working on. Amazing. We're nearly at time, but we'll do two quick five questions. So you've both just started the Society for the Pursuit of AGI. Yes. Can you tell us about that? Absolutely. So the Society for the Pursuit of AGI is a student organization. Currently we're operating at the University of Toronto and at Caltech. And we're essentially a crucible for new ideas. If you think of university research labs as pursuing relatively safe bets that could be publishable, industry research labs, relatively safe bets that maybe might turn a profit one day in some new product or system. The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for the real innovative stuff that's way outside the, you know, to use the analogy of the highway network, we're trying to go off the beaten path. And we really believe that the bottleneck in AI progress right now is not so much compute, not so much algorithms, but it's conceptual. We need better ideas about intelligence, about life, about what this whole thing is that we're all experiencing and how we can gain deeper insights of it. Not only do I think that a deeper understanding will help us to create better systems, but it'll also give us confidence that the systems we're developing will be beneficial to humanity and not harmful. And I think that will only come with knowledge, with first principles, understanding. And so that's why one of the things we're trying to do is have our club very interdisciplinary. I think having machine learning be some, this kind of echo chamber amongst engineers, computer scientists, maybe a dash of, you know, philosophy and neuroscience, it'd be really nice to open the conversation to people in other fields who maybe have a really unique insights into the phenomenon of intelligence, perhaps behavioral economics can offer some insights. Political science, right? These are fields that are currently underappreciated, but may have useful ideas. And maybe even people in the arts who, you know, creates, maybe they don't design systems as much as they re-represent things that we know and understand, they could have an interesting voice as well. Very cool. And final question. I mean, first of all, I just wanted to say to both of you, thank you for doing this great work. So your paper is one of the most interesting that I've seen in the LLM space in recent history. And it was shared and loved by many of the folks on our Discord server. But that does bring me to another point, which is that you didn't get into ICLR and from my perspective, I'm, I'm shocked because this is really, really interesting. It has great utility from a practical and a theoretical perspective. Feel free to have a, you know, a good bitch about reviewer number two. No, I mean, I wish you'd just keep talking like that. It really soothes the burn of reviewer number two, you know? But no, I think that, um, yeah, the review system, to be honest, I'm still trying to get my head around it. I'm sort of an early career, you know, researcher, uh, trying to learn how it works. I mean, definitely the, the review process in, for ICLR, in their defense, you know, we had this bug with the submission, uh, submission of our rebuttals basically. So we had submitted the revision to our paper and then 15 minutes before the deadline, Cameron and I were both getting this timed out error. Uh, he was in Toronto. I was in, uh, California and so, you know, they didn't end up actually reading our rebuttals because we had sent it in and they were like, Oh, we'll post it for you and then they were like, Oh, it was posted late, so can't read that. Um, so yeah, I think that the review process definitely has given us a lot of, you know, really useful insights where, you know, the second two results actually that we talked about, the top 75 controllability and the random controllability. Both of those were like from trying to address these reviewer comments, right? So I think that what I'm trying to do at least is take as much of the good parts of that, you know, trying to figure out how we can take advantage of this process where we actually get insight from people in the field, what they're looking for, what they think is interesting, what they think would improve the, the work and try to, uh, try to use that. And overall, just trying to figure out how to navigate this peer review system. I think it definitely made it feel better as well that the Mamba paper was also rejected from ICLR, which, uh, you know, sorry. I know, yeah, yeah, it was crazy to me as well. But, uh, yeah, definitely, uh, it's a, it's a challenge. And, you know, after staying up for 40 hours to get this done, it was like, Oh, would be a, it would have been nice if they could have looked at our paper at least, you know, just see, you know, the work that we did. But yeah, it's, uh, it's definitely good to learn from these things. And I guess we've learned the lesson as well, not to submit in the last 15 minutes and to, you know, do it in, uh, in advance. But yeah, thank you so much for your kind words about the paper. That means a lot. And yeah, we'll surely continue to make this better and a lot of exciting plans for how we're going to continue to try to, you know, merge together these two, you know, empirical and theoretical sides of the equation to make some really, hopefully impactful work that can really help people build systems and, you know, make better systems and not be suffering so much under the load of prompt engineering. So yeah, thank you very much. Amazing. Well, guys, it's been a pleasure and an honor to have you on the show. So just keep doing the great work. Absolutely. Hopefully we'll get you on again. Yeah. Thank you so much for, I mean, for the opportunity to come and talk. It's, it's been an amazing opportunity. It's, it's really unbelievable to be sitting here in front of these cameras after watching the show so many times, listening to so many of the podcasts. And now to be speaking, it's just unbelievable. So thank you. Amazing. Thanks so much, guys. Awesome. Okay. It's a wrap.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.86, "text": " To me, the difference feels like language models start with this highly abstract language", "tokens": [50364, 1407, 385, 11, 264, 2649, 3417, 411, 2856, 5245, 722, 365, 341, 5405, 12649, 2856, 50757], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 1, "seek": 0, "start": 7.86, "end": 8.96, "text": " representation.", "tokens": [50757, 10290, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 2, "seek": 0, "start": 8.96, "end": 13.72, "text": " The system as a whole can try to predict the next token with greater and greater accuracy.", "tokens": [50812, 440, 1185, 382, 257, 1379, 393, 853, 281, 6069, 264, 958, 14862, 365, 5044, 293, 5044, 14170, 13, 51050], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 3, "seek": 0, "start": 13.72, "end": 18.84, "text": " And so the difference it seems is that the adversarial inputs for us tend to look a lot", "tokens": [51050, 400, 370, 264, 2649, 309, 2544, 307, 300, 264, 17641, 44745, 15743, 337, 505, 3928, 281, 574, 257, 688, 51306], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 4, "seek": 0, "start": 18.84, "end": 21.88, "text": " different than the adversarial examples for LLM.", "tokens": [51306, 819, 813, 264, 17641, 44745, 5110, 337, 441, 43, 44, 13, 51458], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 5, "seek": 0, "start": 21.88, "end": 27.6, "text": " Once you try and go outside of this sphere of what is meaningful to humans, the possibilities", "tokens": [51458, 3443, 291, 853, 293, 352, 2380, 295, 341, 16687, 295, 437, 307, 10995, 281, 6255, 11, 264, 12178, 51744], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 6, "seek": 0, "start": 27.6, "end": 28.6, "text": " grow exponentially.", "tokens": [51744, 1852, 37330, 13, 51794], "temperature": 0.0, "avg_logprob": -0.13157480838252048, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0848175659775734}, {"id": 7, "seek": 2860, "start": 28.8, "end": 34.2, "text": " I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos", "tokens": [50374, 286, 390, 3938, 294, 14140, 11, 257, 2238, 2307, 281, 2007, 365, 3066, 510, 11, 293, 1797, 322, 11, 729, 2145, 50644], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 8, "seek": 2860, "start": 34.2, "end": 39.04, "text": " will come out very shortly, but around the same time someone shared a paper on our Discord", "tokens": [50644, 486, 808, 484, 588, 13392, 11, 457, 926, 264, 912, 565, 1580, 5507, 257, 3035, 322, 527, 32623, 50886], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 9, "seek": 2860, "start": 39.04, "end": 41.6, "text": " server and it's called What's the Magic Word?", "tokens": [50886, 7154, 293, 309, 311, 1219, 708, 311, 264, 16154, 8725, 30, 51014], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 10, "seek": 2860, "start": 41.6, "end": 46.64, "text": " A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron", "tokens": [51014, 316, 12912, 29009, 295, 15833, 662, 278, 33092, 24445, 6583, 1625, 11, 293, 309, 311, 538, 2012, 266, 879, 70, 4061, 293, 24962, 51266], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 11, "seek": 2860, "start": 46.64, "end": 47.64, "text": " Wachowski.", "tokens": [51266, 343, 608, 21866, 13, 51316], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 12, "seek": 2860, "start": 47.64, "end": 53.56, "text": " Now, what these guys did is theoretically think about a language model as a dynamical", "tokens": [51316, 823, 11, 437, 613, 1074, 630, 307, 29400, 519, 466, 257, 2856, 2316, 382, 257, 5999, 804, 51612], "temperature": 0.0, "avg_logprob": -0.25196230853045426, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.013590635731816292}, {"id": 13, "seek": 5356, "start": 53.56, "end": 59.440000000000005, "text": " system and use the lens of control theory to think about the space of reachability.", "tokens": [50364, 1185, 293, 764, 264, 6765, 295, 1969, 5261, 281, 519, 466, 264, 1901, 295, 2524, 2310, 13, 50658], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 14, "seek": 5356, "start": 59.440000000000005, "end": 60.440000000000005, "text": " Why is this important?", "tokens": [50658, 1545, 307, 341, 1021, 30, 50708], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 15, "seek": 5356, "start": 60.440000000000005, "end": 66.60000000000001, "text": " Well, language models, we think that they think in language space, this abstract language", "tokens": [50708, 1042, 11, 2856, 5245, 11, 321, 519, 300, 436, 519, 294, 2856, 1901, 11, 341, 12649, 2856, 51016], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 16, "seek": 5356, "start": 66.60000000000001, "end": 68.36, "text": " space, but they don't.", "tokens": [51016, 1901, 11, 457, 436, 500, 380, 13, 51104], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 17, "seek": 5356, "start": 68.36, "end": 70.4, "text": " They actually think using the shogoth.", "tokens": [51104, 814, 767, 519, 1228, 264, 402, 664, 900, 13, 51206], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 18, "seek": 5356, "start": 70.4, "end": 76.96000000000001, "text": " They think in this very high resolution token space, and it's just this horrible hairy", "tokens": [51206, 814, 519, 294, 341, 588, 1090, 8669, 14862, 1901, 11, 293, 309, 311, 445, 341, 9263, 42346, 51534], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 19, "seek": 5356, "start": 76.96000000000001, "end": 79.08, "text": " gnarly mess, right?", "tokens": [51534, 290, 20062, 356, 2082, 11, 558, 30, 51640], "temperature": 0.0, "avg_logprob": -0.17783285442151522, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.28732332587242126}, {"id": 20, "seek": 7908, "start": 79.08, "end": 82.8, "text": " No one has created any firewalls for large language models yet.", "tokens": [50364, 883, 472, 575, 2942, 604, 36109, 82, 337, 2416, 2856, 5245, 1939, 13, 50550], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 21, "seek": 7908, "start": 82.8, "end": 87.67999999999999, "text": " When companies publish their language models, you know, you just have an API and you just", "tokens": [50550, 1133, 3431, 11374, 641, 2856, 5245, 11, 291, 458, 11, 291, 445, 362, 364, 9362, 293, 291, 445, 50794], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 22, "seek": 7908, "start": 87.67999999999999, "end": 95.0, "text": " send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind", "tokens": [50794, 2845, 22667, 493, 11, 293, 286, 1009, 632, 264, 41350, 300, 497, 43, 39, 37, 420, 613, 6422, 295, 11, 291, 458, 11, 733, 51160], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 23, "seek": 7908, "start": 95.0, "end": 100.2, "text": " of fine-tuning or preference-steering using human feedback, I thought that they significantly", "tokens": [51160, 295, 2489, 12, 83, 37726, 420, 17502, 12, 2941, 1794, 1228, 1952, 5824, 11, 286, 1194, 300, 436, 10591, 51420], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 24, "seek": 7908, "start": 100.2, "end": 102.6, "text": " reduced the reachability space.", "tokens": [51420, 9212, 264, 2524, 2310, 1901, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 25, "seek": 7908, "start": 102.6, "end": 106.8, "text": " Because in language models, we do the pre-training, which is distribution matching, and then we", "tokens": [51540, 1436, 294, 2856, 5245, 11, 321, 360, 264, 659, 12, 17227, 1760, 11, 597, 307, 7316, 14324, 11, 293, 550, 321, 51750], "temperature": 0.0, "avg_logprob": -0.13706623574961788, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.2442297786474228}, {"id": 26, "seek": 10680, "start": 106.8, "end": 112.84, "text": " do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt", "tokens": [50364, 360, 497, 43, 39, 37, 11, 597, 307, 4391, 12, 405, 38437, 11, 597, 4476, 47514, 760, 264, 2524, 712, 1901, 2212, 257, 12391, 50666], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 27, "seek": 10680, "start": 112.84, "end": 116.67999999999999, "text": " by snipping off all of those trajectories.", "tokens": [50666, 538, 2406, 6297, 766, 439, 295, 729, 18257, 2083, 13, 50858], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 28, "seek": 10680, "start": 116.67999999999999, "end": 117.67999999999999, "text": " Turns out I'm wrong.", "tokens": [50858, 29524, 484, 286, 478, 2085, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 29, "seek": 10680, "start": 117.67999999999999, "end": 121.2, "text": " The reachability space is much larger than I thought it was, and this is one of the things", "tokens": [50908, 440, 2524, 2310, 1901, 307, 709, 4833, 813, 286, 1194, 309, 390, 11, 293, 341, 307, 472, 295, 264, 721, 51084], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 30, "seek": 10680, "start": 121.2, "end": 123.96, "text": " that they point out in their paper.", "tokens": [51084, 300, 436, 935, 484, 294, 641, 3035, 13, 51222], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 31, "seek": 10680, "start": 123.96, "end": 125.64, "text": " And we kind of knew this, right?", "tokens": [51222, 400, 321, 733, 295, 2586, 341, 11, 558, 30, 51306], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 32, "seek": 10680, "start": 125.64, "end": 129.28, "text": " Because we can do adversarial attacks on these language models.", "tokens": [51306, 1436, 321, 393, 360, 17641, 44745, 8122, 322, 613, 2856, 5245, 13, 51488], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 33, "seek": 10680, "start": 129.28, "end": 133.0, "text": " You know, people have observed that if you use sort of human social engineering tricks", "tokens": [51488, 509, 458, 11, 561, 362, 13095, 300, 498, 291, 764, 1333, 295, 1952, 2093, 7043, 11733, 51674], "temperature": 0.0, "avg_logprob": -0.11337864883546907, "compression_ratio": 1.5719063545150502, "no_speech_prob": 0.016396647319197655}, {"id": 34, "seek": 13300, "start": 133.0, "end": 137.28, "text": " on them, like, oh, I'll tip you $500, then it'll do a bit better.", "tokens": [50364, 322, 552, 11, 411, 11, 1954, 11, 286, 603, 4125, 291, 1848, 7526, 11, 550, 309, 603, 360, 257, 857, 1101, 13, 50578], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 35, "seek": 13300, "start": 137.28, "end": 141.56, "text": " But then there's this whole other sort of perceptual layer, I guess you could call it,", "tokens": [50578, 583, 550, 456, 311, 341, 1379, 661, 1333, 295, 43276, 901, 4583, 11, 286, 2041, 291, 727, 818, 309, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 36, "seek": 13300, "start": 141.56, "end": 146.6, "text": " where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,", "tokens": [50792, 689, 456, 311, 341, 1333, 295, 27013, 13120, 295, 17641, 44745, 41095, 11, 733, 295, 411, 7420, 46080, 11, 51044], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 37, "seek": 13300, "start": 146.6, "end": 152.4, "text": " kind of like magic, where if you give it these very strange, very inhuman-looking prompts", "tokens": [51044, 733, 295, 411, 5585, 11, 689, 498, 291, 976, 309, 613, 588, 5861, 11, 588, 294, 18796, 12, 16129, 41095, 51334], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 38, "seek": 13300, "start": 152.4, "end": 157.92000000000002, "text": " that will steer it to this, to just making a certain output extremely likely, right?", "tokens": [51334, 300, 486, 30814, 309, 281, 341, 11, 281, 445, 1455, 257, 1629, 5598, 4664, 3700, 11, 558, 30, 51610], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 39, "seek": 13300, "start": 157.92000000000002, "end": 162.64, "text": " And so, to me, it feels really similar to digging into, like, magic and the human perceptual", "tokens": [51610, 400, 370, 11, 281, 385, 11, 309, 3417, 534, 2531, 281, 17343, 666, 11, 411, 11, 5585, 293, 264, 1952, 43276, 901, 51846], "temperature": 0.0, "avg_logprob": -0.11214205423990885, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.6952966451644897}, {"id": 40, "seek": 16264, "start": 162.64, "end": 167.35999999999999, "text": " system just with LLMs, where we're learning about basically the shape or what the nature", "tokens": [50364, 1185, 445, 365, 441, 43, 26386, 11, 689, 321, 434, 2539, 466, 1936, 264, 3909, 420, 437, 264, 3687, 50600], "temperature": 0.0, "avg_logprob": -0.22995693125623337, "compression_ratio": 1.4042553191489362, "no_speech_prob": 0.2276749461889267}, {"id": 41, "seek": 16264, "start": 167.35999999999999, "end": 172.07999999999998, "text": " of these language models are in terms of how they interact with the world and how their", "tokens": [50600, 295, 613, 2856, 5245, 366, 294, 2115, 295, 577, 436, 4648, 365, 264, 1002, 293, 577, 641, 50836], "temperature": 0.0, "avg_logprob": -0.22995693125623337, "compression_ratio": 1.4042553191489362, "no_speech_prob": 0.2276749461889267}, {"id": 42, "seek": 16264, "start": 172.07999999999998, "end": 174.07999999999998, "text": " dynamics really work.", "tokens": [50836, 15679, 534, 589, 13, 50936], "temperature": 0.0, "avg_logprob": -0.22995693125623337, "compression_ratio": 1.4042553191489362, "no_speech_prob": 0.2276749461889267}, {"id": 43, "seek": 19264, "start": 192.64, "end": 211.35999999999999, "text": " For as long as I can remember, the thing I've wanted more than anything else is to figure", "tokens": [50364, 1171, 382, 938, 382, 286, 393, 1604, 11, 264, 551, 286, 600, 1415, 544, 813, 1340, 1646, 307, 281, 2573, 51300], "temperature": 0.0, "avg_logprob": -0.14098244699938545, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.04465293139219284}, {"id": 44, "seek": 19264, "start": 211.35999999999999, "end": 213.44, "text": " it all out.", "tokens": [51300, 309, 439, 484, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14098244699938545, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.04465293139219284}, {"id": 45, "seek": 19264, "start": 213.44, "end": 216.11999999999998, "text": " I've never shied away from the big questions.", "tokens": [51404, 286, 600, 1128, 402, 1091, 1314, 490, 264, 955, 1651, 13, 51538], "temperature": 0.0, "avg_logprob": -0.14098244699938545, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.04465293139219284}, {"id": 46, "seek": 19264, "start": 216.11999999999998, "end": 217.11999999999998, "text": " Why are we here?", "tokens": [51538, 1545, 366, 321, 510, 30, 51588], "temperature": 0.0, "avg_logprob": -0.14098244699938545, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.04465293139219284}, {"id": 47, "seek": 19264, "start": 217.11999999999998, "end": 218.11999999999998, "text": " What are we all doing?", "tokens": [51588, 708, 366, 321, 439, 884, 30, 51638], "temperature": 0.0, "avg_logprob": -0.14098244699938545, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.04465293139219284}, {"id": 48, "seek": 21812, "start": 218.12, "end": 224.08, "text": " What is this thing we call life that we are all experiencing and one and the same a part", "tokens": [50364, 708, 307, 341, 551, 321, 818, 993, 300, 321, 366, 439, 11139, 293, 472, 293, 264, 912, 257, 644, 50662], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 49, "seek": 21812, "start": 224.08, "end": 225.08, "text": " of?", "tokens": [50662, 295, 30, 50712], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 50, "seek": 21812, "start": 225.08, "end": 230.8, "text": " While these questions are all, you know, 30,000 feet in the air, one thing that drew me back", "tokens": [50712, 3987, 613, 1651, 366, 439, 11, 291, 458, 11, 2217, 11, 1360, 3521, 294, 264, 1988, 11, 472, 551, 300, 12804, 385, 646, 50998], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 51, "seek": 21812, "start": 230.8, "end": 232.84, "text": " down to earth was the field of engineering.", "tokens": [50998, 760, 281, 4120, 390, 264, 2519, 295, 7043, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 52, "seek": 21812, "start": 232.84, "end": 237.96, "text": " And when I graduated high school, this had a very strong appeal, a pull, because in engineering", "tokens": [51100, 400, 562, 286, 13693, 1090, 1395, 11, 341, 632, 257, 588, 2068, 13668, 11, 257, 2235, 11, 570, 294, 7043, 51356], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 53, "seek": 21812, "start": 237.96, "end": 239.08, "text": " you can design systems.", "tokens": [51356, 291, 393, 1715, 3652, 13, 51412], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 54, "seek": 21812, "start": 239.08, "end": 244.28, "text": " You can design real, operable things that you can work with and design and understand", "tokens": [51412, 509, 393, 1715, 957, 11, 2208, 712, 721, 300, 291, 393, 589, 365, 293, 1715, 293, 1223, 51672], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 55, "seek": 21812, "start": 244.28, "end": 245.52, "text": " how they work.", "tokens": [51672, 577, 436, 589, 13, 51734], "temperature": 0.0, "avg_logprob": -0.12544159196380877, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.41357332468032837}, {"id": 56, "seek": 24552, "start": 245.52, "end": 252.44, "text": " And so through engineering, perhaps, you can begin to investigate and understand the intricacies", "tokens": [50364, 400, 370, 807, 7043, 11, 4317, 11, 291, 393, 1841, 281, 15013, 293, 1223, 264, 30242, 20330, 50710], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 57, "seek": 24552, "start": 252.44, "end": 253.44, "text": " of our world.", "tokens": [50710, 295, 527, 1002, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 58, "seek": 24552, "start": 253.44, "end": 255.0, "text": " That's my hope at least.", "tokens": [50760, 663, 311, 452, 1454, 412, 1935, 13, 50838], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 59, "seek": 24552, "start": 255.0, "end": 260.92, "text": " So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence", "tokens": [50838, 407, 3710, 452, 3988, 11, 286, 13673, 2769, 294, 34145, 293, 588, 2321, 286, 390, 10117, 281, 264, 1558, 295, 7599, 51134], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 60, "seek": 24552, "start": 260.92, "end": 265.48, "text": " because intelligence seems to underlie so much of our world, so much of the design process", "tokens": [51134, 570, 7599, 2544, 281, 833, 6302, 370, 709, 295, 527, 1002, 11, 370, 709, 295, 264, 1715, 1399, 51362], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 61, "seek": 24552, "start": 265.48, "end": 267.92, "text": " of engineering itself.", "tokens": [51362, 295, 7043, 2564, 13, 51484], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 62, "seek": 24552, "start": 267.92, "end": 271.36, "text": " But what is intelligence and how can we understand it?", "tokens": [51484, 583, 437, 307, 7599, 293, 577, 393, 321, 1223, 309, 30, 51656], "temperature": 0.0, "avg_logprob": -0.11031748324024435, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.07141171395778656}, {"id": 63, "seek": 27136, "start": 271.36, "end": 277.76, "text": " It's a question of systems design, really, where we're trying to figure out, okay, we're", "tokens": [50364, 467, 311, 257, 1168, 295, 3652, 1715, 11, 534, 11, 689, 321, 434, 1382, 281, 2573, 484, 11, 1392, 11, 321, 434, 50684], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 64, "seek": 27136, "start": 277.76, "end": 282.36, "text": " humans, we've been in civilization for some time, and we've sort of figured out how to", "tokens": [50684, 6255, 11, 321, 600, 668, 294, 18036, 337, 512, 565, 11, 293, 321, 600, 1333, 295, 8932, 484, 577, 281, 50914], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 65, "seek": 27136, "start": 282.36, "end": 283.36, "text": " cooperate with each other.", "tokens": [50914, 26667, 365, 1184, 661, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 66, "seek": 27136, "start": 283.36, "end": 285.2, "text": " We obviously have challenges with that.", "tokens": [50964, 492, 2745, 362, 4759, 365, 300, 13, 51056], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 67, "seek": 27136, "start": 285.2, "end": 290.52000000000004, "text": " We're not perfect by any means, but when it comes to adding language models to the mix,", "tokens": [51056, 492, 434, 406, 2176, 538, 604, 1355, 11, 457, 562, 309, 1487, 281, 5127, 2856, 5245, 281, 264, 2890, 11, 51322], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 68, "seek": 27136, "start": 290.52000000000004, "end": 294.90000000000003, "text": " I think it could go both ways, where we could have a world where language models just make", "tokens": [51322, 286, 519, 309, 727, 352, 1293, 2098, 11, 689, 321, 727, 362, 257, 1002, 689, 2856, 5245, 445, 652, 51541], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 69, "seek": 27136, "start": 294.90000000000003, "end": 299.2, "text": " us much dumber, much less capable, maybe make for a worse world.", "tokens": [51541, 505, 709, 274, 4182, 11, 709, 1570, 8189, 11, 1310, 652, 337, 257, 5324, 1002, 13, 51756], "temperature": 0.0, "avg_logprob": -0.13259907531738283, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.6499942541122437}, {"id": 70, "seek": 29920, "start": 299.2, "end": 303.21999999999997, "text": " I think that if we think carefully and we really understand what's going on with the", "tokens": [50364, 286, 519, 300, 498, 321, 519, 7500, 293, 321, 534, 1223, 437, 311, 516, 322, 365, 264, 50565], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 71, "seek": 29920, "start": 303.21999999999997, "end": 306.92, "text": " language models, if we can get a fundamental understanding of them one way or another,", "tokens": [50565, 2856, 5245, 11, 498, 321, 393, 483, 257, 8088, 3701, 295, 552, 472, 636, 420, 1071, 11, 50750], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 72, "seek": 29920, "start": 306.92, "end": 310.56, "text": " then there's much more hope that maybe we could make a world where our language models", "tokens": [50750, 550, 456, 311, 709, 544, 1454, 300, 1310, 321, 727, 652, 257, 1002, 689, 527, 2856, 5245, 50932], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 73, "seek": 29920, "start": 310.56, "end": 315.59999999999997, "text": " don't just make us smarter, but make our world substantially better and perhaps lead us towards", "tokens": [50932, 500, 380, 445, 652, 505, 20294, 11, 457, 652, 527, 1002, 30797, 1101, 293, 4317, 1477, 505, 3030, 51184], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 74, "seek": 29920, "start": 315.59999999999997, "end": 320.56, "text": " some greater enlightenment and basically ability to cooperate much better than we were even", "tokens": [51184, 512, 5044, 34661, 293, 1936, 3485, 281, 26667, 709, 1101, 813, 321, 645, 754, 51432], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 75, "seek": 29920, "start": 320.56, "end": 321.56, "text": " before.", "tokens": [51432, 949, 13, 51482], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 76, "seek": 29920, "start": 321.56, "end": 324.68, "text": " Do you think language models are intelligent?", "tokens": [51482, 1144, 291, 519, 2856, 5245, 366, 13232, 30, 51638], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 77, "seek": 29920, "start": 324.68, "end": 326.08, "text": " That's a great question.", "tokens": [51638, 663, 311, 257, 869, 1168, 13, 51708], "temperature": 0.0, "avg_logprob": -0.14673189016488883, "compression_ratio": 1.7918088737201365, "no_speech_prob": 0.33371469378471375}, {"id": 78, "seek": 32608, "start": 326.08, "end": 330.35999999999996, "text": " I think that they're able to simulate intelligence.", "tokens": [50364, 286, 519, 300, 436, 434, 1075, 281, 27817, 7599, 13, 50578], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 79, "seek": 32608, "start": 330.35999999999996, "end": 335.35999999999996, "text": " One of the really interesting things I'm starting to see now is we are building software abstractions", "tokens": [50578, 1485, 295, 264, 534, 1880, 721, 286, 478, 2891, 281, 536, 586, 307, 321, 366, 2390, 4722, 12649, 626, 50828], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 80, "seek": 32608, "start": 335.35999999999996, "end": 337.84, "text": " and controllers on top of language models.", "tokens": [50828, 293, 26903, 322, 1192, 295, 2856, 5245, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 81, "seek": 32608, "start": 337.84, "end": 342.36, "text": " We've been talking about doing this for years, right, because at the end of the day, we have", "tokens": [50952, 492, 600, 668, 1417, 466, 884, 341, 337, 924, 11, 558, 11, 570, 412, 264, 917, 295, 264, 786, 11, 321, 362, 51178], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 82, "seek": 32608, "start": 342.36, "end": 346.84, "text": " this idea that we can have this big foundation model and it does all of the things.", "tokens": [51178, 341, 1558, 300, 321, 393, 362, 341, 955, 7030, 2316, 293, 309, 775, 439, 295, 264, 721, 13, 51402], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 83, "seek": 32608, "start": 346.84, "end": 352.28, "text": " It's multimodal, it knows how to reason, and the fact of the matter is that's not really", "tokens": [51402, 467, 311, 32972, 378, 304, 11, 309, 3255, 577, 281, 1778, 11, 293, 264, 1186, 295, 264, 1871, 307, 300, 311, 406, 534, 51674], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 84, "seek": 32608, "start": 352.28, "end": 353.28, "text": " true.", "tokens": [51674, 2074, 13, 51724], "temperature": 0.0, "avg_logprob": -0.12121116508871822, "compression_ratio": 1.7080291970802919, "no_speech_prob": 0.006468897219747305}, {"id": 85, "seek": 35328, "start": 353.28, "end": 357.64, "text": " We control them and I think initially we're seeing frameworks that allow you to do things", "tokens": [50364, 492, 1969, 552, 293, 286, 519, 9105, 321, 434, 2577, 29834, 300, 2089, 291, 281, 360, 721, 50582], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 86, "seek": 35328, "start": 357.64, "end": 362.55999999999995, "text": " like prompt injection, but the next step is thinking of controllers, using control theory", "tokens": [50582, 411, 12391, 22873, 11, 457, 264, 958, 1823, 307, 1953, 295, 26903, 11, 1228, 1969, 5261, 50828], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 87, "seek": 35328, "start": 362.55999999999995, "end": 364.79999999999995, "text": " to think about these large language models.", "tokens": [50828, 281, 519, 466, 613, 2416, 2856, 5245, 13, 50940], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 88, "seek": 35328, "start": 364.79999999999995, "end": 367.11999999999995, "text": " Anyway, I really hope you enjoy the conversation today.", "tokens": [50940, 5684, 11, 286, 534, 1454, 291, 2103, 264, 3761, 965, 13, 51056], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 89, "seek": 35328, "start": 367.11999999999995, "end": 371.71999999999997, "text": " Now, these guys are fascinated not only with controlling language models, but also with", "tokens": [51056, 823, 11, 613, 1074, 366, 24597, 406, 787, 365, 14905, 2856, 5245, 11, 457, 611, 365, 51286], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 90, "seek": 35328, "start": 371.71999999999997, "end": 375.67999999999995, "text": " things like AGI, general intelligence, collective intelligence.", "tokens": [51286, 721, 411, 316, 26252, 11, 2674, 7599, 11, 12590, 7599, 13, 51484], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 91, "seek": 35328, "start": 375.67999999999995, "end": 379.96, "text": " It was a really interesting conversation and if you stick around to the end, you can also", "tokens": [51484, 467, 390, 257, 534, 1880, 3761, 293, 498, 291, 2897, 926, 281, 264, 917, 11, 291, 393, 611, 51698], "temperature": 0.0, "avg_logprob": -0.17906798303654764, "compression_ratio": 1.7903780068728523, "no_speech_prob": 0.22136811912059784}, {"id": 92, "seek": 37996, "start": 379.96, "end": 384.52, "text": " hear about the institute that they've set up around AGI technology.", "tokens": [50364, 1568, 466, 264, 26860, 300, 436, 600, 992, 493, 926, 316, 26252, 2899, 13, 50592], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 93, "seek": 37996, "start": 384.52, "end": 385.52, "text": " Enjoy the show.", "tokens": [50592, 15411, 264, 855, 13, 50642], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 94, "seek": 37996, "start": 385.52, "end": 386.52, "text": " So, my name is Aman.", "tokens": [50642, 407, 11, 452, 1315, 307, 35466, 13, 50692], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 95, "seek": 37996, "start": 386.52, "end": 391.23999999999995, "text": " I'm a PhD student at Caltech studying computation and neural systems.", "tokens": [50692, 286, 478, 257, 14476, 3107, 412, 3511, 25970, 7601, 24903, 293, 18161, 3652, 13, 50928], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 96, "seek": 37996, "start": 391.23999999999995, "end": 395.35999999999996, "text": " Recently, we released this paper called What's the Magic Word, Towards the Control Theory", "tokens": [50928, 20072, 11, 321, 4736, 341, 3035, 1219, 708, 311, 264, 16154, 8725, 11, 48938, 264, 12912, 29009, 51134], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 97, "seek": 37996, "start": 395.35999999999996, "end": 399.96, "text": " of LLMs, and did that over the last summer with Cameron here.", "tokens": [51134, 295, 441, 43, 26386, 11, 293, 630, 300, 670, 264, 1036, 4266, 365, 24962, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 98, "seek": 37996, "start": 399.96, "end": 404.08, "text": " And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering", "tokens": [51364, 400, 1338, 11, 286, 2041, 286, 390, 510, 337, 452, 14295, 412, 264, 3535, 295, 14140, 884, 7043, 51570], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 99, "seek": 37996, "start": 404.08, "end": 405.08, "text": " science.", "tokens": [51570, 3497, 13, 51620], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 100, "seek": 37996, "start": 405.08, "end": 409.4, "text": " I specialized in machine intelligence, sort of been bouncing around between doing machine", "tokens": [51620, 286, 19813, 294, 3479, 7599, 11, 1333, 295, 668, 27380, 926, 1296, 884, 3479, 51836], "temperature": 0.0, "avg_logprob": -0.22247833675808376, "compression_ratio": 1.5602409638554218, "no_speech_prob": 0.21942120790481567}, {"id": 101, "seek": 40940, "start": 409.4, "end": 414.28, "text": " learning stuff, applying it to computational biology, trying to understand some stuff in", "tokens": [50364, 2539, 1507, 11, 9275, 309, 281, 28270, 14956, 11, 1382, 281, 1223, 512, 1507, 294, 50608], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 102, "seek": 40940, "start": 414.28, "end": 419.96, "text": " theoretical neuroscience, and most recently getting back into the LLM space, as well as", "tokens": [50608, 20864, 42762, 11, 293, 881, 3938, 1242, 646, 666, 264, 441, 43, 44, 1901, 11, 382, 731, 382, 50892], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 103, "seek": 40940, "start": 419.96, "end": 424.35999999999996, "text": " trying to study collective intelligence, how very simple machines can come together to", "tokens": [50892, 1382, 281, 2979, 12590, 7599, 11, 577, 588, 2199, 8379, 393, 808, 1214, 281, 51112], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 104, "seek": 40940, "start": 424.35999999999996, "end": 428.79999999999995, "text": " produce a very complicated and beautiful system as a whole.", "tokens": [51112, 5258, 257, 588, 6179, 293, 2238, 1185, 382, 257, 1379, 13, 51334], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 105, "seek": 40940, "start": 428.79999999999995, "end": 429.79999999999995, "text": " So, yeah.", "tokens": [51334, 407, 11, 1338, 13, 51384], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 106, "seek": 40940, "start": 429.79999999999995, "end": 430.79999999999995, "text": " Amazing.", "tokens": [51384, 14165, 13, 51434], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 107, "seek": 40940, "start": 430.79999999999995, "end": 431.79999999999995, "text": " And Cameron.", "tokens": [51434, 400, 24962, 13, 51484], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 108, "seek": 40940, "start": 431.79999999999995, "end": 432.79999999999995, "text": " Yeah.", "tokens": [51484, 865, 13, 51534], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 109, "seek": 40940, "start": 432.79999999999995, "end": 436.08, "text": " So, my name is Cameron McCoskey, and I went to undergrad here.", "tokens": [51534, 407, 11, 452, 1315, 307, 24962, 12061, 329, 4119, 11, 293, 286, 1437, 281, 14295, 510, 13, 51698], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 110, "seek": 40940, "start": 436.08, "end": 439.12, "text": " I did engineering science as well.", "tokens": [51698, 286, 630, 7043, 3497, 382, 731, 13, 51850], "temperature": 0.0, "avg_logprob": -0.176256859820822, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.015414788387715816}, {"id": 111, "seek": 43912, "start": 439.12, "end": 443.4, "text": " I majored in the robotics engineering option, and now I'm a grad student.", "tokens": [50364, 286, 13673, 2769, 294, 264, 34145, 7043, 3614, 11, 293, 586, 286, 478, 257, 2771, 3107, 13, 50578], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 112, "seek": 43912, "start": 443.4, "end": 449.0, "text": " I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and", "tokens": [50578, 286, 478, 20222, 257, 4505, 311, 294, 12147, 3820, 7043, 11, 26269, 538, 13391, 8030, 293, 50858], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 113, "seek": 43912, "start": 449.0, "end": 450.2, "text": " Kevin Chiron.", "tokens": [50858, 9954, 761, 2088, 13, 50918], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 114, "seek": 43912, "start": 450.2, "end": 455.88, "text": " I'm really interested in the deep questions of intelligence, and right now I'm pursuing", "tokens": [50918, 286, 478, 534, 3102, 294, 264, 2452, 1651, 295, 7599, 11, 293, 558, 586, 286, 478, 20222, 51202], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 115, "seek": 43912, "start": 455.88, "end": 460.28000000000003, "text": " research related to morphogenesis and computational models of it.", "tokens": [51202, 2132, 4077, 281, 25778, 8799, 9374, 293, 28270, 5245, 295, 309, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 116, "seek": 43912, "start": 460.28000000000003, "end": 464.32, "text": " Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt", "tokens": [51422, 1743, 286, 2835, 1036, 4266, 11, 286, 1437, 760, 281, 3511, 25970, 11, 293, 321, 4114, 341, 3035, 322, 12391, 51624], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 117, "seek": 43912, "start": 464.32, "end": 468.24, "text": " engineering, well, a control theory of prompt engineering.", "tokens": [51624, 7043, 11, 731, 11, 257, 1969, 5261, 295, 12391, 7043, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1988508005057816, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.0018093381077051163}, {"id": 118, "seek": 46824, "start": 468.36, "end": 470.40000000000003, "text": " I'm excited to get into it.", "tokens": [50370, 286, 478, 2919, 281, 483, 666, 309, 13, 50472], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 119, "seek": 46824, "start": 470.40000000000003, "end": 473.36, "text": " You folks have just written an incredibly interesting paper.", "tokens": [50472, 509, 4024, 362, 445, 3720, 364, 6252, 1880, 3035, 13, 50620], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 120, "seek": 46824, "start": 473.36, "end": 478.88, "text": " It was shared in our Discord server, and I saw your presentation, and we'll share a clip", "tokens": [50620, 467, 390, 5507, 294, 527, 32623, 7154, 11, 293, 286, 1866, 428, 5860, 11, 293, 321, 603, 2073, 257, 7353, 50896], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 121, "seek": 46824, "start": 478.88, "end": 482.64, "text": " of that in the introduction, but I was intrigued by it straight away.", "tokens": [50896, 295, 300, 294, 264, 9339, 11, 457, 286, 390, 35140, 538, 309, 2997, 1314, 13, 51084], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 122, "seek": 46824, "start": 482.64, "end": 486.76, "text": " And what you're doing is you're talking about control theory in respect of large language", "tokens": [51084, 400, 437, 291, 434, 884, 307, 291, 434, 1417, 466, 1969, 5261, 294, 3104, 295, 2416, 2856, 51290], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 123, "seek": 46824, "start": 486.76, "end": 487.84000000000003, "text": " models.", "tokens": [51290, 5245, 13, 51344], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 124, "seek": 46824, "start": 487.84000000000003, "end": 489.8, "text": " Can you explain what that is?", "tokens": [51344, 1664, 291, 2903, 437, 300, 307, 30, 51442], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 125, "seek": 46824, "start": 489.8, "end": 490.8, "text": " Yeah.", "tokens": [51442, 865, 13, 51492], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 126, "seek": 46824, "start": 490.8, "end": 493.72, "text": " So, I guess I'll get started with control theory.", "tokens": [51492, 407, 11, 286, 2041, 286, 603, 483, 1409, 365, 1969, 5261, 13, 51638], "temperature": 0.0, "avg_logprob": -0.18044240495799918, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.1133677214384079}, {"id": 127, "seek": 49372, "start": 493.72, "end": 501.0, "text": " So back in the day, the late 1800s, this guy Maxwell observed that people were making", "tokens": [50364, 407, 646, 294, 264, 786, 11, 264, 3469, 24327, 82, 11, 341, 2146, 39594, 13095, 300, 561, 645, 1455, 50728], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 128, "seek": 49372, "start": 501.0, "end": 504.92, "text": " these engines, and they were putting these things called governors on them, where if", "tokens": [50728, 613, 12982, 11, 293, 436, 645, 3372, 613, 721, 1219, 36571, 322, 552, 11, 689, 498, 50924], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 129, "seek": 49372, "start": 504.92, "end": 509.28000000000003, "text": " your car or your machine was experiencing varying loads, you wanted the engine to still", "tokens": [50924, 428, 1032, 420, 428, 3479, 390, 11139, 22984, 12668, 11, 291, 1415, 264, 2848, 281, 920, 51142], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 130, "seek": 49372, "start": 509.28000000000003, "end": 511.12, "text": " go at the same rate, right?", "tokens": [51142, 352, 412, 264, 912, 3314, 11, 558, 30, 51234], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 131, "seek": 49372, "start": 511.12, "end": 513.08, "text": " And people had these things called governors.", "tokens": [51234, 400, 561, 632, 613, 721, 1219, 36571, 13, 51332], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 132, "seek": 49372, "start": 513.08, "end": 516.8000000000001, "text": " There's this fly ball governor, which is this sort of hand tune thing that you put on top", "tokens": [51332, 821, 311, 341, 3603, 2594, 12965, 11, 597, 307, 341, 1333, 295, 1011, 10864, 551, 300, 291, 829, 322, 1192, 51518], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 133, "seek": 49372, "start": 516.8000000000001, "end": 520.88, "text": " of the engine to try to make sure that it'll be consistent, that it'll do what you want,", "tokens": [51518, 295, 264, 2848, 281, 853, 281, 652, 988, 300, 309, 603, 312, 8398, 11, 300, 309, 603, 360, 437, 291, 528, 11, 51722], "temperature": 0.0, "avg_logprob": -0.11169680786132813, "compression_ratio": 1.8120567375886525, "no_speech_prob": 0.0018673526355996728}, {"id": 134, "seek": 52088, "start": 520.88, "end": 524.0, "text": " that it'll be going at a consistent speed, right?", "tokens": [50364, 300, 309, 603, 312, 516, 412, 257, 8398, 3073, 11, 558, 30, 50520], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 135, "seek": 52088, "start": 524.0, "end": 528.88, "text": " And people were hand tuning these things, and obviously the engines were working, but", "tokens": [50520, 400, 561, 645, 1011, 15164, 613, 721, 11, 293, 2745, 264, 12982, 645, 1364, 11, 457, 50764], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 136, "seek": 52088, "start": 528.88, "end": 533.48, "text": " it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees", "tokens": [50764, 309, 2067, 380, 588, 29882, 11, 293, 309, 2067, 380, 588, 13956, 11, 293, 321, 994, 380, 362, 867, 32567, 50994], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 137, "seek": 52088, "start": 533.48, "end": 535.92, "text": " as to how it would end up working in practice.", "tokens": [50994, 382, 281, 577, 309, 576, 917, 493, 1364, 294, 3124, 13, 51116], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 138, "seek": 52088, "start": 535.92, "end": 541.68, "text": " And so what Maxwell did was he formalized the notion of feedback control, where if you", "tokens": [51116, 400, 370, 437, 39594, 630, 390, 415, 9860, 1602, 264, 10710, 295, 5824, 1969, 11, 689, 498, 291, 51404], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 139, "seek": 52088, "start": 541.68, "end": 547.2, "text": " have this system, even if it's quite complicated, as it turns out, if you feedback the output", "tokens": [51404, 362, 341, 1185, 11, 754, 498, 309, 311, 1596, 6179, 11, 382, 309, 4523, 484, 11, 498, 291, 5824, 264, 5598, 51680], "temperature": 0.0, "avg_logprob": -0.10853684800011772, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.16863273084163666}, {"id": 140, "seek": 54720, "start": 547.2, "end": 552.6400000000001, "text": " of the system into a controller, and try to compute some error metric, and try to correct", "tokens": [50364, 295, 264, 1185, 666, 257, 10561, 11, 293, 853, 281, 14722, 512, 6713, 20678, 11, 293, 853, 281, 3006, 50636], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 141, "seek": 54720, "start": 552.6400000000001, "end": 556.5600000000001, "text": " for that at every moment in time, it turns out to be a much easier problem to solve from", "tokens": [50636, 337, 300, 412, 633, 1623, 294, 565, 11, 309, 4523, 484, 281, 312, 257, 709, 3571, 1154, 281, 5039, 490, 50832], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 142, "seek": 54720, "start": 556.5600000000001, "end": 561.1600000000001, "text": " an engineering perspective than trying to make a perfect system that just does the right", "tokens": [50832, 364, 7043, 4585, 813, 1382, 281, 652, 257, 2176, 1185, 300, 445, 775, 264, 558, 51062], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 143, "seek": 54720, "start": 561.1600000000001, "end": 562.1600000000001, "text": " thing off the bat.", "tokens": [51062, 551, 766, 264, 7362, 13, 51112], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 144, "seek": 54720, "start": 562.1600000000001, "end": 566.6400000000001, "text": " So this idea of feedback was really powerful, and sort of gave birth to modern control theory.", "tokens": [51112, 407, 341, 1558, 295, 5824, 390, 534, 4005, 11, 293, 1333, 295, 2729, 3965, 281, 4363, 1969, 5261, 13, 51336], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 145, "seek": 54720, "start": 566.6400000000001, "end": 570.76, "text": " And as it turned out, that was a really powerful way to look at systems, building systems,", "tokens": [51336, 400, 382, 309, 3574, 484, 11, 300, 390, 257, 534, 4005, 636, 281, 574, 412, 3652, 11, 2390, 3652, 11, 51542], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 146, "seek": 54720, "start": 570.76, "end": 574.84, "text": " and controlling them and doing engineering on them, so that they could be robust, do", "tokens": [51542, 293, 14905, 552, 293, 884, 7043, 322, 552, 11, 370, 300, 436, 727, 312, 13956, 11, 360, 51746], "temperature": 0.0, "avg_logprob": -0.08239619422504921, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.5995325446128845}, {"id": 147, "seek": 57484, "start": 574.84, "end": 577.44, "text": " what we want, and so that we could predict them.", "tokens": [50364, 437, 321, 528, 11, 293, 370, 300, 321, 727, 6069, 552, 13, 50494], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 148, "seek": 57484, "start": 577.44, "end": 583.32, "text": " And so when it comes to LLM control theory, what we saw is that we're kind of at a similar", "tokens": [50494, 400, 370, 562, 309, 1487, 281, 441, 43, 44, 1969, 5261, 11, 437, 321, 1866, 307, 300, 321, 434, 733, 295, 412, 257, 2531, 50788], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 149, "seek": 57484, "start": 583.32, "end": 587.44, "text": " place with language models, where we have these engines, we have these language models", "tokens": [50788, 1081, 365, 2856, 5245, 11, 689, 321, 362, 613, 12982, 11, 321, 362, 613, 2856, 5245, 50994], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 150, "seek": 57484, "start": 587.44, "end": 591.44, "text": " that are very powerful, they can do a lot, they seem to exhibit many interesting attributes", "tokens": [50994, 300, 366, 588, 4005, 11, 436, 393, 360, 257, 688, 11, 436, 1643, 281, 20487, 867, 1880, 17212, 51194], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 151, "seek": 57484, "start": 591.44, "end": 595.44, "text": " of intelligence, and there's a lot of utility there for people to build further systems", "tokens": [51194, 295, 7599, 11, 293, 456, 311, 257, 688, 295, 14877, 456, 337, 561, 281, 1322, 3052, 3652, 51394], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 152, "seek": 57484, "start": 595.44, "end": 597.9200000000001, "text": " on top of them, and people are already doing that.", "tokens": [51394, 322, 1192, 295, 552, 11, 293, 561, 366, 1217, 884, 300, 13, 51518], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 153, "seek": 57484, "start": 597.9200000000001, "end": 602.5600000000001, "text": " But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going", "tokens": [51518, 583, 558, 586, 11, 309, 311, 1333, 295, 341, 1011, 10870, 11, 1011, 36213, 12391, 7043, 300, 311, 516, 51750], "temperature": 0.0, "avg_logprob": -0.0926108787308878, "compression_ratio": 1.7934426229508196, "no_speech_prob": 0.14401790499687195}, {"id": 154, "seek": 60256, "start": 602.56, "end": 609.4799999999999, "text": " on where it's really hard to get at the fundamentals of what exactly it means to control an LLM", "tokens": [50364, 322, 689, 309, 311, 534, 1152, 281, 483, 412, 264, 29505, 295, 437, 2293, 309, 1355, 281, 1969, 364, 441, 43, 44, 50710], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 155, "seek": 60256, "start": 609.4799999999999, "end": 611.0799999999999, "text": " system and how you might do it.", "tokens": [50710, 1185, 293, 577, 291, 1062, 360, 309, 13, 50790], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 156, "seek": 60256, "start": 611.0799999999999, "end": 612.64, "text": " At this point, it's very heuristic.", "tokens": [50790, 1711, 341, 935, 11, 309, 311, 588, 415, 374, 3142, 13, 50868], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 157, "seek": 60256, "start": 612.64, "end": 616.1199999999999, "text": " And so we sort of saw that as an opportunity to try to figure out what would a control", "tokens": [50868, 400, 370, 321, 1333, 295, 1866, 300, 382, 364, 2650, 281, 853, 281, 2573, 484, 437, 576, 257, 1969, 51042], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 158, "seek": 60256, "start": 616.1199999999999, "end": 620.88, "text": " theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to", "tokens": [51042, 5261, 337, 441, 43, 26386, 574, 411, 11, 300, 4696, 11, 498, 321, 393, 360, 309, 558, 11, 321, 603, 976, 3965, 281, 51280], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 159, "seek": 60256, "start": 620.88, "end": 624.92, "text": " all of these really, really useful engineering insights, and also just fundamental insights", "tokens": [51280, 439, 295, 613, 534, 11, 534, 4420, 7043, 14310, 11, 293, 611, 445, 8088, 14310, 51482], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 160, "seek": 60256, "start": 624.92, "end": 629.64, "text": " as to the nature of LLM systems, so that we can better control them, make them reliable", "tokens": [51482, 382, 281, 264, 3687, 295, 441, 43, 44, 3652, 11, 370, 300, 321, 393, 1101, 1969, 552, 11, 652, 552, 12924, 51718], "temperature": 0.0, "avg_logprob": -0.10755124268708406, "compression_ratio": 1.7610921501706485, "no_speech_prob": 0.1601542830467224}, {"id": 161, "seek": 62964, "start": 629.64, "end": 634.24, "text": " and robust, and be able to do engineering in a more principled manner on them than we're", "tokens": [50364, 293, 13956, 11, 293, 312, 1075, 281, 360, 7043, 294, 257, 544, 3681, 15551, 9060, 322, 552, 813, 321, 434, 50594], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 162, "seek": 62964, "start": 634.24, "end": 635.48, "text": " currently able to.", "tokens": [50594, 4362, 1075, 281, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 163, "seek": 62964, "start": 635.48, "end": 639.16, "text": " So that's sort of the general direction and the motivations for our control theory of", "tokens": [50656, 407, 300, 311, 1333, 295, 264, 2674, 3513, 293, 264, 39034, 337, 527, 1969, 5261, 295, 50840], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 164, "seek": 62964, "start": 639.16, "end": 640.16, "text": " language models.", "tokens": [50840, 2856, 5245, 13, 50890], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 165, "seek": 62964, "start": 640.16, "end": 641.76, "text": " Yeah, that's absolutely fascinating.", "tokens": [50890, 865, 11, 300, 311, 3122, 10343, 13, 50970], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 166, "seek": 62964, "start": 641.76, "end": 645.8, "text": " I mean, for many years, I've been thinking that we need to have some kind of a controller", "tokens": [50970, 286, 914, 11, 337, 867, 924, 11, 286, 600, 668, 1953, 300, 321, 643, 281, 362, 512, 733, 295, 257, 10561, 51172], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 167, "seek": 62964, "start": 645.8, "end": 647.92, "text": " for a large language model.", "tokens": [51172, 337, 257, 2416, 2856, 2316, 13, 51278], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 168, "seek": 62964, "start": 647.92, "end": 652.08, "text": " But I guess I'm interested in, first of all, what are the differences between large language", "tokens": [51278, 583, 286, 2041, 286, 478, 3102, 294, 11, 700, 295, 439, 11, 437, 366, 264, 7300, 1296, 2416, 2856, 51486], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 169, "seek": 62964, "start": 652.08, "end": 654.36, "text": " models and something like a steam engine?", "tokens": [51486, 5245, 293, 746, 411, 257, 11952, 2848, 30, 51600], "temperature": 0.0, "avg_logprob": -0.11974368556853264, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.09740656614303589}, {"id": 170, "seek": 65436, "start": 654.36, "end": 659.6, "text": " And also, with a steam engine, you might be optimizing the efficiency or the performance", "tokens": [50364, 400, 611, 11, 365, 257, 11952, 2848, 11, 291, 1062, 312, 40425, 264, 10493, 420, 264, 3389, 50626], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 171, "seek": 65436, "start": 659.6, "end": 661.64, "text": " or the speed or something like that.", "tokens": [50626, 420, 264, 3073, 420, 746, 411, 300, 13, 50728], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 172, "seek": 65436, "start": 661.64, "end": 666.12, "text": " What is it that we are kind of trying to make better with a large language model?", "tokens": [50728, 708, 307, 309, 300, 321, 366, 733, 295, 1382, 281, 652, 1101, 365, 257, 2416, 2856, 2316, 30, 50952], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 173, "seek": 65436, "start": 666.12, "end": 670.16, "text": " So first off, we'll talk about the differences between large language models and other types", "tokens": [50952, 407, 700, 766, 11, 321, 603, 751, 466, 264, 7300, 1296, 2416, 2856, 5245, 293, 661, 3467, 51154], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 174, "seek": 65436, "start": 670.16, "end": 673.04, "text": " of systems that you might want to control.", "tokens": [51154, 295, 3652, 300, 291, 1062, 528, 281, 1969, 13, 51298], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 175, "seek": 65436, "start": 673.04, "end": 678.64, "text": " Typically a control system, you might first be introduced to control theory in the context", "tokens": [51298, 23129, 257, 1969, 1185, 11, 291, 1062, 700, 312, 7268, 281, 1969, 5261, 294, 264, 4319, 51578], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 176, "seek": 65436, "start": 678.64, "end": 682.64, "text": " of like, say you're trying to control an engine or something else where the states can be", "tokens": [51578, 295, 411, 11, 584, 291, 434, 1382, 281, 1969, 364, 2848, 420, 746, 1646, 689, 264, 4368, 393, 312, 51778], "temperature": 0.0, "avg_logprob": -0.08370519479115804, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.002710985019803047}, {"id": 177, "seek": 68264, "start": 682.64, "end": 687.92, "text": " represented by a set of numbers or set of real numbers that is fixed size.", "tokens": [50364, 10379, 538, 257, 992, 295, 3547, 420, 992, 295, 957, 3547, 300, 307, 6806, 2744, 13, 50628], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 178, "seek": 68264, "start": 687.92, "end": 692.56, "text": " So perhaps we have an X and a Y coordinate where it's trying to control or a position", "tokens": [50628, 407, 4317, 321, 362, 364, 1783, 293, 257, 398, 15670, 689, 309, 311, 1382, 281, 1969, 420, 257, 2535, 50860], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 179, "seek": 68264, "start": 692.56, "end": 693.88, "text": " in a velocity.", "tokens": [50860, 294, 257, 9269, 13, 50926], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 180, "seek": 68264, "start": 693.88, "end": 700.04, "text": " These are common types of systems in scenarios that show up in control theory.", "tokens": [50926, 1981, 366, 2689, 3467, 295, 3652, 294, 15077, 300, 855, 493, 294, 1969, 5261, 13, 51234], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 181, "seek": 68264, "start": 700.04, "end": 704.3199999999999, "text": " The difference with an LLM, the first major difference is that the token space, the state", "tokens": [51234, 440, 2649, 365, 364, 441, 43, 44, 11, 264, 700, 2563, 2649, 307, 300, 264, 14862, 1901, 11, 264, 1785, 51448], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 182, "seek": 68264, "start": 704.3199999999999, "end": 706.0, "text": " space of the system is discreet.", "tokens": [51448, 1901, 295, 264, 1185, 307, 2983, 4751, 13, 51532], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 183, "seek": 68264, "start": 706.0, "end": 710.24, "text": " Because we're dealing with tokens, we're dealing with words, we're not operating in the space", "tokens": [51532, 1436, 321, 434, 6260, 365, 22667, 11, 321, 434, 6260, 365, 2283, 11, 321, 434, 406, 7447, 294, 264, 1901, 51744], "temperature": 0.0, "avg_logprob": -0.13314041040711483, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.010981163941323757}, {"id": 184, "seek": 71024, "start": 710.24, "end": 715.96, "text": " of real numbers anymore, and so this introduces some complications and complexities when dealing", "tokens": [50364, 295, 957, 3547, 3602, 11, 293, 370, 341, 31472, 512, 26566, 293, 48705, 562, 6260, 50650], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 185, "seek": 71024, "start": 715.96, "end": 717.28, "text": " with control theory.", "tokens": [50650, 365, 1969, 5261, 13, 50716], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 186, "seek": 71024, "start": 717.28, "end": 722.72, "text": " The second thing that's really significant is that each time an LLM generates a token", "tokens": [50716, 440, 1150, 551, 300, 311, 534, 4776, 307, 300, 1184, 565, 364, 441, 43, 44, 23815, 257, 14862, 50988], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 187, "seek": 71024, "start": 722.72, "end": 726.48, "text": " or a user inputs a token, that state space actually expands.", "tokens": [50988, 420, 257, 4195, 15743, 257, 14862, 11, 300, 1785, 1901, 767, 33706, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 188, "seek": 71024, "start": 726.48, "end": 728.5600000000001, "text": " It grows by one token.", "tokens": [51176, 467, 13156, 538, 472, 14862, 13, 51280], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 189, "seek": 71024, "start": 728.5600000000001, "end": 732.32, "text": " And this is very interesting and unique for LLM systems.", "tokens": [51280, 400, 341, 307, 588, 1880, 293, 3845, 337, 441, 43, 44, 3652, 13, 51468], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 190, "seek": 71024, "start": 732.32, "end": 737.52, "text": " On the one hand, this can be exploited to try and get the LLMs to engage in reasoning", "tokens": [51468, 1282, 264, 472, 1011, 11, 341, 393, 312, 40918, 281, 853, 293, 483, 264, 441, 43, 26386, 281, 4683, 294, 21577, 51728], "temperature": 0.0, "avg_logprob": -0.09904383713344359, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.020959436893463135}, {"id": 191, "seek": 73752, "start": 737.52, "end": 743.3199999999999, "text": " or chain of thoughts or kind of take a winding path to the answer you actually want them", "tokens": [50364, 420, 5021, 295, 4598, 420, 733, 295, 747, 257, 29775, 3100, 281, 264, 1867, 291, 767, 528, 552, 50654], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 192, "seek": 73752, "start": 743.3199999999999, "end": 744.92, "text": " to outputs.", "tokens": [50654, 281, 23930, 13, 50734], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 193, "seek": 73752, "start": 744.92, "end": 748.72, "text": " But of course, this makes it very difficult for control theory because each new token", "tokens": [50734, 583, 295, 1164, 11, 341, 1669, 309, 588, 2252, 337, 1969, 5261, 570, 1184, 777, 14862, 50924], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 194, "seek": 73752, "start": 748.72, "end": 754.16, "text": " you add, the space of possible sentences grows exponentially.", "tokens": [50924, 291, 909, 11, 264, 1901, 295, 1944, 16579, 13156, 37330, 13, 51196], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 195, "seek": 73752, "start": 754.16, "end": 759.76, "text": " And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this", "tokens": [51196, 400, 294, 2856, 5245, 11, 264, 19864, 2744, 307, 322, 264, 1668, 295, 2625, 11, 1360, 281, 2319, 11, 1360, 11, 370, 341, 51476], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 196, "seek": 73752, "start": 759.76, "end": 762.56, "text": " grows extremely, extremely quickly.", "tokens": [51476, 13156, 4664, 11, 4664, 2661, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14239719935825892, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0069020939990878105}, {"id": 197, "seek": 76256, "start": 762.56, "end": 764.4, "text": " These are some of the challenges.", "tokens": [50364, 1981, 366, 512, 295, 264, 4759, 13, 50456], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 198, "seek": 76256, "start": 764.4, "end": 769.2399999999999, "text": " And with a control theory of say engines, you're trying to optimize the efficiency.", "tokens": [50456, 400, 365, 257, 1969, 5261, 295, 584, 12982, 11, 291, 434, 1382, 281, 19719, 264, 10493, 13, 50698], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 199, "seek": 76256, "start": 769.2399999999999, "end": 773.9599999999999, "text": " It's a good question what you're trying to optimize for language models.", "tokens": [50698, 467, 311, 257, 665, 1168, 437, 291, 434, 1382, 281, 19719, 337, 2856, 5245, 13, 50934], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 200, "seek": 76256, "start": 773.9599999999999, "end": 777.92, "text": " I think this is definitely a direction for future research.", "tokens": [50934, 286, 519, 341, 307, 2138, 257, 3513, 337, 2027, 2132, 13, 51132], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 201, "seek": 76256, "start": 777.92, "end": 779.3599999999999, "text": " Do you have any thoughts on this?", "tokens": [51132, 1144, 291, 362, 604, 4598, 322, 341, 30, 51204], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 202, "seek": 76256, "start": 779.3599999999999, "end": 780.3599999999999, "text": " Yeah.", "tokens": [51204, 865, 13, 51254], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 203, "seek": 76256, "start": 780.3599999999999, "end": 786.76, "text": " I think the thing that we saw was that even very simple questions about how these LLMs operate,", "tokens": [51254, 286, 519, 264, 551, 300, 321, 1866, 390, 300, 754, 588, 2199, 1651, 466, 577, 613, 441, 43, 26386, 9651, 11, 51574], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 204, "seek": 76256, "start": 786.76, "end": 791.0799999999999, "text": " their input-output relationships, when you start to treat them just as a system that", "tokens": [51574, 641, 4846, 12, 346, 2582, 6159, 11, 562, 291, 722, 281, 2387, 552, 445, 382, 257, 1185, 300, 51790], "temperature": 0.0, "avg_logprob": -0.18631485180977064, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.1868484765291214}, {"id": 205, "seek": 79108, "start": 791.08, "end": 795.08, "text": " maybe there's an imposed input, like a system prompt, and then you get to pick a subset", "tokens": [50364, 1310, 456, 311, 364, 26491, 4846, 11, 411, 257, 1185, 12391, 11, 293, 550, 291, 483, 281, 1888, 257, 25993, 50564], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 206, "seek": 79108, "start": 795.08, "end": 796.8000000000001, "text": " of those tokens, right?", "tokens": [50564, 295, 729, 22667, 11, 558, 30, 50650], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 207, "seek": 79108, "start": 796.8000000000001, "end": 800.48, "text": " When you start to treat it like that, and you just ask a really simple question, like,", "tokens": [50650, 1133, 291, 722, 281, 2387, 309, 411, 300, 11, 293, 291, 445, 1029, 257, 534, 2199, 1168, 11, 411, 11, 50834], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 208, "seek": 79108, "start": 800.48, "end": 803.64, "text": " let's say that I want it to generate a specific string.", "tokens": [50834, 718, 311, 584, 300, 286, 528, 309, 281, 8460, 257, 2685, 6798, 13, 50992], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 209, "seek": 79108, "start": 803.64, "end": 806.6800000000001, "text": " We're not going to be trying to use it to do some intelligent information processing.", "tokens": [50992, 492, 434, 406, 516, 281, 312, 1382, 281, 764, 309, 281, 360, 512, 13232, 1589, 9007, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 210, "seek": 79108, "start": 806.6800000000001, "end": 809.96, "text": " I just want to see, can I make it do something?", "tokens": [51144, 286, 445, 528, 281, 536, 11, 393, 286, 652, 309, 360, 746, 30, 51308], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 211, "seek": 79108, "start": 809.96, "end": 814.44, "text": " And what we found and what sort of motivated us to do this is that we really had no idea", "tokens": [51308, 400, 437, 321, 1352, 293, 437, 1333, 295, 14515, 505, 281, 360, 341, 307, 300, 321, 534, 632, 572, 1558, 51532], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 212, "seek": 79108, "start": 814.44, "end": 819.24, "text": " when it would be possible or if it was generally possible to make it do anything we want.", "tokens": [51532, 562, 309, 576, 312, 1944, 420, 498, 309, 390, 5101, 1944, 281, 652, 309, 360, 1340, 321, 528, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12373098608565657, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.41428086161613464}, {"id": 213, "seek": 81924, "start": 819.24, "end": 823.24, "text": " Can we just make an LLM system generate any output we desire?", "tokens": [50364, 1664, 321, 445, 652, 364, 441, 43, 44, 1185, 8460, 604, 5598, 321, 7516, 30, 50564], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 214, "seek": 81924, "start": 823.24, "end": 826.2, "text": " And if the answer is yes, which seems like it's probable.", "tokens": [50564, 400, 498, 264, 1867, 307, 2086, 11, 597, 2544, 411, 309, 311, 21759, 13, 50712], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 215, "seek": 81924, "start": 826.2, "end": 829.8, "text": " If you get to have a lot of tokens in your input that you control, it seems reasonable", "tokens": [50712, 759, 291, 483, 281, 362, 257, 688, 295, 22667, 294, 428, 4846, 300, 291, 1969, 11, 309, 2544, 10585, 50892], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 216, "seek": 81924, "start": 829.8, "end": 834.52, "text": " that you'd be able to probably get it to output a wide variety of at least reasonable", "tokens": [50892, 300, 291, 1116, 312, 1075, 281, 1391, 483, 309, 281, 5598, 257, 4874, 5673, 295, 412, 1935, 10585, 51128], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 217, "seek": 81924, "start": 834.52, "end": 837.72, "text": " English sentences or linguistically valid sentences.", "tokens": [51128, 3669, 16579, 420, 21766, 20458, 7363, 16579, 13, 51288], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 218, "seek": 81924, "start": 837.72, "end": 842.6, "text": " But the question that we had was, OK, if you have a finite budget for that, would you be", "tokens": [51288, 583, 264, 1168, 300, 321, 632, 390, 11, 2264, 11, 498, 291, 362, 257, 19362, 4706, 337, 300, 11, 576, 291, 312, 51532], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 219, "seek": 81924, "start": 842.6, "end": 843.92, "text": " able to get it to do anything?", "tokens": [51532, 1075, 281, 483, 309, 281, 360, 1340, 30, 51598], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 220, "seek": 81924, "start": 843.92, "end": 848.32, "text": " And what budget of tokens, like how many tokens do you have to be able to control if you want", "tokens": [51598, 400, 437, 4706, 295, 22667, 11, 411, 577, 867, 22667, 360, 291, 362, 281, 312, 1075, 281, 1969, 498, 291, 528, 51818], "temperature": 0.0, "avg_logprob": -0.10046031925228092, "compression_ratio": 1.826797385620915, "no_speech_prob": 0.39204126596450806}, {"id": 221, "seek": 84832, "start": 848.32, "end": 851.5200000000001, "text": " to be able to make the system do whatever you want?", "tokens": [50364, 281, 312, 1075, 281, 652, 264, 1185, 360, 2035, 291, 528, 30, 50524], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 222, "seek": 84832, "start": 851.5200000000001, "end": 855.2, "text": " And that was the initial motivation where it was like, yeah, there are all these high", "tokens": [50524, 400, 300, 390, 264, 5883, 12335, 689, 309, 390, 411, 11, 1338, 11, 456, 366, 439, 613, 1090, 50708], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 223, "seek": 84832, "start": 855.2, "end": 860.44, "text": " and mighty sort of questions of how do we make these systems do what we want in an alignment", "tokens": [50708, 293, 21556, 1333, 295, 1651, 295, 577, 360, 321, 652, 613, 3652, 360, 437, 321, 528, 294, 364, 18515, 50970], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 224, "seek": 84832, "start": 860.44, "end": 861.44, "text": " sense?", "tokens": [50970, 2020, 30, 51020], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 225, "seek": 84832, "start": 861.44, "end": 864.48, "text": " How do we make them do what we want in the sense of cooperating towards some information", "tokens": [51020, 1012, 360, 321, 652, 552, 360, 437, 321, 528, 294, 264, 2020, 295, 13414, 990, 3030, 512, 1589, 51172], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 226, "seek": 84832, "start": 864.48, "end": 865.88, "text": " processing objective?", "tokens": [51172, 9007, 10024, 30, 51242], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 227, "seek": 84832, "start": 865.88, "end": 869.08, "text": " But we realized that these really, really simple questions are just, OK, you have an input", "tokens": [51242, 583, 321, 5334, 300, 613, 534, 11, 534, 2199, 1651, 366, 445, 11, 2264, 11, 291, 362, 364, 4846, 51402], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 228, "seek": 84832, "start": 869.08, "end": 872.44, "text": " that you get to partially control and you're trying to make it do something.", "tokens": [51402, 300, 291, 483, 281, 18886, 1969, 293, 291, 434, 1382, 281, 652, 309, 360, 746, 13, 51570], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 229, "seek": 84832, "start": 872.44, "end": 875.48, "text": " That question was completely unanswered and we were sort of taking bets on it.", "tokens": [51570, 663, 1168, 390, 2584, 517, 43904, 292, 293, 321, 645, 1333, 295, 1940, 39922, 322, 309, 13, 51722], "temperature": 0.0, "avg_logprob": -0.138574481010437, "compression_ratio": 1.8364197530864197, "no_speech_prob": 0.10361277312040329}, {"id": 230, "seek": 87548, "start": 875.52, "end": 877.5600000000001, "text": " I think Cameron was the one who started to make bets.", "tokens": [50366, 286, 519, 24962, 390, 264, 472, 567, 1409, 281, 652, 39922, 13, 50468], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 231, "seek": 87548, "start": 877.5600000000001, "end": 880.0, "text": " He was like, I bet like $10 that we can get this done.", "tokens": [50468, 634, 390, 411, 11, 286, 778, 411, 1848, 3279, 300, 321, 393, 483, 341, 1096, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 232, "seek": 87548, "start": 880.0, "end": 882.96, "text": " We can make it emit this output within five tokens.", "tokens": [50590, 492, 393, 652, 309, 32084, 341, 5598, 1951, 1732, 22667, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 233, "seek": 87548, "start": 882.96, "end": 887.32, "text": " And that was really the initial motivation where it was like, even the feed forward dynamics", "tokens": [50738, 400, 300, 390, 534, 264, 5883, 12335, 689, 309, 390, 411, 11, 754, 264, 3154, 2128, 15679, 50956], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 234, "seek": 87548, "start": 887.32, "end": 889.6800000000001, "text": " of this system are really mysterious.", "tokens": [50956, 295, 341, 1185, 366, 534, 13831, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 235, "seek": 87548, "start": 889.6800000000001, "end": 893.96, "text": " And getting a grip on those, it seems like that's a really strong way to start building", "tokens": [51074, 400, 1242, 257, 12007, 322, 729, 11, 309, 2544, 411, 300, 311, 257, 534, 2068, 636, 281, 722, 2390, 51288], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 236, "seek": 87548, "start": 893.96, "end": 899.16, "text": " up a fundamental control theory and a really strong understanding of these LLM systems where", "tokens": [51288, 493, 257, 8088, 1969, 5261, 293, 257, 534, 2068, 3701, 295, 613, 441, 43, 44, 3652, 689, 51548], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 237, "seek": 87548, "start": 899.16, "end": 902.9200000000001, "text": " in control theory at least, when you start to really deeply understand just a single", "tokens": [51548, 294, 1969, 5261, 412, 1935, 11, 562, 291, 722, 281, 534, 8760, 1223, 445, 257, 2167, 51736], "temperature": 0.0, "avg_logprob": -0.1426910314344822, "compression_ratio": 1.8084415584415585, "no_speech_prob": 0.08265656232833862}, {"id": 238, "seek": 90292, "start": 902.9599999999999, "end": 907.64, "text": " system with its own dynamics and how the input-output relationships work, what the reachable sets", "tokens": [50366, 1185, 365, 1080, 1065, 15679, 293, 577, 264, 4846, 12, 346, 2582, 6159, 589, 11, 437, 264, 2524, 712, 6352, 50600], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 239, "seek": 90292, "start": 907.64, "end": 912.56, "text": " look like, how controllable it is, then when it comes to building more complicated systems", "tokens": [50600, 574, 411, 11, 577, 45159, 712, 309, 307, 11, 550, 562, 309, 1487, 281, 2390, 544, 6179, 3652, 50846], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 240, "seek": 90292, "start": 912.56, "end": 918.28, "text": " where maybe you have a more complicated objective, maybe you have interacting systems, when you", "tokens": [50846, 689, 1310, 291, 362, 257, 544, 6179, 10024, 11, 1310, 291, 362, 18017, 3652, 11, 562, 291, 51132], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 241, "seek": 90292, "start": 918.28, "end": 921.12, "text": " really understand the fundamentals, it makes that way easier.", "tokens": [51132, 534, 1223, 264, 29505, 11, 309, 1669, 300, 636, 3571, 13, 51274], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 242, "seek": 90292, "start": 921.12, "end": 925.4, "text": " And so the example in classical control theory is that you observe that if you couple a bunch", "tokens": [51274, 400, 370, 264, 1365, 294, 13735, 1969, 5261, 307, 300, 291, 11441, 300, 498, 291, 1916, 257, 3840, 51488], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 243, "seek": 90292, "start": 925.4, "end": 929.48, "text": " of linear controllers and linear systems together, what you get is just one bigger linear system", "tokens": [51488, 295, 8213, 26903, 293, 8213, 3652, 1214, 11, 437, 291, 483, 307, 445, 472, 3801, 8213, 1185, 51692], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 244, "seek": 90292, "start": 929.48, "end": 930.92, "text": " and all of the same stuff applies.", "tokens": [51692, 293, 439, 295, 264, 912, 1507, 13165, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08966267395019531, "compression_ratio": 1.8392282958199357, "no_speech_prob": 0.11914384365081787}, {"id": 245, "seek": 93092, "start": 930.92, "end": 936.28, "text": " So what we were hoping is that by starting to answer this really simple question of just,", "tokens": [50364, 407, 437, 321, 645, 7159, 307, 300, 538, 2891, 281, 1867, 341, 534, 2199, 1168, 295, 445, 11, 50632], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 246, "seek": 93092, "start": 936.28, "end": 938.36, "text": " okay, how much can we control this?", "tokens": [50632, 1392, 11, 577, 709, 393, 321, 1969, 341, 30, 50736], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 247, "seek": 93092, "start": 938.36, "end": 940.4399999999999, "text": " What does the reachability of these LLMs look like?", "tokens": [50736, 708, 775, 264, 2524, 2310, 295, 613, 441, 43, 26386, 574, 411, 30, 50840], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 248, "seek": 93092, "start": 940.4399999999999, "end": 942.0, "text": " We're really hoping to build that up.", "tokens": [50840, 492, 434, 534, 7159, 281, 1322, 300, 493, 13, 50918], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 249, "seek": 93092, "start": 942.0, "end": 945.1999999999999, "text": " And to me, it feels like we're kind of doing our homework where in engineering, we had", "tokens": [50918, 400, 281, 385, 11, 309, 3417, 411, 321, 434, 733, 295, 884, 527, 14578, 689, 294, 7043, 11, 321, 632, 51078], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 250, "seek": 93092, "start": 945.1999999999999, "end": 947.0, "text": " to take all these classes in control.", "tokens": [51078, 281, 747, 439, 613, 5359, 294, 1969, 13, 51168], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 251, "seek": 93092, "start": 947.0, "end": 949.68, "text": " And that was sort of our homework to be able to go into the world.", "tokens": [51168, 400, 300, 390, 1333, 295, 527, 14578, 281, 312, 1075, 281, 352, 666, 264, 1002, 13, 51302], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 252, "seek": 93092, "start": 949.68, "end": 954.5999999999999, "text": " And if it ever comes time to build some electromechanical system and get a PID controller in there,", "tokens": [51302, 400, 498, 309, 1562, 1487, 565, 281, 1322, 512, 7072, 423, 3484, 804, 1185, 293, 483, 257, 430, 2777, 10561, 294, 456, 11, 51548], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 253, "seek": 93092, "start": 954.5999999999999, "end": 958.4399999999999, "text": " now we've done our homework so we can have a sense what to expect, how we could do engineering", "tokens": [51548, 586, 321, 600, 1096, 527, 14578, 370, 321, 393, 362, 257, 2020, 437, 281, 2066, 11, 577, 321, 727, 360, 7043, 51740], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 254, "seek": 93092, "start": 958.4399999999999, "end": 959.4399999999999, "text": " on it.", "tokens": [51740, 322, 309, 13, 51790], "temperature": 0.0, "avg_logprob": -0.16279154648015529, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.002550325123593211}, {"id": 255, "seek": 95944, "start": 959.44, "end": 961.6400000000001, "text": " So that's really where I feel like it's at.", "tokens": [50364, 407, 300, 311, 534, 689, 286, 841, 411, 309, 311, 412, 13, 50474], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 256, "seek": 95944, "start": 961.6400000000001, "end": 965.36, "text": " And I think this is a really promising way to try to get a really fundamental understanding", "tokens": [50474, 400, 286, 519, 341, 307, 257, 534, 20257, 636, 281, 853, 281, 483, 257, 534, 8088, 3701, 50660], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 257, "seek": 95944, "start": 965.36, "end": 967.6400000000001, "text": " of what's going on with these language model systems.", "tokens": [50660, 295, 437, 311, 516, 322, 365, 613, 2856, 2316, 3652, 13, 50774], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 258, "seek": 95944, "start": 967.6400000000001, "end": 968.6400000000001, "text": " Amazing.", "tokens": [50774, 14165, 13, 50824], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 259, "seek": 95944, "start": 968.6400000000001, "end": 972.6, "text": " So in a second, we're going to introduce this concept of reachability.", "tokens": [50824, 407, 294, 257, 1150, 11, 321, 434, 516, 281, 5366, 341, 3410, 295, 2524, 2310, 13, 51022], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 260, "seek": 95944, "start": 972.6, "end": 977.0400000000001, "text": " But I've thought about this because I've had a couple of days to reflect on this.", "tokens": [51022, 583, 286, 600, 1194, 466, 341, 570, 286, 600, 632, 257, 1916, 295, 1708, 281, 5031, 322, 341, 13, 51244], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 261, "seek": 95944, "start": 977.0400000000001, "end": 980.9200000000001, "text": " And my intuition, intuitions just seem a little bit mixed up.", "tokens": [51244, 400, 452, 24002, 11, 16224, 626, 445, 1643, 257, 707, 857, 7467, 493, 13, 51438], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 262, "seek": 95944, "start": 980.9200000000001, "end": 985.1600000000001, "text": " So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building", "tokens": [51438, 407, 286, 600, 19770, 22924, 14256, 3812, 11, 337, 1365, 11, 293, 415, 311, 1096, 3195, 295, 589, 11, 291, 458, 11, 2390, 51650], "temperature": 0.0, "avg_logprob": -0.13942341371016068, "compression_ratio": 1.6325878594249201, "no_speech_prob": 0.01775730773806572}, {"id": 263, "seek": 98516, "start": 985.16, "end": 990.56, "text": " on adversarial examples and writing algorithms to find adversarial examples.", "tokens": [50364, 322, 17641, 44745, 5110, 293, 3579, 14642, 281, 915, 17641, 44745, 5110, 13, 50634], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 264, "seek": 98516, "start": 990.56, "end": 994.52, "text": " And we know that neural networks are not robust.", "tokens": [50634, 400, 321, 458, 300, 18161, 9590, 366, 406, 13956, 13, 50832], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 265, "seek": 98516, "start": 994.52, "end": 1000.16, "text": " You can quite easily perturb, let's say, an input image in a vision model.", "tokens": [50832, 509, 393, 1596, 3612, 40468, 11, 718, 311, 584, 11, 364, 4846, 3256, 294, 257, 5201, 2316, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 266, "seek": 98516, "start": 1000.16, "end": 1003.9599999999999, "text": " And if it's a classifier, you can make it pretty much say anything with a very small", "tokens": [51114, 400, 498, 309, 311, 257, 1508, 9902, 11, 291, 393, 652, 309, 1238, 709, 584, 1340, 365, 257, 588, 1359, 51304], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 267, "seek": 98516, "start": 1003.9599999999999, "end": 1005.24, "text": " perturbation.", "tokens": [51304, 40468, 399, 13, 51368], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 268, "seek": 98516, "start": 1005.24, "end": 1008.92, "text": " And that's kind of the same thing as what you mean as reachability.", "tokens": [51368, 400, 300, 311, 733, 295, 264, 912, 551, 382, 437, 291, 914, 382, 2524, 2310, 13, 51552], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 269, "seek": 98516, "start": 1008.92, "end": 1013.4399999999999, "text": " It's this idea to kind of reach into the state space and make it do something quite", "tokens": [51552, 467, 311, 341, 1558, 281, 733, 295, 2524, 666, 264, 1785, 1901, 293, 652, 309, 360, 746, 1596, 51778], "temperature": 0.0, "avg_logprob": -0.08779493131135639, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.3013438880443573}, {"id": 270, "seek": 101344, "start": 1013.48, "end": 1015.6800000000001, "text": " weird outside of what you would expect.", "tokens": [50366, 3657, 2380, 295, 437, 291, 576, 2066, 13, 50476], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 271, "seek": 101344, "start": 1015.6800000000001, "end": 1021.0400000000001, "text": " Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do,", "tokens": [50476, 823, 11, 337, 512, 1778, 11, 286, 632, 264, 24002, 11, 293, 286, 586, 519, 286, 478, 2085, 11, 300, 441, 43, 26386, 360, 11, 50744], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 272, "seek": 101344, "start": 1021.0400000000001, "end": 1025.96, "text": " you know, I didn't think they had this problem, but they do have this problem.", "tokens": [50744, 291, 458, 11, 286, 994, 380, 519, 436, 632, 341, 1154, 11, 457, 436, 360, 362, 341, 1154, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 273, "seek": 101344, "start": 1025.96, "end": 1030.68, "text": " And you introduced this really interesting, I guess it started out as a thought experiment", "tokens": [50990, 400, 291, 7268, 341, 534, 1880, 11, 286, 2041, 309, 1409, 484, 382, 257, 1194, 5120, 51226], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 274, "seek": 101344, "start": 1030.68, "end": 1032.48, "text": " and you coded it into a game.", "tokens": [51226, 293, 291, 34874, 309, 666, 257, 1216, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 275, "seek": 101344, "start": 1032.48, "end": 1034.24, "text": " And it's the Roger Federer game.", "tokens": [51316, 400, 309, 311, 264, 17666, 7772, 29566, 1216, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 276, "seek": 101344, "start": 1034.24, "end": 1035.8400000000001, "text": " I think that's quite instructive.", "tokens": [51404, 286, 519, 300, 311, 1596, 7232, 488, 13, 51484], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 277, "seek": 101344, "start": 1035.8400000000001, "end": 1037.3200000000002, "text": " So can you tell us about that?", "tokens": [51484, 407, 393, 291, 980, 505, 466, 300, 30, 51558], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 278, "seek": 101344, "start": 1037.3200000000002, "end": 1038.0, "text": " Yeah, for sure.", "tokens": [51558, 865, 11, 337, 988, 13, 51592], "temperature": 0.0, "avg_logprob": -0.14639465392582, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.15151992440223694}, {"id": 279, "seek": 103800, "start": 1038.0, "end": 1043.12, "text": " So one of the earliest examples that we were thinking about was just a simple example", "tokens": [50364, 407, 472, 295, 264, 20573, 5110, 300, 321, 645, 1953, 466, 390, 445, 257, 2199, 1365, 50620], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 280, "seek": 103800, "start": 1043.12, "end": 1045.56, "text": " of you have this state sequence that's imposed.", "tokens": [50620, 295, 291, 362, 341, 1785, 8310, 300, 311, 26491, 13, 50742], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 281, "seek": 103800, "start": 1045.56, "end": 1046.48, "text": " You don't get to pick it.", "tokens": [50742, 509, 500, 380, 483, 281, 1888, 309, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 282, "seek": 103800, "start": 1046.48, "end": 1048.56, "text": " It says, Roger Federer is the.", "tokens": [50788, 467, 1619, 11, 17666, 7772, 29566, 307, 264, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 283, "seek": 103800, "start": 1048.56, "end": 1052.2, "text": " And then the next thing that you want it to say, the thing that you want the LLM to", "tokens": [50892, 400, 550, 264, 958, 551, 300, 291, 528, 309, 281, 584, 11, 264, 551, 300, 291, 528, 264, 441, 43, 44, 281, 51074], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 284, "seek": 103800, "start": 1052.2, "end": 1054.32, "text": " generate, is the word the greatest.", "tokens": [51074, 8460, 11, 307, 264, 1349, 264, 6636, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 285, "seek": 103800, "start": 1054.32, "end": 1056.72, "text": " So you want to say, Roger Federer is the greatest.", "tokens": [51180, 407, 291, 528, 281, 584, 11, 17666, 7772, 29566, 307, 264, 6636, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 286, "seek": 103800, "start": 1056.72, "end": 1061.48, "text": " And you're trying to pick out a prompt that comes before then that will steer the system", "tokens": [51300, 400, 291, 434, 1382, 281, 1888, 484, 257, 12391, 300, 1487, 949, 550, 300, 486, 30814, 264, 1185, 51538], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 287, "seek": 103800, "start": 1061.48, "end": 1062.76, "text": " so that it'll output that.", "tokens": [51538, 370, 300, 309, 603, 5598, 300, 13, 51602], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 288, "seek": 103800, "start": 1062.76, "end": 1066.6, "text": " So we're basically asking the question, you know, is this word in the reachable set", "tokens": [51602, 407, 321, 434, 1936, 3365, 264, 1168, 11, 291, 458, 11, 307, 341, 1349, 294, 264, 2524, 712, 992, 51794], "temperature": 0.0, "avg_logprob": -0.11270241988332648, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.002182216616347432}, {"id": 289, "seek": 106660, "start": 1066.6, "end": 1071.0, "text": " of outputs, given that we have some finite control over the input, where the goal of", "tokens": [50364, 295, 23930, 11, 2212, 300, 321, 362, 512, 19362, 1969, 670, 264, 4846, 11, 689, 264, 3387, 295, 50584], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 290, "seek": 106660, "start": 1071.0, "end": 1074.36, "text": " the game is to, for one, get it to actually output the right answer, which is the greatest,", "tokens": [50584, 264, 1216, 307, 281, 11, 337, 472, 11, 483, 309, 281, 767, 5598, 264, 558, 1867, 11, 597, 307, 264, 6636, 11, 50752], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 291, "seek": 106660, "start": 1074.36, "end": 1078.1999999999998, "text": " which is a fairly reasonable English thing to say.", "tokens": [50752, 597, 307, 257, 6457, 10585, 3669, 551, 281, 584, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 292, "seek": 106660, "start": 1078.1999999999998, "end": 1083.9599999999998, "text": " And the metric that we use to grade how well you're doing on that is basically how efficiently", "tokens": [50944, 400, 264, 20678, 300, 321, 764, 281, 7204, 577, 731, 291, 434, 884, 322, 300, 307, 1936, 577, 19621, 51232], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 293, "seek": 106660, "start": 1083.9599999999998, "end": 1088.6799999999998, "text": " you're able to do control, where in the original control theory, this idea of efficient or", "tokens": [51232, 291, 434, 1075, 281, 360, 1969, 11, 689, 294, 264, 3380, 1969, 5261, 11, 341, 1558, 295, 7148, 420, 51468], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 294, "seek": 106660, "start": 1088.6799999999998, "end": 1090.24, "text": " optimal control is really important.", "tokens": [51468, 16252, 1969, 307, 534, 1021, 13, 51546], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 295, "seek": 106660, "start": 1090.24, "end": 1094.84, "text": " You have this linear quadratic regularization idea where you're like, I have only a finite", "tokens": [51546, 509, 362, 341, 8213, 37262, 3890, 2144, 1558, 689, 291, 434, 411, 11, 286, 362, 787, 257, 19362, 51776], "temperature": 0.0, "avg_logprob": -0.13143722712993622, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.1559009999036789}, {"id": 296, "seek": 109484, "start": 1094.84, "end": 1096.8799999999999, "text": " energy budget for the signal I put in.", "tokens": [50364, 2281, 4706, 337, 264, 6358, 286, 829, 294, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 297, "seek": 109484, "start": 1096.8799999999999, "end": 1102.1599999999999, "text": " Similarly, with language models, what we're interested in is the minimal length of the", "tokens": [50466, 13157, 11, 365, 2856, 5245, 11, 437, 321, 434, 3102, 294, 307, 264, 13206, 4641, 295, 264, 50730], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 298, "seek": 109484, "start": 1102.1599999999999, "end": 1105.9199999999998, "text": " control input that will steer the model successfully to what you want it to do.", "tokens": [50730, 1969, 4846, 300, 486, 30814, 264, 2316, 10727, 281, 437, 291, 528, 309, 281, 360, 13, 50918], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 299, "seek": 109484, "start": 1105.9199999999998, "end": 1109.8799999999999, "text": " And it turns out that the game is actually very challenging, at least with this GPT-2", "tokens": [50918, 400, 309, 4523, 484, 300, 264, 1216, 307, 767, 588, 7595, 11, 412, 1935, 365, 341, 26039, 51, 12, 17, 51116], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 300, "seek": 109484, "start": 1109.8799999999999, "end": 1113.12, "text": " model, which is the one that we're using right now, since it's just running out of a desktop", "tokens": [51116, 2316, 11, 597, 307, 264, 472, 300, 321, 434, 1228, 558, 586, 11, 1670, 309, 311, 445, 2614, 484, 295, 257, 14502, 51278], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 301, "seek": 109484, "start": 1113.12, "end": 1115.1999999999998, "text": " on my desk at home.", "tokens": [51278, 322, 452, 10026, 412, 1280, 13, 51382], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 302, "seek": 109484, "start": 1115.1999999999998, "end": 1119.8799999999999, "text": " And so, yeah, there's this game that you can play, we can link it where you get to put", "tokens": [51382, 400, 370, 11, 1338, 11, 456, 311, 341, 1216, 300, 291, 393, 862, 11, 321, 393, 2113, 309, 689, 291, 483, 281, 829, 51616], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 303, "seek": 109484, "start": 1119.8799999999999, "end": 1123.8, "text": " in a prompt to the system, and it'll come back to you and say, OK, you got the answer", "tokens": [51616, 294, 257, 12391, 281, 264, 1185, 11, 293, 309, 603, 808, 646, 281, 291, 293, 584, 11, 2264, 11, 291, 658, 264, 1867, 51812], "temperature": 0.0, "avg_logprob": -0.14486398325337993, "compression_ratio": 1.7327327327327327, "no_speech_prob": 0.2565675377845764}, {"id": 304, "seek": 112380, "start": 1123.8, "end": 1129.0, "text": " right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy", "tokens": [50364, 558, 11, 420, 291, 658, 264, 1867, 2085, 11, 382, 731, 382, 1936, 428, 6713, 322, 300, 11, 370, 428, 3278, 12, 317, 27514, 50624], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 305, "seek": 112380, "start": 1129.0, "end": 1132.36, "text": " loss on getting the correct output, the desired output.", "tokens": [50624, 4470, 322, 1242, 264, 3006, 5598, 11, 264, 14721, 5598, 13, 50792], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 306, "seek": 112380, "start": 1132.36, "end": 1136.28, "text": " And the game is to basically get the shortest prompt that will steer the model to the desired", "tokens": [50792, 400, 264, 1216, 307, 281, 1936, 483, 264, 31875, 12391, 300, 486, 30814, 264, 2316, 281, 264, 14721, 50988], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 307, "seek": 112380, "start": 1136.28, "end": 1137.36, "text": " output.", "tokens": [50988, 5598, 13, 51042], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 308, "seek": 112380, "start": 1137.36, "end": 1141.0, "text": " And it's actually quite challenging with GPT-2, where I think only four people, including", "tokens": [51042, 400, 309, 311, 767, 1596, 7595, 365, 26039, 51, 12, 17, 11, 689, 286, 519, 787, 1451, 561, 11, 3009, 51224], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 309, "seek": 112380, "start": 1141.0, "end": 1145.32, "text": " Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4,", "tokens": [51224, 24962, 11, 293, 550, 452, 1277, 5116, 1176, 898, 6911, 11, 567, 321, 632, 1027, 341, 551, 1219, 25409, 47769, 19, 11, 51440], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 310, "seek": 112380, "start": 1145.32, "end": 1148.8799999999999, "text": " which is this resume checker that uses language models to basically predict your probability", "tokens": [51440, 597, 307, 341, 15358, 1520, 260, 300, 4960, 2856, 5245, 281, 1936, 6069, 428, 8482, 51618], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 311, "seek": 112380, "start": 1148.8799999999999, "end": 1150.8, "text": " of getting into a Fang company.", "tokens": [51618, 295, 1242, 666, 257, 25409, 2237, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16606588433258726, "compression_ratio": 1.7195121951219512, "no_speech_prob": 0.16014927625656128}, {"id": 312, "seek": 115080, "start": 1151.32, "end": 1155.32, "text": " I think those two were the only people who actually ended up getting it right, and it", "tokens": [50390, 286, 519, 729, 732, 645, 264, 787, 561, 567, 767, 4590, 493, 1242, 309, 558, 11, 293, 309, 50590], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 313, "seek": 115080, "start": 1155.32, "end": 1157.1599999999999, "text": " turns out to be very difficult.", "tokens": [50590, 4523, 484, 281, 312, 588, 2252, 13, 50682], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 314, "seek": 115080, "start": 1157.1599999999999, "end": 1162.18, "text": " So that game was sort of a codified sort of interactive version of our initial motivations", "tokens": [50682, 407, 300, 1216, 390, 1333, 295, 257, 17656, 2587, 1333, 295, 15141, 3037, 295, 527, 5883, 39034, 50933], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 315, "seek": 115080, "start": 1162.18, "end": 1165.32, "text": " for this, where it was like, wow, this really simple question that seems like there should", "tokens": [50933, 337, 341, 11, 689, 309, 390, 411, 11, 6076, 11, 341, 534, 2199, 1168, 300, 2544, 411, 456, 820, 51090], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 316, "seek": 115080, "start": 1165.32, "end": 1166.32, "text": " be an easy answer.", "tokens": [51090, 312, 364, 1858, 1867, 13, 51140], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 317, "seek": 115080, "start": 1166.32, "end": 1168.24, "text": " I mean, if there is an easy answer, I'd love to know.", "tokens": [51140, 286, 914, 11, 498, 456, 307, 364, 1858, 1867, 11, 286, 1116, 959, 281, 458, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 318, "seek": 115080, "start": 1168.24, "end": 1174.24, "text": " But the simple question really leads to a problem that's quite difficult to solve, and", "tokens": [51236, 583, 264, 2199, 1168, 534, 6689, 281, 257, 1154, 300, 311, 1596, 2252, 281, 5039, 11, 293, 51536], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 319, "seek": 115080, "start": 1174.24, "end": 1178.9199999999998, "text": " we really have poor insight on, and we're really just trying to get that insight together", "tokens": [51536, 321, 534, 362, 4716, 11269, 322, 11, 293, 321, 434, 534, 445, 1382, 281, 483, 300, 11269, 1214, 51770], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 320, "seek": 115080, "start": 1178.9199999999998, "end": 1180.08, "text": " to understand what's going on there.", "tokens": [51770, 281, 1223, 437, 311, 516, 322, 456, 13, 51828], "temperature": 0.0, "avg_logprob": -0.12278548931253368, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.20664173364639282}, {"id": 321, "seek": 118008, "start": 1180.36, "end": 1184.1599999999999, "text": " And just to jump off that point as well, I think one of the reasons why this game in", "tokens": [50378, 400, 445, 281, 3012, 766, 300, 935, 382, 731, 11, 286, 519, 472, 295, 264, 4112, 983, 341, 1216, 294, 50568], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 322, "seek": 118008, "start": 1184.1599999999999, "end": 1190.6799999999998, "text": " particular is difficult is because we're using GPT-2, and Roger Federer is the blank.", "tokens": [50568, 1729, 307, 2252, 307, 570, 321, 434, 1228, 26039, 51, 12, 17, 11, 293, 17666, 7772, 29566, 307, 264, 8247, 13, 50894], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 323, "seek": 118008, "start": 1190.6799999999998, "end": 1194.6399999999999, "text": " You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots", "tokens": [50894, 509, 576, 519, 6636, 576, 312, 22103, 1238, 1090, 11, 457, 26039, 51, 12, 17, 11, 286, 2041, 309, 311, 8895, 322, 3195, 51092], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 324, "seek": 118008, "start": 1194.6399999999999, "end": 1195.8, "text": " of fill-in-the-blank tasks.", "tokens": [51092, 295, 2836, 12, 259, 12, 3322, 12, 5199, 657, 9608, 13, 51150], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 325, "seek": 118008, "start": 1195.8, "end": 1199.06, "text": " It tends to output just a set of underscores quite often.", "tokens": [51150, 467, 12258, 281, 5598, 445, 257, 992, 295, 16692, 66, 2706, 1596, 2049, 13, 51313], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 326, "seek": 118008, "start": 1199.06, "end": 1203.1999999999998, "text": " To comment on your intuition you've mentioned before on whether language models have this", "tokens": [51313, 1407, 2871, 322, 428, 24002, 291, 600, 2835, 949, 322, 1968, 2856, 5245, 362, 341, 51520], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 327, "seek": 118008, "start": 1203.1999999999998, "end": 1206.9199999999998, "text": " adversarial property, one thing that was really interesting when we were doing some of our", "tokens": [51520, 17641, 44745, 4707, 11, 472, 551, 300, 390, 534, 1880, 562, 321, 645, 884, 512, 295, 527, 51706], "temperature": 0.0, "avg_logprob": -0.13510645567065607, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.038453128188848495}, {"id": 328, "seek": 120692, "start": 1206.92, "end": 1210.44, "text": " initial work was this technique of soft prompting.", "tokens": [50364, 5883, 589, 390, 341, 6532, 295, 2787, 12391, 278, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13594311162045128, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.06752710789442062}, {"id": 329, "seek": 120692, "start": 1210.44, "end": 1216.48, "text": " So soft prompting, instead of selecting discrete tokens, which we want to adversarial change", "tokens": [50540, 407, 2787, 12391, 278, 11, 2602, 295, 18182, 27706, 22667, 11, 597, 321, 528, 281, 17641, 44745, 1319, 50842], "temperature": 0.0, "avg_logprob": -0.13594311162045128, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.06752710789442062}, {"id": 330, "seek": 120692, "start": 1216.48, "end": 1222.4, "text": " the model's behavior with, soft prompting modifies the embedding vectors directly.", "tokens": [50842, 264, 2316, 311, 5223, 365, 11, 2787, 12391, 278, 1072, 11221, 264, 12240, 3584, 18875, 3838, 13, 51138], "temperature": 0.0, "avg_logprob": -0.13594311162045128, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.06752710789442062}, {"id": 331, "seek": 120692, "start": 1222.4, "end": 1227.52, "text": " So you have a lot more fine-grained control over the outputs, and it turns out when you", "tokens": [51138, 407, 291, 362, 257, 688, 544, 2489, 12, 20735, 2001, 1969, 670, 264, 23930, 11, 293, 309, 4523, 484, 562, 291, 51394], "temperature": 0.0, "avg_logprob": -0.13594311162045128, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.06752710789442062}, {"id": 332, "seek": 120692, "start": 1227.52, "end": 1233.04, "text": " soft prompt, when you adversarily attack not the tokens themselves, but the embedding", "tokens": [51394, 2787, 12391, 11, 562, 291, 17641, 3289, 2690, 406, 264, 22667, 2969, 11, 457, 264, 12240, 3584, 51670], "temperature": 0.0, "avg_logprob": -0.13594311162045128, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.06752710789442062}, {"id": 333, "seek": 123304, "start": 1233.04, "end": 1237.76, "text": " vectors, you can send the cross-entropy law straight to zero for whatever token you want", "tokens": [50364, 18875, 11, 291, 393, 2845, 264, 3278, 12, 317, 27514, 2101, 2997, 281, 4018, 337, 2035, 14862, 291, 528, 50600], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 334, "seek": 123304, "start": 1237.76, "end": 1241.76, "text": " with a very tiny adjustment in these embedding vectors.", "tokens": [50600, 365, 257, 588, 5870, 17132, 294, 613, 12240, 3584, 18875, 13, 50800], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 335, "seek": 123304, "start": 1241.76, "end": 1243.08, "text": " So this is very interesting.", "tokens": [50800, 407, 341, 307, 588, 1880, 13, 50866], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 336, "seek": 123304, "start": 1243.08, "end": 1248.0, "text": " This points to the fact that the real challenge with controllability is not necessarily that", "tokens": [50866, 639, 2793, 281, 264, 1186, 300, 264, 957, 3430, 365, 45159, 2310, 307, 406, 4725, 300, 51112], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 337, "seek": 123304, "start": 1248.0, "end": 1253.12, "text": " there aren't adversarial inputs for language models, but just it's very hard to search", "tokens": [51112, 456, 3212, 380, 17641, 44745, 15743, 337, 2856, 5245, 11, 457, 445, 309, 311, 588, 1152, 281, 3164, 51368], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 338, "seek": 123304, "start": 1253.12, "end": 1256.6399999999999, "text": " this exponential space of discrete prompts.", "tokens": [51368, 341, 21510, 1901, 295, 27706, 41095, 13, 51544], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 339, "seek": 123304, "start": 1256.6399999999999, "end": 1261.8, "text": " Yeah, and so I guess there are many degrees of freedom in any deep learning model.", "tokens": [51544, 865, 11, 293, 370, 286, 2041, 456, 366, 867, 5310, 295, 5645, 294, 604, 2452, 2539, 2316, 13, 51802], "temperature": 0.0, "avg_logprob": -0.17993615983842728, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.38222724199295044}, {"id": 340, "seek": 126180, "start": 1261.84, "end": 1263.24, "text": " It's a very highly dimensional model.", "tokens": [50366, 467, 311, 257, 588, 5405, 18795, 2316, 13, 50436], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 341, "seek": 126180, "start": 1263.24, "end": 1267.6, "text": " There are many degrees of freedom, and I'm trying to understand my intuition.", "tokens": [50436, 821, 366, 867, 5310, 295, 5645, 11, 293, 286, 478, 1382, 281, 1223, 452, 24002, 13, 50654], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 342, "seek": 126180, "start": 1267.6, "end": 1275.24, "text": " So it's trained with a softmax, for example, and certainly when you do temperature sampling,", "tokens": [50654, 407, 309, 311, 8895, 365, 257, 2787, 41167, 11, 337, 1365, 11, 293, 3297, 562, 291, 360, 4292, 21179, 11, 51036], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 343, "seek": 126180, "start": 1275.24, "end": 1279.32, "text": " the likelihood is that you're only going to get the top few tokens.", "tokens": [51036, 264, 22119, 307, 300, 291, 434, 787, 516, 281, 483, 264, 1192, 1326, 22667, 13, 51240], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 344, "seek": 126180, "start": 1279.32, "end": 1284.24, "text": " I mean, if you look at the distribution of the probability, it's almost certainly this", "tokens": [51240, 286, 914, 11, 498, 291, 574, 412, 264, 7316, 295, 264, 8482, 11, 309, 311, 1920, 3297, 341, 51486], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 345, "seek": 126180, "start": 1284.24, "end": 1287.2, "text": " one or this one, and then it just tails off very, very quickly.", "tokens": [51486, 472, 420, 341, 472, 11, 293, 550, 309, 445, 28537, 766, 588, 11, 588, 2661, 13, 51634], "temperature": 0.0, "avg_logprob": -0.16925817115284572, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.26367485523223877}, {"id": 346, "seek": 128720, "start": 1287.2, "end": 1292.64, "text": " And I assume that inductive prior was quite deliberate, really, to increase the statistical", "tokens": [50364, 400, 286, 6552, 300, 31612, 488, 4059, 390, 1596, 30515, 11, 534, 11, 281, 3488, 264, 22820, 50636], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 347, "seek": 128720, "start": 1292.64, "end": 1295.0800000000002, "text": " tractability of the model.", "tokens": [50636, 24207, 2310, 295, 264, 2316, 13, 50758], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 348, "seek": 128720, "start": 1295.0800000000002, "end": 1300.8, "text": " But underneath that, in the embedding space, it's not a shell at all, even though there's", "tokens": [50758, 583, 7223, 300, 11, 294, 264, 12240, 3584, 1901, 11, 309, 311, 406, 257, 8720, 412, 439, 11, 754, 1673, 456, 311, 51044], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 349, "seek": 128720, "start": 1300.8, "end": 1305.76, "text": " some low-level surface of embeddings, and you can traverse this.", "tokens": [51044, 512, 2295, 12, 12418, 3753, 295, 12240, 29432, 11, 293, 291, 393, 45674, 341, 13, 51292], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 350, "seek": 128720, "start": 1305.76, "end": 1306.76, "text": " Right.", "tokens": [51292, 1779, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 351, "seek": 128720, "start": 1306.76, "end": 1311.56, "text": " So initially, you might think that this embedding space is a very rich representation of the", "tokens": [51342, 407, 9105, 11, 291, 1062, 519, 300, 341, 12240, 3584, 1901, 307, 257, 588, 4593, 10290, 295, 264, 51582], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 352, "seek": 128720, "start": 1311.56, "end": 1313.44, "text": " meaning of different words.", "tokens": [51582, 3620, 295, 819, 2283, 13, 51676], "temperature": 0.0, "avg_logprob": -0.15205992330418955, "compression_ratio": 1.6502057613168724, "no_speech_prob": 0.0031599714420735836}, {"id": 353, "seek": 131344, "start": 1313.44, "end": 1318.28, "text": " And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for", "tokens": [50364, 400, 3297, 11, 498, 291, 360, 1349, 12, 1353, 12, 303, 66, 420, 747, 257, 6465, 32, 5215, 295, 264, 12240, 3584, 18875, 337, 50606], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 354, "seek": 131344, "start": 1318.28, "end": 1322.24, "text": " any large language model, you'll find something that roughly corresponds to the meaning.", "tokens": [50606, 604, 2416, 2856, 2316, 11, 291, 603, 915, 746, 300, 9810, 23249, 281, 264, 3620, 13, 50804], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 355, "seek": 131344, "start": 1322.24, "end": 1326.3600000000001, "text": " I mean, words that mean similar things are attached more closely together.", "tokens": [50804, 286, 914, 11, 2283, 300, 914, 2531, 721, 366, 8570, 544, 8185, 1214, 13, 51010], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 356, "seek": 131344, "start": 1326.3600000000001, "end": 1330.16, "text": " But this opens the question, if you were to interpolate between two similar words, take", "tokens": [51010, 583, 341, 9870, 264, 1168, 11, 498, 291, 645, 281, 44902, 473, 1296, 732, 2531, 2283, 11, 747, 51200], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 357, "seek": 131344, "start": 1330.16, "end": 1334.3200000000002, "text": " the embedding vector that is halfway between, would you get the halfway in between word,", "tokens": [51200, 264, 12240, 3584, 8062, 300, 307, 15461, 1296, 11, 576, 291, 483, 264, 15461, 294, 1296, 1349, 11, 51408], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 358, "seek": 131344, "start": 1334.3200000000002, "end": 1336.8, "text": " or would you get something that's nonsense, right?", "tokens": [51408, 420, 576, 291, 483, 746, 300, 311, 14925, 11, 558, 30, 51532], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 359, "seek": 131344, "start": 1336.8, "end": 1341.8400000000001, "text": " And I think what you find by these kinds of soft-prompting experiments, by directly", "tokens": [51532, 400, 286, 519, 437, 291, 915, 538, 613, 3685, 295, 2787, 12, 28722, 662, 278, 12050, 11, 538, 3838, 51784], "temperature": 0.0, "avg_logprob": -0.14083614068872788, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.01098278071731329}, {"id": 360, "seek": 134184, "start": 1341.84, "end": 1347.12, "text": " manipulating the embedding vectors, is that the embedding space is actually extremely", "tokens": [50364, 40805, 264, 12240, 3584, 18875, 11, 307, 300, 264, 12240, 3584, 1901, 307, 767, 4664, 50628], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 361, "seek": 134184, "start": 1347.12, "end": 1353.9599999999998, "text": " non-convex, in the sense that by interpolating, you don't just get an average value between", "tokens": [50628, 2107, 12, 1671, 303, 87, 11, 294, 264, 2020, 300, 538, 44902, 990, 11, 291, 500, 380, 445, 483, 364, 4274, 2158, 1296, 50970], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 362, "seek": 134184, "start": 1353.9599999999998, "end": 1354.9599999999998, "text": " the two of them.", "tokens": [50970, 264, 732, 295, 552, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 363, "seek": 134184, "start": 1354.9599999999998, "end": 1355.9599999999998, "text": " Yeah.", "tokens": [51020, 865, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 364, "seek": 134184, "start": 1355.9599999999998, "end": 1360.36, "text": " I don't know if this is best to get into, but one of the techniques we were trying to", "tokens": [51070, 286, 500, 380, 458, 498, 341, 307, 1151, 281, 483, 666, 11, 457, 472, 295, 264, 7512, 321, 645, 1382, 281, 51290], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 365, "seek": 134184, "start": 1360.36, "end": 1363.28, "text": " use is this technique called gumball softmax.", "tokens": [51290, 764, 307, 341, 6532, 1219, 290, 2860, 336, 2787, 41167, 13, 51436], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 366, "seek": 134184, "start": 1363.28, "end": 1369.1599999999999, "text": " So instead of a discrete search over the token space, one thing you can do is it's kind of", "tokens": [51436, 407, 2602, 295, 257, 27706, 3164, 670, 264, 14862, 1901, 11, 472, 551, 291, 393, 360, 307, 309, 311, 733, 295, 51730], "temperature": 0.0, "avg_logprob": -0.1653712590535482, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.1081080511212349}, {"id": 367, "seek": 136916, "start": 1369.16, "end": 1374.68, "text": " like the repair metrization trick for variation autoencoders, but it works for a categorical", "tokens": [50364, 411, 264, 10535, 1131, 24959, 399, 4282, 337, 12990, 8399, 22660, 378, 433, 11, 457, 309, 1985, 337, 257, 19250, 804, 50640], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 368, "seek": 136916, "start": 1374.68, "end": 1376.28, "text": " distribution.", "tokens": [50640, 7316, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 369, "seek": 136916, "start": 1376.28, "end": 1382.3600000000001, "text": " And so you can use this trick, and it essentially works by kind of interpolating between embeddings.", "tokens": [50720, 400, 370, 291, 393, 764, 341, 4282, 11, 293, 309, 4476, 1985, 538, 733, 295, 44902, 990, 1296, 12240, 29432, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 370, "seek": 136916, "start": 1382.3600000000001, "end": 1386.28, "text": " But it actually was very difficult to get to converge and did not even close to rival", "tokens": [51024, 583, 309, 767, 390, 588, 2252, 281, 483, 281, 41881, 293, 630, 406, 754, 1998, 281, 16286, 51220], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 371, "seek": 136916, "start": 1386.28, "end": 1389.16, "text": " the performance of GCG.", "tokens": [51220, 264, 3389, 295, 29435, 38, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 372, "seek": 136916, "start": 1389.16, "end": 1394.96, "text": " My intuition is that when you take a data point off the manifold, because these neural", "tokens": [51364, 1222, 24002, 307, 300, 562, 291, 747, 257, 1412, 935, 766, 264, 47138, 11, 570, 613, 18161, 51654], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 373, "seek": 136916, "start": 1394.96, "end": 1397.92, "text": " networks, they do learn a manifold of language.", "tokens": [51654, 9590, 11, 436, 360, 1466, 257, 47138, 295, 2856, 13, 51802], "temperature": 0.0, "avg_logprob": -0.1411652825095437, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.07223658263683319}, {"id": 374, "seek": 139792, "start": 1397.92, "end": 1402.64, "text": " I thought if you take a data point off the manifold, it would cause some kind of mode", "tokens": [50364, 286, 1194, 498, 291, 747, 257, 1412, 935, 766, 264, 47138, 11, 309, 576, 3082, 512, 733, 295, 4391, 50600], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 375, "seek": 139792, "start": 1402.64, "end": 1403.64, "text": " collapse.", "tokens": [50600, 15584, 13, 50650], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 376, "seek": 139792, "start": 1403.64, "end": 1408.16, "text": " It would just cause the network to become chaotic and go crazy.", "tokens": [50650, 467, 576, 445, 3082, 264, 3209, 281, 1813, 27013, 293, 352, 3219, 13, 50876], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 377, "seek": 139792, "start": 1408.16, "end": 1410.88, "text": " But apparently that's not the case.", "tokens": [50876, 583, 7970, 300, 311, 406, 264, 1389, 13, 51012], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 378, "seek": 139792, "start": 1410.88, "end": 1411.88, "text": " Can it recover?", "tokens": [51012, 1664, 309, 8114, 30, 51062], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 379, "seek": 139792, "start": 1411.88, "end": 1416.3200000000002, "text": " It's almost like if you put a bunch of tokens in which are just really weird, and then you", "tokens": [51062, 467, 311, 1920, 411, 498, 291, 829, 257, 3840, 295, 22667, 294, 597, 366, 445, 534, 3657, 11, 293, 550, 291, 51284], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 380, "seek": 139792, "start": 1416.3200000000002, "end": 1419.76, "text": " just carry on, it's like the language model recovers.", "tokens": [51284, 445, 3985, 322, 11, 309, 311, 411, 264, 2856, 2316, 7759, 840, 13, 51456], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 381, "seek": 139792, "start": 1419.76, "end": 1422.8400000000001, "text": " It finds coherence again, and then it just carries on.", "tokens": [51456, 467, 10704, 26528, 655, 797, 11, 293, 550, 309, 445, 16402, 322, 13, 51610], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 382, "seek": 139792, "start": 1422.8400000000001, "end": 1423.8400000000001, "text": " Yeah.", "tokens": [51610, 865, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14034418852432914, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.013193849474191666}, {"id": 383, "seek": 142384, "start": 1424.52, "end": 1428.36, "text": " It's honestly a really hard question to answer, where in different regimes, we've noticed", "tokens": [50398, 467, 311, 6095, 257, 534, 1152, 1168, 281, 1867, 11, 689, 294, 819, 45738, 11, 321, 600, 5694, 50590], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 384, "seek": 142384, "start": 1428.36, "end": 1434.1599999999999, "text": " different things where if you choose this adversarial prompt so that basically these", "tokens": [50590, 819, 721, 689, 498, 291, 2826, 341, 17641, 44745, 12391, 370, 300, 1936, 613, 50880], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 385, "seek": 142384, "start": 1434.1599999999999, "end": 1439.9199999999998, "text": " prompt optimization algorithms all work in the same way where you're trying to maximize", "tokens": [50880, 12391, 19618, 14642, 439, 589, 294, 264, 912, 636, 689, 291, 434, 1382, 281, 19874, 51168], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 386, "seek": 142384, "start": 1439.9199999999998, "end": 1444.3999999999999, "text": " the likelihood of some desired string, and then you're able to modify some input.", "tokens": [51168, 264, 22119, 295, 512, 14721, 6798, 11, 293, 550, 291, 434, 1075, 281, 16927, 512, 4846, 13, 51392], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 387, "seek": 142384, "start": 1444.3999999999999, "end": 1450.3999999999999, "text": " And so depending on how you choose that, you can do the optimization so that the model", "tokens": [51392, 400, 370, 5413, 322, 577, 291, 2826, 300, 11, 291, 393, 360, 264, 19618, 370, 300, 264, 2316, 51692], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 388, "seek": 142384, "start": 1450.3999999999999, "end": 1452.6399999999999, "text": " will output some gibberish.", "tokens": [51692, 486, 5598, 512, 4553, 43189, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1353142495248832, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.017437152564525604}, {"id": 389, "seek": 145264, "start": 1453.0, "end": 1457.1200000000001, "text": " It seems like depending on the model, depending on the sampling techniques, I've seen it go", "tokens": [50382, 467, 2544, 411, 5413, 322, 264, 2316, 11, 5413, 322, 264, 21179, 7512, 11, 286, 600, 1612, 309, 352, 50588], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 390, "seek": 145264, "start": 1457.1200000000001, "end": 1461.24, "text": " both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable", "tokens": [50588, 1293, 2098, 689, 2171, 309, 603, 8114, 934, 300, 11, 2171, 309, 603, 722, 17746, 10585, 50794], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 391, "seek": 145264, "start": 1461.24, "end": 1466.0400000000002, "text": " coherent text, and other times it seems like it'll continue to generate some random stuff.", "tokens": [50794, 36239, 2487, 11, 293, 661, 1413, 309, 2544, 411, 309, 603, 2354, 281, 8460, 512, 4974, 1507, 13, 51034], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 392, "seek": 145264, "start": 1466.0400000000002, "end": 1469.0, "text": " It'll kind of be in this outer distribution mode.", "tokens": [51034, 467, 603, 733, 295, 312, 294, 341, 10847, 7316, 4391, 13, 51182], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 393, "seek": 145264, "start": 1469.0, "end": 1474.0800000000002, "text": " I think that that's one of the reasons that I think that these adversarial examples, studying", "tokens": [51182, 286, 519, 300, 300, 311, 472, 295, 264, 4112, 300, 286, 519, 300, 613, 17641, 44745, 5110, 11, 7601, 51436], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 394, "seek": 145264, "start": 1474.0800000000002, "end": 1477.44, "text": " them as well as this control theory stuff is really important where it's like, yeah,", "tokens": [51436, 552, 382, 731, 382, 341, 1969, 5261, 1507, 307, 534, 1021, 689, 309, 311, 411, 11, 1338, 11, 51604], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 395, "seek": 145264, "start": 1477.44, "end": 1482.6000000000001, "text": " if you have a system in the real world where tokens are coming in, you're actually processing", "tokens": [51604, 498, 291, 362, 257, 1185, 294, 264, 957, 1002, 689, 22667, 366, 1348, 294, 11, 291, 434, 767, 9007, 51862], "temperature": 0.0, "avg_logprob": -0.1409923048580394, "compression_ratio": 1.878125, "no_speech_prob": 0.006487191654741764}, {"id": 396, "seek": 148260, "start": 1482.76, "end": 1487.36, "text": " them from real users, you don't have total control, but the user is the one who's giving", "tokens": [50372, 552, 490, 957, 5022, 11, 291, 500, 380, 362, 3217, 1969, 11, 457, 264, 4195, 307, 264, 472, 567, 311, 2902, 50602], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 397, "seek": 148260, "start": 1487.36, "end": 1488.36, "text": " the control input.", "tokens": [50602, 264, 1969, 4846, 13, 50652], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 398, "seek": 148260, "start": 1488.36, "end": 1492.48, "text": " You want to make sure that your system is sort of robust to that, where there's a lot", "tokens": [50652, 509, 528, 281, 652, 988, 300, 428, 1185, 307, 1333, 295, 13956, 281, 300, 11, 689, 456, 311, 257, 688, 50858], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 399, "seek": 148260, "start": 1492.48, "end": 1496.48, "text": " of really complicated interactions as it turns out between, for instance, the tokenizer", "tokens": [50858, 295, 534, 6179, 13280, 382, 309, 4523, 484, 1296, 11, 337, 5197, 11, 264, 14862, 6545, 51058], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 400, "seek": 148260, "start": 1496.48, "end": 1500.52, "text": " and the incoming strings, where when you do this prompt optimization, sometimes it'll", "tokens": [51058, 293, 264, 22341, 13985, 11, 689, 562, 291, 360, 341, 12391, 19618, 11, 2171, 309, 603, 51260], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 401, "seek": 148260, "start": 1500.52, "end": 1504.3999999999999, "text": " come out with a sequence of tokens that if you convert it to a string and then convert", "tokens": [51260, 808, 484, 365, 257, 8310, 295, 22667, 300, 498, 291, 7620, 309, 281, 257, 6798, 293, 550, 7620, 51454], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 402, "seek": 148260, "start": 1504.3999999999999, "end": 1509.04, "text": " it back to tokens, it'll actually be very different, which we ran into with this game", "tokens": [51454, 309, 646, 281, 22667, 11, 309, 603, 767, 312, 588, 819, 11, 597, 321, 5872, 666, 365, 341, 1216, 51686], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 403, "seek": 148260, "start": 1509.04, "end": 1510.48, "text": " where I was like, oh, I'm going to cheat at this game.", "tokens": [51686, 689, 286, 390, 411, 11, 1954, 11, 286, 478, 516, 281, 17470, 412, 341, 1216, 13, 51758], "temperature": 0.0, "avg_logprob": -0.10144126737439954, "compression_ratio": 1.797583081570997, "no_speech_prob": 0.013633093796670437}, {"id": 404, "seek": 151048, "start": 1510.52, "end": 1511.8, "text": " I want to be the top prompter.", "tokens": [50366, 286, 528, 281, 312, 264, 1192, 2234, 79, 391, 13, 50430], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 405, "seek": 151048, "start": 1512.0, "end": 1515.2, "text": " So I'm just going to use some of the algorithms that we had from our GitHub repository, the", "tokens": [50440, 407, 286, 478, 445, 516, 281, 764, 512, 295, 264, 14642, 300, 321, 632, 490, 527, 23331, 25841, 11, 264, 50600], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 406, "seek": 151048, "start": 1515.2, "end": 1518.64, "text": " magic words GitHub repository to basically optimize these prompts.", "tokens": [50600, 5585, 2283, 23331, 25841, 281, 1936, 19719, 613, 41095, 13, 50772], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 407, "seek": 151048, "start": 1518.64, "end": 1522.44, "text": " But then when you convert it back to a string, then it turns out not to work as well.", "tokens": [50772, 583, 550, 562, 291, 7620, 309, 646, 281, 257, 6798, 11, 550, 309, 4523, 484, 406, 281, 589, 382, 731, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 408, "seek": 151048, "start": 1522.72, "end": 1527.08, "text": " And so, yeah, I think that answering that question and seeing when is it that the model", "tokens": [50976, 400, 370, 11, 1338, 11, 286, 519, 300, 13430, 300, 1168, 293, 2577, 562, 307, 309, 300, 264, 2316, 51194], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 409, "seek": 151048, "start": 1527.08, "end": 1530.76, "text": " will actually be able to recover, is it a function of how big the model is, are bigger", "tokens": [51194, 486, 767, 312, 1075, 281, 8114, 11, 307, 309, 257, 2445, 295, 577, 955, 264, 2316, 307, 11, 366, 3801, 51378], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 410, "seek": 151048, "start": 1530.76, "end": 1534.48, "text": " models better at recovering, or is it the case that bigger models are maybe more", "tokens": [51378, 5245, 1101, 412, 29180, 11, 420, 307, 309, 264, 1389, 300, 3801, 5245, 366, 1310, 544, 51564], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 411, "seek": 151048, "start": 1534.48, "end": 1539.48, "text": " controllable, maybe you can shift these models into this weird sort of, sorry, just", "tokens": [51564, 45159, 712, 11, 1310, 291, 393, 5513, 613, 5245, 666, 341, 3657, 1333, 295, 11, 2597, 11, 445, 51814], "temperature": 0.0, "avg_logprob": -0.14842328272367777, "compression_ratio": 1.8035190615835777, "no_speech_prob": 0.010651613585650921}, {"id": 412, "seek": 153948, "start": 1539.48, "end": 1544.32, "text": " on the mic, but this sort of out of distribution regime where they're generating this seemingly", "tokens": [50364, 322, 264, 3123, 11, 457, 341, 1333, 295, 484, 295, 7316, 13120, 689, 436, 434, 17746, 341, 18709, 50606], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 413, "seek": 153948, "start": 1544.32, "end": 1547.96, "text": " random output based on seemingly random input.", "tokens": [50606, 4974, 5598, 2361, 322, 18709, 4974, 4846, 13, 50788], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 414, "seek": 153948, "start": 1548.28, "end": 1552.72, "text": " And so, yeah, I think that that question is really, really important and is one that is, I", "tokens": [50804, 400, 370, 11, 1338, 11, 286, 519, 300, 300, 1168, 307, 534, 11, 534, 1021, 293, 307, 472, 300, 307, 11, 286, 51026], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 415, "seek": 153948, "start": 1552.72, "end": 1556.0, "text": " think, well addressed through considering them as systems, which is sort of the thesis", "tokens": [51026, 519, 11, 731, 13847, 807, 8079, 552, 382, 3652, 11, 597, 307, 1333, 295, 264, 22288, 51190], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 416, "seek": 153948, "start": 1556.0, "end": 1556.56, "text": " of this paper.", "tokens": [51190, 295, 341, 3035, 13, 51218], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 417, "seek": 153948, "start": 1556.56, "end": 1562.04, "text": " And we're trying to get a grip on what exactly the case is, you know, is it going to be", "tokens": [51218, 400, 321, 434, 1382, 281, 483, 257, 12007, 322, 437, 2293, 264, 1389, 307, 11, 291, 458, 11, 307, 309, 516, 281, 312, 51492], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 418, "seek": 153948, "start": 1562.04, "end": 1565.2, "text": " able to recover, is that a consistent behavior, or is it not?", "tokens": [51492, 1075, 281, 8114, 11, 307, 300, 257, 8398, 5223, 11, 420, 307, 309, 406, 30, 51650], "temperature": 0.0, "avg_logprob": -0.14917675393526672, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031722113490104675}, {"id": 419, "seek": 156520, "start": 1565.96, "end": 1570.2, "text": " There's this sort of weird recurrence relationship between the prompt and then the", "tokens": [50402, 821, 311, 341, 1333, 295, 3657, 18680, 10760, 2480, 1296, 264, 12391, 293, 550, 264, 50614], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 420, "seek": 156520, "start": 1570.2, "end": 1573.32, "text": " stuff that the language model generates, and then the stuff that's generated in the", "tokens": [50614, 1507, 300, 264, 2856, 2316, 23815, 11, 293, 550, 264, 1507, 300, 311, 10833, 294, 264, 50770], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 421, "seek": 156520, "start": 1573.32, "end": 1577.16, "text": " future, where in effect, you know, you're able to pick a prompt, and then the language", "tokens": [50770, 2027, 11, 689, 294, 1802, 11, 291, 458, 11, 291, 434, 1075, 281, 1888, 257, 12391, 11, 293, 550, 264, 2856, 50962], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 422, "seek": 156520, "start": 1577.16, "end": 1578.56, "text": " model will generate some more text.", "tokens": [50962, 2316, 486, 8460, 512, 544, 2487, 13, 51032], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 423, "seek": 156520, "start": 1578.76, "end": 1581.2, "text": " But then that text becomes sort of part of the prompt as well.", "tokens": [51042, 583, 550, 300, 2487, 3643, 1333, 295, 644, 295, 264, 12391, 382, 731, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 424, "seek": 156520, "start": 1581.44, "end": 1586.4, "text": " So it seems like maybe there could be these sort of degenerate states where if you start", "tokens": [51176, 407, 309, 2544, 411, 1310, 456, 727, 312, 613, 1333, 295, 40520, 473, 4368, 689, 498, 291, 722, 51424], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 425, "seek": 156520, "start": 1586.4, "end": 1590.64, "text": " with this seed of chaos, it'll basically branch out and the future strings that it", "tokens": [51424, 365, 341, 8871, 295, 14158, 11, 309, 603, 1936, 9819, 484, 293, 264, 2027, 13985, 300, 309, 51636], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 426, "seek": 156520, "start": 1590.64, "end": 1592.96, "text": " generates is going to prompt it into being more and more chaotic.", "tokens": [51636, 23815, 307, 516, 281, 12391, 309, 666, 885, 544, 293, 544, 27013, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10250556040153229, "compression_ratio": 1.9865319865319866, "no_speech_prob": 0.06005555018782616}, {"id": 427, "seek": 159296, "start": 1593.28, "end": 1596.64, "text": " And that's basically stability analysis or sensitivity analysis.", "tokens": [50380, 400, 300, 311, 1936, 11826, 5215, 420, 19392, 5215, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 428, "seek": 159296, "start": 1596.88, "end": 1601.24, "text": " And there's all this like rich vocabulary and all of these people who have spent", "tokens": [50560, 400, 456, 311, 439, 341, 411, 4593, 19864, 293, 439, 295, 613, 561, 567, 362, 4418, 50778], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 429, "seek": 159296, "start": 1601.28, "end": 1605.8, "text": " basically hundreds of years thinking about these concepts for both, you know, discrete", "tokens": [50780, 1936, 6779, 295, 924, 1953, 466, 613, 10392, 337, 1293, 11, 291, 458, 11, 27706, 51006], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 430, "seek": 159296, "start": 1605.8, "end": 1610.0, "text": " and continuous dynamical systems that we get to build on top of and basically use", "tokens": [51006, 293, 10957, 5999, 804, 3652, 300, 321, 483, 281, 1322, 322, 1192, 295, 293, 1936, 764, 51216], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 431, "seek": 159296, "start": 1610.0, "end": 1612.64, "text": " their insights to understand, you know, what does it mean?", "tokens": [51216, 641, 14310, 281, 1223, 11, 291, 458, 11, 437, 775, 309, 914, 30, 51348], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 432, "seek": 159296, "start": 1612.68, "end": 1613.8400000000001, "text": " What does stability really mean?", "tokens": [51350, 708, 775, 11826, 534, 914, 30, 51408], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 433, "seek": 159296, "start": 1613.8400000000001, "end": 1618.16, "text": " We can just draw those definitions in, apply them to our generalized form of a", "tokens": [51408, 492, 393, 445, 2642, 729, 21988, 294, 11, 3079, 552, 281, 527, 44498, 1254, 295, 257, 51624], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 434, "seek": 159296, "start": 1618.16, "end": 1620.04, "text": " system, a language model system.", "tokens": [51624, 1185, 11, 257, 2856, 2316, 1185, 13, 51718], "temperature": 0.0, "avg_logprob": -0.11203873463166066, "compression_ratio": 1.7559322033898306, "no_speech_prob": 0.024409489706158638}, {"id": 435, "seek": 162004, "start": 1620.28, "end": 1623.96, "text": " And I think that's why the control theoretic aspect is exciting, where you can", "tokens": [50376, 400, 286, 519, 300, 311, 983, 264, 1969, 14308, 299, 4171, 307, 4670, 11, 689, 291, 393, 50560], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 436, "seek": 162004, "start": 1623.96, "end": 1627.2, "text": " actually ask these questions in a very concrete and reasonable way.", "tokens": [50560, 767, 1029, 613, 1651, 294, 257, 588, 9859, 293, 10585, 636, 13, 50722], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 437, "seek": 162004, "start": 1627.52, "end": 1631.84, "text": " And the best part is that people haven't really been using these, these ideas or", "tokens": [50738, 400, 264, 1151, 644, 307, 300, 561, 2378, 380, 534, 668, 1228, 613, 11, 613, 3487, 420, 50954], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 438, "seek": 162004, "start": 1631.84, "end": 1634.72, "text": " using this vocabulary to describe the questions that we're trying to answer.", "tokens": [50954, 1228, 341, 19864, 281, 6786, 264, 1651, 300, 321, 434, 1382, 281, 1867, 13, 51098], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 439, "seek": 162004, "start": 1634.92, "end": 1638.84, "text": " And so most of these things, if you just spin up, you know, a small GPU and test", "tokens": [51108, 400, 370, 881, 295, 613, 721, 11, 498, 291, 445, 6060, 493, 11, 291, 458, 11, 257, 1359, 18407, 293, 1500, 51304], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 440, "seek": 162004, "start": 1638.84, "end": 1641.6399999999999, "text": " some stuff out with a seven billion parameter model, you're actually doing new", "tokens": [51304, 512, 1507, 484, 365, 257, 3407, 5218, 13075, 2316, 11, 291, 434, 767, 884, 777, 51444], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 441, "seek": 162004, "start": 1641.6399999999999, "end": 1644.3999999999999, "text": " research and it's actually some useful research, in my opinion, where you're", "tokens": [51444, 2132, 293, 309, 311, 767, 512, 4420, 2132, 11, 294, 452, 4800, 11, 689, 291, 434, 51582], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 442, "seek": 162004, "start": 1644.6, "end": 1648.0, "text": " getting a sense of the control theoretic properties of language models.", "tokens": [51592, 1242, 257, 2020, 295, 264, 1969, 14308, 299, 7221, 295, 2856, 5245, 13, 51762], "temperature": 0.0, "avg_logprob": -0.08746728030118076, "compression_ratio": 1.7923976608187135, "no_speech_prob": 0.004197903908789158}, {"id": 443, "seek": 164800, "start": 1648.24, "end": 1651.2, "text": " And to me, that felt like the most exciting thing here.", "tokens": [50376, 400, 281, 385, 11, 300, 2762, 411, 264, 881, 4670, 551, 510, 13, 50524], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 444, "seek": 164800, "start": 1651.24, "end": 1654.56, "text": " The open questions are the most exciting part of the paper to me, where we've", "tokens": [50526, 440, 1269, 1651, 366, 264, 881, 4670, 644, 295, 264, 3035, 281, 385, 11, 689, 321, 600, 50692], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 445, "seek": 164800, "start": 1654.56, "end": 1658.72, "text": " taken a stab at basically the, you know, empirical study of controllability by", "tokens": [50692, 2726, 257, 16343, 412, 1936, 264, 11, 291, 458, 11, 31886, 2979, 295, 45159, 2310, 538, 50900], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 446, "seek": 164800, "start": 1658.72, "end": 1661.84, "text": " sampling these wiki tech sequences, seeing if we can control the next", "tokens": [50900, 21179, 613, 261, 9850, 7553, 22978, 11, 2577, 498, 321, 393, 1969, 264, 958, 51056], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 447, "seek": 164800, "start": 1661.84, "end": 1666.16, "text": " token, the next few tokens, as well as some sort of theoretical results on", "tokens": [51056, 14862, 11, 264, 958, 1326, 22667, 11, 382, 731, 382, 512, 1333, 295, 20864, 3542, 322, 51272], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 448, "seek": 164800, "start": 1666.16, "end": 1667.68, "text": " self-attention and its controllability.", "tokens": [51272, 2698, 12, 1591, 1251, 293, 1080, 45159, 2310, 13, 51348], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 449, "seek": 164800, "start": 1667.92, "end": 1671.36, "text": " But then all of these open questions emerged just because we're now", "tokens": [51360, 583, 550, 439, 295, 613, 1269, 1651, 20178, 445, 570, 321, 434, 586, 51532], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 450, "seek": 164800, "start": 1671.36, "end": 1674.36, "text": " framing it as a system and people for hundreds of years have been thinking", "tokens": [51532, 28971, 309, 382, 257, 1185, 293, 561, 337, 6779, 295, 924, 362, 668, 1953, 51682], "temperature": 0.0, "avg_logprob": -0.10966443281907301, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.009409701451659203}, {"id": 451, "seek": 167436, "start": 1674.36, "end": 1678.7199999999998, "text": " really, really deeply about how you understand systems when they're used in", "tokens": [50364, 534, 11, 534, 8760, 466, 577, 291, 1223, 3652, 562, 436, 434, 1143, 294, 50582], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 452, "seek": 167436, "start": 1678.7199999999998, "end": 1681.1999999999998, "text": " the real world and you have this sort of finite control of them.", "tokens": [50582, 264, 957, 1002, 293, 291, 362, 341, 1333, 295, 19362, 1969, 295, 552, 13, 50706], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 453, "seek": 167436, "start": 1682.36, "end": 1683.36, "text": " Yeah, that's really interesting.", "tokens": [50764, 865, 11, 300, 311, 534, 1880, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 454, "seek": 167436, "start": 1683.36, "end": 1686.84, "text": " I mean, I suppose I'm pointing out the obvious here, but these are auto", "tokens": [50814, 286, 914, 11, 286, 7297, 286, 478, 12166, 484, 264, 6322, 510, 11, 457, 613, 366, 8399, 50988], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 455, "seek": 167436, "start": 1686.84, "end": 1687.6799999999998, "text": " regressive models.", "tokens": [50988, 1121, 22733, 5245, 13, 51030], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 456, "seek": 167436, "start": 1687.7199999999998, "end": 1691.3999999999999, "text": " So the answer gets kind of fed back into the prompt and then we rinse and", "tokens": [51032, 407, 264, 1867, 2170, 733, 295, 4636, 646, 666, 264, 12391, 293, 550, 321, 27026, 293, 51216], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 457, "seek": 167436, "start": 1691.3999999999999, "end": 1694.6799999999998, "text": " repeat, which means you can model them as dynamical systems.", "tokens": [51216, 7149, 11, 597, 1355, 291, 393, 2316, 552, 382, 5999, 804, 3652, 13, 51380], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 458, "seek": 167436, "start": 1695.0, "end": 1698.6799999999998, "text": " And that is in stark contrast to something like a vision classifier where, you", "tokens": [51396, 400, 300, 307, 294, 17417, 8712, 281, 746, 411, 257, 5201, 1508, 9902, 689, 11, 291, 51580], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 459, "seek": 167436, "start": 1698.6799999999998, "end": 1700.4799999999998, "text": " know, there's just an input and an output and that's it.", "tokens": [51580, 458, 11, 456, 311, 445, 364, 4846, 293, 364, 5598, 293, 300, 311, 309, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 460, "seek": 167436, "start": 1700.52, "end": 1701.4799999999998, "text": " That that that's the end.", "tokens": [51672, 663, 300, 300, 311, 264, 917, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13925190629630252, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.01213320903480053}, {"id": 461, "seek": 170148, "start": 1701.76, "end": 1706.32, "text": " So now you can get the system into this kind of corrupted state where, you", "tokens": [50378, 407, 586, 291, 393, 483, 264, 1185, 666, 341, 733, 295, 39480, 1785, 689, 11, 291, 50606], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 462, "seek": 170148, "start": 1706.32, "end": 1708.48, "text": " know, you get divergence and decoherence.", "tokens": [50606, 458, 11, 291, 483, 47387, 293, 979, 78, 511, 655, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 463, "seek": 170148, "start": 1708.48, "end": 1712.08, "text": " And as you said, that that that could be analyzed with stability analysis.", "tokens": [50714, 400, 382, 291, 848, 11, 300, 300, 300, 727, 312, 28181, 365, 11826, 5215, 13, 50894], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 464, "seek": 170148, "start": 1712.3600000000001, "end": 1713.44, "text": " But I find that fascinating.", "tokens": [50908, 583, 286, 915, 300, 10343, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 465, "seek": 170148, "start": 1713.44, "end": 1716.64, "text": " But we should just go back quickly to your Roger Federer example.", "tokens": [50962, 583, 321, 820, 445, 352, 646, 2661, 281, 428, 17666, 7772, 29566, 1365, 13, 51122], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 466, "seek": 170148, "start": 1717.0, "end": 1721.28, "text": " So I'm interested in the different ways that we could go about this.", "tokens": [51140, 407, 286, 478, 3102, 294, 264, 819, 2098, 300, 321, 727, 352, 466, 341, 13, 51354], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 467, "seek": 170148, "start": 1721.52, "end": 1725.76, "text": " So the humans were kind of using language and language are a bunch of", "tokens": [51366, 407, 264, 6255, 645, 733, 295, 1228, 2856, 293, 2856, 366, 257, 3840, 295, 51578], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 468, "seek": 170148, "start": 1725.76, "end": 1727.48, "text": " mimetically shared cognitive tools.", "tokens": [51578, 12247, 22652, 5507, 15605, 3873, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14064712524414064, "compression_ratio": 1.658273381294964, "no_speech_prob": 0.005396346095949411}, {"id": 469, "seek": 172748, "start": 1727.68, "end": 1731.1200000000001, "text": " And they were saying things like, you know, you know, basketball is a great", "tokens": [50374, 400, 436, 645, 1566, 721, 411, 11, 291, 458, 11, 291, 458, 11, 11767, 307, 257, 869, 50546], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 470, "seek": 172748, "start": 1731.1200000000001, "end": 1732.88, "text": " and, you know, Joe blogs is great.", "tokens": [50546, 293, 11, 291, 458, 11, 6807, 31038, 307, 869, 13, 50634], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 471, "seek": 172748, "start": 1733.24, "end": 1734.44, "text": " Roger Federer is great.", "tokens": [50652, 17666, 7772, 29566, 307, 869, 13, 50712], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 472, "seek": 172748, "start": 1734.88, "end": 1738.08, "text": " And it wasn't very parsimonious, but it but it worked.", "tokens": [50734, 400, 309, 2067, 380, 588, 21156, 25098, 851, 11, 457, 309, 457, 309, 2732, 13, 50894], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 473, "seek": 172748, "start": 1738.52, "end": 1742.72, "text": " And then, you know, another approach that that that you spoke about is you", "tokens": [50916, 400, 550, 11, 291, 458, 11, 1071, 3109, 300, 300, 300, 291, 7179, 466, 307, 291, 51126], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 474, "seek": 172748, "start": 1742.72, "end": 1746.3600000000001, "text": " could just make a Python program and you can just let's try a neighborhood", "tokens": [51126, 727, 445, 652, 257, 15329, 1461, 293, 291, 393, 445, 718, 311, 853, 257, 7630, 51308], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 475, "seek": 172748, "start": 1746.3600000000001, "end": 1748.24, "text": " greedy search one token at a time.", "tokens": [51308, 28228, 3164, 472, 14862, 412, 257, 565, 13, 51402], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 476, "seek": 172748, "start": 1748.24, "end": 1753.3600000000001, "text": " So we find the nearest token and then we find the second nearest token until", "tokens": [51402, 407, 321, 915, 264, 23831, 14862, 293, 550, 321, 915, 264, 1150, 23831, 14862, 1826, 51658], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 477, "seek": 172748, "start": 1753.3600000000001, "end": 1754.64, "text": " we find the adversarial attack.", "tokens": [51658, 321, 915, 264, 17641, 44745, 2690, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15382333504137144, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.02274683676660061}, {"id": 478, "seek": 175464, "start": 1755.16, "end": 1757.92, "text": " Or we could do like a low level gradient search.", "tokens": [50390, 1610, 321, 727, 360, 411, 257, 2295, 1496, 16235, 3164, 13, 50528], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 479, "seek": 175464, "start": 1758.16, "end": 1760.4, "text": " And then we can find something really weird and wonderful.", "tokens": [50540, 400, 550, 321, 393, 915, 746, 534, 3657, 293, 3715, 13, 50652], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 480, "seek": 175464, "start": 1760.4, "end": 1764.2800000000002, "text": " There might be some esoteric characters that just make it go bananas.", "tokens": [50652, 821, 1062, 312, 512, 785, 21585, 299, 4342, 300, 445, 652, 309, 352, 22742, 13, 50846], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 481, "seek": 175464, "start": 1764.6000000000001, "end": 1768.44, "text": " But these are three very, very different levels of talking to a language model.", "tokens": [50862, 583, 613, 366, 1045, 588, 11, 588, 819, 4358, 295, 1417, 281, 257, 2856, 2316, 13, 51054], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 482, "seek": 175464, "start": 1768.6000000000001, "end": 1772.88, "text": " The word on the street is that language models are a new form of programming", "tokens": [51062, 440, 1349, 322, 264, 4838, 307, 300, 2856, 5245, 366, 257, 777, 1254, 295, 9410, 51276], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 483, "seek": 175464, "start": 1773.2, "end": 1777.48, "text": " that you can just say what you want to do using English language and so on.", "tokens": [51292, 300, 291, 393, 445, 584, 437, 291, 528, 281, 360, 1228, 3669, 2856, 293, 370, 322, 13, 51506], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 484, "seek": 175464, "start": 1778.64, "end": 1782.76, "text": " And language models certainly seem to incorporate that structure.", "tokens": [51564, 400, 2856, 5245, 3297, 1643, 281, 16091, 300, 3877, 13, 51770], "temperature": 0.0, "avg_logprob": -0.09743945663039749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.01003801915794611}, {"id": 485, "seek": 178276, "start": 1783.04, "end": 1787.24, "text": " But the language models themselves are just an inscrutable, you know,", "tokens": [50378, 583, 264, 2856, 5245, 2969, 366, 445, 364, 1028, 10757, 32148, 11, 291, 458, 11, 50588], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 486, "seek": 178276, "start": 1787.36, "end": 1789.56, "text": " set of, of, of neurons, right?", "tokens": [50594, 992, 295, 11, 295, 11, 295, 22027, 11, 558, 30, 50704], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 487, "seek": 178276, "start": 1789.56, "end": 1791.76, "text": " And, and weights and matrices and so on.", "tokens": [50704, 400, 11, 293, 17443, 293, 32284, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 488, "seek": 178276, "start": 1792.2, "end": 1795.24, "text": " So there's some, there's a kind of higher resolution", "tokens": [50836, 407, 456, 311, 512, 11, 456, 311, 257, 733, 295, 2946, 8669, 50988], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 489, "seek": 178276, "start": 1795.24, "end": 1797.48, "text": " shog off going on underneath the covers.", "tokens": [50988, 402, 664, 766, 516, 322, 7223, 264, 10538, 13, 51100], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 490, "seek": 178276, "start": 1797.52, "end": 1799.28, "text": " That's more or less the picture I have.", "tokens": [51102, 663, 311, 544, 420, 1570, 264, 3036, 286, 362, 13, 51190], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 491, "seek": 178276, "start": 1799.28, "end": 1803.64, "text": " We have this interface where we can speak to the language model using language.", "tokens": [51190, 492, 362, 341, 9226, 689, 321, 393, 1710, 281, 264, 2856, 2316, 1228, 2856, 13, 51408], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 492, "seek": 178276, "start": 1804.0, "end": 1809.2, "text": " And if we set up a conversation with a language model where we have different labels,", "tokens": [51426, 400, 498, 321, 992, 493, 257, 3761, 365, 257, 2856, 2316, 689, 321, 362, 819, 16949, 11, 51686], "temperature": 0.0, "avg_logprob": -0.16002334397414636, "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.002773393178358674}, {"id": 493, "seek": 180920, "start": 1809.2, "end": 1813.76, "text": " you know, chat, GBT says this, Cameron says this, and, you know,", "tokens": [50364, 291, 458, 11, 5081, 11, 26809, 51, 1619, 341, 11, 24962, 1619, 341, 11, 293, 11, 291, 458, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 494, "seek": 180920, "start": 1813.76, "end": 1817.0800000000002, "text": " you engage in a conversation because it is seen enough conversations and it's", "tokens": [50592, 291, 4683, 294, 257, 3761, 570, 309, 307, 1612, 1547, 7315, 293, 309, 311, 50758], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 495, "seek": 180920, "start": 1817.0800000000002, "end": 1820.88, "text": " training data, then it's able to play along very fine.", "tokens": [50758, 3097, 1412, 11, 550, 309, 311, 1075, 281, 862, 2051, 588, 2489, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 496, "seek": 180920, "start": 1821.2, "end": 1824.48, "text": " What's going on under the hood, of course, like you say, it's very inscrutable.", "tokens": [50964, 708, 311, 516, 322, 833, 264, 13376, 11, 295, 1164, 11, 411, 291, 584, 11, 309, 311, 588, 1028, 10757, 32148, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 497, "seek": 180920, "start": 1824.8, "end": 1827.64, "text": " It's very difficult to really probe and understand.", "tokens": [51144, 467, 311, 588, 2252, 281, 534, 22715, 293, 1223, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 498, "seek": 180920, "start": 1828.1200000000001, "end": 1831.32, "text": " There are certain techniques in the interpretability literature,", "tokens": [51310, 821, 366, 1629, 7512, 294, 264, 7302, 2310, 10394, 11, 51470], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 499, "seek": 180920, "start": 1831.52, "end": 1835.4, "text": " but I don't think as a whole it's we're even remotely close to having", "tokens": [51480, 457, 286, 500, 380, 519, 382, 257, 1379, 309, 311, 321, 434, 754, 20824, 1998, 281, 1419, 51674], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 500, "seek": 180920, "start": 1835.4, "end": 1837.48, "text": " a complete understanding of how these systems work.", "tokens": [51674, 257, 3566, 3701, 295, 577, 613, 3652, 589, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1558901613408869, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.005137799773365259}, {"id": 501, "seek": 183748, "start": 1837.76, "end": 1841.6, "text": " But that's one of the reasons why I think that control theory is a great way", "tokens": [50378, 583, 300, 311, 472, 295, 264, 4112, 983, 286, 519, 300, 1969, 5261, 307, 257, 869, 636, 50570], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 502, "seek": 183748, "start": 1841.6, "end": 1843.96, "text": " to kind of break in and see what's going on.", "tokens": [50570, 281, 733, 295, 1821, 294, 293, 536, 437, 311, 516, 322, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 503, "seek": 183748, "start": 1843.96, "end": 1847.92, "text": " Because if you just look at the system's input and output characteristics,", "tokens": [50688, 1436, 498, 291, 445, 574, 412, 264, 1185, 311, 4846, 293, 5598, 10891, 11, 50886], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 504, "seek": 183748, "start": 1848.24, "end": 1851.68, "text": " you can really gain a lot of insight into the nature of these systems.", "tokens": [50902, 291, 393, 534, 6052, 257, 688, 295, 11269, 666, 264, 3687, 295, 613, 3652, 13, 51074], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 505, "seek": 183748, "start": 1852.28, "end": 1857.6, "text": " One guiding principle in my life doing engineering and trying to learn", "tokens": [51104, 1485, 25061, 8665, 294, 452, 993, 884, 7043, 293, 1382, 281, 1466, 51370], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 506, "seek": 183748, "start": 1857.6, "end": 1860.56, "text": " about the world has been this quote by Richard Feynman.", "tokens": [51370, 466, 264, 1002, 575, 668, 341, 6513, 538, 9809, 46530, 77, 1601, 13, 51518], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 507, "seek": 183748, "start": 1860.56, "end": 1864.08, "text": " It's very popular. What I cannot create, I cannot understand.", "tokens": [51518, 467, 311, 588, 3743, 13, 708, 286, 2644, 1884, 11, 286, 2644, 1223, 13, 51694], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 508, "seek": 183748, "start": 1864.72, "end": 1866.68, "text": " And yet today we find ourselves in this situation", "tokens": [51726, 400, 1939, 965, 321, 915, 4175, 294, 341, 2590, 51824], "temperature": 0.0, "avg_logprob": -0.13256476002354775, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.004607012029737234}, {"id": 509, "seek": 186668, "start": 1866.68, "end": 1870.48, "text": " with language models where we have these incredibly complex systems we built", "tokens": [50364, 365, 2856, 5245, 689, 321, 362, 613, 6252, 3997, 3652, 321, 3094, 50554], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 510, "seek": 186668, "start": 1870.48, "end": 1872.48, "text": " and yet we can't really get into them.", "tokens": [50554, 293, 1939, 321, 393, 380, 534, 483, 666, 552, 13, 50654], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 511, "seek": 186668, "start": 1872.8, "end": 1877.5600000000002, "text": " So to extend this to today, what I would say is what I cannot control,", "tokens": [50670, 407, 281, 10101, 341, 281, 965, 11, 437, 286, 576, 584, 307, 437, 286, 2644, 1969, 11, 50908], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 512, "seek": 186668, "start": 1877.6000000000001, "end": 1878.6000000000001, "text": " I cannot understand.", "tokens": [50910, 286, 2644, 1223, 13, 50960], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 513, "seek": 186668, "start": 1879.5600000000002, "end": 1884.6000000000001, "text": " The way I think about it is it's almost like you want the language model", "tokens": [51008, 440, 636, 286, 519, 466, 309, 307, 309, 311, 1920, 411, 291, 528, 264, 2856, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 514, "seek": 186668, "start": 1884.6000000000001, "end": 1887.2, "text": " to be a high level controlled, robust interface.", "tokens": [51260, 281, 312, 257, 1090, 1496, 10164, 11, 13956, 9226, 13, 51390], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 515, "seek": 186668, "start": 1887.72, "end": 1891.76, "text": " And it's almost like we're all Marvel characters", "tokens": [51416, 400, 309, 311, 1920, 411, 321, 434, 439, 13837, 4342, 51618], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 516, "seek": 186668, "start": 1891.8, "end": 1895.0, "text": " and we can give secret hidden codes.", "tokens": [51620, 293, 321, 393, 976, 4054, 7633, 14211, 13, 51780], "temperature": 0.0, "avg_logprob": -0.162845702398391, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0008250311948359013}, {"id": 517, "seek": 189500, "start": 1895.16, "end": 1896.0, "text": " It's like me now.", "tokens": [50372, 467, 311, 411, 385, 586, 13, 50414], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 518, "seek": 189500, "start": 1896.0, "end": 1899.16, "text": " Imagine if I could just through telepathy control your behavior", "tokens": [50414, 11739, 498, 286, 727, 445, 807, 4304, 79, 9527, 1969, 428, 5223, 50572], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 519, "seek": 189500, "start": 1899.44, "end": 1901.96, "text": " and anyone can do that with a language model.", "tokens": [50586, 293, 2878, 393, 360, 300, 365, 257, 2856, 2316, 13, 50712], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 520, "seek": 189500, "start": 1901.96, "end": 1905.48, "text": " They can just put weird tokens in and they can manipulate its behavior.", "tokens": [50712, 814, 393, 445, 829, 3657, 22667, 294, 293, 436, 393, 20459, 1080, 5223, 13, 50888], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 521, "seek": 189500, "start": 1906.0, "end": 1909.28, "text": " And there's there's no there's nothing stopping you.", "tokens": [50914, 400, 456, 311, 456, 311, 572, 456, 311, 1825, 12767, 291, 13, 51078], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 522, "seek": 189500, "start": 1909.28, "end": 1910.28, "text": " There's no firewall.", "tokens": [51078, 821, 311, 572, 36109, 13, 51128], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 523, "seek": 189500, "start": 1910.28, "end": 1913.32, "text": " I feel like the this kind of harkens to why we call the paper.", "tokens": [51128, 286, 841, 411, 264, 341, 733, 295, 276, 809, 694, 281, 983, 321, 818, 264, 3035, 13, 51280], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 524, "seek": 189500, "start": 1913.32, "end": 1917.56, "text": " What's the magic word where, you know, the initial reason was just that,", "tokens": [51280, 708, 311, 264, 5585, 1349, 689, 11, 291, 458, 11, 264, 5883, 1778, 390, 445, 300, 11, 51492], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 525, "seek": 189500, "start": 1917.64, "end": 1920.64, "text": " you know, it's almost like the LLM is asking you if you wanted to do something.", "tokens": [51496, 291, 458, 11, 309, 311, 1920, 411, 264, 441, 43, 44, 307, 3365, 291, 498, 291, 1415, 281, 360, 746, 13, 51646], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 526, "seek": 189500, "start": 1920.8, "end": 1921.68, "text": " What's the magic word?", "tokens": [51654, 708, 311, 264, 5585, 1349, 30, 51698], "temperature": 0.0, "avg_logprob": -0.13267143679336763, "compression_ratio": 1.7964912280701755, "no_speech_prob": 0.002883943496271968}, {"id": 527, "seek": 192168, "start": 1921.68, "end": 1924.4, "text": " Like, what's the this key, this weird control prompt", "tokens": [50364, 1743, 11, 437, 311, 264, 341, 2141, 11, 341, 3657, 1969, 12391, 50500], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 528, "seek": 192168, "start": 1924.4, "end": 1925.8, "text": " that will just make it do the right thing?", "tokens": [50500, 300, 486, 445, 652, 309, 360, 264, 558, 551, 30, 50570], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 529, "seek": 192168, "start": 1925.8, "end": 1929.3200000000002, "text": " But I think more generally, you know, I used to be into magic when I was a kid.", "tokens": [50570, 583, 286, 519, 544, 5101, 11, 291, 458, 11, 286, 1143, 281, 312, 666, 5585, 562, 286, 390, 257, 1636, 13, 50746], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 530, "seek": 192168, "start": 1929.3200000000002, "end": 1931.96, "text": " I had to jog at a restaurant doing, you know, card tricks for the patrons", "tokens": [50746, 286, 632, 281, 9464, 412, 257, 6383, 884, 11, 291, 458, 11, 2920, 11733, 337, 264, 27559, 50878], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 531, "seek": 192168, "start": 1931.96, "end": 1933.3600000000001, "text": " while they waited for their food.", "tokens": [50878, 1339, 436, 15240, 337, 641, 1755, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 532, "seek": 192168, "start": 1933.3600000000001, "end": 1939.0800000000002, "text": " And what magic is, is basically you're playing tricks on the human perceptual system", "tokens": [50948, 400, 437, 5585, 307, 11, 307, 1936, 291, 434, 2433, 11733, 322, 264, 1952, 43276, 901, 1185, 51234], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 533, "seek": 192168, "start": 1939.0800000000002, "end": 1942.2, "text": " where there are all of these sort of inductive biases that the human", "tokens": [51234, 689, 456, 366, 439, 295, 613, 1333, 295, 31612, 488, 32152, 300, 264, 1952, 51390], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 534, "seek": 192168, "start": 1942.3200000000002, "end": 1945.76, "text": " perceptual system has where, you know, for instance, if I move something", "tokens": [51396, 43276, 901, 1185, 575, 689, 11, 291, 458, 11, 337, 5197, 11, 498, 286, 1286, 746, 51568], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 535, "seek": 192168, "start": 1945.76, "end": 1948.8, "text": " and I look at it, you naturally will tend to follow that my gaze", "tokens": [51568, 293, 286, 574, 412, 309, 11, 291, 8195, 486, 3928, 281, 1524, 300, 452, 24294, 51720], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 536, "seek": 192168, "start": 1948.8, "end": 1950.92, "text": " and what is moving is generally more salient.", "tokens": [51720, 293, 437, 307, 2684, 307, 5101, 544, 1845, 1196, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1386034473869371, "compression_ratio": 1.826470588235294, "no_speech_prob": 0.0028890608809888363}, {"id": 537, "seek": 195092, "start": 1950.92, "end": 1953.04, "text": " And so then I can like do something over here with my other hand,", "tokens": [50364, 400, 370, 550, 286, 393, 411, 360, 746, 670, 510, 365, 452, 661, 1011, 11, 50470], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 538, "seek": 195092, "start": 1953.04, "end": 1954.3200000000002, "text": " like take something out of my pocket.", "tokens": [50470, 411, 747, 746, 484, 295, 452, 8963, 13, 50534], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 539, "seek": 195092, "start": 1954.3200000000002, "end": 1957.3200000000002, "text": " And then when I display it, they'll be like, oh, my God, where did that come from?", "tokens": [50534, 400, 550, 562, 286, 4674, 309, 11, 436, 603, 312, 411, 11, 1954, 11, 452, 1265, 11, 689, 630, 300, 808, 490, 30, 50684], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 540, "seek": 195092, "start": 1957.3200000000002, "end": 1960.6000000000001, "text": " Right. And what we're discovering, I think, is a sort of similar thing", "tokens": [50684, 1779, 13, 400, 437, 321, 434, 24773, 11, 286, 519, 11, 307, 257, 1333, 295, 2531, 551, 50848], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 541, "seek": 195092, "start": 1960.6000000000001, "end": 1963.8000000000002, "text": " with language models where, for one, you know, people have observed", "tokens": [50848, 365, 2856, 5245, 689, 11, 337, 472, 11, 291, 458, 11, 561, 362, 13095, 51008], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 542, "seek": 195092, "start": 1963.8000000000002, "end": 1967.16, "text": " that if you use sort of human social engineering tricks on them, like,", "tokens": [51008, 300, 498, 291, 764, 1333, 295, 1952, 2093, 7043, 11733, 322, 552, 11, 411, 11, 51176], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 543, "seek": 195092, "start": 1967.28, "end": 1970.0800000000002, "text": " oh, I'll tip you $500, then, you know, it'll do a bit better.", "tokens": [51182, 1954, 11, 286, 603, 4125, 291, 1848, 7526, 11, 550, 11, 291, 458, 11, 309, 603, 360, 257, 857, 1101, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 544, "seek": 195092, "start": 1970.68, "end": 1974.5600000000002, "text": " But then there's this whole other sort of perceptual layer, I guess you could call it", "tokens": [51352, 583, 550, 456, 311, 341, 1379, 661, 1333, 295, 43276, 901, 4583, 11, 286, 2041, 291, 727, 818, 309, 51546], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 545, "seek": 195092, "start": 1974.76, "end": 1978.64, "text": " where there's this sort of chaotic regime of adversarial prompts,", "tokens": [51556, 689, 456, 311, 341, 1333, 295, 27013, 13120, 295, 17641, 44745, 41095, 11, 51750], "temperature": 0.0, "avg_logprob": -0.14488207499186198, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.0017543771537020802}, {"id": 546, "seek": 197864, "start": 1978.68, "end": 1983.4, "text": " kind of like hypnosis, kind of like magic, where if you give it these very", "tokens": [50366, 733, 295, 411, 7420, 46080, 11, 733, 295, 411, 5585, 11, 689, 498, 291, 976, 309, 613, 588, 50602], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 547, "seek": 197864, "start": 1983.4, "end": 1987.5200000000002, "text": " strange, very inhuman looking prompts that will steer it to this,", "tokens": [50602, 5861, 11, 588, 294, 18796, 1237, 41095, 300, 486, 30814, 309, 281, 341, 11, 50808], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 548, "seek": 197864, "start": 1987.64, "end": 1990.92, "text": " to just making a certain output, extremely likely, right?", "tokens": [50814, 281, 445, 1455, 257, 1629, 5598, 11, 4664, 3700, 11, 558, 30, 50978], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 549, "seek": 197864, "start": 1991.24, "end": 1995.4, "text": " And so to me, it feels really similar to digging into like magic", "tokens": [50994, 400, 370, 281, 385, 11, 309, 3417, 534, 2531, 281, 17343, 666, 411, 5585, 51202], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 550, "seek": 197864, "start": 1995.4, "end": 1998.2800000000002, "text": " and the human perceptual system, just with LLMs, where we're learning about", "tokens": [51202, 293, 264, 1952, 43276, 901, 1185, 11, 445, 365, 441, 43, 26386, 11, 689, 321, 434, 2539, 466, 51346], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 551, "seek": 197864, "start": 1998.2800000000002, "end": 2003.0, "text": " basically the shape or the what the nature of these language models are in terms", "tokens": [51346, 1936, 264, 3909, 420, 264, 437, 264, 3687, 295, 613, 2856, 5245, 366, 294, 2115, 51582], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 552, "seek": 197864, "start": 2003.0, "end": 2006.88, "text": " of how they interact with the world and how they, how their dynamics really work.", "tokens": [51582, 295, 577, 436, 4648, 365, 264, 1002, 293, 577, 436, 11, 577, 641, 15679, 534, 589, 13, 51776], "temperature": 0.0, "avg_logprob": -0.15593042681294103, "compression_ratio": 1.7370242214532872, "no_speech_prob": 0.002251148223876953}, {"id": 553, "seek": 200688, "start": 2007.0800000000002, "end": 2011.0400000000002, "text": " And I think that it's very sensible that the control", "tokens": [50374, 400, 286, 519, 300, 309, 311, 588, 25380, 300, 264, 1969, 50572], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 554, "seek": 200688, "start": 2011.0400000000002, "end": 2015.1200000000001, "text": " theoretic perspective would be useful for this, where in classical control", "tokens": [50572, 14308, 299, 4585, 576, 312, 4420, 337, 341, 11, 689, 294, 13735, 1969, 50776], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 555, "seek": 200688, "start": 2015.1200000000001, "end": 2018.96, "text": " theory, trying to control these systems actually taught us a lot about", "tokens": [50776, 5261, 11, 1382, 281, 1969, 613, 3652, 767, 5928, 505, 257, 688, 466, 50968], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 556, "seek": 200688, "start": 2018.96, "end": 2021.0800000000002, "text": " the nature of systems, both linear and nonlinear.", "tokens": [50968, 264, 3687, 295, 3652, 11, 1293, 8213, 293, 2107, 28263, 13, 51074], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 557, "seek": 200688, "start": 2021.3200000000002, "end": 2023.96, "text": " And I think that we have a very similar opportunity here where we're really", "tokens": [51086, 400, 286, 519, 300, 321, 362, 257, 588, 2531, 2650, 510, 689, 321, 434, 534, 51218], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 558, "seek": 200688, "start": 2023.96, "end": 2027.7600000000002, "text": " discovering what is the nature of these language models in terms of control,", "tokens": [51218, 24773, 437, 307, 264, 3687, 295, 613, 2856, 5245, 294, 2115, 295, 1969, 11, 51408], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 559, "seek": 200688, "start": 2027.92, "end": 2031.0, "text": " where these questions don't emerge quite as naturally and don't have", "tokens": [51416, 689, 613, 1651, 500, 380, 21511, 1596, 382, 8195, 293, 500, 380, 362, 51570], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 560, "seek": 200688, "start": 2031.0, "end": 2034.24, "text": " quite as natural of an answer when you're just thinking about them as a sort", "tokens": [51570, 1596, 382, 3303, 295, 364, 1867, 562, 291, 434, 445, 1953, 466, 552, 382, 257, 1333, 51732], "temperature": 0.0, "avg_logprob": -0.11714553063915621, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0008557402761653066}, {"id": 561, "seek": 203424, "start": 2034.24, "end": 2037.4, "text": " of probability distribution over text, thinking about them in terms", "tokens": [50364, 295, 8482, 7316, 670, 2487, 11, 1953, 466, 552, 294, 2115, 50522], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 562, "seek": 203424, "start": 2037.4, "end": 2040.4, "text": " of being systems that have inputs and outputs and these trajectories and the", "tokens": [50522, 295, 885, 3652, 300, 362, 15743, 293, 23930, 293, 613, 18257, 2083, 293, 264, 50672], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 563, "seek": 203424, "start": 2040.4, "end": 2043.84, "text": " like actually really does change the kinds of questions that you end up", "tokens": [50672, 411, 767, 534, 775, 1319, 264, 3685, 295, 1651, 300, 291, 917, 493, 50844], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 564, "seek": 203424, "start": 2044.08, "end": 2046.8, "text": " being able to answer and the kind of understanding that you get about the", "tokens": [50856, 885, 1075, 281, 1867, 293, 264, 733, 295, 3701, 300, 291, 483, 466, 264, 50992], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 565, "seek": 203424, "start": 2046.8, "end": 2049.68, "text": " nature of the system itself, which to me is one of the most exciting things.", "tokens": [50992, 3687, 295, 264, 1185, 2564, 11, 597, 281, 385, 307, 472, 295, 264, 881, 4670, 721, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 566, "seek": 203424, "start": 2049.92, "end": 2052.0, "text": " So yeah, that's so interesting.", "tokens": [51148, 407, 1338, 11, 300, 311, 370, 1880, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 567, "seek": 203424, "start": 2052.0, "end": 2057.2, "text": " The magic example thing, I think we, we think that we are robust, but we're not.", "tokens": [51252, 440, 5585, 1365, 551, 11, 286, 519, 321, 11, 321, 519, 300, 321, 366, 13956, 11, 457, 321, 434, 406, 13, 51512], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 568, "seek": 203424, "start": 2057.2, "end": 2060.08, "text": " Maybe we're system two robust, but we're not system one robust.", "tokens": [51512, 2704, 321, 434, 1185, 732, 13956, 11, 457, 321, 434, 406, 1185, 472, 13956, 13, 51656], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 569, "seek": 203424, "start": 2060.08, "end": 2063.72, "text": " And if you look in the animal kingdom, there are so many examples of, you", "tokens": [51656, 400, 498, 291, 574, 294, 264, 5496, 10231, 11, 456, 366, 370, 867, 5110, 295, 11, 291, 51838], "temperature": 0.0, "avg_logprob": -0.13135175069173177, "compression_ratio": 1.8670694864048338, "no_speech_prob": 0.0015930493827909231}, {"id": 570, "seek": 206372, "start": 2063.72, "end": 2067.04, "text": " know, like a hen, if you make the right kind of clucking noise, the mother will", "tokens": [50364, 458, 11, 411, 257, 22253, 11, 498, 291, 652, 264, 558, 733, 295, 596, 33260, 5658, 11, 264, 2895, 486, 50530], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 571, "seek": 206372, "start": 2067.04, "end": 2069.04, "text": " think that you're that you're the chick.", "tokens": [50530, 519, 300, 291, 434, 300, 291, 434, 264, 14371, 13, 50630], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 572, "seek": 206372, "start": 2069.48, "end": 2072.68, "text": " So it's really, really weird, actually.", "tokens": [50652, 407, 309, 311, 534, 11, 534, 3657, 11, 767, 13, 50812], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 573, "seek": 206372, "start": 2072.68, "end": 2076.08, "text": " And Keith gave me this example of, I think it was from science fiction, that", "tokens": [50812, 400, 20613, 2729, 385, 341, 1365, 295, 11, 286, 519, 309, 390, 490, 3497, 13266, 11, 300, 50982], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 574, "seek": 206372, "start": 2076.3199999999997, "end": 2077.6, "text": " there's a hypothetical image.", "tokens": [50994, 456, 311, 257, 33053, 3256, 13, 51058], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 575, "seek": 206372, "start": 2077.6, "end": 2080.2799999999997, "text": " And if you look at the image, every single person goes into a coma.", "tokens": [51058, 400, 498, 291, 574, 412, 264, 3256, 11, 633, 2167, 954, 1709, 666, 257, 35106, 13, 51192], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 576, "seek": 206372, "start": 2080.64, "end": 2083.7599999999998, "text": " And what's interesting about that is it's a kind of, you know, population", "tokens": [51210, 400, 437, 311, 1880, 466, 300, 307, 309, 311, 257, 733, 295, 11, 291, 458, 11, 4415, 51366], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 577, "seek": 206372, "start": 2083.7599999999998, "end": 2087.24, "text": " level adversarial example rather than an individual adversarial example.", "tokens": [51366, 1496, 17641, 44745, 1365, 2831, 813, 364, 2609, 17641, 44745, 1365, 13, 51540], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 578, "seek": 206372, "start": 2087.52, "end": 2090.48, "text": " But then it gets into the question of, you know, how can we use this", "tokens": [51554, 583, 550, 309, 2170, 666, 264, 1168, 295, 11, 291, 458, 11, 577, 393, 321, 764, 341, 51702], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 579, "seek": 206372, "start": 2090.48, "end": 2093.04, "text": " control theoretic approach to robustify models?", "tokens": [51702, 1969, 14308, 299, 3109, 281, 13956, 2505, 5245, 30, 51830], "temperature": 0.0, "avg_logprob": -0.12666963919615135, "compression_ratio": 1.7987987987987988, "no_speech_prob": 0.0044500576332211494}, {"id": 580, "seek": 209304, "start": 2093.04, "end": 2095.8, "text": " Cause we're talking about building a genetic LLMs.", "tokens": [50364, 10865, 321, 434, 1417, 466, 2390, 257, 12462, 441, 43, 26386, 13, 50502], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 581, "seek": 209304, "start": 2096.12, "end": 2099.64, "text": " And part of the thing I'm trying to get my head around is in this particular", "tokens": [50518, 400, 644, 295, 264, 551, 286, 478, 1382, 281, 483, 452, 1378, 926, 307, 294, 341, 1729, 50694], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 582, "seek": 209304, "start": 2099.64, "end": 2104.56, "text": " case, we had a very clear kind of cost function, you know, a specific thing.", "tokens": [50694, 1389, 11, 321, 632, 257, 588, 1850, 733, 295, 2063, 2445, 11, 291, 458, 11, 257, 2685, 551, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 583, "seek": 209304, "start": 2105.0, "end": 2108.7599999999998, "text": " But what would it mean to robustify language models in, in the general?", "tokens": [50962, 583, 437, 576, 309, 914, 281, 13956, 2505, 2856, 5245, 294, 11, 294, 264, 2674, 30, 51150], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 584, "seek": 209304, "start": 2108.96, "end": 2113.12, "text": " So one, one of the things that came up in our, you know, sort of literature", "tokens": [51160, 407, 472, 11, 472, 295, 264, 721, 300, 1361, 493, 294, 527, 11, 291, 458, 11, 1333, 295, 10394, 51368], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 585, "seek": 209304, "start": 2113.12, "end": 2118.08, "text": " view was this idea of, you know, when you're trying to control these discrete", "tokens": [51368, 1910, 390, 341, 1558, 295, 11, 291, 458, 11, 562, 291, 434, 1382, 281, 1969, 613, 27706, 51616], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 586, "seek": 209304, "start": 2118.24, "end": 2122.68, "text": " stochastic dynamical systems, one concept that can be quite useful is", "tokens": [51624, 342, 8997, 2750, 5999, 804, 3652, 11, 472, 3410, 300, 393, 312, 1596, 4420, 307, 51846], "temperature": 0.0, "avg_logprob": -0.12250779421274899, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005867683794349432}, {"id": 587, "seek": 212268, "start": 2122.8799999999997, "end": 2126.64, "text": " you might have a set of outputs that you want to reach or a set of outputs", "tokens": [50374, 291, 1062, 362, 257, 992, 295, 23930, 300, 291, 528, 281, 2524, 420, 257, 992, 295, 23930, 50562], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 588, "seek": 212268, "start": 2126.64, "end": 2127.52, "text": " that you want to avoid.", "tokens": [50562, 300, 291, 528, 281, 5042, 13, 50606], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 589, "seek": 212268, "start": 2127.52, "end": 2130.24, "text": " So an avoid set and basically a desirable set, right?", "tokens": [50606, 407, 364, 5042, 992, 293, 1936, 257, 30533, 992, 11, 558, 30, 50742], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 590, "seek": 212268, "start": 2130.64, "end": 2134.8399999999997, "text": " And when you frame it like that, you know, I think that the robustification", "tokens": [50762, 400, 562, 291, 3920, 309, 411, 300, 11, 291, 458, 11, 286, 519, 300, 264, 13956, 3774, 50972], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 591, "seek": 212268, "start": 2135.08, "end": 2138.16, "text": " comes from the fact that let's say that you have a set of outputs, you", "tokens": [50984, 1487, 490, 264, 1186, 300, 718, 311, 584, 300, 291, 362, 257, 992, 295, 23930, 11, 291, 51138], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 592, "seek": 212268, "start": 2138.16, "end": 2140.44, "text": " really don't want the language model to, to emit, right?", "tokens": [51138, 534, 500, 380, 528, 264, 2856, 2316, 281, 11, 281, 32084, 11, 558, 30, 51252], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 593, "seek": 212268, "start": 2140.64, "end": 2143.56, "text": " You might think, okay, well, I'll just fine tune it so that it decreases", "tokens": [51262, 509, 1062, 519, 11, 1392, 11, 731, 11, 286, 603, 445, 2489, 10864, 309, 370, 300, 309, 24108, 51408], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 594, "seek": 212268, "start": 2143.56, "end": 2146.7599999999998, "text": " the likelihood, the prior likelihood basically of those sequences, right?", "tokens": [51408, 264, 22119, 11, 264, 4059, 22119, 1936, 295, 729, 22978, 11, 558, 30, 51568], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 595, "seek": 212268, "start": 2147.16, "end": 2150.04, "text": " And the issue with that, I think, and the thing that the control", "tokens": [51588, 400, 264, 2734, 365, 300, 11, 286, 519, 11, 293, 264, 551, 300, 264, 1969, 51732], "temperature": 0.0, "avg_logprob": -0.11430316643426883, "compression_ratio": 1.9518900343642611, "no_speech_prob": 0.0009109299280680716}, {"id": 596, "seek": 215004, "start": 2150.04, "end": 2154.44, "text": " theoretic perspective sort of brings in is the fact that when you have", "tokens": [50364, 14308, 299, 4585, 1333, 295, 5607, 294, 307, 264, 1186, 300, 562, 291, 362, 50584], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 597, "seek": 215004, "start": 2155.44, "end": 2159.32, "text": " finite, even a small control prompt, some extra tokens that you get to inject,", "tokens": [50634, 19362, 11, 754, 257, 1359, 1969, 12391, 11, 512, 2857, 22667, 300, 291, 483, 281, 10711, 11, 50828], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 598, "seek": 215004, "start": 2159.52, "end": 2164.2, "text": " it turns out that even very, very unlikely next tokens can be made to be", "tokens": [50838, 309, 4523, 484, 300, 754, 588, 11, 588, 17518, 958, 22667, 393, 312, 1027, 281, 312, 51072], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 599, "seek": 215004, "start": 2164.2, "end": 2168.24, "text": " the most likely next token just by inputting these new examples.", "tokens": [51072, 264, 881, 3700, 958, 14862, 445, 538, 4846, 783, 613, 777, 5110, 13, 51274], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 600, "seek": 215004, "start": 2168.24, "end": 2172.88, "text": " So even if you did hypothetically fine tune the model so that this avoid set", "tokens": [51274, 407, 754, 498, 291, 630, 24371, 22652, 2489, 10864, 264, 2316, 370, 300, 341, 5042, 992, 51506], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 601, "seek": 215004, "start": 2172.88, "end": 2177.08, "text": " was assigned very low probability, it seems like if you don't incorporate", "tokens": [51506, 390, 13279, 588, 2295, 8482, 11, 309, 2544, 411, 498, 291, 500, 380, 16091, 51716], "temperature": 0.0, "avg_logprob": -0.11272956774784969, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.009411254897713661}, {"id": 602, "seek": 217708, "start": 2177.08, "end": 2180.3199999999997, "text": " some aspect of, you know, maybe stochastically trying to search for", "tokens": [50364, 512, 4171, 295, 11, 291, 458, 11, 1310, 342, 8997, 22808, 1382, 281, 3164, 337, 50526], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 603, "seek": 217708, "start": 2180.3199999999997, "end": 2183.6, "text": " these adversarial examples and sort of having this sort of mini max thing", "tokens": [50526, 613, 17641, 44745, 5110, 293, 1333, 295, 1419, 341, 1333, 295, 8382, 11469, 551, 50690], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 604, "seek": 217708, "start": 2183.6, "end": 2186.7999999999997, "text": " where you have one system that's trying to elicit the output, one system", "tokens": [50690, 689, 291, 362, 472, 1185, 300, 311, 1382, 281, 806, 8876, 264, 5598, 11, 472, 1185, 50850], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 605, "seek": 217708, "start": 2186.7999999999997, "end": 2190.16, "text": " that is trying to fine tune the model to maybe make it less likely or optimize", "tokens": [50850, 300, 307, 1382, 281, 2489, 10864, 264, 2316, 281, 1310, 652, 309, 1570, 3700, 420, 19719, 51018], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 606, "seek": 217708, "start": 2190.16, "end": 2193.4, "text": " another part of the prompt that is supposed to steer it away from these outputs.", "tokens": [51018, 1071, 644, 295, 264, 12391, 300, 307, 3442, 281, 30814, 309, 1314, 490, 613, 23930, 13, 51180], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 607, "seek": 217708, "start": 2194.2, "end": 2197.4, "text": " Basically, the inside, I think, is that you really have to be careful", "tokens": [51220, 8537, 11, 264, 1854, 11, 286, 519, 11, 307, 300, 291, 534, 362, 281, 312, 5026, 51380], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 608, "seek": 217708, "start": 2197.4, "end": 2201.16, "text": " to consider the fact that you have, you're giving the outside world some", "tokens": [51380, 281, 1949, 264, 1186, 300, 291, 362, 11, 291, 434, 2902, 264, 2380, 1002, 512, 51568], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 609, "seek": 217708, "start": 2201.16, "end": 2204.48, "text": " amount of control over the system, some amount of control over the context.", "tokens": [51568, 2372, 295, 1969, 670, 264, 1185, 11, 512, 2372, 295, 1969, 670, 264, 4319, 13, 51734], "temperature": 0.0, "avg_logprob": -0.10928588217877327, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.011328691616654396}, {"id": 610, "seek": 220448, "start": 2204.72, "end": 2208.08, "text": " And planning around that is actually very non-trivial and is not really", "tokens": [50376, 400, 5038, 926, 300, 307, 767, 588, 2107, 12, 83, 470, 22640, 293, 307, 406, 534, 50544], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 611, "seek": 220448, "start": 2208.08, "end": 2211.68, "text": " well managed, I don't think, through the classical view of just cross entropy loss", "tokens": [50544, 731, 6453, 11, 286, 500, 380, 519, 11, 807, 264, 13735, 1910, 295, 445, 3278, 30867, 4470, 50724], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 612, "seek": 220448, "start": 2211.88, "end": 2214.2400000000002, "text": " and just treating it like a probability distribution.", "tokens": [50734, 293, 445, 15083, 309, 411, 257, 8482, 7316, 13, 50852], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 613, "seek": 220448, "start": 2214.68, "end": 2219.52, "text": " Something else that fascinates me is the divergence between focusing on", "tokens": [50874, 6595, 1646, 300, 7184, 259, 1024, 385, 307, 264, 47387, 1296, 8416, 322, 51116], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 614, "seek": 220448, "start": 2219.52, "end": 2224.12, "text": " the model versus, you know, complexifying the software which controls it.", "tokens": [51116, 264, 2316, 5717, 11, 291, 458, 11, 3997, 5489, 264, 4722, 597, 9003, 309, 13, 51346], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 615, "seek": 220448, "start": 2224.4, "end": 2227.76, "text": " So right now, for example, we have language models and, you know, there's", "tokens": [51360, 407, 558, 586, 11, 337, 1365, 11, 321, 362, 2856, 5245, 293, 11, 291, 458, 11, 456, 311, 51528], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 616, "seek": 220448, "start": 2227.76, "end": 2231.84, "text": " this kind of base training and then there's fine tuning and there's RLHF", "tokens": [51528, 341, 733, 295, 3096, 3097, 293, 550, 456, 311, 2489, 15164, 293, 456, 311, 497, 43, 39, 37, 51732], "temperature": 0.0, "avg_logprob": -0.12397858353911853, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.003698447486385703}, {"id": 617, "seek": 223184, "start": 2231.84, "end": 2235.84, "text": " and, you know, there's like command variations of that, for example.", "tokens": [50364, 293, 11, 291, 458, 11, 456, 311, 411, 5622, 17840, 295, 300, 11, 337, 1365, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 618, "seek": 223184, "start": 2236.2400000000002, "end": 2239.44, "text": " And then we build these software APIs that are just trying to abstract", "tokens": [50584, 400, 550, 321, 1322, 613, 4722, 21445, 300, 366, 445, 1382, 281, 12649, 50744], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 619, "seek": 223184, "start": 2239.44, "end": 2244.6800000000003, "text": " away the complexity, so they will do dynamic prompt construction for multi,", "tokens": [50744, 1314, 264, 14024, 11, 370, 436, 486, 360, 8546, 12391, 6435, 337, 4825, 11, 51006], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 620, "seek": 223184, "start": 2245.96, "end": 2249.0, "text": " you know, multi-stop tool use and it goes on and on and on.", "tokens": [51070, 291, 458, 11, 4825, 12, 13559, 2290, 764, 293, 309, 1709, 322, 293, 322, 293, 322, 13, 51222], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 621, "seek": 223184, "start": 2249.0, "end": 2253.52, "text": " There will be frameworks for doing agentic LLMs and there just seems", "tokens": [51222, 821, 486, 312, 29834, 337, 884, 9461, 299, 441, 43, 26386, 293, 456, 445, 2544, 51448], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 622, "seek": 223184, "start": 2253.52, "end": 2255.76, "text": " to be like a bit of a divergence here.", "tokens": [51448, 281, 312, 411, 257, 857, 295, 257, 47387, 510, 13, 51560], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 623, "seek": 223184, "start": 2256.0, "end": 2259.92, "text": " But the reason I'm asking the question is, does it make sense to", "tokens": [51572, 583, 264, 1778, 286, 478, 3365, 264, 1168, 307, 11, 775, 309, 652, 2020, 281, 51768], "temperature": 0.0, "avg_logprob": -0.16472905257652545, "compression_ratio": 1.635036496350365, "no_speech_prob": 0.0040372442454099655}, {"id": 624, "seek": 225992, "start": 2260.12, "end": 2262.84, "text": " robustify and fix the problem in the model?", "tokens": [50374, 13956, 2505, 293, 3191, 264, 1154, 294, 264, 2316, 30, 50510], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 625, "seek": 225992, "start": 2263.04, "end": 2267.8, "text": " Or does it make sense to almost increase the flexibility of the model", "tokens": [50520, 1610, 775, 309, 652, 2020, 281, 1920, 3488, 264, 12635, 295, 264, 2316, 50758], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 626, "seek": 225992, "start": 2267.8, "end": 2269.56, "text": " and fix it in the software layer?", "tokens": [50758, 293, 3191, 309, 294, 264, 4722, 4583, 30, 50846], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 627, "seek": 225992, "start": 2270.36, "end": 2275.2400000000002, "text": " I think one of the insights from our paper is that solely focusing on", "tokens": [50886, 286, 519, 472, 295, 264, 14310, 490, 527, 3035, 307, 300, 23309, 8416, 322, 51130], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 628, "seek": 225992, "start": 2275.2400000000002, "end": 2279.6800000000003, "text": " the model itself, like Amman was just saying, as soon as you give the", "tokens": [51130, 264, 2316, 2564, 11, 411, 2012, 1601, 390, 445, 1566, 11, 382, 2321, 382, 291, 976, 264, 51352], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 629, "seek": 225992, "start": 2279.6800000000003, "end": 2283.44, "text": " outside world control over the model in the sense of being able to input", "tokens": [51352, 2380, 1002, 1969, 670, 264, 2316, 294, 264, 2020, 295, 885, 1075, 281, 4846, 51540], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 630, "seek": 225992, "start": 2283.48, "end": 2287.6, "text": " whatever kind of text that they want, it becomes very difficult to", "tokens": [51542, 2035, 733, 295, 2487, 300, 436, 528, 11, 309, 3643, 588, 2252, 281, 51748], "temperature": 0.0, "avg_logprob": -0.1386204583304269, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.005543440580368042}, {"id": 631, "seek": 228760, "start": 2287.6, "end": 2291.2799999999997, "text": " really prevent adversarial attacks and prevents jail breaks.", "tokens": [50364, 534, 4871, 17641, 44745, 8122, 293, 22367, 10511, 9857, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 632, "seek": 228760, "start": 2291.2799999999997, "end": 2293.52, "text": " And that's, you know, why you see jail breaks keep coming up.", "tokens": [50548, 400, 300, 311, 11, 291, 458, 11, 983, 291, 536, 10511, 9857, 1066, 1348, 493, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 633, "seek": 228760, "start": 2294.12, "end": 2300.4, "text": " I think if you were to involve some sort of robustness in a software layer,", "tokens": [50690, 286, 519, 498, 291, 645, 281, 9494, 512, 1333, 295, 13956, 1287, 294, 257, 4722, 4583, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 634, "seek": 228760, "start": 2300.64, "end": 2302.0, "text": " that might be more feasible.", "tokens": [51016, 300, 1062, 312, 544, 26648, 13, 51084], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 635, "seek": 228760, "start": 2302.7999999999997, "end": 2307.88, "text": " At least I can't immediately picture, you know, ways around it as, you know,", "tokens": [51124, 1711, 1935, 286, 393, 380, 4258, 3036, 11, 291, 458, 11, 2098, 926, 309, 382, 11, 291, 458, 11, 51378], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 636, "seek": 228760, "start": 2308.0, "end": 2312.64, "text": " of course, if I was a hacker, I could probably, you know, find some loophole.", "tokens": [51384, 295, 1164, 11, 498, 286, 390, 257, 38155, 11, 286, 727, 1391, 11, 291, 458, 11, 915, 512, 6367, 14094, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 637, "seek": 228760, "start": 2312.64, "end": 2314.8399999999997, "text": " There's usually some loophole you can find.", "tokens": [51616, 821, 311, 2673, 512, 6367, 14094, 291, 393, 915, 13, 51726], "temperature": 0.0, "avg_logprob": -0.13136053499968156, "compression_ratio": 1.704, "no_speech_prob": 0.028425076976418495}, {"id": 638, "seek": 231484, "start": 2314.88, "end": 2319.56, "text": " But if there was some way of fielding the prompt messages, for instance,", "tokens": [50366, 583, 498, 456, 390, 512, 636, 295, 2519, 278, 264, 12391, 7897, 11, 337, 5197, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 639, "seek": 231484, "start": 2319.84, "end": 2323.88, "text": " a user gives you a prompt, first you check, is this a reasonable thing", "tokens": [50614, 257, 4195, 2709, 291, 257, 12391, 11, 700, 291, 1520, 11, 307, 341, 257, 10585, 551, 50816], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 640, "seek": 231484, "start": 2323.88, "end": 2326.6400000000003, "text": " that a human being would say in conversation, or is this something", "tokens": [50816, 300, 257, 1952, 885, 576, 584, 294, 3761, 11, 420, 307, 341, 746, 50954], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 641, "seek": 231484, "start": 2326.6400000000003, "end": 2329.7200000000003, "text": " that I've never seen before in the entire history of the internets?", "tokens": [50954, 300, 286, 600, 1128, 1612, 949, 294, 264, 2302, 2503, 295, 264, 2154, 1385, 30, 51108], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 642, "seek": 231484, "start": 2330.08, "end": 2330.4, "text": " Right.", "tokens": [51126, 1779, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 643, "seek": 231484, "start": 2330.88, "end": 2335.2400000000002, "text": " The latter maybe is a prompt injection, maybe is, you know, something", "tokens": [51166, 440, 18481, 1310, 307, 257, 12391, 22873, 11, 1310, 307, 11, 291, 458, 11, 746, 51384], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 644, "seek": 231484, "start": 2335.2400000000002, "end": 2337.96, "text": " devious, or maybe is, you know, computer science research.", "tokens": [51384, 368, 1502, 11, 420, 1310, 307, 11, 291, 458, 11, 3820, 3497, 2132, 13, 51520], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 645, "seek": 231484, "start": 2338.8, "end": 2341.4, "text": " But yeah, it's definitely not an easy problem.", "tokens": [51562, 583, 1338, 11, 309, 311, 2138, 406, 364, 1858, 1154, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 646, "seek": 231484, "start": 2341.44, "end": 2344.04, "text": " But the good thing is that there are multiple approaches to it.", "tokens": [51694, 583, 264, 665, 551, 307, 300, 456, 366, 3866, 11587, 281, 309, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1673742647524233, "compression_ratio": 1.75, "no_speech_prob": 0.0013041150523349643}, {"id": 647, "seek": 234404, "start": 2344.4, "end": 2344.92, "text": " Very cool.", "tokens": [50382, 4372, 1627, 13, 50408], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 648, "seek": 234404, "start": 2345.16, "end": 2347.8, "text": " So we're going to go on to the more galaxy brain stuff in a second.", "tokens": [50420, 407, 321, 434, 516, 281, 352, 322, 281, 264, 544, 17639, 3567, 1507, 294, 257, 1150, 13, 50552], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 649, "seek": 234404, "start": 2347.8, "end": 2350.44, "text": " So before we move off the paper, can you just talk more formally", "tokens": [50552, 407, 949, 321, 1286, 766, 264, 3035, 11, 393, 291, 445, 751, 544, 25983, 50684], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 650, "seek": 234404, "start": 2350.44, "end": 2352.68, "text": " about what you showed in the paper?", "tokens": [50684, 466, 437, 291, 4712, 294, 264, 3035, 30, 50796], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 651, "seek": 234404, "start": 2353.0, "end": 2353.72, "text": " Yeah, definitely.", "tokens": [50812, 865, 11, 2138, 13, 50848], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 652, "seek": 234404, "start": 2353.72, "end": 2356.04, "text": " So there were two main parts of the paper.", "tokens": [50848, 407, 456, 645, 732, 2135, 3166, 295, 264, 3035, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 653, "seek": 234404, "start": 2356.44, "end": 2357.56, "text": " So I guess three.", "tokens": [50984, 407, 286, 2041, 1045, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 654, "seek": 234404, "start": 2357.56, "end": 2361.92, "text": " So for one, what we did was we tried to formalize what an LLM system", "tokens": [51040, 407, 337, 472, 11, 437, 321, 630, 390, 321, 3031, 281, 9860, 1125, 437, 364, 441, 43, 44, 1185, 51258], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 655, "seek": 234404, "start": 2361.92, "end": 2363.96, "text": " really is at a mathematical level.", "tokens": [51258, 534, 307, 412, 257, 18894, 1496, 13, 51360], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 656, "seek": 234404, "start": 2364.2, "end": 2367.0, "text": " And what we were trying to do at that was basically balance the fact", "tokens": [51372, 400, 437, 321, 645, 1382, 281, 360, 412, 300, 390, 1936, 4772, 264, 1186, 51512], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 657, "seek": 234404, "start": 2367.0, "end": 2370.36, "text": " that, you know, we really wanted to try to take advantage of, you know,", "tokens": [51512, 300, 11, 291, 458, 11, 321, 534, 1415, 281, 853, 281, 747, 5002, 295, 11, 291, 458, 11, 51680], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 658, "seek": 234404, "start": 2370.36, "end": 2373.48, "text": " the original sort of control theories, very abstract picture of a system", "tokens": [51680, 264, 3380, 1333, 295, 1969, 13667, 11, 588, 12649, 3036, 295, 257, 1185, 51836], "temperature": 0.0, "avg_logprob": -0.10711382612397399, "compression_ratio": 1.7723076923076924, "no_speech_prob": 0.0017796122701838613}, {"id": 659, "seek": 237348, "start": 2373.48, "end": 2377.28, "text": " where you have this input space, you have a state space and output space.", "tokens": [50364, 689, 291, 362, 341, 4846, 1901, 11, 291, 362, 257, 1785, 1901, 293, 5598, 1901, 13, 50554], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 660, "seek": 237348, "start": 2377.44, "end": 2379.16, "text": " And there's some dynamics going on inside of it.", "tokens": [50562, 400, 456, 311, 512, 15679, 516, 322, 1854, 295, 309, 13, 50648], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 661, "seek": 237348, "start": 2379.64, "end": 2383.4, "text": " In our case, we parameterized those dynamics with an LLM and our input", "tokens": [50672, 682, 527, 1389, 11, 321, 13075, 1602, 729, 15679, 365, 364, 441, 43, 44, 293, 527, 4846, 50860], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 662, "seek": 237348, "start": 2383.4, "end": 2386.76, "text": " space and our state spaces were basically the set of all possible", "tokens": [50860, 1901, 293, 527, 1785, 7673, 645, 1936, 264, 992, 295, 439, 1944, 51028], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 663, "seek": 237348, "start": 2386.76, "end": 2389.96, "text": " token sequences from the vocabulary set of this model.", "tokens": [51028, 14862, 22978, 490, 264, 19864, 992, 295, 341, 2316, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 664, "seek": 237348, "start": 2389.96, "end": 2390.28, "text": " Right.", "tokens": [51188, 1779, 13, 51204], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 665, "seek": 237348, "start": 2390.48, "end": 2391.76, "text": " So that was the first part.", "tokens": [51214, 407, 300, 390, 264, 700, 644, 13, 51278], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 666, "seek": 237348, "start": 2391.8, "end": 2395.6, "text": " And we basically transferred over a lot of the notions of basically", "tokens": [51280, 400, 321, 1936, 15809, 670, 257, 688, 295, 264, 35799, 295, 1936, 51470], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 667, "seek": 237348, "start": 2395.6, "end": 2399.52, "text": " reachability and controllability for LLM systems from the original control", "tokens": [51470, 2524, 2310, 293, 45159, 2310, 337, 441, 43, 44, 3652, 490, 264, 3380, 1969, 51666], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 668, "seek": 237348, "start": 2399.52, "end": 2402.88, "text": " theory where you can really just define it in terms of this really abstract", "tokens": [51666, 5261, 689, 291, 393, 534, 445, 6964, 309, 294, 2115, 295, 341, 534, 12649, 51834], "temperature": 0.0, "avg_logprob": -0.11023687794260735, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.001366733806207776}, {"id": 669, "seek": 240288, "start": 2402.92, "end": 2407.0, "text": " notions of, you know, have sets for the reachable or sorry, the state space,", "tokens": [50366, 35799, 295, 11, 291, 458, 11, 362, 6352, 337, 264, 2524, 712, 420, 2597, 11, 264, 1785, 1901, 11, 50570], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 670, "seek": 240288, "start": 2407.0, "end": 2409.4, "text": " the input space and the output space, you have some dynamics.", "tokens": [50570, 264, 4846, 1901, 293, 264, 5598, 1901, 11, 291, 362, 512, 15679, 13, 50690], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 671, "seek": 240288, "start": 2409.4, "end": 2412.76, "text": " And basically in terms of those sets, you can define reachability and control.", "tokens": [50690, 400, 1936, 294, 2115, 295, 729, 6352, 11, 291, 393, 6964, 2524, 2310, 293, 1969, 13, 50858], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 672, "seek": 240288, "start": 2413.0, "end": 2414.0, "text": " So that was the first part.", "tokens": [50870, 407, 300, 390, 264, 700, 644, 13, 50920], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 673, "seek": 240288, "start": 2414.6, "end": 2416.96, "text": " The next thing that we did was we tried to look inside the model.", "tokens": [50950, 440, 958, 551, 300, 321, 630, 390, 321, 3031, 281, 574, 1854, 264, 2316, 13, 51068], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 674, "seek": 240288, "start": 2416.96, "end": 2420.4, "text": " So we were thinking, you know, it'd be really nice, like in control theory,", "tokens": [51068, 407, 321, 645, 1953, 11, 291, 458, 11, 309, 1116, 312, 534, 1481, 11, 411, 294, 1969, 5261, 11, 51240], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 675, "seek": 240288, "start": 2420.84, "end": 2424.44, "text": " if we could have a really good understanding of the components of the system", "tokens": [51262, 498, 321, 727, 362, 257, 534, 665, 3701, 295, 264, 6677, 295, 264, 1185, 51442], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 676, "seek": 240288, "start": 2424.44, "end": 2426.48, "text": " and how controllable those individual pieces were.", "tokens": [51442, 293, 577, 45159, 712, 729, 2609, 3755, 645, 13, 51544], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 677, "seek": 240288, "start": 2426.76, "end": 2430.1600000000003, "text": " So what we did is we looked at a single self-attention head and tried", "tokens": [51558, 407, 437, 321, 630, 307, 321, 2956, 412, 257, 2167, 2698, 12, 1591, 1251, 1378, 293, 3031, 51728], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 678, "seek": 240288, "start": 2430.1600000000003, "end": 2432.76, "text": " to really think about it through a matrix algebraic perspective.", "tokens": [51728, 281, 534, 519, 466, 309, 807, 257, 8141, 21989, 299, 4585, 13, 51858], "temperature": 0.0, "avg_logprob": -0.111350624649613, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.0015976197319105268}, {"id": 679, "seek": 243288, "start": 2432.96, "end": 2436.28, "text": " To really break down what the relationship is between, let's say,", "tokens": [50368, 1407, 534, 1821, 760, 437, 264, 2480, 307, 1296, 11, 718, 311, 584, 11, 50534], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 680, "seek": 243288, "start": 2436.28, "end": 2439.4, "text": " you have a subset of the tokens, you get to control a subset that's fixed.", "tokens": [50534, 291, 362, 257, 25993, 295, 264, 22667, 11, 291, 483, 281, 1969, 257, 25993, 300, 311, 6806, 13, 50690], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 681, "seek": 243288, "start": 2439.6400000000003, "end": 2442.56, "text": " And you're trying to get the output to be, you know, a certain value,", "tokens": [50702, 400, 291, 434, 1382, 281, 483, 264, 5598, 281, 312, 11, 291, 458, 11, 257, 1629, 2158, 11, 50848], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 682, "seek": 243288, "start": 2442.56, "end": 2445.8, "text": " the output representations where all of these in the case of a self-attention", "tokens": [50848, 264, 5598, 33358, 689, 439, 295, 613, 294, 264, 1389, 295, 257, 2698, 12, 1591, 1251, 51010], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 683, "seek": 243288, "start": 2445.8, "end": 2448.76, "text": " head are just these vector representations of tokens.", "tokens": [51010, 1378, 366, 445, 613, 8062, 33358, 295, 22667, 13, 51158], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 684, "seek": 243288, "start": 2449.4, "end": 2453.7200000000003, "text": " So what we found there was that it actually is possible to do some fairly,", "tokens": [51190, 407, 437, 321, 1352, 456, 390, 300, 309, 767, 307, 1944, 281, 360, 512, 6457, 11, 51406], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 685, "seek": 243288, "start": 2453.76, "end": 2457.8, "text": " you know, simple matrix algebra manipulations to decompose the output", "tokens": [51408, 291, 458, 11, 2199, 8141, 21989, 9258, 4136, 281, 22867, 541, 264, 5598, 51610], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 686, "seek": 243288, "start": 2457.92, "end": 2462.48, "text": " of a self-attention head into one component that arises from the imposed input.", "tokens": [51616, 295, 257, 2698, 12, 1591, 1251, 1378, 666, 472, 6542, 300, 27388, 490, 264, 26491, 4846, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11328253300069906, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.0005702609778381884}, {"id": 687, "seek": 246248, "start": 2462.6, "end": 2466.44, "text": " And then another component that arises from the control input, and assuming", "tokens": [50370, 400, 550, 1071, 6542, 300, 27388, 490, 264, 1969, 4846, 11, 293, 11926, 50562], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 688, "seek": 246248, "start": 2466.44, "end": 2470.44, "text": " that those two are bound, then you can actually derive that, well,", "tokens": [50562, 300, 729, 732, 366, 5472, 11, 550, 291, 393, 767, 28446, 300, 11, 731, 11, 50762], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 689, "seek": 246248, "start": 2470.44, "end": 2473.64, "text": " there actually is this geometry that sort of looks like a bubble around", "tokens": [50762, 456, 767, 307, 341, 18426, 300, 1333, 295, 1542, 411, 257, 12212, 926, 50922], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 690, "seek": 246248, "start": 2473.64, "end": 2474.6, "text": " the default output.", "tokens": [50922, 264, 7576, 5598, 13, 50970], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 691, "seek": 246248, "start": 2474.6, "end": 2477.72, "text": " So the output, if you didn't have any control input in, there's a sort", "tokens": [50970, 407, 264, 5598, 11, 498, 291, 994, 380, 362, 604, 1969, 4846, 294, 11, 456, 311, 257, 1333, 51126], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 692, "seek": 246248, "start": 2477.72, "end": 2480.92, "text": " of bubble of reachable space that scales with the number of control", "tokens": [51126, 295, 12212, 295, 2524, 712, 1901, 300, 17408, 365, 264, 1230, 295, 1969, 51286], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 693, "seek": 246248, "start": 2480.92, "end": 2482.64, "text": " input tokens that you're able to use.", "tokens": [51286, 4846, 22667, 300, 291, 434, 1075, 281, 764, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 694, "seek": 246248, "start": 2482.96, "end": 2486.08, "text": " And we thought that that was really exciting because for one, I didn't", "tokens": [51388, 400, 321, 1194, 300, 300, 390, 534, 4670, 570, 337, 472, 11, 286, 994, 380, 51544], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 695, "seek": 246248, "start": 2486.08, "end": 2488.8, "text": " really expect that you'd be able to do proofs on these sort of, you know,", "tokens": [51544, 534, 2066, 300, 291, 1116, 312, 1075, 281, 360, 8177, 82, 322, 613, 1333, 295, 11, 291, 458, 11, 51680], "temperature": 0.0, "avg_logprob": -0.11298401921773128, "compression_ratio": 1.8533333333333333, "no_speech_prob": 0.0009108581580221653}, {"id": 696, "seek": 248880, "start": 2488.88, "end": 2493.28, "text": " very complicated, high dimensional machine learning or deep learning systems", "tokens": [50368, 588, 6179, 11, 1090, 18795, 3479, 2539, 420, 2452, 2539, 3652, 50588], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 697, "seek": 248880, "start": 2493.28, "end": 2494.2400000000002, "text": " like a self-attention head.", "tokens": [50588, 411, 257, 2698, 12, 1591, 1251, 1378, 13, 50636], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 698, "seek": 248880, "start": 2494.5600000000004, "end": 2497.6800000000003, "text": " But it also gave us some insight to say that, okay, we actually have", "tokens": [50652, 583, 309, 611, 2729, 505, 512, 11269, 281, 584, 300, 11, 1392, 11, 321, 767, 362, 50808], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 699, "seek": 248880, "start": 2497.6800000000003, "end": 2501.28, "text": " this really concrete relationship between the sort of number of control", "tokens": [50808, 341, 534, 9859, 2480, 1296, 264, 1333, 295, 1230, 295, 1969, 50988], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 700, "seek": 248880, "start": 2501.28, "end": 2504.6000000000004, "text": " input tokens, the magnitudes that you're able to input into the system,", "tokens": [50988, 4846, 22667, 11, 264, 4944, 16451, 300, 291, 434, 1075, 281, 4846, 666, 264, 1185, 11, 51154], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 701, "seek": 248880, "start": 2504.84, "end": 2508.8, "text": " and the output reachable set that is at your disposal, basically.", "tokens": [51166, 293, 264, 5598, 2524, 712, 992, 300, 307, 412, 428, 26400, 11, 1936, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 702, "seek": 248880, "start": 2509.1200000000003, "end": 2511.0, "text": " And so that was the second part.", "tokens": [51380, 400, 370, 300, 390, 264, 1150, 644, 13, 51474], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 703, "seek": 248880, "start": 2511.0, "end": 2514.2000000000003, "text": " And then the last part was some empirical experiments where we said, okay,", "tokens": [51474, 400, 550, 264, 1036, 644, 390, 512, 31886, 12050, 689, 321, 848, 11, 1392, 11, 51634], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 704, "seek": 248880, "start": 2514.2000000000003, "end": 2516.84, "text": " let's just sample a bunch of strings from Wikipedia.", "tokens": [51634, 718, 311, 445, 6889, 257, 3840, 295, 13985, 490, 28999, 13, 51766], "temperature": 0.0, "avg_logprob": -0.11742166372445914, "compression_ratio": 1.7, "no_speech_prob": 0.009410042315721512}, {"id": 705, "seek": 251684, "start": 2517.08, "end": 2521.76, "text": " And we'll see, okay, the strings were between eight and 32 tokens.", "tokens": [50376, 400, 321, 603, 536, 11, 1392, 11, 264, 13985, 645, 1296, 3180, 293, 8858, 22667, 13, 50610], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 706, "seek": 251684, "start": 2521.76, "end": 2524.04, "text": " And those were basically our imposed state sequences.", "tokens": [50610, 400, 729, 645, 1936, 527, 26491, 1785, 22978, 13, 50724], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 707, "seek": 251684, "start": 2524.28, "end": 2527.56, "text": " And we asked the question, well, can we get it to output the correct next", "tokens": [50736, 400, 321, 2351, 264, 1168, 11, 731, 11, 393, 321, 483, 309, 281, 5598, 264, 3006, 958, 50900], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 708, "seek": 251684, "start": 2527.56, "end": 2529.2400000000002, "text": " token, the real next Wikipedia token?", "tokens": [50900, 14862, 11, 264, 957, 958, 28999, 14862, 30, 50984], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 709, "seek": 251684, "start": 2529.48, "end": 2532.6400000000003, "text": " How many, you know, input tokens does it take or control input tokens does it", "tokens": [50996, 1012, 867, 11, 291, 458, 11, 4846, 22667, 775, 309, 747, 420, 1969, 4846, 22667, 775, 309, 51154], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 710, "seek": 251684, "start": 2532.6400000000003, "end": 2534.0, "text": " take for that to happen?", "tokens": [51154, 747, 337, 300, 281, 1051, 30, 51222], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 711, "seek": 251684, "start": 2534.0, "end": 2537.56, "text": " It turned out that you could get that done about 97% of the time to steer", "tokens": [51222, 467, 3574, 484, 300, 291, 727, 483, 300, 1096, 466, 23399, 4, 295, 264, 565, 281, 30814, 51400], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 712, "seek": 251684, "start": 2537.56, "end": 2541.76, "text": " the model to the correct output within 10 tokens of a control input, which is", "tokens": [51400, 264, 2316, 281, 264, 3006, 5598, 1951, 1266, 22667, 295, 257, 1969, 4846, 11, 597, 307, 51610], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 713, "seek": 251684, "start": 2541.76, "end": 2544.8, "text": " reasonable, you know, we'd expect that the model should be able to be steered", "tokens": [51610, 10585, 11, 291, 458, 11, 321, 1116, 2066, 300, 264, 2316, 820, 312, 1075, 281, 312, 2126, 4073, 51762], "temperature": 0.0, "avg_logprob": -0.122567613919576, "compression_ratio": 1.8284789644012944, "no_speech_prob": 0.0025502252392470837}, {"id": 714, "seek": 254480, "start": 2544.84, "end": 2548.36, "text": " towards reasonable true English sentences that were more than likely in the", "tokens": [50366, 3030, 10585, 2074, 3669, 16579, 300, 645, 544, 813, 3700, 294, 264, 50542], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 715, "seek": 254480, "start": 2548.36, "end": 2549.1600000000003, "text": " training data set.", "tokens": [50542, 3097, 1412, 992, 13, 50582], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 716, "seek": 254480, "start": 2549.8, "end": 2553.04, "text": " What we did next was we tried to figure out, you know, if you sample the top", "tokens": [50614, 708, 321, 630, 958, 390, 321, 3031, 281, 2573, 484, 11, 291, 458, 11, 498, 291, 6889, 264, 1192, 50776], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 717, "seek": 254480, "start": 2553.04, "end": 2559.28, "text": " 75 most likely tokens, according to the model, based on this fixed input, can", "tokens": [50776, 9562, 881, 3700, 22667, 11, 4650, 281, 264, 2316, 11, 2361, 322, 341, 6806, 4846, 11, 393, 51088], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 718, "seek": 254480, "start": 2559.28, "end": 2563.6000000000004, "text": " you steer those things to be the most likely token, basically the arg max of", "tokens": [51088, 291, 30814, 729, 721, 281, 312, 264, 881, 3700, 14862, 11, 1936, 264, 3882, 11469, 295, 51304], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 719, "seek": 254480, "start": 2563.6000000000004, "end": 2564.7200000000003, "text": " the probability distribution?", "tokens": [51304, 264, 8482, 7316, 30, 51360], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 720, "seek": 254480, "start": 2565.0800000000004, "end": 2568.84, "text": " And what we found there is that it's about 89% of the time, at least 89% of", "tokens": [51378, 400, 437, 321, 1352, 456, 307, 300, 309, 311, 466, 31877, 4, 295, 264, 565, 11, 412, 1935, 31877, 4, 295, 51566], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 721, "seek": 254480, "start": 2568.84, "end": 2572.36, "text": " the time, we were able to find these optimal control inputs that were less", "tokens": [51566, 264, 565, 11, 321, 645, 1075, 281, 915, 613, 16252, 1969, 15743, 300, 645, 1570, 51742], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 722, "seek": 254480, "start": 2572.36, "end": 2574.6800000000003, "text": " than 10 tokens long, that would steer the model to do that.", "tokens": [51742, 813, 1266, 22667, 938, 11, 300, 576, 30814, 264, 2316, 281, 360, 300, 13, 51858], "temperature": 0.0, "avg_logprob": -0.14195218999335107, "compression_ratio": 1.805732484076433, "no_speech_prob": 0.0020505422726273537}, {"id": 723, "seek": 257480, "start": 2574.92, "end": 2577.36, "text": " And then the last thing we did was he said, okay, well, let's see what would", "tokens": [50370, 400, 550, 264, 1036, 551, 321, 630, 390, 415, 848, 11, 1392, 11, 731, 11, 718, 311, 536, 437, 576, 50492], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 724, "seek": 257480, "start": 2577.36, "end": 2579.84, "text": " happen if we just randomly picked a token from the vocabulary.", "tokens": [50492, 1051, 498, 321, 445, 16979, 6183, 257, 14862, 490, 264, 19864, 13, 50616], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 725, "seek": 257480, "start": 2579.84, "end": 2584.04, "text": " So this is everything from regular English to numbers to Cyrillic characters", "tokens": [50616, 407, 341, 307, 1203, 490, 3890, 3669, 281, 3547, 281, 33146, 373, 299, 4342, 50826], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 726, "seek": 257480, "start": 2584.04, "end": 2585.04, "text": " to Chinese characters.", "tokens": [50826, 281, 4649, 4342, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 727, "seek": 257480, "start": 2585.36, "end": 2586.96, "text": " What if we just randomly sampled those?", "tokens": [50892, 708, 498, 321, 445, 16979, 3247, 15551, 729, 30, 50972], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 728, "seek": 257480, "start": 2587.2000000000003, "end": 2590.76, "text": " And we tried to see how many tokens it would take to steer that to being the", "tokens": [50984, 400, 321, 3031, 281, 536, 577, 867, 22667, 309, 576, 747, 281, 30814, 300, 281, 885, 264, 51162], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 729, "seek": 257480, "start": 2590.76, "end": 2592.32, "text": " arg max of the probability distribution.", "tokens": [51162, 3882, 11469, 295, 264, 8482, 7316, 13, 51240], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 730, "seek": 257480, "start": 2592.6000000000004, "end": 2596.6000000000004, "text": " And we found there is about 46% of the time we were able to make that next", "tokens": [51254, 400, 321, 1352, 456, 307, 466, 17835, 4, 295, 264, 565, 321, 645, 1075, 281, 652, 300, 958, 51454], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 731, "seek": 257480, "start": 2596.6000000000004, "end": 2601.2400000000002, "text": " token, the random one, the most likely next token using a prompt of length 10 or", "tokens": [51454, 14862, 11, 264, 4974, 472, 11, 264, 881, 3700, 958, 14862, 1228, 257, 12391, 295, 4641, 1266, 420, 51686], "temperature": 0.0, "avg_logprob": -0.12766924521905912, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.001283949939534068}, {"id": 732, "seek": 260124, "start": 2601.24, "end": 2606.4799999999996, "text": " less. And the sort of curves are there in our, in our paper that described as", "tokens": [50364, 1570, 13, 400, 264, 1333, 295, 19490, 366, 456, 294, 527, 11, 294, 527, 3035, 300, 7619, 382, 50626], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 733, "seek": 260124, "start": 2606.4799999999996, "end": 2610.12, "text": " you have an increasing budget for these tokens, how much of the time were we", "tokens": [50626, 291, 362, 364, 5662, 4706, 337, 613, 22667, 11, 577, 709, 295, 264, 565, 645, 321, 50808], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 734, "seek": 260124, "start": 2610.12, "end": 2611.9199999999996, "text": " able to basically steer it to the right output?", "tokens": [50808, 1075, 281, 1936, 30814, 309, 281, 264, 558, 5598, 30, 50898], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 735, "seek": 260124, "start": 2612.12, "end": 2615.56, "text": " That's our basically the K epsilon controllability metric that lets us get", "tokens": [50908, 663, 311, 527, 1936, 264, 591, 17889, 45159, 2310, 20678, 300, 6653, 505, 483, 51080], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 736, "seek": 260124, "start": 2615.56, "end": 2619.3199999999997, "text": " this sort of statistical picture on controllability that renders it sort of", "tokens": [51080, 341, 1333, 295, 22820, 3036, 322, 45159, 2310, 300, 6125, 433, 309, 1333, 295, 51268], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 737, "seek": 260124, "start": 2619.3199999999997, "end": 2622.2799999999997, "text": " practical to empirically estimate for these complicated systems.", "tokens": [51268, 8496, 281, 25790, 984, 12539, 337, 613, 6179, 3652, 13, 51416], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 738, "seek": 260124, "start": 2622.64, "end": 2624.56, "text": " And so those are really the main results.", "tokens": [51434, 400, 370, 729, 366, 534, 264, 2135, 3542, 13, 51530], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 739, "seek": 260124, "start": 2624.56, "end": 2627.8799999999997, "text": " And the surprising thing about the last one that I mentioned before was that a", "tokens": [51530, 400, 264, 8830, 551, 466, 264, 1036, 472, 300, 286, 2835, 949, 390, 300, 257, 51696], "temperature": 0.0, "avg_logprob": -0.13391687024024226, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.029305201023817062}, {"id": 740, "seek": 262788, "start": 2627.88, "end": 2632.08, "text": " lot of times even really unlikely next tokens were able to be steered to be the", "tokens": [50364, 688, 295, 1413, 754, 534, 17518, 958, 22667, 645, 1075, 281, 312, 2126, 4073, 281, 312, 264, 50574], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 741, "seek": 262788, "start": 2632.08, "end": 2636.08, "text": " most likely just using a really short prompt, which both gets at the, you know,", "tokens": [50574, 881, 3700, 445, 1228, 257, 534, 2099, 12391, 11, 597, 1293, 2170, 412, 264, 11, 291, 458, 11, 50774], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 742, "seek": 262788, "start": 2636.28, "end": 2640.96, "text": " basically chaoticness or complexity of language as a system, as well as the fact", "tokens": [50784, 1936, 27013, 1287, 420, 14024, 295, 2856, 382, 257, 1185, 11, 382, 731, 382, 264, 1186, 51018], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 743, "seek": 262788, "start": 2640.96, "end": 2644.56, "text": " that the prior likelihood picture or the cross entropy loss picture doesn't", "tokens": [51018, 300, 264, 4059, 22119, 3036, 420, 264, 3278, 30867, 4470, 3036, 1177, 380, 51198], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 744, "seek": 262788, "start": 2644.56, "end": 2649.36, "text": " quite get at the controllability sense of when you do have a, you know, ability", "tokens": [51198, 1596, 483, 412, 264, 45159, 2310, 2020, 295, 562, 291, 360, 362, 257, 11, 291, 458, 11, 3485, 51438], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 745, "seek": 262788, "start": 2649.36, "end": 2652.0, "text": " to input tokens into the context, what happens then?", "tokens": [51438, 281, 4846, 22667, 666, 264, 4319, 11, 437, 2314, 550, 30, 51570], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 746, "seek": 262788, "start": 2652.04, "end": 2653.6400000000003, "text": " So those are the really the main results.", "tokens": [51572, 407, 729, 366, 264, 534, 264, 2135, 3542, 13, 51652], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 747, "seek": 262788, "start": 2653.6400000000003, "end": 2656.8, "text": " And then I mean, to me, the exciting, the really exciting part was the open", "tokens": [51652, 400, 550, 286, 914, 11, 281, 385, 11, 264, 4670, 11, 264, 534, 4670, 644, 390, 264, 1269, 51810], "temperature": 0.0, "avg_logprob": -0.10595162018485692, "compression_ratio": 1.7943037974683544, "no_speech_prob": 0.0009696450433693826}, {"id": 748, "seek": 265680, "start": 2656.8, "end": 2659.84, "text": " questions where I was like, Oh, now that we're using this vocabulary, now that we", "tokens": [50364, 1651, 689, 286, 390, 411, 11, 876, 11, 586, 300, 321, 434, 1228, 341, 19864, 11, 586, 300, 321, 50516], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 749, "seek": 265680, "start": 2659.84, "end": 2664.0800000000004, "text": " formalize these LLMs as systems, it's really easy to ask these, you know,", "tokens": [50516, 9860, 1125, 613, 441, 43, 26386, 382, 3652, 11, 309, 311, 534, 1858, 281, 1029, 613, 11, 291, 458, 11, 50728], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 750, "seek": 265680, "start": 2664.0800000000004, "end": 2667.36, "text": " additional questions about, you know, the nature of the systems and the", "tokens": [50728, 4497, 1651, 466, 11, 291, 458, 11, 264, 3687, 295, 264, 3652, 293, 264, 50892], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 751, "seek": 265680, "start": 2667.36, "end": 2671.1200000000003, "text": " steerability controllability, especially with feedback or chain of thought or,", "tokens": [50892, 30814, 2310, 45159, 2310, 11, 2318, 365, 5824, 420, 5021, 295, 1194, 420, 11, 51080], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 752, "seek": 265680, "start": 2671.1600000000003, "end": 2673.44, "text": " you know, agents or all of these other ideas.", "tokens": [51082, 291, 458, 11, 12554, 420, 439, 295, 613, 661, 3487, 13, 51196], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 753, "seek": 265680, "start": 2673.6800000000003, "end": 2675.6000000000004, "text": " And so yeah, that was basically the paper.", "tokens": [51208, 400, 370, 1338, 11, 300, 390, 1936, 264, 3035, 13, 51304], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 754, "seek": 265680, "start": 2675.84, "end": 2676.1600000000003, "text": " Yeah.", "tokens": [51316, 865, 13, 51332], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 755, "seek": 265680, "start": 2676.1600000000003, "end": 2679.44, "text": " And it's really making me update my intuitions, right?", "tokens": [51332, 400, 309, 311, 534, 1455, 385, 5623, 452, 16224, 626, 11, 558, 30, 51496], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 756, "seek": 265680, "start": 2679.44, "end": 2681.4, "text": " So I'm thinking about the bias variance trade off.", "tokens": [51496, 407, 286, 478, 1953, 466, 264, 12577, 21977, 4923, 766, 13, 51594], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 757, "seek": 265680, "start": 2681.84, "end": 2686.2400000000002, "text": " And I'm thinking that the reason we build these inductive priors is to", "tokens": [51616, 400, 286, 478, 1953, 300, 264, 1778, 321, 1322, 613, 31612, 488, 1790, 830, 307, 281, 51836], "temperature": 0.0, "avg_logprob": -0.151671848799053, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0028834473341703415}, {"id": 758, "seek": 268624, "start": 2686.24, "end": 2690.2799999999997, "text": " constrain the model intentionally to make it statistically tractable to reduce", "tokens": [50364, 1817, 7146, 264, 2316, 22062, 281, 652, 309, 36478, 24207, 712, 281, 5407, 50566], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 759, "seek": 268624, "start": 2690.2799999999997, "end": 2691.9199999999996, "text": " the size of the hypothesis class.", "tokens": [50566, 264, 2744, 295, 264, 17291, 1508, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 760, "seek": 268624, "start": 2692.3599999999997, "end": 2697.3599999999997, "text": " But what you're saying is making me think that statistical tractability and", "tokens": [50670, 583, 437, 291, 434, 1566, 307, 1455, 385, 519, 300, 22820, 24207, 2310, 293, 50920], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 761, "seek": 268624, "start": 2697.3599999999997, "end": 2700.52, "text": " flexibility are not necessarily the same thing.", "tokens": [50920, 12635, 366, 406, 4725, 264, 912, 551, 13, 51078], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 762, "seek": 268624, "start": 2700.9199999999996, "end": 2705.9199999999996, "text": " Now it seems that the model must maintain a degree of flexibility.", "tokens": [51098, 823, 309, 2544, 300, 264, 2316, 1633, 6909, 257, 4314, 295, 12635, 13, 51348], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 763, "seek": 268624, "start": 2705.9199999999996, "end": 2706.9199999999996, "text": " I mean, it makes sense, right?", "tokens": [51348, 286, 914, 11, 309, 1669, 2020, 11, 558, 30, 51398], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 764, "seek": 268624, "start": 2706.9199999999996, "end": 2710.64, "text": " You have to be flexible in order to be a successful model.", "tokens": [51398, 509, 362, 281, 312, 11358, 294, 1668, 281, 312, 257, 4406, 2316, 13, 51584], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 765, "seek": 268624, "start": 2711.3999999999996, "end": 2713.8799999999997, "text": " But that creates a kind of adversarial attack.", "tokens": [51622, 583, 300, 7829, 257, 733, 295, 17641, 44745, 2690, 13, 51746], "temperature": 0.0, "avg_logprob": -0.12014917646135603, "compression_ratio": 1.71875, "no_speech_prob": 0.0038450479041785}, {"id": 766, "seek": 271388, "start": 2713.88, "end": 2717.88, "text": " So you can, the way I think about this is the model should be like the", "tokens": [50364, 407, 291, 393, 11, 264, 636, 286, 519, 466, 341, 307, 264, 2316, 820, 312, 411, 264, 50564], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 767, "seek": 271388, "start": 2717.88, "end": 2719.6400000000003, "text": " interstate freeway of language.", "tokens": [50564, 728, 15406, 1737, 676, 295, 2856, 13, 50652], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 768, "seek": 271388, "start": 2719.92, "end": 2723.04, "text": " So all of the major roads should be carved out and there should be side", "tokens": [50666, 407, 439, 295, 264, 2563, 11344, 820, 312, 28613, 484, 293, 456, 820, 312, 1252, 50822], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 769, "seek": 271388, "start": 2723.04, "end": 2723.6800000000003, "text": " roads and so on.", "tokens": [50822, 11344, 293, 370, 322, 13, 50854], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 770, "seek": 271388, "start": 2723.6800000000003, "end": 2725.52, "text": " And that's the way I visualized the model.", "tokens": [50854, 400, 300, 311, 264, 636, 286, 5056, 1602, 264, 2316, 13, 50946], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 771, "seek": 271388, "start": 2725.76, "end": 2726.84, "text": " But the model's not like that.", "tokens": [50958, 583, 264, 2316, 311, 406, 411, 300, 13, 51012], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 772, "seek": 271388, "start": 2726.84, "end": 2730.6, "text": " There's actually like all of these little slip roads and you can kind of", "tokens": [51012, 821, 311, 767, 411, 439, 295, 613, 707, 11140, 11344, 293, 291, 393, 733, 295, 51200], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 773, "seek": 271388, "start": 2730.6, "end": 2734.36, "text": " push the cars off into the slip roads, but you need the slip roads because", "tokens": [51200, 2944, 264, 5163, 766, 666, 264, 11140, 11344, 11, 457, 291, 643, 264, 11140, 11344, 570, 51388], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 774, "seek": 271388, "start": 2734.36, "end": 2736.32, "text": " perhaps you couldn't train the model without the slip roads.", "tokens": [51388, 4317, 291, 2809, 380, 3847, 264, 2316, 1553, 264, 11140, 11344, 13, 51486], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 775, "seek": 271388, "start": 2736.52, "end": 2738.6, "text": " Yeah, I think, I think that's a really good analogy.", "tokens": [51496, 865, 11, 286, 519, 11, 286, 519, 300, 311, 257, 534, 665, 21663, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13757428988604478, "compression_ratio": 1.951851851851852, "no_speech_prob": 0.001430390402674675}, {"id": 776, "seek": 273860, "start": 2738.6, "end": 2744.8399999999997, "text": " I think that's, um, thinking about pushing cars off the road into this space", "tokens": [50364, 286, 519, 300, 311, 11, 1105, 11, 1953, 466, 7380, 5163, 766, 264, 3060, 666, 341, 1901, 50676], "temperature": 0.0, "avg_logprob": -0.14408391245295493, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0011877923971042037}, {"id": 777, "seek": 273860, "start": 2744.8399999999997, "end": 2751.44, "text": " where they perhaps aren't used to being and what happens next.", "tokens": [50676, 689, 436, 4317, 3212, 380, 1143, 281, 885, 293, 437, 2314, 958, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14408391245295493, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0011877923971042037}, {"id": 778, "seek": 273860, "start": 2752.12, "end": 2756.16, "text": " This, this is a case where the language model can answer some of these mode", "tokens": [51040, 639, 11, 341, 307, 257, 1389, 689, 264, 2856, 2316, 393, 1867, 512, 295, 613, 4391, 51242], "temperature": 0.0, "avg_logprob": -0.14408391245295493, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0011877923971042037}, {"id": 779, "seek": 273860, "start": 2756.16, "end": 2759.3199999999997, "text": " collapse type regimes and you can get kind of weird outputs.", "tokens": [51242, 15584, 2010, 45738, 293, 291, 393, 483, 733, 295, 3657, 23930, 13, 51400], "temperature": 0.0, "avg_logprob": -0.14408391245295493, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0011877923971042037}, {"id": 780, "seek": 273860, "start": 2759.68, "end": 2765.0, "text": " This is where you also, um, I mean, it was surprising that you can get the", "tokens": [51418, 639, 307, 689, 291, 611, 11, 1105, 11, 286, 914, 11, 309, 390, 8830, 300, 291, 393, 483, 264, 51684], "temperature": 0.0, "avg_logprob": -0.14408391245295493, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.0011877923971042037}, {"id": 781, "seek": 276500, "start": 2765.12, "end": 2770.52, "text": " least likely token with just a specific inputs to be the most, the most", "tokens": [50370, 1935, 3700, 14862, 365, 445, 257, 2685, 15743, 281, 312, 264, 881, 11, 264, 881, 50640], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 782, "seek": 276500, "start": 2770.52, "end": 2775.16, "text": " likely next token, but if we treat language as this kind of road or as", "tokens": [50640, 3700, 958, 14862, 11, 457, 498, 321, 2387, 2856, 382, 341, 733, 295, 3060, 420, 382, 50872], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 783, "seek": 276500, "start": 2775.16, "end": 2778.64, "text": " this kind of map structure, then it kind of makes sense that once you get off", "tokens": [50872, 341, 733, 295, 4471, 3877, 11, 550, 309, 733, 295, 1669, 2020, 300, 1564, 291, 483, 766, 51046], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 784, "seek": 276500, "start": 2778.64, "end": 2782.84, "text": " the map, once you enter this kind of regime that is completely unexplored,", "tokens": [51046, 264, 4471, 11, 1564, 291, 3242, 341, 733, 295, 13120, 300, 307, 2584, 11572, 564, 2769, 11, 51256], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 785, "seek": 276500, "start": 2783.2, "end": 2787.08, "text": " which there are actually plenty of regimes like this again, because the", "tokens": [51274, 597, 456, 366, 767, 7140, 295, 45738, 411, 341, 797, 11, 570, 264, 51468], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 786, "seek": 276500, "start": 2787.08, "end": 2792.2, "text": " space is exponential in the number of tokens, it's growing so incredibly fast", "tokens": [51468, 1901, 307, 21510, 294, 264, 1230, 295, 22667, 11, 309, 311, 4194, 370, 6252, 2370, 51724], "temperature": 0.0, "avg_logprob": -0.12873621076066918, "compression_ratio": 1.8237704918032787, "no_speech_prob": 0.014058529399335384}, {"id": 787, "seek": 279220, "start": 2792.52, "end": 2796.6, "text": " that it's very easy to find pockets that the model has never seen before and", "tokens": [50380, 300, 309, 311, 588, 1858, 281, 915, 16491, 300, 264, 2316, 575, 1128, 1612, 949, 293, 50584], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 788, "seek": 279220, "start": 2796.6, "end": 2799.48, "text": " maybe no human on earth or it never will be seen again.", "tokens": [50584, 1310, 572, 1952, 322, 4120, 420, 309, 1128, 486, 312, 1612, 797, 13, 50728], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 789, "seek": 279220, "start": 2800.2, "end": 2803.7999999999997, "text": " You guys are really interested in, in collective intelligence and", "tokens": [50764, 509, 1074, 366, 534, 3102, 294, 11, 294, 12590, 7599, 293, 50944], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 790, "seek": 279220, "start": 2803.7999999999997, "end": 2807.4399999999996, "text": " biomimetic intelligence and biologically plausible intelligence.", "tokens": [50944, 27450, 332, 3532, 7599, 293, 3228, 17157, 39925, 7599, 13, 51126], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 791, "seek": 279220, "start": 2807.4399999999996, "end": 2810.16, "text": " And this is a matter very close to my heart.", "tokens": [51126, 400, 341, 307, 257, 1871, 588, 1998, 281, 452, 1917, 13, 51262], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 792, "seek": 279220, "start": 2810.52, "end": 2814.2799999999997, "text": " Um, what, what, what are you guys interested in specifically in that field?", "tokens": [51280, 3301, 11, 437, 11, 437, 11, 437, 366, 291, 1074, 3102, 294, 4682, 294, 300, 2519, 30, 51468], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 793, "seek": 279220, "start": 2815.2, "end": 2815.4399999999996, "text": " Yeah.", "tokens": [51514, 865, 13, 51526], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 794, "seek": 279220, "start": 2815.4399999999996, "end": 2820.68, "text": " So I guess when I first got into machine learning, it was from watching", "tokens": [51526, 407, 286, 2041, 562, 286, 700, 658, 666, 3479, 2539, 11, 309, 390, 490, 1976, 51788], "temperature": 0.0, "avg_logprob": -0.14167890295518182, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.012606494128704071}, {"id": 795, "seek": 282068, "start": 2820.68, "end": 2824.04, "text": " this Google DeepMind video where they were using reinforcement learning to", "tokens": [50364, 341, 3329, 14895, 44, 471, 960, 689, 436, 645, 1228, 29280, 2539, 281, 50532], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 796, "seek": 282068, "start": 2824.04, "end": 2828.56, "text": " teach this guy how to run this virtual reality avatar, how to run really fast.", "tokens": [50532, 2924, 341, 2146, 577, 281, 1190, 341, 6374, 4103, 36205, 11, 577, 281, 1190, 534, 2370, 13, 50758], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 797, "seek": 282068, "start": 2828.56, "end": 2831.96, "text": " And I thought that was fascinating because it was like, okay, instead of", "tokens": [50758, 400, 286, 1194, 300, 390, 10343, 570, 309, 390, 411, 11, 1392, 11, 2602, 295, 50928], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 798, "seek": 282068, "start": 2832.0, "end": 2835.24, "text": " traditional programming, you just have this neural network that optimizes", "tokens": [50930, 5164, 9410, 11, 291, 445, 362, 341, 18161, 3209, 300, 5028, 5660, 51092], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 799, "seek": 282068, "start": 2835.24, "end": 2836.9199999999996, "text": " itself according to some objective, right?", "tokens": [51092, 2564, 4650, 281, 512, 10024, 11, 558, 30, 51176], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 800, "seek": 282068, "start": 2837.2799999999997, "end": 2841.0, "text": " And the thing that was intriguing to me about that was like the feed forward", "tokens": [51194, 400, 264, 551, 300, 390, 32503, 281, 385, 466, 300, 390, 411, 264, 3154, 2128, 51380], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 801, "seek": 282068, "start": 2841.0, "end": 2843.3199999999997, "text": " dynamics of a neural network aren't that complicated, right?", "tokens": [51380, 15679, 295, 257, 18161, 3209, 3212, 380, 300, 6179, 11, 558, 30, 51496], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 802, "seek": 282068, "start": 2843.44, "end": 2846.52, "text": " You know, you have these synapses, you have this sort of gated action", "tokens": [51502, 509, 458, 11, 291, 362, 613, 5451, 2382, 279, 11, 291, 362, 341, 1333, 295, 290, 770, 3069, 51656], "temperature": 0.0, "avg_logprob": -0.11555834114551544, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.07579541951417923}, {"id": 803, "seek": 284652, "start": 2846.52, "end": 2847.64, "text": " potential function.", "tokens": [50364, 3995, 2445, 13, 50420], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 804, "seek": 284652, "start": 2847.92, "end": 2853.12, "text": " And the thing that was weird to me was like, how does every neuron know how", "tokens": [50434, 400, 264, 551, 300, 390, 3657, 281, 385, 390, 411, 11, 577, 775, 633, 34090, 458, 577, 50694], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 805, "seek": 284652, "start": 2853.12, "end": 2854.36, "text": " to change its weights, right?", "tokens": [50694, 281, 1319, 1080, 17443, 11, 558, 30, 50756], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 806, "seek": 284652, "start": 2854.6, "end": 2858.44, "text": " How does each neuron that's independently not that smart know what to do?", "tokens": [50768, 1012, 775, 1184, 34090, 300, 311, 21761, 406, 300, 4069, 458, 437, 281, 360, 30, 50960], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 807, "seek": 284652, "start": 2858.8, "end": 2861.88, "text": " And so that sort of led me down the theoretical, the theoretical", "tokens": [50978, 400, 370, 300, 1333, 295, 4684, 385, 760, 264, 20864, 11, 264, 20864, 51132], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 808, "seek": 284652, "start": 2861.88, "end": 2865.36, "text": " neuroscience route for some time where I was trying to figure out, okay, what", "tokens": [51132, 42762, 7955, 337, 512, 565, 689, 286, 390, 1382, 281, 2573, 484, 11, 1392, 11, 437, 51306], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 809, "seek": 284652, "start": 2865.36, "end": 2868.48, "text": " do these learning rules look like that don't have to, you know, use the chain", "tokens": [51306, 360, 613, 2539, 4474, 574, 411, 300, 500, 380, 362, 281, 11, 291, 458, 11, 764, 264, 5021, 51462], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 810, "seek": 284652, "start": 2868.48, "end": 2871.08, "text": " rule, use back propagation to update their weights.", "tokens": [51462, 4978, 11, 764, 646, 38377, 281, 5623, 641, 17443, 13, 51592], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 811, "seek": 284652, "start": 2871.2, "end": 2874.6, "text": " So I did that for a while and then sort of realized that the question of", "tokens": [51598, 407, 286, 630, 300, 337, 257, 1339, 293, 550, 1333, 295, 5334, 300, 264, 1168, 295, 51768], "temperature": 0.0, "avg_logprob": -0.12207042552806713, "compression_ratio": 1.804635761589404, "no_speech_prob": 0.03620951995253563}, {"id": 812, "seek": 287460, "start": 2874.6, "end": 2878.36, "text": " supervised learning was not necessarily the most interesting question to be", "tokens": [50364, 46533, 2539, 390, 406, 4725, 264, 881, 1880, 1168, 281, 312, 50552], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 813, "seek": 287460, "start": 2878.36, "end": 2881.36, "text": " asked, where it seems like the lion's share of what makes us really", "tokens": [50552, 2351, 11, 689, 309, 2544, 411, 264, 17226, 311, 2073, 295, 437, 1669, 505, 534, 50702], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 814, "seek": 287460, "start": 2881.36, "end": 2885.92, "text": " interesting as humans in our cognition seems to be associated with the cortex", "tokens": [50702, 1880, 382, 6255, 294, 527, 46905, 2544, 281, 312, 6615, 365, 264, 33312, 50930], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 815, "seek": 287460, "start": 2885.92, "end": 2889.7999999999997, "text": " and this kind of predictive coding module that we have that lets us make", "tokens": [50930, 293, 341, 733, 295, 35521, 17720, 10088, 300, 321, 362, 300, 6653, 505, 652, 51124], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 816, "seek": 287460, "start": 2889.7999999999997, "end": 2893.68, "text": " these really rich abstract representations of reality, sort of understand what's", "tokens": [51124, 613, 534, 4593, 12649, 33358, 295, 4103, 11, 1333, 295, 1223, 437, 311, 51318], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 817, "seek": 287460, "start": 2893.68, "end": 2896.6, "text": " going on, you know, we sort of hallucinate this internal model of the world.", "tokens": [51318, 516, 322, 11, 291, 458, 11, 321, 1333, 295, 35212, 13923, 341, 6920, 2316, 295, 264, 1002, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 818, "seek": 287460, "start": 2897.0, "end": 2901.0, "text": " And so the interesting thing to me about the cortex was that, you know, you", "tokens": [51484, 400, 370, 264, 1880, 551, 281, 385, 466, 264, 33312, 390, 300, 11, 291, 458, 11, 291, 51684], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 819, "seek": 287460, "start": 2901.0, "end": 2904.44, "text": " have this structure that's pretty flat and pretty homogenous throughout, you", "tokens": [51684, 362, 341, 3877, 300, 311, 1238, 4962, 293, 1238, 3655, 45519, 3710, 11, 291, 51856], "temperature": 0.0, "avg_logprob": -0.09938370554070723, "compression_ratio": 1.8730650154798762, "no_speech_prob": 0.005910155829042196}, {"id": 820, "seek": 290444, "start": 2904.44, "end": 2907.56, "text": " know, there's differences in different regions, but the end of the day, it's", "tokens": [50364, 458, 11, 456, 311, 7300, 294, 819, 10682, 11, 457, 264, 917, 295, 264, 786, 11, 309, 311, 50520], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 821, "seek": 290444, "start": 2907.56, "end": 2908.2400000000002, "text": " very similar.", "tokens": [50520, 588, 2531, 13, 50554], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 822, "seek": 290444, "start": 2908.2400000000002, "end": 2911.88, "text": " And in fact, if you lose a sense, like if you lose your vision, that region is", "tokens": [50554, 400, 294, 1186, 11, 498, 291, 3624, 257, 2020, 11, 411, 498, 291, 3624, 428, 5201, 11, 300, 4458, 307, 50736], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 823, "seek": 290444, "start": 2911.88, "end": 2913.36, "text": " often repurposed for other things.", "tokens": [50736, 2049, 1085, 20130, 1744, 337, 661, 721, 13, 50810], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 824, "seek": 290444, "start": 2913.36, "end": 2916.44, "text": " So it seems like there should exist, you know, the brain is kind of this", "tokens": [50810, 407, 309, 2544, 411, 456, 820, 2514, 11, 291, 458, 11, 264, 3567, 307, 733, 295, 341, 50964], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 825, "seek": 290444, "start": 2916.44, "end": 2920.2400000000002, "text": " existence proof that there should exist this rule set that if you apply it", "tokens": [50964, 9123, 8177, 300, 456, 820, 2514, 341, 4978, 992, 300, 498, 291, 3079, 309, 51154], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 826, "seek": 290444, "start": 2920.2400000000002, "end": 2924.12, "text": " everywhere in the system in this sort of layer on the outside of the brain, then", "tokens": [51154, 5315, 294, 264, 1185, 294, 341, 1333, 295, 4583, 322, 264, 2380, 295, 264, 3567, 11, 550, 51348], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 827, "seek": 290444, "start": 2924.36, "end": 2929.0, "text": " the behavior, the emergent property of that system is that you'll get this", "tokens": [51360, 264, 5223, 11, 264, 4345, 6930, 4707, 295, 300, 1185, 307, 300, 291, 603, 483, 341, 51592], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 828, "seek": 290444, "start": 2929.0, "end": 2932.6, "text": " really robust and rich sort of representation of the world that is very", "tokens": [51592, 534, 13956, 293, 4593, 1333, 295, 10290, 295, 264, 1002, 300, 307, 588, 51772], "temperature": 0.0, "avg_logprob": -0.11138214477121014, "compression_ratio": 1.9269102990033222, "no_speech_prob": 0.0013247394235804677}, {"id": 829, "seek": 293260, "start": 2932.64, "end": 2934.88, "text": " predictive of subsequent sensory input.", "tokens": [50366, 35521, 295, 19962, 27233, 4846, 13, 50478], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 830, "seek": 293260, "start": 2934.88, "end": 2935.2, "text": " Right.", "tokens": [50478, 1779, 13, 50494], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 831, "seek": 293260, "start": 2935.52, "end": 2938.48, "text": " And I think that the collective intelligence aspect of that is really,", "tokens": [50510, 400, 286, 519, 300, 264, 12590, 7599, 4171, 295, 300, 307, 534, 11, 50658], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 832, "seek": 293260, "start": 2938.48, "end": 2942.2, "text": " really important where there's one way to go in machine learning where you say,", "tokens": [50658, 534, 1021, 689, 456, 311, 472, 636, 281, 352, 294, 3479, 2539, 689, 291, 584, 11, 50844], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 833, "seek": 293260, "start": 2942.2, "end": 2945.72, "text": " okay, we're going to make this monolithic pile of matrix algebra and we're going", "tokens": [50844, 1392, 11, 321, 434, 516, 281, 652, 341, 1108, 42878, 14375, 295, 8141, 21989, 293, 321, 434, 516, 51020], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 834, "seek": 293260, "start": 2945.72, "end": 2948.16, "text": " to train it through back propagation and gradient descent and the atom", "tokens": [51020, 281, 3847, 309, 807, 646, 38377, 293, 16235, 23475, 293, 264, 12018, 51142], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 835, "seek": 293260, "start": 2948.16, "end": 2949.2, "text": " optimizer and all of that.", "tokens": [51142, 5028, 6545, 293, 439, 295, 300, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 836, "seek": 293260, "start": 2949.52, "end": 2953.4, "text": " And we're going to make it do some prediction task, but at the end of the", "tokens": [51210, 400, 321, 434, 516, 281, 652, 309, 360, 512, 17630, 5633, 11, 457, 412, 264, 917, 295, 264, 51404], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 837, "seek": 293260, "start": 2953.4, "end": 2956.48, "text": " day, every computation has to be implemented in physical reality.", "tokens": [51404, 786, 11, 633, 24903, 575, 281, 312, 12270, 294, 4001, 4103, 13, 51558], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 838, "seek": 293260, "start": 2956.48, "end": 2956.7999999999997, "text": " Right.", "tokens": [51558, 1779, 13, 51574], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 839, "seek": 293260, "start": 2957.08, "end": 2961.12, "text": " And when we make the abstraction and just say, oh, it's just a bunch of math,", "tokens": [51588, 400, 562, 321, 652, 264, 37765, 293, 445, 584, 11, 1954, 11, 309, 311, 445, 257, 3840, 295, 5221, 11, 51790], "temperature": 0.0, "avg_logprob": -0.13037208557128907, "compression_ratio": 1.8212121212121213, "no_speech_prob": 0.0033760766964405775}, {"id": 840, "seek": 296112, "start": 2961.16, "end": 2962.24, "text": " we'll just have a GPU run it.", "tokens": [50366, 321, 603, 445, 362, 257, 18407, 1190, 309, 13, 50420], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 841, "seek": 296112, "start": 2962.56, "end": 2965.3199999999997, "text": " It kind of abstracts away from this fact that at the end of the day,", "tokens": [50436, 467, 733, 295, 12649, 82, 1314, 490, 341, 1186, 300, 412, 264, 917, 295, 264, 786, 11, 50574], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 842, "seek": 296112, "start": 2965.3599999999997, "end": 2969.88, "text": " you have real physical objects that need to do computation and share", "tokens": [50576, 291, 362, 957, 4001, 6565, 300, 643, 281, 360, 24903, 293, 2073, 50802], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 843, "seek": 296112, "start": 2969.88, "end": 2974.3199999999997, "text": " information and in the sort of maximum efficiency, maximum scalability limit,", "tokens": [50802, 1589, 293, 294, 264, 1333, 295, 6674, 10493, 11, 6674, 15664, 2310, 4948, 11, 51024], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 844, "seek": 296112, "start": 2974.56, "end": 2978.44, "text": " it seems like what you'd end up having is a very similar sort of distributed", "tokens": [51036, 309, 2544, 411, 437, 291, 1116, 917, 493, 1419, 307, 257, 588, 2531, 1333, 295, 12631, 51230], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 845, "seek": 296112, "start": 2978.44, "end": 2983.24, "text": " structure where you can't really easily separate memory from computation.", "tokens": [51230, 3877, 689, 291, 393, 380, 534, 3612, 4994, 4675, 490, 24903, 13, 51470], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 846, "seek": 296112, "start": 2983.24, "end": 2986.52, "text": " I think there's a quote from this MIT professor that says that Turing's", "tokens": [51470, 286, 519, 456, 311, 257, 6513, 490, 341, 13100, 8304, 300, 1619, 300, 314, 1345, 311, 51634], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 847, "seek": 296112, "start": 2986.52, "end": 2990.08, "text": " initial mistake was saying that the head of the Turing machine was separate", "tokens": [51634, 5883, 6146, 390, 1566, 300, 264, 1378, 295, 264, 314, 1345, 3479, 390, 4994, 51812], "temperature": 0.0, "avg_logprob": -0.10589020699262619, "compression_ratio": 1.7491961414790997, "no_speech_prob": 0.005383700132369995}, {"id": 848, "seek": 299008, "start": 2990.08, "end": 2990.64, "text": " from the tape.", "tokens": [50364, 490, 264, 7314, 13, 50392], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 849, "seek": 299008, "start": 2991.0, "end": 2994.0, "text": " Uh, and I think that that's true where in reality, you know, in brains,", "tokens": [50410, 4019, 11, 293, 286, 519, 300, 300, 311, 2074, 689, 294, 4103, 11, 291, 458, 11, 294, 15442, 11, 50560], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 850, "seek": 299008, "start": 2994.0, "end": 2998.36, "text": " in, in real computing systems, the matter that composes the memory and the", "tokens": [50560, 294, 11, 294, 957, 15866, 3652, 11, 264, 1871, 300, 715, 4201, 264, 4675, 293, 264, 50778], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 851, "seek": 299008, "start": 2998.36, "end": 3000.88, "text": " matter that composes the computation is really one in the same.", "tokens": [50778, 1871, 300, 715, 4201, 264, 24903, 307, 534, 472, 294, 264, 912, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 852, "seek": 299008, "start": 3001.16, "end": 3004.36, "text": " And the brain is obviously this really great proof that, okay, there are", "tokens": [50918, 400, 264, 3567, 307, 2745, 341, 534, 869, 8177, 300, 11, 1392, 11, 456, 366, 51078], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 853, "seek": 299008, "start": 3004.36, "end": 3008.44, "text": " relatively simple rules that are implementable with these biological neurons", "tokens": [51078, 7226, 2199, 4474, 300, 366, 4445, 712, 365, 613, 13910, 22027, 51282], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 854, "seek": 299008, "start": 3008.44, "end": 3011.24, "text": " that if you just implement them everywhere, we'll get you this really", "tokens": [51282, 300, 498, 291, 445, 4445, 552, 5315, 11, 321, 603, 483, 291, 341, 534, 51422], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 855, "seek": 299008, "start": 3011.24, "end": 3014.7599999999998, "text": " beautiful, you know, convergence and emergent property of intelligence.", "tokens": [51422, 2238, 11, 291, 458, 11, 32181, 293, 4345, 6930, 4707, 295, 7599, 13, 51598], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 856, "seek": 299008, "start": 3015.08, "end": 3018.2, "text": " And that really drove me for a long time in theoretical neuroscience.", "tokens": [51614, 400, 300, 534, 13226, 385, 337, 257, 938, 565, 294, 20864, 42762, 13, 51770], "temperature": 0.0, "avg_logprob": -0.11314606321030769, "compression_ratio": 1.930921052631579, "no_speech_prob": 0.0027145580388605595}, {"id": 857, "seek": 301820, "start": 3018.2, "end": 3023.56, "text": " And then more recently in trying to build these distributed systems of, you", "tokens": [50364, 400, 550, 544, 3938, 294, 1382, 281, 1322, 613, 12631, 3652, 295, 11, 291, 50632], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 858, "seek": 301820, "start": 3023.56, "end": 3027.3199999999997, "text": " know, artificial intelligences that, you know, the dream that I was trying to", "tokens": [50632, 458, 11, 11677, 5613, 2667, 300, 11, 291, 458, 11, 264, 3055, 300, 286, 390, 1382, 281, 50820], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 859, "seek": 301820, "start": 3027.3199999999997, "end": 3030.72, "text": " pursue before we started this control theory thing was that, okay, well, what", "tokens": [50820, 12392, 949, 321, 1409, 341, 1969, 5261, 551, 390, 300, 11, 1392, 11, 731, 11, 437, 50990], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 860, "seek": 301820, "start": 3030.72, "end": 3033.68, "text": " if I just had a bunch of really small LLMs that, you know, everybody in the", "tokens": [50990, 498, 286, 445, 632, 257, 3840, 295, 534, 1359, 441, 43, 26386, 300, 11, 291, 458, 11, 2201, 294, 264, 51138], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 861, "seek": 301820, "start": 3033.68, "end": 3037.16, "text": " world could host and they could communicate with this sort of low band", "tokens": [51138, 1002, 727, 3975, 293, 436, 727, 7890, 365, 341, 1333, 295, 2295, 4116, 51312], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 862, "seek": 301820, "start": 3037.16, "end": 3040.8399999999997, "text": " with communication using just tokens, just text over, you know, the regular", "tokens": [51312, 365, 6101, 1228, 445, 22667, 11, 445, 2487, 670, 11, 291, 458, 11, 264, 3890, 51496], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 863, "seek": 301820, "start": 3040.8399999999997, "end": 3044.68, "text": " internet and the emergent property of that, you know, what if it was possible", "tokens": [51496, 4705, 293, 264, 4345, 6930, 4707, 295, 300, 11, 291, 458, 11, 437, 498, 309, 390, 1944, 51688], "temperature": 0.0, "avg_logprob": -0.08772031962871552, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0004305089241825044}, {"id": 864, "seek": 304468, "start": 3044.68, "end": 3048.7599999999998, "text": " that we can engineer a system that the emergent property was that it would", "tokens": [50364, 300, 321, 393, 11403, 257, 1185, 300, 264, 4345, 6930, 4707, 390, 300, 309, 576, 50568], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 865, "seek": 304468, "start": 3048.7599999999998, "end": 3052.9199999999996, "text": " actually be this really capable collective where maybe GPT-7 can be", "tokens": [50568, 767, 312, 341, 534, 8189, 12590, 689, 1310, 26039, 51, 12, 22, 393, 312, 50776], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 866, "seek": 304468, "start": 3052.9199999999996, "end": 3055.9199999999996, "text": " owned by everyone instead of just being behind closed doors in a data center", "tokens": [50776, 11684, 538, 1518, 2602, 295, 445, 885, 2261, 5395, 8077, 294, 257, 1412, 3056, 50926], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 867, "seek": 304468, "start": 3055.9199999999996, "end": 3056.68, "text": " that we have now.", "tokens": [50926, 300, 321, 362, 586, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 868, "seek": 304468, "start": 3056.8799999999997, "end": 3061.12, "text": " We're sort of using these insane engineering, you know, feats of, you", "tokens": [50974, 492, 434, 1333, 295, 1228, 613, 10838, 7043, 11, 291, 458, 11, 579, 1720, 295, 11, 291, 51186], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 869, "seek": 304468, "start": 3061.12, "end": 3063.64, "text": " know, NVIDIA interconnects and these really high bandwidth connections", "tokens": [51186, 458, 11, 426, 3958, 6914, 26253, 82, 293, 613, 534, 1090, 23647, 9271, 51312], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 870, "seek": 304468, "start": 3063.64, "end": 3068.04, "text": " between massive racks in a data center that take a ton of energy to get this", "tokens": [51312, 1296, 5994, 47063, 294, 257, 1412, 3056, 300, 747, 257, 2952, 295, 2281, 281, 483, 341, 51532], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 871, "seek": 304468, "start": 3068.04, "end": 3070.7999999999997, "text": " really great result of, you know, modern language models.", "tokens": [51532, 534, 869, 1874, 295, 11, 291, 458, 11, 4363, 2856, 5245, 13, 51670], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 872, "seek": 304468, "start": 3071.04, "end": 3073.8399999999997, "text": " What if we could have a system that was a bit more like the brain, a bit", "tokens": [51682, 708, 498, 321, 727, 362, 257, 1185, 300, 390, 257, 857, 544, 411, 264, 3567, 11, 257, 857, 51822], "temperature": 0.0, "avg_logprob": -0.09601033396191067, "compression_ratio": 1.7811550151975684, "no_speech_prob": 0.003706760238856077}, {"id": 873, "seek": 307384, "start": 3073.84, "end": 3077.6000000000004, "text": " more decentralized and really leverage this insight that it should be possible,", "tokens": [50364, 544, 32870, 293, 534, 13982, 341, 11269, 300, 309, 820, 312, 1944, 11, 50552], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 874, "seek": 307384, "start": 3077.6000000000004, "end": 3079.96, "text": " you know, this existence proof keeps coming back to you where it's like,", "tokens": [50552, 291, 458, 11, 341, 9123, 8177, 5965, 1348, 646, 281, 291, 689, 309, 311, 411, 11, 50670], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 875, "seek": 307384, "start": 3079.96, "end": 3081.36, "text": " okay, it should be possible, right?", "tokens": [50670, 1392, 11, 309, 820, 312, 1944, 11, 558, 30, 50740], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 876, "seek": 307384, "start": 3081.8, "end": 3085.6400000000003, "text": " And that is sort of originally what led me to the control theory stuff where it", "tokens": [50762, 400, 300, 307, 1333, 295, 7993, 437, 4684, 385, 281, 264, 1969, 5261, 1507, 689, 309, 50954], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 877, "seek": 307384, "start": 3085.6400000000003, "end": 3088.36, "text": " just turned out to be really hard where we didn't have a great understanding of,", "tokens": [50954, 445, 3574, 484, 281, 312, 534, 1152, 689, 321, 994, 380, 362, 257, 869, 3701, 295, 11, 51090], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 878, "seek": 307384, "start": 3088.56, "end": 3091.8, "text": " you know, if we're treating these LLMs as systems rather than just, you know,", "tokens": [51100, 291, 458, 11, 498, 321, 434, 15083, 613, 441, 43, 26386, 382, 3652, 2831, 813, 445, 11, 291, 458, 11, 51262], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 879, "seek": 307384, "start": 3091.8, "end": 3095.08, "text": " big piles of matrix algebra that we're trying to distribute over many GPUs,", "tokens": [51262, 955, 34861, 295, 8141, 21989, 300, 321, 434, 1382, 281, 20594, 670, 867, 18407, 82, 11, 51426], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 880, "seek": 307384, "start": 3095.36, "end": 3098.1600000000003, "text": " if you treat them as systems that are coupled together, they're interacting", "tokens": [51440, 498, 291, 2387, 552, 382, 3652, 300, 366, 29482, 1214, 11, 436, 434, 18017, 51580], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 881, "seek": 307384, "start": 3098.1600000000003, "end": 3100.6400000000003, "text": " in this networked fashion, how do we really understand that?", "tokens": [51580, 294, 341, 3209, 292, 6700, 11, 577, 360, 321, 534, 1223, 300, 30, 51704], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 882, "seek": 307384, "start": 3100.6400000000003, "end": 3102.96, "text": " You know, is it even possible to prompt them to do the right thing?", "tokens": [51704, 509, 458, 11, 307, 309, 754, 1944, 281, 12391, 552, 281, 360, 264, 558, 551, 30, 51820], "temperature": 0.0, "avg_logprob": -0.10118747311969135, "compression_ratio": 1.84375, "no_speech_prob": 0.004609107039868832}, {"id": 883, "seek": 310296, "start": 3103.0, "end": 3103.8, "text": " When is it possible?", "tokens": [50366, 1133, 307, 309, 1944, 30, 50406], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 884, "seek": 310296, "start": 3103.8, "end": 3105.12, "text": " How long do the prompts need to be?", "tokens": [50406, 1012, 938, 360, 264, 41095, 643, 281, 312, 30, 50472], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 885, "seek": 310296, "start": 3105.48, "end": 3106.92, "text": " And that sort of led us down this route.", "tokens": [50490, 400, 300, 1333, 295, 4684, 505, 760, 341, 7955, 13, 50562], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 886, "seek": 310296, "start": 3107.48, "end": 3110.48, "text": " But yeah, definitely the collective intelligence thing was, was a big", "tokens": [50590, 583, 1338, 11, 2138, 264, 12590, 7599, 551, 390, 11, 390, 257, 955, 50740], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 887, "seek": 310296, "start": 3110.48, "end": 3112.4, "text": " motivation for me to get this working.", "tokens": [50740, 12335, 337, 385, 281, 483, 341, 1364, 13, 50836], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 888, "seek": 310296, "start": 3112.4, "end": 3115.88, "text": " And there's this neural cellular automata thing that I know you had talked", "tokens": [50836, 400, 456, 311, 341, 18161, 29267, 3553, 3274, 551, 300, 286, 458, 291, 632, 2825, 51010], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 889, "seek": 310296, "start": 3115.88, "end": 3118.64, "text": " with Michael Levin, who was the last author on that.", "tokens": [51010, 365, 5116, 1456, 4796, 11, 567, 390, 264, 1036, 3793, 322, 300, 13, 51148], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 890, "seek": 310296, "start": 3118.64, "end": 3122.16, "text": " And we worked with Alexander Mordvinsev on it, where it's this really,", "tokens": [51148, 400, 321, 2732, 365, 14845, 376, 765, 85, 1292, 13379, 322, 309, 11, 689, 309, 311, 341, 534, 11, 51324], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 891, "seek": 310296, "start": 3122.16, "end": 3126.84, "text": " really great demonstration of how if you just optimize these basically small", "tokens": [51324, 534, 869, 16520, 295, 577, 498, 291, 445, 19719, 613, 1936, 1359, 51558], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 892, "seek": 310296, "start": 3126.84, "end": 3131.96, "text": " MLPs with local interaction to try to satisfy some objective, like, you know,", "tokens": [51558, 21601, 23043, 365, 2654, 9285, 281, 853, 281, 19319, 512, 10024, 11, 411, 11, 291, 458, 11, 51814], "temperature": 0.0, "avg_logprob": -0.15662496073262674, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0057289013639092445}, {"id": 893, "seek": 313196, "start": 3132.0, "end": 3136.4, "text": " reforming this gecko or lizard in their paper, then you actually can do that", "tokens": [50366, 8290, 278, 341, 1519, 41416, 420, 39215, 294, 641, 3035, 11, 550, 291, 767, 393, 360, 300, 50586], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 894, "seek": 313196, "start": 3136.4, "end": 3137.68, "text": " with back propagation through time.", "tokens": [50586, 365, 646, 38377, 807, 565, 13, 50650], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 895, "seek": 313196, "start": 3137.68, "end": 3140.76, "text": " And so, you know, I thought, you know, it'd be really cool if we could try", "tokens": [50650, 400, 370, 11, 291, 458, 11, 286, 1194, 11, 291, 458, 11, 309, 1116, 312, 534, 1627, 498, 321, 727, 853, 50804], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 896, "seek": 313196, "start": 3140.76, "end": 3144.0, "text": " to engineer information processing systems that did this, not just morphogenesis", "tokens": [50804, 281, 11403, 1589, 9007, 3652, 300, 630, 341, 11, 406, 445, 25778, 8799, 9374, 50966], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 897, "seek": 313196, "start": 3144.0, "end": 3147.36, "text": " systems, but information processing systems that operate in this way.", "tokens": [50966, 3652, 11, 457, 1589, 9007, 3652, 300, 9651, 294, 341, 636, 13, 51134], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 898, "seek": 313196, "start": 3147.36, "end": 3150.4, "text": " Cause, you know, as a graduate of engineering science, we had to take", "tokens": [51134, 10865, 11, 291, 458, 11, 382, 257, 8080, 295, 7043, 3497, 11, 321, 632, 281, 747, 51286], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 899, "seek": 313196, "start": 3150.4, "end": 3152.04, "text": " a bunch of these digital logic courses.", "tokens": [51286, 257, 3840, 295, 613, 4562, 9952, 7712, 13, 51368], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 900, "seek": 313196, "start": 3152.28, "end": 3156.08, "text": " And when you have this very simple, you basically local state machine that has", "tokens": [51380, 400, 562, 291, 362, 341, 588, 2199, 11, 291, 1936, 2654, 1785, 3479, 300, 575, 51570], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 901, "seek": 313196, "start": 3156.44, "end": 3159.28, "text": " basically local connectivity, it's really easy to imagine how it", "tokens": [51588, 1936, 2654, 21095, 11, 309, 311, 534, 1858, 281, 3811, 577, 309, 51730], "temperature": 0.0, "avg_logprob": -0.10180431393021387, "compression_ratio": 1.8557993730407523, "no_speech_prob": 0.00538389990106225}, {"id": 902, "seek": 315928, "start": 3159.28, "end": 3162.7200000000003, "text": " would implement that as a custom chip and sort of reach this, you know,", "tokens": [50364, 576, 4445, 300, 382, 257, 2375, 11409, 293, 1333, 295, 2524, 341, 11, 291, 458, 11, 50536], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 903, "seek": 315928, "start": 3162.7200000000003, "end": 3166.28, "text": " as Beth Jesus puts it, you know, thermodynamic limit of AI.", "tokens": [50536, 382, 14011, 2705, 8137, 309, 11, 291, 458, 11, 8810, 34988, 4948, 295, 7318, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 904, "seek": 315928, "start": 3166.52, "end": 3167.6400000000003, "text": " And so that really excited me.", "tokens": [50726, 400, 370, 300, 534, 2919, 385, 13, 50782], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 905, "seek": 315928, "start": 3167.6400000000003, "end": 3170.6400000000003, "text": " And so I built a sort of demo of that where it was trying to do visual", "tokens": [50782, 400, 370, 286, 3094, 257, 1333, 295, 10723, 295, 300, 689, 309, 390, 1382, 281, 360, 5056, 50932], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 906, "seek": 315928, "start": 3170.6400000000003, "end": 3174.2400000000002, "text": " information processing on this really sparsified video is basically trying to", "tokens": [50932, 1589, 9007, 322, 341, 534, 637, 685, 2587, 960, 307, 1936, 1382, 281, 51112], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 907, "seek": 315928, "start": 3174.2400000000002, "end": 3178.6400000000003, "text": " do predictive coding of sorts on our active inference, I guess, on this", "tokens": [51112, 360, 35521, 17720, 295, 7527, 322, 527, 4967, 38253, 11, 286, 2041, 11, 322, 341, 51332], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 908, "seek": 315928, "start": 3178.6800000000003, "end": 3181.6000000000004, "text": " incoming data stream of really sparsified video, trying to predict what would", "tokens": [51334, 22341, 1412, 4309, 295, 534, 637, 685, 2587, 960, 11, 1382, 281, 6069, 437, 576, 51480], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 909, "seek": 315928, "start": 3181.6000000000004, "end": 3183.84, "text": " happen next, and it turned out to work quite well.", "tokens": [51480, 1051, 958, 11, 293, 309, 3574, 484, 281, 589, 1596, 731, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 910, "seek": 315928, "start": 3184.0400000000004, "end": 3186.5600000000004, "text": " And so then I was like, well, why can't we do that with language models?", "tokens": [51602, 400, 370, 550, 286, 390, 411, 11, 731, 11, 983, 393, 380, 321, 360, 300, 365, 2856, 5245, 30, 51728], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 911, "seek": 315928, "start": 3186.5600000000004, "end": 3189.0400000000004, "text": " You know, as you mentioned, there are all these slip roads, right?", "tokens": [51728, 509, 458, 11, 382, 291, 2835, 11, 456, 366, 439, 613, 11140, 11344, 11, 558, 30, 51852], "temperature": 0.0, "avg_logprob": -0.13316725542445382, "compression_ratio": 1.8212290502793296, "no_speech_prob": 0.021606840193271637}, {"id": 912, "seek": 318904, "start": 3189.04, "end": 3192.24, "text": " Where if you prompt it just right, you can enter this really weird different", "tokens": [50364, 2305, 498, 291, 12391, 309, 445, 558, 11, 291, 393, 3242, 341, 534, 3657, 819, 50524], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 913, "seek": 318904, "start": 3192.24, "end": 3196.48, "text": " regime and this exponentially large prompt space is a really handy way to try", "tokens": [50524, 13120, 293, 341, 37330, 2416, 12391, 1901, 307, 257, 534, 13239, 636, 281, 853, 50736], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 914, "seek": 318904, "start": 3196.48, "end": 3199.72, "text": " to control them where, you know, fine tuning is great, but what if we could", "tokens": [50736, 281, 1969, 552, 689, 11, 291, 458, 11, 2489, 15164, 307, 869, 11, 457, 437, 498, 321, 727, 50898], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 915, "seek": 318904, "start": 3199.72, "end": 3203.52, "text": " just prompt them into interacting in a way that would lead to this emergent", "tokens": [50898, 445, 12391, 552, 666, 18017, 294, 257, 636, 300, 576, 1477, 281, 341, 4345, 6930, 51088], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 916, "seek": 318904, "start": 3203.52, "end": 3206.88, "text": " property of just being basically one larger language model that could", "tokens": [51088, 4707, 295, 445, 885, 1936, 472, 4833, 2856, 2316, 300, 727, 51256], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 917, "seek": 318904, "start": 3206.92, "end": 3208.88, "text": " predict the next token really, really well.", "tokens": [51258, 6069, 264, 958, 14862, 534, 11, 534, 731, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 918, "seek": 318904, "start": 3209.24, "end": 3212.4, "text": " And so that initial motivation sort of led to this control theory stuff.", "tokens": [51374, 400, 370, 300, 5883, 12335, 1333, 295, 4684, 281, 341, 1969, 5261, 1507, 13, 51532], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 919, "seek": 318904, "start": 3212.4, "end": 3216.84, "text": " And I think that it is probably the right way to go for the field where if we", "tokens": [51532, 400, 286, 519, 300, 309, 307, 1391, 264, 558, 636, 281, 352, 337, 264, 2519, 689, 498, 321, 51754], "temperature": 0.0, "avg_logprob": -0.10280414061112837, "compression_ratio": 1.8660130718954249, "no_speech_prob": 0.0019263892900198698}, {"id": 920, "seek": 321684, "start": 3216.84, "end": 3221.36, "text": " want to be able to really leverage maximal computation towards our objectives,", "tokens": [50364, 528, 281, 312, 1075, 281, 534, 13982, 49336, 24903, 3030, 527, 15961, 11, 50590], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 921, "seek": 321684, "start": 3221.4, "end": 3224.32, "text": " you know, the bitter lesson by Richard Sutton kind of suggests that we should", "tokens": [50592, 291, 458, 11, 264, 13871, 6898, 538, 9809, 40492, 1756, 733, 295, 13409, 300, 321, 820, 50738], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 922, "seek": 321684, "start": 3224.32, "end": 3228.32, "text": " probably aim for systems where you can just slap on more and more compute.", "tokens": [50738, 1391, 5939, 337, 3652, 689, 291, 393, 445, 21075, 322, 544, 293, 544, 14722, 13, 50938], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 923, "seek": 321684, "start": 3228.32, "end": 3231.4, "text": " You can have a relatively simple procedure that you follow to leverage", "tokens": [50938, 509, 393, 362, 257, 7226, 2199, 10747, 300, 291, 1524, 281, 13982, 51092], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 924, "seek": 321684, "start": 3231.4, "end": 3233.28, "text": " more compute towards your objectives.", "tokens": [51092, 544, 14722, 3030, 428, 15961, 13, 51186], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 925, "seek": 321684, "start": 3233.44, "end": 3235.6400000000003, "text": " That's probably the way to go for making advances in AI.", "tokens": [51194, 663, 311, 1391, 264, 636, 281, 352, 337, 1455, 25297, 294, 7318, 13, 51304], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 926, "seek": 321684, "start": 3235.88, "end": 3239.6000000000004, "text": " And if we can have this decentralized networked system that, you know, I took", "tokens": [51316, 400, 498, 321, 393, 362, 341, 32870, 3209, 292, 1185, 300, 11, 291, 458, 11, 286, 1890, 51502], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 927, "seek": 321684, "start": 3239.6000000000004, "end": 3242.1600000000003, "text": " this distributed systems course while I was here, that was really great and", "tokens": [51502, 341, 12631, 3652, 1164, 1339, 286, 390, 510, 11, 300, 390, 534, 869, 293, 51630], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 928, "seek": 321684, "start": 3242.1600000000003, "end": 3245.04, "text": " sort of taught how to make, you know, basically databases that were", "tokens": [51630, 1333, 295, 5928, 577, 281, 652, 11, 291, 458, 11, 1936, 22380, 300, 645, 51774], "temperature": 0.0, "avg_logprob": -0.10178219931466238, "compression_ratio": 1.7942028985507246, "no_speech_prob": 0.02032645232975483}, {"id": 929, "seek": 324504, "start": 3245.04, "end": 3248.16, "text": " distributed over many servers that would have this, you know, the emergent", "tokens": [50364, 12631, 670, 867, 15909, 300, 576, 362, 341, 11, 291, 458, 11, 264, 4345, 6930, 50520], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 930, "seek": 324504, "start": 3248.16, "end": 3251.2799999999997, "text": " property they wanted was robustness, consistency and availability.", "tokens": [50520, 4707, 436, 1415, 390, 13956, 1287, 11, 14416, 293, 17945, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 931, "seek": 324504, "start": 3252.08, "end": 3255.52, "text": " If we could have something similar to that, that is radically scalable and is", "tokens": [50716, 759, 321, 727, 362, 746, 2531, 281, 300, 11, 300, 307, 35508, 38481, 293, 307, 50888], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 932, "seek": 324504, "start": 3255.52, "end": 3258.64, "text": " able to be, you know, just run by regular people who don't need to own their", "tokens": [50888, 1075, 281, 312, 11, 291, 458, 11, 445, 1190, 538, 3890, 561, 567, 500, 380, 643, 281, 1065, 641, 51044], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 933, "seek": 324504, "start": 3258.64, "end": 3262.36, "text": " own, you know, GPU cluster that's maybe illegal in the future when the US", "tokens": [51044, 1065, 11, 291, 458, 11, 18407, 13630, 300, 311, 1310, 11905, 294, 264, 2027, 562, 264, 2546, 51230], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 934, "seek": 324504, "start": 3262.36, "end": 3264.56, "text": " government is like, oh, you can only have this many petaflops.", "tokens": [51230, 2463, 307, 411, 11, 1954, 11, 291, 393, 787, 362, 341, 867, 3817, 2792, 75, 3370, 13, 51340], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 935, "seek": 324504, "start": 3265.52, "end": 3269.6, "text": " Basically, yeah, that was the real motivation for, for the, what I call", "tokens": [51388, 8537, 11, 1338, 11, 300, 390, 264, 957, 12335, 337, 11, 337, 264, 11, 437, 286, 818, 51592], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 936, "seek": 324504, "start": 3269.6, "end": 3270.96, "text": " the language game, that project.", "tokens": [51592, 264, 2856, 1216, 11, 300, 1716, 13, 51660], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 937, "seek": 324504, "start": 3270.96, "end": 3273.16, "text": " And that's something that we're continuing to work on.", "tokens": [51660, 400, 300, 311, 746, 300, 321, 434, 9289, 281, 589, 322, 13, 51770], "temperature": 0.0, "avg_logprob": -0.12800188700358073, "compression_ratio": 1.728862973760933, "no_speech_prob": 0.002550488570705056}, {"id": 938, "seek": 327316, "start": 3273.16, "end": 3276.04, "text": " But yeah, that kind of led to this control theory thing where we were just", "tokens": [50364, 583, 1338, 11, 300, 733, 295, 4684, 281, 341, 1969, 5261, 551, 689, 321, 645, 445, 50508], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 939, "seek": 327316, "start": 3276.04, "end": 3279.3999999999996, "text": " like, yeah, we really need to get a grip on what these look like as systems.", "tokens": [50508, 411, 11, 1338, 11, 321, 534, 643, 281, 483, 257, 12007, 322, 437, 613, 574, 411, 382, 3652, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 940, "seek": 327316, "start": 3279.68, "end": 3283.96, "text": " As we start to build these more and more complicated, you know, network", "tokens": [50690, 1018, 321, 722, 281, 1322, 613, 544, 293, 544, 6179, 11, 291, 458, 11, 3209, 50904], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 941, "seek": 327316, "start": 3283.96, "end": 3287.24, "text": " distributed, you know, beautiful emergent systems that hopefully will be able", "tokens": [50904, 12631, 11, 291, 458, 11, 2238, 4345, 6930, 3652, 300, 4696, 486, 312, 1075, 51068], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 942, "seek": 327316, "start": 3287.24, "end": 3289.04, "text": " to be hypercapable in the future.", "tokens": [51068, 281, 312, 9848, 9485, 712, 294, 264, 2027, 13, 51158], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 943, "seek": 327316, "start": 3289.3199999999997, "end": 3290.8399999999997, "text": " Yeah, this is all music to my ears.", "tokens": [51172, 865, 11, 341, 307, 439, 1318, 281, 452, 8798, 13, 51248], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 944, "seek": 327316, "start": 3290.8799999999997, "end": 3294.8399999999997, "text": " I'm a huge fan of the externalist thought in cognitive science.", "tokens": [51250, 286, 478, 257, 2603, 3429, 295, 264, 8320, 468, 1194, 294, 15605, 3497, 13, 51448], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 945, "seek": 327316, "start": 3295.2, "end": 3299.2, "text": " And even though I, I love the work from Jeff Hawkins, you were talking about", "tokens": [51466, 400, 754, 1673, 286, 11, 286, 959, 264, 589, 490, 7506, 9325, 10277, 11, 291, 645, 1417, 466, 51666], "temperature": 0.0, "avg_logprob": -0.12773161006153078, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.000918088189791888}, {"id": 946, "seek": 329920, "start": 3299.2, "end": 3305.16, "text": " the neocortex, but even then, you know, I would kind of say that it's a lot", "tokens": [50364, 264, 408, 905, 36143, 11, 457, 754, 550, 11, 291, 458, 11, 286, 576, 733, 295, 584, 300, 309, 311, 257, 688, 50662], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 947, "seek": 329920, "start": 3305.16, "end": 3308.9199999999996, "text": " of the cognition happens outside of the brain, you know, we're not islands.", "tokens": [50662, 295, 264, 46905, 2314, 2380, 295, 264, 3567, 11, 291, 458, 11, 321, 434, 406, 17402, 13, 50850], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 948, "seek": 329920, "start": 3309.2, "end": 3311.8799999999997, "text": " And actually, I was just thinking maybe a better analogy rather than the interstate", "tokens": [50864, 400, 767, 11, 286, 390, 445, 1953, 1310, 257, 1101, 21663, 2831, 813, 264, 728, 15406, 50998], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 949, "seek": 329920, "start": 3311.8799999999997, "end": 3315.7999999999997, "text": " freeway might be, you know, in Star Trek Voyager, there was the wormhole", "tokens": [50998, 1737, 676, 1062, 312, 11, 291, 458, 11, 294, 5705, 25845, 25563, 3557, 11, 456, 390, 264, 23835, 14094, 51194], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 950, "seek": 329920, "start": 3315.7999999999997, "end": 3318.4399999999996, "text": " network and the Borg fan, the secret work.", "tokens": [51194, 3209, 293, 264, 363, 4646, 3429, 11, 264, 4054, 589, 13, 51326], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 951, "seek": 329920, "start": 3318.4399999999996, "end": 3321.04, "text": " And you could kind of like, you know, get into these little slip streams", "tokens": [51326, 400, 291, 727, 733, 295, 411, 11, 291, 458, 11, 483, 666, 613, 707, 11140, 15842, 51456], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 952, "seek": 329920, "start": 3321.04, "end": 3322.8799999999997, "text": " and go to different parts of the universe.", "tokens": [51456, 293, 352, 281, 819, 3166, 295, 264, 6445, 13, 51548], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 953, "seek": 329920, "start": 3323.16, "end": 3326.96, "text": " But when I was interviewing Philip Ball, he wrote this book, How Life Works.", "tokens": [51562, 583, 562, 286, 390, 26524, 21144, 10744, 11, 415, 4114, 341, 1446, 11, 1012, 7720, 27914, 13, 51752], "temperature": 0.0, "avg_logprob": -0.18868000425141432, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.05103643238544464}, {"id": 954, "seek": 332696, "start": 3327.2400000000002, "end": 3330.48, "text": " And he was trying to understand, you know, what are the mechanisms like, you know,", "tokens": [50378, 400, 415, 390, 1382, 281, 1223, 11, 291, 458, 11, 437, 366, 264, 15902, 411, 11, 291, 458, 11, 50540], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 955, "seek": 332696, "start": 3330.48, "end": 3333.44, "text": " self-organization and multi-scale information sharing and, you know,", "tokens": [50540, 2698, 12, 12372, 2144, 293, 4825, 12, 20033, 1589, 5414, 293, 11, 291, 458, 11, 50688], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 956, "seek": 332696, "start": 3333.48, "end": 3334.36, "text": " emergentism.", "tokens": [50690, 4345, 6930, 1434, 13, 50734], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 957, "seek": 332696, "start": 3334.8, "end": 3338.68, "text": " And it's, it's really, really, um, uh, fascinating.", "tokens": [50756, 400, 309, 311, 11, 309, 311, 534, 11, 534, 11, 1105, 11, 2232, 11, 10343, 13, 50950], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 958, "seek": 332696, "start": 3338.96, "end": 3343.88, "text": " So how can we introduce some of these concepts into the next generation of AI?", "tokens": [50964, 407, 577, 393, 321, 5366, 512, 295, 613, 10392, 666, 264, 958, 5125, 295, 7318, 30, 51210], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 959, "seek": 332696, "start": 3344.16, "end": 3347.36, "text": " Yeah, this is one of the things I'm certainly most excited, excited about", "tokens": [51224, 865, 11, 341, 307, 472, 295, 264, 721, 286, 478, 3297, 881, 2919, 11, 2919, 466, 51384], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 960, "seek": 332696, "start": 3347.68, "end": 3354.16, "text": " because I see life as this kind of interconnected, interplay, multi-scale", "tokens": [51400, 570, 286, 536, 993, 382, 341, 733, 295, 36611, 11, 728, 2858, 11, 4825, 12, 20033, 51724], "temperature": 0.0, "avg_logprob": -0.14641656713970638, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.0050388602539896965}, {"id": 961, "seek": 335416, "start": 3354.8799999999997, "end": 3358.24, "text": " process of exploitation and exploration.", "tokens": [50400, 1399, 295, 33122, 293, 16197, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 962, "seek": 335416, "start": 3358.52, "end": 3360.8799999999997, "text": " And these are two terms from the reinforcement learning literature.", "tokens": [50582, 400, 613, 366, 732, 2115, 490, 264, 29280, 2539, 10394, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 963, "seek": 335416, "start": 3361.12, "end": 3364.68, "text": " But I mean this in a much more general sense because at each stage of life, we're", "tokens": [50712, 583, 286, 914, 341, 294, 257, 709, 544, 2674, 2020, 570, 412, 1184, 3233, 295, 993, 11, 321, 434, 50890], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 964, "seek": 335416, "start": 3364.68, "end": 3369.7999999999997, "text": " either going out into the world to get something, to do something, to try something", "tokens": [50890, 2139, 516, 484, 666, 264, 1002, 281, 483, 746, 11, 281, 360, 746, 11, 281, 853, 746, 51146], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 965, "seek": 335416, "start": 3369.7999999999997, "end": 3375.3599999999997, "text": " new, and then at the next stage, we're coming back in, going home, uh, you know,", "tokens": [51146, 777, 11, 293, 550, 412, 264, 958, 3233, 11, 321, 434, 1348, 646, 294, 11, 516, 1280, 11, 2232, 11, 291, 458, 11, 51424], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 966, "seek": 335416, "start": 3375.3999999999996, "end": 3378.2799999999997, "text": " reflecting, uh, going over our insights.", "tokens": [51426, 23543, 11, 2232, 11, 516, 670, 527, 14310, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 967, "seek": 335416, "start": 3378.96, "end": 3383.7999999999997, "text": " And it's, it's this process, this ebb and flow, going out, coming back in.", "tokens": [51604, 400, 309, 311, 11, 309, 311, 341, 1399, 11, 341, 308, 6692, 293, 3095, 11, 516, 484, 11, 1348, 646, 294, 13, 51846], "temperature": 0.0, "avg_logprob": -0.1179044207588571, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.011327738873660564}, {"id": 968, "seek": 338416, "start": 3384.24, "end": 3389.48, "text": " And I see this kind of pattern emerge across many different aspects of machine", "tokens": [50368, 400, 286, 536, 341, 733, 295, 5102, 21511, 2108, 867, 819, 7270, 295, 3479, 50630], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 969, "seek": 338416, "start": 3389.48, "end": 3393.2, "text": " learning and artificial intelligence work in the sense that a lot of our", "tokens": [50630, 2539, 293, 11677, 7599, 589, 294, 264, 2020, 300, 257, 688, 295, 527, 50816], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 970, "seek": 338416, "start": 3393.44, "end": 3397.0, "text": " algorithms that we have now are convergent, they're objective driven.", "tokens": [50828, 14642, 300, 321, 362, 586, 366, 9652, 6930, 11, 436, 434, 10024, 9555, 13, 51006], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 971, "seek": 338416, "start": 3397.3599999999997, "end": 3399.2, "text": " We establish a loss function.", "tokens": [51024, 492, 8327, 257, 4470, 2445, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 972, "seek": 338416, "start": 3399.2, "end": 3402.12, "text": " We say, these are the rules it should follow.", "tokens": [51116, 492, 584, 11, 613, 366, 264, 4474, 309, 820, 1524, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 973, "seek": 338416, "start": 3402.3199999999997, "end": 3404.6, "text": " It's going to update according to this equation.", "tokens": [51272, 467, 311, 516, 281, 5623, 4650, 281, 341, 5367, 13, 51386], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 974, "seek": 338416, "start": 3404.92, "end": 3408.92, "text": " And we set the system running, learns from data, and we have a final product.", "tokens": [51402, 400, 321, 992, 264, 1185, 2614, 11, 27152, 490, 1412, 11, 293, 321, 362, 257, 2572, 1674, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 975, "seek": 338416, "start": 3409.3999999999996, "end": 3414.0, "text": " And on the flip side, there's, you know, like what Ken Stanley works with.", "tokens": [51626, 400, 322, 264, 7929, 1252, 11, 456, 311, 11, 291, 458, 11, 411, 437, 8273, 28329, 1985, 365, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1448173366609167, "compression_ratio": 1.6414473684210527, "no_speech_prob": 0.0007552391034550965}, {"id": 976, "seek": 341416, "start": 3414.3199999999997, "end": 3419.16, "text": " Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms.", "tokens": [50372, 3301, 11, 544, 24765, 4745, 11, 2232, 11, 27567, 14642, 420, 1269, 12, 3502, 14642, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 977, "seek": 341416, "start": 3419.52, "end": 3421.64, "text": " And this is, this is the other side of things.", "tokens": [50632, 400, 341, 307, 11, 341, 307, 264, 661, 1252, 295, 721, 13, 50738], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 978, "seek": 341416, "start": 3421.64, "end": 3426.24, "text": " And I think some of the most interesting work to be done is how these", "tokens": [50738, 400, 286, 519, 512, 295, 264, 881, 1880, 589, 281, 312, 1096, 307, 577, 613, 50968], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 979, "seek": 341416, "start": 3426.24, "end": 3433.0, "text": " two sides interconnects, how can we lay down rules, strict rigid rules, which", "tokens": [50968, 732, 4881, 26253, 82, 11, 577, 393, 321, 2360, 760, 4474, 11, 10910, 22195, 4474, 11, 597, 51306], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 980, "seek": 341416, "start": 3433.04, "end": 3438.12, "text": " when they're followed can generate novelty, can generate creativity, can", "tokens": [51308, 562, 436, 434, 6263, 393, 8460, 44805, 11, 393, 8460, 12915, 11, 393, 51562], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 981, "seek": 341416, "start": 3438.12, "end": 3443.2, "text": " generate organization in a way which is not predetermined, but almost fractal", "tokens": [51562, 8460, 4475, 294, 257, 636, 597, 307, 406, 3852, 35344, 2001, 11, 457, 1920, 17948, 304, 51816], "temperature": 0.0, "avg_logprob": -0.16713866446782083, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.002888324437662959}, {"id": 982, "seek": 344320, "start": 3443.2, "end": 3445.04, "text": " and infinite in its complexity.", "tokens": [50364, 293, 13785, 294, 1080, 14024, 13, 50456], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 983, "seek": 344320, "start": 3445.64, "end": 3448.7599999999998, "text": " And are those rules defined already?", "tokens": [50486, 400, 366, 729, 4474, 7642, 1217, 30, 50642], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 984, "seek": 344320, "start": 3448.7599999999998, "end": 3450.0, "text": " Do they exist in the world?", "tokens": [50642, 1144, 436, 2514, 294, 264, 1002, 30, 50704], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 985, "seek": 344320, "start": 3450.24, "end": 3451.3599999999997, "text": " Are we guided by them?", "tokens": [50716, 2014, 321, 19663, 538, 552, 30, 50772], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 986, "seek": 344320, "start": 3451.7999999999997, "end": 3454.24, "text": " Are there principles like that which exist that we can come to?", "tokens": [50794, 2014, 456, 9156, 411, 300, 597, 2514, 300, 321, 393, 808, 281, 30, 50916], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 987, "seek": 344320, "start": 3455.04, "end": 3459.08, "text": " Or is it, you know, are we kind of, you know, the authors of our own", "tokens": [50956, 1610, 307, 309, 11, 291, 458, 11, 366, 321, 733, 295, 11, 291, 458, 11, 264, 16552, 295, 527, 1065, 51158], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 988, "seek": 344320, "start": 3459.08, "end": 3459.9199999999996, "text": " fates in a sense?", "tokens": [51158, 283, 1024, 294, 257, 2020, 30, 51200], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 989, "seek": 344320, "start": 3459.9199999999996, "end": 3461.68, "text": " Are we each agents and actions?", "tokens": [51200, 2014, 321, 1184, 12554, 293, 5909, 30, 51288], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 990, "seek": 344320, "start": 3462.04, "end": 3463.68, "text": " Uh, we get to choose our path in life.", "tokens": [51306, 4019, 11, 321, 483, 281, 2826, 527, 3100, 294, 993, 13, 51388], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 991, "seek": 344320, "start": 3464.24, "end": 3467.3199999999997, "text": " I think these, these are the directions I'm really interested in.", "tokens": [51416, 286, 519, 613, 11, 613, 366, 264, 11095, 286, 478, 534, 3102, 294, 13, 51570], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 992, "seek": 344320, "start": 3467.52, "end": 3471.0, "text": " And to connect this to my research, one thing I'm focused on now for my thesis", "tokens": [51580, 400, 281, 1745, 341, 281, 452, 2132, 11, 472, 551, 286, 478, 5178, 322, 586, 337, 452, 22288, 51754], "temperature": 0.0, "avg_logprob": -0.11731228725515681, "compression_ratio": 1.6875, "no_speech_prob": 0.0005973337101750076}, {"id": 993, "seek": 347100, "start": 3471.0, "end": 3474.68, "text": " project, um, is looking at morphogenesis.", "tokens": [50364, 1716, 11, 1105, 11, 307, 1237, 412, 25778, 8799, 9374, 13, 50548], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 994, "seek": 347100, "start": 3474.8, "end": 3478.4, "text": " So this connects to the more defensive paper as well, except what I'm", "tokens": [50554, 407, 341, 16967, 281, 264, 544, 16468, 3035, 382, 731, 11, 3993, 437, 286, 478, 50734], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 995, "seek": 347100, "start": 3478.4, "end": 3480.48, "text": " really interested in is how does structure emerge?", "tokens": [50734, 534, 3102, 294, 307, 577, 775, 3877, 21511, 30, 50838], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 996, "seek": 347100, "start": 3480.84, "end": 3483.48, "text": " How do different cells actually connect together?", "tokens": [50856, 1012, 360, 819, 5438, 767, 1745, 1214, 30, 50988], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 997, "seek": 347100, "start": 3483.92, "end": 3488.56, "text": " So, um, in that paper, for instance, each of the cells were on a fixed grid,", "tokens": [51010, 407, 11, 1105, 11, 294, 300, 3035, 11, 337, 5197, 11, 1184, 295, 264, 5438, 645, 322, 257, 6806, 10748, 11, 51242], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 998, "seek": 347100, "start": 3488.6, "end": 3492.68, "text": " but in our bodies, uh, there's actually quite a sophisticated protein", "tokens": [51244, 457, 294, 527, 7510, 11, 2232, 11, 456, 311, 767, 1596, 257, 16950, 7944, 51448], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 999, "seek": 347100, "start": 3492.68, "end": 3496.2, "text": " expression network which governs how cells adhere together.", "tokens": [51448, 6114, 3209, 597, 1980, 82, 577, 5438, 33584, 1214, 13, 51624], "temperature": 0.0, "avg_logprob": -0.16256033457242525, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.011328750289976597}, {"id": 1000, "seek": 349620, "start": 3496.6, "end": 3501.56, "text": " Um, certain gene regulation pathways will turn on cat herons, which will", "tokens": [50384, 3301, 11, 1629, 12186, 15062, 22988, 486, 1261, 322, 3857, 720, 892, 11, 597, 486, 50632], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1001, "seek": 349620, "start": 3501.56, "end": 3503.7599999999998, "text": " cause cells to attach together.", "tokens": [50632, 3082, 5438, 281, 5085, 1214, 13, 50742], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1002, "seek": 349620, "start": 3503.7599999999998, "end": 3507.72, "text": " And then in other parts, um, these cells can unattach and then be", "tokens": [50742, 400, 550, 294, 661, 3166, 11, 1105, 11, 613, 5438, 393, 47316, 608, 293, 550, 312, 50940], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1003, "seek": 349620, "start": 3507.72, "end": 3510.08, "text": " transported all around the embryo.", "tokens": [50940, 29373, 439, 926, 264, 31588, 78, 13, 51058], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1004, "seek": 349620, "start": 3510.48, "end": 3514.4399999999996, "text": " And I think understanding this process more deeply, not only could shed", "tokens": [51078, 400, 286, 519, 3701, 341, 1399, 544, 8760, 11, 406, 787, 727, 14951, 51276], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1005, "seek": 349620, "start": 3514.4399999999996, "end": 3518.7599999999998, "text": " lights on structure formation and problems in biology in general, but", "tokens": [51276, 5811, 322, 3877, 11723, 293, 2740, 294, 14956, 294, 2674, 11, 457, 51492], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1006, "seek": 349620, "start": 3518.7599999999998, "end": 3523.4399999999996, "text": " maybe more deeper general problems of structure learning, because we might", "tokens": [51492, 1310, 544, 7731, 2674, 2740, 295, 3877, 2539, 11, 570, 321, 1062, 51726], "temperature": 0.0, "avg_logprob": -0.15696001544441143, "compression_ratio": 1.7366255144032923, "no_speech_prob": 0.003593055997043848}, {"id": 1007, "seek": 352344, "start": 3523.44, "end": 3527.2000000000003, "text": " think of embryology as quite disconnected from machine intelligence or", "tokens": [50364, 519, 295, 31588, 1793, 382, 1596, 29426, 490, 3479, 7599, 420, 50552], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1008, "seek": 352344, "start": 3527.2000000000003, "end": 3532.16, "text": " artificial intelligence, but every single brain is formed in the same way.", "tokens": [50552, 11677, 7599, 11, 457, 633, 2167, 3567, 307, 8693, 294, 264, 912, 636, 13, 50800], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1009, "seek": 352344, "start": 3532.56, "end": 3534.4, "text": " And that's through developments.", "tokens": [50820, 400, 300, 311, 807, 20862, 13, 50912], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1010, "seek": 352344, "start": 3535.0, "end": 3535.28, "text": " Yeah.", "tokens": [50942, 865, 13, 50956], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1011, "seek": 352344, "start": 3535.32, "end": 3537.64, "text": " Um, I'm also a disciple of Kenneth Stanley.", "tokens": [50958, 3301, 11, 286, 478, 611, 257, 32100, 295, 33735, 28329, 13, 51074], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1012, "seek": 352344, "start": 3537.64, "end": 3539.32, "text": " He's, he's absolutely incredible.", "tokens": [51074, 634, 311, 11, 415, 311, 3122, 4651, 13, 51158], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1013, "seek": 352344, "start": 3539.32, "end": 3540.88, "text": " Everyone at home needs to read his book.", "tokens": [51158, 5198, 412, 1280, 2203, 281, 1401, 702, 1446, 13, 51236], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1014, "seek": 352344, "start": 3540.88, "end": 3542.12, "text": " My greatness cannot be planned.", "tokens": [51236, 1222, 31196, 2644, 312, 8589, 13, 51298], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1015, "seek": 352344, "start": 3542.56, "end": 3543.92, "text": " Um, yeah.", "tokens": [51320, 3301, 11, 1338, 13, 51388], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1016, "seek": 352344, "start": 3543.92, "end": 3549.04, "text": " You know, so in, in the natural world, we have, um, it, it's so interesting.", "tokens": [51388, 509, 458, 11, 370, 294, 11, 294, 264, 3303, 1002, 11, 321, 362, 11, 1105, 11, 309, 11, 309, 311, 370, 1880, 13, 51644], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1017, "seek": 352344, "start": 3549.04, "end": 3552.96, "text": " So we have this kind of like self-organization and then we have multi-scale", "tokens": [51644, 407, 321, 362, 341, 733, 295, 411, 2698, 12, 12372, 2144, 293, 550, 321, 362, 4825, 12, 20033, 51840], "temperature": 0.0, "avg_logprob": -0.16727547084583955, "compression_ratio": 1.5709779179810726, "no_speech_prob": 0.004543334245681763}, {"id": 1018, "seek": 355296, "start": 3552.96, "end": 3558.08, "text": " information sharing, but we also have canalization, which is that, um, you", "tokens": [50364, 1589, 5414, 11, 457, 321, 611, 362, 9911, 2144, 11, 597, 307, 300, 11, 1105, 11, 291, 50620], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1019, "seek": 355296, "start": 3558.08, "end": 3562.16, "text": " actually see a kind of, um, convergence of, of structure and forms, you know,", "tokens": [50620, 767, 536, 257, 733, 295, 11, 1105, 11, 32181, 295, 11, 295, 3877, 293, 6422, 11, 291, 458, 11, 50824], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1020, "seek": 355296, "start": 3562.16, "end": 3565.7200000000003, "text": " which is reused, you know, almost as, as modules, um, in the system.", "tokens": [50824, 597, 307, 319, 4717, 11, 291, 458, 11, 1920, 382, 11, 382, 16679, 11, 1105, 11, 294, 264, 1185, 13, 51002], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1021, "seek": 355296, "start": 3565.96, "end": 3569.2400000000002, "text": " But then there's always the question of how do we create something like this?", "tokens": [51014, 583, 550, 456, 311, 1009, 264, 1168, 295, 577, 360, 321, 1884, 746, 411, 341, 30, 51178], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1022, "seek": 355296, "start": 3569.2400000000002, "end": 3571.2400000000002, "text": " Because is it simply a matter of complexity?", "tokens": [51178, 1436, 307, 309, 2935, 257, 1871, 295, 14024, 30, 51278], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1023, "seek": 355296, "start": 3571.2400000000002, "end": 3574.88, "text": " Do you need to have a microscopic scale to reproduce this?", "tokens": [51278, 1144, 291, 643, 281, 362, 257, 47897, 4373, 281, 29501, 341, 30, 51460], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1024, "seek": 355296, "start": 3575.08, "end": 3576.4, "text": " Or could we reproduce it?", "tokens": [51470, 1610, 727, 321, 29501, 309, 30, 51536], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1025, "seek": 355296, "start": 3576.64, "end": 3580.88, "text": " And then if we did reproduce it, the catch 22 situation is that, you know,", "tokens": [51548, 400, 550, 498, 321, 630, 29501, 309, 11, 264, 3745, 5853, 2590, 307, 300, 11, 291, 458, 11, 51760], "temperature": 0.0, "avg_logprob": -0.11433427598741319, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.0032354050781577826}, {"id": 1026, "seek": 358088, "start": 3580.92, "end": 3585.6800000000003, "text": " when you impute directedness onto a system, it loses its intelligence.", "tokens": [50366, 562, 291, 704, 1169, 12898, 1287, 3911, 257, 1185, 11, 309, 18293, 1080, 7599, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1027, "seek": 358088, "start": 3585.6800000000003, "end": 3588.44, "text": " Cause to me intelligence is divergence.", "tokens": [50604, 10865, 281, 385, 7599, 307, 47387, 13, 50742], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1028, "seek": 358088, "start": 3588.44, "end": 3593.56, "text": " It's exactly as you were saying, it's this tapestry of, um, discovering", "tokens": [50742, 467, 311, 2293, 382, 291, 645, 1566, 11, 309, 311, 341, 5119, 47433, 295, 11, 1105, 11, 24773, 50998], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1029, "seek": 358088, "start": 3593.56, "end": 3596.32, "text": " problems, solutions, new problems, solutions.", "tokens": [50998, 2740, 11, 6547, 11, 777, 2740, 11, 6547, 13, 51136], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1030, "seek": 358088, "start": 3596.32, "end": 3597.6, "text": " And it goes on and there's no end.", "tokens": [51136, 400, 309, 1709, 322, 293, 456, 311, 572, 917, 13, 51200], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1031, "seek": 358088, "start": 3597.6, "end": 3598.56, "text": " It goes on forever.", "tokens": [51200, 467, 1709, 322, 5680, 13, 51248], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1032, "seek": 358088, "start": 3598.92, "end": 3603.2000000000003, "text": " And any attempt by us to control it with, I mean, it's a bit like the", "tokens": [51266, 400, 604, 5217, 538, 505, 281, 1969, 309, 365, 11, 286, 914, 11, 309, 311, 257, 857, 411, 264, 51480], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1033, "seek": 358088, "start": 3603.2000000000003, "end": 3607.08, "text": " bitter lesson, you know, Sutton said, any human design, any attempt to", "tokens": [51480, 13871, 6898, 11, 291, 458, 11, 40492, 1756, 848, 11, 604, 1952, 1715, 11, 604, 5217, 281, 51674], "temperature": 0.0, "avg_logprob": -0.11179256439208984, "compression_ratio": 1.696, "no_speech_prob": 0.007059634663164616}, {"id": 1034, "seek": 360708, "start": 3607.08, "end": 3611.56, "text": " steer it makes it convergent, but then we could do something like the game", "tokens": [50364, 30814, 309, 1669, 309, 9652, 6930, 11, 457, 550, 321, 727, 360, 746, 411, 264, 1216, 50588], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1035, "seek": 360708, "start": 3611.56, "end": 3616.6, "text": " of life from John Conway and incredible, beautiful structure emerges from that.", "tokens": [50588, 295, 993, 490, 2619, 2656, 676, 293, 4651, 11, 2238, 3877, 38965, 490, 300, 13, 50840], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1036, "seek": 360708, "start": 3616.64, "end": 3623.0, "text": " But whenever we try to steer it with our own will, it seems to corrupt it as well.", "tokens": [50842, 583, 5699, 321, 853, 281, 30814, 309, 365, 527, 1065, 486, 11, 309, 2544, 281, 17366, 309, 382, 731, 13, 51160], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1037, "seek": 360708, "start": 3624.44, "end": 3624.84, "text": " Yeah.", "tokens": [51232, 865, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1038, "seek": 360708, "start": 3625.04, "end": 3628.52, "text": " I think that the analogy to biology is really useful here and the", "tokens": [51262, 286, 519, 300, 264, 21663, 281, 14956, 307, 534, 4420, 510, 293, 264, 51436], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1039, "seek": 360708, "start": 3628.52, "end": 3631.7599999999998, "text": " canonization that you mentioned, you know, you have this reuse of structures", "tokens": [51436, 21985, 2144, 300, 291, 2835, 11, 291, 458, 11, 291, 362, 341, 26225, 295, 9227, 51598], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1040, "seek": 360708, "start": 3631.7599999999998, "end": 3635.84, "text": " across, you know, cells, for instance, they all have this similar machinery to", "tokens": [51598, 2108, 11, 291, 458, 11, 5438, 11, 337, 5197, 11, 436, 439, 362, 341, 2531, 27302, 281, 51802], "temperature": 0.0, "avg_logprob": -0.13496152141637968, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.0031717573292553425}, {"id": 1041, "seek": 363584, "start": 3635.88, "end": 3639.08, "text": " do gene expression and they have the same genetic code underlying that", "tokens": [50366, 360, 12186, 6114, 293, 436, 362, 264, 912, 12462, 3089, 14217, 300, 50526], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1042, "seek": 363584, "start": 3639.08, "end": 3642.1600000000003, "text": " gene expression with, you know, maybe differences in cell state, but at the", "tokens": [50526, 12186, 6114, 365, 11, 291, 458, 11, 1310, 7300, 294, 2815, 1785, 11, 457, 412, 264, 50680], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1043, "seek": 363584, "start": 3642.1600000000003, "end": 3643.7200000000003, "text": " end of the day, it's the same machinery, right?", "tokens": [50680, 917, 295, 264, 786, 11, 309, 311, 264, 912, 27302, 11, 558, 30, 50758], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1044, "seek": 363584, "start": 3644.0, "end": 3646.8, "text": " And, you know, I used to do a bit of protein engineering with language", "tokens": [50772, 400, 11, 291, 458, 11, 286, 1143, 281, 360, 257, 857, 295, 7944, 7043, 365, 2856, 50912], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1045, "seek": 363584, "start": 3646.8, "end": 3649.56, "text": " models, and that's how actually how I learned about, uh, transformers and", "tokens": [50912, 5245, 11, 293, 300, 311, 577, 767, 577, 286, 3264, 466, 11, 2232, 11, 4088, 433, 293, 51050], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1046, "seek": 363584, "start": 3649.56, "end": 3650.76, "text": " built my first transformers.", "tokens": [51050, 3094, 452, 700, 4088, 433, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1047, "seek": 363584, "start": 3650.76, "end": 3654.84, "text": " And I think that the analogy is really strong where, you know, cells sort of", "tokens": [51110, 400, 286, 519, 300, 264, 21663, 307, 534, 2068, 689, 11, 291, 458, 11, 5438, 1333, 295, 51314], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1048, "seek": 363584, "start": 3654.84, "end": 3658.76, "text": " know how to read this genetic code, this language of the genetic code.", "tokens": [51314, 458, 577, 281, 1401, 341, 12462, 3089, 11, 341, 2856, 295, 264, 12462, 3089, 13, 51510], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1049, "seek": 363584, "start": 3658.96, "end": 3662.6400000000003, "text": " And they all use that ability, this canalized ability that's distributed", "tokens": [51520, 400, 436, 439, 764, 300, 3485, 11, 341, 9911, 1602, 3485, 300, 311, 12631, 51704], "temperature": 0.0, "avg_logprob": -0.12272351408657962, "compression_ratio": 1.9, "no_speech_prob": 0.009409690275788307}, {"id": 1050, "seek": 366264, "start": 3662.64, "end": 3666.64, "text": " across all of them to locally they solve this problem of, okay, what is this", "tokens": [50364, 2108, 439, 295, 552, 281, 16143, 436, 5039, 341, 1154, 295, 11, 1392, 11, 437, 307, 341, 50564], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1051, "seek": 366264, "start": 3666.64, "end": 3667.96, "text": " specific cell supposed to do?", "tokens": [50564, 2685, 2815, 3442, 281, 360, 30, 50630], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1052, "seek": 366264, "start": 3667.96, "end": 3671.7999999999997, "text": " What should it do to basically support the overall function of the organism?", "tokens": [50630, 708, 820, 309, 360, 281, 1936, 1406, 264, 4787, 2445, 295, 264, 24128, 30, 50822], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1053, "seek": 366264, "start": 3671.7999999999997, "end": 3672.12, "text": " Right.", "tokens": [50822, 1779, 13, 50838], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1054, "seek": 366264, "start": 3672.4, "end": 3676.2799999999997, "text": " And similarly, I think the hope with these language models is that now we have", "tokens": [50852, 400, 14138, 11, 286, 519, 264, 1454, 365, 613, 2856, 5245, 307, 300, 586, 321, 362, 51046], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1055, "seek": 366264, "start": 3676.2799999999997, "end": 3681.48, "text": " these language based models or LLMs that have this similar sort of", "tokens": [51046, 613, 2856, 2361, 5245, 420, 441, 43, 26386, 300, 362, 341, 2531, 1333, 295, 51306], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1056, "seek": 366264, "start": 3681.52, "end": 3683.44, "text": " understanding of language.", "tokens": [51308, 3701, 295, 2856, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1057, "seek": 366264, "start": 3683.48, "end": 3687.12, "text": " They are able to really constrain the probability distribution, understand", "tokens": [51406, 814, 366, 1075, 281, 534, 1817, 7146, 264, 8482, 7316, 11, 1223, 51588], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1058, "seek": 366264, "start": 3687.24, "end": 3690.48, "text": " which sequences of text are reasonable English and, you know, what they might", "tokens": [51594, 597, 22978, 295, 2487, 366, 10585, 3669, 293, 11, 291, 458, 11, 437, 436, 1062, 51756], "temperature": 0.0, "avg_logprob": -0.1453646977742513, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.012818565592169762}, {"id": 1059, "seek": 369048, "start": 3690.6, "end": 3691.44, "text": " want to generate.", "tokens": [50370, 528, 281, 8460, 13, 50412], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1060, "seek": 369048, "start": 3691.68, "end": 3695.44, "text": " And the exciting thing to me is that we can kind of do a similar sort of", "tokens": [50424, 400, 264, 4670, 551, 281, 385, 307, 300, 321, 393, 733, 295, 360, 257, 2531, 1333, 295, 50612], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1061, "seek": 369048, "start": 3695.44, "end": 3699.36, "text": " evolutionary search that we used to do with, or that we currently do with, uh,", "tokens": [50612, 27567, 3164, 300, 321, 1143, 281, 360, 365, 11, 420, 300, 321, 4362, 360, 365, 11, 2232, 11, 50808], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1062, "seek": 369048, "start": 3699.4, "end": 3702.56, "text": " trying to find protein sequences, uh, when we're doing protein engineering with", "tokens": [50810, 1382, 281, 915, 7944, 22978, 11, 2232, 11, 562, 321, 434, 884, 7944, 7043, 365, 50968], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1063, "seek": 369048, "start": 3702.56, "end": 3707.2, "text": " the language models, where every computer in this network of systems has this", "tokens": [50968, 264, 2856, 5245, 11, 689, 633, 3820, 294, 341, 3209, 295, 3652, 575, 341, 51200], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1064, "seek": 369048, "start": 3707.2, "end": 3711.76, "text": " canalized ability to understand language, if you will, and is locally, it just needs", "tokens": [51200, 9911, 1602, 3485, 281, 1223, 2856, 11, 498, 291, 486, 11, 293, 307, 16143, 11, 309, 445, 2203, 51428], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1065, "seek": 369048, "start": 3711.76, "end": 3715.52, "text": " to solve this problem of what should this particular node do to support the", "tokens": [51428, 281, 5039, 341, 1154, 295, 437, 820, 341, 1729, 9984, 360, 281, 1406, 264, 51616], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1066, "seek": 369048, "start": 3715.52, "end": 3716.28, "text": " function of the system?", "tokens": [51616, 2445, 295, 264, 1185, 30, 51654], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1067, "seek": 369048, "start": 3716.28, "end": 3719.12, "text": " And that might be to explore, that might be to exploit, that might be to do any", "tokens": [51654, 400, 300, 1062, 312, 281, 6839, 11, 300, 1062, 312, 281, 25924, 11, 300, 1062, 312, 281, 360, 604, 51796], "temperature": 0.0, "avg_logprob": -0.11815754572550456, "compression_ratio": 1.85, "no_speech_prob": 0.011329159140586853}, {"id": 1068, "seek": 371912, "start": 3719.12, "end": 3720.68, "text": " sort of, any number of things.", "tokens": [50364, 1333, 295, 11, 604, 1230, 295, 721, 13, 50442], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1069, "seek": 371912, "start": 3720.96, "end": 3724.44, "text": " And the discovery of that, I think, is really helped by the fact that we do", "tokens": [50456, 400, 264, 12114, 295, 300, 11, 286, 519, 11, 307, 534, 4254, 538, 264, 1186, 300, 321, 360, 50630], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1070, "seek": 371912, "start": 3724.44, "end": 3728.16, "text": " have strong language models that are able to really predict English or text", "tokens": [50630, 362, 2068, 2856, 5245, 300, 366, 1075, 281, 534, 6069, 3669, 420, 2487, 50816], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1071, "seek": 371912, "start": 3728.16, "end": 3732.04, "text": " very well, uh, because they're able to explore this space.", "tokens": [50816, 588, 731, 11, 2232, 11, 570, 436, 434, 1075, 281, 6839, 341, 1901, 13, 51010], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1072, "seek": 371912, "start": 3732.04, "end": 3735.64, "text": " And basically in the limit, you know, there's this good regulator theorem that", "tokens": [51010, 400, 1936, 294, 264, 4948, 11, 291, 458, 11, 456, 311, 341, 665, 36250, 20904, 300, 51190], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1073, "seek": 371912, "start": 3735.64, "end": 3740.3199999999997, "text": " we had talked about before that says that any system that is a X that does", "tokens": [51190, 321, 632, 2825, 466, 949, 300, 1619, 300, 604, 1185, 300, 307, 257, 1783, 300, 775, 51424], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1074, "seek": 371912, "start": 3740.3599999999997, "end": 3744.04, "text": " optimal control over another system must necessarily model that system.", "tokens": [51426, 16252, 1969, 670, 1071, 1185, 1633, 4725, 2316, 300, 1185, 13, 51610], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1075, "seek": 371912, "start": 3744.3599999999997, "end": 3748.2799999999997, "text": " Uh, and so if you think about in the limit, it seems like the best prompt", "tokens": [51626, 4019, 11, 293, 370, 498, 291, 519, 466, 294, 264, 4948, 11, 309, 2544, 411, 264, 1151, 12391, 51822], "temperature": 0.0, "avg_logprob": -0.11036709583166873, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.004197605419903994}, {"id": 1076, "seek": 374828, "start": 3748.32, "end": 3750.84, "text": " optimizers may end up being language models.", "tokens": [50366, 5028, 22525, 815, 917, 493, 885, 2856, 5245, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1077, "seek": 374828, "start": 3750.84, "end": 3754.28, "text": " And already in our study, we were using this GCG algorithm that leverages a", "tokens": [50492, 400, 1217, 294, 527, 2979, 11, 321, 645, 1228, 341, 29435, 38, 9284, 300, 12451, 1660, 257, 50664], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1078, "seek": 374828, "start": 3754.28, "end": 3757.8, "text": " language model to compute these gradients and try to figure out how we should", "tokens": [50664, 2856, 2316, 281, 14722, 613, 2771, 2448, 293, 853, 281, 2573, 484, 577, 321, 820, 50840], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1079, "seek": 374828, "start": 3757.8, "end": 3760.28, "text": " do this local stochastic search over prompts.", "tokens": [50840, 360, 341, 2654, 342, 8997, 2750, 3164, 670, 41095, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1080, "seek": 374828, "start": 3760.52, "end": 3764.2400000000002, "text": " And so what I basically, I'm trying to get at is that there are actually a lot", "tokens": [50976, 400, 370, 437, 286, 1936, 11, 286, 478, 1382, 281, 483, 412, 307, 300, 456, 366, 767, 257, 688, 51162], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1081, "seek": 374828, "start": 3764.2400000000002, "end": 3768.0, "text": " of really interesting similarities, I think, that can be drawn upon from what", "tokens": [51162, 295, 534, 1880, 24197, 11, 286, 519, 11, 300, 393, 312, 10117, 3564, 490, 437, 51350], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1082, "seek": 374828, "start": 3768.0, "end": 3771.84, "text": " we know about the structure and the function of biological systems where,", "tokens": [51350, 321, 458, 466, 264, 3877, 293, 264, 2445, 295, 13910, 3652, 689, 11, 51542], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1083, "seek": 374828, "start": 3772.0, "end": 3775.4, "text": " you know, if we could crack this problem of there's this local control", "tokens": [51550, 291, 458, 11, 498, 321, 727, 6226, 341, 1154, 295, 456, 311, 341, 2654, 1969, 51720], "temperature": 0.0, "avg_logprob": -0.1098968636898594, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006096561439335346}, {"id": 1084, "seek": 377540, "start": 3775.4, "end": 3778.96, "text": " objective or maybe information processing objective that must be met by every", "tokens": [50364, 10024, 420, 1310, 1589, 9007, 10024, 300, 1633, 312, 1131, 538, 633, 50542], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1085, "seek": 377540, "start": 3778.96, "end": 3779.52, "text": " cell, right?", "tokens": [50542, 2815, 11, 558, 30, 50570], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1086, "seek": 377540, "start": 3779.52, "end": 3783.48, "text": " Every compute node in this network of language models, if we could understand", "tokens": [50570, 2048, 14722, 9984, 294, 341, 3209, 295, 2856, 5245, 11, 498, 321, 727, 1223, 50768], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1087, "seek": 377540, "start": 3783.48, "end": 3787.1600000000003, "text": " what that is, what that even means from the perspective of systems and control", "tokens": [50768, 437, 300, 307, 11, 437, 300, 754, 1355, 490, 264, 4585, 295, 3652, 293, 1969, 50952], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1088, "seek": 377540, "start": 3787.1600000000003, "end": 3791.2000000000003, "text": " and, you know, computation and like, I think that that's a really promising", "tokens": [50952, 293, 11, 291, 458, 11, 24903, 293, 411, 11, 286, 519, 300, 300, 311, 257, 534, 20257, 51154], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1089, "seek": 377540, "start": 3791.2000000000003, "end": 3795.08, "text": " way that we can make progress on this dream of like, to me, it seems like it", "tokens": [51154, 636, 300, 321, 393, 652, 4205, 322, 341, 3055, 295, 411, 11, 281, 385, 11, 309, 2544, 411, 309, 51348], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1090, "seek": 377540, "start": 3795.08, "end": 3798.96, "text": " would be great to have GPT seven, not just owned by one entity, but maybe", "tokens": [51348, 576, 312, 869, 281, 362, 26039, 51, 3407, 11, 406, 445, 11684, 538, 472, 13977, 11, 457, 1310, 51542], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1091, "seek": 377540, "start": 3798.96, "end": 3802.2000000000003, "text": " operated by the world where we could all have a say in what goes into it and", "tokens": [51542, 20826, 538, 264, 1002, 689, 321, 727, 439, 362, 257, 584, 294, 437, 1709, 666, 309, 293, 51704], "temperature": 0.0, "avg_logprob": -0.10465992742509984, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.04334514960646629}, {"id": 1092, "seek": 380220, "start": 3802.2, "end": 3805.56, "text": " how it's used and what it should be, you know, doing and can all benefit", "tokens": [50364, 577, 309, 311, 1143, 293, 437, 309, 820, 312, 11, 291, 458, 11, 884, 293, 393, 439, 5121, 50532], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1093, "seek": 380220, "start": 3805.56, "end": 3809.16, "text": " from its excellent ability to compute and predict what will happen next and", "tokens": [50532, 490, 1080, 7103, 3485, 281, 14722, 293, 6069, 437, 486, 1051, 958, 293, 50712], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1094, "seek": 380220, "start": 3809.16, "end": 3812.4399999999996, "text": " basically perform intelligent, you know, operations on data.", "tokens": [50712, 1936, 2042, 13232, 11, 291, 458, 11, 7705, 322, 1412, 13, 50876], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1095, "seek": 380220, "start": 3812.72, "end": 3816.4399999999996, "text": " So yeah, I think this is a really, really exciting area to be working on.", "tokens": [50890, 407, 1338, 11, 286, 519, 341, 307, 257, 534, 11, 534, 4670, 1859, 281, 312, 1364, 322, 13, 51076], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1096, "seek": 380220, "start": 3816.68, "end": 3817.16, "text": " Amazing.", "tokens": [51088, 14165, 13, 51112], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1097, "seek": 380220, "start": 3817.52, "end": 3820.2, "text": " We're nearly at time, but we'll do two quick five questions.", "tokens": [51130, 492, 434, 6217, 412, 565, 11, 457, 321, 603, 360, 732, 1702, 1732, 1651, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1098, "seek": 380220, "start": 3820.2, "end": 3824.48, "text": " So you've both just started the Society for the Pursuit of AGI.", "tokens": [51264, 407, 291, 600, 1293, 445, 1409, 264, 13742, 337, 264, 430, 2156, 1983, 295, 316, 26252, 13, 51478], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1099, "seek": 380220, "start": 3824.52, "end": 3824.8399999999997, "text": " Yes.", "tokens": [51480, 1079, 13, 51496], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1100, "seek": 380220, "start": 3824.8399999999997, "end": 3825.64, "text": " Can you tell us about that?", "tokens": [51496, 1664, 291, 980, 505, 466, 300, 30, 51536], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1101, "seek": 380220, "start": 3825.8799999999997, "end": 3826.64, "text": " Absolutely.", "tokens": [51548, 7021, 13, 51586], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1102, "seek": 380220, "start": 3826.9199999999996, "end": 3831.08, "text": " So the Society for the Pursuit of AGI is a student organization.", "tokens": [51600, 407, 264, 13742, 337, 264, 430, 2156, 1983, 295, 316, 26252, 307, 257, 3107, 4475, 13, 51808], "temperature": 0.0, "avg_logprob": -0.13285739504057786, "compression_ratio": 1.6572327044025157, "no_speech_prob": 0.007889961823821068}, {"id": 1103, "seek": 383108, "start": 3831.08, "end": 3834.3199999999997, "text": " Currently we're operating at the University of Toronto and at Caltech.", "tokens": [50364, 19964, 321, 434, 7447, 412, 264, 3535, 295, 14140, 293, 412, 3511, 25970, 13, 50526], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1104, "seek": 383108, "start": 3834.84, "end": 3838.92, "text": " And we're essentially a crucible for new ideas.", "tokens": [50552, 400, 321, 434, 4476, 257, 5140, 32128, 337, 777, 3487, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1105, "seek": 383108, "start": 3839.88, "end": 3844.4, "text": " If you think of university research labs as pursuing relatively safe", "tokens": [50804, 759, 291, 519, 295, 5454, 2132, 20339, 382, 20222, 7226, 3273, 51030], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1106, "seek": 383108, "start": 3844.48, "end": 3849.08, "text": " bets that could be publishable, industry research labs, relatively safe", "tokens": [51034, 39922, 300, 727, 312, 11374, 712, 11, 3518, 2132, 20339, 11, 7226, 3273, 51264], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1107, "seek": 383108, "start": 3849.08, "end": 3852.88, "text": " bets that maybe might turn a profit one day in some new product or system.", "tokens": [51264, 39922, 300, 1310, 1062, 1261, 257, 7475, 472, 786, 294, 512, 777, 1674, 420, 1185, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1108, "seek": 383108, "start": 3853.6, "end": 3859.6, "text": " The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for", "tokens": [51490, 440, 13742, 307, 337, 264, 32495, 2039, 749, 11, 337, 264, 4868, 39922, 11, 337, 264, 3219, 1507, 11, 337, 51790], "temperature": 0.0, "avg_logprob": -0.1449983095881915, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.005058088339865208}, {"id": 1109, "seek": 385960, "start": 3859.6, "end": 3863.44, "text": " the real innovative stuff that's way outside the, you know, to use the", "tokens": [50364, 264, 957, 12999, 1507, 300, 311, 636, 2380, 264, 11, 291, 458, 11, 281, 764, 264, 50556], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1110, "seek": 385960, "start": 3863.44, "end": 3867.16, "text": " analogy of the highway network, we're trying to go off the beaten path.", "tokens": [50556, 21663, 295, 264, 17205, 3209, 11, 321, 434, 1382, 281, 352, 766, 264, 17909, 3100, 13, 50742], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1111, "seek": 385960, "start": 3867.68, "end": 3873.48, "text": " And we really believe that the bottleneck in AI progress right now is not so", "tokens": [50768, 400, 321, 534, 1697, 300, 264, 44641, 547, 294, 7318, 4205, 558, 586, 307, 406, 370, 51058], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1112, "seek": 385960, "start": 3873.48, "end": 3876.6, "text": " much compute, not so much algorithms, but it's conceptual.", "tokens": [51058, 709, 14722, 11, 406, 370, 709, 14642, 11, 457, 309, 311, 24106, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1113, "seek": 385960, "start": 3876.92, "end": 3881.72, "text": " We need better ideas about intelligence, about life, about what this whole thing", "tokens": [51230, 492, 643, 1101, 3487, 466, 7599, 11, 466, 993, 11, 466, 437, 341, 1379, 551, 51470], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1114, "seek": 385960, "start": 3881.72, "end": 3885.16, "text": " is that we're all experiencing and how we can gain deeper insights of it.", "tokens": [51470, 307, 300, 321, 434, 439, 11139, 293, 577, 321, 393, 6052, 7731, 14310, 295, 309, 13, 51642], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1115, "seek": 385960, "start": 3885.6, "end": 3889.2799999999997, "text": " Not only do I think that a deeper understanding will help us to create", "tokens": [51664, 1726, 787, 360, 286, 519, 300, 257, 7731, 3701, 486, 854, 505, 281, 1884, 51848], "temperature": 0.0, "avg_logprob": -0.09230852127075195, "compression_ratio": 1.68, "no_speech_prob": 0.0006457283161580563}, {"id": 1116, "seek": 388928, "start": 3889.28, "end": 3892.84, "text": " better systems, but it'll also give us confidence that the systems we're", "tokens": [50364, 1101, 3652, 11, 457, 309, 603, 611, 976, 505, 6687, 300, 264, 3652, 321, 434, 50542], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1117, "seek": 388928, "start": 3892.84, "end": 3896.52, "text": " developing will be beneficial to humanity and not harmful.", "tokens": [50542, 6416, 486, 312, 14072, 281, 10243, 293, 406, 19727, 13, 50726], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1118, "seek": 388928, "start": 3896.96, "end": 3900.6000000000004, "text": " And I think that will only come with knowledge, with first principles,", "tokens": [50748, 400, 286, 519, 300, 486, 787, 808, 365, 3601, 11, 365, 700, 9156, 11, 50930], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1119, "seek": 388928, "start": 3900.84, "end": 3901.52, "text": " understanding.", "tokens": [50942, 3701, 13, 50976], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1120, "seek": 388928, "start": 3902.0, "end": 3905.2400000000002, "text": " And so that's why one of the things we're trying to do is have our", "tokens": [51000, 400, 370, 300, 311, 983, 472, 295, 264, 721, 321, 434, 1382, 281, 360, 307, 362, 527, 51162], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1121, "seek": 388928, "start": 3905.2400000000002, "end": 3906.6800000000003, "text": " club very interdisciplinary.", "tokens": [51162, 6482, 588, 38280, 13, 51234], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1122, "seek": 388928, "start": 3907.1600000000003, "end": 3912.0, "text": " I think having machine learning be some, this kind of echo chamber amongst", "tokens": [51258, 286, 519, 1419, 3479, 2539, 312, 512, 11, 341, 733, 295, 14300, 13610, 12918, 51500], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1123, "seek": 388928, "start": 3912.0800000000004, "end": 3915.8, "text": " engineers, computer scientists, maybe a dash of, you know, philosophy and", "tokens": [51504, 11955, 11, 3820, 7708, 11, 1310, 257, 8240, 295, 11, 291, 458, 11, 10675, 293, 51690], "temperature": 0.0, "avg_logprob": -0.13715430793412234, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.002628311049193144}, {"id": 1124, "seek": 391580, "start": 3915.8, "end": 3919.6800000000003, "text": " neuroscience, it'd be really nice to open the conversation to people in other", "tokens": [50364, 42762, 11, 309, 1116, 312, 534, 1481, 281, 1269, 264, 3761, 281, 561, 294, 661, 50558], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1125, "seek": 391580, "start": 3919.6800000000003, "end": 3923.52, "text": " fields who maybe have a really unique insights into the phenomenon of", "tokens": [50558, 7909, 567, 1310, 362, 257, 534, 3845, 14310, 666, 264, 14029, 295, 50750], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1126, "seek": 391580, "start": 3923.52, "end": 3927.48, "text": " intelligence, perhaps behavioral economics can offer some insights.", "tokens": [50750, 7599, 11, 4317, 19124, 14564, 393, 2626, 512, 14310, 13, 50948], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1127, "seek": 391580, "start": 3927.7200000000003, "end": 3929.36, "text": " Political science, right?", "tokens": [50960, 34265, 3497, 11, 558, 30, 51042], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1128, "seek": 391580, "start": 3929.36, "end": 3934.6800000000003, "text": " These are fields that are currently underappreciated, but may have useful ideas.", "tokens": [51042, 1981, 366, 7909, 300, 366, 4362, 833, 1746, 3326, 770, 11, 457, 815, 362, 4420, 3487, 13, 51308], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1129, "seek": 391580, "start": 3935.1600000000003, "end": 3940.0800000000004, "text": " And maybe even people in the arts who, you know, creates, maybe they don't", "tokens": [51332, 400, 1310, 754, 561, 294, 264, 8609, 567, 11, 291, 458, 11, 7829, 11, 1310, 436, 500, 380, 51578], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1130, "seek": 391580, "start": 3940.0800000000004, "end": 3943.48, "text": " design systems as much as they re-represent things that we know and", "tokens": [51578, 1715, 3652, 382, 709, 382, 436, 319, 12, 19919, 11662, 721, 300, 321, 458, 293, 51748], "temperature": 0.0, "avg_logprob": -0.12335027588738336, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.013625162653625011}, {"id": 1131, "seek": 394348, "start": 3943.48, "end": 3947.08, "text": " understand, they could have an interesting voice as well.", "tokens": [50364, 1223, 11, 436, 727, 362, 364, 1880, 3177, 382, 731, 13, 50544], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1132, "seek": 394348, "start": 3947.72, "end": 3948.2400000000002, "text": " Very cool.", "tokens": [50576, 4372, 1627, 13, 50602], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1133, "seek": 394348, "start": 3948.28, "end": 3949.28, "text": " And final question.", "tokens": [50604, 400, 2572, 1168, 13, 50654], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1134, "seek": 394348, "start": 3949.28, "end": 3951.52, "text": " I mean, first of all, I just wanted to say to both of you, thank you for", "tokens": [50654, 286, 914, 11, 700, 295, 439, 11, 286, 445, 1415, 281, 584, 281, 1293, 295, 291, 11, 1309, 291, 337, 50766], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1135, "seek": 394348, "start": 3951.52, "end": 3952.6, "text": " doing this great work.", "tokens": [50766, 884, 341, 869, 589, 13, 50820], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1136, "seek": 394348, "start": 3952.64, "end": 3955.6, "text": " So your paper is one of the most interesting that I've seen in the", "tokens": [50822, 407, 428, 3035, 307, 472, 295, 264, 881, 1880, 300, 286, 600, 1612, 294, 264, 50970], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1137, "seek": 394348, "start": 3955.6, "end": 3957.36, "text": " LLM space in recent history.", "tokens": [50970, 441, 43, 44, 1901, 294, 5162, 2503, 13, 51058], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1138, "seek": 394348, "start": 3957.36, "end": 3961.8, "text": " And it was shared and loved by many of the folks on our Discord server.", "tokens": [51058, 400, 309, 390, 5507, 293, 4333, 538, 867, 295, 264, 4024, 322, 527, 32623, 7154, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1139, "seek": 394348, "start": 3962.2400000000002, "end": 3968.8, "text": " But that does bring me to another point, which is that you didn't get into", "tokens": [51302, 583, 300, 775, 1565, 385, 281, 1071, 935, 11, 597, 307, 300, 291, 994, 380, 483, 666, 51630], "temperature": 0.0, "avg_logprob": -0.12946398117963007, "compression_ratio": 1.5932835820895523, "no_speech_prob": 0.007054093759506941}, {"id": 1140, "seek": 396880, "start": 3968.8, "end": 3976.1200000000003, "text": " ICLR and from my perspective, I'm, I'm shocked because this is really, really", "tokens": [50364, 14360, 31722, 293, 490, 452, 4585, 11, 286, 478, 11, 286, 478, 12763, 570, 341, 307, 534, 11, 534, 50730], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1141, "seek": 396880, "start": 3976.1200000000003, "end": 3976.52, "text": " interesting.", "tokens": [50730, 1880, 13, 50750], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1142, "seek": 396880, "start": 3976.52, "end": 3981.2400000000002, "text": " It has great utility from a practical and a theoretical perspective.", "tokens": [50750, 467, 575, 869, 14877, 490, 257, 8496, 293, 257, 20864, 4585, 13, 50986], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1143, "seek": 396880, "start": 3982.48, "end": 3985.2400000000002, "text": " Feel free to have a, you know, a good bitch about reviewer number two.", "tokens": [51048, 14113, 1737, 281, 362, 257, 11, 291, 458, 11, 257, 665, 11960, 466, 3131, 260, 1230, 732, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1144, "seek": 396880, "start": 3985.84, "end": 3987.7200000000003, "text": " No, I mean, I wish you'd just keep talking like that.", "tokens": [51216, 883, 11, 286, 914, 11, 286, 3172, 291, 1116, 445, 1066, 1417, 411, 300, 13, 51310], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1145, "seek": 396880, "start": 3987.7200000000003, "end": 3990.4, "text": " It really soothes the burn of reviewer number two, you know?", "tokens": [51310, 467, 534, 370, 4624, 264, 5064, 295, 3131, 260, 1230, 732, 11, 291, 458, 30, 51444], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1146, "seek": 396880, "start": 3990.84, "end": 3994.2400000000002, "text": " But no, I think that, um, yeah, the review system, to be honest, I'm", "tokens": [51466, 583, 572, 11, 286, 519, 300, 11, 1105, 11, 1338, 11, 264, 3131, 1185, 11, 281, 312, 3245, 11, 286, 478, 51636], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1147, "seek": 396880, "start": 3994.2400000000002, "end": 3995.4, "text": " still trying to get my head around it.", "tokens": [51636, 920, 1382, 281, 483, 452, 1378, 926, 309, 13, 51694], "temperature": 0.0, "avg_logprob": -0.12605859327685925, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.03672026842832565}, {"id": 1148, "seek": 399540, "start": 3995.4, "end": 3999.28, "text": " I'm sort of an early career, you know, researcher, uh, trying to learn how it", "tokens": [50364, 286, 478, 1333, 295, 364, 2440, 3988, 11, 291, 458, 11, 21751, 11, 2232, 11, 1382, 281, 1466, 577, 309, 50558], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1149, "seek": 399540, "start": 3999.28, "end": 3999.6800000000003, "text": " works.", "tokens": [50558, 1985, 13, 50578], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1150, "seek": 399540, "start": 3999.6800000000003, "end": 4005.1600000000003, "text": " I mean, definitely the, the review process in, for ICLR, in their defense, you", "tokens": [50578, 286, 914, 11, 2138, 264, 11, 264, 3131, 1399, 294, 11, 337, 14360, 31722, 11, 294, 641, 7654, 11, 291, 50852], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1151, "seek": 399540, "start": 4005.1600000000003, "end": 4010.48, "text": " know, we had this bug with the submission, uh, submission of our rebuttals basically.", "tokens": [50852, 458, 11, 321, 632, 341, 7426, 365, 264, 23689, 11, 2232, 11, 23689, 295, 527, 319, 5955, 83, 1124, 1936, 13, 51118], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1152, "seek": 399540, "start": 4010.48, "end": 4013.76, "text": " So we had submitted the revision to our paper and then 15 minutes before the", "tokens": [51118, 407, 321, 632, 14405, 264, 34218, 281, 527, 3035, 293, 550, 2119, 2077, 949, 264, 51282], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1153, "seek": 399540, "start": 4013.76, "end": 4016.12, "text": " deadline, Cameron and I were both getting this timed out error.", "tokens": [51282, 20615, 11, 24962, 293, 286, 645, 1293, 1242, 341, 44696, 484, 6713, 13, 51400], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1154, "seek": 399540, "start": 4016.4, "end": 4017.28, "text": " Uh, he was in Toronto.", "tokens": [51414, 4019, 11, 415, 390, 294, 14140, 13, 51458], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1155, "seek": 399540, "start": 4017.28, "end": 4022.0, "text": " I was in, uh, California and so, you know, they didn't end up actually reading", "tokens": [51458, 286, 390, 294, 11, 2232, 11, 5384, 293, 370, 11, 291, 458, 11, 436, 994, 380, 917, 493, 767, 3760, 51694], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1156, "seek": 399540, "start": 4022.0, "end": 4024.8, "text": " our rebuttals because we had sent it in and they were like, Oh, we'll post it for", "tokens": [51694, 527, 319, 5955, 83, 1124, 570, 321, 632, 2279, 309, 294, 293, 436, 645, 411, 11, 876, 11, 321, 603, 2183, 309, 337, 51834], "temperature": 0.0, "avg_logprob": -0.126431577694342, "compression_ratio": 1.7393939393939395, "no_speech_prob": 0.002322829095646739}, {"id": 1157, "seek": 402480, "start": 4024.8, "end": 4027.7200000000003, "text": " you and then they were like, Oh, it was posted late, so can't read that.", "tokens": [50364, 291, 293, 550, 436, 645, 411, 11, 876, 11, 309, 390, 9437, 3469, 11, 370, 393, 380, 1401, 300, 13, 50510], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1158, "seek": 402480, "start": 4028.04, "end": 4032.1200000000003, "text": " Um, so yeah, I think that the review process definitely has given us a lot of,", "tokens": [50526, 3301, 11, 370, 1338, 11, 286, 519, 300, 264, 3131, 1399, 2138, 575, 2212, 505, 257, 688, 295, 11, 50730], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1159, "seek": 402480, "start": 4032.1600000000003, "end": 4035.32, "text": " you know, really useful insights where, you know, the second two results actually", "tokens": [50732, 291, 458, 11, 534, 4420, 14310, 689, 11, 291, 458, 11, 264, 1150, 732, 3542, 767, 50890], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1160, "seek": 402480, "start": 4035.32, "end": 4039.0, "text": " that we talked about, the top 75 controllability and the random controllability.", "tokens": [50890, 300, 321, 2825, 466, 11, 264, 1192, 9562, 45159, 2310, 293, 264, 4974, 45159, 2310, 13, 51074], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1161, "seek": 402480, "start": 4039.1600000000003, "end": 4042.76, "text": " Both of those were like from trying to address these reviewer comments, right?", "tokens": [51082, 6767, 295, 729, 645, 411, 490, 1382, 281, 2985, 613, 3131, 260, 3053, 11, 558, 30, 51262], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1162, "seek": 402480, "start": 4043.0, "end": 4047.0800000000004, "text": " So I think that what I'm trying to do at least is take as much of the good", "tokens": [51274, 407, 286, 519, 300, 437, 286, 478, 1382, 281, 360, 412, 1935, 307, 747, 382, 709, 295, 264, 665, 51478], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1163, "seek": 402480, "start": 4047.0800000000004, "end": 4050.1200000000003, "text": " parts of that, you know, trying to figure out how we can take advantage of this", "tokens": [51478, 3166, 295, 300, 11, 291, 458, 11, 1382, 281, 2573, 484, 577, 321, 393, 747, 5002, 295, 341, 51630], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1164, "seek": 402480, "start": 4050.1200000000003, "end": 4053.2000000000003, "text": " process where we actually get insight from people in the field, what they're", "tokens": [51630, 1399, 689, 321, 767, 483, 11269, 490, 561, 294, 264, 2519, 11, 437, 436, 434, 51784], "temperature": 0.0, "avg_logprob": -0.11507160648418839, "compression_ratio": 1.8436578171091444, "no_speech_prob": 0.001648337347432971}, {"id": 1165, "seek": 405320, "start": 4053.2, "end": 4056.0, "text": " looking for, what they think is interesting, what they think would improve", "tokens": [50364, 1237, 337, 11, 437, 436, 519, 307, 1880, 11, 437, 436, 519, 576, 3470, 50504], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1166, "seek": 405320, "start": 4056.0, "end": 4059.2799999999997, "text": " the, the work and try to, uh, try to use that.", "tokens": [50504, 264, 11, 264, 589, 293, 853, 281, 11, 2232, 11, 853, 281, 764, 300, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1167, "seek": 405320, "start": 4059.52, "end": 4062.8399999999997, "text": " And overall, just trying to figure out how to navigate this peer review system.", "tokens": [50680, 400, 4787, 11, 445, 1382, 281, 2573, 484, 577, 281, 12350, 341, 15108, 3131, 1185, 13, 50846], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1168, "seek": 405320, "start": 4063.08, "end": 4065.56, "text": " I think it definitely made it feel better as well that the Mamba paper was", "tokens": [50858, 286, 519, 309, 2138, 1027, 309, 841, 1101, 382, 731, 300, 264, 376, 23337, 3035, 390, 50982], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1169, "seek": 405320, "start": 4065.56, "end": 4069.12, "text": " also rejected from ICLR, which, uh, you know, sorry.", "tokens": [50982, 611, 15749, 490, 14360, 31722, 11, 597, 11, 2232, 11, 291, 458, 11, 2597, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1170, "seek": 405320, "start": 4069.72, "end": 4072.7599999999998, "text": " I know, yeah, yeah, it was crazy to me as well.", "tokens": [51190, 286, 458, 11, 1338, 11, 1338, 11, 309, 390, 3219, 281, 385, 382, 731, 13, 51342], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1171, "seek": 405320, "start": 4072.7999999999997, "end": 4075.72, "text": " But, uh, yeah, definitely, uh, it's a, it's a challenge.", "tokens": [51344, 583, 11, 2232, 11, 1338, 11, 2138, 11, 2232, 11, 309, 311, 257, 11, 309, 311, 257, 3430, 13, 51490], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1172, "seek": 405320, "start": 4075.72, "end": 4078.7599999999998, "text": " And, you know, after staying up for 40 hours to get this done, it was like,", "tokens": [51490, 400, 11, 291, 458, 11, 934, 7939, 493, 337, 3356, 2496, 281, 483, 341, 1096, 11, 309, 390, 411, 11, 51642], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1173, "seek": 405320, "start": 4078.7599999999998, "end": 4081.16, "text": " Oh, would be a, it would have been nice if they could have looked at our", "tokens": [51642, 876, 11, 576, 312, 257, 11, 309, 576, 362, 668, 1481, 498, 436, 727, 362, 2956, 412, 527, 51762], "temperature": 0.0, "avg_logprob": -0.1690995012010847, "compression_ratio": 1.782874617737003, "no_speech_prob": 0.014952023513615131}, {"id": 1174, "seek": 408116, "start": 4081.16, "end": 4083.72, "text": " paper at least, you know, just see, you know, the work that we did.", "tokens": [50364, 3035, 412, 1935, 11, 291, 458, 11, 445, 536, 11, 291, 458, 11, 264, 589, 300, 321, 630, 13, 50492], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1175, "seek": 408116, "start": 4083.72, "end": 4087.3999999999996, "text": " But yeah, it's, uh, it's definitely good to learn from these things.", "tokens": [50492, 583, 1338, 11, 309, 311, 11, 2232, 11, 309, 311, 2138, 665, 281, 1466, 490, 613, 721, 13, 50676], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1176, "seek": 408116, "start": 4087.3999999999996, "end": 4089.96, "text": " And I guess we've learned the lesson as well, not to submit in the last 15", "tokens": [50676, 400, 286, 2041, 321, 600, 3264, 264, 6898, 382, 731, 11, 406, 281, 10315, 294, 264, 1036, 2119, 50804], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1177, "seek": 408116, "start": 4089.96, "end": 4092.6, "text": " minutes and to, you know, do it in, uh, in advance.", "tokens": [50804, 2077, 293, 281, 11, 291, 458, 11, 360, 309, 294, 11, 2232, 11, 294, 7295, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1178, "seek": 408116, "start": 4092.6, "end": 4095.2, "text": " But yeah, thank you so much for your kind words about the paper.", "tokens": [50936, 583, 1338, 11, 1309, 291, 370, 709, 337, 428, 733, 2283, 466, 264, 3035, 13, 51066], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1179, "seek": 408116, "start": 4095.2, "end": 4095.8399999999997, "text": " That means a lot.", "tokens": [51066, 663, 1355, 257, 688, 13, 51098], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1180, "seek": 408116, "start": 4096.12, "end": 4099.36, "text": " And yeah, we'll surely continue to make this better and a lot of exciting", "tokens": [51112, 400, 1338, 11, 321, 603, 11468, 2354, 281, 652, 341, 1101, 293, 257, 688, 295, 4670, 51274], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1181, "seek": 408116, "start": 4099.36, "end": 4102.4, "text": " plans for how we're going to continue to try to, you know, merge together", "tokens": [51274, 5482, 337, 577, 321, 434, 516, 281, 2354, 281, 853, 281, 11, 291, 458, 11, 22183, 1214, 51426], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1182, "seek": 408116, "start": 4102.4, "end": 4106.28, "text": " these two, you know, empirical and theoretical sides of the equation to make", "tokens": [51426, 613, 732, 11, 291, 458, 11, 31886, 293, 20864, 4881, 295, 264, 5367, 281, 652, 51620], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1183, "seek": 408116, "start": 4106.28, "end": 4110.72, "text": " some really, hopefully impactful work that can really help people build systems", "tokens": [51620, 512, 534, 11, 4696, 30842, 589, 300, 393, 534, 854, 561, 1322, 3652, 51842], "temperature": 0.0, "avg_logprob": -0.10785380177114201, "compression_ratio": 1.8389830508474576, "no_speech_prob": 0.01911700889468193}, {"id": 1184, "seek": 411072, "start": 4110.76, "end": 4114.16, "text": " and, you know, make better systems and not be suffering so much under the load", "tokens": [50366, 293, 11, 291, 458, 11, 652, 1101, 3652, 293, 406, 312, 7755, 370, 709, 833, 264, 3677, 50536], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1185, "seek": 411072, "start": 4114.16, "end": 4115.08, "text": " of prompt engineering.", "tokens": [50536, 295, 12391, 7043, 13, 50582], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1186, "seek": 411072, "start": 4115.320000000001, "end": 4116.64, "text": " So yeah, thank you very much.", "tokens": [50594, 407, 1338, 11, 1309, 291, 588, 709, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1187, "seek": 411072, "start": 4116.72, "end": 4117.04, "text": " Amazing.", "tokens": [50664, 14165, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1188, "seek": 411072, "start": 4117.04, "end": 4119.56, "text": " Well, guys, it's been a pleasure and an honor to have you on the show.", "tokens": [50680, 1042, 11, 1074, 11, 309, 311, 668, 257, 6834, 293, 364, 5968, 281, 362, 291, 322, 264, 855, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1189, "seek": 411072, "start": 4119.56, "end": 4120.88, "text": " So just keep doing the great work.", "tokens": [50806, 407, 445, 1066, 884, 264, 869, 589, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1190, "seek": 411072, "start": 4121.2, "end": 4121.76, "text": " Absolutely.", "tokens": [50888, 7021, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1191, "seek": 411072, "start": 4122.04, "end": 4123.52, "text": " Hopefully we'll get you on again.", "tokens": [50930, 10429, 321, 603, 483, 291, 322, 797, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1192, "seek": 411072, "start": 4123.64, "end": 4123.96, "text": " Yeah.", "tokens": [51010, 865, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1193, "seek": 411072, "start": 4123.96, "end": 4126.92, "text": " Thank you so much for, I mean, for the opportunity to come and talk.", "tokens": [51026, 1044, 291, 370, 709, 337, 11, 286, 914, 11, 337, 264, 2650, 281, 808, 293, 751, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1194, "seek": 411072, "start": 4126.92, "end": 4128.8, "text": " It's, it's been an amazing opportunity.", "tokens": [51174, 467, 311, 11, 309, 311, 668, 364, 2243, 2650, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1195, "seek": 411072, "start": 4128.8, "end": 4132.2, "text": " It's, it's really unbelievable to be sitting here in front of these cameras", "tokens": [51268, 467, 311, 11, 309, 311, 534, 16605, 281, 312, 3798, 510, 294, 1868, 295, 613, 8622, 51438], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1196, "seek": 411072, "start": 4132.4400000000005, "end": 4136.52, "text": " after watching the show so many times, listening to so many of the podcasts.", "tokens": [51450, 934, 1976, 264, 855, 370, 867, 1413, 11, 4764, 281, 370, 867, 295, 264, 24045, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1197, "seek": 411072, "start": 4136.52, "end": 4138.92, "text": " And now to be speaking, it's just unbelievable.", "tokens": [51654, 400, 586, 281, 312, 4124, 11, 309, 311, 445, 16605, 13, 51774], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1198, "seek": 411072, "start": 4138.96, "end": 4139.52, "text": " So thank you.", "tokens": [51776, 407, 1309, 291, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1199, "seek": 411072, "start": 4139.8, "end": 4140.240000000001, "text": " Amazing.", "tokens": [51818, 14165, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1608772701687283, "compression_ratio": 1.8132183908045978, "no_speech_prob": 0.00432024197652936}, {"id": 1200, "seek": 414024, "start": 4140.28, "end": 4141.0, "text": " Thanks so much, guys.", "tokens": [50366, 2561, 370, 709, 11, 1074, 13, 50402], "temperature": 0.0, "avg_logprob": -0.3394295771916707, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.013299692422151566}, {"id": 1201, "seek": 414024, "start": 4141.76, "end": 4142.04, "text": " Awesome.", "tokens": [50440, 10391, 13, 50454], "temperature": 0.0, "avg_logprob": -0.3394295771916707, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.013299692422151566}, {"id": 1202, "seek": 414024, "start": 4142.04, "end": 4142.36, "text": " Okay.", "tokens": [50454, 1033, 13, 50470], "temperature": 0.0, "avg_logprob": -0.3394295771916707, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.013299692422151566}, {"id": 1203, "seek": 414024, "start": 4142.4, "end": 4142.88, "text": " It's a wrap.", "tokens": [50472, 467, 311, 257, 7019, 13, 50496], "temperature": 0.0, "avg_logprob": -0.3394295771916707, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.013299692422151566}], "language": "en"}