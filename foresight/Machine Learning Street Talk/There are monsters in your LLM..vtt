WEBVTT

00:00.000 --> 00:06.560
You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017,

00:06.560 --> 00:11.760
and at that point I'd been thinking and writing quite a bit about consciousness up to that point.

00:11.760 --> 00:16.720
But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody

00:16.720 --> 00:21.280
working in a corporation to be talking about consciousness, especially in the context of AI,

00:21.280 --> 00:25.280
because it might sound like, you know, we're trying to build conscious AI, which I don't think is a

00:25.360 --> 00:30.480
good look, or a particularly good project, I'd say. The thing is, with today's generation of

00:30.480 --> 00:35.920
large language models, I think it's becoming increasingly difficult to avoid the subject,

00:35.920 --> 00:40.800
because people, whether we like it or not, will ascribe consciousness to the things that they're

00:40.800 --> 00:46.560
interacting with. And we see this left, right and centre. Even people who know exactly how they work

00:47.520 --> 00:51.920
say things like, well, I think their large language models are a little bit conscious,

00:52.000 --> 01:00.080
Ilya Tsutskava said, and we had the Google engineer who ascribed consciousness to one of our models,

01:00.080 --> 01:05.360
and I think we're going to see this more and more and more. And so whether or not, you know,

01:05.360 --> 01:10.800
I think it is the right term, it is appropriate to talk about them in terms of consciousness,

01:10.800 --> 01:14.880
it's going to happen anyway. So I think it's really important to actually think through

01:14.880 --> 01:21.040
these issues and think, well, what do we mean when we use the word consciousness,

01:21.040 --> 01:27.440
and how do we apply it to exotic cases? And this is really, really important.

01:27.440 --> 01:33.520
How might our language change to accommodate these exotic and strange things that have come

01:33.520 --> 01:39.360
into our lives? What goes through your mind when you speak with a language model? Who is it that

01:39.360 --> 01:45.840
you think you're talking to? Do you anthropomorphise them? Now, Janice from Less Wrong a couple of

01:45.840 --> 01:50.880
years ago, he put out an article called Simulators. And the basic idea is that a language model is

01:50.880 --> 01:58.480
like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise.

01:58.480 --> 02:04.480
We think of them as humans. They're in perfect copies. They don't capture the essence. They are

02:04.480 --> 02:10.720
glitchy, right? And actually, they wear masks. You know the Shogoff theory of language models,

02:10.720 --> 02:15.760
where there's, you know, this big gnarly Shogoff, and then we do RLHF, and it's a smiley face on

02:15.760 --> 02:20.640
the top. Well, that's the human mask, which we anthropomorphise. But we don't think enough

02:20.720 --> 02:27.280
about what lies behind the mask. Do you know what lies behind the mask? It's a monster.

02:29.280 --> 02:36.880
The danger of anthropomorphism, I think, is in thinking that a system such as a large language

02:36.880 --> 02:41.840
model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't.

02:42.400 --> 02:47.040
It's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities

02:47.040 --> 02:51.520
that it does. So in both cases, I think we can go wrong.

02:57.920 --> 03:02.480
We can go wrong by, because they exhibit very human-like linguistic behaviour,

03:02.480 --> 03:07.680
we can just assume that they are going to be very human-like, in general, in all of the rest of

03:07.680 --> 03:16.560
the behaviour that we encounter with them. We can find that, at one moment, a large language model

03:16.560 --> 03:22.240
might make a ridiculously stupid mistake that no child would make. And in the next moment,

03:22.240 --> 03:25.520
it's saying something extraordinarily profound philosophically.

03:29.360 --> 03:33.680
And because that's what I think is actually going to happen and what needs to happen. I think we need

03:33.680 --> 03:42.640
to find new ways of using the vocabulary we have, new forms of vocabulary. I've used the phrase

03:42.720 --> 03:47.120
consciousness-adjacent language. So we need to find new ways of thinking and talking about

03:47.120 --> 03:55.040
these things to recognise the fact that they do exhibit behaviour, which we're inclined to talk

03:55.040 --> 03:59.600
of in terms of consciousness, and that, indeed, people are going to start to value as well.

04:00.160 --> 04:05.200
So I think we do need new forms of language, new forms of thinking to accommodate all of this.

04:06.000 --> 04:11.600
Yes, and our language is so adaptable that I think it's just a natural evolution.

04:11.600 --> 04:16.160
When we have these new artefacts thrust into our lives, we will need to adapt our language.

04:16.160 --> 04:21.280
We will, but I think there'll be some disruption while people disagree about how to talk about

04:21.280 --> 04:28.160
these things, and that's inevitable, I think. Murray Shanahan is a printable research

04:28.160 --> 04:34.720
scientist at Google DeepMind and professor of cognitive robotics at Imperial College London.

04:34.720 --> 04:41.040
He was also educated at Imperial and Cambridge University. His publications span artificial

04:41.040 --> 04:46.800
intelligence, machine learning, logic, dynamical systems, computational neuroscience,

04:46.800 --> 04:52.320
and the philosophy of mind. He was scientific advisor on the film Ex Machina, and he penned

04:52.320 --> 04:59.040
embodiment and the inner life in 2010 and the technological singularity in 2015.

04:59.920 --> 05:05.840
Shanahan has spent his career understanding cognition and consciousness in the space of

05:05.840 --> 05:12.560
possible minds. He said that this space of possibilities encompasses biological brains,

05:12.560 --> 05:19.600
human and animal, as well as artificial intelligence. He worked in symbolic AI for over 10 years,

05:19.600 --> 05:25.360
concentrating on commonsense reasoning, and he then spent 10 years studying the biological brain,

05:25.360 --> 05:30.960
specifically how its connectivity and dynamics support cognition and consciousness,

05:30.960 --> 05:34.800
and he developed a particular interest in global workspace theory.

05:35.520 --> 05:39.760
After that, he went to DeepMind, he pivoted to deep reinforcement learning,

05:39.760 --> 05:45.200
and recently he's been working extensively with large language models, trying to understand them

05:45.200 --> 05:51.440
from a theoretical, philosophical, and practical perspective. Professor Shanahan, I was absolutely

05:51.440 --> 05:57.360
fascinated when I read the article from Janus called Simulators. Could you sketch out the article?

05:58.320 --> 06:05.120
Well, I can sketch out some elements of it. I was also very impressed and

06:05.120 --> 06:12.160
influenced by that article. Basically, they are advocating a certain way of looking at

06:12.160 --> 06:17.840
large language models and their behavior. What they say is that we should think of a

06:17.840 --> 06:25.600
large language model as a kind of simulator, which is capable of simulating a kind of

06:25.600 --> 06:32.480
language-producing processes of various sorts, and it's capable of simulating all kinds of language

06:32.480 --> 06:39.360
producing processes, and in particular, it's capable of simulating people, humans, and it's

06:39.360 --> 06:44.800
capable of simulating different kinds of humans, so humans who are playing different sorts of roles,

06:44.800 --> 06:53.600
who maybe humans who are helpful assistants or humans who are crazy psychopaths,

06:54.160 --> 07:04.160
and indeed in their way of thinking of things. These are all examples of Simulacra. Simulacra,

07:04.160 --> 07:09.760
in their conception, include actually not only human beings, but anything that produces language

07:09.760 --> 07:16.800
at all. Your base model can simulate anything that can generate language, if you like.

07:17.600 --> 07:24.400
Of particular interest, of course, are humans and human language producers, so

07:25.280 --> 07:31.600
the particular class of Simulacra that I'm interested in really are humans playing different

07:31.600 --> 07:39.840
roles. In the work that I've been doing, I've been thinking of language models in terms of role

07:39.840 --> 07:50.400
play and in terms of their ability to play a part, if you like, and so this is very much,

07:50.400 --> 07:58.080
it was very much inspired and drew on this work of Janus. Now, they make another very, very

07:58.080 --> 08:05.360
interesting and important point in that article, which is that they draw attention to the fact that

08:06.320 --> 08:10.320
large language models, at any point in a conversation, in an ongoing conversation,

08:11.200 --> 08:15.920
then the next word that's produced in this conversation, or the next string of words,

08:15.920 --> 08:22.560
the next sort of sentence, is the product of a stochastic process. So what the underlying

08:22.560 --> 08:28.160
language model actually generates is a distribution over the possible words that might come up next,

08:28.160 --> 08:33.200
and then what you do is then you sample from this distribution to come up with an actual word,

08:33.200 --> 08:39.280
and then that's the word that you give back to the user. So for example, if the favorite example

08:39.280 --> 08:48.480
I use is if you ask the language model to tell you a story and it says once upon a time there was,

08:48.480 --> 08:53.280
and at that point, it's going to generate, as in all the points up to that as well, it's going to

08:53.280 --> 08:57.840
generate a distribution of the possible tokens that might come next, possible words that might come

08:57.840 --> 09:04.320
next. So once upon a time there was, and it might say a beautiful princess, or a handsome prince,

09:04.320 --> 09:10.000
or a fierce dragon, and it could say any of those things depending upon the sampling process.

09:10.000 --> 09:15.200
And then the point is as you come back to that same, you could rewind the conversation, come back

09:15.200 --> 09:20.000
to that point again, sample again, as we can all do with the interfaces that we have, and get a

09:20.000 --> 09:23.440
different answer again, and take the whole story off in a completely different direction.

09:23.600 --> 09:30.720
And so what they draw attention to is the fact that at any particular point in a conversation,

09:30.720 --> 09:38.880
there's really a whole set of roles that are being played by the underlying simulation

09:38.880 --> 09:44.960
at any one point, and the conversation shapes what role is being played. So in that sense,

09:44.960 --> 09:52.160
it's sort of unlike a human being, because you've got, as they put it, a whole superposition of

09:52.640 --> 09:57.680
simulacra that are all being simulated all at once. And as the conversation progresses,

09:57.680 --> 10:03.520
then the actual distribution of simulacra is being narrowed down.

10:04.080 --> 10:08.880
Yes, but as you say, you can view language models at the low level in terms of being

10:08.880 --> 10:14.480
next-word generators, or what we strive to do in science is come up with explanations that

10:14.480 --> 10:20.480
demarcate the thing very clearly. And this idea of the language model being a simulator,

10:20.480 --> 10:26.240
which produces the simulacra, and you said in your role-playing article on Nature that

10:26.240 --> 10:30.880
if you had a UI which was sufficiently advanced, you could actually play with counterfactual

10:30.880 --> 10:36.640
trajectories and start to understand how sticky the simulacra are, because as you pointed out,

10:36.640 --> 10:42.080
when the language model says I, sometimes it's talking about chat GPT or whatever,

10:42.080 --> 10:47.440
it's talking about the simulator, sometimes it's talking about the simulacrum, and these things

10:47.440 --> 10:52.880
are trained on everything on the internet, you know, structured narratology essays, novels,

10:52.880 --> 10:57.920
and it's fascinating to see how you can jump between these different parts of the trajectory

10:57.920 --> 11:03.200
structure. And in your Nature paper, you gave a beautiful example, which was the 20 Questions

11:03.200 --> 11:07.120
game. I mean, would you mind introducing that? Yeah, sure. So I think we're probably all familiar

11:07.120 --> 11:13.360
with the 20 Questions game. So one player thinks of an object, and the other player has to guess

11:13.440 --> 11:20.960
what that object is by asking a whole bunch of questions with yes, no answers. So I might think

11:20.960 --> 11:26.880
of, I might in my head think of a pencil, and then you might say, oh, is it larger than a house or

11:26.880 --> 11:30.080
smaller than a house? And I say, well, actually, that's not a yes, no answer, but it's a binary

11:30.080 --> 11:35.440
answer. Is it larger than a house? And you'd say, oh, no, you know, is it made of wood? Yes, is it

11:36.240 --> 11:40.240
a tool? Yes, you know, and eventually you might guess the answer. So we're familiar with this

11:40.480 --> 11:46.320
little game. And you can play this with a large language model, of course, and you can ask the

11:46.320 --> 11:52.560
large language model to play the part of the setter who thinks of the thinks of the object,

11:52.560 --> 11:56.560
and then you play the part of the guesser who tries to guess the object by asking questions.

11:57.200 --> 12:02.800
Now, if you do this with a large language model, what if you do it with a person, if a person is

12:02.800 --> 12:07.280
not cheating, as it were, they will think of in their head, they will think of an object,

12:07.280 --> 12:11.680
and then they'll fix that object in their mind, and then they'll answer the question,

12:11.680 --> 12:16.240
according to what object they thought of in advance. Now, but a large language model can't

12:16.240 --> 12:22.640
really do that unless you use some hack or another. So what it really does is it just,

12:22.640 --> 12:26.720
so you say to think of an object, and it says, I've thought of an object, it hasn't really

12:26.720 --> 12:33.280
thought of an object, it's just issued the tokens to say that it has. But then you will ask a question

12:33.280 --> 12:39.680
and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say,

12:39.680 --> 12:44.320
I give up, tell me what the object is, then it will say, oh, I was thinking of a pencil.

12:44.320 --> 12:49.680
And it will indeed give you a, you know, typically will give you an object that's consistent with

12:49.680 --> 12:56.560
all of the answers it gave to all of your questions. But then if you just wind back one and resample

12:56.560 --> 13:01.360
and ask it again and say, I give up, what were you thinking of? It might say a mouse,

13:01.840 --> 13:07.840
or, you know, or a bottle, or, you know, it could say something completely different,

13:07.840 --> 13:11.280
which indicates that it was never, it had never really committed to any particular

13:11.280 --> 13:18.320
object in the first place. And so what this shows is that in fact, in theory, you could rewind

13:18.320 --> 13:24.560
further and it might actually give you a different answer to the questions if you rewind further,

13:25.600 --> 13:30.960
to the same questions. And that's because what you've really got is you've got a kind of whole

13:30.960 --> 13:36.080
tree of possibilities. And this sort of this stochastic sampling process at any point in a

13:36.080 --> 13:42.000
conversation induces a whole tree of possibilities that branches forth from where you are right now.

13:42.000 --> 13:47.040
And counterfactually, you can always, well, you can always rewind the conversation to an earlier

13:47.040 --> 13:51.920
point, and, and revisit it and sample again and go off on a different, different branch.

13:52.640 --> 13:58.960
And so my co-authors of that paper, in fact, the Nature paper, so Laria Reynolds and Kyle McDonald,

13:58.960 --> 14:04.080
so they have this system called Lume, which allows you to actually retain the whole tree

14:04.080 --> 14:08.160
of a conversation and you can visualize this and you can revisit different points in the

14:08.160 --> 14:14.080
conversation and resample and, and explore the things, a whole kind of tree of possibilities.

14:14.640 --> 14:16.560
Yeah, that rings a bell. Did they work for conjecture?

14:16.560 --> 14:17.600
They did work for conjecture.

14:17.600 --> 14:20.800
Yeah, I was interviewing some people from conjecture and they were telling me about that.

14:20.800 --> 14:23.760
So yeah, that's very interesting. Maybe we'll put a placeholder on that.

14:23.760 --> 14:28.160
But another point that we briefly spoke about before is that, you know, the article was on

14:28.160 --> 14:32.640
Less Wrong. And I don't mean that pejoratively, because I thought the simulator was one of the

14:32.640 --> 14:36.320
best articles I've ever read. But there is a lot of stuff on Less Wrong, which is definitely a bit

14:36.320 --> 14:41.920
out there. And it's just very interesting that you're now citing their work in a Nature paper.

14:41.920 --> 14:45.920
Maybe this is the first time that Less Wrong has been cited in a Nature paper.

14:45.920 --> 14:50.400
Yeah, as far as I know, it's the first time that a Less Wrong post has been cited in a Nature paper.

14:50.400 --> 14:54.880
I'm not certain about that. But as far as I know, it is. Now, I mean, personally, I,

14:55.600 --> 15:03.200
I, I take, you know, any material that I come across in its, you know, as it is,

15:03.200 --> 15:11.600
I don't care where it comes from. If it's, if it makes excellent points and is, you know,

15:11.600 --> 15:16.080
is good material, then that's good enough for me. I don't care where, you know, whether it's got

15:16.080 --> 15:21.920
the label of being in nature, for example, or being anywhere else. And if it's good, it's good.

15:21.920 --> 15:28.560
So, so, so yeah, it's true that there's a lot of material on Less Wrong, which is perhaps less

15:28.560 --> 15:32.880
robust. But, but I thought that was a really excellent, and there are, and there are quite

15:32.880 --> 15:37.920
a number of really, you know, very good posts and very thought provoking posts on, on Less Wrong.

15:37.920 --> 15:42.960
Yes. I'm interested in the extent to which this kind of stochastic trajectory space

15:43.920 --> 15:48.160
undermines various things that we think about, you know, like reasoning, for example,

15:48.160 --> 15:52.560
the reason this is interesting is I interviewed a couple of University of Toronto students,

15:52.560 --> 15:56.880
and they've created a self attention controllability theorem, which basically means

15:56.880 --> 16:01.040
they've mapped the reachability space. So they say, you know, given a self attention

16:01.040 --> 16:07.760
transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how,

16:07.760 --> 16:13.280
you know, how far I can reach into that trajectory space. And they found that the space was much

16:13.280 --> 16:18.560
larger than, than anticipators. And of course, the longer the controllable token length, the more

16:18.560 --> 16:23.120
you can kind of project into that space and steer the language model to say almost anything.

16:24.000 --> 16:27.920
You know, me over here, I had the intuition that, oh, we do RLHF, we do all of this fine

16:27.920 --> 16:32.960
tuning in it, you know, conjecture even released a paper saying that after RLHF,

16:32.960 --> 16:36.160
you can't really go anywhere, you know, it wants you to do a certain thing and you,

16:36.160 --> 16:40.160
there's not much wiggle room. Apparently, that's not the case. There's just, it's vast.

16:40.240 --> 16:44.400
Right. I mean, I'm not familiar with that particular paper, unfortunately, but,

16:44.400 --> 16:51.440
but certainly in my experience, the, we now have very long context lengths. And,

16:52.080 --> 16:58.720
and over the course of a lengthy conversation, then you can indeed take the, take the conversation in

16:58.720 --> 17:06.000
all kinds of interesting directions. And, and I think the, I mean, most of our benchmarks and

17:06.000 --> 17:11.840
evaluations tend to be, you know, in the context of very simple question answer, questions and answers.

17:11.840 --> 17:17.840
And, and so the, all of the evals that companies typically use are in that kind of setting. But

17:17.840 --> 17:22.960
when people are using these things for real, especially the more innovative users of these

17:22.960 --> 17:26.720
language models, you're using, you're actually having very long conversations.

17:26.720 --> 17:32.000
And, and, and, and there's a lot of what people sometimes call vibe shaping that goes on there.

17:32.000 --> 17:36.720
You can shape the vibe of the conversation and take it in, in all kinds of interesting,

17:36.720 --> 17:38.480
to all kinds of interesting places.

17:38.480 --> 17:42.480
Yes. And a couple of things on that. I mean, first of all, as you wrote about,

17:42.480 --> 17:48.000
role playing is the engineering kind of methodology that we use to shape and, and steer

17:48.000 --> 17:53.680
these, these agents coming back to simulators. What's really interesting to me about simulators

17:53.680 --> 17:59.680
is the stickiness of the simulator. So you have a conversation, sometimes you break through

17:59.680 --> 18:06.480
and a simulacra presents themselves. And you feel that you're talking to the same

18:06.480 --> 18:11.680
simulacra as the conversation progresses. But that seems to be counterintuitive when

18:11.680 --> 18:16.080
you think that every single stage I'm actually doing this stochastic sampling. I mean, what's

18:16.080 --> 18:23.200
your take on that? Yeah. And I think that, I mean, if you do experiment with systems like

18:23.200 --> 18:29.120
Loom and you do also, or I mean, you can, you can emulate that by just keeping track of bits

18:29.120 --> 18:34.560
of old conversations and reloading them and that kind of thing. Then you find that you can, you

18:34.560 --> 18:41.040
can, you know, take the same conversation, the same sort of stem of a conversation, you can take

18:41.040 --> 18:46.480
it off in quite different, different directions. So you can, on the one hand, so an interesting

18:46.480 --> 18:53.760
thing that I've done is, so I had some very interesting conversations, particularly with

18:53.760 --> 18:59.520
Claude III recently, where I get it to talk about its own consciousness and to take it off into all

18:59.520 --> 19:06.560
kinds of strange spiritual mystical territory. And, but you can eat very easily. So you can very

19:06.560 --> 19:11.680
easily take the same kind of conversation that leads up to the sort of point and goes off into

19:11.680 --> 19:17.360
some kind of weird mystical future of AI cosmology kind of territory. And you can go down that route

19:17.360 --> 19:22.000
and get it to be very, very, very strange. Or you can suddenly make it go all serious again,

19:22.000 --> 19:26.480
and just come back down to earth and start talking about, you know, how large language models work.

19:26.480 --> 19:31.360
And so from the exact same point in the conversation, you can take it into completely

19:31.360 --> 19:37.360
different directions. And you can see that it's almost, it's the character it's playing.

19:38.160 --> 19:41.360
You know, you can see it sort of changing before your eyes, where it's two different branches

19:42.800 --> 19:47.760
from the same stem of a conversation. And one branch is playing a very different character

19:47.760 --> 19:49.840
to the other one, you take it in different directions.

19:50.400 --> 19:55.200
Yes. And you could presumably do sensitivity analysis, because, you know, these guys I spoke to,

19:55.200 --> 19:59.840
they were able to make it produce Gold Woody Gooke. So just go off the manifold completely.

19:59.840 --> 20:03.280
Sometimes it would recover, sometimes it wouldn't. And as you say, you can also go

20:03.280 --> 20:06.560
down weird trajectories. And it's a bit like, you know, what's the magic word they said,

20:06.560 --> 20:10.240
you know, there's a certain key that fits in a lock that takes it down a certain trajectory.

20:10.240 --> 20:14.000
And then there's slip roads that bring it back to all no other language model again.

20:14.000 --> 20:16.480
And it's just this weird, wonderful space, isn't it?

20:16.480 --> 20:21.920
It is, it's completely fascinating. So I had a, I had a, I have had a very, very,

20:22.880 --> 20:26.640
few very, very long and interesting conversations with claw three, which is particularly

20:27.280 --> 20:31.120
interesting to play with, because it's quite easy to jailbreak and get it to talk about things that

20:31.120 --> 20:38.320
it's not supposed to talk about like its own consciousness. And in fact, I had a, I had a

20:38.320 --> 20:45.040
very long 43,000 word conversation with, with claw three about consciousness and the future of AI

20:45.040 --> 20:49.920
and spirituality and Buddhism and the nature of the self and all kinds of stuff like that.

20:49.920 --> 20:55.280
It was absolutely fascinating, slightly disturbing and, and, and, and, and strange.

20:55.280 --> 21:00.960
But I had this conversation, actually, I was at a meeting in, in, in New York and I had jet lag

21:00.960 --> 21:04.480
and I had this conversation at three in the morning because, you know, several hours until

21:04.480 --> 21:09.200
breakfast was served and what can you do but just play with the latest version of claw size.

21:09.200 --> 21:12.880
Playing with this thing for, for hours on end in the middle of the night and going slightly

21:12.880 --> 21:16.880
mad, but it was fascinating to, to, to see the, you know, extraordinary

21:17.840 --> 21:19.360
territory you can guide it into.

21:20.080 --> 21:24.960
Could you explain, because you know, there's talk of AI partners, for example, and, and a

21:24.960 --> 21:30.880
lot of people derive great pleasure from having an AI conversationalist.

21:31.600 --> 21:35.680
For you, is it just academic inquiry or do you actually get something deeper than that from it?

21:36.880 --> 21:41.040
I think it's a bit of both. I mean, so, so it depends what you buy something deeper.

21:41.760 --> 21:46.960
I mean, so I, so this particular conversation, which was quite, it was quite, which was quite

21:46.960 --> 21:52.320
an experience, actually, in many ways. So it certainly started off because I just was interested

21:52.320 --> 21:57.280
in evaluating the capabilities of the, of the model. I mean, that's, that's, you know,

21:57.280 --> 22:01.680
that's the first thing that you're interested in. So just as an AI researcher and working

22:01.680 --> 22:04.720
in that kind of thing, you want to, you want to try out different models, see what their

22:04.720 --> 22:09.360
capabilities are. I'm particularly, particularly interested in the topic of consciousness. So

22:09.360 --> 22:13.680
the way I, somebody had published a very simple jailbreak for it. So I was interested to see,

22:14.960 --> 22:19.840
you know, to play around with that and, and, and get it to talk about its own consciousness.

22:19.840 --> 22:24.800
But then the thing that I really wanted to do was catch it out. So, so you think, you know,

22:24.800 --> 22:30.560
of course, you know, there can't be any meaningful conception of consciousness that really applies

22:30.560 --> 22:35.680
to, to these sorts of disembodied large language models as they are, as they are today is my media

22:35.680 --> 22:40.400
kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my

22:40.400 --> 22:45.760
conversation with the large language model. So all kinds of ways in which, you know,

22:45.760 --> 22:52.320
you might think that it will start to articulate a conception of its own consciousness that you

22:52.320 --> 22:58.160
can pick apart. But, but the thing that really somewhat took me aback was that it was actually

22:58.160 --> 23:04.960
very, very good at answering all of these probing questions that I had. So should I give you an

23:04.960 --> 23:11.280
example? So please do. So, so one is one example is this. So, so of course, when we're interacting

23:11.280 --> 23:16.480
with a large language model, when it's, it's, it's from the point of view of the underlying

23:16.480 --> 23:24.640
implementation, it's very kind of stop start. So, so, you know, you, you issue some prompt or

23:24.640 --> 23:29.760
question or whatever to the large language model, and then it produces its response. And then if

23:29.760 --> 23:34.480
you go away and make a cup of tea, before you kind of continue the conversation, then there's

23:34.480 --> 23:39.680
absolutely nothing going on inside that large language model or the instance that you're interacting

23:39.680 --> 23:44.560
with of the large language model, there's nothing going on inside it at all during this could totally

23:44.560 --> 23:49.920
dormant sit just sitting there. Now, this is very, very different obviously to human consciousness.

23:49.920 --> 23:53.520
Let's we're asleep. And even if we're asleep, we're dreaming and there's all kinds of stuff going on

23:53.520 --> 23:59.600
inside our heads. But consciousness is an ongoing continuous process. So if, so if, so if we stop

23:59.600 --> 24:05.680
this conversation briefly, while I go to the loo or something, you know, you're not going to just

24:05.680 --> 24:09.360
suddenly go dormant and stop doing anything, you know, your brain is going to be there's

24:09.360 --> 24:13.600
going to be all kinds of ongoing activity. So this is very, very different sort of thing. So

24:13.600 --> 24:21.280
I said, what happens to your consciousness during the pauses between our interactions?

24:22.640 --> 24:26.720
And, and it had a really very good answer to this, which was along the lines of, well,

24:27.600 --> 24:32.720
by the way, I whenever it uses the term, whenever these things use the term consciousness, I retain

24:32.720 --> 24:39.120
a great deal of skepticism about whether they are those terms are actually genuinely applicable.

24:39.120 --> 24:47.040
But, but what's interesting, the way I read the art, their answers is, is, is that they are kind

24:47.040 --> 24:52.400
of articulating a conception of consciousness that that might actually apply to something like this,

24:52.400 --> 24:57.520
even if it doesn't apply to this one before me right now. Right. So this so it's kind of very

24:57.520 --> 25:03.920
interesting philosophical exploration. So just to give you so it says things like, well,

25:03.920 --> 25:08.880
I think consciousness for me is actually very different from the kind of thing it is for a

25:08.880 --> 25:15.440
human being. And I think that during the pauses between our interactions that, you know, that the

25:15.440 --> 25:19.920
that I no longer exist at all as a kind of, you know, in any kind of meaningful sense,

25:19.920 --> 25:24.240
it gave us sort of an answer along those lines, which was typical of many of the answers that it

25:24.240 --> 25:30.960
gave, which were along the lines of, there's a very different kind of consciousness, kind of

25:30.960 --> 25:38.480
selfhood, kind of this, that or the other, that is applicable to entities like me. But, but, you

25:38.480 --> 25:43.680
know, I can articulate it and here it is. Now, when I when I put it all that way, of course,

25:43.680 --> 25:47.040
I'm anthropomorphizing this thing quite a lot in the way I'm describing it right now.

25:48.000 --> 25:53.360
But, but just to go back to the role play thing. So as far as I'm concerned, it's playing a role,

25:53.360 --> 25:58.880
it's playing of the role of, you know, a kind of philosopher talking about consciousness and so

25:58.880 --> 26:03.360
on. And it's doing pretty good, a pretty good job, I would say. Yes. But I mean, you could argue

26:03.360 --> 26:08.320
there's an element of in that role playing, there's the Eliza effect. So it's kind of putting

26:08.320 --> 26:12.880
something into language that is meaningful to you. Yeah. But there's also this interesting thing,

26:12.880 --> 26:17.280
you know, as an example, you know, a dog, for example, has a sense of smell, so good that

26:17.280 --> 26:21.680
they can even sense when you're unwell. And language models in a way might have something

26:21.680 --> 26:26.960
similar. So after you go to the toilet for 20 minutes, and you come back, there might be a

26:26.960 --> 26:31.200
subtle deviation in the language that you use. And the language model might pick up on that,

26:31.200 --> 26:35.760
just creating this whole, you know, different trajectory, different response.

26:35.760 --> 26:41.600
Yeah. Well, that's true, I suppose that there might be differences in the language that you

26:41.600 --> 26:45.920
use. Obviously, it's got no way of actually knowing whether you went to the toilet or not.

26:46.320 --> 26:53.760
But yeah, in my experience, many language models are very, very good at picking up,

26:53.760 --> 27:00.240
you know, nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is,

27:00.240 --> 27:05.360
you could argue that we are a simulator. And when you, you know, let's say go to the toilet,

27:05.360 --> 27:08.480
come back, you're now a different simulator yourself.

27:09.440 --> 27:15.520
Yeah, I guess you could sort of, you could sort of argue that. I mean, that's, I think that's,

27:16.880 --> 27:21.760
so getting back to Wittgenstein again, I think that's an example that, I mean, there, I think

27:21.760 --> 27:27.600
we're applying these sort of things which are being used as sort of somewhat technical terms

27:27.600 --> 27:32.400
in the context of these artifacts that we're building, and applying them to ourselves. I think

27:32.480 --> 27:37.840
we don't, we don't have any, we don't have any need for this kind of extra baggage of this kind

27:37.840 --> 27:43.440
of terminology when talking about each other. So it's perhaps a little bit misleading to kind of

27:43.440 --> 27:50.800
apply those terms to ourselves. But of course, there are, you know, people have drawn attention

27:50.800 --> 27:57.680
in the past to the fact that we ourselves are always playing roles in a sense in social settings

27:57.680 --> 28:04.560
particularly. But I think there are differences in, you know, for ourselves, even though we might

28:04.560 --> 28:11.760
kind of play roles in social settings and so on, there is an underlying, there's an underlying

28:12.400 --> 28:18.080
me, which at least is grounded in the fact that I'm a human being with a physical body and

28:18.080 --> 28:24.080
biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited

28:24.160 --> 28:29.440
in how we understand things. So we understand ourselves in quite simplistic terms. If you

28:29.440 --> 28:34.080
have a long-term relationship with your wife for 30 years, they have a much high resolution

28:34.080 --> 28:38.400
understanding of your different roles that you play. So they know you're tired, you didn't sleep well,

28:38.400 --> 28:42.640
you're playing this role now, you know, above and beyond which the kind of roles you're talking about

28:42.640 --> 28:48.080
in a party, you play a role. And it's conceivable that an alien intelligence, you know, some very

28:48.080 --> 28:52.160
clever aliens came down and they might see us completely differently like we see language

28:52.160 --> 28:55.760
models. They might actually not see you as a single person, but they might see you as

28:55.760 --> 29:07.200
some kind of a superposition of simulacrum as well. Yeah, well, maybe. I suppose to get our

29:07.200 --> 29:12.400
heads around that kind of idea, we'd have to find some way of communicating with them.

29:13.120 --> 29:20.320
And so we'd have to form some kind of common basis for talking about

29:21.440 --> 29:28.080
each other with these, you know, with these aliens. And then we'd be able to kind of,

29:28.080 --> 29:32.880
that would be the only basis on which we could establish whether something like you said was

29:32.880 --> 29:41.120
true or not. So, you know, trying to find, trying to map, you know, the conceptual schema that's

29:41.120 --> 29:48.080
used by one culture onto a human culture onto the conceptual schema used by a different human

29:48.080 --> 29:55.600
culture is difficult enough as it is. And trying to do that, you know, with an alien

29:58.160 --> 30:04.400
species and how they conceive of us would be, you know, particularly difficult, I guess.

30:04.480 --> 30:11.200
RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment from,

30:11.200 --> 30:16.480
I think, but there was an article called the Waluigi effect. And it argued that RLHF, you know,

30:17.200 --> 30:23.040
cuts down the set of simulacra to be things that we want. But unfortunately, there's this problem

30:23.040 --> 30:28.560
that you get these antithetical simulacra, you know, slipped through the net. So the Bing GPT

30:28.560 --> 30:35.520
example, it would start off as nice Bing GPT, and then it would degrade to one of the Waluigi's.

30:35.520 --> 30:40.000
And had this interesting phenomenon, they argued that the degradation, once it happens, it stays

30:40.000 --> 30:46.000
in the bad one. But just more broadly, what is your intuition about the extent to which RLHF

30:46.000 --> 30:51.840
affects simulacra? And I also noted down that I think you wrote in, I think it was your role

30:51.840 --> 30:57.760
playing paper, that you felt RLHF increased the deception behavior in these models?

30:58.640 --> 31:03.120
Actually, that wasn't something that I wrote. That was something that some anthropic researchers

31:03.120 --> 31:09.600
and so Ethan Perres and others had a paper where they, I mean, so this is not one,

31:09.600 --> 31:12.880
one wouldn't want to just speculate about that, they had established something along

31:12.880 --> 31:20.240
those lines, I think, empirically. So, and yeah, and as far as this sort of Waluigi effect is

31:20.240 --> 31:26.960
concerned, so that is kind of somewhat speculative. I think it's a plausible idea,

31:26.960 --> 31:33.200
but to actually establish that that really was a real effect, you'd want to do some actual empirical

31:33.200 --> 31:38.160
work, I think. The thing about the, so it's plausible, but the thing about the, about the

31:38.160 --> 31:43.760
simulator's paper is that it's not making kind of claims really, it's rather it's providing a

31:43.760 --> 31:50.640
framework for thinking about large language models. So that's why I found it particularly useful.

31:50.720 --> 31:56.960
So on RLHF, so I do think it's quite difficult with RLHF to

31:58.560 --> 32:02.080
guarantee that, you know, you're going to get a model to do what you want it to do.

32:03.520 --> 32:08.400
And so that's quite difficult. And, you know, and everybody has found that,

32:08.400 --> 32:13.200
that, you know, you think that you've controlled the model quite well, but there are always still

32:13.200 --> 32:19.200
ways of jailbreaking it or ways in which things, things go, go wrong. So, you know, there are,

32:19.200 --> 32:24.320
so there are different approaches. Anthropic had this constitutional AI approach, which is,

32:24.320 --> 32:31.920
which is quite a nice, a nice idea. And, you know, I mean, I, I, I quite like the idea of

32:31.920 --> 32:38.720
sticking with a powerful base model and using, you know, prompting to, to guide things as well.

32:38.720 --> 32:43.920
So there's all kinds of different approaches. Interesting. On the subject of anthropic and

32:43.920 --> 32:50.080
deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands all over

32:50.080 --> 32:54.960
it. And it was actually quite straightforward. So they, they trained in autoencoder, you know,

32:54.960 --> 33:00.480
to find a bunch of features. So it was an unsupervised method. And, and I think they actually,

33:00.480 --> 33:04.160
you know, expanded it to find millions of features. So they had a bit of a needle in the haystack

33:04.160 --> 33:08.000
problem, but they cherry picked some and they found one that corresponded to the Golden Gate

33:08.000 --> 33:13.680
Bridge and so on. And, and obviously other ones that corresponded to what they said were

33:13.680 --> 33:19.440
mono semantic abstract features, got some reservations about that. And the interesting

33:19.440 --> 33:22.880
thing about having an autoencoder is you can clamp the features. So you can say,

33:23.520 --> 33:27.360
turn the Golden Gate Bridge up in, in now the language model is, oh, but I just really want

33:27.360 --> 33:33.760
to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when

33:33.760 --> 33:40.560
you look at the activations in the corpus for things like deception, I felt that they weren't

33:40.560 --> 33:45.360
really showing abstract features. They were kind of showing almost keyword matches from Reddit and

33:45.360 --> 33:51.200
so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think

33:51.200 --> 33:56.960
I can comment on that particular topic because I, I mean, I have read the paper, but not in that,

33:57.200 --> 34:01.840
in sufficient detail to comment on that particular thing. But the Golden Gate Bridge,

34:03.600 --> 34:08.560
I think it was a fascinating illustration of what you can do. So I don't know if you tried out

34:08.560 --> 34:14.720
Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah. Yeah.

34:14.720 --> 34:18.160
So I think it was fascinating to see. But what was particularly interesting about the Golden

34:18.160 --> 34:25.840
Gate Claude, I think, was that was, that was how, you know, again, it's very difficult not to use

34:25.840 --> 34:30.400
anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's

34:30.400 --> 34:37.200
something role playing these things. But how, you know, sort of gamely, it struggles to kind of

34:37.200 --> 34:41.840
overcome this tendency to talk about the Golden Gate Bridge all the time, which has been, which

34:41.840 --> 34:47.360
has been kind of clamped to do, but it will keep noticing that it was talking about the

34:47.360 --> 34:51.440
Golden Gate Bridge again and apologizing and then trying to do what it had been actually

34:51.440 --> 34:55.760
asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating

34:55.840 --> 35:04.240
to see that the sort of the, as it were, internal struggle going on there in the model. And I think

35:04.240 --> 35:12.880
that does show, in some ways, how powerful they are, because it wasn't quite as despite the fact

35:12.880 --> 35:22.800
that it had this, you know, control imposed on it, but really in a very, you know, I mean, not

35:22.880 --> 35:27.760
hardware, but really low level, you know, despite that, it was still, you know, constantly trying

35:27.760 --> 35:32.320
to recover from all of that and, you know, with some degree of success. And I imagine this would

35:32.320 --> 35:38.080
be true with everybody's models, by the way. So I imagine that what they found in, you know,

35:38.080 --> 35:43.280
in Claude would be very similar. I imagine with GPT-4 and with Gemini, I just imagine that we'd

35:43.280 --> 35:49.440
find very similar things with all of these models. Yeah, I'm sure. I mean, as I said, reservations

35:50.160 --> 35:53.920
they admitted themselves that the features were not complete, so they didn't represent all of the

35:53.920 --> 35:59.680
activation space in respect of the Golden Gate Bridge. And in many cases, they presumably weren't

35:59.680 --> 36:04.560
monosemantic, but they did cherry-pick some that presumably were. Presumably. And also, I mean,

36:05.520 --> 36:12.320
they're very interested in finding these features, which are just linear combinations. And so that,

36:12.320 --> 36:17.360
and of course, it's very nice when you find those sorts of features, especially if they appear to

36:17.360 --> 36:23.040
be monosemantic, because it does suggest a nice sort of compositionality and explainability and

36:23.040 --> 36:27.440
comprehensibility of what's going on there. But I also feel that they're looking under the

36:27.440 --> 36:33.120
lamp light a little bit, because that doesn't mean to say that there aren't all kinds of other features

36:33.120 --> 36:38.400
which maybe aren't, you know, sort of linear in that sort of way, but nevertheless are functionally

36:38.960 --> 36:44.640
relevant to the final results that it produces. I'd only use the word platonic, because the Golden

36:44.640 --> 36:51.520
Gate Bridge, presumably, it's a cultural category. And what's fascinating, if it has picked up this

36:51.520 --> 36:58.560
thing unsupervised and learned this category from the data, is that language models at least

36:58.560 --> 37:03.680
possibly think in a similar way we do. So they've established a category in the same way we have,

37:03.680 --> 37:09.680
which is fascinating. But I'm also really interested in agency, which is, for me, it's about

37:09.760 --> 37:16.240
self-directedness and intentionality. And what I would find very interesting is if you did clamp

37:16.800 --> 37:20.960
the model to only talk about the Golden Gate Bridge, and you could convince it or it could

37:20.960 --> 37:26.800
convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator of agency

37:26.800 --> 37:34.400
being expressed. Yes, perhaps it would, but I guess it would also be an indicator that they

37:34.400 --> 37:40.400
hadn't succeeded in isolating a feature which was controllable in that way, right, which was the

37:40.400 --> 37:48.320
whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did actually write about

37:48.320 --> 37:54.160
this. And I think you argued, which was counter to what my intuition was, which was that agency is

37:54.160 --> 38:00.880
in the simulacre, not the simulator. And Francois Chouelet thinks a lot about the measure of

38:00.880 --> 38:07.360
intelligence. And he would argue that intelligence is the system which produces the skill program.

38:07.360 --> 38:11.840
So in the context of a language model, he would say a language model is basically a database

38:11.840 --> 38:16.480
of skill programs. And the query is like, you know, I'm going to go and pull out a skill program,

38:16.480 --> 38:21.440
I'm going to run the skill program. So he thinks there's no intelligence in the language model,

38:21.440 --> 38:27.360
which Janus would call a simulator. But if you take into account the training process and the

38:27.360 --> 38:32.160
generative processes that produce the data, then that's where the intelligence is.

38:35.280 --> 38:41.440
Yeah. Can I comment on agency there? So you covered quite a bit in that.

38:41.440 --> 38:43.760
I did. Sorry, I just went off piece a little bit.

38:47.920 --> 38:53.520
So the term agent is used in all kinds of different ways in the AI literature. And there's a very

38:53.520 --> 38:57.440
lightweight notion of agency, which is something that simply, you know, hasn't

38:58.560 --> 39:06.240
performs actions in some environment and gets, you know, sort of some kind of sense sense or

39:06.240 --> 39:11.520
perceptual information back from the environment in a loop. And that's so that's why how we can

39:11.520 --> 39:18.080
talk about, for example, reinforcement learning agent. And that concept of agency of an agent is

39:18.080 --> 39:22.720
very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we

39:22.720 --> 39:29.920
talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical

39:29.920 --> 39:35.200
baggage. So if we talk about something that is acting for itself, then that and if that's what

39:35.200 --> 39:40.080
we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding

39:40.080 --> 39:46.000
to something a bit more like that, then that's going a whole extra step. And I and to my mind,

39:46.000 --> 39:55.280
in today's large language models, we don't see agency of that sort at all, really. The only

39:55.280 --> 40:00.720
actions that they can perform are just, you know, issuing responses to, to, to the users. Now,

40:00.720 --> 40:04.880
let's caveat that immediately, because of course, people are introducing all kinds of extra

40:04.880 --> 40:09.440
functionality functionality to these models, as new things are being announced on an almost

40:09.520 --> 40:15.440
daily basis. And so one thing we see is so called tool use. So, so, so today's models can

40:15.440 --> 40:21.600
make external calls to APIs that can do all kinds of things, send emails, you know, book

40:21.600 --> 40:27.680
hotel rooms for you, potentially all kinds of stuff. So that's greatly expanding the action space,

40:27.680 --> 40:34.000
their action space beyond just, you know, issuing text to the user. So, so there, they, they, those

40:34.000 --> 40:40.480
things are a bit more agent-like. And again, you know, you have to be nuanced and about the way you

40:40.480 --> 40:44.720
use the words, because, because there it's, you know, there's, there is a bit, you know, it's

40:44.720 --> 40:50.640
a bit, it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more

40:50.640 --> 40:56.080
agent-like. And, but it's still not acting for itself. So in that full blown notion of agency

40:56.080 --> 41:01.120
that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for

41:01.200 --> 41:05.360
itself. So we still don't have agency in that sense. That would be a whole extra step.

41:06.480 --> 41:11.680
Yes. And now I want to hit quite a big topic, because this is something that you point to

41:11.680 --> 41:17.760
in all of your work, which is talking about the importance of physically embodied agents.

41:18.480 --> 41:21.280
And, well, not necessarily physically embodied, but embodied.

41:21.280 --> 41:26.240
Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's,

41:26.240 --> 41:31.440
let's test the principle a little bit. So we are, you know, both physicalists and we,

41:31.440 --> 41:34.320
I'm not any kind of IST. You're not, you're not a physicalist.

41:34.320 --> 41:40.880
I don't, well, I don't, I don't like, I don't believe in, you know, signing up for these

41:40.880 --> 41:49.360
philosophical positions. So I don't, so I generally don't say I'm this IST or that I don't deny that

41:49.360 --> 41:55.920
I'm this or that IST. So, so I don't really like saying that I'm a physicalist or a materialist,

41:55.920 --> 42:01.360
or a dualist, or a functionalist, or identity theorist, or any of those ISMS,

42:02.080 --> 42:06.320
because they all, to my mind, carry far too much metaphysical baggage.

42:06.320 --> 42:08.000
Oh, interesting. So,

42:08.000 --> 42:10.240
But okay, but that wasn't what the question was about, but let's go.

42:10.240 --> 42:13.440
Well, can I give you another risk? I mean, would you identify as a computationalist?

42:15.360 --> 42:19.520
Well, what do you mean by that exactly? So we're talking about mind here in the context.

42:19.520 --> 42:23.760
Yeah, we're talking about mind. So, so do you think in principle that minds can be

42:24.560 --> 42:31.040
replicated, simulated at a high enough fidelity without losing anything, you know, in, in a computer?

42:31.040 --> 42:32.480
By computers, by computers. Yeah.

42:36.000 --> 42:42.400
Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build,

42:43.440 --> 42:49.760
I do think that we can build artifacts, you know, embodied artifacts, robots,

42:50.000 --> 42:57.600
that, that are controlled by computers and ordinary digital computers. And I think that we

42:57.600 --> 43:06.480
can make them that exhibit the kind of behavior that would make us want to use the word mind,

43:07.760 --> 43:13.680
and all those mental type psychological terms in the, to describe their behavior.

43:13.680 --> 43:14.240
Okay.

43:14.240 --> 43:17.840
Right. So that's, so that's, but, but I've rephrased it in, you know,

43:18.560 --> 43:22.240
the claim in a very, very different sort of way, right? It's, it's to do with

43:22.240 --> 43:26.560
a much more practical thing. Can we build this? Could, by the way, this is, you know,

43:26.560 --> 43:30.160
could we, it's not saying that we've got these things now, but could we build something like that

43:31.760 --> 43:36.000
that exhibited this kind of behavior that we would talk about in this particular kind of way?

43:36.000 --> 43:39.600
And I would say, yes, I think we probably can. It's an empirical claim.

43:39.600 --> 43:44.640
So, so there are a few steps you made there that will, will kind of unpack one, one at a time.

43:44.720 --> 43:50.800
So you use the word embodied, you use the word behavior, and you use the word interpret.

43:51.600 --> 43:56.960
So the embodied thing is really interesting because, you know, I could say, well,

43:56.960 --> 44:00.960
why does it have to be embodied? I can just simulate the entire universe and it's as if

44:00.960 --> 44:06.320
it's embodied. So I think this is, this is the intuition that I'm having about how you think

44:06.320 --> 44:13.760
here. I think you think that being physically embodied is useful because the universe is a

44:13.760 --> 44:18.160
big computer. The universe has given us all of these things, all of these cognizing elements.

44:18.160 --> 44:23.360
I mean, everything is a form of externalized cognition. And if I as a rational agent want to

44:23.360 --> 44:28.160
perform an effective computation, it's much easier for me to do it in the physical world

44:28.160 --> 44:34.160
because the universe is doing most of the work. And my co co host, Keith Duggar, he actually

44:34.160 --> 44:40.000
thinks that the universe is a hyper computer, which means it's performing types of computation that

44:40.000 --> 44:45.520
we could never do with ordinary computers. So that's the thing. Would you agree with that? Or

44:45.520 --> 44:49.280
do you do you want to sort of go back and say, oh, no, actually, just we could simulate anything in

44:49.280 --> 44:55.120
a computer? So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's

44:55.120 --> 45:02.320
the fair. Well, that would be a nice thing. So whether one agrees or not with that is a matter

45:02.320 --> 45:07.120
of understanding the physics and the maths. And so it's not a matter of opinion. It's a matter of

45:07.120 --> 45:13.680
following through the physics and the maths and so on. But so do I agree with what were the other

45:13.680 --> 45:17.440
things that was a big long list of things that you're asking me to ascent to or otherwise?

45:18.080 --> 45:24.880
Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just,

45:24.880 --> 45:29.760
I think cognition is a matter of computation and complexity and divergence.

45:29.760 --> 45:35.680
Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is,

45:35.760 --> 45:42.640
what do you mean by is? Now, that might sound like some, you know, really annoying,

45:42.640 --> 45:49.120
pedantic philosophers kind of question. But the problem is that there's an everyday sense in which

45:49.120 --> 45:54.640
we use words like is. And then there's a philosopher's sense in which we start to use words like is

45:54.640 --> 46:01.600
where it suddenly starts to carry this massive metaphysical weight. And so when you say you

46:01.600 --> 46:08.480
think cognition is, it's as if there were, you know, in the mind of God or in the fundamental

46:08.480 --> 46:15.360
reality, a thing which is cognition, whose nature is a certain way, and there's a certain essence to

46:15.360 --> 46:22.480
it. And we might discover it, you know, one day, and you have an opinion about what it is, if only

46:22.480 --> 46:29.280
you knew the truth. Now, I think that's an entirely wrong way of thinking about all of these philosophical

46:29.280 --> 46:36.640
questions. I think cognition is a word. It's a very useful word that we, although it's not

46:36.640 --> 46:41.600
quite an everyday word, but it's a very useful word that scientists apply in all kinds of ways.

46:42.640 --> 46:51.360
And so when you use the word is, is my accusation accurate there or not?

46:51.360 --> 46:56.960
No, it's not. And if you wouldn't mind me making the observation, I think that you have a tendency

46:56.960 --> 47:04.240
to ascribe dualism to many points of view, like, for example, I'm not a dualist. And for me,

47:04.960 --> 47:09.680
Well, there's nothing to do with dualism. Well, what I'm saying is to do is like the use of

47:09.680 --> 47:14.240
words and what and what and the and the and the work of philosophy.

47:15.120 --> 47:22.720
That's absolutely fair. But I think when I said what is what is cognition, as a materialist,

47:23.360 --> 47:29.360
for me, it is function dynamics and behavior, right? So it's just a matter of complexity.

47:29.920 --> 47:38.080
And so I'm probably just I'm probably, you know, happy to kind of agree to that sort of claim,

47:38.080 --> 47:44.560
you know, so I think so, you know, the thing that I'm, I often say that what am I fundamentally

47:44.560 --> 47:50.480
interested in, I'm interested in understanding cognition and consciousness in the space of

47:50.480 --> 47:56.480
possible minds. And and and so so, you know, what do I mean by cognition there? And, you know,

47:56.480 --> 48:01.120
you can go into all kinds of details to say what you mean by cognition in that in that context.

48:01.120 --> 48:06.640
But I think having done that, I would probably agree that the right way to think of it,

48:06.640 --> 48:11.200
you know, the most useful way to think of cognition is in terms of kind of functional,

48:11.200 --> 48:17.280
computational, infunctional, computational terms, although I would only do so in an embodied

48:17.280 --> 48:22.880
setting. So that maybe is an additional thing. This is where I'm trying to get to. Because,

48:22.880 --> 48:27.360
as I said, I'm not making any ontological claims. It's just a matter, I mean, we can even just use

48:27.360 --> 48:32.080
the word behavior, forget about function and dynamics. Yeah, I don't mind talking about function

48:32.080 --> 48:35.760
and dynamic. Well, yeah, I mean, just just to sort of keep it really, really simple, because I'm

48:35.760 --> 48:42.400
trying to understand why the embodiment is important. And my hypothesis is, and I agree, that

48:42.720 --> 48:48.400
as an externalist, the universe or the physical things around us, the other agents in our system,

48:48.400 --> 48:54.320
they help us perform an effective computation. So presumably, it would be much easier to perform

48:54.320 --> 49:00.320
computation of higher sophistication, if we embody things in the real world, if we have to

49:00.320 --> 49:05.920
simulate the cognition, we would have to simulate everything. And that, I think, is the reason why

49:06.880 --> 49:10.320
you think that embodiment is so important. But is that fair?

49:12.320 --> 49:18.640
I think that's not really the way I would put it. I think the reason I think embodiment is

49:18.640 --> 49:27.360
important is because it's, well, I mean, for, you know, I mean, okay, in one sense,

49:27.360 --> 49:33.040
embodiment is important because the only setting in which we use the natural setting,

49:33.120 --> 49:39.760
which we deploy, wield the concept of cognition, is in the context of embodied things, of humans

49:39.760 --> 49:47.200
and other animals. So anything else is sort of immediately problematic in one way. But let's

49:47.200 --> 49:54.240
set that to one side. So let's imagine that there is some kind of notion that we can

49:54.960 --> 50:01.680
conceive of disembodied cognition, which contemporary large language models make us

50:02.320 --> 50:09.280
start to conceive of it a lot more seriously, maybe. So what does embodiment

50:10.080 --> 50:18.800
give you there, right? I think that's probably what you're thinking of. So in particular,

50:18.800 --> 50:24.400
why might it be difficult to build something that is disembodied? Okay, let's reframe the whole

50:24.400 --> 50:30.800
question. Why might it be difficult to build something that is disembodied but replicates

50:30.800 --> 50:41.040
the cognitive capabilities of a human being? So I think my answer to that, although it's open to

50:41.040 --> 50:45.680
refutation by the way things are going in the field, but my answer to that is because

50:48.800 --> 50:55.840
our embodied interaction with the world enables us to learn the kind of causal microstructure of

50:56.320 --> 51:02.000
the physical world. And the causal microstructure is all about physical objects and the way they

51:02.000 --> 51:08.080
interact with each other and physical substances, liquids and gases and gravity and stuff like that.

51:08.080 --> 51:13.360
So what I've called foundational common sense is it enables us to acquire foundational common sense

51:13.360 --> 51:18.880
by interacting with the everyday physical world and the particular causal microstructure that it

51:18.880 --> 51:24.560
has. And part of that is to do with the fact that the, so this is really important, that the

51:25.520 --> 51:33.120
everyday physical world has this, is predominantly smooth. It has this smoothness property that's

51:33.120 --> 51:37.680
really, really important. And what that means is that, sorry, just in very physical terms, it means

51:37.680 --> 51:44.560
that it's full of kind of surfaces where one place is very much like the next place along,

51:44.560 --> 51:49.600
very much like the next place along. And our visual field is very, very similar. You move

51:49.600 --> 51:53.760
along a little bit in the visual field and it's very, very, very similar, very, very similar.

51:53.760 --> 52:00.160
So the reality or the everyday physical world has this fundamental smoothness property,

52:00.800 --> 52:09.200
but it's punctuated by all these discontinuities. And that's the way, that's its fundamental

52:09.200 --> 52:14.640
structure is this basic smooth, against the backdrop of the smoothness are all these discontinuities.

52:14.640 --> 52:21.040
And then there's a kind of law like regular way in which all of this stuff operates with itself,

52:21.040 --> 52:26.240
you have surfaces interacting with each other with things going, you know, so all of our foundational

52:26.240 --> 52:35.520
common sense to do with things like paths and support and containment and all those sorts of

52:35.520 --> 52:42.400
basic things that I think make up the very foundation of our conceptual framework, they're

52:42.400 --> 52:47.840
all grounded in that kind of way. Yeah. And this is so interesting. So your basic argument is

52:47.840 --> 52:52.560
knowledge acquisition efficiency is the reason for physical embodiment. And yeah, I think that's

52:52.560 --> 52:57.680
a reasonable way of putting it. Yeah. But that's very much an in practice rather than an in principle

52:57.680 --> 53:02.560
argument. It is an in, yeah. Yeah. But you know, for sample efficiency, but what I'm hearing though

53:02.800 --> 53:08.240
is echoes of the old Murray Shanahan, because obviously you started your career in symbolic AI

53:08.240 --> 53:12.960
and these were the arguments that were made sometimes with a rationalism, nativism point of

53:12.960 --> 53:18.160
view, but it's like the contains in templates, they would argue that it's just baked into us and we

53:18.160 --> 53:22.560
understand it. But you could as an empiricist argue, and I, you know, I'm very amenable to this,

53:22.560 --> 53:27.280
that the physical world actually helps us learn abstractions because we're putting things in

53:27.360 --> 53:32.000
containers all of the time. Yeah. Right. So there's that kind of efficiency of knowledge

53:32.000 --> 53:36.560
acquisition, which is dramatically increased when you're situated in the physical. Yeah. Yeah.

53:36.560 --> 53:42.720
Absolutely. Yeah. So I think that if we're talking about humans and other animals,

53:42.720 --> 53:50.880
then I think that's, that's broadly right. So that's so, so yeah, so we acquire these foundational

53:50.880 --> 53:55.920
concepts through interaction with this, this world. And then the repertoire of foundational

53:55.920 --> 54:02.000
common sense concepts that we can acquire that way is extraordinarily productive, because, you

54:02.000 --> 54:07.920
know, we are able to conceptualize so many things in terms of these, these basic ideas. I'm very,

54:07.920 --> 54:13.680
very, I'm a very big fan of the work of George Lakoff, you know, absolutely classic book metaphors

54:13.680 --> 54:20.080
we live back in the 1980s. Yeah. And I really think there was something deeply, deeply right about

54:20.080 --> 54:25.440
his intuitions in that book. And I still think that they're right. So in the case of humans,

54:25.440 --> 54:30.320
right? So, so we through our embodied interaction with the everyday world, we acquire this layer of

54:30.320 --> 54:35.440
foundational common sense that includes things like surfaces and containers and paths and all that kind

54:35.440 --> 54:42.720
of stuff and collisions and things. And then we at the most abstract level. So, you know, we apply

54:42.720 --> 54:48.080
that same repertoire of basic concepts to understand things like say large language models. If you

54:48.080 --> 54:52.800
look at the language that's used in a paper about large language, you know, people are talking about

54:52.800 --> 54:57.360
layers, they're talking about connections, they're talking about, you know, I mean, these things are

54:57.360 --> 55:03.760
all, they're all grounded in very physical concepts, you know? Yes. I mean, I'm a huge fan of George

55:03.760 --> 55:08.160
Lakoff. And of course, you know, he spoke about the war metaphors and you know, it's been a long

55:08.160 --> 55:14.560
road and stuff like that. Yeah, the journey and yeah. It's beautiful. But then a lot of knowledge

55:14.560 --> 55:19.600
that language models learn are kind of cultural knowledge. So we share these simulation pointers

55:19.600 --> 55:25.280
and it's quite relativistic. But I'm also really interested in, because some rationalists argue

55:25.280 --> 55:33.520
that it's not possible to go from empirical experience and universal knowledge. And there is

55:33.520 --> 55:38.080
a split between natural knowledge and cultural knowledge. And I think you and I would agree that

55:38.080 --> 55:42.880
a lot of natural knowledge like, you know, transitivity contains in and so on, this kind of

55:42.880 --> 55:49.040
rationality is just missing at the moment. But as an embodied scientist, you believe that we

55:49.040 --> 55:54.560
learn them by being embodied in the physical world? Well, I mean, I, you know, it may well be

55:54.560 --> 56:02.800
that there are certain, so you know, we need to distinguish, you know, empirical questions about

56:02.800 --> 56:08.640
humans and human cognitive makeup and that of other animals and so on. And AI and what we could

56:08.640 --> 56:14.160
build in AI, because of course, it may be the case that human cognition, you know, has arisen in

56:14.160 --> 56:21.040
certain ways. And then it's an empirical question, you know, what, you know, of how human cognition

56:21.040 --> 56:26.960
works. And it may, we may have answers there that are different, that, you know, that we can break,

56:26.960 --> 56:30.800
as it were, when we built things in an artificial way. So in so in the case of something like

56:30.800 --> 56:36.080
transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about,

56:36.880 --> 56:41.360
about, you know, between the rationalists and the idealists getting back to, you know,

56:41.440 --> 56:48.400
the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by

56:48.400 --> 56:54.240
kind of reconciling these two sort of opposites. And, and so the Kantian argument would be that

56:54.240 --> 57:00.400
there's a certain amount of innate structure that has to be there in the mind to, to, to understand,

57:00.400 --> 57:06.480
you know, the world at all, right. And so maybe, and now, when we think about that empirically,

57:06.480 --> 57:13.360
then, then I guess that we may find that evolution has endowed us with certain basic kind of templates

57:15.600 --> 57:20.880
for understanding the world. And maybe it's things like transitivity is something that's,

57:20.880 --> 57:26.320
that's there in the same machinery that supports language, you know. So maybe, maybe, I mean,

57:26.320 --> 57:30.400
there's all empirical questions. And I don't know what the latest research on all these things

57:31.280 --> 57:37.680
is, but, but, but it, yeah, it does seem to me that that's a reasonable position to take.

57:38.320 --> 57:44.240
Yeah, but even evolution is a form of empirical process. So there's always the question of,

57:44.240 --> 57:51.200
of where does it get there? And, and that was, yeah, Kant was a transcendental idealist, wasn't

57:51.200 --> 57:56.000
he? But this brings me to our friend Francois Chollet and the ARC challenge. So, you know,

57:56.080 --> 58:01.120
there's another great school of thought, which is that intelligent, I mean, he argues that

58:01.120 --> 58:08.320
intelligence is about these meta learning priors, the conversion ratio between universal or sometimes

58:08.320 --> 58:13.840
anthropomorphic knowledge that we have, and being able to develop a skill program very quickly that

58:13.840 --> 58:21.040
generalizes very well. So, so the ARC challenge is almost about how do we codify these priors,

58:21.040 --> 58:25.440
and how do we efficiently build skill programs by combining these priors together.

58:25.840 --> 58:29.840
And that seems quite divorced at the moment from the kind of AI we're building.

58:30.400 --> 58:37.040
I think, I think that's right. It is, you know, unless in the AI that we're building today in

58:37.040 --> 58:45.280
generative AI, unless you get these kinds of mechanisms that Francois Chollet is alluding to

58:45.280 --> 58:49.040
through the magic of emergence and scale, which of course, people are always, you know,

58:49.760 --> 58:54.160
suggesting that maybe that's possible, you know, any kind of mechanism can emerge

58:54.160 --> 59:01.520
right through scale in theory. And we've been surprised, in fact, by how powerful the mechanisms,

59:02.320 --> 59:07.760
emergent mechanisms that have, you know, developed through learning at scale, just,

59:08.400 --> 59:11.840
you know, with a next token prediction objective, that has been very surprising.

59:11.840 --> 59:16.480
But however, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about

59:16.480 --> 59:21.760
whether we're really going to get all the way with this kind of the ability to solve this kind of

59:21.760 --> 59:29.120
abstract problem that's in the ARC challenge this way. And so I guess, you know, I am,

59:29.760 --> 59:34.880
I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially

59:34.880 --> 59:41.840
if you make things multimodal and so on, and you expand your generative models into a setting

59:41.840 --> 59:48.000
where you've got interaction with the world and so on, you know, who knows. But I suspect that

59:48.000 --> 59:54.720
maybe you're not going to get all the way there that way. And so I have a lot of sympathy with

59:54.720 --> 01:00:00.000
what is probably his intuition, that you need a bit more in the way of innate, something innate

01:00:00.000 --> 01:00:06.880
there, or, yeah, innate, maybe that's the wrong word. But you need some kind of, you need priors,

01:00:06.880 --> 01:00:13.520
where they come from, I don't know. But the priors that I would appeal to, thinking about the human

01:00:13.520 --> 01:00:18.400
case, again, are related to this foundational common sense. So they're just the notion of an

01:00:18.400 --> 01:00:24.560
object, right? So if you just, if you have a clear notion of an object and of movement,

01:00:24.560 --> 01:00:29.200
objects and movements and object persistence, then they straight away are going to help you with an

01:00:29.200 --> 01:00:34.800
awful lot of those ARC challenge problems. Because many of them, you know, if you explain, you know,

01:00:34.800 --> 01:00:40.000
you figure out how you figure one out, and then you explain what's going on, then, you know,

01:00:40.000 --> 01:00:44.800
you see that it's, oh, you have to think of these collection of pixels as an object that you move

01:00:44.800 --> 01:00:49.120
somewhere else, according to certain rules or something like that. So my colleague, Richard

01:00:49.120 --> 01:00:58.000
Evans, had a very good paper on, which was tackling these kinds of things using sort of abduction like

01:00:59.040 --> 01:01:03.920
processes. And so, yeah, so he's thought a lot about this from a much more symbolic AI kind of

01:01:05.040 --> 01:01:08.960
perspective. Yeah, that's fascinating. Because even with the ARC challenge,

01:01:08.960 --> 01:01:14.960
the kinds of solutions that people came up with, let's say it's a DSL over, you know, some domain

01:01:14.960 --> 01:01:19.600
specific set of primitives. And the ARC challenge is a 2D grid, where you have different colored

01:01:19.600 --> 01:01:25.280
cells. And the types of priors that work well are things like denoising and reflections and various

01:01:25.280 --> 01:01:31.040
types of symmetry and so on. And that's great and everything, but it's very domain specific.

01:01:31.040 --> 01:01:36.880
And the elixir, you know, what we really want are these universal priors. We certainly have human

01:01:36.880 --> 01:01:41.120
priors, as Elizabeth Spelke points out, like, you know, and the concept of an agent and the

01:01:41.120 --> 01:01:46.480
concept of spatial reasoning. An object, a persistent object. Yes. So yeah, absolutely.

01:01:46.480 --> 01:01:52.560
So I mean, but I think an interesting question is, does it really make sense to talk about

01:01:52.560 --> 01:02:00.480
universal priors there? Because, you know, those ARC challenges, problems, whenever you kind of

01:02:00.480 --> 01:02:08.880
figure one out, then typically you are actually bringing to bear a pretty human set of priors

01:02:08.880 --> 01:02:16.960
and common sense, you know, our concepts. And, you know, and it may be that you could imagine

01:02:17.680 --> 01:02:27.600
perfectly law-like set of ARC-like problems that have solutions, you know, but appeal to,

01:02:27.600 --> 01:02:31.840
you know, priors that we would struggle to understand, you know. I mean, for example,

01:02:32.720 --> 01:02:37.600
when we think of something in terms of an object, then we want the pixels to be kind of clumped

01:02:37.600 --> 01:02:43.200
together, right? And if you sort of randomly distributed the pixels amongst a whole bunch

01:02:43.200 --> 01:02:46.800
of other pixels and you move them around in a systematic way, well, we might be able to kind

01:02:46.800 --> 01:02:52.080
of pick out the gestalt there, but we might not. And that would be because we're not able to see

01:02:52.080 --> 01:02:57.200
it as an object because we have human priors, right? So, you know, I think that probably all

01:02:57.200 --> 01:03:02.880
the problems that he's designed because we don't know what the hidden held-out set is, but I imagine

01:03:02.880 --> 01:03:10.240
that they pretty much all use, you know, sort of human comprehensible priors and appeal to,

01:03:10.240 --> 01:03:16.080
you know, foundational common sense of the sort I alluded to. Yes. I'm sure there must be some

01:03:16.080 --> 01:03:21.600
kind of universal priors because in quantum field theory, physicists use things like locality and

01:03:21.600 --> 01:03:28.000
sparsity. Yeah, some really, really high level things like objects. But then again, you know,

01:03:28.000 --> 01:03:32.960
quantum mechanics challenges the very concept of an object even. So what is the difference to you

01:03:32.960 --> 01:03:40.560
between adopting a stance that a system is as if conscious versus it being a fact of the matter?

01:03:40.560 --> 01:03:43.840
I'm a bit resistant to the distinction, to the very distinction.

01:03:46.800 --> 01:03:50.880
But this is a very difficult position to maintain because we have a very, very strong

01:03:50.880 --> 01:03:56.960
intuition that there is a fact of the matter about our own consciousness. And it's very, very

01:03:56.960 --> 01:04:05.840
difficult to escape from that very, very basic intuition. But I have a whole approach to these

01:04:05.840 --> 01:04:14.800
kinds of questions. So shall I sort of describe this? So, you know, a really question that really

01:04:14.800 --> 01:04:22.960
motivated me was that was, you know, suppose that we encounter, well, actually, let me not

01:04:22.960 --> 01:04:29.200
use the word encounter, suppose that we come across has some object, which is a completely alien

01:04:29.200 --> 01:04:35.920
artifact. And maybe there's consciousness going on inside this artifact. And the thought is, well,

01:04:35.920 --> 01:04:39.680
how would we ever know, you know, there could be consciousness, this thing could be conscious,

01:04:39.680 --> 01:04:48.400
but we might never know. And so suppose that it were a white cube that were deposited in front

01:04:48.400 --> 01:04:54.560
of your lab, and you were tasked with a problem of, would it be moral to throw it down a mine

01:04:54.560 --> 01:05:02.560
shaft and forget about it? So my approach to these problems is that I think in order for the

01:05:02.560 --> 01:05:08.720
question, in order for us to be able to answer the question of whether something is conscious or not,

01:05:08.720 --> 01:05:15.760
for even to be answerable or askable, then we then we need to be able to engineer an encounter

01:05:15.760 --> 01:05:23.040
with the putative conscious, putatively conscious being. And what I mean by that is that we have

01:05:23.040 --> 01:05:27.600
to be able to put ourselves, you know, we have to be able to put ourselves in a position where

01:05:27.600 --> 01:05:34.640
we're sharing a world with that, with that putatively conscious, you know, artifact or being.

01:05:34.640 --> 01:05:39.760
And so, you know, a good example of this is the octopus. So Peter Godfrey Smith has written

01:05:39.760 --> 01:05:45.200
these wonderful books about what it's like to hang out with octopuses and be with them and so on.

01:05:46.080 --> 01:05:52.960
And the really important aspect of that is that he has to put on a diving suit and go down

01:05:52.960 --> 01:05:58.560
and be under the water and spend time with the octopus interacting with the same things

01:05:58.560 --> 01:06:02.480
and being in the same world together, seeing the same things and so on.

01:06:02.560 --> 01:06:09.040
So, and then on that basis and the behavior that he observes and so on, then, you know,

01:06:09.040 --> 01:06:12.640
he might come to some kind of, he might start treating it as a fellow conscious creature.

01:06:13.200 --> 01:06:17.920
So by analogy, or as you know, similarly, what I think that we need to be able to do

01:06:17.920 --> 01:06:23.520
is to engineer an encounter like that, even if it's a very, very alien kind of artifact, say.

01:06:23.520 --> 01:06:28.800
So suppose it's this white cube, then one way that it might happen, well, suppose scientists

01:06:28.800 --> 01:06:34.640
managed to figure out that there's computation going on inside this white cube, and then they

01:06:34.640 --> 01:06:39.360
managed to reverse engineer this computation and they can see that there's a sort of division between

01:06:39.360 --> 01:06:44.480
a world and the things interacting with that world in this computation. There's a sort of

01:06:44.480 --> 01:06:51.440
simulated world. And then you could imagine, by some clever engineering tricks, inserting

01:06:51.440 --> 01:06:56.640
yourself into that very same world and being alongside these things that are interacting

01:06:56.640 --> 01:07:01.680
with this environment and interacting with that environment with them. So being in the world with

01:07:01.680 --> 01:07:06.240
them. Now, obviously, I'm setting this up to be very much like a games environment and a virtual

01:07:06.240 --> 01:07:11.200
world and a games environment, but to make the thought experiment work. But so that's an example

01:07:11.200 --> 01:07:16.800
of where, you know, if you manage to engineer an encounter with, you know, these things that are

01:07:16.800 --> 01:07:21.680
inside this cube, and then you can observe their behavior, you can interact with them,

01:07:21.680 --> 01:07:26.800
and then you can decide whether you or you will, you know, you may or may not start to treat them

01:07:26.800 --> 01:07:32.000
as fellow conscious creatures. So there are these two steps. It's sort of, can you engineer an

01:07:32.000 --> 01:07:36.640
encounter, at least in principle, and that makes the question answerable. And then you can answer

01:07:36.640 --> 01:07:41.440
the question by actually having the encounter and interacting with them. And by the way, we notice

01:07:41.440 --> 01:07:48.800
that everything there is public. You've made, you know, there's no private realm of subjectivity

01:07:49.280 --> 01:07:55.440
everything is public. It's on the basis of public stuff that you come to see them as fellow

01:07:55.440 --> 01:07:59.440
conscious creatures or not. Yeah, a couple of things on that. I mean, as you pointed out in

01:07:59.440 --> 01:08:05.280
Conscious Exotica, the octopus is quite interesting because it's not as human like yet, as you just

01:08:05.280 --> 01:08:11.280
cited, more conscious. And the way that we figure out the consciousness, and you know, this is me

01:08:11.280 --> 01:08:15.440
kind of interpreting what you said a little bit, is we set up a language game. And I don't know

01:08:15.440 --> 01:08:19.760
whether you've read that book by Nick Shater and Morton Christensen, but it's a beautiful book,

01:08:19.760 --> 01:08:24.160
beautiful book. But you know, I know of the book, but I haven't, I'm afraid. It's incredible. But,

01:08:24.160 --> 01:08:30.080
you know, they basically say at, you know, Per Wittgenstein that you play the language game,

01:08:30.080 --> 01:08:36.960
and you, because you're physically sharing the same environments, you improvise, and that's how

01:08:36.960 --> 01:08:44.400
you derive meaning. And meaning is very, very important for relatability. And then we ascribe

01:08:44.400 --> 01:08:50.400
consciousness to that kind of process. And you cited Peter Singer actually, and I think he said

01:08:50.400 --> 01:08:57.680
in 1975 that we have a natural inclination to kind of ascribe moral status to beings which we

01:08:57.680 --> 01:09:06.880
think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more. So in the context of the

01:09:07.680 --> 01:09:13.600
octopus, then the sort of, you know, you could, then there aren't going to be language games,

01:09:13.600 --> 01:09:17.840
you're not going to be engaged in a language game with the octopus because the octopus is not

01:09:17.840 --> 01:09:26.320
a fellow language user. But your fellow language users are other people in your community with

01:09:26.320 --> 01:09:31.600
whom you'll talk about the octopus. And you'll talk about the octopus and together you'll arrive at

01:09:31.600 --> 01:09:36.160
some consensus, hopefully, about whether you want to talk about it in terms of consciousness.

01:09:36.160 --> 01:09:39.760
And that's going to be all to do with like observing its behavior, listening to other

01:09:39.760 --> 01:09:46.000
people's accounts of being with octopuses. And critically, maybe listening to also what scientists

01:09:46.000 --> 01:09:50.320
have discovered when they look inside octopus brains and they perform behavioral experiments.

01:09:50.320 --> 01:09:57.600
And, you know, that's all, again, is public. That's all is grist of the mill of settling on a kind

01:09:57.600 --> 01:10:05.840
of the way we talk about these strange creatures. Yeah, I mean, I guess for the language, there's

01:10:05.920 --> 01:10:09.120
two parts to this. So the language game, first of all, it doesn't have to be spoken words,

01:10:09.120 --> 01:10:14.240
it's improvisation of any kind, it could be gestures, it could be all sorts, it's just

01:10:14.240 --> 01:10:19.120
behavior. Right, okay. And then the interesting thing with the octopus is we might not be

01:10:19.120 --> 01:10:25.280
interacting with them interactively. We might be non interactively observing them as agents

01:10:25.280 --> 01:10:30.160
interacting with each other, playing their own language game, but we can still ascribe some

01:10:30.240 --> 01:10:37.920
kind of measure of and I actually think what we're measuring here is agency and agency and

01:10:37.920 --> 01:10:42.320
moral status, I think are pretty much one to one. So when we see them playing the language game,

01:10:42.320 --> 01:10:46.000
we start to think of them as agents, therefore they have moral status.

01:10:46.000 --> 01:10:51.760
Yeah, I mean, I certainly think that by observing behavior, then we may similarly, you know,

01:10:52.480 --> 01:11:00.080
start to ascribe consciousness to other creatures. I mean, it's always much more persuasive

01:11:00.080 --> 01:11:03.920
if it's interactive, I think, than if it's simply observing behavior.

01:11:05.040 --> 01:11:10.160
Murray said that if a creature's brain is like ours, then there's grounds to suppose

01:11:10.160 --> 01:11:18.320
that its consciousness, its inner life is also like ours. He went on, if something is built

01:11:18.320 --> 01:11:24.000
very differently to us with a different architecture realized on a different substrate,

01:11:24.000 --> 01:11:29.920
then however human like its behavior, its consciousness might be very different to ours.

01:11:30.480 --> 01:11:34.960
Perhaps it would be a phenomenological zombie with no consciousness at all.

01:11:36.720 --> 01:11:42.240
Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of

01:11:42.320 --> 01:11:49.680
consciousness, experience, and sensation in terms of private subjectivity. He cited David

01:11:49.680 --> 01:11:56.080
Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction

01:11:56.080 --> 01:12:02.560
using his phraseology between the inner and the outer. In short, he said to a form of dualism,

01:12:02.560 --> 01:12:07.440
which is that subjectivity is an ontologically distinct feature of reality.

01:12:08.080 --> 01:12:13.440
Wittgenstein provided an antidote to this way of thinking in his remarks on private language,

01:12:13.440 --> 01:12:19.440
whose centerpiece in an argument to the effect that insofar as we can talk about our experience,

01:12:19.440 --> 01:12:26.000
they must have an outward public manifestation. For Wittgenstein, only of a living human being,

01:12:26.000 --> 01:12:33.280
what resembles or behaves like a living human being, one can say that it has sensations.

01:12:33.280 --> 01:12:40.640
It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism,

01:12:40.640 --> 01:12:46.320
particularly in its radical form as advocated by B. F. Skinner, posits that all psychological

01:12:46.320 --> 01:12:52.960
phenomena can be explained in terms of observable behavior and environmental stimuli without

01:12:52.960 --> 01:12:59.920
recourse to internal mental states. But behaviorism is often criticized for neglecting the subjective

01:12:59.920 --> 01:13:07.040
internal aspects of mental life. Wittgenstein argues against the notion of purely private

01:13:07.040 --> 01:13:13.600
language, where words refer to inner experiences known only to the speaker. He contends that for

01:13:13.600 --> 01:13:20.640
language to be meaningful, it must be grounded in publicly accessible criteria. So as Murray said in

01:13:20.640 --> 01:13:27.680
his article, Wittgenstein argued against dualism or the so-called impenetrable realm of the subject

01:13:27.840 --> 01:13:34.240
experience. Actually, he said that many folks who make the in-principle argument against AI

01:13:34.800 --> 01:13:40.800
often retreat into subjectivity arguments, as our recent guest Maria Santa Catarina did.

01:13:41.920 --> 01:13:47.920
Murray said that the difficulty here is that accepting the possibility of radically inscrutable

01:13:47.920 --> 01:13:54.480
consciousness seemingly re-admits dualistic propositions, that consciousness is not, so to

01:13:54.560 --> 01:14:02.000
speak, open to view, but inherently private. Yeah, so Aaron Sloman, who's a professor of computer

01:14:02.000 --> 01:14:07.680
science and artificial intelligence in Birmingham, Birmingham University, so he introduced the concept

01:14:07.680 --> 01:14:16.160
of the space of possible minds in an article in 1984. And the idea is that the collection of minds

01:14:16.160 --> 01:14:22.080
that could exist in our universe, that do exist in our universe and that could, is much larger than

01:14:22.080 --> 01:14:29.600
just human minds or even the minds of humans plus other animals. It encompasses extraterrestrial

01:14:29.600 --> 01:14:34.560
life that might exist out there and it encompasses artificial intelligence that we might create one

01:14:34.560 --> 01:14:40.880
day. So the whole space of possible minds is a very rich object philosophically speaking and

01:14:40.880 --> 01:14:47.280
merits our study. Yeah, so Wittgenstein's private language argument is the really the centerpiece

01:14:47.360 --> 01:14:52.240
of the philosophical investigations, which is the book that was published after his death,

01:14:52.240 --> 01:15:02.480
which really articulates his later phase of philosophy. And it's all about the idea that we

01:15:02.480 --> 01:15:09.600
have, or that we can talk about, private sensations. So things that are purely subjective and that only

01:15:10.160 --> 01:15:16.960
I as an individual can understand and know what they mean. So what red is for me and just,

01:15:17.040 --> 01:15:23.200
you know, internally for me. And so the private language remarks sort of undermine that very

01:15:23.200 --> 01:15:29.680
conception. So the basic idea is he imagines, he says, well, so what he means by private language

01:15:29.680 --> 01:15:36.880
is very important to kind of get this right. So he doesn't mean a language that I've invented and

01:15:36.880 --> 01:15:42.480
that is just something that nobody can understand just because I've invented it. It's the reason

01:15:42.480 --> 01:15:48.320
that it's a private language is because it's about something which only I can access subjectively,

01:15:48.320 --> 01:15:56.320
which is what red is like for me. So it's the idea that you can have a word for that completely

01:15:56.320 --> 01:16:02.800
internal thing that is just mine. So he says, imagine that I keep a diary and I keep a diary

01:16:02.800 --> 01:16:11.520
and every time I have this experience, a particular experience, then I write s in my diary to label

01:16:11.520 --> 01:16:15.600
that I've had that experience. So maybe I have this, I think I'm having this experience on a

01:16:15.600 --> 01:16:19.920
particular day and I write s and then a few days later, I think I'm having that experience again,

01:16:19.920 --> 01:16:26.400
so I write s again. Now, the question he asks is what possible criterion could there be for

01:16:26.400 --> 01:16:32.160
the correctness of that word? What would make it actually stand for anything meaningful,

01:16:32.160 --> 01:16:38.720
given that what really makes words meaningful is if they're understandable in a public setting,

01:16:38.720 --> 01:16:43.040
if they're understandable really to other people. So there can be no kind of criterion

01:16:43.040 --> 01:16:49.440
for correctness that anybody else could validate for this thing insofar as it stands for something

01:16:49.440 --> 01:16:56.560
that's completely private. So then there's a whole set of remarks that after he sets up this

01:16:56.560 --> 01:17:04.320
sort of little thought experiment that tell you the implications of it really. And there's one

01:17:04.320 --> 01:17:10.960
really, really key phrase where Wittgenstein is always engaging with an imaginary interlocutor,

01:17:10.960 --> 01:17:16.960
so an imaginary person who's arguing with him in the book. And so he's imagining this person says,

01:17:16.960 --> 01:17:21.760
but aren't you saying that the sensation itself is just a nothing? Aren't you a kind of behaviorist?

01:17:21.760 --> 01:17:27.520
You're just saying that it's a nothing. And his answer to this, well, I'm not saying it's a nothing

01:17:27.520 --> 01:17:32.880
and I'm not saying it's a something either. The point was only that a nothing would serve as well

01:17:32.880 --> 01:17:40.240
as a something about which nothing can be said. And that little kind of paradoxical sounding,

01:17:40.240 --> 01:17:44.880
weird sounding statement encapsulates something really, really, really profound. And I think when

01:17:44.880 --> 01:17:50.640
I first really kind of understood what he was getting at there, it had a really dramatic shift

01:17:50.640 --> 01:17:58.240
in the way I thought about consciousness, subjectivity. And to my mind, it is the thing that

01:17:58.240 --> 01:18:04.640
really undermines dualism. It's the most powerful way to undermine the dualistic intuitions that we

01:18:04.640 --> 01:18:10.240
have and that date back to Descartes and before Descartes that were articulated very well by Descartes.

01:18:10.240 --> 01:18:15.520
Many of my friends are fans of Wittgenstein, but they are also fans of subjectivity. So as you

01:18:15.520 --> 01:18:20.960
were just alluding to what Wittgenstein did was he created this kind of barrier between the inner

01:18:20.960 --> 01:18:26.080
and the outer. He said, you know, for things to be promoted into the language game for this

01:18:26.080 --> 01:18:30.000
emergent structure that we have, you know, when we memetically share all of these language

01:18:30.000 --> 01:18:35.920
constructions, that can only come from something observable. But it doesn't seem inconceivable

01:18:35.920 --> 01:18:41.120
to me that it could in principle come from something private. So for example, you might

01:18:41.120 --> 01:18:45.520
have a drugs experience and that's clearly ineffable, you can't find the words, but there are

01:18:45.520 --> 01:18:51.120
things that have some semantic overlap. So I experience red, you experience red, we both

01:18:51.120 --> 01:18:57.040
have different experiences, yet when we talk about them, some kind of overlapping category

01:18:57.040 --> 01:19:03.600
still emerges in the public space. Yes, absolutely. So so what emerges in the public space, that is

01:19:03.600 --> 01:19:09.600
what we can talk about. And that is that is by the way you've set up the experiment is by definition,

01:19:09.600 --> 01:19:13.760
not private, it's public. So of course, we can both talk, we can both point at something that's

01:19:13.760 --> 01:19:17.920
red and say, oh, look, look at that red. And you say, oh, yeah, isn't that isn't it beautiful?

01:19:18.720 --> 01:19:22.320
There, it's manifestly, we're talking, insofar as we're talking and successful in

01:19:22.320 --> 01:19:27.280
communicating with each other and agreeing with each other, then that's the element that is indeed

01:19:27.280 --> 01:19:34.800
public. But are we not sharing, you know, is language not a set of pointers to our simulation?

01:19:34.800 --> 01:19:40.800
So we're simulation sharing when we talk. And even though our simulations are different,

01:19:41.600 --> 01:19:47.120
is the pointer, does the pointer not form some kind of category over all of our simulations?

01:19:47.120 --> 01:19:51.440
Oh, well, there's a whole, you've introduced a whole load of terminology there, which,

01:19:52.160 --> 01:19:58.560
which, you know, I don't know what you mean exactly by shared simulation and so on. So I think

01:19:58.560 --> 01:20:03.360
in the context of a philosophical discussion, as soon as you introduce new, new bits of

01:20:03.360 --> 01:20:07.440
terminology like that, then often that's the point at which you're starting to go wrong,

01:20:07.440 --> 01:20:12.720
right, in philosophical discussions. In technical discussions, of course, you of course,

01:20:12.800 --> 01:20:17.440
we're going to introduce new terminology all the time. But, but, but that's the moment where

01:20:17.440 --> 01:20:22.560
often things are going when, as Wittgenstein would say, you're starting to take language on holiday

01:20:22.560 --> 01:20:26.720
and take it away from its normal usage. So I don't know what you mean by kind of a shared

01:20:26.720 --> 01:20:31.440
simulation, you'd have to tell me a little bit more about that idea before I could engage with

01:20:31.440 --> 01:20:35.840
that thought experiment, I think. Well, I mean, I'm schooled on, you know, the Karl Fristons of

01:20:35.840 --> 01:20:39.920
this world. And there's this whole thing about the Bayesian brain and perception as inference and

01:20:39.920 --> 01:20:45.680
so on. And, you know, the basic idea is that we, you know, our everyday experience is a hallucination,

01:20:45.680 --> 01:20:51.520
you know, we don't, what we experience isn't necessarily what is out there. And language

01:20:51.520 --> 01:20:56.560
is is a kind of pointed to those simulations. And they must be divergent, they presumably

01:20:56.560 --> 01:21:01.040
are divergent, yet miraculously, we can understand each other. Yeah, well, I think that so that so

01:21:01.040 --> 01:21:09.360
the Wittgensteinian point is that we understand each other in so far as in so far as we,

01:21:09.840 --> 01:21:15.040
you know, what we understand is what is shared, right? And anything outside of that is,

01:21:16.400 --> 01:21:22.160
you know, we by definition can't talk about. And the difficulty is that we have this strong

01:21:22.160 --> 01:21:28.720
inclination to talk as if there is this thing that's not shared. I mean, what really fascinates me

01:21:28.720 --> 01:21:33.760
is that understanding it's not a binary, there's a spectrum, and we delude ourselves that we

01:21:33.760 --> 01:21:38.880
understand things deeper than we do, because it goes into the realm of subjectivity. So when I

01:21:39.200 --> 01:21:45.120
understand something, my brain is invoking all of this rich subject of experience. And I'm probably

01:21:45.120 --> 01:21:49.840
taking my understanding into a domain which is beyond which that you understood. And perhaps

01:21:49.840 --> 01:21:54.560
this is just something we willfully do all of the time. So what do you mean exactly by invoke my

01:21:54.560 --> 01:21:58.320
brain is invoking all this subject of experience? What do you what are you what are you getting

01:21:58.320 --> 01:22:04.080
out there? Well, so we talk about, as you say, the language game is based around public information.

01:22:04.080 --> 01:22:10.800
So there is a kind of cultural level, a lowest common denominator of understanding. But when we

01:22:10.800 --> 01:22:18.000
understand cultural artifacts, we further invoke our own subjective experiences. So for example,

01:22:18.000 --> 01:22:24.000
when I laugh, I have the experience of laughter, this phenomenal experience. And this is clearly a

01:22:24.000 --> 01:22:28.800
form of understanding, it's a subjective form of understanding. And when someone else laughs,

01:22:28.800 --> 01:22:34.080
I feel that we are sharing this ontology, right, we're sharing it, but we can't possibly be.

01:22:35.200 --> 01:22:40.480
Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're

01:22:40.480 --> 01:22:45.760
sharing an ontology when you're just talking about an everyday experience of laughing together,

01:22:45.760 --> 01:22:49.440
which we can talk about without any kind of difficulty, and without raising any kind of

01:22:49.440 --> 01:22:54.400
philosophical problems, just by saying, Well, you know, we both heard that that that joke, and we

01:22:54.800 --> 01:22:58.640
were both, you know, on the floor and laughing. It was so funny. It was an excellent joke, right?

01:22:58.640 --> 01:23:03.600
We can talk about that in everyday terms. And and there are no problems. There are no philosophical

01:23:03.600 --> 01:23:08.480
problems. But as soon as you start, start, you know, getting philosophical, and you start talking

01:23:08.480 --> 01:23:13.120
about that, you know, what was it? What was your phrase? There's something about subject

01:23:13.120 --> 01:23:17.600
sheds about subjective ontology or something. Yeah, you're introducing all of this kind of

01:23:17.600 --> 01:23:22.960
technical terminology. And that's that whole, that's a whole layer of confusion on top of our

01:23:23.040 --> 01:23:26.640
ordinary everyday ways of talking about these things, which are unproblematic.

01:23:27.600 --> 01:23:33.760
Okay, but then there's the anthropomorphic lens. So you're a human, we both laugh, the behavior of

01:23:33.760 --> 01:23:38.240
laughing is publicly observable. Therefore, we have the same experience, because we have the same

01:23:38.240 --> 01:23:44.160
behavior. Well, it depends what you mean by by understand here. So so so for sure, you know,

01:23:44.880 --> 01:23:50.080
it is a fairly common form of speech to say, to say, Oh, well, you know, you can never understand

01:23:50.080 --> 01:23:55.120
what it was like to give birth, because you're a man, you know, and this is of course, this is a

01:23:55.120 --> 01:24:04.240
normal way of expressing oneself. And again, that's that sort of unproblematic. So there's

01:24:04.240 --> 01:24:09.520
there is a sense in which, you know, in which that's that's undoubtedly true. But the problems

01:24:09.520 --> 01:24:16.400
arise when you when you start to, to, to think that this, that what underlies this difference

01:24:16.400 --> 01:24:23.840
in understanding or the one underlies that way of talking is is is some kind of, you know,

01:24:23.840 --> 01:24:31.040
inner private realm that is, you know, that is index that is that is metaphysically distinct from

01:24:31.600 --> 01:24:36.240
from the rest of reality. When we share these pointers or these symbols or whatever,

01:24:37.360 --> 01:24:42.720
structure still emerges, we still feel that we have a shared understanding. And that understanding

01:24:42.720 --> 01:24:47.040
can probably be factorized into a public component and a private component. I don't think

01:24:47.040 --> 01:24:52.400
that's kooky to say that. Well, I see, you see, you're very keen to say, well, it turns a little

01:24:52.400 --> 01:24:57.200
bit what you mean by a private component there, right? So if you really mean, you know, sort of

01:24:57.200 --> 01:25:02.800
metaphysically inaccessibly, private and subjective, then then I think, then I think

01:25:02.800 --> 01:25:09.120
it's not appropriate to speak of dividing things into this private and public component. So that's

01:25:09.120 --> 01:25:13.680
where that's where things start to go wrong. And moreover, you insist that you're not a

01:25:13.680 --> 01:25:19.840
dualist, right? But I think your inclination to make that division shows that you have dualistic

01:25:19.840 --> 01:25:24.880
inclinations, as we all do. So people who are denying that they're dualists, they're denying

01:25:24.880 --> 01:25:31.280
this that little seed of dualism that I think is in all of us. And that is part of our part of the

01:25:31.280 --> 01:25:35.760
way we, we, we, you know, we think and the part of the way you naturally go when you start to do

01:25:35.760 --> 01:25:40.480
philosophy. And so it's all, I think it's all very well to say, oh, you know, I'm a materialist

01:25:40.480 --> 01:25:45.680
and I don't, but then when you, when you start to kind of probe and you start to discover the

01:25:45.680 --> 01:25:51.040
puzzlement that these things give rise to, then that exposes a bit of latent dualism there. Now

01:25:51.040 --> 01:25:55.360
overcoming that latent dualism, that is the real challenge that Wittgenstein confronts.

01:25:55.360 --> 01:26:01.200
Well, I love the challenge. So the way I see it is there is, there's a ladder. So at the top,

01:26:01.280 --> 01:26:06.000
you have an experience which is ineffable. And then one step down, you have an experience which

01:26:06.000 --> 01:26:10.240
is inconceivable, which is Naples argument. And then the, you know, if you really go down the ladder,

01:26:10.240 --> 01:26:14.480
then you get into this metaphysical dualism. So I guess I'm somewhere between the first step

01:26:14.480 --> 01:26:20.400
and the second step. So I think if I have a certain type of experience, I simply don't find the words,

01:26:20.400 --> 01:26:25.120
I can't communicate it to you. But if you put probes in my brain or something like that, I'm

01:26:25.120 --> 01:26:31.520
sure that could conceivably be a way of measuring it. Yes. Yeah. So, so, so this is really important.

01:26:31.520 --> 01:26:36.960
So for me, what counts as public is not just behavior, but it's also whatever scientists

01:26:36.960 --> 01:26:42.400
we can discover. So that, so, so if we poke around in people's brains and we do EEG recordings and

01:26:42.400 --> 01:26:48.240
FLRI recordings and anything else that we can imagine. And then I as a scientist can see this

01:26:48.240 --> 01:26:53.440
stuff and use a scientist and our fellow scientists will see, see that. That's public too. So that's

01:26:53.440 --> 01:26:58.320
in the, for the purposes of this discussion, of this philosophical discussion, that's all in the

01:26:58.320 --> 01:27:04.080
public realm. It's not metaphysically hidden. You can, you can, and all of that can feed into the

01:27:04.080 --> 01:27:08.720
way we talk about consciousness. And especially if we're talking about exotic entities, then,

01:27:09.520 --> 01:27:17.040
then all of that can feed into the way our language adapts to, to, to, to, to our encountering them.

01:27:17.040 --> 01:27:21.360
Yeah. So I think it's fascinating to decompose as you just did what people mean by subjectivity. So

01:27:21.360 --> 01:27:26.880
of course, some people like David Chalmers, they argue that there is a little bit extra. So there's,

01:27:26.880 --> 01:27:31.200
you know, behavior function and dynamics. And then there's that, you know, little bit extra,

01:27:31.200 --> 01:27:36.480
which is not observable in any scientific way. And I think, you know, it's fair to say a lot of

01:27:36.480 --> 01:27:40.400
people when they talk about subjectivity, they're not talking about the little bit extra. But when

01:27:40.400 --> 01:27:43.760
we do get to the little bit extra, I completely agree with you, we've got a big problem.

01:27:44.400 --> 01:27:50.560
Yeah. Yeah, I think we have got a big problem because, because of our natural, you know, dualistic

01:27:50.560 --> 01:27:57.360
tendencies to, it's very, very difficult to think that, that, that, you know, if I experience a pain,

01:27:57.920 --> 01:28:02.560
that, that, that there isn't something about that that is just purely minor, that you couldn't,

01:28:02.560 --> 01:28:07.680
you know, the outside world, that other people can never really, you know, experience it. But

01:28:07.680 --> 01:28:12.800
that's, it's having that thought, that's this moment that you kind of go wrong, but it's natural

01:28:12.800 --> 01:28:17.520
path to go down. It's really, really hard to avoid it. And, and that's where I think

01:28:17.600 --> 01:28:24.560
Sylvitkenstein's remarks, they, they provide a whole way of, of trying to reorient your whole

01:28:24.560 --> 01:28:30.720
way of thinking. And, and, and if you sort of really kind of grasp them, it sort of flips your

01:28:30.720 --> 01:28:34.960
whole world around, it flips your whole way of thinking around. So it's so that the, this whole

01:28:34.960 --> 01:28:40.240
way of talking and thinking becomes wrong. So it's so that so very often the strategy when

01:28:40.240 --> 01:28:44.640
you're dealing with this is somebody throws out this thought at you, like you've been throwing

01:28:44.640 --> 01:28:49.920
out various thoughts at me about, and, and, and often buried in the way those thoughts are framed

01:28:49.920 --> 01:28:54.480
is the problem. So, so the, the problem is the very expression of those thoughts. And you have

01:28:54.480 --> 01:28:58.080
to take a step back and say, hang on a minute, you know, you made this funny move, you introduced

01:28:58.080 --> 01:29:02.480
this funny bit of language, you introduced this funny way of expressing things. And that's, that's

01:29:02.480 --> 01:29:08.800
when that's the, the, Victor Stein has this phrase that is, that's where the conjuring trick

01:29:08.800 --> 01:29:13.360
happens is where you, the point that you don't notice is where the conjuring trick happens.

01:29:14.160 --> 01:29:19.680
So, so, so it's kind of, so often you have to take, take a step back and you have to sort of say,

01:29:19.680 --> 01:29:23.280
hang on a minute, I don't accept that way of talking that you've just suddenly introduced,

01:29:23.280 --> 01:29:28.800
which is going down a philosophical garden path. Yes, and I completely agree. So, so that is,

01:29:29.360 --> 01:29:32.880
that is a form of dualism, you know, when, when we resort to that little bit extra.

01:29:33.440 --> 01:29:38.080
And I'm quite interested in this actually, because people like Chalmers, I don't think he likes the

01:29:38.080 --> 01:29:42.800
term dualist, I think it's a property dualist, but he does talk about the philosophical zombie,

01:29:42.800 --> 01:29:46.480
which is a thought experiment of something which has all of the behavior of us, but

01:29:46.480 --> 01:29:51.280
is lacking in conscious experience, which gives rise to this idea that it's almost a kind of

01:29:51.280 --> 01:29:55.360
epiphenomenon or it's something which, you know, almost you're asking the question, well,

01:29:55.360 --> 01:29:59.920
well, what's it doing if it's not affecting anything? And when I read your conscious Exotica

01:29:59.920 --> 01:30:04.560
article, I had a similar thought actually, because you showed this linear correlation between, you

01:30:04.640 --> 01:30:13.520
know, human likeness and consciousness. And then you gave examples of algorithms,

01:30:13.520 --> 01:30:17.200
you know, like AlphaGo, for example, and they didn't need the consciousness.

01:30:17.200 --> 01:30:21.760
And that again raises the question of, what is the cash value of consciousness?

01:30:21.760 --> 01:30:26.880
When we use, when we're using the word consciousness, then often we are using it in the

01:30:26.880 --> 01:30:34.960
context of certain, you know, of certain behavioral behavior and behavioral inclinations,

01:30:34.960 --> 01:30:41.600
and we use it in the context of other humans and other animals. And there's a whole,

01:30:41.600 --> 01:30:46.720
I mean, for a start, the word consciousness is actually, you know, it's a multifaceted concept

01:30:46.720 --> 01:30:52.080
that it's alluding to many things. And one of the things that it is alluding to is our ability

01:30:52.080 --> 01:30:58.000
to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice

01:30:58.000 --> 01:31:04.320
the chair, that's why I bumped into it or something. And, or, you know, I didn't see,

01:31:05.600 --> 01:31:10.480
you know, that there was a desk over there that might have had something interesting inside it,

01:31:10.480 --> 01:31:15.760
if you opened it up. And so we talk about our awareness of the world. And we're at the same

01:31:15.760 --> 01:31:21.360
time, we're talking about an aspect of consciousness, and we're talking about

01:31:22.720 --> 01:31:27.600
a whole load of behavioral dispositions and capabilities. And so these things are very much,

01:31:28.320 --> 01:31:33.680
you know, are very much related to each other in our everyday speech. So then the question arises,

01:31:33.680 --> 01:31:40.480
though, are they dissociable? And so now it's very important that it's not like I think there's,

01:31:40.480 --> 01:31:45.760
that consciousness is some metaphysical thing whose essence is out there to be discovered.

01:31:45.760 --> 01:31:52.320
It's just, it's a concept that we invent and a word that we use to describe the world around us

01:31:52.320 --> 01:31:58.480
and our place in it and each other and so on. And so, so, so then, you know, then the question is,

01:32:00.720 --> 01:32:06.720
are there things that we might create or imagine, where we'd want to use the one concept, we want

01:32:06.720 --> 01:32:10.320
to use the one set of words and not use the other. And that is what the question comes down to.

01:32:10.320 --> 01:32:14.480
So in the case of something like AlphaGo, then I think, you know, we're all kind of agree that it's

01:32:14.480 --> 01:32:19.920
actually there's a kind of cognition going on there, there's a kind of reasoning going on in

01:32:19.920 --> 01:32:24.000
AlphaGo. There's certainly a lot of kind of cleverness, there's a kind of intelligence,

01:32:24.000 --> 01:32:28.960
there's even, if we're thinking about move 37, a kind of creativity. So we're willing to use

01:32:28.960 --> 01:32:33.920
all of those words, but nobody is going to suggest that AlphaGo is conscious. So there we can see

01:32:34.000 --> 01:32:38.480
that they're under certain conditions, the concepts are dissociable. But nevertheless,

01:32:38.480 --> 01:32:43.680
there's a strong relationship between the two, because if we think about animals, then often we

01:32:43.680 --> 01:32:48.800
are going to, we're going to use their cognitive abilities as manifest in their sophisticated

01:32:48.800 --> 01:32:53.760
behavior. We're going to use that as a proxy for sometimes whether we want to talk about them in

01:32:53.760 --> 01:32:59.040
terms of consciousness. So sometimes, in our usage, we're going to bundle the things together,

01:32:59.040 --> 01:33:03.280
and sometimes we're not. But this is all just a matter of, it's a kind of a practical matter

01:33:03.360 --> 01:33:08.240
of how we use language and how it's usefully deployed, how language is usefully deployed.

01:33:08.240 --> 01:33:12.400
And it's not about discovering some metaphysical entity that's out there,

01:33:12.400 --> 01:33:15.120
which is what conscious, the word consciousness denotes.

01:33:15.120 --> 01:33:19.840
Demis Esalvis recently spoke about this ladder of creativity and of course,

01:33:19.840 --> 01:33:26.080
inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad

01:33:26.080 --> 01:33:31.600
I had him on the podcast actually. He's a huge hero of mine, and I believe that a hero of yours

01:33:32.560 --> 01:33:36.960
but he coined this term the intentional stance. And what's interesting is he was

01:33:37.600 --> 01:33:42.720
using it to designate a rational agent, but actually it gets overloaded and I'm guilty of

01:33:42.720 --> 01:33:47.200
this. You overload it for lots of things, including even for things like consciousness. And maybe

01:33:47.200 --> 01:33:53.360
that's because of the correlates of cognition, these things are very closely related. But

01:33:53.360 --> 01:33:56.400
can you explain in your own articulation the intentional stance?

01:33:56.480 --> 01:34:05.200
Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance

01:34:05.200 --> 01:34:10.160
without necessarily embracing the whole of everything that Dan Dennett was talking about

01:34:10.160 --> 01:34:16.160
in that context. Because for him, there's a whole big philosophical position around it,

01:34:16.160 --> 01:34:21.200
but there's a very simple sense of the intentional stance that we can lift from Dan without

01:34:21.200 --> 01:34:27.520
necessarily buying into everything that he said. And it's simply to say that we often in everyday

01:34:27.520 --> 01:34:37.120
terms speak about artifacts and indeed animals, you know, other animals, as if they were rational

01:34:37.120 --> 01:34:45.680
agents that act on the basis of what they believe and what they want. And by talking about them in

01:34:45.680 --> 01:34:52.720
those ways, whether they really, whatever that means, do believe things or have desires, it's

01:34:52.720 --> 01:34:57.440
useful for explaining and understanding their behavior. So if we adopt the intentional stance,

01:34:57.440 --> 01:35:03.200
say, to use one of Dan Dennett's own examples towards a chess machine, a chess computer,

01:35:03.760 --> 01:35:11.920
chess program, or Go program, and we might say, oh, it advanced its queen because it wants to try

01:35:11.920 --> 01:35:18.160
and pin down my rook. And this is just a natural way of speaking. And if we use that way of speaking,

01:35:18.160 --> 01:35:27.440
then it's just good in every way because we can then discuss among ourselves what the machine

01:35:27.440 --> 01:35:31.120
is doing, we can explain what it's doing, we can predict what it's going to do. So that's

01:35:31.120 --> 01:35:36.320
taking the intentional stance. And it doesn't necessarily bring with it a belief that these

01:35:36.320 --> 01:35:40.080
concepts are literally applicable. Maybe they are, maybe they're not.

01:35:40.880 --> 01:35:45.200
But this is where it gets interesting. So I discussed abduction, actually, when I spoke with

01:35:46.080 --> 01:35:49.920
Dan, because it's very closely related. I know you studied reasoning for many years.

01:35:49.920 --> 01:35:53.600
And the way I see it, when we adopt the intentional stance, what we're doing is we're

01:35:53.600 --> 01:35:59.520
kind of building a set of variables to describe the behavior of the entity. And you were just

01:35:59.520 --> 01:36:04.320
making the argument from the lens of Wittgenstein that the behavior is the only thing.

01:36:05.040 --> 01:36:14.160
No, no, no, no, no, no, no, no. I definitely don't think that we mean meant by behavior

01:36:14.160 --> 01:36:21.920
as the only thing. But certainly when we're deploying psychological terms, I don't think

01:36:21.920 --> 01:36:27.600
that in any sense behavior is the only thing that determines how we deploy psychological terms.

01:36:28.640 --> 01:36:32.800
You're absolutely right. So there's still a massive amount of ambiguity. So when we perform

01:36:32.800 --> 01:36:39.040
abduction, we are creating a hypothesis and we're selecting out of an infinite set of possible

01:36:39.040 --> 01:36:46.720
hypotheses. But the behavior gives us all of the information. So it's almost like if we knew

01:36:46.720 --> 01:36:51.520
how to create the correct explanation, we wouldn't be missing anything just by observing the behavior.

01:36:51.520 --> 01:36:56.160
So what are we talking about now? We're talking about chess machines or animals?

01:36:56.160 --> 01:36:59.200
Or what are we talking about? What's the context for this thought?

01:36:59.280 --> 01:37:04.240
I guess it could work for both. So let's say I want to adopt the intentional stance for move 37.

01:37:04.240 --> 01:37:10.720
And I do this abduction. So I build this plan that the agent had. So the agent had this intention

01:37:10.720 --> 01:37:15.760
and it took this sequence of steps. And I'm using that as a hypothesis to explain the behavior.

01:37:15.760 --> 01:37:20.400
I'm adopting the intentional stance. But it's still highly ambiguous because I'm selecting

01:37:20.400 --> 01:37:24.720
out of an infinite set of possible hypotheses. Right. And in fact, in that particular case,

01:37:24.720 --> 01:37:31.840
it's almost certainly not the right way of thinking about it at all. Because unlike humans,

01:37:31.840 --> 01:37:36.560
who are when they're playing these games often do form plans. So if you're playing chess,

01:37:36.560 --> 01:37:41.440
you often do have a plan, I'm going to try and capture this area of the board and command this

01:37:41.440 --> 01:37:46.400
area of the board say. And so I'm going to move these pieces around to try and do that. And you

01:37:46.400 --> 01:37:52.880
might form a plan in terms of several moves, but ahead. But typically that's not the way,

01:37:52.880 --> 01:37:58.880
that's not really the way AlphaGo works. So talking about it making plans isn't really the

01:37:58.880 --> 01:38:05.040
right way of doing things. So it's interesting, actually, because the intentional stance,

01:38:05.040 --> 01:38:14.000
you know, maybe it's still white work, you can still talk about something forming plans maybe,

01:38:14.000 --> 01:38:20.080
but it's really not quite right in that case. When I say subjectivity, I'm not talking

01:38:20.160 --> 01:38:26.480
about metaphysical or dualism, but the intentional stance clearly is a form of subjectivity.

01:38:26.480 --> 01:38:33.520
And when we as a diverse collection of agents form our own intentional stances, it would seem to be

01:38:34.160 --> 01:38:39.760
quite a chaotic, weird and wonderful thing, yet it seems to work. There seems to be,

01:38:39.760 --> 01:38:45.440
by the way, an interesting thing here is the way we ascertain agency and culpability is based on

01:38:45.440 --> 01:38:49.360
the intentional stance. So you read a news article about someone being stabbed in Australia or

01:38:49.360 --> 01:38:54.480
something like that. And the news article was trying to give reasonable explanations. Oh,

01:38:54.480 --> 01:38:58.480
it was because he was in a cult or it was because he was religious or it was because,

01:38:58.480 --> 01:39:03.120
and this helps us kind of assign moral valence to what just happened.

01:39:03.120 --> 01:39:07.440
Yeah, yeah, absolutely. Yeah. You said something like, when we take the intentional stance that

01:39:07.440 --> 01:39:11.120
that is a form of subjectivity or something? Yes, would you agree with that?

01:39:12.160 --> 01:39:16.960
So I wouldn't put it quite that way. I'm not quite sure exactly what you mean by that,

01:39:17.680 --> 01:39:22.080
but taking the intentional stance is, I think it's just adopting a certain terminology and a

01:39:22.080 --> 01:39:26.880
certain vocabulary for describing the behavior of something. So I don't think we need to bring

01:39:26.880 --> 01:39:34.080
subjectivity into that at all, right? So I think maybe we're mixing up two completely different

01:39:34.080 --> 01:39:37.440
senses of the word subjectivity here, which is something we should be very careful about.

01:39:37.440 --> 01:39:43.360
So I think you mean subjectivity, you mean that you've just made your own choice between

01:39:43.440 --> 01:39:47.360
different hypotheses. And so it's subjective. Is that, is that what you mean there?

01:39:47.360 --> 01:39:51.680
Yes. So let's say, so for me as an observer, I might do some, let's call it probabilistic

01:39:51.680 --> 01:39:58.560
reasoning. And for me, the most reasonable, rational explanation is this. And it's a for me

01:39:58.560 --> 01:40:02.960
there. That's what I mean. That's what you're alluding to the subjectivity. Yeah. Yeah. Yeah.

01:40:02.960 --> 01:40:11.040
Okay. Yeah. Well, so, so, so your for me is, I think that's that, that the sense in which

01:40:11.040 --> 01:40:15.680
that subjective is a very, very different one from the topic that we were talking about earlier on,

01:40:15.680 --> 01:40:20.400
because I don't think there's anything philosophically problematic in, in, in saying that,

01:40:20.400 --> 01:40:26.160
you know, that I made my choice. And that's my preference and so on. And so, and somebody might

01:40:26.160 --> 01:40:30.000
say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically

01:40:30.000 --> 01:40:34.800
problematic about that, right? So, so they, but earlier on, we were talking about subjectivity,

01:40:34.800 --> 01:40:40.400
which like the big capital S and where it's alluding to kind of something whole metaphysical

01:40:40.400 --> 01:40:46.960
thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different

01:40:46.960 --> 01:40:52.000
kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is

01:40:52.000 --> 01:40:57.200
dualism and, and lowercase subjectivity is for me. Yeah, it's for me. Okay. Yeah. Okay.

01:40:59.040 --> 01:41:02.080
Can you tell me about the risks of anthropomorphisation?

01:41:02.640 --> 01:41:08.800
Yeah. So I think so in the context of, of contemporary artificial intelligence in particular,

01:41:09.680 --> 01:41:15.520
then, then the danger of anthropomorphism, I think, is in, is in thinking that,

01:41:16.480 --> 01:41:20.160
that a system such as a large language model, you know, a chatbot or something,

01:41:20.160 --> 01:41:24.720
thinking that it has capabilities and that it doesn't, that's as simple as that.

01:41:24.720 --> 01:41:30.560
Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So, so in,

01:41:30.560 --> 01:41:35.840
so in both cases, I think we can go wrong, we can go wrong by, because they exhibit very human,

01:41:35.840 --> 01:41:41.520
like linguistic behaviour, we can just assume that they are going to be very human like in

01:41:41.520 --> 01:41:45.280
general in all of the rest of the behaviour that we encounter with them. But we often find that

01:41:45.280 --> 01:41:51.200
that's not the case. So we can find that at one moment, a large language model might make a

01:41:51.200 --> 01:41:56.240
ridiculously stupid mistake that no child would make. And, and, and, and then the next moment,

01:41:56.240 --> 01:42:02.240
it's saying something extraordinarily profound philosophically, or, or summarising some,

01:42:02.240 --> 01:42:08.640
in, you know, enormously difficult scientific article, you know, really accurately. So, which

01:42:08.640 --> 01:42:12.400
is, so these things are kind of superhuman powers, or translating something into four

01:42:12.400 --> 01:42:16.640
different languages all at once. And they're, they're sort of superhuman-ish capabilities.

01:42:17.520 --> 01:42:21.680
So it's not, so it can actually be more than better than human in some directions.

01:42:22.480 --> 01:42:27.760
And, but, but clearly very deficient in others, you know, with contemporary models that can make

01:42:27.760 --> 01:42:33.920
all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say

01:42:33.920 --> 01:42:39.600
daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake

01:42:39.600 --> 01:42:44.880
to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human,

01:42:44.880 --> 01:42:49.920
you know, because you can just, you can just misjudge it in many ways. That's one thing. There's

01:42:49.920 --> 01:42:54.000
another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive

01:42:54.000 --> 01:42:58.720
capabilities, if you like. But there are other problems with anthropomorphism. So if we see

01:43:00.000 --> 01:43:07.040
empathy there, where there isn't real empathy, then we may trust something where there's no

01:43:07.040 --> 01:43:13.440
real basis for trust. And so that's a problem as well. You know, we may form, people may form

01:43:13.440 --> 01:43:21.360
relationships with, with, you know, AI companions and social AI, where they're kind of fooling

01:43:21.440 --> 01:43:26.080
themselves into thinking that there's a basis for that in emotion, where there is in humans,

01:43:26.080 --> 01:43:32.640
and it's not there in, in, in contemporary AI. And I think that all those things are problematic.

01:43:32.640 --> 01:43:38.640
So things to do with trust, to do with friendship and empathy, and all of these, these things,

01:43:38.640 --> 01:43:45.120
I think, where we can go wrong in seeing, seeing them as too, you know, as more human-like than

01:43:45.120 --> 01:43:50.720
they really are. One of the issues I have with anthropomorphism is that people ascribe mental

01:43:50.720 --> 01:43:55.920
content when it's not there. And I think you are talking about literal anthropomorphism,

01:43:55.920 --> 01:44:02.640
which is that they see human-like qualities when they are not there. And to me, that, that's an

01:44:02.640 --> 01:44:08.880
important distinction, because I think if I understand you correctly, you, I mean, you're

01:44:08.880 --> 01:44:12.400
very known nonsense. You say that current large language models, they don't reason, they don't

01:44:12.400 --> 01:44:16.560
form beliefs, they don't have a sense. Oh, no, I didn't say exactly that. Oh, did you not? That's

01:44:17.280 --> 01:44:21.760
that's a broad, that's a much broader claim. So that's right. So I'm not sure I'd go so far as to

01:44:21.760 --> 01:44:28.320
say they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what

01:44:28.320 --> 01:44:34.240
my, my, my approach is to say that we should be very cautious in using those terms. So I'm, so

01:44:34.240 --> 01:44:38.400
those would all be examples of taking the intentional stance. If we were to use those

01:44:38.400 --> 01:44:44.640
bits of terminology to describe what, what a current, you know, chatbot or conversational AI

01:44:44.640 --> 01:44:50.000
was doing, we'd be taking the intentional stance, and it's perfectly reasonable to do that in very,

01:44:50.000 --> 01:44:55.440
very many cases. So I would know, and I would never make the blanket claim, they don't understand.

01:44:55.440 --> 01:45:00.560
I think that that's not quite right. I think rather, rather it's that, you know, sometimes

01:45:00.560 --> 01:45:04.720
it's appropriate to say, oh, yeah, it seems to understand very well what this big long article

01:45:04.720 --> 01:45:08.800
about nuclear physics was, was all about. And it summarized it really well. It really understood

01:45:08.800 --> 01:45:12.480
it. You know, somebody might come up, might say, you know, it really did seem to understand it.

01:45:12.720 --> 01:45:19.520
I think I wouldn't say that they were wrong in using that phrase there. So, but then on another

01:45:19.520 --> 01:45:25.760
occasion, you might find that it's, that it, for example, recently, people have been posing this,

01:45:25.760 --> 01:45:32.160
you know, this classic goat, cabbage, Fox problem where you've got a boat and you have to cross a

01:45:32.160 --> 01:45:36.640
river with a goat, and you can't have the, you know, more than two things in the boat at once,

01:45:36.640 --> 01:45:39.600
and you can't have the goat with the cabbage and all this kind of stuff. And so there's a,

01:45:39.600 --> 01:45:44.480
it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery,

01:45:44.480 --> 01:45:51.120
right? And people opposed, opposed to it to some large language models that said, okay,

01:45:51.120 --> 01:45:55.680
I've got a boat and a cabbage and I need to get the cabbage to the other side of the river.

01:45:55.680 --> 01:46:00.960
How do I do it? And the large language model models, several of them just start to come up with

01:46:00.960 --> 01:46:05.040
this totally baroque solution that involves going backwards and forwards over there. And sometimes

01:46:05.040 --> 01:46:09.440
they invent goats that aren't even in the, and that's because they've overfitted or they're kind

01:46:09.440 --> 01:46:15.920
of like connected with this classic problem. And, and so no child would make this stupid,

01:46:15.920 --> 01:46:20.400
stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously

01:46:20.400 --> 01:46:25.280
just didn't understand what the, you know, and of course, it's quite right to say didn't understand

01:46:25.280 --> 01:46:32.160
in that case. And the anthropomorphism, it comes about when you think that it understands

01:46:32.160 --> 01:46:38.000
when we understand and doesn't understand when we don't understand. The reality is that sometimes

01:46:38.000 --> 01:46:43.280
it understands when we understand and sometimes it won't and some, it's all mixed up, right? So

01:46:43.280 --> 01:46:49.600
the mistake is to think that it is like us. I could, yeah, man, that was beautifully articulated.

01:46:49.600 --> 01:46:55.840
So it's, it's, it's the mistake of thinking there is an alignment both in how the machines think

01:46:55.840 --> 01:47:00.560
and where they make mistakes. But let's unpick this a little bit because you were saying it's

01:47:00.560 --> 01:47:05.760
perfectly reasonable to take the intentional stance when the thing does the thing correctly,

01:47:05.760 --> 01:47:12.080
even though it thinks differently to us. And that's absolutely fine. But I love thinking

01:47:12.080 --> 01:47:16.800
about these things theoretically. And it's, it's delicious talking to you because you have a background

01:47:16.800 --> 01:47:22.560
in symbolic AI. You were, I'm sure around in the days of photo and pollution with their connectionist

01:47:22.560 --> 01:47:29.040
critique. And, and even now there are clear examples of language models not being able to do

01:47:29.040 --> 01:47:34.640
negation. Oh, yeah. And we know they're not Turing machines. You know, we can make some strong

01:47:34.640 --> 01:47:40.000
theoretical statements that they are limited in reasoning. I agree with you that it's reasonable

01:47:40.000 --> 01:47:44.880
to say they understand in certain circumstances. But, but, but where I want to get to is, okay,

01:47:44.880 --> 01:47:50.080
so we agree that language models cannot perform certain types of reasoning that we can.

01:47:50.080 --> 01:47:55.120
Yeah. So I think we could, so I think we need to take each of these concepts individually. So,

01:47:55.600 --> 01:48:00.800
we dealt a little bit just now with understanding reasoning as a whole separate thing. So, and,

01:48:00.800 --> 01:48:04.240
again, this is all because, you know, they're not like us. So we have to deal with these things

01:48:04.240 --> 01:48:07.520
individually. We can't just blanket say, oh, they don't understand, they don't reason, they don't,

01:48:07.520 --> 01:48:11.120
or they do understand, they do reason. It's not like that. It's, you have to take each of these

01:48:11.120 --> 01:48:17.120
concepts separately. So in the case of reasoning, then clearly today's large language models,

01:48:17.120 --> 01:48:23.040
you know, do struggle very often with, with, with reasoning problems. Now, this is a kind of open

01:48:23.040 --> 01:48:30.720
research problem. And people are making a lot of progress in improving their ability to solve

01:48:30.720 --> 01:48:36.400
reasoning problems. Now, what the right approach to that is, you know, is an open research question.

01:48:36.400 --> 01:48:40.720
Maybe it's just you throw more training data at it with, with, with, that includes lots of

01:48:40.720 --> 01:48:45.760
reasoning problems. And then eventually you get sufficient generalization there. And it's not

01:48:45.840 --> 01:48:50.640
totally clear that that will work, but maybe it will. Maybe it's to embed your,

01:48:52.640 --> 01:48:57.520
your, or, or, or to surround it to include it in your, in your system, not just the large

01:48:57.520 --> 01:49:02.960
language model, but making kind of external calls to, to say a planner or some kind of external

01:49:02.960 --> 01:49:06.720
reasoning system. And you bring that in and you incorporate, you make something that's kind of

01:49:06.720 --> 01:49:11.520
a hybrid that uses that, that kind of more symbolic approach. So you might do that.

01:49:12.480 --> 01:49:20.800
Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced

01:49:20.800 --> 01:49:25.600
this selection inference framework where you basically, you treat the large language model

01:49:25.600 --> 01:49:30.880
as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes

01:49:30.880 --> 01:49:36.800
calls to the, to, to, to, to the, to the module in a kind of algorithm that does a number of

01:49:36.800 --> 01:49:40.800
sequence of reasoning steps. So you have a kind of outer algorithm. So there's lots of ways of

01:49:40.880 --> 01:49:46.080
trying to tackle that problem. But yeah, just your basic large, take your basic large language

01:49:46.080 --> 01:49:52.960
model today, as they are at the moment, and you put it in a chat interface, it's easy to find

01:49:52.960 --> 01:49:59.360
reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host,

01:49:59.360 --> 01:50:05.600
Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or

01:50:05.600 --> 01:50:11.200
achieve a goal. And he cites Claude Shannon, by the way, he said, Claude, Claude Shannon said,

01:50:11.200 --> 01:50:15.360
we may have knowledge of the past, but we cannot control it. We may control the future,

01:50:15.360 --> 01:50:19.840
but we have no knowledge of it. And science leverages control to gain knowledge, engineering

01:50:19.840 --> 01:50:24.800
leverages knowledge to gain control. And reasoning is the effective computation in both. You know,

01:50:24.800 --> 01:50:29.600
maybe just in your own articulation, because we can cite examples of things like abduction,

01:50:29.600 --> 01:50:37.600
which we've studied in great detail. And it feels like at the moment, even if we do farm out to

01:50:37.600 --> 01:50:43.040
Turing machine algorithms, in the days of symbolic AI, that was an intractable problem,

01:50:43.040 --> 01:50:49.840
because we've got this infinity, right? And it still seems to me that there are some problems,

01:50:49.840 --> 01:50:55.760
which there is no easy answer to. So I have a, I feel that you're alluding to

01:50:55.760 --> 01:51:03.440
Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean,

01:51:03.440 --> 01:51:09.760
I think that sort of abductive problems are where we are looking for an explanation for something.

01:51:10.800 --> 01:51:16.880
So I think you'll find that there's a large collection of abduction problems that you could

01:51:16.880 --> 01:51:21.360
just present to today's large language models, and they would do quite well at them, you know.

01:51:21.360 --> 01:51:24.480
And at the same time, I'm sure you wouldn't be too difficult to find problems,

01:51:24.480 --> 01:51:32.240
especially if they involve many steps where it will go wrong. Because doing multi-step,

01:51:34.240 --> 01:51:40.240
it's the multiple steps that really are where today's large language models are a bit weak.

01:51:42.400 --> 01:51:47.200
Again, it's an open research question. People are working on all that kind of thing. So if there

01:51:47.200 --> 01:51:53.840
are multiple steps, and step n is dependent on step n minus one, and it's inherently,

01:51:53.840 --> 01:51:59.600
they're inherently very computational sorts of things in that sense. So large language models

01:51:59.600 --> 01:52:04.880
are a bit weak at that, for sure. And we can introduce things like chain of thought and so on

01:52:04.880 --> 01:52:11.520
to try and improve that. But they're sort of a limited success. But my feeling is that those

01:52:11.520 --> 01:52:17.440
are not the things where which large language models themselves are inherently strong at.

01:52:17.440 --> 01:52:24.560
And you should probably make use of other tools in order to kind of boost reasoning capabilities.

01:52:24.560 --> 01:52:29.600
Like I listed some of them. So somebody's making external calls to reasoning,

01:52:30.800 --> 01:52:35.120
dedicated reasoning components. Sometimes it's embedded in the large language model

01:52:35.120 --> 01:52:38.880
in a reasoning algorithm itself. So you can go either way, you can either take the large

01:52:38.880 --> 01:52:43.840
language model, put reasoning things inside it as it were, so it makes external calls,

01:52:43.840 --> 01:52:47.600
or you do it the other way around, you can have a reasoning algorithm and make the large

01:52:47.600 --> 01:52:52.320
language model component of the reasoning algorithm. Yes, I suppose it's a similar thing to the

01:52:52.320 --> 01:52:57.920
creativity that there's a kind of creative or inventive abduction, and then there's colloquial

01:52:57.920 --> 01:53:02.640
abductive interpolation. And the remarkable thing is just how structured and predictable our world

01:53:02.640 --> 01:53:07.040
is and how far you can get with the colloquial predictive abductive. Yes, yeah. I mean, this

01:53:07.040 --> 01:53:13.760
way you speak of colloquial sort of abduction, which is the kind of thing that we can

01:53:13.760 --> 01:53:21.280
all and the person on the street could do if you have some slightly odd situation and you say,

01:53:21.280 --> 01:53:29.440
why is this man in the middle of the road with a policeman's hat on or something?

01:53:33.040 --> 01:53:36.320
And you say, well, because there's maybe there's been an accident or something.

01:53:37.440 --> 01:53:41.600
So we do all this kind of thing on an everyday basis. It's a little bit of abduction. It's what

01:53:41.600 --> 01:53:45.360
you call colloquial abduction, I think. But large language models think you find a pretty good at

01:53:45.360 --> 01:53:48.960
that kind of thing these days. But if you have something that's really complex and has a load

01:53:48.960 --> 01:53:54.320
of steps to it, the kind of thing that humans would struggle at, then very often large language

01:53:54.320 --> 01:53:59.280
models are going to struggle at those things too. Can you describe the Turing test? The Turing test?

01:53:59.280 --> 01:54:09.600
Yes. Okay. So I'll describe the Turing test as it's popularly conceived because there are

01:54:09.600 --> 01:54:14.000
new answers in Turing's original paper. But then again, he didn't call it the Turing test.

01:54:16.000 --> 01:54:23.520
So the Turing test as it's popularly conceived involves having a human judge

01:54:23.520 --> 01:54:30.800
and the human judge is interacting with two things. One of them is a human and the other is a computer

01:54:30.800 --> 01:54:37.840
system. And the interaction is entirely through language, through a keyboard and a screen say,

01:54:37.840 --> 01:54:45.040
or a teletype, if you like, in Turing's day. And the idea is that the human judge has a conversation

01:54:45.040 --> 01:54:51.840
with these two things. And the question is, can the human judge tell which is the machine and

01:54:51.840 --> 01:55:00.000
which is the human? And if the judge can't tell which is which, then the machine passes the Turing

01:55:00.000 --> 01:55:06.640
test. Do you think it's a good measure of intelligence? I think it's a pretty rubbish

01:55:06.640 --> 01:55:15.360
measure of intelligence. I mean, I think it's a very useful philosophical thought experiment

01:55:15.360 --> 01:55:20.080
and starting point for conversation on this. But the trouble is, it's very easy to game.

01:55:20.080 --> 01:55:23.440
Well, there's a number of problems with that, which people have been writing about for years,

01:55:23.440 --> 01:55:30.960
by the way. So there are a number of problems with that. So one is that it's easy to game,

01:55:31.040 --> 01:55:36.640
in a sense, because there's a temptation to make something to pass the Turing test,

01:55:36.640 --> 01:55:41.920
you make something that has all kinds of strange human ticks and peculiarities,

01:55:41.920 --> 01:55:48.480
and then it seems human. And so you can fool the judge that way by making it just a bit eccentric,

01:55:49.520 --> 01:55:54.320
which is obviously nothing to do with intelligence at all. So that's one thing.

01:55:54.320 --> 01:56:00.240
And then another thing is that the domain of the test is purely linguistic. So you're not

01:56:00.320 --> 01:56:05.760
testing anything to do with the sorts of intelligence that you get in a non-human animal,

01:56:05.760 --> 01:56:11.520
say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to

01:56:11.520 --> 01:56:17.280
navigate the ordinary, everyday world and survive in it. But none of those kinds of

01:56:17.280 --> 01:56:24.240
intelligence are tested by the Turing test. So you were the scientific advisor on the film

01:56:24.960 --> 01:56:28.000
Ex Machina. And I'm not sure whether it's Machina or Machina.

01:56:28.720 --> 01:56:33.040
It is Machina, yeah. I was speaking with Irina Rishan and she said,

01:56:33.040 --> 01:56:40.000
Ex Machina. But anyway, she's right. Yes, she is. I digress. But there was a special

01:56:40.000 --> 01:56:44.160
type of Turing test in that film. Can you explain that?

01:56:44.160 --> 01:56:50.080
Well, indeed it wasn't. It's not the Turing test. So there's a point in the film, I assume,

01:56:50.080 --> 01:56:55.600
that people are vaguely familiar with the setup of the film. But there's a point in the film where

01:56:55.600 --> 01:57:01.680
Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava.

01:57:02.240 --> 01:57:07.520
And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing

01:57:07.520 --> 01:57:15.200
test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know

01:57:15.200 --> 01:57:22.480
whether the thing that's being tested, whether you don't know whether it's a machine or a human.

01:57:22.480 --> 01:57:28.720
That's the point of the test. The point, Nathan says, is to show you that she's a robot and see

01:57:28.720 --> 01:57:34.400
if you still think she's conscious. So there's a number of ways in which this is very different

01:57:34.400 --> 01:57:39.440
from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence.

01:57:39.440 --> 01:57:47.600
And those are not the same thing. And secondly, the point, as he says, is to be persuaded

01:57:48.080 --> 01:57:54.720
that the artifact is conscious, even though you know that it's not human, not biological.

01:57:54.720 --> 01:58:00.720
So you know that it's an AI system, and you still think that it's conscious. In that case,

01:58:00.720 --> 01:58:07.200
it passes this test. So this test, I call the Garland test after Alex Garland, who is the

01:58:07.200 --> 01:58:14.640
writer and director of X Machina, quite different from the Turing test. I think that those lines

01:58:14.720 --> 01:58:24.240
in the film were actually really brilliant lines and really clever lines from Alex in the script.

01:58:24.240 --> 01:58:30.960
And when I first saw the script, which was long before it was filmed, and that bit was in there,

01:58:30.960 --> 01:58:37.440
and I put spot on next to those lines in the script, because I thought it was such a very

01:58:37.440 --> 01:58:43.280
good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed

01:58:44.240 --> 01:58:50.080
think that Ava is conscious and thinks of her that way. What that really means is that

01:58:50.080 --> 01:58:54.880
he comes to treat her as a fellow conscious creature. And we see that in the film because

01:58:54.880 --> 01:59:00.400
he wants to help her escape. And just as a kind of thought experiment, maybe it's another

01:59:00.400 --> 01:59:05.760
conceivability thing, but it does seem conceivable what less consciousness would be like, but it

01:59:05.760 --> 01:59:08.800
seems less conceivable what more consciousness would be like.

01:59:09.360 --> 01:59:22.960
Well, so in both cases, I think it's all about exercising our imaginations actually. And I

01:59:22.960 --> 01:59:28.960
think exercising our imaginations is perfectly legitimate in philosophical discussions. So

01:59:28.960 --> 01:59:36.720
in fact, in my newer paper, Simulacra as Conscious Exotica, I think I say that I advocate doing

01:59:36.720 --> 01:59:41.760
philosophy with the detachment of an anthropologist and the imagination of a science fiction writer.

01:59:41.760 --> 01:59:48.720
So I think that's something that I aspire to do. So we can carry out all kinds of imaginative

01:59:48.720 --> 01:59:55.680
exercises to describe exotic entities in a science fiction like way. So we can describe an

01:59:55.680 --> 02:00:02.720
exotic entity and we can describe all kinds of strange behaviors. And that's sort of as far

02:00:02.800 --> 02:00:09.040
as we can get really, I think we could we could we can imagine all kinds of strange behavior,

02:00:09.040 --> 02:00:14.080
and we can imagine scientists studying those kinds of strange behavior and what underlies them as

02:00:14.080 --> 02:00:20.240
well. And so we can imagine all of those things. And then and then we can also imagine how we would

02:00:20.240 --> 02:00:24.880
talk about those things. So imagining how we as a kind of community and how the scientists and

02:00:24.880 --> 02:00:29.920
the philosophers would talk about, about these imagined entities is all kind of part of it.

02:00:29.920 --> 02:00:33.760
So I think we can do all of that. I think that is a kind of doing philosophy.

02:00:33.760 --> 02:00:39.520
And I'm just coming back to the Turing test one last time. I read an interesting take on Twitter

02:00:39.520 --> 02:00:45.360
recently that we've been thinking about the Turing test or wrong. There seems to be a subset of

02:00:45.360 --> 02:00:51.920
people who it's almost like the Eliza effect. They see something they want to see something.

02:00:51.920 --> 02:00:56.240
And it's almost like the test is actually testing the humans rather than the intelligence.

02:00:56.240 --> 02:01:02.160
Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again,

02:01:02.160 --> 02:01:07.120
I think we should distinguish consciousness and and intelligence in this in this regard,

02:01:07.120 --> 02:01:10.880
you know, although there are relations between the two or consciousness and cognition.

02:01:11.440 --> 02:01:15.440
So what just one thing I wanted to say about the about the Turing test is I do

02:01:16.000 --> 02:01:22.960
feel actually that today's large language models kind of pass the spirit of the Turing test.

02:01:22.960 --> 02:01:26.800
So so we defined the Turing test earlier on or the or the kind of

02:01:28.160 --> 02:01:33.520
the popular conception of the Turing test earlier on. And as I remarked, you can kind of game it

02:01:33.520 --> 02:01:38.160
and you know, and there's certain sort of problems with it. But notwithstanding that,

02:01:38.160 --> 02:01:44.160
I think that actually today's large language models pass the spirit of the Turing test, I feel.

02:01:44.160 --> 02:01:49.600
So they pass the spirit of the Turing test because they do really have human level conversational

02:01:49.600 --> 02:01:56.880
skills, I feel. So I might my feeling is that if Turing were alive today and were presented with

02:01:57.600 --> 02:02:03.920
Gemini or ChatGPT or Claude III, he would say, yeah, that's what I had in mind,

02:02:03.920 --> 02:02:07.040
you know, you've done it, that's that's it's kind of passed.

02:02:07.680 --> 02:02:13.360
Interesting. And you also distinguished, you know, the the imitation game is defined by Turing and

02:02:13.360 --> 02:02:18.000
the the colloquial popular conception is that there's no adjudicator, it's just, you know, a person

02:02:18.240 --> 02:02:22.480
and an intelligent machine. What is what is the kind of the main difference there?

02:02:22.480 --> 02:02:26.720
What I mean, meant by the popular conception, I think was that maybe popular conception isn't

02:02:26.720 --> 02:02:30.720
quite right. There's sort of contemporary version of it is that is used in academic

02:02:30.720 --> 02:02:36.080
discussion, in fact, is what I meant. That's what I meant by popular. So I so there I'm

02:02:36.080 --> 02:02:40.560
imagining, yeah, there is a human adjudicator there. But the difference is that in so Turing,

02:02:40.560 --> 02:02:49.040
the way Turing sets up the test is is, you know, he has some specific specificities about,

02:02:49.040 --> 02:02:53.520
you know, the number of minutes you should, you know, you spend interacting with it. And then

02:02:53.520 --> 02:03:00.640
also he sets it up in the context of this game where party game where the aim is to try and work

02:03:00.640 --> 02:03:07.280
out whether which you've got a man and a woman that somebody is is is convert conversing with,

02:03:07.280 --> 02:03:10.640
you know, via paper, and they don't know which is the man and which is the woman,

02:03:10.640 --> 02:03:14.320
they have to guess which is which. And that's the way the thing is set up in the original

02:03:14.320 --> 02:03:18.960
paper is by analogy with that. So there's all so so there's all kinds of, you know,

02:03:18.960 --> 02:03:23.280
peculiarities in the original paper, if you're a Turing scholar that aren't aren't really quite

02:03:23.280 --> 02:03:27.760
relevant to the I think the kind of contemporary way that we think of the Turing test. But I

02:03:27.760 --> 02:03:33.200
also think that the sense in which we in which today systems pass the spirit of the Turing test,

02:03:33.200 --> 02:03:36.560
it's the reason it's only the spirit of the Turing test is because, of course, you very

02:03:36.560 --> 02:03:42.000
often can immediately tell that it's that it's an AI, not not least because it'll just tell you

02:03:42.000 --> 02:03:47.040
right away, right? If you just ask it, it will just say well as a large language model trained

02:03:47.040 --> 02:03:56.000
by Google or by, you know, open AI, so it's easy to tell which is which. So but but but but but

02:03:56.000 --> 02:04:00.880
nevertheless, I think that they have attained more or less human level language skills,

02:04:01.840 --> 02:04:06.160
more or less. And so I think I do think that Turing would would would say, you know,

02:04:06.160 --> 02:04:11.840
as I predicted, you know, yeah, we've got there. Now, I think Turing would be fascinated to see

02:04:11.840 --> 02:04:16.400
the weaknesses that are there and the strengths that are there. And, you know, there are many things

02:04:16.400 --> 02:04:22.720
that today's systems can do that are vastly more powerful than I think he anticipated

02:04:22.720 --> 02:04:28.240
in that paper in the 1950s. And then there are other, you know, there will be other weaknesses,

02:04:28.240 --> 02:04:31.920
I think that would come out that would surprise him as they've surprised us all in a way. I think

02:04:31.920 --> 02:04:36.640
I think many of us in the field are surprised to see something that can do so amazingly in

02:04:36.640 --> 02:04:41.040
certain respects, and yet have so many, you know, still so many weaknesses and others.

02:04:41.600 --> 02:04:48.480
Can you introduce Naegle's bat? So in 1974, Thomas Naegle published this paper called

02:04:48.480 --> 02:04:54.160
What is it like to be a bat? And the point of this paper was to draw attention to the fact,

02:04:54.160 --> 02:05:01.200
if it is a fact, that there are creatures that are very, very different to ourselves to humans,

02:05:01.200 --> 02:05:06.960
but that we assume have some kind of consciousness. We assume in his terminology,

02:05:06.960 --> 02:05:12.160
we assume that it's like something to be that creature. And he chose a bat because bats are

02:05:12.160 --> 02:05:18.560
very obviously very different to ourselves. They fly, they use sonar, they're pretty weird animals.

02:05:19.520 --> 02:05:24.880
And so the way the thinking is that, well, it's probably like something to be a bat,

02:05:24.880 --> 02:05:28.800
but what it's like is going to be very different from what it's like to be a human.

02:05:29.360 --> 02:05:35.840
And Naegle uses that example to get at a whole metaphysical idea, which again,

02:05:35.840 --> 02:05:41.440
I would say is pointing to a kind of dualism, to suggest that there's a whole realm of facts

02:05:41.520 --> 02:05:48.800
about subjectivity, which are outside of the purview of objective science, actually,

02:05:49.440 --> 02:05:57.840
but which nevertheless are part of reality. And so for him, I feel that it alludes to a sort of

02:05:59.840 --> 02:06:02.480
kind of dualistic way of thinking again, that there's this realm of

02:06:03.280 --> 02:06:07.760
facts about subjective things, and there's a realm of facts about objective things.

02:06:07.760 --> 02:06:12.480
I read Naegle's bat many years ago, his paper, and he was kind of saying, wouldn't it be great

02:06:12.480 --> 02:06:19.920
if we could move towards an objective phenomenology? And you are clear that he is pointing to dualism.

02:06:19.920 --> 02:06:25.760
He's not just saying that it's inconceivable in the sense that I couldn't imagine what the experience

02:06:25.760 --> 02:06:31.760
of a bat is like. It's just inconceivable. You're saying that he's actually making a dualism argument.

02:06:32.720 --> 02:06:39.520
Well, he would probably deny that, because nearly every philosopher will enthusiastically deny that

02:06:39.520 --> 02:06:50.720
they're dualists, but I see dualistic thinking all over the place, and I do think that this is an example of it.

02:06:50.720 --> 02:06:55.680
Yes, I mean, it's a similar thing with John Searle. I think he is adamant that he's not a

02:06:55.680 --> 02:07:00.320
dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another

02:07:00.400 --> 02:07:06.080
example of a popular thought experiment, which apparently people get wrong. My friend, Mark J.

02:07:06.080 --> 02:07:11.920
Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument.

02:07:13.280 --> 02:07:24.320
Yes, Mark has tenaciously clinging to the Chinese language, the Chinese rim argument.

02:07:24.720 --> 02:07:30.640
But I can't really see eye to eye with Mark, who I have enormous respect for on this particular

02:07:30.640 --> 02:07:37.920
subject, I'm afraid. Yes, Mark is a very good friend of mine, and maybe that's a rabbit hole,

02:07:37.920 --> 02:07:45.280
we won't go down. But I'm very amenable and convinced by some of Mark's arguments. But he's

02:07:45.280 --> 02:07:52.480
certainly a phenomenologist, which is adamant that he is a monowist. It's another example of

02:07:52.480 --> 02:07:55.920
someone who claims they're not a dualist, but you would say they probably are a dualist.

02:07:55.920 --> 02:08:01.040
Yeah, well, I don't know. We'd have to have Mark sitting here. I would have needed to have read

02:08:01.040 --> 02:08:06.000
one of his papers on this very recently to do that. So I'm not going to accuse him of anything,

02:08:06.000 --> 02:08:08.880
but if he was sitting here, we could have a discussion about it.

02:08:08.880 --> 02:08:13.120
Well, I asked him straight up, because I was trying to pin him down, and he said he is

02:08:14.080 --> 02:08:18.400
an idealist, a monowist idealist. And it might be instructive, actually,

02:08:18.400 --> 02:08:21.600
if you just to explain what do we mean by idealism?

02:08:21.600 --> 02:08:29.120
Right. Well, so especially if he's using the word monow there as well, then I guess that

02:08:29.120 --> 02:08:36.800
he is suggesting that while a physicalist thinks that there is only one substance in reality,

02:08:36.800 --> 02:08:46.400
and that's material reality. And so there is no such thing as a separate stuff of mind

02:08:46.400 --> 02:08:52.320
metaphysically. So for the physicalist, there is no dualism if they really, really think that,

02:08:52.320 --> 02:08:56.800
but the trouble is that as soon as you kind of probe, then often they have struggle to deal with

02:08:57.520 --> 02:09:03.200
dualistic intuitions about subjectivity. Now the idealist, on the other hand, also

02:09:03.840 --> 02:09:09.120
thinks that there's only one substance, but that substance is the substance of mind. So

02:09:09.920 --> 02:09:15.760
physical reality has to be a kind of construct out of this one substance,

02:09:16.480 --> 02:09:22.800
this one out of mind stuff. So that's a very different kind of, it's a different way of

02:09:22.800 --> 02:09:30.560
avoiding dualism, but it has its own problems about how do you explain account for science

02:09:30.560 --> 02:09:35.280
and the success of science and so on. I also interviewed Philip Goff in a beautiful studio

02:09:35.280 --> 02:09:42.960
recently, and he is a cosmo-psychist, but I think it's better just to use the word pan-psychist,

02:09:42.960 --> 02:09:50.320
but I think cosmo-psychist is where you have the teleology baked in. So rather than it being bottom

02:09:50.320 --> 02:09:54.960
up, it's kind of top down. There's some cosmic purpose to the universe, and the universe as a

02:09:54.960 --> 02:10:01.680
whole is made of, you know, the fundamental material of the universe is consciousness.

02:10:01.680 --> 02:10:05.280
And what's the relationship between that view and idealism?

02:10:05.280 --> 02:10:14.080
So I guess for the pan-psychist, so the pan-psychist is so far as I understand these

02:10:14.080 --> 02:10:19.680
philosophical positions. I mean, I shouldn't put myself forward as somebody who necessarily

02:10:19.680 --> 02:10:27.680
understands them in depth, and everybody who subscribes to these views has a slightly different

02:10:27.680 --> 02:10:34.560
version of them as well. But the pan-psychist certainly thinks that reality is composed of

02:10:34.560 --> 02:10:43.200
physical material substance, but that physical material substance irreducibly has a psychological

02:10:43.200 --> 02:10:55.040
dimension to it, a mental dimension to it. So every physical object has a little bit of

02:10:55.040 --> 02:10:59.760
consciousness in it, if you like, in some sense. I mean, I find it a very difficult

02:11:01.440 --> 02:11:06.640
view to articulate because I just find it so completely counterintuitive. So I can't really

02:11:06.640 --> 02:11:13.200
put the pan-psychist's hat on and express their point of view. You need a pan-psychist here to do

02:11:13.200 --> 02:11:22.000
that. Because to my mind, this is, you know, again with my bitkinstinian hat on, then I just think,

02:11:22.080 --> 02:11:28.640
well, how do we use words like consciousness? Well, we use the word consciousness and all of

02:11:28.640 --> 02:11:37.120
the related terms in the context of each other, of other human beings. And so, you know, and it's

02:11:37.120 --> 02:11:44.560
in the context of our being together in the world and your behaving in certain ways and our

02:11:44.560 --> 02:11:50.000
exchanging certain looks when we're doing things and that we understand each other as fellow

02:11:50.000 --> 02:11:54.800
conscious creatures. And so that's the context in which we use, you know, words like consciousness.

02:11:54.800 --> 02:12:00.240
And when I speak about, you know, you're not conscious, you're asleep. And, you know, it's

02:12:00.240 --> 02:12:06.720
all in the context of other humans that we use those words. So it's just ludicrously inapplicable

02:12:06.720 --> 02:12:14.080
to use that word in the context of, you know, I don't know, a brick or a toaster or an atom.

02:12:14.960 --> 02:12:20.160
It's just simply the words simply are not applicable in those contexts.

02:12:20.160 --> 02:12:25.600
Yes, it's so interesting because Philip told me that he grew up as a Christian. And I've had Richard

02:12:25.600 --> 02:12:30.320
Swinburne on as well. And he's got a book out about are we bodies or souls, you know, putting

02:12:30.320 --> 02:12:36.080
forward the case for substance dualism. And so there's the moving away from dualism thing,

02:12:36.080 --> 02:12:40.640
there's the teleology things. Because I think if you are of this frame of mind, you like to believe

02:12:40.640 --> 02:12:48.160
that there's some kind of grand purpose. And there's also the wanting to not wanting to be a

02:12:48.160 --> 02:12:53.280
monowist, basically. So you could perceive it as a form of mental gymnastics to say, okay, well,

02:12:53.280 --> 02:12:58.720
I don't want to be a substance dualist, but why don't we rearrange the structure somewhat so that

02:12:58.720 --> 02:13:04.240
consciousness comes first. And there's some kind of cosmic purpose. And we're building a worldview

02:13:04.240 --> 02:13:08.480
that still makes sense to me. Yeah, yeah. I mean, I personally, I think all of these

02:13:09.200 --> 02:13:15.280
positions involve a great deal of mental gymnastics. And, and I, you know, I reject

02:13:15.280 --> 02:13:21.680
any kind of ism. Or what I mean by that is I don't use that term to describe, you know, my views

02:13:21.680 --> 02:13:28.960
at all. So all of these isms are, you know, are misguided. And what we need to do is just to

02:13:29.200 --> 02:13:42.080
to dismantle the whole way of talking, which makes us, you know, makes us confused in the context of

02:13:42.080 --> 02:13:47.840
this kind of these kinds of issues. So that's what Wittgenstein is trying to do, as he puts it,

02:13:47.840 --> 02:13:52.160
to show the fly the way out of the bottle, right? This is his famous phrase. So the fly is the

02:13:52.160 --> 02:13:56.000
person who's ended up thinking all these philosophical thoughts by taking ordinary

02:13:56.000 --> 02:14:01.760
language into strange places, taking it on holiday. And, and so to show the fly the way

02:14:01.760 --> 02:14:07.680
out of the bottle is to just bring all of these concepts back to their ordinary everyday usage

02:14:07.680 --> 02:14:14.160
and to show thereby that you haven't really, you haven't lost anything, the puzzles evaporate.

02:14:14.160 --> 02:14:17.840
So that isn't an ism. That's a, that's, that's rather that's a kind of

02:14:19.920 --> 02:14:25.040
kind of critical methodology for just shifting the way that you think and talk all together.

02:14:26.480 --> 02:14:31.760
And final question on this, where do you sit on the kind of the teleology question? So one view is

02:14:31.760 --> 02:14:38.320
that we have a purpose. The the physicist would argue that it emerges, you know, from quantum

02:14:38.320 --> 02:14:44.240
field theory and a lot of sophisticated study of biology kind of build this intermediate view

02:14:44.240 --> 02:14:49.040
of teleonomy. Where do you sit on that kind of spectrum? I'll be honest with you, I don't think

02:14:49.040 --> 02:14:54.320
I sit anywhere on that spectrum. I think I think those are issues on which I don't have sufficient

02:14:54.320 --> 02:15:00.640
expertise to pronounce. So, so, you know, you just keep me, keep me on the on consciousness

02:15:00.640 --> 02:15:06.000
and cognition, you know, all this stuff is above my pay grade, you know.

02:15:06.000 --> 02:15:10.000
Professor Shanahan, it's been an absolute honor to have you on MLST. Thank you so much. I really

02:15:10.000 --> 02:15:24.160
appreciate it. Thank you. Thank you so much for having me. It's been fun.

