start	end	text
0	19920	Qualcomm AI Research is hiring for several machine learning openings, so please check
19920	25200	out their careers website if you're excited about solving the biggest problems with cutting
25200	34960	edge AI research and improving the lives of billions of people.
34960	40880	Today we got to speak with one of our heroes in machine learning, Professor Max Welling.
40880	45600	It was good, the questions were really fantastic actually and I've never done this with the
45600	50000	three of you but having a team of three people asking questions is really, it's a good idea
50000	53920	and of course you're really smart people knowing what you're talking about so that went really
54720	59600	well I think. Needs three brains to match yours. We asked Max some of your favorite questions
59600	63760	from Reddit. Hi Max, when will you be changing your last name to pooling?
65920	71360	Max has pioneered the discipline of non-euclidean geometric deep learning.
71360	77600	So what is actually geometric deep learning? It's the idea of performing deep learning or
77600	82800	machine learning more generally but let's say deep learning on data that is not euclidean in some
82800	92480	sense so not a nice chain structure for audio or a planar structure for images but perhaps a sphere
92480	98000	or graph or something more exotic like some kind of manifold with arbitrary curvature.
98720	102960	You might want to model weather patterns or social interaction data. There are many types
102960	108720	of data out there that are non-euclidean. Actually if you've been playing with graph neural networks
108720	114080	then you've already been doing non-euclidean or geometric deep learning. So to make this work
114080	119680	you just need to abstract some concepts so the euclidean distance or your neighborhood
119680	125360	it becomes a function of connectedness just like on a social graph. Am I connected to John? Does John
125360	131760	know Bob? Simple as that. Actually this kind of abstraction works in many areas of mathematics
131760	136720	which Max will get into today as well as making neural networks work on non-euclidean data.
136720	142000	The other thing that Max has really pioneered is this idea of recognizing symmetries
142000	147440	in different manifolds. So in this blank slate paradigm that we have now in neural networks
147440	152640	we're essentially wasting the representational capacity of the neural network because we're
152640	158560	just learning the same thing again and again. For example in a fully connected neural network
158560	163760	we would have to learn the dog in the top right corner and the top left corner because there's
163760	169200	no translational symmetry. And it was exactly this reason why convolutional neural networks were so
169200	175200	powerful because they introduced this concept of translational weight sharing. So you had this
175200	181520	filter that you could shine over the entire planar manifold and it meant that those parameters could
181520	187280	be reused and you could learn concepts in different parts of the visual field. It was an incredible
187280	192800	breakthrough. Imputing this kind of knowledge into a deep learning model this is called an
192800	197840	inductive prior. It means that we can take some prior knowledge about how things in the world
197840	203360	works and we can impute them into our models. It makes our models more sample efficient and it
203360	209440	makes them generalize better. When it comes to sophisticated inductive priors Max Welling is the
209440	215920	king. When we think about AI and its capability to actually help us to enrich our lives we know
215920	222160	we need to first help machines see and understand like humans do. Take this drone collecting data
222160	229920	in 3D or this autonomous vehicle with cameras covering a 360 degrees view. Current deep learning
229920	235680	technology can analyze 2D images very well. But how can we teach a machine to make sense of image
235680	241600	data from a curved object like a sphere? And because we want this processing to happen on the
241600	247760	device itself for reliability, immediacy and privacy reasons how can we achieve this in a
247760	253280	power efficient manner? It turns out we can do this by applying the mathematics behind general
253280	258880	relativity and quantum field theory to deep learning. Our neural network takes in data on
258880	264560	virtually any kind of curved object and applies a new type of convolution to it. We can move the
264560	270320	shape around and the AI will still recognize it. This is just one example of the exciting research
270320	276480	we're doing at Falkamea research to shape AI in the near future. Anyway it turns out that these
276480	282160	symmetries are absolutely everywhere. If you wanted any further proof of how useful these kind
282160	288000	of equivariants and symmetries and manifolds can be, look no further than the recent announcement
288000	298480	from DeepMind AlphaFold. It will change everything. DeepMind solves 50 year old grand challenge.
298480	306160	The game has changed. So proteins are the structures that fold in a given way. The result
306160	313520	of this year's competition came out and they looked something like this. Namely every entry here you
313520	321840	see is a team participating in that competition of protein folding prediction and there is one team
321840	331120	which is DeepMind's system AlphaFold 2 which completely dominates all the others to the point
331120	338160	where the problem is now considered to be solved. By the way if this is not a great meme template
338160	345040	I don't know what is. Just saying. Just saying. They say a folded protein can be thought of as a spatial
345040	353920	graph. This here attention-based okay. So I'm going to guess for sure that they've replaced this convet
353920	360480	with a transformer style with an attention layer or multiple attention layers. I would guess this
360480	364960	is a big transformer right here. So there was a really interesting article that came out called
364960	371600	AlphaFold and Equivariance by Justas Duparas and Fabian Fuchs. I'm so sorry Fabian I don't
371600	376800	know how to pronounce your name but it does sound like a swear word. Justas and Fabian Etow
376800	383120	comment on the announcement from DeepMind and they said in short this module is a neural network
383120	388400	that iteratively refines the structure predictions while respecting and leveraging
388480	394880	an important symmetry of the problem. Namely that of rototranslations. At this point DeepMind has not
394880	400240	yet published a paper so we don't know exactly how they address this. However from their presentations
400240	408400	it seems possible that part of their architecture is similar to the SE3 transformer. What's the SE3
408400	417440	transformer? Lo and behold our friend Max Welling has had his hands all over it. So in the abstract
417440	423360	it says the SE3 transformer a variant of the self-attention module for 3d point clouds and graphs
423360	430080	which is equivariant under continuous 3d rototranslations. Equivariant is important to ensure
430080	435280	stable and predictable performance in the presence of nuisance transformations of the data input.
435280	440000	Right by the way you might be wondering what SE3 is. Let's have a quick look at the Wikipedia
440000	444960	page. We are getting into group theory which is quite an abstract concept in mathematics but
444960	451840	the Euclidean group which is SE3 it talks about all of the symmetries or the group transformations
451840	459440	that can be applied to Euclidean data to preserve certain properties. Namely let's say the Euclidean
459440	463840	distance between two points. Well these are things like translations and rotations and
463840	469200	reflections. Very interesting that you can kind of abstract one level up in mathematics and that's
469200	473360	what group theory is. The other comment I want to make is that all of these folks are independently
473440	477200	amazing. I've watched presentations by most of them so that there's Fabian Fuchs,
477200	484400	Daniel Worrell, Volker Fischer, fantastic. By the way when we look at Fabian's About Me page
484400	489200	he's a machine learning PhD student at Oxford University. His research topic is learning
489200	494160	invariant representations. Simply put where most of deep learning is concerned with finding the
494160	500560	important information in an input he focuses on ignoring harmful or irrelevant parts of information.
500560	505840	This can be important to counteract biases or to better leverage structure in the data.
505840	509760	Structure in the data that's interesting. That's quite a cool point actually because if you think
509760	515200	about it you could naively if you're doing a vision classifier you could naively just look at
515200	519360	all of the pixels and what they are or if you're being smart about it you go one level up and you
519360	524480	look for the hidden structure in the data and that is precisely what he's talking about things like
524480	530320	the symmetries that are inherent in pretty much every type of data. Okay so one last thing
530320	536880	DeepMind released an official PowerPoint deck on Alpha Fold 2 and it talks about they're on a long
536880	542720	term mission to advance scientific progress. Now here are some of the protein examples. Now they
542720	547200	specifically call out inductive biases for deep learning models where this is exactly what we're
547200	553040	talking about so clearly convolutional neural networks are one such bias which has the translational
553040	558800	weight sharing. It talks about graph networks and recurrent networks and indeed attention networks
558800	564000	which is very much a generalisation of pretty much all of the others. They say that they are
564000	569520	putting their protein knowledge into the model so physical insights are built into the network
569520	576160	structure not just the process around it and these biases reflected their knowledge of protein
576160	581280	physics and geometry and you can see here that there are residues in a protein so they're modelling
581280	587040	topologically which residues are connected to which other residues in this kind of 3d space.
587040	591760	They specifically call out here on the structure model page that they are building a 3d
591760	597520	equivariant transformer architecture so anyway if this doesn't motivate you that symmetries
597520	602640	and manifolds are an exciting idea in deep learning I don't know what will. So clearly Max has been in
602640	609440	this game for a long time now back in 2004 with Kingma he invented the variational Bayes auto
609440	616320	encoder. It's only recently that well relatively recently that Max has been focusing in on deep
616320	623280	learning. Clearly like any other field also machine learning is subject to fashion right and so if
623280	629120	there is a five to ten year cycles where people get really excited about a certain topic either
629120	636800	because the theory is very beautiful or it just works really well. I started in biographical models
636800	642800	and independent components analysis was the talk of the day and the support vector machines and
642880	648320	basically non-parametric methods and then came Bayesian methods and non-parametric Bayesian
648320	653600	methods and now it's all about deep learning. So what you see is that the field is subject to these
653600	660400	sort of fashions and I think it's fine because we zoom in a new very promising tool and then we
660400	666160	work it out and we get the most out of it. Max is a vice president at Qualcomm so clearly he
666160	671520	thinks that computation is going to be absolutely critical for the future of artificial intelligence
671520	675840	but having said that he also thinks that we need to be more efficient with our hardware
675840	681920	tomorrow than we are today. That's just a reality that we all have to accept. So the more compute
681920	687840	we throw at it the bigger we make our models somehow the better they perform and we don't know
687840	693200	precisely why that is but we do know that they will use increasingly more energy to do the
693200	698080	computations for us and at some point that's just not a viable economic model anymore. We'll see a
698080	704080	continuation in making deep learning and machine learning more energy efficient. So there's a really
704080	711200	interesting interplay between priors, experience and generalization. We want to have machine learning
711200	717440	models that generalize really well to things that they haven't seen during training. If you move them
717440	723440	into a new orientation or in a new situation in the context and that's what we think of when we say
724320	728480	artificial general AI which means like not just something you train on one specific topic and
728480	732720	then you ask it to do that and it does it very well but if you then move it into a new context
732720	737760	it just completely fails as narrow AI. So humans are clearly much more flexible if you learn something
737760	742080	in one context and then when you get put into a new context that we've never seen before suddenly
742080	748480	we can still do very well and so we want our agents our artificial agents also to have this property.
748480	753840	Max is also a huge proponent of generative models. He thinks that generative models might be the
753840	759440	future of artificial intelligence so funnily enough I think Max and Carl Friston that we had on a
759440	765040	couple of episodes ago I think they would see eye to eye. Basically what everybody else in the
765040	769360	scientific community does which is write down a model of the world which we call a generative
769360	775440	model which is how do I imagine that the world that I'm seeing in my measurement apparatus could
775440	781680	have been generated by nature. We all have the matrix going on inside our heads. We are running
781680	788640	simulations of reality and we're kind of integrating over the expected value of those simulations.
788640	794080	This is just something that we do all the time. That seems to be the real trick for intelligence
794080	801040	at least in humans so our ability to generate the world. Max also thinks that we need to be learning
801120	805920	causal relationships in our models. Causal relationships have this really interesting
805920	812640	property that they generalize better so Max comes up with this wonderful example of a certain color
812640	818880	of car in the Netherlands might be associated with a higher accident rate but that probably
818880	824160	wouldn't generalize very well to other countries because it's just a colloquialism whereas male
824160	829440	testosterone levels that's a causal factor and that's going to generalize far better to other
829440	836160	countries. So try to figure out what the true physics of the world is what causes what and if
836160	841920	you have this causal structure of the world you understand much more about the actual world
841920	846560	and then if you move it to a new context you can generalize a lot better in this new context.
846560	851680	At this stage Max has bet on so many winning horses that you've got to wonder how the hell
851680	858080	does he do it so we ask him what his secret is. It's incredibly hard to predict what will become
858080	862880	well known. Sometimes you just happen to be working on something that takes off like a rocket.
862880	868800	When we did things like the VAE or graph neural nets it didn't feel at all like this was going
868800	875200	to be a big hit. When we read some of the research from Max's students we were just blown away it
875200	880880	sometimes we've got to just remind ourselves that these are fairly young folks that are in their
880880	886480	early 20s that you know they've just come out of university. How is this even possible?
886560	891280	I've been very blessed with being able even with my industry funding to
891280	896640	provide this level of freedom to the students and I think this is really key.
896640	901440	So one of the things we asked Max was how does he select his research directions?
901440	905440	One of the interesting things is that he's a physicist right so many of the things
905440	909840	that he's been doing are straight out of his operating playbook from the physics world.
909840	913680	So things like symmetries and manifolds and even quantum.
913760	919040	Like symmetries have this this deep feeling right? Symmetries pervade basically all theories
919040	924160	of physics and they have this profound impact on how you formulate the mathematics of a theory
924960	929840	especially when it becomes almost mysterious right? Quantum mechanics is almost mysterious.
929840	934160	How on earth is quantum mechanics possible? The fascinating thing here as we discussed on our
934160	940800	GPT3 episode is that many of these roads actually lead back to computation itself.
940800	945040	How does the brain compute things also feels like a very deep question right? How do we even
945040	950400	compute things? What is computation even and does the universe compute its solution?
950400	956560	What does it mean to be predictable? Can you compute faster than the universe can compute?
956560	961600	One of the key concepts that we talk about in the show this evening is the bias variance trade-off.
961600	966160	Nothing comes for free. There is no machine learning without assumptions. You have to
966160	970640	interpolate between the dots and to interpolate means that you have to make assumptions on
970640	976880	smoothness or something like that. These prior assumptions will help you transfer from one
976880	980960	domain to another domain. One of the topics we've been discussing a lot on machine learning
980960	986720	street talk recently is this notion of how far can we take data-driven approaches?
986720	992560	Will they take us all the way to AGI or is it just like building a tower and trying to get
992560	998880	closer to the moon? Perhaps we could generate more data with data augmentation or even a simulator.
998880	1003440	Perhaps we could use data more efficiently with machine teaching or active learning or some kind
1003440	1010480	of controller on how we train the model but ultimately how far can we really go?
1010480	1016400	The big question in some sense over time is can we simply take the data-driven approach
1017200	1023840	and extend it all the way to AGI? So Max tells us about all the different schools of thoughts
1023840	1029840	in the AI community and of course one interesting school of thought is the likes of Gary Marcus
1029840	1034320	and Wallyed Subba that we had on the show a few weeks ago. These people think that we need to
1034320	1039360	have an explicit model of the world. And then on the other side we just want a classical AI
1039920	1043040	sort of community which is no, no, no, that's going to be ridiculous. You will never be able
1043040	1048240	to do that. You really need to imbue these models with the structure of the world.
1048240	1054160	In the show, Max tells us where he's placing his bets but we're not going to spoil the surprise.
1054160	1059520	So as we said before, Max is extremely well known for creating these inductive priors and
1059520	1063760	putting them into machine learning models, helping them generalize better and be more
1063760	1070320	sample efficient. The whole endeavor of machine learning is defining the right inductive biases
1071120	1076800	and leaving whatever you don't know to the data. If you put the wrong inductive bias into things
1076800	1083600	we'll, things can actually deteriorate. We talk about Hinton's capsule networks. They tell you
1083600	1091520	well we'll just keep the abstract nature of what we want which is some stack of things that transform
1091520	1096480	in some way that we can vaguely specify and then we ask it to learn all these things.
1097360	1102240	We talk about Professor Kenneth Stanley's Greatness Can't Be Planned book and also
1102240	1108080	Sarah Hooker's The Lottery Paper. The thing that both of these ideas have in common is that
1108080	1112400	they posit that we are locked in by the decisions of our past.
1112400	1117840	And I do feel very strongly that as a field we need to open up. So we shoot value
1118560	1121440	original ideas much more than we currently do.
1122320	1127280	So Professor Kenneth Stanley has a fascinating take on this. He thinks that we should be
1127280	1132400	treasure hunters. We should find interesting and novel stepping stones that might lead us
1132400	1137120	somewhere interesting. He thinks we should do this in all aspects of our lives.
1137120	1141280	So we all want to monotonically increase our objectives and what we should be is treasure
1141280	1145360	hunters. Science should be about exploration not exploitation.
1145360	1150880	How do we extend this to peer review in science? Ironically having a consensus peer review
1150880	1156640	encourages groupthink and convergent behavior. If we genuinely want to have an exploratory
1156640	1161280	divergent process we should almost optimize for people disagreeing with each other in the
1161280	1167680	peer review process. I think the reviewing in our community is far too grumpy.
1167680	1173360	I'm continuously amazed when I read these old papers from let's say Schmidhuber and like the
1173360	1179040	first RL papers that just came up with a bit of an idea and then they had a bit of toy data and
1179040	1183520	right and that's a paper and it's cool. There's a dichotomy between on the one hand
1183520	1188960	having a stamp of approval having a paper published and presenting about it and on the other hand
1188960	1195840	having a continuous stream of research which is peer reviewed online and with some accountability.
1196400	1202640	Yeah I think we really need to disrupt the field a little bit. Quantum machine learning is a bit
1202640	1209840	of a mystery to most people I feel including myself and even though I learned something in
1209840	1215680	this conversation paradoxically it's more of a mystery than before the conversation.
1216400	1222960	Crucially Max thinks that quantum computing will hugely impact the machine learning world in the
1222960	1229520	future. So you can think of quantum mechanics as another theory of statistics in some sense right.
1229520	1236880	Essentially quantum neural networks have nothing to do with particles necessarily or physics.
1236880	1244720	It's applying the math behind quantum mechanics to machine learning and building neural networks
1244720	1251520	as layers of functions of these quantum operations that forward propagate some signal
1252480	1258320	as Max describes really nicely in this conversation. This is the counterintuitive part which is
1258320	1262320	you can have a probability for an event or an amplitude for an event and then you have an
1262320	1266720	amplitude for another event and you would think that if there's two probabilities for that event
1266720	1271360	to happen then the probability of that event should grow but in quantum mechanics they can
1271360	1276560	cancel and then the probability is suddenly zero that the event happens. So this seems bizarre but
1276560	1284160	nature has chosen this theory of statistics anyway. I really felt like an ELI 5 here.
1284160	1290320	Instead of calculating with probabilities you calculate with something like the square root
1290320	1297520	of probabilities and thus events that can only stack in classical probability theory
1297520	1303840	can all of a sudden cancel each other out and that gives rise to really interesting math.
1303840	1308400	We talk about Max's recent quantum paper that just got released and so that was a paper that we
1309120	1314240	recently pushed on the archive which is quantum deformed neural networks which we basically
1314240	1318960	first say okay what if we would take a normal neural net and implement it on a quantum computer
1318960	1325200	and then we slightly deform it into something where states get entangled. So by doing it in
1325200	1329600	this particular way we could still run it efficiently on a classical computer. What this
1329600	1336240	paper here did was to build a particular type of neural network of quantum neural network
1336720	1344720	that can under the correct assumptions be simulated efficiently on a classical computer
1344720	1350480	but also once we have a quantum computer it can release its full power basically.
1351120	1356960	If you want to do classical predictions does it actually help to build a neural network that
1356960	1362480	can run efficiently on a quantum computer that can do these predictions much better. Can you write down
1362720	1370560	maybe even normal classical problems more conveniently in this quantum statistics.
1370560	1375520	I found the conversation with Max to be extremely helpful here and he does a great
1375520	1381520	job of explaining what's going on. Max has another exciting paper out. Probabilistic
1381520	1386480	numeric convolutional neural networks it's a paper by Mark Finzi, Roberto Bondeson and of
1386480	1390720	course Max Welling and it looks at what you can do with computer vision models if you move away
1390720	1395920	from the assumption of discreetly sampled pixel grids and move to a continuous representation
1395920	1401120	that's more like what an actual object in the real world projected on a screen behaves like.
1401120	1407920	The observation is when we write down a deep learning algorithm let's say on for an image
1408560	1413440	then we sort of treat the image as pixels and we think that's the real signal that we are looking
1413440	1419280	at but you can also ask yourself what if I remove every second pixel now actually I have a very
1419280	1423440	different neural network but should I have a very different neural network or what if the pixels are
1423440	1430240	actually quite randomly distributed in the plane it's just some random places where I do measurements
1430240	1435520	maybe more on the left upper corner and and fear on the left lower corner what the predictor should
1435520	1440720	behave in a certain consistent way and so of course then you come to realize that really what
1440720	1446720	you're doing is with a pixel grid is sampling an underlying continuous signal. So to get away
1446720	1451360	from this assumption of this discreet even sampling they use these objects called Gaussian
1451360	1456480	processes to model the data and a Gaussian process it's basically a universal function
1456480	1462080	approximated like in your own network but it gives you a measure of uncertainty and the reason you
1462080	1466400	might want to do this are many but in short it allows you to average over every possible model
1466400	1471840	that describes your data and gives you a better result. In doing so you can start to do really
1471840	1477760	interesting things like subpixel sampling or work with very sparse locations but in order to do that
1477760	1482320	you need to re-conceptualize a lot of the familiar operators that work on our linear algebra
1482320	1488160	representations such as like the the convolutional translation operation of our weights. The way
1488160	1493120	they got around this was super interesting. So there's a very interesting tool which is called
1493120	1499280	the Gaussian process it's basically interpolates between dots but in places where you don't have
1499280	1505680	a lot of data you create uncertainty because you don't know what the real signal is. What does it mean
1505680	1511920	to do a convolution on this space? The most interesting way to describe that is by looking
1511920	1517600	at it as a partial differential equation. So they reframe this transformation as a differential
1517600	1522880	equation that could just be parameterized calculated out in a closed form and directly
1522880	1526560	applied to the parameters of the model that means you don't need to like do any sampling or anything
1526560	1530000	like that you literally just calculate this thing apply it. It would be worth going into the
1530000	1535600	differential equation stuff by itself but it gets very complicated very quickly needless to say it
1535600	1541040	generalizes not just translation but also things like rotations and scaling but the way that they
1541040	1545280	really did this was by finding very clever representations. It boiled everything down to
1545280	1549840	normal distributions or almost everything could just be done in closed form which things have been
1549840	1554160	done with the Gaussian processes in the past but they're typically computationally expensive so if
1554160	1560000	you can do all these updates without constant re-computation then that's a huge computation
1560000	1564480	and an advantage. The paper does some really cool things. Some of the benefits are now that
1564480	1569280	first of all of course you cannot work on a unstructured set of points doesn't have to be a
1569280	1574960	grid and you can even learn the positions of those points so you cannot direct the observations
1575760	1580720	in places where you really need to do your observations in order to improve your prediction.
1580720	1586640	So it turns out that all of this can be remapped back onto the quantum paradigm. I must admit I'm
1586640	1591680	almost gutted that I didn't study physics at university. Physics seems to be one of the most
1591680	1597600	robust scientific disciplines and the folks are just so smart because it's really really difficult
1597600	1602560	and what I notice is that it's very very difficult for external folks to get anything published in
1602560	1607520	the physics world but there's an asymmetry the reverse isn't true loads of these physicists
1607520	1611600	are coming into the machine learning world and they're just implementing all of these things
1611600	1617920	whether it's symmetries manifold topology chaos it's really really interesting to see this unfold.
1617920	1625040	We also get a take from Max about GPT-3 and so you say GPT-3 isn't very good maybe but it's a
1625040	1631120	receding horizon right. I had a chat with my old colleague from Microsoft Ilya Karmanov about 18
1631120	1637440	months ago he introduced me to Max Welling's work it absolutely fascinated me ever since
1637440	1643360	and guess what Ilya left Microsoft and he went to Qualcomm. Hey Tim how's it going? Ilya is going
1643360	1650640	great how are you? I'm good different country different job different universe it seems but
1650640	1657040	I'm doing pretty well. Ilya and I used to be work colleagues at Microsoft UK and I left Microsoft
1657040	1661760	about a year ago and actually you left as well didn't you Ilya? Yeah we have a joint pact it was
1661760	1668560	like you have to keep both of us or we leave. Indeed now Ilya and I made a YouTube video
1668560	1674160	just over a year ago and it was all about Max Welling's work with Tako Kohen all about symmetries
1674160	1680640	and manifolds and this work was hugely inspiring for me how did you discover it? I discovered it
1680640	1685840	because my colleague Matthew and I whom you also interviewed and you should follow up with
1685840	1692960	that. We were at Ilya and we saw Tako's talk about spherical CNNs which was a bit late already into
1692960	1697360	his work which started with group equilibrium convolutions and I think both of us just thought
1697360	1701680	it was really cool it was our favorite talk for the day because it was so different and it felt
1701680	1706720	like it was setting up a different stream of research it wasn't necessarily about chasing
1706720	1712000	SOTA it was just about really improving taking what makes convolutions great and making them even
1712000	1717440	better and that was awesome. Oh amazing well we made that video together on Machine Learning Dojo
1717440	1723040	and I must admit it was hugely inspiring for me and I reached out to Max Welling about two months
1723040	1727760	ago and he actually came onto our podcast we interviewed him yesterday but yeah it all came
1727760	1731920	from you and you know you introduced all of this stuff to me and I've been going through some of
1731920	1737360	Max's work with some of his recent students and it's just incredible it's because he came from
1737360	1742400	the physics world and all of this knowledge that he has around quantum and symmetries and
1742400	1746880	topologies and manifolds that's his operating playbook and he's just taken it into the machine
1746880	1751040	learning world and he's just been executing on it. Max is involved in a lot of papers
1752160	1756640	as you would expect and a fair few of them are really fascinating. Yeah one of the things we
1756640	1762080	spoke about was just how he nurtures his PhD students because some of these papers are just
1762080	1766880	incredible and presumably these students have gone from nothing to producing that level of
1766880	1770160	research in a very short period of time but presumably this was one of the reasons why you
1770160	1776960	decided to apply for Qualcomm. Yeah I was chasing something that was publishing papers in the field
1776960	1782640	of computer vision and it's one of the places in Europe, perhaps Zurich is another location
1782640	1787120	when you have this kind of research. I thought it was extremely different and a super interesting
1787760	1793360	research area so to speak to get into. Fantastic and what are you working on at the moment?
1793360	1798640	We have just submitted actually our paper to CVPR this morning, the deadlines in a few days so
1798640	1803520	that's pretty good I think and then maybe after that as well we have a few more topics and video
1803520	1808560	basically self-training, how to improve representation learning, it's a mix of knowledge
1808560	1815040	distillation and self-training and then also we have some interesting work with radio signals so
1815040	1820720	it's like video in the sense that it's from that we extract the spatial and temporal signal
1820800	1826080	but it's extremely different to video and that also makes it super fun. Amazing when I was
1826080	1831120	discussing machine learning with Ilya at Microsoft we were fascinated by 3d convolution on your
1831120	1836720	networks and i3d and video action detection and I know you are working on a 3d segmentation and a
1836720	1841360	whole bunch of cool things like that but anyway I would love to get you on the show in the next
1841360	1844960	few weeks to talk about some of your research and for those of you in the comments if you want to
1844960	1849920	have more from Ilya let us know are you going to give us a demonstration of your front lever?
1850720	1855840	Okay single leg. When Ilya comes on the show properly we're going to be doing a front lever
1855840	1864080	competition that's pretty good so not only is Ilya a specialist in machine learning
1864080	1869360	he also absolutely smashes it in the body weight game. No that wasn't smashing it that was after
1869360	1874320	a climbing session it's actually really cool I met this guy here who's a calisthenics instructor
1874320	1880240	called Soli and he just started climbing and yeah so we met up and we went climbing this morning
1880240	1884560	and he was crazy good as you would expect and he gave me some tips on my front lever as well
1884560	1888640	he was saying I should work more on the tuck instead of the single leg so hopefully you'll
1888640	1893680	see much better than that in the future. Amazing Ilya thank you so much for coming on the show we
1893680	1899200	look forward to interviewing you in a few weeks time. Thanks for helping me and thanks a lot for
1899200	1903600	interviewing Max I'm like super excited to see that in a few days. Anyway I really hope you've
1903600	1908640	enjoyed the show today this has been such a special episode for us because Max Welling is
1908640	1915440	literally one of my heroes so anyway remember to like comment and subscribe we love reading
1915440	1920400	your comments we really do actually we're getting so many amazing comments in the comment section
1920400	1929280	so keep them coming and we will see you back next week. Welcome back to the Machine Learning
1929280	1935920	Street Talk YouTube channel and podcast with my two compadres Alex Stenlake and Yannick Kiltcher
1935920	1941680	and today we have someone who doesn't really need any introduction at all clearly one of the most
1941680	1947760	impactful researchers in the ML world and has as near as makes no difference 40 000 citations
1947760	1952640	he's on the executive board at NeurIPS he's a research chair and full professor at the AMLAB
1952640	1959280	University of Amsterdam and co-director of the CUVA lab and Delta lab Max Welling. Max is a strong
1959280	1963600	believer in the power of computation and its relevance to machine learning which is one of
1963600	1968240	the reasons why he holds a vice president position at Qualcomm. He thinks the fastest way to make
1968240	1973520	progress in artificial intelligence is to make specialized hardware for AI computation. He wrote
1973520	1978000	a response to Rich Sutton's The Bitter Lesson but essentially agrees with him in the sense that one
1978000	1983120	should work on scalable methods that maximally leverage compute but Max thinks that data is
1983120	1987520	the fundamental ingredient of deep learning and you can't always generate it yourself like an
1987520	1992720	AlphaGo which amounts to an interpolation problem. Much of Max's research portfolio is currently
1992720	1996480	based on deep learning. He thinks it's the biggest hammer that we've produced thus far
1996480	2001120	and we witness its impact every single day. He thinks that AGI is a possibility and it will
2001120	2006240	manifest in a forward generative and causal direction. There's a really interesting cross
2006240	2011120	pollination story here Max has a physics background he did a PhD in physics he knows all about
2011120	2016080	manifolds and topologies and symmetries and quantum and actually this has been his operating
2016080	2020080	playbook he's brought all of these incredible concepts in from the physics world to machine
2020080	2025200	learning. Now there's a fundamental blank slate paradigm in machine learning experience and data
2025200	2030720	currently rule the roost but Max wants to build a house on top of that blank slate. Max thinks
2030720	2035760	that there are no predictions without assumptions no generalization without inductive bias. The
2035760	2040080	bias variance trade-off tells us that we need to use additional human knowledge when data is
2040080	2044720	insufficient. I think it's fair to say that Max Welling has pioneered many of the most sophisticated
2044720	2049760	inductive priors and deep learning models developed in recent years. An example of an inductive prior
2049760	2054800	is the CNN which means we can model local connectivity, weight sharing and equivalence to
2054800	2060160	translational symmetries in gridded vision data. This is imputing human domain knowledge into the
2060160	2066080	architecture it makes the model significantly more robust and sample efficient. Assumptions are
2066080	2070320	everywhere even fully connected networks assume that there is a hierarchical organization of
2070320	2074480	concepts and even further assumptions about the smoothness of the underlying function we're
2074480	2080400	estimating. Max and many of his collaborators for example Tako Kohen took this idea so much
2080400	2084560	further they introduced rotational equivalence and then they built models which would work
2084560	2091120	extremely efficiently on non-geometric curved manifolds meshes or even graphs. Max wants to
2091120	2095920	reduce the need for data in deep learning models increasing the representational fidelity of neural
2095920	2100720	networks subject to discretization and sampling errors and improving the computational techniques
2100720	2106640	to process them more efficiently. Max has recently put out two new papers Quantum Deformed Neural
2106640	2111360	Networks and Probabilistic Numeric Convolutional Neural Networks which we'll be talking about
2111360	2116720	today. Anyway Max it's an absolute pleasure welcome to the show. Thank you very much Tim for a very
2116720	2125520	nice introduction it almost sounded like it's not me but it was a lot. Do you feel that this
2126160	2131920	it describes you not maybe accurately but do you feel like there's a parts of your work that are
2131920	2138320	overly well known and there may be parts of your work that you wish would be more well known?
2140080	2145040	It's hard to say it's overly well known because of course it's very enjoyable when you can make a
2145040	2151120	big impact but what I can say is that it's incredibly hard to predict what will become well
2151120	2155120	known of course if you could predict that you would only write papers with like gazillions of
2155120	2161520	citations. When we did things like the VAE or graph neural nets it didn't feel at all like this was
2161520	2168720	going to be a big hit and some of these things are being singled out and they fly and precisely what
2168720	2174960	makes these papers fly is in a way that's a big puzzle in a way and some other papers you can be
2174960	2180720	very proud of and it takes so much time to actually get published it's a huge uphill battle
2180720	2184800	you think why do the reviewers not understand better but we really want to do here and then
2184800	2189360	yeah and so they I guess there's a lot of good work which disappears into oblivion
2189360	2196480	and from many people and yeah it's mysterious but anyway. Your hits definitely seem to be
2196480	2201760	more than your misses you're a prolific researcher yourself but you've nurtured some of the best
2201760	2206400	and brightest minds across in not just deep learning but like the wider machine learning field
2207120	2210800	how do you consistently do that is it fantastic mentorship or is it more
2210800	2216000	finding the right spark in a student and nurturing that and that's a really good question and I should
2216000	2220720	say that I've been extremely blessed by all these fantastic students right from the beginning
2221520	2227760	but I do think there is something to nurturing talent so I think what doesn't work is to basically
2228720	2232960	tell to be very constrained to a particular topic sometimes you see this happen if you write a
2232960	2238160	grant proposal and then the grant proposal is about topic A and then really the student
2238880	2242800	starts at topic A but figures out after a couple of months that they don't really like
2242800	2247440	topic A and they want to move on to B and it's just very painful then to say no no no you cannot
2247440	2254080	do that you have to be doing A and so I've been very blessed with being able even with my industry
2254080	2261200	funding to provide this level of freedom to the students and I think this is really key so the
2261200	2265520	other thing which I find really key is that the relationship you have with the student is
2265520	2269840	very important first of all it changes over the years which is also very beautiful so you start
2269840	2276000	off with much more guidance and towards the end you should actually not be doing any supervision
2276000	2281440	you should just having a conversation at that point on equal footing and you see about halfway
2281440	2286800	through a phd like it's like a flower that opens and then it's now they get it suddenly right now
2286800	2292000	they get it and they go and they have a while huge interesting ideas in all directions and they
2292000	2297280	can write all these papers and stuff so that's a beautiful moment when that happens and the other
2297280	2303840	thing I think is that I think of supervision as nudging in the sense that I have a big a lot of
2304560	2311280	experience and where is where is the interesting stuff to be found right where is the next wave
2311280	2315840	that we can get people enthusiastic about what are the important questions to address in the
2315840	2321280	community and things like that so that's where my experience lies now I'm not doing a lot of
2321280	2326560	coding myself in fact I'm just all doing almost zero coding which I regret for this life and the
2326560	2332160	other thing is that even in terms of math it's limited right now right but most maybe two pages
2332160	2338320	of math to verify something or to compute something quickly but not like a lot of math anymore I just
2338320	2344160	try to keep up with literature mostly and the students do though so they do the hard work
2344160	2348400	literally so they really should they should do all that work and it's this interesting
2348400	2353840	relationship where you have a discussion where you say I think you know this is an important
2353840	2358720	direction an interesting direction and here are some other things which are connected to it very
2358720	2363440	intuitively right so you may want to look there and then a good student will just pick up these
2363440	2370160	ideas and we'll run with it and then come up with new ideas and then you could say it is maybe be
2370160	2374160	careful about this direction don't go too deep or maybe this is more an interesting direction
2374160	2380400	stuff like that but even there I've learned to be very careful and if a student comes up with a
2380400	2385680	good idea and intuitively I think that's actually not a great idea this is going to be a dead end
2386560	2391760	that I'm not going to tell the student that very soon so I'm just going to certainly leave the
2391760	2398000	student about a month to explore that idea for sure and I've been surprised right I've been
2398000	2402560	surprised and basically it turned out it was a great idea and I was wrong and so I've been very
2402560	2408160	careful with these things too so I feel it's a very careful dance between the student and the
2408160	2414480	supervisor with not too much direction also it's a very personal so some students like more direction
2414480	2419360	and other students like less direction but I think it is a bit of an art that I've learned
2420080	2424880	to appreciate it is a little bit of an art to have the right type of relationship with students
2424880	2429440	yeah but of course it's all about them they are the ones that need to shine in in the end
2429440	2435680	after four years and they need to get the good jobs and become famous in terms of that guidance and
2435680	2440560	specifically what you said with respect to this direction might be interesting these are the
2440560	2447600	interesting research directions is this something that you just have to develop or do you have some
2447600	2454240	general can you give some high level patterns that you've observed throughout the years where
2454880	2461120	you see recurring things and and you say oh that's another one of those probably like short term
2461120	2466400	hypes or yeah have you observed some general patterns there yeah so there's two things right so
2466400	2472240	there's some things where I think why what is the big deal why is everybody chasing this particular
2472240	2479280	direction so that's can you predict what the crowd will follow that's one thing seems pretty hard
2480000	2485920	the other one is to find directions which may be on longer time scales
2486960	2493520	are impactful and interesting and for the second one it is deeply intuitive and it's very hard to
2493520	2499760	figure out precisely what it is what features there are but for me I have to get a sense that
2500640	2505680	there is some something very deep going on that I want to pursue like for instance
2506640	2512000	so I clearly in physics so if you can think about gauge symmetry like symmetries have this this
2512000	2517200	deep feeling right symmetries pervade basically all theories of physics and they have this
2517200	2522720	profound impact on how you formulate the mathematics of a theory and so there's something very deep
2522720	2528800	about symmetries and and about sort of manifolds and doing things on curved spaces and so that's
2528800	2533200	I could sort of naturally drawn into this thing not now it's more quantum mechanics and there's
2533200	2538480	something very deep and especially when it becomes almost mysterious right quantum mechanics is
2538480	2542880	almost mysterious how on earth is quantum mechanics possible if you dive a little bit into this
2542880	2548400	phenomenon of there's a two slit experiment where you have these individual photons which which go
2548400	2553520	over two paths and if it's a wave that's perfectly fine they can interfere with each other but now
2553520	2558640	these photons can go one by one and somehow they have to be aware of this other possibility that
2558640	2563600	they could have taken to interfere with that other possibility I just think that's crazy what's
2563600	2568240	going on here and so I'm naturally drawn into sort of these kinds of mysteries in some sense
2568960	2573200	yeah and there's plenty more and the other one is also computation clearly right how does the brain
2573200	2577600	computing also feels like a very deep question right how do we even compute things what is
2577600	2583680	computation even and does the universe compute its solution what does it mean to be predictable can
2583680	2589280	you predict can you compute faster than the universe can compute and so there's all these very
2589280	2594080	deep questions about computation as well that you can ask but there's a mixture between
2595040	2599680	things that are attractive in that sort of mysterious sense there's something very deep
2599680	2604880	that needs to be pursued and things which are also highly practical which is sometimes it's also
2604880	2609360	a lot of fun to work on something that where you can actually make a big impact for instance
2609360	2617120	speed up MRI imaging with a factor of 10 so now suddenly you can actually both image and
2617120	2622320	radiate cancer at the same time which could have a huge impact in the future and feeling that level
2622320	2628480	of impact is also quite exciting I think amazing so I wanted to frame up some of the work that you've
2628480	2634240	done around symmetries and manifolds it's absolutely fascinating that prevailing idea is that we are
2634240	2637840	wasting the representational capacity of neural networks because we're essentially learning the
2637920	2644400	same thing many times and your work absolutely pioneered this starting with sort of rotational
2644400	2650240	equivariance on on CNNs and then moving on to meshes and graphs and different types of topology
2650240	2655760	it's absolutely fascinating but philosophically the modus operandi in deep learning is this blank
2655760	2662000	slate idea this idea that if we look at data and nothing else then we can learn everything we need
2662000	2667120	to presumably not in a very sample efficient way transformers seems to be going in this direction
2667120	2671760	in the natural language processing world that we just ingest infinite amounts of data and we can
2671760	2677440	learn everything we need to and we spoke to a good old-fashioned AI person while it's over
2677440	2682160	last week and and his argument was that the information is not in the data he was arguing
2682160	2688080	that we have a kind of ontology or knowledge built into us which we can use to disambiguate
2688080	2693760	information that we receive so fundamentally speaking do you believe that we can be data-driven
2693760	2698080	and can you introduce some of the work you've done with some of these priors in deep learning
2698640	2704160	yes so this is a very fundamental debate clearly but i think it's not all that black and white right
2704160	2709840	so there is a basically at the core of machine learning there is basically trade-offs the the
2709840	2714560	buy is very in straight-off for instance it clearly expresses this right the first thing i want to
2714560	2719920	say there is no machine learning without assumptions it just basically you have to interpolate between
2719920	2724560	the dots and to interpolate means that you have to make assumptions on smoothness or something
2724560	2728960	like that so the machine learning doesn't exist without assumptions i think that's very clear
2728960	2734400	clearly it's a dial right so you can have on the one end you can have problems with a huge amount
2734400	2740720	of data it has to be available clearly and there you can dial down your inductive biases you can
2740720	2748000	basically say let that the data do most of the work in some sense and let me make my prior assumptions
2748080	2753280	quite minimal and with minimal i think i'm interested in a smooth mapping right the mapping
2753280	2758320	needs to be smooth like that's a very minimal assumption but the disadvantage of that is if
2758320	2765440	you don't put any prior assumptions is that if you need to take whatever you've learned into a new
2765440	2772080	domain where this model wasn't learned it will very quickly break down because these prior assumptions
2772080	2779040	will help you transfer from one domain to another domain and causality does play a big
2779040	2783760	role here but we can talk about this later and then on the other hand there is basically what
2783760	2788320	everybody else in the scientific community does which is write down a model of the world which
2788320	2793600	we call a generative model which is how do i imagine that the world that i'm seeing in my
2793600	2799680	measurement apparatus could have been generated by nature and and that's that you can put a lot of
2800240	2805840	intuitive knowledge there because you could think the world is described by a PDE or some kind of
2805840	2811760	generative model so the people in our community often call this probabilistic programming models
2811760	2816960	created by probabilistic programs or graphical models but they are highly intuitive highly
2816960	2822640	interpretable and because they describe the generative process they are often also causal
2822640	2826480	because you can think of these variables and one causes the other variable to happen etc
2827200	2833600	and because they are causal they really generalize very well which means that if i train you know
2833600	2839920	and someone in one context i say i learn to drive in the Netherlands i'm driving on the right side
2839920	2846640	on the road i have particular kind of traffic signs etc so now i can take whatever i've learned
2846640	2851120	sort of these rules or whatever i've learned and now i can move to another country where you
2851120	2854960	drive on the left hand side of the road completely different traffic signs and i can still survive
2854960	2860320	so this is typically something that the the the purity data driven methods have a much harder
2861040	2867920	time doing this sort of generalization so i think this is basically a trade of now it's it's the big
2867920	2875920	question in some sense over time is can we simply take the data driven approach and extended all
2875920	2882560	the way to agi but there are people on one side of the fence that are claiming that this is possible
2882800	2888080	of course we also need to amplify computation right so we're just going to build faster and
2888080	2895440	faster computers that can digest more and more data and at some point we'll just have agi emerge
2895440	2900960	out of this kind of process and then on the other side we just want a classical ai sort of community
2900960	2904960	which is no no no that's going to be ridiculous you will never be able to do that you really need
2904960	2911840	to imbue these models with the structure of the world which which i take as how does physics work
2911840	2916800	how does the world work can i tell you something about how data really gets generated in this work
2916800	2922000	this will cut down the number of parameters to learn dramatically and because i'm following
2922000	2928960	causality i can now basically generalize and create agi in this way and so this is going to be
2928960	2933760	very interesting how this is going to play out and you know to be honest so i feel that i'm slightly
2933760	2939440	in the camp of you really need to put generative information into your models but i've been continually
2939440	2944400	surprised by what's happening on the other side of course lots of my work is also on the other side
2945200	2951600	in the sense that gpt3 you know is completely 100 data driven and did we expect that it would
2951600	2957920	do so well no so he is another big surprise right and so that's i think that's the fun part but it
2957920	2962800	kind of doesn't do well though it doesn't have any reversibility so if you ask it how many feet fit
2962800	2969040	in a shoe or we did the example last week so the corner table wants another beer it doesn't know
2969040	2973200	that the corner table is a person because that's missing information we would fill in those gaps
2973200	2978000	but it does raise the question though of the dichotomy between memorization and compute
2978000	2983120	and the guy we were speaking to last week just said that even if you had an infinite amount of
2983120	2988640	memory and the data is just not there you couldn't do it when you were responding to rich sutton and
2988640	2992880	you actually spoke about all the different schools of thought in machine learning so you said compute
2992880	2998640	driven versus knowledge and model driven or data driven and symbolic or statistical and white box
2998640	3003200	or black box and generative and discriminative the generative thing is fascinating because our
3003200	3007360	brains it's a bit like we've got the matrix or we've got a simulation going on behind the scenes
3007360	3011600	haven't we we're always thinking about all these potential situations and possibly integrating
3011600	3017680	between them yes i i do agree that seems to be the real trick for intelligence at least in humans
3017680	3024800	so our ability to generate the world at least at a symbolic level we don't generate like high
3024800	3031200	resolution videos in our brain but we do generate objects and interactions between objects and sort
3031200	3037200	of how things will play out and this will also help us imagine things like what would have happened
3037200	3041920	if i would have done this so now i can play out this alternative world and say that was bad let
3041920	3046800	let me not do this now so i think that is going to be a key so that's the generative part of the
3046800	3052640	modeling because you can generate you understand how the world works the physics of the world works
3052640	3057440	and so you can generate possible futures to me i feel that's going to be a really important part
3057440	3064720	of intelligence and i do agree that it's for me also very hard to see that you can generate enough
3064720	3070720	data to cover all corner cases it's just very tough if you do it in the wrong direction which is the
3070720	3076560	discriminative direction but again i have been surprised by how good these models really are
3076560	3083680	and so you say gpt3 isn't very good maybe but it's a receding horizon right people may have not
3083680	3088880	thought this was true or bet on something like gpt3 before it appeared and then it appeared and
3088880	3092800	people were extremely impressed and then of course some people poke it and say but it doesn't
3092800	3097840	understand this and this and then excitement goes away again a little bit but it is a bit of a
3097840	3101920	receding horizon but i have generally be very impressed also with for instance the fact that
3101920	3108320	we cannot generate faces of people that that don't exist we can create billions of faces that
3108320	3113680	that don't exist on this planet and that look absolutely realistic would i have expected this
3113680	3118480	no probably not so no and then of course there's alpha go and things like this which we also wouldn't
3118480	3123760	have expected right before it happens let me play a bit of devil's advocate with respect to
3124320	3131760	building priors into models it's of course like some of the easiest priors we can think of are
3131760	3138000	let's say translation invariance in a cnn you can also extend this to rotational invariance and so
3138000	3145200	but if we look at a true practical problem we say yes it makes sense that there is a rotational
3145200	3152960	invariance in the world however on the image net dataset like for a real practical problem the sky
3152960	3159200	is usually up and the object is usually in the center it's not like to the side it's it's usually
3159200	3166960	in the center so in a way it seems like if we actually hit the true invariance that the world
3167520	3174400	adheres to it's certainly beneficial but if we even slightly deviate if we build in a different
3174400	3180480	invariance it seems like there is a level of accuracy and if we want to get past that these
3180480	3186400	invariance seems to be hurting do you have a sense of can it be counterproductive or when is it
3186400	3192800	counterproductive to build in such invariances it's a very good question and so this goes to the
3192800	3199760	point of the bias variance decomposition again so if you hit the right bias then it can be beneficial
3200480	3206160	if you you know impose the wrong bias then it's going to hurt you and this is a well-known trade
3206160	3212480	off so of course the whole endeavor of machine earning is defining the right inductive biases
3213280	3219760	and leaving whatever you don't know to the data and then basically learning to focus your models
3219760	3224560	on the data that you're actually seeing but I agree if you put the wrong inductive bias in it
3224560	3231840	things will be things can actually deteriorate now I should say here that for the rotation
3231840	3236560	invariance or equivariance things are not as bad as you might think so you said if you just have
3236560	3243120	slightly wrong inductive bias then it hurts but that happens to be not so much the case because
3243120	3250800	there's objects inside images that do have if you turn a cat upside down or a tree upside down we
3250800	3256560	still recognize it as a tree in some sense and it does give you a sort of robustness to to certain
3256560	3261920	transformations on these objects that you would otherwise maybe try to model by data augmentation
3262000	3267760	and stuff like that now for the sky maybe you're right that similarly in the digits a six and a
3267760	3273040	nine you know you will start to confuse a six and a nine if you build in rotation and equivariance
3273040	3277680	right and so there it will actually hurt but it's been surprisingly robust actually because
3277680	3281920	basically because you also cut down on a number of parameters and by cutting down on the number of
3281920	3288480	parameters you will you can actually help the system generalize better so the inductive bias
3288480	3293760	doesn't have to be perfect and it can still help could we touch on the dichotomy between the work
3293760	3298400	you've done and capsule networks for example as well as the sample efficiency thing for example
3298400	3304720	with translational equivariance it means that you can you can move the dog and then the response map
3304720	3308640	the dog has moved as well and much of that is about allowing neural networks to learn patterns more
3308640	3313760	easily because they can map in every single layer so with capsule networks that's still a blank slate
3313840	3316640	philosophy so you don't explicitly say what the capsules are
3317360	3322400	whereas with with your approaches you explicitly define the priors with capsules it seems to be
3322400	3327840	defined by the data you give it so if you train a capsule network on MNIST data it might inadvertently
3327840	3332560	learn that one of the capsules is how bendy the stroke width is on the seven or it might learn
3332560	3337760	that there's a rotation on the car because you've given it lots of rotated versions of the same car
3337760	3342640	but it seems quite arbitrary and the algorithm is hideously inefficient and what's much more
3342640	3348080	exciting to me is the kind of baked-in priors that you've designed in the encoder stage so could
3348080	3353040	you draw the dots up between those two approaches yeah i think you actually you said it quite right
3353040	3358880	so one is a much more constrained system than the other one but the actual representations that we
3358880	3366960	put in our hidden layers in both cases are very similar they are stacks vectors and these vectors
3366960	3373520	transform under certain operations so if i rotate the input then there is some operation on this
3373520	3380160	stack of vectors which is they do rotate in the x y plane but they also permute in the sort of
3380160	3386880	vector dimension and so that we tell it very explicitly how to transform we just say under
3386880	3391920	these transformations you have to transform like this and we can do this because these these geometric
3392560	3398320	transformations we know them that they appear in the real world but so it's also constraining
3398320	3403520	because there is many other transformations that either we don't know precisely what the
3403520	3410640	mathematics for the representations looks like or for instance like groups that are that are not
3410640	3415440	compact maybe maybe we have looked at scaling but it's already a more of a stretch but there's of
3415440	3420240	course you can do many other types of transformations that don't even have to be groups there could be
3420240	3425840	other types of transformations like lighting changes or whatever if you wanted to incorporate
3425840	3430160	all of these you would have to build the mathematical representation theory for each of them and then
3430160	3434320	it would actually also explode in a number of feature maps that you would have to maintain
3434320	3440080	and it's not a very practical approach so this works up to the transformation groups that we
3440080	3445040	understand and that are everywhere around us if we want to go beyond it then basically something
3445040	3450640	like capsules are very nice because they tell you well we'll just keep the abstract nature
3450640	3457040	of what we want which is some stack of things that transform in some way that we can vaguely
3457040	3462720	specify and then we ask it to learn all these things and we are actually ourselves also looking
3462720	3468160	at these sort of more relaxed notions of equivariance where we don't tell the system precisely
3468160	3472960	how to change we just want this to emerge automatically and again here the connection
3472960	3479040	with the brain is very interesting in the brain we do seem to have all sorts of filters which are
3479040	3484640	related by not only by rotations but all sorts of other transformations and they are topographically
3484640	3490640	organized so they are right the ones that are related like a slightly rotated version is sitting
3490640	3497200	right next to the other one in your brain and so presumably your brain have figured this out by
3497200	3501680	just looking into the world for a long time and it's organized all these filters that way
3502320	3508160	and it's known that if you prevent let's say a cat from seeing then it will not come up with
3508160	3513360	this nice organization so you really have to get that by looking into the world a lot and that's
3513360	3518480	super fascinating and I think that's where some of our research is being directed now can we learn
3518480	3522880	from how this happens in the brain is there a connection between these topographic maps and
3522880	3528800	equivariance somehow and between capsules and all of these things and I believe that it is a good
3528880	3534880	strategy to to take the general ideas equivariance and then slowly relax it and let the system learn
3534880	3543280	more and more so these capsule networks they've been a bit hyped when they were not really developed
3543280	3551440	named first by Jeff Hinton and he has this concept or at least had it at the beginning that it's some
3551440	3558080	sort of like an inverse rendering pipeline so the sort of the capsule networks do some sort of
3558160	3565200	they take in the world and they inverse render it into these capsules how much do you agree with
3565200	3570000	that type of formulation it seems what you've described is more of a forward way of looking at
3570000	3576000	capsules where we have these invariances yeah yeah so I don't I very much agree with this idea that
3576000	3581360	you have smaller things and they can be used in multiple ways but you have to align them in a
3581360	3586800	particular way so that they build something at the higher level and obviously you can invert that
3586800	3593200	idea too in order to start at something very abstract and then generate certain things this way
3593920	3600960	and there is a lot of work now actually going into equivariant generative models for instance
3600960	3605440	equivariant flows a prime example is for instance in physics and what's called quantum
3605440	3609360	quorum or dynamics there is there's a theory that has a huge number of symmetries called
3609360	3616400	gauge symmetries and if you transform these quarks in a way in a particular way the physics
3616400	3620800	doesn't change you will have exactly the same observations right but still you need these
3620800	3627040	all these symmetries to conveniently describe this model and so now when you generate you if you
3627040	3632160	want to generate quark fields or something like this right then gauge fields then you can generate
3632160	3636320	all these symmetries and it's not very helpful because you generate one configuration but you
3636320	3642160	then if you generate all these sort of equivalent things which are only you know different by symmetry
3642160	3645680	then you haven't really done much so understanding how to generate with these
3645680	3651280	equivalents in it is actually a big topic of research in many groups I think you know Danilo
3651280	3656800	Rosenda and this friend in DeepMind has done a lot of work and there's physicists at MIT and who
3656800	3662400	don't work and we are with a group of students and physicists at Amsterdam we're also looking at these
3662400	3668720	types of questions so that's I guess the inverse problem where also equivalents is playing an
3668720	3674400	increasingly important role not many people are working on capsules I feel they've fallen
3674400	3682240	out of the favor of the public because I don't know they're maybe hard to implement or they don't
3682240	3688160	really work as advertised let's say do you have general thoughts about capsule networks I think
3688160	3693360	with many of these things there is an underlying intuition which is correct so I haven't really
3693360	3699440	worked myself in trying to implement them and so what's what you often see in this field is that
3699440	3705440	there is an intuition about how something should work and often that's that is the correct intuition
3705440	3709040	especially when it's coming from Jeff Hinton it is very likely to be the correct intuition
3709600	3714960	now then there's the next step which is how do you make something practically implementable
3714960	3719600	and these days that means that you have to run it super fast right you have to be able to implement
3719600	3726320	it in GPUs all these kinds of constraints otherwise you will be so much slower than just an ordinary
3726320	3732160	cnn and you will basically not be able to train as long as a cnn and you cannot train as many
3732160	3736160	parameters as an ordinary cnn and you will not beat it and if you don't have the bold numbers
3736720	3741600	hard to publish and so that might impede progress in something like this but then what happens is
3741600	3746800	you wait and then for five or ten years and then the computers have become faster and then people
3746800	3750240	go back to these ideas and then they think oh that was actually very interesting let me try again
3750800	3756160	and then suddenly things start to work now that's of course the story of deep learning
3756160	3761280	more generally speaking right because we had you know neural networks like a long time ago
3761920	3767920	right in the 80s it was actually quite popular to work on these things but they didn't quite
3767920	3772800	take off because we didn't have the compute power and maybe also not the the data to really train
3772800	3779200	them well and it's only when we took them out of the the closet again and said hey man this thing
3779200	3784400	actually works if you throw a whole bunch of GPUs at it that's when people then became popular again
3784400	3789280	and so something like this might well happen again with with capsules or it is something like
3789280	3794720	capsules in then by five to ten years do you have any other than capsule networks are there
3794720	3800480	things that right now we are not looking back on but that would you know be worthy of of a revisit
3801120	3808160	oh yeah that's very tough but i'm personally looking at things like ICA and topographic ICA
3808160	3814320	so i think there's an interesting body of ideas there of course i risk now to mow away the grass
3814320	3820560	before my own feet but okay let me let me entertain that and then probably marker
3820560	3825120	random fields and things like this will probably make a comeback at some point or graphical models
3825200	3830800	more generally will probably make a comeback or maybe that integrated with deep learning and
3830800	3834960	some people have already attempted going in that direction energy based models have made a comeback
3834960	3841280	already so yeah it is often going back to older ideas and there's probably a lot more that other
3841280	3846800	people can sort of name i do have a prediction maybe for the future that people really haven't
3846800	3851520	looked at yet in my opinion it's going to be quantum things so i think many people don't
3851520	3858480	actually know understand the language of quantum mechanics or mathematics and i do think that first
3858480	3862080	of all that language is very interesting it's a bit different than our normal probabilities it's
3862080	3866800	like square roots of probabilities and with the advent of quantum computers which at some point
3866800	3874080	will come we as a community will have to dive into that and maybe make it part of our curriculum
3874080	3880000	and in university and then i think that will become that will start to boom could i just
3880000	3884720	quickly introduce before we get to quantum i don't know if you read sarah hookers the hardware
3884720	3890320	lottery paper and this is fascinating for you of course working at qualcomm but her idea was that
3890320	3894720	there are certain things that cause a inertia or friction in the marketplace of ideas so is it a
3894720	3900080	meritocracy of ideas or do the previous kind of hardware decisions and hardware landscape
3900080	3904240	does it enslave us you know ideas succeed if they're compatible with the hardware and the
3904240	3908800	software at the time and this is what she called a hardware lottery and she says the machine learning
3908800	3912640	community is exceptional because the pace of innovation is so fast it's not like in the
3912640	3917120	hardware world which you all know so well where it costs so much money to develop new hardware
3917120	3922800	and the cost of being scooped is so high but i wanted to just come at this from i don't know
3922800	3927360	how you see this right so with capsule networks the reason they're so slow is because it's a kind
3927360	3932560	of sequential computing paradigm and no amount of hardware is going to solve that but there are
3932560	3936800	entirely different paradigms of hardware like quantum which could potentially change the game
3936800	3941520	but how much could they change the game is there still some limit on it there's always a limit
3941520	3947200	clearly but i think the the situation is maybe slightly more subtle which is that working in a
3947200	3953920	hardware company i can also see the other side of the coin a little bit so there is also a race
3953920	3960720	in the hardware companies to build ASIC designs like which is specialized hardware to run the
3960720	3965760	latest and the greatest machine learning algorithms which are being developed so it's not just rich
3965760	3970560	machine learning algorithms work well on the current hardware that us being enslaved to the
3970560	3976560	hardware there is actually a feedback which where now the companies are trying to build ASIC to first
3976560	3984800	of all run the confolutions very efficiently and soon we'll have probably transformers run very
3984800	3990000	efficiently and so that's the fascinating thing of course it's hard to get your paper published
3990000	3995040	perhaps if if you're ahead of the game too much which i see a little bit in the machine learning
3995040	4000000	community which is if you look at the papers which are published in the physics community they work
4000000	4005440	with images of four by four pixels that's what they can do because otherwise you need a quantum
4005440	4011280	computer obviously to run your algorithm and it's being looked down upon a lot by the machine
4011280	4016320	learners basically saying what do we you know what why is that interesting and i do feel very
4016320	4023200	strongly that as a field we need to open up so we we should value original ideas much more
4023200	4027440	than we currently do and i don't know you know you can probably have a whole conversation on
4027440	4034880	where this is coming from i think the reviewing in our community is far too grumpy i think people
4034880	4040720	if it's not completely finished polished paper then you know they'll find a hole somewhere and
4040720	4047200	they start pushing on it and i think you should look also at a sort of more holistic how original
4047200	4052640	is this idea right can you be excited about the originality and the creativity of the idea that
4052640	4059600	went into that and trust that maybe it takes the community a couple of years to further develop this
4059600	4065600	and and and some things will die and that's fine but let all these flowers grow in a way and yeah so
4065600	4070400	i i do feel a little bit that sometimes is a bit negative and i and that's maybe where some of
4070400	4075360	that friction is coming from that yeah just on that it's fascinating we're talking to Kenneth
4075360	4080400	Stanley on monday and he wrote a book greatness can't be planned and his big thing is exactly what
4080400	4084880	you've just said that we have this convergent behavior in so many of our systems whether it's
4084880	4091120	science or academia and it's because of this objective obsession so we all want to monotonically
4091120	4095520	increase our objectives and what we should be is treasure hunters yes science should be about
4095520	4100960	exploration not exploitation exploitation is one step away you already know how to build the bridge
4100960	4106800	we don't seem to have this paradigm at the moment even when you submit your paper to be reviewed
4106800	4111680	there's a consensus mechanism isn't there because you need to have multiple accepts from people and
4111680	4117280	science advances one funeral at a time yes do you think this is a huge problem yeah i think it is a
4117280	4121280	big problem but i think also we will probably i think it's a big problem because it will hold us
4121280	4126960	back and it will also hold very brilliant students back so what do i advise my students now i advise
4126960	4133200	them to have a mixed model a mixed sort of policy which is on the one hand you work on some papers
4133200	4138560	which are easy to score on things that are very popular in the community and then on the other
4138560	4144080	hand you work on things which might be you know huge innovations that are much more uncertain
4144080	4149600	they might fail but then they might also be really big innovations and that way you get your papers
4149600	4154080	and you can become famous but at least also you work on things which are highly risky but in fact
4154720	4159840	it's a bit cynical to have to do that it would be much nicer if there would be much more appreciation
4159840	4166400	for just originality and but i do also believe there is a solution to this so i think we are in
4166400	4171040	a sort of a local minimum as a community in this sense but i think there's a way out and one way
4171040	4176720	out which is basically and this has been already proposed a long time ago i think young mcconn
4176720	4181840	and yasha benji were also talking about this and we are actually trying to implement this for a
4181840	4188640	beige and deep learning workshop is to sort of to throw papers on the archive and not necessarily
4188640	4196800	submit to conferences and to have a open reviewing of that and to give people reputation indices so if
4196800	4202560	you do if you give a good review you can publish your own review or state it in your cv and people
4202560	4208960	can rate your review and if you do poorer reviews you'll get horrible ratings and then your reputation
4208960	4214000	will come down so there is some kind of way that you can probably design is that people are incentivized
4214000	4219440	to give good reviews and to actually use these reviews as a half a paper that you can also be
4219440	4224320	proud of and then good things will come up right they will at some point people will point to
4224320	4229920	interesting ideas maybe we need some kind of recommender to make sure it's a bit unbiased in
4229920	4234400	the sense that it's not only the famous people that will get their papers exposed but also less
4234400	4237920	famous people so we need to have sort of maybe build a recommender around something like that
4238640	4243440	every so now and then a conference comes by and it sort of harvests in this field of sort of
4243440	4248880	papers and say that one you're all reviewed they have great reviews i'll take one or two more reviews
4248880	4254240	anonymously and i'll then publish and then i invite you to present your paper in our conference
4254240	4261120	that to me sounds like a much more natural way to proceed i also find it very demotivating for
4261120	4265840	my students who have these ideas maybe this is the worst part so you're a student you're working on
4265840	4270720	this thing which is not completely mainstream and then you get rejected two or three times from a
4270720	4276720	conference right this is so demotivating for a student to then continue right at this case at
4276720	4281040	least you just you push us on the archive and you engage with the community around your paper
4281680	4285840	and that's a much more it's much less demotivating than these constant rejections
4286400	4291440	from the big prizes right the NERIAPS paper or the ICML paper that everybody wants
4291440	4296160	is like guys the idea at the moment a lot of people are talking about this how can we improve
4296160	4301600	peer review like in every field people are moving towards this open review model but
4301600	4305840	collectively as a research community we don't really have the collaboration tools at this
4305840	4311280	point in time to take advantage of it open review is willing to implement this actually so yeah i
4311280	4317760	think it will happen yeah it it's a good system to pivot back into new ideas and sort of exciting
4317760	4322160	concepts that are coming out machine learning at the moment you mentioned quantum computing is this
4322160	4326160	paradigm that's really critical that's not really well understood by the machine learning
4326160	4331440	community would you be able to give our listeners like the five-minute spiel about quantum probability
4331440	4336160	how it differs from the probabilities that we're used to yeah so you can think of quantum mechanics
4336160	4342800	as another theory of statistics in some sense right so in AI for everything we can't totally
4342800	4348000	observe we write down probabilities of things happening but of course underlying there is
4348000	4352960	processes but we just don't observe everything and so we describe it by probability now in quantum
4352960	4358560	mechanics it's very similar to taking the square root of a negative number in some sense it's like
4358560	4363760	you let me put another way so it's very much like taking the square root of a probability
4363760	4369600	which can actually become negative so minus one squared is one right okay or let's say
4369600	4375920	minus two squared is four four is your probability and minus two could be your quantum amplitude
4376000	4382560	this thing can be negative and the bizarre thing is that if you describe a system by these
4382560	4388080	quantum amplitude these square roots then they can cancel which is this this is the counter
4388080	4393120	intuitive part which is you can have a probability for an event or an amplitude for an event and then
4393120	4397520	for you have an amplitude for another event and you would think that if there's two probabilities
4397520	4402320	for that event to happen then the probability of that event should grow but in quantum mechanics
4402320	4407040	they can cancel and then the probability is suddenly zero that the event happens so this
4407040	4413360	seems bizarre but nature has chosen this theory of statistics anyway and so it behooves us to
4413360	4422800	look into this more so first question is can you write down maybe even normal classical problems
4422800	4428320	more conveniently in this quantum statistics and here I always remind myself when I first
4428320	4434480	learned complex numbers when you learn to solve the damped oscillator equation you can do it in
4434480	4440560	a complicated way or you can go to complex numbers and then suddenly it gets very easy to do it and
4440560	4447440	so you can imagine that there is things to compute in classical statistics that are actually either
4447440	4453680	shortcuts by using quantum mechanics somehow and and so the first thing that we've tried to do with
4453760	4460880	quantum mechanics in deep learning is to say can we just design an architecture that would be
4460880	4466640	naturally a natural fit to this quantum mechanical description of the world but we still want to be
4466640	4471840	able to run it on a classical computer so we just want to describe this we just want to
4472480	4477600	harvest this new degrees of freedom that we have from quantum mechanics and so that was a paper that
4477600	4483520	we recently pushed on the archive which is quantum deformed neural networks which we basically
4483520	4488320	first say okay what if we would take a normal neural net and implement it on a quantum computer
4488320	4494560	and then we slightly deform it into something where states get entangled and this entanglement is
4494560	4501440	another strange phenomenon in quantum mechanics where you can create states which you cannot
4501440	4507360	really create classically superpositions of states and and so by doing it in this particular way
4507360	4512000	we could still run it efficiently on a classical computer but it's just a very different beast
4512000	4516720	than a normal neural network so that's already to me very interesting and then of course the big
4516720	4523040	prize the big bonus is that if you adhere to this way of describing what's happening is that there
4523040	4527440	is the opportunity to be able to run things very efficiently on a quantum computer so now you can
4527440	4533120	design your neural network in such a way that classically actually it will be very hard to
4533120	4539040	simulate it but then on a quantum computer you could potentially simulate it very efficiently
4539040	4543760	and and of course we don't have quantum computer so it's very hard to actually prove your point
4543760	4548960	but that also what makes it somewhat exciting in that paper specifically you make you make lots of
4548960	4554960	references and connections to the Bayesian way of doing machine learning could you what's the
4554960	4561120	connection there because it seems different both are I agree both are statistics and you already
4561120	4566720	mentioned the square roots of probabilities but how do you connect the sort of uncertainty
4566720	4575120	quantification in the Bayesian way with how particles move quantum mechanics is not necessarily
4575120	4582480	about particles so you can just you can write quantum mechanics on just like states you can just
4582480	4588720	write down a number of classical states like I say a sequence of zeros and ones and there's an
4588720	4594000	exponential number of these states and then you can say classically I can only be in one of these
4594000	4598320	states but in quantum mechanics I can be in any linear combination of these states which
4598320	4606080	should have is a bigger space now what we did in that paper was to say we can treat both the world
4606080	4613200	state as well as the parameter state as a quantum we describe it by a quantum wave function and then
4613280	4618720	we entangle these different states which is similar to saying that I take my state classical
4618720	4625440	state x I multiply it by a matrix of parameters and I get a new state out so here the analogy would
4625440	4631040	be I have my quantum superposition of classical states I have a quantum superposition of
4631680	4636080	parameter states and then there are some processes where we get entangled together
4636720	4641520	and then I do a measurement which is now a function of both the parameters as well as the
4642080	4648160	inputs and you train it to give you measurements that with high probability give you the answer
4648160	4652160	that you want so that would be the training process now there is actually a very precise
4652160	4658880	way in which you can relate Bayesian posterior inference in quantum mechanics but that's a fairly
4658880	4664000	technical story but there is a using density matrices there is a fairly precise way in which
4664000	4669120	you can say I have a state described by a density matrix and if I do a measurement I condition on
4669120	4674640	something and a renormalize and stuff like that so that's possible so there are two things like
4674640	4681920	first of all the quantum neural network formulation can be very slow on a classic computer but
4681920	4688320	fast on a quantum computer on the other hand people do run like Bayesian inference on classical
4688320	4696400	computers what makes the quantum neural networks that much harder to compute yeah it's this
4696480	4702320	entanglement issue yeah so in so classically I agree there is an analogy in classical
4703200	4709600	statistics where this looks very similar which is for instance if I have a exponentially large
4709600	4715360	state space and I write down a probability distribution over all of these possible states
4716080	4720560	where they have a number a positive number that sums to one for each one of these exponentially
4720560	4725440	large states and if I ask you now compute an average of a function over this probability
4725440	4729040	distribution you can't do it because there's an exponentially large number of things that you
4729040	4734720	would have to sum and so we have ways to deal with it which is sampling from these distributions or
4734720	4741280	variational approximations and anyway we have to approximate this state of affairs now in quantum
4741280	4747760	that's fairly similar so there's you you face a similar exponential problem and you can also do
4747760	4753520	approximations to get around that and but the interesting part is that in quantum mechanics
4753600	4758880	you can for instance do a measurement and a measurement is something that you know that is
4759520	4764560	it gets a physical thing and it's not very hard to do but it will be an operation which looks
4764560	4769520	like sampling something down to a particular classical state again and it does look like the
4769520	4775520	sampling operation that we do in sort of artificially in probability theory but it's also true that
4775520	4779360	quantum computers can in principle compute things that classical computers can't compute and they
4779840	4784000	can actually compute it much faster whether that actually maps to the things that we are
4784000	4789280	interested in is not so clear so that's it's not at all clear right now that we will actually build
4789280	4797920	quantum neural networks that are generalizing a lot better on classical problems right if you
4797920	4804240	want to do classical predictions does it actually help to build a neural network that can run
4804240	4808880	efficiently on a quantum computer that can do these predictions much better that's not known
4808880	4813040	but that's what makes it exciting in my opinion because you can try to do it now there's also
4814160	4818640	functions that you can't even do classically you have to do quantum mechanically but I don't
4818640	4825600	know how relevant they are for AI fascinating can we conceivably say that at least one let's say
4825600	4831440	applications are way for these neural networks or for the quantum neural networks to come in
4831440	4837040	is in in the place where right now we have these renormalization problems let's say
4837760	4843280	big word embeddings or yeah as you mentioned things like variational inference any anywhere
4843280	4851840	where you have a partition function that you let's say have to sample to compute now we potentially
4851840	4857280	introduce this new way of doing this yeah so I would say that is a different set of problems so
4857280	4864080	there is some sampling algorithms which can be sped up by quantum sampling algorithms but I think
4864160	4869840	the maximum speed up is like a square root so it's not insignificant but it's also not exponential
4869840	4873840	okay right you can do something in square root time of what a normal classical computer could do
4874400	4880720	and then there is these very interesting stories where people thought that they could do things much
4880720	4885360	faster on a quantum computer but then somebody's thought really hard about it and they then invented
4885360	4891200	actually a quantum inspired classical random algorithm which would do about the same speed or
4892080	4899040	close close at least so it's very uncertain precisely what we can speed up but that what
4899040	4904000	makes it interesting right especially if you can predict what's going to happen in some sense
4904000	4909760	it's just a matter of executing right but if you don't know if they're what they're what the low
4909760	4914800	hanging fruit is and if there is low hanging fruit and what the possible benefits in benefits are the
4914800	4919040	possible bonus that you can get by doing these things then it gets really interesting in my opinion
4919840	4925040	amazing now might be a good time to talk about your other paper that's just come out Max which
4925040	4929920	is probabilistic numeric convolutional neural networks and this was also with Mark Finsey who
4929920	4934320	we just discovered this morning just brought out a really interesting paper about equivalents on
4934320	4939120	light groups so that might be a potential digression later but this works really fascinating because
4939120	4944560	it's in the setting of irregularly sample data and we use these Gaussian processes to represent
4944560	4948880	that and we can continuously interpolate between them in this convolutional setting absolutely
4948880	4954720	fascinating could you give us the the elevator pitch yeah first let me say again that Mark Finsey
4954720	4960960	was an intern at Qualcomm and and Roberto Bondeson was is the other person who was also working with
4960960	4967360	me on the quantum stuff so those of my collaborators in this project and of course Mark did the bulk
4967360	4973440	of the work for this paper so he should deserve much of the credit for it but here's the observation
4973520	4980880	that we had the observation is when we write down a deep learning algorithm let's say on for an image
4981520	4986560	then we sort of treat the image as pixels and we think that's the real signal that we are looking at
4987200	4992240	but you can also ask yourself what if I remove every second pixel now actually I have a very
4992240	4996400	different neural network but should I have a very different neural network or what if the pixels are
4996400	5003200	actually quite randomly distributed in the plane it's just some random places where I do measurements
5003200	5008320	maybe more on the left upper corner and and fear on the left lower corner what the predictor
5008320	5013520	should behave in a certain consistent way and so of course then you come to realize that really
5013520	5017840	what you're doing is with a pixel grid is sampling an underlying continuous signal
5018720	5023760	so then we just started thinking how do you best deal with this so how do you how can you
5023760	5029280	build this in and so there's a very interesting tool which is called the Gaussian process it's
5029280	5035680	basically interpolates between dots but in places where you don't have a lot of data you create
5035680	5041040	uncertainty because you don't know what the real signal is so you basically get some kind of interval
5041760	5046800	which says okay I think the signal is somewhere in this interval with 95 percent you know certainty
5046800	5054000	but I don't know precisely where now the mean function is a smooth actual continuous function
5054960	5059600	and then the next step was say okay what what does it mean to do a convolution on this space
5059600	5066480	this is the new Gaussian process interpolated space and what we found is that the most interesting
5066480	5072160	way to describe that is by looking at it as a partial differential equation and so this ties
5072160	5077200	back into another really interesting line of work which was started by David Duvenaux and authors
5077200	5083520	on thinking of a neural network as an OD as an ordinary differential equation so here we're talking
5083520	5089840	about a PDE basically because we have spatial extent and so we are looking at sort of derivatives
5089840	5095440	and second-order derivatives in in the plane basically which which we apply on the continuous
5095440	5100560	function so this is literally what people do when they solve a PDE is that they have some operator
5100560	5106400	which is consists of derivatives which they apply to the function and then they have a time component
5106400	5112320	which evolves this thing forward in time basically and it turns out that's a very natural way to
5112320	5117920	describe a convolution you can also add symmetries in a very natural way by looking at that operator
5117920	5122160	that sort of moves things forward and making sure it's invariant under certain transformations
5122960	5128720	we had a bit of trouble really handling the nonlinearity that falls that happens then so we
5128720	5133120	had to then project it back onto something that would then again easily handle by a Gaussian
5133120	5138560	process etc so we had to do some work there but in the end this thing was now actually very general
5138560	5146000	and interesting tool which is apply a Gaussian process apply PDE apply nonlinearity repeat
5146000	5152000	and then in the end collect all your information and make a prediction and it so some of the benefits
5152000	5157040	are now that first of all of course you cannot work on a unstructured set of points doesn't
5157040	5162000	have to be a grid and you can even learn the positions of those points so you can now direct
5162000	5168320	the observations in places where you really need to do observations in order to improve
5168320	5172720	your prediction so it basically becomes a numerical integration procedure where you can
5172720	5178640	learn where to move your integration points and what I also found very is fascinating is that
5178640	5185680	this same paradigm can be mapped on again onto a quantum paradigm where you can think of that
5186240	5192880	PDE that evolves now as a Schrodinger equation that sort of evolves like a wave function so it
5192880	5197680	maps very nicely also again to a quantum problem and that's what we are working on now
5198400	5202160	something that's really fascinating that keeps coming up again and again and these sorts of
5202160	5207360	research programs is the matrix exponential like it's our connection to groups and algebras or
5207360	5212880	like group representations and algebras and of course we use it to evolve our ODEs and PDEs
5213760	5218320	I guess as a physicist you've probably got a deeper appreciation of this particular object
5218320	5223600	but it's something that's still quite alien to a lot of people I know that work in applied
5223600	5228720	machine learning what's the significance of the matrix exponential why does it connect all these
5228720	5234800	really fundamental objects to things like Lie groups and stuff like that yeah so it's interesting
5234800	5239920	that we actually just got a paper accepted in noreps on this and it's called the convolution
5239920	5246000	exponential and you can look it up and Emil Hogebaum is the sort of the main author and
5246000	5253920	generator of that idea and yeah so I guess it because it is the solution to the ODE or the PDE
5253920	5260000	right so if you write down something that's very fundamental that is the first order differential
5260000	5266880	equation which is d dt the derivative with respect to t of a state is some operator times that state
5267680	5274320	then the solution of that thing will be the state over time is the matrix exponential times t
5274880	5280160	times the state so that's I think where it comes from and so one other way to look at it is that
5280880	5285360	in physics it's called the Green's function so it's basically the solution to this ODE so you
5285360	5293760	can think of a neural net as basically we tend to describe it as a discrete you know map from one
5293760	5298320	point to another point but if you think of it as a continuous process which is what we learned from
5298320	5305120	the ODE description of a neural net if you think of it as a continuous process then it's really
5305840	5311600	you can just think of that convolution this map you can just think of it as the matrix exponential
5311600	5317040	solution to this to this ODE in math literature you call this the Green's function so you can
5317040	5322240	think of a convolution basically as the Green's function of a partial differential equation I
5322240	5326560	think that's where the word where this feels like a very fundamental object in some sense
5327360	5333600	so in in a talk you gave recently on the future of graph neural networks you were talking about a
5333600	5339200	number of ideas from physics that hadn't really made it into machine learning among them things
5339200	5345040	like renormalization chaos and holography would you care to unpack these ideas a little bit and
5345040	5350640	tell us where you see the future is in these ideas yeah so the reason I mentioned these because I
5350640	5355840	think there's a lot of really cool ideas in physics which are still remain unexplored but there
5355840	5359760	is more and more physicists who are moving into the field and some of these ideas are actually
5359760	5365040	you know being worked out as we speak so I recently saw about two papers on renormalization
5365040	5371040	so renormalization is something in in physics which basically you start with a system with a
5371040	5376320	whole lot of degrees of freedom like say particles moving around or something like this and then
5376640	5384000	coarse grain the system slowly and what means is that by coarse graining you zoom out and you
5384000	5388800	build an effective theory of the underlying theory in the same sense as thermodynamics is an effective
5388800	5394000	theory of statistical mechanics where basically all the particles are now removed but you now
5394000	5398400	have an effective sort of description of your world this is the same as what happens in neural
5398400	5402240	nets right the neural nets we talk about pixels at the bottom layer and maybe edge detectors and
5402240	5407440	things at the very top of it we're talking about objects and relations between objects which are
5407440	5414480	aggregated emergent properties from this neural net and ideas from renormalization theory might
5414480	5420400	very nicely apply to this particular problem and indeed have already been applied with some success
5420400	5425920	the other one which you mentioned was chaos and I think there is a very nice connection actually
5425920	5432640	with chaos theory going back to work I did a long time ago which are called herding in particular
5432640	5438800	you can think of sampling from a particular distribution you can do it either by the typical
5438800	5443200	way is first of all you can think of it as a dynamical system as a stochastic dynamical system
5443200	5448080	and you think of it as there's a you're at a particular point and then you propose to go
5448080	5452000	somewhere and then you accept or reject a particular point and then you just jump through
5452000	5458240	the space and you collect your the points that you jumped to then you look at that collection and
5458240	5462640	that collection should then actually distribute according to the probability distribution that
5462640	5468080	you're sampling from now that's a stochastic process but if you think very hard about that
5468800	5473600	in fact it's a deterministic process even if you try to make it stochastic and the reason is that
5473600	5478080	every you know you're doing a whole bunch of calculations and so now and then you call a
5478080	5483200	random number generator but the random number generator really is a pseudo random number generator
5483200	5487680	it is also a deterministic calculation that you're doing so the whole thing end to end is just a
5488400	5492880	a deterministic calculation but because you're calling the pseudo random number generator it
5492880	5498800	looks very stochastic but truly it is a chaotic process and so you should really be able to describe
5498800	5504080	the system by chaos theory and the theory of nonlinear dynamical systems now what I've been
5504080	5512240	working on with my postdoc and Roberto we've been working on is thinking about let's make it a little
5512240	5518480	bit less chaotic so let's make this actually a deterministic system which is maybe at the edge
5518480	5523840	of chaos and again this is one of these very deep questions that's in my head so I think so there's
5524480	5528480	there is something very interesting and deep here which is if you do if you try to
5529120	5535920	do a computation on the one hand you want to store information things that you've calculated
5535920	5541840	and for that things need to be stable on the other hand you want to transform information
5541840	5546240	because that's what a calculation is right and so there you want to be in this sort of
5546240	5551360	more chaotic domain and it turns out that the best place to be is at the edge of two things
5551360	5555120	where you can go to the right a little bit and be more stable and go to the left a little bit
5555120	5561520	and you can transform things and compute things and so I also think that when you're trying to
5561520	5567040	sample or in you know sampling can be equated with learning if you're Bayesian about things because
5567040	5571360	in learning is basically sampling from the posterior distribution and that's same as learning
5572400	5579120	you can if you can design samplers that are not completely chaotic as the ones that we describe
5579120	5584000	now but they're more structured and less chaotic and more deterministic moving through the space
5584000	5588880	you can learn a lot faster and I find that and then you can actually start to map it
5588880	5594400	onto sort of complexity theory notions if you think of this sampling from a discrete set of
5594400	5600080	states what kind of properties do the sequences that I generate have what is the entropy of the
5600080	5605280	sequences that I'm generating for instance or what kind of substructures is it for instance going to
5605280	5609760	be periodic or are there periodic substructures inside of it or all these things and these are
5609760	5614800	studied by the theory of chaos and nonlinear dynamical systems so connecting these two fields
5615680	5621280	feels to me like a very fundamental thing to try and do and some people have tried a few things
5621280	5625760	people have looked at well if you look at a neural net there's an iterated map you map things to
5625760	5631840	hidden layers in later if you think of that iterated map and think of it as is that map chaotic
5631840	5638080	being on the edge of chaos is the best thing you shouldn't be completely or nonmovable because
5638160	5642720	then everything you put in is going to be mapped to the same point very uninteresting you also
5642720	5647760	shouldn't be super chaotic because or whatever you put in you're going to some random point in space
5647760	5652800	and that's not very predictive so you need to be at this intersection space between chaos and non-
5652800	5657840	chaos and then you can do interesting computations so this is the same idea right so to me that's
5657840	5663840	exciting because now suddenly a whole field of exciting mathematics is cracked open and you can
5663840	5669440	start to use all these tools in machine learning awesome thank you fantastic now might be a good
5669440	5676720	time to go over to reddit we asked reddit for questions and the top rated question is by tsa
5677360	5680480	hi max when will you be changing your last name to pooling
5684240	5690160	so actually there is a paper that a colleague of mine wrote and i think they had an operator
5690960	5697200	instead of pooling you could you could do a a welling operator so and instead of changing
5697200	5701920	my name i i propose that we just change the operators that we use and change to welling
5701920	5707760	operators that's wonderful in the thread on reddit there were a few variations as well so maybe max
5707760	5714560	power and someone asserted that pooling is your brother but anyway red portal says the conventional
5714560	5719600	approach for analyzing continuous convolution would be Fourier analysis what was the rationale
5719600	5724880	behind the investigating continuous convolutions using probabilistic numerics that's a good question
5724880	5731040	so to me Fourier analysis it's true that you can i guess i could still do a Fourier analysis
5731040	5736880	right because a Gaussian process you can decompose in terms of its Fourier waves and then it's the
5736880	5743280	primal versus the dual view of a sort of any sort of kernel method so i could certainly go to the
5743840	5748320	Fourier domain and do my calculations in the Fourier domain the quantum mechanics this is
5748320	5752480	just another basis you just think of this as another basis you know not only quantum
5752480	5757600	mechanics in any signal processing sense and it's true that a convolution is easier there
5757600	5765680	because just multiplication on the other hand convolutions are very efficient in modern software
5765680	5772800	packages for gpu so sometimes it's also not necessarily faster to do that but it's a good
5772800	5778240	suggestion and maybe something nice happens when you go to Fourier space and i just didn't explore
5778240	5786720	that fantastic we've also got jimmy the ant lion says hi max i notice your co-authors come from a
5786720	5792800	physics background can you explain why there are so many x physicists in deep learning yeah so that's
5792800	5798480	interesting i think there's just a lot of physicists and a fraction of those physicists is looking for
5798480	5804160	other for greener pastures and i'm myself on one of those that i was looking for greener pastures
5804880	5810480	and they bring a really good toolbox so if you're done physics you're you have just a very good
5811040	5816400	mathematical toolbox but also very good intuition about PDEs and other world works and
5816400	5821760	symmetries and all these kinds of things you bring and i think in some sense physics is also a bit
5821760	5826240	of a container right if you do physics you can still do anything else afterwards in some sense
5826800	5832080	and i think just there's just people who are naturally interested in in ai of course ai became
5832080	5837280	very popular at some point and so you have automatically people flock into that into that
5837280	5843120	field but yeah in general they're smart people so i guess it's nice to work with them maybe just
5843120	5848240	circle back and close the loop to the beginning and we were talking about the research community
5848240	5855360	and kind of the machine learning research field i i loved what you suggested and as i understand
5855360	5861760	this is not fully your suggestion but the suggestion of let's say having a more open review kind of
5861760	5867680	system where a review could be as powerful as a paper itself i've been screaming for this for
5867680	5874960	a few years now and could i ask if you if you ever have the chance to propagate this what do you
5874960	5880800	think of the idea of having a continuous research like this paper notion that we have now i think
5880800	5886880	it's so outdated and once my paper is published i have no incentive to update that thing what what
5886880	5892640	if we do research in in this much more continuous way and then there's comments and then in response
5892640	5899040	to the comments everything changes and so on yeah no it's very good point it's it's so this is indeed
5899040	5905360	exactly part of this idea that we are trying open review to implement yeah but it's the idea is that
5905360	5910000	in open review you have a conversation with your reviewers and it's nice if the reviewers are not
5910000	5914560	anonymous and just you just have your conversation and other people can even contribute to the project
5914560	5919680	in a more open science way but it is also nice for now and then to present your work and so
5919680	5926240	that's why i say so now and then a conference might come in and harvest papers and just invite
5926240	5930960	people to present their work in sort of slightly more formal way and maybe put a stamp of approval
5930960	5936320	on it and say this conference has published or this particular paper with some independent reviews
5936320	5941360	and we think it's a great paper and so you get that stamp so it and i guess there should also be a
5941360	5947600	way to close off a particular project to move on to a new project but i also have the same view as you
5947600	5954720	have as this being a far more continuous process where you know if you didn't get picked this time
5954720	5959680	next time somebody some conference will come by and pick you out this it's much more like a marketplace
5959680	5965280	where ideas go around conferences come in and ask you to publish things and it's just you then
5965280	5969920	present it and then you can just continue with your research or stop and then go to a new piece of
5969920	5975280	work or something like this so yeah i i share that vision basically that's it's amazing i'm
5975280	5981520	continuously amazed when i read these old papers from let's say schmid uber and like the first rl
5981520	5986640	papers that just came up with a bit of an idea and then they had a bit of toy data and right and
5986640	5993520	that's a paper and and it's cool do you have any do you have any kind of thoughts about or recommendations
5993520	5998880	for the new generation of researchers that are now flooding the fields of how can we get to a
5998880	6005360	better field what kind of tips would you give the yeah we i think we really need to disrupt the field
6005360	6010720	a little bit and so i think we i think the new i think it's particularly tough for new researchers
6011360	6017200	because it's the acceptance rates for these conferences are very low and it feels like much
6017200	6022640	of your future career depends on getting papers in there and it's a fairly random process as well
6023520	6026960	so i think we just need to disrupt the field and there's enough people
6028400	6033840	with influence who want that so it's just a matter of actually executing on it and so that's
6033840	6039920	what we do it now for the bayesian deep learning workshop that we are organizing this we want this
6039920	6048160	to be a an off-split from new rips it was a very popular workshop there and somehow we got rejected
6048160	6053520	this year and we thought okay we'll just do it ourselves we do have actually a meet-up but then
6053520	6058480	next year we want to be our standalone conference but for that conference we want to implement this
6058480	6063600	plan and so we are working with open review to actually implement this for us and jaren gall is
6063600	6069520	working hard to try to actually roll this out we're talking to yashua benjo about it and he's
6069520	6073760	very supportive and there's a whole lot of people who are supportive about it but so if this can help
6074720	6079280	to make this a popular model then that will be a fantastic result of this interview
6079280	6083120	but i think people should just push for it and just say okay i'm just fed up with the
6083120	6088160	current way of doing things we should really change things and just a shout out and say
6088160	6095280	this is what we want and let's go for it awesome amazing professor max welling it's been an absolute
6095280	6100240	honor and a pleasure to have you on the show thank you so much for joining us today it was great
6100240	6104400	with the three of you asking questions that works really well fantastic thank you so much
6104400	6110160	thank you amazing it was good the questions were really fantastic actually and i've never
6110160	6114400	done this with the three of you but having a team of three people asking questions is really
6114400	6118640	it's a good idea and of course you're really smart people knowing what you're talking about so that
6118640	6126560	went really well i think needs three brains to match yours anyway i really hope you've enjoyed
6126560	6132080	the show today this has been such a special episode for us because max welling is is literally one
6132080	6138560	one of my heroes so anyway remember to like comment and subscribe we love reading your
6138560	6143600	comments we really do actually we're getting so many amazing comments in the comment section so
6143600	6151440	keep them coming and we will see you back next week
