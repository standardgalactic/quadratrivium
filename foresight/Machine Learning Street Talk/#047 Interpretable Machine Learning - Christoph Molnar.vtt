WEBVTT

00:00.000 --> 00:05.280
Coming up later in today's presentation, I'm wondering at what point we're just developing

00:06.160 --> 00:11.040
complex math models to explain complex math models and we really haven't, you know,

00:11.040 --> 00:16.720
made much progress along the interpretability axis. So you have something you don't understand

00:16.720 --> 00:21.760
and you explain it with something you don't understand? If I have if I have some general

00:21.760 --> 00:26.320
formula, just some very general formula, and then I go in there and I go, you know what,

00:26.320 --> 00:34.240
this formula has five parameters. And if I make this 1.75 and that one one-third and this one two

00:34.240 --> 00:41.520
and that one zero, and I call this the Megatron activation potential, and I go and write a paper

00:41.520 --> 00:47.040
about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a

00:47.040 --> 00:52.160
fancy mathematical passport and you got it published in some journal. And now everybody

00:52.160 --> 00:57.440
has to memorize that as you know, the Megatron potential and kind of learn about it. And that's

00:57.440 --> 01:08.080
a lot of what's going on right now is that it's really just a bunch of hacking.

01:14.960 --> 01:19.760
Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning.

01:19.760 --> 01:27.120
Enjoy. Interpretability has become one of the most important topics in machine learning.

01:27.120 --> 01:32.560
And it's something that every data scientist needs to be familiar with. For hundreds of years,

01:32.560 --> 01:39.280
we've had simple interpretable models like linear regression and rules-based systems.

01:40.320 --> 01:47.360
But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models.

01:48.320 --> 01:55.440
And of course, predictions from these models are not always so easy to explain. So as we start to use

01:55.440 --> 02:03.360
these more powerful, nonlinear models to actually make decisions on real world matters, then it's

02:03.360 --> 02:09.520
inevitable that our attention must now turn to interpretability and explainability. When I

02:09.520 --> 02:15.040
first started learning about machine learning algorithms, I was told they could be dangerous.

02:15.120 --> 02:22.800
They were hard to understand. They were black boxes. But as Christoph lays out, it turns out

02:22.800 --> 02:27.440
there is a whole plethora of techniques out there to explain why a model made a certain

02:27.440 --> 02:33.120
prediction. Some models like low-dimensional linear regression are intrinsically interpretable.

02:33.920 --> 02:39.120
You can just look at the model coefficients and that tells you exactly how the model is working

02:39.120 --> 02:44.880
under the hood. Then there is a whole suite of methods that will actually work with any ML model.

02:45.280 --> 02:50.640
Like training a local surrogate or a global surrogate. There's also Shapley values,

02:50.640 --> 02:57.440
which is a really cool technique that allows you to distribute blame for the prediction amongst

02:57.440 --> 03:03.840
the input features in a really theoretically sound, really principled way. And then there are domain

03:03.840 --> 03:09.840
specific methods. For example, to explain image models, you can try to highlight the most relevant

03:09.840 --> 03:16.640
parts of an input image by making saliency maps. And there's more. You can look at things like

03:16.640 --> 03:23.040
example-based explanations where you try to find the smallest change in the input data that would

03:23.040 --> 03:29.920
cause the output prediction to change. So maybe with this awesome new interpretability toolkit,

03:29.920 --> 03:37.120
we can start to dispel that myth that machine learning models are all just black boxes that

03:37.120 --> 03:43.680
can't be understood and can't be trusted. Christoph Molnar is one of the most important people

03:43.680 --> 03:49.760
in the interpretable machine learning space. In 2018, he released his magnum opus,

03:49.760 --> 03:55.440
interpretable machine learning, a guide for making black box models explainable.

03:55.440 --> 04:00.880
Interpretability is often a deciding factor when a machine learning model is used in a product,

04:00.880 --> 04:07.040
a decision process or research. Interpretability methods can be used to discover knowledge,

04:07.120 --> 04:14.480
to debug or justify a model and its predictions, to control and improve the model, to reason about

04:14.480 --> 04:20.400
potential biases in the model, as well as increase the societal acceptance of models.

04:21.040 --> 04:27.200
But interpretability methods can be quite esoteric. They add an additional layer of complexity

04:27.200 --> 04:32.640
and the potential pitfalls require expert understanding. Machine learning models are

04:32.640 --> 04:38.560
inherently less interpretable than classical statistical models, but typically they have a

04:38.560 --> 04:43.440
better predictive performance and that's because of their ability to handle nonlinear

04:43.440 --> 04:49.760
relationships and also higher order feature interactions automatically. But do we have

04:49.760 --> 04:56.080
to suffer this implicit trade-off between the complexity of a model and the lack of our ability

04:56.080 --> 05:02.160
to understand it? Simplistic model approximations can often mask important information and be

05:02.160 --> 05:08.640
misleading as a result. In classical statistics there's an entire field called model diagnostics

05:08.640 --> 05:14.560
to do exactly this, to check that assumptions and simplifications have not been violated.

05:14.560 --> 05:18.240
This is something that does not yet exist in interpretable machine learning.

05:18.880 --> 05:24.800
Interpretability has exploded and matured in the last few years, in particular since the

05:24.800 --> 05:31.120
deep learning revolution. We now have a better understanding of the weaknesses and strengths

05:31.200 --> 05:36.800
of interpretability methods. A growing number of techniques are available at our fingertips

05:36.800 --> 05:43.680
that can lead to the wrong conclusions if applied incorrectly. Is it even possible to

05:43.680 --> 05:49.600
understand complex models or even humans for that matter in any meaningful way?

05:50.320 --> 05:53.280
That is one of the questions that we're going to be discussing this evening.

05:54.320 --> 06:00.480
Molnar also recently released a couple of papers where he discusses some of the important pitfalls

06:00.480 --> 06:04.800
of interpretable machine learning methods. So some of the things that Christoph Molnar is

06:04.800 --> 06:10.800
really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician.

06:10.800 --> 06:17.680
Also he is exasperated with some of the misguided causal interpretations from some of these IML

06:17.680 --> 06:22.480
methods. He also points out feature dependence or situations where you have shared information

06:22.480 --> 06:27.200
between features. It completely breaks many of the IML methods and this is something that he

06:27.200 --> 06:33.600
focuses on a lot. He also focuses philosophically on the broader impact of interpretability and

06:34.480 --> 06:40.160
what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick

06:40.160 --> 06:44.800
through this paper, interpretable machine learning, a brief history, state of the art and challenges

06:44.800 --> 06:48.880
and as well as pointing out some of the history of IML methods, we'll jump straight into one of

06:48.880 --> 06:54.160
the challenges which is feature dependence. Molnar points out that feature dependence makes

06:54.160 --> 06:58.960
attribution and extrapolation problematic. This is exactly what happens in partial dependency

06:58.960 --> 07:04.720
plots for example. We are basically extrapolating and we are creating fictitious data points that

07:04.720 --> 07:09.840
didn't really exist and these fictitious data points probably exist outside of the data distribution.

07:10.480 --> 07:17.360
So Molnar thinks that the models that we build should reflect the causal structure in the world

07:17.360 --> 07:22.960
but of course that is not really the case most of the time and he points out that statistical

07:22.960 --> 07:28.880
learning is just reflecting surface feature correlations not the true causal structure beneath

07:28.880 --> 07:34.240
the scenes. Causal structures would be more robust if we could actually capture them

07:34.240 --> 07:39.760
and the predicted performance and learning causal factors is a conflicting goal which I think not

07:39.760 --> 07:44.720
many people have thought about. So we need to think about when we can make causal interpretations

07:44.720 --> 07:49.520
and a lot of work is underway in this field but being completely frank this is very nascent. There's

07:49.520 --> 07:55.920
not really much out there at the moment. Molnar also points out this lack of statistical rigor

07:56.560 --> 08:01.120
having been a statistician himself. He was exasperated when he came into the IML field just

08:01.120 --> 08:06.720
to see that most IML methods do not even give you confidence estimates something which is

08:06.720 --> 08:12.080
completely standard in the statistical world. Models and explanations are computed from data

08:12.080 --> 08:17.360
which means they are subject to uncertainty but this is something which is just not captured

08:17.360 --> 08:22.640
using current methods. He says that we need to be making distributional and structural assumptions.

08:22.640 --> 08:29.280
He points out this risk of p-hacking something which is prevalence in the natural sciences.

08:29.280 --> 08:34.000
This is something that could be coming to the world of IML very soon if we don't start thinking

08:34.000 --> 08:39.520
about this more carefully. Molnar also points out that there is no accepted definition of

08:39.520 --> 08:45.920
interpretable machine learning methods so it's not entirely clear how we can compare IML methods to

08:45.920 --> 08:51.520
machine learning models. It's really easy to assess machine learning models because we have

08:51.520 --> 08:56.880
benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well

08:56.880 --> 09:02.800
but we can't really quantify how correct an explanation is and it doesn't really help that

09:02.800 --> 09:07.680
there's a taxonomy of interpretability methods. There are objective methods like sparsity and

09:07.680 --> 09:14.000
interaction strength and there are human-centered evaluations from domain experts or from lay people

09:14.000 --> 09:18.640
and quite often you need to have quite a lot of technical knowledge to even understand

09:18.640 --> 09:24.720
these assessments. He says that the setting of machine learning is too static. It doesn't reflect

09:24.720 --> 09:29.360
how these models are actually used in practice and I really love this idea of thinking about a

09:29.360 --> 09:34.320
process rather than thinking about just the model so he says we need to have a holistic view of the

09:34.320 --> 09:39.520
entire process. He thinks that we need to think about how we explain predictions to folks from

09:39.520 --> 09:45.760
diverse backgrounds, how we have interpretability at the societal level or at the institutional level

09:45.760 --> 09:49.920
thinking much more broadly than we are at the moment. He also thinks that we need to reach out

09:49.920 --> 09:55.280
to other disciplines for example psychologists and social scientists and he thinks that there's

09:55.280 --> 09:59.760
lots of rich knowledge in computer science and statistics that we're just not using yet.

09:59.760 --> 10:05.600
So in July of last year he also released this paper pitfalls to avoid when interpreting machine

10:05.600 --> 10:09.920
learning models. In this paper he points out that there's a growing number of techniques

10:09.920 --> 10:14.800
providing model interpretations but many will lead to the wrong conclusions if used incorrectly

10:14.800 --> 10:19.440
and he goes on to point out many of those pitfalls. For example the first one is

10:19.440 --> 10:24.160
assuming that the model generalizes well so assuming that the model has been fit correctly

10:24.160 --> 10:29.440
if the model is underfit or overfit then the interpretation method will perform badly as well

10:29.440 --> 10:35.680
and interpretation can only be as good as the model underlying it. So the next pitfall he points

10:35.680 --> 10:41.440
out is the unnecessary use of complex models which is to say the use of opaque or complex

10:41.440 --> 10:46.960
machine learning models when an interpretable model would have sufficed which is to say when

10:46.960 --> 10:51.600
the performance of an interpretable model is only negligibly worse than one of these black box

10:51.600 --> 10:56.640
models and to be honest this is something I see all the time I think the gratuitous use of complex

10:56.640 --> 11:00.880
machine learning models is something which is really serious. One of the things I don't like

11:00.880 --> 11:07.120
about machine learning is the laziness. I think we should always seek to understand and simplify

11:07.120 --> 11:11.120
problems wherever we can it's the same thing in software engineering. We should always be trying

11:11.120 --> 11:17.680
to create the most elegant and simple and maintainable solution. We shouldn't be trying to over

11:17.680 --> 11:22.800
complicate things and I think that's a very you know the kiss principle is very generalizable here.

11:23.440 --> 11:28.560
So he recommends to start with simple interpretable models like generalized linear models or lasso

11:28.560 --> 11:34.160
models or additive models decision trees or decision rules and gradually ratcheting up the

11:34.160 --> 11:40.480
complexity as required. So he also points out that ignoring feature dependence is super important

11:40.480 --> 11:45.200
right and this is a problem that many of the IML methods have so he gives an example of

11:45.200 --> 11:50.640
partial dependency plots where they extrapolate in areas where the model has little training data

11:50.640 --> 11:55.840
and it can cause misleading interpretations. So these perturbations produce artificial data

11:55.840 --> 12:00.800
points that are used for model predictions which in turn are aggregated to produce global

12:00.800 --> 12:05.760
interpretation so that's a big problem. Another thing he points out is confusing correlation

12:05.760 --> 12:10.720
with dependence so he gives an example here features with a Pearson correlation coefficient

12:10.720 --> 12:15.280
close to zero can still be dependent and cause misleading model interpretations

12:15.280 --> 12:20.160
while independence between two features implies that the Pearson correlation coefficient is zero

12:20.160 --> 12:25.600
the converse is generally false. So there's a pretty cool example here this is a couple of features

12:25.600 --> 12:31.200
that absolutely have a dependence on each other you can see it visualized here but you wouldn't

12:31.200 --> 12:35.360
know that if you looked at the Pearson correlation it would have said that it wasn't significant.

12:35.360 --> 12:40.960
Another one misleading effect due to interactions so there's a couple of things here there's the

12:40.960 --> 12:46.320
partial dependency plots on a couple of dependent features and then he's used a simulation to kind

12:46.400 --> 12:52.320
of trace all of these different features to see what the predicted label was and according to

12:52.320 --> 12:58.560
these IML methods there is actually no clear dependency between these features and the predicted

12:58.560 --> 13:02.640
outcome whereas you can see that that's just blatantly false. So something I've been meaning

13:02.640 --> 13:08.240
to do for more than a year now is to go through Molnar's interpretability book and to make some

13:08.240 --> 13:12.960
bite-sized videos on every single approach well Connor and I are actually going to do that over

13:12.960 --> 13:18.080
on Machine Learning Dojo with the first one next week on Shapley Values so make sure you

13:18.080 --> 13:23.680
subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your

13:23.680 --> 13:31.680
comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube

13:31.680 --> 13:38.080
channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society

13:38.160 --> 13:46.320
and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision

13:46.320 --> 13:52.960
and these days interpretable machine learning. We have an exemplar German on the show Christoph

13:52.960 --> 14:00.640
Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable

14:00.640 --> 14:06.160
machine learning a guide for making black box models explainable. If a machine learning model

14:06.160 --> 14:11.840
performs well why don't we just trust the model and ignore why it made a certain decision? Well

14:11.840 --> 14:17.120
the problem is that a single metric such as classification accuracy is an incomplete description

14:17.120 --> 14:23.680
of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he

14:23.680 --> 14:28.960
introduces the importance of interpretability and reports an incredibly detailed taxonomy

14:28.960 --> 14:34.000
of interpretability methods and his style of writing is at times entertaining and entirely

14:34.000 --> 14:40.320
absent of hype and nonsense. He runs the gamut of interpretability models so for example model

14:40.320 --> 14:45.520
agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual

14:45.520 --> 14:50.560
examples and adversarial examples he motivates the importance of interpretability methods

14:50.560 --> 14:56.080
but he's also extremely transparent about its current weaknesses and pitfalls. He's currently

14:56.080 --> 15:01.920
finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich

15:01.920 --> 15:06.560
after getting a stats master's from the same institution. He's recently written several

15:06.560 --> 15:11.040
very interesting papers on interpretable machine learning for example pitfalls to avoid when

15:11.040 --> 15:16.880
interpreting machine learning models in July of 2020 where Christoph detailed several problematic

15:16.880 --> 15:21.760
model interpretations for example ignoring estimation uncertainty feature interactions

15:21.760 --> 15:26.640
or confusing correlations with dependence. More recently he published a paper called

15:26.640 --> 15:31.600
interpretable machine learning a brief history state of the art and challenges while he acknowledged

15:31.600 --> 15:36.480
that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods

15:36.480 --> 15:41.200
such as the lack of statistical uncertainty, shared information between features, lack of a

15:41.200 --> 15:46.160
clear definition of interpretability and the need for a more holistic view. Christoph Mulner

15:46.160 --> 15:51.120
it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here.

15:51.760 --> 15:56.000
You know Christoph I have to say I really enjoyed your book. I read this actually

15:56.000 --> 16:01.120
some months back in preparation for a completely different show. I loved how scientific it was

16:01.680 --> 16:06.880
you know it was it was very much laying out essentially a survey of the facts a lay of the

16:06.880 --> 16:12.800
land very objective evaluation. It had both the pros and cons you know of different approaches

16:12.800 --> 16:17.920
examples to make them you know more understandable so kudos to you. I thought it was a great book

16:17.920 --> 16:23.040
very enjoyable and very informative. I also loved how it lays out the beginning you know what the

16:24.080 --> 16:28.560
goals that we're trying to achieve with with interpretability are especially kind of the

16:28.560 --> 16:35.360
human goals right like what does it mean for an explanation to be good for people what kind of

16:35.360 --> 16:40.400
explanations do people like and sometimes there can be conflicting conflicting goals there and

16:40.400 --> 16:46.080
I think one thing that that I realized from reading your book is that that actually explanations can

16:46.080 --> 16:53.840
be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have

16:53.840 --> 17:01.200
to look for contrastive explanations or counterfactual explanations like and principle it seems good

17:01.200 --> 17:08.560
it's kind of like you know I'm sorry we can't give you this loan you know well why not like why

17:08.560 --> 17:13.120
can't you give this loan well well we've detected really that you're a that you're a deadbeat what

17:13.120 --> 17:17.600
do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look

17:17.600 --> 17:22.160
through this and we find a decision tree here and and some big decision tree and we get to this one

17:22.160 --> 17:28.800
little point what says here you know you didn't pay this furniture bill back in 2018 you know if

17:28.800 --> 17:33.600
if only you'd have paid that furniture bill like we'd be able to give you the loan right but the

17:33.600 --> 17:38.320
truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree

17:38.320 --> 17:44.320
right with so many contributing points yeah I think I like this chapter that you referenced

17:44.320 --> 17:49.520
was about like kind of from the social view or the human view what what what people like or

17:49.520 --> 17:55.360
prefer is explanations and the whole chapter is based on I forgot the title of the paper it's

17:55.360 --> 18:01.040
like from Miller about like what we can learn from the social sciences about what a good

18:01.040 --> 18:07.760
explanation is and was like a paper where I learned a lot and it was super interesting also to see

18:07.760 --> 18:11.840
how like what people think are good explanation as you mentioned they should be contrastive

18:11.840 --> 18:16.560
they should be short but they should also confirm to some prior knowledge that the people have

18:17.440 --> 18:22.320
and I mean like objectively a lot of those things might not like you wouldn't say these are good

18:22.320 --> 18:29.680
explanations in some sense like maybe maybe it's not good to give an explanation that fits with

18:29.680 --> 18:36.800
a prior knowledge because it's not the correct one maybe so it was quite interesting to learn

18:36.800 --> 18:41.520
and to think about like what's the human side of it that's a very cool part of your book I thought

18:41.520 --> 18:46.000
the fact that it's actually quite interesting thinking about what we really want out of an

18:46.000 --> 18:50.640
explanation I remember first of all looking at you know sharp values that are very fair and

18:50.640 --> 18:55.760
will distribute the blame equally amongst all the different relevant features and then you turn

18:55.760 --> 19:01.360
to something else like you know selective interpretations that in a way are way less

19:01.360 --> 19:06.320
good because they're kind of arbitrary they'll just select a few a few a subset of the features

19:06.320 --> 19:10.000
and give them all the blame but then it turns out that apparently that's what people actually

19:10.000 --> 19:16.160
want as a useful intuptable explanation yeah so as I see there's like many many dimensions of

19:16.160 --> 19:23.680
explainability or like what what can be a good explanation and one of these dimensions is maybe

19:23.680 --> 19:28.400
sparsity that you have a short explanation with just a few facts that's something that people

19:28.400 --> 19:34.800
prefer maybe but this might be in conflict with other dimensions of a good explanation

19:34.880 --> 19:40.000
which should be maybe that all causes should be addressed by the explanation that plays some

19:40.000 --> 19:46.240
role at least but this is of course in conflict with sparsity if you want this full attribution

19:46.240 --> 19:52.880
like you get with shepley values for example so I that's why I also think there's not like just

19:52.880 --> 19:58.320
one correct explanation but there's like many attributes or many dimensions on which you can

19:58.320 --> 20:03.520
judge explanations yeah I think this is one of the problems because even machine learning is really

20:03.520 --> 20:08.640
difficult right because we use benchmarks and benchmarks are just something that people have

20:08.640 --> 20:13.440
come up with but you say in your you know you talk about one of the problems being that there's no

20:13.440 --> 20:18.880
definition of IML methods to start with but at least in machine learning methods we have ground

20:18.880 --> 20:26.160
truth which is which is significantly better in a way but if we if we can't quantify how good

20:26.160 --> 20:31.200
an explanation is then where are we really because you talk about a kind of taxonomy of

20:31.200 --> 20:35.760
interpretability methods right you say that there are objective evaluations like sparsity and

20:35.760 --> 20:41.520
interaction strength and fidelity and humans human-centered evaluations you know which might

20:41.520 --> 20:47.200
come from domain experts or lay people so I suppose you're just hitting straight on the

20:47.200 --> 20:53.520
fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this

20:53.520 --> 20:59.040
big criticisms okay this is not scientific or not well defined at least what interpretability is

20:59.040 --> 21:05.680
how can we even do research in this area but I have a bit more relaxed view I mean otherwise I

21:05.680 --> 21:10.560
should have stopped writing the book before I really started so I kind of see like this

21:11.680 --> 21:17.200
endeavor of giving interpretability or bringing interpretability to machine learning it's more

21:17.200 --> 21:23.520
like a first of all it's just a keyword so it's it kind of bundles all the methods together

21:23.520 --> 21:31.120
that kind of aim to reduce this high-dimensional function to something well mostly it's something

21:31.120 --> 21:36.080
in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine

21:36.080 --> 21:42.240
and it's I think part of science to find out like or to yeah some part of analysis to find out what

21:42.240 --> 21:49.840
part gets lost so when you for example look at just some feature importance values for example

21:49.840 --> 21:58.800
of course it's a summary of your model and a lot of information gets lost but I still think

21:59.600 --> 22:03.680
it's useful to have obviously so many people use it but it's useful to have these

22:04.400 --> 22:11.440
tools and we just have to understand what they do and how to interpret the results so how do

22:11.440 --> 22:16.640
you interpret when like the feature importance is zero of a feature could that be quite dangerous

22:16.640 --> 22:21.360
though because this you gave the example of random forests when you have a lot of shared

22:21.360 --> 22:27.360
information between the features it would actually tell you that these correlated features have a

22:27.360 --> 22:33.040
higher feature importance than you might otherwise expect so does this imply that we need to have

22:33.040 --> 22:37.680
very detailed knowledge of how we should how we should use this information that we get from

22:37.680 --> 22:43.200
IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls

22:44.000 --> 22:49.280
to avoid and stuff like this so I think so these are just tools so they do something with the

22:49.280 --> 22:54.640
model a kind of distill some knowledge so for example for feature importance you kind of

22:56.160 --> 23:00.400
measure how well does your model perform and then you measure again after you

23:00.400 --> 23:05.440
shuffle one of the features and and then then you get something out of it so then we can ask

23:05.440 --> 23:11.440
questions is this interpretable or not and it's kind of well not so relevant the question because

23:11.440 --> 23:16.080
you just have to understand what what happens when you shuffle feature and one is for example

23:16.720 --> 23:24.800
you kind of break the association between the feature and the prediction because now it doesn't

23:24.800 --> 23:29.120
carry the information about the target anymore because you're shuffled in randomly in your model

23:29.840 --> 23:35.360
so you kind of this this feature importance now measures how much performance you lose because

23:35.440 --> 23:41.680
of this break of information but then you also when you think about this method and want to use

23:41.680 --> 23:46.320
it you also have to understand that this shuffling for example breaks also association with your

23:46.320 --> 23:51.120
other data feature like the features in your data so this is a limitation of the method and

23:52.000 --> 23:58.480
what I think is needed is that we understand in which way these methods break or which

23:58.480 --> 24:03.600
scenarios we're allowed to use them or how we are allowed to interpret them and I think the

24:03.600 --> 24:09.760
situation is kind of similar to statistics where you have these models and and then you

24:09.760 --> 24:15.360
interpret like the coefficients of your models you still like have to learn how you do the

24:15.360 --> 24:19.520
interpretation what are the assumptions that have to be met that you're allowed to do this

24:19.520 --> 24:25.920
interpretation and I think it's we're in a similar situation here with interpretability of machine

24:25.920 --> 24:31.440
learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality

24:31.520 --> 24:37.200
in the thread because you make a very good point in the book which is look even these so-called

24:37.200 --> 24:42.720
intrinsically interpretable models are only interpretable up to a certain dimensionality

24:42.720 --> 24:48.000
and you know I have I have tons of experience with with multi-linear regression right and and I can

24:48.000 --> 24:54.160
guarantee that beyond a very small number of dimensions those coefficients are not interpretable

24:54.160 --> 24:59.040
because it starts to play a bunch of games where it's inflating one coefficient and another because

24:59.040 --> 25:03.440
their difference is important and you know whatever else is happening a lot of correlated

25:03.440 --> 25:08.560
structures are all essentially getting compressed into the small number of small number of weights

25:08.560 --> 25:14.080
right and so as the dimensionality goes up I would say like no model is is intrinsically

25:14.080 --> 25:18.960
interpretable same can be said of decision trees like anybody who's looked at a decision tree that's

25:18.960 --> 25:23.520
come from real data you're going to find out it's not interpretable it's like oh look at this you

25:23.520 --> 25:28.480
know market capitalization matters oh and it matters over here too and down here and and

25:28.480 --> 25:33.520
actually I have to go through five checks on market capitalization before I get down to this

25:33.520 --> 25:39.120
decision and maybe the features aren't that intuitive either and then you have to kind of

25:39.120 --> 25:46.400
like mentally stack up like until you get to the decision like five decisions and then we have a

25:46.400 --> 25:56.560
very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean

25:56.640 --> 26:00.800
I have I have this distinction the book like interpretable models and not so interpretable

26:00.800 --> 26:07.680
models um but it's as you say it's like a gray like it's a scale really people could definitely

26:07.680 --> 26:13.120
overhype how interpretable these white box models are right whether it's linear models

26:13.120 --> 26:19.200
as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use

26:19.200 --> 26:24.880
models like a decision tree because it's possible in theory to write down on a piece of paper exactly

26:24.880 --> 26:30.240
how the decision is made right yeah you can trace every decision but that's never actually useful

26:30.240 --> 26:35.440
in practice is it since when have you ever looked at a decision tree fitted to data there's any

26:35.440 --> 26:40.240
complexity and the fact that in theory it's possible to go and examine how it works yeah it's

26:40.240 --> 26:44.560
completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision

26:44.560 --> 26:49.760
tree sometimes it but in practice it will not like give you probably the best predictions

26:50.720 --> 26:54.640
um but it might be useful sometimes to shorten it artificially so even like

26:54.640 --> 26:59.680
you're throwing away some some predictive accuracy um but you shorten it so you understand

26:59.680 --> 27:06.240
that somehow it's manageable you can have a look at it and and see what's going on well there's also

27:06.240 --> 27:10.800
you know the other issue is that as I was looking through a lot of the methods that you describe

27:10.800 --> 27:16.720
and you survey in your book you know some of them um are not simple I mean if you start looking at

27:16.720 --> 27:22.560
partial dependency plots and trying to explain what those are I mean you know you have to almost

27:22.560 --> 27:27.360
have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering

27:27.360 --> 27:34.560
at what point we're just developing complex math models to explain complex math models and we really

27:34.560 --> 27:40.720
haven't you know made much progress along the interpretability axis yeah yeah it's true it's

27:40.720 --> 27:45.440
also like the criticism too like um so you have something you don't understand and you explain it

27:45.440 --> 27:52.640
with something you don't understand um I think some methods are complex um but for some at least

27:52.640 --> 27:58.800
there's some intuition how they work for partial dependence plot is kind of your this um intuition

27:58.800 --> 28:03.760
that you do some intervention on your more or intervention on your data so you replace all your

28:04.320 --> 28:10.080
like for one feature you replace all the values with one fixed value and kind of look at the

28:10.080 --> 28:14.800
average prediction that you get afterwards and do this for a lot of points and then you connect

28:14.800 --> 28:20.880
the points and you have this curve so kind of gives gives you the expected change over the feature

28:20.880 --> 28:27.120
range maybe there was already a bit complex I don't know um maybe I'm too deep into the method

28:27.120 --> 28:34.560
already um but yeah of course it's it's something additional people have to learn or if they agree

28:34.560 --> 28:41.440
to use it of course could I get a quick take from you on saliency maps as an example because you

28:41.440 --> 28:46.560
said in one of your youtube videos that saliency maps are glorified edge detectors they are not

28:46.560 --> 28:52.320
good explanation at all and I've noticed now that many machine learning platform providers are building

28:52.320 --> 28:57.760
these kind of um saliency maps into their models you know into their platforms and then it becomes a

28:57.760 --> 29:02.320
kind of box ticking exercise where you can say okay well yeah we've done interoperability now

29:02.320 --> 29:07.360
that's all you need to know and that really is quite a false sense of security isn't it

29:07.360 --> 29:12.880
it's funny you mentioned the saliency maps because I'm writing a book chapter about it and

29:12.880 --> 29:19.280
actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has

29:19.280 --> 29:23.840
been a long time in the making and it was very very frustrating like by far the most frustrating

29:23.840 --> 29:30.160
chapter to write um number one reason is because there's so many methods out there uh reason number

29:30.160 --> 29:39.040
two is I I can't judge really or if they work and it seems like they mostly don't or it's it's

29:39.040 --> 29:44.800
still unclear like how you say you would judge that they work so they're like dozens of these

29:44.800 --> 29:51.360
like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance

29:51.360 --> 29:59.040
propagation in 10 variants um so and then I mean you in the end you when you apply these methods on

29:59.040 --> 30:03.440
they are also like for image classification and you get these nice-looking images and some areas

30:03.440 --> 30:08.960
are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if

30:09.840 --> 30:15.600
if it kind of makes sense then you maybe would be inclined to trust the method um but then there's

30:15.600 --> 30:21.440
this uh paper which is called um sanity checks for saliency maps and they kind of found out that

30:21.440 --> 30:28.720
they the most of the methods are very similar to edge detectors meaning that they are kind of

30:28.720 --> 30:34.960
insensitive to the model and the data which is very bad of course well if you change the model um

30:34.960 --> 30:41.360
the explanation should obviously change um could could you expand on that a little bit so you said

30:41.360 --> 30:47.040
it wasn't really reflection of the model or the data but what what would a perfect saliency map

30:47.040 --> 30:54.240
look like well I don't know myself actually so I mean the the ideas that you saw the basic idea

30:54.240 --> 30:59.360
of most of these methods is that you you have your class prediction or your class score and you

30:59.360 --> 31:06.560
want to back propagate it not you want to back propagate it to the original image so you look

31:06.560 --> 31:13.120
at the gradient um with respect to your input pixels and that's there's no not not one way to

31:13.120 --> 31:18.480
do this but there are many different ways so that's also why we have so many different methods

31:18.480 --> 31:26.000
and then they highlight which pixels were relevant for the classification um but yeah

31:26.640 --> 31:31.680
they these these methods they have like a lot of like issues for example there's the issue of

31:31.680 --> 31:37.440
saturation for example because of the real unit where you have flat parts of the gradient so if

31:37.440 --> 31:43.280
you pass the gradient through that then um your method would say that some some neuron might not

31:43.280 --> 31:49.680
be uh important at all and there's a lot of these little issues that these methods have um

31:50.800 --> 31:55.920
yeah so but but back to your question like I think that's also the issue that I don't

31:55.920 --> 32:00.480
wouldn't know how to answer I mean obviously it should be some area that should be highlighted on

32:00.480 --> 32:05.600
this aliens method was important for the for the neural network um but then again I don't know how

32:05.600 --> 32:11.760
the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean

32:11.760 --> 32:16.880
I could highlight the part where I think the network should look but then again I mean there are

32:16.880 --> 32:22.240
lots of papers like the clever hands paper which saw like the reveal that there are some

32:23.360 --> 32:29.520
sometimes it would look at watermarks on on the photo um so these are like these things that we

32:29.520 --> 32:35.600
just don't know uh what the neural network basis this on have if I could take a stab at that answer

32:36.240 --> 32:42.640
for one I think just the idea of quote a saliency map is a problem like there isn't one

32:43.520 --> 32:47.760
map of of the importance of the pixels it's like they're they're operating on multiple

32:48.480 --> 32:52.720
multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you

32:52.720 --> 32:58.320
know why is this image a dog you know well for one thing it's it's the overall shape you know it has

32:58.320 --> 33:04.720
four legs and you know two ears sticking out over here that's one saliency another is that

33:04.720 --> 33:09.040
it's it's got a certain color you know and it and it's coat and that's a that's a different

33:09.040 --> 33:14.000
concept of what's salient and another is that there's a frisbee flying at it and its mouth is open

33:14.000 --> 33:19.200
and it's about to catch it and I know dogs do that so they're kind of you know when your mind

33:19.200 --> 33:24.640
analyzes an image it breaks it down into these many large scale kind of structural features

33:24.640 --> 33:29.200
and I think that gets completely lost and most of the approach is the saliency maps this is

33:30.000 --> 33:36.480
really important point actually because if you're just looking at the pixels on this kind of 2d

33:36.480 --> 33:42.240
planar manifold that's only a very it is quite literally a surface view and I think Christoph you

33:42.240 --> 33:48.880
said that there are all sorts of causal structures and even in the model itself right there are

33:49.520 --> 33:54.560
these entangled neurons and surely that's giving me more insight into what's actually happening

33:54.560 --> 33:58.160
just seeing a bunch of pixels and the other thing is that these models that they are completely

33:58.160 --> 34:02.400
lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has

34:02.400 --> 34:09.520
just got completely broken right yeah so um but in in that vein some of these feature visualization

34:09.520 --> 34:14.960
techniques you know like the deep dream type stuff maybe maybe that's a better way of of

34:14.960 --> 34:19.920
interpreting these models yeah um so like for the one point you mentioned about the adversarial

34:19.920 --> 34:27.360
examples so there's also a paper I forgot to title again um which that manipulated neural networks

34:27.360 --> 34:32.480
so they would give the same prediction for all the images but different explanation like different

34:32.480 --> 34:39.200
saliency maps so this is perfectly possible to create different explanations um for these saliency

34:39.200 --> 34:46.320
maps um but but keeping the model like at least for the predictions the same there's another criticism

34:46.320 --> 34:50.320
you can throw at saliency maps where they they can be quite deceiving you think they're useful and

34:50.320 --> 34:54.320
they turn out not to be useful yeah there's a classic example of looking at you know comparing

34:54.320 --> 34:59.040
a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful

34:59.040 --> 35:03.520
sometimes it highlights the animal and you think okay I understand it's looking at the face that's

35:03.520 --> 35:07.920
why it thinks it's a dog because it's in the face and then you look at the predicted class for something

35:07.920 --> 35:13.120
else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah

35:13.120 --> 35:17.120
so the saliency map for all these different classes looks the same and when you realize that

35:17.120 --> 35:22.640
you realize this this saliency map hasn't actually told you anything about why it's gone for one class

35:22.640 --> 35:26.480
versus the other all it said is that it's just highlighted the thing in the middle of the picture

35:26.480 --> 35:31.680
yeah I think that's especially also when when you look at images you know like we're very good

35:32.640 --> 35:36.720
with images yeah like we were very quick to see what's happening on a scene and such

35:36.720 --> 35:41.360
so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense

35:41.360 --> 35:46.400
it's more difficult to interpret like if you have like a graph and there's like things going on inside

35:46.400 --> 35:51.760
you have to like now understand what the method does and stuff like this but for an image like a

35:51.760 --> 35:57.200
heatmap IR this area is highlighted makes sense case closed I like the method

35:59.120 --> 36:05.680
yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining

36:05.680 --> 36:10.640
complex things with complex things we don't understand you know so I think a lot of people

36:10.640 --> 36:14.800
if they looked at it and again one of the points of this interpretability is really the social

36:14.800 --> 36:21.200
aspects of it right like being able to convince people to be at ease with machine learning models

36:21.280 --> 36:27.920
or to accept the results of of a machine learned you know decision process and I think if somebody

36:27.920 --> 36:32.320
looks at an image of a dog you know they have no problem understanding that but if you showed them a

36:32.320 --> 36:39.200
bunch of salience maps or or any of the other sort of you know feature projections if you will

36:39.200 --> 36:44.880
like you said it takes a lot of deep understanding to understand those whereas the image is kind of

36:44.880 --> 36:50.560
immediately obvious I think two of the main themes that you touch on is we'll get to the

36:51.360 --> 36:56.640
to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue

36:56.640 --> 37:02.560
that you point out is feature dependence okay and and you say that when you have feature dependence

37:02.560 --> 37:07.840
it makes attribution and extrapolation problematic so a dependence just means that you got correlated

37:07.840 --> 37:12.560
or shared information between your features right so you say that in feature permutation methods

37:13.520 --> 37:17.200
these things basically break everything when you have the shared information

37:17.200 --> 37:22.400
and the extrapolated data points are no longer in the distribution and you say that there are

37:22.400 --> 37:26.960
conditional permutation schemes you know that try and and maintain that joint distribution but

37:26.960 --> 37:31.040
those things sometimes make it even worse right so do you think that's one of the most important

37:31.040 --> 37:37.120
things that people should think about when using iml methods yeah at least so that's at least like a

37:37.120 --> 37:45.440
very um deep issue I would say which is inherent in in most of the model agnostic methods where you

37:45.440 --> 37:51.280
manipulate your data see what how the model prediction changes and then create your explanations

37:51.280 --> 37:56.560
out of this sort of select the shadowy value line partial dependence plot feature importance

37:56.560 --> 38:03.600
they all work with this mechanism of manipulation of the data prediction and then kind of aggregating

38:03.600 --> 38:12.880
the results and most manipulations happen in isolation so that you for example when you

38:14.640 --> 38:19.840
for feature importance you can meet one of the features as I like said before and then

38:21.120 --> 38:24.240
well you break the association of target but also a few other features

38:24.960 --> 38:30.800
but similar things happen if you use lines or you kind of replace parts of your image

38:30.800 --> 38:35.440
but then again you also have to replace it with something like which is I think in line the

38:35.440 --> 38:41.120
defaults with just a gray image and then of course it's not like it's outside of your data

38:41.120 --> 38:46.640
distribution subtly because your network was not confronted with like these patchy images before

38:46.640 --> 38:52.080
they had like just normal photographs usually and depends on your neural network but I mean you

38:52.080 --> 38:57.680
certainly didn't train it on on images where parts were grayed out so it's pretty likely what the

38:57.680 --> 39:03.760
model should predict and what will predict at this point but but you use these images to

39:04.480 --> 39:08.960
create your data set like you send it through the neural network you get predictions and

39:08.960 --> 39:15.600
you kind of aggregate from this your explanation but you left your data distribution and your model

39:15.600 --> 39:21.920
can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know

39:22.640 --> 39:28.800
like it like a simple example from the medical field would be that you know height and weight

39:28.800 --> 39:35.920
are highly correlated right and and on the other hand sort of the ratio or some relationship between

39:35.920 --> 39:41.520
your your weight to your height that actually has very important medical consequences right that's

39:41.520 --> 39:47.120
that's the measure of of health and so if I were to sit there and just permute say the height index

39:47.120 --> 39:50.800
and create a whole bunch of people that had all these bizarre combinations of

39:51.360 --> 39:55.920
of height and weight you know first of all those don't even probably exist in the data set and the

39:55.920 --> 40:02.800
ones that do exist in the data set probably had some medical issues right yeah well you actually

40:02.800 --> 40:06.560
gave a similar example I think you gave the example Christoph of a baby that earns a hundred

40:06.560 --> 40:11.040
thousand dollars a year which is which is insane but when you talk about something like lime

40:11.040 --> 40:16.400
maybe that's different because the cnn the what you know it shines a flashlight over the input

40:16.400 --> 40:21.440
space in it and it's a kind of local method so in some sense you could argue that it doesn't

40:21.440 --> 40:25.280
matter that you've grayed out all this other stuff because if the model was sufficiently well

40:25.280 --> 40:29.440
trained in the first place it should hopefully learn to ignore the background or is that just

40:29.440 --> 40:33.920
wishful thinking that's an interesting thought I haven't thought about it because like the property

40:33.920 --> 40:41.120
of like that you have these filters that trust a wander over the image yeah maybe it would make

40:41.120 --> 40:47.200
it more robust for these kind of interventions that we do when we create these images with lime

40:47.200 --> 40:54.720
and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these

40:54.720 --> 41:00.400
ways in which interpretability methods can go wrong right how the model might not be a realistic

41:00.400 --> 41:04.880
the interpretability model might not be a good approximation to the actual ml model

41:04.880 --> 41:09.280
so some people a bit controversially perhaps take the idea and remember that and say you're just

41:09.280 --> 41:14.080
working up completely the wrong tree and you should give up using interpretability

41:14.080 --> 41:19.520
interpretability methods to explain these black boxes this dish them instead just use an interpretable

41:19.520 --> 41:24.960
model to begin with use a white box model I think there's an example um from compass in the US which

41:24.960 --> 41:30.880
is that model to predict reoffending and I think quite famously there was a investigative journalists

41:30.880 --> 41:36.320
that tried to interpret this model it was a black box model because it's proprietary right it's a

41:36.320 --> 41:42.320
trade secret and they they fit it a proxy model a kind of a linear model and they made a report

41:42.320 --> 41:47.120
saying okay we think your model is racist because it looks like it's it's taking race as a factor

41:47.680 --> 41:51.680
and then some further work was done and they came back and said well actually you've just used a

41:51.680 --> 41:56.240
uh interpretability model that doesn't really fit our model very well you've made some assumptions

41:56.240 --> 42:00.320
that don't hold if you use a different interpretability model you get a completely different answer

42:00.320 --> 42:05.040
that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong

42:05.040 --> 42:10.640
assumption by using a bad interpretability model and I think they were saying that this model

42:10.640 --> 42:15.680
instead you could get just just as good a model of reoffending with like three FL statements you

42:15.680 --> 42:22.160
know ditching this massive complex 100 and something features and just use three FL statements

42:22.160 --> 42:27.600
based on I think age and reoffending so is that was that what we should do should we just drop

42:27.600 --> 42:32.560
these methods and start using white boxes instead so I mean like one one thing to mention here is

42:32.560 --> 42:38.320
that a white box is very soon also like a gray or black box if you add interactions if you have

42:38.320 --> 42:44.160
many features and so on but putting that aside I would agree if you're in first place like that you

42:44.160 --> 42:49.840
say you should start with like a white box so if you start modeling then then you then you should

42:51.040 --> 42:55.360
consider these first like maybe they already solved your problem then it's perfect and you have a model

42:55.360 --> 43:04.880
that is quite I mean stable it's interpretable I think that would be great but then I think the

43:04.880 --> 43:09.520
next step would be to see like what like a black box or a machine learning model would give you in

43:09.520 --> 43:15.440
terms of performance and then maybe if you see the gap is really big then maybe you can try

43:15.440 --> 43:20.800
some feature engineering and close the gap maybe from the interpretative model to the machine learning

43:20.800 --> 43:27.200
model but then you're probably already infusing some features that are not so interpretable

43:27.200 --> 43:32.720
or maybe if you're using a linear regression model you're maybe using then splines and

43:32.720 --> 43:38.800
interactions so you're already moving towards more complex models usually but then if you still have

43:38.800 --> 43:46.240
a gap then I think you have to decide is the gap and predictive performance like worth changing to

43:46.240 --> 43:52.480
a black box model so I think that's your decision will be different in many cases

43:53.760 --> 43:56.720
as you relates back to the point you made on your paper about criticisms of using

43:57.360 --> 44:01.200
interpretable machine learning models that some people leap straight away at using an overly

44:01.200 --> 44:06.640
complex model and sometimes depending on the situation sometimes you know a linear model can

44:06.640 --> 44:11.360
do just as well and have all these advantages it's so much easier to explain do you have a

44:11.360 --> 44:17.760
philosophy from a high level here right because if it if it were a human if it were an airplane

44:17.760 --> 44:24.880
pilot we don't really understand how the brain works right we would just test the pilot you've

44:24.880 --> 44:31.680
got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't

44:31.680 --> 44:35.520
really seek to understand how his or her brain works but with machine learning models there's

44:35.520 --> 44:40.720
this continuum right so if you use these complex black box models the predictive performance is

44:40.720 --> 44:46.160
usually better but you're trading off understandability and assuming those things are completely

44:46.160 --> 44:50.640
mutually exclusive what kind of decision process do you go through when you select these models

44:50.640 --> 44:55.600
but by the way with machine learning right the reason why we use machine learning is because

44:55.600 --> 45:02.640
we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say

45:02.880 --> 45:08.960
um when when it ate us like so high-dimensional so complex many interactions and so on

45:10.240 --> 45:16.000
that your simple models don't cover the complex cannot cover the complexity I think then you

45:16.000 --> 45:22.560
need machine learning would you rather understand exactly how the plane worked or would you

45:22.560 --> 45:26.480
rather I mean if I was saying to you you can go and fly in a plane would you rather that you

45:26.480 --> 45:31.200
understood how the plane worked or would you rather that the plane was tested why not both

45:32.000 --> 45:40.160
so um I think we can do both it's to some degree so um of course with black box model we don't

45:40.160 --> 45:46.000
exactly understand how they work um but in comparison to a pilot we can test them for

45:46.000 --> 45:52.720
three more or less um so because I mean maybe it's not as good as an interpretable model

45:53.520 --> 45:59.520
but we still can use a lot of methods to at least approximate and and try to understand a few

45:59.520 --> 46:05.840
properties of this model so I think we are even in a situation where we don't have like these

46:05.840 --> 46:13.280
complete like A or B decisions but we can have so if if the machine learning works much better

46:13.280 --> 46:19.280
and it's like really robustly tested with lots of different data I would prefer machine learning

46:19.280 --> 46:28.480
model I guess um but then I would also want to like people to to apply all these methods that

46:28.480 --> 46:32.720
are available even if they are not perfect but still they give you something they give you some

46:32.720 --> 46:39.920
insights so yeah and I think so Tim one answer to your question is that a lot of people's response

46:40.640 --> 46:44.960
here and kind of demanding interpretability and having concerns about machine learning

46:44.960 --> 46:49.600
it all comes down to generalizability and we've seen through using machine learning

46:50.160 --> 46:57.520
that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser

46:57.520 --> 47:04.320
you know is really great at dispensing it dispensing soap you know 87 percent of time but

47:04.320 --> 47:10.640
it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to

47:10.640 --> 47:15.200
people with a certain skin color like we've kind of decided is a society that are that there are

47:15.200 --> 47:21.360
certain generalizations or certain dimensions along which our models just have to perform

47:21.360 --> 47:26.720
and also because a lot of these these things that break machine learning models are things

47:26.720 --> 47:32.640
that happen quite quite regularly in the real world it's like a pilot you know a human being

47:32.640 --> 47:38.160
pilot flying around if he looks down at the ground and sees a hot air balloon with a big

47:38.160 --> 47:43.120
smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot

47:43.120 --> 47:48.320
about the uh the hot air balloon contest that's going on today where's a machine learning model

47:48.320 --> 47:53.440
if it looks out a camera and sees something with a particular shape of lightning bolt you know it

47:53.440 --> 47:57.840
might just decide it's time to like dive for the ground right and crash the plane like that's

47:57.840 --> 48:03.040
sort of what these adversarial examples kind of show and I think that's why people are really

48:03.040 --> 48:09.280
hungering for human understandable explanations because still to this day the human brain

48:10.000 --> 48:16.960
is the only AGI really that that we have around yeah but deep learning models that they they

48:16.960 --> 48:22.960
essentially memorize lots and lots of things and they have this sparse coding so in a way it's just

48:22.960 --> 48:27.360
like the white box model even if we use interpretability methods we could enumerate all of the

48:27.360 --> 48:33.040
things that they are learning and one of those things might be a sensitivity or lack of sensitivity

48:33.040 --> 48:38.160
to hot air balloons or smiley faces on but even if we could enumerate all the things that they are

48:38.160 --> 48:43.440
learning we wouldn't understand that either in the same way we don't understand how a real human's

48:43.440 --> 48:48.080
brain works and I'm not sure whether we should view a human brain as a computer program and whether

48:48.080 --> 48:54.320
that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have

48:54.320 --> 48:59.600
to accept that we're not going to understand a totally it's actually it's a good comparison

48:59.600 --> 49:04.320
I think with humans you know if you're interviewing and you want to hire let's say a software developer

49:05.040 --> 49:09.200
you tend to set them a coding interview you wouldn't think of taking them into surgery

49:09.200 --> 49:12.880
opening up their brain and trying to find the neuron that predicts what the next you know

49:12.880 --> 49:17.200
but if code is going to be and understanding how that works it's just why would you do it that way

49:17.200 --> 49:22.000
instead you learn to trust humans by working with them giving them a test seeing how they

49:22.000 --> 49:27.120
perform in the real world and I know maybe we're asking too much of a machine learning model if

49:27.120 --> 49:32.720
you want to be able to understand these complex things in terms of like a bottom-up white box

49:33.280 --> 49:39.280
set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can

49:40.400 --> 49:45.200
cut off the like cut open the brain of a machine learning model without breaking it

49:45.200 --> 49:54.160
and without hurting it hopefully and we can do all these try out all these interpretation methods

49:54.160 --> 49:59.280
see how it behaves under certain situations and I also would make a distinction between

49:59.280 --> 50:04.800
we understand what's going on inside and doing like this kind of sensitivity analysis

50:04.800 --> 50:11.360
where we just try out what happens in certain scenarios so it always do that that we can like

50:12.320 --> 50:17.680
check like how it behaves so I mean feature importance is basically like a way to see

50:17.680 --> 50:21.760
like how does it behave if we break some features and then we rank the features by this

50:21.760 --> 50:28.000
as an importance we can do it and that's also the big difference between humans because we

50:28.000 --> 50:33.520
can't test in the same way and yeah shouldn't probably. I think there's one other difference

50:33.520 --> 50:39.200
though which is to do with the substrate of how neural networks work I think if I'm giving someone

50:39.200 --> 50:44.960
a job interview or something I mean of course it's a very valuable process but I'm looking at

50:44.960 --> 50:49.360
their values and I'm looking to try and understand how they would behave in different situations

50:49.360 --> 50:53.920
and I'm coming up with lots of illustrative examples but the difference is with humans

50:53.920 --> 51:01.120
we have that level of generalization we have a kind of guiding taxonomy of behaviors which means

51:01.120 --> 51:07.040
if I know if I have guiding examples of what a human will do in certain situations I expect

51:07.040 --> 51:12.720
that to generalize whereas my hypothesis is is that a deep neural network model is almost like

51:12.720 --> 51:17.440
an infinite number of rules and there's absolutely no carryover between the rules

51:17.440 --> 51:23.360
so knowing even some or even most of the rules doesn't really tell me about those edge cases.

51:25.920 --> 51:32.000
Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know

51:32.000 --> 51:37.280
that they exist like with adversarial examples so even if we don't know like exactly what they

51:37.280 --> 51:42.800
would look like or there's even an infinite amount of uh I mean there's an infinite amount of like

51:42.800 --> 51:50.480
how you can change the image to make it like have a different class so we know so I think it's important

51:50.480 --> 51:56.480
that we know that these exist at least. Yeah I love the point you made though that we have we

51:56.480 --> 52:02.640
have the luxury to do analysis on these on these boxes because we can open them up and that that's

52:02.640 --> 52:06.880
another point I'm pretty sure that you make this in your book as well which is that part of this is

52:06.880 --> 52:14.240
just scientific inquiry it's it's like understanding better how to interpret and explain machine

52:14.240 --> 52:20.160
learning models will probably actually contribute to us being able to construct even better machine

52:20.160 --> 52:25.520
learning models isn't that true? So so you're basically saying that um also interpretability

52:25.520 --> 52:31.120
might help to to be better at like create better machine learning models themselves?

52:32.240 --> 52:37.200
Yeah like as we as we develop these interpretability methods because in a sense like you point out

52:37.200 --> 52:43.040
earlier there are statistical projections of kind of the behavior of the model and so like a saliency

52:43.040 --> 52:48.480
map you know if we can if we can kind of use that to learn the way in which the the neural networks

52:48.480 --> 52:54.720
are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I

52:54.720 --> 53:01.920
also have seen approaches where they try like kind of fuse also these two worlds like interpretative

53:01.920 --> 53:06.880
models or white box models and black box models so that you try to to generate features out of

53:06.880 --> 53:14.960
the black box model which you then use in your more understandable white box model so um I think

53:15.040 --> 53:21.440
this and and this also like by using similar techniques um to which you would use for interpretability

53:21.440 --> 53:28.080
like detecting interactions for examples for example so um yeah these can be used also to

53:28.080 --> 53:34.400
to build better models and also to build better interpretable models. The other and I'll make

53:34.400 --> 53:40.160
one last point here which is another social good that can come out of interpretability is

53:40.160 --> 53:45.440
imagine we've got you know an ML model that's not trying to make any decisions but it's just

53:45.440 --> 53:52.080
trying to figure out what leads to happiness and success in life you know and so we analyze a whole

53:52.080 --> 53:57.360
bunch of data and we find out well it's really important if you graduate from high school and

53:57.360 --> 54:01.680
it's really important if you you know don't have children before you're married and you know all

54:01.680 --> 54:07.440
these other factors if we can dive in and kind of isolate those factors it actually allows people

54:07.440 --> 54:12.320
to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of

54:12.320 --> 54:17.920
data and it actually has some some understandable recommendations for how to lead a healthier

54:17.920 --> 54:24.000
life or a better life or whatever. Yeah I think that this very good example were the fact that

54:24.000 --> 54:31.760
you have a prediction model doesn't solve your problem so actually it's just a means to to some

54:31.840 --> 54:38.640
other goal in this case understanding like what are the factors for happiness and one example I am

54:38.640 --> 54:42.800
from a friend who worked at a telecom company and they built like a churn prediction model

54:44.080 --> 54:51.920
to see like who will quit the telecom contract and then they started like the ones with the

54:51.920 --> 54:56.640
highest likelihood they started sending out emails say maybe offering them a better deal

54:57.440 --> 55:04.480
but actually the outcome was that well they they when they once they wrote to the customers they

55:04.480 --> 55:11.920
well left and quit their contract so it's kind of had like so this is a case where the prediction

55:11.920 --> 55:16.240
model actually works but then they people leave in this case probably because they

55:16.240 --> 55:23.920
realized ah shit I have this contract still going on time to quit now so if you knew the

55:23.920 --> 55:30.800
reasons why they are likely to churn then you could like better select like when you write some

55:30.800 --> 55:37.040
email or maybe some other campaign or when maybe not to write anything at all yeah right now um

55:37.040 --> 55:42.080
Christoph you have a background in stats which means you I mean you like Connor as well we take

55:42.080 --> 55:47.360
an incredibly dim view of machine learning and you wonder how how is it possible for us to be

55:47.360 --> 55:51.920
stabbing in the dark like this but you know you said that we need to be more rigorous and

55:52.000 --> 55:57.520
there's no quantification of uncertainty with the current IML methods and I suspect you might

55:57.520 --> 56:01.840
be working on some methods behind the the scenes on this but you know when you have models and

56:01.840 --> 56:07.280
explanations which are computed from data they are subject to uncertainty and that's just not

56:07.280 --> 56:11.760
modeled at all at the moment right so we need to be making some distributional and structural

56:11.760 --> 56:15.920
assumptions that we're not making now and you point out that there's this phenomenon of p-hacking

56:15.920 --> 56:20.320
which is a huge problem in the natural sciences which hasn't quite made its way to IML methods

56:20.320 --> 56:27.520
yet but probably will do yeah so yeah the I think and in statistics we're really good at

56:27.520 --> 56:32.400
quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on

56:32.400 --> 56:38.240
but I still would say it's better to have um not only just one number or one explanation

56:38.240 --> 56:45.440
but also have the distribution to do of this explanation or this number and to quantify what

56:45.440 --> 56:50.800
uncertainties behind computing this number so when you have a linear model then you get

56:52.160 --> 56:56.960
some coefficient which you interpret in the end but usually you don't just interpret the

56:56.960 --> 57:02.560
coefficient but you look at the confidence intervals but we don't do it at the moment for

57:02.560 --> 57:08.400
interpretability so you maybe get the saliency maps but how certain are you about maybe it's

57:08.400 --> 57:13.920
a bad example because we don't so much on it but if you have like a feature importance value and

57:13.920 --> 57:19.040
you get some result how like what's the range actually like how much variance is behind it

57:19.040 --> 57:24.480
if I were to use slightly different data or refit my model again how similar would the number be

57:25.600 --> 57:30.400
and I think that's something that will or should come to interpretability as well

57:31.840 --> 57:36.080
it's funny how when we come to machine learning it's almost like open season and forgetting

57:36.080 --> 57:39.680
everything you know about maths and stats you throw it all out the window okay so excited

57:39.680 --> 57:45.360
about these algorithms right like one example is if you take a if you're fitting a model to predict

57:45.360 --> 57:50.240
something that was unlikely I don't know maybe it was like a covid test for example and then

57:50.240 --> 57:53.760
if you know the prevalence of covid you get it back you kind of know what the false positive

57:53.760 --> 57:58.640
rate is going to be and so you notice that you think it's the multiple comparison issue right

57:58.640 --> 58:01.920
you know that you're expecting a certain level of false positives when it comes to doing something

58:01.920 --> 58:08.080
like feature importance or looking at interpretability from a thousand features and then five come

58:08.160 --> 58:12.800
through is really really important you mentioned sometimes we just forget that multiple comparison

58:12.800 --> 58:16.800
issue forget the fact that probably these five are going to be completely false positives and

58:16.800 --> 58:21.520
probably completely meaningless yeah I agree especially if you have like these high-dimensionality

58:21.520 --> 58:26.880
features and for the record I have to say I mean there are already approaches so especially for

58:26.880 --> 58:33.200
feature importance because there's like a huge community in random forests for example and they

58:33.200 --> 58:37.760
thought a lot about these issues and their tests for this and stuff like it but for the rest of

58:37.760 --> 58:42.560
interpretability I think it could gain a lot thinking more about I mean this is very simple

58:42.560 --> 58:48.480
stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians

58:48.480 --> 58:55.520
think like a long time already about it and I mean if you even if you leave the area of

58:55.520 --> 59:00.640
interpretability and look at the benchmarks so even like if you have like accuracy like a table

59:00.640 --> 59:05.440
and you see accuracies in it but there's no variance attached to it then it should be like

59:05.440 --> 59:10.880
suspicious of it but because if you just retrain your neural network with a different seed you might

59:10.880 --> 59:15.440
end up with a different accuracy in the end so and if you want to say a method is better than

59:15.440 --> 59:22.720
another method you want to quantify how larger ranges of uncertainty do you I mean there are a

59:22.720 --> 59:27.200
lot of things like the choice of data choice of splitting points and training and test data

59:27.360 --> 59:34.800
weight initialization and so on so I think a lot of this rigor from statistics could help

59:35.360 --> 59:38.720
the machine learning community and and machine learning science to become better

59:39.840 --> 59:44.320
yeah but let's let's never forget this uh quite quite well known saying which is there are three

59:44.320 --> 59:51.520
kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right

59:51.520 --> 59:57.280
is is you know fundamentally whenever we go measure data and we have a model what we're

59:57.280 --> 01:00:03.120
actually able to extract from that data and and the model is inherently probabilistic

01:00:03.120 --> 01:00:08.480
it's a probability distribution right at the end of the day and we get into trouble anytime we try

01:00:08.480 --> 01:00:14.480
to take that probability distribution and project it to numbers i.e. statistics like as soon as we

01:00:14.480 --> 01:00:20.640
start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval

01:00:20.640 --> 01:00:24.960
or whatever the fact is we're throwing away information the totality of the information

01:00:24.960 --> 01:00:31.440
sitting there in that weird multimodal you know spread out distribution right and then we we find

01:00:31.440 --> 01:00:36.320
some way to simplify it and project it down to a set of numbers we've got a problem like that's

01:00:36.320 --> 01:00:41.200
and if you forget that that's happening if you forget that you're throwing away all this information

01:00:41.200 --> 01:00:46.560
I think that you know the tendency to do that isn't just simplification it's that oftentimes we

01:00:46.560 --> 01:00:51.680
have to use these probabilistic things to reach a decision and as soon as we get to the point where

01:00:51.680 --> 01:00:56.800
look i've got to choose either to go left or right give the loan or not give the loan as soon as we

01:00:56.800 --> 01:01:02.080
get down to some point where we have to make a concrete decision we're forced you know we're

01:01:02.080 --> 01:01:08.240
forced to project it right but along the way it's important not to lose sight of the the fact that

01:01:08.240 --> 01:01:14.320
we're throwing away information fantastic well and by the way you also said something interesting

01:01:14.400 --> 01:01:18.800
a minute ago Christoph which is about at least in most machine learning algorithms if you change

01:01:18.800 --> 01:01:22.960
the random seed you know that there's enough stability there that it still gives you roughly

01:01:22.960 --> 01:01:28.800
the the same model every time but in reinforcement learning if you change the random seed the entire

01:01:28.800 --> 01:01:34.720
thing is completely broken but but yeah what Keith was saying about this this this information

01:01:34.720 --> 01:01:39.280
and structure in models I think that's really interesting because people have said with reinforcement

01:01:39.280 --> 01:01:43.680
learning you can actually learn causal factors right but that's not really true you're interacting

01:01:43.680 --> 01:01:47.840
with the system but what you're learning is is a surface representation of causal factors so you

01:01:47.840 --> 01:01:54.320
might learn that there's a causal factor between like a hose putting out fire but it wouldn't

01:01:54.320 --> 01:01:58.960
actually learn that it was the water that put out you know that there was a causal relationship

01:01:58.960 --> 01:02:03.040
between the water and the fire and this is the case with so many of our models as we were saying

01:02:03.040 --> 01:02:06.960
earlier that there's there's just a a surface representation which doesn't actually represent

01:02:06.960 --> 01:02:11.680
the reality of our world at all but this brings me on to the next point because you have a real

01:02:11.680 --> 01:02:17.520
problem with causal interpretations of some of these iml metrics right and you you say that models

01:02:17.520 --> 01:02:21.600
well the the goal of models is that they should reflect the causal structure right this is what

01:02:21.600 --> 01:02:26.480
we want to do in science but most statistical learning just reflects these surface feature

01:02:26.480 --> 01:02:30.960
correlations they they don't even scratch the surface of what we want so what are we going

01:02:30.960 --> 01:02:35.760
to do right are you doing some work in this field to help us out here and and why are people making

01:02:35.760 --> 01:02:42.800
these fallacious interpretations yeah so so i'm not not working on anything causality related at the

01:02:42.800 --> 01:02:50.480
moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and

01:02:50.480 --> 01:02:58.160
master and zero i mean the only time we were talked about causality was when i heard the

01:02:58.160 --> 01:03:05.120
sentence correlation does not imply causality and it was really about it so um i think it's

01:03:05.200 --> 01:03:11.040
like really um yeah should be taught a lot more like how to think about causality like

01:03:11.040 --> 01:03:17.680
just super simple things like you should include confounders or um like what types of features

01:03:17.680 --> 01:03:23.520
if you include them in the model um like destroy your causal interpretation of another features

01:03:23.520 --> 01:03:28.960
these these are not super difficult things so um then you don't have to like learn any like

01:03:28.960 --> 01:03:35.680
difficult frameworks to work with or read like causality books on it that's like super simple

01:03:37.200 --> 01:03:43.520
yeah rules of thumb for your features even um yeah and i think you also have to decide

01:03:44.080 --> 01:03:50.800
or distinguish between like what's the goal of your model do you want a causal interpretation

01:03:52.640 --> 01:03:57.200
or do you want to like because in a sense and you have to also distinguish between

01:03:57.200 --> 01:04:01.760
two levels you have the real world level and the model level i mean once you use features for the

01:04:01.760 --> 01:04:06.240
model they are causal for the model prediction of course because you designed it that way

01:04:06.240 --> 01:04:10.000
and the question is when you are you allowed to go to the real world level where you say

01:04:10.000 --> 01:04:14.400
okay this um the feature importance that i see here or also the feature dependence

01:04:15.120 --> 01:04:23.600
plot that i see is also causal um or and may interpret it as a cause and or as a causal effect

01:04:23.600 --> 01:04:31.040
also for for the real world and i think that also depends like if you need this interpretation

01:04:31.760 --> 01:04:38.480
if you do scientific modeling for example then you probably want it um but that there can also

01:04:38.480 --> 01:04:42.960
be good reasons to include non-causal features into your model if your goal is really just

01:04:42.960 --> 01:04:48.640
prediction and and some feature might help you with the with a good prediction um but it might

01:04:48.720 --> 01:04:53.920
not be causal at all yes but the problem is when we're using these deep learning models

01:04:54.560 --> 01:05:00.720
they they will learn a structure which probably has no relationship to the real world whatsoever

01:05:00.720 --> 01:05:07.600
but um i think causal um factors do generalize much better there's the example of um i don't know

01:05:07.600 --> 01:05:13.360
with car crashes right male testosterone levels is a causal factor so that will probably generalize

01:05:13.360 --> 01:05:18.240
to other locations where you didn't train your data on but unfortunately models don't really do that

01:05:18.960 --> 01:05:24.000
well but so just real quickly on that tim like the only reason that we know testosterone

01:05:24.560 --> 01:05:30.400
is a causal factor is is not from that data set it's from a bunch of mechanistic you know

01:05:30.400 --> 01:05:37.440
scientific research and biology and and elsewhere um so you know i i'm kind of wondering how

01:05:38.320 --> 01:05:42.800
it would be nice if at least machine learning methods could indicate that there may be the

01:05:42.800 --> 01:05:49.600
possibility of a causal structure so just looking for underlying hidden structures um that that you

01:05:49.600 --> 01:05:55.440
know they're more generalizable that could explain large pieces of the of the data and give kind of

01:05:55.440 --> 01:06:01.680
a list of hey there might be a causal factor here like go investigate it but on on that there's a

01:06:01.680 --> 01:06:06.560
difference between a causal factor and a causal structure i think that the challenge is that we

01:06:06.560 --> 01:06:11.120
don't have enough fidelity in the structure that benjo by the way is doing some interesting work on

01:06:11.120 --> 01:06:17.680
this using data driven approaches to um you know learn causal factors but it's the structure of the

01:06:17.680 --> 01:06:21.360
factor graph which i think is the important thing i mean this is one of the most interesting parts

01:06:21.360 --> 01:06:25.840
i think of machine learning actually trying to learn causation from a data set which you can do

01:06:25.840 --> 01:06:30.320
right things like beta networks where you specify all your variables you connect these nodes with

01:06:30.320 --> 01:06:36.400
edges and you can try to learn the optimal structure like the simplest structure and that

01:06:36.400 --> 01:06:42.240
sometimes turns out to be the real life causal effect if you do it well but the difference is

01:06:42.240 --> 01:06:49.200
you as a human you you know the causal structure and you've you've created that that graph so it's

01:06:49.200 --> 01:06:53.760
not learned you've created it you can learn these things from data right you can actually you can

01:06:53.760 --> 01:06:59.200
search over the set of all possible graphs all the possible edges and you have a bit of a loss

01:06:59.200 --> 01:07:03.360
function you try to find a graph that fits the data well so it's got enough edges but it's not too

01:07:03.360 --> 01:07:08.160
complex you're not relating everything to everything and so just from data without any human input

01:07:08.160 --> 01:07:13.680
with structure learning you can sometimes get a model that kind of out of nothing will give you

01:07:13.680 --> 01:07:18.560
the cause of relationships sometimes there is redundancy right because a graph that says a

01:07:18.560 --> 01:07:25.600
implied near causes b equals a c that's identical to c causes b causes a right but even then with

01:07:25.600 --> 01:07:31.280
structure learning you you've got this adjacency matrix and all of those nodes you've already come

01:07:31.280 --> 01:07:36.800
up with a priori so what you want to learn is what the nodes are themselves right yeah i think

01:07:36.800 --> 01:07:41.200
like what kona mentioned there's lots of moda setting where you have well defined features

01:07:41.920 --> 01:07:47.280
and i think what tim referred to was more like the you don't even know what the features are like

01:07:47.280 --> 01:07:53.200
if you have a convolutional neural network and and like what's an object um what is a feature that

01:07:53.200 --> 01:07:59.280
like is disentangled also from other objects um so i think also there is the big issue that you

01:07:59.280 --> 01:08:05.120
have this entanglement between concepts that i don't know that the frisbee is always with uh on

01:08:05.120 --> 01:08:12.960
the same image as a as a dog so um maybe the the neural network can't even separate these two things

01:08:12.960 --> 01:08:19.600
then because they are too entangled in the data set to even discover the structure that that is

01:08:19.600 --> 01:08:24.880
really underlying the the real world in this case that's a fascinating point actually because one of

01:08:24.880 --> 01:08:29.600
the reasons why there's no easy solution to adversarial examples is because you you learn these

01:08:29.600 --> 01:08:35.120
these non robust features and you might just think to yourself well um fur is a low magnitude

01:08:35.120 --> 01:08:39.120
feature it's really easy just to kind of create fur on anything and for the neural network into

01:08:39.120 --> 01:08:43.440
thinking it's a cat and you just say well this is obvious right you just create some rules to say

01:08:43.440 --> 01:08:49.760
well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled

01:08:49.760 --> 01:08:54.480
in this complex neural network so you can't do that but i wanted to move the discussion on a bit

01:08:54.480 --> 01:08:59.920
so you said that there are some really interesting challenges ahead in in iml and what's fascinating

01:08:59.920 --> 01:09:03.760
is you start talking about the process so you say that the setting of machine learning is too static

01:09:03.760 --> 01:09:08.400
it doesn't reflect how these models are used in reality and models are embedded in a process or a

01:09:08.400 --> 01:09:13.280
product or even complex people interactions and i love this right because i talk about ml devops

01:09:13.280 --> 01:09:17.680
and machine learning models in isolation are irrelevant it's the people in the process that's

01:09:17.680 --> 01:09:22.560
where the complexity is even with intelligence itself it's a process right you know intelligence

01:09:22.560 --> 01:09:27.680
is the interaction between a brain a body and an environment and you know within the context of

01:09:27.680 --> 01:09:31.760
this process you know we've got all of this rich information that we could be bringing in from other

01:09:31.760 --> 01:09:36.480
disciplines and you're saying we should bring in compsci and stats folks and we should be bringing

01:09:36.480 --> 01:09:41.440
in psychologists and social scientists and we need to also have interpretability at a higher level

01:09:41.440 --> 01:09:46.480
at the institutional level right or at the society level so when you kind of broaden the discussion

01:09:46.480 --> 01:09:53.280
out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist

01:09:53.280 --> 01:09:59.280
it's so convenient to just have this fixed model a fixed data set and then you just geek out and

01:09:59.280 --> 01:10:05.600
invent all these methods and so on but reality is that that you use the method some place and then

01:10:05.600 --> 01:10:11.600
it interacts with the institution it's built with the developers it's built by with the people it

01:10:11.600 --> 01:10:18.720
affects and my favorite example there is when you have this closed loop where your model makes

01:10:18.720 --> 01:10:25.680
predictions and these predictions generate the next generation's data so for the next generation

01:10:25.680 --> 01:10:31.840
of the model it produces the data so there's this example of the rent index where you have this model

01:10:31.840 --> 01:10:38.560
that tells you how much rent you should like pay for a certain kind of apartment and so on

01:10:39.040 --> 01:10:47.040
and this is actually like um legally binding so if you're a land lord you have to accept kind of

01:10:47.040 --> 01:10:52.800
the range that is outputted by the model which also means that the data that is produced so the

01:10:52.800 --> 01:10:59.760
new flats that are rented out in new apartments they all have to fit the model kind of and then

01:10:59.760 --> 01:11:05.040
but then you use this data again to train your model so you have this very weird feedback loop

01:11:05.760 --> 01:11:11.680
and I think it's also difficult to wrap your head around it and understand implications of it

01:11:13.600 --> 01:11:21.040
that that same thing a very similar feedback loop was a fear in the you know in our algo

01:11:21.040 --> 01:11:27.360
shambles video about the uk testing since they couldn't conduct the the uh what was it the

01:11:27.360 --> 01:11:32.320
a-level test right Tim they they built some some models around that and so it would do things like

01:11:32.320 --> 01:11:37.520
well you know if this school historically never had anyone in this grade bucket then we're not

01:11:37.520 --> 01:11:43.040
going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating

01:11:43.600 --> 01:11:50.880
you know feedback loop we were reading through a lot of your work and you I mean I'm just going

01:11:50.880 --> 01:11:56.640
to hit this point head on um you don't really talk that much about AI ethics and you know there's the

01:11:56.640 --> 01:12:01.600
f word which is the fairness word and and I don't I don't recall you ever using that word

01:12:01.600 --> 01:12:09.040
and is that something that you've deliberately shied away from um yeah I just like um

01:12:10.320 --> 01:12:16.400
define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness

01:12:16.400 --> 01:12:21.920
metrics or so on because I think there's a really big field on its own and I just don't know as much

01:12:21.920 --> 01:12:26.160
like about all these things so I know a little bit like about the fairness metrics that they're

01:12:26.160 --> 01:12:34.080
out there um I also think I mean they're kind of like research wise a little bit overlapping but

01:12:34.080 --> 01:12:39.040
more or less separate fields I think interpretability and fairness um but of course they have some

01:12:39.040 --> 01:12:46.160
commonalities that I mean when you kind of to for fairness you have to look it's not necessarily

01:12:46.160 --> 01:12:52.960
inside the model but you have to study how the model behaves um and that's kind of the connection

01:12:53.040 --> 01:12:58.800
to interpretability I would say yeah well where yeah where I see the connection is work

01:12:58.800 --> 01:13:06.240
like yours is helping to build the tool set that will allow people to apply human you know intuition

01:13:06.240 --> 01:13:12.560
and and ethics and evaluations to machine learning because at the end of the day a lot of these are

01:13:12.560 --> 01:13:18.640
human moral judgments or ethical judgments and it's important that people be happy with them

01:13:18.640 --> 01:13:24.240
because you know we have to have the population as a whole understand and accept and be able to

01:13:24.240 --> 01:13:30.000
move forward with the increasing role that machine learning is having in our lives and building that

01:13:30.000 --> 01:13:34.800
tool set is necessary so it's like you said very early in this talk you know what do we do just

01:13:34.800 --> 01:13:39.760
stick our heads in the sand and ignore it and just accept machine learning models are going to do

01:13:39.760 --> 01:13:45.120
whatever they do as long as they fly the plane or you know don't kill too many people we're okay

01:13:45.120 --> 01:13:49.520
like I don't think that's going to work like we have to build the tool set that you're talking

01:13:49.520 --> 01:13:56.080
about and continue this process of exploring how to better explain and interpret ML models so that

01:13:56.080 --> 01:14:01.200
human beings can have that oversight because it's the only thing that's going to give us

01:14:01.200 --> 01:14:06.960
comfort really as a society I suppose the reason I segue to this is we were just talking about

01:14:06.960 --> 01:14:12.800
the process and you you mentioned some of these feedback loops because we can have a very superficial

01:14:12.800 --> 01:14:18.720
discussion and you could say well we need to be able to represent reality better than we do and

01:14:18.720 --> 01:14:24.560
we have a whole tool set here to identify sources of bias or you know lack of robustness etc in

01:14:24.560 --> 01:14:29.920
models but it's so much more complex than that because these models are used in a very complex

01:14:29.920 --> 01:14:35.760
process and you get these very very complex dynamics emerging as a result of that and I think

01:14:35.760 --> 01:14:41.520
we're only really just scratching the surface of understanding those dynamics yeah I think so too

01:14:42.000 --> 01:14:49.360
as I said I think in science it's always very easy to to study things in isolation like study one

01:14:49.360 --> 01:14:56.640
type of model study one type of adjustment for a deep neural network and hopefully we will see more

01:14:56.640 --> 01:15:02.720
work emerge on this I think I've never read the paper like I mean of course discussed implications

01:15:02.720 --> 01:15:09.440
but really like analyze like what happens in terms of the data and the model when we have like

01:15:09.440 --> 01:15:14.640
multiple generations for example of a model and how it changes over time but this thing I mean to

01:15:14.640 --> 01:15:21.280
study those things also means that you have to wait for a long time until you have these dynamics

01:15:21.280 --> 01:15:27.120
and I think in many cases it's just starting that we use these models more extensively in our daily

01:15:27.120 --> 01:15:34.640
life I have a question is is anyone because look interpretability metrics whatever they are say

01:15:34.720 --> 01:15:39.680
saliency maps and you know could be partial dependency plots whatever you could actually

01:15:39.680 --> 01:15:45.360
build in some requirements of those into the objective functions when you go to train models

01:15:45.360 --> 01:15:49.520
so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at

01:15:49.520 --> 01:15:55.920
all but somebody could say look I want all my saliency maps to be you know sets of of a

01:15:55.920 --> 01:15:59.360
bezier curves or something like that like they have to have a certain smoothness

01:16:00.000 --> 01:16:04.880
property and you could actually put that as a constraint in the objective function has anybody

01:16:04.880 --> 01:16:10.160
tried anything like that yeah there are approaches so I saw one paper they added some

01:16:12.080 --> 01:16:18.000
some parts to their objective function so that when you create line explanations with line that

01:16:18.000 --> 01:16:25.200
they were most more stable and there are a lot of things like for neural networks you have

01:16:25.200 --> 01:16:33.280
disentanglement that you try that the feature maps or that the nodes learn disentangled concepts

01:16:34.640 --> 01:16:40.320
there are ways to introduce like monotonicity so that a feature can always go into the effect of

01:16:40.320 --> 01:16:47.600
a feature can always be in one direction not to like zigzag around so there are approaches to do

01:16:47.600 --> 01:16:57.200
this to like have okay like interpretability constraints in your modeling here yeah because

01:16:57.200 --> 01:17:00.960
I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we

01:17:00.960 --> 01:17:07.280
just create white box models maybe we can use if the definition to a human being of white box is

01:17:07.280 --> 01:17:12.000
that it's interpretable and understandable if we can build into the objective functions when we're

01:17:12.000 --> 01:17:17.200
actually training the network that it has these properties then we'll actually be helping to create

01:17:17.600 --> 01:17:23.440
more white box you know models even if they are complex that's definitely an option but I think the

01:17:24.480 --> 01:17:29.200
issue remains the same that you with it's similar to a white box model I mean you'll make some trade

01:17:29.200 --> 01:17:35.840
offs in the end you have to make the judgment whether so when you put more constraints I mean

01:17:35.840 --> 01:17:40.960
you can actually also help the model of course that if you I mean if you have some inductive biases

01:17:40.960 --> 01:17:47.760
also which which you infuse into the model which help with predicting or be more more stable

01:17:49.280 --> 01:17:54.960
but sometimes you might maybe also trade off with accuracy and you just have to like in the end you

01:17:54.960 --> 01:18:03.120
have this yeah this set of models where some are more accurate some better than this one

01:18:03.120 --> 01:18:09.440
interpretability dimension the other is cheaper to deploy and then you have this so this kind of

01:18:09.440 --> 01:18:14.160
going into direction of like automatic machine learning and you don't get like just the best

01:18:14.800 --> 01:18:20.000
performing one but you have this parater set like well so we have multiple objectives that you want

01:18:20.000 --> 01:18:24.880
to hit and then there's not one model that works best but you have a set of models that

01:18:24.880 --> 01:18:30.160
that have different trade-offs between these objectives and then you have to decide what

01:18:30.160 --> 01:18:36.960
is the trade-off that you want to do that you want to have I guess there's no getting away from it is

01:18:37.040 --> 01:18:41.680
the interpretability it is going to get more important and more important I think you mentioned

01:18:41.680 --> 01:18:46.240
Christoph that you know we've had linear models for hundreds of years and then there's been this

01:18:46.240 --> 01:18:52.800
big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability

01:18:52.800 --> 01:18:56.720
is really kicked off I what do you think do you think where's it going are we just going to get

01:18:56.720 --> 01:19:02.960
more and more attention paid to this area I don't know if you can get more than than this I don't

01:19:02.960 --> 01:19:10.560
know yeah but I think it's at least here to stay and I think it's important I mean it has been

01:19:10.560 --> 01:19:18.960
important before but of course with like the push from deep learning especially and that it just

01:19:18.960 --> 01:19:23.600
became more clear to a lot of people that we need interpretability in some sense at least

01:19:24.960 --> 01:19:31.120
yeah of course people have attempted it before and worked on it before it's just more urgent now

01:19:31.360 --> 01:19:37.760
I really like the bit in your book talking about what's changed recently how interpretability is

01:19:37.760 --> 01:19:41.920
coming together as a field you know with this a unification so you know in physics we love a

01:19:41.920 --> 01:19:45.920
big unification when you take all these different things in the past and say oh they're all just

01:19:45.920 --> 01:19:51.120
part of this one big framework and it was chap that chap paper was amazing wasn't it saying things

01:19:51.120 --> 01:19:57.520
like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of

01:19:57.520 --> 01:20:02.480
these additive feature attribution methods and we can prove that this is the only one that's

01:20:02.480 --> 01:20:07.440
theoretically valid because it has these properties of symmetry it's got the stummy property so

01:20:08.160 --> 01:20:11.040
you know everything that's been done before in interpretability well they're all in our

01:20:11.040 --> 01:20:16.080
framework now and shapely values are the way forward yeah they're quite uh quite famous to

01:20:16.080 --> 01:20:21.120
shapely values yeah would you believe them then I mean I guess in their paper they're kind of

01:20:21.120 --> 01:20:24.960
they kind of disagree with lime a bit don't they they say well lime is a count of this but

01:20:25.040 --> 01:20:29.760
they're going to be breaking our properties of efficiency and symmetry so lime is using the

01:20:29.760 --> 01:20:34.240
wrong weights right they should be using this yeah kernel shape weights rather than the line weights

01:20:34.240 --> 01:20:40.320
I think that's just a different approach also to think about it I mean you don't maybe you don't

01:20:40.320 --> 01:20:48.320
I think I think the properties are quite attractive or meaningful at least um but also also the

01:20:48.480 --> 01:20:55.840
line approach I'm very critical about lime because it um I think it's difficult to have the correct

01:20:56.560 --> 01:21:05.600
to know like how to parameterize your your local models um so I think I'm a bit more of a fan

01:21:05.600 --> 01:21:11.440
of shapely values because of the theoretical properties it comes with are you talking about

01:21:11.440 --> 01:21:15.840
that distance measure in lime where you have to be able to quantify how far away is the permutation

01:21:15.840 --> 01:21:23.600
yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and

01:21:23.600 --> 01:21:28.960
I mean it's it's a very difficult question it goes to the heart of like what's local um

01:21:29.840 --> 01:21:33.840
because like I mean you have this this kernel that decides like how much you weight

01:21:33.840 --> 01:21:39.920
all the data points around the point you want to explain and and like how how big is this area

01:21:39.920 --> 01:21:45.200
I think this is very dependent on your model and your data and there's no answer no easy answer to

01:21:45.200 --> 01:21:51.200
how to set it yeah or even more generally than that this this whole notion of what does it mean

01:21:51.200 --> 01:21:56.720
to have a local interpretation method in you know in text or vision so in in vision there's this

01:21:56.720 --> 01:22:01.120
super pixel concept which is something that seems to make intuitive sense but but does it you know

01:22:01.120 --> 01:22:06.800
when you create all of these different uh maskings of different parts of the input space but um

01:22:06.800 --> 01:22:12.000
with shapely values as well that they they are a beautiful a beautiful technique especially

01:22:12.080 --> 01:22:18.000
because the the values are are quite meaningful but if you have shared information between the

01:22:18.000 --> 01:22:22.240
features I mean Connor and I were talking about this for example you if you had the same model

01:22:22.240 --> 01:22:28.080
where you were predicting someone's income and you put their I don't know let's say you had salary

01:22:28.080 --> 01:22:34.960
in the model twice then the shapely value would be divided between the two duplicate fields right

01:22:34.960 --> 01:22:41.520
so there just seems to be so much esoterica in these IML methods right are we expected to know

01:22:41.520 --> 01:22:46.960
all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things

01:22:46.960 --> 01:22:51.520
that you have to know all the these these uh disadvantages of the methods where I try to

01:22:51.520 --> 01:22:58.640
be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools

01:22:58.640 --> 01:23:05.520
that we usually have um also with statistics and so on you you have to know like um these like as

01:23:05.520 --> 01:23:09.840
you mentioned this if you have salary twice then it will just I mean depends also on what your model

01:23:09.840 --> 01:23:16.160
does if it just picks one of the salary features or if it itself uses both so this also something

01:23:16.160 --> 01:23:22.640
that will define how the shapely value will look like later on um but you have to know these things

01:23:22.640 --> 01:23:27.520
if you want to use shapely values and interpret interpret them correctly yeah because I think

01:23:27.520 --> 01:23:32.800
philosophically we've got we've got the real behavior and then we use these interpretability

01:23:32.800 --> 01:23:37.760
methods and then we've got the kind of perceived behavior so we've got these these levels of

01:23:38.400 --> 01:23:44.720
modeling or or do you know what I mean simplification and it's all well and true if you are dealing

01:23:44.720 --> 01:23:51.520
with data scientists who understand how these you know methods work that's fine but invariably

01:23:51.520 --> 01:23:57.760
data scientists need to present this information to lay people and they are not going to understand

01:23:58.480 --> 01:24:02.480
all of the various different trade-offs and how information is being compressed and and lost and

01:24:02.480 --> 01:24:08.880
so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any

01:24:08.880 --> 01:24:17.920
number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh

01:24:17.920 --> 01:24:23.040
outcomes of statistical models that uh well everyone can understand well of course not because

01:24:23.040 --> 01:24:28.080
you need training to understand how to interpret a linear model or any regression model yeah there's

01:24:28.160 --> 01:24:34.080
difficulty but I don't think it's new in any sense um because there's always I mean any number that

01:24:34.080 --> 01:24:40.800
you read anywhere has a very complex process um so I don't know if you have like COVID testing

01:24:40.800 --> 01:24:46.640
numbers it's very complex like how the number was generated maybe like how it was aggregated over many

01:24:46.640 --> 01:24:52.400
states and like what cases it includes and which it doesn't and so the number looks very innocent

01:24:52.400 --> 01:24:58.000
and simple but there's a very long process behind it to produce it um maybe this process is a bit

01:24:58.560 --> 01:25:04.240
uh a bit more black box or a bit more difficult if it comes out if there's some machine learning

01:25:04.240 --> 01:25:10.160
in between machine learning model in between to generate a number um but yeah I think this

01:25:11.040 --> 01:25:19.120
problem is well old well let me let me challenge a little bit here on something which is okay if

01:25:19.120 --> 01:25:24.960
I have if I have some general formula just some very general formula and then I go in there and I go

01:25:24.960 --> 01:25:31.200
you know what this formula has five parameters and if I make this one point seven five and that one

01:25:31.200 --> 01:25:39.840
one third and this one two and that one zero and I call this the megatron you know uh activation

01:25:39.840 --> 01:25:45.760
potential and I go and write a paper about it that's really just an arbitrary you know kind of

01:25:45.760 --> 01:25:51.120
selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it

01:25:51.200 --> 01:25:57.120
published in some journal and now everybody has to memorize that as you know the megatron potential

01:25:57.120 --> 01:26:01.920
and kind of learn about it and that's a lot of what's going on right now is that it's really just

01:26:01.920 --> 01:26:06.560
a bunch of hacking like it's people just they don't really know a general solution and they don't

01:26:06.560 --> 01:26:11.520
know how to solve like in general the problem they're trying to solve and so they just hack around and

01:26:11.520 --> 01:26:17.120
then the ones that are kind of famous or demonstrate some success in a particular combination you know

01:26:17.200 --> 01:26:21.760
competition over in this corner or something it now becomes something that's part of the lexicon

01:26:21.760 --> 01:26:26.400
that we all have to learn and I think like I look back on this like imagine what physics was like

01:26:26.400 --> 01:26:31.840
before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole

01:26:31.840 --> 01:26:36.960
bunch of little purpose built kind of formulas and then along comes a general framework which

01:26:36.960 --> 01:26:42.240
now we can just learn calculus and derive the special circumstances as needed. You're onto

01:26:42.240 --> 01:26:48.080
something really interesting there which is that with IML methods we are we are kind of

01:26:48.080 --> 01:26:53.440
compressing information down into a representation you know and then that that is a transport that

01:26:53.440 --> 01:26:58.160
can be understood by different people but there's a trade-off right because as you said you can learn

01:26:58.160 --> 01:27:03.040
calculus and that's a compact framework for doing lots of stuff but it's all about the amount of

01:27:03.040 --> 01:27:10.240
common knowledge that is required so it's possible to compress something down just to one symbol

01:27:10.240 --> 01:27:13.840
and that symbol could represent all of that knowledge but it doesn't help you because I still

01:27:13.840 --> 01:27:20.640
need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple

01:27:20.640 --> 01:27:26.160
framework that I could learn and then once I learned that simple thing I could go and solve all

01:27:26.160 --> 01:27:30.320
kinds of problems with it that before I would have to memorize specific solutions or like the

01:27:30.320 --> 01:27:35.840
quadratic formula for example is a student I didn't actually memorize the quadratic formula I just

01:27:35.920 --> 01:27:40.240
learned how to complete the square and then I would just do complete the square and if somebody

01:27:40.240 --> 01:27:44.720
asked me what the quadratic formula was I would just quickly derive it right because it was

01:27:44.720 --> 01:27:50.080
easier to memorize the rule and then apply the rule to any situation rather than to memorize

01:27:50.080 --> 01:27:55.840
all these little one-off you know kinds of hacks that we come up with. You're not normal Keith

01:27:55.840 --> 01:28:00.640
right so most people won't be able to go and understand this because I think it's well no

01:28:00.640 --> 01:28:07.520
these IML methods are brilliant for data scientists who can it's a framework right it's a

01:28:07.520 --> 01:28:12.000
reference of understanding so assuming that people can understand how Shapley values work

01:28:12.000 --> 01:28:18.320
then this is a beautiful representation to reason about the behavior of models. Sure but when I

01:28:18.320 --> 01:28:23.840
first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis

01:28:23.840 --> 01:28:29.520
to marginalization you know all we're really doing here is computing the expected marginal you know

01:28:29.520 --> 01:28:35.280
contribution to this value it's not a probability but it's still the same procedure being done right

01:28:35.280 --> 01:28:40.000
and I think I'm going to throw myself in with the lay people to a degree because the reason I'm

01:28:40.000 --> 01:28:46.320
always striving for simplifications is because I don't have the capacity to memorize all these little

01:28:46.320 --> 01:28:52.080
arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I

01:28:52.080 --> 01:28:57.360
said you know previously in some other videos statistics made no sense to me until I learned

01:28:57.360 --> 01:29:04.080
the Bayesian framework because that was based on very simple rules that I could then reapply as needed.

01:29:05.600 --> 01:29:12.720
I think that what you refer to Keith maybe the worst situation is with the saliency maps because

01:29:12.720 --> 01:29:19.360
you have so many methods and they all like back propagate the gradient and to do input pixels

01:29:19.920 --> 01:29:27.120
and to um now do you have to like learn like how dozens of these framework works or like

01:29:28.240 --> 01:29:33.280
how to interpret do interpretation with all of these and they're all kind of variants of each

01:29:33.280 --> 01:29:41.680
other so mostly because they just there's some ambiguity how you how you back propagate the

01:29:41.680 --> 01:29:48.800
gradient because because of the non-linear units and stuff and a little bit differences how you

01:29:48.880 --> 01:29:58.320
can define this and so you have this huge like a sea of many different methods I think it would be

01:29:58.320 --> 01:30:03.280
nice therefore as you said to have some like simplification where you say okay this is like

01:30:04.480 --> 01:30:09.280
all these methods work under this one principle basically and we have these two parameters

01:30:10.320 --> 01:30:16.640
and that's how they differ I think that's also that some I think I wrote something in a chapter

01:30:16.720 --> 01:30:25.840
that the police stop inventing new methods for saliency maps so I think it's enough and we

01:30:25.840 --> 01:30:33.040
should focus more on like doing this consolidation to like understand the limitations of the methods

01:30:33.040 --> 01:30:38.720
and consolidate them to see like what's the commonalities in which ways do they differ and so on

01:30:38.720 --> 01:30:42.960
that's probably actually my first impression actually when I first opened the interpretable ML

01:30:42.960 --> 01:30:47.680
book I was amazed how many different things there are I've you know heard people say ah

01:30:47.680 --> 01:30:52.080
you can't use machine learning it's just a black box so many times it'd almost been drilled into my

01:30:52.080 --> 01:30:58.240
head then seeing all the things you know from white box models ways of training salient models

01:30:58.240 --> 01:31:02.400
counterfactual explanations it's what a wonderful recipe right there are so many different things

01:31:02.400 --> 01:31:07.840
that you can do I feel like now I trust ML models more than other kinds of things because I have

01:31:07.840 --> 01:31:13.520
this amazing toolbox of ways to understand them the thing that strikes me though is

01:31:14.400 --> 01:31:18.800
most of these methods as we were just saying they require interpretation by a human and a human who

01:31:18.800 --> 01:31:24.480
understands how the method works I love this concept of turning machine learning into an

01:31:24.480 --> 01:31:32.400
engineering discipline and being able to do a lot of these tests non-interactively and I think

01:31:33.120 --> 01:31:37.600
Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping

01:31:37.600 --> 01:31:43.360
and what excites me about these methods is they seem like methods that we could actually run

01:31:43.360 --> 01:31:48.400
as part of an automated process we still have to set thresholds maybe we could set a threshold

01:31:48.400 --> 01:31:53.840
that said if this if this counterfactual example flips the switch on more than one percent of

01:31:53.840 --> 01:31:59.200
examples then fail the build that seems reasonable but a saliency method I mean how the hell do you

01:31:59.200 --> 01:32:03.840
say well if there's lots of red pixels over here then break the build I mean it's just ridiculous

01:32:04.720 --> 01:32:12.640
yeah yeah so I've seen interesting approaches to like using interpretability also more

01:32:12.640 --> 01:32:21.680
automatically like when you do model monitoring you can do things like create interpretations

01:32:21.680 --> 01:32:27.920
and see if they significantly change over time for example so then have thresholds that warn you

01:32:27.920 --> 01:32:34.880
that hey something's going on with your model so I think that's also interesting approaches there

01:32:35.520 --> 01:32:41.760
yeah you know Tim to your point of making this an engineering field and even making interpretability

01:32:41.760 --> 01:32:46.640
and and understandability an engineering field I mean I think that maybe that's why I like your

01:32:46.640 --> 01:32:51.520
book so much Christoph as I think it's it's a step towards that direction it's like let's

01:32:51.520 --> 01:32:59.280
survey everything and more importantly let's create a finite and hopefully smallish set of simple

01:32:59.280 --> 01:33:05.760
concepts that we can all agree on and understand that we can use to catalog you know what's out there

01:33:06.560 --> 01:33:11.680
so please keep up the good work you know I'm interested to see where this goes so final

01:33:11.680 --> 01:33:15.440
question for you Christoph then I wonder what's what's next for interpretability like are we going

01:33:15.440 --> 01:33:19.920
to the point where it's going to be almost a box ticking exercise where we can say yes our process

01:33:19.920 --> 01:33:25.840
we've done the standard interpretability step I mean is it is computing power going to change it

01:33:25.840 --> 01:33:31.680
I remember when Shaq the library came out it made that approach possible whereas previously you know

01:33:31.680 --> 01:33:36.320
it was very hard very computation infusible my friend Angem who's a wonderful data scientist

01:33:36.320 --> 01:33:41.520
sent me n-video rapids they've got that running on GPUs way faster than of course before is it

01:33:41.520 --> 01:33:46.400
just going to become a standard step do you think or is it going to be something where you need

01:33:46.400 --> 01:33:51.840
decent subject matter expertise and some real thought to do to really understand how a model

01:33:51.840 --> 01:33:58.640
works so well predictions about the future are always hard so maybe more like what I wish or

01:33:58.640 --> 01:34:04.320
what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot

01:34:04.320 --> 01:34:10.320
of implementations of these methods so they're kind of getting a commonality a rook one can use it

01:34:10.320 --> 01:34:18.720
very easily there's a lot of libraries out there in python r but also in like this machine learning

01:34:19.680 --> 01:34:26.960
cloud tools they also have a lot of interpretation methods available now so in that sense I think

01:34:27.600 --> 01:34:34.800
and it's maturing a lot I still believe that we need some expertise to understand them or at

01:34:34.800 --> 01:34:39.200
least some good references and there will also be hopefully more than my book maybe have some

01:34:40.800 --> 01:34:46.480
documentation when for these tools and people answering on stack overflow questions and whatnot

01:34:48.240 --> 01:34:54.240
so I think um yeah it's it's getting we're getting that everyone can use it easily

01:34:55.840 --> 01:35:01.840
I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics

01:35:02.720 --> 01:35:06.400
you know governance process or something the last thing you want is for it just to be an

01:35:06.400 --> 01:35:11.120
automatic response so I've just you know yeah I've thought about AI ethics um it needs to be

01:35:11.120 --> 01:35:16.400
something that that we really engage with I think we need to abstract away a lot of the complexity

01:35:16.400 --> 01:35:22.080
at the moment I think it's possible to come up with an interface to standardize the way that we

01:35:22.080 --> 01:35:28.800
do interpretability and we can reduce down what we have now to certain primitives which means that

01:35:28.800 --> 01:35:33.280
it can plug into an engineering process and it also means that we can abstract away some of the

01:35:33.280 --> 01:35:38.880
complexity I think that's possible yeah I also would agree that it shouldn't be like just box

01:35:38.880 --> 01:35:44.400
ticking but you can like for the initial um when you start interpreting a model that you just have

01:35:44.400 --> 01:35:49.200
like with a click you have a report and then it shows you the most basic things but then you still

01:35:49.200 --> 01:35:53.360
like should ask the like the question like does it really make sense that this feature is the most

01:35:53.360 --> 01:35:57.680
important one or what's happening there with these weird interactions between the two features

01:35:57.680 --> 01:36:04.720
let's dig a bit deeper here and see what's going on so I think um there's this one portion that is

01:36:04.720 --> 01:36:12.160
just like this automated reporting thing um but this should then be like the starting point for

01:36:12.160 --> 01:36:19.200
more critical uh questioning of the model and and and checking what's going on um for some specific

01:36:20.160 --> 01:36:26.480
problems maybe so it's going to be you click on the molnar report and it gives you the the report

01:36:26.480 --> 01:36:31.840
from the book right yeah there will be convenience well um christoph molnar thank you very much for

01:36:31.840 --> 01:36:35.600
joining us today it's an absolute honor to have you on the show thanks for having me hey folks this

01:36:35.600 --> 01:36:38.960
is tim in post script there's just a couple of thoughts that didn't come to my mind during the

01:36:38.960 --> 01:36:43.600
interview that I think I'd like to quickly cover now the first thing is on the lack of fairness

01:36:43.600 --> 01:36:50.160
the reason why I raised that is most folks who talk about AI ethics and fairness they use the

01:36:50.160 --> 01:36:55.600
toolkit of interpretability methods quite often you know to apply their trade there are tools out

01:36:55.600 --> 01:37:02.000
there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this

01:37:02.000 --> 01:37:10.400
what we really need is an operating model or a set of guidelines on how to implement these tools

01:37:11.120 --> 01:37:17.760
how do I identify sources of problematic correlations we need to have a database of problematic

01:37:17.760 --> 01:37:23.840
correlations having a tool that allows me to identify and mitigate bias frankly is useless

01:37:24.560 --> 01:37:30.160
what do I do with that as we mentioned on the show many of the machine learning cloud providers

01:37:30.160 --> 01:37:36.560
whether it's data iq or azure ml and sage maker they all have these interpretability methods built

01:37:36.560 --> 01:37:42.640
in now including saliency maps and it's just a box ticking exercise frankly it's completely useless

01:37:42.640 --> 01:37:49.680
there is no accepted guidance on how these tools should be used right so if I'm a large company

01:37:49.680 --> 01:37:55.600
and I'm building an operating model around how to implement fairness techniques just having the

01:37:55.600 --> 01:38:01.120
technology is irrelevant it's about the people and the process and the the kind of operating

01:38:01.120 --> 01:38:07.280
model of how we implement it and there is basically no useful information out there to help us do that

01:38:07.280 --> 01:38:12.960
the other thing is we spoke about this becoming an engineering discipline which is to say what if we

01:38:12.960 --> 01:38:18.720
could create an interface to abstract away some of the vagaries and esoteric of interpretability

01:38:18.720 --> 01:38:24.400
methods we might come up with some primitives or some common language and then we can hide the

01:38:24.400 --> 01:38:29.440
complexity behind the interface this is kind of what we do with mo dev ops already we automate

01:38:29.440 --> 01:38:34.960
as much as we can then we templatize and remove friction out of the process we even create building

01:38:34.960 --> 01:38:42.640
blocks using domain specific languages or yaml files and pipelines and and so on so what we do

01:38:42.640 --> 01:38:48.800
is is we create a level of abstraction where people can compose together pipelines remember

01:38:48.800 --> 01:38:53.600
when Conor made the comment that this might just become a box ticking exercise and this is something

01:38:53.600 --> 01:39:00.640
we see in security and AI ethics already we can't really trust people to self report that the model

01:39:00.640 --> 01:39:07.040
is behaving correctly or that the project has no concerns from an AI ethics point of view the whole

01:39:07.040 --> 01:39:12.960
point here is process if we want to create an operating model and ensure best practices are

01:39:12.960 --> 01:39:19.360
followed or any kind of standardization in a large organization we have to design a process

01:39:19.360 --> 01:39:27.760
and many eyes make shallow holes so the process would mandate that a certain number of stakeholders

01:39:27.760 --> 01:39:34.320
were involved in assessing the particular iml technique and validating it essentially and

01:39:34.320 --> 01:39:39.760
then we would need to record that assessment so who said what when and then if the company ever

01:39:39.760 --> 01:39:44.960
became audited or if god forbid there was some kind of a problem where the iml model did something

01:39:44.960 --> 01:39:49.920
wrong and it caused the company lots of damage or it harmed the environment or society or something

01:39:49.920 --> 01:39:55.040
like that we would then be able to rewind the clock and say okay well joe blogs said it was okay

01:39:55.040 --> 01:40:02.960
because of xyz so that is an operating model it's a process and how to design such a process again

01:40:02.960 --> 01:40:09.600
is completely absent speaking as a chief data scientist myself that's the kind of thing that

01:40:09.600 --> 01:40:14.560
i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the

01:40:14.560 --> 01:40:19.680
episode today we've had so much fun making it remember to like comment and subscribe and we'll

01:40:19.680 --> 01:40:25.520
see you back next week

