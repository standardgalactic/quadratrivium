start	end	text
0	5680	He moved to Indiana University in 1989, where he obtained his PhD in philosophy and cognitive
5680	11360	science, working for the legendary Douglas Hofstadter, by the way. I've got his book here
11360	16880	in the Center for Research on Concepts and Cognition. Douglas, by the way, is one of the most
16880	21360	legendary figures in the AI space. We also had the pleasure of interviewing one of his other
21360	27120	PhD students, Professor Melanie Mitchell. Now, David recently wrote this fascinating book called
27120	34960	Reality Plus. In that book, he discussed the three central philosophical questions, actually.
34960	40320	The reality question, which is to say, are virtual worlds real? His answer to that is yes.
40960	45600	The knowledge question, which is to say, can we know whether or not we're in a virtual world?
45600	53440	His answer to that is no. Also, the value question, which is to say, can you lead a good life
53440	59280	in a virtual world? And his answer to that is a resounding yes. Now, you probably also heard
59280	64800	of this notion of an extended mind, something which David formulated with Professor Andy Clark,
64800	71360	and they described the idea as active externalism, based on the active role of the environment in
71360	76400	driving cognitive processes, or put simply, you might think of your phone as being an extension
76400	81680	of your mind, for example. What was it like to work with Douglas during your PhD? Oh, he was great.
81680	91200	He was so interested in so many things. It was officially, it was an AI lab, and most of the
91200	96640	people there were AI researchers. You mentioned Melanie Mitchell. She was my colleague there.
96640	104800	Bob French, who's done important work on AI, Gary McGraw, Jim Marshall, Wong-Pay, and others.
105760	109920	I was the only philosopher. People were interested in so many things, whether it was,
110640	119920	we had workshops on humor, or creativity, on mathematics, on politics, on everything,
119920	126800	as well as all the stuff on AI, analogy, concepts. It was also a very exciting time to be there,
126800	137440	because this was one of the, in the boom and bust cycle of machine learning and of neural
137440	144240	networks. This was one of the boom periods, the early 1990s. The PDP books had just come out,
144240	151360	parallel distributed processing by Rubble Hart, McClelland, and Hinton. There was so much excitement
151360	159120	about the capacities of neural networks. I ended up writing a few papers on machine learning
159120	164720	back then. One was on the evolution of learning systems. One was on getting machines to learn
165360	174080	structural generalizations. All with fairly basic neural networks. A few years after that,
174080	179760	the bottom fell out of neural networks for another 15 years or so, but this was a very
179760	184480	exciting period to be there. One thing about Doug that you don't quite get from his books,
186720	191280	his books, he's so enthusiastic about everything. He's not into all these ideas and so on, but it
191280	198000	turns out there's 5% of things in the world or 1% that he's enthusiastic about. The other 99% he hates.
200400	207040	He's like, he likes certain approaches to AI he loved, but even neural networks, he was like,
208160	215520	he was not a big fan of most research in neural networks back then. He said,
215520	220560	I think it was a, this is a bandwagon or even he said a bad wagon. Likewise for philosophy,
220560	224320	there's bits of philosophy he loves. There's a lot of it. There's a lot of it he doesn't like.
224320	230160	So maybe in person, you get more of a, more of that very opinionated side, but really he's a,
231520	236080	he's just, you know, that book Go to Lesha Bark was just, I mean, it was what drew me into philosophy
236080	241680	and AI as a teenager and it's so rich. And you go back to that book, there's still so many ideas
241680	245920	there. The Mind's Eye is another book he edited with Dan Dennett. It was reading the Mind's Eye
245920	251280	that really first got me thinking about issues about consciousness and the mind-body problem.
251280	255840	So yeah, he's a very rich thinker. Yeah. Well, thank God for Douglas Tostata.
255840	261040	Yeah, I love the Mind's Eye. It has one of the, one of the eeriest stories of all. What was it?
261040	266080	The riddle of the universe and its solution where there's, there's some image or some text that if
266080	271600	you read it, it causes your brain basically to core dump. And so it becomes this infectious thing
271600	275920	where anybody who goes into that office and reads that thing, they just crash and they never,
275920	280320	they go into a coma and they never wake up again. And so, you know, there's a huge investigation
280320	284560	to figure out what's going on and more and more people keep going into comas, you know, because
284560	290400	of this crazy ideas in that book. Could I just pick up on this point because you were saying that
290400	294640	Douglas Tostata turned his nose up at Neural Networks. And I hope he still would actually,
294640	298480	and Melanie Mitchell certainly does. And I mean, Douglas had that paper out called
298480	303520	The Core of Cognition. He was always huge about this idea of the primacy of analogy making.
303520	308000	And actually, there's a few researchers today that are mirroring that, that idea that Francois
308000	312880	Chollet, for example, he says intelligence is literally sensitivity to abstract analogies.
312880	317040	It's not memorizing the internet. So I think, thank God that we do have people out there
317040	321680	thinking slightly differently because the modus operandi today is that we need to build a big
321680	326240	hash table of everything. And even the way we formalize intelligence, as we were just saying
326240	331840	previously, we're not really paying attention to why or how the things do what they do. We're
331840	335760	just looking at the behavioral output bit like the Turing test, as long as it looks and smells
335760	342720	like a duck, then it must be a duck. So thank God for Douglas Tostata. Yeah, I think he's always been
343360	348880	at the same time an enthusiast about the possibility of AI while being somewhat skeptical
348880	356000	about the capacities of existing AI and about the kind of hype that suggests that AI might just be
356480	361680	10 or 20 years around the corner. So in the early 90s, I think, yeah, that was,
363040	368080	that was especially, especially natural. Back around then, people would say a year spent working
368080	373360	in AI is enough to make you believe in God. It was so hard to get anything even like
375680	380560	any kind of intelligence out of an AI system. And I think, you know, Doug was equally skeptical
380560	386720	of both the symbolic and the connectionist on neural network approaches back then.
387840	392880	I think ultimately his sympathies lay with it on the neural network side of things,
392880	397200	with the idea that intelligence could in principle bubble up from a million, you know,
397200	402000	from 100 billion separate little interactions, intelligence could bubble up from there. But
402000	407520	I still think he'd be inclined to think that current approaches are too statistical, too simple,
407520	413760	and so on. That said, you have to look at the, at the progress in machine learning over the last
414800	418400	10 years. And it's been amazing and surprising. And I think even somebody like,
419280	422800	even people like Melanie or like Doug are going to have to say this has been
422800	428400	something they did not expect. And that they did not predict. So I actually, I was back in
428400	434480	Indiana just over two years ago, just before the pandemic got going February 2020. And
435440	438720	I don't know, maybe that was before GBT three, but still there've been all these amazing
439840	444880	developments in machine learning over the last few years. I asked Doug about this,
444880	448720	and what do you make of this? And because he's on the record of saying, you know,
448720	453360	there will be AI eventually, but it's going to have to be involve all these new kinds of complexity,
453360	457680	not not something simple like this. And he says, Yeah, well, this is, this is troubling.
457680	461760	This is concerning, you know, it could be, I don't know yet, but it could be that I was wrong.
462320	467440	It could be there are simpler passes, paths to AI. And his attitude was that would be very
467440	472400	disappointing. It turned out that you could actually train up an AGI, just using those
473120	477440	simple methods to human levels. That would make, I think Doug's view was that would make kind of
477440	484560	human level intelligence less, less grand and remarkable than, than he had thought. Well,
484560	489360	actually, back in, back in Goethe-les-Sherbach, I think he said that even to have a machine that
489360	494000	could beat a human at chess, it would have to be good at everything would be a good composer,
494000	499040	it could tell jokes and so on. Okay, that one, that view got rolled out back in the,
499040	507920	back in the 90s. So the question is, you know, is this, is this ever growing progress of just,
507920	511200	of the kind of machine learning that says just throw a whole lot of compute,
511200	516240	and a whole lot of data at it, and see what happens. Is that eventually going to get us to
516320	519840	human level intelligence? Or is it, is it just going to get us so far
520800	524560	with, and there's going to be principled limitations? I've always been on the side of,
525360	530560	they'll probably only get us so far. But I have to say those principled limitations,
530560	537840	those obstacles that have not yet been conquered are getting smaller and smaller. And the progress,
537840	541120	if the progress of the last five or 10 years continues for another five or 10 years,
541680	546960	then who's to say what's going to be left? Yeah, there was a fascinating anecdote in,
546960	552400	in Melanie's book about how she and Hofstadter went to the Googleplex one time. And basically,
552400	557360	as you said, Douglas was terrified that intelligence might be disappointingly simple to
557360	562800	mechanize because he felt of the mind of Chopin as being infinitely nuanced. And just,
562800	566960	just the incredible process that must have gone through his mind when he, when he produced his
566960	572640	music. But I wanted to, and just quickly, by the way, you said that there was a conception in,
572640	578080	in the 70s that task specific skill was what was required for intelligence or a collection of,
578080	584160	of specific skills. And, and now the mindset is much more towards task acquisition efficiency
584160	589440	and generalization. But I wanted to just quickly pick you up on the so-called intelligence
589440	594400	explosion question. So this is a subject which Nick Bostrom has popularized after his book,
594480	599680	Superintelligence. Personally speaking, we're not particularly sympathetic to this view. And
599680	604560	Saigre Francois-Labe, he said in a blog post recently that this line of reasoning represents
604560	609040	a misunderstanding of intelligence. He said that in his opinion, intelligence is situational.
609680	613840	He said that our environment puts a hard limit on our individual intelligences. He said that
613840	618720	most of our intelligence is not in the brain, it's externalized as civilization. And that an
618720	623840	individual brain cannot implement recursive intelligence augmentation like a Godel machine.
623840	628560	He also said that there are already many examples of recursively self-improving systems.
628560	632880	Even personal investing, for example, is a recursively self-improving system. The more
632880	638640	money you have, the more money you make. Anyway, so Bostrom described a thought experiment in 2003.
639280	643280	I'm sure you've heard of this. The scenario describes an artificial, you know, like a very
643280	647840	advanced artificial intelligence task with manufacturing paper clips. If such a machine
647840	652000	were not programmed to value human life, then given enough power over its environment,
652000	656240	it would try to turn all the matter in the universe, including human beings,
656240	661040	into paper clips or machines which could manufacture paper clips. Do you think we might
661040	665440	be on the precipice of being turned into paper clips, as Bostrom famously described in his
665440	670800	thought experiment? Yeah, look, it's there's two different issues here. One is, will we get to
670800	677520	some kind of much greater than human superintelligence relatively soon by some kind of intelligence
677520	683680	explosion process? And second, if that happens, are there major dangers around? Yeah, I wrote
683680	689360	about both of these things back in 2009. I had an article called, yeah, the Singularity
689360	695360	of Philosophical Analysis, where I tried to take this line of reasoning for an intelligence
695360	702240	explosion through recursive, through basically through recursive design of ever more sophisticated
702240	706480	AIs. I tried to take that and turn it into an argument. I mean, the classic statement of this
706480	713520	comes from I.J. Goode, the statistician and philosopher back in 1965 on the design of an
713520	718800	ultra intelligent machine where he puts the basic idea right there that once you've got a machine
718800	726560	which is smarter than a human, it will be able to design a machine which is smarter still,
726560	732400	and then you're going to get recursive, runaway explosion of intelligence. I tried to analyze
732400	738480	that to set out that article, that argument in as much detail as I could, analyze where it could
738480	742880	go right, where it could go wrong, what the possible obstacles would be, and it's a long story.
743840	750160	If anyone wants to look it up, it's out there on my website. But I in the end became convinced
750160	756640	this is a pretty powerful argument. There's only so many ways it could go wrong. I think it's
756640	761600	important that not every recursive augmentation process is going to lead to an intelligence
761680	766880	explosion. It could easily bottom out, could asymptote before human intelligence. But I do
766880	771200	think that once we start from greater than human intelligence, we have to find some way to get
771200	776080	to greater than human intelligence first. This explosion won't get you that. But once you get
776080	782240	there, then there's pretty good reason to think things in principle can take off from there.
782240	788880	If intelligence is extended, I'm a big fan of the idea that intelligence is extended into the
788880	795280	environment. But as far as I can tell, all that can in principle be augmented too. We develop
795280	801760	extended systems, which are smarter than humans, and then they'll be able to design even better
801760	809200	extended systems. And we could then have an intelligence explosion of extended intelligences.
809200	814720	So I'm actually, nothing about this gets you to human level intelligence. But once we get to human
814720	820560	level intelligence and a little bit beyond, then I think there's a pretty good case that there's some
820560	826160	kind of potential explosion in the offing. Then the other issue you mentioned Bostrom and the paper
826160	832800	clips is, yeah, what does this mean for the future of humanity? I guess I don't know what I'd say
832800	838480	about the probabilities, but I'd say, yeah, once you have greater than human artificial general
838480	844320	intelligence, then there's many ways that can go wrong for the obvious reasons that such a being
844320	850000	is going to be extremely powerful. The most intelligent beings in the universe tend to be
850000	856400	the most powerful for obvious reasons. Whatever they want, they have the capacity to get. So it's
856400	863600	going to be extremely important that our AGI systems want the right things. That is, they have
863600	869120	the right kind of goals. Or as people put it, these days that they are aligned with human goals.
870000	875760	Because if they're even a little bit misaligned, then there's going to be the capacity for things
875760	879840	to go very badly wrong. I know there are some people who think that the alignment is going to
879840	885680	have to be so precise that, you know, missed by just the tiniest bit and will destroy the universe,
885680	891600	whereas others think it's extremely robust. It may be more robust than that. I'm not
891600	895200	totally sure about that. But I'm certainly on the side of people who think we have to take this
895200	901200	issue extremely seriously. And there is at least potential existential risks here that if AGI is
901200	907200	produced in an unthinking way, perhaps say in a military or a financial context where there's
907200	917920	an AI arms race, and we suddenly have greater than human AIs that can achieve arbitrary goals,
917920	924960	then suddenly it becomes an extremely sensitive matter what their goals are. So I'm certainly on
924960	928160	Bostrom's side when it comes to, yeah, this is something we should take seriously.
928880	932560	But isn't there a bit of, you know, it's a big distance to go from
933760	939280	superior to human intelligence and achieve anything you want. I mean, I'm relatively
939280	946320	intelligent, but I can't achieve flight, you know, by myself without, you know, apparatus to do that
946320	951040	and airplane wings, whatever. I mean, there are physical limitations in the world. And I think
951040	955200	sometimes there's this assumption that intelligence can kind of go to infinity,
955200	960800	where in fact, maybe intelligence itself kind of bottoms out at IQ 1000 or something, there's
960800	965360	just not much, you know, you can do beyond that certain IQ. I mean, isn't there a degree of
966160	972400	kind of speculative, you know, extrapolation that we need to account for there?
973840	977920	I would say this is certainly one way that things could, that the argument could fail,
977920	981440	is if it turns out that basically there are diminishing, there's some kind of intelligence
981440	987680	ceiling, and there's some kind of diminishing returns towards this. Just there is such a
987680	993120	ceiling that we might find that when we make a being which is 10% smarter than us on some scale,
993680	1000240	it could only make a being which is 5% smarter than it. And that being will make a machine,
1000240	1005920	make a being which is only 2.5% smarter than it. And all this will kind of asymptote to some
1006880	1012000	intelligence ceiling. And I don't know, this turns on very subtle issues about the structure
1012000	1017680	of intelligence space. I'm rather doubtful there is such an intelligence ceiling, or if there is
1017680	1022400	one, maybe it's something like, you know, the limits of computability compared to, you know,
1022400	1027840	hypercomputation that an infinite system could do. But I think that ceiling is so high that there's
1027840	1034080	room for an awful lot of super intelligence before we get there. But in any case, I would say that,
1034080	1039040	you know, for the purposes of, say, caution and thinking about the future, I would just turn the
1039040	1043280	point back on you and say that the thought that there is such an intelligence ceiling is itself
1043280	1048400	an extremely speculative one. I wouldn't want to rely on this, on this extremely speculative thought
1048400	1053840	to kind of protect us from, from the, you know, potential risks of AGI in the future. If there's
1053840	1059840	only a 20% chance there's not such an intelligence ceiling, then this is something that we very
1059840	1064720	much need to be, to be worrying about. Yeah, I mean, fair enough, it's certainly a risk factor.
1064720	1070400	It's certainly something that we need to need to keep a handle on. Well, let me ask you about one
1070400	1076320	specific time there. So I'm thinking you're probably familiar with Carl Friston and, you know,
1076320	1080960	his free energy principle. And he sends his regards, by the way, we talked to him a couple
1080960	1086160	weeks back. And, and he wanted to ask you about kind of one line of thinking that he's been exploring
1086800	1092720	lately. And I want to give you a quote from his 2018 article and my self conscious,
1092720	1098800	or does self organization entail self consciousness. And he said, the proposal on offer here
1099520	1104960	is that the mind comes into being when self evidencing has a temporal thickness,
1105680	1111760	or counterfactual depth, which grounds inferences about the consequences of my action.
1112480	1118720	On this view, consciousness is nothing more than the inference about my future, namely,
1118720	1125120	self evidencing consequences of what I could do. What do you think about that, that perspective?
1126080	1130560	Yeah, I'd have to know more about the connection to consciousness. I know that yeah, Friston is
1131280	1137040	very has developed very deeply the idea of the mind as a prediction machine, a mind which is
1137040	1145200	basically set up to, you know, predict whatever signal is coming next. And that's with that one
1145200	1150640	basic key loss, you know, predict what's next, what's next, what's next, then you get to build these
1150640	1157760	amazing models of the world with all of these, all of these, these capacities. And that's a
1157760	1163440	really interesting perspective thinking about the mind and intelligence in general. And it's
1163440	1169760	got to be at least one huge part of the story, even if it's not the whole story as Friston thinks
1169760	1176080	it is, but I've never really understood the distinct what this kind of predictive approach has to say
1176080	1182240	distinctively about consciousness. Because presumably there's a whole lot of different
1182240	1188640	predictive processes at all kinds of levels of the hierarchy, including at the very early vision
1188640	1194400	and very late cognition, and the whole mind is engaged in coming up with these predictions,
1194400	1201600	but only some limited part of it is conscious. What you just said about, yeah, trying to figure out
1201600	1205920	the predictions consequent on our actions, sounds to me like a very general statement of
1207040	1210880	what the predictive approach says about the mind in general. And I haven't yet heard what is the
1210880	1216160	part that corresponds to consciousness. Why, for example, or some representations get to be
1216160	1221040	conscious where so much of it in the brain is not, I can give you I can give you a bit more
1221040	1227040	detail, which may be helpful, because we did dig into him with a on a bit. And he said, for one thing,
1227040	1232800	he expected that perhaps part of your response might might entail or talk about the meta hard
1232800	1238320	problem. You know, why is it that certain beings, i.e. things like philosophers, and people like you
1238320	1242960	and me puzzle so much about our qualitative experience. And the argument he makes there,
1242960	1248320	he says that if we are inference machines that are built to actively self evidence,
1248320	1253840	then that necessarily entails we need to have a generative model about our experienced world.
1254480	1259760	And if we have that that generative model about our experience world, our experience world,
1259760	1266640	then we have to entertain the hypothesis that we are things having a qualitative experience,
1266640	1271760	along with the alternate to that hypothesis, which is that we're not having qualitative
1271760	1277040	experiences. And so essentially that the capability to model the world generatively
1277600	1282960	really requires that we entertain this hypothesis that we're actually having qualitative experiences
1282960	1288640	or maybe not. And that's why we pontificate about it. Yeah, it's interesting. And I think the meta
1288640	1294480	problem is a, yeah, as a promising approach is the meta problem is your why do we say and think
1294480	1299040	the things we do about consciousness, instead of explaining consciousness directly,
1299040	1306320	let's explain, you know, our internal model of consciousness. And yeah, there's got to be
1306320	1312000	such a model. So I think this is a promising approach to take. I still don't fully, I mean,
1312000	1316480	I think if you take the predictive approach, so what you would expect is, is the system would have
1316480	1322240	many different models, you know, a big complex model of the world at all levels, it doesn't
1322240	1329360	just correspond to experienced reality, but the models the world way beyond what's experienced,
1329360	1336640	it would also you'd also expect the model to have a model of the mind to have a model of ourselves
1336640	1342000	and relation to the world. But what actually happens in the in the human mind is we have,
1343120	1347280	we have models at all levels, you know, there's like so many different levels of say of
1347280	1354960	representation, even in the visual hierarchy. And somehow, though, only one of those
1354960	1358720	levels seems to correspond to consciousness. The question is, why now do we need a distinctive
1358720	1366560	model of those representations in us, which correspond to conscious experience? One idea,
1366560	1371040	I think, one idea I quite like is that this could be like a simplification. In fact,
1371040	1376160	we have millions of layers of representation of the world. But to build all that into our model
1376160	1381360	of ourselves, and our relation of the world is going to be too complex. So we basically,
1382000	1387280	we oversimplify by saying, ah, there's this one special relationship we have to the world,
1387280	1393120	we call it consciousness or experience. And yet we experience certain things and then we use them
1393120	1398880	to reason about them. And this is massively oversimplified as a model of the mind. But it could
1398880	1404240	be that that simplification is then what actually gives us the sense that we have this special
1404240	1408560	thing called consciousness. At least maybe that could explain why it seems to us that we have
1408560	1415280	some special representations of the world. It's a further question why those conscious
1415280	1421360	representations should seem to be so ineffable and subjective and hard to explain. And what
1421360	1426160	in what Carl has written about this, I think he and Andy Clark had some ideas about the meta
1426160	1431120	problem to try and push on this. Maybe that maybe there'd be certain representations that
1431200	1435520	we'd have to be especially certain that we have them. Maybe that would give rise to
1436320	1441680	the Descartes idea that, well, I'm not sure about the world, but I know that I'm thinking.
1442640	1447840	I think, therefore, I am. And they had some kind of story about how this could get the whole,
1447840	1453600	I think, therefore, I am certainty in one's own mind going. Anyway, I think it's an interesting
1453600	1456720	approach and I'll be very cool to see if they can develop it further.
1456960	1461600	Fascinating. I wanted to dig into this modeling thing. I was even thinking a second ago when
1461600	1466080	you were talking about intelligence, that straight away you did the Hutter thing and
1466080	1471040	we're talking about agents performing in environments and so on. And even that is a model.
1471040	1475280	And of course, we're talking about complex phenomena and the way we model things depends
1475280	1480640	on the level of analysis. But I'm really fascinated by this idea that some phenomena is so complex
1480640	1485520	that it cannot be formalized or communicated, almost as if there's a representation problem.
1485520	1489200	Now, you discussed in your consciousness book whether consciousness itself could be
1489200	1494000	reductively explained and your knowledge argument, you spoke of this neuroscientist Mary
1494000	1497600	that had been brought up in a black and white room. She's never seen any colors except for
1497600	1501440	black and white and shades of gray. She's nevertheless one of the world's leading
1501440	1505600	neuroscientists specializing in neurophysiology of color vision. She knows everything there is
1505600	1510240	to know about neural processes involved in visual information processing, about the physics of
1510240	1514960	optical processes, about the physical makeup of objects in the environment. But she doesn't know
1515040	1519600	what it's like to see red. No amount of reasoning from physical facts alone will give her this
1519600	1525520	knowledge. Physical facts about systems do not tell us what their conscious experiences are like.
1525520	1530800	Now, you're speaking about this phenomenon in respect of the conscious or the phenomenological
1530800	1535280	experience. But I think it's a much bigger problem of representation with any complex system, right?
1535280	1539360	So what I find fascinating is that all of us have a conscious experience, but it's completely
1539360	1543680	ineffable, as you just said, it's impossible for us to communicate it to others. And whenever
1543680	1548080	we try to do so, we're reaching, right? Just like the blind men in the elephant, we end up defining
1548080	1553440	some weird abstract motif, right? Chopping off 90% of the truth. The thing that fascinates me is
1553440	1558080	that we need to have some kind of formalism or reduction in order to communicate, you know,
1558080	1562560	in order to know or even understand anything. But so often is the case that all of the nuance
1562560	1568080	and richness of the phenomena is lost in doing so. So I suspect that any formalism of a complex
1568080	1572160	system might blind us from discovering a much better and richer formalism later because it
1572160	1577440	kind of frames our thinking in quite a pernicious way in your book. So as I said, you were trying
1577440	1582480	to separate the phenomenological experience as something that couldn't be described. But do
1582480	1590240	you think it could be extended to any complex system? Well, we don't. As far as we know,
1590240	1595920	you know, some complex systems actually have conscious subjective experience. But, you know,
1596000	1602880	most of them don't. You know, this Mac that I'm using right now is a very complex system,
1602880	1607840	but not much reason to think that it's conscious despite the complexity of what's going on
1607840	1613360	within it. So certain kinds of complexity go along with consciousness. But if we were to kind of
1613360	1618640	return to that meta problem approach for a moment, maybe there are certain kinds of properties
1619520	1624960	of a complex system that tend to produce reports, for example, that the system
1624960	1632560	is conscious. So maybe some complex systems have the capacity for a certain kind of direct
1632560	1637520	self-modeling that corresponds to what we think of as introspection. We have introspection,
1637520	1642400	which is a way of saying, this is what I'm perceiving right now. This is what I'm thinking
1642480	1648320	right now. This is what I'm feeling right now. And we build a model of ourselves,
1648320	1652960	and it may well be that that model is highly oversimplified. You don't have access
1652960	1659600	to all these facts about ourselves. So perhaps you could tell a story where the kinds of complex
1659600	1664800	systems that give rise at least to this capacity for introspection are then at least going to report
1664800	1673200	themselves as being conscious. And maybe that could get at some element, maybe sort of the
1673200	1681840	ineffability of consciousness. You'd expect to build these very simplified self-models. We wouldn't
1681840	1690640	know immediately how to extend to other people. I mean, I still think, in principle, you could
1690640	1695520	take Mary, who knows all about the human brain, and she could come to know all about those models
1696080	1701680	in other people. But it still seems that she's never actually experienced red for herself.
1701680	1706080	There's still something really crucial about this objective experience that she doesn't know.
1706080	1710800	She doesn't know what it's like to experience red, and knowing all about the details of the model
1710800	1716400	still hasn't told her that. So I think that's still something that needs explaining. Some people at
1716400	1723120	this point just say that sense of something extra is an illusion. Something extra that the model
1723120	1727440	hasn't explained is an illusion. But that's really where a lot of the action is at then.
1730080	1733920	Just quickly, do you think there could be a sense of something extra to intelligence,
1733920	1741600	as well as consciousness? Probably, yeah, we model our own intelligence with massively
1741600	1750400	oversimplified self-models that were programmed into us by nature that model us as these agents
1750400	1760320	with incredible capacities, free will, rationality, reason. It probably, again, it will, yeah, maybe
1760960	1764800	I talked about consciousness is involving the subjective elements, intelligence is involving
1764800	1770400	the objective elements. But yeah, we probably have oversimplified models of those conscious
1772400	1778800	of those behavioral elements as well, perhaps that make us out to be more rational, or more free,
1779760	1787920	or more capable than we actually are. I wanted to ask, what is an interesting simulation? Is
1787920	1793280	our universe interesting or not? Because we represent just a pinprick of intelligence. So
1793280	1798800	should intelligence be more spread out in the eyes of the simulator? Or in the vast majority of
1798800	1803520	instances, would there just be gas everywhere or a singularity? Maybe stars can't form.
1804400	1810320	Maybe the interesting phenomena itself is on the boundary between chaos and order,
1811040	1815440	or between order and disorder, I should say, which is just a tiny sliver. So what do you think
1815440	1820720	makes an interesting simulation? I don't know. I think it probably depends on your perspective,
1820720	1826960	and it might, for example, depend on the perspective of the simulators, what they're after. One thing
1826960	1832400	that a simulator might be doing is just create a whole lot of different universes with different
1832400	1839600	potential laws of physics that they're simulating just to see what happens. And maybe if they're
1839600	1843840	interested in, say, life or intelligence, then it could be that they're going to find that, okay,
1843840	1849360	well, 99% of these simulations don't produce anything like life or intelligence. And yeah,
1849360	1856080	1% of them produce life, and 0.01% lead to intelligence. So if that's what they're interested in,
1857040	1862720	in studying, fantastic. But they might be interested in who's to say the laws of physics or
1862720	1868560	galaxy formation, more generally, totally independent of life and intelligence. So I don't
1868560	1874640	think there's any single standard of what's interesting. I mean, to me, as a philosopher
1874640	1879440	interested in consciousness, I'm especially interested in this question of what kinds of
1879440	1886240	simulations might actually develop conscious beings within them, not least because that's
1886240	1890880	going to be especially relevant to our situation. If we're in a simulation, it seems we're conscious.
1891520	1897840	So there's a question about just how this kind of simulation might get set up. But I think this
1897840	1903440	whole, I mean, already simulations are used in actual practice for a million different purposes
1903440	1908160	by scientists studying this phenomenon or that phenomenon, by people doing entertainment, by
1908160	1913040	people doing prediction of the future, by people doing simulation of the past. And I guess when
1913040	1918640	it comes to simulated universes, all of those sources of interest may themselves be present.
