{"text": " So, you're talking about the symmetries in data. Could these analogies also be represented using the kind of symmetries that you're talking about? Good question. Let me think of it a bit. Modern machine learning operates with large, high-quality data sets, which together with appropriate computational resources, motivates the design of rich function spaces with the capacity to interpolate over the data points. Now, this mindset plays well with neural networks since even the simplest choices of inductive prior yield a dense class of functions. Now, symmetry, as wide or narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection. And that was a quote from Hermann Weil, a German mathematician who was born in the 19th century. Now, since the early days, researchers have adapted neural networks to exploit the low-dimensional geometry arising from physical measurements, for example, grids in images, sequences in time series, or position and momentum in molecules and their associated symmetries, such as translation or rotation. Now, folks, this is an epic special edition of MLST. We've been working on this since May of this year, so please use the table of contents on YouTube if you want to skip around. The show is about three and a half hours long. The second half of the show, roughly speaking, is a traditional style MLST episode, but the beginning part is a bit of an experiment for us, maybe a bit of a departure. We want to make some Netflix-style content, and we've even been filming on location with our guests, so I hope you enjoy the show and let us know what you think in the YouTube comments. Many people intuit that there are some deep theoretical links between some of the recent deep learning model architectures, particularly the ones on sets, actually. And this may be why so many popular architectures keep getting reinvented. Now, the other day, Fabian Fuchs from Oxford University released a really cool blog post about deep learning on sets, elucidating a math-heavy paper that he co-authored with Edward Wagstaff et al. Now, he wanted to understand why so many neural network architectures for sets resemble either deep sets or self-attention, because sets come in any ordering. There are many opportunities to design inductive priors to capture the symmetries. So, raises the question, how do we design deep learning algorithms that are invariant to semantically-equivalent transformations while maintaining maximum expressivity? Now, Fabian pointed out that the so-called Genosi-pooling framework gives a satisfying explanation. Genosi-pooling is when you generate all of the k-tuples of a set, an average over your target function, on those permutations. It gives you a computationally tractable way of achieving permutation invariance. So, rather than computing n factorial combinations of the examples, you compute n factorial divided by n minus k factorial, which for small k is very tractable. Now, clearly, setting k to n gives you the most expressive yet the most expensive model, but that would be cool. It would model the high-order interactions between the examples, but it turns out that deep sets are this configuration with k equals 1, and self-attention is this configuration with k equals 2. Now, Fabian also spoke about approximate permutation invariance, which is when you set k to n, the number of examples, but you sample the permutations. It turns out you don't have to sample very many of them to get good results. But anyway, if you want to check out that in a little bit more detail, go and check out Fabian's blog. I've put a link in the video description. High-dimensional learning is impossible due to the curse of dimensionality. It only works if we make some very strong assumptions about the regularities of the space of functions that we need to search through. Now, the classical assumptions that we make in machine learning are no longer relevant. Now, in general, learning in high dimensions is intractable. The number of samples grows exponentially with the number of dimensions. The universal function approximation theorem popularized in the 1990s states that for the class of shallow neural network functions, you can approximate any continuous function to arbitrary precision by just stacking the neurons, assuming that you had enough of them. So it's a bit like kind of sparse coding, if you like. The curse of dimensionality refers to the various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings, such as the three-dimensional physical space of everyday experience. Now, the common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data effectively becomes sparse. This sparsity is problematic for any method that requires statistical significance. Now, in order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Most of the information in data has regularities. Now, what this means in plain English is, just like on a kaleidoscope, most of the information which has been generated by the physical world is actually redundant, just many repeated semantically equivalent replicas of the same thing. The world is full of simulacrums. Machine learning algorithms need to encode the appropriate notion of regularity to cut down the search space of possible functions. You might have heard this idea referred to as an inductive bias. Now, machine learning is about trading off these three sources of error, statistical error from approximating the expectations on a finite sample, and this grows as you increase your hypothesis space. Approximation error, which is how good is your model in that hypothesis space? If your function space is too small, then the one that you find will incur a lot of approximation error. And finally, optimization error, which is the ability to find a global optimum. Now, even if we make strong assumptions about our hypothesis space, you know, we should say that it should be lip sheets or in plain English, it should be locally smooth. It's still way too large. We want to have a way to search through the space to get anywhere. So the statistical error is cursed by the dimensionality. If we make the hypothesis or the function space really small, then the search space is smaller, but the approximation error is cursed by dimensionality. So we need to define better function spaces to search through. But how? We need to move towards a new class of function spaces, which is to say, geometrically inspired function spaces. Let's exploit the underlying low dimensional structure of the high dimensional input space. The geometric domain can give us entirely new notions of regularity, which we can exploit. Now using geometrical priors, which is to say, only allowing equivariant functions or ones which respect a particular geometrical principle, this will reduce the space of possible functions that we search through, which means less risk of statistical error and less risk of overfitting. We should be able to do this without increasing approximation error, because we should know for sure that the true function has a certain geometrical property, which will bias into the model. So introducing the geometrical deep learning proto book. So recently, Professor Michael Bronstein, Professor Joanne Bruner, Dr. Tako Kohen and Dr. Peta Velichkovich released an epic proto book called Geometric Deep Learning, Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical theory and machine learning and geometry and group theory to deep learning, which is fascinating. Now the proto book is beautifully written, right? It's presented so well, it even has some helpful margin notes. And honestly, I could read sections of it out loud on MLST with scant need to change a single word. It's that well written. I mean, it is, there's a lot of maths in there as well, let's be honest. I can't dodge that. But I often try to impress upon people that if your writing sounds weird when you say it out loud, then you're probably writing it the wrong way. But these guys have written it really well. Now they've essentially created an abstraction or a blueprint, as they call it, which prototypically describes all of the deep learning architectures, the geometrical priors that they have described so far. They don't prescribe a specific architecture, but rather a series of necessary conditions. The book provides a mathematical framework to study this field. And it's essentially a mindset on, you know, how to build new architectures. It gives constructive, you know, procedures to incorporate prior physical knowledge into neural architectures. And it provides a principled way to build future architectures, which have not yet been invented. The researchers have also recently released a series of 12 brilliant lectures on all of the material in the book. And I've linked these in the video description. Now, what are the core domains of geometric deep learning? So in geometric deep learning, the data lives on a domain. This domain is a set. It might have additional structure, like a neighborhood in a graph, or it might have a metric such as, you know, what's the distance between two points in the set. But most of the time, the data isn't the domain itself. It's a representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries. Symmetries are really important to understand this framework. So a symmetry of an object is simply a transformation of that object, which leaves it unchanged. Now, there are many different types of symmetries in deep learning. I mean, for example, there are symmetries of the weights. If you take two neurons in a neural network, and you swap them, the neural network is still graph isomorphic. There are symmetries of the label function, which means that an image is still a dog, even if you apply a rotation transformation to it. Actually, if we knew all of the symmetries of a certain class, we would only need one labeled example, right, because we would recognize any other examples that you give it as kind of semantically equivalent transformations. But we can't do that, right, because the learning problem is difficult, which means we don't actually know all of the symmetries in advance. Now, in the context of geometric deep learning, we talk about symmetries of the core structured geometric domains that we're interested in. So grids or graphs, for example, a symmetry is any transformation which preserves the structure of the geometric domain that the signal lives on. So for example, permutations of a set preserves the set membership or Euclidean transformations like rotations or reflections preserve distances and angles. There are a few rules to remember, because the way we deal with this paradigm is we talk about how composable those symmetries are. The identity transformation is always a symmetry. Composing a symmetry transformation is always a symmetry. The inverse of a symmetry is always a symmetry. We can formulate this with this mathematically abstract notion of a group. Group theory in mathematics is fascinating, because it concerns only with how elements compose with each other, not what they actually are. So different kinds of objects may have the same symmetry group. For example, the group of rotational and reflection symmetries of a triangle is the same as the group of permutations of sequences of three elements. So let's talk about the blueprint itself. The blueprint has three core principles. Symmetry, scale separation and geometric stability. In machine learning, multi-scale representations and local invariance are the fundamental mathematical principles underpinning the efficiency of convolutional neural networks and graph neural networks. They are typically implemented in the form of local pooling in some sense. Now these principles give us a very general blueprint of geometric deep learning that can be recognized in the majority of popular deep neural network architectures. A typical design consists of a sequence of locally-equivariant layers. I mean, think of the convolution layers in a CNN, then a pooling or a coarsening layer. So you recognize those in CNNs as well. And finally, followed by a globally invariant pooling layer. So that might be your classification head. Now these building blocks provide a rich approximation space, which have prescribed invariance and stability properties by combining them together into a scheme that these researchers refer to as the geometric deep learning blueprint. Now the researchers also introduced the concept of geometric stability, which extends the notion of group invariance and equivalence to approximate symmetry or transformations around the group. They quantify this in some sense by looking at a metric space between the transformations themselves. This is Professor Michael Bronstein. The problem is that traditional machine learning techniques work well with images or audio, but they are not designed to deal with network structure data. In order to address this challenge, we've developed a new framework that we call geometric deep learning. It allowed us to learn the network effects of clinically approved drugs and to predict anti-cancer drug-like properties of other molecules. For example, molecules contained in food. Neural networks have exploded, leading to several success stories in industrial applications. And I think it's quite indicative that last year two major biological journals featured geometric deep learning papers on their cover, which means that it has already become mainstream and possibly will lead to new exciting results in fundamental sciences. The book I hold is called The Role to Reality. It's written by a British mathematician and recent Nobel laureate, Roger Penrose, a professor at Oxford, and it's really probably one of the most complete attempts to write and describe modern physics and its mathematical underpinning. And you can see it's very heavy. But if I were to compress the thousand-plus pages of this book into just a single concept, I can capture it in one word. And this is symmetry. And symmetry is really fundamental concept and fundamental idea that underpins all modern physics as we know it. So, for example, the standard model of particle physics can entirely be derived from the considerations of symmetry. And that's the kind of idea that we try to use in deep learning to derive and create new neural network architectures entirely from fundamental concepts and fundamental principles of symmetry. In the past decade, deep learning has brought a revolution in data science and made possible many tasks previously thought to be unreached. On the other hand, we now have a zoo of different neural network architectures for different types of data, but few unifying principles. The authors also point out that different geometric deep learning methods differ in their choice of domain or symmetry group or the implementation specific details of those building blocks that we spoke about. But many of the deep learning architectures currently in use fall into this scheme and can thus be derived from common geometrical principles. As a consequence, it is difficult to understand the relations between different methods, which inevitably leads to the reinvention and rebranding of the same concepts. So, we need some form of geometric unification in the spirit of the Erlangen program that I call geometric deep learning. It serves two purposes. First, to provide a common mathematical framework to derive the most successful neural network architectures. And second, to give a constructive procedure to build future architectures in a principled way. This is a very general design that can be applied to different types of geometric structures such as grids, homogeneous spaces with global transformation groups, crafts and manifolds where we have global isometry invariants as well as local gauge symmetries. We call these the 5G of geometric deep learning. The implementation of these principles leads to some of the most popular architectures that exist today in deep learning, such as convolutional networks emerging from translational symmetry, craft neural networks, deep sets and transformers implementing permutation invariants and intrinsic mesh CNNs using computer graphics and vision that can be derived from gauge symmetries. People are quite cynical about the interpolative nature of deep learning and I think that finding this structure, this deeper structure could allow us to extrapolate in a way which is significantly better than we can now. And I asked whether he thought deep learning could get us all the way to artificial general intelligence? It's a hard question because it has several terms that are not well defined. What do you define by intelligence? So we don't understand what is human intelligence. Everybody probably gives a different meaning to this term. So it's hard for me to even to define and quantify artificial intelligence. I don't think that we necessarily need to emulate human intelligence and, as you mentioned, in the past we thought of artificial intelligence as being able to solve certain tasks and it's a kind of a moving target. We thought of, I don't know, playing intelligent games or perception of the visual world like computer vision or understanding and translating language or even creativity and today we have machine learning systems that are able to address at least to some extent all of these tasks, sometimes even better than humans and are we there yet artificial intelligence? I don't think so. And probably artificial intelligence will look differently from human intelligence. It doesn't need to look like human intelligence. It's of course an interesting scientific question whether we can reproduce a human in silico, but for solving practical problems that will make this technology useful for the humanity, for the humankind, we probably need something different. It will certainly involve a certain level of abstraction that we currently don't have. It will probably require methods that we currently don't have, but it doesn't necessarily need to look like a recreation of a human. This is Dr. Petar Velichkovich. By now you'll have probably seen or heard something about our recently released proto book on geometric deep learning on grids, graphs, groups, geodesics, and gauges, or as we like to call it the 5Gs of geometric deep learning, which I've co-authored alongside Michael Bronstein, John Brunner, and Taco Cohen. And you might be wondering what all the fuss is about, because there's already a lot of really high quality synthesis textbooks on the field of deep learning in general, and also on some sub areas of geometric deep learning, such as graph neural networks, where Will Hamilton recently released a super high quality textbook on that area. This is Dr. Taco Cohen. So what we've been trying to do in our book project is to show that this geometric deep learning mindset is not just useful when tackling a new problem, but actually allows you to derive from first principles of symmetry and skill separation, many of the architectures and architectural primitives like convolution, attention, graph convolution, and so forth, that have become popular over the last few years. Even in cases where these considerations of active variance and skill separation were not felt at center when the methods were first discovered. Now we think that this is useful for a number of reasons. First of all, it might help to avoid reinventing the same ideas over and over. And this can easily happen when the number of papers that come out every day is far larger than what any one person can possibly read, and when different sub fields use different language to describe their ideas. Furthermore, it might help to clarify when a particular method is useful. A geometric deep learning method is useful when the problem domain has the particular symmetries that are built into the architecture. And finally, we hope that by making explicit the commonalities between seemingly different methods, it will become easier for new cars to learn geometric deep learning. Ideally, one would not have to go through the large number of architectures that have been designed, but just learn the general ideas of groups, equivariance, group representations and feature spaces and so on, and then see, for the particular instances, you're interested in how that fits into the general pattern. To really illustrate why do we think that such a synthesis is important and relevant for deep learning research going forward, we have to go way back, way back in the time of Euclid, around 300 years BC. And as you might know, Euclid is the founding father of Euclidean geometry, which for many, many years was the only way to do geometry. It relied on a certain set of postulates that Euclid had that governed all the laws of the geometry that he proposed. All of this started to drastically change around the 1800s when several mathematicians, in an effort to prove that Euclid's geometry is the geometry to be following, ended up assuming that one of the postulates is false and failing to drive a contradiction. They actually ended up deriving a completely new set of self-consistent geometries, all with their own set of laws and rules, and also quite differing terminologies. Among some of these popular variants are the hyperbolic geometry of Lobachevsky and Bolyai, and the elliptic geometries of Riemann. And for a very long time, because all of these geometries had completely different sets of rules and they were all self-consistent, people were generally wondering what is the one true geometry that we should be studying. A solution to this problem came several decades later through the work of a young German mathematician by the name of Felix Klein, who had just been appointed for a professorship position at the small Bavarian University of Erlangen, the so-called Friedrich Alexander University in Erlangen, Nuremberg. While he was at this post, he had proposed a direction that would eventually enable us to unify all of the geometries that were in existence at the time through the lens of invariances and symmetry using the language of group theory. And his work is now eponymously known as the Erlangen program. And there is no way to overstate how much of an important effect the Erlangen program had on mathematics and beyond. Because of the fact that it provided a unifying lens of studying geometry, suddenly people didn't need to hunt for the one true geometry they had a blueprint they could use to drive whatever geometry was necessary for the problem they were solving. And besides just mathematics, it had amazing spillover effects to other very important fields of science. For example, in physics, the Erlangen program spilled over through the work of Emy Nerther, demonstrated that all of the conservation laws in physics, which previously had to be validated through extensive experimental evaluation, could be completely derivable through the principles of symmetry. And needless to say, this is a very fundamental and game changing result in physics, which also allowed us to classify some elementary particles in what is now known as the standard model. Thinking back towards theoretical computer science, the Erlangen program also had a spillover effect into category theory, which is one of the most abstractified areas of theoretical computer science with a lot of potential for unifying various directions in mathematics. And actually in the words of the founders of category theory, the whole field of category theory can be seen as an extension of Felix Klein's Erlangen program. So the Erlangen program demonstrated how it's possible to take a small set of guiding principles of invariance and symmetry and use it to unify something as broad as geometry. I like to think of geometric deep learning as not a single method or architecture, but as a mindset. It's a way of looking at machine learning problems from the first principles of symmetry and invariance. And symmetry is a key idea that underpins our physical world and the data that is created by physical processes. And accounting for this structure allows us to beat the curse of dimensionality in machine learning problems. It is really a very powerful principle and very generic blueprint. And we find its instances in some of today's most popular deep learning architectures, whether it's convolutional neural networks, graph neural networks, transformers, LSTMs and many more. And it is also a way to design new machine learning architectures that are yet to be invented, maybe in the future, not based on back propagation and incorporate inductive bias in a principled way. Being a professor and a teacher, I would also like to emphasize the pedagogical dimension of this geometric unification. What I often see in deep learning when deep learning is taught is it appears as a bunch of hacks with weaker or no justification. And I think it is best illustrated with how, for example, the concept of convolution is explained. It is often given as a formula just out of the blue, maybe with a bit of hand waving. But what we try to show is that you can derive convolution from first principles, in this particular case, of translational symmetry. And I think the difference in this approach is best captured by what Elvetsos once said that the knowledge of principles easily compensates the lack of knowledge effects. Professor Bronstein has been a professor at Imperial College in London for the last three years and received his PhD with distinction from Technion, the Israeli Institute of Technology, in 2007. He's held visiting academic positions at MIT, Harvard and Stanford, and his work has been cited over 21,000 times. His main expertise is in theoretical and computational geometric methods for machine learning and data science, and his research encompasses a broad spectrum of applications ranging from computer vision and pattern recognition to geometry processing, computer graphics and biomedicine. Professor Bronstein coined and popularized the term geometric deep learning. His startup company, Fabula AI, which was acquired by Twitter in 2019, was one of the first applications of graph ML to the problem of misinformation detection. I think it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert in graph representation learning research. We are really probably some of the nicest locations in London, which is Kensington, if you're familiar, so we have the the Natural History Museum, the Science Museum and the Victoria and Albert Museum and Imperial College is right here, so I think it's as central as you can get. And if you walk all the way there, then you have Hyde Park, which is probably one of the nicest parks in London. I'm a professor in the department of computing at Imperial College London and head of graph learning research at Twitter. I work on geometric deep learning in particular on graph neural networks and their applications from computer vision and graphics to computational biology and drug design. Dr Petar Velichkovich is a senior research scientist at DeepMind in London and he obtained his PhD from Trinity College in Cambridge. His research has been focused on geometric deep learning and in particular devising neural network architectures for graph representation learning and its applications in algorithmic reasoning and computational biology. Petar's work has been published in the leading machine learning venues. Petar was the first author of graph attention networks, a popular convolutional layer for graphs and deep graph infomax, a scalable unsupervised learning pipeline for graphs. Hi everyone, my name is Petar Velichkovich and I'm a senior research scientist at DeepMind and previously I have done my PhD in computer science at the University of Cambridge where I'm actually still based and today we're actually here in Cambridge filming these shots and it is my great pleasure to be talking to you today about our work on geometric deep learning and related topics. I first got into computer science through competitive programming contests and classical algorithms the likes of which you might find in a traditional theoretical computer science textbook and this was primarily influenced by the way schooling worked for gifted students back in my hometown of Belgrade in Serbia where students were generally encouraged to take part in these theoretical contests and try to write programs that are just going to finish as fast as possible or work as efficiently as possible over a certain set of carefully contrived problems. All of this changed when I actually started my computer science degree here at Cambridge where I was suddenly exposed to a much wider wealth of computer science topics than just theoretical computer science and algorithms and for a brief moment my interests drifted elsewhere. Everything started to come back together when I started my final year project with Professor Pietro Leo at Cambridge and I had heard that bioinformatics is a topic that's brimming with classical algorithms and competitive programming algorithms specifically so I thought a project in this area would be a great way to bring these two closer. Unfortunately that was not to be as my mentor very quickly drew me into machine learning and that kind of spiraled out into my PhD topics where I was for a brief moment focused on computational biology topics before eventually drifting to graph representation learning and eventually geometric deep learning. My journey into geometric deep learning started actually through investigating graph representation learning which I think for a very long time these two areas have been seen as almost synonymous with one another because almost everything you come up with in the area of geometric deep learning can be if you squint hard enough seen as a special case of graph representation learning. What originally brought me into this was an internship at Montreal's Artificial Intelligence Institute Miele where I worked alongside Joshua Bengio and Adriana Romero on initially methodologies for processing data that lives on meshes of the human brain. We found out that the existing proposals for processing data over such a mesh both in graph neural networks and otherwise were not the most adequate for the kind of data processing that we needed to do and we needed something that would be aligned more with image convolutions in spirit in a way that allows us to give different influences to different neighbors in the mesh and this led us to propose graph attention networks which was a paper that we published at Eichler 2018. It was actually my first top tier conference publication and what I'm probably most well known for nowadays. The field of graph representation learning has then spiraled completely out of control in terms of the quantity of papers being proposed. Only one year after the graph attention network paper came out I was reviewing for some of the conferences in the area and I found on my reviewing stack four or five papers that were extending graph attention nets in one way or another so the field certainly has become a lot more vibrant because of a nice barrier of entry which is not too high. Recently Pettai has been doing some really interesting research in algorithmic reasoning. Part of the skill of a software engineer lies in choosing which algorithm to use only rarely will an entirely novel algorithm be warranted. The key guarantee which traditional symbolic algorithms give us is generalization to new situations. Traditional algorithms and the predictions given by deep learning models have very different properties. The former provides strong guarantees but are inflexible to the problem being tackled while the latter provide few guarantees but can adapt to a wide range of problems. Now Pettai in his paper proposed a neural architecture which can take in natural inputs but output a graph of abstract outputs as well as natural outputs. Pettai believes that neural algorithmic reasoning will allow us to apply classical algorithms on inputs that they were never originally designed for. I am studying algorithmic reasoning which is a novel area of representation learning that seeks to find neural networks that are as good as possible at imitating the computations of exactly the kind of classical algorithms that initially brought me to computer science. It turns out that this area is remarkably rich and could have remarkably big implications for machine learning in general because it could bring the best of the algorithmic domain into the domain of neural networks and if you look at the pros and cons of the two you'll find that they're very complementary. So the fusion of the two can really bring the kinds of benefits we haven't seen before. So I am very pleased to say that I'm among those researchers that is extremely proud and happy of what I do because it brings together some of my earliest passions in computer science with the latest trends in machine learning and especially geometric deep learning which we recently released a proto book about with Joan, Michael and Taco. We spoke to Christian Saagedi and he's doing some interesting work with algorithmic reasoning creating abstract syntax trees to represent mathematical theorems for example and then he believes that in that representation space he projects it all into a Euclidean space that there's some interesting interpolative points in that space but again surely there must be some deeper structure which analogizes mathematics which would allow us to extrapolate and discover new interesting mathematics. It just feels that what we're missing is the right kind of structure. I think for mathematics it's relatively easy to formalize it because well we can write logic rules and basically we can build mathematics axiomatically from very basic principles. These methods are already being used for computer proof of certain theorems. I think it's not well regarded by the purists in pure mathematics but probably they will need to accept it and well you know maybe there will be fields medal that will be given for a proof that is done by a computer I think even recently there are some important breakthrough results proofs that were given by a computer so it is probably just the beginning of a new way of doing science essentially even as pure science as creative science as mathematics which was considered really the hallmark of human intelligence it can be maybe if not replaced assisted by by artificial intelligence. Petr invokes Daniel Kahneman's system one and system two. He thinks that we need something like system two to achieve the kind of reasoning and generalization which currently eludes us in deep learning models. What I'm holding in my hands right now is the international bestseller on thinking fast and slow from the famous Nobel Prize winner Daniel Kahneman. This book can be seen as one of the main theses behind my ongoing work in algorithmic reasoning and what it stands for because it argues that fundamentally we as humans employ two different systems that operate at different rates system one which primarily deals with perceptive tasks and system two which deals with longer range reasoning tasks and it is my belief that currently where our research in neural networks has taken us is to get really really good at automating away system one so being able to perform perceptive tasks from large quantities of observed data probably in a not too dissimilar manner from the way humans do it. What I feel is really missing from these architectures nowadays is the system two aspect being able to actually take these percepts that we've acquired from the environment and actually properly do rigid reasoning over them in a manner that will stay consistent even if we drastically change the number of objects slightly perturb the laws of physics or something like that. In my opinion algorithmic reasoning the art of capturing these kinds of reasoning computations inside a neural network that was trained specifically for that purpose and then slotting that neural network into a different architecture that works with raw percepts is one potentially very promising blueprint that will take the space of classical algorithms that we have been building in this system two space and carry them over into raw perceptive inputs which these algorithms were very rarely designed to work over. This is Dr. Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher at Qualco AI Research and I work on geometric deep learning, equivariate networks and more recently also on causal inference and causal representation learning. Now I've been interested for a number of years already since about 2013 in the application of ideas around symmetry, invariance, equivariance and the underlying mathematics of group theory and group representation theory to machine learning and deep learning specifically. And so it's been quite exciting to see over the last few years really the blossoming of this field that we now call geometric deep learning. Many new methods such as various kinds of equivariate convolutions for different spaces, different groups of symmetries, different geometric feature types, equivariate transformers and attention mechanisms, point cloud networks, graph neural networks and so forth. And along with these new methods also a large number of applications that have been tackled. Anything from medical imaging to the analysis of global weather and climate data to the analysis of DNA sequences and proteins and other kinds of molecules. So if you apply this mindset, the first question you ask when faced with such a new problem is what are the symmetries? What are the transformations that I can apply to my data that may change the numerical representation of my data as stored in my computer, but that nevertheless don't change the underlying objects we're interested in. Whether that's reordering the nodes of a graph, rotating a molecule in 3D or many other kinds of symmetries. Once you know the group of symmetries, you can then develop a neural network that's equivariate symmetries. And hopefully is a universal approximator among equivariate functions. What we found, what many others have found time and again, is that if you build this symmetry prior to your network, if you make your network equivariate, it is bound to be much more data efficient and to generalize much better. Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this book is that really a lot of different architectures, they can be understood in one as essentially a special cases of one generic structure that we call the geometric deep learning blueprint. So to explain a little bit about what this is all about, the blueprint refers to first of all feature spaces. So I'll explain how we model those. And then it refers to maps between feature spaces or layers of the network. And they also have to somehow respect the structure of the feature spaces. So in all cases, whether it's, you know, graph neural net, a network for processing images on the plane, or a network for processing signals on a sphere like global weather data, we're dealing with data that lives on a domain. So the domain in the examples I just gave would be the set of nodes of the graph, or perhaps also the set of edges, the points on the plane, or the points on the sphere. And of course, you can think of many other examples as well. This is what we call the domain. We write it as omega. It's typically, it's a set, first of all, and it may have some additional structure. So in the case of the sphere, it has some interesting topology. And typically, we also want to think about the geometry, want to think about distances and angles. So it's a set with some kind of structure. And in addition, it has some symmetries, meaning there's some transformations we can do to this set, that will preserve the structure that we think is important. So if we think distances are important on, let's say the sphere, when the kinds of symmetries we end up with our rotations, three dimensional rotations of the sphere, perhaps also reflections. In the case of a graph, the symmetries would be permutations of the nodes and also corresponding permutation of the edges. So you just change the order of the nodes. If node one and two were connected, you apply permutation and move those to node three and four, then now three and four have to be connected. So that's a symmetry of our space overgon. Now the data, this is a very important point, the data are typically not points in this space. So when we're classifying images, well, our space is the plane, but the data are not points in the plane, they're not the two dimensional vectors, the data is really a signal on the plane, a two dimensional image, which so you can think of that as a function from for each point in the plane, we have a pixel value. So in the more general cases, it might be something called a field. So you might say have wind direction on on earth, that's a vector field on the sphere. Now the symmetries that we talked about, they act on this space, you could show how they automatically also act on the space of signals. So those are the key ingredients to define what a feature space is, you have your your space, omega, so set of nodes, sphere, whatever, then you have a group of symmetries of that space. And then you have the space of signals or feature maps on this space, and you have the group acting on your signal. So you can rotate a vector field, or you can shift a planar image or etc. So those are the key ingredients to define a feature space. And then we can talk about layers. And so the layers or maps of the network, they have to respect this structure. So if we have a signal on on the sphere, and we believe that rotating it doesn't change it in any essential way, or we permute the nodes in the graph, but it's still the same graph, then we want the layers of the network to respect that. And to respect the structure means to be actually variant to the symmetries. So if we apply our transformation to our signal, and then we apply our network layer, it should be the same as applying the network layer to the original input, and then applying a transformation in the output space. Now how the transformation acts in the output space could be different from the input space. So for example, you could have a vector field as input, and a scalar field as output, they transform differently. So that's why for each layer and network, you're going to have a different feature space with a different action of the group. But it's the same group acting in each feature space, the maps, they should be equivariant. And these maps, they include both the linear maps, which are typically the learnable layers, and the non linearities. Now for linear maps, you can study all sorts of interesting questions, you can ask what is the most general kind of equivariant linear map. And it turns out that for a large class of group actions, or linear group actions, the most general kind of equivariant linear maps are generalized forms of convolutions. So that's really an explanation for why convolutions are so ubiquitous. The final ingredient that I think is key to the success of many architectures is some kind of pooling or cautioning operation. So the structures we've talked about so far are, you know, this global space and the symmetries on it, etc. But typically, there's also a notion of distance or locality in our space. And if we just enforce that our layers have to respect the symmetries, well that would force us to use a convolution in many cases, and I just mentioned, but not forces to use local filters. And we all know that using sort of global filters is not going to be very effective use of parameters. So locality is another key thing, locality in the filters. And also localities exploited via some kind of pooling or caution operation. Now, going forward to say the year 2020, deep learning is all the craze right now. And so many different deep learning architectures are being proposed, convolutional neural networks, graph neural networks, LSTMs and so on. When these architectures are being proposed, different terminologies are used because people tend to come from different areas when they're proposing them. And also, they are usually followed by kinds of bombastic statements such as everything can be seen as a special case of a convolutional neural network, transformers use self-attention and attention is all you need. Graph neural networks work on graphs and everything can be represented as a graph. And LSTMs are turing complete, so why would you ever need anything else? So I hope that this illustrates how the field of deep learning in the year of 2020 is really not all that different from the state of geometry in the 1800s. And if history teaches anything about how we can unify these fields together, now is the right time for us to look back, study the geometric principles underlying the architectures that we use. And as a result, we might just derive a blueprint that will allow us to reason about all the architectures we have in the past, but also any architectures that we might come up with in the future. And in my opinion, that is the key selling point of our recently released proto book. And I hope that it is helpful in guiding deep learning research going forward. I should highlight that I was also extremely, extremely honored to deliver the first version of the talk presenting our proto book at Frederic Alexander University of Erlang in exactly the same place where the Erlang program was originally brought up, albeit because of the existing COVID restrictions, I had to do so in a virtual manner. So I think a lot of people understand convolutions in the way of a traditional plain us, CNN, and the mathematical notion of a convolution, which is very closely related to, you know, a Fourier transform, for example. But graph convolutional networks, they seem to abstract the notion of a convolution into some kind of concept of the neighborhood and local connectivity. And some of your work as well with, you know, equivariate convolutional neural networks, I think do the same thing. So when we talk about convolution, are we talking about a very abstract notion of it? Yeah, that's a great question. I think that there are so many different ways to get at convolution. You can think of it in sort of you've trying to think of it in terms of sliding a filter over some space, right? So you put it in a canonical position, and then you slide it around. That is an idea that you can generalize to not just sliding over a plane, but say applying some transformation from a group to your filter. So let's say you're up, you have a filter on the sphere, then you can, you have, you know, the sphere has a symmetry group, mainly three dimensional rotations, group SO3, you can apply an element of SO3, a three dimensional rotation to your filter, even sort of slide it over, over the sphere. That leads to something called group convolution. So that's one way to think about it. There is indeed, as you mentioned, the spectral wave looking at it. So you could think there's the famous Fourier convolution theorem, which says that in the spectrum, convolution is just a point wise product. So one way to implement a convolution would be to take a Fourier transform of your signal, your feature map, and a Fourier transform of your filter, multiply them point wise, and then inverse Fourier transform to get the result. And this perspective also generalizes. So it generalizes to graphs, where the Fourier transform, or something analogous to it, can be obtained via graph laplacians, as well as some other ideas. That is actually historically how some of the first graph neural nets were implemented and motivated. There's also a spectral theory for group convolutions. So indeed, the Fourier transform can be generalized, or the standard Fourier transform that we all know, is actually the Fourier transform for the plane, the plane being a particular group, the translation group in two dimensions. So there is a whole, a very beautiful theory of, let's say, Fourier generalized Fourier transforms for groups, where now the spectrum is indexed not by the just by the integers, as it is the case for the for the for the line or the plane, but by something called irreducible representations. In the case of the plane, those are indeed indexed by integers. And the the spectrum is not just scalar valued, or complex scalar valued, but it can be matrix valued. If you're interested in this sort of stuff, you can, you want sort of a very high level description, you can check out our paper on spherical CNNs, where we actually implement convolution on a sphere, using this kind of generalized Fourier transform. So that's the Fourier perspective on convolutions. And there's a final perspective, which I think is quite intuitive, which is that the convolution is the most general kind of equivariant linear map between certain group between certain linear group actions. So specifically, these group actions are the way that groups tend to act on the space of signals on your space. So you might have space of scalar signals on the sphere. And your feature map, you might want to have another scalar signal in the sphere or a vector field on the sphere or something. Then you can ask what is the most general kind of equivariant linear map. And the answer is, it's a convolution. And that is also true in it in a very general setting. So that that to me is the most intuitive way of understanding convolutions as the most general kind of maps that are linear maps that are equivariate to certain kinds of group actions. Professor Bruner, welcome to MLST. It's an absolutely honor to have you on. Introduce yourself. Yeah. Hi, Tim. I'm Joanne Bruner. I'm a associate professor at the Quran Institute and Center for Data Science at New York University. And I'm very happy to be here chatting with you at MLST. Joanne, you just released this geometric deep learning proto book. You know, what does it mean to you? What was your kind of intellectual journey that led to this? Yeah. So this journey, in fact, started many years ago. So I would say even like during my postdoc, I was a postdoc. I was doing my postdoc here at NYU with Yanle Kun. And that was the time 2013, where, you know, confnets were already showing a big promise in image tasks. And discussing with Yan, the questions are, okay, how about domains that are not like grades, right? So, and that was the beginning of a journey that also included my former collaborator, Arthur Slump, who was like another researcher in the lab that had a similar background as me, like coming from applied math, but looking into into into more and more deep learning. So I would say that the genesis was our first attempt at extending the success of convolutional networks to these regular domains. And we published the paper at iClear 2014. And after that, I think things started like quite naturally because other researchers that had, I would say, came from the same background, like, you know, maybe from geometric background, also started to realize that there was something there, maybe bigger than these particular papers, in particular, Michael Brownstone, it's kind of reached out to us just afterwards saying, oh, and we are also looking at similar ideas. I think we should just team together and start to think about this more globally. And so, you know, things that developed from there. And we wrote this journal paper, like a review paper in 2017, with Arthur, Yan, Michael, the airbender guys, who is another very well known figure in this area. And so from there, I think that, yeah, things like started to slowly take off. We had tutorials and new ribs that we had a very successful workshop at IPAM. After this, I think that the thing were clear that, okay, maybe at some point in the future, we should try to stamp all these things into a book that tries to reflect something a bit more, let's say, mature and, you know, what is our, if we wanted to have like some kind of legacy for future generations on how to implement and communicate these methods, how would that be? So I guess that was the genesis of the book. And so very soon we said with Michael that we also would like to have some, you know, fresh minds and fresh energy on board. So naturally, the names of Taco and better came very naturally to us as people who had been doing excellent work in the domain that would very nicely complement our skills. So the team was created. And that's the project. And so, yeah, I mean, I think it's been very interesting so far. Of course, I guess that, as you know, this is just an ongoing project, right? So it's still not finalized. But hopefully we're getting interest from the community. And this gives us some kind of, I would say positive vibes to finish it on time. As you know, writing books is always like this never ending process. So I think that, yeah, that's, it's been an interesting endeavor so far, for sure. I asked Professor Bromstein what his most passionately held belief is about machine learning. I think machine learning is such a field where holding strong beliefs is often counterproductive. It happened to me multiple times that something that I thought or said was very quickly overturned. And what I mean is that milestones or progress was achieved much faster than I could even imagine in a wild dream. So making predictions about machine learning is, to some extent, an ungrateful job. I do, however, believe that in order to make progress to the next level and make machine learning achieve its potential to become the transformative technology we trust and use ubiquitously, it must be built on solid mathematical foundations. And I also think that machine learning will drive future scientific breakthroughs. And probably a good litmus test would be a Nobel Prize awarded for a discovery made by or with the help of an ML system. It might already happen in the next decade. I wanted to go into a few questions actually about the book. So first of all, Joanne, in your 2017 paper, The Mathematics of Deep Learning, you cited the universal function approximation theorem, which is to say the ability of a shallow neural network to approximate arbitrary functions. But the performance of wide and shallow neural networks can be significantly beaten by deep networks. And you said that one of the possible explanations is that deep architectures are able to better capture invariant properties of the data compared to their shallow counterparts. Could you just briefly introduce the universal function approximation theorem? And do you think it's still relevant for today's neural networks? Yeah, I mean, that's a very deep and an important question. Yeah, so universal approximation theorem, it refers to this very general principle that once you define parametric class, let's say you're you are on a learn functions using neural nets, it just describes your ability that as you put more and more parameters into your class, you're going to able to to approximate essentially anything that data nature throws at you. And so this might seem like a very powerful property. But in fact, in fact, it's it's something that you have probably already encountered many times during your undergrad. I mean, if you have any, let's say, background in, you know, single processing electrical engineering, there's many ways in which students have learned how to represent data, like signals, for example, using Fourier transform. So Fourier transforms are an instance of a class that has universal approximation. So in that sense, it's a it's a I think, going back to the second part of your question, how relevant it is to in the context of neural networks, and how far does this thing pushes towards understanding why deep learning works. So I would say there's a there's two sides of the answer. On one hand, I think that universal approximation is is a tool that when you combine it with other elements, it becomes something that provides good guiding principles. For instance, universal approximation of a generic function, we know that yeah, we can as I said, we can obtain it with, you know, very, very naive architectures, for example, just a shallow neural network without any kind of physical structure, special structure already has this property. Does it actually help us to to learn very efficiently? No, right, and I'm going to go at this afterwards. But when you combine it, for example, with, you know, let's say that now your data lives on a graph, or your data, I don't know, has a certain like a come from a physical lab that has certain properties, let's say that it's rotational invariant. Like the first thing that that the designer, like a domain scientist would like to know, if you come there and you design your neural net, look, I have a neural net that takes your data and has very good performance. The first thing that he will ask is, okay, how general is your architecture, right? Can it explain anything that they could throw at you? It seems like it's a, in that sense, I would, I would present it more as a sufficient condition, like a check mark that your, your, you know, your architecture needs to fall, right? If you make more and more parameters, can you express more and more elements functions from your class? But then, as I said, is it far from being sufficient, right? It's like, it's, sorry, it's a, it's a necessary condition, but it's far from being sufficient in the sense that universal approximation has a flavor is a result that does not quantify how many parameters do I need, right? Like, you know, if I want to approximate function, let's say I want to classify between different dog breeds, it doesn't tell me this theorem doesn't tell me how many parameters, how many neurons do I need for that, right? It's, it's a statement that in that sense, it lets, it leaves you a little bit with your, like, say, like, you know, like, like the, it's a bittersweet result, right? It doesn't, it doesn't really tell you anything actual. So that's why, and that's why we enter these, these other questions that is actually much deeper. And to some extent, still reading them pretty much open, that is, how do you actually go from this, this statement to something that is quantitative, right? Something that tells you, okay, you know, you need that many layers, you need that many parameters. And so this is where the role of depth in neural networks is, you know, becomes essentially the key open question. And, and yeah, so in this, in this quote that you, that you brought from this paper, that kind of reflected our understanding at the time of maybe the true power of universal approximation is, you know, when, as you combine it with these other prior, that is the asymmetries of the data, like the invariances. So I don't want to represent arbitrary functions, I only want to represent functions that are invariant to certain transformations of the input. So in fact, our, at least my particular view of the problem, analysis of the problem has somehow evolved in the last years, right? Of course, through research that I've done together with my collaborators. And now, and this is actually the way we present it in the book, we, we, we kind of identify two different flavors, two different sources of prior information that one needs to bake into the problem, right? To, to really go beyond this like basic approximation result of neural nets. The first one you need is invariance, right, is a disability that you need to, like the fact that you actually put symmetries into the architecture is certainly going to have a benefit in terms of sample complexity, right? They, I mean, you are going to learn more efficiently if your model is aware of the symmetries of the world. But in fact, this, this prior, in fact, we know now that it's not sufficient, right? If you only like agnosticially build your learning system, just with these symmetries in mind, indeed, you are going to become more efficient that a system that is completely agnostic to symmetries. But it might not be, you might not be able to formally establish what we call like a, like a learning guarantee that has good sample complexity. And I guess that I don't want to go too much into the jargon and the details of what this means. But the idea is that if I want to, you know, learn this function with certain precision, how many examples, how many training examples do I need to kind of give you a certificate for authentication, like a guarantee that I'm going to be able to do that. So with symmetries alone, it's not something that we know how to do. In fact, we are, we would believe we have strong beliefs that it's not possible, right? There's examples out there that I could, I could, you know, construct a function that has the right symmetries, has the right priors, if you want, but still needs a lot, a lot, a lot of examples to build to learn. So what we need is to add something else into the mix. And this is this something that we call in the, in the book, this scale separation. And, and, and if you want, I can try to very briefly give you an intuitive idea of what this means. So if you think about the problem of classifying an image, like a dog or the cat. So what is given to you is like a big branch of pixels, right? Every pixel has a color value. So somehow you need to figure out the thing that you're looking for is lying in some kind of like, it's really through the interactions between pixels that you get the answer, right? And the question is, of course, if I have a thousand pixels, how many possible interactions do I have between thousand elements? So this is where this exponential or the curse of dimensionality appears, right? I need to, a priori, I should be looking at all possible families of interactions between pixels. And this is where maybe where my signal would be lived. Of course, if I need to look for all of these things, it's impossible, right? There's just too many things. If I tell you that the, you know, these interactions are such that there's this translation symmetry. Well, you might not be, you might not be needing to look at all of them. But in fact, you don't need, you do not throw enough. So what is really something that is powerful is that I tell you that maybe the interactions that matter the most are those between a pixel and its neighbors, right? And if you understand very well, if you base your initial learning steps into understanding well, which local interaction matters, maybe you can use them to bootstrap the interactions that go to look at this neighborhood to slightly bigger neighborhoods, right? And so this idea that you can break a very complicated problem into a families of sub problems that lives in different scales. This is at the, I would say, at the intuitive level, something print like at the core of the essence of why these architectures are so efficient. This idea, as you might imagine, is not new. It's not specific to deep learning. The idea that you can take a complicated system of interacting particles and break it into different scales. This is at the, at the basis of essentially all of physics and chemistry, right? There's many, many, you know, like when people study, even like biology life, right? You have, you have experts that are very experts at the molecular level. Then you have experts that, you know, might understand like, you know, doctors that understand things at the level of okay functions. And then there's maybe experts at the level of the society, right? But this, you know, it's pretty natural to break the very complicated thing into different scales. And so deep neural networks somehow are able to do that. We don't have the full mathematical picture, right? Or for example, why this scale separation is strictly necessary. What we know from empirical evidence, like that is now I would say indisputable, is that this is an efficient way to do that, right? Because when I was, when I was reading the prototype book, I noticed that there was a separation between the symmetries and the scale separation. Could you explain in simple terms, why is the scale separation not just a symmetry as well? Because it seemed a little bit, I don't want to say kluge, but it seemed like you had this scale matter and you dealt with it separately. Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate these two would be if you think about like an algorithmic instantiation, if you want to have a network that would just break the problem into different scales, it would be like a neural network that would operate at different patches. And for every patch of the image, I could be learning independent set of parameters, right? So that would be a model that is only told that it should be breaking the problem into different regions. But it's not necessarily told that, you know, there's a weight sharing, right? There's some kind of like parameter sharing that somehow is, you are able that, you know, in a sense is helping you to learn with fewer number of parameters. So somehow these two, these two conditions are slightly complementary. We, as I said, there's still like a lot of mathematical puzzles as to how these things interact optimally. And I think that the, the one of the reasons why we chose to explain the story into two different, in these two different priors is that they all survive this quest for generality in the sense that these two principles are, again, something that you can think about for grids, for groups, for graphs, you can also see these principles appearing completely everywhere as you study physical systems, right? Like the scale and the symmetry is really at the core of, of many physical theories. And I would say that there's also at the more maybe technical level, these two priors somehow have been instrumental to organize, like to basically to have a kind of a recipe to build architectures, right? So, so maybe now we don't even think about it, right? But when you have a new problem, a new domain, and you need to build an efficient neural network, we automatically have this idea that, okay, we are going to start learning by composition, right? So we are going to extract information one layer at a time. That's the first appearance of scale. And we know that the way we need to organize these layers, right? How do you parameterize a layer that takes some input features and produces maybe better features? This idea that we do that by understanding this kind of equivarian structure, right? We have this notion of filters, right? In convolutional networks, we have this, as I said, we organize everything in terms of filters. When we talk about message graph neural networks, we have this kind of like diffusion filters, right? And so there's this object that we extract from the domain that is helping us, that giving us something very constructive, very, you know, very relevant, like very practical. And so this is really the underlying group of transformations that is acting on our domain. And so I would say that, you know, from a practitioner's perspective, these two principles, right, that I'm going to learn by composing some fundamental layers that I repeat all the time. And the way this layer is organized, is structured through this group transformation, this has been, I would say, like a trademark of, you know, the success of neural networks. Of course, as also we mentioned in the book, these are, I would say, proto, like meta, you know, meta parameterization, right, in the sense that there's, as you know, many, many, many variants that people have come up with. Many, many, let's say, yeah, like modifications on the basic architecture that have really make dramatic changes in performance, right. So there's, of course, as I say, like the devil sometimes is in the details, right. And so as we are writing the book, we are realizing exactly, you know, how some of the changes in the architectures are actually fit into this, what we call this blueprint, right, this symmetrically learning blueprint. Fascinating. Okay, so I wanted to come back to what you were saying a few minutes ago about the the sample efficiency of these models. Now, with graph neural networks, for example, there are factorially many permutations of adjacency matrices for a given graph. And I want to talk about a sorting algorithm, right. So Fran\u00e7ois Chollet came on the podcast and he said that in order to learn a sorting algorithm that generalizes, you would need to learn point by point, you would need to see factorially many examples of permutations of numbers. Do you think that we could train a neural network to learn a sorting? I guess what I'm asking in a roundabout way is, do you think there's a kind of geometry to computation itself? Good. That's a very good question. And in fact, we have some, some recent work with some collaborators in my group, where we kind of take up this question from and we try to formalize it mathematically, and we give answers to this question. And so many of these like a computational tasks that you were mentioning, for example, sorting or, you know, like algorithmic tasks, they are, if one wants to put them into some mathematical context, the first thing that comes to mind is these are functions that already enjoy some symmetries. For example, like a sorting algorithm, right, is invariant to permutations, right. So the function that you are trying to learn is an arbitrary function that has this symmetric class. And so as such, as I was saying in the beginning, you can try to address this question saying, okay, now you give me an arbitrary, so the question would be relative to an arbitrary learning learner that is agnostic to symmetry, how much does a symmetric learner gain, right? So you can basically try to understand, quantify the gains of sample complexity of learning without symmetry versus learning with symmetry. And so the punchline of this work, the recent work that we completed, is that one can actually quantify the sample complexity gains, and these are of the order of the size of the group. And so here in the case of like permutations, what it means is that if a learner is aware of this symmetry, like one training example of the symmetric learner is rosary equivalent to n factorial samples of the agnostic learner, right, which is something that you would expect, like if you think in terms of data mutation, right, like if I tell you in advance that your function is symmetric, is invariant to permutations, it's, you know, like a brute force approach would say, okay, you give me an instance training an input, right, and instead of giving you this input, I'm just going to, you know, like permute, like have any possible permutation of the input, and you already know the output, right? So you can as well feed it to the learner. So this is like horrific at this addition turns out to be mathematically correct, precise, at least, you know, under some conditions, right? But the, I guess the whole point is that these gains might look amazing, like might look like a, you know, like a big boost in sample complexity. As I said before, there's a grain of salt here is that the, in these conditions, in general, general conditions, you are already fighting an essential and impossible problem in the sense that the rate, like the sample complexity is dominated by a rate of basically the rate in which you learn is what we call course by dimension. And what it means is that if I want to, you know, I have a certain performance generalization error, I'm going to say that now I want to ask you the question, if I want to divide this generalization error by two, how many more samples do I need to give you? Right? Like what? So if I want to double, you know, like double, like reduce the error by half, by how much do I need to give you more samples? So this dependency, in fact, is exponential in dimension, right? So basically the, the, the sample complexity gains by invariance, they are exponential in dimension, but they are fighting an impossible problem that is already caused by dimension. So what it means is that at the end of the day, this is what, you know, what was in the, in the, like in the heart of what I was saying before, is that invariance alone might not be efficient, might not be sufficient, right? Because you are, okay, you are taking a very hard problem, you are removing an exponential factor, but you still have many, you know, you have still have something in the exponent that is exponential, right? So, so, so what it means is that that, I mean, that's what really underpins why we think in these terms of combining symmetry prior with the scale separation prior. But certainly the algorithmic tasks are very interesting playground because I think that for the case of sorting, I mean, as you know, scale separation is also an issue. It's also a very important thing, right? I mean, it, this is what basically is at the heart of the dynamic programming approaches, right? Like these efficient algorithms that are not only officially statistically, but also officially computation, right? This idea that you can divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual physical prior, right? Of a scale and invariance. On the course of dimensionality, there's this issue where you have a data point and you want to surround it by other data points in two dimensions to create a convex hole. And as you increase the number of dimensions, the number of data points you need to create this covering to create a kind of interpolative space increases exponentially. And when you get past a certain number of dimensions, let's say 16 or not, not very many, you would need essentially more data points than there are atoms in the universe. So this leads to a very interesting realization. I think some people refer to it as the manifold hypothesis, which is that most natural data is only really spatially novel on very few dimensions. And a lot of data falls on very smooth low dimensional manifolds. But what are the implications of this? Essentially, all machine learning problems that we need to deal nowadays are extremely high dimensional. So even if we take very modestly sized images, they live in thousands or even in millions of dimensions. And if you think of machine learning or at least the simplest setting of machine learning as a kind of glorified function interpolation, the standard approach is to function interpolation as just use the data points to predict the values of your function will simply not work because of the phenomenon of cursive, the recursive dimensionality that increasing the number of dimensions, the number of such points blows up exponentially. So what you really need to take into account and probably this is really what makes machine learning work in practice is the assumption that there is some intrinsic structure to the data and it can be captured in different ways. So it's either the manifold assumption where you can assume that the data, even though it lives in a very high dimensional space intrinsically, it is low dimensional. This can be captured also in the form of symmetry. For example, image is not just a high dimensional vector. It has underlying grid structure and grid structure has symmetry. This is what captured in convolutional networks in the form of shared weights that translates into the convolution operation. I think the symmetries are part of the magic here because it's not just the interesting observation that natural data is only spatially novel in so few dimensions. There's something magic about symmetries. And when we spoke to Fran\u00e7ois Chalet recently, he invoked the kaleidoscope effect, which is this notion that almost all information in reality is a copy of some other information. Probably here it will be a little bit stretching, but I would say that because data comes from nature, from physical processes that produce it, physics and nature itself is in a sense low dimensional. So it's application of simple rules at multiple scales. You can create very complex systems with very simple rules. And this is probably how our data that we are mostly interested in in machine learning is structured. So you have this manifestation of symmetry and self-similarity through different scales, the principles of symmetry and certain environs or equivalents and of scale separation, where you can separate your problem to multiple scales and deal with it at different levels of resolution. And this is captured, for example, in pooling operations in convolutional neural networks and in other deep learning architectures. This is what makes deep learning systems work. Fascinating. It's so good that you raised the curse of dimensionality because I was going to ask you about that. Could you explain in really simple terms, so not invoking lipships, I can't even say it properly now, but not invoking a mathematical jargon. And why exactly in your articulation does geometric deep learning reduce the impact of the curse of dimensionality? Yeah. So the curse of dimensionality, it refers generally to the inability of algorithms to keep certifying certain performance as the data becomes more complex. And data becoming more complex here means that you have more and more dimensions, more and more pixels. And so this inability of scaling, basically it really says that if I scale up the input, my algorithm is going to have more and more trouble to keep the base. And so this curse can take different flavors. So this curse might have a statistical reason in the sense that as I make my input space bigger, there would be many, many, many much exponentially more functions, real functions out there that would explain the training set that would basically pass through the training points. And so the more dimensions I add, the more uncertainty I have about the true function. So I would need more and more training samples to keep the base. This curse can also be from the approximation side. So in the sense that the number of neurons that I'm considering to approximate my target function, I need to keep adding more and more neurons at the rate that is exponential in dimension. And the curse can also be from the computational side. The sense that if I keep adding parameters and parameters to my training model, I might have to optimize to solve an optimization problem that becomes exponentially harder. And so you can see that you are basically bombarded by all angles. And so an algorithm like here in the context of statistical learning or learning theory, if you want, having a kind of a theorem that would say, yes, I can promise you that you can learn, you need to actually solve these three problems at once. You need to be able to say that in the conditions that you're studying, you have an algorithm that it does not suffer from approximation nor statistical nor computational curses. So as you can imagine, it's very hard because you need to master many things at the same time. So why do we think that geometric deep learning is at least an important piece to overcome this curse? As I said before, so geometric deep learning is really a device to put more structure into the target function. So basically to make the learning problem easier because we are promising the learner more properties of a target function. We are basically making the hypothesis class if you want smaller. That said, as I said, there's still some path to go. We're describing just a bunch of principles that make these hypothesis spaces smaller and more adapted to the real world. But one thing that we are still lacking, for example, is the guarantee in terms of optimization. I mean, I described that the depth of these architectures is somehow something that is akin associated with the scale, the fact that you need to understand things at different scales. So as you know, from the optimization side, there's still some open questions and open mysteries as to why the gradient descent, for example, is able to find good solutions. So these are things that we believe that these architectures can be optimized efficiently just because we have these experimental evidence that is piling up. But we are, for example, we are still lacking theoretical guarantees. For the approximation, it's a bit of the same story. So we understand very well approximation properties of shallow networks, starting from universal approximation, but of course, many, many recent interesting work. But we are also still lagging a little bit behind in approximation properties for deeper networks. So as you see, it's like you can see from this discussion that, yes, we have some good reasons to believe that these are fundamental principles of learning. But there's also a bunch of mathematical questions that are still open. And this is also one of the things that I like about writing a book on this topic, because it's a very life domain. As you can see, the field is still evolving. And I think it's a good time to... Yeah, it's a good time. I mean, researchers out there are listening to us. It's a good time to think and to work and to join this program. Amazing. Will we ever understand the approximation properties of deep networks? Because with the shallow function approximation algorithm, you can almost think of a neural network as being kind of like sparse coding. And the more neurons you have, you're just kind of discreetly fitting this arbitrary function. But you don't have that visual intuition quite so much with the deep networks. Yeah, I mean, it's an important... And it's a deep question. So indeed, shallow neural networks are really, really correspond to this idea that you learn a function by stacking a linear combination of basis elements. And this is really at the roots of essentially all of harmonic analysis or functionalized. I typically think about the basis and you ask questions about the linear approximation or the approximation, etc. Deep neural networks, they introduce a fundamentally different way to approximate functions that is by composing, by composition. And so you're right. Our knowledge about this question right now is mostly concentrated in what we call separation results, like a depth separation, which consists in trying to find construct mathematical examples of functions that cannot be approximated with shallow neural networks with certain number of neurons. But indeed, they can be much better approximated with deep neural networks. So this is really understanding which kinds of functions benefit fundamentally from composition rather than from addition. And so there's a certain mathematical vision and mathematical intuition that is building up. But of course, it's still very far from explaining the true power of depth. And just to give you like a final pointer here, there's a very related question that replaces neural networks as we understand them with what we call Boolean functions, right? Like these are just circuits, arithmetic circuits that take as input some bit string and they can manipulate the bits by, you know, or operations and operations and they can keep adding gates. And so this question about what is the ability of a certain circuit architecture to approximate certain Boolean functions is actually a notoriously hard and basically has been studied in the theoretical computer science community since the 50s and the 60s, right? And there's actually very, very, very deep results and very challenging actually open questions concerning these things. So this is really, we are really touching here questions that are pretty serious at like the deep mathematical and theoretical level. And so, yes, you should not expect that in the year in the next year or two, we have a complete understanding of, you know, approximation powers of any architecture with any depth. But I think you should expect that the theory like this will continue to try to catch up with the experiments. And so we are, I think we are hoping to get like a more precise mathematical understanding of the role of depth. And as I said before, there's one thing that is fascinating about this domain that is maybe very unique to deep learning is this very strong interaction between optimization, statistics and approximation, right? Maybe it turns out that, you know, the huge depth that we have in these residual neural networks might not be necessary from the approximation side, but in fact, it's so useful for the optimization that overall is a big winner, right? So there's always these like twists about this question that are fundamentally mixing these three sources of error. I'm sorry for asking you this question, but can neural networks extrapolate outside of the training data? That's a good question. I would say that the answer, I guess, depends on your specifications, right? So I guess that the conservative answer of a statistical learning person would be no, because we don't have good theorems right now that tell us that this is the case. There's very like, you know, like a strong effort both from the practical and the theoretical community to really understand this question, like by trying to formalize it a bit more. I mean, what do we mean by distribution shift? You know, what kind of training procedures you can come up with that would give you precisely this kind of robustness? There's of course, what we call these biases, right? I mean, I can always take it like a training distribution, I make a choice of a certain architecture, I'm going to learn a function, and obviously, there's some directions if you want in the space of distributions, for which my hypothesis will turn out to be have good generalization, there might be other direction in which the contrary is true. So there's actually very nice work from Stephanie Gejalka's group at MIT, where they, for example, they studied this question in the context of value networks, also including graphs, where they, for example, they discover or they identify this strong preference for value networks to generalize along linear directions, right? In the sense that if I just decide to now, you know, shift my data in linear directions, then my function has no trouble generalizing. Maybe there's other directions, right, in which this thing is actually catastrophic. So I think that, yeah, the question, I think it's very important from, let's say, it's very important from a kind of a practitioner perspective, right? That's typically, that's clearly something that a user would like to know. But I think that from a more mathematical or theoretical level, I think we are still at the stage of trying to formalize like, okay, what do we exactly mean by extrapolation? And what are the kind of the conditions for which architecture can do it? And I think that, yeah, this is a, yeah, it's an important question. But yeah, I think we are still pretty far from having a full answer. Amazing. And final question, what areas of mathematics do people need to study before reading the proto book? Good question. So I think that our objective and our really like the idea is really to have something that is quite self-contained in the sense that we are going to provide appendices that expand on the areas that is maybe they're not typically kind of the bread and butter of machine learning people. For example, we are going to have an appendix on group theory, differential geometry, harmonic analysis. So these are areas that I think are going to be there for you to delve into, to get like the most of the paper, the most of the book. But I think that other than that, any basic, you know, any basic knowledge of linear algebra, statistics and analysis will do. So like, if you have taken a graduate level class in machine learning, you should be ready to go. Rather than just being applied in a lot of really important branches of research problems, people outside of pure research have recognized that a lot of the data that comes to us from nature is most naturally represented in a graph-structured form. Very rarely will nature give us something that can be representable as an image or a sequence. That's super rare. So very often, the structure is more irregular, more graph-like. And therefore, graph neural networks have already seen a lot of applications in domains where the data is supernaturally represented as a graph. In the domain of computational chemistry, where you can represent molecules as graphs of atoms and bonds between them, the graph neural networks have already proven impactful in detecting novel potent antibiotics that previously were completely overlooked because of their unusual structure. In the area of chip design, graph neural networks are powering systems that are developing the latest generation of Google's machine learning chips, the TPU. Furthermore, graph-structured data is super ubiquitous in social networks and the kinds of networks maintained by many big industry players. And accordingly, graph neural networks are already used to serve various kinds of content in production to billions of users on a daily basis. In fact, the recommendation system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation system for Uber Eats, all of them are powered using a graph neural network that helps serve the most relevant content to users on a daily basis. And on a slightly personal note, graph neural networks have also been used to significantly improve travel time predictions in Google Maps, which is used also by billions of people every day. So whenever you type a query, how do I get from point A to point B in the most efficient way? The travel time prediction that you get is powered by a graph neural network that we have developed that defined in collaboration with the Google Maps team. And this is of high importance not only to users that use the app on a daily basis to find the most efficient way to travel. It's also used by the various companies that leverage the Maps API so they can tell their customers what's the time it will take for a certain vehicle to arrive to them. So companies such as food delivery companies and ride-sharing companies have also extensively profited from this system, which in cities such as Sydney has reduced the relative amount of negative user outcomes in terms of badly predicted travel times by over 40%, making it one way in which graph representation learning techniques that I have co-developed are actively impacting billions of people on a daily basis. When I was an undergraduate student, I was interested in image processing and was excited about variational methods. I think it's a very elegant idea that you can define a functional that serves as a model for your ideal image and then use the optimality conditions to derive a differential equation that flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel, where you could think of an image as a manifold or a high-dimensional surface and use an energy that originated in string theory and particle physics to derive a non-euclidean diffusion PDE called Beltrami Flow that acts as a non-linear image filter. And this is what made me fall in love with differential geometry and I did a PhD with Ron on this topic. And I think these were really beautiful and deep ideas that unfortunately now are almost forgotten in the era of deep learning. And it's a pity that the machine learning research community has such a short memory because many modern concepts have really ancient roots. Ironically, we've recently used non-euclidean diffusion equations as a way to reinterpret graph neural networks as neural PDEs. And I think it really helps sometimes to have a longer time window. Now, equivariated networks tend to generalize much better and require much less data if the data indeed has the symmetry that you assumed in your model. But people often ask, you know, why do we even care about data efficiency when we can just collect more data? We live in the era of big data, right? And I think the answer why you might still be interested in data efficiency. First of all, there are applications like say medical imaging, where acquiring labeled data simply is very cost. You have to get patients, you're dealing with privacy restrictions, you're dealing with costly, highly trained doctors who have to annotate the data, come together in a committee to decide on questionable cases and so forth. So this is very expensive. And if you can improve the data efficiency by a factor of two or 10, or whatever it may be, you might just take a problem that was in the realm of economically infeasible and take it into the realm of the economically feasible, which is a very useful thing. There are other cases like graph neural nets, where the group of symmetries is so large, in this case, n factorial number of permutations, that no amount of data or data augmentation in practice is going to allow you to learn the symmetry or to learn the invariance or equivariate in your NAPO. So indeed, you see that in this space of graph neural nets, everybody uses equivariate permutation, equivariate network architectures. And then finally, you can think about the grand problems of AGI, artificial general intelligence and so forth. And here I think that we will most certainly need large data sets, large networks, a lot of compute power, and so forth. The current architectures we have clearly are performing far fewer computations than the human brain. So there's a ways to go there. But I also think that one essential characteristic of intelligence is the ability to learn quickly in new situations, situations that are not similar to the ones you've seen in your training data. And so data efficiency to me is an essential characteristic of intelligence. It's almost like an action that you want your methods to be data efficient. And so I think one of the big challenges for the field right now is to try to think of very generic priors, priors that apply in a wide range of situations. And to give you, even though they're generic and abstract, they give you a lot of bang for the buck in terms of improved generalization and data efficiency. I think that the beauty of science and research is in connecting the dots. And I find it fascinating that, for example, graph neural networks are connected to the work of Weissfeller and Lehmann from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry. And chemistry was also the field that drove the research into modern formulation of diffusion equations that were adopted in image processing community in the 90s and came back recently as a way to reinterpret graph neural networks. And I think such connections give really a new and deep perspective. And probably the deeper you dive, the broader they become. But it's really an ever-ending story. Today is an incredibly special episode and we're filming at 9 o'clock in the morning. It's really rare for us to film this early when I'm still caffeinated. Many of our guests are over in the States. It's an absolute honor to have you both on MLST. And Professor Bronstein, could you start by briefly telling us how the young mathematician Enne Offer used symmetries to discover the conservation laws in physics? Maybe I should take a step back and describe the situation that happened in the field of geometry towards the end of the 19th century. And it was an incredibly fruitful period of time for mathematicians working in this field with the discovery and development of different kinds of geometries. So a young mathematician based in Germany called Felix Klein proposed this quite remarkable and groundbreaking idea that you can define geometry by studying the groups of symmetries, basically the kinds of transformations to which you can subject geometric forms and seeing how different properties are preserved or not under these transformations. So these ideas appear to be very powerful. And what Amy Neuter showed in her work, and she actually worked in the same institution where Klein ended up in G\u00f6ttingen in Germany. And she showed that you can take a physical system that is described as functional as a variational system and associate different conservation laws with different symmetries of this system. And it was a pretty remarkable result because before that, conservation laws were purely empirical. You would make an experiment many times and measure, for example, the energy before or after some physical process or chemical reaction, and you would come to the conclusion that the energy is preserved. So this is how, for example, I think Lavoisier has discovered the conservation of energy. So it was probably for the first time that you could derive these laws from first principles. So you would need to assume in case of conservation of energy, the symmetry of time. We decided to take a tour into the world of algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks to find neural networks that are good at imitating the classical algorithms that initially brought you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning, and it's often claimed that neural networks are turing complete. And, you know, we're told that we can think of training neural networks as being a kind of program search. But you argued in your paper that algorithms possess fundamentally different qualities to deep learning methods. Francois Chollet actually often points out that deep learning algorithms would struggle to represent a sorting algorithm without learning point by point, you know, which is to say without any generalization power whatsoever. But you seem to be making the argument that the interpolative function space of neural networks can model algorithms more closely to real world problems, potentially finding more efficient and pragmatic solutions than those classically proposed by computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept of classical algorithms, as opposed to deep neural networks, at least the way we're currently applying them makes all these points about turing completeness a little bit moot, because there's quite a few proofs out there saying that you can use neural networks or more recently graph neural networks in particular to simulate a particular algorithm perfectly. But all of these proofs are sort of a best case scenario. They're basically saying, I can set the weights of my neural network to these particular values, and voila, I am imitating the algorithm perfectly, right? So all these best case scenarios are wonderful. But in practice, we don't use this kind of best case optimization, we use stochastic gradient descent. And we're stuck with whatever stochastic gradient descent gives us. So in practice, the fact that a neural network is capable for a particular setting of weights to do something doesn't mean that it will actually do that when trained from data. So essentially, this is the kind of the big divide that separates deep learning from traditional algorithms. And it has a number of other issues as well, not just the fact that we cannot find the best case solution. Also, the fact that we are working in this high dimensional space, which is not necessarily easily interpretable or composable, because you have no easy way of saying, for example, in theoretical computer science, if you want to compose two algorithms, you're working with them in a very abstract space, which means that, you know, you can easily reason about stitching the output of one to the input of another. Whereas you cannot make that easy of a claim about latent spaces of two neural networks, right? So all these kinds of properties, interpretability, compositionality, and obviously also out of distribution generalization are plagued not by the fact that neural networks don't have the capacity to do this. But the routines we use to optimize them are not good enough to to across that divide. So in neural algorithmic reasoning, all that we're really trying to do is to bring these two sides closer together by making changes either in the structure of the neural network or the training regime of the neural network or the kinds of data that we'll let the neural network see so that hopefully it's going to generalize better and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems that we might see in a computer science textbook. And lastly, I think I'd just like to address the point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit that in Europe's data set track. I think it should be public even now on GitHub, because that's the requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution, these graph neural networks are capable of imitating the steps of, say, insertion sort. So I will say not all is lost if you're very careful about how you tune them. But obviously, there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk a little bit about how even though we cannot perfectly mimic algorithms, we can still use this concept of algorithmic execution today now to help expand the space of applicability of algorithm. Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people point out as being the limitations of deep learning, right? Sholay spoke about this, but I don't think that geometric deep learning would help a neural network learner sorting function, because discrete problems in general don't seem amenable to vector spaces, either because the representation would be glitchy, or the problem is not interpolative in nature or not learnable with stochastic gradient descent. So it would be fascinating if we could overcome these problems using continuous neural networks as an algorithmic substrate. Do you think we could? I think that it is possible, but it will require us potentially to broaden our lens on what we mean by geometric deep learning. And this is something we're already very actively thinking about. I think one of our co-authors, Taco Co, and actually thought much more deeply about this in recent times. But basically, the idea is we looked at geometric deep learning from a group symmetry point of view, which is a very nice way to describe spatial regularities and spatial symmetries. But it's not necessarily the best way to talk about, say, invariance of generic computation, which you would find in algorithms, right? It's like, I have input that satisfies certain preconditions. I want to say something about once I push it through this function, it should satisfy certain post conditions. This is not the kind of thing we can very easily express using the language of group theory. However, it is something that perhaps we could express more nicely using the language of category theory, which is an area of math that I still don't know enough about. I'm currently actively learning it. But basically, in the language of category theory, groups are super simple categories that have just one node, right? You can do a lot more complicated things if you use this more broader abstract language. And, you know, you talk about basically anything of interest there in terms of these commutative diagrams. And Taco actually recently had a really interesting paper called natural graph networks, where they basically generalize the notion of permutation, equivariance that you might find in graph nets to this more general concept of natural transformations. So now suddenly, you don't have to have a network that does exactly the same transformation in every single part of the graph. What you actually need is something a bit more fine grained. You just need for all like locally isomorphic parts of the graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think that this kind of language, like moving a bit away from the group formalism would allow us to talk about say algorithmic invariance and things like this. I don't yet have any theory to properly prove this, but it's something that I'm actively working on. And I guess I would say, you know, the only question is, would you still call this geometric deep learning? And in my opinion, the very creators of category theory have said that category theory is a direct extension of Felix Klein's Erlangin program. So since the founders of the field have already made this connection, I would expect that, you know, it would be pretty applicable under a geometric lens. So it seems to come back to it seems to come back from both of your sides to essentially graphs and working on on sort of graphs to capture on the one side, the sort of symmetries that are that you either assume in the problem or that you know, or that you want to impose on the other side on the other side. Now, these computations can may be well represented in in graphs. What's what's so special about graphs in your estimations? Is there something fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I asked him the same question, what's so special about graphs and his argument was essentially, well, it's not about graphs, it's simply a good representation of the problem that we can make efficient operations on. Do you have a different opinion on that? Is a graph something fundamental that we should look more at than, for example, a tensor? Graphs are abstract models for systems of relations or interactions. I should maybe specify pairwise relations and interactions. And it happens that a lot of physical or biological even social systems can be described at least at some level of abstraction as a graph. So that's why it is a so popular modeling tool in many fields. You can also obtain other structures such as grids as particular cases. I wouldn't call it fundamental, but it is a very convenient and very common, I would say ubiquitous model. Now, what I personally find disturbing and we can talk about it in more detail later on is that if you look at the different geometric structures for Gamble that we consider in the book, whether it's Euclidean spaces or many phones, they all have the discrete counterparts. So you have a plane and you can discretize it as a grid. You have a manifold, you can discretize it as a mesh. A graph is inherently discrete. And this is something that I find disturbing. There is, in fact, an entire field that is called network geometry that tries to look at graphs as continuous objects. So for example, certain types of graph that look like social networks, what is called scale free graphs, can be represented as nearest-neighbor graphs in some a little bit exotic space with hyperbolic geometry. So if we take this analogy, I think it is very powerful because now you can consider graphs as a discretization of something continuous and then think for example of graph neural networks as certain types of diffusion processes that are just discretized in a certain way. And by making potentially possible different discretizations, you will get maybe better performing architectures. One of the core dichotomies we talk about on Street Talk is the apparent dichotomy between discrete and continuous. And as Yannick was saying, there are folks out there who want our knowledge substrate to be discrete but still distributed, record them sub-symbolic folks. And this network geometry is fascinating as well because you're saying in some sense you can think of there being some unknown continuous geometry. So you're saying there is no dichotomy? This is probably a little bit of a wishful thinking as it happens with every model. So I would probably phrase it carefully for some kinds of graphs, you can make this continuous model for others, maybe not. Fascinating. Well, on to the subject of vector spaces versus discrete, you know, geometric deep learning is all about making any domain amenable to vector spaces, right? And indeed artificial neural networks. But could these geometric principles be applied to another form of machine learning, let's say discrete program synthesis? Certainly a very important question, Tim. And yeah, thanks for asking that. I think that there are many ways in which geometric deep learning is already at least implicitly powering discrete approaches such as program synthesis because there is a pretty big movement on these so-called dual approaches where you stick a geometric deep learning architecture within a discrete tool that searches for the best solution. So, for example, in combinatorial optimization, a very popular approach recently for a neural-resolving mixed integer programs is to like have your typical off-the-shelf mip solver that selects variables to optimize one at a time. And, you know, with these kinds of algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough about how, in what order you select these variables, you can actually solve the problem in linear time, which is something we would like to strive towards. And the exact way in which we select these variables is a bit of a black magic, like humans have come up with a few heuristics, but they don't always work. And whenever you have this kind of setting, as long as you're assuming that you're naturally occurring data isn't always throwing the worst possible cases or adversarial cases at you, you can usually rely on some kind of modeling technique, for example, a neural network to figure out which decisions the model should be taking. So, in this case, for example, for mip solving, DeepMind recently published a paper on this where you can treat mip problems as bipartite graphs, where you have variables on one side and the constraints on the other. And you link them together if a variable appears in a constraint. Then they run a graph neural network, which, as we just discussed, is one of the flagship models in geometric deep learning, over this bipartite graph to decide which variable the model should select next. And you can train this either as a separate kind of supervised technique to learn some kind of heuristic, or you can learn it as part of a more broader reinforcement learning framework, right, where the reward you get is how close you are to the solution or something like this. So this is one kind of clear way in which you can kind of have this synergy between geometric deep learning architectures and solutions for, for example, program synthesis. But I would just like to offer another angle in which you can think of program synthesis as nothing other than just one more way to do language modeling, right, because synthesizing a program is not that different to synthesizing a sentence, maybe with a more stringent check on syntax and so forth. But, you know, any technique that is applied to language modeling could, in principle, be applied for program synthesis. And something that we will be discussing, I believe, later during this conversation, one of the flagship models of geometric deep learning is indeed the transformer, which we show in our book, and elucidate why it can be seen as a very specific case of a graph neural network. And that's one of the flagship models of language modeling. So basically, that's also one more way to to unify. Like, you know, just because the end output is discrete doesn't mean that you cannot reason about it using representations that are internally vector vector based. Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information into a continuous representation. But the manifold needs to be smooth, it needs to be learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete program searches. But then you have this exponential blow up. But maybe that search space, because it's interpolative could be found using stochastic gradient descent, if you embed the discrete information into some kind of vector space. But Professor Bronstein, I wanted to throw it back over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because everything we're doing here is embedding discrete information into these Euclidean vector spaces. Why are we doing that? There are multiple reasons why vector spaces are so popular in representation learning. Vectors are probably the most convenient representation for both humans and computers. We can do for a number of operations with them, like addition or subtraction. We can represent them as arrays in the memory of the computer. They are also continuous objects, so it is very easy to use continuous optimization techniques in the vector spaces. It is difficult for Gamble to optimize a graph because it is discrete and requires combinatorial techniques. But in a vector representation, I just have a bunch of points that I can continuously move in a kind of dimensional space using standard gradient based techniques. Perhaps a more nuanced question is what kind of structures can be represented in a vector space? And a typical structure is some notion of similarity or distance. We want that the vector representations preserve the distances between, let's say, original data points. And here we usually assume that the vector space is equipped with the standard Euclidean metric or norm, and we have a problem from the domain of metric geometry of representing one metric space in another. And unfortunately, the general answer here is negative. You cannot exactly embed an arbitrary metric in Euclidean space, but there are, of course, some results such as bogains theorem that, for Gamble, provides bounds on the metric distortion in such cases. And in graph learning spaces with other more exotic geometries such as hyperbolic spaces, you have recently become popular with, for example, papers of Ben Chamberlain, my colleague from Twitter, or Max Nicol from Facebook. And you can see that in certain types of graphs, the number of neighbors grows exponentially with the radios. If you look, for example, at the number of friends of friends and so on in a social network, where we have this small world phenomenon, you can see that it becomes exponentially large with the growth of the radios. And now when you try to embed this graph in Euclidean space, it will become very crowded because in the Euclidean space, the volume of the metric ball grows polynomially with the radios. Think of the two-dimensional case that we all know from school, the area of a circle is pi radius squared, right? The volume of a ball is exponential with a dimension. So we inevitably need to increase the dimension of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very different because the volume grows exponentially with the radios. So it is way more convenient to use these spaces for graph embeddings. And in fact, recent papers show that to achieve the same error in embedding of a graph in the hyperbolic space with, let's say, 10 dimensions, you would require something like a 100-dimensional Euclidean space. Of course, I should say that metrics are just one example of a structure. So the general answer to the question whether a vector space is a good model for representing data is, as usual, it depends. You mentioned language, Peter, and maybe to both of you, do you think there is a geometry to language itself? I mean, obviously, we know about embedding spaces and close things somehow share meaning and so on. Do you think it goes beyond that? Because, like, do you think there is an inherent geometry to language itself and sort of the meaning of what we want to transmit and how that relates to each other? What you probably mentioned is the famous series of papers from Facebook where unsupervised language translation can be done by a geometric alignment of the latent spaces. In my opinion, it's not something that describes geometry of the language. It probably describes in a geometric way some semantics of the world. And even though we have, linguistically speaking, very different languages like, let's say, English and Chinese, yet they describe the same reality. They describe the same world where humans act. So it is probably reasonable to assume that the concepts that they describe are similar. And also, while there are some theories and linguistics about certain universal structures in languages that are shared, even though the specifics are different, I think it's interesting to look maybe at non-human communications. I wouldn't probably use the term language because it's a little bit loaded and probably some purists will be shocked by me saying that, for example, whales have a language, but we are studying the communication of slow whales. So this is a big international collaboration called Project. And I don't think that you can really model the concepts that whales need to describe and to deal with in the same way as we humans do. So maybe a silly example, we can say in human languages, and probably it applies to every language, we can express a concept that something got wet. I don't think that a whale would even understand what it means by being wet because the whale always lives in water. I would add to that maybe a slightly different view of geometry, but it's all about the question of how far are you willing to go and still call it geometry. Based on our proto book, at least, I tend to think of graph structures also as a form of geometry, even though it's a bit more abstract. And within language, people might not always agree what this structure is like. But I think we can be fairly certain that there are explicit links between individual words as and when you use them in different forms, syntax trees, or just one word precedes another and so on and so forth. And while we may not be necessarily able to easily say what is the geometric significance of one word, what we can look at is what is the local geometry of the words that you tend to use around it. And I mean, this kind of principle has been used all over the place. That has then been extended to graph structured observations, generally with models like deep walk and note to back basically the same idea, treat a nodes representation as everything that's around it. Basically, the reason why I think that analyzing this local topology of how words are used with each other is very powerful. I've reinforced that recently, precisely because of the fact I've been delving into category theory, because in category theory, your nodes are basically atoms, you're not allowed to look inside them, you assume they're this undivisible unit of information. And everything you can conclude about the atoms comes from the arrows between them. So using this very simple concept with a few additional constraints like compositionality, you can, for example, tell me what are all the elements of a set, even though you've abstracted that sets to a single point, just by analyzing the arrows between all sets, you can tell me what are all the elements inside a set. So thinking about this, I do believe that it is possible to reason about geometric, you know, word to vex, for example, does this with the assumption that the structure of the are we approaching this at the right level, though, because people have said for quite a long time that there's a difference between syntax and semantics. And you could look at the geometrical structure of spoken language. Or, for example, you could look at the topology of the connections in your brain, the topology of, you know, reference frames in your brain is how you actually have learned concepts. Would looking at the topology of spoken language tell you enough about abstract categories? That's a good question. I think that if that kind of information is necessary, like if the atoms by themselves won't tell you everything. One thing that we actually very commonly do in graph representation learning is assume this sort of hierarchical approach, where you have like the ground level with your actual individual notes, and then you come up with some kind of additional hierarchy that tells you either something about intermediate topologies in a graph, or intermediate structures that you care about in this graph, or any abstract concepts you might have extracted. And then there's additional links being drawn between these to kind of reinforce the knowledge that the graph net can capture. So I think if you have knowledge of some abstract concepts that are relevant for your particular task, you can attach them as additional pieces of information to this topology. Of course, the more exciting part is could we maybe discover them automatically? But that is something that I don't think is potentially in scope for this question. When human interpreters need to translate from one language to another, they often need to deal with different structures. I think Turkish is actually an extreme example where the order of words is completely reversed. It implies that you need probably to hold well in computer science terms some kind of a buffer in your brain before you can make the translation to another language. So it definitely imposes certain biological network structure in the brain. Another interesting observation that I read somewhere about the way that people remember certain facts when they speak a certain language. So the particular example that was given is a person can remember a perpetrator of a crime and then gives testimony in court. And the reason is that in some languages, it is more common to use impersonal pronouns and the personal phrases. So you can say, for example, the object was broken. And in some languages, you would say that somebody broke the object. So it appeared that languages were of these more impersonal constructions. People speaking these languages, I have hard time to remember the perpetrator. So the language probably imposes a lot about the way that we perceive world, but it is probably not studied sufficiently. But there may be some fuzzy graph isomorphisms, though, between the languages. I think there's something really magic about graphs. I think that's what we get into, because your lecture series inspired me, actually, Professor Bronstein, where you were talking about all the different applications of graphs. But something that a lot of our guests talk about are knowledge graphs. Expert systems and the knowledge acquisition bottleneck were the cause of the abject failure of good old fashioned AI or some symbolic AI systems in the 1980s. And many hybrid or neuro symbolic folks today are still arguing that we need to have a discrete knowledge graph, either human designed or learned or evolved or emerged or some combination of those things I just said, depending on who you talk to. Now, critically, many go fi people think that most knowledge we have is acquired through reasoning, not learning, right, which is really, really interesting. So by reasoning, I mean extrapolating new knowledge from existing knowledge. It feels like graph neural networks could at least be part of the solution here. And in your lecture series, you mentioned the work by Kramner, which was explainable GNN, where they use some kind of symbolic regression to get a symbolic model from a graph neural network. So do you think there's some really cool work we can do here? There is a little bit of divide in graph learning literature. So people working on graph neural networks, and working on knowledge graphs, even though, at least in principle, the methods are similar. For example, you typically do some form of embedding of the nodes of the graph. Somehow these are distinct communities, probably, historically, they evolved in different fields. Yeah, so the paper of Krammer, this is really interesting because they use graphs to model physical systems, for example, and body problem when they have particles that interact. You can describe these interactions as a graph, and you can use standard generic message passing functions to model the interactions. Now, the step forward that they do is they replace these generic message passing functions with symbolic equations. And not only that this allows to generalize better, but you also have an interpretable system, you can recover from your data the laws of motion, right? And if you think of how much time it took, historically, to people like Johannes Kepler, for example, he spent his entire life on analyzing astronomical observations to derive a law that now bears his name, that describes the elliptic orbits of planets. Nowadays, with these methods, you can probably do it in a matter of seconds or maybe minutes. I think the point that particularly caught my attention in what you asked, Tim, was this interplay between graphs and reasoning and extrapolation and how that supports knowledge. Now, when it comes to how critical is this going to be, it depends on the environment in which you put your agent. Like, is it a closed environment, or is it an open ended environment where new information and new knowledge can come in in principle at any time? This basically do want to build a neural scientist, or do you just want to build a neural exploiter that takes all the information available right now and then draws conclusions based on that. So if the system is closed worlds, you'll probably be able to get away without very explicit reasoning, especially if you have tons of data, because we've seen time and time again that large scale models can kind of pick up on these regularities if they've seen it often enough. But if I give you a solution that involves stacking, for example, n objects, and now I ask you to do the same kind of reasoning with two times n objects, the way in which we optimize neural networks at least today is typically going to completely fall on its back when you do something like this. So if you truly want to take whatever regularities you have come across in the world of the training data and hope to at least reasonably gracefully apply them to new kinds of rules that come in the future, then you probably want your model to extrapolate to a certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a very natural area to study under this lens, because they trivially extrapolate, you write an algorithm that does a particular thing on a set of n nodes, you can be you can usually mathematically prove that it's going to do the same thing equally properly, maybe a bit more slowly, if you give it two times n nodes, right? This kind of guarantee typically doesn't come that easily with neural networks. And we found that you have to very carefully massage the way you train them, the kinds of data you feed to them, the kinds of inductive biases you feed into them, in order to get them to do something like this. So if extrapolation is something you truly need, and you know, I think for artificial general intelligence, we're going to want to have at least some degree of extrapolation as new information will become available to our neural scientists, just as you follow the era of time. Basically, for doing something like this, graph neural networks have arisen as a very attractive primitive, because there's been a few really exciting theoretical results coming out in recent years, saying that the operations of a graph neural network align really, really well with dynamic programming algorithms. And dynamic programming is a very standard computational primitive, using which you can express most polynomial time heuristics. So essentially, that's a really good, you know, that's a really good piece of mind result. The unfortunate side of it is that it's a best case result, right? So you can set the weights of a neural network of a graph neural network to mimic a dynamic programming algorithm, more efficiently or with smaller sample complexity. But, you know, there's still a big problem of how do I learn it in a way that it still works when I double the size of my input. And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make that happen. It's not easy. If you throw the vanilla graph neural network and just input output pairs of an algorithm, it will learn to fit them in distribution the moment you give, like, ask it to sort and array that's twice as big, it's going to completely collapse. So this is the number one thing that the neurosymbolic people say. They say neural networks, they don't extrapolate. They only interpolate, you know, it just, it's a continuous geometric model, learns point by point, transforms the data onto some continuous, smooth, learnable manifold, you interpolate between the data points, you want to have a smooth, you want to have a dense sampling of your data. But you're talking about dynamic programming problems, these are discrete problems that the structure is discontinuous. But how could you possibly learn that within your network? Well, the dynamic programming algorithm could be, could have a discontinuous component for example, if you're searching for shortest paths at some point, you will take an argmax over all of your neighbor's computed distances and use that to decide what the path is. But before you come to the argmax part, there is usually some fairly smooth function being computed actually. So in the case of shortest path computations, you know, Bellman Ford or something like this, you say something very simple, like, I have a value d of s in every single one of my nodes, which is initially infinity everywhere and zero in the source vertex. And then at every point, I say, the distance of my particular node is the minimum of all the distances of my neighbors plus the edge weight, right. And this kind of function is generally more graceful than than taking an argmax. And you can also think of, for example, if you have to compute expected values or something like this, using dynamic programming, that's also one example where actually summing is what you need to do across all of your neighbors or something like this. So yeah, it is true that like, across individual steps, you may be doing like discrete optimization steps. But usually, it's propelled by some kind of continuous computation under the hood. So that's the part that the graph neural network actually simulates. And then the part which does the argmax would be some kind of classifier that you stitch on top of that. So in principle, it's not, yeah, it's not too challenging to massage it into a neural network framework. So one of the, I think one of you mentioned this before, brought up transformers. And, you know, in recent years, we've had, I think about 10 different papers saying transformers are something there is transformers are RNNs, transformers are Hopfield networks. And also transformers are graph neural networks or compute some kind of graph neural networks. Can you maybe speak a bit to that? Are transformers specifically graph neural networks? Or are they just so general that you can also formulate a graph problem in terms of a transformer? Okay, that's a very good question. I would start off by saying, like, I don't want to start this discussion just by saying, yes, transformers are graph neural networks. This is why end of story, because I feel like, you know, that doesn't touch upon the whole picture. So let's let's look at this from a natural language processing angle, which is how most people have come to know about transformers. So imagine that you have a task which is specified on a sentence. And you want to exploit the fact that words in the sentence interact, right? It's not just a bag of words. There is there's some interesting structure inside this bunch of words that you might want to exploit. When we were using recurrent neural networks, we assume that the structure between the words was a line graph. So basically, every word is preceding another word and so on and so forth. And you kind of just linearly process them with a model like LSTM or something. But, you know, basically line graphs, as we know, are not the way language is actually structured. There can be super long range interactions inside language. So subjects and objects in the same sentence could appear miles away from each other. So using the line graph is not the most optimal way of getting that information in the fastest possible in the fastest possible way. So, okay, there's clearly some kind of non trivial graph structure. What is it? Well, it turns out that people cannot really agree what this optimal graph structure is, and it may well be task dependent, actually. So just consider syntax trees, for example, like there's not always a unique way of decomposing a sentence into a syntax tree. And the exact kind of tree you might wish to use to represent a sentence may be different depending on what is the actual thing that you're solving. So, okay, we have a situation where we know that there's some connectivity between the words, but we don't know what that connectivity is. So in graph representation learning, what we typically do when we don't know the graph, as long as the number of objects is not huge, is to assume a complete graph and let the graph neural network figure out by itself what the important connections are. And if I now stitch an attentional message passing mechanism onto this graph neural network, I have effectively rederived the transformer model equation without ever like using this specific transformer lingo. So from this kind of angle, the fact that it's a model that operates over a complete graph individual words, in a way that you know, once you've put all the embeddings to them is permutation equivalent, this describes the central equations of self attention that the transformer uses. The part which I think causes a bit of a divide here is the fact that transformers like the model that was originally presented are not just the equations of a transformer, they're also the positional embeddings of a transformer. And that's the part that sort of gives it a bit more of a central structure. Well, actually, if you look at these sine and cosine waves that get attached to the individual words in an input to a transformer, you will see that you can you can actually derive a pretty good connection between them and the discrete Fourier transform, which actually turned out to be the eigenvectors of a graph Laplacian for a line graph. So essentially these positional embeddings are hinting to the model that you are the decent that these words in a sentence are arranged in a particular way, and you can use that information. But because it's fed in as features, the model doesn't have to use any of that information, like sometimes bag of words is the right thing to do, for example, right. So essentially, the transformer has a bit of a light hint that there's a central structure in there in the form of a line graph. But, you know, the model itself is a permutation equivalent model over a complete graph. And from our lens of the geometric deep learning, it is effectively a special case of an attentional GNN. Now, I think this positional embedding aspect is a super important one. And it could hint to how we might extend these transformers from sentences to more general structures. And I think, Michael, you might have a lot more thoughts on that than I do. So maybe you can say a bit about that. Yeah, so positional encoding has been done for graphs as well. As Petter mentioned, in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates that are used in transformers using the eigenvectors of the Laplacian. There are other techniques you can actually show that you can make it a message passing type neural network strictly more powerful than traditional message passing. The equivalent vise for 11 graphics or morphism test by using a special kind of structure where positional encoding, for example, if you can count substructures of the graph, such as cycles or rings and so on. And this way, you have a message passing algorithm that is specialized to the particular position in the graph and can, for example, detect structures that the traditional message passing cannot detect. So it is at least not less powerful than the vise for 11 algorithm. And we can actually show examples on which vise for 11 algorithm or traditional message passing fails, whereas this kind of approach succeeds. So the thing I've always wondered about transformers networks are the position tokens. I really don't like them and I want them to go away because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher order structure in the language. And it kind of felt like we were using the position tokens to cheat a little bit. So what I'm trying to get across here is that I think the position of a token in an utterance should be invariant. I mean, clearly, in different languages, the tokens are in different places. In Turkish, the order is completely reversed. And I would like to think that our internal language representation ignores the transmission arrangement given the particular language and the constraint that we only communicate sequential streams of words. However, I do appreciate what Michael is saying above that the position encodings can actually encode more powerful structures like cycles and rings. The key question is, do we actually need to have these structures in natural language? I don't agree that you want to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the really key characteristics of graphs and sets more generally that you don't have the ordering of the nodes. The situations and the problems where transformers are applied, you actually do have an order. But you use the graph as Petra described to model different long distance relations between different tokens or words in a sentence. So you want to incorporate this prior knowledge that these nodes are not in arbitrary order, that they have some sentence order. And this principle applied more generally, you can use positional encoding to tell message passing not to apply exactly the same function everywhere on the graph, but to make it specialized for different portions of the graphs or at least make it a possibility. And then the training will decide whether to use this information or not or in which way. It's strange because I don't know whether the order is just a function of the communication medium. So we transmit the tokens in a sequence. And could we then represent them in our brains in a completely different domain where the sequence is no longer relevant? Well, actually a lot of neuroscientists think that our brain is a prediction machine and it's a sequence prediction machine. So the sequence is kind of fundamentally important. Yeah, it's also a function of the specific language that you use. And as we discussed before, there are languages which convey the same meaning with a difference in structure. I want to get a little bit into what you said about essentially what we're doing with these positional encodings is we hint. We hint to the model that there is something here, which is a big break from sort of the old approach, let's say, of an LSTM to say, this is the structure. So with the world of geometric deep learning, I often have the feeling people talk about, they talk about symmetries and we need to exploit these symmetries that are present in the world. And there's almost to me two different groups of these symmetries. So one group is maybe you would call them like exact symmetries or something like this. When I think about Alpha fold, and I think about like a protein, it doesn't, I don't care which side is up, right? Like the protein is the same, the same protein, and there's no reason to prefer any direction over any other direction. However, if I think of like, because people have made this argument for CNNs, for example, they say, well, a CNN is a good architecture because it's translation invariant, right? And essentially, if we want to do object recognition or image classification, translation invariance is like a given, but but it's not, right? It's not a given the pictures that we feed to these algorithms, most often the object is in the middle, most often, you know, it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera like this, but I don't, right? So it, it seems to be, it seems to be in many cases better to not put the symmetries in there until we're like, really, really, really, really sure that these are actual symmetries, because with more data, it seems the model that does not have the prior inside becomes better than the model that does have the prior inside, if that prior doesn't exactly match the world is, do you think that's a fair characterization of, for example, why transformers with large data all of a sudden beat classic CNN models or come close to them? I think it's, it's always this question of the trade off between how much your, your model and how much you learn and I remember when I was a student, there was this maxim that that machine learning is always the second best solution. And maybe nowadays with deep learning showing some remarkable set of successes, I'm probably less confident in this statement, but it's probably still quite true that the more you know about your problem, the better chances that machine learning will work for it. To me, it makes sense to model as much as possible and learn what is current or impossible to model. And in practice, of course, there is a spectrum of possibilities of how much of these prior assumptions are hardwired into the architecture. And usually, it's a trade off between the, for example, computational complexity availability of the data also hardware friendliness. And if you think of what happened in computer vision, it's probably a good illustration that that convolutional networks have translational environments, for example, as you mentioned, but in many problems, you might benefit from other symmetries such as rotations, again, depends on the application, but imagine that you want to recognize, I don't know, traffic signs, when you can also tilt your car. And you may ask why in these applications as well CNNs are still so popular. And probably one of the reasons is that they met very well to the single instruction multiple data type of hardware architectures that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry with data augmentation, and more complex architectures and larger training sets. And this is exactly what happened a decade ago, this convergence of three trends, the availability of compute power, right, the GPUs, algorithms that map well to these computational architectures, and these happen to be convolutional networks, and also very large data sets that you can train these architectures on, such as ImageNet. So many of the choices that become popular in the literature are maybe not necessarily theoretically the best ones. So I think in hardware design, there is this phenomenon that is called the hardware lottery, when it's not necessarily the best algorithm and the best hardware that solve the problem, it's just some lucky coincidence, and they are happy marriage that makes them successful. We keep raising this point about how transformers can be seen as special cases of attentional GNNs, but the status quo is that people will use transformers for very many tasks nowadays, and there may well be a good argument for considering them in this completely separate light, and one possible explanation or justification for this is the hardware lottery, because, yes, sure, transformers perform permutation equivalent operations over a complete graph, but they do so in a way that is very, very highly amenable to the kind of matrix multiplication routines that we can support very efficiently on GPUs nowadays. So basically, they can be seen as the graph neural network that has won the current hardware lottery, even in cases where maybe it will make more sense to consider a more constrained graph, especially if you have low data environments or something like this, the potential overheads of running a full graph neural network solution with message passing, which, given its extremely sparse nature, doesn't align that well with GPUs and TPUs nowadays. Sometimes just using a complete graph neural network is the more economical option when you take all factors into account. And that's, and also the fact that they use an attention mechanism, which is kind of a middle ground between a simple diffusion process on a graph, which we just kind of average things together based on the topology, and the full on message passing where you actually compute a full on vector message to be sent across the edges. Like it strikes a nice balance of scalability and still being able to represent a lot of functions of interest, especially when your inputs are just word tokens, right? So like, you know, in a way, it's a GNN that strikes a very nice sweet spot. And that's probably the reason why it's become so popular in current times. Now, of course, there is a chance that hardware, and there's actually a pretty high probability that hardware will catch up to the trends in graph representation learning, and we will start to see a bit more graph oriented hardware. But at least for the time being, yeah, there's a bit of a combination of what's theoretically making the most sense for your problem, and what the hardware that you have right now will support the most easily. Yeah. There is a British startup, I think they have reached recently a unicorn status called Graphcore. And you can already hear from the name of the company, that there is a graph inside, they try to develop hardware that goes beyond the traditional paradigm. Yeah, I'm really interested in the hardware lottery. We had Sarah Hooker on that massive shout out to Sarah. And Janik made a video on the hardware lottery paper as well. I mean, I'd push back a little bit. I think there's also a bit of an optimization and an algorithmic lottery going on. I think there's something very, very interesting about stochastic gradient descent and the kind of data that we're working with. But this actually gets to my next question, which is about why exactly geometry is a good prior and how principled it is. So it seems like these geometric priors are principled. And they have utility because they are low level primitives, right? They're ubiquitous and natural data. But why exactly is it a principled approach to start with things we know, which is to say geometric primitives, and to work upwards from there? You know, what would it look like if we went top down instead? And what makes a good prior? I mean, one way to think about it is the actual function space that you're searching through, you know, this hypothesis space, it's not just about being able to find the function easily in that space, or the simplicity of the function you find. Chalet would say it's the information conversion ratio of that function. So, you know, can you use this function that you found to convert a very small piece of information and experience space into new knowledge or a new ability? But how do you find these functions? One of the points that we try to make in the book is the separation between the domain and the group that you assume on the domain, the symmetry group. So you might have the same domain, like two dimensional grid or two dimensional plane. And for example, the translation group or the group of rotations and translations or the group of rigid motions that also include reflections. So these are completely separate notions. And which one to choose depends on the problem. The choice of the domain really comes from the structure of your data. So if your data comes as an image, then of course, you use a grid to represent it as the choice of the domain. Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road, usually the signs will have certain orientation. It's very unlikely that you will see it upside down. So really the only invariance or the only kind of symmetry you have is translation. So CNNs in this case would work perfectly well. So for example, we have histopathological samples. So you have a slice of tissue that you need to put under the microscope. So you can naturally flip the glass. You don't know how it is oriented. So reflections are also initial transformation. So in the traffic signs, of course, this is not physical unless you see your sign in the back mirror. But in this histopathology example, it is an initial transformation. So the choice of the symmetry group and what makes a good geometric prior is really dictated by the specific problem. It's often very hard, though, to actually choose because we often don't really know, right, coming back to what I said before, if we actually hit the group correctly. And in fact, we've sort of seen the more successful approach. And this might be hardware specific, but it seems the more successful approach is often to actually make data augmentations with respect to what we assume are symmetries. So to know, I think of all the color distortions that we do to images, we rotate them a bit, we rescale them and so on, it will be definitely possible to build architectures that are just invariant to those things. However, it seems to be a more successful approach in practice to put this all into the data augmentations. What's your take on that? How do we choose between putting prior knowledge into augmentations versus putting prior knowledge into the architecture? It's not a binary choice. It's not either your model or your augment with data. One of the key principles that we also emphasize in the book is that, of course, this perfect invariance or equivariance is a wishful thinking. In many cases, you want to get the property that we call geometric stability. You have some transformation that is approximately a group. Or imagine that you have a video where two objects are moving, let's say one car moves left and another car moves right. So there is no global translation that describes the relation between the two frames in this video. The geometric stability principle tells you that if you are close enough to an element of the group, if you can describe these transformations as an approximate translation, then you will be approximately invariant or approximately equivalent. This is actually what happens in CNN. This was shown by Joan. They use this motivation to explain why convolutional neural networks are so powerful. Roughly speaking, if I don't have a translation, but for example, if I have an MNIS digit and you have different styles of the digits, you can think of them as warping of some canonical digits. So in this case, even though it's not described as a translation and the neural network will not be invariant or equivariant to this kind of transformation, it will be stable under these transformations. And that's why data augmentation works in some sense that you're extending your invariance or equivariance class to approximate invariance and equivariance. Taco also had a few interesting things to say about data augmentations versus building the inductive priors into the model. This is Taco. Is your preference towards... I assume it is towards creating inductive priors in the architecture around geometry instead of data augmentation? Oh, that's a good question. I think it depends on the on the problem. Like in some cases, you don't have a choice. So graphs are a great example. The group of permutations is n factorial elements. If n is large, you have 1000 nodes, you're never ever going to be able to exhaustively sample that group. And so it's better to just build it in. And that's also why no graph neural net doesn't respect the symmetry. Nobody's suggesting you should do that by data augmentation. In some other cases, it is somewhat possible to sample a reasonably dense grid of transformations in your group. And indeed, augmentation is turning out to be very important in unsupervised learning and self-supervised learning techniques. So I am actually... I look very positively towards that. I don't think it's wrong to put in this knowledge using data augmentation. But in some cases, like let's say when you're on classifying medical data like cells in a dish, a histopathology image or something, you just know for sure there's translation and rotation symmetries. The cells don't have a natural orientation. And in those cases, I do think it makes sense to build it into the architecture for the simple reason that if you build it into the architecture, you're guaranteed that the network will be equivariant, not just at the training data, but also at the test data. So you never have that the network would make the correct classification for your test data when you have it in one orientation, but when you rotate it, it suddenly does something different, which can happen if even when you present your training images in all possible orientations. So I think for that reason, equivariance does tend to work better in those cases where there's an exact symmetry in the data. We have actually demonstrated that empirically, where for example, in a medical imaging problem of detecting lung nodules in three dimensional CT scans, we started off with a convolutional network with a data augmentation pipeline, which completely tuned what state-of-the-art method at the time. And we simply replaced all the convolutions by group convolutions that respect to rotational symmetries as well as the translations. And we get very significant improvement in performance. So that goes to show that in practice, often data augmentation can't get you all the way. So for the cases where there's an exact symmetry, or where the group of symmetries is very large, I think building it into the network is the way to go for the foreseeable future. But there are many cases where augmentation is also well, where augmentation is the way to go. Fascinating. I'm really interested in this notion that could these symmetries actually be harmful? I know Joanne, for example, spoke about the three sources of error in machine learning models. And one of them is the approximation error. And normally when we get signals, they come to us in a contrived form, don't know, they might be projected onto a planar manifold. And you know, the sky is always up, for example. Does it really help us having these geometrics symmetries as primitives in the model? Yeah, that's a good question. The simple answer is, if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data, building in a covariance is going to be harmful. And it's better to learn the true structure of the data, this approximate symmetry, which you should be able to pick up from the data alone. So that's one thing that still means in a low data regime, it can be very useful, even when the symmetry is approximate. But I would also say that sometimes, in machine learning, we have a tendency to put too much faith into the evaluation metrics and data sets. So we say, we want to solve computer vision. And what we mean is we want to get a high score on ImageNet. And certainly it's true in ImageNet, the images tend to appear in upright position, and they are photographed by humans. So the key objects are in the center most of the time, etc. So these are biases that you could exploit, and you might stop yourself from exploiting them if you build in the symmetry. But that's only a problem if you put on your blinders and you say ImageNet accuracy is the only thing that counts. You might very well think, if you want to build a very general vision engine, it is useful that it still works if suddenly the robot falls over and has to look at the world upside down. So the symmetry can still be there in principle, even if it's not there in practice in your data set. And then there's maybe a robustness versus computational efficiency tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still want it to work. So you want that rotation equivariance. But then again, if we don't have to process the images upside down, in the 99% of cases where the images are upright, we gain some computational efficiency. So there's a tradeoff there. And yeah, I don't think there's a right or wrong answer. It's something you have to look at on a case by case basis. When I put this to Professor Bronstein as well, he also said that you folks were looking at trying to remember how he described it. I think he said there was a kind of representational stability which allowed for approximate symmetries. So it's not necessarily that you're going for these precise symmetries, you're actually looking for a little sort of margin of robustness going to moving into approximate symmetries that you might not have explicitly captured. I agree. I think that's also a very important philosophy and approach. To take the group, think of it as somehow embedded in a larger group, like most of the geometrical symmetries that we think about are somehow a subgroup of diffeomorphisms. And then if you say, I don't want invariance or equivariance, but some kind of stability or smoothness to elements in the group plus small diffeomorphisms, for instance, you might get some of the generalization benefit without unduly limiting the capacity of your model. Is there a hope though that we can get this approximate? Because I see your point, I think, is that, or one of the points is, I think, is that if we program like a symmetry into the architecture, it will be rather fixed, right? We make an architecture translation invariant, it's going to be fully translation invariant. Are there good ways to bring approximate invariances into the space of the architectures that we work with? Yeah, I mean, I have a very quick answer, maybe not too satisfying, but one very simple one, if you think of neural network blocks as like components that implement different symmetries, and then you think of like a calculus of these blocks as you know, building your deep learning architecture. One very simple representational tool that we can use to allow the model to use the symmetry, but also not use the symmetry is the skip connection. So essentially, you could have a model that processes, for example, your graph data using a particular connectivity structure that you want to be invariant to. And you can also use say a transformer that processes things in a completely permutation invariant way over the complete graph. And you can just shortcut the results of one model over the other model, if you want to give also the model the choice to ignore the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one simple way in which we could do something like this. And in some of our more recent algorithmic reasoning blueprint papers, we do exactly this, because one very important thing that we're trying to solve is apply classical algorithms to problems that would need them. But the data is super rich, and it's really hard to, you know, massage it into the abstractified form that the algorithm needs. For example, you want to find shortest paths in a road network, a real world road network, you cannot just take all the complexity of changing weather conditions, changing diffusion patterns on the on the roads and the roadblocks and all these kinds of things and turn that into this abstractified graph with exactly one scalar per each edge. So you can apply dykstra or something like this, like, it's just not feasible without losing a ton of information. So what we're doing here is we make this high dimensional neural network component that simulates the effects of dykstra. But we're also mindful of the fact that to compute say the expected travel time, there's more factors at play than just the output of a shortest path algorithm, right? There could well also be some flow related elements, maybe just some elements related to the current time of day, human psychology, whatnot, right? So we start off by assuming the algorithm does not give the complete picture in this high dimensional noisy world. So we always, as default, as part of our architecture, incorporate a skip connection from just, you know, a raw neural network encoder over the algorithm. So in case there's any model free information that you want to extract without looking at what the algorithm tells you, you can do that. So maybe I don't know, Yannick, if that answers your question about approximate symmetries, but that's, that's the kind of divide by God's when I heard the question. I mean, that's a very, that's a very practical answer for sure that that, you know, you can actually get out there. It even opens the possibility to having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe another thing we've talked about, you know, there are symmetries, you want to incorporate them into your problem and so on. And we've also talked about the symbolic regression beforehand to maybe parse out symmetries of the underlying problem. What are the current best approaches if we don't know the symmetries? So we have a bunch of data, we suspect there must be some kind of symmetries at play because they're usually are in the world, right? And they, if we knew them, we could describe our problems in very compact forms and solve them very efficiently, but we don't often know. So what are the current state of the art? When I don't know the symmetries, how do I discover what group structure is at play in a particular problem? I don't think that there is a single approach that solves this problem in a satisfactory manner. And one of the reasons why because the problem is ambiguous. So maybe an example, think of objects mostly translate horizontally, but you also have a little bit of vertical translation. What is the right symmetry structure to model? Is it a one dimensional translation group or a two dimensional translation group? Do we want to absorb the vertical, the slight vertical motions as the noise and deal with it as a data augmentation, or you want to describe it in the structure of the group that you discover? So there is no single answer. So you cannot say that one is correct and another one is wrong. Yeah, I think this was kind of where I was going with the question of how principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that geometries are hierarchical. You were saying that, for example, the projective geometry is kind of subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers, coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the simple rule which produced this pattern. Now what kind of regularities would you look for in the model that you built? I mean, it seems obvious that there would be an expanding scale symmetry, which might resemble the original rule. But it feels like there'd be plenty of other emergent, abstract symmetries which are not obviously related to the simple rule which produced the pattern. I mean, Janik was just saying, when you look at computer vision, you see a kind of regularity or invariance to color shifts, for example. So our fractals are good analogy for physical reality. And should we be looking for the low-level primitive regularities which I think you're advocating for? Or should we be looking at more abstract emergent symmetries which appear? In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claimed really unbelievable compression ratios for natural images. And the way it worked was to try to reassemble the image from parts of itself. And possibly, of course, you can take parts and subject them to some geometric transformation. So roughly, if you have a page of pixels, you can approximate it as another page taken from somewhere else in the image that you translate, rotate, and scale. And then the image was represented as an operator that makes such a decomposition. And this operator was constructed in a special way to be contractive. And then they used the Banach-Fix point theorem that you can apply this operator to any image. So you can start with the noise for example, completely random image. And you have the target image emerge after a few iterations. So that this iterative scheme will converge to the fixed point of the operator, which is the image itself. And it was actually used in the industry, well, Microsoft and CARTA encyclopedia. I don't know how many viewers are old enough to remember it. But the main issue was the difficulty to build such operations. The compression was very asymmetric. It was very easy to decode. You just take any image and apply this operator multiple times. But it was really very hard to encode. And in fact, some of these constructions that showed remarkable compression ratios were constructed semi by hand. I should say that in more recent times in computer vision, for example, the group of Michali Rani from the Weizmann Institute in Israel used similar ideas for super resolution and image denoising where you can build the clean or higher resolution image from bits and pieces of the image itself. So it's a single image denoising or super resolution. But what you do is you try to use similarities across different positions and scales. That's absolutely fascinating. I mean, I spend a lot of time thinking about this because my intuition is that deep learning works quite well because of the strict structural limitations of the data which is produced by our physical world, right? And would you say that physical reality is highly dimensional or not? If it's highly dimensional, is it because it emerged from a simple set of rules or relations like we were just talking about? Because I think what you're arguing for is that it could be collapsible in some sense. Probably the term dimension is a bit frivolously used here. But I would say that it's probably fair to say that at some scale, many physical systems can be described with a small number of degrees of freedom, parametres that capture the system. And as we are talking, I'm sitting in a room, I'm surrounded by probably a quadrillion of gas molecules in the air that fly through the room and collide with each other and the walls of the room. So at the microscopic level, the dimension is very high. So it's absolutely intractable if I were to model each molecule and how it collides, I will have a huge number of degrees of freedom. And yet if we zoom out, we can model the system statistically, and that's exactly the main idea of thermodynamics and statistical mechanics. And this macroscopic system is surprisingly simple. It can be described by just a few parameters such as temperature. And the example of fractals that you brought up before essentially show that you can create very complex patterns with very simple rules that apply locally in a repeated way. This might be a question for you, Peter. The geometric blueprint works brilliantly in the ideal world where we can compute all of the possible group actions. But graph neural networks, for example, you know, the permutation group is factorial in size, which means we need to rely on heuristics like graph convolutions. So how much better would graph neural networks be if we could compute all of the permutations? I mean, are you happy with these heuristics in general? So that is a very good question. And yes, so let's just start from stating the obvious. If you want to explicitly express every possible operation that properly commutes with the graph structure, and in that sense is a graph convolution, you would not be able to represent that properly as a neural network operation because you have to store in principle a vector for every single element of the permutation group. So unless your graph is super tiny, that is just not going to work. So on one hand, this is a potentially annoying result. On another hand, it is also exciting because we know that even though we ended up like doing most of our graph neural network research in this very restricted regime of I'm going to define a permutation invariant function over my immediate neighbors, and that will as a result translate into a permutation equivalent function over the whole graph. Even though most of our research has happened in that particular area, we know from this result that there actually exists a huge wealth of very interesting architectures beyond that. And I think one of the potentially like earliest examples that have demonstrated that there exists this wealth of space is actually one of Jean Bruno's earlier papers on the graph Fourier transform that, you know, analyzing from a pure signal processing angle, they have shown that you can represent basically every proper graph convolution as just, you know, parameterizing its eigenvalues with respect to the eigenvectors of the graph Laplacian. So, but the big issue that kind of limits us from going further in this direction right now is the issue that Michael highlighted of geometric stability. So basically, a lot of these additional graph neural networks that do something more interesting than one hop spatial message passing pay the price in being very geometrically unstable. So the graph Fourier transform in its most generic form will have every single node in the graph be updated based on whatever is located in any other node in the graph, very conditional on the graph topology. So if you imagine any kind of approximate symmetry, any kind of perturbation either in the node features or the edge structure of the graph, this, you basically don't have any protection against that like that error is going to immediately propagate everywhere. And as a result, you'll end up with a layer that theoretically works really well. But in practice is very unstable to these kinds of numerical or inaccuracy issues. One thing that's also very important to note is that often in graph neural networks, we have this subtle assumption of we have the graph and we're using this graph that's given to us. But who guarantees that the graph that's given to you is the correct one actually very often, we estimate these graphs based on very, very weird heuristics ourselves. So basically, all of these kinds of perfection assumptions are what might limit the applicability of these kinds of layers. But that being said, I find it comforting that these layers exist, which means that there are meaningful ways to push our research forward to potentially discover new, you know, wonderful basins of geometric stability inside these different, you know, layers that may not just do one hop message passing. So that's my take on this, like it gives me, it gives me faith that there's more interesting things to be discovered. That being said, it is pretty tricky to find stable layers in that vast landscape. Michael, do you have some thoughts on this as well? I know you've worked quite a bit on these geometric stability aspects. I just wanted to add one thought about it that essentially, locality is a feature, not a bug in many situations. And in convolutional neural networks, actually, if you look again, historically, the early architectures like AlexNet, they started with very large filters and the few layers or relatively few layers, I think something like five or six. And nowadays, what you see is very small filters and hundreds of layers. One of the reasons why you can do it is because of compositionality properties. So you can, you can create complex features from, from simple primitives. So in some other cases, like, like manifolds, there are deeper geometric considerations why you must be local, so related to what is called the injectivity radius of the manifold. On graphs, well, maybe we like a little bit the theoretical necessity to be local, besides, of course, the computational complexity. But in many cases, it is actually a good property because many problems do not really depend on distant interactions. So if you think of social networks, probably most of the information comes from your immediate neighbors. Is there some sort of, let's assume I, you know, I have a graph, and I have my, my symmetries, my groups that I suspect there are in the problem, or I want to be invariant to, is there like, can you give us a bit of a practical blueprint of how would I build a network that, you know, takes this as an input and applies this? How would you go about this, you know, what would be the building blocks that you choose, the orders and so on? Is there overarching principles in, in how to do? I don't think that there is really a general recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic structure that they have in graph is a privatization invariance. This has to do with the structure of the graph itself. It says nothing about the structure of the features. You might also have some secondary symmetry structure in the feature space. In case of molecules, for example, you might have a combination of features. Some of them are geometric. So it's actually not an abstract topological graph. It's a geometric graph. A molecule is a graph that lives in three dimensional space. And so the features are the positional coordinates of the nodes, as well as some chemical properties such as atomic numbers. Now, when you deal with the molecule, you usually don't care about how it is positioned in space. It wants to be equivariant to rigid transformations. And therefore you need to treat accordingly the geometric coordinates of the nodes of this graph. And this is actually what has been successfully done. So when you do, for example, virtual drag screening, architectures that do message passing, but in a way that is equivariant to these rigid transformations actually are more successful than generic graph neural networks. Also, this principle was exploited in the recent version of AlphaFold, where I think they call it point invariant attention, which is essentially a form of a latent graph neural network or a transformer architecture with equivariant message passing. Yeah, I'd just like to add one more point to this conversation, which is maybe a bit more philosophical, but it relates to this aspect of building the overarching symmetry discovering procedures, which I think would be a really fantastic thing to have in general. And I hope that some component of a true AGI is going to be figuring out, making sense of the data you're receiving and figuring out what's the right symmetry to bake into it. I don't necessarily have a good opinion on what this model might look like. But what I do say is just looking at the immediate utility of the geometric deep learning blueprint, we are like, I think very strictly saying that we don't want to use this blueprint to propose, you know, the one true architecture. Rather, we make the argument that different problems require different specifications, and we provide a common language that will allow, say, someone who works on primarily grid data to speak with someone who works on manifold data without necessarily thinking that, you know, you know, somebody might say, commonets are the ultimate architecture. Someone else might say GNNs are the ultimate architecture. And in some ways, they could both be right and they could both be wrong. But this blueprint kind of just provides a clear delimiting aspect to these things, just like in the 1800s, you had all these different types of geometries that basically lived on completely different kinds of geometric objects, right? So hyperbolic, elliptic, and so on and so forth. And what Klein-Zerlangen program allowed us to do was, among other things, reason about all of these geometries using the same language of group invariance and symmetries, right? But in principle, the specifics of whether you want to use a hyperbolic geometry or whether you want to use an elliptic geometry, partly rests on your assumption what the main do you actually live in, right? When you're doing these computations. So I think just generally speaking, I think that having a divide is a potentially useful thing, as long as you have a language that you can use to index into this divide, if that makes sense. It does make sense. But I have a feeling that some people could benefit from geometric deep learning in their runaways. I mean, I don't want you guys to motivate geometric deep learning in general, because I think, you know, a lot of deep learning with a structured prior is already geometric deep learning is as you folks demonstrated in your blueprint, you know, like RNNs and CNNs, for example. So, you know, like it or not, we're already all using geometric deep learning. But some of the esoteric flavors of geometric deep learning, particularly on irregular meshes, they seem a little bit out there, don't they? I mean, it's possible that many people could benefit from this, but they just don't know about it yet. I was thinking that, for example, if I had a LiDAR scanner on my phone, and the result is a point cloud, which is not particularly useful. But I would presumably transform it into a mesh, which would be more useful. But is it possible that loads of data scientists out there are sitting on data sets that they could be thinking about geometrically, but they're not? Many folks are exotic. It's probably in the eyes of the beholder. And well, in machine learning, probably they are, to some extent, exotic. But joking apart, many folks are a convenient model for all sorts of data. And the data might be a high dimensional, but still have a low intrinsic dimensionality or can be explained by a small number of parameters or degrees of freedom. And this is really the premise of nonlinear dimensionality reduction. And for example, the reasons why data visualization techniques such as TSE and E at all work. And maybe the key question, as you're asking is, how much of the manifold structure of the continuous manifold you actually leverage? And in the TSE example, the only structure that you really use is local distances. So if you think of a point cloud, of course, you can deal with it as a set. But if you assume that it comes from sampling of some continuous surface, you can probably say more. And this is forgivable what we tried to do in some of our works on geometric deep learning in applications in computer vision and graphics. And measures are one way of thinking of them is as graphs and steroids, where we have additional structure to leverage. So it's not only nodes and edges, but also faces. And in fact, measures are what is called simplicial complexes. As to the practical usefulness, computer vision and graphics are obviously the two fields where geometric deep learning on measures is important. And just to give a recent example of a commercial success. There was a British startup called the AI. It was founded by my colleague and friend, Yasunos Kokinos. I was also one of the investors. And we had a collaboration on three different reconstruction using geometric neural networks. And the company was acquired last year by SNAP and these technologies already now part of SNAP products. So you see it in the form of some 3D avatars or virtual and augmented reality applications that SNAP is developed. Professor Bronstein, I saw that you were doing some really cool stuff with the higher order simplicial coverings in graphs. And actually, I was going to call out your recent work on diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to actually enable a little of this analysis. But there was a question from my good friend, Zach Jost, who's one of our staff members here. And he says, what do you think is the most important problem to solve with message passing graph neural networks? And what's the most promising path forward? Probably we first need to agree about terminology. And to me, message passing and here I agree with Petra is just a very generic mechanism for propagating information on the graph. Now, traditional message passing that is used in graph neural networks uses the input graph for this propagation. And we know, right, as we discussed that it is equivalent to device for a lemon graph isomorphism test that has limitations in the kinds of structures it can detect. Now, there exists topological constructions that go beyond graphs, such as simplicial and cell complexes that you mentioned. And what we did in our recent works is developing a message passing mechanism that is able to work on such structures. And of course, you may ask whether we do encounter such structures in real life. So first of all, we do the measures that I mentioned before are in fact, simplicial complexes. But secondly, what we show in the paper is that we can take a traditional graph and lift it into a cell or simplicial complex. And probably a good example here is from the domain of computational chemistry, the graph neural network that you apply to a molecular graph would consider a molecule just as a collection of nodes and edges, atoms, and chemical bonds between them. But this is not how chemists think of molecules. They think of them as structures such as, for example, aromatic rings. And with our approach, we can regard the rings as cells. So we have a special new object, and we can do a different form of message passing on them. And we can also show from the theoretical perspective that this kind of message passing is strictly more powerful than the vice for a lemon algorithm. Do you see entirely new problems opening up that we wouldn't even have, let's say, we wouldn't even have dared to touch before, you know, in, let's say, we simply have our classic neural networks or whatnot, or even our classic graph message passing algorithms. Do you see new problems that are now in reach that previously with none of these methods were really, let's say, better than random guessing? It's a very interesting question that I think I'll answer from two angles, because you could, like, there could be like some longstanding problem that you knew about and wouldn't dare to attack. And now maybe you feel a bit more confident to attack it. There's also the aspect of uncovering a problem, because when you start thinking about things in this particular way, you might realize, hang on, to make this work, I made some assumptions. And those assumptions actually don't really hold at all in principle. So how do I make things, you know, a little bit better? So I'll try to give an example for both of those. So in terms of a problem that previously, I don't think was very easy to attack. And now we might have some tools that could help us attack it better. I have a longstanding interest in reinforcement learning. Actually, when I started my PhD, I spent six months attacking a reinforcement learning problem with one super tiny GPU. And that was, at that time, a massive time sink. Actually, DeepMind ended up scooping my work sometime after that. And I quickly moved to things that were more, you know, doable with the kind of hardware that I had at the time. But, you know, I always had a big interest in this area. And after joining DeepMind, I started to contribute to these kinds of directions more and more. And I think that basically, there are a lot of problems in reinforcement learning concerning data efficiency. So when you have to learn how to meaningfully act and do stuff, which is actually a fairly like low dimensional signal compared to the potential richness of the trajectories that you have to go through before you get that useful signal, like long term credit assignment, all these kinds of problems, I feel like we can start to get more data efficient reinforcement learning architectures by leveraging geometric concepts and also algorithmic concepts. So to give you one example of this, we have some months ago put out a paper on the archive called the executed latent value iteration network or x selvin, where we have captured the essence of an algorithm in RL, which perfectly solves the RL problem. So the value iteration algorithm, assuming you give it a Markov decision process will give you the perfect policy for that Markov decision process. So it's a super attractive algorithm to think about when you do RL, big caveat, right? You need to know all the dynamics about your environment, and you need to know all the reward models of the environment before you can apply the algorithm. So this obviously limits its use in the more generic deep reinforcement learning setting. But now with the knowledge of the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning blueprint, we actually taught the graph neural network, which aligns super well with value iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation, and then we stitched it into a planning algorithm in a deep reinforcement learning setting. And just by like training this pipeline end to end with a model free loss, we were able to get interesting returns in Atari games much sooner than some of the competing approaches. So it's a very small step. It still requires, you know, 100,000 200,000 iterations of playing before you get meaning, some meaningful behaviors start to come out. But it's a sign that we might be able to move the needle a bit backwards and not require, you know, billions and billions of transitions before we start to see meaningful behavior emerge. And I think that's very important because in most real world applications of reinforcement learning, you don't have a budget for billions of interactions before you have to already learn a meaningful policy. So that's one side, I think. And just generally in reinforcement learning, graphs appear left, right and center, not just in the algorithms, but also in the structure of the environment and these kinds of things. So I think that's one area where geometric deep learning could really help us, you know, get better behaviors faster, not necessarily solve it better than the standard deep RL, but, you know, get the better behaviors and fewer interactions. And as for one problem that we have uncovered through this kind of observational lens, you know, as I said, often in graph representation learning, we assume innocently that the graph is given to us, whereas very often this is not the case. So this divide has brought about this new emerging area of latent graph learning or latent graph inference, where the objective is to learn the graph simultaneously with using it for your underlying decision problem. And this is a big issue for neural network optimization, because you're fundamentally making a discrete decision in there. And if the number of nodes is huge, you cannot afford to start with the n squared approach and then gradually refine it. So currently, the state of the art in many regards of what we have here is to do a K nearest neighbor graph in the feature space, and just hope that that gets us most of the way there. And usually this works quite well for getting, you know, interesting answers, because, you know, if you have a decent ish enough KNN graph, you will cover everything that you need reasonably quickly. But, you know, then there that raises the issue of what if the graph itself is a meaningful output of your problem, what if you're a causality researcher that wants to figure out how different, you know, parts of information interact to them, they probably wouldn't be satisfied with a K nearest neighbor graph as an output of the system. So yeah, I feel like there's a lot of work to be done to actually scalably and usefully do something like this. And I don't have a better answer than what I just said is the state of the art. So a potential open problem for everybody in the audience today. Absolutely. And you touch on some really interesting things that I think causality is a huge area that we could be looking at graphs on. And also we had Dr. Tom Zahavi from DeepMind, one of your colleagues, and he said that, you know, he looks at meta learning and also diversity preservation in in agent based learning. But he thinks that reinforcement learning is just about to have its image net moment where we can discover a lot of the structure in these problems, which is fascinating. I would like to bring up one application where maybe quantitative improvement that is afforded by a genetic deep learning can lead to a qualitative breakthrough. And this is a problem of structural biology. Alpha fold is one such example for correctly geometrically modeling the problem you get a breakthrough in the performance. So it's indeed an image net moment that happened in this field. And now once you have sufficiently accurate prediction of 3D structure of proteins, it suddenly enables a lot of interesting applications for example, in the field of drug design. So potentially entire pharmaceutical pipelines we invented with the use of this technology. And the impact can be extraordinary. It's so true. I mean, Professor Bronstein, when I was watching your lecture series, it blew me away when you were talking about all of the applications. I think Yannick said a minute ago that it's almost as if some of these applications are just so ambitious that the thought wouldn't even have crossed our mind that we might be able to do it before, like for example, being able to predict facial geometry from a DNA sequence. So we might be able to look at an old DNA sequence and actually see what that person looked like. That would have been unimaginable just a few years ago. So that's incredible. I'm really interested in your definition of intelligence, right? And whether you think neural networks could ever be intelligent. Douglas Hofstadter, for example, he wrote the famous book Godel Escher Bach. It was a Pulitzer Prize winning book in the 1970s, but he made the argument that analogy is the core of cognition. He said that analogies are a bit like the interstate freeway of cognition. They're not little modules on the side or something like that. And I think that in a way, analogies are also symmetries, right? So when I say that someone is firewalling a person, it means that they don't want to talk with that person. It's a symmetry between the abstract representation of a real network firewall and an abstract social category. So does this require a different neural network architecture or could geometric deep learning already deliver the goods, right? It's almost as if it's just a representation problem. I think it's a very important question, one which I cannot claim to have the right answer to. And my definition is I guess a little bit skewed by the specific research that I do and the engineering approaches that I do. But I think in large, I agree with the idea of analogy making, and maybe I would take it a step further, right? Where you have a particular set of knowledge and conclusions that you've derived so far, a set of primitives that you can use once your information comes in to figure out how to recompose them and either discover new analogies or just discover new conclusions that you can use in the next step of reasoning. And it just feels really amenable to a kind of synergy of, as Daniel Kahneman puts it, System 1 and System 2, right? You have the perceptive component that feeds in the raw information that you receive as your input data, transforms it into some kind of abstract conceptual information. And then in the System 2 land, you have access to this kind of reasoning procedures that are able to take all of these concepts and derive new ones from hopefully a nice and not very high dimensional set of roles. And this is why I believe that if we, that in terms of like moving towards the, an architecture that supports something like this, I think we have a lot of the building blocks in place with geometric deep learning, especially if we're willing to, as I said, kind of broaden the definition of geometric deep learning to also include category theory concepts because that might allow us to reconcile algorithmic computation as well into the blueprint. So the idea is, you know, you have this, I mean, there's no need to talk at length about all these great perceptive architectures. So I think we're already at a point of, if we show our AGI lots and lots of data, it's going to be able to pick up on a lot of the interesting things just by observing, like, you know, self-supervised learning, unsupervised learning is already showing a lot of promise there. But then the question is, where I think we still have quite a bit of work to do is once we have these concepts, let's even assume that they're perfect. What do we do with them? How do we meaningfully use them? And I think the reason why I believe there's a lot of work to be done there is because one of the very key things that I do on a day to day basis is teach graph neural networks to imitate algorithms from perfect data. So I give them exactly the abstract input that the algorithm would expect. And I ask them, hey, simulate this algorithm for me, please. And it turns out that that is super, super hard. Well, it's super easy to do it in distribution, but you're not algorithmic if you don't extrapolate. And that's, I think, one of the big challenges that we need to work towards addressing. Will geometric deep learning be enough to encompass the ultimate solution? I have a feeling that it will. But, you know, I don't necessarily just based on the empirical evidence we've been seeing in the recent papers that we've put out. But yeah, I don't I don't have a very strong theoretical reason why I think it's going to be enough. Yeah, I'm fascinated by this notion that intelligence isn't mysterious as we might think it is. It's a it's a receding horizon. And it might in the end be disappointingly simple to mechanize. Actually, if you take the term literally, intelligence come from the Latin word that means to understand. And I think what is meant by understanding is really a very vague question. And probably if you ask different people, they will give you different definitions. I will define it as the faculty to abstract information. And in particular, information that is obtained in one context, the ability to use it in other contexts, this is what we usually call learning. The way that this information is abstracted and represented might be very different in a biological neural network in our brain versus an artificial intelligence system. So if you hear some people saying that that the brain probably doesn't really do geometric computations, my answer to that would be that we don't necessarily need to imitate exactly the way that the brain works. We just need probably to try to achieve this high level mechanism that is able to abstract information and applied as knowledge to different problems. The definitions of artificial intelligence that are being used, like the famous Turing test, I find is very disturbing that it's very anthropocentric. And it is actually probably very characteristic of the human species more broadly. And this way, by judging what is intelligent, what is not, we might potentially rule out other intelligent species because they are very different from us. I may be obsessed with sperm whales because I'm working on studying their communication. They are definitely intelligent creatures, but would they pass the Turing test? Probably not. It's like subjecting a cat to a swimming test or a fish to climbing on a tree. So I would just like to add to Michael's great answer, one quote that I think is very popular and applies really well in this setting. The question of whether computers can think is about as relevant as whether submarines can swim. When you built submarines, you weren't necessarily trying to copy fish. You were solving a problem that was fundamentally slightly different. So could be relevant in this case as well. We spend quite a lot of time on this podcast talking about whether we should have an anthropocentric conception of intelligence. A corporation is intelligent. I'm starting to come around to the view of embodiment and thinking that there is something very human like about our particular flavour of intelligence. But maybe there is a kind of pure intelligence as well. And this was the end of my conversation with Tako Kohen. One of the really interesting things is you're getting on to some of the work that you've done are being able to think of group convolutions on homogeneous objects like spheres, for example, but also you moved on to irregular objects like any mesh and you looked into things like fibre bundles and local convolutions. Because these are objects, I think you said a homogeneous object is where you can't perform some transformation to get from one place of the object to the other part of the object. So what work did you do there on those irregular objects? Yeah, that's a great question. So when we think of convolution, we're sort of putting together a whole bunch of things, namely this idea of locality. So typically our filters are local, but that's a choice ultimately. Convolution doesn't have to use a local filter, though in practice we know that works very well. And there's the idea of weight sharing between different positions and potentially also between different orientations of the filter. And as I mentioned before, this weight sharing really comes from the symmetry. So the fact that you use the same filter at each position in your image when you're doing a two-dimensional convolution, that's because you want to respect the translation symmetry acting on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry. If you think of a mesh representing a human figure, or let's say it's some complicated protein structure, there may not be a global symmetry. Or sometimes in the case of say a molecule might have some six-fold rotational symmetry, this symmetry may not be transitive as it's called, meaning you cannot take any two points on your manifold and map one to the other using a symmetry. In the case of a sphere, you can do that. Any two points in the sphere are related by rotation. So we say a sphere is a homogeneous space, but these let's say this protein shape is not homogeneous, even if it has some kind of symmetry. So in that case, if you try to motivate the weight sharing via symmetry, you try to take your filter, put it in one position, move it around to a different position using a symmetry, you're not going to be able to hit all positions. So you're not going to get weight sharing globally. And that just is what it is. If you say I have a signal on this manifold and I want to respect the symmetry, well, if there are no global symmetries, there's nothing to respect. So you get no code strain. So you can just use arbitrary linear map. Now, it turns out there are certain other kinds of symmetries called gauge symmetries that you might still want to respect. And in practice, what respecting gauge symmetry will do is we'll put some constraints on the filter at a particular position. So for example, that might have to be a rotationally equivariant filter, but it doesn't tie the weights of filters at different positions. So if you want that as well, then you can maybe motivate it via some kind of notion of local symmetry. I have something on a local symmetry group point to motivate that in my thesis. But there isn't a very principled way to motivate weight sharing on general manifolds between different locations. Yeah, I'm just trying to get my head around this. So you're saying, because the whole point that we're trying to achieve here is to have a parameter-efficient neural network that uses local connectivity and weight sharing, as we do with, let's say, plain RCNN, whereas when you have an irregular object, it's very, very difficult to do that. So you're saying, in some restricted domain, you can do it. Let's say if you have a, let's say, rotation equivariance, but you can't do the other forms of weight sharing. I'm just trying to get my head around this, because with a graph convolution on your network, for example, it seems like you can abstract the local neighborhood. This node is connected to these other nodes. And potentially, that could translate to a different part of the irregular mesh. So why can't you do it more than you suggested? I think if you want to be precise, you just have to say, what are the symmetries that we're talking about here? And in a graph, the most obvious one is the global permutation symmetry. So you can reorder the nodes of a graph and really any graph neural net, whether they're coming from an equivariance perspective or not, all graph neural nets in existence that have been proposed, they respect this permutation symmetry. And typically, this happens through, let's say, in the most simple kind of graph convolutional nets, like the ones by Kip van Belling, for example, there's a sum operation, some messages from your neighbors. And some operation does depend on the border of the summands. So it doesn't depend on the border of the neighbors. And that's why the whole thing becomes every variant. So that's a global symmetry that all graph networks respect. On that, though, could you not create a local, let's say if there was a local graph isomorphism, and so I have an irregular object, but it has a local isomorphism, could I not use something like a GCN, a local version of it to capture that isomorphism? Yeah. So actually, this was something we proposed to do in our paper, Natural Graph Networks. So this paper really has two aspects to it. One is the naturality as a generalization of equivariance, I can talk about that. But another key point was that we can not just develop a global natural graph network, as we call it, but also a local one. And what the local one will do is it will look at certain local motifs. So maybe if you're analyzing molecular graphs, one motif that you often find is, you know, aromatic ring or something, some ring with, let's say, six corners, various other kinds of little small graph motifs. And these motifs might appear multiple times in the same molecule or across different molecules. And so what this method is doing is it's essentially finding those using some kind of graph isomorph, local graph isomorphism, and then making sure that whenever we encounter this particular motif, we process it in the same way, i.e. using the same weights. And if the local motif has some kind of symmetry, like this aromatic ring, you can rotate it six times, and it gets back to the origin or you can flip it over. So that's the symmetry of this graph structure or an automorphism of this graph. And then the weights will also be constrained by this automorphism group, this group of symmetries of the local motif. And various other authors also have, I think, even Michael Bronstein and students have developed methods based on similar ideas. Awesome. Taiko, it's been such an honor having you on the show. And actually, you're coming back on the show in a few weeks' time, so we don't want to spoil the surprise. But looking on this proto book that you've written with the other folks, what's the main thing that sticks out to you as being the coolest thing in the book? I think there's any one particular thing. What excites me is to put some order to the chaos of the zoo of architectures and to see, actually, that there is something that they all have in common. And I really think this can help new people who are new to the field to learn more quickly, to get an overview of all the things that are out there. And I also think that this is the start of at least one way in which we can take the black box of deep learning, which often is viewed as completely inscrutable and actually start to open it and start to understand how the pieces connect, which can then perhaps inform future developments that are guided by both empirical results and an understanding of what's going on. Amazing. Thanks so much, Taiko. Thanks for having me. It's been a pleasure. Joan, thank you so much for joining us. This has been amazing. Okay, no, thank you so much, Tim. It was very fun and best of luck. And I think I'm let's maybe get in touch. Thank you very much. It's very nice to be talking to you today about these completely random topics. Yeah.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.48, "text": " So, you're talking about the symmetries in data.", "tokens": [50364, 407, 11, 291, 434, 1417, 466, 264, 14232, 302, 2244, 294, 1412, 13, 50588], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 1, "seek": 0, "start": 4.48, "end": 9.68, "text": " Could these analogies also be represented using the kind of symmetries that you're talking about?", "tokens": [50588, 7497, 613, 16660, 530, 611, 312, 10379, 1228, 264, 733, 295, 14232, 302, 2244, 300, 291, 434, 1417, 466, 30, 50848], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 2, "seek": 0, "start": 9.68, "end": 11.68, "text": " Good question. Let me think of it a bit.", "tokens": [50848, 2205, 1168, 13, 961, 385, 519, 295, 309, 257, 857, 13, 50948], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 3, "seek": 0, "start": 14.16, "end": 18.72, "text": " Modern machine learning operates with large, high-quality data sets,", "tokens": [51072, 19814, 3479, 2539, 22577, 365, 2416, 11, 1090, 12, 11286, 1412, 6352, 11, 51300], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 4, "seek": 0, "start": 18.72, "end": 21.84, "text": " which together with appropriate computational resources,", "tokens": [51300, 597, 1214, 365, 6854, 28270, 3593, 11, 51456], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 5, "seek": 0, "start": 21.84, "end": 28.560000000000002, "text": " motivates the design of rich function spaces with the capacity to interpolate over the data points.", "tokens": [51456, 42569, 264, 1715, 295, 4593, 2445, 7673, 365, 264, 6042, 281, 44902, 473, 670, 264, 1412, 2793, 13, 51792], "temperature": 0.0, "avg_logprob": -0.21105660994847616, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.004122599493712187}, {"id": 6, "seek": 2856, "start": 28.56, "end": 33.839999999999996, "text": " Now, this mindset plays well with neural networks since even the simplest choices of inductive", "tokens": [50364, 823, 11, 341, 12543, 5749, 731, 365, 18161, 9590, 1670, 754, 264, 22811, 7994, 295, 31612, 488, 50628], "temperature": 0.0, "avg_logprob": -0.1183219102712778, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.0004582233668770641}, {"id": 7, "seek": 2856, "start": 33.839999999999996, "end": 42.72, "text": " prior yield a dense class of functions. Now, symmetry, as wide or narrow as you may define its", "tokens": [50628, 4059, 11257, 257, 18011, 1508, 295, 6828, 13, 823, 11, 25440, 11, 382, 4874, 420, 9432, 382, 291, 815, 6964, 1080, 51072], "temperature": 0.0, "avg_logprob": -0.1183219102712778, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.0004582233668770641}, {"id": 8, "seek": 2856, "start": 42.72, "end": 50.8, "text": " meaning, is one idea by which man through the ages has tried to comprehend and create order,", "tokens": [51072, 3620, 11, 307, 472, 1558, 538, 597, 587, 807, 264, 12357, 575, 3031, 281, 38183, 293, 1884, 1668, 11, 51476], "temperature": 0.0, "avg_logprob": -0.1183219102712778, "compression_ratio": 1.492063492063492, "no_speech_prob": 0.0004582233668770641}, {"id": 9, "seek": 5080, "start": 51.44, "end": 58.48, "text": " beauty, and perfection. And that was a quote from Hermann Weil, a German mathematician", "tokens": [50396, 6643, 11, 293, 19708, 13, 400, 300, 390, 257, 6513, 490, 21842, 969, 492, 388, 11, 257, 6521, 48281, 50748], "temperature": 0.0, "avg_logprob": -0.11559454600016277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 0.02094680815935135}, {"id": 10, "seek": 5080, "start": 58.48, "end": 65.2, "text": " who was born in the 19th century. Now, since the early days, researchers have adapted neural", "tokens": [50748, 567, 390, 4232, 294, 264, 1294, 392, 4901, 13, 823, 11, 1670, 264, 2440, 1708, 11, 10309, 362, 20871, 18161, 51084], "temperature": 0.0, "avg_logprob": -0.11559454600016277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 0.02094680815935135}, {"id": 11, "seek": 5080, "start": 65.2, "end": 70.72, "text": " networks to exploit the low-dimensional geometry arising from physical measurements,", "tokens": [51084, 9590, 281, 25924, 264, 2295, 12, 18759, 18426, 44900, 490, 4001, 15383, 11, 51360], "temperature": 0.0, "avg_logprob": -0.11559454600016277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 0.02094680815935135}, {"id": 12, "seek": 5080, "start": 70.72, "end": 78.32, "text": " for example, grids in images, sequences in time series, or position and momentum in molecules", "tokens": [51360, 337, 1365, 11, 677, 3742, 294, 5267, 11, 22978, 294, 565, 2638, 11, 420, 2535, 293, 11244, 294, 13093, 51740], "temperature": 0.0, "avg_logprob": -0.11559454600016277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 0.02094680815935135}, {"id": 13, "seek": 7832, "start": 78.39999999999999, "end": 82.39999999999999, "text": " and their associated symmetries, such as translation or rotation.", "tokens": [50368, 293, 641, 6615, 14232, 302, 2244, 11, 1270, 382, 12853, 420, 12447, 13, 50568], "temperature": 0.0, "avg_logprob": -0.07977317941599879, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.008842014707624912}, {"id": 14, "seek": 7832, "start": 83.27999999999999, "end": 89.19999999999999, "text": " Now, folks, this is an epic special edition of MLST. We've been working on this since May of this", "tokens": [50612, 823, 11, 4024, 11, 341, 307, 364, 13581, 2121, 11377, 295, 376, 19198, 51, 13, 492, 600, 668, 1364, 322, 341, 1670, 1891, 295, 341, 50908], "temperature": 0.0, "avg_logprob": -0.07977317941599879, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.008842014707624912}, {"id": 15, "seek": 7832, "start": 89.19999999999999, "end": 93.67999999999999, "text": " year, so please use the table of contents on YouTube if you want to skip around. The show is", "tokens": [50908, 1064, 11, 370, 1767, 764, 264, 3199, 295, 15768, 322, 3088, 498, 291, 528, 281, 10023, 926, 13, 440, 855, 307, 51132], "temperature": 0.0, "avg_logprob": -0.07977317941599879, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.008842014707624912}, {"id": 16, "seek": 7832, "start": 93.67999999999999, "end": 99.35999999999999, "text": " about three and a half hours long. The second half of the show, roughly speaking, is a traditional", "tokens": [51132, 466, 1045, 293, 257, 1922, 2496, 938, 13, 440, 1150, 1922, 295, 264, 855, 11, 9810, 4124, 11, 307, 257, 5164, 51416], "temperature": 0.0, "avg_logprob": -0.07977317941599879, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.008842014707624912}, {"id": 17, "seek": 7832, "start": 99.35999999999999, "end": 104.32, "text": " style MLST episode, but the beginning part is a bit of an experiment for us, maybe a bit of a", "tokens": [51416, 3758, 376, 19198, 51, 3500, 11, 457, 264, 2863, 644, 307, 257, 857, 295, 364, 5120, 337, 505, 11, 1310, 257, 857, 295, 257, 51664], "temperature": 0.0, "avg_logprob": -0.07977317941599879, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.008842014707624912}, {"id": 18, "seek": 10432, "start": 104.32, "end": 109.83999999999999, "text": " departure. We want to make some Netflix-style content, and we've even been filming on location", "tokens": [50364, 25866, 13, 492, 528, 281, 652, 512, 12778, 12, 15014, 2701, 11, 293, 321, 600, 754, 668, 8869, 322, 4914, 50640], "temperature": 0.0, "avg_logprob": -0.07092623277143999, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.011857077479362488}, {"id": 19, "seek": 10432, "start": 109.83999999999999, "end": 114.32, "text": " with our guests, so I hope you enjoy the show and let us know what you think in the YouTube comments.", "tokens": [50640, 365, 527, 9804, 11, 370, 286, 1454, 291, 2103, 264, 855, 293, 718, 505, 458, 437, 291, 519, 294, 264, 3088, 3053, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07092623277143999, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.011857077479362488}, {"id": 20, "seek": 10432, "start": 115.11999999999999, "end": 120.32, "text": " Many people intuit that there are some deep theoretical links between some of the recent", "tokens": [50904, 5126, 561, 16224, 300, 456, 366, 512, 2452, 20864, 6123, 1296, 512, 295, 264, 5162, 51164], "temperature": 0.0, "avg_logprob": -0.07092623277143999, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.011857077479362488}, {"id": 21, "seek": 10432, "start": 120.32, "end": 125.91999999999999, "text": " deep learning model architectures, particularly the ones on sets, actually. And this may be why", "tokens": [51164, 2452, 2539, 2316, 6331, 1303, 11, 4098, 264, 2306, 322, 6352, 11, 767, 13, 400, 341, 815, 312, 983, 51444], "temperature": 0.0, "avg_logprob": -0.07092623277143999, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.011857077479362488}, {"id": 22, "seek": 10432, "start": 125.91999999999999, "end": 131.51999999999998, "text": " so many popular architectures keep getting reinvented. Now, the other day, Fabian Fuchs from", "tokens": [51444, 370, 867, 3743, 6331, 1303, 1066, 1242, 33477, 292, 13, 823, 11, 264, 661, 786, 11, 17440, 952, 479, 37503, 490, 51724], "temperature": 0.0, "avg_logprob": -0.07092623277143999, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.011857077479362488}, {"id": 23, "seek": 13152, "start": 131.60000000000002, "end": 136.32000000000002, "text": " Oxford University released a really cool blog post about deep learning on sets,", "tokens": [50368, 24786, 3535, 4736, 257, 534, 1627, 6968, 2183, 466, 2452, 2539, 322, 6352, 11, 50604], "temperature": 0.0, "avg_logprob": -0.09021635850270589, "compression_ratio": 1.5043103448275863, "no_speech_prob": 0.003427582560107112}, {"id": 24, "seek": 13152, "start": 136.32000000000002, "end": 141.68, "text": " elucidating a math-heavy paper that he co-authored with Edward Wagstaff et al.", "tokens": [50604, 806, 1311, 327, 990, 257, 5221, 12, 37157, 3035, 300, 415, 598, 12, 40198, 2769, 365, 18456, 49921, 372, 2518, 1030, 419, 13, 50872], "temperature": 0.0, "avg_logprob": -0.09021635850270589, "compression_ratio": 1.5043103448275863, "no_speech_prob": 0.003427582560107112}, {"id": 25, "seek": 13152, "start": 142.32000000000002, "end": 148.64000000000001, "text": " Now, he wanted to understand why so many neural network architectures for sets resemble either", "tokens": [50904, 823, 11, 415, 1415, 281, 1223, 983, 370, 867, 18161, 3209, 6331, 1303, 337, 6352, 36870, 2139, 51220], "temperature": 0.0, "avg_logprob": -0.09021635850270589, "compression_ratio": 1.5043103448275863, "no_speech_prob": 0.003427582560107112}, {"id": 26, "seek": 13152, "start": 148.64000000000001, "end": 155.92000000000002, "text": " deep sets or self-attention, because sets come in any ordering. There are many opportunities to", "tokens": [51220, 2452, 6352, 420, 2698, 12, 1591, 1251, 11, 570, 6352, 808, 294, 604, 21739, 13, 821, 366, 867, 4786, 281, 51584], "temperature": 0.0, "avg_logprob": -0.09021635850270589, "compression_ratio": 1.5043103448275863, "no_speech_prob": 0.003427582560107112}, {"id": 27, "seek": 15592, "start": 155.92, "end": 162.16, "text": " design inductive priors to capture the symmetries. So, raises the question,", "tokens": [50364, 1715, 31612, 488, 1790, 830, 281, 7983, 264, 14232, 302, 2244, 13, 407, 11, 19658, 264, 1168, 11, 50676], "temperature": 0.0, "avg_logprob": -0.13643955602878477, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.005383754149079323}, {"id": 28, "seek": 15592, "start": 162.79999999999998, "end": 168.16, "text": " how do we design deep learning algorithms that are invariant to semantically-equivalent", "tokens": [50708, 577, 360, 321, 1715, 2452, 2539, 14642, 300, 366, 33270, 394, 281, 4361, 49505, 12, 12816, 3576, 317, 50976], "temperature": 0.0, "avg_logprob": -0.13643955602878477, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.005383754149079323}, {"id": 29, "seek": 15592, "start": 168.16, "end": 174.95999999999998, "text": " transformations while maintaining maximum expressivity? Now, Fabian pointed out that", "tokens": [50976, 34852, 1339, 14916, 6674, 5109, 4253, 30, 823, 11, 17440, 952, 10932, 484, 300, 51316], "temperature": 0.0, "avg_logprob": -0.13643955602878477, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.005383754149079323}, {"id": 30, "seek": 15592, "start": 174.95999999999998, "end": 181.2, "text": " the so-called Genosi-pooling framework gives a satisfying explanation. Genosi-pooling is when", "tokens": [51316, 264, 370, 12, 11880, 3632, 21521, 12, 17374, 278, 8388, 2709, 257, 18348, 10835, 13, 3632, 21521, 12, 17374, 278, 307, 562, 51628], "temperature": 0.0, "avg_logprob": -0.13643955602878477, "compression_ratio": 1.5336322869955157, "no_speech_prob": 0.005383754149079323}, {"id": 31, "seek": 18120, "start": 181.2, "end": 186.64, "text": " you generate all of the k-tuples of a set, an average over your target function, on those", "tokens": [50364, 291, 8460, 439, 295, 264, 350, 12, 9179, 2622, 295, 257, 992, 11, 364, 4274, 670, 428, 3779, 2445, 11, 322, 729, 50636], "temperature": 0.0, "avg_logprob": -0.11343650195909583, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006001939065754414}, {"id": 32, "seek": 18120, "start": 186.64, "end": 192.64, "text": " permutations. It gives you a computationally tractable way of achieving permutation invariance.", "tokens": [50636, 4784, 325, 763, 13, 467, 2709, 291, 257, 24903, 379, 24207, 712, 636, 295, 19626, 4784, 11380, 33270, 719, 13, 50936], "temperature": 0.0, "avg_logprob": -0.11343650195909583, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006001939065754414}, {"id": 33, "seek": 18120, "start": 193.2, "end": 199.6, "text": " So, rather than computing n factorial combinations of the examples, you compute n factorial divided", "tokens": [50964, 407, 11, 2831, 813, 15866, 297, 36916, 21267, 295, 264, 5110, 11, 291, 14722, 297, 36916, 6666, 51284], "temperature": 0.0, "avg_logprob": -0.11343650195909583, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006001939065754414}, {"id": 34, "seek": 18120, "start": 199.6, "end": 206.64, "text": " by n minus k factorial, which for small k is very tractable. Now, clearly, setting k to n", "tokens": [51284, 538, 297, 3175, 350, 36916, 11, 597, 337, 1359, 350, 307, 588, 24207, 712, 13, 823, 11, 4448, 11, 3287, 350, 281, 297, 51636], "temperature": 0.0, "avg_logprob": -0.11343650195909583, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006001939065754414}, {"id": 35, "seek": 20664, "start": 206.64, "end": 212.55999999999997, "text": " gives you the most expressive yet the most expensive model, but that would be cool. It would", "tokens": [50364, 2709, 291, 264, 881, 40189, 1939, 264, 881, 5124, 2316, 11, 457, 300, 576, 312, 1627, 13, 467, 576, 50660], "temperature": 0.0, "avg_logprob": -0.0850240843636649, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.005059173796325922}, {"id": 36, "seek": 20664, "start": 212.55999999999997, "end": 217.83999999999997, "text": " model the high-order interactions between the examples, but it turns out that deep sets are", "tokens": [50660, 2316, 264, 1090, 12, 4687, 13280, 1296, 264, 5110, 11, 457, 309, 4523, 484, 300, 2452, 6352, 366, 50924], "temperature": 0.0, "avg_logprob": -0.0850240843636649, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.005059173796325922}, {"id": 37, "seek": 20664, "start": 217.83999999999997, "end": 224.32, "text": " this configuration with k equals 1, and self-attention is this configuration with k equals 2.", "tokens": [50924, 341, 11694, 365, 350, 6915, 502, 11, 293, 2698, 12, 1591, 1251, 307, 341, 11694, 365, 350, 6915, 568, 13, 51248], "temperature": 0.0, "avg_logprob": -0.0850240843636649, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.005059173796325922}, {"id": 38, "seek": 20664, "start": 225.27999999999997, "end": 231.44, "text": " Now, Fabian also spoke about approximate permutation invariance, which is when you set k to n,", "tokens": [51296, 823, 11, 17440, 952, 611, 7179, 466, 30874, 4784, 11380, 33270, 719, 11, 597, 307, 562, 291, 992, 350, 281, 297, 11, 51604], "temperature": 0.0, "avg_logprob": -0.0850240843636649, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.005059173796325922}, {"id": 39, "seek": 20664, "start": 232.07999999999998, "end": 236.0, "text": " the number of examples, but you sample the permutations. It turns out you don't have to", "tokens": [51636, 264, 1230, 295, 5110, 11, 457, 291, 6889, 264, 4784, 325, 763, 13, 467, 4523, 484, 291, 500, 380, 362, 281, 51832], "temperature": 0.0, "avg_logprob": -0.0850240843636649, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.005059173796325922}, {"id": 40, "seek": 23600, "start": 236.0, "end": 240.0, "text": " sample very many of them to get good results. But anyway, if you want to check out that in a", "tokens": [50364, 6889, 588, 867, 295, 552, 281, 483, 665, 3542, 13, 583, 4033, 11, 498, 291, 528, 281, 1520, 484, 300, 294, 257, 50564], "temperature": 0.0, "avg_logprob": -0.05392506363195017, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.000687561696395278}, {"id": 41, "seek": 23600, "start": 240.0, "end": 244.64, "text": " little bit more detail, go and check out Fabian's blog. I've put a link in the video description.", "tokens": [50564, 707, 857, 544, 2607, 11, 352, 293, 1520, 484, 17440, 952, 311, 6968, 13, 286, 600, 829, 257, 2113, 294, 264, 960, 3855, 13, 50796], "temperature": 0.0, "avg_logprob": -0.05392506363195017, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.000687561696395278}, {"id": 42, "seek": 23600, "start": 245.28, "end": 250.8, "text": " High-dimensional learning is impossible due to the curse of dimensionality. It only works if we", "tokens": [50828, 5229, 12, 18759, 2539, 307, 6243, 3462, 281, 264, 17139, 295, 10139, 1860, 13, 467, 787, 1985, 498, 321, 51104], "temperature": 0.0, "avg_logprob": -0.05392506363195017, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.000687561696395278}, {"id": 43, "seek": 23600, "start": 250.8, "end": 255.6, "text": " make some very strong assumptions about the regularities of the space of functions that we", "tokens": [51104, 652, 512, 588, 2068, 17695, 466, 264, 3890, 1088, 295, 264, 1901, 295, 6828, 300, 321, 51344], "temperature": 0.0, "avg_logprob": -0.05392506363195017, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.000687561696395278}, {"id": 44, "seek": 23600, "start": 255.6, "end": 260.64, "text": " need to search through. Now, the classical assumptions that we make in machine learning", "tokens": [51344, 643, 281, 3164, 807, 13, 823, 11, 264, 13735, 17695, 300, 321, 652, 294, 3479, 2539, 51596], "temperature": 0.0, "avg_logprob": -0.05392506363195017, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.000687561696395278}, {"id": 45, "seek": 26064, "start": 260.71999999999997, "end": 266.4, "text": " are no longer relevant. Now, in general, learning in high dimensions is intractable.", "tokens": [50368, 366, 572, 2854, 7340, 13, 823, 11, 294, 2674, 11, 2539, 294, 1090, 12819, 307, 560, 1897, 712, 13, 50652], "temperature": 0.0, "avg_logprob": -0.0888661543528239, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.0073432414792478085}, {"id": 46, "seek": 26064, "start": 266.96, "end": 271.91999999999996, "text": " The number of samples grows exponentially with the number of dimensions. The universal function", "tokens": [50680, 440, 1230, 295, 10938, 13156, 37330, 365, 264, 1230, 295, 12819, 13, 440, 11455, 2445, 50928], "temperature": 0.0, "avg_logprob": -0.0888661543528239, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.0073432414792478085}, {"id": 47, "seek": 26064, "start": 271.91999999999996, "end": 278.24, "text": " approximation theorem popularized in the 1990s states that for the class of shallow neural", "tokens": [50928, 28023, 20904, 3743, 1602, 294, 264, 13384, 82, 4368, 300, 337, 264, 1508, 295, 20488, 18161, 51244], "temperature": 0.0, "avg_logprob": -0.0888661543528239, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.0073432414792478085}, {"id": 48, "seek": 26064, "start": 278.24, "end": 284.32, "text": " network functions, you can approximate any continuous function to arbitrary precision by", "tokens": [51244, 3209, 6828, 11, 291, 393, 30874, 604, 10957, 2445, 281, 23211, 18356, 538, 51548], "temperature": 0.0, "avg_logprob": -0.0888661543528239, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.0073432414792478085}, {"id": 49, "seek": 26064, "start": 284.32, "end": 289.28, "text": " just stacking the neurons, assuming that you had enough of them. So it's a bit like kind of", "tokens": [51548, 445, 41376, 264, 22027, 11, 11926, 300, 291, 632, 1547, 295, 552, 13, 407, 309, 311, 257, 857, 411, 733, 295, 51796], "temperature": 0.0, "avg_logprob": -0.0888661543528239, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.0073432414792478085}, {"id": 50, "seek": 28928, "start": 289.28, "end": 295.67999999999995, "text": " sparse coding, if you like. The curse of dimensionality refers to the various phenomena that arise", "tokens": [50364, 637, 11668, 17720, 11, 498, 291, 411, 13, 440, 17139, 295, 10139, 1860, 14942, 281, 264, 3683, 22004, 300, 20288, 50684], "temperature": 0.0, "avg_logprob": -0.07005733313019742, "compression_ratio": 1.819277108433735, "no_speech_prob": 0.0002453415945637971}, {"id": 51, "seek": 28928, "start": 295.67999999999995, "end": 300.88, "text": " when analyzing and organizing data in high-dimensional spaces that do not occur", "tokens": [50684, 562, 23663, 293, 17608, 1412, 294, 1090, 12, 18759, 7673, 300, 360, 406, 5160, 50944], "temperature": 0.0, "avg_logprob": -0.07005733313019742, "compression_ratio": 1.819277108433735, "no_speech_prob": 0.0002453415945637971}, {"id": 52, "seek": 28928, "start": 300.88, "end": 306.64, "text": " in low-dimensional settings, such as the three-dimensional physical space of everyday experience.", "tokens": [50944, 294, 2295, 12, 18759, 6257, 11, 1270, 382, 264, 1045, 12, 18759, 4001, 1901, 295, 7429, 1752, 13, 51232], "temperature": 0.0, "avg_logprob": -0.07005733313019742, "compression_ratio": 1.819277108433735, "no_speech_prob": 0.0002453415945637971}, {"id": 53, "seek": 28928, "start": 307.28, "end": 312.08, "text": " Now, the common theme of these problems is that when the dimensionality increases,", "tokens": [51264, 823, 11, 264, 2689, 6314, 295, 613, 2740, 307, 300, 562, 264, 10139, 1860, 8637, 11, 51504], "temperature": 0.0, "avg_logprob": -0.07005733313019742, "compression_ratio": 1.819277108433735, "no_speech_prob": 0.0002453415945637971}, {"id": 54, "seek": 28928, "start": 312.08, "end": 318.23999999999995, "text": " the volume of the space increases so fast that the available data effectively becomes sparse.", "tokens": [51504, 264, 5523, 295, 264, 1901, 8637, 370, 2370, 300, 264, 2435, 1412, 8659, 3643, 637, 11668, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07005733313019742, "compression_ratio": 1.819277108433735, "no_speech_prob": 0.0002453415945637971}, {"id": 55, "seek": 31824, "start": 318.32, "end": 323.44, "text": " This sparsity is problematic for any method that requires statistical significance. Now,", "tokens": [50368, 639, 637, 685, 507, 307, 19011, 337, 604, 3170, 300, 7029, 22820, 17687, 13, 823, 11, 50624], "temperature": 0.0, "avg_logprob": -0.05909083366394043, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0001739890140015632}, {"id": 56, "seek": 31824, "start": 323.44, "end": 328.72, "text": " in order to obtain a statistically sound and reliable result, the amount of data needed", "tokens": [50624, 294, 1668, 281, 12701, 257, 36478, 1626, 293, 12924, 1874, 11, 264, 2372, 295, 1412, 2978, 50888], "temperature": 0.0, "avg_logprob": -0.05909083366394043, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0001739890140015632}, {"id": 57, "seek": 31824, "start": 328.72, "end": 334.96000000000004, "text": " to support the result often grows exponentially with the dimensionality. Most of the information", "tokens": [50888, 281, 1406, 264, 1874, 2049, 13156, 37330, 365, 264, 10139, 1860, 13, 4534, 295, 264, 1589, 51200], "temperature": 0.0, "avg_logprob": -0.05909083366394043, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0001739890140015632}, {"id": 58, "seek": 31824, "start": 334.96000000000004, "end": 341.84000000000003, "text": " in data has regularities. Now, what this means in plain English is, just like on a kaleidoscope,", "tokens": [51200, 294, 1412, 575, 3890, 1088, 13, 823, 11, 437, 341, 1355, 294, 11121, 3669, 307, 11, 445, 411, 322, 257, 34699, 7895, 13960, 11, 51544], "temperature": 0.0, "avg_logprob": -0.05909083366394043, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0001739890140015632}, {"id": 59, "seek": 31824, "start": 341.84000000000003, "end": 346.96000000000004, "text": " most of the information which has been generated by the physical world is actually redundant,", "tokens": [51544, 881, 295, 264, 1589, 597, 575, 668, 10833, 538, 264, 4001, 1002, 307, 767, 40997, 11, 51800], "temperature": 0.0, "avg_logprob": -0.05909083366394043, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0001739890140015632}, {"id": 60, "seek": 34696, "start": 346.96, "end": 353.68, "text": " just many repeated semantically equivalent replicas of the same thing. The world is full", "tokens": [50364, 445, 867, 10477, 4361, 49505, 10344, 3248, 9150, 295, 264, 912, 551, 13, 440, 1002, 307, 1577, 50700], "temperature": 0.0, "avg_logprob": -0.06527937720803653, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.00010889449185924605}, {"id": 61, "seek": 34696, "start": 353.68, "end": 360.4, "text": " of simulacrums. Machine learning algorithms need to encode the appropriate notion of regularity", "tokens": [50700, 295, 1034, 425, 326, 6247, 82, 13, 22155, 2539, 14642, 643, 281, 2058, 1429, 264, 6854, 10710, 295, 3890, 507, 51036], "temperature": 0.0, "avg_logprob": -0.06527937720803653, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.00010889449185924605}, {"id": 62, "seek": 34696, "start": 360.4, "end": 365.84, "text": " to cut down the search space of possible functions. You might have heard this idea referred to", "tokens": [51036, 281, 1723, 760, 264, 3164, 1901, 295, 1944, 6828, 13, 509, 1062, 362, 2198, 341, 1558, 10839, 281, 51308], "temperature": 0.0, "avg_logprob": -0.06527937720803653, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.00010889449185924605}, {"id": 63, "seek": 34696, "start": 365.84, "end": 372.0, "text": " as an inductive bias. Now, machine learning is about trading off these three sources of error,", "tokens": [51308, 382, 364, 31612, 488, 12577, 13, 823, 11, 3479, 2539, 307, 466, 9529, 766, 613, 1045, 7139, 295, 6713, 11, 51616], "temperature": 0.0, "avg_logprob": -0.06527937720803653, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.00010889449185924605}, {"id": 64, "seek": 37200, "start": 372.0, "end": 376.64, "text": " statistical error from approximating the expectations on a finite sample,", "tokens": [50364, 22820, 6713, 490, 8542, 990, 264, 9843, 322, 257, 19362, 6889, 11, 50596], "temperature": 0.0, "avg_logprob": -0.06877039432525635, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.001244738814420998}, {"id": 65, "seek": 37200, "start": 376.64, "end": 382.56, "text": " and this grows as you increase your hypothesis space. Approximation error, which is how good", "tokens": [50596, 293, 341, 13156, 382, 291, 3488, 428, 17291, 1901, 13, 29551, 3081, 399, 6713, 11, 597, 307, 577, 665, 50892], "temperature": 0.0, "avg_logprob": -0.06877039432525635, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.001244738814420998}, {"id": 66, "seek": 37200, "start": 382.56, "end": 388.0, "text": " is your model in that hypothesis space? If your function space is too small, then the one that", "tokens": [50892, 307, 428, 2316, 294, 300, 17291, 1901, 30, 759, 428, 2445, 1901, 307, 886, 1359, 11, 550, 264, 472, 300, 51164], "temperature": 0.0, "avg_logprob": -0.06877039432525635, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.001244738814420998}, {"id": 67, "seek": 37200, "start": 388.0, "end": 393.92, "text": " you find will incur a lot of approximation error. And finally, optimization error, which is the", "tokens": [51164, 291, 915, 486, 35774, 257, 688, 295, 28023, 6713, 13, 400, 2721, 11, 19618, 6713, 11, 597, 307, 264, 51460], "temperature": 0.0, "avg_logprob": -0.06877039432525635, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.001244738814420998}, {"id": 68, "seek": 37200, "start": 393.92, "end": 399.52, "text": " ability to find a global optimum. Now, even if we make strong assumptions about our hypothesis", "tokens": [51460, 3485, 281, 915, 257, 4338, 39326, 13, 823, 11, 754, 498, 321, 652, 2068, 17695, 466, 527, 17291, 51740], "temperature": 0.0, "avg_logprob": -0.06877039432525635, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.001244738814420998}, {"id": 69, "seek": 39952, "start": 399.52, "end": 404.15999999999997, "text": " space, you know, we should say that it should be lip sheets or in plain English, it should be", "tokens": [50364, 1901, 11, 291, 458, 11, 321, 820, 584, 300, 309, 820, 312, 8280, 15421, 420, 294, 11121, 3669, 11, 309, 820, 312, 50596], "temperature": 0.0, "avg_logprob": -0.10107203533774928, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.002251607133075595}, {"id": 70, "seek": 39952, "start": 404.15999999999997, "end": 409.84, "text": " locally smooth. It's still way too large. We want to have a way to search through the space to get", "tokens": [50596, 16143, 5508, 13, 467, 311, 920, 636, 886, 2416, 13, 492, 528, 281, 362, 257, 636, 281, 3164, 807, 264, 1901, 281, 483, 50880], "temperature": 0.0, "avg_logprob": -0.10107203533774928, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.002251607133075595}, {"id": 71, "seek": 39952, "start": 409.84, "end": 416.08, "text": " anywhere. So the statistical error is cursed by the dimensionality. If we make the hypothesis or the", "tokens": [50880, 4992, 13, 407, 264, 22820, 6713, 307, 29498, 538, 264, 10139, 1860, 13, 759, 321, 652, 264, 17291, 420, 264, 51192], "temperature": 0.0, "avg_logprob": -0.10107203533774928, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.002251607133075595}, {"id": 72, "seek": 39952, "start": 416.08, "end": 421.91999999999996, "text": " function space really small, then the search space is smaller, but the approximation error is cursed", "tokens": [51192, 2445, 1901, 534, 1359, 11, 550, 264, 3164, 1901, 307, 4356, 11, 457, 264, 28023, 6713, 307, 29498, 51484], "temperature": 0.0, "avg_logprob": -0.10107203533774928, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.002251607133075595}, {"id": 73, "seek": 39952, "start": 421.91999999999996, "end": 428.4, "text": " by dimensionality. So we need to define better function spaces to search through. But how?", "tokens": [51484, 538, 10139, 1860, 13, 407, 321, 643, 281, 6964, 1101, 2445, 7673, 281, 3164, 807, 13, 583, 577, 30, 51808], "temperature": 0.0, "avg_logprob": -0.10107203533774928, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.002251607133075595}, {"id": 74, "seek": 42840, "start": 429.35999999999996, "end": 433.2, "text": " We need to move towards a new class of function spaces, which is to say,", "tokens": [50412, 492, 643, 281, 1286, 3030, 257, 777, 1508, 295, 2445, 7673, 11, 597, 307, 281, 584, 11, 50604], "temperature": 0.0, "avg_logprob": -0.08380043383726139, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00011235157580813393}, {"id": 75, "seek": 42840, "start": 433.84, "end": 440.0, "text": " geometrically inspired function spaces. Let's exploit the underlying low dimensional structure", "tokens": [50636, 12956, 81, 984, 7547, 2445, 7673, 13, 961, 311, 25924, 264, 14217, 2295, 18795, 3877, 50944], "temperature": 0.0, "avg_logprob": -0.08380043383726139, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00011235157580813393}, {"id": 76, "seek": 42840, "start": 440.0, "end": 444.88, "text": " of the high dimensional input space. The geometric domain can give us entirely new", "tokens": [50944, 295, 264, 1090, 18795, 4846, 1901, 13, 440, 33246, 9274, 393, 976, 505, 7696, 777, 51188], "temperature": 0.0, "avg_logprob": -0.08380043383726139, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00011235157580813393}, {"id": 77, "seek": 42840, "start": 444.88, "end": 450.96, "text": " notions of regularity, which we can exploit. Now using geometrical priors, which is to say,", "tokens": [51188, 35799, 295, 3890, 507, 11, 597, 321, 393, 25924, 13, 823, 1228, 12956, 15888, 1790, 830, 11, 597, 307, 281, 584, 11, 51492], "temperature": 0.0, "avg_logprob": -0.08380043383726139, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00011235157580813393}, {"id": 78, "seek": 42840, "start": 450.96, "end": 456.47999999999996, "text": " only allowing equivariant functions or ones which respect a particular geometrical principle,", "tokens": [51492, 787, 8293, 48726, 3504, 394, 6828, 420, 2306, 597, 3104, 257, 1729, 12956, 15888, 8665, 11, 51768], "temperature": 0.0, "avg_logprob": -0.08380043383726139, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00011235157580813393}, {"id": 79, "seek": 45648, "start": 456.48, "end": 460.0, "text": " this will reduce the space of possible functions that we search through,", "tokens": [50364, 341, 486, 5407, 264, 1901, 295, 1944, 6828, 300, 321, 3164, 807, 11, 50540], "temperature": 0.0, "avg_logprob": -0.10033762336957573, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.00080400105798617}, {"id": 80, "seek": 45648, "start": 460.0, "end": 465.28000000000003, "text": " which means less risk of statistical error and less risk of overfitting. We should be able to", "tokens": [50540, 597, 1355, 1570, 3148, 295, 22820, 6713, 293, 1570, 3148, 295, 670, 69, 2414, 13, 492, 820, 312, 1075, 281, 50804], "temperature": 0.0, "avg_logprob": -0.10033762336957573, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.00080400105798617}, {"id": 81, "seek": 45648, "start": 465.28000000000003, "end": 470.32, "text": " do this without increasing approximation error, because we should know for sure that the true", "tokens": [50804, 360, 341, 1553, 5662, 28023, 6713, 11, 570, 321, 820, 458, 337, 988, 300, 264, 2074, 51056], "temperature": 0.0, "avg_logprob": -0.10033762336957573, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.00080400105798617}, {"id": 82, "seek": 45648, "start": 470.32, "end": 476.08000000000004, "text": " function has a certain geometrical property, which will bias into the model. So introducing the", "tokens": [51056, 2445, 575, 257, 1629, 12956, 15888, 4707, 11, 597, 486, 12577, 666, 264, 2316, 13, 407, 15424, 264, 51344], "temperature": 0.0, "avg_logprob": -0.10033762336957573, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.00080400105798617}, {"id": 83, "seek": 45648, "start": 476.08000000000004, "end": 482.88, "text": " geometrical deep learning proto book. So recently, Professor Michael Bronstein, Professor Joanne Bruner,", "tokens": [51344, 12956, 15888, 2452, 2539, 47896, 1446, 13, 407, 3938, 11, 8419, 5116, 19544, 9089, 11, 8419, 3139, 12674, 1603, 409, 260, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10033762336957573, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.00080400105798617}, {"id": 84, "seek": 48288, "start": 482.96, "end": 490.4, "text": " Dr. Tako Kohen and Dr. Peta Velichkovich released an epic proto book called Geometric Deep Learning,", "tokens": [50368, 2491, 13, 9118, 78, 30861, 268, 293, 2491, 13, 430, 7664, 17814, 480, 33516, 480, 4736, 364, 13581, 47896, 1446, 1219, 2876, 29470, 14895, 15205, 11, 50740], "temperature": 0.0, "avg_logprob": -0.1641357362884836, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.004680067300796509}, {"id": 85, "seek": 48288, "start": 491.2, "end": 498.71999999999997, "text": " Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical", "tokens": [50780, 2606, 3742, 11, 10500, 82, 11, 21884, 82, 11, 2876, 4789, 1167, 293, 460, 1660, 13, 1981, 10309, 366, 14459, 3627, 25775, 13735, 51156], "temperature": 0.0, "avg_logprob": -0.1641357362884836, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.004680067300796509}, {"id": 86, "seek": 48288, "start": 498.71999999999997, "end": 504.15999999999997, "text": " theory and machine learning and geometry and group theory to deep learning, which is fascinating.", "tokens": [51156, 5261, 293, 3479, 2539, 293, 18426, 293, 1594, 5261, 281, 2452, 2539, 11, 597, 307, 10343, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1641357362884836, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.004680067300796509}, {"id": 87, "seek": 48288, "start": 504.8, "end": 510.4, "text": " Now the proto book is beautifully written, right? It's presented so well, it even has some helpful", "tokens": [51460, 823, 264, 47896, 1446, 307, 16525, 3720, 11, 558, 30, 467, 311, 8212, 370, 731, 11, 309, 754, 575, 512, 4961, 51740], "temperature": 0.0, "avg_logprob": -0.1641357362884836, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.004680067300796509}, {"id": 88, "seek": 51040, "start": 510.4, "end": 515.52, "text": " margin notes. And honestly, I could read sections of it out loud on MLST with scant need to change a", "tokens": [50364, 10270, 5570, 13, 400, 6095, 11, 286, 727, 1401, 10863, 295, 309, 484, 6588, 322, 376, 19198, 51, 365, 795, 394, 643, 281, 1319, 257, 50620], "temperature": 0.0, "avg_logprob": -0.13023023307323456, "compression_ratio": 1.6588628762541806, "no_speech_prob": 0.020212309435009956}, {"id": 89, "seek": 51040, "start": 515.52, "end": 519.1999999999999, "text": " single word. It's that well written. I mean, it is, there's a lot of maths in there as well,", "tokens": [50620, 2167, 1349, 13, 467, 311, 300, 731, 3720, 13, 286, 914, 11, 309, 307, 11, 456, 311, 257, 688, 295, 36287, 294, 456, 382, 731, 11, 50804], "temperature": 0.0, "avg_logprob": -0.13023023307323456, "compression_ratio": 1.6588628762541806, "no_speech_prob": 0.020212309435009956}, {"id": 90, "seek": 51040, "start": 519.1999999999999, "end": 524.72, "text": " let's be honest. I can't dodge that. But I often try to impress upon people that if your writing", "tokens": [50804, 718, 311, 312, 3245, 13, 286, 393, 380, 27238, 300, 13, 583, 286, 2049, 853, 281, 6729, 3564, 561, 300, 498, 428, 3579, 51080], "temperature": 0.0, "avg_logprob": -0.13023023307323456, "compression_ratio": 1.6588628762541806, "no_speech_prob": 0.020212309435009956}, {"id": 91, "seek": 51040, "start": 524.72, "end": 528.56, "text": " sounds weird when you say it out loud, then you're probably writing it the wrong way. But these guys", "tokens": [51080, 3263, 3657, 562, 291, 584, 309, 484, 6588, 11, 550, 291, 434, 1391, 3579, 309, 264, 2085, 636, 13, 583, 613, 1074, 51272], "temperature": 0.0, "avg_logprob": -0.13023023307323456, "compression_ratio": 1.6588628762541806, "no_speech_prob": 0.020212309435009956}, {"id": 92, "seek": 51040, "start": 528.56, "end": 534.72, "text": " have written it really well. Now they've essentially created an abstraction or a blueprint, as they call", "tokens": [51272, 362, 3720, 309, 534, 731, 13, 823, 436, 600, 4476, 2942, 364, 37765, 420, 257, 35868, 11, 382, 436, 818, 51580], "temperature": 0.0, "avg_logprob": -0.13023023307323456, "compression_ratio": 1.6588628762541806, "no_speech_prob": 0.020212309435009956}, {"id": 93, "seek": 53472, "start": 534.8000000000001, "end": 538.96, "text": " it, which prototypically describes all of the deep learning architectures,", "tokens": [50368, 309, 11, 597, 46219, 79, 984, 15626, 439, 295, 264, 2452, 2539, 6331, 1303, 11, 50576], "temperature": 0.0, "avg_logprob": -0.07211910509595684, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.1786259114742279}, {"id": 94, "seek": 53472, "start": 538.96, "end": 544.5600000000001, "text": " the geometrical priors that they have described so far. They don't prescribe a specific architecture,", "tokens": [50576, 264, 12956, 15888, 1790, 830, 300, 436, 362, 7619, 370, 1400, 13, 814, 500, 380, 49292, 257, 2685, 9482, 11, 50856], "temperature": 0.0, "avg_logprob": -0.07211910509595684, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.1786259114742279}, {"id": 95, "seek": 53472, "start": 544.5600000000001, "end": 550.0, "text": " but rather a series of necessary conditions. The book provides a mathematical framework to study", "tokens": [50856, 457, 2831, 257, 2638, 295, 4818, 4487, 13, 440, 1446, 6417, 257, 18894, 8388, 281, 2979, 51128], "temperature": 0.0, "avg_logprob": -0.07211910509595684, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.1786259114742279}, {"id": 96, "seek": 53472, "start": 550.0, "end": 556.32, "text": " this field. And it's essentially a mindset on, you know, how to build new architectures. It gives", "tokens": [51128, 341, 2519, 13, 400, 309, 311, 4476, 257, 12543, 322, 11, 291, 458, 11, 577, 281, 1322, 777, 6331, 1303, 13, 467, 2709, 51444], "temperature": 0.0, "avg_logprob": -0.07211910509595684, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.1786259114742279}, {"id": 97, "seek": 53472, "start": 556.32, "end": 563.36, "text": " constructive, you know, procedures to incorporate prior physical knowledge into neural architectures.", "tokens": [51444, 30223, 11, 291, 458, 11, 13846, 281, 16091, 4059, 4001, 3601, 666, 18161, 6331, 1303, 13, 51796], "temperature": 0.0, "avg_logprob": -0.07211910509595684, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.1786259114742279}, {"id": 98, "seek": 56336, "start": 563.36, "end": 569.12, "text": " And it provides a principled way to build future architectures, which have not yet been invented.", "tokens": [50364, 400, 309, 6417, 257, 3681, 15551, 636, 281, 1322, 2027, 6331, 1303, 11, 597, 362, 406, 1939, 668, 14479, 13, 50652], "temperature": 0.0, "avg_logprob": -0.06936924934387206, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.00044311911915428936}, {"id": 99, "seek": 56336, "start": 569.12, "end": 573.52, "text": " The researchers have also recently released a series of 12 brilliant lectures", "tokens": [50652, 440, 10309, 362, 611, 3938, 4736, 257, 2638, 295, 2272, 10248, 16564, 50872], "temperature": 0.0, "avg_logprob": -0.06936924934387206, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.00044311911915428936}, {"id": 100, "seek": 56336, "start": 573.52, "end": 576.64, "text": " on all of the material in the book. And I've linked these in the video description.", "tokens": [50872, 322, 439, 295, 264, 2527, 294, 264, 1446, 13, 400, 286, 600, 9408, 613, 294, 264, 960, 3855, 13, 51028], "temperature": 0.0, "avg_logprob": -0.06936924934387206, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.00044311911915428936}, {"id": 101, "seek": 56336, "start": 577.36, "end": 582.24, "text": " Now, what are the core domains of geometric deep learning? So in geometric deep learning,", "tokens": [51064, 823, 11, 437, 366, 264, 4965, 25514, 295, 33246, 2452, 2539, 30, 407, 294, 33246, 2452, 2539, 11, 51308], "temperature": 0.0, "avg_logprob": -0.06936924934387206, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.00044311911915428936}, {"id": 102, "seek": 56336, "start": 582.24, "end": 588.72, "text": " the data lives on a domain. This domain is a set. It might have additional structure,", "tokens": [51308, 264, 1412, 2909, 322, 257, 9274, 13, 639, 9274, 307, 257, 992, 13, 467, 1062, 362, 4497, 3877, 11, 51632], "temperature": 0.0, "avg_logprob": -0.06936924934387206, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.00044311911915428936}, {"id": 103, "seek": 58872, "start": 588.72, "end": 593.2, "text": " like a neighborhood in a graph, or it might have a metric such as, you know, what's the", "tokens": [50364, 411, 257, 7630, 294, 257, 4295, 11, 420, 309, 1062, 362, 257, 20678, 1270, 382, 11, 291, 458, 11, 437, 311, 264, 50588], "temperature": 0.0, "avg_logprob": -0.09300125969780816, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0012836643727496266}, {"id": 104, "seek": 58872, "start": 593.2, "end": 598.64, "text": " distance between two points in the set. But most of the time, the data isn't the domain itself.", "tokens": [50588, 4560, 1296, 732, 2793, 294, 264, 992, 13, 583, 881, 295, 264, 565, 11, 264, 1412, 1943, 380, 264, 9274, 2564, 13, 50860], "temperature": 0.0, "avg_logprob": -0.09300125969780816, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0012836643727496266}, {"id": 105, "seek": 58872, "start": 598.64, "end": 604.8000000000001, "text": " It's a representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries.", "tokens": [50860, 467, 311, 257, 10290, 420, 309, 311, 257, 6358, 11, 597, 307, 322, 257, 19914, 4290, 1901, 13, 961, 311, 751, 466, 14232, 302, 2244, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09300125969780816, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0012836643727496266}, {"id": 106, "seek": 58872, "start": 604.8000000000001, "end": 609.9200000000001, "text": " Symmetries are really important to understand this framework. So a symmetry of an object", "tokens": [51168, 3902, 2174, 302, 2244, 366, 534, 1021, 281, 1223, 341, 8388, 13, 407, 257, 25440, 295, 364, 2657, 51424], "temperature": 0.0, "avg_logprob": -0.09300125969780816, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0012836643727496266}, {"id": 107, "seek": 58872, "start": 609.9200000000001, "end": 615.6, "text": " is simply a transformation of that object, which leaves it unchanged. Now, there are", "tokens": [51424, 307, 2935, 257, 9887, 295, 300, 2657, 11, 597, 5510, 309, 44553, 13, 823, 11, 456, 366, 51708], "temperature": 0.0, "avg_logprob": -0.09300125969780816, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0012836643727496266}, {"id": 108, "seek": 61560, "start": 615.6, "end": 619.76, "text": " many different types of symmetries in deep learning. I mean, for example, there are symmetries", "tokens": [50364, 867, 819, 3467, 295, 14232, 302, 2244, 294, 2452, 2539, 13, 286, 914, 11, 337, 1365, 11, 456, 366, 14232, 302, 2244, 50572], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 109, "seek": 61560, "start": 619.76, "end": 624.4, "text": " of the weights. If you take two neurons in a neural network, and you swap them,", "tokens": [50572, 295, 264, 17443, 13, 759, 291, 747, 732, 22027, 294, 257, 18161, 3209, 11, 293, 291, 18135, 552, 11, 50804], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 110, "seek": 61560, "start": 624.4, "end": 629.12, "text": " the neural network is still graph isomorphic. There are symmetries of the label function,", "tokens": [50804, 264, 18161, 3209, 307, 920, 4295, 307, 32702, 299, 13, 821, 366, 14232, 302, 2244, 295, 264, 7645, 2445, 11, 51040], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 111, "seek": 61560, "start": 629.12, "end": 633.84, "text": " which means that an image is still a dog, even if you apply a rotation transformation to it.", "tokens": [51040, 597, 1355, 300, 364, 3256, 307, 920, 257, 3000, 11, 754, 498, 291, 3079, 257, 12447, 9887, 281, 309, 13, 51276], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 112, "seek": 61560, "start": 634.64, "end": 639.6, "text": " Actually, if we knew all of the symmetries of a certain class, we would only need one labeled", "tokens": [51316, 5135, 11, 498, 321, 2586, 439, 295, 264, 14232, 302, 2244, 295, 257, 1629, 1508, 11, 321, 576, 787, 643, 472, 21335, 51564], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 113, "seek": 61560, "start": 639.6, "end": 643.76, "text": " example, right, because we would recognize any other examples that you give it as kind of", "tokens": [51564, 1365, 11, 558, 11, 570, 321, 576, 5521, 604, 661, 5110, 300, 291, 976, 309, 382, 733, 295, 51772], "temperature": 0.0, "avg_logprob": -0.05116064990008319, "compression_ratio": 1.8527397260273972, "no_speech_prob": 0.00012709235306829214}, {"id": 114, "seek": 64376, "start": 643.76, "end": 648.3199999999999, "text": " semantically equivalent transformations. But we can't do that, right, because the learning problem", "tokens": [50364, 4361, 49505, 10344, 34852, 13, 583, 321, 393, 380, 360, 300, 11, 558, 11, 570, 264, 2539, 1154, 50592], "temperature": 0.0, "avg_logprob": -0.0691676139831543, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0005189548246562481}, {"id": 115, "seek": 64376, "start": 648.3199999999999, "end": 653.52, "text": " is difficult, which means we don't actually know all of the symmetries in advance. Now,", "tokens": [50592, 307, 2252, 11, 597, 1355, 321, 500, 380, 767, 458, 439, 295, 264, 14232, 302, 2244, 294, 7295, 13, 823, 11, 50852], "temperature": 0.0, "avg_logprob": -0.0691676139831543, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0005189548246562481}, {"id": 116, "seek": 64376, "start": 653.52, "end": 658.48, "text": " in the context of geometric deep learning, we talk about symmetries of the core structured", "tokens": [50852, 294, 264, 4319, 295, 33246, 2452, 2539, 11, 321, 751, 466, 14232, 302, 2244, 295, 264, 4965, 18519, 51100], "temperature": 0.0, "avg_logprob": -0.0691676139831543, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0005189548246562481}, {"id": 117, "seek": 64376, "start": 658.48, "end": 664.96, "text": " geometric domains that we're interested in. So grids or graphs, for example, a symmetry is any", "tokens": [51100, 33246, 25514, 300, 321, 434, 3102, 294, 13, 407, 677, 3742, 420, 24877, 11, 337, 1365, 11, 257, 25440, 307, 604, 51424], "temperature": 0.0, "avg_logprob": -0.0691676139831543, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0005189548246562481}, {"id": 118, "seek": 64376, "start": 664.96, "end": 670.24, "text": " transformation which preserves the structure of the geometric domain that the signal lives on.", "tokens": [51424, 9887, 597, 1183, 9054, 264, 3877, 295, 264, 33246, 9274, 300, 264, 6358, 2909, 322, 13, 51688], "temperature": 0.0, "avg_logprob": -0.0691676139831543, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0005189548246562481}, {"id": 119, "seek": 67024, "start": 670.24, "end": 676.5600000000001, "text": " So for example, permutations of a set preserves the set membership or Euclidean transformations", "tokens": [50364, 407, 337, 1365, 11, 4784, 325, 763, 295, 257, 992, 1183, 9054, 264, 992, 16560, 420, 462, 1311, 31264, 282, 34852, 50680], "temperature": 0.0, "avg_logprob": -0.06733952022734142, "compression_ratio": 1.9116465863453815, "no_speech_prob": 0.0006260736845433712}, {"id": 120, "seek": 67024, "start": 676.5600000000001, "end": 682.8, "text": " like rotations or reflections preserve distances and angles. There are a few rules to remember,", "tokens": [50680, 411, 44796, 420, 30679, 15665, 22182, 293, 14708, 13, 821, 366, 257, 1326, 4474, 281, 1604, 11, 50992], "temperature": 0.0, "avg_logprob": -0.06733952022734142, "compression_ratio": 1.9116465863453815, "no_speech_prob": 0.0006260736845433712}, {"id": 121, "seek": 67024, "start": 682.8, "end": 689.2, "text": " because the way we deal with this paradigm is we talk about how composable those symmetries are.", "tokens": [50992, 570, 264, 636, 321, 2028, 365, 341, 24709, 307, 321, 751, 466, 577, 10199, 712, 729, 14232, 302, 2244, 366, 13, 51312], "temperature": 0.0, "avg_logprob": -0.06733952022734142, "compression_ratio": 1.9116465863453815, "no_speech_prob": 0.0006260736845433712}, {"id": 122, "seek": 67024, "start": 689.2, "end": 694.16, "text": " The identity transformation is always a symmetry. Composing a symmetry transformation is always", "tokens": [51312, 440, 6575, 9887, 307, 1009, 257, 25440, 13, 6620, 6110, 257, 25440, 9887, 307, 1009, 51560], "temperature": 0.0, "avg_logprob": -0.06733952022734142, "compression_ratio": 1.9116465863453815, "no_speech_prob": 0.0006260736845433712}, {"id": 123, "seek": 67024, "start": 694.16, "end": 699.2, "text": " a symmetry. The inverse of a symmetry is always a symmetry. We can formulate this with this", "tokens": [51560, 257, 25440, 13, 440, 17340, 295, 257, 25440, 307, 1009, 257, 25440, 13, 492, 393, 47881, 341, 365, 341, 51812], "temperature": 0.0, "avg_logprob": -0.06733952022734142, "compression_ratio": 1.9116465863453815, "no_speech_prob": 0.0006260736845433712}, {"id": 124, "seek": 69920, "start": 699.2, "end": 704.1600000000001, "text": " mathematically abstract notion of a group. Group theory in mathematics is fascinating,", "tokens": [50364, 44003, 12649, 10710, 295, 257, 1594, 13, 10500, 5261, 294, 18666, 307, 10343, 11, 50612], "temperature": 0.0, "avg_logprob": -0.05611428829154583, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00040444073965772986}, {"id": 125, "seek": 69920, "start": 704.1600000000001, "end": 710.1600000000001, "text": " because it concerns only with how elements compose with each other, not what they actually are.", "tokens": [50612, 570, 309, 7389, 787, 365, 577, 4959, 35925, 365, 1184, 661, 11, 406, 437, 436, 767, 366, 13, 50912], "temperature": 0.0, "avg_logprob": -0.05611428829154583, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00040444073965772986}, {"id": 126, "seek": 69920, "start": 710.1600000000001, "end": 713.9200000000001, "text": " So different kinds of objects may have the same symmetry group. For example,", "tokens": [50912, 407, 819, 3685, 295, 6565, 815, 362, 264, 912, 25440, 1594, 13, 1171, 1365, 11, 51100], "temperature": 0.0, "avg_logprob": -0.05611428829154583, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00040444073965772986}, {"id": 127, "seek": 69920, "start": 713.9200000000001, "end": 721.12, "text": " the group of rotational and reflection symmetries of a triangle is the same as the group of permutations", "tokens": [51100, 264, 1594, 295, 45420, 293, 12914, 14232, 302, 2244, 295, 257, 13369, 307, 264, 912, 382, 264, 1594, 295, 4784, 325, 763, 51460], "temperature": 0.0, "avg_logprob": -0.05611428829154583, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00040444073965772986}, {"id": 128, "seek": 69920, "start": 721.12, "end": 727.2, "text": " of sequences of three elements. So let's talk about the blueprint itself. The blueprint has", "tokens": [51460, 295, 22978, 295, 1045, 4959, 13, 407, 718, 311, 751, 466, 264, 35868, 2564, 13, 440, 35868, 575, 51764], "temperature": 0.0, "avg_logprob": -0.05611428829154583, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00040444073965772986}, {"id": 129, "seek": 72720, "start": 727.2800000000001, "end": 736.0, "text": " three core principles. Symmetry, scale separation and geometric stability. In machine learning,", "tokens": [50368, 1045, 4965, 9156, 13, 3902, 2174, 9889, 11, 4373, 14634, 293, 33246, 11826, 13, 682, 3479, 2539, 11, 50804], "temperature": 0.0, "avg_logprob": -0.0849400395932405, "compression_ratio": 1.75, "no_speech_prob": 0.0010000913171097636}, {"id": 130, "seek": 72720, "start": 736.0, "end": 741.6800000000001, "text": " multi-scale representations and local invariance are the fundamental mathematical principles", "tokens": [50804, 4825, 12, 20033, 33358, 293, 2654, 33270, 719, 366, 264, 8088, 18894, 9156, 51088], "temperature": 0.0, "avg_logprob": -0.0849400395932405, "compression_ratio": 1.75, "no_speech_prob": 0.0010000913171097636}, {"id": 131, "seek": 72720, "start": 741.6800000000001, "end": 746.48, "text": " underpinning the efficiency of convolutional neural networks and graph neural networks.", "tokens": [51088, 833, 17836, 773, 264, 10493, 295, 45216, 304, 18161, 9590, 293, 4295, 18161, 9590, 13, 51328], "temperature": 0.0, "avg_logprob": -0.0849400395932405, "compression_ratio": 1.75, "no_speech_prob": 0.0010000913171097636}, {"id": 132, "seek": 72720, "start": 746.48, "end": 751.6800000000001, "text": " They are typically implemented in the form of local pooling in some sense. Now these principles", "tokens": [51328, 814, 366, 5850, 12270, 294, 264, 1254, 295, 2654, 7005, 278, 294, 512, 2020, 13, 823, 613, 9156, 51588], "temperature": 0.0, "avg_logprob": -0.0849400395932405, "compression_ratio": 1.75, "no_speech_prob": 0.0010000913171097636}, {"id": 133, "seek": 72720, "start": 751.6800000000001, "end": 756.32, "text": " give us a very general blueprint of geometric deep learning that can be recognized in the", "tokens": [51588, 976, 505, 257, 588, 2674, 35868, 295, 33246, 2452, 2539, 300, 393, 312, 9823, 294, 264, 51820], "temperature": 0.0, "avg_logprob": -0.0849400395932405, "compression_ratio": 1.75, "no_speech_prob": 0.0010000913171097636}, {"id": 134, "seek": 75632, "start": 756.32, "end": 762.96, "text": " majority of popular deep neural network architectures. A typical design consists of a sequence of", "tokens": [50364, 6286, 295, 3743, 2452, 18161, 3209, 6331, 1303, 13, 316, 7476, 1715, 14689, 295, 257, 8310, 295, 50696], "temperature": 0.0, "avg_logprob": -0.10007037591496738, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00017380030476488173}, {"id": 135, "seek": 75632, "start": 762.96, "end": 769.44, "text": " locally-equivariant layers. I mean, think of the convolution layers in a CNN, then a pooling or", "tokens": [50696, 16143, 12, 12816, 592, 3504, 394, 7914, 13, 286, 914, 11, 519, 295, 264, 45216, 7914, 294, 257, 24859, 11, 550, 257, 7005, 278, 420, 51020], "temperature": 0.0, "avg_logprob": -0.10007037591496738, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00017380030476488173}, {"id": 136, "seek": 75632, "start": 769.44, "end": 775.2, "text": " a coarsening layer. So you recognize those in CNNs as well. And finally, followed by a globally", "tokens": [51020, 257, 598, 685, 4559, 4583, 13, 407, 291, 5521, 729, 294, 24859, 82, 382, 731, 13, 400, 2721, 11, 6263, 538, 257, 18958, 51308], "temperature": 0.0, "avg_logprob": -0.10007037591496738, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00017380030476488173}, {"id": 137, "seek": 75632, "start": 775.2, "end": 779.2800000000001, "text": " invariant pooling layer. So that might be your classification head. Now these building blocks", "tokens": [51308, 33270, 394, 7005, 278, 4583, 13, 407, 300, 1062, 312, 428, 21538, 1378, 13, 823, 613, 2390, 8474, 51512], "temperature": 0.0, "avg_logprob": -0.10007037591496738, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00017380030476488173}, {"id": 138, "seek": 75632, "start": 779.2800000000001, "end": 784.8800000000001, "text": " provide a rich approximation space, which have prescribed invariance and stability properties", "tokens": [51512, 2893, 257, 4593, 28023, 1901, 11, 597, 362, 29099, 33270, 719, 293, 11826, 7221, 51792], "temperature": 0.0, "avg_logprob": -0.10007037591496738, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00017380030476488173}, {"id": 139, "seek": 78488, "start": 784.88, "end": 789.92, "text": " by combining them together into a scheme that these researchers refer to as the geometric", "tokens": [50364, 538, 21928, 552, 1214, 666, 257, 12232, 300, 613, 10309, 2864, 281, 382, 264, 33246, 50616], "temperature": 0.0, "avg_logprob": -0.08701001273261176, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0007776000420562923}, {"id": 140, "seek": 78488, "start": 789.92, "end": 796.8, "text": " deep learning blueprint. Now the researchers also introduced the concept of geometric stability,", "tokens": [50616, 2452, 2539, 35868, 13, 823, 264, 10309, 611, 7268, 264, 3410, 295, 33246, 11826, 11, 50960], "temperature": 0.0, "avg_logprob": -0.08701001273261176, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0007776000420562923}, {"id": 141, "seek": 78488, "start": 796.8, "end": 802.56, "text": " which extends the notion of group invariance and equivalence to approximate symmetry or", "tokens": [50960, 597, 26448, 264, 10710, 295, 1594, 33270, 719, 293, 9052, 655, 281, 30874, 25440, 420, 51248], "temperature": 0.0, "avg_logprob": -0.08701001273261176, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0007776000420562923}, {"id": 142, "seek": 78488, "start": 802.56, "end": 808.24, "text": " transformations around the group. They quantify this in some sense by looking at a metric space", "tokens": [51248, 34852, 926, 264, 1594, 13, 814, 40421, 341, 294, 512, 2020, 538, 1237, 412, 257, 20678, 1901, 51532], "temperature": 0.0, "avg_logprob": -0.08701001273261176, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0007776000420562923}, {"id": 143, "seek": 80824, "start": 808.24, "end": 813.2, "text": " between the transformations themselves. This is Professor Michael Bronstein.", "tokens": [50364, 1296, 264, 34852, 2969, 13, 639, 307, 8419, 5116, 19544, 9089, 13, 50612], "temperature": 0.0, "avg_logprob": -0.07122040439296413, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.017412690445780754}, {"id": 144, "seek": 80824, "start": 813.84, "end": 819.6, "text": " The problem is that traditional machine learning techniques work well with images or audio,", "tokens": [50644, 440, 1154, 307, 300, 5164, 3479, 2539, 7512, 589, 731, 365, 5267, 420, 6278, 11, 50932], "temperature": 0.0, "avg_logprob": -0.07122040439296413, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.017412690445780754}, {"id": 145, "seek": 80824, "start": 820.16, "end": 825.6, "text": " but they are not designed to deal with network structure data. In order to address this challenge,", "tokens": [50960, 457, 436, 366, 406, 4761, 281, 2028, 365, 3209, 3877, 1412, 13, 682, 1668, 281, 2985, 341, 3430, 11, 51232], "temperature": 0.0, "avg_logprob": -0.07122040439296413, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.017412690445780754}, {"id": 146, "seek": 80824, "start": 826.16, "end": 833.36, "text": " we've developed a new framework that we call geometric deep learning. It allowed us to learn", "tokens": [51260, 321, 600, 4743, 257, 777, 8388, 300, 321, 818, 33246, 2452, 2539, 13, 467, 4350, 505, 281, 1466, 51620], "temperature": 0.0, "avg_logprob": -0.07122040439296413, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.017412690445780754}, {"id": 147, "seek": 83336, "start": 833.36, "end": 840.64, "text": " the network effects of clinically approved drugs and to predict anti-cancer drug-like", "tokens": [50364, 264, 3209, 5065, 295, 48392, 10826, 7766, 293, 281, 6069, 6061, 12, 7035, 1776, 4110, 12, 4092, 50728], "temperature": 0.0, "avg_logprob": -0.09596216201782226, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.00460283737629652}, {"id": 148, "seek": 83336, "start": 840.64, "end": 848.24, "text": " properties of other molecules. For example, molecules contained in food. Neural networks", "tokens": [50728, 7221, 295, 661, 13093, 13, 1171, 1365, 11, 13093, 16212, 294, 1755, 13, 1734, 1807, 9590, 51108], "temperature": 0.0, "avg_logprob": -0.09596216201782226, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.00460283737629652}, {"id": 149, "seek": 83336, "start": 848.24, "end": 853.52, "text": " have exploded, leading to several success stories in industrial applications. And I think it's", "tokens": [51108, 362, 27049, 11, 5775, 281, 2940, 2245, 3676, 294, 9987, 5821, 13, 400, 286, 519, 309, 311, 51372], "temperature": 0.0, "avg_logprob": -0.09596216201782226, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.00460283737629652}, {"id": 150, "seek": 83336, "start": 853.52, "end": 858.64, "text": " quite indicative that last year two major biological journals featured geometric deep learning papers on", "tokens": [51372, 1596, 47513, 300, 1036, 1064, 732, 2563, 13910, 29621, 13822, 33246, 2452, 2539, 10577, 322, 51628], "temperature": 0.0, "avg_logprob": -0.09596216201782226, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.00460283737629652}, {"id": 151, "seek": 85864, "start": 858.64, "end": 863.68, "text": " their cover, which means that it has already become mainstream and possibly will lead to new", "tokens": [50364, 641, 2060, 11, 597, 1355, 300, 309, 575, 1217, 1813, 15960, 293, 6264, 486, 1477, 281, 777, 50616], "temperature": 0.0, "avg_logprob": -0.11093521118164062, "compression_ratio": 1.5039370078740157, "no_speech_prob": 0.018366029486060143}, {"id": 152, "seek": 85864, "start": 863.68, "end": 869.1999999999999, "text": " exciting results in fundamental sciences. The book I hold is called The Role to Reality. It's", "tokens": [50616, 4670, 3542, 294, 8088, 17677, 13, 440, 1446, 286, 1797, 307, 1219, 440, 3101, 306, 281, 33822, 13, 467, 311, 50892], "temperature": 0.0, "avg_logprob": -0.11093521118164062, "compression_ratio": 1.5039370078740157, "no_speech_prob": 0.018366029486060143}, {"id": 153, "seek": 85864, "start": 869.1999999999999, "end": 876.3199999999999, "text": " written by a British mathematician and recent Nobel laureate, Roger Penrose, a professor at Oxford,", "tokens": [50892, 3720, 538, 257, 6221, 48281, 293, 5162, 24611, 49469, 473, 11, 17666, 10571, 37841, 11, 257, 8304, 412, 24786, 11, 51248], "temperature": 0.0, "avg_logprob": -0.11093521118164062, "compression_ratio": 1.5039370078740157, "no_speech_prob": 0.018366029486060143}, {"id": 154, "seek": 85864, "start": 876.88, "end": 884.48, "text": " and it's really probably one of the most complete attempts to write and describe modern physics", "tokens": [51276, 293, 309, 311, 534, 1391, 472, 295, 264, 881, 3566, 15257, 281, 2464, 293, 6786, 4363, 10649, 51656], "temperature": 0.0, "avg_logprob": -0.11093521118164062, "compression_ratio": 1.5039370078740157, "no_speech_prob": 0.018366029486060143}, {"id": 155, "seek": 88448, "start": 884.48, "end": 890.96, "text": " and its mathematical underpinning. And you can see it's very heavy. But if I were to compress the", "tokens": [50364, 293, 1080, 18894, 833, 17836, 773, 13, 400, 291, 393, 536, 309, 311, 588, 4676, 13, 583, 498, 286, 645, 281, 14778, 264, 50688], "temperature": 0.0, "avg_logprob": -0.0854852278153975, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.025763921439647675}, {"id": 156, "seek": 88448, "start": 890.96, "end": 896.96, "text": " thousand-plus pages of this book into just a single concept, I can capture it in one word.", "tokens": [50688, 4714, 12, 18954, 7183, 295, 341, 1446, 666, 445, 257, 2167, 3410, 11, 286, 393, 7983, 309, 294, 472, 1349, 13, 50988], "temperature": 0.0, "avg_logprob": -0.0854852278153975, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.025763921439647675}, {"id": 157, "seek": 88448, "start": 896.96, "end": 903.44, "text": " And this is symmetry. And symmetry is really fundamental concept and fundamental idea that", "tokens": [50988, 400, 341, 307, 25440, 13, 400, 25440, 307, 534, 8088, 3410, 293, 8088, 1558, 300, 51312], "temperature": 0.0, "avg_logprob": -0.0854852278153975, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.025763921439647675}, {"id": 158, "seek": 88448, "start": 903.44, "end": 912.0, "text": " underpins all modern physics as we know it. So, for example, the standard model of particle physics", "tokens": [51312, 833, 79, 1292, 439, 4363, 10649, 382, 321, 458, 309, 13, 407, 11, 337, 1365, 11, 264, 3832, 2316, 295, 12359, 10649, 51740], "temperature": 0.0, "avg_logprob": -0.0854852278153975, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.025763921439647675}, {"id": 159, "seek": 91200, "start": 912.0, "end": 917.92, "text": " can entirely be derived from the considerations of symmetry. And that's the kind of idea that", "tokens": [50364, 393, 7696, 312, 18949, 490, 264, 24070, 295, 25440, 13, 400, 300, 311, 264, 733, 295, 1558, 300, 50660], "temperature": 0.0, "avg_logprob": -0.060914275622127034, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002328506438061595}, {"id": 160, "seek": 91200, "start": 917.92, "end": 925.36, "text": " we try to use in deep learning to derive and create new neural network architectures entirely from", "tokens": [50660, 321, 853, 281, 764, 294, 2452, 2539, 281, 28446, 293, 1884, 777, 18161, 3209, 6331, 1303, 7696, 490, 51032], "temperature": 0.0, "avg_logprob": -0.060914275622127034, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002328506438061595}, {"id": 161, "seek": 91200, "start": 925.36, "end": 931.04, "text": " fundamental concepts and fundamental principles of symmetry. In the past decade, deep learning has", "tokens": [51032, 8088, 10392, 293, 8088, 9156, 295, 25440, 13, 682, 264, 1791, 10378, 11, 2452, 2539, 575, 51316], "temperature": 0.0, "avg_logprob": -0.060914275622127034, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002328506438061595}, {"id": 162, "seek": 91200, "start": 931.04, "end": 935.44, "text": " brought a revolution in data science and made possible many tasks previously thought to be", "tokens": [51316, 3038, 257, 8894, 294, 1412, 3497, 293, 1027, 1944, 867, 9608, 8046, 1194, 281, 312, 51536], "temperature": 0.0, "avg_logprob": -0.060914275622127034, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002328506438061595}, {"id": 163, "seek": 91200, "start": 935.44, "end": 941.2, "text": " unreached. On the other hand, we now have a zoo of different neural network architectures for", "tokens": [51536, 20584, 15095, 13, 1282, 264, 661, 1011, 11, 321, 586, 362, 257, 25347, 295, 819, 18161, 3209, 6331, 1303, 337, 51824], "temperature": 0.0, "avg_logprob": -0.060914275622127034, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002328506438061595}, {"id": 164, "seek": 94120, "start": 941.2, "end": 946.72, "text": " different types of data, but few unifying principles. The authors also point out that", "tokens": [50364, 819, 3467, 295, 1412, 11, 457, 1326, 517, 5489, 9156, 13, 440, 16552, 611, 935, 484, 300, 50640], "temperature": 0.0, "avg_logprob": -0.0651968755220112, "compression_ratio": 1.7, "no_speech_prob": 0.00021349580492824316}, {"id": 165, "seek": 94120, "start": 946.72, "end": 953.2, "text": " different geometric deep learning methods differ in their choice of domain or symmetry group or the", "tokens": [50640, 819, 33246, 2452, 2539, 7150, 743, 294, 641, 3922, 295, 9274, 420, 25440, 1594, 420, 264, 50964], "temperature": 0.0, "avg_logprob": -0.0651968755220112, "compression_ratio": 1.7, "no_speech_prob": 0.00021349580492824316}, {"id": 166, "seek": 94120, "start": 953.2, "end": 958.4000000000001, "text": " implementation specific details of those building blocks that we spoke about. But many of the deep", "tokens": [50964, 11420, 2685, 4365, 295, 729, 2390, 8474, 300, 321, 7179, 466, 13, 583, 867, 295, 264, 2452, 51224], "temperature": 0.0, "avg_logprob": -0.0651968755220112, "compression_ratio": 1.7, "no_speech_prob": 0.00021349580492824316}, {"id": 167, "seek": 94120, "start": 958.4000000000001, "end": 964.72, "text": " learning architectures currently in use fall into this scheme and can thus be derived from common", "tokens": [51224, 2539, 6331, 1303, 4362, 294, 764, 2100, 666, 341, 12232, 293, 393, 8807, 312, 18949, 490, 2689, 51540], "temperature": 0.0, "avg_logprob": -0.0651968755220112, "compression_ratio": 1.7, "no_speech_prob": 0.00021349580492824316}, {"id": 168, "seek": 94120, "start": 964.72, "end": 969.6800000000001, "text": " geometrical principles. As a consequence, it is difficult to understand the relations between", "tokens": [51540, 12956, 15888, 9156, 13, 1018, 257, 18326, 11, 309, 307, 2252, 281, 1223, 264, 2299, 1296, 51788], "temperature": 0.0, "avg_logprob": -0.0651968755220112, "compression_ratio": 1.7, "no_speech_prob": 0.00021349580492824316}, {"id": 169, "seek": 96968, "start": 969.68, "end": 975.1999999999999, "text": " different methods, which inevitably leads to the reinvention and rebranding of the same concepts.", "tokens": [50364, 819, 7150, 11, 597, 28171, 6689, 281, 264, 6561, 6411, 293, 12970, 3699, 278, 295, 264, 912, 10392, 13, 50640], "temperature": 0.0, "avg_logprob": -0.08265640150825933, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006704069674015045}, {"id": 170, "seek": 96968, "start": 975.1999999999999, "end": 980.3199999999999, "text": " So, we need some form of geometric unification in the spirit of the Erlangen program that I call", "tokens": [50640, 407, 11, 321, 643, 512, 1254, 295, 33246, 517, 3774, 294, 264, 3797, 295, 264, 3300, 75, 10784, 1461, 300, 286, 818, 50896], "temperature": 0.0, "avg_logprob": -0.08265640150825933, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006704069674015045}, {"id": 171, "seek": 96968, "start": 980.3199999999999, "end": 985.52, "text": " geometric deep learning. It serves two purposes. First, to provide a common mathematical framework", "tokens": [50896, 33246, 2452, 2539, 13, 467, 13451, 732, 9932, 13, 2386, 11, 281, 2893, 257, 2689, 18894, 8388, 51156], "temperature": 0.0, "avg_logprob": -0.08265640150825933, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006704069674015045}, {"id": 172, "seek": 96968, "start": 985.52, "end": 990.64, "text": " to derive the most successful neural network architectures. And second, to give a constructive", "tokens": [51156, 281, 28446, 264, 881, 4406, 18161, 3209, 6331, 1303, 13, 400, 1150, 11, 281, 976, 257, 30223, 51412], "temperature": 0.0, "avg_logprob": -0.08265640150825933, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006704069674015045}, {"id": 173, "seek": 96968, "start": 990.64, "end": 995.52, "text": " procedure to build future architectures in a principled way. This is a very general design that", "tokens": [51412, 10747, 281, 1322, 2027, 6331, 1303, 294, 257, 3681, 15551, 636, 13, 639, 307, 257, 588, 2674, 1715, 300, 51656], "temperature": 0.0, "avg_logprob": -0.08265640150825933, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006704069674015045}, {"id": 174, "seek": 99552, "start": 995.52, "end": 999.76, "text": " can be applied to different types of geometric structures such as grids, homogeneous spaces", "tokens": [50364, 393, 312, 6456, 281, 819, 3467, 295, 33246, 9227, 1270, 382, 677, 3742, 11, 42632, 7673, 50576], "temperature": 0.0, "avg_logprob": -0.1094792876580749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0002594989782664925}, {"id": 175, "seek": 99552, "start": 999.76, "end": 1004.96, "text": " with global transformation groups, crafts and manifolds where we have global isometry invariants", "tokens": [50576, 365, 4338, 9887, 3935, 11, 27831, 293, 8173, 31518, 689, 321, 362, 4338, 307, 34730, 33270, 1719, 50836], "temperature": 0.0, "avg_logprob": -0.1094792876580749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0002594989782664925}, {"id": 176, "seek": 99552, "start": 1004.96, "end": 1011.12, "text": " as well as local gauge symmetries. We call these the 5G of geometric deep learning. The", "tokens": [50836, 382, 731, 382, 2654, 17924, 14232, 302, 2244, 13, 492, 818, 613, 264, 1025, 38, 295, 33246, 2452, 2539, 13, 440, 51144], "temperature": 0.0, "avg_logprob": -0.1094792876580749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0002594989782664925}, {"id": 177, "seek": 99552, "start": 1011.12, "end": 1015.68, "text": " implementation of these principles leads to some of the most popular architectures that exist today", "tokens": [51144, 11420, 295, 613, 9156, 6689, 281, 512, 295, 264, 881, 3743, 6331, 1303, 300, 2514, 965, 51372], "temperature": 0.0, "avg_logprob": -0.1094792876580749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0002594989782664925}, {"id": 178, "seek": 99552, "start": 1015.68, "end": 1020.88, "text": " in deep learning, such as convolutional networks emerging from translational symmetry, craft neural", "tokens": [51372, 294, 2452, 2539, 11, 1270, 382, 45216, 304, 9590, 14989, 490, 5105, 1478, 25440, 11, 8448, 18161, 51632], "temperature": 0.0, "avg_logprob": -0.1094792876580749, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.0002594989782664925}, {"id": 179, "seek": 102088, "start": 1020.88, "end": 1026.24, "text": " networks, deep sets and transformers implementing permutation invariants and intrinsic mesh CNNs", "tokens": [50364, 9590, 11, 2452, 6352, 293, 4088, 433, 18114, 4784, 11380, 33270, 1719, 293, 35698, 17407, 24859, 82, 50632], "temperature": 0.0, "avg_logprob": -0.12367088794708252, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.0007906843675300479}, {"id": 180, "seek": 102088, "start": 1026.24, "end": 1031.76, "text": " using computer graphics and vision that can be derived from gauge symmetries. People are quite", "tokens": [50632, 1228, 3820, 11837, 293, 5201, 300, 393, 312, 18949, 490, 17924, 14232, 302, 2244, 13, 3432, 366, 1596, 50908], "temperature": 0.0, "avg_logprob": -0.12367088794708252, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.0007906843675300479}, {"id": 181, "seek": 102088, "start": 1031.76, "end": 1037.44, "text": " cynical about the interpolative nature of deep learning and I think that finding this structure,", "tokens": [50908, 46345, 466, 264, 44902, 1166, 3687, 295, 2452, 2539, 293, 286, 519, 300, 5006, 341, 3877, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12367088794708252, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.0007906843675300479}, {"id": 182, "seek": 102088, "start": 1037.44, "end": 1044.16, "text": " this deeper structure could allow us to extrapolate in a way which is significantly better than we", "tokens": [51192, 341, 7731, 3877, 727, 2089, 505, 281, 48224, 473, 294, 257, 636, 597, 307, 10591, 1101, 813, 321, 51528], "temperature": 0.0, "avg_logprob": -0.12367088794708252, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.0007906843675300479}, {"id": 183, "seek": 104416, "start": 1044.16, "end": 1050.64, "text": " can now. And I asked whether he thought deep learning could get us all the way to artificial", "tokens": [50364, 393, 586, 13, 400, 286, 2351, 1968, 415, 1194, 2452, 2539, 727, 483, 505, 439, 264, 636, 281, 11677, 50688], "temperature": 0.0, "avg_logprob": -0.0840156811934251, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.30944347381591797}, {"id": 184, "seek": 104416, "start": 1050.64, "end": 1055.76, "text": " general intelligence? It's a hard question because it has several terms that are not well defined.", "tokens": [50688, 2674, 7599, 30, 467, 311, 257, 1152, 1168, 570, 309, 575, 2940, 2115, 300, 366, 406, 731, 7642, 13, 50944], "temperature": 0.0, "avg_logprob": -0.0840156811934251, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.30944347381591797}, {"id": 185, "seek": 104416, "start": 1055.76, "end": 1060.24, "text": " What do you define by intelligence? So we don't understand what is human intelligence. Everybody", "tokens": [50944, 708, 360, 291, 6964, 538, 7599, 30, 407, 321, 500, 380, 1223, 437, 307, 1952, 7599, 13, 7646, 51168], "temperature": 0.0, "avg_logprob": -0.0840156811934251, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.30944347381591797}, {"id": 186, "seek": 104416, "start": 1060.24, "end": 1065.68, "text": " probably gives a different meaning to this term. So it's hard for me to even to define and quantify", "tokens": [51168, 1391, 2709, 257, 819, 3620, 281, 341, 1433, 13, 407, 309, 311, 1152, 337, 385, 281, 754, 281, 6964, 293, 40421, 51440], "temperature": 0.0, "avg_logprob": -0.0840156811934251, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.30944347381591797}, {"id": 187, "seek": 104416, "start": 1066.3200000000002, "end": 1071.44, "text": " artificial intelligence. I don't think that we necessarily need to emulate human intelligence", "tokens": [51472, 11677, 7599, 13, 286, 500, 380, 519, 300, 321, 4725, 643, 281, 45497, 1952, 7599, 51728], "temperature": 0.0, "avg_logprob": -0.0840156811934251, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.30944347381591797}, {"id": 188, "seek": 107144, "start": 1071.44, "end": 1077.44, "text": " and, as you mentioned, in the past we thought of artificial intelligence as being able to solve", "tokens": [50364, 293, 11, 382, 291, 2835, 11, 294, 264, 1791, 321, 1194, 295, 11677, 7599, 382, 885, 1075, 281, 5039, 50664], "temperature": 0.0, "avg_logprob": -0.11310160160064697, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.016674237325787544}, {"id": 189, "seek": 107144, "start": 1077.44, "end": 1081.68, "text": " certain tasks and it's a kind of a moving target. We thought of, I don't know, playing intelligent", "tokens": [50664, 1629, 9608, 293, 309, 311, 257, 733, 295, 257, 2684, 3779, 13, 492, 1194, 295, 11, 286, 500, 380, 458, 11, 2433, 13232, 50876], "temperature": 0.0, "avg_logprob": -0.11310160160064697, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.016674237325787544}, {"id": 190, "seek": 107144, "start": 1081.68, "end": 1086.88, "text": " games or perception of the visual world like computer vision or understanding and translating", "tokens": [50876, 2813, 420, 12860, 295, 264, 5056, 1002, 411, 3820, 5201, 420, 3701, 293, 35030, 51136], "temperature": 0.0, "avg_logprob": -0.11310160160064697, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.016674237325787544}, {"id": 191, "seek": 107144, "start": 1086.88, "end": 1092.48, "text": " language or even creativity and today we have machine learning systems that are able to address", "tokens": [51136, 2856, 420, 754, 12915, 293, 965, 321, 362, 3479, 2539, 3652, 300, 366, 1075, 281, 2985, 51416], "temperature": 0.0, "avg_logprob": -0.11310160160064697, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.016674237325787544}, {"id": 192, "seek": 107144, "start": 1092.48, "end": 1097.8400000000001, "text": " at least to some extent all of these tasks, sometimes even better than humans and are we there yet", "tokens": [51416, 412, 1935, 281, 512, 8396, 439, 295, 613, 9608, 11, 2171, 754, 1101, 813, 6255, 293, 366, 321, 456, 1939, 51684], "temperature": 0.0, "avg_logprob": -0.11310160160064697, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.016674237325787544}, {"id": 193, "seek": 109784, "start": 1098.8, "end": 1103.6799999999998, "text": " artificial intelligence? I don't think so. And probably artificial intelligence will look", "tokens": [50412, 11677, 7599, 30, 286, 500, 380, 519, 370, 13, 400, 1391, 11677, 7599, 486, 574, 50656], "temperature": 0.0, "avg_logprob": -0.12015443099172492, "compression_ratio": 1.8515625, "no_speech_prob": 0.011685908772051334}, {"id": 194, "seek": 109784, "start": 1103.6799999999998, "end": 1107.4399999999998, "text": " differently from human intelligence. It doesn't need to look like human intelligence. It's of", "tokens": [50656, 7614, 490, 1952, 7599, 13, 467, 1177, 380, 643, 281, 574, 411, 1952, 7599, 13, 467, 311, 295, 50844], "temperature": 0.0, "avg_logprob": -0.12015443099172492, "compression_ratio": 1.8515625, "no_speech_prob": 0.011685908772051334}, {"id": 195, "seek": 109784, "start": 1107.4399999999998, "end": 1113.1999999999998, "text": " course an interesting scientific question whether we can reproduce a human in silico, but for solving", "tokens": [50844, 1164, 364, 1880, 8134, 1168, 1968, 321, 393, 29501, 257, 1952, 294, 3425, 2789, 11, 457, 337, 12606, 51132], "temperature": 0.0, "avg_logprob": -0.12015443099172492, "compression_ratio": 1.8515625, "no_speech_prob": 0.011685908772051334}, {"id": 196, "seek": 109784, "start": 1113.1999999999998, "end": 1118.72, "text": " practical problems that will make this technology useful for the humanity, for the humankind,", "tokens": [51132, 8496, 2740, 300, 486, 652, 341, 2899, 4420, 337, 264, 10243, 11, 337, 264, 1484, 40588, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12015443099172492, "compression_ratio": 1.8515625, "no_speech_prob": 0.011685908772051334}, {"id": 197, "seek": 109784, "start": 1119.6799999999998, "end": 1124.6399999999999, "text": " we probably need something different. It will certainly involve a certain level of abstraction", "tokens": [51456, 321, 1391, 643, 746, 819, 13, 467, 486, 3297, 9494, 257, 1629, 1496, 295, 37765, 51704], "temperature": 0.0, "avg_logprob": -0.12015443099172492, "compression_ratio": 1.8515625, "no_speech_prob": 0.011685908772051334}, {"id": 198, "seek": 112464, "start": 1124.64, "end": 1128.64, "text": " that we currently don't have. It will probably require methods that we currently don't have,", "tokens": [50364, 300, 321, 4362, 500, 380, 362, 13, 467, 486, 1391, 3651, 7150, 300, 321, 4362, 500, 380, 362, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13804794947306315, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.006780849304050207}, {"id": 199, "seek": 112464, "start": 1129.6000000000001, "end": 1135.8400000000001, "text": " but it doesn't necessarily need to look like a recreation of a human. This is Dr. Petar Velichkovich.", "tokens": [50612, 457, 309, 1177, 380, 4725, 643, 281, 574, 411, 257, 31573, 295, 257, 1952, 13, 639, 307, 2491, 13, 10472, 289, 17814, 480, 33516, 480, 13, 50924], "temperature": 0.0, "avg_logprob": -0.13804794947306315, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.006780849304050207}, {"id": 200, "seek": 112464, "start": 1136.5600000000002, "end": 1141.0400000000002, "text": " By now you'll have probably seen or heard something about our recently released proto book on", "tokens": [50960, 3146, 586, 291, 603, 362, 1391, 1612, 420, 2198, 746, 466, 527, 3938, 4736, 47896, 1446, 322, 51184], "temperature": 0.0, "avg_logprob": -0.13804794947306315, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.006780849304050207}, {"id": 201, "seek": 112464, "start": 1141.0400000000002, "end": 1146.5600000000002, "text": " geometric deep learning on grids, graphs, groups, geodesics, and gauges, or as we like to call it", "tokens": [51184, 33246, 2452, 2539, 322, 677, 3742, 11, 24877, 11, 3935, 11, 1519, 4789, 1167, 11, 293, 5959, 39064, 11, 420, 382, 321, 411, 281, 818, 309, 51460], "temperature": 0.0, "avg_logprob": -0.13804794947306315, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.006780849304050207}, {"id": 202, "seek": 112464, "start": 1146.5600000000002, "end": 1151.44, "text": " the 5Gs of geometric deep learning, which I've co-authored alongside Michael Bronstein,", "tokens": [51460, 264, 1025, 33715, 295, 33246, 2452, 2539, 11, 597, 286, 600, 598, 12, 40198, 2769, 12385, 5116, 19544, 9089, 11, 51704], "temperature": 0.0, "avg_logprob": -0.13804794947306315, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.006780849304050207}, {"id": 203, "seek": 115144, "start": 1151.44, "end": 1156.3200000000002, "text": " John Brunner, and Taco Cohen. And you might be wondering what all the fuss is about, because", "tokens": [50364, 2619, 1603, 409, 1193, 11, 293, 37992, 32968, 13, 400, 291, 1062, 312, 6359, 437, 439, 264, 34792, 307, 466, 11, 570, 50608], "temperature": 0.0, "avg_logprob": -0.12165141774115161, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.004753607325255871}, {"id": 204, "seek": 115144, "start": 1156.3200000000002, "end": 1161.68, "text": " there's already a lot of really high quality synthesis textbooks on the field of deep learning", "tokens": [50608, 456, 311, 1217, 257, 688, 295, 534, 1090, 3125, 30252, 33587, 322, 264, 2519, 295, 2452, 2539, 50876], "temperature": 0.0, "avg_logprob": -0.12165141774115161, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.004753607325255871}, {"id": 205, "seek": 115144, "start": 1161.68, "end": 1165.76, "text": " in general, and also on some sub areas of geometric deep learning, such as graph neural", "tokens": [50876, 294, 2674, 11, 293, 611, 322, 512, 1422, 3179, 295, 33246, 2452, 2539, 11, 1270, 382, 4295, 18161, 51080], "temperature": 0.0, "avg_logprob": -0.12165141774115161, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.004753607325255871}, {"id": 206, "seek": 115144, "start": 1165.76, "end": 1170.88, "text": " networks, where Will Hamilton recently released a super high quality textbook on that area.", "tokens": [51080, 9590, 11, 689, 3099, 18484, 3938, 4736, 257, 1687, 1090, 3125, 25591, 322, 300, 1859, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12165141774115161, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.004753607325255871}, {"id": 207, "seek": 115144, "start": 1170.88, "end": 1178.3200000000002, "text": " This is Dr. Taco Cohen. So what we've been trying to do in our book project is to show", "tokens": [51336, 639, 307, 2491, 13, 37992, 32968, 13, 407, 437, 321, 600, 668, 1382, 281, 360, 294, 527, 1446, 1716, 307, 281, 855, 51708], "temperature": 0.0, "avg_logprob": -0.12165141774115161, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.004753607325255871}, {"id": 208, "seek": 117832, "start": 1178.32, "end": 1184.96, "text": " that this geometric deep learning mindset is not just useful when tackling a new problem,", "tokens": [50364, 300, 341, 33246, 2452, 2539, 12543, 307, 406, 445, 4420, 562, 34415, 257, 777, 1154, 11, 50696], "temperature": 0.0, "avg_logprob": -0.086614105436537, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.02367810532450676}, {"id": 209, "seek": 117832, "start": 1184.96, "end": 1190.72, "text": " but actually allows you to derive from first principles of symmetry and skill separation,", "tokens": [50696, 457, 767, 4045, 291, 281, 28446, 490, 700, 9156, 295, 25440, 293, 5389, 14634, 11, 50984], "temperature": 0.0, "avg_logprob": -0.086614105436537, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.02367810532450676}, {"id": 210, "seek": 117832, "start": 1190.72, "end": 1196.08, "text": " many of the architectures and architectural primitives like convolution, attention,", "tokens": [50984, 867, 295, 264, 6331, 1303, 293, 26621, 2886, 38970, 411, 45216, 11, 3202, 11, 51252], "temperature": 0.0, "avg_logprob": -0.086614105436537, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.02367810532450676}, {"id": 211, "seek": 117832, "start": 1196.08, "end": 1201.6799999999998, "text": " graph convolution, and so forth, that have become popular over the last few years.", "tokens": [51252, 4295, 45216, 11, 293, 370, 5220, 11, 300, 362, 1813, 3743, 670, 264, 1036, 1326, 924, 13, 51532], "temperature": 0.0, "avg_logprob": -0.086614105436537, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.02367810532450676}, {"id": 212, "seek": 120168, "start": 1202.16, "end": 1207.44, "text": " Even in cases where these considerations of active variance and skill separation", "tokens": [50388, 2754, 294, 3331, 689, 613, 24070, 295, 4967, 21977, 293, 5389, 14634, 50652], "temperature": 0.0, "avg_logprob": -0.10837886810302734, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.30360421538352966}, {"id": 213, "seek": 120168, "start": 1207.44, "end": 1211.6000000000001, "text": " were not felt at center when the methods were first discovered.", "tokens": [50652, 645, 406, 2762, 412, 3056, 562, 264, 7150, 645, 700, 6941, 13, 50860], "temperature": 0.0, "avg_logprob": -0.10837886810302734, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.30360421538352966}, {"id": 214, "seek": 120168, "start": 1212.72, "end": 1219.2, "text": " Now we think that this is useful for a number of reasons. First of all, it might help to avoid", "tokens": [50916, 823, 321, 519, 300, 341, 307, 4420, 337, 257, 1230, 295, 4112, 13, 2386, 295, 439, 11, 309, 1062, 854, 281, 5042, 51240], "temperature": 0.0, "avg_logprob": -0.10837886810302734, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.30360421538352966}, {"id": 215, "seek": 120168, "start": 1220.0800000000002, "end": 1225.6000000000001, "text": " reinventing the same ideas over and over. And this can easily happen when the number of papers", "tokens": [51284, 33477, 278, 264, 912, 3487, 670, 293, 670, 13, 400, 341, 393, 3612, 1051, 562, 264, 1230, 295, 10577, 51560], "temperature": 0.0, "avg_logprob": -0.10837886810302734, "compression_ratio": 1.5904761904761904, "no_speech_prob": 0.30360421538352966}, {"id": 216, "seek": 122560, "start": 1225.6, "end": 1232.8799999999999, "text": " that come out every day is far larger than what any one person can possibly read,", "tokens": [50364, 300, 808, 484, 633, 786, 307, 1400, 4833, 813, 437, 604, 472, 954, 393, 6264, 1401, 11, 50728], "temperature": 0.0, "avg_logprob": -0.07441961459624462, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02330922521650791}, {"id": 217, "seek": 122560, "start": 1233.76, "end": 1240.3999999999999, "text": " and when different sub fields use different language to describe their ideas. Furthermore,", "tokens": [50772, 293, 562, 819, 1422, 7909, 764, 819, 2856, 281, 6786, 641, 3487, 13, 23999, 11, 51104], "temperature": 0.0, "avg_logprob": -0.07441961459624462, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02330922521650791}, {"id": 218, "seek": 122560, "start": 1240.3999999999999, "end": 1246.8, "text": " it might help to clarify when a particular method is useful. A geometric deep learning method is", "tokens": [51104, 309, 1062, 854, 281, 17594, 562, 257, 1729, 3170, 307, 4420, 13, 316, 33246, 2452, 2539, 3170, 307, 51424], "temperature": 0.0, "avg_logprob": -0.07441961459624462, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02330922521650791}, {"id": 219, "seek": 122560, "start": 1246.8, "end": 1252.56, "text": " useful when the problem domain has the particular symmetries that are built into the architecture.", "tokens": [51424, 4420, 562, 264, 1154, 9274, 575, 264, 1729, 14232, 302, 2244, 300, 366, 3094, 666, 264, 9482, 13, 51712], "temperature": 0.0, "avg_logprob": -0.07441961459624462, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02330922521650791}, {"id": 220, "seek": 125256, "start": 1253.36, "end": 1261.28, "text": " And finally, we hope that by making explicit the commonalities between seemingly different", "tokens": [50404, 400, 2721, 11, 321, 1454, 300, 538, 1455, 13691, 264, 2689, 16110, 1296, 18709, 819, 50800], "temperature": 0.0, "avg_logprob": -0.09476434320643329, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.002800063230097294}, {"id": 221, "seek": 125256, "start": 1261.28, "end": 1267.76, "text": " methods, it will become easier for new cars to learn geometric deep learning. Ideally,", "tokens": [50800, 7150, 11, 309, 486, 1813, 3571, 337, 777, 5163, 281, 1466, 33246, 2452, 2539, 13, 40817, 11, 51124], "temperature": 0.0, "avg_logprob": -0.09476434320643329, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.002800063230097294}, {"id": 222, "seek": 125256, "start": 1267.76, "end": 1273.84, "text": " one would not have to go through the large number of architectures that have been designed,", "tokens": [51124, 472, 576, 406, 362, 281, 352, 807, 264, 2416, 1230, 295, 6331, 1303, 300, 362, 668, 4761, 11, 51428], "temperature": 0.0, "avg_logprob": -0.09476434320643329, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.002800063230097294}, {"id": 223, "seek": 125256, "start": 1273.84, "end": 1277.84, "text": " but just learn the general ideas of groups,", "tokens": [51428, 457, 445, 1466, 264, 2674, 3487, 295, 3935, 11, 51628], "temperature": 0.0, "avg_logprob": -0.09476434320643329, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.002800063230097294}, {"id": 224, "seek": 127784, "start": 1277.9199999999998, "end": 1283.76, "text": " equivariance, group representations and feature spaces and so on, and then see,", "tokens": [50368, 1267, 592, 3504, 719, 11, 1594, 33358, 293, 4111, 7673, 293, 370, 322, 11, 293, 550, 536, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11668190675623277, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00529327942058444}, {"id": 225, "seek": 127784, "start": 1283.76, "end": 1288.8, "text": " for the particular instances, you're interested in how that fits into the general pattern.", "tokens": [50660, 337, 264, 1729, 14519, 11, 291, 434, 3102, 294, 577, 300, 9001, 666, 264, 2674, 5102, 13, 50912], "temperature": 0.0, "avg_logprob": -0.11668190675623277, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00529327942058444}, {"id": 226, "seek": 127784, "start": 1289.52, "end": 1295.12, "text": " To really illustrate why do we think that such a synthesis is important and relevant for", "tokens": [50948, 1407, 534, 23221, 983, 360, 321, 519, 300, 1270, 257, 30252, 307, 1021, 293, 7340, 337, 51228], "temperature": 0.0, "avg_logprob": -0.11668190675623277, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00529327942058444}, {"id": 227, "seek": 127784, "start": 1295.12, "end": 1300.72, "text": " deep learning research going forward, we have to go way back, way back in the time of Euclid,", "tokens": [51228, 2452, 2539, 2132, 516, 2128, 11, 321, 362, 281, 352, 636, 646, 11, 636, 646, 294, 264, 565, 295, 462, 1311, 75, 327, 11, 51508], "temperature": 0.0, "avg_logprob": -0.11668190675623277, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00529327942058444}, {"id": 228, "seek": 130072, "start": 1300.72, "end": 1308.0, "text": " around 300 years BC. And as you might know, Euclid is the founding father of Euclidean geometry,", "tokens": [50364, 926, 6641, 924, 14359, 13, 400, 382, 291, 1062, 458, 11, 462, 1311, 75, 327, 307, 264, 22223, 3086, 295, 462, 1311, 31264, 282, 18426, 11, 50728], "temperature": 0.0, "avg_logprob": -0.06540753403488471, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.20921216905117035}, {"id": 229, "seek": 130072, "start": 1308.0, "end": 1314.08, "text": " which for many, many years was the only way to do geometry. It relied on a certain set of postulates", "tokens": [50728, 597, 337, 867, 11, 867, 924, 390, 264, 787, 636, 281, 360, 18426, 13, 467, 35463, 322, 257, 1629, 992, 295, 2183, 26192, 51032], "temperature": 0.0, "avg_logprob": -0.06540753403488471, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.20921216905117035}, {"id": 230, "seek": 130072, "start": 1314.08, "end": 1320.16, "text": " that Euclid had that governed all the laws of the geometry that he proposed. All of this started", "tokens": [51032, 300, 462, 1311, 75, 327, 632, 300, 35529, 439, 264, 6064, 295, 264, 18426, 300, 415, 10348, 13, 1057, 295, 341, 1409, 51336], "temperature": 0.0, "avg_logprob": -0.06540753403488471, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.20921216905117035}, {"id": 231, "seek": 130072, "start": 1320.16, "end": 1325.84, "text": " to drastically change around the 1800s when several mathematicians, in an effort to prove", "tokens": [51336, 281, 29673, 1319, 926, 264, 24327, 82, 562, 2940, 32811, 2567, 11, 294, 364, 4630, 281, 7081, 51620], "temperature": 0.0, "avg_logprob": -0.06540753403488471, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.20921216905117035}, {"id": 232, "seek": 132584, "start": 1325.84, "end": 1331.76, "text": " that Euclid's geometry is the geometry to be following, ended up assuming that one of the", "tokens": [50364, 300, 462, 1311, 75, 327, 311, 18426, 307, 264, 18426, 281, 312, 3480, 11, 4590, 493, 11926, 300, 472, 295, 264, 50660], "temperature": 0.0, "avg_logprob": -0.05858398823255903, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.010799775831401348}, {"id": 233, "seek": 132584, "start": 1331.76, "end": 1336.6399999999999, "text": " postulates is false and failing to drive a contradiction. They actually ended up deriving", "tokens": [50660, 2183, 26192, 307, 7908, 293, 18223, 281, 3332, 257, 34937, 13, 814, 767, 4590, 493, 1163, 2123, 50904], "temperature": 0.0, "avg_logprob": -0.05858398823255903, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.010799775831401348}, {"id": 234, "seek": 132584, "start": 1336.6399999999999, "end": 1342.08, "text": " a completely new set of self-consistent geometries, all with their own set of laws and rules,", "tokens": [50904, 257, 2584, 777, 992, 295, 2698, 12, 21190, 25367, 12956, 2244, 11, 439, 365, 641, 1065, 992, 295, 6064, 293, 4474, 11, 51176], "temperature": 0.0, "avg_logprob": -0.05858398823255903, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.010799775831401348}, {"id": 235, "seek": 132584, "start": 1342.08, "end": 1348.9599999999998, "text": " and also quite differing terminologies. Among some of these popular variants are the hyperbolic", "tokens": [51176, 293, 611, 1596, 743, 278, 10761, 6204, 13, 16119, 512, 295, 613, 3743, 21669, 366, 264, 9848, 65, 7940, 51520], "temperature": 0.0, "avg_logprob": -0.05858398823255903, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.010799775831401348}, {"id": 236, "seek": 134896, "start": 1348.96, "end": 1355.8400000000001, "text": " geometry of Lobachevsky and Bolyai, and the elliptic geometries of Riemann. And for a very long time,", "tokens": [50364, 18426, 295, 30719, 6000, 85, 25810, 293, 3286, 356, 1301, 11, 293, 264, 8284, 22439, 299, 12956, 2244, 295, 497, 4907, 969, 13, 400, 337, 257, 588, 938, 565, 11, 50708], "temperature": 0.0, "avg_logprob": -0.10492862539088472, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.13995963335037231}, {"id": 237, "seek": 134896, "start": 1355.8400000000001, "end": 1362.0, "text": " because all of these geometries had completely different sets of rules and they were all self-consistent,", "tokens": [50708, 570, 439, 295, 613, 12956, 2244, 632, 2584, 819, 6352, 295, 4474, 293, 436, 645, 439, 2698, 12, 21190, 25367, 11, 51016], "temperature": 0.0, "avg_logprob": -0.10492862539088472, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.13995963335037231}, {"id": 238, "seek": 134896, "start": 1362.0, "end": 1366.88, "text": " people were generally wondering what is the one true geometry that we should be studying.", "tokens": [51016, 561, 645, 5101, 6359, 437, 307, 264, 472, 2074, 18426, 300, 321, 820, 312, 7601, 13, 51260], "temperature": 0.0, "avg_logprob": -0.10492862539088472, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.13995963335037231}, {"id": 239, "seek": 134896, "start": 1366.88, "end": 1374.16, "text": " A solution to this problem came several decades later through the work of a young German mathematician", "tokens": [51260, 316, 3827, 281, 341, 1154, 1361, 2940, 7878, 1780, 807, 264, 589, 295, 257, 2037, 6521, 48281, 51624], "temperature": 0.0, "avg_logprob": -0.10492862539088472, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.13995963335037231}, {"id": 240, "seek": 137416, "start": 1374.16, "end": 1380.0, "text": " by the name of Felix Klein, who had just been appointed for a professorship position at the", "tokens": [50364, 538, 264, 1315, 295, 30169, 33327, 11, 567, 632, 445, 668, 17653, 337, 257, 2668, 14752, 2535, 412, 264, 50656], "temperature": 0.0, "avg_logprob": -0.06303308343374601, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.04140090569853783}, {"id": 241, "seek": 137416, "start": 1380.0, "end": 1385.76, "text": " small Bavarian University of Erlangen, the so-called Friedrich Alexander University in Erlangen,", "tokens": [50656, 1359, 363, 706, 10652, 3535, 295, 3300, 75, 10784, 11, 264, 370, 12, 11880, 17605, 10794, 14845, 3535, 294, 3300, 75, 10784, 11, 50944], "temperature": 0.0, "avg_logprob": -0.06303308343374601, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.04140090569853783}, {"id": 242, "seek": 137416, "start": 1385.76, "end": 1392.4, "text": " Nuremberg. While he was at this post, he had proposed a direction that would eventually enable", "tokens": [50944, 426, 540, 4508, 70, 13, 3987, 415, 390, 412, 341, 2183, 11, 415, 632, 10348, 257, 3513, 300, 576, 4728, 9528, 51276], "temperature": 0.0, "avg_logprob": -0.06303308343374601, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.04140090569853783}, {"id": 243, "seek": 137416, "start": 1392.4, "end": 1397.8400000000001, "text": " us to unify all of the geometries that were in existence at the time through the lens of", "tokens": [51276, 505, 281, 517, 2505, 439, 295, 264, 12956, 2244, 300, 645, 294, 9123, 412, 264, 565, 807, 264, 6765, 295, 51548], "temperature": 0.0, "avg_logprob": -0.06303308343374601, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.04140090569853783}, {"id": 244, "seek": 139784, "start": 1397.84, "end": 1404.24, "text": " invariances and symmetry using the language of group theory. And his work is now eponymously", "tokens": [50364, 33270, 2676, 293, 25440, 1228, 264, 2856, 295, 1594, 5261, 13, 400, 702, 589, 307, 586, 2388, 12732, 5098, 50684], "temperature": 0.0, "avg_logprob": -0.08263148901597508, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.046687763184309006}, {"id": 245, "seek": 139784, "start": 1404.24, "end": 1410.8799999999999, "text": " known as the Erlangen program. And there is no way to overstate how much of an important effect", "tokens": [50684, 2570, 382, 264, 3300, 75, 10784, 1461, 13, 400, 456, 307, 572, 636, 281, 670, 15406, 577, 709, 295, 364, 1021, 1802, 51016], "temperature": 0.0, "avg_logprob": -0.08263148901597508, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.046687763184309006}, {"id": 246, "seek": 139784, "start": 1410.8799999999999, "end": 1416.1599999999999, "text": " the Erlangen program had on mathematics and beyond. Because of the fact that it provided", "tokens": [51016, 264, 3300, 75, 10784, 1461, 632, 322, 18666, 293, 4399, 13, 1436, 295, 264, 1186, 300, 309, 5649, 51280], "temperature": 0.0, "avg_logprob": -0.08263148901597508, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.046687763184309006}, {"id": 247, "seek": 139784, "start": 1416.1599999999999, "end": 1421.84, "text": " a unifying lens of studying geometry, suddenly people didn't need to hunt for the one true", "tokens": [51280, 257, 517, 5489, 6765, 295, 7601, 18426, 11, 5800, 561, 994, 380, 643, 281, 12454, 337, 264, 472, 2074, 51564], "temperature": 0.0, "avg_logprob": -0.08263148901597508, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.046687763184309006}, {"id": 248, "seek": 139784, "start": 1421.84, "end": 1426.72, "text": " geometry they had a blueprint they could use to drive whatever geometry was necessary for the", "tokens": [51564, 18426, 436, 632, 257, 35868, 436, 727, 764, 281, 3332, 2035, 18426, 390, 4818, 337, 264, 51808], "temperature": 0.0, "avg_logprob": -0.08263148901597508, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.046687763184309006}, {"id": 249, "seek": 142672, "start": 1426.72, "end": 1431.68, "text": " problem they were solving. And besides just mathematics, it had amazing spillover effects", "tokens": [50364, 1154, 436, 645, 12606, 13, 400, 11868, 445, 18666, 11, 309, 632, 2243, 22044, 3570, 5065, 50612], "temperature": 0.0, "avg_logprob": -0.09290634155273438, "compression_ratio": 1.6548042704626333, "no_speech_prob": 0.0023960925173014402}, {"id": 250, "seek": 142672, "start": 1431.68, "end": 1437.28, "text": " to other very important fields of science. For example, in physics, the Erlangen program spilled", "tokens": [50612, 281, 661, 588, 1021, 7909, 295, 3497, 13, 1171, 1365, 11, 294, 10649, 11, 264, 3300, 75, 10784, 1461, 37833, 50892], "temperature": 0.0, "avg_logprob": -0.09290634155273438, "compression_ratio": 1.6548042704626333, "no_speech_prob": 0.0023960925173014402}, {"id": 251, "seek": 142672, "start": 1437.28, "end": 1441.92, "text": " over through the work of Emy Nerther, demonstrated that all of the conservation laws in physics,", "tokens": [50892, 670, 807, 264, 589, 295, 462, 2226, 36536, 616, 11, 18772, 300, 439, 295, 264, 16185, 6064, 294, 10649, 11, 51124], "temperature": 0.0, "avg_logprob": -0.09290634155273438, "compression_ratio": 1.6548042704626333, "no_speech_prob": 0.0023960925173014402}, {"id": 252, "seek": 142672, "start": 1441.92, "end": 1446.32, "text": " which previously had to be validated through extensive experimental evaluation, could be", "tokens": [51124, 597, 8046, 632, 281, 312, 40693, 807, 13246, 17069, 13344, 11, 727, 312, 51344], "temperature": 0.0, "avg_logprob": -0.09290634155273438, "compression_ratio": 1.6548042704626333, "no_speech_prob": 0.0023960925173014402}, {"id": 253, "seek": 142672, "start": 1446.32, "end": 1451.68, "text": " completely derivable through the principles of symmetry. And needless to say, this is a very", "tokens": [51344, 2584, 10151, 712, 807, 264, 9156, 295, 25440, 13, 400, 643, 1832, 281, 584, 11, 341, 307, 257, 588, 51612], "temperature": 0.0, "avg_logprob": -0.09290634155273438, "compression_ratio": 1.6548042704626333, "no_speech_prob": 0.0023960925173014402}, {"id": 254, "seek": 145168, "start": 1451.76, "end": 1457.44, "text": " fundamental and game changing result in physics, which also allowed us to classify some elementary", "tokens": [50368, 8088, 293, 1216, 4473, 1874, 294, 10649, 11, 597, 611, 4350, 505, 281, 33872, 512, 16429, 50652], "temperature": 0.0, "avg_logprob": -0.06534816279555812, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.0161507036536932}, {"id": 255, "seek": 145168, "start": 1457.44, "end": 1461.92, "text": " particles in what is now known as the standard model. Thinking back towards theoretical computer", "tokens": [50652, 10007, 294, 437, 307, 586, 2570, 382, 264, 3832, 2316, 13, 24460, 646, 3030, 20864, 3820, 50876], "temperature": 0.0, "avg_logprob": -0.06534816279555812, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.0161507036536932}, {"id": 256, "seek": 145168, "start": 1461.92, "end": 1468.4, "text": " science, the Erlangen program also had a spillover effect into category theory, which is one of the", "tokens": [50876, 3497, 11, 264, 3300, 75, 10784, 1461, 611, 632, 257, 22044, 3570, 1802, 666, 7719, 5261, 11, 597, 307, 472, 295, 264, 51200], "temperature": 0.0, "avg_logprob": -0.06534816279555812, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.0161507036536932}, {"id": 257, "seek": 145168, "start": 1468.4, "end": 1472.96, "text": " most abstractified areas of theoretical computer science with a lot of potential for unifying", "tokens": [51200, 881, 12649, 2587, 3179, 295, 20864, 3820, 3497, 365, 257, 688, 295, 3995, 337, 517, 5489, 51428], "temperature": 0.0, "avg_logprob": -0.06534816279555812, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.0161507036536932}, {"id": 258, "seek": 145168, "start": 1472.96, "end": 1477.76, "text": " various directions in mathematics. And actually in the words of the founders of category theory,", "tokens": [51428, 3683, 11095, 294, 18666, 13, 400, 767, 294, 264, 2283, 295, 264, 25608, 295, 7719, 5261, 11, 51668], "temperature": 0.0, "avg_logprob": -0.06534816279555812, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.0161507036536932}, {"id": 259, "seek": 147776, "start": 1477.76, "end": 1483.04, "text": " the whole field of category theory can be seen as an extension of Felix Klein's Erlangen program.", "tokens": [50364, 264, 1379, 2519, 295, 7719, 5261, 393, 312, 1612, 382, 364, 10320, 295, 30169, 33327, 311, 3300, 75, 10784, 1461, 13, 50628], "temperature": 0.0, "avg_logprob": -0.05973491873792423, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0013645341387018561}, {"id": 260, "seek": 147776, "start": 1483.68, "end": 1491.52, "text": " So the Erlangen program demonstrated how it's possible to take a small set of guiding principles", "tokens": [50660, 407, 264, 3300, 75, 10784, 1461, 18772, 577, 309, 311, 1944, 281, 747, 257, 1359, 992, 295, 25061, 9156, 51052], "temperature": 0.0, "avg_logprob": -0.05973491873792423, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0013645341387018561}, {"id": 261, "seek": 147776, "start": 1491.52, "end": 1497.28, "text": " of invariance and symmetry and use it to unify something as broad as geometry. I like to think", "tokens": [51052, 295, 33270, 719, 293, 25440, 293, 764, 309, 281, 517, 2505, 746, 382, 4152, 382, 18426, 13, 286, 411, 281, 519, 51340], "temperature": 0.0, "avg_logprob": -0.05973491873792423, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0013645341387018561}, {"id": 262, "seek": 147776, "start": 1497.28, "end": 1503.6, "text": " of geometric deep learning as not a single method or architecture, but as a mindset. It's a way of", "tokens": [51340, 295, 33246, 2452, 2539, 382, 406, 257, 2167, 3170, 420, 9482, 11, 457, 382, 257, 12543, 13, 467, 311, 257, 636, 295, 51656], "temperature": 0.0, "avg_logprob": -0.05973491873792423, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0013645341387018561}, {"id": 263, "seek": 150360, "start": 1503.6, "end": 1508.6399999999999, "text": " looking at machine learning problems from the first principles of symmetry and invariance.", "tokens": [50364, 1237, 412, 3479, 2539, 2740, 490, 264, 700, 9156, 295, 25440, 293, 33270, 719, 13, 50616], "temperature": 0.0, "avg_logprob": -0.06414316138442681, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.008693236857652664}, {"id": 264, "seek": 150360, "start": 1509.4399999999998, "end": 1515.52, "text": " And symmetry is a key idea that underpins our physical world and the data that is created by", "tokens": [50656, 400, 25440, 307, 257, 2141, 1558, 300, 833, 79, 1292, 527, 4001, 1002, 293, 264, 1412, 300, 307, 2942, 538, 50960], "temperature": 0.0, "avg_logprob": -0.06414316138442681, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.008693236857652664}, {"id": 265, "seek": 150360, "start": 1515.52, "end": 1521.84, "text": " physical processes. And accounting for this structure allows us to beat the curse of dimensionality in", "tokens": [50960, 4001, 7555, 13, 400, 19163, 337, 341, 3877, 4045, 505, 281, 4224, 264, 17139, 295, 10139, 1860, 294, 51276], "temperature": 0.0, "avg_logprob": -0.06414316138442681, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.008693236857652664}, {"id": 266, "seek": 150360, "start": 1521.84, "end": 1527.6799999999998, "text": " machine learning problems. It is really a very powerful principle and very generic blueprint.", "tokens": [51276, 3479, 2539, 2740, 13, 467, 307, 534, 257, 588, 4005, 8665, 293, 588, 19577, 35868, 13, 51568], "temperature": 0.0, "avg_logprob": -0.06414316138442681, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.008693236857652664}, {"id": 267, "seek": 150360, "start": 1528.24, "end": 1532.8799999999999, "text": " And we find its instances in some of today's most popular deep learning architectures,", "tokens": [51596, 400, 321, 915, 1080, 14519, 294, 512, 295, 965, 311, 881, 3743, 2452, 2539, 6331, 1303, 11, 51828], "temperature": 0.0, "avg_logprob": -0.06414316138442681, "compression_ratio": 1.768939393939394, "no_speech_prob": 0.008693236857652664}, {"id": 268, "seek": 153288, "start": 1532.88, "end": 1538.4, "text": " whether it's convolutional neural networks, graph neural networks, transformers, LSTMs and many", "tokens": [50364, 1968, 309, 311, 45216, 304, 18161, 9590, 11, 4295, 18161, 9590, 11, 4088, 433, 11, 441, 6840, 26386, 293, 867, 50640], "temperature": 0.0, "avg_logprob": -0.07200477804456439, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.004868256859481335}, {"id": 269, "seek": 153288, "start": 1538.4, "end": 1544.3200000000002, "text": " more. And it is also a way to design new machine learning architectures that are yet to be invented,", "tokens": [50640, 544, 13, 400, 309, 307, 611, 257, 636, 281, 1715, 777, 3479, 2539, 6331, 1303, 300, 366, 1939, 281, 312, 14479, 11, 50936], "temperature": 0.0, "avg_logprob": -0.07200477804456439, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.004868256859481335}, {"id": 270, "seek": 153288, "start": 1544.3200000000002, "end": 1551.2800000000002, "text": " maybe in the future, not based on back propagation and incorporate inductive bias in a principled way.", "tokens": [50936, 1310, 294, 264, 2027, 11, 406, 2361, 322, 646, 38377, 293, 16091, 31612, 488, 12577, 294, 257, 3681, 15551, 636, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07200477804456439, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.004868256859481335}, {"id": 271, "seek": 153288, "start": 1551.2800000000002, "end": 1556.0, "text": " Being a professor and a teacher, I would also like to emphasize the pedagogical dimension of", "tokens": [51284, 8891, 257, 8304, 293, 257, 5027, 11, 286, 576, 611, 411, 281, 16078, 264, 5670, 31599, 804, 10139, 295, 51520], "temperature": 0.0, "avg_logprob": -0.07200477804456439, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.004868256859481335}, {"id": 272, "seek": 153288, "start": 1556.0, "end": 1562.16, "text": " this geometric unification. What I often see in deep learning when deep learning is taught is", "tokens": [51520, 341, 33246, 517, 3774, 13, 708, 286, 2049, 536, 294, 2452, 2539, 562, 2452, 2539, 307, 5928, 307, 51828], "temperature": 0.0, "avg_logprob": -0.07200477804456439, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.004868256859481335}, {"id": 273, "seek": 156216, "start": 1562.96, "end": 1569.1200000000001, "text": " it appears as a bunch of hacks with weaker or no justification. And I think it is best illustrated", "tokens": [50404, 309, 7038, 382, 257, 3840, 295, 33617, 365, 24286, 420, 572, 31591, 13, 400, 286, 519, 309, 307, 1151, 33875, 50712], "temperature": 0.0, "avg_logprob": -0.10671638798069309, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.008617819286882877}, {"id": 274, "seek": 156216, "start": 1569.1200000000001, "end": 1574.72, "text": " with how, for example, the concept of convolution is explained. It is often given as a formula just", "tokens": [50712, 365, 577, 11, 337, 1365, 11, 264, 3410, 295, 45216, 307, 8825, 13, 467, 307, 2049, 2212, 382, 257, 8513, 445, 50992], "temperature": 0.0, "avg_logprob": -0.10671638798069309, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.008617819286882877}, {"id": 275, "seek": 156216, "start": 1574.72, "end": 1580.88, "text": " out of the blue, maybe with a bit of hand waving. But what we try to show is that you can derive", "tokens": [50992, 484, 295, 264, 3344, 11, 1310, 365, 257, 857, 295, 1011, 35347, 13, 583, 437, 321, 853, 281, 855, 307, 300, 291, 393, 28446, 51300], "temperature": 0.0, "avg_logprob": -0.10671638798069309, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.008617819286882877}, {"id": 276, "seek": 156216, "start": 1580.88, "end": 1586.0800000000002, "text": " convolution from first principles, in this particular case, of translational symmetry.", "tokens": [51300, 45216, 490, 700, 9156, 11, 294, 341, 1729, 1389, 11, 295, 5105, 1478, 25440, 13, 51560], "temperature": 0.0, "avg_logprob": -0.10671638798069309, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.008617819286882877}, {"id": 277, "seek": 156216, "start": 1586.64, "end": 1591.1200000000001, "text": " And I think the difference in this approach is best captured by what Elvetsos once said", "tokens": [51588, 400, 286, 519, 264, 2649, 294, 341, 3109, 307, 1151, 11828, 538, 437, 2699, 85, 1385, 329, 1564, 848, 51812], "temperature": 0.0, "avg_logprob": -0.10671638798069309, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.008617819286882877}, {"id": 278, "seek": 159112, "start": 1591.12, "end": 1595.6799999999998, "text": " that the knowledge of principles easily compensates the lack of knowledge effects.", "tokens": [50364, 300, 264, 3601, 295, 9156, 3612, 11598, 1024, 264, 5011, 295, 3601, 5065, 13, 50592], "temperature": 0.0, "avg_logprob": -0.08877677786840152, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0026931886095553637}, {"id": 279, "seek": 159112, "start": 1597.04, "end": 1604.08, "text": " Professor Bronstein has been a professor at Imperial College in London for the last three years", "tokens": [50660, 8419, 19544, 9089, 575, 668, 257, 8304, 412, 21395, 6745, 294, 7042, 337, 264, 1036, 1045, 924, 51012], "temperature": 0.0, "avg_logprob": -0.08877677786840152, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0026931886095553637}, {"id": 280, "seek": 159112, "start": 1604.08, "end": 1611.4399999999998, "text": " and received his PhD with distinction from Technion, the Israeli Institute of Technology,", "tokens": [51012, 293, 4613, 702, 14476, 365, 16844, 490, 8337, 313, 11, 264, 19974, 9446, 295, 15037, 11, 51380], "temperature": 0.0, "avg_logprob": -0.08877677786840152, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0026931886095553637}, {"id": 281, "seek": 159112, "start": 1611.4399999999998, "end": 1619.04, "text": " in 2007. He's held visiting academic positions at MIT, Harvard and Stanford,", "tokens": [51380, 294, 12656, 13, 634, 311, 5167, 11700, 7778, 8432, 412, 13100, 11, 13378, 293, 20374, 11, 51760], "temperature": 0.0, "avg_logprob": -0.08877677786840152, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0026931886095553637}, {"id": 282, "seek": 161904, "start": 1619.04, "end": 1627.68, "text": " and his work has been cited over 21,000 times. His main expertise is in theoretical and computational", "tokens": [50364, 293, 702, 589, 575, 668, 30134, 670, 5080, 11, 1360, 1413, 13, 2812, 2135, 11769, 307, 294, 20864, 293, 28270, 50796], "temperature": 0.0, "avg_logprob": -0.07133835867831581, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0023669456131756306}, {"id": 283, "seek": 161904, "start": 1627.68, "end": 1633.36, "text": " geometric methods for machine learning and data science, and his research encompasses a broad", "tokens": [50796, 33246, 7150, 337, 3479, 2539, 293, 1412, 3497, 11, 293, 702, 2132, 49866, 257, 4152, 51080], "temperature": 0.0, "avg_logprob": -0.07133835867831581, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0023669456131756306}, {"id": 284, "seek": 161904, "start": 1633.36, "end": 1640.6399999999999, "text": " spectrum of applications ranging from computer vision and pattern recognition to geometry processing,", "tokens": [51080, 11143, 295, 5821, 25532, 490, 3820, 5201, 293, 5102, 11150, 281, 18426, 9007, 11, 51444], "temperature": 0.0, "avg_logprob": -0.07133835867831581, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0023669456131756306}, {"id": 285, "seek": 161904, "start": 1640.6399999999999, "end": 1647.84, "text": " computer graphics and biomedicine. Professor Bronstein coined and popularized the term", "tokens": [51444, 3820, 11837, 293, 3228, 17671, 299, 533, 13, 8419, 19544, 9089, 45222, 293, 3743, 1602, 264, 1433, 51804], "temperature": 0.0, "avg_logprob": -0.07133835867831581, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0023669456131756306}, {"id": 286, "seek": 164784, "start": 1647.84, "end": 1656.08, "text": " geometric deep learning. His startup company, Fabula AI, which was acquired by Twitter in 2019,", "tokens": [50364, 33246, 2452, 2539, 13, 2812, 18578, 2237, 11, 17440, 3780, 7318, 11, 597, 390, 17554, 538, 5794, 294, 6071, 11, 50776], "temperature": 0.0, "avg_logprob": -0.09158161540090302, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0044258954003453255}, {"id": 287, "seek": 164784, "start": 1656.08, "end": 1662.56, "text": " was one of the first applications of graph ML to the problem of misinformation detection. I think", "tokens": [50776, 390, 472, 295, 264, 700, 5821, 295, 4295, 21601, 281, 264, 1154, 295, 34238, 17784, 13, 286, 519, 51100], "temperature": 0.0, "avg_logprob": -0.09158161540090302, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0044258954003453255}, {"id": 288, "seek": 164784, "start": 1662.56, "end": 1668.9599999999998, "text": " it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert", "tokens": [51100, 309, 311, 572, 19123, 399, 281, 584, 300, 8419, 19544, 9089, 307, 264, 1002, 311, 881, 40757, 5844, 51420], "temperature": 0.0, "avg_logprob": -0.09158161540090302, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0044258954003453255}, {"id": 289, "seek": 164784, "start": 1668.9599999999998, "end": 1674.9599999999998, "text": " in graph representation learning research. We are really probably some of the nicest locations", "tokens": [51420, 294, 4295, 10290, 2539, 2132, 13, 492, 366, 534, 1391, 512, 295, 264, 45516, 9253, 51720], "temperature": 0.0, "avg_logprob": -0.09158161540090302, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.0044258954003453255}, {"id": 290, "seek": 167496, "start": 1674.96, "end": 1681.6000000000001, "text": " in London, which is Kensington, if you're familiar, so we have the the Natural History Museum, the Science", "tokens": [50364, 294, 7042, 11, 597, 307, 33265, 20251, 11, 498, 291, 434, 4963, 11, 370, 321, 362, 264, 264, 20137, 12486, 10967, 11, 264, 8976, 50696], "temperature": 0.0, "avg_logprob": -0.1197188327186986, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.20445546507835388}, {"id": 291, "seek": 167496, "start": 1681.6000000000001, "end": 1689.76, "text": " Museum and the Victoria and Albert Museum and Imperial College is right here, so I think it's", "tokens": [50696, 10967, 293, 264, 16656, 293, 20812, 10967, 293, 21395, 6745, 307, 558, 510, 11, 370, 286, 519, 309, 311, 51104], "temperature": 0.0, "avg_logprob": -0.1197188327186986, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.20445546507835388}, {"id": 292, "seek": 167496, "start": 1689.76, "end": 1695.2, "text": " as central as you can get. And if you walk all the way there, then you have Hyde Park,", "tokens": [51104, 382, 5777, 382, 291, 393, 483, 13, 400, 498, 291, 1792, 439, 264, 636, 456, 11, 550, 291, 362, 5701, 1479, 4964, 11, 51376], "temperature": 0.0, "avg_logprob": -0.1197188327186986, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.20445546507835388}, {"id": 293, "seek": 167496, "start": 1696.16, "end": 1700.8, "text": " which is probably one of the nicest parks in London. I'm a professor in the department of", "tokens": [51424, 597, 307, 1391, 472, 295, 264, 45516, 16213, 294, 7042, 13, 286, 478, 257, 8304, 294, 264, 5882, 295, 51656], "temperature": 0.0, "avg_logprob": -0.1197188327186986, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.20445546507835388}, {"id": 294, "seek": 170080, "start": 1700.8, "end": 1705.2, "text": " computing at Imperial College London and head of graph learning research at Twitter.", "tokens": [50364, 15866, 412, 21395, 6745, 7042, 293, 1378, 295, 4295, 2539, 2132, 412, 5794, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12224692802924614, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.004865436349064112}, {"id": 295, "seek": 170080, "start": 1706.0, "end": 1711.12, "text": " I work on geometric deep learning in particular on graph neural networks and their applications from", "tokens": [50624, 286, 589, 322, 33246, 2452, 2539, 294, 1729, 322, 4295, 18161, 9590, 293, 641, 5821, 490, 50880], "temperature": 0.0, "avg_logprob": -0.12224692802924614, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.004865436349064112}, {"id": 296, "seek": 170080, "start": 1711.12, "end": 1717.52, "text": " computer vision and graphics to computational biology and drug design. Dr Petar Velichkovich", "tokens": [50880, 3820, 5201, 293, 11837, 281, 28270, 14956, 293, 4110, 1715, 13, 2491, 10472, 289, 17814, 480, 33516, 480, 51200], "temperature": 0.0, "avg_logprob": -0.12224692802924614, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.004865436349064112}, {"id": 297, "seek": 170080, "start": 1717.52, "end": 1724.8, "text": " is a senior research scientist at DeepMind in London and he obtained his PhD from Trinity College", "tokens": [51200, 307, 257, 7965, 2132, 12662, 412, 14895, 44, 471, 294, 7042, 293, 415, 14879, 702, 14476, 490, 33121, 6745, 51564], "temperature": 0.0, "avg_logprob": -0.12224692802924614, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.004865436349064112}, {"id": 298, "seek": 172480, "start": 1724.8799999999999, "end": 1731.04, "text": " in Cambridge. His research has been focused on geometric deep learning and in particular", "tokens": [50368, 294, 24876, 13, 2812, 2132, 575, 668, 5178, 322, 33246, 2452, 2539, 293, 294, 1729, 50676], "temperature": 0.0, "avg_logprob": -0.04597200474268954, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.06409920752048492}, {"id": 299, "seek": 172480, "start": 1731.04, "end": 1736.96, "text": " devising neural network architectures for graph representation learning and its applications", "tokens": [50676, 1905, 3436, 18161, 3209, 6331, 1303, 337, 4295, 10290, 2539, 293, 1080, 5821, 50972], "temperature": 0.0, "avg_logprob": -0.04597200474268954, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.06409920752048492}, {"id": 300, "seek": 172480, "start": 1736.96, "end": 1742.72, "text": " in algorithmic reasoning and computational biology. Petar's work has been published in the", "tokens": [50972, 294, 9284, 299, 21577, 293, 28270, 14956, 13, 10472, 289, 311, 589, 575, 668, 6572, 294, 264, 51260], "temperature": 0.0, "avg_logprob": -0.04597200474268954, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.06409920752048492}, {"id": 301, "seek": 172480, "start": 1742.72, "end": 1748.6399999999999, "text": " leading machine learning venues. Petar was the first author of graph attention networks,", "tokens": [51260, 5775, 3479, 2539, 32882, 13, 10472, 289, 390, 264, 700, 3793, 295, 4295, 3202, 9590, 11, 51556], "temperature": 0.0, "avg_logprob": -0.04597200474268954, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.06409920752048492}, {"id": 302, "seek": 174864, "start": 1748.64, "end": 1755.0400000000002, "text": " a popular convolutional layer for graphs and deep graph infomax, a scalable unsupervised", "tokens": [50364, 257, 3743, 45216, 304, 4583, 337, 24877, 293, 2452, 4295, 1536, 298, 2797, 11, 257, 38481, 2693, 12879, 24420, 50684], "temperature": 0.0, "avg_logprob": -0.06835426794034298, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.010119603015482426}, {"id": 303, "seek": 174864, "start": 1755.0400000000002, "end": 1760.64, "text": " learning pipeline for graphs. Hi everyone, my name is Petar Velichkovich and I'm a senior", "tokens": [50684, 2539, 15517, 337, 24877, 13, 2421, 1518, 11, 452, 1315, 307, 10472, 289, 17814, 480, 33516, 480, 293, 286, 478, 257, 7965, 50964], "temperature": 0.0, "avg_logprob": -0.06835426794034298, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.010119603015482426}, {"id": 304, "seek": 174864, "start": 1760.64, "end": 1765.8400000000001, "text": " research scientist at DeepMind and previously I have done my PhD in computer science at the", "tokens": [50964, 2132, 12662, 412, 14895, 44, 471, 293, 8046, 286, 362, 1096, 452, 14476, 294, 3820, 3497, 412, 264, 51224], "temperature": 0.0, "avg_logprob": -0.06835426794034298, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.010119603015482426}, {"id": 305, "seek": 174864, "start": 1765.8400000000001, "end": 1770.96, "text": " University of Cambridge where I'm actually still based and today we're actually here in Cambridge", "tokens": [51224, 3535, 295, 24876, 689, 286, 478, 767, 920, 2361, 293, 965, 321, 434, 767, 510, 294, 24876, 51480], "temperature": 0.0, "avg_logprob": -0.06835426794034298, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.010119603015482426}, {"id": 306, "seek": 174864, "start": 1770.96, "end": 1777.0400000000002, "text": " filming these shots and it is my great pleasure to be talking to you today about our work on", "tokens": [51480, 8869, 613, 8305, 293, 309, 307, 452, 869, 6834, 281, 312, 1417, 281, 291, 965, 466, 527, 589, 322, 51784], "temperature": 0.0, "avg_logprob": -0.06835426794034298, "compression_ratio": 1.6062717770034842, "no_speech_prob": 0.010119603015482426}, {"id": 307, "seek": 177704, "start": 1777.04, "end": 1783.92, "text": " geometric deep learning and related topics. I first got into computer science through competitive", "tokens": [50364, 33246, 2452, 2539, 293, 4077, 8378, 13, 286, 700, 658, 666, 3820, 3497, 807, 10043, 50708], "temperature": 0.0, "avg_logprob": -0.05371532440185547, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.010294981300830841}, {"id": 308, "seek": 177704, "start": 1783.92, "end": 1788.3999999999999, "text": " programming contests and classical algorithms the likes of which you might find in a traditional", "tokens": [50708, 9410, 660, 4409, 293, 13735, 14642, 264, 5902, 295, 597, 291, 1062, 915, 294, 257, 5164, 50932], "temperature": 0.0, "avg_logprob": -0.05371532440185547, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.010294981300830841}, {"id": 309, "seek": 177704, "start": 1788.3999999999999, "end": 1795.04, "text": " theoretical computer science textbook and this was primarily influenced by the way schooling", "tokens": [50932, 20864, 3820, 3497, 25591, 293, 341, 390, 10029, 15269, 538, 264, 636, 41677, 51264], "temperature": 0.0, "avg_logprob": -0.05371532440185547, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.010294981300830841}, {"id": 310, "seek": 177704, "start": 1795.04, "end": 1800.56, "text": " worked for gifted students back in my hometown of Belgrade in Serbia where students were generally", "tokens": [51264, 2732, 337, 27104, 1731, 646, 294, 452, 22112, 295, 6248, 8692, 294, 39461, 689, 1731, 645, 5101, 51540], "temperature": 0.0, "avg_logprob": -0.05371532440185547, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.010294981300830841}, {"id": 311, "seek": 177704, "start": 1800.56, "end": 1805.2, "text": " encouraged to take part in these theoretical contests and try to write programs that are just", "tokens": [51540, 14658, 281, 747, 644, 294, 613, 20864, 660, 4409, 293, 853, 281, 2464, 4268, 300, 366, 445, 51772], "temperature": 0.0, "avg_logprob": -0.05371532440185547, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.010294981300830841}, {"id": 312, "seek": 180520, "start": 1805.2, "end": 1811.6000000000001, "text": " going to finish as fast as possible or work as efficiently as possible over a certain set of", "tokens": [50364, 516, 281, 2413, 382, 2370, 382, 1944, 420, 589, 382, 19621, 382, 1944, 670, 257, 1629, 992, 295, 50684], "temperature": 0.0, "avg_logprob": -0.041976297895113625, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.005723326001316309}, {"id": 313, "seek": 180520, "start": 1811.6000000000001, "end": 1817.2, "text": " carefully contrived problems. All of this changed when I actually started my computer science degree", "tokens": [50684, 7500, 660, 470, 937, 2740, 13, 1057, 295, 341, 3105, 562, 286, 767, 1409, 452, 3820, 3497, 4314, 50964], "temperature": 0.0, "avg_logprob": -0.041976297895113625, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.005723326001316309}, {"id": 314, "seek": 180520, "start": 1817.2, "end": 1822.8, "text": " here at Cambridge where I was suddenly exposed to a much wider wealth of computer science topics", "tokens": [50964, 510, 412, 24876, 689, 286, 390, 5800, 9495, 281, 257, 709, 11842, 7203, 295, 3820, 3497, 8378, 51244], "temperature": 0.0, "avg_logprob": -0.041976297895113625, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.005723326001316309}, {"id": 315, "seek": 180520, "start": 1822.8, "end": 1827.2, "text": " than just theoretical computer science and algorithms and for a brief moment my interests", "tokens": [51244, 813, 445, 20864, 3820, 3497, 293, 14642, 293, 337, 257, 5353, 1623, 452, 8847, 51464], "temperature": 0.0, "avg_logprob": -0.041976297895113625, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.005723326001316309}, {"id": 316, "seek": 180520, "start": 1827.2, "end": 1833.44, "text": " drifted elsewhere. Everything started to come back together when I started my final year project", "tokens": [51464, 19699, 292, 14517, 13, 5471, 1409, 281, 808, 646, 1214, 562, 286, 1409, 452, 2572, 1064, 1716, 51776], "temperature": 0.0, "avg_logprob": -0.041976297895113625, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.005723326001316309}, {"id": 317, "seek": 183344, "start": 1833.44, "end": 1839.76, "text": " with Professor Pietro Leo at Cambridge and I had heard that bioinformatics is a topic that's", "tokens": [50364, 365, 8419, 41970, 340, 19344, 412, 24876, 293, 286, 632, 2198, 300, 12198, 37811, 30292, 307, 257, 4829, 300, 311, 50680], "temperature": 0.0, "avg_logprob": -0.07951525847117107, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.011309187859296799}, {"id": 318, "seek": 183344, "start": 1839.76, "end": 1844.3200000000002, "text": " brimming with classical algorithms and competitive programming algorithms specifically so I thought", "tokens": [50680, 738, 40471, 365, 13735, 14642, 293, 10043, 9410, 14642, 4682, 370, 286, 1194, 50908], "temperature": 0.0, "avg_logprob": -0.07951525847117107, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.011309187859296799}, {"id": 319, "seek": 183344, "start": 1844.3200000000002, "end": 1850.0, "text": " a project in this area would be a great way to bring these two closer. Unfortunately that was not", "tokens": [50908, 257, 1716, 294, 341, 1859, 576, 312, 257, 869, 636, 281, 1565, 613, 732, 4966, 13, 8590, 300, 390, 406, 51192], "temperature": 0.0, "avg_logprob": -0.07951525847117107, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.011309187859296799}, {"id": 320, "seek": 183344, "start": 1850.0, "end": 1857.28, "text": " to be as my mentor very quickly drew me into machine learning and that kind of spiraled out into my", "tokens": [51192, 281, 312, 382, 452, 14478, 588, 2661, 12804, 385, 666, 3479, 2539, 293, 300, 733, 295, 10733, 5573, 484, 666, 452, 51556], "temperature": 0.0, "avg_logprob": -0.07951525847117107, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.011309187859296799}, {"id": 321, "seek": 185728, "start": 1857.36, "end": 1863.76, "text": " PhD topics where I was for a brief moment focused on computational biology topics before", "tokens": [50368, 14476, 8378, 689, 286, 390, 337, 257, 5353, 1623, 5178, 322, 28270, 14956, 8378, 949, 50688], "temperature": 0.0, "avg_logprob": -0.050219472632350694, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.0779128223657608}, {"id": 322, "seek": 185728, "start": 1863.76, "end": 1869.04, "text": " eventually drifting to graph representation learning and eventually geometric deep learning.", "tokens": [50688, 4728, 37973, 281, 4295, 10290, 2539, 293, 4728, 33246, 2452, 2539, 13, 50952], "temperature": 0.0, "avg_logprob": -0.050219472632350694, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.0779128223657608}, {"id": 323, "seek": 185728, "start": 1869.04, "end": 1875.12, "text": " My journey into geometric deep learning started actually through investigating", "tokens": [50952, 1222, 4671, 666, 33246, 2452, 2539, 1409, 767, 807, 22858, 51256], "temperature": 0.0, "avg_logprob": -0.050219472632350694, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.0779128223657608}, {"id": 324, "seek": 185728, "start": 1875.84, "end": 1881.12, "text": " graph representation learning which I think for a very long time these two areas have been seen", "tokens": [51292, 4295, 10290, 2539, 597, 286, 519, 337, 257, 588, 938, 565, 613, 732, 3179, 362, 668, 1612, 51556], "temperature": 0.0, "avg_logprob": -0.050219472632350694, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.0779128223657608}, {"id": 325, "seek": 185728, "start": 1881.12, "end": 1885.6, "text": " as almost synonymous with one another because almost everything you come up with in the area", "tokens": [51556, 382, 1920, 5451, 18092, 365, 472, 1071, 570, 1920, 1203, 291, 808, 493, 365, 294, 264, 1859, 51780], "temperature": 0.0, "avg_logprob": -0.050219472632350694, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.0779128223657608}, {"id": 326, "seek": 188560, "start": 1885.6, "end": 1890.6399999999999, "text": " of geometric deep learning can be if you squint hard enough seen as a special case of graph", "tokens": [50364, 295, 33246, 2452, 2539, 393, 312, 498, 291, 2339, 686, 1152, 1547, 1612, 382, 257, 2121, 1389, 295, 4295, 50616], "temperature": 0.0, "avg_logprob": -0.08683466660348993, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.003701915265992284}, {"id": 327, "seek": 188560, "start": 1890.6399999999999, "end": 1896.8799999999999, "text": " representation learning. What originally brought me into this was an internship at Montreal's", "tokens": [50616, 10290, 2539, 13, 708, 7993, 3038, 385, 666, 341, 390, 364, 16861, 412, 34180, 311, 50928], "temperature": 0.0, "avg_logprob": -0.08683466660348993, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.003701915265992284}, {"id": 328, "seek": 188560, "start": 1896.8799999999999, "end": 1903.36, "text": " Artificial Intelligence Institute Miele where I worked alongside Joshua Bengio and Adriana Romero", "tokens": [50928, 5735, 10371, 27274, 9446, 376, 15949, 689, 286, 2732, 12385, 24005, 3964, 17862, 293, 32447, 2095, 10141, 2032, 51252], "temperature": 0.0, "avg_logprob": -0.08683466660348993, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.003701915265992284}, {"id": 329, "seek": 188560, "start": 1903.36, "end": 1910.0, "text": " on initially methodologies for processing data that lives on meshes of the human brain.", "tokens": [51252, 322, 9105, 3170, 6204, 337, 9007, 1412, 300, 2909, 322, 3813, 8076, 295, 264, 1952, 3567, 13, 51584], "temperature": 0.0, "avg_logprob": -0.08683466660348993, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.003701915265992284}, {"id": 330, "seek": 188560, "start": 1910.6399999999999, "end": 1915.12, "text": " We found out that the existing proposals for processing data over such a mesh", "tokens": [51616, 492, 1352, 484, 300, 264, 6741, 20198, 337, 9007, 1412, 670, 1270, 257, 17407, 51840], "temperature": 0.0, "avg_logprob": -0.08683466660348993, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.003701915265992284}, {"id": 331, "seek": 191512, "start": 1915.12, "end": 1919.4399999999998, "text": " both in graph neural networks and otherwise were not the most adequate for the kind of data", "tokens": [50364, 1293, 294, 4295, 18161, 9590, 293, 5911, 645, 406, 264, 881, 20927, 337, 264, 733, 295, 1412, 50580], "temperature": 0.0, "avg_logprob": -0.09557868996445014, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0019556803163141012}, {"id": 332, "seek": 191512, "start": 1919.4399999999998, "end": 1924.32, "text": " processing that we needed to do and we needed something that would be aligned more with image", "tokens": [50580, 9007, 300, 321, 2978, 281, 360, 293, 321, 2978, 746, 300, 576, 312, 17962, 544, 365, 3256, 50824], "temperature": 0.0, "avg_logprob": -0.09557868996445014, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0019556803163141012}, {"id": 333, "seek": 191512, "start": 1924.32, "end": 1930.08, "text": " convolutions in spirit in a way that allows us to give different influences to different", "tokens": [50824, 3754, 15892, 294, 3797, 294, 257, 636, 300, 4045, 505, 281, 976, 819, 21222, 281, 819, 51112], "temperature": 0.0, "avg_logprob": -0.09557868996445014, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0019556803163141012}, {"id": 334, "seek": 191512, "start": 1930.08, "end": 1936.08, "text": " neighbors in the mesh and this led us to propose graph attention networks which was a paper that", "tokens": [51112, 12512, 294, 264, 17407, 293, 341, 4684, 505, 281, 17421, 4295, 3202, 9590, 597, 390, 257, 3035, 300, 51412], "temperature": 0.0, "avg_logprob": -0.09557868996445014, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0019556803163141012}, {"id": 335, "seek": 191512, "start": 1936.08, "end": 1941.6799999999998, "text": " we published at Eichler 2018. It was actually my first top tier conference publication and", "tokens": [51412, 321, 6572, 412, 462, 480, 1918, 6096, 13, 467, 390, 767, 452, 700, 1192, 12362, 7586, 19953, 293, 51692], "temperature": 0.0, "avg_logprob": -0.09557868996445014, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.0019556803163141012}, {"id": 336, "seek": 194168, "start": 1942.4, "end": 1947.6000000000001, "text": " what I'm probably most well known for nowadays. The field of graph representation learning has", "tokens": [50400, 437, 286, 478, 1391, 881, 731, 2570, 337, 13434, 13, 440, 2519, 295, 4295, 10290, 2539, 575, 50660], "temperature": 0.0, "avg_logprob": -0.05991037037907815, "compression_ratio": 1.7330827067669172, "no_speech_prob": 0.011675966903567314}, {"id": 337, "seek": 194168, "start": 1947.6000000000001, "end": 1952.72, "text": " then spiraled completely out of control in terms of the quantity of papers being proposed.", "tokens": [50660, 550, 10733, 5573, 2584, 484, 295, 1969, 294, 2115, 295, 264, 11275, 295, 10577, 885, 10348, 13, 50916], "temperature": 0.0, "avg_logprob": -0.05991037037907815, "compression_ratio": 1.7330827067669172, "no_speech_prob": 0.011675966903567314}, {"id": 338, "seek": 194168, "start": 1953.6000000000001, "end": 1958.64, "text": " Only one year after the graph attention network paper came out I was reviewing for", "tokens": [50960, 5686, 472, 1064, 934, 264, 4295, 3202, 3209, 3035, 1361, 484, 286, 390, 19576, 337, 51212], "temperature": 0.0, "avg_logprob": -0.05991037037907815, "compression_ratio": 1.7330827067669172, "no_speech_prob": 0.011675966903567314}, {"id": 339, "seek": 194168, "start": 1958.64, "end": 1963.52, "text": " some of the conferences in the area and I found on my reviewing stack four or five papers that", "tokens": [51212, 512, 295, 264, 22032, 294, 264, 1859, 293, 286, 1352, 322, 452, 19576, 8630, 1451, 420, 1732, 10577, 300, 51456], "temperature": 0.0, "avg_logprob": -0.05991037037907815, "compression_ratio": 1.7330827067669172, "no_speech_prob": 0.011675966903567314}, {"id": 340, "seek": 194168, "start": 1963.52, "end": 1967.68, "text": " were extending graph attention nets in one way or another so the field certainly has become a lot", "tokens": [51456, 645, 24360, 4295, 3202, 36170, 294, 472, 636, 420, 1071, 370, 264, 2519, 3297, 575, 1813, 257, 688, 51664], "temperature": 0.0, "avg_logprob": -0.05991037037907815, "compression_ratio": 1.7330827067669172, "no_speech_prob": 0.011675966903567314}, {"id": 341, "seek": 196768, "start": 1967.68, "end": 1973.76, "text": " more vibrant because of a nice barrier of entry which is not too high. Recently Pettai has been", "tokens": [50364, 544, 21571, 570, 295, 257, 1481, 13357, 295, 8729, 597, 307, 406, 886, 1090, 13, 20072, 430, 3093, 1301, 575, 668, 50668], "temperature": 0.0, "avg_logprob": -0.07288425783567791, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0034139789640903473}, {"id": 342, "seek": 196768, "start": 1973.76, "end": 1979.92, "text": " doing some really interesting research in algorithmic reasoning. Part of the skill of a software", "tokens": [50668, 884, 512, 534, 1880, 2132, 294, 9284, 299, 21577, 13, 4100, 295, 264, 5389, 295, 257, 4722, 50976], "temperature": 0.0, "avg_logprob": -0.07288425783567791, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0034139789640903473}, {"id": 343, "seek": 196768, "start": 1979.92, "end": 1986.64, "text": " engineer lies in choosing which algorithm to use only rarely will an entirely novel algorithm be", "tokens": [50976, 11403, 9134, 294, 10875, 597, 9284, 281, 764, 787, 13752, 486, 364, 7696, 7613, 9284, 312, 51312], "temperature": 0.0, "avg_logprob": -0.07288425783567791, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0034139789640903473}, {"id": 344, "seek": 196768, "start": 1986.64, "end": 1993.28, "text": " warranted. The key guarantee which traditional symbolic algorithms give us is generalization", "tokens": [51312, 16354, 292, 13, 440, 2141, 10815, 597, 5164, 25755, 14642, 976, 505, 307, 2674, 2144, 51644], "temperature": 0.0, "avg_logprob": -0.07288425783567791, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0034139789640903473}, {"id": 345, "seek": 199328, "start": 1993.28, "end": 1999.12, "text": " to new situations. Traditional algorithms and the predictions given by deep learning models", "tokens": [50364, 281, 777, 6851, 13, 46738, 14642, 293, 264, 21264, 2212, 538, 2452, 2539, 5245, 50656], "temperature": 0.0, "avg_logprob": -0.05470290741363129, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.005908109247684479}, {"id": 346, "seek": 199328, "start": 1999.12, "end": 2004.8, "text": " have very different properties. The former provides strong guarantees but are inflexible", "tokens": [50656, 362, 588, 819, 7221, 13, 440, 5819, 6417, 2068, 32567, 457, 366, 1536, 2021, 964, 50940], "temperature": 0.0, "avg_logprob": -0.05470290741363129, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.005908109247684479}, {"id": 347, "seek": 199328, "start": 2004.8, "end": 2010.6399999999999, "text": " to the problem being tackled while the latter provide few guarantees but can adapt to a wide", "tokens": [50940, 281, 264, 1154, 885, 9426, 1493, 1339, 264, 18481, 2893, 1326, 32567, 457, 393, 6231, 281, 257, 4874, 51232], "temperature": 0.0, "avg_logprob": -0.05470290741363129, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.005908109247684479}, {"id": 348, "seek": 199328, "start": 2010.6399999999999, "end": 2016.8, "text": " range of problems. Now Pettai in his paper proposed a neural architecture which can take in natural", "tokens": [51232, 3613, 295, 2740, 13, 823, 430, 3093, 1301, 294, 702, 3035, 10348, 257, 18161, 9482, 597, 393, 747, 294, 3303, 51540], "temperature": 0.0, "avg_logprob": -0.05470290741363129, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.005908109247684479}, {"id": 349, "seek": 201680, "start": 2016.8, "end": 2023.76, "text": " inputs but output a graph of abstract outputs as well as natural outputs. Pettai believes that", "tokens": [50364, 15743, 457, 5598, 257, 4295, 295, 12649, 23930, 382, 731, 382, 3303, 23930, 13, 430, 3093, 1301, 12307, 300, 50712], "temperature": 0.0, "avg_logprob": -0.05063485170339609, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.001622040756046772}, {"id": 350, "seek": 201680, "start": 2023.76, "end": 2029.2, "text": " neural algorithmic reasoning will allow us to apply classical algorithms on inputs that they were", "tokens": [50712, 18161, 9284, 299, 21577, 486, 2089, 505, 281, 3079, 13735, 14642, 322, 15743, 300, 436, 645, 50984], "temperature": 0.0, "avg_logprob": -0.05063485170339609, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.001622040756046772}, {"id": 351, "seek": 201680, "start": 2029.2, "end": 2036.8799999999999, "text": " never originally designed for. I am studying algorithmic reasoning which is a novel area of", "tokens": [50984, 1128, 7993, 4761, 337, 13, 286, 669, 7601, 9284, 299, 21577, 597, 307, 257, 7613, 1859, 295, 51368], "temperature": 0.0, "avg_logprob": -0.05063485170339609, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.001622040756046772}, {"id": 352, "seek": 201680, "start": 2036.8799999999999, "end": 2042.96, "text": " representation learning that seeks to find neural networks that are as good as possible at", "tokens": [51368, 10290, 2539, 300, 28840, 281, 915, 18161, 9590, 300, 366, 382, 665, 382, 1944, 412, 51672], "temperature": 0.0, "avg_logprob": -0.05063485170339609, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.001622040756046772}, {"id": 353, "seek": 204296, "start": 2042.96, "end": 2047.52, "text": " imitating the computations of exactly the kind of classical algorithms that initially brought me", "tokens": [50364, 566, 16350, 264, 2807, 763, 295, 2293, 264, 733, 295, 13735, 14642, 300, 9105, 3038, 385, 50592], "temperature": 0.0, "avg_logprob": -0.048226150811887254, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.06747075915336609}, {"id": 354, "seek": 204296, "start": 2047.52, "end": 2053.76, "text": " to computer science. It turns out that this area is remarkably rich and could have remarkably big", "tokens": [50592, 281, 3820, 3497, 13, 467, 4523, 484, 300, 341, 1859, 307, 37381, 4593, 293, 727, 362, 37381, 955, 50904], "temperature": 0.0, "avg_logprob": -0.048226150811887254, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.06747075915336609}, {"id": 355, "seek": 204296, "start": 2053.76, "end": 2059.76, "text": " implications for machine learning in general because it could bring the best of the algorithmic", "tokens": [50904, 16602, 337, 3479, 2539, 294, 2674, 570, 309, 727, 1565, 264, 1151, 295, 264, 9284, 299, 51204], "temperature": 0.0, "avg_logprob": -0.048226150811887254, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.06747075915336609}, {"id": 356, "seek": 204296, "start": 2059.76, "end": 2065.6, "text": " domain into the domain of neural networks and if you look at the pros and cons of the two you'll", "tokens": [51204, 9274, 666, 264, 9274, 295, 18161, 9590, 293, 498, 291, 574, 412, 264, 6267, 293, 1014, 295, 264, 732, 291, 603, 51496], "temperature": 0.0, "avg_logprob": -0.048226150811887254, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.06747075915336609}, {"id": 357, "seek": 204296, "start": 2065.6, "end": 2069.84, "text": " find that they're very complementary. So the fusion of the two can really bring the kinds of", "tokens": [51496, 915, 300, 436, 434, 588, 40705, 13, 407, 264, 23100, 295, 264, 732, 393, 534, 1565, 264, 3685, 295, 51708], "temperature": 0.0, "avg_logprob": -0.048226150811887254, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.06747075915336609}, {"id": 358, "seek": 206984, "start": 2069.84, "end": 2076.6400000000003, "text": " benefits we haven't seen before. So I am very pleased to say that I'm among those researchers", "tokens": [50364, 5311, 321, 2378, 380, 1612, 949, 13, 407, 286, 669, 588, 10587, 281, 584, 300, 286, 478, 3654, 729, 10309, 50704], "temperature": 0.0, "avg_logprob": -0.10698833850899128, "compression_ratio": 1.6159169550173011, "no_speech_prob": 0.010942060500383377}, {"id": 359, "seek": 206984, "start": 2076.6400000000003, "end": 2082.6400000000003, "text": " that is extremely proud and happy of what I do because it brings together some of my earliest", "tokens": [50704, 300, 307, 4664, 4570, 293, 2055, 295, 437, 286, 360, 570, 309, 5607, 1214, 512, 295, 452, 20573, 51004], "temperature": 0.0, "avg_logprob": -0.10698833850899128, "compression_ratio": 1.6159169550173011, "no_speech_prob": 0.010942060500383377}, {"id": 360, "seek": 206984, "start": 2082.6400000000003, "end": 2087.36, "text": " passions in computer science with the latest trends in machine learning and especially", "tokens": [51004, 30640, 294, 3820, 3497, 365, 264, 6792, 13892, 294, 3479, 2539, 293, 2318, 51240], "temperature": 0.0, "avg_logprob": -0.10698833850899128, "compression_ratio": 1.6159169550173011, "no_speech_prob": 0.010942060500383377}, {"id": 361, "seek": 206984, "start": 2087.36, "end": 2093.44, "text": " geometric deep learning which we recently released a proto book about with Joan, Michael and Taco.", "tokens": [51240, 33246, 2452, 2539, 597, 321, 3938, 4736, 257, 47896, 1446, 466, 365, 25748, 11, 5116, 293, 37992, 13, 51544], "temperature": 0.0, "avg_logprob": -0.10698833850899128, "compression_ratio": 1.6159169550173011, "no_speech_prob": 0.010942060500383377}, {"id": 362, "seek": 206984, "start": 2093.44, "end": 2098.8, "text": " We spoke to Christian Saagedi and he's doing some interesting work with algorithmic reasoning", "tokens": [51544, 492, 7179, 281, 5778, 6299, 559, 10323, 293, 415, 311, 884, 512, 1880, 589, 365, 9284, 299, 21577, 51812], "temperature": 0.0, "avg_logprob": -0.10698833850899128, "compression_ratio": 1.6159169550173011, "no_speech_prob": 0.010942060500383377}, {"id": 363, "seek": 209880, "start": 2098.8, "end": 2104.88, "text": " creating abstract syntax trees to represent mathematical theorems for example and then", "tokens": [50364, 4084, 12649, 28431, 5852, 281, 2906, 18894, 10299, 2592, 337, 1365, 293, 550, 50668], "temperature": 0.0, "avg_logprob": -0.07245664094623766, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.004573870915919542}, {"id": 364, "seek": 209880, "start": 2104.88, "end": 2109.92, "text": " he believes that in that representation space he projects it all into a Euclidean space that", "tokens": [50668, 415, 12307, 300, 294, 300, 10290, 1901, 415, 4455, 309, 439, 666, 257, 462, 1311, 31264, 282, 1901, 300, 50920], "temperature": 0.0, "avg_logprob": -0.07245664094623766, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.004573870915919542}, {"id": 365, "seek": 209880, "start": 2109.92, "end": 2115.44, "text": " there's some interesting interpolative points in that space but again surely there must be some", "tokens": [50920, 456, 311, 512, 1880, 44902, 1166, 2793, 294, 300, 1901, 457, 797, 11468, 456, 1633, 312, 512, 51196], "temperature": 0.0, "avg_logprob": -0.07245664094623766, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.004573870915919542}, {"id": 366, "seek": 209880, "start": 2115.44, "end": 2120.32, "text": " deeper structure which analogizes mathematics which would allow us to extrapolate and discover", "tokens": [51196, 7731, 3877, 597, 16660, 5660, 18666, 597, 576, 2089, 505, 281, 48224, 473, 293, 4411, 51440], "temperature": 0.0, "avg_logprob": -0.07245664094623766, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.004573870915919542}, {"id": 367, "seek": 209880, "start": 2120.32, "end": 2125.28, "text": " new interesting mathematics. It just feels that what we're missing is the right kind of structure.", "tokens": [51440, 777, 1880, 18666, 13, 467, 445, 3417, 300, 437, 321, 434, 5361, 307, 264, 558, 733, 295, 3877, 13, 51688], "temperature": 0.0, "avg_logprob": -0.07245664094623766, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.004573870915919542}, {"id": 368, "seek": 212528, "start": 2125.28, "end": 2130.32, "text": " I think for mathematics it's relatively easy to formalize it because well we can write logic", "tokens": [50364, 286, 519, 337, 18666, 309, 311, 7226, 1858, 281, 9860, 1125, 309, 570, 731, 321, 393, 2464, 9952, 50616], "temperature": 0.0, "avg_logprob": -0.0662078857421875, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.002558925421908498}, {"id": 369, "seek": 212528, "start": 2130.32, "end": 2136.1600000000003, "text": " rules and basically we can build mathematics axiomatically from very basic principles. These", "tokens": [50616, 4474, 293, 1936, 321, 393, 1322, 18666, 6360, 72, 298, 5030, 490, 588, 3875, 9156, 13, 1981, 50908], "temperature": 0.0, "avg_logprob": -0.0662078857421875, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.002558925421908498}, {"id": 370, "seek": 212528, "start": 2136.1600000000003, "end": 2141.44, "text": " methods are already being used for computer proof of certain theorems. I think it's not", "tokens": [50908, 7150, 366, 1217, 885, 1143, 337, 3820, 8177, 295, 1629, 10299, 2592, 13, 286, 519, 309, 311, 406, 51172], "temperature": 0.0, "avg_logprob": -0.0662078857421875, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.002558925421908498}, {"id": 371, "seek": 212528, "start": 2141.44, "end": 2147.0400000000004, "text": " well regarded by the purists in pure mathematics but probably they will need to accept it and well", "tokens": [51172, 731, 26047, 538, 264, 1864, 1751, 294, 6075, 18666, 457, 1391, 436, 486, 643, 281, 3241, 309, 293, 731, 51452], "temperature": 0.0, "avg_logprob": -0.0662078857421875, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.002558925421908498}, {"id": 372, "seek": 212528, "start": 2147.0400000000004, "end": 2152.0, "text": " you know maybe there will be fields medal that will be given for a proof that is done by a computer", "tokens": [51452, 291, 458, 1310, 456, 486, 312, 7909, 21364, 300, 486, 312, 2212, 337, 257, 8177, 300, 307, 1096, 538, 257, 3820, 51700], "temperature": 0.0, "avg_logprob": -0.0662078857421875, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.002558925421908498}, {"id": 373, "seek": 215200, "start": 2152.0, "end": 2158.16, "text": " I think even recently there are some important breakthrough results proofs that were given by a", "tokens": [50364, 286, 519, 754, 3938, 456, 366, 512, 1021, 22397, 3542, 8177, 82, 300, 645, 2212, 538, 257, 50672], "temperature": 0.0, "avg_logprob": -0.11036151885986328, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0028980460483580828}, {"id": 374, "seek": 215200, "start": 2158.16, "end": 2165.44, "text": " computer so it is probably just the beginning of a new way of doing science essentially even", "tokens": [50672, 3820, 370, 309, 307, 1391, 445, 264, 2863, 295, 257, 777, 636, 295, 884, 3497, 4476, 754, 51036], "temperature": 0.0, "avg_logprob": -0.11036151885986328, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0028980460483580828}, {"id": 375, "seek": 215200, "start": 2165.44, "end": 2170.96, "text": " as pure science as creative science as mathematics which was considered really the hallmark of human", "tokens": [51036, 382, 6075, 3497, 382, 5880, 3497, 382, 18666, 597, 390, 4888, 534, 264, 6500, 5638, 295, 1952, 51312], "temperature": 0.0, "avg_logprob": -0.11036151885986328, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0028980460483580828}, {"id": 376, "seek": 215200, "start": 2170.96, "end": 2176.08, "text": " intelligence it can be maybe if not replaced assisted by by artificial intelligence.", "tokens": [51312, 7599, 309, 393, 312, 1310, 498, 406, 10772, 30291, 538, 538, 11677, 7599, 13, 51568], "temperature": 0.0, "avg_logprob": -0.11036151885986328, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0028980460483580828}, {"id": 377, "seek": 217608, "start": 2177.04, "end": 2182.96, "text": " Petr invokes Daniel Kahneman's system one and system two. He thinks that we need something", "tokens": [50412, 10472, 81, 1048, 8606, 8033, 591, 12140, 15023, 311, 1185, 472, 293, 1185, 732, 13, 634, 7309, 300, 321, 643, 746, 50708], "temperature": 0.0, "avg_logprob": -0.10676811518294088, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.005054908338934183}, {"id": 378, "seek": 217608, "start": 2182.96, "end": 2189.92, "text": " like system two to achieve the kind of reasoning and generalization which currently eludes us", "tokens": [50708, 411, 1185, 732, 281, 4584, 264, 733, 295, 21577, 293, 2674, 2144, 597, 4362, 806, 10131, 505, 51056], "temperature": 0.0, "avg_logprob": -0.10676811518294088, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.005054908338934183}, {"id": 379, "seek": 217608, "start": 2190.56, "end": 2195.92, "text": " in deep learning models. What I'm holding in my hands right now is the international bestseller", "tokens": [51088, 294, 2452, 2539, 5245, 13, 708, 286, 478, 5061, 294, 452, 2377, 558, 586, 307, 264, 5058, 1151, 405, 4658, 51356], "temperature": 0.0, "avg_logprob": -0.10676811518294088, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.005054908338934183}, {"id": 380, "seek": 217608, "start": 2195.92, "end": 2202.96, "text": " on thinking fast and slow from the famous Nobel Prize winner Daniel Kahneman. This book can be seen", "tokens": [51356, 322, 1953, 2370, 293, 2964, 490, 264, 4618, 24611, 22604, 8507, 8033, 591, 12140, 15023, 13, 639, 1446, 393, 312, 1612, 51708], "temperature": 0.0, "avg_logprob": -0.10676811518294088, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.005054908338934183}, {"id": 381, "seek": 220296, "start": 2202.96, "end": 2208.7200000000003, "text": " as one of the main theses behind my ongoing work in algorithmic reasoning and what it stands for", "tokens": [50364, 382, 472, 295, 264, 2135, 264, 6196, 2261, 452, 10452, 589, 294, 9284, 299, 21577, 293, 437, 309, 7382, 337, 50652], "temperature": 0.0, "avg_logprob": -0.06911684305239947, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.02126118913292885}, {"id": 382, "seek": 220296, "start": 2208.7200000000003, "end": 2214.32, "text": " because it argues that fundamentally we as humans employ two different systems that operate at", "tokens": [50652, 570, 309, 38218, 300, 17879, 321, 382, 6255, 3188, 732, 819, 3652, 300, 9651, 412, 50932], "temperature": 0.0, "avg_logprob": -0.06911684305239947, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.02126118913292885}, {"id": 383, "seek": 220296, "start": 2214.32, "end": 2220.4, "text": " different rates system one which primarily deals with perceptive tasks and system two which deals", "tokens": [50932, 819, 6846, 1185, 472, 597, 10029, 11215, 365, 43276, 488, 9608, 293, 1185, 732, 597, 11215, 51236], "temperature": 0.0, "avg_logprob": -0.06911684305239947, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.02126118913292885}, {"id": 384, "seek": 220296, "start": 2220.4, "end": 2227.04, "text": " with longer range reasoning tasks and it is my belief that currently where our research in neural", "tokens": [51236, 365, 2854, 3613, 21577, 9608, 293, 309, 307, 452, 7107, 300, 4362, 689, 527, 2132, 294, 18161, 51568], "temperature": 0.0, "avg_logprob": -0.06911684305239947, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.02126118913292885}, {"id": 385, "seek": 222704, "start": 2227.04, "end": 2233.2799999999997, "text": " networks has taken us is to get really really good at automating away system one so being able to", "tokens": [50364, 9590, 575, 2726, 505, 307, 281, 483, 534, 534, 665, 412, 3553, 990, 1314, 1185, 472, 370, 885, 1075, 281, 50676], "temperature": 0.0, "avg_logprob": -0.07123515217803246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.05495702847838402}, {"id": 386, "seek": 222704, "start": 2233.2799999999997, "end": 2240.24, "text": " perform perceptive tasks from large quantities of observed data probably in a not too dissimilar", "tokens": [50676, 2042, 43276, 488, 9608, 490, 2416, 22927, 295, 13095, 1412, 1391, 294, 257, 406, 886, 7802, 332, 2202, 51024], "temperature": 0.0, "avg_logprob": -0.07123515217803246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.05495702847838402}, {"id": 387, "seek": 222704, "start": 2240.24, "end": 2246.4, "text": " manner from the way humans do it. What I feel is really missing from these architectures nowadays", "tokens": [51024, 9060, 490, 264, 636, 6255, 360, 309, 13, 708, 286, 841, 307, 534, 5361, 490, 613, 6331, 1303, 13434, 51332], "temperature": 0.0, "avg_logprob": -0.07123515217803246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.05495702847838402}, {"id": 388, "seek": 222704, "start": 2246.4, "end": 2251.6, "text": " is the system two aspect being able to actually take these percepts that we've acquired from the", "tokens": [51332, 307, 264, 1185, 732, 4171, 885, 1075, 281, 767, 747, 613, 43276, 82, 300, 321, 600, 17554, 490, 264, 51592], "temperature": 0.0, "avg_logprob": -0.07123515217803246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.05495702847838402}, {"id": 389, "seek": 225160, "start": 2251.6, "end": 2257.7599999999998, "text": " environment and actually properly do rigid reasoning over them in a manner that will stay", "tokens": [50364, 2823, 293, 767, 6108, 360, 22195, 21577, 670, 552, 294, 257, 9060, 300, 486, 1754, 50672], "temperature": 0.0, "avg_logprob": -0.05336775779724121, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.011866067536175251}, {"id": 390, "seek": 225160, "start": 2257.7599999999998, "end": 2263.2, "text": " consistent even if we drastically change the number of objects slightly perturb the laws of physics", "tokens": [50672, 8398, 754, 498, 321, 29673, 1319, 264, 1230, 295, 6565, 4748, 40468, 264, 6064, 295, 10649, 50944], "temperature": 0.0, "avg_logprob": -0.05336775779724121, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.011866067536175251}, {"id": 391, "seek": 225160, "start": 2263.2, "end": 2268.64, "text": " or something like that. In my opinion algorithmic reasoning the art of capturing these kinds of", "tokens": [50944, 420, 746, 411, 300, 13, 682, 452, 4800, 9284, 299, 21577, 264, 1523, 295, 23384, 613, 3685, 295, 51216], "temperature": 0.0, "avg_logprob": -0.05336775779724121, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.011866067536175251}, {"id": 392, "seek": 225160, "start": 2268.64, "end": 2273.52, "text": " reasoning computations inside a neural network that was trained specifically for that purpose", "tokens": [51216, 21577, 2807, 763, 1854, 257, 18161, 3209, 300, 390, 8895, 4682, 337, 300, 4334, 51460], "temperature": 0.0, "avg_logprob": -0.05336775779724121, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.011866067536175251}, {"id": 393, "seek": 225160, "start": 2273.52, "end": 2277.8399999999997, "text": " and then slotting that neural network into a different architecture that works with raw", "tokens": [51460, 293, 550, 14747, 783, 300, 18161, 3209, 666, 257, 819, 9482, 300, 1985, 365, 8936, 51676], "temperature": 0.0, "avg_logprob": -0.05336775779724121, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.011866067536175251}, {"id": 394, "seek": 227784, "start": 2277.84, "end": 2283.36, "text": " percepts is one potentially very promising blueprint that will take the space of classical", "tokens": [50364, 43276, 82, 307, 472, 7263, 588, 20257, 35868, 300, 486, 747, 264, 1901, 295, 13735, 50640], "temperature": 0.0, "avg_logprob": -0.12804020676657418, "compression_ratio": 1.6360424028268552, "no_speech_prob": 0.013213508762419224}, {"id": 395, "seek": 227784, "start": 2283.36, "end": 2288.6400000000003, "text": " algorithms that we have been building in this system two space and carry them over into raw", "tokens": [50640, 14642, 300, 321, 362, 668, 2390, 294, 341, 1185, 732, 1901, 293, 3985, 552, 670, 666, 8936, 50904], "temperature": 0.0, "avg_logprob": -0.12804020676657418, "compression_ratio": 1.6360424028268552, "no_speech_prob": 0.013213508762419224}, {"id": 396, "seek": 227784, "start": 2288.6400000000003, "end": 2293.6000000000004, "text": " perceptive inputs which these algorithms were very rarely designed to work over. This is Dr.", "tokens": [50904, 43276, 488, 15743, 597, 613, 14642, 645, 588, 13752, 4761, 281, 589, 670, 13, 639, 307, 2491, 13, 51152], "temperature": 0.0, "avg_logprob": -0.12804020676657418, "compression_ratio": 1.6360424028268552, "no_speech_prob": 0.013213508762419224}, {"id": 397, "seek": 227784, "start": 2293.6000000000004, "end": 2300.96, "text": " Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher at Qualco AI Research and I work on", "tokens": [51152, 9118, 78, 30861, 268, 13, 2425, 11, 286, 478, 9118, 78, 30861, 268, 13, 286, 478, 257, 21751, 412, 13616, 1291, 7318, 10303, 293, 286, 589, 322, 51520], "temperature": 0.0, "avg_logprob": -0.12804020676657418, "compression_ratio": 1.6360424028268552, "no_speech_prob": 0.013213508762419224}, {"id": 398, "seek": 227784, "start": 2300.96, "end": 2306.96, "text": " geometric deep learning, equivariate networks and more recently also on causal inference and causal", "tokens": [51520, 33246, 2452, 2539, 11, 1267, 592, 3504, 473, 9590, 293, 544, 3938, 611, 322, 38755, 38253, 293, 38755, 51820], "temperature": 0.0, "avg_logprob": -0.12804020676657418, "compression_ratio": 1.6360424028268552, "no_speech_prob": 0.013213508762419224}, {"id": 399, "seek": 230696, "start": 2306.96, "end": 2313.92, "text": " representation learning. Now I've been interested for a number of years already since about 2013", "tokens": [50364, 10290, 2539, 13, 823, 286, 600, 668, 3102, 337, 257, 1230, 295, 924, 1217, 1670, 466, 9012, 50712], "temperature": 0.0, "avg_logprob": -0.08496929724005205, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005813791882246733}, {"id": 400, "seek": 230696, "start": 2313.92, "end": 2319.28, "text": " in the application of ideas around symmetry, invariance, equivariance and the underlying", "tokens": [50712, 294, 264, 3861, 295, 3487, 926, 25440, 11, 33270, 719, 11, 1267, 592, 3504, 719, 293, 264, 14217, 50980], "temperature": 0.0, "avg_logprob": -0.08496929724005205, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005813791882246733}, {"id": 401, "seek": 230696, "start": 2319.28, "end": 2325.44, "text": " mathematics of group theory and group representation theory to machine learning and deep learning", "tokens": [50980, 18666, 295, 1594, 5261, 293, 1594, 10290, 5261, 281, 3479, 2539, 293, 2452, 2539, 51288], "temperature": 0.0, "avg_logprob": -0.08496929724005205, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005813791882246733}, {"id": 402, "seek": 230696, "start": 2325.44, "end": 2330.7200000000003, "text": " specifically. And so it's been quite exciting to see over the last few years really the blossoming", "tokens": [51288, 4682, 13, 400, 370, 309, 311, 668, 1596, 4670, 281, 536, 670, 264, 1036, 1326, 924, 534, 264, 22956, 10539, 51552], "temperature": 0.0, "avg_logprob": -0.08496929724005205, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005813791882246733}, {"id": 403, "seek": 233072, "start": 2330.72, "end": 2337.12, "text": " of this field that we now call geometric deep learning. Many new methods such as various kinds", "tokens": [50364, 295, 341, 2519, 300, 321, 586, 818, 33246, 2452, 2539, 13, 5126, 777, 7150, 1270, 382, 3683, 3685, 50684], "temperature": 0.0, "avg_logprob": -0.07004286294960115, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.03408762812614441}, {"id": 404, "seek": 233072, "start": 2337.12, "end": 2343.52, "text": " of equivariate convolutions for different spaces, different groups of symmetries, different geometric", "tokens": [50684, 295, 1267, 592, 3504, 473, 3754, 15892, 337, 819, 7673, 11, 819, 3935, 295, 14232, 302, 2244, 11, 819, 33246, 51004], "temperature": 0.0, "avg_logprob": -0.07004286294960115, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.03408762812614441}, {"id": 405, "seek": 233072, "start": 2343.52, "end": 2349.52, "text": " feature types, equivariate transformers and attention mechanisms, point cloud networks,", "tokens": [51004, 4111, 3467, 11, 1267, 592, 3504, 473, 4088, 433, 293, 3202, 15902, 11, 935, 4588, 9590, 11, 51304], "temperature": 0.0, "avg_logprob": -0.07004286294960115, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.03408762812614441}, {"id": 406, "seek": 233072, "start": 2349.52, "end": 2355.12, "text": " graph neural networks and so forth. And along with these new methods also a large number of", "tokens": [51304, 4295, 18161, 9590, 293, 370, 5220, 13, 400, 2051, 365, 613, 777, 7150, 611, 257, 2416, 1230, 295, 51584], "temperature": 0.0, "avg_logprob": -0.07004286294960115, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.03408762812614441}, {"id": 407, "seek": 235512, "start": 2355.68, "end": 2361.8399999999997, "text": " applications that have been tackled. Anything from medical imaging to the analysis of global", "tokens": [50392, 5821, 300, 362, 668, 9426, 1493, 13, 11998, 490, 4625, 25036, 281, 264, 5215, 295, 4338, 50700], "temperature": 0.0, "avg_logprob": -0.061011757961539334, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.05661926418542862}, {"id": 408, "seek": 235512, "start": 2361.8399999999997, "end": 2369.44, "text": " weather and climate data to the analysis of DNA sequences and proteins and other kinds of molecules.", "tokens": [50700, 5503, 293, 5659, 1412, 281, 264, 5215, 295, 8272, 22978, 293, 15577, 293, 661, 3685, 295, 13093, 13, 51080], "temperature": 0.0, "avg_logprob": -0.061011757961539334, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.05661926418542862}, {"id": 409, "seek": 235512, "start": 2369.44, "end": 2374.56, "text": " So if you apply this mindset, the first question you ask when faced with such a new problem is", "tokens": [51080, 407, 498, 291, 3079, 341, 12543, 11, 264, 700, 1168, 291, 1029, 562, 11446, 365, 1270, 257, 777, 1154, 307, 51336], "temperature": 0.0, "avg_logprob": -0.061011757961539334, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.05661926418542862}, {"id": 410, "seek": 235512, "start": 2374.56, "end": 2381.2799999999997, "text": " what are the symmetries? What are the transformations that I can apply to my data that may change the", "tokens": [51336, 437, 366, 264, 14232, 302, 2244, 30, 708, 366, 264, 34852, 300, 286, 393, 3079, 281, 452, 1412, 300, 815, 1319, 264, 51672], "temperature": 0.0, "avg_logprob": -0.061011757961539334, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.05661926418542862}, {"id": 411, "seek": 238128, "start": 2381.28, "end": 2387.6000000000004, "text": " numerical representation of my data as stored in my computer, but that nevertheless don't change", "tokens": [50364, 29054, 10290, 295, 452, 1412, 382, 12187, 294, 452, 3820, 11, 457, 300, 26924, 500, 380, 1319, 50680], "temperature": 0.0, "avg_logprob": -0.12329795796384094, "compression_ratio": 1.6, "no_speech_prob": 0.013633852824568748}, {"id": 412, "seek": 238128, "start": 2387.6000000000004, "end": 2393.76, "text": " the underlying objects we're interested in. Whether that's reordering the nodes of a graph,", "tokens": [50680, 264, 14217, 6565, 321, 434, 3102, 294, 13, 8503, 300, 311, 319, 765, 1794, 264, 13891, 295, 257, 4295, 11, 50988], "temperature": 0.0, "avg_logprob": -0.12329795796384094, "compression_ratio": 1.6, "no_speech_prob": 0.013633852824568748}, {"id": 413, "seek": 238128, "start": 2394.32, "end": 2402.32, "text": " rotating a molecule in 3D or many other kinds of symmetries. Once you know the group of symmetries,", "tokens": [51016, 19627, 257, 15582, 294, 805, 35, 420, 867, 661, 3685, 295, 14232, 302, 2244, 13, 3443, 291, 458, 264, 1594, 295, 14232, 302, 2244, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12329795796384094, "compression_ratio": 1.6, "no_speech_prob": 0.013633852824568748}, {"id": 414, "seek": 238128, "start": 2402.32, "end": 2409.28, "text": " you can then develop a neural network that's equivariate symmetries. And hopefully is a", "tokens": [51416, 291, 393, 550, 1499, 257, 18161, 3209, 300, 311, 1267, 592, 3504, 473, 14232, 302, 2244, 13, 400, 4696, 307, 257, 51764], "temperature": 0.0, "avg_logprob": -0.12329795796384094, "compression_ratio": 1.6, "no_speech_prob": 0.013633852824568748}, {"id": 415, "seek": 240928, "start": 2409.36, "end": 2416.1600000000003, "text": " universal approximator among equivariate functions. What we found, what many others have found time", "tokens": [50368, 11455, 8542, 1639, 3654, 1267, 592, 3504, 473, 6828, 13, 708, 321, 1352, 11, 437, 867, 2357, 362, 1352, 565, 50708], "temperature": 0.0, "avg_logprob": -0.11414940482691714, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.020935503765940666}, {"id": 416, "seek": 240928, "start": 2416.1600000000003, "end": 2422.2400000000002, "text": " and again, is that if you build this symmetry prior to your network, if you make your network", "tokens": [50708, 293, 797, 11, 307, 300, 498, 291, 1322, 341, 25440, 4059, 281, 428, 3209, 11, 498, 291, 652, 428, 3209, 51012], "temperature": 0.0, "avg_logprob": -0.11414940482691714, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.020935503765940666}, {"id": 417, "seek": 240928, "start": 2422.2400000000002, "end": 2427.76, "text": " equivariate, it is bound to be much more data efficient and to generalize much better.", "tokens": [51012, 1267, 592, 3504, 473, 11, 309, 307, 5472, 281, 312, 709, 544, 1412, 7148, 293, 281, 2674, 1125, 709, 1101, 13, 51288], "temperature": 0.0, "avg_logprob": -0.11414940482691714, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.020935503765940666}, {"id": 418, "seek": 240928, "start": 2428.5600000000004, "end": 2438.4, "text": " Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this book", "tokens": [51328, 1911, 7172, 11, 869, 281, 312, 510, 13, 407, 1338, 11, 264, 35868, 13, 407, 437, 321, 5334, 382, 321, 645, 3579, 341, 1446, 51820], "temperature": 0.0, "avg_logprob": -0.11414940482691714, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.020935503765940666}, {"id": 419, "seek": 243840, "start": 2438.4, "end": 2444.32, "text": " is that really a lot of different architectures, they can be understood in one as essentially a", "tokens": [50364, 307, 300, 534, 257, 688, 295, 819, 6331, 1303, 11, 436, 393, 312, 7320, 294, 472, 382, 4476, 257, 50660], "temperature": 0.0, "avg_logprob": -0.09438710553305489, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002671598456799984}, {"id": 420, "seek": 243840, "start": 2444.32, "end": 2450.4, "text": " special cases of one generic structure that we call the geometric deep learning blueprint.", "tokens": [50660, 2121, 3331, 295, 472, 19577, 3877, 300, 321, 818, 264, 33246, 2452, 2539, 35868, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09438710553305489, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002671598456799984}, {"id": 421, "seek": 243840, "start": 2451.12, "end": 2458.96, "text": " So to explain a little bit about what this is all about, the blueprint refers to first of all", "tokens": [51000, 407, 281, 2903, 257, 707, 857, 466, 437, 341, 307, 439, 466, 11, 264, 35868, 14942, 281, 700, 295, 439, 51392], "temperature": 0.0, "avg_logprob": -0.09438710553305489, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002671598456799984}, {"id": 422, "seek": 243840, "start": 2458.96, "end": 2466.32, "text": " feature spaces. So I'll explain how we model those. And then it refers to maps between feature", "tokens": [51392, 4111, 7673, 13, 407, 286, 603, 2903, 577, 321, 2316, 729, 13, 400, 550, 309, 14942, 281, 11317, 1296, 4111, 51760], "temperature": 0.0, "avg_logprob": -0.09438710553305489, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002671598456799984}, {"id": 423, "seek": 246632, "start": 2466.32, "end": 2472.48, "text": " spaces or layers of the network. And they also have to somehow respect the structure of the", "tokens": [50364, 7673, 420, 7914, 295, 264, 3209, 13, 400, 436, 611, 362, 281, 6063, 3104, 264, 3877, 295, 264, 50672], "temperature": 0.0, "avg_logprob": -0.10333764422070849, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010323766618967056}, {"id": 424, "seek": 246632, "start": 2472.48, "end": 2478.7200000000003, "text": " feature spaces. So in all cases, whether it's, you know, graph neural net, a network for processing", "tokens": [50672, 4111, 7673, 13, 407, 294, 439, 3331, 11, 1968, 309, 311, 11, 291, 458, 11, 4295, 18161, 2533, 11, 257, 3209, 337, 9007, 50984], "temperature": 0.0, "avg_logprob": -0.10333764422070849, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010323766618967056}, {"id": 425, "seek": 246632, "start": 2478.7200000000003, "end": 2486.0800000000004, "text": " images on the plane, or a network for processing signals on a sphere like global weather data,", "tokens": [50984, 5267, 322, 264, 5720, 11, 420, 257, 3209, 337, 9007, 12354, 322, 257, 16687, 411, 4338, 5503, 1412, 11, 51352], "temperature": 0.0, "avg_logprob": -0.10333764422070849, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010323766618967056}, {"id": 426, "seek": 246632, "start": 2486.88, "end": 2493.1200000000003, "text": " we're dealing with data that lives on a domain. So the domain in the examples I just gave would", "tokens": [51392, 321, 434, 6260, 365, 1412, 300, 2909, 322, 257, 9274, 13, 407, 264, 9274, 294, 264, 5110, 286, 445, 2729, 576, 51704], "temperature": 0.0, "avg_logprob": -0.10333764422070849, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010323766618967056}, {"id": 427, "seek": 249312, "start": 2493.12, "end": 2499.68, "text": " be the set of nodes of the graph, or perhaps also the set of edges, the points on the plane,", "tokens": [50364, 312, 264, 992, 295, 13891, 295, 264, 4295, 11, 420, 4317, 611, 264, 992, 295, 8819, 11, 264, 2793, 322, 264, 5720, 11, 50692], "temperature": 0.0, "avg_logprob": -0.10414538344716638, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.03020191192626953}, {"id": 428, "seek": 249312, "start": 2499.68, "end": 2505.3599999999997, "text": " or the points on the sphere. And of course, you can think of many other examples as well.", "tokens": [50692, 420, 264, 2793, 322, 264, 16687, 13, 400, 295, 1164, 11, 291, 393, 519, 295, 867, 661, 5110, 382, 731, 13, 50976], "temperature": 0.0, "avg_logprob": -0.10414538344716638, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.03020191192626953}, {"id": 429, "seek": 249312, "start": 2505.3599999999997, "end": 2511.52, "text": " This is what we call the domain. We write it as omega. It's typically, it's a set, first of all,", "tokens": [50976, 639, 307, 437, 321, 818, 264, 9274, 13, 492, 2464, 309, 382, 10498, 13, 467, 311, 5850, 11, 309, 311, 257, 992, 11, 700, 295, 439, 11, 51284], "temperature": 0.0, "avg_logprob": -0.10414538344716638, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.03020191192626953}, {"id": 430, "seek": 249312, "start": 2511.52, "end": 2516.64, "text": " and it may have some additional structure. So in the case of the sphere, it has some interesting", "tokens": [51284, 293, 309, 815, 362, 512, 4497, 3877, 13, 407, 294, 264, 1389, 295, 264, 16687, 11, 309, 575, 512, 1880, 51540], "temperature": 0.0, "avg_logprob": -0.10414538344716638, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.03020191192626953}, {"id": 431, "seek": 249312, "start": 2516.64, "end": 2522.56, "text": " topology. And typically, we also want to think about the geometry, want to think about distances", "tokens": [51540, 1192, 1793, 13, 400, 5850, 11, 321, 611, 528, 281, 519, 466, 264, 18426, 11, 528, 281, 519, 466, 22182, 51836], "temperature": 0.0, "avg_logprob": -0.10414538344716638, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.03020191192626953}, {"id": 432, "seek": 252256, "start": 2522.64, "end": 2529.2, "text": " and angles. So it's a set with some kind of structure. And in addition, it has some symmetries,", "tokens": [50368, 293, 14708, 13, 407, 309, 311, 257, 992, 365, 512, 733, 295, 3877, 13, 400, 294, 4500, 11, 309, 575, 512, 14232, 302, 2244, 11, 50696], "temperature": 0.0, "avg_logprob": -0.08038653718664292, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.005465006455779076}, {"id": 433, "seek": 252256, "start": 2529.2, "end": 2535.12, "text": " meaning there's some transformations we can do to this set, that will preserve the structure that", "tokens": [50696, 3620, 456, 311, 512, 34852, 321, 393, 360, 281, 341, 992, 11, 300, 486, 15665, 264, 3877, 300, 50992], "temperature": 0.0, "avg_logprob": -0.08038653718664292, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.005465006455779076}, {"id": 434, "seek": 252256, "start": 2535.12, "end": 2541.2, "text": " we think is important. So if we think distances are important on, let's say the sphere, when the", "tokens": [50992, 321, 519, 307, 1021, 13, 407, 498, 321, 519, 22182, 366, 1021, 322, 11, 718, 311, 584, 264, 16687, 11, 562, 264, 51296], "temperature": 0.0, "avg_logprob": -0.08038653718664292, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.005465006455779076}, {"id": 435, "seek": 252256, "start": 2541.2, "end": 2546.24, "text": " kinds of symmetries we end up with our rotations, three dimensional rotations of the sphere,", "tokens": [51296, 3685, 295, 14232, 302, 2244, 321, 917, 493, 365, 527, 44796, 11, 1045, 18795, 44796, 295, 264, 16687, 11, 51548], "temperature": 0.0, "avg_logprob": -0.08038653718664292, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.005465006455779076}, {"id": 436, "seek": 254624, "start": 2546.3199999999997, "end": 2554.16, "text": " perhaps also reflections. In the case of a graph, the symmetries would be permutations", "tokens": [50368, 4317, 611, 30679, 13, 682, 264, 1389, 295, 257, 4295, 11, 264, 14232, 302, 2244, 576, 312, 4784, 325, 763, 50760], "temperature": 0.0, "avg_logprob": -0.10266250568431812, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0025896190200001}, {"id": 437, "seek": 254624, "start": 2554.16, "end": 2562.7999999999997, "text": " of the nodes and also corresponding permutation of the edges. So you just change the order of", "tokens": [50760, 295, 264, 13891, 293, 611, 11760, 4784, 11380, 295, 264, 8819, 13, 407, 291, 445, 1319, 264, 1668, 295, 51192], "temperature": 0.0, "avg_logprob": -0.10266250568431812, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0025896190200001}, {"id": 438, "seek": 254624, "start": 2562.7999999999997, "end": 2568.3199999999997, "text": " the nodes. If node one and two were connected, you apply permutation and move those to node three", "tokens": [51192, 264, 13891, 13, 759, 9984, 472, 293, 732, 645, 4582, 11, 291, 3079, 4784, 11380, 293, 1286, 729, 281, 9984, 1045, 51468], "temperature": 0.0, "avg_logprob": -0.10266250568431812, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0025896190200001}, {"id": 439, "seek": 254624, "start": 2568.3199999999997, "end": 2573.52, "text": " and four, then now three and four have to be connected. So that's a symmetry of our space", "tokens": [51468, 293, 1451, 11, 550, 586, 1045, 293, 1451, 362, 281, 312, 4582, 13, 407, 300, 311, 257, 25440, 295, 527, 1901, 51728], "temperature": 0.0, "avg_logprob": -0.10266250568431812, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0025896190200001}, {"id": 440, "seek": 257352, "start": 2573.52, "end": 2580.72, "text": " overgon. Now the data, this is a very important point, the data are typically not points in this", "tokens": [50364, 670, 10660, 13, 823, 264, 1412, 11, 341, 307, 257, 588, 1021, 935, 11, 264, 1412, 366, 5850, 406, 2793, 294, 341, 50724], "temperature": 0.0, "avg_logprob": -0.1184848149617513, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0142797427251935}, {"id": 441, "seek": 257352, "start": 2580.72, "end": 2586.4, "text": " space. So when we're classifying images, well, our space is the plane, but the data are not", "tokens": [50724, 1901, 13, 407, 562, 321, 434, 1508, 5489, 5267, 11, 731, 11, 527, 1901, 307, 264, 5720, 11, 457, 264, 1412, 366, 406, 51008], "temperature": 0.0, "avg_logprob": -0.1184848149617513, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0142797427251935}, {"id": 442, "seek": 257352, "start": 2586.4, "end": 2591.2, "text": " points in the plane, they're not the two dimensional vectors, the data is really a signal", "tokens": [51008, 2793, 294, 264, 5720, 11, 436, 434, 406, 264, 732, 18795, 18875, 11, 264, 1412, 307, 534, 257, 6358, 51248], "temperature": 0.0, "avg_logprob": -0.1184848149617513, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0142797427251935}, {"id": 443, "seek": 257352, "start": 2592.16, "end": 2597.28, "text": " on the plane, a two dimensional image, which so you can think of that as a function from for each", "tokens": [51296, 322, 264, 5720, 11, 257, 732, 18795, 3256, 11, 597, 370, 291, 393, 519, 295, 300, 382, 257, 2445, 490, 337, 1184, 51552], "temperature": 0.0, "avg_logprob": -0.1184848149617513, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0142797427251935}, {"id": 444, "seek": 259728, "start": 2597.28, "end": 2605.6000000000004, "text": " point in the plane, we have a pixel value. So in the more general cases, it might be", "tokens": [50364, 935, 294, 264, 5720, 11, 321, 362, 257, 19261, 2158, 13, 407, 294, 264, 544, 2674, 3331, 11, 309, 1062, 312, 50780], "temperature": 0.0, "avg_logprob": -0.1291130610874721, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.012429553084075451}, {"id": 445, "seek": 259728, "start": 2605.6000000000004, "end": 2611.52, "text": " something called a field. So you might say have wind direction on on earth, that's a", "tokens": [50780, 746, 1219, 257, 2519, 13, 407, 291, 1062, 584, 362, 2468, 3513, 322, 322, 4120, 11, 300, 311, 257, 51076], "temperature": 0.0, "avg_logprob": -0.1291130610874721, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.012429553084075451}, {"id": 446, "seek": 259728, "start": 2612.1600000000003, "end": 2617.6000000000004, "text": " vector field on the sphere. Now the symmetries that we talked about, they act on this space,", "tokens": [51108, 8062, 2519, 322, 264, 16687, 13, 823, 264, 14232, 302, 2244, 300, 321, 2825, 466, 11, 436, 605, 322, 341, 1901, 11, 51380], "temperature": 0.0, "avg_logprob": -0.1291130610874721, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.012429553084075451}, {"id": 447, "seek": 259728, "start": 2618.2400000000002, "end": 2623.76, "text": " you could show how they automatically also act on the space of signals. So those are the key", "tokens": [51412, 291, 727, 855, 577, 436, 6772, 611, 605, 322, 264, 1901, 295, 12354, 13, 407, 729, 366, 264, 2141, 51688], "temperature": 0.0, "avg_logprob": -0.1291130610874721, "compression_ratio": 1.6435185185185186, "no_speech_prob": 0.012429553084075451}, {"id": 448, "seek": 262376, "start": 2623.76, "end": 2630.48, "text": " ingredients to define what a feature space is, you have your your space, omega, so set of nodes,", "tokens": [50364, 6952, 281, 6964, 437, 257, 4111, 1901, 307, 11, 291, 362, 428, 428, 1901, 11, 10498, 11, 370, 992, 295, 13891, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1276939923001319, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01495420653373003}, {"id": 449, "seek": 262376, "start": 2630.48, "end": 2635.76, "text": " sphere, whatever, then you have a group of symmetries of that space. And then you have", "tokens": [50700, 16687, 11, 2035, 11, 550, 291, 362, 257, 1594, 295, 14232, 302, 2244, 295, 300, 1901, 13, 400, 550, 291, 362, 50964], "temperature": 0.0, "avg_logprob": -0.1276939923001319, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01495420653373003}, {"id": 450, "seek": 262376, "start": 2635.76, "end": 2641.84, "text": " the space of signals or feature maps on this space, and you have the group acting on your", "tokens": [50964, 264, 1901, 295, 12354, 420, 4111, 11317, 322, 341, 1901, 11, 293, 291, 362, 264, 1594, 6577, 322, 428, 51268], "temperature": 0.0, "avg_logprob": -0.1276939923001319, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01495420653373003}, {"id": 451, "seek": 262376, "start": 2641.84, "end": 2649.0400000000004, "text": " signal. So you can rotate a vector field, or you can shift a planar image or etc. So those are the", "tokens": [51268, 6358, 13, 407, 291, 393, 13121, 257, 8062, 2519, 11, 420, 291, 393, 5513, 257, 1393, 289, 3256, 420, 5183, 13, 407, 729, 366, 264, 51628], "temperature": 0.0, "avg_logprob": -0.1276939923001319, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01495420653373003}, {"id": 452, "seek": 264904, "start": 2649.04, "end": 2656.16, "text": " key ingredients to define a feature space. And then we can talk about layers. And so the layers or", "tokens": [50364, 2141, 6952, 281, 6964, 257, 4111, 1901, 13, 400, 550, 321, 393, 751, 466, 7914, 13, 400, 370, 264, 7914, 420, 50720], "temperature": 0.0, "avg_logprob": -0.0661215107850354, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.0038841029163450003}, {"id": 453, "seek": 264904, "start": 2656.16, "end": 2664.08, "text": " maps of the network, they have to respect this structure. So if we have a signal on on the sphere,", "tokens": [50720, 11317, 295, 264, 3209, 11, 436, 362, 281, 3104, 341, 3877, 13, 407, 498, 321, 362, 257, 6358, 322, 322, 264, 16687, 11, 51116], "temperature": 0.0, "avg_logprob": -0.0661215107850354, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.0038841029163450003}, {"id": 454, "seek": 264904, "start": 2664.88, "end": 2671.2799999999997, "text": " and we believe that rotating it doesn't change it in any essential way, or we permute the nodes in", "tokens": [51156, 293, 321, 1697, 300, 19627, 309, 1177, 380, 1319, 309, 294, 604, 7115, 636, 11, 420, 321, 4784, 1169, 264, 13891, 294, 51476], "temperature": 0.0, "avg_logprob": -0.0661215107850354, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.0038841029163450003}, {"id": 455, "seek": 264904, "start": 2671.2799999999997, "end": 2677.2799999999997, "text": " the graph, but it's still the same graph, then we want the layers of the network to respect that.", "tokens": [51476, 264, 4295, 11, 457, 309, 311, 920, 264, 912, 4295, 11, 550, 321, 528, 264, 7914, 295, 264, 3209, 281, 3104, 300, 13, 51776], "temperature": 0.0, "avg_logprob": -0.0661215107850354, "compression_ratio": 1.7356828193832599, "no_speech_prob": 0.0038841029163450003}, {"id": 456, "seek": 267728, "start": 2677.28, "end": 2683.44, "text": " And to respect the structure means to be actually variant to the symmetries. So if we apply our", "tokens": [50364, 400, 281, 3104, 264, 3877, 1355, 281, 312, 767, 17501, 281, 264, 14232, 302, 2244, 13, 407, 498, 321, 3079, 527, 50672], "temperature": 0.0, "avg_logprob": -0.08967640183188698, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0013669566251337528}, {"id": 457, "seek": 267728, "start": 2683.44, "end": 2691.44, "text": " transformation to our signal, and then we apply our network layer, it should be the same as applying", "tokens": [50672, 9887, 281, 527, 6358, 11, 293, 550, 321, 3079, 527, 3209, 4583, 11, 309, 820, 312, 264, 912, 382, 9275, 51072], "temperature": 0.0, "avg_logprob": -0.08967640183188698, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0013669566251337528}, {"id": 458, "seek": 267728, "start": 2691.44, "end": 2696.6400000000003, "text": " the network layer to the original input, and then applying a transformation in the output space.", "tokens": [51072, 264, 3209, 4583, 281, 264, 3380, 4846, 11, 293, 550, 9275, 257, 9887, 294, 264, 5598, 1901, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08967640183188698, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0013669566251337528}, {"id": 459, "seek": 267728, "start": 2697.44, "end": 2701.52, "text": " Now how the transformation acts in the output space could be different from the input space. So for", "tokens": [51372, 823, 577, 264, 9887, 10672, 294, 264, 5598, 1901, 727, 312, 819, 490, 264, 4846, 1901, 13, 407, 337, 51576], "temperature": 0.0, "avg_logprob": -0.08967640183188698, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0013669566251337528}, {"id": 460, "seek": 270152, "start": 2701.52, "end": 2710.16, "text": " example, you could have a vector field as input, and a scalar field as output, they transform", "tokens": [50364, 1365, 11, 291, 727, 362, 257, 8062, 2519, 382, 4846, 11, 293, 257, 39684, 2519, 382, 5598, 11, 436, 4088, 50796], "temperature": 0.0, "avg_logprob": -0.1065920464535977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0015977645525708795}, {"id": 461, "seek": 270152, "start": 2710.16, "end": 2713.7599999999998, "text": " differently. So that's why for each layer and network, you're going to have a different feature", "tokens": [50796, 7614, 13, 407, 300, 311, 983, 337, 1184, 4583, 293, 3209, 11, 291, 434, 516, 281, 362, 257, 819, 4111, 50976], "temperature": 0.0, "avg_logprob": -0.1065920464535977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0015977645525708795}, {"id": 462, "seek": 270152, "start": 2713.7599999999998, "end": 2720.16, "text": " space with a different action of the group. But it's the same group acting in each feature space,", "tokens": [50976, 1901, 365, 257, 819, 3069, 295, 264, 1594, 13, 583, 309, 311, 264, 912, 1594, 6577, 294, 1184, 4111, 1901, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1065920464535977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0015977645525708795}, {"id": 463, "seek": 270152, "start": 2720.16, "end": 2725.44, "text": " the maps, they should be equivariant. And these maps, they include both the linear maps, which are", "tokens": [51296, 264, 11317, 11, 436, 820, 312, 48726, 3504, 394, 13, 400, 613, 11317, 11, 436, 4090, 1293, 264, 8213, 11317, 11, 597, 366, 51560], "temperature": 0.0, "avg_logprob": -0.1065920464535977, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0015977645525708795}, {"id": 464, "seek": 272544, "start": 2725.44, "end": 2732.88, "text": " typically the learnable layers, and the non linearities. Now for linear maps, you can study all", "tokens": [50364, 5850, 264, 1466, 712, 7914, 11, 293, 264, 2107, 8213, 1088, 13, 823, 337, 8213, 11317, 11, 291, 393, 2979, 439, 50736], "temperature": 0.0, "avg_logprob": -0.1446655591328939, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.0128175038844347}, {"id": 465, "seek": 272544, "start": 2732.88, "end": 2737.76, "text": " sorts of interesting questions, you can ask what is the most general kind of equivariant linear", "tokens": [50736, 7527, 295, 1880, 1651, 11, 291, 393, 1029, 437, 307, 264, 881, 2674, 733, 295, 48726, 3504, 394, 8213, 50980], "temperature": 0.0, "avg_logprob": -0.1446655591328939, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.0128175038844347}, {"id": 466, "seek": 272544, "start": 2737.76, "end": 2747.04, "text": " map. And it turns out that for a large class of group actions, or linear group actions, the most", "tokens": [50980, 4471, 13, 400, 309, 4523, 484, 300, 337, 257, 2416, 1508, 295, 1594, 5909, 11, 420, 8213, 1594, 5909, 11, 264, 881, 51444], "temperature": 0.0, "avg_logprob": -0.1446655591328939, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.0128175038844347}, {"id": 467, "seek": 272544, "start": 2747.04, "end": 2752.4, "text": " general kind of equivariant linear maps are generalized forms of convolutions. So that's", "tokens": [51444, 2674, 733, 295, 48726, 3504, 394, 8213, 11317, 366, 44498, 6422, 295, 3754, 15892, 13, 407, 300, 311, 51712], "temperature": 0.0, "avg_logprob": -0.1446655591328939, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.0128175038844347}, {"id": 468, "seek": 275240, "start": 2752.4, "end": 2760.7200000000003, "text": " really an explanation for why convolutions are so ubiquitous. The final ingredient that I think", "tokens": [50364, 534, 364, 10835, 337, 983, 3754, 15892, 366, 370, 43868, 39831, 13, 440, 2572, 14751, 300, 286, 519, 50780], "temperature": 0.0, "avg_logprob": -0.09631237230802837, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.005058292765170336}, {"id": 469, "seek": 275240, "start": 2760.7200000000003, "end": 2766.7200000000003, "text": " is key to the success of many architectures is some kind of pooling or cautioning operation.", "tokens": [50780, 307, 2141, 281, 264, 2245, 295, 867, 6331, 1303, 307, 512, 733, 295, 7005, 278, 420, 23585, 278, 6916, 13, 51080], "temperature": 0.0, "avg_logprob": -0.09631237230802837, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.005058292765170336}, {"id": 470, "seek": 275240, "start": 2767.6, "end": 2772.8, "text": " So the structures we've talked about so far are, you know, this global space and the symmetries on", "tokens": [51124, 407, 264, 9227, 321, 600, 2825, 466, 370, 1400, 366, 11, 291, 458, 11, 341, 4338, 1901, 293, 264, 14232, 302, 2244, 322, 51384], "temperature": 0.0, "avg_logprob": -0.09631237230802837, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.005058292765170336}, {"id": 471, "seek": 275240, "start": 2772.8, "end": 2780.56, "text": " it, etc. But typically, there's also a notion of distance or locality in our space. And if we", "tokens": [51384, 309, 11, 5183, 13, 583, 5850, 11, 456, 311, 611, 257, 10710, 295, 4560, 420, 1628, 1860, 294, 527, 1901, 13, 400, 498, 321, 51772], "temperature": 0.0, "avg_logprob": -0.09631237230802837, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.005058292765170336}, {"id": 472, "seek": 278056, "start": 2781.2, "end": 2787.68, "text": " just enforce that our layers have to respect the symmetries, well that would force us to use a", "tokens": [50396, 445, 24825, 300, 527, 7914, 362, 281, 3104, 264, 14232, 302, 2244, 11, 731, 300, 576, 3464, 505, 281, 764, 257, 50720], "temperature": 0.0, "avg_logprob": -0.12104103300306532, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0023945278953760862}, {"id": 473, "seek": 278056, "start": 2787.68, "end": 2795.2, "text": " convolution in many cases, and I just mentioned, but not forces to use local filters. And we all", "tokens": [50720, 45216, 294, 867, 3331, 11, 293, 286, 445, 2835, 11, 457, 406, 5874, 281, 764, 2654, 15995, 13, 400, 321, 439, 51096], "temperature": 0.0, "avg_logprob": -0.12104103300306532, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0023945278953760862}, {"id": 474, "seek": 278056, "start": 2795.2, "end": 2800.0, "text": " know that using sort of global filters is not going to be very effective use of parameters.", "tokens": [51096, 458, 300, 1228, 1333, 295, 4338, 15995, 307, 406, 516, 281, 312, 588, 4942, 764, 295, 9834, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12104103300306532, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0023945278953760862}, {"id": 475, "seek": 278056, "start": 2801.2, "end": 2809.7599999999998, "text": " So locality is another key thing, locality in the filters. And also localities exploited via", "tokens": [51396, 407, 1628, 1860, 307, 1071, 2141, 551, 11, 1628, 1860, 294, 264, 15995, 13, 400, 611, 2654, 1088, 40918, 5766, 51824], "temperature": 0.0, "avg_logprob": -0.12104103300306532, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.0023945278953760862}, {"id": 476, "seek": 280976, "start": 2809.76, "end": 2817.2000000000003, "text": " some kind of pooling or caution operation. Now, going forward to say the year 2020, deep learning", "tokens": [50364, 512, 733, 295, 7005, 278, 420, 23585, 6916, 13, 823, 11, 516, 2128, 281, 584, 264, 1064, 4808, 11, 2452, 2539, 50736], "temperature": 0.0, "avg_logprob": -0.0825534785559418, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0018950516823679209}, {"id": 477, "seek": 280976, "start": 2817.2000000000003, "end": 2821.92, "text": " is all the craze right now. And so many different deep learning architectures are being proposed,", "tokens": [50736, 307, 439, 264, 2094, 1381, 558, 586, 13, 400, 370, 867, 819, 2452, 2539, 6331, 1303, 366, 885, 10348, 11, 50972], "temperature": 0.0, "avg_logprob": -0.0825534785559418, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0018950516823679209}, {"id": 478, "seek": 280976, "start": 2821.92, "end": 2827.0400000000004, "text": " convolutional neural networks, graph neural networks, LSTMs and so on. When these architectures are", "tokens": [50972, 45216, 304, 18161, 9590, 11, 4295, 18161, 9590, 11, 441, 6840, 26386, 293, 370, 322, 13, 1133, 613, 6331, 1303, 366, 51228], "temperature": 0.0, "avg_logprob": -0.0825534785559418, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0018950516823679209}, {"id": 479, "seek": 280976, "start": 2827.0400000000004, "end": 2831.36, "text": " being proposed, different terminologies are used because people tend to come from different areas", "tokens": [51228, 885, 10348, 11, 819, 10761, 6204, 366, 1143, 570, 561, 3928, 281, 808, 490, 819, 3179, 51444], "temperature": 0.0, "avg_logprob": -0.0825534785559418, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0018950516823679209}, {"id": 480, "seek": 280976, "start": 2831.36, "end": 2836.8, "text": " when they're proposing them. And also, they are usually followed by kinds of bombastic statements", "tokens": [51444, 562, 436, 434, 29939, 552, 13, 400, 611, 11, 436, 366, 2673, 6263, 538, 3685, 295, 7851, 2750, 12363, 51716], "temperature": 0.0, "avg_logprob": -0.0825534785559418, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0018950516823679209}, {"id": 481, "seek": 283680, "start": 2836.8, "end": 2840.5600000000004, "text": " such as everything can be seen as a special case of a convolutional neural network,", "tokens": [50364, 1270, 382, 1203, 393, 312, 1612, 382, 257, 2121, 1389, 295, 257, 45216, 304, 18161, 3209, 11, 50552], "temperature": 0.0, "avg_logprob": -0.0874283123860317, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0036491244100034237}, {"id": 482, "seek": 283680, "start": 2840.5600000000004, "end": 2845.6000000000004, "text": " transformers use self-attention and attention is all you need. Graph neural networks work on graphs", "tokens": [50552, 4088, 433, 764, 2698, 12, 1591, 1251, 293, 3202, 307, 439, 291, 643, 13, 21884, 18161, 9590, 589, 322, 24877, 50804], "temperature": 0.0, "avg_logprob": -0.0874283123860317, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0036491244100034237}, {"id": 483, "seek": 283680, "start": 2845.6000000000004, "end": 2851.52, "text": " and everything can be represented as a graph. And LSTMs are turing complete, so why would you ever", "tokens": [50804, 293, 1203, 393, 312, 10379, 382, 257, 4295, 13, 400, 441, 6840, 26386, 366, 256, 1345, 3566, 11, 370, 983, 576, 291, 1562, 51100], "temperature": 0.0, "avg_logprob": -0.0874283123860317, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0036491244100034237}, {"id": 484, "seek": 283680, "start": 2851.52, "end": 2858.8, "text": " need anything else? So I hope that this illustrates how the field of deep learning in the year of 2020", "tokens": [51100, 643, 1340, 1646, 30, 407, 286, 1454, 300, 341, 41718, 577, 264, 2519, 295, 2452, 2539, 294, 264, 1064, 295, 4808, 51464], "temperature": 0.0, "avg_logprob": -0.0874283123860317, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0036491244100034237}, {"id": 485, "seek": 283680, "start": 2858.8, "end": 2864.6400000000003, "text": " is really not all that different from the state of geometry in the 1800s. And if history teaches", "tokens": [51464, 307, 534, 406, 439, 300, 819, 490, 264, 1785, 295, 18426, 294, 264, 24327, 82, 13, 400, 498, 2503, 16876, 51756], "temperature": 0.0, "avg_logprob": -0.0874283123860317, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0036491244100034237}, {"id": 486, "seek": 286464, "start": 2864.64, "end": 2869.2, "text": " anything about how we can unify these fields together, now is the right time for us to look", "tokens": [50364, 1340, 466, 577, 321, 393, 517, 2505, 613, 7909, 1214, 11, 586, 307, 264, 558, 565, 337, 505, 281, 574, 50592], "temperature": 0.0, "avg_logprob": -0.04951656910411099, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.03902497887611389}, {"id": 487, "seek": 286464, "start": 2869.2, "end": 2874.08, "text": " back, study the geometric principles underlying the architectures that we use. And as a result,", "tokens": [50592, 646, 11, 2979, 264, 33246, 9156, 14217, 264, 6331, 1303, 300, 321, 764, 13, 400, 382, 257, 1874, 11, 50836], "temperature": 0.0, "avg_logprob": -0.04951656910411099, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.03902497887611389}, {"id": 488, "seek": 286464, "start": 2874.08, "end": 2879.12, "text": " we might just derive a blueprint that will allow us to reason about all the architectures we have", "tokens": [50836, 321, 1062, 445, 28446, 257, 35868, 300, 486, 2089, 505, 281, 1778, 466, 439, 264, 6331, 1303, 321, 362, 51088], "temperature": 0.0, "avg_logprob": -0.04951656910411099, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.03902497887611389}, {"id": 489, "seek": 286464, "start": 2879.12, "end": 2884.48, "text": " in the past, but also any architectures that we might come up with in the future. And in my opinion,", "tokens": [51088, 294, 264, 1791, 11, 457, 611, 604, 6331, 1303, 300, 321, 1062, 808, 493, 365, 294, 264, 2027, 13, 400, 294, 452, 4800, 11, 51356], "temperature": 0.0, "avg_logprob": -0.04951656910411099, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.03902497887611389}, {"id": 490, "seek": 286464, "start": 2884.48, "end": 2891.3599999999997, "text": " that is the key selling point of our recently released proto book. And I hope that it is helpful", "tokens": [51356, 300, 307, 264, 2141, 6511, 935, 295, 527, 3938, 4736, 47896, 1446, 13, 400, 286, 1454, 300, 309, 307, 4961, 51700], "temperature": 0.0, "avg_logprob": -0.04951656910411099, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.03902497887611389}, {"id": 491, "seek": 289136, "start": 2891.36, "end": 2896.0, "text": " in guiding deep learning research going forward. I should highlight that I was also extremely,", "tokens": [50364, 294, 25061, 2452, 2539, 2132, 516, 2128, 13, 286, 820, 5078, 300, 286, 390, 611, 4664, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10928793627806384, "compression_ratio": 1.6042402826855124, "no_speech_prob": 0.05713570490479469}, {"id": 492, "seek": 289136, "start": 2897.1200000000003, "end": 2903.6, "text": " extremely honored to deliver the first version of the talk presenting our proto book at", "tokens": [50652, 4664, 14556, 281, 4239, 264, 700, 3037, 295, 264, 751, 15578, 527, 47896, 1446, 412, 50976], "temperature": 0.0, "avg_logprob": -0.10928793627806384, "compression_ratio": 1.6042402826855124, "no_speech_prob": 0.05713570490479469}, {"id": 493, "seek": 289136, "start": 2903.6, "end": 2910.08, "text": " Frederic Alexander University of Erlang in exactly the same place where the Erlang program was", "tokens": [50976, 27535, 299, 14845, 3535, 295, 3300, 25241, 294, 2293, 264, 912, 1081, 689, 264, 3300, 25241, 1461, 390, 51300], "temperature": 0.0, "avg_logprob": -0.10928793627806384, "compression_ratio": 1.6042402826855124, "no_speech_prob": 0.05713570490479469}, {"id": 494, "seek": 289136, "start": 2910.08, "end": 2916.0, "text": " originally brought up, albeit because of the existing COVID restrictions, I had to do so", "tokens": [51300, 7993, 3038, 493, 11, 43654, 570, 295, 264, 6741, 4566, 14191, 11, 286, 632, 281, 360, 370, 51596], "temperature": 0.0, "avg_logprob": -0.10928793627806384, "compression_ratio": 1.6042402826855124, "no_speech_prob": 0.05713570490479469}, {"id": 495, "seek": 289136, "start": 2916.0, "end": 2920.96, "text": " in a virtual manner. So I think a lot of people understand convolutions in the way of a", "tokens": [51596, 294, 257, 6374, 9060, 13, 407, 286, 519, 257, 688, 295, 561, 1223, 3754, 15892, 294, 264, 636, 295, 257, 51844], "temperature": 0.0, "avg_logprob": -0.10928793627806384, "compression_ratio": 1.6042402826855124, "no_speech_prob": 0.05713570490479469}, {"id": 496, "seek": 292096, "start": 2920.96, "end": 2926.16, "text": " traditional plain us, CNN, and the mathematical notion of a convolution, which is very closely", "tokens": [50364, 5164, 11121, 505, 11, 24859, 11, 293, 264, 18894, 10710, 295, 257, 45216, 11, 597, 307, 588, 8185, 50624], "temperature": 0.0, "avg_logprob": -0.1167031659020318, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00485353684052825}, {"id": 497, "seek": 292096, "start": 2926.16, "end": 2931.44, "text": " related to, you know, a Fourier transform, for example. But graph convolutional networks,", "tokens": [50624, 4077, 281, 11, 291, 458, 11, 257, 36810, 4088, 11, 337, 1365, 13, 583, 4295, 45216, 304, 9590, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1167031659020318, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00485353684052825}, {"id": 498, "seek": 292096, "start": 2931.44, "end": 2937.12, "text": " they seem to abstract the notion of a convolution into some kind of concept of the neighborhood", "tokens": [50888, 436, 1643, 281, 12649, 264, 10710, 295, 257, 45216, 666, 512, 733, 295, 3410, 295, 264, 7630, 51172], "temperature": 0.0, "avg_logprob": -0.1167031659020318, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00485353684052825}, {"id": 499, "seek": 292096, "start": 2937.12, "end": 2943.76, "text": " and local connectivity. And some of your work as well with, you know, equivariate convolutional", "tokens": [51172, 293, 2654, 21095, 13, 400, 512, 295, 428, 589, 382, 731, 365, 11, 291, 458, 11, 1267, 592, 3504, 473, 45216, 304, 51504], "temperature": 0.0, "avg_logprob": -0.1167031659020318, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00485353684052825}, {"id": 500, "seek": 292096, "start": 2943.76, "end": 2947.12, "text": " neural networks, I think do the same thing. So when we talk about convolution, are we talking", "tokens": [51504, 18161, 9590, 11, 286, 519, 360, 264, 912, 551, 13, 407, 562, 321, 751, 466, 45216, 11, 366, 321, 1417, 51672], "temperature": 0.0, "avg_logprob": -0.1167031659020318, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00485353684052825}, {"id": 501, "seek": 294712, "start": 2947.12, "end": 2952.24, "text": " about a very abstract notion of it? Yeah, that's a great question. I think that", "tokens": [50364, 466, 257, 588, 12649, 10710, 295, 309, 30, 865, 11, 300, 311, 257, 869, 1168, 13, 286, 519, 300, 50620], "temperature": 0.0, "avg_logprob": -0.090380917424741, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.019399942830204964}, {"id": 502, "seek": 294712, "start": 2952.96, "end": 2960.64, "text": " there are so many different ways to get at convolution. You can think of it in sort of", "tokens": [50656, 456, 366, 370, 867, 819, 2098, 281, 483, 412, 45216, 13, 509, 393, 519, 295, 309, 294, 1333, 295, 51040], "temperature": 0.0, "avg_logprob": -0.090380917424741, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.019399942830204964}, {"id": 503, "seek": 294712, "start": 2960.64, "end": 2965.52, "text": " you've trying to think of it in terms of sliding a filter over some space, right? So you put it", "tokens": [51040, 291, 600, 1382, 281, 519, 295, 309, 294, 2115, 295, 21169, 257, 6608, 670, 512, 1901, 11, 558, 30, 407, 291, 829, 309, 51284], "temperature": 0.0, "avg_logprob": -0.090380917424741, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.019399942830204964}, {"id": 504, "seek": 294712, "start": 2965.52, "end": 2972.24, "text": " in a canonical position, and then you slide it around. That is an idea that you can generalize", "tokens": [51284, 294, 257, 46491, 2535, 11, 293, 550, 291, 4137, 309, 926, 13, 663, 307, 364, 1558, 300, 291, 393, 2674, 1125, 51620], "temperature": 0.0, "avg_logprob": -0.090380917424741, "compression_ratio": 1.6081081081081081, "no_speech_prob": 0.019399942830204964}, {"id": 505, "seek": 297224, "start": 2972.24, "end": 2978.8799999999997, "text": " to not just sliding over a plane, but say applying some transformation from a group", "tokens": [50364, 281, 406, 445, 21169, 670, 257, 5720, 11, 457, 584, 9275, 512, 9887, 490, 257, 1594, 50696], "temperature": 0.0, "avg_logprob": -0.17060853784734553, "compression_ratio": 1.8305785123966942, "no_speech_prob": 0.03209177032113075}, {"id": 506, "seek": 297224, "start": 2978.8799999999997, "end": 2983.52, "text": " to your filter. So let's say you're up, you have a filter on the sphere, then you can,", "tokens": [50696, 281, 428, 6608, 13, 407, 718, 311, 584, 291, 434, 493, 11, 291, 362, 257, 6608, 322, 264, 16687, 11, 550, 291, 393, 11, 50928], "temperature": 0.0, "avg_logprob": -0.17060853784734553, "compression_ratio": 1.8305785123966942, "no_speech_prob": 0.03209177032113075}, {"id": 507, "seek": 297224, "start": 2984.0, "end": 2988.72, "text": " you have, you know, the sphere has a symmetry group, mainly three dimensional rotations,", "tokens": [50952, 291, 362, 11, 291, 458, 11, 264, 16687, 575, 257, 25440, 1594, 11, 8704, 1045, 18795, 44796, 11, 51188], "temperature": 0.0, "avg_logprob": -0.17060853784734553, "compression_ratio": 1.8305785123966942, "no_speech_prob": 0.03209177032113075}, {"id": 508, "seek": 297224, "start": 2988.72, "end": 2993.3599999999997, "text": " group SO3, you can apply an element of SO3, a three dimensional rotation to your filter,", "tokens": [51188, 1594, 10621, 18, 11, 291, 393, 3079, 364, 4478, 295, 10621, 18, 11, 257, 1045, 18795, 12447, 281, 428, 6608, 11, 51420], "temperature": 0.0, "avg_logprob": -0.17060853784734553, "compression_ratio": 1.8305785123966942, "no_speech_prob": 0.03209177032113075}, {"id": 509, "seek": 297224, "start": 2993.3599999999997, "end": 2999.2, "text": " even sort of slide it over, over the sphere. That leads to something called group convolution.", "tokens": [51420, 754, 1333, 295, 4137, 309, 670, 11, 670, 264, 16687, 13, 663, 6689, 281, 746, 1219, 1594, 45216, 13, 51712], "temperature": 0.0, "avg_logprob": -0.17060853784734553, "compression_ratio": 1.8305785123966942, "no_speech_prob": 0.03209177032113075}, {"id": 510, "seek": 299920, "start": 3000.0, "end": 3004.7999999999997, "text": " So that's one way to think about it. There is indeed, as you mentioned, the spectral", "tokens": [50404, 407, 300, 311, 472, 636, 281, 519, 466, 309, 13, 821, 307, 6451, 11, 382, 291, 2835, 11, 264, 42761, 50644], "temperature": 0.0, "avg_logprob": -0.12314696539015997, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.0011334145674481988}, {"id": 511, "seek": 299920, "start": 3004.7999999999997, "end": 3010.3999999999996, "text": " wave looking at it. So you could think there's the famous Fourier convolution theorem,", "tokens": [50644, 5772, 1237, 412, 309, 13, 407, 291, 727, 519, 456, 311, 264, 4618, 36810, 45216, 20904, 11, 50924], "temperature": 0.0, "avg_logprob": -0.12314696539015997, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.0011334145674481988}, {"id": 512, "seek": 299920, "start": 3011.3599999999997, "end": 3019.6, "text": " which says that in the spectrum, convolution is just a point wise product. So one way to", "tokens": [50972, 597, 1619, 300, 294, 264, 11143, 11, 45216, 307, 445, 257, 935, 10829, 1674, 13, 407, 472, 636, 281, 51384], "temperature": 0.0, "avg_logprob": -0.12314696539015997, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.0011334145674481988}, {"id": 513, "seek": 299920, "start": 3019.6, "end": 3024.72, "text": " implement a convolution would be to take a Fourier transform of your signal, your feature map,", "tokens": [51384, 4445, 257, 45216, 576, 312, 281, 747, 257, 36810, 4088, 295, 428, 6358, 11, 428, 4111, 4471, 11, 51640], "temperature": 0.0, "avg_logprob": -0.12314696539015997, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.0011334145674481988}, {"id": 514, "seek": 302472, "start": 3024.72, "end": 3030.3199999999997, "text": " and a Fourier transform of your filter, multiply them point wise, and then inverse Fourier", "tokens": [50364, 293, 257, 36810, 4088, 295, 428, 6608, 11, 12972, 552, 935, 10829, 11, 293, 550, 17340, 36810, 50644], "temperature": 0.0, "avg_logprob": -0.12501663881189684, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0026725921779870987}, {"id": 515, "seek": 302472, "start": 3030.3199999999997, "end": 3037.2799999999997, "text": " transform to get the result. And this perspective also generalizes. So it generalizes to graphs,", "tokens": [50644, 4088, 281, 483, 264, 1874, 13, 400, 341, 4585, 611, 2674, 5660, 13, 407, 309, 2674, 5660, 281, 24877, 11, 50992], "temperature": 0.0, "avg_logprob": -0.12501663881189684, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0026725921779870987}, {"id": 516, "seek": 302472, "start": 3037.8399999999997, "end": 3045.2, "text": " where the Fourier transform, or something analogous to it, can be obtained via graph", "tokens": [51020, 689, 264, 36810, 4088, 11, 420, 746, 16660, 563, 281, 309, 11, 393, 312, 14879, 5766, 4295, 51388], "temperature": 0.0, "avg_logprob": -0.12501663881189684, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0026725921779870987}, {"id": 517, "seek": 302472, "start": 3045.2, "end": 3052.7999999999997, "text": " laplacians, as well as some other ideas. That is actually historically how some of the first", "tokens": [51388, 635, 564, 326, 2567, 11, 382, 731, 382, 512, 661, 3487, 13, 663, 307, 767, 16180, 577, 512, 295, 264, 700, 51768], "temperature": 0.0, "avg_logprob": -0.12501663881189684, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0026725921779870987}, {"id": 518, "seek": 305280, "start": 3052.8, "end": 3060.4, "text": " graph neural nets were implemented and motivated. There's also a spectral theory for group", "tokens": [50364, 4295, 18161, 36170, 645, 12270, 293, 14515, 13, 821, 311, 611, 257, 42761, 5261, 337, 1594, 50744], "temperature": 0.0, "avg_logprob": -0.1313865096480758, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.00152442732360214}, {"id": 519, "seek": 305280, "start": 3060.4, "end": 3069.6800000000003, "text": " convolutions. So indeed, the Fourier transform can be generalized, or the standard Fourier", "tokens": [50744, 3754, 15892, 13, 407, 6451, 11, 264, 36810, 4088, 393, 312, 44498, 11, 420, 264, 3832, 36810, 51208], "temperature": 0.0, "avg_logprob": -0.1313865096480758, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.00152442732360214}, {"id": 520, "seek": 305280, "start": 3069.6800000000003, "end": 3075.6000000000004, "text": " transform that we all know, is actually the Fourier transform for the plane, the plane being", "tokens": [51208, 4088, 300, 321, 439, 458, 11, 307, 767, 264, 36810, 4088, 337, 264, 5720, 11, 264, 5720, 885, 51504], "temperature": 0.0, "avg_logprob": -0.1313865096480758, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.00152442732360214}, {"id": 521, "seek": 305280, "start": 3075.6000000000004, "end": 3082.0, "text": " a particular group, the translation group in two dimensions. So there is a whole, a very beautiful", "tokens": [51504, 257, 1729, 1594, 11, 264, 12853, 1594, 294, 732, 12819, 13, 407, 456, 307, 257, 1379, 11, 257, 588, 2238, 51824], "temperature": 0.0, "avg_logprob": -0.1313865096480758, "compression_ratio": 1.7268518518518519, "no_speech_prob": 0.00152442732360214}, {"id": 522, "seek": 308200, "start": 3082.0, "end": 3090.88, "text": " theory of, let's say, Fourier generalized Fourier transforms for groups, where now the spectrum", "tokens": [50364, 5261, 295, 11, 718, 311, 584, 11, 36810, 44498, 36810, 35592, 337, 3935, 11, 689, 586, 264, 11143, 50808], "temperature": 0.0, "avg_logprob": -0.16012218033058057, "compression_ratio": 1.6411764705882352, "no_speech_prob": 0.001323818345554173}, {"id": 523, "seek": 308200, "start": 3092.0, "end": 3101.68, "text": " is indexed not by the just by the integers, as it is the case for the for the for the line or the", "tokens": [50864, 307, 8186, 292, 406, 538, 264, 445, 538, 264, 41674, 11, 382, 309, 307, 264, 1389, 337, 264, 337, 264, 337, 264, 1622, 420, 264, 51348], "temperature": 0.0, "avg_logprob": -0.16012218033058057, "compression_ratio": 1.6411764705882352, "no_speech_prob": 0.001323818345554173}, {"id": 524, "seek": 308200, "start": 3101.68, "end": 3107.04, "text": " plane, but by something called irreducible representations. In the case of the plane,", "tokens": [51348, 5720, 11, 457, 538, 746, 1219, 16014, 769, 32128, 33358, 13, 682, 264, 1389, 295, 264, 5720, 11, 51616], "temperature": 0.0, "avg_logprob": -0.16012218033058057, "compression_ratio": 1.6411764705882352, "no_speech_prob": 0.001323818345554173}, {"id": 525, "seek": 310704, "start": 3107.04, "end": 3114.32, "text": " those are indeed indexed by integers. And the the spectrum is not just scalar valued,", "tokens": [50364, 729, 366, 6451, 8186, 292, 538, 41674, 13, 400, 264, 264, 11143, 307, 406, 445, 39684, 22608, 11, 50728], "temperature": 0.0, "avg_logprob": -0.12168930572213478, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.01081620529294014}, {"id": 526, "seek": 310704, "start": 3114.32, "end": 3119.36, "text": " or complex scalar valued, but it can be matrix valued. If you're interested in this sort of", "tokens": [50728, 420, 3997, 39684, 22608, 11, 457, 309, 393, 312, 8141, 22608, 13, 759, 291, 434, 3102, 294, 341, 1333, 295, 50980], "temperature": 0.0, "avg_logprob": -0.12168930572213478, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.01081620529294014}, {"id": 527, "seek": 310704, "start": 3119.36, "end": 3124.0, "text": " stuff, you can, you want sort of a very high level description, you can check out our paper on", "tokens": [50980, 1507, 11, 291, 393, 11, 291, 528, 1333, 295, 257, 588, 1090, 1496, 3855, 11, 291, 393, 1520, 484, 527, 3035, 322, 51212], "temperature": 0.0, "avg_logprob": -0.12168930572213478, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.01081620529294014}, {"id": 528, "seek": 310704, "start": 3124.0, "end": 3129.68, "text": " spherical CNNs, where we actually implement convolution on a sphere, using this kind of", "tokens": [51212, 37300, 24859, 82, 11, 689, 321, 767, 4445, 45216, 322, 257, 16687, 11, 1228, 341, 733, 295, 51496], "temperature": 0.0, "avg_logprob": -0.12168930572213478, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.01081620529294014}, {"id": 529, "seek": 310704, "start": 3129.68, "end": 3135.52, "text": " generalized Fourier transform. So that's the Fourier perspective on convolutions.", "tokens": [51496, 44498, 36810, 4088, 13, 407, 300, 311, 264, 36810, 4585, 322, 3754, 15892, 13, 51788], "temperature": 0.0, "avg_logprob": -0.12168930572213478, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.01081620529294014}, {"id": 530, "seek": 313552, "start": 3136.32, "end": 3142.88, "text": " And there's a final perspective, which I think is quite intuitive, which is that", "tokens": [50404, 400, 456, 311, 257, 2572, 4585, 11, 597, 286, 519, 307, 1596, 21769, 11, 597, 307, 300, 50732], "temperature": 0.0, "avg_logprob": -0.13346589695323596, "compression_ratio": 1.605095541401274, "no_speech_prob": 0.006794837769120932}, {"id": 531, "seek": 313552, "start": 3143.7599999999998, "end": 3152.32, "text": " the convolution is the most general kind of equivariant linear map between certain group", "tokens": [50776, 264, 45216, 307, 264, 881, 2674, 733, 295, 48726, 3504, 394, 8213, 4471, 1296, 1629, 1594, 51204], "temperature": 0.0, "avg_logprob": -0.13346589695323596, "compression_ratio": 1.605095541401274, "no_speech_prob": 0.006794837769120932}, {"id": 532, "seek": 313552, "start": 3152.32, "end": 3159.84, "text": " between certain linear group actions. So specifically, these group actions are the", "tokens": [51204, 1296, 1629, 8213, 1594, 5909, 13, 407, 4682, 11, 613, 1594, 5909, 366, 264, 51580], "temperature": 0.0, "avg_logprob": -0.13346589695323596, "compression_ratio": 1.605095541401274, "no_speech_prob": 0.006794837769120932}, {"id": 533, "seek": 315984, "start": 3159.84, "end": 3166.6400000000003, "text": " way that groups tend to act on the space of signals on your space. So you might have space", "tokens": [50364, 636, 300, 3935, 3928, 281, 605, 322, 264, 1901, 295, 12354, 322, 428, 1901, 13, 407, 291, 1062, 362, 1901, 50704], "temperature": 0.0, "avg_logprob": -0.09269540737836789, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.006096681114286184}, {"id": 534, "seek": 315984, "start": 3166.6400000000003, "end": 3172.8, "text": " of scalar signals on the sphere. And your feature map, you might want to have another scalar signal", "tokens": [50704, 295, 39684, 12354, 322, 264, 16687, 13, 400, 428, 4111, 4471, 11, 291, 1062, 528, 281, 362, 1071, 39684, 6358, 51012], "temperature": 0.0, "avg_logprob": -0.09269540737836789, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.006096681114286184}, {"id": 535, "seek": 315984, "start": 3172.8, "end": 3177.6800000000003, "text": " in the sphere or a vector field on the sphere or something. Then you can ask what is the most", "tokens": [51012, 294, 264, 16687, 420, 257, 8062, 2519, 322, 264, 16687, 420, 746, 13, 1396, 291, 393, 1029, 437, 307, 264, 881, 51256], "temperature": 0.0, "avg_logprob": -0.09269540737836789, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.006096681114286184}, {"id": 536, "seek": 315984, "start": 3177.6800000000003, "end": 3183.04, "text": " general kind of equivariant linear map. And the answer is, it's a convolution. And that is also", "tokens": [51256, 2674, 733, 295, 48726, 3504, 394, 8213, 4471, 13, 400, 264, 1867, 307, 11, 309, 311, 257, 45216, 13, 400, 300, 307, 611, 51524], "temperature": 0.0, "avg_logprob": -0.09269540737836789, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.006096681114286184}, {"id": 537, "seek": 315984, "start": 3183.04, "end": 3189.04, "text": " true in it in a very general setting. So that that to me is the most intuitive way of understanding", "tokens": [51524, 2074, 294, 309, 294, 257, 588, 2674, 3287, 13, 407, 300, 300, 281, 385, 307, 264, 881, 21769, 636, 295, 3701, 51824], "temperature": 0.0, "avg_logprob": -0.09269540737836789, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.006096681114286184}, {"id": 538, "seek": 318904, "start": 3189.12, "end": 3195.2, "text": " convolutions as the most general kind of maps that are linear maps that are equivariate to", "tokens": [50368, 3754, 15892, 382, 264, 881, 2674, 733, 295, 11317, 300, 366, 8213, 11317, 300, 366, 1267, 592, 3504, 473, 281, 50672], "temperature": 0.0, "avg_logprob": -0.22328930089969445, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.0028666520956903696}, {"id": 539, "seek": 318904, "start": 3195.2, "end": 3200.32, "text": " certain kinds of group actions. Professor Bruner, welcome to MLST. It's an absolutely", "tokens": [50672, 1629, 3685, 295, 1594, 5909, 13, 8419, 1603, 409, 260, 11, 2928, 281, 376, 19198, 51, 13, 467, 311, 364, 3122, 50928], "temperature": 0.0, "avg_logprob": -0.22328930089969445, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.0028666520956903696}, {"id": 540, "seek": 318904, "start": 3200.32, "end": 3207.12, "text": " honor to have you on. Introduce yourself. Yeah. Hi, Tim. I'm Joanne Bruner. I'm a associate professor", "tokens": [50928, 5968, 281, 362, 291, 322, 13, 27193, 384, 1803, 13, 865, 13, 2421, 11, 7172, 13, 286, 478, 3139, 12674, 1603, 409, 260, 13, 286, 478, 257, 14644, 8304, 51268], "temperature": 0.0, "avg_logprob": -0.22328930089969445, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.0028666520956903696}, {"id": 541, "seek": 318904, "start": 3207.12, "end": 3212.72, "text": " at the Quran Institute and Center for Data Science at New York University. And I'm very happy to be", "tokens": [51268, 412, 264, 19375, 9446, 293, 5169, 337, 11888, 8976, 412, 1873, 3609, 3535, 13, 400, 286, 478, 588, 2055, 281, 312, 51548], "temperature": 0.0, "avg_logprob": -0.22328930089969445, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.0028666520956903696}, {"id": 542, "seek": 321272, "start": 3212.72, "end": 3220.3199999999997, "text": " here chatting with you at MLST. Joanne, you just released this geometric deep learning proto book.", "tokens": [50364, 510, 24654, 365, 291, 412, 376, 19198, 51, 13, 3139, 12674, 11, 291, 445, 4736, 341, 33246, 2452, 2539, 47896, 1446, 13, 50744], "temperature": 0.0, "avg_logprob": -0.1487520933151245, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03919468820095062}, {"id": 543, "seek": 321272, "start": 3220.3199999999997, "end": 3225.2, "text": " You know, what does it mean to you? What was your kind of intellectual journey that led to this?", "tokens": [50744, 509, 458, 11, 437, 775, 309, 914, 281, 291, 30, 708, 390, 428, 733, 295, 12576, 4671, 300, 4684, 281, 341, 30, 50988], "temperature": 0.0, "avg_logprob": -0.1487520933151245, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03919468820095062}, {"id": 544, "seek": 321272, "start": 3226.3999999999996, "end": 3234.08, "text": " Yeah. So this journey, in fact, started many years ago. So I would say even like during my postdoc,", "tokens": [51048, 865, 13, 407, 341, 4671, 11, 294, 1186, 11, 1409, 867, 924, 2057, 13, 407, 286, 576, 584, 754, 411, 1830, 452, 2183, 39966, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1487520933151245, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03919468820095062}, {"id": 545, "seek": 321272, "start": 3234.08, "end": 3240.48, "text": " I was a postdoc. I was doing my postdoc here at NYU with Yanle Kun. And that was the time", "tokens": [51432, 286, 390, 257, 2183, 39966, 13, 286, 390, 884, 452, 2183, 39966, 510, 412, 42682, 365, 13633, 306, 19089, 13, 400, 300, 390, 264, 565, 51752], "temperature": 0.0, "avg_logprob": -0.1487520933151245, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03919468820095062}, {"id": 546, "seek": 324048, "start": 3241.04, "end": 3249.6, "text": " 2013, where, you know, confnets were already showing a big promise in image tasks. And", "tokens": [50392, 9012, 11, 689, 11, 291, 458, 11, 1497, 77, 1385, 645, 1217, 4099, 257, 955, 6228, 294, 3256, 9608, 13, 400, 50820], "temperature": 0.0, "avg_logprob": -0.22968714724304856, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.003808208741247654}, {"id": 547, "seek": 324048, "start": 3250.16, "end": 3256.16, "text": " discussing with Yan, the questions are, okay, how about domains that are not like grades, right? So,", "tokens": [50848, 10850, 365, 13633, 11, 264, 1651, 366, 11, 1392, 11, 577, 466, 25514, 300, 366, 406, 411, 18041, 11, 558, 30, 407, 11, 51148], "temperature": 0.0, "avg_logprob": -0.22968714724304856, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.003808208741247654}, {"id": 548, "seek": 324048, "start": 3256.16, "end": 3261.92, "text": " and that was the beginning of a journey that also included my former collaborator, Arthur Slump,", "tokens": [51148, 293, 300, 390, 264, 2863, 295, 257, 4671, 300, 611, 5556, 452, 5819, 5091, 1639, 11, 19624, 6187, 1420, 11, 51436], "temperature": 0.0, "avg_logprob": -0.22968714724304856, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.003808208741247654}, {"id": 549, "seek": 324048, "start": 3261.92, "end": 3267.44, "text": " who was like another researcher in the lab that had a similar background as me, like coming from", "tokens": [51436, 567, 390, 411, 1071, 21751, 294, 264, 2715, 300, 632, 257, 2531, 3678, 382, 385, 11, 411, 1348, 490, 51712], "temperature": 0.0, "avg_logprob": -0.22968714724304856, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.003808208741247654}, {"id": 550, "seek": 326744, "start": 3267.44, "end": 3272.8, "text": " applied math, but looking into into into more and more deep learning. So I would say that the", "tokens": [50364, 6456, 5221, 11, 457, 1237, 666, 666, 666, 544, 293, 544, 2452, 2539, 13, 407, 286, 576, 584, 300, 264, 50632], "temperature": 0.0, "avg_logprob": -0.1569280357004326, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0016461958875879645}, {"id": 551, "seek": 326744, "start": 3272.8, "end": 3279.04, "text": " genesis was our first attempt at extending the success of convolutional networks to these", "tokens": [50632, 1049, 9374, 390, 527, 700, 5217, 412, 24360, 264, 2245, 295, 45216, 304, 9590, 281, 613, 50944], "temperature": 0.0, "avg_logprob": -0.1569280357004326, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0016461958875879645}, {"id": 552, "seek": 326744, "start": 3279.04, "end": 3285.2000000000003, "text": " regular domains. And we published the paper at iClear 2014. And after that, I think things started", "tokens": [50944, 3890, 25514, 13, 400, 321, 6572, 264, 3035, 412, 741, 34, 5797, 8227, 13, 400, 934, 300, 11, 286, 519, 721, 1409, 51252], "temperature": 0.0, "avg_logprob": -0.1569280357004326, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0016461958875879645}, {"id": 553, "seek": 326744, "start": 3285.2000000000003, "end": 3291.04, "text": " like quite naturally because other researchers that had, I would say, came from the same background,", "tokens": [51252, 411, 1596, 8195, 570, 661, 10309, 300, 632, 11, 286, 576, 584, 11, 1361, 490, 264, 912, 3678, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1569280357004326, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0016461958875879645}, {"id": 554, "seek": 326744, "start": 3291.04, "end": 3296.56, "text": " like, you know, maybe from geometric background, also started to realize that there was something", "tokens": [51544, 411, 11, 291, 458, 11, 1310, 490, 33246, 3678, 11, 611, 1409, 281, 4325, 300, 456, 390, 746, 51820], "temperature": 0.0, "avg_logprob": -0.1569280357004326, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0016461958875879645}, {"id": 555, "seek": 329656, "start": 3296.56, "end": 3300.16, "text": " there, maybe bigger than these particular papers, in particular, Michael Brownstone,", "tokens": [50364, 456, 11, 1310, 3801, 813, 613, 1729, 10577, 11, 294, 1729, 11, 5116, 8030, 11243, 11, 50544], "temperature": 0.0, "avg_logprob": -0.2181351812262284, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.0015464029274880886}, {"id": 556, "seek": 329656, "start": 3301.44, "end": 3306.0, "text": " it's kind of reached out to us just afterwards saying, oh, and we are also looking at similar", "tokens": [50608, 309, 311, 733, 295, 6488, 484, 281, 505, 445, 10543, 1566, 11, 1954, 11, 293, 321, 366, 611, 1237, 412, 2531, 50836], "temperature": 0.0, "avg_logprob": -0.2181351812262284, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.0015464029274880886}, {"id": 557, "seek": 329656, "start": 3306.0, "end": 3311.2, "text": " ideas. I think we should just team together and start to think about this more globally. And so,", "tokens": [50836, 3487, 13, 286, 519, 321, 820, 445, 1469, 1214, 293, 722, 281, 519, 466, 341, 544, 18958, 13, 400, 370, 11, 51096], "temperature": 0.0, "avg_logprob": -0.2181351812262284, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.0015464029274880886}, {"id": 558, "seek": 329656, "start": 3312.24, "end": 3318.7999999999997, "text": " you know, things that developed from there. And we wrote this journal paper, like a review paper", "tokens": [51148, 291, 458, 11, 721, 300, 4743, 490, 456, 13, 400, 321, 4114, 341, 6708, 3035, 11, 411, 257, 3131, 3035, 51476], "temperature": 0.0, "avg_logprob": -0.2181351812262284, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.0015464029274880886}, {"id": 559, "seek": 329656, "start": 3318.7999999999997, "end": 3325.92, "text": " in 2017, with Arthur, Yan, Michael, the airbender guys, who is another very well known figure in", "tokens": [51476, 294, 6591, 11, 365, 19624, 11, 13633, 11, 5116, 11, 264, 1988, 65, 3216, 1074, 11, 567, 307, 1071, 588, 731, 2570, 2573, 294, 51832], "temperature": 0.0, "avg_logprob": -0.2181351812262284, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.0015464029274880886}, {"id": 560, "seek": 332592, "start": 3326.0, "end": 3331.28, "text": " this area. And so from there, I think that, yeah, things like started to slowly take off. We had", "tokens": [50368, 341, 1859, 13, 400, 370, 490, 456, 11, 286, 519, 300, 11, 1338, 11, 721, 411, 1409, 281, 5692, 747, 766, 13, 492, 632, 50632], "temperature": 0.0, "avg_logprob": -0.14532326113793156, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0016185387503355742}, {"id": 561, "seek": 332592, "start": 3331.28, "end": 3337.6, "text": " tutorials and new ribs that we had a very successful workshop at IPAM. After this, I think that the", "tokens": [50632, 17616, 293, 777, 21400, 300, 321, 632, 257, 588, 4406, 13541, 412, 8671, 2865, 13, 2381, 341, 11, 286, 519, 300, 264, 50948], "temperature": 0.0, "avg_logprob": -0.14532326113793156, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0016185387503355742}, {"id": 562, "seek": 332592, "start": 3338.56, "end": 3342.64, "text": " thing were clear that, okay, maybe at some point in the future, we should try to stamp all these", "tokens": [50996, 551, 645, 1850, 300, 11, 1392, 11, 1310, 412, 512, 935, 294, 264, 2027, 11, 321, 820, 853, 281, 9921, 439, 613, 51200], "temperature": 0.0, "avg_logprob": -0.14532326113793156, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0016185387503355742}, {"id": 563, "seek": 332592, "start": 3342.64, "end": 3348.48, "text": " things into a book that tries to reflect something a bit more, let's say, mature and, you know, what", "tokens": [51200, 721, 666, 257, 1446, 300, 9898, 281, 5031, 746, 257, 857, 544, 11, 718, 311, 584, 11, 14442, 293, 11, 291, 458, 11, 437, 51492], "temperature": 0.0, "avg_logprob": -0.14532326113793156, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0016185387503355742}, {"id": 564, "seek": 332592, "start": 3348.48, "end": 3354.96, "text": " is our, if we wanted to have like some kind of legacy for future generations on how to implement", "tokens": [51492, 307, 527, 11, 498, 321, 1415, 281, 362, 411, 512, 733, 295, 11711, 337, 2027, 10593, 322, 577, 281, 4445, 51816], "temperature": 0.0, "avg_logprob": -0.14532326113793156, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0016185387503355742}, {"id": 565, "seek": 335496, "start": 3354.96, "end": 3359.76, "text": " and communicate these methods, how would that be? So I guess that was the genesis of the book. And so", "tokens": [50364, 293, 7890, 613, 7150, 11, 577, 576, 300, 312, 30, 407, 286, 2041, 300, 390, 264, 1049, 9374, 295, 264, 1446, 13, 400, 370, 50604], "temperature": 0.0, "avg_logprob": -0.1331037603398805, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0008150615612976253}, {"id": 566, "seek": 335496, "start": 3361.52, "end": 3366.96, "text": " very soon we said with Michael that we also would like to have some, you know, fresh minds and fresh", "tokens": [50692, 588, 2321, 321, 848, 365, 5116, 300, 321, 611, 576, 411, 281, 362, 512, 11, 291, 458, 11, 4451, 9634, 293, 4451, 50964], "temperature": 0.0, "avg_logprob": -0.1331037603398805, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0008150615612976253}, {"id": 567, "seek": 335496, "start": 3366.96, "end": 3373.12, "text": " energy on board. So naturally, the names of Taco and better came very naturally to us as people", "tokens": [50964, 2281, 322, 3150, 13, 407, 8195, 11, 264, 5288, 295, 37992, 293, 1101, 1361, 588, 8195, 281, 505, 382, 561, 51272], "temperature": 0.0, "avg_logprob": -0.1331037603398805, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0008150615612976253}, {"id": 568, "seek": 335496, "start": 3373.12, "end": 3378.8, "text": " who had been doing excellent work in the domain that would very nicely complement our skills.", "tokens": [51272, 567, 632, 668, 884, 7103, 589, 294, 264, 9274, 300, 576, 588, 9594, 17103, 527, 3942, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1331037603398805, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0008150615612976253}, {"id": 569, "seek": 337880, "start": 3378.8, "end": 3385.36, "text": " So the team was created. And that's the project. And so, yeah, I mean, I think it's been very", "tokens": [50364, 407, 264, 1469, 390, 2942, 13, 400, 300, 311, 264, 1716, 13, 400, 370, 11, 1338, 11, 286, 914, 11, 286, 519, 309, 311, 668, 588, 50692], "temperature": 0.0, "avg_logprob": -0.12249303095549055, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00841481052339077}, {"id": 570, "seek": 337880, "start": 3385.36, "end": 3391.92, "text": " interesting so far. Of course, I guess that, as you know, this is just an ongoing project, right?", "tokens": [50692, 1880, 370, 1400, 13, 2720, 1164, 11, 286, 2041, 300, 11, 382, 291, 458, 11, 341, 307, 445, 364, 10452, 1716, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.12249303095549055, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00841481052339077}, {"id": 571, "seek": 337880, "start": 3391.92, "end": 3398.0800000000004, "text": " So it's still not finalized. But hopefully we're getting interest from the community. And this", "tokens": [51020, 407, 309, 311, 920, 406, 2572, 1602, 13, 583, 4696, 321, 434, 1242, 1179, 490, 264, 1768, 13, 400, 341, 51328], "temperature": 0.0, "avg_logprob": -0.12249303095549055, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00841481052339077}, {"id": 572, "seek": 337880, "start": 3398.0800000000004, "end": 3403.04, "text": " gives us some kind of, I would say positive vibes to finish it on time. As you know, writing books", "tokens": [51328, 2709, 505, 512, 733, 295, 11, 286, 576, 584, 3353, 27636, 281, 2413, 309, 322, 565, 13, 1018, 291, 458, 11, 3579, 3642, 51576], "temperature": 0.0, "avg_logprob": -0.12249303095549055, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.00841481052339077}, {"id": 573, "seek": 340304, "start": 3403.04, "end": 3408.8, "text": " is always like this never ending process. So I think that, yeah, that's, it's been an", "tokens": [50364, 307, 1009, 411, 341, 1128, 8121, 1399, 13, 407, 286, 519, 300, 11, 1338, 11, 300, 311, 11, 309, 311, 668, 364, 50652], "temperature": 0.0, "avg_logprob": -0.1071769532703218, "compression_ratio": 1.6418439716312057, "no_speech_prob": 0.018501944839954376}, {"id": 574, "seek": 340304, "start": 3408.8, "end": 3414.8, "text": " interesting endeavor so far, for sure. I asked Professor Bromstein what his most passionately", "tokens": [50652, 1880, 34975, 370, 1400, 11, 337, 988, 13, 286, 2351, 8419, 1603, 298, 9089, 437, 702, 881, 5418, 1592, 50952], "temperature": 0.0, "avg_logprob": -0.1071769532703218, "compression_ratio": 1.6418439716312057, "no_speech_prob": 0.018501944839954376}, {"id": 575, "seek": 340304, "start": 3414.8, "end": 3419.84, "text": " held belief is about machine learning. I think machine learning is such a field where", "tokens": [50952, 5167, 7107, 307, 466, 3479, 2539, 13, 286, 519, 3479, 2539, 307, 1270, 257, 2519, 689, 51204], "temperature": 0.0, "avg_logprob": -0.1071769532703218, "compression_ratio": 1.6418439716312057, "no_speech_prob": 0.018501944839954376}, {"id": 576, "seek": 340304, "start": 3419.84, "end": 3425.6, "text": " holding strong beliefs is often counterproductive. It happened to me multiple times that something", "tokens": [51204, 5061, 2068, 13585, 307, 2049, 5682, 14314, 20221, 13, 467, 2011, 281, 385, 3866, 1413, 300, 746, 51492], "temperature": 0.0, "avg_logprob": -0.1071769532703218, "compression_ratio": 1.6418439716312057, "no_speech_prob": 0.018501944839954376}, {"id": 577, "seek": 340304, "start": 3425.6, "end": 3432.56, "text": " that I thought or said was very quickly overturned. And what I mean is that milestones or progress", "tokens": [51492, 300, 286, 1194, 420, 848, 390, 588, 2661, 42865, 292, 13, 400, 437, 286, 914, 307, 300, 42038, 420, 4205, 51840], "temperature": 0.0, "avg_logprob": -0.1071769532703218, "compression_ratio": 1.6418439716312057, "no_speech_prob": 0.018501944839954376}, {"id": 578, "seek": 343304, "start": 3433.04, "end": 3438.72, "text": " was achieved much faster than I could even imagine in a wild dream. So making predictions about", "tokens": [50364, 390, 11042, 709, 4663, 813, 286, 727, 754, 3811, 294, 257, 4868, 3055, 13, 407, 1455, 21264, 466, 50648], "temperature": 0.0, "avg_logprob": -0.054188908882511475, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.009276657365262508}, {"id": 579, "seek": 343304, "start": 3438.72, "end": 3444.64, "text": " machine learning is, to some extent, an ungrateful job. I do, however, believe that in order to make", "tokens": [50648, 3479, 2539, 307, 11, 281, 512, 8396, 11, 364, 517, 861, 7529, 1691, 13, 286, 360, 11, 4461, 11, 1697, 300, 294, 1668, 281, 652, 50944], "temperature": 0.0, "avg_logprob": -0.054188908882511475, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.009276657365262508}, {"id": 580, "seek": 343304, "start": 3444.64, "end": 3449.92, "text": " progress to the next level and make machine learning achieve its potential to become the", "tokens": [50944, 4205, 281, 264, 958, 1496, 293, 652, 3479, 2539, 4584, 1080, 3995, 281, 1813, 264, 51208], "temperature": 0.0, "avg_logprob": -0.054188908882511475, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.009276657365262508}, {"id": 581, "seek": 343304, "start": 3449.92, "end": 3455.6, "text": " transformative technology we trust and use ubiquitously, it must be built on solid mathematical", "tokens": [51208, 36070, 2899, 321, 3361, 293, 764, 43868, 270, 5098, 11, 309, 1633, 312, 3094, 322, 5100, 18894, 51492], "temperature": 0.0, "avg_logprob": -0.054188908882511475, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.009276657365262508}, {"id": 582, "seek": 343304, "start": 3455.6, "end": 3461.2, "text": " foundations. And I also think that machine learning will drive future scientific breakthroughs.", "tokens": [51492, 22467, 13, 400, 286, 611, 519, 300, 3479, 2539, 486, 3332, 2027, 8134, 22397, 82, 13, 51772], "temperature": 0.0, "avg_logprob": -0.054188908882511475, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.009276657365262508}, {"id": 583, "seek": 346120, "start": 3461.2, "end": 3466.7999999999997, "text": " And probably a good litmus test would be a Nobel Prize awarded for a discovery made by", "tokens": [50364, 400, 1391, 257, 665, 7997, 18761, 1500, 576, 312, 257, 24611, 22604, 19100, 337, 257, 12114, 1027, 538, 50644], "temperature": 0.0, "avg_logprob": -0.09998544057210286, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.004239346366375685}, {"id": 584, "seek": 346120, "start": 3466.7999999999997, "end": 3471.4399999999996, "text": " or with the help of an ML system. It might already happen in the next decade.", "tokens": [50644, 420, 365, 264, 854, 295, 364, 21601, 1185, 13, 467, 1062, 1217, 1051, 294, 264, 958, 10378, 13, 50876], "temperature": 0.0, "avg_logprob": -0.09998544057210286, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.004239346366375685}, {"id": 585, "seek": 346120, "start": 3472.08, "end": 3478.3999999999996, "text": " I wanted to go into a few questions actually about the book. So first of all, Joanne, in your", "tokens": [50908, 286, 1415, 281, 352, 666, 257, 1326, 1651, 767, 466, 264, 1446, 13, 407, 700, 295, 439, 11, 3139, 12674, 11, 294, 428, 51224], "temperature": 0.0, "avg_logprob": -0.09998544057210286, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.004239346366375685}, {"id": 586, "seek": 346120, "start": 3478.3999999999996, "end": 3484.7999999999997, "text": " 2017 paper, The Mathematics of Deep Learning, you cited the universal function approximation theorem,", "tokens": [51224, 6591, 3035, 11, 440, 15776, 37541, 295, 14895, 15205, 11, 291, 30134, 264, 11455, 2445, 28023, 20904, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09998544057210286, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.004239346366375685}, {"id": 587, "seek": 346120, "start": 3484.7999999999997, "end": 3489.3599999999997, "text": " which is to say the ability of a shallow neural network to approximate arbitrary functions.", "tokens": [51544, 597, 307, 281, 584, 264, 3485, 295, 257, 20488, 18161, 3209, 281, 30874, 23211, 6828, 13, 51772], "temperature": 0.0, "avg_logprob": -0.09998544057210286, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.004239346366375685}, {"id": 588, "seek": 348936, "start": 3489.36, "end": 3494.08, "text": " But the performance of wide and shallow neural networks can be significantly beaten by deep", "tokens": [50364, 583, 264, 3389, 295, 4874, 293, 20488, 18161, 9590, 393, 312, 10591, 17909, 538, 2452, 50600], "temperature": 0.0, "avg_logprob": -0.05620699026146714, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0008104786975309253}, {"id": 589, "seek": 348936, "start": 3494.08, "end": 3498.8, "text": " networks. And you said that one of the possible explanations is that deep architectures are able", "tokens": [50600, 9590, 13, 400, 291, 848, 300, 472, 295, 264, 1944, 28708, 307, 300, 2452, 6331, 1303, 366, 1075, 50836], "temperature": 0.0, "avg_logprob": -0.05620699026146714, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0008104786975309253}, {"id": 590, "seek": 348936, "start": 3498.8, "end": 3504.56, "text": " to better capture invariant properties of the data compared to their shallow counterparts.", "tokens": [50836, 281, 1101, 7983, 33270, 394, 7221, 295, 264, 1412, 5347, 281, 641, 20488, 33287, 13, 51124], "temperature": 0.0, "avg_logprob": -0.05620699026146714, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0008104786975309253}, {"id": 591, "seek": 348936, "start": 3504.56, "end": 3508.96, "text": " Could you just briefly introduce the universal function approximation theorem? And do you think", "tokens": [51124, 7497, 291, 445, 10515, 5366, 264, 11455, 2445, 28023, 20904, 30, 400, 360, 291, 519, 51344], "temperature": 0.0, "avg_logprob": -0.05620699026146714, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0008104786975309253}, {"id": 592, "seek": 348936, "start": 3508.96, "end": 3515.52, "text": " it's still relevant for today's neural networks? Yeah, I mean, that's a very deep and an important", "tokens": [51344, 309, 311, 920, 7340, 337, 965, 311, 18161, 9590, 30, 865, 11, 286, 914, 11, 300, 311, 257, 588, 2452, 293, 364, 1021, 51672], "temperature": 0.0, "avg_logprob": -0.05620699026146714, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0008104786975309253}, {"id": 593, "seek": 351552, "start": 3515.52, "end": 3521.52, "text": " question. Yeah, so universal approximation theorem, it refers to this very general principle that", "tokens": [50364, 1168, 13, 865, 11, 370, 11455, 28023, 20904, 11, 309, 14942, 281, 341, 588, 2674, 8665, 300, 50664], "temperature": 0.0, "avg_logprob": -0.16868501551011028, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0036420326214283705}, {"id": 594, "seek": 351552, "start": 3521.52, "end": 3527.2, "text": " once you define parametric class, let's say you're you are on a learn functions using neural", "tokens": [50664, 1564, 291, 6964, 6220, 17475, 1508, 11, 718, 311, 584, 291, 434, 291, 366, 322, 257, 1466, 6828, 1228, 18161, 50948], "temperature": 0.0, "avg_logprob": -0.16868501551011028, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0036420326214283705}, {"id": 595, "seek": 351552, "start": 3527.2, "end": 3532.8, "text": " nets, it just describes your ability that as you put more and more parameters into your class,", "tokens": [50948, 36170, 11, 309, 445, 15626, 428, 3485, 300, 382, 291, 829, 544, 293, 544, 9834, 666, 428, 1508, 11, 51228], "temperature": 0.0, "avg_logprob": -0.16868501551011028, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0036420326214283705}, {"id": 596, "seek": 351552, "start": 3532.8, "end": 3538.24, "text": " you're going to able to to approximate essentially anything that data nature throws at you. And so", "tokens": [51228, 291, 434, 516, 281, 1075, 281, 281, 30874, 4476, 1340, 300, 1412, 3687, 19251, 412, 291, 13, 400, 370, 51500], "temperature": 0.0, "avg_logprob": -0.16868501551011028, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0036420326214283705}, {"id": 597, "seek": 353824, "start": 3538.24, "end": 3546.16, "text": " this might seem like a very powerful property. But in fact, in fact, it's it's something that", "tokens": [50364, 341, 1062, 1643, 411, 257, 588, 4005, 4707, 13, 583, 294, 1186, 11, 294, 1186, 11, 309, 311, 309, 311, 746, 300, 50760], "temperature": 0.0, "avg_logprob": -0.14644366044264573, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00010547915735514835}, {"id": 598, "seek": 353824, "start": 3546.8799999999997, "end": 3551.8399999999997, "text": " you have probably already encountered many times during your undergrad. I mean, if you have any,", "tokens": [50796, 291, 362, 1391, 1217, 20381, 867, 1413, 1830, 428, 14295, 13, 286, 914, 11, 498, 291, 362, 604, 11, 51044], "temperature": 0.0, "avg_logprob": -0.14644366044264573, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00010547915735514835}, {"id": 599, "seek": 353824, "start": 3551.8399999999997, "end": 3557.12, "text": " let's say, background in, you know, single processing electrical engineering, there's many", "tokens": [51044, 718, 311, 584, 11, 3678, 294, 11, 291, 458, 11, 2167, 9007, 12147, 7043, 11, 456, 311, 867, 51308], "temperature": 0.0, "avg_logprob": -0.14644366044264573, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00010547915735514835}, {"id": 600, "seek": 353824, "start": 3557.12, "end": 3562.24, "text": " ways in which students have learned how to represent data, like signals, for example,", "tokens": [51308, 2098, 294, 597, 1731, 362, 3264, 577, 281, 2906, 1412, 11, 411, 12354, 11, 337, 1365, 11, 51564], "temperature": 0.0, "avg_logprob": -0.14644366044264573, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00010547915735514835}, {"id": 601, "seek": 353824, "start": 3562.24, "end": 3567.12, "text": " using Fourier transform. So Fourier transforms are an instance of a class that has universal", "tokens": [51564, 1228, 36810, 4088, 13, 407, 36810, 35592, 366, 364, 5197, 295, 257, 1508, 300, 575, 11455, 51808], "temperature": 0.0, "avg_logprob": -0.14644366044264573, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00010547915735514835}, {"id": 602, "seek": 356712, "start": 3567.2, "end": 3572.3199999999997, "text": " approximation. So in that sense, it's a it's a I think, going back to the second part of your", "tokens": [50368, 28023, 13, 407, 294, 300, 2020, 11, 309, 311, 257, 309, 311, 257, 286, 519, 11, 516, 646, 281, 264, 1150, 644, 295, 428, 50624], "temperature": 0.0, "avg_logprob": -0.09684782839835958, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005519838887266815}, {"id": 603, "seek": 356712, "start": 3572.3199999999997, "end": 3578.4, "text": " question, how relevant it is to in the context of neural networks, and how far does this thing", "tokens": [50624, 1168, 11, 577, 7340, 309, 307, 281, 294, 264, 4319, 295, 18161, 9590, 11, 293, 577, 1400, 775, 341, 551, 50928], "temperature": 0.0, "avg_logprob": -0.09684782839835958, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005519838887266815}, {"id": 604, "seek": 356712, "start": 3578.4, "end": 3584.56, "text": " pushes towards understanding why deep learning works. So I would say there's a there's two sides", "tokens": [50928, 21020, 3030, 3701, 983, 2452, 2539, 1985, 13, 407, 286, 576, 584, 456, 311, 257, 456, 311, 732, 4881, 51236], "temperature": 0.0, "avg_logprob": -0.09684782839835958, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005519838887266815}, {"id": 605, "seek": 356712, "start": 3584.56, "end": 3592.48, "text": " of the answer. On one hand, I think that universal approximation is is a tool that when you combine", "tokens": [51236, 295, 264, 1867, 13, 1282, 472, 1011, 11, 286, 519, 300, 11455, 28023, 307, 307, 257, 2290, 300, 562, 291, 10432, 51632], "temperature": 0.0, "avg_logprob": -0.09684782839835958, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005519838887266815}, {"id": 606, "seek": 359248, "start": 3592.48, "end": 3600.16, "text": " it with other elements, it becomes something that provides good guiding principles. For instance,", "tokens": [50364, 309, 365, 661, 4959, 11, 309, 3643, 746, 300, 6417, 665, 25061, 9156, 13, 1171, 5197, 11, 50748], "temperature": 0.0, "avg_logprob": -0.12204310537754805, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001029923208989203}, {"id": 607, "seek": 359248, "start": 3600.16, "end": 3604.88, "text": " universal approximation of a generic function, we know that yeah, we can as I said, we can obtain", "tokens": [50748, 11455, 28023, 295, 257, 19577, 2445, 11, 321, 458, 300, 1338, 11, 321, 393, 382, 286, 848, 11, 321, 393, 12701, 50984], "temperature": 0.0, "avg_logprob": -0.12204310537754805, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001029923208989203}, {"id": 608, "seek": 359248, "start": 3604.88, "end": 3610.08, "text": " it with, you know, very, very naive architectures, for example, just a shallow neural network without", "tokens": [50984, 309, 365, 11, 291, 458, 11, 588, 11, 588, 29052, 6331, 1303, 11, 337, 1365, 11, 445, 257, 20488, 18161, 3209, 1553, 51244], "temperature": 0.0, "avg_logprob": -0.12204310537754805, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001029923208989203}, {"id": 609, "seek": 359248, "start": 3610.08, "end": 3615.92, "text": " any kind of physical structure, special structure already has this property. Does it actually help", "tokens": [51244, 604, 733, 295, 4001, 3877, 11, 2121, 3877, 1217, 575, 341, 4707, 13, 4402, 309, 767, 854, 51536], "temperature": 0.0, "avg_logprob": -0.12204310537754805, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.001029923208989203}, {"id": 610, "seek": 361592, "start": 3616.0, "end": 3622.16, "text": " us to to learn very efficiently? No, right, and I'm going to go at this afterwards. But when you", "tokens": [50368, 505, 281, 281, 1466, 588, 19621, 30, 883, 11, 558, 11, 293, 286, 478, 516, 281, 352, 412, 341, 10543, 13, 583, 562, 291, 50676], "temperature": 0.0, "avg_logprob": -0.13719786008199056, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.0018671697471290827}, {"id": 611, "seek": 361592, "start": 3622.16, "end": 3628.4, "text": " combine it, for example, with, you know, let's say that now your data lives on a graph, or your data,", "tokens": [50676, 10432, 309, 11, 337, 1365, 11, 365, 11, 291, 458, 11, 718, 311, 584, 300, 586, 428, 1412, 2909, 322, 257, 4295, 11, 420, 428, 1412, 11, 50988], "temperature": 0.0, "avg_logprob": -0.13719786008199056, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.0018671697471290827}, {"id": 612, "seek": 361592, "start": 3628.4, "end": 3632.7200000000003, "text": " I don't know, has a certain like a come from a physical lab that has certain properties,", "tokens": [50988, 286, 500, 380, 458, 11, 575, 257, 1629, 411, 257, 808, 490, 257, 4001, 2715, 300, 575, 1629, 7221, 11, 51204], "temperature": 0.0, "avg_logprob": -0.13719786008199056, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.0018671697471290827}, {"id": 613, "seek": 361592, "start": 3632.7200000000003, "end": 3637.2000000000003, "text": " let's say that it's rotational invariant. Like the first thing that that the designer,", "tokens": [51204, 718, 311, 584, 300, 309, 311, 45420, 33270, 394, 13, 1743, 264, 700, 551, 300, 300, 264, 11795, 11, 51428], "temperature": 0.0, "avg_logprob": -0.13719786008199056, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.0018671697471290827}, {"id": 614, "seek": 361592, "start": 3637.2000000000003, "end": 3641.12, "text": " like a domain scientist would like to know, if you come there and you design your neural", "tokens": [51428, 411, 257, 9274, 12662, 576, 411, 281, 458, 11, 498, 291, 808, 456, 293, 291, 1715, 428, 18161, 51624], "temperature": 0.0, "avg_logprob": -0.13719786008199056, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.0018671697471290827}, {"id": 615, "seek": 364112, "start": 3641.12, "end": 3646.96, "text": " net, look, I have a neural net that takes your data and has very good performance. The first thing", "tokens": [50364, 2533, 11, 574, 11, 286, 362, 257, 18161, 2533, 300, 2516, 428, 1412, 293, 575, 588, 665, 3389, 13, 440, 700, 551, 50656], "temperature": 0.0, "avg_logprob": -0.14642814762336165, "compression_ratio": 1.7526881720430108, "no_speech_prob": 0.0012044216273352504}, {"id": 616, "seek": 364112, "start": 3646.96, "end": 3652.08, "text": " that he will ask is, okay, how general is your architecture, right? Can it explain anything", "tokens": [50656, 300, 415, 486, 1029, 307, 11, 1392, 11, 577, 2674, 307, 428, 9482, 11, 558, 30, 1664, 309, 2903, 1340, 50912], "temperature": 0.0, "avg_logprob": -0.14642814762336165, "compression_ratio": 1.7526881720430108, "no_speech_prob": 0.0012044216273352504}, {"id": 617, "seek": 364112, "start": 3652.08, "end": 3657.2, "text": " that they could throw at you? It seems like it's a, in that sense, I would, I would present it more", "tokens": [50912, 300, 436, 727, 3507, 412, 291, 30, 467, 2544, 411, 309, 311, 257, 11, 294, 300, 2020, 11, 286, 576, 11, 286, 576, 1974, 309, 544, 51168], "temperature": 0.0, "avg_logprob": -0.14642814762336165, "compression_ratio": 1.7526881720430108, "no_speech_prob": 0.0012044216273352504}, {"id": 618, "seek": 364112, "start": 3657.8399999999997, "end": 3663.04, "text": " as a sufficient condition, like a check mark that your, your, you know, your architecture needs to", "tokens": [51200, 382, 257, 11563, 4188, 11, 411, 257, 1520, 1491, 300, 428, 11, 428, 11, 291, 458, 11, 428, 9482, 2203, 281, 51460], "temperature": 0.0, "avg_logprob": -0.14642814762336165, "compression_ratio": 1.7526881720430108, "no_speech_prob": 0.0012044216273352504}, {"id": 619, "seek": 364112, "start": 3663.04, "end": 3667.3599999999997, "text": " fall, right? If you make more and more parameters, can you express more and more elements functions", "tokens": [51460, 2100, 11, 558, 30, 759, 291, 652, 544, 293, 544, 9834, 11, 393, 291, 5109, 544, 293, 544, 4959, 6828, 51676], "temperature": 0.0, "avg_logprob": -0.14642814762336165, "compression_ratio": 1.7526881720430108, "no_speech_prob": 0.0012044216273352504}, {"id": 620, "seek": 366736, "start": 3667.36, "end": 3674.32, "text": " from your class? But then, as I said, is it far from being sufficient, right? It's like, it's,", "tokens": [50364, 490, 428, 1508, 30, 583, 550, 11, 382, 286, 848, 11, 307, 309, 1400, 490, 885, 11563, 11, 558, 30, 467, 311, 411, 11, 309, 311, 11, 50712], "temperature": 0.0, "avg_logprob": -0.14380312747642643, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0012240811483934522}, {"id": 621, "seek": 366736, "start": 3674.32, "end": 3678.4, "text": " sorry, it's a, it's a necessary condition, but it's far from being sufficient in the sense that", "tokens": [50712, 2597, 11, 309, 311, 257, 11, 309, 311, 257, 4818, 4188, 11, 457, 309, 311, 1400, 490, 885, 11563, 294, 264, 2020, 300, 50916], "temperature": 0.0, "avg_logprob": -0.14380312747642643, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0012240811483934522}, {"id": 622, "seek": 366736, "start": 3680.0, "end": 3687.04, "text": " universal approximation has a flavor is a result that does not quantify how many parameters", "tokens": [50996, 11455, 28023, 575, 257, 6813, 307, 257, 1874, 300, 775, 406, 40421, 577, 867, 9834, 51348], "temperature": 0.0, "avg_logprob": -0.14380312747642643, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0012240811483934522}, {"id": 623, "seek": 366736, "start": 3687.04, "end": 3691.6, "text": " do I need, right? Like, you know, if I want to approximate function, let's say I want to classify", "tokens": [51348, 360, 286, 643, 11, 558, 30, 1743, 11, 291, 458, 11, 498, 286, 528, 281, 30874, 2445, 11, 718, 311, 584, 286, 528, 281, 33872, 51576], "temperature": 0.0, "avg_logprob": -0.14380312747642643, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0012240811483934522}, {"id": 624, "seek": 366736, "start": 3691.6, "end": 3696.32, "text": " between different dog breeds, it doesn't tell me this theorem doesn't tell me how many parameters,", "tokens": [51576, 1296, 819, 3000, 41609, 11, 309, 1177, 380, 980, 385, 341, 20904, 1177, 380, 980, 385, 577, 867, 9834, 11, 51812], "temperature": 0.0, "avg_logprob": -0.14380312747642643, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0012240811483934522}, {"id": 625, "seek": 369632, "start": 3696.32, "end": 3700.0800000000004, "text": " how many neurons do I need for that, right? It's, it's a statement that in that sense,", "tokens": [50364, 577, 867, 22027, 360, 286, 643, 337, 300, 11, 558, 30, 467, 311, 11, 309, 311, 257, 5629, 300, 294, 300, 2020, 11, 50552], "temperature": 0.0, "avg_logprob": -0.13964077758789062, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0005033730994910002}, {"id": 626, "seek": 369632, "start": 3701.1200000000003, "end": 3704.48, "text": " it lets, it leaves you a little bit with your, like, say, like, you know, like,", "tokens": [50604, 309, 6653, 11, 309, 5510, 291, 257, 707, 857, 365, 428, 11, 411, 11, 584, 11, 411, 11, 291, 458, 11, 411, 11, 50772], "temperature": 0.0, "avg_logprob": -0.13964077758789062, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0005033730994910002}, {"id": 627, "seek": 369632, "start": 3705.1200000000003, "end": 3709.04, "text": " like the, it's a bittersweet result, right? It doesn't, it doesn't really tell you anything", "tokens": [50804, 411, 264, 11, 309, 311, 257, 857, 1559, 10354, 1874, 11, 558, 30, 467, 1177, 380, 11, 309, 1177, 380, 534, 980, 291, 1340, 51000], "temperature": 0.0, "avg_logprob": -0.13964077758789062, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0005033730994910002}, {"id": 628, "seek": 369632, "start": 3709.04, "end": 3715.6000000000004, "text": " actual. So that's why, and that's why we enter these, these other questions that is actually", "tokens": [51000, 3539, 13, 407, 300, 311, 983, 11, 293, 300, 311, 983, 321, 3242, 613, 11, 613, 661, 1651, 300, 307, 767, 51328], "temperature": 0.0, "avg_logprob": -0.13964077758789062, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0005033730994910002}, {"id": 629, "seek": 369632, "start": 3715.6000000000004, "end": 3722.0800000000004, "text": " much deeper. And to some extent, still reading them pretty much open, that is, how do you actually", "tokens": [51328, 709, 7731, 13, 400, 281, 512, 8396, 11, 920, 3760, 552, 1238, 709, 1269, 11, 300, 307, 11, 577, 360, 291, 767, 51652], "temperature": 0.0, "avg_logprob": -0.13964077758789062, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0005033730994910002}, {"id": 630, "seek": 372208, "start": 3722.08, "end": 3726.4, "text": " go from this, this statement to something that is quantitative, right? Something that tells you,", "tokens": [50364, 352, 490, 341, 11, 341, 5629, 281, 746, 300, 307, 27778, 11, 558, 30, 6595, 300, 5112, 291, 11, 50580], "temperature": 0.0, "avg_logprob": -0.10440960797396573, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0005432917387224734}, {"id": 631, "seek": 372208, "start": 3726.4, "end": 3732.0, "text": " okay, you know, you need that many layers, you need that many parameters. And so this is where", "tokens": [50580, 1392, 11, 291, 458, 11, 291, 643, 300, 867, 7914, 11, 291, 643, 300, 867, 9834, 13, 400, 370, 341, 307, 689, 50860], "temperature": 0.0, "avg_logprob": -0.10440960797396573, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0005432917387224734}, {"id": 632, "seek": 372208, "start": 3732.0, "end": 3738.3199999999997, "text": " the role of depth in neural networks is, you know, becomes essentially the key open question.", "tokens": [50860, 264, 3090, 295, 7161, 294, 18161, 9590, 307, 11, 291, 458, 11, 3643, 4476, 264, 2141, 1269, 1168, 13, 51176], "temperature": 0.0, "avg_logprob": -0.10440960797396573, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0005432917387224734}, {"id": 633, "seek": 372208, "start": 3738.88, "end": 3745.04, "text": " And, and yeah, so in this, in this quote that you, that you brought from this paper,", "tokens": [51204, 400, 11, 293, 1338, 11, 370, 294, 341, 11, 294, 341, 6513, 300, 291, 11, 300, 291, 3038, 490, 341, 3035, 11, 51512], "temperature": 0.0, "avg_logprob": -0.10440960797396573, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0005432917387224734}, {"id": 634, "seek": 372208, "start": 3745.68, "end": 3751.68, "text": " that kind of reflected our understanding at the time of maybe the true power of universal", "tokens": [51544, 300, 733, 295, 15502, 527, 3701, 412, 264, 565, 295, 1310, 264, 2074, 1347, 295, 11455, 51844], "temperature": 0.0, "avg_logprob": -0.10440960797396573, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0005432917387224734}, {"id": 635, "seek": 375168, "start": 3751.68, "end": 3757.68, "text": " approximation is, you know, when, as you combine it with these other prior, that is the asymmetries", "tokens": [50364, 28023, 307, 11, 291, 458, 11, 562, 11, 382, 291, 10432, 309, 365, 613, 661, 4059, 11, 300, 307, 264, 37277, 302, 2244, 50664], "temperature": 0.0, "avg_logprob": -0.16850708900613987, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0002097201650030911}, {"id": 636, "seek": 375168, "start": 3757.68, "end": 3762.3999999999996, "text": " of the data, like the invariances. So I don't want to represent arbitrary functions, I only want to", "tokens": [50664, 295, 264, 1412, 11, 411, 264, 33270, 2676, 13, 407, 286, 500, 380, 528, 281, 2906, 23211, 6828, 11, 286, 787, 528, 281, 50900], "temperature": 0.0, "avg_logprob": -0.16850708900613987, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0002097201650030911}, {"id": 637, "seek": 375168, "start": 3762.3999999999996, "end": 3772.64, "text": " represent functions that are invariant to certain transformations of the input. So in fact, our,", "tokens": [50900, 2906, 6828, 300, 366, 33270, 394, 281, 1629, 34852, 295, 264, 4846, 13, 407, 294, 1186, 11, 527, 11, 51412], "temperature": 0.0, "avg_logprob": -0.16850708900613987, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0002097201650030911}, {"id": 638, "seek": 375168, "start": 3772.64, "end": 3779.2799999999997, "text": " at least my particular view of the problem, analysis of the problem has somehow evolved in the last", "tokens": [51412, 412, 1935, 452, 1729, 1910, 295, 264, 1154, 11, 5215, 295, 264, 1154, 575, 6063, 14178, 294, 264, 1036, 51744], "temperature": 0.0, "avg_logprob": -0.16850708900613987, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0002097201650030911}, {"id": 639, "seek": 377928, "start": 3779.36, "end": 3783.76, "text": " years, right? Of course, through research that I've done together with my collaborators.", "tokens": [50368, 924, 11, 558, 30, 2720, 1164, 11, 807, 2132, 300, 286, 600, 1096, 1214, 365, 452, 39789, 13, 50588], "temperature": 0.0, "avg_logprob": -0.13396122973898184, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0007782616885378957}, {"id": 640, "seek": 377928, "start": 3783.76, "end": 3791.1200000000003, "text": " And now, and this is actually the way we present it in the book, we, we, we kind of identify two", "tokens": [50588, 400, 586, 11, 293, 341, 307, 767, 264, 636, 321, 1974, 309, 294, 264, 1446, 11, 321, 11, 321, 11, 321, 733, 295, 5876, 732, 50956], "temperature": 0.0, "avg_logprob": -0.13396122973898184, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0007782616885378957}, {"id": 641, "seek": 377928, "start": 3791.1200000000003, "end": 3797.1200000000003, "text": " different flavors, two different sources of prior information that one needs to bake into the problem,", "tokens": [50956, 819, 16303, 11, 732, 819, 7139, 295, 4059, 1589, 300, 472, 2203, 281, 16562, 666, 264, 1154, 11, 51256], "temperature": 0.0, "avg_logprob": -0.13396122973898184, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0007782616885378957}, {"id": 642, "seek": 377928, "start": 3797.1200000000003, "end": 3802.32, "text": " right? To, to really go beyond this like basic approximation result of neural nets. The first", "tokens": [51256, 558, 30, 1407, 11, 281, 534, 352, 4399, 341, 411, 3875, 28023, 1874, 295, 18161, 36170, 13, 440, 700, 51516], "temperature": 0.0, "avg_logprob": -0.13396122973898184, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0007782616885378957}, {"id": 643, "seek": 377928, "start": 3802.32, "end": 3808.6400000000003, "text": " one you need is invariance, right, is a disability that you need to, like the fact that you actually", "tokens": [51516, 472, 291, 643, 307, 33270, 719, 11, 558, 11, 307, 257, 11090, 300, 291, 643, 281, 11, 411, 264, 1186, 300, 291, 767, 51832], "temperature": 0.0, "avg_logprob": -0.13396122973898184, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0007782616885378957}, {"id": 644, "seek": 380864, "start": 3808.64, "end": 3814.3199999999997, "text": " put symmetries into the architecture is certainly going to have a benefit in terms of sample", "tokens": [50364, 829, 14232, 302, 2244, 666, 264, 9482, 307, 3297, 516, 281, 362, 257, 5121, 294, 2115, 295, 6889, 50648], "temperature": 0.0, "avg_logprob": -0.10272698442475135, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0002909886243287474}, {"id": 645, "seek": 380864, "start": 3814.3199999999997, "end": 3818.3199999999997, "text": " complexity, right? They, I mean, you are going to learn more efficiently if your model is aware", "tokens": [50648, 14024, 11, 558, 30, 814, 11, 286, 914, 11, 291, 366, 516, 281, 1466, 544, 19621, 498, 428, 2316, 307, 3650, 50848], "temperature": 0.0, "avg_logprob": -0.10272698442475135, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0002909886243287474}, {"id": 646, "seek": 380864, "start": 3818.3199999999997, "end": 3825.44, "text": " of the symmetries of the world. But in fact, this, this prior, in fact, we know now that it's not", "tokens": [50848, 295, 264, 14232, 302, 2244, 295, 264, 1002, 13, 583, 294, 1186, 11, 341, 11, 341, 4059, 11, 294, 1186, 11, 321, 458, 586, 300, 309, 311, 406, 51204], "temperature": 0.0, "avg_logprob": -0.10272698442475135, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0002909886243287474}, {"id": 647, "seek": 380864, "start": 3825.44, "end": 3831.2799999999997, "text": " sufficient, right? If you only like agnosticially build your learning system, just with these", "tokens": [51204, 11563, 11, 558, 30, 759, 291, 787, 411, 623, 77, 19634, 2270, 1322, 428, 2539, 1185, 11, 445, 365, 613, 51496], "temperature": 0.0, "avg_logprob": -0.10272698442475135, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0002909886243287474}, {"id": 648, "seek": 380864, "start": 3831.2799999999997, "end": 3836.0, "text": " symmetries in mind, indeed, you are going to become more efficient that a system that is", "tokens": [51496, 14232, 302, 2244, 294, 1575, 11, 6451, 11, 291, 366, 516, 281, 1813, 544, 7148, 300, 257, 1185, 300, 307, 51732], "temperature": 0.0, "avg_logprob": -0.10272698442475135, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0002909886243287474}, {"id": 649, "seek": 383600, "start": 3836.0, "end": 3841.76, "text": " completely agnostic to symmetries. But it might not be, you might not be able to formally establish", "tokens": [50364, 2584, 623, 77, 19634, 281, 14232, 302, 2244, 13, 583, 309, 1062, 406, 312, 11, 291, 1062, 406, 312, 1075, 281, 25983, 8327, 50652], "temperature": 0.0, "avg_logprob": -0.09093375967330292, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0003197404439561069}, {"id": 650, "seek": 383600, "start": 3841.76, "end": 3847.44, "text": " what we call like a, like a learning guarantee that has good sample complexity. And I guess that I", "tokens": [50652, 437, 321, 818, 411, 257, 11, 411, 257, 2539, 10815, 300, 575, 665, 6889, 14024, 13, 400, 286, 2041, 300, 286, 50936], "temperature": 0.0, "avg_logprob": -0.09093375967330292, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0003197404439561069}, {"id": 651, "seek": 383600, "start": 3847.44, "end": 3852.48, "text": " don't want to go too much into the jargon and the details of what this means. But the idea is that if", "tokens": [50936, 500, 380, 528, 281, 352, 886, 709, 666, 264, 15181, 10660, 293, 264, 4365, 295, 437, 341, 1355, 13, 583, 264, 1558, 307, 300, 498, 51188], "temperature": 0.0, "avg_logprob": -0.09093375967330292, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0003197404439561069}, {"id": 652, "seek": 383600, "start": 3852.48, "end": 3857.76, "text": " I want to, you know, learn this function with certain precision, how many examples, how many", "tokens": [51188, 286, 528, 281, 11, 291, 458, 11, 1466, 341, 2445, 365, 1629, 18356, 11, 577, 867, 5110, 11, 577, 867, 51452], "temperature": 0.0, "avg_logprob": -0.09093375967330292, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0003197404439561069}, {"id": 653, "seek": 383600, "start": 3857.76, "end": 3862.24, "text": " training examples do I need to kind of give you a certificate for authentication, like a guarantee", "tokens": [51452, 3097, 5110, 360, 286, 643, 281, 733, 295, 976, 291, 257, 15953, 337, 26643, 11, 411, 257, 10815, 51676], "temperature": 0.0, "avg_logprob": -0.09093375967330292, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0003197404439561069}, {"id": 654, "seek": 386224, "start": 3862.24, "end": 3867.52, "text": " that I'm going to be able to do that. So with symmetries alone, it's not something that we", "tokens": [50364, 300, 286, 478, 516, 281, 312, 1075, 281, 360, 300, 13, 407, 365, 14232, 302, 2244, 3312, 11, 309, 311, 406, 746, 300, 321, 50628], "temperature": 0.0, "avg_logprob": -0.1105598076841885, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0008955284138210118}, {"id": 655, "seek": 386224, "start": 3867.52, "end": 3872.24, "text": " know how to do. In fact, we are, we would believe we have strong beliefs that it's not possible,", "tokens": [50628, 458, 577, 281, 360, 13, 682, 1186, 11, 321, 366, 11, 321, 576, 1697, 321, 362, 2068, 13585, 300, 309, 311, 406, 1944, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1105598076841885, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0008955284138210118}, {"id": 656, "seek": 386224, "start": 3872.24, "end": 3879.2799999999997, "text": " right? There's examples out there that I could, I could, you know, construct a function that has", "tokens": [50864, 558, 30, 821, 311, 5110, 484, 456, 300, 286, 727, 11, 286, 727, 11, 291, 458, 11, 7690, 257, 2445, 300, 575, 51216], "temperature": 0.0, "avg_logprob": -0.1105598076841885, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0008955284138210118}, {"id": 657, "seek": 386224, "start": 3879.2799999999997, "end": 3885.6, "text": " the right symmetries, has the right priors, if you want, but still needs a lot, a lot, a lot of", "tokens": [51216, 264, 558, 14232, 302, 2244, 11, 575, 264, 558, 1790, 830, 11, 498, 291, 528, 11, 457, 920, 2203, 257, 688, 11, 257, 688, 11, 257, 688, 295, 51532], "temperature": 0.0, "avg_logprob": -0.1105598076841885, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0008955284138210118}, {"id": 658, "seek": 386224, "start": 3885.6, "end": 3891.12, "text": " examples to build to learn. So what we need is to add something else into the mix. And this is this", "tokens": [51532, 5110, 281, 1322, 281, 1466, 13, 407, 437, 321, 643, 307, 281, 909, 746, 1646, 666, 264, 2890, 13, 400, 341, 307, 341, 51808], "temperature": 0.0, "avg_logprob": -0.1105598076841885, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0008955284138210118}, {"id": 659, "seek": 389112, "start": 3891.12, "end": 3896.4, "text": " something that we call in the, in the book, this scale separation. And, and, and if you want, I can", "tokens": [50364, 746, 300, 321, 818, 294, 264, 11, 294, 264, 1446, 11, 341, 4373, 14634, 13, 400, 11, 293, 11, 293, 498, 291, 528, 11, 286, 393, 50628], "temperature": 0.0, "avg_logprob": -0.08789389250708408, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.00037968202377669513}, {"id": 660, "seek": 389112, "start": 3896.4, "end": 3903.3599999999997, "text": " try to very briefly give you an intuitive idea of what this means. So if you think about the problem", "tokens": [50628, 853, 281, 588, 10515, 976, 291, 364, 21769, 1558, 295, 437, 341, 1355, 13, 407, 498, 291, 519, 466, 264, 1154, 50976], "temperature": 0.0, "avg_logprob": -0.08789389250708408, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.00037968202377669513}, {"id": 661, "seek": 389112, "start": 3903.3599999999997, "end": 3908.56, "text": " of classifying an image, like a dog or the cat. So what is given to you is like a big", "tokens": [50976, 295, 1508, 5489, 364, 3256, 11, 411, 257, 3000, 420, 264, 3857, 13, 407, 437, 307, 2212, 281, 291, 307, 411, 257, 955, 51236], "temperature": 0.0, "avg_logprob": -0.08789389250708408, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.00037968202377669513}, {"id": 662, "seek": 389112, "start": 3908.56, "end": 3912.72, "text": " branch of pixels, right? Every pixel has a color value. So somehow you need to figure out", "tokens": [51236, 9819, 295, 18668, 11, 558, 30, 2048, 19261, 575, 257, 2017, 2158, 13, 407, 6063, 291, 643, 281, 2573, 484, 51444], "temperature": 0.0, "avg_logprob": -0.08789389250708408, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.00037968202377669513}, {"id": 663, "seek": 389112, "start": 3913.68, "end": 3917.92, "text": " the thing that you're looking for is lying in some kind of like, it's really through the", "tokens": [51492, 264, 551, 300, 291, 434, 1237, 337, 307, 8493, 294, 512, 733, 295, 411, 11, 309, 311, 534, 807, 264, 51704], "temperature": 0.0, "avg_logprob": -0.08789389250708408, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.00037968202377669513}, {"id": 664, "seek": 391792, "start": 3917.92, "end": 3921.44, "text": " interactions between pixels that you get the answer, right? And the question is,", "tokens": [50364, 13280, 1296, 18668, 300, 291, 483, 264, 1867, 11, 558, 30, 400, 264, 1168, 307, 11, 50540], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 665, "seek": 391792, "start": 3922.48, "end": 3926.96, "text": " of course, if I have a thousand pixels, how many possible interactions do I have between", "tokens": [50592, 295, 1164, 11, 498, 286, 362, 257, 4714, 18668, 11, 577, 867, 1944, 13280, 360, 286, 362, 1296, 50816], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 666, "seek": 391792, "start": 3926.96, "end": 3931.36, "text": " thousand elements? So this is where this exponential or the curse of dimensionality appears, right?", "tokens": [50816, 4714, 4959, 30, 407, 341, 307, 689, 341, 21510, 420, 264, 17139, 295, 10139, 1860, 7038, 11, 558, 30, 51036], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 667, "seek": 391792, "start": 3931.36, "end": 3936.32, "text": " I need to, a priori, I should be looking at all possible families of interactions between", "tokens": [51036, 286, 643, 281, 11, 257, 4059, 72, 11, 286, 820, 312, 1237, 412, 439, 1944, 4466, 295, 13280, 1296, 51284], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 668, "seek": 391792, "start": 3936.32, "end": 3940.8, "text": " pixels. And this is where maybe where my signal would be lived. Of course, if I need to look for", "tokens": [51284, 18668, 13, 400, 341, 307, 689, 1310, 689, 452, 6358, 576, 312, 5152, 13, 2720, 1164, 11, 498, 286, 643, 281, 574, 337, 51508], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 669, "seek": 391792, "start": 3940.8, "end": 3947.44, "text": " all of these things, it's impossible, right? There's just too many things. If I tell you that the,", "tokens": [51508, 439, 295, 613, 721, 11, 309, 311, 6243, 11, 558, 30, 821, 311, 445, 886, 867, 721, 13, 759, 286, 980, 291, 300, 264, 11, 51840], "temperature": 0.0, "avg_logprob": -0.10205135060780084, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0006441789446398616}, {"id": 670, "seek": 394744, "start": 3947.44, "end": 3951.68, "text": " you know, these interactions are such that there's this translation symmetry. Well, you might not", "tokens": [50364, 291, 458, 11, 613, 13280, 366, 1270, 300, 456, 311, 341, 12853, 25440, 13, 1042, 11, 291, 1062, 406, 50576], "temperature": 0.0, "avg_logprob": -0.09966361415278804, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.0005786651745438576}, {"id": 671, "seek": 394744, "start": 3951.68, "end": 3957.92, "text": " be, you might not be needing to look at all of them. But in fact, you don't need, you do not", "tokens": [50576, 312, 11, 291, 1062, 406, 312, 18006, 281, 574, 412, 439, 295, 552, 13, 583, 294, 1186, 11, 291, 500, 380, 643, 11, 291, 360, 406, 50888], "temperature": 0.0, "avg_logprob": -0.09966361415278804, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.0005786651745438576}, {"id": 672, "seek": 394744, "start": 3957.92, "end": 3963.84, "text": " throw enough. So what is really something that is powerful is that I tell you that maybe the", "tokens": [50888, 3507, 1547, 13, 407, 437, 307, 534, 746, 300, 307, 4005, 307, 300, 286, 980, 291, 300, 1310, 264, 51184], "temperature": 0.0, "avg_logprob": -0.09966361415278804, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.0005786651745438576}, {"id": 673, "seek": 394744, "start": 3963.84, "end": 3969.2000000000003, "text": " interactions that matter the most are those between a pixel and its neighbors, right? And if you", "tokens": [51184, 13280, 300, 1871, 264, 881, 366, 729, 1296, 257, 19261, 293, 1080, 12512, 11, 558, 30, 400, 498, 291, 51452], "temperature": 0.0, "avg_logprob": -0.09966361415278804, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.0005786651745438576}, {"id": 674, "seek": 394744, "start": 3969.2000000000003, "end": 3974.4, "text": " understand very well, if you base your initial learning steps into understanding well, which", "tokens": [51452, 1223, 588, 731, 11, 498, 291, 3096, 428, 5883, 2539, 4439, 666, 3701, 731, 11, 597, 51712], "temperature": 0.0, "avg_logprob": -0.09966361415278804, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.0005786651745438576}, {"id": 675, "seek": 397440, "start": 3974.4, "end": 3980.56, "text": " local interaction matters, maybe you can use them to bootstrap the interactions that go to look at", "tokens": [50364, 2654, 9285, 7001, 11, 1310, 291, 393, 764, 552, 281, 11450, 372, 4007, 264, 13280, 300, 352, 281, 574, 412, 50672], "temperature": 0.0, "avg_logprob": -0.1507732326334173, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.0007535538752563298}, {"id": 676, "seek": 397440, "start": 3980.56, "end": 3985.44, "text": " this neighborhood to slightly bigger neighborhoods, right? And so this idea that you can break a very", "tokens": [50672, 341, 7630, 281, 4748, 3801, 20052, 11, 558, 30, 400, 370, 341, 1558, 300, 291, 393, 1821, 257, 588, 50916], "temperature": 0.0, "avg_logprob": -0.1507732326334173, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.0007535538752563298}, {"id": 677, "seek": 397440, "start": 3985.44, "end": 3991.92, "text": " complicated problem into a families of sub problems that lives in different scales. This is at the,", "tokens": [50916, 6179, 1154, 666, 257, 4466, 295, 1422, 2740, 300, 2909, 294, 819, 17408, 13, 639, 307, 412, 264, 11, 51240], "temperature": 0.0, "avg_logprob": -0.1507732326334173, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.0007535538752563298}, {"id": 678, "seek": 397440, "start": 3991.92, "end": 3997.04, "text": " I would say, at the intuitive level, something print like at the core of the essence of why", "tokens": [51240, 286, 576, 584, 11, 412, 264, 21769, 1496, 11, 746, 4482, 411, 412, 264, 4965, 295, 264, 12801, 295, 983, 51496], "temperature": 0.0, "avg_logprob": -0.1507732326334173, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.0007535538752563298}, {"id": 679, "seek": 399704, "start": 3997.04, "end": 4004.64, "text": " these architectures are so efficient. This idea, as you might imagine, is not new. It's not specific", "tokens": [50364, 613, 6331, 1303, 366, 370, 7148, 13, 639, 1558, 11, 382, 291, 1062, 3811, 11, 307, 406, 777, 13, 467, 311, 406, 2685, 50744], "temperature": 0.0, "avg_logprob": -0.09758359660273012, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0010637021623551846}, {"id": 680, "seek": 399704, "start": 4004.64, "end": 4009.6, "text": " to deep learning. The idea that you can take a complicated system of interacting particles", "tokens": [50744, 281, 2452, 2539, 13, 440, 1558, 300, 291, 393, 747, 257, 6179, 1185, 295, 18017, 10007, 50992], "temperature": 0.0, "avg_logprob": -0.09758359660273012, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0010637021623551846}, {"id": 681, "seek": 399704, "start": 4010.16, "end": 4015.52, "text": " and break it into different scales. This is at the, at the basis of essentially all of physics", "tokens": [51020, 293, 1821, 309, 666, 819, 17408, 13, 639, 307, 412, 264, 11, 412, 264, 5143, 295, 4476, 439, 295, 10649, 51288], "temperature": 0.0, "avg_logprob": -0.09758359660273012, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0010637021623551846}, {"id": 682, "seek": 399704, "start": 4015.52, "end": 4020.16, "text": " and chemistry, right? There's many, many, you know, like when people study, even like biology life,", "tokens": [51288, 293, 12558, 11, 558, 30, 821, 311, 867, 11, 867, 11, 291, 458, 11, 411, 562, 561, 2979, 11, 754, 411, 14956, 993, 11, 51520], "temperature": 0.0, "avg_logprob": -0.09758359660273012, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0010637021623551846}, {"id": 683, "seek": 399704, "start": 4020.16, "end": 4026.0, "text": " right? You have, you have experts that are very experts at the molecular level. Then you have", "tokens": [51520, 558, 30, 509, 362, 11, 291, 362, 8572, 300, 366, 588, 8572, 412, 264, 19046, 1496, 13, 1396, 291, 362, 51812], "temperature": 0.0, "avg_logprob": -0.09758359660273012, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0010637021623551846}, {"id": 684, "seek": 402600, "start": 4026.0, "end": 4029.6, "text": " experts that, you know, might understand like, you know, doctors that understand things at the", "tokens": [50364, 8572, 300, 11, 291, 458, 11, 1062, 1223, 411, 11, 291, 458, 11, 8778, 300, 1223, 721, 412, 264, 50544], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 685, "seek": 402600, "start": 4029.6, "end": 4033.68, "text": " level of okay functions. And then there's maybe experts at the level of the society, right? But", "tokens": [50544, 1496, 295, 1392, 6828, 13, 400, 550, 456, 311, 1310, 8572, 412, 264, 1496, 295, 264, 4086, 11, 558, 30, 583, 50748], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 686, "seek": 402600, "start": 4033.68, "end": 4039.28, "text": " this, you know, it's pretty natural to break the very complicated thing into different scales.", "tokens": [50748, 341, 11, 291, 458, 11, 309, 311, 1238, 3303, 281, 1821, 264, 588, 6179, 551, 666, 819, 17408, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 687, "seek": 402600, "start": 4039.28, "end": 4043.84, "text": " And so deep neural networks somehow are able to do that. We don't have the full mathematical", "tokens": [51028, 400, 370, 2452, 18161, 9590, 6063, 366, 1075, 281, 360, 300, 13, 492, 500, 380, 362, 264, 1577, 18894, 51256], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 688, "seek": 402600, "start": 4043.84, "end": 4049.44, "text": " picture, right? Or for example, why this scale separation is strictly necessary. What we know", "tokens": [51256, 3036, 11, 558, 30, 1610, 337, 1365, 11, 983, 341, 4373, 14634, 307, 20792, 4818, 13, 708, 321, 458, 51536], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 689, "seek": 402600, "start": 4049.44, "end": 4054.96, "text": " from empirical evidence, like that is now I would say indisputable, is that this is an efficient way", "tokens": [51536, 490, 31886, 4467, 11, 411, 300, 307, 586, 286, 576, 584, 1016, 271, 2582, 712, 11, 307, 300, 341, 307, 364, 7148, 636, 51812], "temperature": 0.0, "avg_logprob": -0.10216964180789777, "compression_ratio": 1.7576687116564418, "no_speech_prob": 0.0005268303793855011}, {"id": 690, "seek": 405496, "start": 4054.96, "end": 4059.36, "text": " to do that, right? Because when I was, when I was reading the prototype book, I noticed that", "tokens": [50364, 281, 360, 300, 11, 558, 30, 1436, 562, 286, 390, 11, 562, 286, 390, 3760, 264, 19475, 1446, 11, 286, 5694, 300, 50584], "temperature": 0.0, "avg_logprob": -0.12126818157377697, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.0036929866764694452}, {"id": 691, "seek": 405496, "start": 4059.36, "end": 4064.08, "text": " there was a separation between the symmetries and the scale separation. Could you explain in", "tokens": [50584, 456, 390, 257, 14634, 1296, 264, 14232, 302, 2244, 293, 264, 4373, 14634, 13, 7497, 291, 2903, 294, 50820], "temperature": 0.0, "avg_logprob": -0.12126818157377697, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.0036929866764694452}, {"id": 692, "seek": 405496, "start": 4064.08, "end": 4069.36, "text": " simple terms, why is the scale separation not just a symmetry as well? Because it seemed a little bit,", "tokens": [50820, 2199, 2115, 11, 983, 307, 264, 4373, 14634, 406, 445, 257, 25440, 382, 731, 30, 1436, 309, 6576, 257, 707, 857, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12126818157377697, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.0036929866764694452}, {"id": 693, "seek": 405496, "start": 4069.36, "end": 4074.7200000000003, "text": " I don't want to say kluge, but it seemed like you had this scale matter and you dealt with it", "tokens": [51084, 286, 500, 380, 528, 281, 584, 350, 2781, 432, 11, 457, 309, 6576, 411, 291, 632, 341, 4373, 1871, 293, 291, 15991, 365, 309, 51352], "temperature": 0.0, "avg_logprob": -0.12126818157377697, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.0036929866764694452}, {"id": 694, "seek": 405496, "start": 4074.7200000000003, "end": 4080.88, "text": " separately. Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate", "tokens": [51352, 14759, 13, 865, 11, 300, 311, 257, 665, 11, 300, 311, 257, 665, 935, 13, 407, 11, 370, 1310, 300, 264, 636, 281, 11, 281, 11, 281, 4994, 51660], "temperature": 0.0, "avg_logprob": -0.12126818157377697, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.0036929866764694452}, {"id": 695, "seek": 408088, "start": 4080.88, "end": 4087.36, "text": " these two would be if you think about like an algorithmic instantiation, if you want to have", "tokens": [50364, 613, 732, 576, 312, 498, 291, 519, 466, 411, 364, 9284, 299, 9836, 6642, 11, 498, 291, 528, 281, 362, 50688], "temperature": 0.0, "avg_logprob": -0.08923421212292593, "compression_ratio": 1.8527131782945736, "no_speech_prob": 0.0005272086127661169}, {"id": 696, "seek": 408088, "start": 4087.36, "end": 4092.96, "text": " a network that would just break the problem into different scales, it would be like a neural network", "tokens": [50688, 257, 3209, 300, 576, 445, 1821, 264, 1154, 666, 819, 17408, 11, 309, 576, 312, 411, 257, 18161, 3209, 50968], "temperature": 0.0, "avg_logprob": -0.08923421212292593, "compression_ratio": 1.8527131782945736, "no_speech_prob": 0.0005272086127661169}, {"id": 697, "seek": 408088, "start": 4092.96, "end": 4098.16, "text": " that would operate at different patches. And for every patch of the image, I could be learning", "tokens": [50968, 300, 576, 9651, 412, 819, 26531, 13, 400, 337, 633, 9972, 295, 264, 3256, 11, 286, 727, 312, 2539, 51228], "temperature": 0.0, "avg_logprob": -0.08923421212292593, "compression_ratio": 1.8527131782945736, "no_speech_prob": 0.0005272086127661169}, {"id": 698, "seek": 408088, "start": 4098.16, "end": 4104.08, "text": " independent set of parameters, right? So that would be a model that is only told that it should be", "tokens": [51228, 6695, 992, 295, 9834, 11, 558, 30, 407, 300, 576, 312, 257, 2316, 300, 307, 787, 1907, 300, 309, 820, 312, 51524], "temperature": 0.0, "avg_logprob": -0.08923421212292593, "compression_ratio": 1.8527131782945736, "no_speech_prob": 0.0005272086127661169}, {"id": 699, "seek": 408088, "start": 4104.08, "end": 4108.16, "text": " breaking the problem into different regions. But it's not necessarily told that, you know,", "tokens": [51524, 7697, 264, 1154, 666, 819, 10682, 13, 583, 309, 311, 406, 4725, 1907, 300, 11, 291, 458, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08923421212292593, "compression_ratio": 1.8527131782945736, "no_speech_prob": 0.0005272086127661169}, {"id": 700, "seek": 410816, "start": 4108.16, "end": 4111.12, "text": " there's a weight sharing, right? There's some kind of like parameter sharing", "tokens": [50364, 456, 311, 257, 3364, 5414, 11, 558, 30, 821, 311, 512, 733, 295, 411, 13075, 5414, 50512], "temperature": 0.0, "avg_logprob": -0.1334482298956977, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007542538805864751}, {"id": 701, "seek": 410816, "start": 4111.12, "end": 4116.72, "text": " that somehow is, you are able that, you know, in a sense is helping you to learn with fewer", "tokens": [50512, 300, 6063, 307, 11, 291, 366, 1075, 300, 11, 291, 458, 11, 294, 257, 2020, 307, 4315, 291, 281, 1466, 365, 13366, 50792], "temperature": 0.0, "avg_logprob": -0.1334482298956977, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007542538805864751}, {"id": 702, "seek": 410816, "start": 4116.72, "end": 4122.48, "text": " number of parameters. So somehow these two, these two conditions are slightly complementary.", "tokens": [50792, 1230, 295, 9834, 13, 407, 6063, 613, 732, 11, 613, 732, 4487, 366, 4748, 40705, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1334482298956977, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007542538805864751}, {"id": 703, "seek": 410816, "start": 4123.28, "end": 4128.4, "text": " We, as I said, there's still like a lot of mathematical puzzles as to how these things", "tokens": [51120, 492, 11, 382, 286, 848, 11, 456, 311, 920, 411, 257, 688, 295, 18894, 24138, 382, 281, 577, 613, 721, 51376], "temperature": 0.0, "avg_logprob": -0.1334482298956977, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007542538805864751}, {"id": 704, "seek": 410816, "start": 4128.4, "end": 4137.2, "text": " interact optimally. And I think that the, the one of the reasons why we chose to explain the story", "tokens": [51376, 4648, 5028, 379, 13, 400, 286, 519, 300, 264, 11, 264, 472, 295, 264, 4112, 983, 321, 5111, 281, 2903, 264, 1657, 51816], "temperature": 0.0, "avg_logprob": -0.1334482298956977, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007542538805864751}, {"id": 705, "seek": 413720, "start": 4137.2, "end": 4143.28, "text": " into two different, in these two different priors is that they all survive this quest for", "tokens": [50364, 666, 732, 819, 11, 294, 613, 732, 819, 1790, 830, 307, 300, 436, 439, 7867, 341, 866, 337, 50668], "temperature": 0.0, "avg_logprob": -0.12098196744918824, "compression_ratio": 1.75, "no_speech_prob": 0.0009530293173156679}, {"id": 706, "seek": 413720, "start": 4143.28, "end": 4149.28, "text": " generality in the sense that these two principles are, again, something that you can think about", "tokens": [50668, 1337, 1860, 294, 264, 2020, 300, 613, 732, 9156, 366, 11, 797, 11, 746, 300, 291, 393, 519, 466, 50968], "temperature": 0.0, "avg_logprob": -0.12098196744918824, "compression_ratio": 1.75, "no_speech_prob": 0.0009530293173156679}, {"id": 707, "seek": 413720, "start": 4149.28, "end": 4158.4, "text": " for grids, for groups, for graphs, you can also see these principles appearing completely everywhere", "tokens": [50968, 337, 677, 3742, 11, 337, 3935, 11, 337, 24877, 11, 291, 393, 611, 536, 613, 9156, 19870, 2584, 5315, 51424], "temperature": 0.0, "avg_logprob": -0.12098196744918824, "compression_ratio": 1.75, "no_speech_prob": 0.0009530293173156679}, {"id": 708, "seek": 413720, "start": 4158.4, "end": 4162.32, "text": " as you study physical systems, right? Like the scale and the symmetry", "tokens": [51424, 382, 291, 2979, 4001, 3652, 11, 558, 30, 1743, 264, 4373, 293, 264, 25440, 51620], "temperature": 0.0, "avg_logprob": -0.12098196744918824, "compression_ratio": 1.75, "no_speech_prob": 0.0009530293173156679}, {"id": 709, "seek": 416232, "start": 4162.32, "end": 4169.04, "text": " is really at the core of, of many physical theories. And I would say that there's also", "tokens": [50364, 307, 534, 412, 264, 4965, 295, 11, 295, 867, 4001, 13667, 13, 400, 286, 576, 584, 300, 456, 311, 611, 50700], "temperature": 0.0, "avg_logprob": -0.13425252748572308, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.0009390507475472987}, {"id": 710, "seek": 416232, "start": 4171.679999999999, "end": 4179.5199999999995, "text": " at the more maybe technical level, these two priors somehow have been instrumental to", "tokens": [50832, 412, 264, 544, 1310, 6191, 1496, 11, 613, 732, 1790, 830, 6063, 362, 668, 17388, 281, 51224], "temperature": 0.0, "avg_logprob": -0.13425252748572308, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.0009390507475472987}, {"id": 711, "seek": 416232, "start": 4180.5599999999995, "end": 4185.5199999999995, "text": " organize, like to basically to have a kind of a recipe to build architectures, right? So,", "tokens": [51276, 13859, 11, 411, 281, 1936, 281, 362, 257, 733, 295, 257, 6782, 281, 1322, 6331, 1303, 11, 558, 30, 407, 11, 51524], "temperature": 0.0, "avg_logprob": -0.13425252748572308, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.0009390507475472987}, {"id": 712, "seek": 416232, "start": 4185.5199999999995, "end": 4191.599999999999, "text": " so maybe now we don't even think about it, right? But when you have a new problem, a new domain,", "tokens": [51524, 370, 1310, 586, 321, 500, 380, 754, 519, 466, 309, 11, 558, 30, 583, 562, 291, 362, 257, 777, 1154, 11, 257, 777, 9274, 11, 51828], "temperature": 0.0, "avg_logprob": -0.13425252748572308, "compression_ratio": 1.5814977973568283, "no_speech_prob": 0.0009390507475472987}, {"id": 713, "seek": 419160, "start": 4191.6, "end": 4197.120000000001, "text": " and you need to build an efficient neural network, we automatically have this idea that, okay,", "tokens": [50364, 293, 291, 643, 281, 1322, 364, 7148, 18161, 3209, 11, 321, 6772, 362, 341, 1558, 300, 11, 1392, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08364933227824274, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0002063882420770824}, {"id": 714, "seek": 419160, "start": 4197.120000000001, "end": 4201.76, "text": " we are going to start learning by composition, right? So we are going to extract information", "tokens": [50640, 321, 366, 516, 281, 722, 2539, 538, 12686, 11, 558, 30, 407, 321, 366, 516, 281, 8947, 1589, 50872], "temperature": 0.0, "avg_logprob": -0.08364933227824274, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0002063882420770824}, {"id": 715, "seek": 419160, "start": 4201.76, "end": 4206.64, "text": " one layer at a time. That's the first appearance of scale. And we know that the way we need to", "tokens": [50872, 472, 4583, 412, 257, 565, 13, 663, 311, 264, 700, 8967, 295, 4373, 13, 400, 321, 458, 300, 264, 636, 321, 643, 281, 51116], "temperature": 0.0, "avg_logprob": -0.08364933227824274, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0002063882420770824}, {"id": 716, "seek": 419160, "start": 4206.64, "end": 4211.52, "text": " organize these layers, right? How do you parameterize a layer that takes some input features and", "tokens": [51116, 13859, 613, 7914, 11, 558, 30, 1012, 360, 291, 13075, 1125, 257, 4583, 300, 2516, 512, 4846, 4122, 293, 51360], "temperature": 0.0, "avg_logprob": -0.08364933227824274, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0002063882420770824}, {"id": 717, "seek": 419160, "start": 4211.52, "end": 4217.52, "text": " produces maybe better features? This idea that we do that by understanding this kind of", "tokens": [51360, 14725, 1310, 1101, 4122, 30, 639, 1558, 300, 321, 360, 300, 538, 3701, 341, 733, 295, 51660], "temperature": 0.0, "avg_logprob": -0.08364933227824274, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0002063882420770824}, {"id": 718, "seek": 421752, "start": 4217.52, "end": 4223.52, "text": " equivarian structure, right? We have this notion of filters, right? In convolutional networks,", "tokens": [50364, 48726, 10652, 3877, 11, 558, 30, 492, 362, 341, 10710, 295, 15995, 11, 558, 30, 682, 45216, 304, 9590, 11, 50664], "temperature": 0.0, "avg_logprob": -0.18749146067768063, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.00166921631898731}, {"id": 719, "seek": 421752, "start": 4223.52, "end": 4228.160000000001, "text": " we have this, as I said, we organize everything in terms of filters. When we talk about message", "tokens": [50664, 321, 362, 341, 11, 382, 286, 848, 11, 321, 13859, 1203, 294, 2115, 295, 15995, 13, 1133, 321, 751, 466, 3636, 50896], "temperature": 0.0, "avg_logprob": -0.18749146067768063, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.00166921631898731}, {"id": 720, "seek": 421752, "start": 4228.72, "end": 4233.68, "text": " graph neural networks, we have this kind of like diffusion filters, right? And so there's this", "tokens": [50924, 4295, 18161, 9590, 11, 321, 362, 341, 733, 295, 411, 25242, 15995, 11, 558, 30, 400, 370, 456, 311, 341, 51172], "temperature": 0.0, "avg_logprob": -0.18749146067768063, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.00166921631898731}, {"id": 721, "seek": 421752, "start": 4233.68, "end": 4237.84, "text": " object that we extract from the domain that is helping us, that giving us something very", "tokens": [51172, 2657, 300, 321, 8947, 490, 264, 9274, 300, 307, 4315, 505, 11, 300, 2902, 505, 746, 588, 51380], "temperature": 0.0, "avg_logprob": -0.18749146067768063, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.00166921631898731}, {"id": 722, "seek": 421752, "start": 4237.84, "end": 4242.88, "text": " constructive, very, you know, very relevant, like very practical. And so this is really", "tokens": [51380, 30223, 11, 588, 11, 291, 458, 11, 588, 7340, 11, 411, 588, 8496, 13, 400, 370, 341, 307, 534, 51632], "temperature": 0.0, "avg_logprob": -0.18749146067768063, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.00166921631898731}, {"id": 723, "seek": 424288, "start": 4242.96, "end": 4248.64, "text": " the underlying group of transformations that is acting on our domain. And so I would say that,", "tokens": [50368, 264, 14217, 1594, 295, 34852, 300, 307, 6577, 322, 527, 9274, 13, 400, 370, 286, 576, 584, 300, 11, 50652], "temperature": 0.0, "avg_logprob": -0.1442395476407783, "compression_ratio": 1.65625, "no_speech_prob": 0.00164660788141191}, {"id": 724, "seek": 424288, "start": 4248.64, "end": 4254.72, "text": " you know, from a practitioner's perspective, these two principles, right, that I'm going to", "tokens": [50652, 291, 458, 11, 490, 257, 32125, 311, 4585, 11, 613, 732, 9156, 11, 558, 11, 300, 286, 478, 516, 281, 50956], "temperature": 0.0, "avg_logprob": -0.1442395476407783, "compression_ratio": 1.65625, "no_speech_prob": 0.00164660788141191}, {"id": 725, "seek": 424288, "start": 4254.72, "end": 4261.4400000000005, "text": " learn by composing some fundamental layers that I repeat all the time. And the way this layer", "tokens": [50956, 1466, 538, 715, 6110, 512, 8088, 7914, 300, 286, 7149, 439, 264, 565, 13, 400, 264, 636, 341, 4583, 51292], "temperature": 0.0, "avg_logprob": -0.1442395476407783, "compression_ratio": 1.65625, "no_speech_prob": 0.00164660788141191}, {"id": 726, "seek": 424288, "start": 4261.4400000000005, "end": 4266.96, "text": " is organized, is structured through this group transformation, this has been, I would say,", "tokens": [51292, 307, 9983, 11, 307, 18519, 807, 341, 1594, 9887, 11, 341, 575, 668, 11, 286, 576, 584, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1442395476407783, "compression_ratio": 1.65625, "no_speech_prob": 0.00164660788141191}, {"id": 727, "seek": 426696, "start": 4267.44, "end": 4273.36, "text": " like a trademark of, you know, the success of neural networks. Of course, as also we mentioned", "tokens": [50388, 411, 257, 31361, 295, 11, 291, 458, 11, 264, 2245, 295, 18161, 9590, 13, 2720, 1164, 11, 382, 611, 321, 2835, 50684], "temperature": 0.0, "avg_logprob": -0.11755548342309817, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005894116126000881}, {"id": 728, "seek": 426696, "start": 4273.36, "end": 4282.24, "text": " in the book, these are, I would say, proto, like meta, you know, meta parameterization, right,", "tokens": [50684, 294, 264, 1446, 11, 613, 366, 11, 286, 576, 584, 11, 47896, 11, 411, 19616, 11, 291, 458, 11, 19616, 13075, 2144, 11, 558, 11, 51128], "temperature": 0.0, "avg_logprob": -0.11755548342309817, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005894116126000881}, {"id": 729, "seek": 426696, "start": 4282.24, "end": 4287.52, "text": " in the sense that there's, as you know, many, many, many variants that people have come up with.", "tokens": [51128, 294, 264, 2020, 300, 456, 311, 11, 382, 291, 458, 11, 867, 11, 867, 11, 867, 21669, 300, 561, 362, 808, 493, 365, 13, 51392], "temperature": 0.0, "avg_logprob": -0.11755548342309817, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005894116126000881}, {"id": 730, "seek": 426696, "start": 4287.52, "end": 4292.96, "text": " Many, many, let's say, yeah, like modifications on the basic architecture that have really", "tokens": [51392, 5126, 11, 867, 11, 718, 311, 584, 11, 1338, 11, 411, 26881, 322, 264, 3875, 9482, 300, 362, 534, 51664], "temperature": 0.0, "avg_logprob": -0.11755548342309817, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005894116126000881}, {"id": 731, "seek": 429296, "start": 4292.96, "end": 4298.56, "text": " make dramatic changes in performance, right. So there's, of course, as I say, like the devil", "tokens": [50364, 652, 12023, 2962, 294, 3389, 11, 558, 13, 407, 456, 311, 11, 295, 1164, 11, 382, 286, 584, 11, 411, 264, 13297, 50644], "temperature": 0.0, "avg_logprob": -0.13674025492625194, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0013449557591229677}, {"id": 732, "seek": 429296, "start": 4298.56, "end": 4304.4800000000005, "text": " sometimes is in the details, right. And so as we are writing the book, we are realizing exactly,", "tokens": [50644, 2171, 307, 294, 264, 4365, 11, 558, 13, 400, 370, 382, 321, 366, 3579, 264, 1446, 11, 321, 366, 16734, 2293, 11, 50940], "temperature": 0.0, "avg_logprob": -0.13674025492625194, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0013449557591229677}, {"id": 733, "seek": 429296, "start": 4304.4800000000005, "end": 4309.12, "text": " you know, how some of the changes in the architectures are actually fit into this,", "tokens": [50940, 291, 458, 11, 577, 512, 295, 264, 2962, 294, 264, 6331, 1303, 366, 767, 3318, 666, 341, 11, 51172], "temperature": 0.0, "avg_logprob": -0.13674025492625194, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0013449557591229677}, {"id": 734, "seek": 429296, "start": 4309.12, "end": 4311.76, "text": " what we call this blueprint, right, this symmetrically learning blueprint.", "tokens": [51172, 437, 321, 818, 341, 35868, 11, 558, 11, 341, 14232, 27965, 984, 2539, 35868, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13674025492625194, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0013449557591229677}, {"id": 735, "seek": 429296, "start": 4312.64, "end": 4317.2, "text": " Fascinating. Okay, so I wanted to come back to what you were saying a few minutes ago about the", "tokens": [51348, 49098, 8205, 13, 1033, 11, 370, 286, 1415, 281, 808, 646, 281, 437, 291, 645, 1566, 257, 1326, 2077, 2057, 466, 264, 51576], "temperature": 0.0, "avg_logprob": -0.13674025492625194, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0013449557591229677}, {"id": 736, "seek": 431720, "start": 4318.08, "end": 4323.5199999999995, "text": " the sample efficiency of these models. Now, with graph neural networks, for example,", "tokens": [50408, 264, 6889, 10493, 295, 613, 5245, 13, 823, 11, 365, 4295, 18161, 9590, 11, 337, 1365, 11, 50680], "temperature": 0.0, "avg_logprob": -0.14583492279052734, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.07157180458307266}, {"id": 737, "seek": 431720, "start": 4323.5199999999995, "end": 4328.88, "text": " there are factorially many permutations of adjacency matrices for a given graph. And", "tokens": [50680, 456, 366, 5952, 2270, 867, 4784, 325, 763, 295, 22940, 3020, 32284, 337, 257, 2212, 4295, 13, 400, 50948], "temperature": 0.0, "avg_logprob": -0.14583492279052734, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.07157180458307266}, {"id": 738, "seek": 431720, "start": 4329.76, "end": 4334.8, "text": " I want to talk about a sorting algorithm, right. So Fran\u00e7ois Chollet came on the podcast and he said", "tokens": [50992, 286, 528, 281, 751, 466, 257, 32411, 9284, 11, 558, 13, 407, 1526, 12368, 7376, 761, 1833, 302, 1361, 322, 264, 7367, 293, 415, 848, 51244], "temperature": 0.0, "avg_logprob": -0.14583492279052734, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.07157180458307266}, {"id": 739, "seek": 431720, "start": 4334.8, "end": 4339.5199999999995, "text": " that in order to learn a sorting algorithm that generalizes, you would need to learn point by", "tokens": [51244, 300, 294, 1668, 281, 1466, 257, 32411, 9284, 300, 2674, 5660, 11, 291, 576, 643, 281, 1466, 935, 538, 51480], "temperature": 0.0, "avg_logprob": -0.14583492279052734, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.07157180458307266}, {"id": 740, "seek": 431720, "start": 4339.5199999999995, "end": 4346.24, "text": " point, you would need to see factorially many examples of permutations of numbers.", "tokens": [51480, 935, 11, 291, 576, 643, 281, 536, 5952, 2270, 867, 5110, 295, 4784, 325, 763, 295, 3547, 13, 51816], "temperature": 0.0, "avg_logprob": -0.14583492279052734, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.07157180458307266}, {"id": 741, "seek": 434624, "start": 4347.04, "end": 4352.32, "text": " Do you think that we could train a neural network to learn a sorting? I guess what I'm", "tokens": [50404, 1144, 291, 519, 300, 321, 727, 3847, 257, 18161, 3209, 281, 1466, 257, 32411, 30, 286, 2041, 437, 286, 478, 50668], "temperature": 0.0, "avg_logprob": -0.10079258680343628, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.001619985094293952}, {"id": 742, "seek": 434624, "start": 4352.32, "end": 4356.719999999999, "text": " asking in a roundabout way is, do you think there's a kind of geometry to computation itself?", "tokens": [50668, 3365, 294, 257, 3098, 21970, 636, 307, 11, 360, 291, 519, 456, 311, 257, 733, 295, 18426, 281, 24903, 2564, 30, 50888], "temperature": 0.0, "avg_logprob": -0.10079258680343628, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.001619985094293952}, {"id": 743, "seek": 434624, "start": 4358.48, "end": 4364.5599999999995, "text": " Good. That's a very good question. And in fact, we have some, some recent work with some collaborators", "tokens": [50976, 2205, 13, 663, 311, 257, 588, 665, 1168, 13, 400, 294, 1186, 11, 321, 362, 512, 11, 512, 5162, 589, 365, 512, 39789, 51280], "temperature": 0.0, "avg_logprob": -0.10079258680343628, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.001619985094293952}, {"id": 744, "seek": 434624, "start": 4364.5599999999995, "end": 4370.4, "text": " in my group, where we kind of take up this question from and we try to formalize it mathematically,", "tokens": [51280, 294, 452, 1594, 11, 689, 321, 733, 295, 747, 493, 341, 1168, 490, 293, 321, 853, 281, 9860, 1125, 309, 44003, 11, 51572], "temperature": 0.0, "avg_logprob": -0.10079258680343628, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.001619985094293952}, {"id": 745, "seek": 437040, "start": 4370.48, "end": 4376.879999999999, "text": " and we give answers to this question. And so many of these like a computational tasks that", "tokens": [50368, 293, 321, 976, 6338, 281, 341, 1168, 13, 400, 370, 867, 295, 613, 411, 257, 28270, 9608, 300, 50688], "temperature": 0.0, "avg_logprob": -0.11964589032259855, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0004171821638010442}, {"id": 746, "seek": 437040, "start": 4376.879999999999, "end": 4383.28, "text": " you were mentioning, for example, sorting or, you know, like algorithmic tasks, they are,", "tokens": [50688, 291, 645, 18315, 11, 337, 1365, 11, 32411, 420, 11, 291, 458, 11, 411, 9284, 299, 9608, 11, 436, 366, 11, 51008], "temperature": 0.0, "avg_logprob": -0.11964589032259855, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0004171821638010442}, {"id": 747, "seek": 437040, "start": 4383.92, "end": 4388.799999999999, "text": " if one wants to put them into some mathematical context, the first thing that comes to mind is", "tokens": [51040, 498, 472, 2738, 281, 829, 552, 666, 512, 18894, 4319, 11, 264, 700, 551, 300, 1487, 281, 1575, 307, 51284], "temperature": 0.0, "avg_logprob": -0.11964589032259855, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0004171821638010442}, {"id": 748, "seek": 437040, "start": 4388.799999999999, "end": 4393.44, "text": " these are functions that already enjoy some symmetries. For example, like a sorting algorithm,", "tokens": [51284, 613, 366, 6828, 300, 1217, 2103, 512, 14232, 302, 2244, 13, 1171, 1365, 11, 411, 257, 32411, 9284, 11, 51516], "temperature": 0.0, "avg_logprob": -0.11964589032259855, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0004171821638010442}, {"id": 749, "seek": 437040, "start": 4393.44, "end": 4398.4, "text": " right, is invariant to permutations, right. So the function that you are trying to learn", "tokens": [51516, 558, 11, 307, 33270, 394, 281, 4784, 325, 763, 11, 558, 13, 407, 264, 2445, 300, 291, 366, 1382, 281, 1466, 51764], "temperature": 0.0, "avg_logprob": -0.11964589032259855, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0004171821638010442}, {"id": 750, "seek": 439840, "start": 4398.4, "end": 4404.48, "text": " is an arbitrary function that has this symmetric class. And so as such, as I was saying in the", "tokens": [50364, 307, 364, 23211, 2445, 300, 575, 341, 32330, 1508, 13, 400, 370, 382, 1270, 11, 382, 286, 390, 1566, 294, 264, 50668], "temperature": 0.0, "avg_logprob": -0.12164638819319479, "compression_ratio": 1.7546296296296295, "no_speech_prob": 6.106945511419326e-05}, {"id": 751, "seek": 439840, "start": 4404.48, "end": 4412.16, "text": " beginning, you can try to address this question saying, okay, now you give me an arbitrary,", "tokens": [50668, 2863, 11, 291, 393, 853, 281, 2985, 341, 1168, 1566, 11, 1392, 11, 586, 291, 976, 385, 364, 23211, 11, 51052], "temperature": 0.0, "avg_logprob": -0.12164638819319479, "compression_ratio": 1.7546296296296295, "no_speech_prob": 6.106945511419326e-05}, {"id": 752, "seek": 439840, "start": 4412.16, "end": 4418.4, "text": " so the question would be relative to an arbitrary learning learner that is agnostic to symmetry,", "tokens": [51052, 370, 264, 1168, 576, 312, 4972, 281, 364, 23211, 2539, 33347, 300, 307, 623, 77, 19634, 281, 25440, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12164638819319479, "compression_ratio": 1.7546296296296295, "no_speech_prob": 6.106945511419326e-05}, {"id": 753, "seek": 439840, "start": 4418.4, "end": 4424.32, "text": " how much does a symmetric learner gain, right? So you can basically try to understand, quantify", "tokens": [51364, 577, 709, 775, 257, 32330, 33347, 6052, 11, 558, 30, 407, 291, 393, 1936, 853, 281, 1223, 11, 40421, 51660], "temperature": 0.0, "avg_logprob": -0.12164638819319479, "compression_ratio": 1.7546296296296295, "no_speech_prob": 6.106945511419326e-05}, {"id": 754, "seek": 442432, "start": 4424.32, "end": 4429.679999999999, "text": " the gains of sample complexity of learning without symmetry versus learning with symmetry.", "tokens": [50364, 264, 16823, 295, 6889, 14024, 295, 2539, 1553, 25440, 5717, 2539, 365, 25440, 13, 50632], "temperature": 0.0, "avg_logprob": -0.12094146555120294, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.00036208401434123516}, {"id": 755, "seek": 442432, "start": 4430.4, "end": 4437.84, "text": " And so the punchline of this work, the recent work that we completed, is that one can actually", "tokens": [50668, 400, 370, 264, 8135, 1889, 295, 341, 589, 11, 264, 5162, 589, 300, 321, 7365, 11, 307, 300, 472, 393, 767, 51040], "temperature": 0.0, "avg_logprob": -0.12094146555120294, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.00036208401434123516}, {"id": 756, "seek": 442432, "start": 4437.84, "end": 4443.679999999999, "text": " quantify the sample complexity gains, and these are of the order of the size of the group. And so", "tokens": [51040, 40421, 264, 6889, 14024, 16823, 11, 293, 613, 366, 295, 264, 1668, 295, 264, 2744, 295, 264, 1594, 13, 400, 370, 51332], "temperature": 0.0, "avg_logprob": -0.12094146555120294, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.00036208401434123516}, {"id": 757, "seek": 442432, "start": 4444.32, "end": 4449.28, "text": " here in the case of like permutations, what it means is that if a learner is aware of this", "tokens": [51364, 510, 294, 264, 1389, 295, 411, 4784, 325, 763, 11, 437, 309, 1355, 307, 300, 498, 257, 33347, 307, 3650, 295, 341, 51612], "temperature": 0.0, "avg_logprob": -0.12094146555120294, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.00036208401434123516}, {"id": 758, "seek": 444928, "start": 4449.28, "end": 4456.0, "text": " symmetry, like one training example of the symmetric learner is rosary equivalent to", "tokens": [50364, 25440, 11, 411, 472, 3097, 1365, 295, 264, 32330, 33347, 307, 18953, 822, 10344, 281, 50700], "temperature": 0.0, "avg_logprob": -0.13533664371656334, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0019863341003656387}, {"id": 759, "seek": 444928, "start": 4456.639999999999, "end": 4461.36, "text": " n factorial samples of the agnostic learner, right, which is something that you would expect,", "tokens": [50732, 297, 36916, 10938, 295, 264, 623, 77, 19634, 33347, 11, 558, 11, 597, 307, 746, 300, 291, 576, 2066, 11, 50968], "temperature": 0.0, "avg_logprob": -0.13533664371656334, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0019863341003656387}, {"id": 760, "seek": 444928, "start": 4461.36, "end": 4467.84, "text": " like if you think in terms of data mutation, right, like if I tell you in advance that your", "tokens": [50968, 411, 498, 291, 519, 294, 2115, 295, 1412, 27960, 11, 558, 11, 411, 498, 286, 980, 291, 294, 7295, 300, 428, 51292], "temperature": 0.0, "avg_logprob": -0.13533664371656334, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0019863341003656387}, {"id": 761, "seek": 444928, "start": 4467.84, "end": 4472.719999999999, "text": " function is symmetric, is invariant to permutations, it's, you know, like a brute force approach would", "tokens": [51292, 2445, 307, 32330, 11, 307, 33270, 394, 281, 4784, 325, 763, 11, 309, 311, 11, 291, 458, 11, 411, 257, 47909, 3464, 3109, 576, 51536], "temperature": 0.0, "avg_logprob": -0.13533664371656334, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0019863341003656387}, {"id": 762, "seek": 444928, "start": 4472.719999999999, "end": 4476.639999999999, "text": " say, okay, you give me an instance training an input, right, and instead of giving you this input,", "tokens": [51536, 584, 11, 1392, 11, 291, 976, 385, 364, 5197, 3097, 364, 4846, 11, 558, 11, 293, 2602, 295, 2902, 291, 341, 4846, 11, 51732], "temperature": 0.0, "avg_logprob": -0.13533664371656334, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0019863341003656387}, {"id": 763, "seek": 447664, "start": 4476.64, "end": 4481.84, "text": " I'm just going to, you know, like permute, like have any possible permutation of the input, and", "tokens": [50364, 286, 478, 445, 516, 281, 11, 291, 458, 11, 411, 4784, 1169, 11, 411, 362, 604, 1944, 4784, 11380, 295, 264, 4846, 11, 293, 50624], "temperature": 0.0, "avg_logprob": -0.13480539321899415, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.00041632045758888125}, {"id": 764, "seek": 447664, "start": 4481.84, "end": 4486.400000000001, "text": " you already know the output, right? So you can as well feed it to the learner. So this is like", "tokens": [50624, 291, 1217, 458, 264, 5598, 11, 558, 30, 407, 291, 393, 382, 731, 3154, 309, 281, 264, 33347, 13, 407, 341, 307, 411, 50852], "temperature": 0.0, "avg_logprob": -0.13480539321899415, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.00041632045758888125}, {"id": 765, "seek": 447664, "start": 4486.400000000001, "end": 4490.88, "text": " horrific at this addition turns out to be mathematically correct, precise, at least, you", "tokens": [50852, 29248, 412, 341, 4500, 4523, 484, 281, 312, 44003, 3006, 11, 13600, 11, 412, 1935, 11, 291, 51076], "temperature": 0.0, "avg_logprob": -0.13480539321899415, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.00041632045758888125}, {"id": 766, "seek": 447664, "start": 4490.88, "end": 4497.52, "text": " know, under some conditions, right? But the, I guess the whole point is that these gains might", "tokens": [51076, 458, 11, 833, 512, 4487, 11, 558, 30, 583, 264, 11, 286, 2041, 264, 1379, 935, 307, 300, 613, 16823, 1062, 51408], "temperature": 0.0, "avg_logprob": -0.13480539321899415, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.00041632045758888125}, {"id": 767, "seek": 447664, "start": 4497.52, "end": 4503.280000000001, "text": " look amazing, like might look like a, you know, like a big boost in sample complexity. As I said", "tokens": [51408, 574, 2243, 11, 411, 1062, 574, 411, 257, 11, 291, 458, 11, 411, 257, 955, 9194, 294, 6889, 14024, 13, 1018, 286, 848, 51696], "temperature": 0.0, "avg_logprob": -0.13480539321899415, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.00041632045758888125}, {"id": 768, "seek": 450328, "start": 4503.28, "end": 4509.12, "text": " before, there's a grain of salt here is that the, in these conditions, in general, general", "tokens": [50364, 949, 11, 456, 311, 257, 12837, 295, 5139, 510, 307, 300, 264, 11, 294, 613, 4487, 11, 294, 2674, 11, 2674, 50656], "temperature": 0.0, "avg_logprob": -0.16759900879441647, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0006153729627840221}, {"id": 769, "seek": 450328, "start": 4509.12, "end": 4514.32, "text": " conditions, you are already fighting an essential and impossible problem in the sense that the rate,", "tokens": [50656, 4487, 11, 291, 366, 1217, 5237, 364, 7115, 293, 6243, 1154, 294, 264, 2020, 300, 264, 3314, 11, 50916], "temperature": 0.0, "avg_logprob": -0.16759900879441647, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0006153729627840221}, {"id": 770, "seek": 450328, "start": 4514.32, "end": 4520.08, "text": " like the sample complexity is dominated by a rate of basically the rate in which you learn", "tokens": [50916, 411, 264, 6889, 14024, 307, 23755, 538, 257, 3314, 295, 1936, 264, 3314, 294, 597, 291, 1466, 51204], "temperature": 0.0, "avg_logprob": -0.16759900879441647, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0006153729627840221}, {"id": 771, "seek": 450328, "start": 4520.08, "end": 4525.28, "text": " is what we call course by dimension. And what it means is that if I want to, you know, I have a", "tokens": [51204, 307, 437, 321, 818, 1164, 538, 10139, 13, 400, 437, 309, 1355, 307, 300, 498, 286, 528, 281, 11, 291, 458, 11, 286, 362, 257, 51464], "temperature": 0.0, "avg_logprob": -0.16759900879441647, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0006153729627840221}, {"id": 772, "seek": 450328, "start": 4525.28, "end": 4530.5599999999995, "text": " certain performance generalization error, I'm going to say that now I want to ask you the question,", "tokens": [51464, 1629, 3389, 2674, 2144, 6713, 11, 286, 478, 516, 281, 584, 300, 586, 286, 528, 281, 1029, 291, 264, 1168, 11, 51728], "temperature": 0.0, "avg_logprob": -0.16759900879441647, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0006153729627840221}, {"id": 773, "seek": 453056, "start": 4530.56, "end": 4536.160000000001, "text": " if I want to divide this generalization error by two, how many more samples do I need to give you?", "tokens": [50364, 498, 286, 528, 281, 9845, 341, 2674, 2144, 6713, 538, 732, 11, 577, 867, 544, 10938, 360, 286, 643, 281, 976, 291, 30, 50644], "temperature": 0.0, "avg_logprob": -0.14729803699558064, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0001822666818043217}, {"id": 774, "seek": 453056, "start": 4536.160000000001, "end": 4542.080000000001, "text": " Right? Like what? So if I want to double, you know, like double, like reduce the error by half,", "tokens": [50644, 1779, 30, 1743, 437, 30, 407, 498, 286, 528, 281, 3834, 11, 291, 458, 11, 411, 3834, 11, 411, 5407, 264, 6713, 538, 1922, 11, 50940], "temperature": 0.0, "avg_logprob": -0.14729803699558064, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0001822666818043217}, {"id": 775, "seek": 453056, "start": 4543.120000000001, "end": 4547.76, "text": " by how much do I need to give you more samples? So this dependency, in fact, is exponential in", "tokens": [50992, 538, 577, 709, 360, 286, 643, 281, 976, 291, 544, 10938, 30, 407, 341, 33621, 11, 294, 1186, 11, 307, 21510, 294, 51224], "temperature": 0.0, "avg_logprob": -0.14729803699558064, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0001822666818043217}, {"id": 776, "seek": 453056, "start": 4547.76, "end": 4553.280000000001, "text": " dimension, right? So basically the, the, the sample complexity gains by invariance, they are", "tokens": [51224, 10139, 11, 558, 30, 407, 1936, 264, 11, 264, 11, 264, 6889, 14024, 16823, 538, 33270, 719, 11, 436, 366, 51500], "temperature": 0.0, "avg_logprob": -0.14729803699558064, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0001822666818043217}, {"id": 777, "seek": 453056, "start": 4553.280000000001, "end": 4557.84, "text": " exponential in dimension, but they are fighting an impossible problem that is already caused by", "tokens": [51500, 21510, 294, 10139, 11, 457, 436, 366, 5237, 364, 6243, 1154, 300, 307, 1217, 7008, 538, 51728], "temperature": 0.0, "avg_logprob": -0.14729803699558064, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0001822666818043217}, {"id": 778, "seek": 455784, "start": 4557.84, "end": 4565.12, "text": " dimension. So what it means is that at the end of the day, this is what, you know, what was in the,", "tokens": [50364, 10139, 13, 407, 437, 309, 1355, 307, 300, 412, 264, 917, 295, 264, 786, 11, 341, 307, 437, 11, 291, 458, 11, 437, 390, 294, 264, 11, 50728], "temperature": 0.0, "avg_logprob": -0.12050219187660823, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0027550230734050274}, {"id": 779, "seek": 455784, "start": 4565.12, "end": 4571.28, "text": " in the, like in the heart of what I was saying before, is that invariance alone might not be", "tokens": [50728, 294, 264, 11, 411, 294, 264, 1917, 295, 437, 286, 390, 1566, 949, 11, 307, 300, 33270, 719, 3312, 1062, 406, 312, 51036], "temperature": 0.0, "avg_logprob": -0.12050219187660823, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0027550230734050274}, {"id": 780, "seek": 455784, "start": 4571.28, "end": 4575.92, "text": " efficient, might not be sufficient, right? Because you are, okay, you are taking a very hard problem,", "tokens": [51036, 7148, 11, 1062, 406, 312, 11563, 11, 558, 30, 1436, 291, 366, 11, 1392, 11, 291, 366, 1940, 257, 588, 1152, 1154, 11, 51268], "temperature": 0.0, "avg_logprob": -0.12050219187660823, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0027550230734050274}, {"id": 781, "seek": 455784, "start": 4575.92, "end": 4580.56, "text": " you are removing an exponential factor, but you still have many, you know, you have still", "tokens": [51268, 291, 366, 12720, 364, 21510, 5952, 11, 457, 291, 920, 362, 867, 11, 291, 458, 11, 291, 362, 920, 51500], "temperature": 0.0, "avg_logprob": -0.12050219187660823, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0027550230734050274}, {"id": 782, "seek": 455784, "start": 4580.56, "end": 4586.32, "text": " have something in the exponent that is exponential, right? So, so, so what it means is that that,", "tokens": [51500, 362, 746, 294, 264, 37871, 300, 307, 21510, 11, 558, 30, 407, 11, 370, 11, 370, 437, 309, 1355, 307, 300, 300, 11, 51788], "temperature": 0.0, "avg_logprob": -0.12050219187660823, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0027550230734050274}, {"id": 783, "seek": 458632, "start": 4586.32, "end": 4593.28, "text": " I mean, that's what really underpins why we think in these terms of combining symmetry prior", "tokens": [50364, 286, 914, 11, 300, 311, 437, 534, 833, 79, 1292, 983, 321, 519, 294, 613, 2115, 295, 21928, 25440, 4059, 50712], "temperature": 0.0, "avg_logprob": -0.12073978217872414, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0010574929183349013}, {"id": 784, "seek": 458632, "start": 4593.28, "end": 4598.719999999999, "text": " with the scale separation prior. But certainly the algorithmic tasks are very interesting playground", "tokens": [50712, 365, 264, 4373, 14634, 4059, 13, 583, 3297, 264, 9284, 299, 9608, 366, 588, 1880, 24646, 50984], "temperature": 0.0, "avg_logprob": -0.12073978217872414, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0010574929183349013}, {"id": 785, "seek": 458632, "start": 4598.719999999999, "end": 4605.44, "text": " because I think that for the case of sorting, I mean, as you know, scale separation is also an", "tokens": [50984, 570, 286, 519, 300, 337, 264, 1389, 295, 32411, 11, 286, 914, 11, 382, 291, 458, 11, 4373, 14634, 307, 611, 364, 51320], "temperature": 0.0, "avg_logprob": -0.12073978217872414, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0010574929183349013}, {"id": 786, "seek": 458632, "start": 4605.44, "end": 4609.92, "text": " issue. It's also a very important thing, right? I mean, it, this is what basically is at the heart", "tokens": [51320, 2734, 13, 467, 311, 611, 257, 588, 1021, 551, 11, 558, 30, 286, 914, 11, 309, 11, 341, 307, 437, 1936, 307, 412, 264, 1917, 51544], "temperature": 0.0, "avg_logprob": -0.12073978217872414, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0010574929183349013}, {"id": 787, "seek": 458632, "start": 4609.92, "end": 4614.48, "text": " of the dynamic programming approaches, right? Like these efficient algorithms that are not only", "tokens": [51544, 295, 264, 8546, 9410, 11587, 11, 558, 30, 1743, 613, 7148, 14642, 300, 366, 406, 787, 51772], "temperature": 0.0, "avg_logprob": -0.12073978217872414, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0010574929183349013}, {"id": 788, "seek": 461448, "start": 4614.48, "end": 4618.639999999999, "text": " officially statistically, but also officially computation, right? This idea that you can", "tokens": [50364, 12053, 36478, 11, 457, 611, 12053, 24903, 11, 558, 30, 639, 1558, 300, 291, 393, 50572], "temperature": 0.0, "avg_logprob": -0.10976946031725085, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0007176143117249012}, {"id": 789, "seek": 461448, "start": 4618.639999999999, "end": 4625.919999999999, "text": " divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual", "tokens": [50572, 9845, 293, 24136, 13, 407, 11, 370, 11, 370, 11, 370, 9284, 299, 9608, 611, 366, 733, 295, 9495, 281, 341, 11848, 50936], "temperature": 0.0, "avg_logprob": -0.10976946031725085, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0007176143117249012}, {"id": 790, "seek": 461448, "start": 4625.919999999999, "end": 4630.959999999999, "text": " physical prior, right? Of a scale and invariance. On the course of dimensionality, there's this", "tokens": [50936, 4001, 4059, 11, 558, 30, 2720, 257, 4373, 293, 33270, 719, 13, 1282, 264, 1164, 295, 10139, 1860, 11, 456, 311, 341, 51188], "temperature": 0.0, "avg_logprob": -0.10976946031725085, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0007176143117249012}, {"id": 791, "seek": 461448, "start": 4630.959999999999, "end": 4636.4, "text": " issue where you have a data point and you want to surround it by other data points in two dimensions", "tokens": [51188, 2734, 689, 291, 362, 257, 1412, 935, 293, 291, 528, 281, 6262, 309, 538, 661, 1412, 2793, 294, 732, 12819, 51460], "temperature": 0.0, "avg_logprob": -0.10976946031725085, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0007176143117249012}, {"id": 792, "seek": 461448, "start": 4636.4, "end": 4642.48, "text": " to create a convex hole. And as you increase the number of dimensions, the number of data points", "tokens": [51460, 281, 1884, 257, 42432, 5458, 13, 400, 382, 291, 3488, 264, 1230, 295, 12819, 11, 264, 1230, 295, 1412, 2793, 51764], "temperature": 0.0, "avg_logprob": -0.10976946031725085, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0007176143117249012}, {"id": 793, "seek": 464248, "start": 4642.48, "end": 4649.12, "text": " you need to create this covering to create a kind of interpolative space increases exponentially.", "tokens": [50364, 291, 643, 281, 1884, 341, 10322, 281, 1884, 257, 733, 295, 44902, 1166, 1901, 8637, 37330, 13, 50696], "temperature": 0.0, "avg_logprob": -0.06491477534456074, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.002062832238152623}, {"id": 794, "seek": 464248, "start": 4649.12, "end": 4653.839999999999, "text": " And when you get past a certain number of dimensions, let's say 16 or not, not very many,", "tokens": [50696, 400, 562, 291, 483, 1791, 257, 1629, 1230, 295, 12819, 11, 718, 311, 584, 3165, 420, 406, 11, 406, 588, 867, 11, 50932], "temperature": 0.0, "avg_logprob": -0.06491477534456074, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.002062832238152623}, {"id": 795, "seek": 464248, "start": 4653.839999999999, "end": 4659.2, "text": " you would need essentially more data points than there are atoms in the universe. So this leads", "tokens": [50932, 291, 576, 643, 4476, 544, 1412, 2793, 813, 456, 366, 16871, 294, 264, 6445, 13, 407, 341, 6689, 51200], "temperature": 0.0, "avg_logprob": -0.06491477534456074, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.002062832238152623}, {"id": 796, "seek": 464248, "start": 4659.2, "end": 4664.719999999999, "text": " to a very interesting realization. I think some people refer to it as the manifold hypothesis,", "tokens": [51200, 281, 257, 588, 1880, 25138, 13, 286, 519, 512, 561, 2864, 281, 309, 382, 264, 47138, 17291, 11, 51476], "temperature": 0.0, "avg_logprob": -0.06491477534456074, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.002062832238152623}, {"id": 797, "seek": 464248, "start": 4664.719999999999, "end": 4670.879999999999, "text": " which is that most natural data is only really spatially novel on very few dimensions. And a", "tokens": [51476, 597, 307, 300, 881, 3303, 1412, 307, 787, 534, 15000, 2270, 7613, 322, 588, 1326, 12819, 13, 400, 257, 51784], "temperature": 0.0, "avg_logprob": -0.06491477534456074, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.002062832238152623}, {"id": 798, "seek": 467088, "start": 4670.88, "end": 4677.36, "text": " lot of data falls on very smooth low dimensional manifolds. But what are the implications of this?", "tokens": [50364, 688, 295, 1412, 8804, 322, 588, 5508, 2295, 18795, 8173, 31518, 13, 583, 437, 366, 264, 16602, 295, 341, 30, 50688], "temperature": 0.0, "avg_logprob": -0.08779267431462853, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0007097909692674875}, {"id": 799, "seek": 467088, "start": 4678.400000000001, "end": 4683.12, "text": " Essentially, all machine learning problems that we need to deal nowadays are extremely high", "tokens": [50740, 23596, 11, 439, 3479, 2539, 2740, 300, 321, 643, 281, 2028, 13434, 366, 4664, 1090, 50976], "temperature": 0.0, "avg_logprob": -0.08779267431462853, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0007097909692674875}, {"id": 800, "seek": 467088, "start": 4683.12, "end": 4688.56, "text": " dimensional. So even if we take very modestly sized images, they live in thousands or even in", "tokens": [50976, 18795, 13, 407, 754, 498, 321, 747, 588, 1072, 11154, 20004, 5267, 11, 436, 1621, 294, 5383, 420, 754, 294, 51248], "temperature": 0.0, "avg_logprob": -0.08779267431462853, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0007097909692674875}, {"id": 801, "seek": 467088, "start": 4688.56, "end": 4694.08, "text": " millions of dimensions. And if you think of machine learning or at least the simplest setting of", "tokens": [51248, 6803, 295, 12819, 13, 400, 498, 291, 519, 295, 3479, 2539, 420, 412, 1935, 264, 22811, 3287, 295, 51524], "temperature": 0.0, "avg_logprob": -0.08779267431462853, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0007097909692674875}, {"id": 802, "seek": 467088, "start": 4694.08, "end": 4699.68, "text": " machine learning as a kind of glorified function interpolation, the standard approach is to function", "tokens": [51524, 3479, 2539, 382, 257, 733, 295, 26623, 2587, 2445, 44902, 399, 11, 264, 3832, 3109, 307, 281, 2445, 51804], "temperature": 0.0, "avg_logprob": -0.08779267431462853, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0007097909692674875}, {"id": 803, "seek": 469968, "start": 4699.68, "end": 4706.400000000001, "text": " interpolation as just use the data points to predict the values of your function will simply", "tokens": [50364, 44902, 399, 382, 445, 764, 264, 1412, 2793, 281, 6069, 264, 4190, 295, 428, 2445, 486, 2935, 50700], "temperature": 0.0, "avg_logprob": -0.09252636773245675, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0028746684547513723}, {"id": 804, "seek": 469968, "start": 4706.400000000001, "end": 4710.88, "text": " not work because of the phenomenon of cursive, the recursive dimensionality that increasing", "tokens": [50700, 406, 589, 570, 295, 264, 14029, 295, 13946, 488, 11, 264, 20560, 488, 10139, 1860, 300, 5662, 50924], "temperature": 0.0, "avg_logprob": -0.09252636773245675, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0028746684547513723}, {"id": 805, "seek": 469968, "start": 4710.88, "end": 4716.16, "text": " the number of dimensions, the number of such points blows up exponentially. So what you really", "tokens": [50924, 264, 1230, 295, 12819, 11, 264, 1230, 295, 1270, 2793, 18458, 493, 37330, 13, 407, 437, 291, 534, 51188], "temperature": 0.0, "avg_logprob": -0.09252636773245675, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0028746684547513723}, {"id": 806, "seek": 469968, "start": 4716.16, "end": 4722.8, "text": " need to take into account and probably this is really what makes machine learning work in practice", "tokens": [51188, 643, 281, 747, 666, 2696, 293, 1391, 341, 307, 534, 437, 1669, 3479, 2539, 589, 294, 3124, 51520], "temperature": 0.0, "avg_logprob": -0.09252636773245675, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0028746684547513723}, {"id": 807, "seek": 469968, "start": 4722.8, "end": 4726.88, "text": " is the assumption that there is some intrinsic structure to the data and it can be captured", "tokens": [51520, 307, 264, 15302, 300, 456, 307, 512, 35698, 3877, 281, 264, 1412, 293, 309, 393, 312, 11828, 51724], "temperature": 0.0, "avg_logprob": -0.09252636773245675, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0028746684547513723}, {"id": 808, "seek": 472688, "start": 4726.88, "end": 4731.4400000000005, "text": " in different ways. So it's either the manifold assumption where you can assume that the data,", "tokens": [50364, 294, 819, 2098, 13, 407, 309, 311, 2139, 264, 47138, 15302, 689, 291, 393, 6552, 300, 264, 1412, 11, 50592], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 809, "seek": 472688, "start": 4731.4400000000005, "end": 4736.0, "text": " even though it lives in a very high dimensional space intrinsically, it is low dimensional.", "tokens": [50592, 754, 1673, 309, 2909, 294, 257, 588, 1090, 18795, 1901, 28621, 984, 11, 309, 307, 2295, 18795, 13, 50820], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 810, "seek": 472688, "start": 4736.0, "end": 4741.52, "text": " This can be captured also in the form of symmetry. For example, image is not just a high dimensional", "tokens": [50820, 639, 393, 312, 11828, 611, 294, 264, 1254, 295, 25440, 13, 1171, 1365, 11, 3256, 307, 406, 445, 257, 1090, 18795, 51096], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 811, "seek": 472688, "start": 4741.52, "end": 4746.08, "text": " vector. It has underlying grid structure and grid structure has symmetry. This is what captured", "tokens": [51096, 8062, 13, 467, 575, 14217, 10748, 3877, 293, 10748, 3877, 575, 25440, 13, 639, 307, 437, 11828, 51324], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 812, "seek": 472688, "start": 4746.08, "end": 4750.8, "text": " in convolutional networks in the form of shared weights that translates into the convolution", "tokens": [51324, 294, 45216, 304, 9590, 294, 264, 1254, 295, 5507, 17443, 300, 28468, 666, 264, 45216, 51560], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 813, "seek": 472688, "start": 4750.8, "end": 4756.0, "text": " operation. I think the symmetries are part of the magic here because it's not just the interesting", "tokens": [51560, 6916, 13, 286, 519, 264, 14232, 302, 2244, 366, 644, 295, 264, 5585, 510, 570, 309, 311, 406, 445, 264, 1880, 51820], "temperature": 0.0, "avg_logprob": -0.06978458404541016, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.005126331467181444}, {"id": 814, "seek": 475600, "start": 4756.0, "end": 4762.16, "text": " observation that natural data is only spatially novel in so few dimensions. There's something", "tokens": [50364, 14816, 300, 3303, 1412, 307, 787, 15000, 2270, 7613, 294, 370, 1326, 12819, 13, 821, 311, 746, 50672], "temperature": 0.0, "avg_logprob": -0.10356343876231801, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0010940926149487495}, {"id": 815, "seek": 475600, "start": 4762.16, "end": 4767.52, "text": " magic about symmetries. And when we spoke to Fran\u00e7ois Chalet recently, he invoked the kaleidoscope", "tokens": [50672, 5585, 466, 14232, 302, 2244, 13, 400, 562, 321, 7179, 281, 1526, 12368, 7376, 761, 49744, 3938, 11, 415, 1048, 9511, 264, 34699, 7895, 13960, 50940], "temperature": 0.0, "avg_logprob": -0.10356343876231801, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0010940926149487495}, {"id": 816, "seek": 475600, "start": 4767.52, "end": 4774.88, "text": " effect, which is this notion that almost all information in reality is a copy of some other", "tokens": [50940, 1802, 11, 597, 307, 341, 10710, 300, 1920, 439, 1589, 294, 4103, 307, 257, 5055, 295, 512, 661, 51308], "temperature": 0.0, "avg_logprob": -0.10356343876231801, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0010940926149487495}, {"id": 817, "seek": 475600, "start": 4774.88, "end": 4780.32, "text": " information. Probably here it will be a little bit stretching, but I would say that because", "tokens": [51308, 1589, 13, 9210, 510, 309, 486, 312, 257, 707, 857, 19632, 11, 457, 286, 576, 584, 300, 570, 51580], "temperature": 0.0, "avg_logprob": -0.10356343876231801, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0010940926149487495}, {"id": 818, "seek": 478032, "start": 4780.32, "end": 4785.84, "text": " data comes from nature, from physical processes that produce it, physics and nature itself", "tokens": [50364, 1412, 1487, 490, 3687, 11, 490, 4001, 7555, 300, 5258, 309, 11, 10649, 293, 3687, 2564, 50640], "temperature": 0.0, "avg_logprob": -0.08535358159229009, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010466084815561771}, {"id": 819, "seek": 478032, "start": 4785.84, "end": 4792.5599999999995, "text": " is in a sense low dimensional. So it's application of simple rules at multiple scales. You can create", "tokens": [50640, 307, 294, 257, 2020, 2295, 18795, 13, 407, 309, 311, 3861, 295, 2199, 4474, 412, 3866, 17408, 13, 509, 393, 1884, 50976], "temperature": 0.0, "avg_logprob": -0.08535358159229009, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010466084815561771}, {"id": 820, "seek": 478032, "start": 4792.5599999999995, "end": 4797.759999999999, "text": " very complex systems with very simple rules. And this is probably how our data that we are mostly", "tokens": [50976, 588, 3997, 3652, 365, 588, 2199, 4474, 13, 400, 341, 307, 1391, 577, 527, 1412, 300, 321, 366, 5240, 51236], "temperature": 0.0, "avg_logprob": -0.08535358159229009, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010466084815561771}, {"id": 821, "seek": 478032, "start": 4797.759999999999, "end": 4802.88, "text": " interested in in machine learning is structured. So you have this manifestation of symmetry and", "tokens": [51236, 3102, 294, 294, 3479, 2539, 307, 18519, 13, 407, 291, 362, 341, 29550, 295, 25440, 293, 51492], "temperature": 0.0, "avg_logprob": -0.08535358159229009, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010466084815561771}, {"id": 822, "seek": 478032, "start": 4802.88, "end": 4807.2, "text": " self-similarity through different scales, the principles of symmetry and certain", "tokens": [51492, 2698, 12, 30937, 2202, 507, 807, 819, 17408, 11, 264, 9156, 295, 25440, 293, 1629, 51708], "temperature": 0.0, "avg_logprob": -0.08535358159229009, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.010466084815561771}, {"id": 823, "seek": 480720, "start": 4807.28, "end": 4812.32, "text": " environs or equivalents and of scale separation, where you can separate your problem to multiple", "tokens": [50368, 2267, 347, 892, 420, 9052, 791, 293, 295, 4373, 14634, 11, 689, 291, 393, 4994, 428, 1154, 281, 3866, 50620], "temperature": 0.0, "avg_logprob": -0.10600081029927956, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.002169350627809763}, {"id": 824, "seek": 480720, "start": 4812.32, "end": 4817.5199999999995, "text": " scales and deal with it at different levels of resolution. And this is captured, for example,", "tokens": [50620, 17408, 293, 2028, 365, 309, 412, 819, 4358, 295, 8669, 13, 400, 341, 307, 11828, 11, 337, 1365, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10600081029927956, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.002169350627809763}, {"id": 825, "seek": 480720, "start": 4817.5199999999995, "end": 4822.48, "text": " in pooling operations in convolutional neural networks and in other deep learning architectures.", "tokens": [50880, 294, 7005, 278, 7705, 294, 45216, 304, 18161, 9590, 293, 294, 661, 2452, 2539, 6331, 1303, 13, 51128], "temperature": 0.0, "avg_logprob": -0.10600081029927956, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.002169350627809763}, {"id": 826, "seek": 480720, "start": 4823.04, "end": 4828.8, "text": " This is what makes deep learning systems work. Fascinating. It's so good that you raised the", "tokens": [51156, 639, 307, 437, 1669, 2452, 2539, 3652, 589, 13, 49098, 8205, 13, 467, 311, 370, 665, 300, 291, 6005, 264, 51444], "temperature": 0.0, "avg_logprob": -0.10600081029927956, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.002169350627809763}, {"id": 827, "seek": 480720, "start": 4828.8, "end": 4832.4, "text": " curse of dimensionality because I was going to ask you about that. Could you explain in really", "tokens": [51444, 17139, 295, 10139, 1860, 570, 286, 390, 516, 281, 1029, 291, 466, 300, 13, 7497, 291, 2903, 294, 534, 51624], "temperature": 0.0, "avg_logprob": -0.10600081029927956, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.002169350627809763}, {"id": 828, "seek": 483240, "start": 4832.4, "end": 4838.639999999999, "text": " simple terms, so not invoking lipships, I can't even say it properly now, but not invoking a", "tokens": [50364, 2199, 2115, 11, 370, 406, 1048, 5953, 10118, 7640, 11, 286, 393, 380, 754, 584, 309, 6108, 586, 11, 457, 406, 1048, 5953, 257, 50676], "temperature": 0.0, "avg_logprob": -0.17372680020022702, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.0015480852453038096}, {"id": 829, "seek": 483240, "start": 4838.639999999999, "end": 4846.16, "text": " mathematical jargon. And why exactly in your articulation does geometric deep learning", "tokens": [50676, 18894, 15181, 10660, 13, 400, 983, 2293, 294, 428, 15228, 2776, 775, 33246, 2452, 2539, 51052], "temperature": 0.0, "avg_logprob": -0.17372680020022702, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.0015480852453038096}, {"id": 830, "seek": 483240, "start": 4846.799999999999, "end": 4849.44, "text": " reduce the impact of the curse of dimensionality?", "tokens": [51084, 5407, 264, 2712, 295, 264, 17139, 295, 10139, 1860, 30, 51216], "temperature": 0.0, "avg_logprob": -0.17372680020022702, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.0015480852453038096}, {"id": 831, "seek": 483240, "start": 4850.639999999999, "end": 4862.0, "text": " Yeah. So the curse of dimensionality, it refers generally to the inability of algorithms to", "tokens": [51276, 865, 13, 407, 264, 17139, 295, 10139, 1860, 11, 309, 14942, 5101, 281, 264, 33162, 295, 14642, 281, 51844], "temperature": 0.0, "avg_logprob": -0.17372680020022702, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.0015480852453038096}, {"id": 832, "seek": 486200, "start": 4862.0, "end": 4867.04, "text": " keep certifying certain performance as the data becomes more complex. And data becoming more", "tokens": [50364, 1066, 5351, 5489, 1629, 3389, 382, 264, 1412, 3643, 544, 3997, 13, 400, 1412, 5617, 544, 50616], "temperature": 0.0, "avg_logprob": -0.09708806692835796, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0008390643633902073}, {"id": 833, "seek": 486200, "start": 4867.04, "end": 4873.2, "text": " complex here means that you have more and more dimensions, more and more pixels. And so this", "tokens": [50616, 3997, 510, 1355, 300, 291, 362, 544, 293, 544, 12819, 11, 544, 293, 544, 18668, 13, 400, 370, 341, 50924], "temperature": 0.0, "avg_logprob": -0.09708806692835796, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0008390643633902073}, {"id": 834, "seek": 486200, "start": 4873.2, "end": 4881.84, "text": " inability of scaling, basically it really says that if I scale up the input, my algorithm is", "tokens": [50924, 33162, 295, 21589, 11, 1936, 309, 534, 1619, 300, 498, 286, 4373, 493, 264, 4846, 11, 452, 9284, 307, 51356], "temperature": 0.0, "avg_logprob": -0.09708806692835796, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0008390643633902073}, {"id": 835, "seek": 486200, "start": 4881.84, "end": 4887.6, "text": " going to have more and more trouble to keep the base. And so this curse can take different", "tokens": [51356, 516, 281, 362, 544, 293, 544, 5253, 281, 1066, 264, 3096, 13, 400, 370, 341, 17139, 393, 747, 819, 51644], "temperature": 0.0, "avg_logprob": -0.09708806692835796, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0008390643633902073}, {"id": 836, "seek": 488760, "start": 4887.6, "end": 4896.160000000001, "text": " flavors. So this curse might have a statistical reason in the sense that as I make my input space", "tokens": [50364, 16303, 13, 407, 341, 17139, 1062, 362, 257, 22820, 1778, 294, 264, 2020, 300, 382, 286, 652, 452, 4846, 1901, 50792], "temperature": 0.0, "avg_logprob": -0.11082456170058833, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0014772090362384915}, {"id": 837, "seek": 488760, "start": 4896.160000000001, "end": 4903.4400000000005, "text": " bigger, there would be many, many, many much exponentially more functions, real functions", "tokens": [50792, 3801, 11, 456, 576, 312, 867, 11, 867, 11, 867, 709, 37330, 544, 6828, 11, 957, 6828, 51156], "temperature": 0.0, "avg_logprob": -0.11082456170058833, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0014772090362384915}, {"id": 838, "seek": 488760, "start": 4903.4400000000005, "end": 4907.76, "text": " out there that would explain the training set that would basically pass through the training points.", "tokens": [51156, 484, 456, 300, 576, 2903, 264, 3097, 992, 300, 576, 1936, 1320, 807, 264, 3097, 2793, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11082456170058833, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0014772090362384915}, {"id": 839, "seek": 488760, "start": 4908.8, "end": 4914.72, "text": " And so the more dimensions I add, the more uncertainty I have about the true function.", "tokens": [51424, 400, 370, 264, 544, 12819, 286, 909, 11, 264, 544, 15697, 286, 362, 466, 264, 2074, 2445, 13, 51720], "temperature": 0.0, "avg_logprob": -0.11082456170058833, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0014772090362384915}, {"id": 840, "seek": 491472, "start": 4914.8, "end": 4920.08, "text": " So I would need more and more training samples to keep the base. This curse can also be from the", "tokens": [50368, 407, 286, 576, 643, 544, 293, 544, 3097, 10938, 281, 1066, 264, 3096, 13, 639, 17139, 393, 611, 312, 490, 264, 50632], "temperature": 0.0, "avg_logprob": -0.10080863382214698, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.0010976825142279267}, {"id": 841, "seek": 491472, "start": 4920.08, "end": 4925.4400000000005, "text": " approximation side. So in the sense that the number of neurons that I'm considering to approximate", "tokens": [50632, 28023, 1252, 13, 407, 294, 264, 2020, 300, 264, 1230, 295, 22027, 300, 286, 478, 8079, 281, 30874, 50900], "temperature": 0.0, "avg_logprob": -0.10080863382214698, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.0010976825142279267}, {"id": 842, "seek": 491472, "start": 4925.4400000000005, "end": 4930.8, "text": " my target function, I need to keep adding more and more neurons at the rate that is exponential", "tokens": [50900, 452, 3779, 2445, 11, 286, 643, 281, 1066, 5127, 544, 293, 544, 22027, 412, 264, 3314, 300, 307, 21510, 51168], "temperature": 0.0, "avg_logprob": -0.10080863382214698, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.0010976825142279267}, {"id": 843, "seek": 491472, "start": 4930.8, "end": 4937.6, "text": " in dimension. And the curse can also be from the computational side. The sense that if I keep", "tokens": [51168, 294, 10139, 13, 400, 264, 17139, 393, 611, 312, 490, 264, 28270, 1252, 13, 440, 2020, 300, 498, 286, 1066, 51508], "temperature": 0.0, "avg_logprob": -0.10080863382214698, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.0010976825142279267}, {"id": 844, "seek": 491472, "start": 4937.6, "end": 4943.92, "text": " adding parameters and parameters to my training model, I might have to optimize to solve an", "tokens": [51508, 5127, 9834, 293, 9834, 281, 452, 3097, 2316, 11, 286, 1062, 362, 281, 19719, 281, 5039, 364, 51824], "temperature": 0.0, "avg_logprob": -0.10080863382214698, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.0010976825142279267}, {"id": 845, "seek": 494392, "start": 4943.92, "end": 4949.6, "text": " optimization problem that becomes exponentially harder. And so you can see that you are basically", "tokens": [50364, 19618, 1154, 300, 3643, 37330, 6081, 13, 400, 370, 291, 393, 536, 300, 291, 366, 1936, 50648], "temperature": 0.0, "avg_logprob": -0.08070794173649379, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0004885718808509409}, {"id": 846, "seek": 494392, "start": 4949.6, "end": 4958.0, "text": " bombarded by all angles. And so an algorithm like here in the context of statistical learning or", "tokens": [50648, 7851, 22803, 538, 439, 14708, 13, 400, 370, 364, 9284, 411, 510, 294, 264, 4319, 295, 22820, 2539, 420, 51068], "temperature": 0.0, "avg_logprob": -0.08070794173649379, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0004885718808509409}, {"id": 847, "seek": 494392, "start": 4958.0, "end": 4963.76, "text": " learning theory, if you want, having a kind of a theorem that would say, yes, I can promise you", "tokens": [51068, 2539, 5261, 11, 498, 291, 528, 11, 1419, 257, 733, 295, 257, 20904, 300, 576, 584, 11, 2086, 11, 286, 393, 6228, 291, 51356], "temperature": 0.0, "avg_logprob": -0.08070794173649379, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0004885718808509409}, {"id": 848, "seek": 494392, "start": 4963.76, "end": 4968.24, "text": " that you can learn, you need to actually solve these three problems at once. You need to be able", "tokens": [51356, 300, 291, 393, 1466, 11, 291, 643, 281, 767, 5039, 613, 1045, 2740, 412, 1564, 13, 509, 643, 281, 312, 1075, 51580], "temperature": 0.0, "avg_logprob": -0.08070794173649379, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0004885718808509409}, {"id": 849, "seek": 494392, "start": 4968.24, "end": 4973.2, "text": " to say that in the conditions that you're studying, you have an algorithm that it does not suffer from", "tokens": [51580, 281, 584, 300, 294, 264, 4487, 300, 291, 434, 7601, 11, 291, 362, 364, 9284, 300, 309, 775, 406, 9753, 490, 51828], "temperature": 0.0, "avg_logprob": -0.08070794173649379, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0004885718808509409}, {"id": 850, "seek": 497320, "start": 4973.2, "end": 4979.12, "text": " approximation nor statistical nor computational curses. So as you can imagine, it's very hard", "tokens": [50364, 28023, 6051, 22820, 6051, 28270, 1262, 6196, 13, 407, 382, 291, 393, 3811, 11, 309, 311, 588, 1152, 50660], "temperature": 0.0, "avg_logprob": -0.0933206421988351, "compression_ratio": 1.76171875, "no_speech_prob": 0.00017661825404502451}, {"id": 851, "seek": 497320, "start": 4979.12, "end": 4984.16, "text": " because you need to master many things at the same time. So why do we think that geometric", "tokens": [50660, 570, 291, 643, 281, 4505, 867, 721, 412, 264, 912, 565, 13, 407, 983, 360, 321, 519, 300, 33246, 50912], "temperature": 0.0, "avg_logprob": -0.0933206421988351, "compression_ratio": 1.76171875, "no_speech_prob": 0.00017661825404502451}, {"id": 852, "seek": 497320, "start": 4984.16, "end": 4991.12, "text": " deep learning is at least an important piece to overcome this curse? As I said before, so", "tokens": [50912, 2452, 2539, 307, 412, 1935, 364, 1021, 2522, 281, 10473, 341, 17139, 30, 1018, 286, 848, 949, 11, 370, 51260], "temperature": 0.0, "avg_logprob": -0.0933206421988351, "compression_ratio": 1.76171875, "no_speech_prob": 0.00017661825404502451}, {"id": 853, "seek": 497320, "start": 4991.12, "end": 4996.5599999999995, "text": " geometric deep learning is really a device to put more structure into the target function.", "tokens": [51260, 33246, 2452, 2539, 307, 534, 257, 4302, 281, 829, 544, 3877, 666, 264, 3779, 2445, 13, 51532], "temperature": 0.0, "avg_logprob": -0.0933206421988351, "compression_ratio": 1.76171875, "no_speech_prob": 0.00017661825404502451}, {"id": 854, "seek": 497320, "start": 4996.5599999999995, "end": 5002.88, "text": " So basically to make the learning problem easier because we are promising the learner", "tokens": [51532, 407, 1936, 281, 652, 264, 2539, 1154, 3571, 570, 321, 366, 20257, 264, 33347, 51848], "temperature": 0.0, "avg_logprob": -0.0933206421988351, "compression_ratio": 1.76171875, "no_speech_prob": 0.00017661825404502451}, {"id": 855, "seek": 500288, "start": 5002.88, "end": 5007.68, "text": " more properties of a target function. We are basically making the hypothesis class if you want", "tokens": [50364, 544, 7221, 295, 257, 3779, 2445, 13, 492, 366, 1936, 1455, 264, 17291, 1508, 498, 291, 528, 50604], "temperature": 0.0, "avg_logprob": -0.1315899782402571, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00045686966041103005}, {"id": 856, "seek": 500288, "start": 5007.68, "end": 5016.24, "text": " smaller. That said, as I said, there's still some path to go. We're describing just a bunch of", "tokens": [50604, 4356, 13, 663, 848, 11, 382, 286, 848, 11, 456, 311, 920, 512, 3100, 281, 352, 13, 492, 434, 16141, 445, 257, 3840, 295, 51032], "temperature": 0.0, "avg_logprob": -0.1315899782402571, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00045686966041103005}, {"id": 857, "seek": 500288, "start": 5016.24, "end": 5021.68, "text": " principles that make these hypothesis spaces smaller and more adapted to the real world.", "tokens": [51032, 9156, 300, 652, 613, 17291, 7673, 4356, 293, 544, 20871, 281, 264, 957, 1002, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1315899782402571, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00045686966041103005}, {"id": 858, "seek": 500288, "start": 5022.32, "end": 5027.36, "text": " But one thing that we are still lacking, for example, is the guarantee in terms of optimization.", "tokens": [51336, 583, 472, 551, 300, 321, 366, 920, 20889, 11, 337, 1365, 11, 307, 264, 10815, 294, 2115, 295, 19618, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1315899782402571, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00045686966041103005}, {"id": 859, "seek": 502736, "start": 5027.599999999999, "end": 5032.48, "text": " I mean, I described that the depth of these architectures is somehow something that is", "tokens": [50376, 286, 914, 11, 286, 7619, 300, 264, 7161, 295, 613, 6331, 1303, 307, 6063, 746, 300, 307, 50620], "temperature": 0.0, "avg_logprob": -0.12732970480825387, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.003636501729488373}, {"id": 860, "seek": 502736, "start": 5032.48, "end": 5036.639999999999, "text": " akin associated with the scale, the fact that you need to understand things at different scales.", "tokens": [50620, 47540, 6615, 365, 264, 4373, 11, 264, 1186, 300, 291, 643, 281, 1223, 721, 412, 819, 17408, 13, 50828], "temperature": 0.0, "avg_logprob": -0.12732970480825387, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.003636501729488373}, {"id": 861, "seek": 502736, "start": 5037.28, "end": 5043.599999999999, "text": " So as you know, from the optimization side, there's still some open questions and open mysteries", "tokens": [50860, 407, 382, 291, 458, 11, 490, 264, 19618, 1252, 11, 456, 311, 920, 512, 1269, 1651, 293, 1269, 30785, 51176], "temperature": 0.0, "avg_logprob": -0.12732970480825387, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.003636501729488373}, {"id": 862, "seek": 502736, "start": 5043.599999999999, "end": 5048.48, "text": " as to why the gradient descent, for example, is able to find good solutions. So these are", "tokens": [51176, 382, 281, 983, 264, 16235, 23475, 11, 337, 1365, 11, 307, 1075, 281, 915, 665, 6547, 13, 407, 613, 366, 51420], "temperature": 0.0, "avg_logprob": -0.12732970480825387, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.003636501729488373}, {"id": 863, "seek": 502736, "start": 5048.48, "end": 5054.5599999999995, "text": " things that we believe that these architectures can be optimized efficiently just because we have", "tokens": [51420, 721, 300, 321, 1697, 300, 613, 6331, 1303, 393, 312, 26941, 19621, 445, 570, 321, 362, 51724], "temperature": 0.0, "avg_logprob": -0.12732970480825387, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.003636501729488373}, {"id": 864, "seek": 505456, "start": 5054.56, "end": 5059.04, "text": " these experimental evidence that is piling up. But we are, for example, we are still lacking", "tokens": [50364, 613, 17069, 4467, 300, 307, 280, 4883, 493, 13, 583, 321, 366, 11, 337, 1365, 11, 321, 366, 920, 20889, 50588], "temperature": 0.0, "avg_logprob": -0.11941150449356942, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0008464319398626685}, {"id": 865, "seek": 505456, "start": 5059.76, "end": 5066.160000000001, "text": " theoretical guarantees. For the approximation, it's a bit of the same story. So we understand", "tokens": [50624, 20864, 32567, 13, 1171, 264, 28023, 11, 309, 311, 257, 857, 295, 264, 912, 1657, 13, 407, 321, 1223, 50944], "temperature": 0.0, "avg_logprob": -0.11941150449356942, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0008464319398626685}, {"id": 866, "seek": 505456, "start": 5066.160000000001, "end": 5071.6, "text": " very well approximation properties of shallow networks, starting from universal approximation,", "tokens": [50944, 588, 731, 28023, 7221, 295, 20488, 9590, 11, 2891, 490, 11455, 28023, 11, 51216], "temperature": 0.0, "avg_logprob": -0.11941150449356942, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0008464319398626685}, {"id": 867, "seek": 505456, "start": 5071.6, "end": 5077.120000000001, "text": " but of course, many, many recent interesting work. But we are also still lagging a little bit behind", "tokens": [51216, 457, 295, 1164, 11, 867, 11, 867, 5162, 1880, 589, 13, 583, 321, 366, 611, 920, 8953, 3249, 257, 707, 857, 2261, 51492], "temperature": 0.0, "avg_logprob": -0.11941150449356942, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0008464319398626685}, {"id": 868, "seek": 505456, "start": 5077.76, "end": 5083.68, "text": " in approximation properties for deeper networks. So as you see, it's like you can see from this", "tokens": [51524, 294, 28023, 7221, 337, 7731, 9590, 13, 407, 382, 291, 536, 11, 309, 311, 411, 291, 393, 536, 490, 341, 51820], "temperature": 0.0, "avg_logprob": -0.11941150449356942, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0008464319398626685}, {"id": 869, "seek": 508368, "start": 5083.76, "end": 5090.240000000001, "text": " discussion that, yes, we have some good reasons to believe that these are fundamental principles", "tokens": [50368, 5017, 300, 11, 2086, 11, 321, 362, 512, 665, 4112, 281, 1697, 300, 613, 366, 8088, 9156, 50692], "temperature": 0.0, "avg_logprob": -0.14071007164157168, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.005413708742707968}, {"id": 870, "seek": 508368, "start": 5090.240000000001, "end": 5097.4400000000005, "text": " of learning. But there's also a bunch of mathematical questions that are still open. And this is also", "tokens": [50692, 295, 2539, 13, 583, 456, 311, 611, 257, 3840, 295, 18894, 1651, 300, 366, 920, 1269, 13, 400, 341, 307, 611, 51052], "temperature": 0.0, "avg_logprob": -0.14071007164157168, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.005413708742707968}, {"id": 871, "seek": 508368, "start": 5097.4400000000005, "end": 5101.76, "text": " one of the things that I like about writing a book on this topic, because it's a very life domain.", "tokens": [51052, 472, 295, 264, 721, 300, 286, 411, 466, 3579, 257, 1446, 322, 341, 4829, 11, 570, 309, 311, 257, 588, 993, 9274, 13, 51268], "temperature": 0.0, "avg_logprob": -0.14071007164157168, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.005413708742707968}, {"id": 872, "seek": 508368, "start": 5103.76, "end": 5109.52, "text": " As you can see, the field is still evolving. And I think it's a good time to... Yeah, it's a good", "tokens": [51368, 1018, 291, 393, 536, 11, 264, 2519, 307, 920, 21085, 13, 400, 286, 519, 309, 311, 257, 665, 565, 281, 485, 865, 11, 309, 311, 257, 665, 51656], "temperature": 0.0, "avg_logprob": -0.14071007164157168, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.005413708742707968}, {"id": 873, "seek": 510952, "start": 5109.52, "end": 5114.320000000001, "text": " time. I mean, researchers out there are listening to us. It's a good time to think and to work and", "tokens": [50364, 565, 13, 286, 914, 11, 10309, 484, 456, 366, 4764, 281, 505, 13, 467, 311, 257, 665, 565, 281, 519, 293, 281, 589, 293, 50604], "temperature": 0.0, "avg_logprob": -0.07657127736884857, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.0032598054967820644}, {"id": 874, "seek": 510952, "start": 5114.320000000001, "end": 5120.4800000000005, "text": " to join this program. Amazing. Will we ever understand the approximation properties of deep", "tokens": [50604, 281, 3917, 341, 1461, 13, 14165, 13, 3099, 321, 1562, 1223, 264, 28023, 7221, 295, 2452, 50912], "temperature": 0.0, "avg_logprob": -0.07657127736884857, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.0032598054967820644}, {"id": 875, "seek": 510952, "start": 5120.4800000000005, "end": 5126.64, "text": " networks? Because with the shallow function approximation algorithm, you can almost think", "tokens": [50912, 9590, 30, 1436, 365, 264, 20488, 2445, 28023, 9284, 11, 291, 393, 1920, 519, 51220], "temperature": 0.0, "avg_logprob": -0.07657127736884857, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.0032598054967820644}, {"id": 876, "seek": 510952, "start": 5126.64, "end": 5131.360000000001, "text": " of a neural network as being kind of like sparse coding. And the more neurons you have, you're", "tokens": [51220, 295, 257, 18161, 3209, 382, 885, 733, 295, 411, 637, 11668, 17720, 13, 400, 264, 544, 22027, 291, 362, 11, 291, 434, 51456], "temperature": 0.0, "avg_logprob": -0.07657127736884857, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.0032598054967820644}, {"id": 877, "seek": 510952, "start": 5131.360000000001, "end": 5136.8, "text": " just kind of discreetly fitting this arbitrary function. But you don't have that visual intuition", "tokens": [51456, 445, 733, 295, 2983, 4751, 356, 15669, 341, 23211, 2445, 13, 583, 291, 500, 380, 362, 300, 5056, 24002, 51728], "temperature": 0.0, "avg_logprob": -0.07657127736884857, "compression_ratio": 1.6713780918727916, "no_speech_prob": 0.0032598054967820644}, {"id": 878, "seek": 513680, "start": 5136.8, "end": 5143.84, "text": " quite so much with the deep networks. Yeah, I mean, it's an important... And it's a deep question.", "tokens": [50364, 1596, 370, 709, 365, 264, 2452, 9590, 13, 865, 11, 286, 914, 11, 309, 311, 364, 1021, 485, 400, 309, 311, 257, 2452, 1168, 13, 50716], "temperature": 0.0, "avg_logprob": -0.16173371707691866, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.0021813532803207636}, {"id": 879, "seek": 513680, "start": 5143.84, "end": 5151.68, "text": " So indeed, shallow neural networks are really, really correspond to this idea that you learn", "tokens": [50716, 407, 6451, 11, 20488, 18161, 9590, 366, 534, 11, 534, 6805, 281, 341, 1558, 300, 291, 1466, 51108], "temperature": 0.0, "avg_logprob": -0.16173371707691866, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.0021813532803207636}, {"id": 880, "seek": 513680, "start": 5151.68, "end": 5157.92, "text": " a function by stacking a linear combination of basis elements. And this is really at the roots", "tokens": [51108, 257, 2445, 538, 41376, 257, 8213, 6562, 295, 5143, 4959, 13, 400, 341, 307, 534, 412, 264, 10669, 51420], "temperature": 0.0, "avg_logprob": -0.16173371707691866, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.0021813532803207636}, {"id": 881, "seek": 513680, "start": 5157.92, "end": 5162.400000000001, "text": " of essentially all of harmonic analysis or functionalized. I typically think about the", "tokens": [51420, 295, 4476, 439, 295, 32270, 5215, 420, 11745, 1602, 13, 286, 5850, 519, 466, 264, 51644], "temperature": 0.0, "avg_logprob": -0.16173371707691866, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.0021813532803207636}, {"id": 882, "seek": 516240, "start": 5162.4, "end": 5166.48, "text": " basis and you ask questions about the linear approximation or the approximation, etc.", "tokens": [50364, 5143, 293, 291, 1029, 1651, 466, 264, 8213, 28023, 420, 264, 28023, 11, 5183, 13, 50568], "temperature": 0.0, "avg_logprob": -0.13358339396390048, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0002527085307519883}, {"id": 883, "seek": 516240, "start": 5167.2, "end": 5171.12, "text": " Deep neural networks, they introduce a fundamentally different way to approximate", "tokens": [50604, 14895, 18161, 9590, 11, 436, 5366, 257, 17879, 819, 636, 281, 30874, 50800], "temperature": 0.0, "avg_logprob": -0.13358339396390048, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0002527085307519883}, {"id": 884, "seek": 516240, "start": 5171.12, "end": 5177.92, "text": " functions that is by composing, by composition. And so you're right. Our knowledge about this", "tokens": [50800, 6828, 300, 307, 538, 715, 6110, 11, 538, 12686, 13, 400, 370, 291, 434, 558, 13, 2621, 3601, 466, 341, 51140], "temperature": 0.0, "avg_logprob": -0.13358339396390048, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0002527085307519883}, {"id": 885, "seek": 516240, "start": 5177.92, "end": 5184.719999999999, "text": " question right now is mostly concentrated in what we call separation results, like a depth", "tokens": [51140, 1168, 558, 586, 307, 5240, 21321, 294, 437, 321, 818, 14634, 3542, 11, 411, 257, 7161, 51480], "temperature": 0.0, "avg_logprob": -0.13358339396390048, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0002527085307519883}, {"id": 886, "seek": 516240, "start": 5184.719999999999, "end": 5190.96, "text": " separation, which consists in trying to find construct mathematical examples of functions", "tokens": [51480, 14634, 11, 597, 14689, 294, 1382, 281, 915, 7690, 18894, 5110, 295, 6828, 51792], "temperature": 0.0, "avg_logprob": -0.13358339396390048, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0002527085307519883}, {"id": 887, "seek": 519096, "start": 5190.96, "end": 5195.76, "text": " that cannot be approximated with shallow neural networks with certain number of neurons.", "tokens": [50364, 300, 2644, 312, 8542, 770, 365, 20488, 18161, 9590, 365, 1629, 1230, 295, 22027, 13, 50604], "temperature": 0.0, "avg_logprob": -0.12359472031288958, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00027226671227253973}, {"id": 888, "seek": 519096, "start": 5195.76, "end": 5200.24, "text": " But indeed, they can be much better approximated with deep neural networks. So this is really", "tokens": [50604, 583, 6451, 11, 436, 393, 312, 709, 1101, 8542, 770, 365, 2452, 18161, 9590, 13, 407, 341, 307, 534, 50828], "temperature": 0.0, "avg_logprob": -0.12359472031288958, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00027226671227253973}, {"id": 889, "seek": 519096, "start": 5200.24, "end": 5206.08, "text": " understanding which kinds of functions benefit fundamentally from composition rather than from", "tokens": [50828, 3701, 597, 3685, 295, 6828, 5121, 17879, 490, 12686, 2831, 813, 490, 51120], "temperature": 0.0, "avg_logprob": -0.12359472031288958, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00027226671227253973}, {"id": 890, "seek": 519096, "start": 5206.08, "end": 5214.32, "text": " addition. And so there's a certain mathematical vision and mathematical intuition that is", "tokens": [51120, 4500, 13, 400, 370, 456, 311, 257, 1629, 18894, 5201, 293, 18894, 24002, 300, 307, 51532], "temperature": 0.0, "avg_logprob": -0.12359472031288958, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00027226671227253973}, {"id": 891, "seek": 519096, "start": 5214.32, "end": 5220.24, "text": " building up. But of course, it's still very far from explaining the true power of depth. And", "tokens": [51532, 2390, 493, 13, 583, 295, 1164, 11, 309, 311, 920, 588, 1400, 490, 13468, 264, 2074, 1347, 295, 7161, 13, 400, 51828], "temperature": 0.0, "avg_logprob": -0.12359472031288958, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00027226671227253973}, {"id": 892, "seek": 522024, "start": 5220.24, "end": 5227.76, "text": " just to give you like a final pointer here, there's a very related question that replaces", "tokens": [50364, 445, 281, 976, 291, 411, 257, 2572, 23918, 510, 11, 456, 311, 257, 588, 4077, 1168, 300, 46734, 50740], "temperature": 0.0, "avg_logprob": -0.12091113819795496, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00021637130703311414}, {"id": 893, "seek": 522024, "start": 5227.76, "end": 5232.08, "text": " neural networks as we understand them with what we call Boolean functions, right? Like these are", "tokens": [50740, 18161, 9590, 382, 321, 1223, 552, 365, 437, 321, 818, 23351, 28499, 6828, 11, 558, 30, 1743, 613, 366, 50956], "temperature": 0.0, "avg_logprob": -0.12091113819795496, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00021637130703311414}, {"id": 894, "seek": 522024, "start": 5232.08, "end": 5238.16, "text": " just circuits, arithmetic circuits that take as input some bit string and they can manipulate the", "tokens": [50956, 445, 26354, 11, 42973, 26354, 300, 747, 382, 4846, 512, 857, 6798, 293, 436, 393, 20459, 264, 51260], "temperature": 0.0, "avg_logprob": -0.12091113819795496, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00021637130703311414}, {"id": 895, "seek": 522024, "start": 5238.16, "end": 5245.84, "text": " bits by, you know, or operations and operations and they can keep adding gates. And so this question", "tokens": [51260, 9239, 538, 11, 291, 458, 11, 420, 7705, 293, 7705, 293, 436, 393, 1066, 5127, 19792, 13, 400, 370, 341, 1168, 51644], "temperature": 0.0, "avg_logprob": -0.12091113819795496, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00021637130703311414}, {"id": 896, "seek": 524584, "start": 5245.84, "end": 5250.4800000000005, "text": " about what is the ability of a certain circuit architecture to approximate certain Boolean", "tokens": [50364, 466, 437, 307, 264, 3485, 295, 257, 1629, 9048, 9482, 281, 30874, 1629, 23351, 28499, 50596], "temperature": 0.0, "avg_logprob": -0.09680986896003645, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0008801402873359621}, {"id": 897, "seek": 524584, "start": 5250.4800000000005, "end": 5257.12, "text": " functions is actually a notoriously hard and basically has been studied in the theoretical", "tokens": [50596, 6828, 307, 767, 257, 46772, 8994, 1152, 293, 1936, 575, 668, 9454, 294, 264, 20864, 50928], "temperature": 0.0, "avg_logprob": -0.09680986896003645, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0008801402873359621}, {"id": 898, "seek": 524584, "start": 5257.12, "end": 5262.24, "text": " computer science community since the 50s and the 60s, right? And there's actually very, very,", "tokens": [50928, 3820, 3497, 1768, 1670, 264, 2625, 82, 293, 264, 4060, 82, 11, 558, 30, 400, 456, 311, 767, 588, 11, 588, 11, 51184], "temperature": 0.0, "avg_logprob": -0.09680986896003645, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0008801402873359621}, {"id": 899, "seek": 524584, "start": 5262.24, "end": 5267.92, "text": " very deep results and very challenging actually open questions concerning these things. So this", "tokens": [51184, 588, 2452, 3542, 293, 588, 7595, 767, 1269, 1651, 18087, 613, 721, 13, 407, 341, 51468], "temperature": 0.0, "avg_logprob": -0.09680986896003645, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0008801402873359621}, {"id": 900, "seek": 524584, "start": 5267.92, "end": 5274.08, "text": " is really, we are really touching here questions that are pretty serious at like the deep mathematical", "tokens": [51468, 307, 534, 11, 321, 366, 534, 11175, 510, 1651, 300, 366, 1238, 3156, 412, 411, 264, 2452, 18894, 51776], "temperature": 0.0, "avg_logprob": -0.09680986896003645, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0008801402873359621}, {"id": 901, "seek": 527408, "start": 5274.08, "end": 5280.0, "text": " and theoretical level. And so, yes, you should not expect that in the year in the next year or two,", "tokens": [50364, 293, 20864, 1496, 13, 400, 370, 11, 2086, 11, 291, 820, 406, 2066, 300, 294, 264, 1064, 294, 264, 958, 1064, 420, 732, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10256268667138141, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.0008153855451382697}, {"id": 902, "seek": 527408, "start": 5280.0, "end": 5285.12, "text": " we have a complete understanding of, you know, approximation powers of any architecture with", "tokens": [50660, 321, 362, 257, 3566, 3701, 295, 11, 291, 458, 11, 28023, 8674, 295, 604, 9482, 365, 50916], "temperature": 0.0, "avg_logprob": -0.10256268667138141, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.0008153855451382697}, {"id": 903, "seek": 527408, "start": 5285.12, "end": 5294.08, "text": " any depth. But I think you should expect that the theory like this will continue to try to catch up", "tokens": [50916, 604, 7161, 13, 583, 286, 519, 291, 820, 2066, 300, 264, 5261, 411, 341, 486, 2354, 281, 853, 281, 3745, 493, 51364], "temperature": 0.0, "avg_logprob": -0.10256268667138141, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.0008153855451382697}, {"id": 904, "seek": 527408, "start": 5294.08, "end": 5302.48, "text": " with the experiments. And so we are, I think we are hoping to get like a more precise mathematical", "tokens": [51364, 365, 264, 12050, 13, 400, 370, 321, 366, 11, 286, 519, 321, 366, 7159, 281, 483, 411, 257, 544, 13600, 18894, 51784], "temperature": 0.0, "avg_logprob": -0.10256268667138141, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.0008153855451382697}, {"id": 905, "seek": 530248, "start": 5302.48, "end": 5310.5599999999995, "text": " understanding of the role of depth. And as I said before, there's one thing that is fascinating", "tokens": [50364, 3701, 295, 264, 3090, 295, 7161, 13, 400, 382, 286, 848, 949, 11, 456, 311, 472, 551, 300, 307, 10343, 50768], "temperature": 0.0, "avg_logprob": -0.11055210490285615, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.000998897012323141}, {"id": 906, "seek": 530248, "start": 5310.5599999999995, "end": 5316.799999999999, "text": " about this domain that is maybe very unique to deep learning is this very strong interaction", "tokens": [50768, 466, 341, 9274, 300, 307, 1310, 588, 3845, 281, 2452, 2539, 307, 341, 588, 2068, 9285, 51080], "temperature": 0.0, "avg_logprob": -0.11055210490285615, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.000998897012323141}, {"id": 907, "seek": 530248, "start": 5316.799999999999, "end": 5323.12, "text": " between optimization, statistics and approximation, right? Maybe it turns out that, you know, the", "tokens": [51080, 1296, 19618, 11, 12523, 293, 28023, 11, 558, 30, 2704, 309, 4523, 484, 300, 11, 291, 458, 11, 264, 51396], "temperature": 0.0, "avg_logprob": -0.11055210490285615, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.000998897012323141}, {"id": 908, "seek": 530248, "start": 5323.12, "end": 5328.16, "text": " huge depth that we have in these residual neural networks might not be necessary from the", "tokens": [51396, 2603, 7161, 300, 321, 362, 294, 613, 27980, 18161, 9590, 1062, 406, 312, 4818, 490, 264, 51648], "temperature": 0.0, "avg_logprob": -0.11055210490285615, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.000998897012323141}, {"id": 909, "seek": 532816, "start": 5328.16, "end": 5333.84, "text": " approximation side, but in fact, it's so useful for the optimization that overall is a big winner,", "tokens": [50364, 28023, 1252, 11, 457, 294, 1186, 11, 309, 311, 370, 4420, 337, 264, 19618, 300, 4787, 307, 257, 955, 8507, 11, 50648], "temperature": 0.0, "avg_logprob": -0.12658894856770833, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0038026815745979548}, {"id": 910, "seek": 532816, "start": 5333.84, "end": 5340.0, "text": " right? So there's always these like twists about this question that are fundamentally mixing these", "tokens": [50648, 558, 30, 407, 456, 311, 1009, 613, 411, 35290, 466, 341, 1168, 300, 366, 17879, 11983, 613, 50956], "temperature": 0.0, "avg_logprob": -0.12658894856770833, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0038026815745979548}, {"id": 911, "seek": 532816, "start": 5340.0, "end": 5345.5199999999995, "text": " three sources of error. I'm sorry for asking you this question, but can neural networks extrapolate", "tokens": [50956, 1045, 7139, 295, 6713, 13, 286, 478, 2597, 337, 3365, 291, 341, 1168, 11, 457, 393, 18161, 9590, 48224, 473, 51232], "temperature": 0.0, "avg_logprob": -0.12658894856770833, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0038026815745979548}, {"id": 912, "seek": 532816, "start": 5345.5199999999995, "end": 5356.24, "text": " outside of the training data? That's a good question. I would say that the answer, I guess,", "tokens": [51232, 2380, 295, 264, 3097, 1412, 30, 663, 311, 257, 665, 1168, 13, 286, 576, 584, 300, 264, 1867, 11, 286, 2041, 11, 51768], "temperature": 0.0, "avg_logprob": -0.12658894856770833, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0038026815745979548}, {"id": 913, "seek": 535624, "start": 5356.32, "end": 5363.36, "text": " depends on your specifications, right? So I guess that the conservative answer", "tokens": [50368, 5946, 322, 428, 29448, 11, 558, 30, 407, 286, 2041, 300, 264, 13780, 1867, 50720], "temperature": 0.0, "avg_logprob": -0.09047943868754822, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0015199624467641115}, {"id": 914, "seek": 535624, "start": 5364.24, "end": 5370.48, "text": " of a statistical learning person would be no, because we don't have good theorems right now", "tokens": [50764, 295, 257, 22820, 2539, 954, 576, 312, 572, 11, 570, 321, 500, 380, 362, 665, 10299, 2592, 558, 586, 51076], "temperature": 0.0, "avg_logprob": -0.09047943868754822, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0015199624467641115}, {"id": 915, "seek": 535624, "start": 5370.48, "end": 5378.24, "text": " that tell us that this is the case. There's very like, you know, like a strong effort both from", "tokens": [51076, 300, 980, 505, 300, 341, 307, 264, 1389, 13, 821, 311, 588, 411, 11, 291, 458, 11, 411, 257, 2068, 4630, 1293, 490, 51464], "temperature": 0.0, "avg_logprob": -0.09047943868754822, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0015199624467641115}, {"id": 916, "seek": 535624, "start": 5378.24, "end": 5383.44, "text": " the practical and the theoretical community to really understand this question, like by trying", "tokens": [51464, 264, 8496, 293, 264, 20864, 1768, 281, 534, 1223, 341, 1168, 11, 411, 538, 1382, 51724], "temperature": 0.0, "avg_logprob": -0.09047943868754822, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0015199624467641115}, {"id": 917, "seek": 538344, "start": 5383.44, "end": 5388.5599999999995, "text": " to formalize it a bit more. I mean, what do we mean by distribution shift? You know, what kind of", "tokens": [50364, 281, 9860, 1125, 309, 257, 857, 544, 13, 286, 914, 11, 437, 360, 321, 914, 538, 7316, 5513, 30, 509, 458, 11, 437, 733, 295, 50620], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 918, "seek": 538344, "start": 5388.5599999999995, "end": 5393.36, "text": " training procedures you can come up with that would give you precisely this kind of robustness?", "tokens": [50620, 3097, 13846, 291, 393, 808, 493, 365, 300, 576, 976, 291, 13402, 341, 733, 295, 13956, 1287, 30, 50860], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 919, "seek": 538344, "start": 5394.16, "end": 5399.759999999999, "text": " There's of course, what we call these biases, right? I mean, I can always take it like a", "tokens": [50900, 821, 311, 295, 1164, 11, 437, 321, 818, 613, 32152, 11, 558, 30, 286, 914, 11, 286, 393, 1009, 747, 309, 411, 257, 51180], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 920, "seek": 538344, "start": 5399.759999999999, "end": 5404.32, "text": " training distribution, I make a choice of a certain architecture, I'm going to learn a function,", "tokens": [51180, 3097, 7316, 11, 286, 652, 257, 3922, 295, 257, 1629, 9482, 11, 286, 478, 516, 281, 1466, 257, 2445, 11, 51408], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 921, "seek": 538344, "start": 5404.32, "end": 5408.639999999999, "text": " and obviously, there's some directions if you want in the space of distributions,", "tokens": [51408, 293, 2745, 11, 456, 311, 512, 11095, 498, 291, 528, 294, 264, 1901, 295, 37870, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 922, "seek": 538344, "start": 5408.639999999999, "end": 5412.32, "text": " for which my hypothesis will turn out to be have good generalization, there might be", "tokens": [51624, 337, 597, 452, 17291, 486, 1261, 484, 281, 312, 362, 665, 2674, 2144, 11, 456, 1062, 312, 51808], "temperature": 0.0, "avg_logprob": -0.11019614226836011, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0020473916083574295}, {"id": 923, "seek": 541232, "start": 5412.32, "end": 5416.5599999999995, "text": " other direction in which the contrary is true. So there's actually very nice work", "tokens": [50364, 661, 3513, 294, 597, 264, 19506, 307, 2074, 13, 407, 456, 311, 767, 588, 1481, 589, 50576], "temperature": 0.0, "avg_logprob": -0.12852275596474702, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0003564670623745769}, {"id": 924, "seek": 541232, "start": 5417.5199999999995, "end": 5422.96, "text": " from Stephanie Gejalka's group at MIT, where they, for example, they studied this question", "tokens": [50624, 490, 18634, 2876, 22600, 2330, 311, 1594, 412, 13100, 11, 689, 436, 11, 337, 1365, 11, 436, 9454, 341, 1168, 50896], "temperature": 0.0, "avg_logprob": -0.12852275596474702, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0003564670623745769}, {"id": 925, "seek": 541232, "start": 5422.96, "end": 5428.32, "text": " in the context of value networks, also including graphs, where they, for example, they discover", "tokens": [50896, 294, 264, 4319, 295, 2158, 9590, 11, 611, 3009, 24877, 11, 689, 436, 11, 337, 1365, 11, 436, 4411, 51164], "temperature": 0.0, "avg_logprob": -0.12852275596474702, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0003564670623745769}, {"id": 926, "seek": 541232, "start": 5428.32, "end": 5434.0, "text": " or they identify this strong preference for value networks to generalize along linear directions,", "tokens": [51164, 420, 436, 5876, 341, 2068, 17502, 337, 2158, 9590, 281, 2674, 1125, 2051, 8213, 11095, 11, 51448], "temperature": 0.0, "avg_logprob": -0.12852275596474702, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0003564670623745769}, {"id": 927, "seek": 541232, "start": 5434.0, "end": 5439.36, "text": " right? In the sense that if I just decide to now, you know, shift my data in linear directions,", "tokens": [51448, 558, 30, 682, 264, 2020, 300, 498, 286, 445, 4536, 281, 586, 11, 291, 458, 11, 5513, 452, 1412, 294, 8213, 11095, 11, 51716], "temperature": 0.0, "avg_logprob": -0.12852275596474702, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0003564670623745769}, {"id": 928, "seek": 543936, "start": 5439.36, "end": 5444.08, "text": " then my function has no trouble generalizing. Maybe there's other directions, right, in which", "tokens": [50364, 550, 452, 2445, 575, 572, 5253, 2674, 3319, 13, 2704, 456, 311, 661, 11095, 11, 558, 11, 294, 597, 50600], "temperature": 0.0, "avg_logprob": -0.10662731881869042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004298957355786115}, {"id": 929, "seek": 543936, "start": 5444.08, "end": 5448.799999999999, "text": " this thing is actually catastrophic. So I think that, yeah, the question, I think it's very important", "tokens": [50600, 341, 551, 307, 767, 34915, 13, 407, 286, 519, 300, 11, 1338, 11, 264, 1168, 11, 286, 519, 309, 311, 588, 1021, 50836], "temperature": 0.0, "avg_logprob": -0.10662731881869042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004298957355786115}, {"id": 930, "seek": 543936, "start": 5449.36, "end": 5456.639999999999, "text": " from, let's say, it's very important from a kind of a practitioner perspective, right? That's typically,", "tokens": [50864, 490, 11, 718, 311, 584, 11, 309, 311, 588, 1021, 490, 257, 733, 295, 257, 32125, 4585, 11, 558, 30, 663, 311, 5850, 11, 51228], "temperature": 0.0, "avg_logprob": -0.10662731881869042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004298957355786115}, {"id": 931, "seek": 543936, "start": 5456.639999999999, "end": 5461.759999999999, "text": " that's clearly something that a user would like to know. But I think that from a more mathematical", "tokens": [51228, 300, 311, 4448, 746, 300, 257, 4195, 576, 411, 281, 458, 13, 583, 286, 519, 300, 490, 257, 544, 18894, 51484], "temperature": 0.0, "avg_logprob": -0.10662731881869042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004298957355786115}, {"id": 932, "seek": 543936, "start": 5461.759999999999, "end": 5465.759999999999, "text": " or theoretical level, I think we are still at the stage of trying to formalize like, okay,", "tokens": [51484, 420, 20864, 1496, 11, 286, 519, 321, 366, 920, 412, 264, 3233, 295, 1382, 281, 9860, 1125, 411, 11, 1392, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10662731881869042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004298957355786115}, {"id": 933, "seek": 546576, "start": 5465.76, "end": 5471.360000000001, "text": " what do we exactly mean by extrapolation? And what are the kind of the conditions for", "tokens": [50364, 437, 360, 321, 2293, 914, 538, 48224, 399, 30, 400, 437, 366, 264, 733, 295, 264, 4487, 337, 50644], "temperature": 0.0, "avg_logprob": -0.15195881403409517, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0012970297830179334}, {"id": 934, "seek": 546576, "start": 5471.360000000001, "end": 5476.08, "text": " which architecture can do it? And I think that, yeah, this is a, yeah, it's an important question.", "tokens": [50644, 597, 9482, 393, 360, 309, 30, 400, 286, 519, 300, 11, 1338, 11, 341, 307, 257, 11, 1338, 11, 309, 311, 364, 1021, 1168, 13, 50880], "temperature": 0.0, "avg_logprob": -0.15195881403409517, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0012970297830179334}, {"id": 935, "seek": 546576, "start": 5476.08, "end": 5480.320000000001, "text": " But yeah, I think we are still pretty far from having a full answer.", "tokens": [50880, 583, 1338, 11, 286, 519, 321, 366, 920, 1238, 1400, 490, 1419, 257, 1577, 1867, 13, 51092], "temperature": 0.0, "avg_logprob": -0.15195881403409517, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0012970297830179334}, {"id": 936, "seek": 546576, "start": 5481.360000000001, "end": 5485.4400000000005, "text": " Amazing. And final question, what areas of mathematics do people need to study", "tokens": [51144, 14165, 13, 400, 2572, 1168, 11, 437, 3179, 295, 18666, 360, 561, 643, 281, 2979, 51348], "temperature": 0.0, "avg_logprob": -0.15195881403409517, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0012970297830179334}, {"id": 937, "seek": 546576, "start": 5485.4400000000005, "end": 5493.52, "text": " before reading the proto book? Good question. So I think that our objective and our really", "tokens": [51348, 949, 3760, 264, 47896, 1446, 30, 2205, 1168, 13, 407, 286, 519, 300, 527, 10024, 293, 527, 534, 51752], "temperature": 0.0, "avg_logprob": -0.15195881403409517, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0012970297830179334}, {"id": 938, "seek": 549352, "start": 5493.52, "end": 5499.360000000001, "text": " like the idea is really to have something that is quite self-contained in the sense that we are", "tokens": [50364, 411, 264, 1558, 307, 534, 281, 362, 746, 300, 307, 1596, 2698, 12, 9000, 3563, 294, 264, 2020, 300, 321, 366, 50656], "temperature": 0.0, "avg_logprob": -0.12285585140963214, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0007432496640831232}, {"id": 939, "seek": 549352, "start": 5499.360000000001, "end": 5505.200000000001, "text": " going to provide appendices that expand on the areas that is maybe they're not typically", "tokens": [50656, 516, 281, 2893, 34116, 1473, 300, 5268, 322, 264, 3179, 300, 307, 1310, 436, 434, 406, 5850, 50948], "temperature": 0.0, "avg_logprob": -0.12285585140963214, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0007432496640831232}, {"id": 940, "seek": 549352, "start": 5506.0, "end": 5510.64, "text": " kind of the bread and butter of machine learning people. For example, we are going to have an", "tokens": [50988, 733, 295, 264, 5961, 293, 5517, 295, 3479, 2539, 561, 13, 1171, 1365, 11, 321, 366, 516, 281, 362, 364, 51220], "temperature": 0.0, "avg_logprob": -0.12285585140963214, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0007432496640831232}, {"id": 941, "seek": 549352, "start": 5510.64, "end": 5517.84, "text": " appendix on group theory, differential geometry, harmonic analysis. So these are areas that I", "tokens": [51220, 34116, 970, 322, 1594, 5261, 11, 15756, 18426, 11, 32270, 5215, 13, 407, 613, 366, 3179, 300, 286, 51580], "temperature": 0.0, "avg_logprob": -0.12285585140963214, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0007432496640831232}, {"id": 942, "seek": 549352, "start": 5517.84, "end": 5523.200000000001, "text": " think are going to be there for you to delve into, to get like the most of the paper,", "tokens": [51580, 519, 366, 516, 281, 312, 456, 337, 291, 281, 43098, 666, 11, 281, 483, 411, 264, 881, 295, 264, 3035, 11, 51848], "temperature": 0.0, "avg_logprob": -0.12285585140963214, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.0007432496640831232}, {"id": 943, "seek": 552320, "start": 5523.2, "end": 5528.4, "text": " the most of the book. But I think that other than that, any basic, you know, any basic", "tokens": [50364, 264, 881, 295, 264, 1446, 13, 583, 286, 519, 300, 661, 813, 300, 11, 604, 3875, 11, 291, 458, 11, 604, 3875, 50624], "temperature": 0.0, "avg_logprob": -0.08940510522751581, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.0006355937803164124}, {"id": 944, "seek": 552320, "start": 5528.4, "end": 5534.0, "text": " knowledge of linear algebra, statistics and analysis will do. So like, if you have taken", "tokens": [50624, 3601, 295, 8213, 21989, 11, 12523, 293, 5215, 486, 360, 13, 407, 411, 11, 498, 291, 362, 2726, 50904], "temperature": 0.0, "avg_logprob": -0.08940510522751581, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.0006355937803164124}, {"id": 945, "seek": 552320, "start": 5534.0, "end": 5539.679999999999, "text": " a graduate level class in machine learning, you should be ready to go. Rather than just being", "tokens": [50904, 257, 8080, 1496, 1508, 294, 3479, 2539, 11, 291, 820, 312, 1919, 281, 352, 13, 16571, 813, 445, 885, 51188], "temperature": 0.0, "avg_logprob": -0.08940510522751581, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.0006355937803164124}, {"id": 946, "seek": 552320, "start": 5539.679999999999, "end": 5546.08, "text": " applied in a lot of really important branches of research problems, people outside of pure", "tokens": [51188, 6456, 294, 257, 688, 295, 534, 1021, 14770, 295, 2132, 2740, 11, 561, 2380, 295, 6075, 51508], "temperature": 0.0, "avg_logprob": -0.08940510522751581, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.0006355937803164124}, {"id": 947, "seek": 552320, "start": 5546.08, "end": 5552.16, "text": " research have recognized that a lot of the data that comes to us from nature is most naturally", "tokens": [51508, 2132, 362, 9823, 300, 257, 688, 295, 264, 1412, 300, 1487, 281, 505, 490, 3687, 307, 881, 8195, 51812], "temperature": 0.0, "avg_logprob": -0.08940510522751581, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.0006355937803164124}, {"id": 948, "seek": 555216, "start": 5552.16, "end": 5556.639999999999, "text": " represented in a graph-structured form. Very rarely will nature give us something", "tokens": [50364, 10379, 294, 257, 4295, 12, 372, 46847, 1254, 13, 4372, 13752, 486, 3687, 976, 505, 746, 50588], "temperature": 0.0, "avg_logprob": -0.09604841991535669, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006002625450491905}, {"id": 949, "seek": 555216, "start": 5556.639999999999, "end": 5562.08, "text": " that can be representable as an image or a sequence. That's super rare. So very often,", "tokens": [50588, 300, 393, 312, 2906, 712, 382, 364, 3256, 420, 257, 8310, 13, 663, 311, 1687, 5892, 13, 407, 588, 2049, 11, 50860], "temperature": 0.0, "avg_logprob": -0.09604841991535669, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006002625450491905}, {"id": 950, "seek": 555216, "start": 5562.08, "end": 5567.2, "text": " the structure is more irregular, more graph-like. And therefore, graph neural networks have already", "tokens": [50860, 264, 3877, 307, 544, 29349, 11, 544, 4295, 12, 4092, 13, 400, 4412, 11, 4295, 18161, 9590, 362, 1217, 51116], "temperature": 0.0, "avg_logprob": -0.09604841991535669, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006002625450491905}, {"id": 951, "seek": 555216, "start": 5567.2, "end": 5573.36, "text": " seen a lot of applications in domains where the data is supernaturally represented as a graph.", "tokens": [51116, 1612, 257, 688, 295, 5821, 294, 25514, 689, 264, 1412, 307, 1687, 12241, 6512, 10379, 382, 257, 4295, 13, 51424], "temperature": 0.0, "avg_logprob": -0.09604841991535669, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006002625450491905}, {"id": 952, "seek": 555216, "start": 5573.36, "end": 5577.92, "text": " In the domain of computational chemistry, where you can represent molecules as graphs of atoms", "tokens": [51424, 682, 264, 9274, 295, 28270, 12558, 11, 689, 291, 393, 2906, 13093, 382, 24877, 295, 16871, 51652], "temperature": 0.0, "avg_logprob": -0.09604841991535669, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006002625450491905}, {"id": 953, "seek": 557792, "start": 5577.92, "end": 5583.04, "text": " and bonds between them, the graph neural networks have already proven impactful in detecting", "tokens": [50364, 293, 14713, 1296, 552, 11, 264, 4295, 18161, 9590, 362, 1217, 12785, 30842, 294, 40237, 50620], "temperature": 0.0, "avg_logprob": -0.06375982450402301, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0031228207517415285}, {"id": 954, "seek": 557792, "start": 5584.16, "end": 5588.56, "text": " novel potent antibiotics that previously were completely overlooked because of their unusual", "tokens": [50676, 7613, 27073, 26922, 300, 8046, 645, 2584, 32269, 570, 295, 641, 10901, 50896], "temperature": 0.0, "avg_logprob": -0.06375982450402301, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0031228207517415285}, {"id": 955, "seek": 557792, "start": 5588.56, "end": 5594.32, "text": " structure. In the area of chip design, graph neural networks are powering systems that are", "tokens": [50896, 3877, 13, 682, 264, 1859, 295, 11409, 1715, 11, 4295, 18161, 9590, 366, 1347, 278, 3652, 300, 366, 51184], "temperature": 0.0, "avg_logprob": -0.06375982450402301, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0031228207517415285}, {"id": 956, "seek": 557792, "start": 5594.32, "end": 5600.4800000000005, "text": " developing the latest generation of Google's machine learning chips, the TPU. Furthermore,", "tokens": [51184, 6416, 264, 6792, 5125, 295, 3329, 311, 3479, 2539, 11583, 11, 264, 314, 8115, 13, 23999, 11, 51492], "temperature": 0.0, "avg_logprob": -0.06375982450402301, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0031228207517415285}, {"id": 957, "seek": 557792, "start": 5600.4800000000005, "end": 5607.52, "text": " graph-structured data is super ubiquitous in social networks and the kinds of networks maintained by", "tokens": [51492, 4295, 12, 372, 46847, 1412, 307, 1687, 43868, 39831, 294, 2093, 9590, 293, 264, 3685, 295, 9590, 17578, 538, 51844], "temperature": 0.0, "avg_logprob": -0.06375982450402301, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0031228207517415285}, {"id": 958, "seek": 560792, "start": 5608.4800000000005, "end": 5614.96, "text": " many big industry players. And accordingly, graph neural networks are already used to serve various", "tokens": [50392, 867, 955, 3518, 4150, 13, 400, 19717, 11, 4295, 18161, 9590, 366, 1217, 1143, 281, 4596, 3683, 50716], "temperature": 0.0, "avg_logprob": -0.05018470400855655, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.0043281870894134045}, {"id": 959, "seek": 560792, "start": 5614.96, "end": 5621.36, "text": " kinds of content in production to billions of users on a daily basis. In fact, the recommendation", "tokens": [50716, 3685, 295, 2701, 294, 4265, 281, 17375, 295, 5022, 322, 257, 5212, 5143, 13, 682, 1186, 11, 264, 11879, 51036], "temperature": 0.0, "avg_logprob": -0.05018470400855655, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.0043281870894134045}, {"id": 960, "seek": 560792, "start": 5621.36, "end": 5627.6, "text": " system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation", "tokens": [51036, 1185, 412, 37986, 11, 264, 1674, 11879, 1185, 412, 6795, 11, 382, 731, 382, 264, 1755, 11879, 51348], "temperature": 0.0, "avg_logprob": -0.05018470400855655, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.0043281870894134045}, {"id": 961, "seek": 560792, "start": 5627.6, "end": 5633.92, "text": " system for Uber Eats, all of them are powered using a graph neural network that helps serve the most", "tokens": [51348, 1185, 337, 21839, 462, 1720, 11, 439, 295, 552, 366, 17786, 1228, 257, 4295, 18161, 3209, 300, 3665, 4596, 264, 881, 51664], "temperature": 0.0, "avg_logprob": -0.05018470400855655, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.0043281870894134045}, {"id": 962, "seek": 563392, "start": 5633.92, "end": 5640.0, "text": " relevant content to users on a daily basis. And on a slightly personal note, graph neural networks", "tokens": [50364, 7340, 2701, 281, 5022, 322, 257, 5212, 5143, 13, 400, 322, 257, 4748, 2973, 3637, 11, 4295, 18161, 9590, 50668], "temperature": 0.0, "avg_logprob": -0.06405589239937919, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.022959336638450623}, {"id": 963, "seek": 563392, "start": 5640.0, "end": 5646.72, "text": " have also been used to significantly improve travel time predictions in Google Maps, which", "tokens": [50668, 362, 611, 668, 1143, 281, 10591, 3470, 3147, 565, 21264, 294, 3329, 28978, 11, 597, 51004], "temperature": 0.0, "avg_logprob": -0.06405589239937919, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.022959336638450623}, {"id": 964, "seek": 563392, "start": 5646.72, "end": 5652.24, "text": " is used also by billions of people every day. So whenever you type a query, how do I get from", "tokens": [51004, 307, 1143, 611, 538, 17375, 295, 561, 633, 786, 13, 407, 5699, 291, 2010, 257, 14581, 11, 577, 360, 286, 483, 490, 51280], "temperature": 0.0, "avg_logprob": -0.06405589239937919, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.022959336638450623}, {"id": 965, "seek": 563392, "start": 5652.24, "end": 5658.16, "text": " point A to point B in the most efficient way? The travel time prediction that you get is powered", "tokens": [51280, 935, 316, 281, 935, 363, 294, 264, 881, 7148, 636, 30, 440, 3147, 565, 17630, 300, 291, 483, 307, 17786, 51576], "temperature": 0.0, "avg_logprob": -0.06405589239937919, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.022959336638450623}, {"id": 966, "seek": 563392, "start": 5658.16, "end": 5662.8, "text": " by a graph neural network that we have developed that defined in collaboration with the Google Maps", "tokens": [51576, 538, 257, 4295, 18161, 3209, 300, 321, 362, 4743, 300, 7642, 294, 9363, 365, 264, 3329, 28978, 51808], "temperature": 0.0, "avg_logprob": -0.06405589239937919, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.022959336638450623}, {"id": 967, "seek": 566280, "start": 5662.8, "end": 5669.2, "text": " team. And this is of high importance not only to users that use the app on a daily basis to find", "tokens": [50364, 1469, 13, 400, 341, 307, 295, 1090, 7379, 406, 787, 281, 5022, 300, 764, 264, 724, 322, 257, 5212, 5143, 281, 915, 50684], "temperature": 0.0, "avg_logprob": -0.0580753530009409, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.009122717194259167}, {"id": 968, "seek": 566280, "start": 5669.2, "end": 5674.64, "text": " the most efficient way to travel. It's also used by the various companies that leverage the Maps", "tokens": [50684, 264, 881, 7148, 636, 281, 3147, 13, 467, 311, 611, 1143, 538, 264, 3683, 3431, 300, 13982, 264, 28978, 50956], "temperature": 0.0, "avg_logprob": -0.0580753530009409, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.009122717194259167}, {"id": 969, "seek": 566280, "start": 5674.64, "end": 5681.360000000001, "text": " API so they can tell their customers what's the time it will take for a certain vehicle to arrive", "tokens": [50956, 9362, 370, 436, 393, 980, 641, 4581, 437, 311, 264, 565, 309, 486, 747, 337, 257, 1629, 5864, 281, 8881, 51292], "temperature": 0.0, "avg_logprob": -0.0580753530009409, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.009122717194259167}, {"id": 970, "seek": 566280, "start": 5681.360000000001, "end": 5686.24, "text": " to them. So companies such as food delivery companies and ride-sharing companies have also", "tokens": [51292, 281, 552, 13, 407, 3431, 1270, 382, 1755, 8982, 3431, 293, 5077, 12, 2716, 1921, 3431, 362, 611, 51536], "temperature": 0.0, "avg_logprob": -0.0580753530009409, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.009122717194259167}, {"id": 971, "seek": 568624, "start": 5686.32, "end": 5692.88, "text": " extensively profited from this system, which in cities such as Sydney has reduced the relative", "tokens": [50368, 32636, 1740, 1226, 490, 341, 1185, 11, 597, 294, 6486, 1270, 382, 21065, 575, 9212, 264, 4972, 50696], "temperature": 0.0, "avg_logprob": -0.06898088332934257, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.03256000950932503}, {"id": 972, "seek": 568624, "start": 5692.88, "end": 5697.92, "text": " amount of negative user outcomes in terms of badly predicted travel times by over 40%,", "tokens": [50696, 2372, 295, 3671, 4195, 10070, 294, 2115, 295, 13425, 19147, 3147, 1413, 538, 670, 3356, 8923, 50948], "temperature": 0.0, "avg_logprob": -0.06898088332934257, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.03256000950932503}, {"id": 973, "seek": 568624, "start": 5698.8, "end": 5703.92, "text": " making it one way in which graph representation learning techniques that I have co-developed", "tokens": [50992, 1455, 309, 472, 636, 294, 597, 4295, 10290, 2539, 7512, 300, 286, 362, 598, 12, 35464, 292, 51248], "temperature": 0.0, "avg_logprob": -0.06898088332934257, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.03256000950932503}, {"id": 974, "seek": 568624, "start": 5703.92, "end": 5711.5199999999995, "text": " are actively impacting billions of people on a daily basis. When I was an undergraduate student,", "tokens": [51248, 366, 13022, 29963, 17375, 295, 561, 322, 257, 5212, 5143, 13, 1133, 286, 390, 364, 19113, 3107, 11, 51628], "temperature": 0.0, "avg_logprob": -0.06898088332934257, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.03256000950932503}, {"id": 975, "seek": 571152, "start": 5711.52, "end": 5716.8, "text": " I was interested in image processing and was excited about variational methods.", "tokens": [50364, 286, 390, 3102, 294, 3256, 9007, 293, 390, 2919, 466, 3034, 1478, 7150, 13, 50628], "temperature": 0.0, "avg_logprob": -0.05698762190969367, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.01200934499502182}, {"id": 976, "seek": 571152, "start": 5717.92, "end": 5723.120000000001, "text": " I think it's a very elegant idea that you can define a functional that serves as a model for", "tokens": [50684, 286, 519, 309, 311, 257, 588, 21117, 1558, 300, 291, 393, 6964, 257, 11745, 300, 13451, 382, 257, 2316, 337, 50944], "temperature": 0.0, "avg_logprob": -0.05698762190969367, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.01200934499502182}, {"id": 977, "seek": 571152, "start": 5723.120000000001, "end": 5728.96, "text": " your ideal image and then use the optimality conditions to derive a differential equation that", "tokens": [50944, 428, 7157, 3256, 293, 550, 764, 264, 5028, 1860, 4487, 281, 28446, 257, 15756, 5367, 300, 51236], "temperature": 0.0, "avg_logprob": -0.05698762190969367, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.01200934499502182}, {"id": 978, "seek": 571152, "start": 5728.96, "end": 5734.160000000001, "text": " flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel,", "tokens": [51236, 12867, 3030, 264, 39326, 13, 400, 257, 4098, 1627, 3109, 390, 10348, 538, 9949, 5652, 10909, 11, 51496], "temperature": 0.0, "avg_logprob": -0.05698762190969367, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.01200934499502182}, {"id": 979, "seek": 571152, "start": 5734.72, "end": 5738.96, "text": " where you could think of an image as a manifold or a high-dimensional surface", "tokens": [51524, 689, 291, 727, 519, 295, 364, 3256, 382, 257, 47138, 420, 257, 1090, 12, 18759, 3753, 51736], "temperature": 0.0, "avg_logprob": -0.05698762190969367, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.01200934499502182}, {"id": 980, "seek": 573896, "start": 5738.96, "end": 5745.2, "text": " and use an energy that originated in string theory and particle physics to derive a non-euclidean", "tokens": [50364, 293, 764, 364, 2281, 300, 31129, 294, 6798, 5261, 293, 12359, 10649, 281, 28446, 257, 2107, 12, 68, 1311, 31264, 282, 50676], "temperature": 0.0, "avg_logprob": -0.10737731864860466, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.007320432458072901}, {"id": 981, "seek": 573896, "start": 5745.2, "end": 5751.12, "text": " diffusion PDE called Beltrami Flow that acts as a non-linear image filter. And this is what", "tokens": [50676, 25242, 10464, 36, 1219, 38869, 2356, 72, 32792, 300, 10672, 382, 257, 2107, 12, 28263, 3256, 6608, 13, 400, 341, 307, 437, 50972], "temperature": 0.0, "avg_logprob": -0.10737731864860466, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.007320432458072901}, {"id": 982, "seek": 573896, "start": 5751.12, "end": 5756.88, "text": " made me fall in love with differential geometry and I did a PhD with Ron on this topic. And I think", "tokens": [50972, 1027, 385, 2100, 294, 959, 365, 15756, 18426, 293, 286, 630, 257, 14476, 365, 9949, 322, 341, 4829, 13, 400, 286, 519, 51260], "temperature": 0.0, "avg_logprob": -0.10737731864860466, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.007320432458072901}, {"id": 983, "seek": 573896, "start": 5756.88, "end": 5762.08, "text": " these were really beautiful and deep ideas that unfortunately now are almost forgotten in the", "tokens": [51260, 613, 645, 534, 2238, 293, 2452, 3487, 300, 7015, 586, 366, 1920, 11832, 294, 264, 51520], "temperature": 0.0, "avg_logprob": -0.10737731864860466, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.007320432458072901}, {"id": 984, "seek": 573896, "start": 5762.08, "end": 5766.96, "text": " era of deep learning. And it's a pity that the machine learning research community has such a", "tokens": [51520, 4249, 295, 2452, 2539, 13, 400, 309, 311, 257, 21103, 300, 264, 3479, 2539, 2132, 1768, 575, 1270, 257, 51764], "temperature": 0.0, "avg_logprob": -0.10737731864860466, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.007320432458072901}, {"id": 985, "seek": 576696, "start": 5766.96, "end": 5773.2, "text": " short memory because many modern concepts have really ancient roots. Ironically, we've recently", "tokens": [50364, 2099, 4675, 570, 867, 4363, 10392, 362, 534, 7832, 10669, 13, 13720, 984, 11, 321, 600, 3938, 50676], "temperature": 0.0, "avg_logprob": -0.06654493288062084, "compression_ratio": 1.55, "no_speech_prob": 0.0025492568966001272}, {"id": 986, "seek": 576696, "start": 5773.2, "end": 5778.88, "text": " used non-euclidean diffusion equations as a way to reinterpret graph neural networks as neural", "tokens": [50676, 1143, 2107, 12, 68, 1311, 31264, 282, 25242, 11787, 382, 257, 636, 281, 319, 41935, 4295, 18161, 9590, 382, 18161, 50960], "temperature": 0.0, "avg_logprob": -0.06654493288062084, "compression_ratio": 1.55, "no_speech_prob": 0.0025492568966001272}, {"id": 987, "seek": 576696, "start": 5778.88, "end": 5784.88, "text": " PDEs. And I think it really helps sometimes to have a longer time window. Now, equivariated", "tokens": [50960, 10464, 20442, 13, 400, 286, 519, 309, 534, 3665, 2171, 281, 362, 257, 2854, 565, 4910, 13, 823, 11, 1267, 592, 3504, 770, 51260], "temperature": 0.0, "avg_logprob": -0.06654493288062084, "compression_ratio": 1.55, "no_speech_prob": 0.0025492568966001272}, {"id": 988, "seek": 576696, "start": 5784.88, "end": 5791.44, "text": " networks tend to generalize much better and require much less data if the data indeed has", "tokens": [51260, 9590, 3928, 281, 2674, 1125, 709, 1101, 293, 3651, 709, 1570, 1412, 498, 264, 1412, 6451, 575, 51588], "temperature": 0.0, "avg_logprob": -0.06654493288062084, "compression_ratio": 1.55, "no_speech_prob": 0.0025492568966001272}, {"id": 989, "seek": 579144, "start": 5791.44, "end": 5797.2, "text": " the symmetry that you assumed in your model. But people often ask, you know, why do we even", "tokens": [50364, 264, 25440, 300, 291, 15895, 294, 428, 2316, 13, 583, 561, 2049, 1029, 11, 291, 458, 11, 983, 360, 321, 754, 50652], "temperature": 0.0, "avg_logprob": -0.10711545681734698, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06949462741613388}, {"id": 990, "seek": 579144, "start": 5797.2, "end": 5802.32, "text": " care about data efficiency when we can just collect more data? We live in the era of big data, right?", "tokens": [50652, 1127, 466, 1412, 10493, 562, 321, 393, 445, 2500, 544, 1412, 30, 492, 1621, 294, 264, 4249, 295, 955, 1412, 11, 558, 30, 50908], "temperature": 0.0, "avg_logprob": -0.10711545681734698, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06949462741613388}, {"id": 991, "seek": 579144, "start": 5803.2, "end": 5807.679999999999, "text": " And I think the answer why you might still be interested in data efficiency. First of all,", "tokens": [50952, 400, 286, 519, 264, 1867, 983, 291, 1062, 920, 312, 3102, 294, 1412, 10493, 13, 2386, 295, 439, 11, 51176], "temperature": 0.0, "avg_logprob": -0.10711545681734698, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06949462741613388}, {"id": 992, "seek": 579144, "start": 5807.679999999999, "end": 5813.919999999999, "text": " there are applications like say medical imaging, where acquiring labeled data simply is very cost.", "tokens": [51176, 456, 366, 5821, 411, 584, 4625, 25036, 11, 689, 37374, 21335, 1412, 2935, 307, 588, 2063, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10711545681734698, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06949462741613388}, {"id": 993, "seek": 579144, "start": 5813.919999999999, "end": 5818.639999999999, "text": " You have to get patients, you're dealing with privacy restrictions, you're dealing with", "tokens": [51488, 509, 362, 281, 483, 4209, 11, 291, 434, 6260, 365, 11427, 14191, 11, 291, 434, 6260, 365, 51724], "temperature": 0.0, "avg_logprob": -0.10711545681734698, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06949462741613388}, {"id": 994, "seek": 581864, "start": 5818.64, "end": 5823.6, "text": " costly, highly trained doctors who have to annotate the data, come together in a committee to", "tokens": [50364, 28328, 11, 5405, 8895, 8778, 567, 362, 281, 25339, 473, 264, 1412, 11, 808, 1214, 294, 257, 7482, 281, 50612], "temperature": 0.0, "avg_logprob": -0.0875790013676196, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.017703181132674217}, {"id": 995, "seek": 581864, "start": 5823.6, "end": 5829.52, "text": " decide on questionable cases and so forth. So this is very expensive. And if you can improve the", "tokens": [50612, 4536, 322, 37158, 3331, 293, 370, 5220, 13, 407, 341, 307, 588, 5124, 13, 400, 498, 291, 393, 3470, 264, 50908], "temperature": 0.0, "avg_logprob": -0.0875790013676196, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.017703181132674217}, {"id": 996, "seek": 581864, "start": 5829.52, "end": 5835.4400000000005, "text": " data efficiency by a factor of two or 10, or whatever it may be, you might just take a problem", "tokens": [50908, 1412, 10493, 538, 257, 5952, 295, 732, 420, 1266, 11, 420, 2035, 309, 815, 312, 11, 291, 1062, 445, 747, 257, 1154, 51204], "temperature": 0.0, "avg_logprob": -0.0875790013676196, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.017703181132674217}, {"id": 997, "seek": 581864, "start": 5835.4400000000005, "end": 5840.72, "text": " that was in the realm of economically infeasible and take it into the realm of the economically", "tokens": [51204, 300, 390, 294, 264, 15355, 295, 26811, 1536, 68, 296, 964, 293, 747, 309, 666, 264, 15355, 295, 264, 26811, 51468], "temperature": 0.0, "avg_logprob": -0.0875790013676196, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.017703181132674217}, {"id": 998, "seek": 581864, "start": 5840.72, "end": 5847.52, "text": " feasible, which is a very useful thing. There are other cases like graph neural nets, where the", "tokens": [51468, 26648, 11, 597, 307, 257, 588, 4420, 551, 13, 821, 366, 661, 3331, 411, 4295, 18161, 36170, 11, 689, 264, 51808], "temperature": 0.0, "avg_logprob": -0.0875790013676196, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.017703181132674217}, {"id": 999, "seek": 584752, "start": 5847.6, "end": 5853.360000000001, "text": " group of symmetries is so large, in this case, n factorial number of permutations,", "tokens": [50368, 1594, 295, 14232, 302, 2244, 307, 370, 2416, 11, 294, 341, 1389, 11, 297, 36916, 1230, 295, 4784, 325, 763, 11, 50656], "temperature": 0.0, "avg_logprob": -0.14113643026759481, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.0020502714905887842}, {"id": 1000, "seek": 584752, "start": 5853.360000000001, "end": 5860.0, "text": " that no amount of data or data augmentation in practice is going to allow you to learn the", "tokens": [50656, 300, 572, 2372, 295, 1412, 420, 1412, 14501, 19631, 294, 3124, 307, 516, 281, 2089, 291, 281, 1466, 264, 50988], "temperature": 0.0, "avg_logprob": -0.14113643026759481, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.0020502714905887842}, {"id": 1001, "seek": 584752, "start": 5860.0, "end": 5865.4400000000005, "text": " symmetry or to learn the invariance or equivariate in your NAPO. So indeed, you see that in this", "tokens": [50988, 25440, 420, 281, 1466, 264, 33270, 719, 420, 1267, 592, 3504, 473, 294, 428, 426, 4715, 46, 13, 407, 6451, 11, 291, 536, 300, 294, 341, 51260], "temperature": 0.0, "avg_logprob": -0.14113643026759481, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.0020502714905887842}, {"id": 1002, "seek": 584752, "start": 5865.4400000000005, "end": 5870.88, "text": " space of graph neural nets, everybody uses equivariate permutation, equivariate network", "tokens": [51260, 1901, 295, 4295, 18161, 36170, 11, 2201, 4960, 1267, 592, 3504, 473, 4784, 11380, 11, 1267, 592, 3504, 473, 3209, 51532], "temperature": 0.0, "avg_logprob": -0.14113643026759481, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.0020502714905887842}, {"id": 1003, "seek": 584752, "start": 5870.88, "end": 5876.160000000001, "text": " architectures. And then finally, you can think about the grand problems of AGI, artificial", "tokens": [51532, 6331, 1303, 13, 400, 550, 2721, 11, 291, 393, 519, 466, 264, 2697, 2740, 295, 316, 26252, 11, 11677, 51796], "temperature": 0.0, "avg_logprob": -0.14113643026759481, "compression_ratio": 1.713740458015267, "no_speech_prob": 0.0020502714905887842}, {"id": 1004, "seek": 587616, "start": 5876.16, "end": 5883.28, "text": " general intelligence and so forth. And here I think that we will most certainly need large data sets,", "tokens": [50364, 2674, 7599, 293, 370, 5220, 13, 400, 510, 286, 519, 300, 321, 486, 881, 3297, 643, 2416, 1412, 6352, 11, 50720], "temperature": 0.0, "avg_logprob": -0.08748786393986192, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.012807176448404789}, {"id": 1005, "seek": 587616, "start": 5885.04, "end": 5890.8, "text": " large networks, a lot of compute power, and so forth. The current architectures we have clearly", "tokens": [50808, 2416, 9590, 11, 257, 688, 295, 14722, 1347, 11, 293, 370, 5220, 13, 440, 2190, 6331, 1303, 321, 362, 4448, 51096], "temperature": 0.0, "avg_logprob": -0.08748786393986192, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.012807176448404789}, {"id": 1006, "seek": 587616, "start": 5890.8, "end": 5898.4, "text": " are performing far fewer computations than the human brain. So there's a ways to go there.", "tokens": [51096, 366, 10205, 1400, 13366, 2807, 763, 813, 264, 1952, 3567, 13, 407, 456, 311, 257, 2098, 281, 352, 456, 13, 51476], "temperature": 0.0, "avg_logprob": -0.08748786393986192, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.012807176448404789}, {"id": 1007, "seek": 587616, "start": 5898.4, "end": 5905.5199999999995, "text": " But I also think that one essential characteristic of intelligence is the ability to learn quickly", "tokens": [51476, 583, 286, 611, 519, 300, 472, 7115, 16282, 295, 7599, 307, 264, 3485, 281, 1466, 2661, 51832], "temperature": 0.0, "avg_logprob": -0.08748786393986192, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.012807176448404789}, {"id": 1008, "seek": 590552, "start": 5905.6, "end": 5912.64, "text": " in new situations, situations that are not similar to the ones you've seen in your training data.", "tokens": [50368, 294, 777, 6851, 11, 6851, 300, 366, 406, 2531, 281, 264, 2306, 291, 600, 1612, 294, 428, 3097, 1412, 13, 50720], "temperature": 0.0, "avg_logprob": -0.06530817075707447, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.005380536429584026}, {"id": 1009, "seek": 590552, "start": 5913.52, "end": 5918.88, "text": " And so data efficiency to me is an essential characteristic of intelligence. It's almost", "tokens": [50764, 400, 370, 1412, 10493, 281, 385, 307, 364, 7115, 16282, 295, 7599, 13, 467, 311, 1920, 51032], "temperature": 0.0, "avg_logprob": -0.06530817075707447, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.005380536429584026}, {"id": 1010, "seek": 590552, "start": 5918.88, "end": 5924.88, "text": " like an action that you want your methods to be data efficient. And so I think one of the", "tokens": [51032, 411, 364, 3069, 300, 291, 528, 428, 7150, 281, 312, 1412, 7148, 13, 400, 370, 286, 519, 472, 295, 264, 51332], "temperature": 0.0, "avg_logprob": -0.06530817075707447, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.005380536429584026}, {"id": 1011, "seek": 590552, "start": 5924.88, "end": 5931.84, "text": " big challenges for the field right now is to try to think of very generic priors, priors that", "tokens": [51332, 955, 4759, 337, 264, 2519, 558, 586, 307, 281, 853, 281, 519, 295, 588, 19577, 1790, 830, 11, 1790, 830, 300, 51680], "temperature": 0.0, "avg_logprob": -0.06530817075707447, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.005380536429584026}, {"id": 1012, "seek": 593184, "start": 5931.84, "end": 5938.8, "text": " apply in a wide range of situations. And to give you, even though they're generic and abstract,", "tokens": [50364, 3079, 294, 257, 4874, 3613, 295, 6851, 13, 400, 281, 976, 291, 11, 754, 1673, 436, 434, 19577, 293, 12649, 11, 50712], "temperature": 0.0, "avg_logprob": -0.07707167086393937, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.005284799262881279}, {"id": 1013, "seek": 593184, "start": 5938.8, "end": 5944.08, "text": " they give you a lot of bang for the buck in terms of improved generalization and data efficiency.", "tokens": [50712, 436, 976, 291, 257, 688, 295, 8550, 337, 264, 14894, 294, 2115, 295, 9689, 2674, 2144, 293, 1412, 10493, 13, 50976], "temperature": 0.0, "avg_logprob": -0.07707167086393937, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.005284799262881279}, {"id": 1014, "seek": 593184, "start": 5944.08, "end": 5950.08, "text": " I think that the beauty of science and research is in connecting the dots. And I find it fascinating", "tokens": [50976, 286, 519, 300, 264, 6643, 295, 3497, 293, 2132, 307, 294, 11015, 264, 15026, 13, 400, 286, 915, 309, 10343, 51276], "temperature": 0.0, "avg_logprob": -0.07707167086393937, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.005284799262881279}, {"id": 1015, "seek": 593184, "start": 5950.08, "end": 5954.4800000000005, "text": " that, for example, graph neural networks are connected to the work of Weissfeller and Lehmann", "tokens": [51276, 300, 11, 337, 1365, 11, 4295, 18161, 9590, 366, 4582, 281, 264, 589, 295, 492, 891, 69, 14983, 293, 1456, 8587, 969, 51496], "temperature": 0.0, "avg_logprob": -0.07707167086393937, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.005284799262881279}, {"id": 1016, "seek": 593184, "start": 5954.4800000000005, "end": 5960.16, "text": " from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry.", "tokens": [51496, 490, 264, 4060, 82, 322, 307, 32702, 1434, 4997, 11, 597, 294, 1261, 390, 7547, 538, 2740, 294, 12558, 13, 51780], "temperature": 0.0, "avg_logprob": -0.07707167086393937, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.005284799262881279}, {"id": 1017, "seek": 596016, "start": 5960.96, "end": 5966.16, "text": " And chemistry was also the field that drove the research into modern formulation of", "tokens": [50404, 400, 12558, 390, 611, 264, 2519, 300, 13226, 264, 2132, 666, 4363, 37642, 295, 50664], "temperature": 0.0, "avg_logprob": -0.08912306415791414, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.006871775723993778}, {"id": 1018, "seek": 596016, "start": 5966.16, "end": 5971.76, "text": " diffusion equations that were adopted in image processing community in the 90s and came back", "tokens": [50664, 25242, 11787, 300, 645, 12175, 294, 3256, 9007, 1768, 294, 264, 4289, 82, 293, 1361, 646, 50944], "temperature": 0.0, "avg_logprob": -0.08912306415791414, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.006871775723993778}, {"id": 1019, "seek": 596016, "start": 5971.76, "end": 5977.68, "text": " recently as a way to reinterpret graph neural networks. And I think such connections give really", "tokens": [50944, 3938, 382, 257, 636, 281, 319, 41935, 4295, 18161, 9590, 13, 400, 286, 519, 1270, 9271, 976, 534, 51240], "temperature": 0.0, "avg_logprob": -0.08912306415791414, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.006871775723993778}, {"id": 1020, "seek": 596016, "start": 5977.68, "end": 5983.44, "text": " a new and deep perspective. And probably the deeper you dive, the broader they become. But", "tokens": [51240, 257, 777, 293, 2452, 4585, 13, 400, 1391, 264, 7731, 291, 9192, 11, 264, 13227, 436, 1813, 13, 583, 51528], "temperature": 0.0, "avg_logprob": -0.08912306415791414, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.006871775723993778}, {"id": 1021, "seek": 596016, "start": 5983.44, "end": 5989.36, "text": " it's really an ever-ending story. Today is an incredibly special episode and we're filming", "tokens": [51528, 309, 311, 534, 364, 1562, 12, 2029, 1657, 13, 2692, 307, 364, 6252, 2121, 3500, 293, 321, 434, 8869, 51824], "temperature": 0.0, "avg_logprob": -0.08912306415791414, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.006871775723993778}, {"id": 1022, "seek": 598936, "start": 5989.36, "end": 5995.44, "text": " at 9 o'clock in the morning. It's really rare for us to film this early when I'm still caffeinated.", "tokens": [50364, 412, 1722, 277, 6, 9023, 294, 264, 2446, 13, 467, 311, 534, 5892, 337, 505, 281, 2007, 341, 2440, 562, 286, 478, 920, 29118, 5410, 13, 50668], "temperature": 0.0, "avg_logprob": -0.10862170525316922, "compression_ratio": 1.5177304964539007, "no_speech_prob": 0.004529778845608234}, {"id": 1023, "seek": 598936, "start": 5995.44, "end": 5999.839999999999, "text": " Many of our guests are over in the States. It's an absolute honor to have you both on MLST.", "tokens": [50668, 5126, 295, 527, 9804, 366, 670, 294, 264, 3040, 13, 467, 311, 364, 8236, 5968, 281, 362, 291, 1293, 322, 21601, 6840, 13, 50888], "temperature": 0.0, "avg_logprob": -0.10862170525316922, "compression_ratio": 1.5177304964539007, "no_speech_prob": 0.004529778845608234}, {"id": 1024, "seek": 598936, "start": 5999.839999999999, "end": 6004.24, "text": " And Professor Bronstein, could you start by briefly telling us how the young mathematician", "tokens": [50888, 400, 8419, 19544, 9089, 11, 727, 291, 722, 538, 10515, 3585, 505, 577, 264, 2037, 48281, 51108], "temperature": 0.0, "avg_logprob": -0.10862170525316922, "compression_ratio": 1.5177304964539007, "no_speech_prob": 0.004529778845608234}, {"id": 1025, "seek": 598936, "start": 6004.24, "end": 6008.32, "text": " Enne Offer used symmetries to discover the conservation laws in physics?", "tokens": [51108, 2193, 716, 6318, 260, 1143, 14232, 302, 2244, 281, 4411, 264, 16185, 6064, 294, 10649, 30, 51312], "temperature": 0.0, "avg_logprob": -0.10862170525316922, "compression_ratio": 1.5177304964539007, "no_speech_prob": 0.004529778845608234}, {"id": 1026, "seek": 598936, "start": 6009.04, "end": 6013.28, "text": " Maybe I should take a step back and describe the situation that happened", "tokens": [51348, 2704, 286, 820, 747, 257, 1823, 646, 293, 6786, 264, 2590, 300, 2011, 51560], "temperature": 0.0, "avg_logprob": -0.10862170525316922, "compression_ratio": 1.5177304964539007, "no_speech_prob": 0.004529778845608234}, {"id": 1027, "seek": 601328, "start": 6014.0, "end": 6020.48, "text": " in the field of geometry towards the end of the 19th century. And it was an incredibly fruitful", "tokens": [50400, 294, 264, 2519, 295, 18426, 3030, 264, 917, 295, 264, 1294, 392, 4901, 13, 400, 309, 390, 364, 6252, 49795, 50724], "temperature": 0.0, "avg_logprob": -0.08479610869759008, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.025728847831487656}, {"id": 1028, "seek": 601328, "start": 6021.2, "end": 6028.96, "text": " period of time for mathematicians working in this field with the discovery and development of", "tokens": [50760, 2896, 295, 565, 337, 32811, 2567, 1364, 294, 341, 2519, 365, 264, 12114, 293, 3250, 295, 51148], "temperature": 0.0, "avg_logprob": -0.08479610869759008, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.025728847831487656}, {"id": 1029, "seek": 601328, "start": 6028.96, "end": 6035.599999999999, "text": " different kinds of geometries. So a young mathematician based in Germany called Felix Klein", "tokens": [51148, 819, 3685, 295, 12956, 2244, 13, 407, 257, 2037, 48281, 2361, 294, 7244, 1219, 30169, 33327, 51480], "temperature": 0.0, "avg_logprob": -0.08479610869759008, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.025728847831487656}, {"id": 1030, "seek": 601328, "start": 6036.32, "end": 6042.96, "text": " proposed this quite remarkable and groundbreaking idea that you can define geometry by studying", "tokens": [51516, 10348, 341, 1596, 12802, 293, 42491, 1558, 300, 291, 393, 6964, 18426, 538, 7601, 51848], "temperature": 0.0, "avg_logprob": -0.08479610869759008, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.025728847831487656}, {"id": 1031, "seek": 604296, "start": 6042.96, "end": 6047.84, "text": " the groups of symmetries, basically the kinds of transformations to which you can subject", "tokens": [50364, 264, 3935, 295, 14232, 302, 2244, 11, 1936, 264, 3685, 295, 34852, 281, 597, 291, 393, 3983, 50608], "temperature": 0.0, "avg_logprob": -0.10855352264089682, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.008859544061124325}, {"id": 1032, "seek": 604296, "start": 6047.84, "end": 6054.96, "text": " geometric forms and seeing how different properties are preserved or not under these", "tokens": [50608, 33246, 6422, 293, 2577, 577, 819, 7221, 366, 22242, 420, 406, 833, 613, 50964], "temperature": 0.0, "avg_logprob": -0.10855352264089682, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.008859544061124325}, {"id": 1033, "seek": 604296, "start": 6054.96, "end": 6060.08, "text": " transformations. So these ideas appear to be very powerful. And what Amy Neuter showed in her", "tokens": [50964, 34852, 13, 407, 613, 3487, 4204, 281, 312, 588, 4005, 13, 400, 437, 12651, 1734, 20314, 4712, 294, 720, 51220], "temperature": 0.0, "avg_logprob": -0.10855352264089682, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.008859544061124325}, {"id": 1034, "seek": 604296, "start": 6060.08, "end": 6065.12, "text": " work, and she actually worked in the same institution where Klein ended up in G\u00f6ttingen in", "tokens": [51220, 589, 11, 293, 750, 767, 2732, 294, 264, 912, 7818, 689, 33327, 4590, 493, 294, 460, 12082, 783, 268, 294, 51472], "temperature": 0.0, "avg_logprob": -0.10855352264089682, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.008859544061124325}, {"id": 1035, "seek": 604296, "start": 6065.12, "end": 6070.64, "text": " Germany. And she showed that you can take a physical system that is described as", "tokens": [51472, 7244, 13, 400, 750, 4712, 300, 291, 393, 747, 257, 4001, 1185, 300, 307, 7619, 382, 51748], "temperature": 0.0, "avg_logprob": -0.10855352264089682, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.008859544061124325}, {"id": 1036, "seek": 607064, "start": 6071.6, "end": 6076.320000000001, "text": " functional as a variational system and associate different conservation laws", "tokens": [50412, 11745, 382, 257, 3034, 1478, 1185, 293, 14644, 819, 16185, 6064, 50648], "temperature": 0.0, "avg_logprob": -0.08715905641254626, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.011531841941177845}, {"id": 1037, "seek": 607064, "start": 6076.320000000001, "end": 6083.360000000001, "text": " with different symmetries of this system. And it was a pretty remarkable result because", "tokens": [50648, 365, 819, 14232, 302, 2244, 295, 341, 1185, 13, 400, 309, 390, 257, 1238, 12802, 1874, 570, 51000], "temperature": 0.0, "avg_logprob": -0.08715905641254626, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.011531841941177845}, {"id": 1038, "seek": 607064, "start": 6083.360000000001, "end": 6089.04, "text": " before that, conservation laws were purely empirical. You would make an experiment many times", "tokens": [51000, 949, 300, 11, 16185, 6064, 645, 17491, 31886, 13, 509, 576, 652, 364, 5120, 867, 1413, 51284], "temperature": 0.0, "avg_logprob": -0.08715905641254626, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.011531841941177845}, {"id": 1039, "seek": 607064, "start": 6089.04, "end": 6094.72, "text": " and measure, for example, the energy before or after some physical process or chemical reaction,", "tokens": [51284, 293, 3481, 11, 337, 1365, 11, 264, 2281, 949, 420, 934, 512, 4001, 1399, 420, 7313, 5480, 11, 51568], "temperature": 0.0, "avg_logprob": -0.08715905641254626, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.011531841941177845}, {"id": 1040, "seek": 607064, "start": 6094.72, "end": 6099.04, "text": " and you would come to the conclusion that the energy is preserved. So this is how, for example,", "tokens": [51568, 293, 291, 576, 808, 281, 264, 10063, 300, 264, 2281, 307, 22242, 13, 407, 341, 307, 577, 11, 337, 1365, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08715905641254626, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.011531841941177845}, {"id": 1041, "seek": 609904, "start": 6099.04, "end": 6104.72, "text": " I think Lavoisier has discovered the conservation of energy. So it was probably for the first time", "tokens": [50364, 286, 519, 30966, 7376, 811, 575, 6941, 264, 16185, 295, 2281, 13, 407, 309, 390, 1391, 337, 264, 700, 565, 50648], "temperature": 0.0, "avg_logprob": -0.07497354582244274, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.0020749885588884354}, {"id": 1042, "seek": 609904, "start": 6105.28, "end": 6110.8, "text": " that you could derive these laws from first principles. So you would need to assume in case", "tokens": [50676, 300, 291, 727, 28446, 613, 6064, 490, 700, 9156, 13, 407, 291, 576, 643, 281, 6552, 294, 1389, 50952], "temperature": 0.0, "avg_logprob": -0.07497354582244274, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.0020749885588884354}, {"id": 1043, "seek": 609904, "start": 6110.8, "end": 6116.16, "text": " of conservation of energy, the symmetry of time. We decided to take a tour into the world of", "tokens": [50952, 295, 16185, 295, 2281, 11, 264, 25440, 295, 565, 13, 492, 3047, 281, 747, 257, 3512, 666, 264, 1002, 295, 51220], "temperature": 0.0, "avg_logprob": -0.07497354582244274, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.0020749885588884354}, {"id": 1044, "seek": 609904, "start": 6116.16, "end": 6121.92, "text": " algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks", "tokens": [51220, 9284, 299, 21577, 13, 6508, 11, 291, 848, 294, 428, 9339, 300, 9284, 299, 21577, 28840, 51508], "temperature": 0.0, "avg_logprob": -0.07497354582244274, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.0020749885588884354}, {"id": 1045, "seek": 609904, "start": 6121.92, "end": 6126.72, "text": " to find neural networks that are good at imitating the classical algorithms that initially brought", "tokens": [51508, 281, 915, 18161, 9590, 300, 366, 665, 412, 566, 16350, 264, 13735, 14642, 300, 9105, 3038, 51748], "temperature": 0.0, "avg_logprob": -0.07497354582244274, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.0020749885588884354}, {"id": 1046, "seek": 612672, "start": 6126.72, "end": 6131.76, "text": " you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning,", "tokens": [50364, 291, 281, 3820, 3497, 13, 407, 291, 3938, 4736, 257, 3035, 322, 341, 1219, 1734, 1807, 35014, 6819, 13195, 39693, 278, 11, 50616], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1047, "seek": 612672, "start": 6131.76, "end": 6136.400000000001, "text": " and it's often claimed that neural networks are turing complete. And, you know, we're told that we", "tokens": [50616, 293, 309, 311, 2049, 12941, 300, 18161, 9590, 366, 256, 1345, 3566, 13, 400, 11, 291, 458, 11, 321, 434, 1907, 300, 321, 50848], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1048, "seek": 612672, "start": 6136.400000000001, "end": 6141.12, "text": " can think of training neural networks as being a kind of program search. But you argued in your", "tokens": [50848, 393, 519, 295, 3097, 18161, 9590, 382, 885, 257, 733, 295, 1461, 3164, 13, 583, 291, 20219, 294, 428, 51084], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1049, "seek": 612672, "start": 6141.12, "end": 6146.16, "text": " paper that algorithms possess fundamentally different qualities to deep learning methods.", "tokens": [51084, 3035, 300, 14642, 17490, 17879, 819, 16477, 281, 2452, 2539, 7150, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1050, "seek": 612672, "start": 6146.16, "end": 6150.08, "text": " Francois Chollet actually often points out that deep learning algorithms would struggle", "tokens": [51336, 34695, 271, 761, 1833, 302, 767, 2049, 2793, 484, 300, 2452, 2539, 14642, 576, 7799, 51532], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1051, "seek": 612672, "start": 6150.08, "end": 6155.68, "text": " to represent a sorting algorithm without learning point by point, you know, which is to say without", "tokens": [51532, 281, 2906, 257, 32411, 9284, 1553, 2539, 935, 538, 935, 11, 291, 458, 11, 597, 307, 281, 584, 1553, 51812], "temperature": 0.0, "avg_logprob": -0.11397787832444714, "compression_ratio": 1.796875, "no_speech_prob": 0.0490531399846077}, {"id": 1052, "seek": 615568, "start": 6155.68, "end": 6160.0, "text": " any generalization power whatsoever. But you seem to be making the argument that the interpolative", "tokens": [50364, 604, 2674, 2144, 1347, 17076, 13, 583, 291, 1643, 281, 312, 1455, 264, 6770, 300, 264, 44902, 1166, 50580], "temperature": 0.0, "avg_logprob": -0.07636070251464844, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.003587049199268222}, {"id": 1053, "seek": 615568, "start": 6160.0, "end": 6164.88, "text": " function space of neural networks can model algorithms more closely to real world problems,", "tokens": [50580, 2445, 1901, 295, 18161, 9590, 393, 2316, 14642, 544, 8185, 281, 957, 1002, 2740, 11, 50824], "temperature": 0.0, "avg_logprob": -0.07636070251464844, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.003587049199268222}, {"id": 1054, "seek": 615568, "start": 6164.88, "end": 6169.280000000001, "text": " potentially finding more efficient and pragmatic solutions than those classically proposed by", "tokens": [50824, 7263, 5006, 544, 7148, 293, 46904, 6547, 813, 729, 1508, 984, 10348, 538, 51044], "temperature": 0.0, "avg_logprob": -0.07636070251464844, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.003587049199268222}, {"id": 1055, "seek": 615568, "start": 6169.280000000001, "end": 6176.08, "text": " computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept", "tokens": [51044, 3820, 7708, 13, 407, 437, 311, 428, 747, 30, 865, 11, 3231, 337, 3365, 300, 11, 7172, 13, 286, 519, 264, 3410, 51384], "temperature": 0.0, "avg_logprob": -0.07636070251464844, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.003587049199268222}, {"id": 1056, "seek": 615568, "start": 6176.08, "end": 6182.72, "text": " of classical algorithms, as opposed to deep neural networks, at least the way we're currently", "tokens": [51384, 295, 13735, 14642, 11, 382, 8851, 281, 2452, 18161, 9590, 11, 412, 1935, 264, 636, 321, 434, 4362, 51716], "temperature": 0.0, "avg_logprob": -0.07636070251464844, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.003587049199268222}, {"id": 1057, "seek": 618272, "start": 6182.72, "end": 6188.56, "text": " applying them makes all these points about turing completeness a little bit moot, because there's", "tokens": [50364, 9275, 552, 1669, 439, 613, 2793, 466, 256, 1345, 1557, 15264, 257, 707, 857, 705, 310, 11, 570, 456, 311, 50656], "temperature": 0.0, "avg_logprob": -0.08763880248463482, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.02974502183496952}, {"id": 1058, "seek": 618272, "start": 6188.56, "end": 6194.320000000001, "text": " quite a few proofs out there saying that you can use neural networks or more recently graph neural", "tokens": [50656, 1596, 257, 1326, 8177, 82, 484, 456, 1566, 300, 291, 393, 764, 18161, 9590, 420, 544, 3938, 4295, 18161, 50944], "temperature": 0.0, "avg_logprob": -0.08763880248463482, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.02974502183496952}, {"id": 1059, "seek": 618272, "start": 6194.320000000001, "end": 6199.84, "text": " networks in particular to simulate a particular algorithm perfectly. But all of these proofs are", "tokens": [50944, 9590, 294, 1729, 281, 27817, 257, 1729, 9284, 6239, 13, 583, 439, 295, 613, 8177, 82, 366, 51220], "temperature": 0.0, "avg_logprob": -0.08763880248463482, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.02974502183496952}, {"id": 1060, "seek": 618272, "start": 6199.84, "end": 6205.12, "text": " sort of a best case scenario. They're basically saying, I can set the weights of my neural network", "tokens": [51220, 1333, 295, 257, 1151, 1389, 9005, 13, 814, 434, 1936, 1566, 11, 286, 393, 992, 264, 17443, 295, 452, 18161, 3209, 51484], "temperature": 0.0, "avg_logprob": -0.08763880248463482, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.02974502183496952}, {"id": 1061, "seek": 618272, "start": 6205.12, "end": 6210.400000000001, "text": " to these particular values, and voila, I am imitating the algorithm perfectly, right? So all", "tokens": [51484, 281, 613, 1729, 4190, 11, 293, 45565, 11, 286, 669, 566, 16350, 264, 9284, 6239, 11, 558, 30, 407, 439, 51748], "temperature": 0.0, "avg_logprob": -0.08763880248463482, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.02974502183496952}, {"id": 1062, "seek": 621040, "start": 6210.4, "end": 6214.96, "text": " these best case scenarios are wonderful. But in practice, we don't use this kind of best case", "tokens": [50364, 613, 1151, 1389, 15077, 366, 3715, 13, 583, 294, 3124, 11, 321, 500, 380, 764, 341, 733, 295, 1151, 1389, 50592], "temperature": 0.0, "avg_logprob": -0.04841760352805809, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.00028683821437880397}, {"id": 1063, "seek": 621040, "start": 6214.96, "end": 6220.16, "text": " optimization, we use stochastic gradient descent. And we're stuck with whatever stochastic gradient", "tokens": [50592, 19618, 11, 321, 764, 342, 8997, 2750, 16235, 23475, 13, 400, 321, 434, 5541, 365, 2035, 342, 8997, 2750, 16235, 50852], "temperature": 0.0, "avg_logprob": -0.04841760352805809, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.00028683821437880397}, {"id": 1064, "seek": 621040, "start": 6220.16, "end": 6225.5199999999995, "text": " descent gives us. So in practice, the fact that a neural network is capable for a particular setting", "tokens": [50852, 23475, 2709, 505, 13, 407, 294, 3124, 11, 264, 1186, 300, 257, 18161, 3209, 307, 8189, 337, 257, 1729, 3287, 51120], "temperature": 0.0, "avg_logprob": -0.04841760352805809, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.00028683821437880397}, {"id": 1065, "seek": 621040, "start": 6225.5199999999995, "end": 6230.5599999999995, "text": " of weights to do something doesn't mean that it will actually do that when trained from data.", "tokens": [51120, 295, 17443, 281, 360, 746, 1177, 380, 914, 300, 309, 486, 767, 360, 300, 562, 8895, 490, 1412, 13, 51372], "temperature": 0.0, "avg_logprob": -0.04841760352805809, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.00028683821437880397}, {"id": 1066, "seek": 621040, "start": 6231.36, "end": 6237.2, "text": " So essentially, this is the kind of the big divide that separates deep learning from traditional", "tokens": [51412, 407, 4476, 11, 341, 307, 264, 733, 295, 264, 955, 9845, 300, 34149, 2452, 2539, 490, 5164, 51704], "temperature": 0.0, "avg_logprob": -0.04841760352805809, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.00028683821437880397}, {"id": 1067, "seek": 623720, "start": 6237.2, "end": 6241.76, "text": " algorithms. And it has a number of other issues as well, not just the fact that we cannot find the", "tokens": [50364, 14642, 13, 400, 309, 575, 257, 1230, 295, 661, 2663, 382, 731, 11, 406, 445, 264, 1186, 300, 321, 2644, 915, 264, 50592], "temperature": 0.0, "avg_logprob": -0.06359018477718387, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.05336935818195343}, {"id": 1068, "seek": 623720, "start": 6241.76, "end": 6248.0, "text": " best case solution. Also, the fact that we are working in this high dimensional space, which is", "tokens": [50592, 1151, 1389, 3827, 13, 2743, 11, 264, 1186, 300, 321, 366, 1364, 294, 341, 1090, 18795, 1901, 11, 597, 307, 50904], "temperature": 0.0, "avg_logprob": -0.06359018477718387, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.05336935818195343}, {"id": 1069, "seek": 623720, "start": 6248.0, "end": 6253.84, "text": " not necessarily easily interpretable or composable, because you have no easy way of saying, for example,", "tokens": [50904, 406, 4725, 3612, 7302, 712, 420, 10199, 712, 11, 570, 291, 362, 572, 1858, 636, 295, 1566, 11, 337, 1365, 11, 51196], "temperature": 0.0, "avg_logprob": -0.06359018477718387, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.05336935818195343}, {"id": 1070, "seek": 623720, "start": 6253.84, "end": 6258.16, "text": " in theoretical computer science, if you want to compose two algorithms, you're working with them", "tokens": [51196, 294, 20864, 3820, 3497, 11, 498, 291, 528, 281, 35925, 732, 14642, 11, 291, 434, 1364, 365, 552, 51412], "temperature": 0.0, "avg_logprob": -0.06359018477718387, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.05336935818195343}, {"id": 1071, "seek": 623720, "start": 6258.16, "end": 6262.639999999999, "text": " in a very abstract space, which means that, you know, you can easily reason about stitching the", "tokens": [51412, 294, 257, 588, 12649, 1901, 11, 597, 1355, 300, 11, 291, 458, 11, 291, 393, 3612, 1778, 466, 30714, 264, 51636], "temperature": 0.0, "avg_logprob": -0.06359018477718387, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.05336935818195343}, {"id": 1072, "seek": 626264, "start": 6262.64, "end": 6268.72, "text": " output of one to the input of another. Whereas you cannot make that easy of a claim about latent", "tokens": [50364, 5598, 295, 472, 281, 264, 4846, 295, 1071, 13, 13813, 291, 2644, 652, 300, 1858, 295, 257, 3932, 466, 48994, 50668], "temperature": 0.0, "avg_logprob": -0.0885739281063988, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.003172139171510935}, {"id": 1073, "seek": 626264, "start": 6268.72, "end": 6273.4400000000005, "text": " spaces of two neural networks, right? So all these kinds of properties, interpretability,", "tokens": [50668, 7673, 295, 732, 18161, 9590, 11, 558, 30, 407, 439, 613, 3685, 295, 7221, 11, 7302, 2310, 11, 50904], "temperature": 0.0, "avg_logprob": -0.0885739281063988, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.003172139171510935}, {"id": 1074, "seek": 626264, "start": 6273.4400000000005, "end": 6279.360000000001, "text": " compositionality, and obviously also out of distribution generalization are plagued not", "tokens": [50904, 12686, 1860, 11, 293, 2745, 611, 484, 295, 7316, 2674, 2144, 366, 33756, 5827, 406, 51200], "temperature": 0.0, "avg_logprob": -0.0885739281063988, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.003172139171510935}, {"id": 1075, "seek": 626264, "start": 6279.360000000001, "end": 6284.64, "text": " by the fact that neural networks don't have the capacity to do this. But the routines we use to", "tokens": [51200, 538, 264, 1186, 300, 18161, 9590, 500, 380, 362, 264, 6042, 281, 360, 341, 13, 583, 264, 33827, 321, 764, 281, 51464], "temperature": 0.0, "avg_logprob": -0.0885739281063988, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.003172139171510935}, {"id": 1076, "seek": 626264, "start": 6284.64, "end": 6290.96, "text": " optimize them are not good enough to to across that divide. So in neural algorithmic reasoning,", "tokens": [51464, 19719, 552, 366, 406, 665, 1547, 281, 281, 2108, 300, 9845, 13, 407, 294, 18161, 9284, 299, 21577, 11, 51780], "temperature": 0.0, "avg_logprob": -0.0885739281063988, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.003172139171510935}, {"id": 1077, "seek": 629096, "start": 6290.96, "end": 6295.6, "text": " all that we're really trying to do is to bring these two sides closer together by making changes", "tokens": [50364, 439, 300, 321, 434, 534, 1382, 281, 360, 307, 281, 1565, 613, 732, 4881, 4966, 1214, 538, 1455, 2962, 50596], "temperature": 0.0, "avg_logprob": -0.06874801876308682, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.01016508974134922}, {"id": 1078, "seek": 629096, "start": 6295.6, "end": 6299.68, "text": " either in the structure of the neural network or the training regime of the neural network or the", "tokens": [50596, 2139, 294, 264, 3877, 295, 264, 18161, 3209, 420, 264, 3097, 13120, 295, 264, 18161, 3209, 420, 264, 50800], "temperature": 0.0, "avg_logprob": -0.06874801876308682, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.01016508974134922}, {"id": 1079, "seek": 629096, "start": 6299.68, "end": 6305.92, "text": " kinds of data that we'll let the neural network see so that hopefully it's going to generalize better", "tokens": [50800, 3685, 295, 1412, 300, 321, 603, 718, 264, 18161, 3209, 536, 370, 300, 4696, 309, 311, 516, 281, 2674, 1125, 1101, 51112], "temperature": 0.0, "avg_logprob": -0.06874801876308682, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.01016508974134922}, {"id": 1080, "seek": 629096, "start": 6305.92, "end": 6311.52, "text": " and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems", "tokens": [51112, 293, 48224, 473, 1101, 13, 400, 2318, 322, 264, 3685, 295, 11, 291, 458, 11, 13735, 9284, 299, 2740, 51392], "temperature": 0.0, "avg_logprob": -0.06874801876308682, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.01016508974134922}, {"id": 1081, "seek": 629096, "start": 6311.52, "end": 6316.32, "text": " that we might see in a computer science textbook. And lastly, I think I'd just like to address the", "tokens": [51392, 300, 321, 1062, 536, 294, 257, 3820, 3497, 25591, 13, 400, 16386, 11, 286, 519, 286, 1116, 445, 411, 281, 2985, 264, 51632], "temperature": 0.0, "avg_logprob": -0.06874801876308682, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.01016508974134922}, {"id": 1082, "seek": 631632, "start": 6316.32, "end": 6325.28, "text": " point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit", "tokens": [50364, 935, 466, 32411, 13, 492, 362, 257, 3035, 322, 9284, 299, 21577, 43751, 300, 321, 366, 466, 281, 10315, 50812], "temperature": 0.0, "avg_logprob": -0.10082345207532246, "compression_ratio": 1.6, "no_speech_prob": 0.0179696436971426}, {"id": 1083, "seek": 631632, "start": 6325.28, "end": 6329.759999999999, "text": " that in Europe's data set track. I think it should be public even now on GitHub, because that's the", "tokens": [50812, 300, 294, 3315, 311, 1412, 992, 2837, 13, 286, 519, 309, 820, 312, 1908, 754, 586, 322, 23331, 11, 570, 300, 311, 264, 51036], "temperature": 0.0, "avg_logprob": -0.10082345207532246, "compression_ratio": 1.6, "no_speech_prob": 0.0179696436971426}, {"id": 1084, "seek": 631632, "start": 6329.759999999999, "end": 6334.719999999999, "text": " requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to", "tokens": [51036, 11695, 337, 264, 7586, 11, 689, 321, 362, 1596, 257, 1326, 9284, 299, 9608, 11, 293, 321, 434, 1382, 281, 51284], "temperature": 0.0, "avg_logprob": -0.10082345207532246, "compression_ratio": 1.6, "no_speech_prob": 0.0179696436971426}, {"id": 1085, "seek": 631632, "start": 6334.719999999999, "end": 6342.24, "text": " force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution,", "tokens": [51284, 3464, 46411, 45, 82, 281, 1466, 552, 13, 400, 321, 360, 362, 2940, 32411, 9608, 294, 456, 13, 400, 412, 1935, 294, 7316, 11, 51660], "temperature": 0.0, "avg_logprob": -0.10082345207532246, "compression_ratio": 1.6, "no_speech_prob": 0.0179696436971426}, {"id": 1086, "seek": 634224, "start": 6342.24, "end": 6347.76, "text": " these graph neural networks are capable of imitating the steps of, say, insertion sort. So", "tokens": [50364, 613, 4295, 18161, 9590, 366, 8189, 295, 566, 16350, 264, 4439, 295, 11, 584, 11, 8969, 313, 1333, 13, 407, 50640], "temperature": 0.0, "avg_logprob": -0.06837309034247148, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.22230657935142517}, {"id": 1087, "seek": 634224, "start": 6347.76, "end": 6351.76, "text": " I will say not all is lost if you're very careful about how you tune them. But obviously,", "tokens": [50640, 286, 486, 584, 406, 439, 307, 2731, 498, 291, 434, 588, 5026, 466, 577, 291, 10864, 552, 13, 583, 2745, 11, 50840], "temperature": 0.0, "avg_logprob": -0.06837309034247148, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.22230657935142517}, {"id": 1088, "seek": 634224, "start": 6351.76, "end": 6356.639999999999, "text": " there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk", "tokens": [50840, 456, 307, 257, 688, 295, 11730, 1720, 13, 400, 286, 1454, 300, 1780, 1830, 341, 5081, 11, 321, 603, 611, 483, 257, 2931, 281, 751, 51084], "temperature": 0.0, "avg_logprob": -0.06837309034247148, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.22230657935142517}, {"id": 1089, "seek": 634224, "start": 6356.639999999999, "end": 6361.599999999999, "text": " a little bit about how even though we cannot perfectly mimic algorithms, we can still use", "tokens": [51084, 257, 707, 857, 466, 577, 754, 1673, 321, 2644, 6239, 31075, 14642, 11, 321, 393, 920, 764, 51332], "temperature": 0.0, "avg_logprob": -0.06837309034247148, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.22230657935142517}, {"id": 1090, "seek": 634224, "start": 6361.599999999999, "end": 6367.84, "text": " this concept of algorithmic execution today now to help expand the space of applicability of algorithm.", "tokens": [51332, 341, 3410, 295, 9284, 299, 15058, 965, 586, 281, 854, 5268, 264, 1901, 295, 2580, 2310, 295, 9284, 13, 51644], "temperature": 0.0, "avg_logprob": -0.06837309034247148, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.22230657935142517}, {"id": 1091, "seek": 636784, "start": 6368.64, "end": 6373.2, "text": " Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people", "tokens": [50404, 865, 11, 341, 307, 3122, 10343, 13, 1436, 341, 2170, 281, 264, 4965, 295, 437, 286, 519, 512, 561, 50632], "temperature": 0.0, "avg_logprob": -0.13623505372267503, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.003012603148818016}, {"id": 1092, "seek": 636784, "start": 6373.2, "end": 6378.64, "text": " point out as being the limitations of deep learning, right? Sholay spoke about this, but", "tokens": [50632, 935, 484, 382, 885, 264, 15705, 295, 2452, 2539, 11, 558, 30, 1160, 401, 320, 7179, 466, 341, 11, 457, 50904], "temperature": 0.0, "avg_logprob": -0.13623505372267503, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.003012603148818016}, {"id": 1093, "seek": 636784, "start": 6378.64, "end": 6383.28, "text": " I don't think that geometric deep learning would help a neural network learner sorting function,", "tokens": [50904, 286, 500, 380, 519, 300, 33246, 2452, 2539, 576, 854, 257, 18161, 3209, 33347, 32411, 2445, 11, 51136], "temperature": 0.0, "avg_logprob": -0.13623505372267503, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.003012603148818016}, {"id": 1094, "seek": 636784, "start": 6383.28, "end": 6389.52, "text": " because discrete problems in general don't seem amenable to vector spaces, either because the", "tokens": [51136, 570, 27706, 2740, 294, 2674, 500, 380, 1643, 18497, 712, 281, 8062, 7673, 11, 2139, 570, 264, 51448], "temperature": 0.0, "avg_logprob": -0.13623505372267503, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.003012603148818016}, {"id": 1095, "seek": 636784, "start": 6389.52, "end": 6394.32, "text": " representation would be glitchy, or the problem is not interpolative in nature or not learnable", "tokens": [51448, 10290, 576, 312, 23552, 88, 11, 420, 264, 1154, 307, 406, 44902, 1166, 294, 3687, 420, 406, 1466, 712, 51688], "temperature": 0.0, "avg_logprob": -0.13623505372267503, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.003012603148818016}, {"id": 1096, "seek": 639432, "start": 6394.32, "end": 6400.24, "text": " with stochastic gradient descent. So it would be fascinating if we could overcome these problems", "tokens": [50364, 365, 342, 8997, 2750, 16235, 23475, 13, 407, 309, 576, 312, 10343, 498, 321, 727, 10473, 613, 2740, 50660], "temperature": 0.0, "avg_logprob": -0.07658115568615141, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.019393345341086388}, {"id": 1097, "seek": 639432, "start": 6400.24, "end": 6404.4, "text": " using continuous neural networks as an algorithmic substrate. Do you think we could?", "tokens": [50660, 1228, 10957, 18161, 9590, 382, 364, 9284, 299, 27585, 13, 1144, 291, 519, 321, 727, 30, 50868], "temperature": 0.0, "avg_logprob": -0.07658115568615141, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.019393345341086388}, {"id": 1098, "seek": 639432, "start": 6406.96, "end": 6413.36, "text": " I think that it is possible, but it will require us potentially to broaden our lens on what we", "tokens": [50996, 286, 519, 300, 309, 307, 1944, 11, 457, 309, 486, 3651, 505, 7263, 281, 47045, 527, 6765, 322, 437, 321, 51316], "temperature": 0.0, "avg_logprob": -0.07658115568615141, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.019393345341086388}, {"id": 1099, "seek": 639432, "start": 6413.36, "end": 6417.44, "text": " mean by geometric deep learning. And this is something we're already very actively thinking", "tokens": [51316, 914, 538, 33246, 2452, 2539, 13, 400, 341, 307, 746, 321, 434, 1217, 588, 13022, 1953, 51520], "temperature": 0.0, "avg_logprob": -0.07658115568615141, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.019393345341086388}, {"id": 1100, "seek": 639432, "start": 6417.44, "end": 6421.84, "text": " about. I think one of our co-authors, Taco Co, and actually thought much more deeply about this", "tokens": [51520, 466, 13, 286, 519, 472, 295, 527, 598, 12, 40198, 830, 11, 37992, 3066, 11, 293, 767, 1194, 709, 544, 8760, 466, 341, 51740], "temperature": 0.0, "avg_logprob": -0.07658115568615141, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.019393345341086388}, {"id": 1101, "seek": 642184, "start": 6422.56, "end": 6428.400000000001, "text": " in recent times. But basically, the idea is we looked at geometric deep learning from a group", "tokens": [50400, 294, 5162, 1413, 13, 583, 1936, 11, 264, 1558, 307, 321, 2956, 412, 33246, 2452, 2539, 490, 257, 1594, 50692], "temperature": 0.0, "avg_logprob": -0.05919981869784269, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.029729820787906647}, {"id": 1102, "seek": 642184, "start": 6428.400000000001, "end": 6433.76, "text": " symmetry point of view, which is a very nice way to describe spatial regularities and spatial", "tokens": [50692, 25440, 935, 295, 1910, 11, 597, 307, 257, 588, 1481, 636, 281, 6786, 23598, 3890, 1088, 293, 23598, 50960], "temperature": 0.0, "avg_logprob": -0.05919981869784269, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.029729820787906647}, {"id": 1103, "seek": 642184, "start": 6433.76, "end": 6439.6, "text": " symmetries. But it's not necessarily the best way to talk about, say, invariance of generic", "tokens": [50960, 14232, 302, 2244, 13, 583, 309, 311, 406, 4725, 264, 1151, 636, 281, 751, 466, 11, 584, 11, 33270, 719, 295, 19577, 51252], "temperature": 0.0, "avg_logprob": -0.05919981869784269, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.029729820787906647}, {"id": 1104, "seek": 642184, "start": 6439.6, "end": 6444.4800000000005, "text": " computation, which you would find in algorithms, right? It's like, I have input that satisfies", "tokens": [51252, 24903, 11, 597, 291, 576, 915, 294, 14642, 11, 558, 30, 467, 311, 411, 11, 286, 362, 4846, 300, 44271, 51496], "temperature": 0.0, "avg_logprob": -0.05919981869784269, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.029729820787906647}, {"id": 1105, "seek": 642184, "start": 6444.4800000000005, "end": 6448.72, "text": " certain preconditions. I want to say something about once I push it through this function,", "tokens": [51496, 1629, 4346, 684, 2451, 13, 286, 528, 281, 584, 746, 466, 1564, 286, 2944, 309, 807, 341, 2445, 11, 51708], "temperature": 0.0, "avg_logprob": -0.05919981869784269, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.029729820787906647}, {"id": 1106, "seek": 644872, "start": 6448.72, "end": 6453.76, "text": " it should satisfy certain post conditions. This is not the kind of thing we can very easily express", "tokens": [50364, 309, 820, 19319, 1629, 2183, 4487, 13, 639, 307, 406, 264, 733, 295, 551, 321, 393, 588, 3612, 5109, 50616], "temperature": 0.0, "avg_logprob": -0.049103864917048705, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.04669242352247238}, {"id": 1107, "seek": 644872, "start": 6453.76, "end": 6458.8, "text": " using the language of group theory. However, it is something that perhaps we could express more", "tokens": [50616, 1228, 264, 2856, 295, 1594, 5261, 13, 2908, 11, 309, 307, 746, 300, 4317, 321, 727, 5109, 544, 50868], "temperature": 0.0, "avg_logprob": -0.049103864917048705, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.04669242352247238}, {"id": 1108, "seek": 644872, "start": 6458.8, "end": 6463.6, "text": " nicely using the language of category theory, which is an area of math that I still don't know", "tokens": [50868, 9594, 1228, 264, 2856, 295, 7719, 5261, 11, 597, 307, 364, 1859, 295, 5221, 300, 286, 920, 500, 380, 458, 51108], "temperature": 0.0, "avg_logprob": -0.049103864917048705, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.04669242352247238}, {"id": 1109, "seek": 644872, "start": 6463.6, "end": 6468.320000000001, "text": " enough about. I'm currently actively learning it. But basically, in the language of category", "tokens": [51108, 1547, 466, 13, 286, 478, 4362, 13022, 2539, 309, 13, 583, 1936, 11, 294, 264, 2856, 295, 7719, 51344], "temperature": 0.0, "avg_logprob": -0.049103864917048705, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.04669242352247238}, {"id": 1110, "seek": 644872, "start": 6468.320000000001, "end": 6474.16, "text": " theory, groups are super simple categories that have just one node, right? You can do a lot more", "tokens": [51344, 5261, 11, 3935, 366, 1687, 2199, 10479, 300, 362, 445, 472, 9984, 11, 558, 30, 509, 393, 360, 257, 688, 544, 51636], "temperature": 0.0, "avg_logprob": -0.049103864917048705, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.04669242352247238}, {"id": 1111, "seek": 647416, "start": 6474.16, "end": 6480.32, "text": " complicated things if you use this more broader abstract language. And, you know, you talk about", "tokens": [50364, 6179, 721, 498, 291, 764, 341, 544, 13227, 12649, 2856, 13, 400, 11, 291, 458, 11, 291, 751, 466, 50672], "temperature": 0.0, "avg_logprob": -0.09641794129913929, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0038832698483020067}, {"id": 1112, "seek": 647416, "start": 6480.32, "end": 6484.96, "text": " basically anything of interest there in terms of these commutative diagrams. And Taco actually", "tokens": [50672, 1936, 1340, 295, 1179, 456, 294, 2115, 295, 613, 800, 325, 1166, 36709, 13, 400, 37992, 767, 50904], "temperature": 0.0, "avg_logprob": -0.09641794129913929, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0038832698483020067}, {"id": 1113, "seek": 647416, "start": 6484.96, "end": 6489.5199999999995, "text": " recently had a really interesting paper called natural graph networks, where they basically", "tokens": [50904, 3938, 632, 257, 534, 1880, 3035, 1219, 3303, 4295, 9590, 11, 689, 436, 1936, 51132], "temperature": 0.0, "avg_logprob": -0.09641794129913929, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0038832698483020067}, {"id": 1114, "seek": 647416, "start": 6489.5199999999995, "end": 6494.88, "text": " generalize the notion of permutation, equivariance that you might find in graph nets to this more", "tokens": [51132, 2674, 1125, 264, 10710, 295, 4784, 11380, 11, 48726, 3504, 719, 300, 291, 1062, 915, 294, 4295, 36170, 281, 341, 544, 51400], "temperature": 0.0, "avg_logprob": -0.09641794129913929, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0038832698483020067}, {"id": 1115, "seek": 647416, "start": 6494.88, "end": 6500.4, "text": " general concept of natural transformations. So now suddenly, you don't have to have a network that", "tokens": [51400, 2674, 3410, 295, 3303, 34852, 13, 407, 586, 5800, 11, 291, 500, 380, 362, 281, 362, 257, 3209, 300, 51676], "temperature": 0.0, "avg_logprob": -0.09641794129913929, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0038832698483020067}, {"id": 1116, "seek": 650040, "start": 6500.4, "end": 6505.36, "text": " does exactly the same transformation in every single part of the graph. What you actually need", "tokens": [50364, 775, 2293, 264, 912, 9887, 294, 633, 2167, 644, 295, 264, 4295, 13, 708, 291, 767, 643, 50612], "temperature": 0.0, "avg_logprob": -0.05889126350139749, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.016147086396813393}, {"id": 1117, "seek": 650040, "start": 6505.36, "end": 6510.799999999999, "text": " is something a bit more fine grained. You just need for all like locally isomorphic parts of the", "tokens": [50612, 307, 746, 257, 857, 544, 2489, 1295, 2001, 13, 509, 445, 643, 337, 439, 411, 16143, 307, 32702, 299, 3166, 295, 264, 50884], "temperature": 0.0, "avg_logprob": -0.05889126350139749, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.016147086396813393}, {"id": 1118, "seek": 650040, "start": 6510.799999999999, "end": 6515.36, "text": " graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think", "tokens": [50884, 4295, 11, 291, 643, 281, 15158, 264, 912, 13, 583, 294, 8665, 11, 309, 2709, 291, 257, 857, 544, 12635, 13, 400, 286, 519, 51112], "temperature": 0.0, "avg_logprob": -0.05889126350139749, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.016147086396813393}, {"id": 1119, "seek": 650040, "start": 6515.36, "end": 6520.32, "text": " that this kind of language, like moving a bit away from the group formalism would allow us to", "tokens": [51112, 300, 341, 733, 295, 2856, 11, 411, 2684, 257, 857, 1314, 490, 264, 1594, 9860, 1434, 576, 2089, 505, 281, 51360], "temperature": 0.0, "avg_logprob": -0.05889126350139749, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.016147086396813393}, {"id": 1120, "seek": 650040, "start": 6520.32, "end": 6525.44, "text": " talk about say algorithmic invariance and things like this. I don't yet have any theory to properly", "tokens": [51360, 751, 466, 584, 9284, 299, 33270, 719, 293, 721, 411, 341, 13, 286, 500, 380, 1939, 362, 604, 5261, 281, 6108, 51616], "temperature": 0.0, "avg_logprob": -0.05889126350139749, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.016147086396813393}, {"id": 1121, "seek": 652544, "start": 6525.44, "end": 6530.639999999999, "text": " prove this, but it's something that I'm actively working on. And I guess I would say, you know,", "tokens": [50364, 7081, 341, 11, 457, 309, 311, 746, 300, 286, 478, 13022, 1364, 322, 13, 400, 286, 2041, 286, 576, 584, 11, 291, 458, 11, 50624], "temperature": 0.0, "avg_logprob": -0.07084100896661932, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.08500561118125916}, {"id": 1122, "seek": 652544, "start": 6530.639999999999, "end": 6534.719999999999, "text": " the only question is, would you still call this geometric deep learning? And in my opinion,", "tokens": [50624, 264, 787, 1168, 307, 11, 576, 291, 920, 818, 341, 33246, 2452, 2539, 30, 400, 294, 452, 4800, 11, 50828], "temperature": 0.0, "avg_logprob": -0.07084100896661932, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.08500561118125916}, {"id": 1123, "seek": 652544, "start": 6534.719999999999, "end": 6539.679999999999, "text": " the very creators of category theory have said that category theory is a direct extension of", "tokens": [50828, 264, 588, 16039, 295, 7719, 5261, 362, 848, 300, 7719, 5261, 307, 257, 2047, 10320, 295, 51076], "temperature": 0.0, "avg_logprob": -0.07084100896661932, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.08500561118125916}, {"id": 1124, "seek": 652544, "start": 6539.679999999999, "end": 6544.639999999999, "text": " Felix Klein's Erlangin program. So since the founders of the field have already made this", "tokens": [51076, 30169, 33327, 311, 3300, 25241, 259, 1461, 13, 407, 1670, 264, 25608, 295, 264, 2519, 362, 1217, 1027, 341, 51324], "temperature": 0.0, "avg_logprob": -0.07084100896661932, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.08500561118125916}, {"id": 1125, "seek": 652544, "start": 6544.639999999999, "end": 6550.0, "text": " connection, I would expect that, you know, it would be pretty applicable under a geometric lens.", "tokens": [51324, 4984, 11, 286, 576, 2066, 300, 11, 291, 458, 11, 309, 576, 312, 1238, 21142, 833, 257, 33246, 6765, 13, 51592], "temperature": 0.0, "avg_logprob": -0.07084100896661932, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.08500561118125916}, {"id": 1126, "seek": 655000, "start": 6550.96, "end": 6557.68, "text": " So it seems to come back to it seems to come back from both of your sides to essentially graphs", "tokens": [50412, 407, 309, 2544, 281, 808, 646, 281, 309, 2544, 281, 808, 646, 490, 1293, 295, 428, 4881, 281, 4476, 24877, 50748], "temperature": 0.0, "avg_logprob": -0.10323277340140394, "compression_ratio": 1.8606965174129353, "no_speech_prob": 0.0029804615769535303}, {"id": 1127, "seek": 655000, "start": 6557.68, "end": 6566.32, "text": " and working on on sort of graphs to capture on the one side, the sort of symmetries that are", "tokens": [50748, 293, 1364, 322, 322, 1333, 295, 24877, 281, 7983, 322, 264, 472, 1252, 11, 264, 1333, 295, 14232, 302, 2244, 300, 366, 51180], "temperature": 0.0, "avg_logprob": -0.10323277340140394, "compression_ratio": 1.8606965174129353, "no_speech_prob": 0.0029804615769535303}, {"id": 1128, "seek": 655000, "start": 6566.32, "end": 6571.92, "text": " that you either assume in the problem or that you know, or that you want to impose on the other", "tokens": [51180, 300, 291, 2139, 6552, 294, 264, 1154, 420, 300, 291, 458, 11, 420, 300, 291, 528, 281, 26952, 322, 264, 661, 51460], "temperature": 0.0, "avg_logprob": -0.10323277340140394, "compression_ratio": 1.8606965174129353, "no_speech_prob": 0.0029804615769535303}, {"id": 1129, "seek": 655000, "start": 6571.92, "end": 6578.16, "text": " side on the other side. Now, these computations can may be well represented in in graphs.", "tokens": [51460, 1252, 322, 264, 661, 1252, 13, 823, 11, 613, 2807, 763, 393, 815, 312, 731, 10379, 294, 294, 24877, 13, 51772], "temperature": 0.0, "avg_logprob": -0.10323277340140394, "compression_ratio": 1.8606965174129353, "no_speech_prob": 0.0029804615769535303}, {"id": 1130, "seek": 657816, "start": 6579.12, "end": 6585.44, "text": " What's what's so special about graphs in your estimations? Is there something", "tokens": [50412, 708, 311, 437, 311, 370, 2121, 466, 24877, 294, 428, 8017, 763, 30, 1119, 456, 746, 50728], "temperature": 0.0, "avg_logprob": -0.11334081932350441, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.002978540724143386}, {"id": 1131, "seek": 657816, "start": 6586.32, "end": 6592.08, "text": " fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I", "tokens": [50772, 8088, 281, 309, 30, 1610, 307, 309, 445, 1071, 636, 30, 509, 458, 11, 321, 632, 3964, 460, 911, 12971, 420, 370, 510, 293, 286, 51060], "temperature": 0.0, "avg_logprob": -0.11334081932350441, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.002978540724143386}, {"id": 1132, "seek": 657816, "start": 6592.08, "end": 6596.88, "text": " asked him the same question, what's so special about graphs and his argument was essentially,", "tokens": [51060, 2351, 796, 264, 912, 1168, 11, 437, 311, 370, 2121, 466, 24877, 293, 702, 6770, 390, 4476, 11, 51300], "temperature": 0.0, "avg_logprob": -0.11334081932350441, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.002978540724143386}, {"id": 1133, "seek": 657816, "start": 6596.88, "end": 6600.8, "text": " well, it's not about graphs, it's simply a good representation of the problem", "tokens": [51300, 731, 11, 309, 311, 406, 466, 24877, 11, 309, 311, 2935, 257, 665, 10290, 295, 264, 1154, 51496], "temperature": 0.0, "avg_logprob": -0.11334081932350441, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.002978540724143386}, {"id": 1134, "seek": 657816, "start": 6601.44, "end": 6607.12, "text": " that we can make efficient operations on. Do you have a different opinion on that? Is a graph", "tokens": [51528, 300, 321, 393, 652, 7148, 7705, 322, 13, 1144, 291, 362, 257, 819, 4800, 322, 300, 30, 1119, 257, 4295, 51812], "temperature": 0.0, "avg_logprob": -0.11334081932350441, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.002978540724143386}, {"id": 1135, "seek": 660712, "start": 6607.12, "end": 6613.68, "text": " something fundamental that we should look more at than, for example, a tensor?", "tokens": [50364, 746, 8088, 300, 321, 820, 574, 544, 412, 813, 11, 337, 1365, 11, 257, 40863, 30, 50692], "temperature": 0.0, "avg_logprob": -0.13455897111159104, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0013379136798903346}, {"id": 1136, "seek": 660712, "start": 6614.48, "end": 6619.92, "text": " Graphs are abstract models for systems of relations or interactions. I should maybe specify", "tokens": [50732, 21884, 82, 366, 12649, 5245, 337, 3652, 295, 2299, 420, 13280, 13, 286, 820, 1310, 16500, 51004], "temperature": 0.0, "avg_logprob": -0.13455897111159104, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0013379136798903346}, {"id": 1137, "seek": 660712, "start": 6620.48, "end": 6627.5199999999995, "text": " pairwise relations and interactions. And it happens that a lot of physical or biological", "tokens": [51032, 6119, 3711, 2299, 293, 13280, 13, 400, 309, 2314, 300, 257, 688, 295, 4001, 420, 13910, 51384], "temperature": 0.0, "avg_logprob": -0.13455897111159104, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0013379136798903346}, {"id": 1138, "seek": 660712, "start": 6628.48, "end": 6633.84, "text": " even social systems can be described at least at some level of abstraction as a graph. So that's", "tokens": [51432, 754, 2093, 3652, 393, 312, 7619, 412, 1935, 412, 512, 1496, 295, 37765, 382, 257, 4295, 13, 407, 300, 311, 51700], "temperature": 0.0, "avg_logprob": -0.13455897111159104, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.0013379136798903346}, {"id": 1139, "seek": 663384, "start": 6633.84, "end": 6640.88, "text": " why it is a so popular modeling tool in many fields. You can also obtain other structures such as", "tokens": [50364, 983, 309, 307, 257, 370, 3743, 15983, 2290, 294, 867, 7909, 13, 509, 393, 611, 12701, 661, 9227, 1270, 382, 50716], "temperature": 0.0, "avg_logprob": -0.09914666079403309, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003744318848475814}, {"id": 1140, "seek": 663384, "start": 6640.88, "end": 6646.96, "text": " grids as particular cases. I wouldn't call it fundamental, but it is a very convenient and", "tokens": [50716, 677, 3742, 382, 1729, 3331, 13, 286, 2759, 380, 818, 309, 8088, 11, 457, 309, 307, 257, 588, 10851, 293, 51020], "temperature": 0.0, "avg_logprob": -0.09914666079403309, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003744318848475814}, {"id": 1141, "seek": 663384, "start": 6646.96, "end": 6653.6, "text": " very common, I would say ubiquitous model. Now, what I personally find disturbing and we can talk", "tokens": [51020, 588, 2689, 11, 286, 576, 584, 43868, 39831, 2316, 13, 823, 11, 437, 286, 5665, 915, 21903, 293, 321, 393, 751, 51352], "temperature": 0.0, "avg_logprob": -0.09914666079403309, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003744318848475814}, {"id": 1142, "seek": 663384, "start": 6653.6, "end": 6659.84, "text": " about it in more detail later on is that if you look at the different geometric structures for", "tokens": [51352, 466, 309, 294, 544, 2607, 1780, 322, 307, 300, 498, 291, 574, 412, 264, 819, 33246, 9227, 337, 51664], "temperature": 0.0, "avg_logprob": -0.09914666079403309, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003744318848475814}, {"id": 1143, "seek": 665984, "start": 6659.84, "end": 6665.84, "text": " Gamble that we consider in the book, whether it's Euclidean spaces or many phones, they all have", "tokens": [50364, 24723, 638, 300, 321, 1949, 294, 264, 1446, 11, 1968, 309, 311, 462, 1311, 31264, 282, 7673, 420, 867, 10216, 11, 436, 439, 362, 50664], "temperature": 0.0, "avg_logprob": -0.13206659845945215, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0021735073532909155}, {"id": 1144, "seek": 665984, "start": 6665.84, "end": 6670.56, "text": " the discrete counterparts. So you have a plane and you can discretize it as a grid. You have a", "tokens": [50664, 264, 27706, 33287, 13, 407, 291, 362, 257, 5720, 293, 291, 393, 25656, 1125, 309, 382, 257, 10748, 13, 509, 362, 257, 50900], "temperature": 0.0, "avg_logprob": -0.13206659845945215, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0021735073532909155}, {"id": 1145, "seek": 665984, "start": 6670.56, "end": 6676.96, "text": " manifold, you can discretize it as a mesh. A graph is inherently discrete. And this is something that", "tokens": [50900, 47138, 11, 291, 393, 25656, 1125, 309, 382, 257, 17407, 13, 316, 4295, 307, 27993, 27706, 13, 400, 341, 307, 746, 300, 51220], "temperature": 0.0, "avg_logprob": -0.13206659845945215, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0021735073532909155}, {"id": 1146, "seek": 665984, "start": 6676.96, "end": 6682.56, "text": " I find disturbing. There is, in fact, an entire field that is called network geometry that tries to", "tokens": [51220, 286, 915, 21903, 13, 821, 307, 11, 294, 1186, 11, 364, 2302, 2519, 300, 307, 1219, 3209, 18426, 300, 9898, 281, 51500], "temperature": 0.0, "avg_logprob": -0.13206659845945215, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0021735073532909155}, {"id": 1147, "seek": 665984, "start": 6682.56, "end": 6687.52, "text": " look at graphs as continuous objects. So for example, certain types of graph that look like", "tokens": [51500, 574, 412, 24877, 382, 10957, 6565, 13, 407, 337, 1365, 11, 1629, 3467, 295, 4295, 300, 574, 411, 51748], "temperature": 0.0, "avg_logprob": -0.13206659845945215, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0021735073532909155}, {"id": 1148, "seek": 668752, "start": 6687.52, "end": 6693.360000000001, "text": " social networks, what is called scale free graphs, can be represented as nearest-neighbor graphs in", "tokens": [50364, 2093, 9590, 11, 437, 307, 1219, 4373, 1737, 24877, 11, 393, 312, 10379, 382, 23831, 12, 716, 910, 3918, 24877, 294, 50656], "temperature": 0.0, "avg_logprob": -0.10653964678446452, "compression_ratio": 1.6, "no_speech_prob": 0.003957800567150116}, {"id": 1149, "seek": 668752, "start": 6693.360000000001, "end": 6699.76, "text": " some a little bit exotic space with hyperbolic geometry. So if we take this analogy, I think", "tokens": [50656, 512, 257, 707, 857, 27063, 1901, 365, 9848, 65, 7940, 18426, 13, 407, 498, 321, 747, 341, 21663, 11, 286, 519, 50976], "temperature": 0.0, "avg_logprob": -0.10653964678446452, "compression_ratio": 1.6, "no_speech_prob": 0.003957800567150116}, {"id": 1150, "seek": 668752, "start": 6699.76, "end": 6705.76, "text": " it is very powerful because now you can consider graphs as a discretization of something continuous", "tokens": [50976, 309, 307, 588, 4005, 570, 586, 291, 393, 1949, 24877, 382, 257, 25656, 2144, 295, 746, 10957, 51276], "temperature": 0.0, "avg_logprob": -0.10653964678446452, "compression_ratio": 1.6, "no_speech_prob": 0.003957800567150116}, {"id": 1151, "seek": 668752, "start": 6705.76, "end": 6713.6, "text": " and then think for example of graph neural networks as certain types of diffusion processes", "tokens": [51276, 293, 550, 519, 337, 1365, 295, 4295, 18161, 9590, 382, 1629, 3467, 295, 25242, 7555, 51668], "temperature": 0.0, "avg_logprob": -0.10653964678446452, "compression_ratio": 1.6, "no_speech_prob": 0.003957800567150116}, {"id": 1152, "seek": 671360, "start": 6713.68, "end": 6721.120000000001, "text": " that are just discretized in a certain way. And by making potentially possible different", "tokens": [50368, 300, 366, 445, 25656, 1602, 294, 257, 1629, 636, 13, 400, 538, 1455, 7263, 1944, 819, 50740], "temperature": 0.0, "avg_logprob": -0.10643284928564932, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0022311529610306025}, {"id": 1153, "seek": 671360, "start": 6721.120000000001, "end": 6726.88, "text": " discretizations, you will get maybe better performing architectures. One of the core", "tokens": [50740, 25656, 14455, 11, 291, 486, 483, 1310, 1101, 10205, 6331, 1303, 13, 1485, 295, 264, 4965, 51028], "temperature": 0.0, "avg_logprob": -0.10643284928564932, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0022311529610306025}, {"id": 1154, "seek": 671360, "start": 6726.88, "end": 6732.240000000001, "text": " dichotomies we talk about on Street Talk is the apparent dichotomy between discrete and continuous.", "tokens": [51028, 10390, 42939, 530, 321, 751, 466, 322, 7638, 8780, 307, 264, 18335, 10390, 310, 8488, 1296, 27706, 293, 10957, 13, 51296], "temperature": 0.0, "avg_logprob": -0.10643284928564932, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0022311529610306025}, {"id": 1155, "seek": 671360, "start": 6732.240000000001, "end": 6737.280000000001, "text": " And as Yannick was saying, there are folks out there who want our knowledge substrate to be", "tokens": [51296, 400, 382, 398, 969, 618, 390, 1566, 11, 456, 366, 4024, 484, 456, 567, 528, 527, 3601, 27585, 281, 312, 51548], "temperature": 0.0, "avg_logprob": -0.10643284928564932, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0022311529610306025}, {"id": 1156, "seek": 671360, "start": 6737.280000000001, "end": 6742.400000000001, "text": " discrete but still distributed, record them sub-symbolic folks. And this network geometry", "tokens": [51548, 27706, 457, 920, 12631, 11, 2136, 552, 1422, 12, 3187, 5612, 299, 4024, 13, 400, 341, 3209, 18426, 51804], "temperature": 0.0, "avg_logprob": -0.10643284928564932, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0022311529610306025}, {"id": 1157, "seek": 674240, "start": 6742.4, "end": 6746.08, "text": " is fascinating as well because you're saying in some sense you can think of there being some", "tokens": [50364, 307, 10343, 382, 731, 570, 291, 434, 1566, 294, 512, 2020, 291, 393, 519, 295, 456, 885, 512, 50548], "temperature": 0.0, "avg_logprob": -0.1254427649758079, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.008654673583805561}, {"id": 1158, "seek": 674240, "start": 6746.08, "end": 6752.16, "text": " unknown continuous geometry. So you're saying there is no dichotomy? This is probably a little", "tokens": [50548, 9841, 10957, 18426, 13, 407, 291, 434, 1566, 456, 307, 572, 10390, 310, 8488, 30, 639, 307, 1391, 257, 707, 50852], "temperature": 0.0, "avg_logprob": -0.1254427649758079, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.008654673583805561}, {"id": 1159, "seek": 674240, "start": 6752.16, "end": 6757.92, "text": " bit of a wishful thinking as it happens with every model. So I would probably phrase it carefully", "tokens": [50852, 857, 295, 257, 3172, 906, 1953, 382, 309, 2314, 365, 633, 2316, 13, 407, 286, 576, 1391, 9535, 309, 7500, 51140], "temperature": 0.0, "avg_logprob": -0.1254427649758079, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.008654673583805561}, {"id": 1160, "seek": 674240, "start": 6757.92, "end": 6765.12, "text": " for some kinds of graphs, you can make this continuous model for others, maybe not. Fascinating.", "tokens": [51140, 337, 512, 3685, 295, 24877, 11, 291, 393, 652, 341, 10957, 2316, 337, 2357, 11, 1310, 406, 13, 49098, 8205, 13, 51500], "temperature": 0.0, "avg_logprob": -0.1254427649758079, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.008654673583805561}, {"id": 1161, "seek": 674240, "start": 6765.12, "end": 6771.2, "text": " Well, on to the subject of vector spaces versus discrete, you know, geometric deep learning is", "tokens": [51500, 1042, 11, 322, 281, 264, 3983, 295, 8062, 7673, 5717, 27706, 11, 291, 458, 11, 33246, 2452, 2539, 307, 51804], "temperature": 0.0, "avg_logprob": -0.1254427649758079, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.008654673583805561}, {"id": 1162, "seek": 677120, "start": 6771.2, "end": 6777.92, "text": " all about making any domain amenable to vector spaces, right? And indeed artificial neural networks.", "tokens": [50364, 439, 466, 1455, 604, 9274, 18497, 712, 281, 8062, 7673, 11, 558, 30, 400, 6451, 11677, 18161, 9590, 13, 50700], "temperature": 0.0, "avg_logprob": -0.09180715460526316, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.001777183380909264}, {"id": 1163, "seek": 677120, "start": 6777.92, "end": 6782.24, "text": " But could these geometric principles be applied to another form of machine learning, let's say", "tokens": [50700, 583, 727, 613, 33246, 9156, 312, 6456, 281, 1071, 1254, 295, 3479, 2539, 11, 718, 311, 584, 50916], "temperature": 0.0, "avg_logprob": -0.09180715460526316, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.001777183380909264}, {"id": 1164, "seek": 677120, "start": 6782.24, "end": 6787.599999999999, "text": " discrete program synthesis? Certainly a very important question, Tim. And yeah, thanks for", "tokens": [50916, 27706, 1461, 30252, 30, 16628, 257, 588, 1021, 1168, 11, 7172, 13, 400, 1338, 11, 3231, 337, 51184], "temperature": 0.0, "avg_logprob": -0.09180715460526316, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.001777183380909264}, {"id": 1165, "seek": 677120, "start": 6787.599999999999, "end": 6794.24, "text": " asking that. I think that there are many ways in which geometric deep learning is already", "tokens": [51184, 3365, 300, 13, 286, 519, 300, 456, 366, 867, 2098, 294, 597, 33246, 2452, 2539, 307, 1217, 51516], "temperature": 0.0, "avg_logprob": -0.09180715460526316, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.001777183380909264}, {"id": 1166, "seek": 677120, "start": 6794.24, "end": 6798.88, "text": " at least implicitly powering discrete approaches such as program synthesis because", "tokens": [51516, 412, 1935, 26947, 356, 1347, 278, 27706, 11587, 1270, 382, 1461, 30252, 570, 51748], "temperature": 0.0, "avg_logprob": -0.09180715460526316, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.001777183380909264}, {"id": 1167, "seek": 679888, "start": 6799.68, "end": 6807.36, "text": " there is a pretty big movement on these so-called dual approaches where you stick a geometric", "tokens": [50404, 456, 307, 257, 1238, 955, 3963, 322, 613, 370, 12, 11880, 11848, 11587, 689, 291, 2897, 257, 33246, 50788], "temperature": 0.0, "avg_logprob": -0.13994361780866793, "compression_ratio": 1.5418502202643172, "no_speech_prob": 0.00911658350378275}, {"id": 1168, "seek": 679888, "start": 6807.36, "end": 6813.36, "text": " deep learning architecture within a discrete tool that searches for the best solution. So,", "tokens": [50788, 2452, 2539, 9482, 1951, 257, 27706, 2290, 300, 26701, 337, 264, 1151, 3827, 13, 407, 11, 51088], "temperature": 0.0, "avg_logprob": -0.13994361780866793, "compression_ratio": 1.5418502202643172, "no_speech_prob": 0.00911658350378275}, {"id": 1169, "seek": 679888, "start": 6813.36, "end": 6818.32, "text": " for example, in combinatorial optimization, a very popular approach recently for a", "tokens": [51088, 337, 1365, 11, 294, 2512, 31927, 831, 19618, 11, 257, 588, 3743, 3109, 3938, 337, 257, 51336], "temperature": 0.0, "avg_logprob": -0.13994361780866793, "compression_ratio": 1.5418502202643172, "no_speech_prob": 0.00911658350378275}, {"id": 1170, "seek": 679888, "start": 6818.32, "end": 6824.88, "text": " neural-resolving mixed integer programs is to like have your typical off-the-shelf", "tokens": [51336, 18161, 12, 495, 401, 798, 7467, 24922, 4268, 307, 281, 411, 362, 428, 7476, 766, 12, 3322, 12, 46626, 51664], "temperature": 0.0, "avg_logprob": -0.13994361780866793, "compression_ratio": 1.5418502202643172, "no_speech_prob": 0.00911658350378275}, {"id": 1171, "seek": 682488, "start": 6825.52, "end": 6830.8, "text": " mip solver that selects variables to optimize one at a time. And, you know, with these kinds of", "tokens": [50396, 275, 647, 1404, 331, 300, 3048, 82, 9102, 281, 19719, 472, 412, 257, 565, 13, 400, 11, 291, 458, 11, 365, 613, 3685, 295, 50660], "temperature": 0.0, "avg_logprob": -0.07401814668074898, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.03357342258095741}, {"id": 1172, "seek": 682488, "start": 6830.8, "end": 6835.76, "text": " algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough about", "tokens": [50660, 14642, 11, 436, 434, 294, 8665, 21510, 565, 13, 583, 498, 291, 434, 6356, 420, 33800, 1547, 466, 50908], "temperature": 0.0, "avg_logprob": -0.07401814668074898, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.03357342258095741}, {"id": 1173, "seek": 682488, "start": 6835.76, "end": 6840.72, "text": " how, in what order you select these variables, you can actually solve the problem in linear time,", "tokens": [50908, 577, 11, 294, 437, 1668, 291, 3048, 613, 9102, 11, 291, 393, 767, 5039, 264, 1154, 294, 8213, 565, 11, 51156], "temperature": 0.0, "avg_logprob": -0.07401814668074898, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.03357342258095741}, {"id": 1174, "seek": 682488, "start": 6840.72, "end": 6845.28, "text": " which is something we would like to strive towards. And the exact way in which we select", "tokens": [51156, 597, 307, 746, 321, 576, 411, 281, 23829, 3030, 13, 400, 264, 1900, 636, 294, 597, 321, 3048, 51384], "temperature": 0.0, "avg_logprob": -0.07401814668074898, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.03357342258095741}, {"id": 1175, "seek": 682488, "start": 6845.28, "end": 6849.4400000000005, "text": " these variables is a bit of a black magic, like humans have come up with a few heuristics, but", "tokens": [51384, 613, 9102, 307, 257, 857, 295, 257, 2211, 5585, 11, 411, 6255, 362, 808, 493, 365, 257, 1326, 415, 374, 6006, 11, 457, 51592], "temperature": 0.0, "avg_logprob": -0.07401814668074898, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.03357342258095741}, {"id": 1176, "seek": 684944, "start": 6849.5199999999995, "end": 6854.96, "text": " they don't always work. And whenever you have this kind of setting, as long as you're assuming that", "tokens": [50368, 436, 500, 380, 1009, 589, 13, 400, 5699, 291, 362, 341, 733, 295, 3287, 11, 382, 938, 382, 291, 434, 11926, 300, 50640], "temperature": 0.0, "avg_logprob": -0.06772370465033878, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.004330982454121113}, {"id": 1177, "seek": 684944, "start": 6854.96, "end": 6860.08, "text": " you're naturally occurring data isn't always throwing the worst possible cases or adversarial", "tokens": [50640, 291, 434, 8195, 18386, 1412, 1943, 380, 1009, 10238, 264, 5855, 1944, 3331, 420, 17641, 44745, 50896], "temperature": 0.0, "avg_logprob": -0.06772370465033878, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.004330982454121113}, {"id": 1178, "seek": 684944, "start": 6860.08, "end": 6866.16, "text": " cases at you, you can usually rely on some kind of modeling technique, for example, a neural network", "tokens": [50896, 3331, 412, 291, 11, 291, 393, 2673, 10687, 322, 512, 733, 295, 15983, 6532, 11, 337, 1365, 11, 257, 18161, 3209, 51200], "temperature": 0.0, "avg_logprob": -0.06772370465033878, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.004330982454121113}, {"id": 1179, "seek": 684944, "start": 6866.16, "end": 6870.879999999999, "text": " to figure out which decisions the model should be taking. So, in this case, for example, for", "tokens": [51200, 281, 2573, 484, 597, 5327, 264, 2316, 820, 312, 1940, 13, 407, 11, 294, 341, 1389, 11, 337, 1365, 11, 337, 51436], "temperature": 0.0, "avg_logprob": -0.06772370465033878, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.004330982454121113}, {"id": 1180, "seek": 684944, "start": 6870.879999999999, "end": 6876.08, "text": " mip solving, DeepMind recently published a paper on this where you can treat mip problems as", "tokens": [51436, 275, 647, 12606, 11, 14895, 44, 471, 3938, 6572, 257, 3035, 322, 341, 689, 291, 393, 2387, 275, 647, 2740, 382, 51696], "temperature": 0.0, "avg_logprob": -0.06772370465033878, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.004330982454121113}, {"id": 1181, "seek": 687608, "start": 6876.08, "end": 6881.04, "text": " bipartite graphs, where you have variables on one side and the constraints on the other.", "tokens": [50364, 28741, 642, 24877, 11, 689, 291, 362, 9102, 322, 472, 1252, 293, 264, 18491, 322, 264, 661, 13, 50612], "temperature": 0.0, "avg_logprob": -0.0785420705687325, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.03112882748246193}, {"id": 1182, "seek": 687608, "start": 6881.04, "end": 6885.6, "text": " And you link them together if a variable appears in a constraint. Then they run a graph neural", "tokens": [50612, 400, 291, 2113, 552, 1214, 498, 257, 7006, 7038, 294, 257, 25534, 13, 1396, 436, 1190, 257, 4295, 18161, 50840], "temperature": 0.0, "avg_logprob": -0.0785420705687325, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.03112882748246193}, {"id": 1183, "seek": 687608, "start": 6885.6, "end": 6890.08, "text": " network, which, as we just discussed, is one of the flagship models in geometric deep learning,", "tokens": [50840, 3209, 11, 597, 11, 382, 321, 445, 7152, 11, 307, 472, 295, 264, 30400, 5245, 294, 33246, 2452, 2539, 11, 51064], "temperature": 0.0, "avg_logprob": -0.0785420705687325, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.03112882748246193}, {"id": 1184, "seek": 687608, "start": 6890.08, "end": 6896.48, "text": " over this bipartite graph to decide which variable the model should select next. And you can train", "tokens": [51064, 670, 341, 28741, 642, 4295, 281, 4536, 597, 7006, 264, 2316, 820, 3048, 958, 13, 400, 291, 393, 3847, 51384], "temperature": 0.0, "avg_logprob": -0.0785420705687325, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.03112882748246193}, {"id": 1185, "seek": 687608, "start": 6896.48, "end": 6901.12, "text": " this either as a separate kind of supervised technique to learn some kind of heuristic,", "tokens": [51384, 341, 2139, 382, 257, 4994, 733, 295, 46533, 6532, 281, 1466, 512, 733, 295, 415, 374, 3142, 11, 51616], "temperature": 0.0, "avg_logprob": -0.0785420705687325, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.03112882748246193}, {"id": 1186, "seek": 690112, "start": 6901.12, "end": 6906.08, "text": " or you can learn it as part of a more broader reinforcement learning framework, right, where", "tokens": [50364, 420, 291, 393, 1466, 309, 382, 644, 295, 257, 544, 13227, 29280, 2539, 8388, 11, 558, 11, 689, 50612], "temperature": 0.0, "avg_logprob": -0.08113041945866176, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0032719632145017385}, {"id": 1187, "seek": 690112, "start": 6906.08, "end": 6911.599999999999, "text": " the reward you get is how close you are to the solution or something like this. So this is one", "tokens": [50612, 264, 7782, 291, 483, 307, 577, 1998, 291, 366, 281, 264, 3827, 420, 746, 411, 341, 13, 407, 341, 307, 472, 50888], "temperature": 0.0, "avg_logprob": -0.08113041945866176, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0032719632145017385}, {"id": 1188, "seek": 690112, "start": 6912.24, "end": 6918.08, "text": " kind of clear way in which you can kind of have this synergy between geometric deep learning", "tokens": [50920, 733, 295, 1850, 636, 294, 597, 291, 393, 733, 295, 362, 341, 50163, 1296, 33246, 2452, 2539, 51212], "temperature": 0.0, "avg_logprob": -0.08113041945866176, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0032719632145017385}, {"id": 1189, "seek": 690112, "start": 6918.08, "end": 6925.12, "text": " architectures and solutions for, for example, program synthesis. But I would just like to", "tokens": [51212, 6331, 1303, 293, 6547, 337, 11, 337, 1365, 11, 1461, 30252, 13, 583, 286, 576, 445, 411, 281, 51564], "temperature": 0.0, "avg_logprob": -0.08113041945866176, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0032719632145017385}, {"id": 1190, "seek": 692512, "start": 6925.12, "end": 6931.44, "text": " offer another angle in which you can think of program synthesis as nothing other than just", "tokens": [50364, 2626, 1071, 5802, 294, 597, 291, 393, 519, 295, 1461, 30252, 382, 1825, 661, 813, 445, 50680], "temperature": 0.0, "avg_logprob": -0.06798984864178825, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.10809442400932312}, {"id": 1191, "seek": 692512, "start": 6931.44, "end": 6937.28, "text": " one more way to do language modeling, right, because synthesizing a program is not that different to", "tokens": [50680, 472, 544, 636, 281, 360, 2856, 15983, 11, 558, 11, 570, 26617, 3319, 257, 1461, 307, 406, 300, 819, 281, 50972], "temperature": 0.0, "avg_logprob": -0.06798984864178825, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.10809442400932312}, {"id": 1192, "seek": 692512, "start": 6937.28, "end": 6943.599999999999, "text": " synthesizing a sentence, maybe with a more stringent check on syntax and so forth. But, you know,", "tokens": [50972, 26617, 3319, 257, 8174, 11, 1310, 365, 257, 544, 6798, 317, 1520, 322, 28431, 293, 370, 5220, 13, 583, 11, 291, 458, 11, 51288], "temperature": 0.0, "avg_logprob": -0.06798984864178825, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.10809442400932312}, {"id": 1193, "seek": 692512, "start": 6943.599999999999, "end": 6948.88, "text": " any technique that is applied to language modeling could, in principle, be applied for", "tokens": [51288, 604, 6532, 300, 307, 6456, 281, 2856, 15983, 727, 11, 294, 8665, 11, 312, 6456, 337, 51552], "temperature": 0.0, "avg_logprob": -0.06798984864178825, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.10809442400932312}, {"id": 1194, "seek": 694888, "start": 6948.96, "end": 6955.4400000000005, "text": " program synthesis. And something that we will be discussing, I believe, later during this conversation,", "tokens": [50368, 1461, 30252, 13, 400, 746, 300, 321, 486, 312, 10850, 11, 286, 1697, 11, 1780, 1830, 341, 3761, 11, 50692], "temperature": 0.0, "avg_logprob": -0.07561184315199262, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.04883961006999016}, {"id": 1195, "seek": 694888, "start": 6956.24, "end": 6961.12, "text": " one of the flagship models of geometric deep learning is indeed the transformer, which", "tokens": [50732, 472, 295, 264, 30400, 5245, 295, 33246, 2452, 2539, 307, 6451, 264, 31782, 11, 597, 50976], "temperature": 0.0, "avg_logprob": -0.07561184315199262, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.04883961006999016}, {"id": 1196, "seek": 694888, "start": 6962.08, "end": 6968.0, "text": " we show in our book, and elucidate why it can be seen as a very specific case of a graph neural", "tokens": [51024, 321, 855, 294, 527, 1446, 11, 293, 806, 1311, 327, 473, 983, 309, 393, 312, 1612, 382, 257, 588, 2685, 1389, 295, 257, 4295, 18161, 51320], "temperature": 0.0, "avg_logprob": -0.07561184315199262, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.04883961006999016}, {"id": 1197, "seek": 694888, "start": 6968.0, "end": 6974.88, "text": " network. And that's one of the flagship models of language modeling. So basically, that's also", "tokens": [51320, 3209, 13, 400, 300, 311, 472, 295, 264, 30400, 5245, 295, 2856, 15983, 13, 407, 1936, 11, 300, 311, 611, 51664], "temperature": 0.0, "avg_logprob": -0.07561184315199262, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.04883961006999016}, {"id": 1198, "seek": 697488, "start": 6974.88, "end": 6980.88, "text": " one more way to to unify. Like, you know, just because the end output is discrete doesn't mean", "tokens": [50364, 472, 544, 636, 281, 281, 517, 2505, 13, 1743, 11, 291, 458, 11, 445, 570, 264, 917, 5598, 307, 27706, 1177, 380, 914, 50664], "temperature": 0.0, "avg_logprob": -0.11293614994395863, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.014274544082581997}, {"id": 1199, "seek": 697488, "start": 6980.88, "end": 6986.400000000001, "text": " that you cannot reason about it using representations that are internally vector vector based.", "tokens": [50664, 300, 291, 2644, 1778, 466, 309, 1228, 33358, 300, 366, 19501, 8062, 8062, 2361, 13, 50940], "temperature": 0.0, "avg_logprob": -0.11293614994395863, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.014274544082581997}, {"id": 1200, "seek": 697488, "start": 6987.28, "end": 6993.76, "text": " Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information", "tokens": [50984, 34695, 271, 761, 1833, 302, 10932, 484, 300, 456, 390, 341, 10390, 310, 8488, 13, 407, 291, 393, 12240, 27706, 1589, 51308], "temperature": 0.0, "avg_logprob": -0.11293614994395863, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.014274544082581997}, {"id": 1201, "seek": 697488, "start": 6993.76, "end": 6998.56, "text": " into a continuous representation. But the manifold needs to be smooth, it needs to be", "tokens": [51308, 666, 257, 10957, 10290, 13, 583, 264, 47138, 2203, 281, 312, 5508, 11, 309, 2203, 281, 312, 51548], "temperature": 0.0, "avg_logprob": -0.11293614994395863, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.014274544082581997}, {"id": 1202, "seek": 697488, "start": 6998.56, "end": 7004.32, "text": " learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete", "tokens": [51548, 1466, 712, 11, 309, 2203, 281, 312, 44902, 1166, 294, 3687, 13, 407, 286, 1194, 300, 390, 983, 321, 362, 613, 27706, 51836], "temperature": 0.0, "avg_logprob": -0.11293614994395863, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.014274544082581997}, {"id": 1203, "seek": 700432, "start": 7004.32, "end": 7009.5199999999995, "text": " program searches. But then you have this exponential blow up. But maybe that search space, because", "tokens": [50364, 1461, 26701, 13, 583, 550, 291, 362, 341, 21510, 6327, 493, 13, 583, 1310, 300, 3164, 1901, 11, 570, 50624], "temperature": 0.0, "avg_logprob": -0.07115394575101835, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.006089154630899429}, {"id": 1204, "seek": 700432, "start": 7009.5199999999995, "end": 7014.799999999999, "text": " it's interpolative could be found using stochastic gradient descent, if you embed the discrete", "tokens": [50624, 309, 311, 44902, 1166, 727, 312, 1352, 1228, 342, 8997, 2750, 16235, 23475, 11, 498, 291, 12240, 264, 27706, 50888], "temperature": 0.0, "avg_logprob": -0.07115394575101835, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.006089154630899429}, {"id": 1205, "seek": 700432, "start": 7014.799999999999, "end": 7019.84, "text": " information into some kind of vector space. But Professor Bronstein, I wanted to throw it back", "tokens": [50888, 1589, 666, 512, 733, 295, 8062, 1901, 13, 583, 8419, 19544, 9089, 11, 286, 1415, 281, 3507, 309, 646, 51140], "temperature": 0.0, "avg_logprob": -0.07115394575101835, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.006089154630899429}, {"id": 1206, "seek": 700432, "start": 7019.84, "end": 7024.32, "text": " over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because", "tokens": [51140, 670, 281, 291, 13, 286, 914, 11, 983, 307, 309, 2726, 382, 257, 2212, 300, 8062, 7673, 366, 257, 665, 551, 30, 1436, 51364], "temperature": 0.0, "avg_logprob": -0.07115394575101835, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.006089154630899429}, {"id": 1207, "seek": 700432, "start": 7024.32, "end": 7030.08, "text": " everything we're doing here is embedding discrete information into these Euclidean vector spaces.", "tokens": [51364, 1203, 321, 434, 884, 510, 307, 12240, 3584, 27706, 1589, 666, 613, 462, 1311, 31264, 282, 8062, 7673, 13, 51652], "temperature": 0.0, "avg_logprob": -0.07115394575101835, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.006089154630899429}, {"id": 1208, "seek": 703008, "start": 7030.16, "end": 7035.44, "text": " Why are we doing that? There are multiple reasons why vector spaces are so popular in", "tokens": [50368, 1545, 366, 321, 884, 300, 30, 821, 366, 3866, 4112, 983, 8062, 7673, 366, 370, 3743, 294, 50632], "temperature": 0.0, "avg_logprob": -0.13439115203253113, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.0022834029514342546}, {"id": 1209, "seek": 703008, "start": 7035.44, "end": 7041.36, "text": " representation learning. Vectors are probably the most convenient representation for both humans", "tokens": [50632, 10290, 2539, 13, 691, 557, 830, 366, 1391, 264, 881, 10851, 10290, 337, 1293, 6255, 50928], "temperature": 0.0, "avg_logprob": -0.13439115203253113, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.0022834029514342546}, {"id": 1210, "seek": 703008, "start": 7041.36, "end": 7046.88, "text": " and computers. We can do for a number of operations with them, like addition or subtraction. We can", "tokens": [50928, 293, 10807, 13, 492, 393, 360, 337, 257, 1230, 295, 7705, 365, 552, 11, 411, 4500, 420, 16390, 313, 13, 492, 393, 51204], "temperature": 0.0, "avg_logprob": -0.13439115203253113, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.0022834029514342546}, {"id": 1211, "seek": 703008, "start": 7046.88, "end": 7052.0, "text": " represent them as arrays in the memory of the computer. They are also continuous objects,", "tokens": [51204, 2906, 552, 382, 41011, 294, 264, 4675, 295, 264, 3820, 13, 814, 366, 611, 10957, 6565, 11, 51460], "temperature": 0.0, "avg_logprob": -0.13439115203253113, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.0022834029514342546}, {"id": 1212, "seek": 703008, "start": 7052.0, "end": 7057.92, "text": " so it is very easy to use continuous optimization techniques in the vector spaces. It is difficult", "tokens": [51460, 370, 309, 307, 588, 1858, 281, 764, 10957, 19618, 7512, 294, 264, 8062, 7673, 13, 467, 307, 2252, 51756], "temperature": 0.0, "avg_logprob": -0.13439115203253113, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.0022834029514342546}, {"id": 1213, "seek": 705792, "start": 7057.92, "end": 7062.4800000000005, "text": " for Gamble to optimize a graph because it is discrete and requires combinatorial techniques.", "tokens": [50364, 337, 24723, 638, 281, 19719, 257, 4295, 570, 309, 307, 27706, 293, 7029, 2512, 31927, 831, 7512, 13, 50592], "temperature": 0.0, "avg_logprob": -0.11915863666337789, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0016087382100522518}, {"id": 1214, "seek": 705792, "start": 7062.4800000000005, "end": 7067.6, "text": " But in a vector representation, I just have a bunch of points that I can continuously move", "tokens": [50592, 583, 294, 257, 8062, 10290, 11, 286, 445, 362, 257, 3840, 295, 2793, 300, 286, 393, 15684, 1286, 50848], "temperature": 0.0, "avg_logprob": -0.11915863666337789, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0016087382100522518}, {"id": 1215, "seek": 705792, "start": 7067.6, "end": 7073.84, "text": " in a kind of dimensional space using standard gradient based techniques. Perhaps a more nuanced", "tokens": [50848, 294, 257, 733, 295, 18795, 1901, 1228, 3832, 16235, 2361, 7512, 13, 10517, 257, 544, 45115, 51160], "temperature": 0.0, "avg_logprob": -0.11915863666337789, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0016087382100522518}, {"id": 1216, "seek": 705792, "start": 7073.84, "end": 7079.2, "text": " question is what kind of structures can be represented in a vector space? And a typical", "tokens": [51160, 1168, 307, 437, 733, 295, 9227, 393, 312, 10379, 294, 257, 8062, 1901, 30, 400, 257, 7476, 51428], "temperature": 0.0, "avg_logprob": -0.11915863666337789, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0016087382100522518}, {"id": 1217, "seek": 705792, "start": 7079.2, "end": 7085.12, "text": " structure is some notion of similarity or distance. We want that the vector representations preserve", "tokens": [51428, 3877, 307, 512, 10710, 295, 32194, 420, 4560, 13, 492, 528, 300, 264, 8062, 33358, 15665, 51724], "temperature": 0.0, "avg_logprob": -0.11915863666337789, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0016087382100522518}, {"id": 1218, "seek": 708512, "start": 7085.12, "end": 7090.08, "text": " the distances between, let's say, original data points. And here we usually assume that", "tokens": [50364, 264, 22182, 1296, 11, 718, 311, 584, 11, 3380, 1412, 2793, 13, 400, 510, 321, 2673, 6552, 300, 50612], "temperature": 0.0, "avg_logprob": -0.1373248273676092, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.0009228602284565568}, {"id": 1219, "seek": 708512, "start": 7090.08, "end": 7095.28, "text": " the vector space is equipped with the standard Euclidean metric or norm, and we have a problem", "tokens": [50612, 264, 8062, 1901, 307, 15218, 365, 264, 3832, 462, 1311, 31264, 282, 20678, 420, 2026, 11, 293, 321, 362, 257, 1154, 50872], "temperature": 0.0, "avg_logprob": -0.1373248273676092, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.0009228602284565568}, {"id": 1220, "seek": 708512, "start": 7095.28, "end": 7101.36, "text": " from the domain of metric geometry of representing one metric space in another. And unfortunately,", "tokens": [50872, 490, 264, 9274, 295, 20678, 18426, 295, 13460, 472, 20678, 1901, 294, 1071, 13, 400, 7015, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1373248273676092, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.0009228602284565568}, {"id": 1221, "seek": 708512, "start": 7101.36, "end": 7106.4, "text": " the general answer here is negative. You cannot exactly embed an arbitrary metric in Euclidean", "tokens": [51176, 264, 2674, 1867, 510, 307, 3671, 13, 509, 2644, 2293, 12240, 364, 23211, 20678, 294, 462, 1311, 31264, 282, 51428], "temperature": 0.0, "avg_logprob": -0.1373248273676092, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.0009228602284565568}, {"id": 1222, "seek": 708512, "start": 7106.4, "end": 7112.0, "text": " space, but there are, of course, some results such as bogains theorem that, for Gamble, provides", "tokens": [51428, 1901, 11, 457, 456, 366, 11, 295, 1164, 11, 512, 3542, 1270, 382, 26132, 2315, 20904, 300, 11, 337, 24723, 638, 11, 6417, 51708], "temperature": 0.0, "avg_logprob": -0.1373248273676092, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.0009228602284565568}, {"id": 1223, "seek": 711200, "start": 7112.8, "end": 7118.24, "text": " bounds on the metric distortion in such cases. And in graph learning spaces with", "tokens": [50404, 29905, 322, 264, 20678, 28426, 294, 1270, 3331, 13, 400, 294, 4295, 2539, 7673, 365, 50676], "temperature": 0.0, "avg_logprob": -0.1422706383925218, "compression_ratio": 1.655430711610487, "no_speech_prob": 0.009650430642068386}, {"id": 1224, "seek": 711200, "start": 7118.24, "end": 7123.28, "text": " other more exotic geometries such as hyperbolic spaces, you have recently become popular with,", "tokens": [50676, 661, 544, 27063, 12956, 2244, 1270, 382, 9848, 65, 7940, 7673, 11, 291, 362, 3938, 1813, 3743, 365, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1422706383925218, "compression_ratio": 1.655430711610487, "no_speech_prob": 0.009650430642068386}, {"id": 1225, "seek": 711200, "start": 7123.28, "end": 7127.6, "text": " for example, papers of Ben Chamberlain, my colleague from Twitter, or Max Nicol from Facebook.", "tokens": [50928, 337, 1365, 11, 10577, 295, 3964, 25401, 75, 491, 11, 452, 13532, 490, 5794, 11, 420, 7402, 14776, 401, 490, 4384, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1422706383925218, "compression_ratio": 1.655430711610487, "no_speech_prob": 0.009650430642068386}, {"id": 1226, "seek": 711200, "start": 7128.24, "end": 7132.88, "text": " And you can see that in certain types of graphs, the number of neighbors", "tokens": [51176, 400, 291, 393, 536, 300, 294, 1629, 3467, 295, 24877, 11, 264, 1230, 295, 12512, 51408], "temperature": 0.0, "avg_logprob": -0.1422706383925218, "compression_ratio": 1.655430711610487, "no_speech_prob": 0.009650430642068386}, {"id": 1227, "seek": 711200, "start": 7132.88, "end": 7138.24, "text": " grows exponentially with the radios. If you look, for example, at the number of friends of friends", "tokens": [51408, 13156, 37330, 365, 264, 2843, 2717, 13, 759, 291, 574, 11, 337, 1365, 11, 412, 264, 1230, 295, 1855, 295, 1855, 51676], "temperature": 0.0, "avg_logprob": -0.1422706383925218, "compression_ratio": 1.655430711610487, "no_speech_prob": 0.009650430642068386}, {"id": 1228, "seek": 713824, "start": 7138.639999999999, "end": 7143.76, "text": " and so on in a social network, where we have this small world phenomenon, you can see that", "tokens": [50384, 293, 370, 322, 294, 257, 2093, 3209, 11, 689, 321, 362, 341, 1359, 1002, 14029, 11, 291, 393, 536, 300, 50640], "temperature": 0.0, "avg_logprob": -0.1482103282007678, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0025000323075801134}, {"id": 1229, "seek": 713824, "start": 7143.76, "end": 7150.24, "text": " it becomes exponentially large with the growth of the radios. And now when you try to embed", "tokens": [50640, 309, 3643, 37330, 2416, 365, 264, 4599, 295, 264, 2843, 2717, 13, 400, 586, 562, 291, 853, 281, 12240, 50964], "temperature": 0.0, "avg_logprob": -0.1482103282007678, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0025000323075801134}, {"id": 1230, "seek": 713824, "start": 7150.24, "end": 7154.48, "text": " this graph in Euclidean space, it will become very crowded because in the Euclidean space,", "tokens": [50964, 341, 4295, 294, 462, 1311, 31264, 282, 1901, 11, 309, 486, 1813, 588, 21634, 570, 294, 264, 462, 1311, 31264, 282, 1901, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1482103282007678, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0025000323075801134}, {"id": 1231, "seek": 713824, "start": 7154.48, "end": 7159.5199999999995, "text": " the volume of the metric ball grows polynomially with the radios. Think of the two-dimensional", "tokens": [51176, 264, 5523, 295, 264, 20678, 2594, 13156, 22560, 2270, 365, 264, 2843, 2717, 13, 6557, 295, 264, 732, 12, 18759, 51428], "temperature": 0.0, "avg_logprob": -0.1482103282007678, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0025000323075801134}, {"id": 1232, "seek": 713824, "start": 7159.5199999999995, "end": 7166.48, "text": " case that we all know from school, the area of a circle is pi radius squared, right? The volume", "tokens": [51428, 1389, 300, 321, 439, 458, 490, 1395, 11, 264, 1859, 295, 257, 6329, 307, 3895, 15845, 8889, 11, 558, 30, 440, 5523, 51776], "temperature": 0.0, "avg_logprob": -0.1482103282007678, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0025000323075801134}, {"id": 1233, "seek": 716648, "start": 7166.48, "end": 7171.679999999999, "text": " of a ball is exponential with a dimension. So we inevitably need to increase the dimension", "tokens": [50364, 295, 257, 2594, 307, 21510, 365, 257, 10139, 13, 407, 321, 28171, 643, 281, 3488, 264, 10139, 50624], "temperature": 0.0, "avg_logprob": -0.07503800921969944, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.003860756754875183}, {"id": 1234, "seek": 716648, "start": 7171.679999999999, "end": 7177.839999999999, "text": " of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very", "tokens": [50624, 295, 264, 12240, 3584, 281, 652, 1901, 337, 613, 12512, 13, 682, 264, 9848, 65, 7940, 1901, 11, 264, 2590, 307, 588, 50932], "temperature": 0.0, "avg_logprob": -0.07503800921969944, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.003860756754875183}, {"id": 1235, "seek": 716648, "start": 7177.839999999999, "end": 7183.28, "text": " different because the volume grows exponentially with the radios. So it is way more convenient", "tokens": [50932, 819, 570, 264, 5523, 13156, 37330, 365, 264, 2843, 2717, 13, 407, 309, 307, 636, 544, 10851, 51204], "temperature": 0.0, "avg_logprob": -0.07503800921969944, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.003860756754875183}, {"id": 1236, "seek": 716648, "start": 7184.48, "end": 7188.48, "text": " to use these spaces for graph embeddings. And in fact, recent papers show that", "tokens": [51264, 281, 764, 613, 7673, 337, 4295, 12240, 29432, 13, 400, 294, 1186, 11, 5162, 10577, 855, 300, 51464], "temperature": 0.0, "avg_logprob": -0.07503800921969944, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.003860756754875183}, {"id": 1237, "seek": 716648, "start": 7189.28, "end": 7193.2, "text": " to achieve the same error in embedding of a graph in the hyperbolic space with, let's say,", "tokens": [51504, 281, 4584, 264, 912, 6713, 294, 12240, 3584, 295, 257, 4295, 294, 264, 9848, 65, 7940, 1901, 365, 11, 718, 311, 584, 11, 51700], "temperature": 0.0, "avg_logprob": -0.07503800921969944, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.003860756754875183}, {"id": 1238, "seek": 719320, "start": 7193.2, "end": 7197.76, "text": " 10 dimensions, you would require something like a 100-dimensional Euclidean space.", "tokens": [50364, 1266, 12819, 11, 291, 576, 3651, 746, 411, 257, 2319, 12, 18759, 462, 1311, 31264, 282, 1901, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1604789243925602, "compression_ratio": 1.5738831615120275, "no_speech_prob": 0.0013034341391175985}, {"id": 1239, "seek": 719320, "start": 7198.72, "end": 7202.72, "text": " Of course, I should say that metrics are just one example of a structure. So the general answer", "tokens": [50640, 2720, 1164, 11, 286, 820, 584, 300, 16367, 366, 445, 472, 1365, 295, 257, 3877, 13, 407, 264, 2674, 1867, 50840], "temperature": 0.0, "avg_logprob": -0.1604789243925602, "compression_ratio": 1.5738831615120275, "no_speech_prob": 0.0013034341391175985}, {"id": 1240, "seek": 719320, "start": 7202.72, "end": 7207.599999999999, "text": " to the question whether a vector space is a good model for representing data is, as usual,", "tokens": [50840, 281, 264, 1168, 1968, 257, 8062, 1901, 307, 257, 665, 2316, 337, 13460, 1412, 307, 11, 382, 7713, 11, 51084], "temperature": 0.0, "avg_logprob": -0.1604789243925602, "compression_ratio": 1.5738831615120275, "no_speech_prob": 0.0013034341391175985}, {"id": 1241, "seek": 719320, "start": 7207.599999999999, "end": 7214.639999999999, "text": " it depends. You mentioned language, Peter, and maybe to both of you, do you think there is a", "tokens": [51084, 309, 5946, 13, 509, 2835, 2856, 11, 6508, 11, 293, 1310, 281, 1293, 295, 291, 11, 360, 291, 519, 456, 307, 257, 51436], "temperature": 0.0, "avg_logprob": -0.1604789243925602, "compression_ratio": 1.5738831615120275, "no_speech_prob": 0.0013034341391175985}, {"id": 1242, "seek": 719320, "start": 7216.16, "end": 7223.04, "text": " geometry to language itself? I mean, obviously, we know about embedding spaces and close things", "tokens": [51512, 18426, 281, 2856, 2564, 30, 286, 914, 11, 2745, 11, 321, 458, 466, 12240, 3584, 7673, 293, 1998, 721, 51856], "temperature": 0.0, "avg_logprob": -0.1604789243925602, "compression_ratio": 1.5738831615120275, "no_speech_prob": 0.0013034341391175985}, {"id": 1243, "seek": 722320, "start": 7223.84, "end": 7230.16, "text": " somehow share meaning and so on. Do you think it goes beyond that? Because, like,", "tokens": [50396, 6063, 2073, 3620, 293, 370, 322, 13, 1144, 291, 519, 309, 1709, 4399, 300, 30, 1436, 11, 411, 11, 50712], "temperature": 0.0, "avg_logprob": -0.11851588487625123, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0028528550174087286}, {"id": 1244, "seek": 722320, "start": 7230.16, "end": 7236.48, "text": " do you think there is an inherent geometry to language itself and sort of the meaning of what", "tokens": [50712, 360, 291, 519, 456, 307, 364, 26387, 18426, 281, 2856, 2564, 293, 1333, 295, 264, 3620, 295, 437, 51028], "temperature": 0.0, "avg_logprob": -0.11851588487625123, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0028528550174087286}, {"id": 1245, "seek": 722320, "start": 7236.48, "end": 7243.28, "text": " we want to transmit and how that relates to each other? What you probably mentioned is the", "tokens": [51028, 321, 528, 281, 17831, 293, 577, 300, 16155, 281, 1184, 661, 30, 708, 291, 1391, 2835, 307, 264, 51368], "temperature": 0.0, "avg_logprob": -0.11851588487625123, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0028528550174087286}, {"id": 1246, "seek": 722320, "start": 7243.28, "end": 7249.84, "text": " famous series of papers from Facebook where unsupervised language translation can be done", "tokens": [51368, 4618, 2638, 295, 10577, 490, 4384, 689, 2693, 12879, 24420, 2856, 12853, 393, 312, 1096, 51696], "temperature": 0.0, "avg_logprob": -0.11851588487625123, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0028528550174087286}, {"id": 1247, "seek": 724984, "start": 7249.84, "end": 7255.6, "text": " by a geometric alignment of the latent spaces. In my opinion, it's not something that describes", "tokens": [50364, 538, 257, 33246, 18515, 295, 264, 48994, 7673, 13, 682, 452, 4800, 11, 309, 311, 406, 746, 300, 15626, 50652], "temperature": 0.0, "avg_logprob": -0.08420636607151405, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.022390196099877357}, {"id": 1248, "seek": 724984, "start": 7255.6, "end": 7262.16, "text": " geometry of the language. It probably describes in a geometric way some semantics of the world.", "tokens": [50652, 18426, 295, 264, 2856, 13, 467, 1391, 15626, 294, 257, 33246, 636, 512, 4361, 45298, 295, 264, 1002, 13, 50980], "temperature": 0.0, "avg_logprob": -0.08420636607151405, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.022390196099877357}, {"id": 1249, "seek": 724984, "start": 7262.16, "end": 7266.08, "text": " And even though we have, linguistically speaking, very different languages like,", "tokens": [50980, 400, 754, 1673, 321, 362, 11, 21766, 20458, 4124, 11, 588, 819, 8650, 411, 11, 51176], "temperature": 0.0, "avg_logprob": -0.08420636607151405, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.022390196099877357}, {"id": 1250, "seek": 724984, "start": 7266.08, "end": 7271.84, "text": " let's say, English and Chinese, yet they describe the same reality. They describe the same world", "tokens": [51176, 718, 311, 584, 11, 3669, 293, 4649, 11, 1939, 436, 6786, 264, 912, 4103, 13, 814, 6786, 264, 912, 1002, 51464], "temperature": 0.0, "avg_logprob": -0.08420636607151405, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.022390196099877357}, {"id": 1251, "seek": 724984, "start": 7271.84, "end": 7277.68, "text": " where humans act. So it is probably reasonable to assume that the concepts that they describe", "tokens": [51464, 689, 6255, 605, 13, 407, 309, 307, 1391, 10585, 281, 6552, 300, 264, 10392, 300, 436, 6786, 51756], "temperature": 0.0, "avg_logprob": -0.08420636607151405, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.022390196099877357}, {"id": 1252, "seek": 727768, "start": 7277.76, "end": 7281.6, "text": " are similar. And also, while there are some theories and linguistics about", "tokens": [50368, 366, 2531, 13, 400, 611, 11, 1339, 456, 366, 512, 13667, 293, 21766, 6006, 466, 50560], "temperature": 0.0, "avg_logprob": -0.10063472901931916, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.012061666697263718}, {"id": 1253, "seek": 727768, "start": 7281.6, "end": 7288.320000000001, "text": " certain universal structures in languages that are shared, even though the specifics are different,", "tokens": [50560, 1629, 11455, 9227, 294, 8650, 300, 366, 5507, 11, 754, 1673, 264, 28454, 366, 819, 11, 50896], "temperature": 0.0, "avg_logprob": -0.10063472901931916, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.012061666697263718}, {"id": 1254, "seek": 727768, "start": 7288.88, "end": 7293.6, "text": " I think it's interesting to look maybe at non-human communications. I wouldn't probably", "tokens": [50924, 286, 519, 309, 311, 1880, 281, 574, 1310, 412, 2107, 12, 18796, 15163, 13, 286, 2759, 380, 1391, 51160], "temperature": 0.0, "avg_logprob": -0.10063472901931916, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.012061666697263718}, {"id": 1255, "seek": 727768, "start": 7293.6, "end": 7299.200000000001, "text": " use the term language because it's a little bit loaded and probably some purists will be shocked", "tokens": [51160, 764, 264, 1433, 2856, 570, 309, 311, 257, 707, 857, 13210, 293, 1391, 512, 1864, 1751, 486, 312, 12763, 51440], "temperature": 0.0, "avg_logprob": -0.10063472901931916, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.012061666697263718}, {"id": 1256, "seek": 727768, "start": 7299.200000000001, "end": 7303.76, "text": " by me saying that, for example, whales have a language, but we are studying the communication", "tokens": [51440, 538, 385, 1566, 300, 11, 337, 1365, 11, 32403, 362, 257, 2856, 11, 457, 321, 366, 7601, 264, 6101, 51668], "temperature": 0.0, "avg_logprob": -0.10063472901931916, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.012061666697263718}, {"id": 1257, "seek": 730376, "start": 7303.76, "end": 7308.96, "text": " of slow whales. So this is a big international collaboration called Project. And I don't think", "tokens": [50364, 295, 2964, 32403, 13, 407, 341, 307, 257, 955, 5058, 9363, 1219, 9849, 13, 400, 286, 500, 380, 519, 50624], "temperature": 0.0, "avg_logprob": -0.12381421602689303, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.03843652456998825}, {"id": 1258, "seek": 730376, "start": 7308.96, "end": 7315.6, "text": " that you can really model the concepts that whales need to describe and to deal with", "tokens": [50624, 300, 291, 393, 534, 2316, 264, 10392, 300, 32403, 643, 281, 6786, 293, 281, 2028, 365, 50956], "temperature": 0.0, "avg_logprob": -0.12381421602689303, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.03843652456998825}, {"id": 1259, "seek": 730376, "start": 7316.56, "end": 7321.52, "text": " in the same way as we humans do. So maybe a silly example, we can say in human languages,", "tokens": [51004, 294, 264, 912, 636, 382, 321, 6255, 360, 13, 407, 1310, 257, 11774, 1365, 11, 321, 393, 584, 294, 1952, 8650, 11, 51252], "temperature": 0.0, "avg_logprob": -0.12381421602689303, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.03843652456998825}, {"id": 1260, "seek": 730376, "start": 7321.52, "end": 7327.12, "text": " and probably it applies to every language, we can express a concept that something got wet.", "tokens": [51252, 293, 1391, 309, 13165, 281, 633, 2856, 11, 321, 393, 5109, 257, 3410, 300, 746, 658, 6630, 13, 51532], "temperature": 0.0, "avg_logprob": -0.12381421602689303, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.03843652456998825}, {"id": 1261, "seek": 730376, "start": 7327.12, "end": 7332.08, "text": " I don't think that a whale would even understand what it means by being wet because", "tokens": [51532, 286, 500, 380, 519, 300, 257, 25370, 576, 754, 1223, 437, 309, 1355, 538, 885, 6630, 570, 51780], "temperature": 0.0, "avg_logprob": -0.12381421602689303, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.03843652456998825}, {"id": 1262, "seek": 733208, "start": 7332.72, "end": 7337.6, "text": " the whale always lives in water. I would add to that maybe a slightly different view of geometry,", "tokens": [50396, 264, 25370, 1009, 2909, 294, 1281, 13, 286, 576, 909, 281, 300, 1310, 257, 4748, 819, 1910, 295, 18426, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08854703143634628, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.006180815864354372}, {"id": 1263, "seek": 733208, "start": 7337.6, "end": 7342.64, "text": " but it's all about the question of how far are you willing to go and still call it geometry.", "tokens": [50640, 457, 309, 311, 439, 466, 264, 1168, 295, 577, 1400, 366, 291, 4950, 281, 352, 293, 920, 818, 309, 18426, 13, 50892], "temperature": 0.0, "avg_logprob": -0.08854703143634628, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.006180815864354372}, {"id": 1264, "seek": 733208, "start": 7342.64, "end": 7347.76, "text": " Based on our proto book, at least, I tend to think of graph structures also as a form of", "tokens": [50892, 18785, 322, 527, 47896, 1446, 11, 412, 1935, 11, 286, 3928, 281, 519, 295, 4295, 9227, 611, 382, 257, 1254, 295, 51148], "temperature": 0.0, "avg_logprob": -0.08854703143634628, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.006180815864354372}, {"id": 1265, "seek": 733208, "start": 7347.76, "end": 7354.0, "text": " geometry, even though it's a bit more abstract. And within language, people might not always agree", "tokens": [51148, 18426, 11, 754, 1673, 309, 311, 257, 857, 544, 12649, 13, 400, 1951, 2856, 11, 561, 1062, 406, 1009, 3986, 51460], "temperature": 0.0, "avg_logprob": -0.08854703143634628, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.006180815864354372}, {"id": 1266, "seek": 733208, "start": 7354.0, "end": 7359.44, "text": " what this structure is like. But I think we can be fairly certain that there are explicit links", "tokens": [51460, 437, 341, 3877, 307, 411, 13, 583, 286, 519, 321, 393, 312, 6457, 1629, 300, 456, 366, 13691, 6123, 51732], "temperature": 0.0, "avg_logprob": -0.08854703143634628, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.006180815864354372}, {"id": 1267, "seek": 735944, "start": 7359.44, "end": 7365.12, "text": " between individual words as and when you use them in different forms, syntax trees, or just one", "tokens": [50364, 1296, 2609, 2283, 382, 293, 562, 291, 764, 552, 294, 819, 6422, 11, 28431, 5852, 11, 420, 445, 472, 50648], "temperature": 0.0, "avg_logprob": -0.07705339141513991, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00247038877569139}, {"id": 1268, "seek": 735944, "start": 7365.12, "end": 7371.2, "text": " word precedes another and so on and so forth. And while we may not be necessarily able to easily", "tokens": [50648, 1349, 16969, 279, 1071, 293, 370, 322, 293, 370, 5220, 13, 400, 1339, 321, 815, 406, 312, 4725, 1075, 281, 3612, 50952], "temperature": 0.0, "avg_logprob": -0.07705339141513991, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00247038877569139}, {"id": 1269, "seek": 735944, "start": 7371.2, "end": 7379.599999999999, "text": " say what is the geometric significance of one word, what we can look at is what is the local", "tokens": [50952, 584, 437, 307, 264, 33246, 17687, 295, 472, 1349, 11, 437, 321, 393, 574, 412, 307, 437, 307, 264, 2654, 51372], "temperature": 0.0, "avg_logprob": -0.07705339141513991, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00247038877569139}, {"id": 1270, "seek": 735944, "start": 7379.599999999999, "end": 7385.44, "text": " geometry of the words that you tend to use around it. And I mean, this kind of principle has been", "tokens": [51372, 18426, 295, 264, 2283, 300, 291, 3928, 281, 764, 926, 309, 13, 400, 286, 914, 11, 341, 733, 295, 8665, 575, 668, 51664], "temperature": 0.0, "avg_logprob": -0.07705339141513991, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00247038877569139}, {"id": 1271, "seek": 738544, "start": 7385.44, "end": 7398.48, "text": " used all over the place. That has then been extended to graph structured observations,", "tokens": [50364, 1143, 439, 670, 264, 1081, 13, 663, 575, 550, 668, 10913, 281, 4295, 18519, 18163, 11, 51016], "temperature": 0.0, "avg_logprob": -0.13869836732938692, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.010979804210364819}, {"id": 1272, "seek": 738544, "start": 7398.48, "end": 7402.5599999999995, "text": " generally with models like deep walk and note to back basically the same idea,", "tokens": [51016, 5101, 365, 5245, 411, 2452, 1792, 293, 3637, 281, 646, 1936, 264, 912, 1558, 11, 51220], "temperature": 0.0, "avg_logprob": -0.13869836732938692, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.010979804210364819}, {"id": 1273, "seek": 738544, "start": 7402.5599999999995, "end": 7408.0, "text": " treat a nodes representation as everything that's around it. Basically, the reason why I think that", "tokens": [51220, 2387, 257, 13891, 10290, 382, 1203, 300, 311, 926, 309, 13, 8537, 11, 264, 1778, 983, 286, 519, 300, 51492], "temperature": 0.0, "avg_logprob": -0.13869836732938692, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.010979804210364819}, {"id": 1274, "seek": 738544, "start": 7408.0, "end": 7412.719999999999, "text": " analyzing this local topology of how words are used with each other is very powerful.", "tokens": [51492, 23663, 341, 2654, 1192, 1793, 295, 577, 2283, 366, 1143, 365, 1184, 661, 307, 588, 4005, 13, 51728], "temperature": 0.0, "avg_logprob": -0.13869836732938692, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.010979804210364819}, {"id": 1275, "seek": 741272, "start": 7413.68, "end": 7419.280000000001, "text": " I've reinforced that recently, precisely because of the fact I've been delving into category theory,", "tokens": [50412, 286, 600, 31365, 300, 3938, 11, 13402, 570, 295, 264, 1186, 286, 600, 668, 1103, 798, 666, 7719, 5261, 11, 50692], "temperature": 0.0, "avg_logprob": -0.07441551885872244, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.028402142226696014}, {"id": 1276, "seek": 741272, "start": 7419.280000000001, "end": 7423.6, "text": " because in category theory, your nodes are basically atoms, you're not allowed to look", "tokens": [50692, 570, 294, 7719, 5261, 11, 428, 13891, 366, 1936, 16871, 11, 291, 434, 406, 4350, 281, 574, 50908], "temperature": 0.0, "avg_logprob": -0.07441551885872244, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.028402142226696014}, {"id": 1277, "seek": 741272, "start": 7423.6, "end": 7428.8, "text": " inside them, you assume they're this undivisible unit of information. And everything you can", "tokens": [50908, 1854, 552, 11, 291, 6552, 436, 434, 341, 674, 592, 271, 964, 4985, 295, 1589, 13, 400, 1203, 291, 393, 51168], "temperature": 0.0, "avg_logprob": -0.07441551885872244, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.028402142226696014}, {"id": 1278, "seek": 741272, "start": 7428.8, "end": 7435.2, "text": " conclude about the atoms comes from the arrows between them. So using this very simple concept", "tokens": [51168, 16886, 466, 264, 16871, 1487, 490, 264, 19669, 1296, 552, 13, 407, 1228, 341, 588, 2199, 3410, 51488], "temperature": 0.0, "avg_logprob": -0.07441551885872244, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.028402142226696014}, {"id": 1279, "seek": 741272, "start": 7435.2, "end": 7441.12, "text": " with a few additional constraints like compositionality, you can, for example, tell me what are all the", "tokens": [51488, 365, 257, 1326, 4497, 18491, 411, 12686, 1860, 11, 291, 393, 11, 337, 1365, 11, 980, 385, 437, 366, 439, 264, 51784], "temperature": 0.0, "avg_logprob": -0.07441551885872244, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.028402142226696014}, {"id": 1280, "seek": 744112, "start": 7441.12, "end": 7446.32, "text": " elements of a set, even though you've abstracted that sets to a single point, just by analyzing", "tokens": [50364, 4959, 295, 257, 992, 11, 754, 1673, 291, 600, 12649, 292, 300, 6352, 281, 257, 2167, 935, 11, 445, 538, 23663, 50624], "temperature": 0.0, "avg_logprob": -0.13386383763066045, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006186708342283964}, {"id": 1281, "seek": 744112, "start": 7446.32, "end": 7451.28, "text": " the arrows between all sets, you can tell me what are all the elements inside a set. So thinking", "tokens": [50624, 264, 19669, 1296, 439, 6352, 11, 291, 393, 980, 385, 437, 366, 439, 264, 4959, 1854, 257, 992, 13, 407, 1953, 50872], "temperature": 0.0, "avg_logprob": -0.13386383763066045, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006186708342283964}, {"id": 1282, "seek": 744112, "start": 7451.28, "end": 7457.04, "text": " about this, I do believe that it is possible to reason about geometric, you know, word to", "tokens": [50872, 466, 341, 11, 286, 360, 1697, 300, 309, 307, 1944, 281, 1778, 466, 33246, 11, 291, 458, 11, 1349, 281, 51160], "temperature": 0.0, "avg_logprob": -0.13386383763066045, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006186708342283964}, {"id": 1283, "seek": 744112, "start": 7457.04, "end": 7461.68, "text": " vex, for example, does this with the assumption that the structure of the", "tokens": [51160, 1241, 87, 11, 337, 1365, 11, 775, 341, 365, 264, 15302, 300, 264, 3877, 295, 264, 51392], "temperature": 0.0, "avg_logprob": -0.13386383763066045, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006186708342283964}, {"id": 1284, "seek": 744112, "start": 7461.68, "end": 7466.72, "text": " are we approaching this at the right level, though, because people have said for quite a", "tokens": [51392, 366, 321, 14908, 341, 412, 264, 558, 1496, 11, 1673, 11, 570, 561, 362, 848, 337, 1596, 257, 51644], "temperature": 0.0, "avg_logprob": -0.13386383763066045, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006186708342283964}, {"id": 1285, "seek": 746672, "start": 7466.72, "end": 7471.76, "text": " long time that there's a difference between syntax and semantics. And you could look at the", "tokens": [50364, 938, 565, 300, 456, 311, 257, 2649, 1296, 28431, 293, 4361, 45298, 13, 400, 291, 727, 574, 412, 264, 50616], "temperature": 0.0, "avg_logprob": -0.08660979704423384, "compression_ratio": 1.8056872037914693, "no_speech_prob": 0.02664434164762497}, {"id": 1286, "seek": 746672, "start": 7471.76, "end": 7476.88, "text": " geometrical structure of spoken language. Or, for example, you could look at the topology of", "tokens": [50616, 12956, 15888, 3877, 295, 10759, 2856, 13, 1610, 11, 337, 1365, 11, 291, 727, 574, 412, 264, 1192, 1793, 295, 50872], "temperature": 0.0, "avg_logprob": -0.08660979704423384, "compression_ratio": 1.8056872037914693, "no_speech_prob": 0.02664434164762497}, {"id": 1287, "seek": 746672, "start": 7476.88, "end": 7481.92, "text": " the connections in your brain, the topology of, you know, reference frames in your brain is how", "tokens": [50872, 264, 9271, 294, 428, 3567, 11, 264, 1192, 1793, 295, 11, 291, 458, 11, 6408, 12083, 294, 428, 3567, 307, 577, 51124], "temperature": 0.0, "avg_logprob": -0.08660979704423384, "compression_ratio": 1.8056872037914693, "no_speech_prob": 0.02664434164762497}, {"id": 1288, "seek": 746672, "start": 7481.92, "end": 7488.400000000001, "text": " you actually have learned concepts. Would looking at the topology of spoken language tell you enough", "tokens": [51124, 291, 767, 362, 3264, 10392, 13, 6068, 1237, 412, 264, 1192, 1793, 295, 10759, 2856, 980, 291, 1547, 51448], "temperature": 0.0, "avg_logprob": -0.08660979704423384, "compression_ratio": 1.8056872037914693, "no_speech_prob": 0.02664434164762497}, {"id": 1289, "seek": 748840, "start": 7488.4, "end": 7497.599999999999, "text": " about abstract categories? That's a good question. I think that if that kind of information is", "tokens": [50364, 466, 12649, 10479, 30, 663, 311, 257, 665, 1168, 13, 286, 519, 300, 498, 300, 733, 295, 1589, 307, 50824], "temperature": 0.0, "avg_logprob": -0.05797527254242258, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08748769015073776}, {"id": 1290, "seek": 748840, "start": 7497.599999999999, "end": 7502.879999999999, "text": " necessary, like if the atoms by themselves won't tell you everything. One thing that we actually", "tokens": [50824, 4818, 11, 411, 498, 264, 16871, 538, 2969, 1582, 380, 980, 291, 1203, 13, 1485, 551, 300, 321, 767, 51088], "temperature": 0.0, "avg_logprob": -0.05797527254242258, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08748769015073776}, {"id": 1291, "seek": 748840, "start": 7502.879999999999, "end": 7507.28, "text": " very commonly do in graph representation learning is assume this sort of hierarchical approach,", "tokens": [51088, 588, 12719, 360, 294, 4295, 10290, 2539, 307, 6552, 341, 1333, 295, 35250, 804, 3109, 11, 51308], "temperature": 0.0, "avg_logprob": -0.05797527254242258, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08748769015073776}, {"id": 1292, "seek": 748840, "start": 7507.28, "end": 7512.799999999999, "text": " where you have like the ground level with your actual individual notes, and then you come up", "tokens": [51308, 689, 291, 362, 411, 264, 2727, 1496, 365, 428, 3539, 2609, 5570, 11, 293, 550, 291, 808, 493, 51584], "temperature": 0.0, "avg_logprob": -0.05797527254242258, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08748769015073776}, {"id": 1293, "seek": 748840, "start": 7512.799999999999, "end": 7517.04, "text": " with some kind of additional hierarchy that tells you either something about intermediate", "tokens": [51584, 365, 512, 733, 295, 4497, 22333, 300, 5112, 291, 2139, 746, 466, 19376, 51796], "temperature": 0.0, "avg_logprob": -0.05797527254242258, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.08748769015073776}, {"id": 1294, "seek": 751704, "start": 7517.04, "end": 7522.88, "text": " topologies in a graph, or intermediate structures that you care about in this graph, or any abstract", "tokens": [50364, 1192, 6204, 294, 257, 4295, 11, 420, 19376, 9227, 300, 291, 1127, 466, 294, 341, 4295, 11, 420, 604, 12649, 50656], "temperature": 0.0, "avg_logprob": -0.06790680151719314, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.011152161285281181}, {"id": 1295, "seek": 751704, "start": 7522.88, "end": 7527.04, "text": " concepts you might have extracted. And then there's additional links being drawn between these to", "tokens": [50656, 10392, 291, 1062, 362, 34086, 13, 400, 550, 456, 311, 4497, 6123, 885, 10117, 1296, 613, 281, 50864], "temperature": 0.0, "avg_logprob": -0.06790680151719314, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.011152161285281181}, {"id": 1296, "seek": 751704, "start": 7527.04, "end": 7533.6, "text": " kind of reinforce the knowledge that the graph net can capture. So I think if you have knowledge of", "tokens": [50864, 733, 295, 22634, 264, 3601, 300, 264, 4295, 2533, 393, 7983, 13, 407, 286, 519, 498, 291, 362, 3601, 295, 51192], "temperature": 0.0, "avg_logprob": -0.06790680151719314, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.011152161285281181}, {"id": 1297, "seek": 751704, "start": 7533.6, "end": 7538.08, "text": " some abstract concepts that are relevant for your particular task, you can attach them as", "tokens": [51192, 512, 12649, 10392, 300, 366, 7340, 337, 428, 1729, 5633, 11, 291, 393, 5085, 552, 382, 51416], "temperature": 0.0, "avg_logprob": -0.06790680151719314, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.011152161285281181}, {"id": 1298, "seek": 751704, "start": 7538.08, "end": 7543.04, "text": " additional pieces of information to this topology. Of course, the more exciting part is could we", "tokens": [51416, 4497, 3755, 295, 1589, 281, 341, 1192, 1793, 13, 2720, 1164, 11, 264, 544, 4670, 644, 307, 727, 321, 51664], "temperature": 0.0, "avg_logprob": -0.06790680151719314, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.011152161285281181}, {"id": 1299, "seek": 754304, "start": 7543.12, "end": 7547.84, "text": " maybe discover them automatically? But that is something that I don't think is potentially in", "tokens": [50368, 1310, 4411, 552, 6772, 30, 583, 300, 307, 746, 300, 286, 500, 380, 519, 307, 7263, 294, 50604], "temperature": 0.0, "avg_logprob": -0.08099860191345215, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.0018338555237278342}, {"id": 1300, "seek": 754304, "start": 7547.84, "end": 7554.32, "text": " scope for this question. When human interpreters need to translate from one language to another,", "tokens": [50604, 11923, 337, 341, 1168, 13, 1133, 1952, 17489, 1559, 643, 281, 13799, 490, 472, 2856, 281, 1071, 11, 50928], "temperature": 0.0, "avg_logprob": -0.08099860191345215, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.0018338555237278342}, {"id": 1301, "seek": 754304, "start": 7554.32, "end": 7559.44, "text": " they often need to deal with different structures. I think Turkish is actually an extreme example", "tokens": [50928, 436, 2049, 643, 281, 2028, 365, 819, 9227, 13, 286, 519, 18565, 307, 767, 364, 8084, 1365, 51184], "temperature": 0.0, "avg_logprob": -0.08099860191345215, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.0018338555237278342}, {"id": 1302, "seek": 754304, "start": 7559.44, "end": 7564.64, "text": " where the order of words is completely reversed. It implies that you need probably to hold well", "tokens": [51184, 689, 264, 1668, 295, 2283, 307, 2584, 30563, 13, 467, 18779, 300, 291, 643, 1391, 281, 1797, 731, 51444], "temperature": 0.0, "avg_logprob": -0.08099860191345215, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.0018338555237278342}, {"id": 1303, "seek": 754304, "start": 7564.64, "end": 7571.04, "text": " in computer science terms some kind of a buffer in your brain before you can make the translation", "tokens": [51444, 294, 3820, 3497, 2115, 512, 733, 295, 257, 21762, 294, 428, 3567, 949, 291, 393, 652, 264, 12853, 51764], "temperature": 0.0, "avg_logprob": -0.08099860191345215, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.0018338555237278342}, {"id": 1304, "seek": 757104, "start": 7571.04, "end": 7576.72, "text": " to another language. So it definitely imposes certain biological network structure in the brain.", "tokens": [50364, 281, 1071, 2856, 13, 407, 309, 2138, 704, 4201, 1629, 13910, 3209, 3877, 294, 264, 3567, 13, 50648], "temperature": 0.0, "avg_logprob": -0.09876474776825347, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.02341069094836712}, {"id": 1305, "seek": 757104, "start": 7576.72, "end": 7583.76, "text": " Another interesting observation that I read somewhere about the way that people remember", "tokens": [50648, 3996, 1880, 14816, 300, 286, 1401, 4079, 466, 264, 636, 300, 561, 1604, 51000], "temperature": 0.0, "avg_logprob": -0.09876474776825347, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.02341069094836712}, {"id": 1306, "seek": 757104, "start": 7583.76, "end": 7590.4, "text": " certain facts when they speak a certain language. So the particular example that was given is a", "tokens": [51000, 1629, 9130, 562, 436, 1710, 257, 1629, 2856, 13, 407, 264, 1729, 1365, 300, 390, 2212, 307, 257, 51332], "temperature": 0.0, "avg_logprob": -0.09876474776825347, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.02341069094836712}, {"id": 1307, "seek": 757104, "start": 7590.4, "end": 7598.4, "text": " person can remember a perpetrator of a crime and then gives testimony in court. And the reason", "tokens": [51332, 954, 393, 1604, 257, 16211, 19802, 295, 257, 7206, 293, 550, 2709, 15634, 294, 4753, 13, 400, 264, 1778, 51732], "temperature": 0.0, "avg_logprob": -0.09876474776825347, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.02341069094836712}, {"id": 1308, "seek": 759840, "start": 7598.4, "end": 7604.719999999999, "text": " is that in some languages, it is more common to use impersonal pronouns and the personal phrases.", "tokens": [50364, 307, 300, 294, 512, 8650, 11, 309, 307, 544, 2689, 281, 764, 38147, 304, 35883, 293, 264, 2973, 20312, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1320740989084994, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.11585882306098938}, {"id": 1309, "seek": 759840, "start": 7604.719999999999, "end": 7610.0, "text": " So you can say, for example, the object was broken. And in some languages, you would say that", "tokens": [50680, 407, 291, 393, 584, 11, 337, 1365, 11, 264, 2657, 390, 5463, 13, 400, 294, 512, 8650, 11, 291, 576, 584, 300, 50944], "temperature": 0.0, "avg_logprob": -0.1320740989084994, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.11585882306098938}, {"id": 1310, "seek": 759840, "start": 7610.0, "end": 7616.639999999999, "text": " somebody broke the object. So it appeared that languages were of these more impersonal constructions.", "tokens": [50944, 2618, 6902, 264, 2657, 13, 407, 309, 8516, 300, 8650, 645, 295, 613, 544, 38147, 304, 7690, 626, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1320740989084994, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.11585882306098938}, {"id": 1311, "seek": 759840, "start": 7616.639999999999, "end": 7622.4, "text": " People speaking these languages, I have hard time to remember the perpetrator. So the language", "tokens": [51276, 3432, 4124, 613, 8650, 11, 286, 362, 1152, 565, 281, 1604, 264, 16211, 19802, 13, 407, 264, 2856, 51564], "temperature": 0.0, "avg_logprob": -0.1320740989084994, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.11585882306098938}, {"id": 1312, "seek": 762240, "start": 7622.48, "end": 7628.719999999999, "text": " probably imposes a lot about the way that we perceive world, but it is probably not studied", "tokens": [50368, 1391, 704, 4201, 257, 688, 466, 264, 636, 300, 321, 20281, 1002, 11, 457, 309, 307, 1391, 406, 9454, 50680], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1313, "seek": 762240, "start": 7628.719999999999, "end": 7634.4, "text": " sufficiently. But there may be some fuzzy graph isomorphisms, though, between the languages.", "tokens": [50680, 31868, 13, 583, 456, 815, 312, 512, 34710, 4295, 307, 32702, 13539, 11, 1673, 11, 1296, 264, 8650, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1314, "seek": 762240, "start": 7635.28, "end": 7638.799999999999, "text": " I think there's something really magic about graphs. I think that's what we get into, because", "tokens": [51008, 286, 519, 456, 311, 746, 534, 5585, 466, 24877, 13, 286, 519, 300, 311, 437, 321, 483, 666, 11, 570, 51184], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1315, "seek": 762240, "start": 7638.799999999999, "end": 7642.4, "text": " your lecture series inspired me, actually, Professor Bronstein, where you were talking", "tokens": [51184, 428, 7991, 2638, 7547, 385, 11, 767, 11, 8419, 19544, 9089, 11, 689, 291, 645, 1417, 51364], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1316, "seek": 762240, "start": 7642.4, "end": 7646.799999999999, "text": " about all the different applications of graphs. But something that a lot of our guests talk about", "tokens": [51364, 466, 439, 264, 819, 5821, 295, 24877, 13, 583, 746, 300, 257, 688, 295, 527, 9804, 751, 466, 51584], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1317, "seek": 762240, "start": 7646.799999999999, "end": 7651.36, "text": " are knowledge graphs. Expert systems and the knowledge acquisition bottleneck were the", "tokens": [51584, 366, 3601, 24877, 13, 41255, 3652, 293, 264, 3601, 21668, 44641, 547, 645, 264, 51812], "temperature": 0.0, "avg_logprob": -0.11877562999725341, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.05139243230223656}, {"id": 1318, "seek": 765136, "start": 7651.36, "end": 7657.44, "text": " cause of the abject failure of good old fashioned AI or some symbolic AI systems in the 1980s. And", "tokens": [50364, 3082, 295, 264, 410, 1020, 7763, 295, 665, 1331, 40646, 7318, 420, 512, 25755, 7318, 3652, 294, 264, 13626, 82, 13, 400, 50668], "temperature": 0.0, "avg_logprob": -0.10411816173129612, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0008575546671636403}, {"id": 1319, "seek": 765136, "start": 7657.44, "end": 7661.759999999999, "text": " many hybrid or neuro symbolic folks today are still arguing that we need to have a discrete", "tokens": [50668, 867, 13051, 420, 16499, 25755, 4024, 965, 366, 920, 19697, 300, 321, 643, 281, 362, 257, 27706, 50884], "temperature": 0.0, "avg_logprob": -0.10411816173129612, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0008575546671636403}, {"id": 1320, "seek": 765136, "start": 7661.759999999999, "end": 7668.0, "text": " knowledge graph, either human designed or learned or evolved or emerged or some combination of those", "tokens": [50884, 3601, 4295, 11, 2139, 1952, 4761, 420, 3264, 420, 14178, 420, 20178, 420, 512, 6562, 295, 729, 51196], "temperature": 0.0, "avg_logprob": -0.10411816173129612, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0008575546671636403}, {"id": 1321, "seek": 765136, "start": 7668.5599999999995, "end": 7672.5599999999995, "text": " things I just said, depending on who you talk to. Now, critically, many go fi people think that", "tokens": [51224, 721, 286, 445, 848, 11, 5413, 322, 567, 291, 751, 281, 13, 823, 11, 22797, 11, 867, 352, 15848, 561, 519, 300, 51424], "temperature": 0.0, "avg_logprob": -0.10411816173129612, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0008575546671636403}, {"id": 1322, "seek": 765136, "start": 7672.5599999999995, "end": 7677.759999999999, "text": " most knowledge we have is acquired through reasoning, not learning, right, which is really,", "tokens": [51424, 881, 3601, 321, 362, 307, 17554, 807, 21577, 11, 406, 2539, 11, 558, 11, 597, 307, 534, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10411816173129612, "compression_ratio": 1.6348122866894197, "no_speech_prob": 0.0008575546671636403}, {"id": 1323, "seek": 767776, "start": 7677.76, "end": 7682.88, "text": " really interesting. So by reasoning, I mean extrapolating new knowledge from existing knowledge.", "tokens": [50364, 534, 1880, 13, 407, 538, 21577, 11, 286, 914, 48224, 990, 777, 3601, 490, 6741, 3601, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09817949930826823, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.013086432591080666}, {"id": 1324, "seek": 767776, "start": 7683.52, "end": 7687.6, "text": " It feels like graph neural networks could at least be part of the solution here. And in your", "tokens": [50652, 467, 3417, 411, 4295, 18161, 9590, 727, 412, 1935, 312, 644, 295, 264, 3827, 510, 13, 400, 294, 428, 50856], "temperature": 0.0, "avg_logprob": -0.09817949930826823, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.013086432591080666}, {"id": 1325, "seek": 767776, "start": 7687.6, "end": 7692.4800000000005, "text": " lecture series, you mentioned the work by Kramner, which was explainable GNN, where they use some", "tokens": [50856, 7991, 2638, 11, 291, 2835, 264, 589, 538, 591, 2356, 1193, 11, 597, 390, 2903, 712, 46411, 45, 11, 689, 436, 764, 512, 51100], "temperature": 0.0, "avg_logprob": -0.09817949930826823, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.013086432591080666}, {"id": 1326, "seek": 767776, "start": 7692.4800000000005, "end": 7697.92, "text": " kind of symbolic regression to get a symbolic model from a graph neural network. So do you think", "tokens": [51100, 733, 295, 25755, 24590, 281, 483, 257, 25755, 2316, 490, 257, 4295, 18161, 3209, 13, 407, 360, 291, 519, 51372], "temperature": 0.0, "avg_logprob": -0.09817949930826823, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.013086432591080666}, {"id": 1327, "seek": 767776, "start": 7697.92, "end": 7703.12, "text": " there's some really cool work we can do here? There is a little bit of divide in graph learning", "tokens": [51372, 456, 311, 512, 534, 1627, 589, 321, 393, 360, 510, 30, 821, 307, 257, 707, 857, 295, 9845, 294, 4295, 2539, 51632], "temperature": 0.0, "avg_logprob": -0.09817949930826823, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.013086432591080666}, {"id": 1328, "seek": 770312, "start": 7703.12, "end": 7708.16, "text": " literature. So people working on graph neural networks, and working on knowledge graphs, even", "tokens": [50364, 10394, 13, 407, 561, 1364, 322, 4295, 18161, 9590, 11, 293, 1364, 322, 3601, 24877, 11, 754, 50616], "temperature": 0.0, "avg_logprob": -0.13374832960275504, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.018635207787156105}, {"id": 1329, "seek": 770312, "start": 7708.16, "end": 7713.28, "text": " though, at least in principle, the methods are similar. For example, you typically do some form", "tokens": [50616, 1673, 11, 412, 1935, 294, 8665, 11, 264, 7150, 366, 2531, 13, 1171, 1365, 11, 291, 5850, 360, 512, 1254, 50872], "temperature": 0.0, "avg_logprob": -0.13374832960275504, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.018635207787156105}, {"id": 1330, "seek": 770312, "start": 7713.28, "end": 7718.24, "text": " of embedding of the nodes of the graph. Somehow these are distinct communities, probably,", "tokens": [50872, 295, 12240, 3584, 295, 264, 13891, 295, 264, 4295, 13, 28357, 613, 366, 10644, 4456, 11, 1391, 11, 51120], "temperature": 0.0, "avg_logprob": -0.13374832960275504, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.018635207787156105}, {"id": 1331, "seek": 770312, "start": 7718.24, "end": 7723.28, "text": " historically, they evolved in different fields. Yeah, so the paper of Krammer, this is really", "tokens": [51120, 16180, 11, 436, 14178, 294, 819, 7909, 13, 865, 11, 370, 264, 3035, 295, 591, 2356, 936, 11, 341, 307, 534, 51372], "temperature": 0.0, "avg_logprob": -0.13374832960275504, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.018635207787156105}, {"id": 1332, "seek": 770312, "start": 7723.28, "end": 7729.28, "text": " interesting because they use graphs to model physical systems, for example, and body problem", "tokens": [51372, 1880, 570, 436, 764, 24877, 281, 2316, 4001, 3652, 11, 337, 1365, 11, 293, 1772, 1154, 51672], "temperature": 0.0, "avg_logprob": -0.13374832960275504, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.018635207787156105}, {"id": 1333, "seek": 772928, "start": 7729.28, "end": 7734.24, "text": " when they have particles that interact. You can describe these interactions as a graph,", "tokens": [50364, 562, 436, 362, 10007, 300, 4648, 13, 509, 393, 6786, 613, 13280, 382, 257, 4295, 11, 50612], "temperature": 0.0, "avg_logprob": -0.13702228546142578, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.009074337780475616}, {"id": 1334, "seek": 772928, "start": 7734.8, "end": 7740.4, "text": " and you can use standard generic message passing functions to model the interactions.", "tokens": [50640, 293, 291, 393, 764, 3832, 19577, 3636, 8437, 6828, 281, 2316, 264, 13280, 13, 50920], "temperature": 0.0, "avg_logprob": -0.13702228546142578, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.009074337780475616}, {"id": 1335, "seek": 772928, "start": 7740.4, "end": 7746.32, "text": " Now, the step forward that they do is they replace these generic message passing functions", "tokens": [50920, 823, 11, 264, 1823, 2128, 300, 436, 360, 307, 436, 7406, 613, 19577, 3636, 8437, 6828, 51216], "temperature": 0.0, "avg_logprob": -0.13702228546142578, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.009074337780475616}, {"id": 1336, "seek": 772928, "start": 7746.32, "end": 7752.96, "text": " with symbolic equations. And not only that this allows to generalize better, but you also have", "tokens": [51216, 365, 25755, 11787, 13, 400, 406, 787, 300, 341, 4045, 281, 2674, 1125, 1101, 11, 457, 291, 611, 362, 51548], "temperature": 0.0, "avg_logprob": -0.13702228546142578, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.009074337780475616}, {"id": 1337, "seek": 775296, "start": 7752.96, "end": 7759.36, "text": " an interpretable system, you can recover from your data the laws of motion, right? And if you", "tokens": [50364, 364, 7302, 712, 1185, 11, 291, 393, 8114, 490, 428, 1412, 264, 6064, 295, 5394, 11, 558, 30, 400, 498, 291, 50684], "temperature": 0.0, "avg_logprob": -0.15529654242775656, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.1555660516023636}, {"id": 1338, "seek": 775296, "start": 7759.36, "end": 7764.08, "text": " think of how much time it took, historically, to people like Johannes Kepler, for example,", "tokens": [50684, 519, 295, 577, 709, 565, 309, 1890, 11, 16180, 11, 281, 561, 411, 48455, 3189, 22732, 11, 337, 1365, 11, 50920], "temperature": 0.0, "avg_logprob": -0.15529654242775656, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.1555660516023636}, {"id": 1339, "seek": 775296, "start": 7764.08, "end": 7771.28, "text": " he spent his entire life on analyzing astronomical observations to derive a law that now bears his", "tokens": [50920, 415, 4418, 702, 2302, 993, 322, 23663, 49035, 18163, 281, 28446, 257, 2101, 300, 586, 17276, 702, 51280], "temperature": 0.0, "avg_logprob": -0.15529654242775656, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.1555660516023636}, {"id": 1340, "seek": 775296, "start": 7771.28, "end": 7777.2, "text": " name, that describes the elliptic orbits of planets. Nowadays, with these methods, you can", "tokens": [51280, 1315, 11, 300, 15626, 264, 8284, 22439, 299, 43522, 295, 15126, 13, 28908, 11, 365, 613, 7150, 11, 291, 393, 51576], "temperature": 0.0, "avg_logprob": -0.15529654242775656, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.1555660516023636}, {"id": 1341, "seek": 777720, "start": 7777.2, "end": 7783.44, "text": " probably do it in a matter of seconds or maybe minutes. I think the point that particularly", "tokens": [50364, 1391, 360, 309, 294, 257, 1871, 295, 3949, 420, 1310, 2077, 13, 286, 519, 264, 935, 300, 4098, 50676], "temperature": 0.0, "avg_logprob": -0.08469967359907171, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.022440342232584953}, {"id": 1342, "seek": 777720, "start": 7783.44, "end": 7788.8, "text": " caught my attention in what you asked, Tim, was this interplay between graphs and reasoning", "tokens": [50676, 5415, 452, 3202, 294, 437, 291, 2351, 11, 7172, 11, 390, 341, 728, 2858, 1296, 24877, 293, 21577, 50944], "temperature": 0.0, "avg_logprob": -0.08469967359907171, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.022440342232584953}, {"id": 1343, "seek": 777720, "start": 7788.8, "end": 7795.5199999999995, "text": " and extrapolation and how that supports knowledge. Now, when it comes to how critical is this going", "tokens": [50944, 293, 48224, 399, 293, 577, 300, 9346, 3601, 13, 823, 11, 562, 309, 1487, 281, 577, 4924, 307, 341, 516, 51280], "temperature": 0.0, "avg_logprob": -0.08469967359907171, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.022440342232584953}, {"id": 1344, "seek": 777720, "start": 7795.5199999999995, "end": 7802.5599999999995, "text": " to be, it depends on the environment in which you put your agent. Like, is it a closed environment,", "tokens": [51280, 281, 312, 11, 309, 5946, 322, 264, 2823, 294, 597, 291, 829, 428, 9461, 13, 1743, 11, 307, 309, 257, 5395, 2823, 11, 51632], "temperature": 0.0, "avg_logprob": -0.08469967359907171, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.022440342232584953}, {"id": 1345, "seek": 780256, "start": 7802.56, "end": 7807.76, "text": " or is it an open ended environment where new information and new knowledge can come in", "tokens": [50364, 420, 307, 309, 364, 1269, 4590, 2823, 689, 777, 1589, 293, 777, 3601, 393, 808, 294, 50624], "temperature": 0.0, "avg_logprob": -0.10979214622860863, "compression_ratio": 1.6607773851590106, "no_speech_prob": 0.007009739987552166}, {"id": 1346, "seek": 780256, "start": 7807.76, "end": 7814.0, "text": " in principle at any time? This basically do want to build a neural scientist, or do you just want", "tokens": [50624, 294, 8665, 412, 604, 565, 30, 639, 1936, 360, 528, 281, 1322, 257, 18161, 12662, 11, 420, 360, 291, 445, 528, 50936], "temperature": 0.0, "avg_logprob": -0.10979214622860863, "compression_ratio": 1.6607773851590106, "no_speech_prob": 0.007009739987552166}, {"id": 1347, "seek": 780256, "start": 7814.0, "end": 7818.56, "text": " to build a neural exploiter that takes all the information available right now and then draws", "tokens": [50936, 281, 1322, 257, 18161, 12382, 1681, 300, 2516, 439, 264, 1589, 2435, 558, 586, 293, 550, 20045, 51164], "temperature": 0.0, "avg_logprob": -0.10979214622860863, "compression_ratio": 1.6607773851590106, "no_speech_prob": 0.007009739987552166}, {"id": 1348, "seek": 780256, "start": 7818.56, "end": 7825.280000000001, "text": " conclusions based on that. So if the system is closed worlds, you'll probably be able to get", "tokens": [51164, 22865, 2361, 322, 300, 13, 407, 498, 264, 1185, 307, 5395, 13401, 11, 291, 603, 1391, 312, 1075, 281, 483, 51500], "temperature": 0.0, "avg_logprob": -0.10979214622860863, "compression_ratio": 1.6607773851590106, "no_speech_prob": 0.007009739987552166}, {"id": 1349, "seek": 780256, "start": 7825.280000000001, "end": 7831.04, "text": " away without very explicit reasoning, especially if you have tons of data, because we've seen time", "tokens": [51500, 1314, 1553, 588, 13691, 21577, 11, 2318, 498, 291, 362, 9131, 295, 1412, 11, 570, 321, 600, 1612, 565, 51788], "temperature": 0.0, "avg_logprob": -0.10979214622860863, "compression_ratio": 1.6607773851590106, "no_speech_prob": 0.007009739987552166}, {"id": 1350, "seek": 783104, "start": 7831.04, "end": 7836.24, "text": " and time again that large scale models can kind of pick up on these regularities if they've seen", "tokens": [50364, 293, 565, 797, 300, 2416, 4373, 5245, 393, 733, 295, 1888, 493, 322, 613, 3890, 1088, 498, 436, 600, 1612, 50624], "temperature": 0.0, "avg_logprob": -0.0666598952218388, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.06269730627536774}, {"id": 1351, "seek": 783104, "start": 7836.24, "end": 7845.2, "text": " it often enough. But if I give you a solution that involves stacking, for example, n objects,", "tokens": [50624, 309, 2049, 1547, 13, 583, 498, 286, 976, 291, 257, 3827, 300, 11626, 41376, 11, 337, 1365, 11, 297, 6565, 11, 51072], "temperature": 0.0, "avg_logprob": -0.0666598952218388, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.06269730627536774}, {"id": 1352, "seek": 783104, "start": 7845.2, "end": 7850.8, "text": " and now I ask you to do the same kind of reasoning with two times n objects, the way in which we", "tokens": [51072, 293, 586, 286, 1029, 291, 281, 360, 264, 912, 733, 295, 21577, 365, 732, 1413, 297, 6565, 11, 264, 636, 294, 597, 321, 51352], "temperature": 0.0, "avg_logprob": -0.0666598952218388, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.06269730627536774}, {"id": 1353, "seek": 783104, "start": 7850.8, "end": 7856.08, "text": " optimize neural networks at least today is typically going to completely fall on its back", "tokens": [51352, 19719, 18161, 9590, 412, 1935, 965, 307, 5850, 516, 281, 2584, 2100, 322, 1080, 646, 51616], "temperature": 0.0, "avg_logprob": -0.0666598952218388, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.06269730627536774}, {"id": 1354, "seek": 785608, "start": 7856.08, "end": 7862.48, "text": " when you do something like this. So if you truly want to take whatever regularities you have come", "tokens": [50364, 562, 291, 360, 746, 411, 341, 13, 407, 498, 291, 4908, 528, 281, 747, 2035, 3890, 1088, 291, 362, 808, 50684], "temperature": 0.0, "avg_logprob": -0.057864585343529194, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.027571283280849457}, {"id": 1355, "seek": 785608, "start": 7862.48, "end": 7870.32, "text": " across in the world of the training data and hope to at least reasonably gracefully apply them to", "tokens": [50684, 2108, 294, 264, 1002, 295, 264, 3097, 1412, 293, 1454, 281, 412, 1935, 23551, 10042, 2277, 3079, 552, 281, 51076], "temperature": 0.0, "avg_logprob": -0.057864585343529194, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.027571283280849457}, {"id": 1356, "seek": 785608, "start": 7870.32, "end": 7879.84, "text": " new kinds of rules that come in the future, then you probably want your model to extrapolate to a", "tokens": [51076, 777, 3685, 295, 4474, 300, 808, 294, 264, 2027, 11, 550, 291, 1391, 528, 428, 2316, 281, 48224, 473, 281, 257, 51552], "temperature": 0.0, "avg_logprob": -0.057864585343529194, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.027571283280849457}, {"id": 1357, "seek": 787984, "start": 7879.84, "end": 7886.64, "text": " certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a", "tokens": [50364, 1629, 8396, 13, 400, 337, 341, 11, 412, 1935, 452, 10452, 9284, 299, 21577, 2132, 14642, 366, 257, 50704], "temperature": 0.0, "avg_logprob": -0.08723580706250536, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.05260331556200981}, {"id": 1358, "seek": 787984, "start": 7886.64, "end": 7894.08, "text": " very natural area to study under this lens, because they trivially extrapolate, you write an algorithm", "tokens": [50704, 588, 3303, 1859, 281, 2979, 833, 341, 6765, 11, 570, 436, 1376, 85, 2270, 48224, 473, 11, 291, 2464, 364, 9284, 51076], "temperature": 0.0, "avg_logprob": -0.08723580706250536, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.05260331556200981}, {"id": 1359, "seek": 787984, "start": 7894.08, "end": 7900.64, "text": " that does a particular thing on a set of n nodes, you can be you can usually mathematically prove", "tokens": [51076, 300, 775, 257, 1729, 551, 322, 257, 992, 295, 297, 13891, 11, 291, 393, 312, 291, 393, 2673, 44003, 7081, 51404], "temperature": 0.0, "avg_logprob": -0.08723580706250536, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.05260331556200981}, {"id": 1360, "seek": 787984, "start": 7900.64, "end": 7905.52, "text": " that it's going to do the same thing equally properly, maybe a bit more slowly, if you give it", "tokens": [51404, 300, 309, 311, 516, 281, 360, 264, 912, 551, 12309, 6108, 11, 1310, 257, 857, 544, 5692, 11, 498, 291, 976, 309, 51648], "temperature": 0.0, "avg_logprob": -0.08723580706250536, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.05260331556200981}, {"id": 1361, "seek": 790552, "start": 7905.52, "end": 7910.0, "text": " two times n nodes, right? This kind of guarantee typically doesn't come that easily with neural", "tokens": [50364, 732, 1413, 297, 13891, 11, 558, 30, 639, 733, 295, 10815, 5850, 1177, 380, 808, 300, 3612, 365, 18161, 50588], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1362, "seek": 790552, "start": 7910.0, "end": 7914.96, "text": " networks. And we found that you have to very carefully massage the way you train them, the", "tokens": [50588, 9590, 13, 400, 321, 1352, 300, 291, 362, 281, 588, 7500, 16145, 264, 636, 291, 3847, 552, 11, 264, 50836], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1363, "seek": 790552, "start": 7914.96, "end": 7919.92, "text": " kinds of data you feed to them, the kinds of inductive biases you feed into them, in order to", "tokens": [50836, 3685, 295, 1412, 291, 3154, 281, 552, 11, 264, 3685, 295, 31612, 488, 32152, 291, 3154, 666, 552, 11, 294, 1668, 281, 51084], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1364, "seek": 790552, "start": 7919.92, "end": 7924.88, "text": " get them to do something like this. So if extrapolation is something you truly need, and you know,", "tokens": [51084, 483, 552, 281, 360, 746, 411, 341, 13, 407, 498, 48224, 399, 307, 746, 291, 4908, 643, 11, 293, 291, 458, 11, 51332], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1365, "seek": 790552, "start": 7924.88, "end": 7929.040000000001, "text": " I think for artificial general intelligence, we're going to want to have at least some degree of", "tokens": [51332, 286, 519, 337, 11677, 2674, 7599, 11, 321, 434, 516, 281, 528, 281, 362, 412, 1935, 512, 4314, 295, 51540], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1366, "seek": 790552, "start": 7929.040000000001, "end": 7934.320000000001, "text": " extrapolation as new information will become available to our neural scientists, just as", "tokens": [51540, 48224, 399, 382, 777, 1589, 486, 1813, 2435, 281, 527, 18161, 7708, 11, 445, 382, 51804], "temperature": 0.0, "avg_logprob": -0.0755825923039363, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.045336127281188965}, {"id": 1367, "seek": 793432, "start": 7934.32, "end": 7941.36, "text": " you follow the era of time. Basically, for doing something like this, graph neural networks have", "tokens": [50364, 291, 1524, 264, 4249, 295, 565, 13, 8537, 11, 337, 884, 746, 411, 341, 11, 4295, 18161, 9590, 362, 50716], "temperature": 0.0, "avg_logprob": -0.044068366289138794, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.003323326585814357}, {"id": 1368, "seek": 793432, "start": 7941.36, "end": 7946.16, "text": " arisen as a very attractive primitive, because there's been a few really exciting theoretical", "tokens": [50716, 594, 11106, 382, 257, 588, 12609, 28540, 11, 570, 456, 311, 668, 257, 1326, 534, 4670, 20864, 50956], "temperature": 0.0, "avg_logprob": -0.044068366289138794, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.003323326585814357}, {"id": 1369, "seek": 793432, "start": 7946.16, "end": 7952.08, "text": " results coming out in recent years, saying that the operations of a graph neural network align", "tokens": [50956, 3542, 1348, 484, 294, 5162, 924, 11, 1566, 300, 264, 7705, 295, 257, 4295, 18161, 3209, 7975, 51252], "temperature": 0.0, "avg_logprob": -0.044068366289138794, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.003323326585814357}, {"id": 1370, "seek": 793432, "start": 7952.08, "end": 7957.679999999999, "text": " really, really well with dynamic programming algorithms. And dynamic programming is a very", "tokens": [51252, 534, 11, 534, 731, 365, 8546, 9410, 14642, 13, 400, 8546, 9410, 307, 257, 588, 51532], "temperature": 0.0, "avg_logprob": -0.044068366289138794, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.003323326585814357}, {"id": 1371, "seek": 793432, "start": 7957.679999999999, "end": 7963.36, "text": " standard computational primitive, using which you can express most polynomial time heuristics.", "tokens": [51532, 3832, 28270, 28540, 11, 1228, 597, 291, 393, 5109, 881, 26110, 565, 415, 374, 6006, 13, 51816], "temperature": 0.0, "avg_logprob": -0.044068366289138794, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.003323326585814357}, {"id": 1372, "seek": 796336, "start": 7963.36, "end": 7968.88, "text": " So essentially, that's a really good, you know, that's a really good piece of mind result. The", "tokens": [50364, 407, 4476, 11, 300, 311, 257, 534, 665, 11, 291, 458, 11, 300, 311, 257, 534, 665, 2522, 295, 1575, 1874, 13, 440, 50640], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1373, "seek": 796336, "start": 7968.88, "end": 7973.36, "text": " unfortunate side of it is that it's a best case result, right? So you can set the weights of", "tokens": [50640, 17843, 1252, 295, 309, 307, 300, 309, 311, 257, 1151, 1389, 1874, 11, 558, 30, 407, 291, 393, 992, 264, 17443, 295, 50864], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1374, "seek": 796336, "start": 7973.36, "end": 7977.12, "text": " a neural network of a graph neural network to mimic a dynamic programming algorithm,", "tokens": [50864, 257, 18161, 3209, 295, 257, 4295, 18161, 3209, 281, 31075, 257, 8546, 9410, 9284, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1375, "seek": 796336, "start": 7977.12, "end": 7982.32, "text": " more efficiently or with smaller sample complexity. But, you know, there's still a big", "tokens": [51052, 544, 19621, 420, 365, 4356, 6889, 14024, 13, 583, 11, 291, 458, 11, 456, 311, 920, 257, 955, 51312], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1376, "seek": 796336, "start": 7982.32, "end": 7986.719999999999, "text": " problem of how do I learn it in a way that it still works when I double the size of my input.", "tokens": [51312, 1154, 295, 577, 360, 286, 1466, 309, 294, 257, 636, 300, 309, 920, 1985, 562, 286, 3834, 264, 2744, 295, 452, 4846, 13, 51532], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1377, "seek": 796336, "start": 7986.719999999999, "end": 7992.08, "text": " And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make", "tokens": [51532, 400, 300, 307, 294, 257, 636, 437, 9284, 299, 21577, 575, 668, 11611, 466, 13, 1743, 11, 321, 434, 1382, 281, 652, 51800], "temperature": 0.0, "avg_logprob": -0.08415472072406407, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.000855696969665587}, {"id": 1378, "seek": 799208, "start": 7992.08, "end": 7997.12, "text": " that happen. It's not easy. If you throw the vanilla graph neural network and just input", "tokens": [50364, 300, 1051, 13, 467, 311, 406, 1858, 13, 759, 291, 3507, 264, 17528, 4295, 18161, 3209, 293, 445, 4846, 50616], "temperature": 0.0, "avg_logprob": -0.1281323186282454, "compression_ratio": 1.65, "no_speech_prob": 0.0030711903236806393}, {"id": 1379, "seek": 799208, "start": 7997.12, "end": 8001.36, "text": " output pairs of an algorithm, it will learn to fit them in distribution the moment you give,", "tokens": [50616, 5598, 15494, 295, 364, 9284, 11, 309, 486, 1466, 281, 3318, 552, 294, 7316, 264, 1623, 291, 976, 11, 50828], "temperature": 0.0, "avg_logprob": -0.1281323186282454, "compression_ratio": 1.65, "no_speech_prob": 0.0030711903236806393}, {"id": 1380, "seek": 799208, "start": 8001.36, "end": 8005.28, "text": " like, ask it to sort and array that's twice as big, it's going to completely collapse. So", "tokens": [50828, 411, 11, 1029, 309, 281, 1333, 293, 10225, 300, 311, 6091, 382, 955, 11, 309, 311, 516, 281, 2584, 15584, 13, 407, 51024], "temperature": 0.0, "avg_logprob": -0.1281323186282454, "compression_ratio": 1.65, "no_speech_prob": 0.0030711903236806393}, {"id": 1381, "seek": 799208, "start": 8005.84, "end": 8011.2, "text": " this is the number one thing that the neurosymbolic people say. They say neural networks, they don't", "tokens": [51052, 341, 307, 264, 1230, 472, 551, 300, 264, 28813, 88, 5612, 299, 561, 584, 13, 814, 584, 18161, 9590, 11, 436, 500, 380, 51320], "temperature": 0.0, "avg_logprob": -0.1281323186282454, "compression_ratio": 1.65, "no_speech_prob": 0.0030711903236806393}, {"id": 1382, "seek": 799208, "start": 8011.2, "end": 8018.32, "text": " extrapolate. They only interpolate, you know, it just, it's a continuous geometric model,", "tokens": [51320, 48224, 473, 13, 814, 787, 44902, 473, 11, 291, 458, 11, 309, 445, 11, 309, 311, 257, 10957, 33246, 2316, 11, 51676], "temperature": 0.0, "avg_logprob": -0.1281323186282454, "compression_ratio": 1.65, "no_speech_prob": 0.0030711903236806393}, {"id": 1383, "seek": 801832, "start": 8018.32, "end": 8024.24, "text": " learns point by point, transforms the data onto some continuous, smooth, learnable manifold,", "tokens": [50364, 27152, 935, 538, 935, 11, 35592, 264, 1412, 3911, 512, 10957, 11, 5508, 11, 1466, 712, 47138, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11670956892125747, "compression_ratio": 1.8735177865612649, "no_speech_prob": 0.006384582258760929}, {"id": 1384, "seek": 801832, "start": 8024.24, "end": 8028.08, "text": " you interpolate between the data points, you want to have a smooth, you want to have a dense", "tokens": [50660, 291, 44902, 473, 1296, 264, 1412, 2793, 11, 291, 528, 281, 362, 257, 5508, 11, 291, 528, 281, 362, 257, 18011, 50852], "temperature": 0.0, "avg_logprob": -0.11670956892125747, "compression_ratio": 1.8735177865612649, "no_speech_prob": 0.006384582258760929}, {"id": 1385, "seek": 801832, "start": 8028.08, "end": 8032.639999999999, "text": " sampling of your data. But you're talking about dynamic programming problems, these are discrete", "tokens": [50852, 21179, 295, 428, 1412, 13, 583, 291, 434, 1417, 466, 8546, 9410, 2740, 11, 613, 366, 27706, 51080], "temperature": 0.0, "avg_logprob": -0.11670956892125747, "compression_ratio": 1.8735177865612649, "no_speech_prob": 0.006384582258760929}, {"id": 1386, "seek": 801832, "start": 8032.639999999999, "end": 8038.4, "text": " problems that the structure is discontinuous. But how could you possibly learn that within your", "tokens": [51080, 2740, 300, 264, 3877, 307, 31420, 12549, 13, 583, 577, 727, 291, 6264, 1466, 300, 1951, 428, 51368], "temperature": 0.0, "avg_logprob": -0.11670956892125747, "compression_ratio": 1.8735177865612649, "no_speech_prob": 0.006384582258760929}, {"id": 1387, "seek": 801832, "start": 8038.4, "end": 8046.799999999999, "text": " network? Well, the dynamic programming algorithm could be, could have a discontinuous component", "tokens": [51368, 3209, 30, 1042, 11, 264, 8546, 9410, 9284, 727, 312, 11, 727, 362, 257, 31420, 12549, 6542, 51788], "temperature": 0.0, "avg_logprob": -0.11670956892125747, "compression_ratio": 1.8735177865612649, "no_speech_prob": 0.006384582258760929}, {"id": 1388, "seek": 804680, "start": 8047.68, "end": 8052.8, "text": " for example, if you're searching for shortest paths at some point, you will take an argmax over", "tokens": [50408, 337, 1365, 11, 498, 291, 434, 10808, 337, 31875, 14518, 412, 512, 935, 11, 291, 486, 747, 364, 3882, 41167, 670, 50664], "temperature": 0.0, "avg_logprob": -0.09712011004806659, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.19173173606395721}, {"id": 1389, "seek": 804680, "start": 8052.8, "end": 8056.88, "text": " all of your neighbor's computed distances and use that to decide what the path is.", "tokens": [50664, 439, 295, 428, 5987, 311, 40610, 22182, 293, 764, 300, 281, 4536, 437, 264, 3100, 307, 13, 50868], "temperature": 0.0, "avg_logprob": -0.09712011004806659, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.19173173606395721}, {"id": 1390, "seek": 804680, "start": 8057.52, "end": 8063.12, "text": " But before you come to the argmax part, there is usually some fairly smooth function being", "tokens": [50900, 583, 949, 291, 808, 281, 264, 3882, 41167, 644, 11, 456, 307, 2673, 512, 6457, 5508, 2445, 885, 51180], "temperature": 0.0, "avg_logprob": -0.09712011004806659, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.19173173606395721}, {"id": 1391, "seek": 804680, "start": 8063.12, "end": 8068.64, "text": " computed actually. So in the case of shortest path computations, you know, Bellman Ford or", "tokens": [51180, 40610, 767, 13, 407, 294, 264, 1389, 295, 31875, 3100, 2807, 763, 11, 291, 458, 11, 11485, 1601, 11961, 420, 51456], "temperature": 0.0, "avg_logprob": -0.09712011004806659, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.19173173606395721}, {"id": 1392, "seek": 804680, "start": 8068.64, "end": 8073.76, "text": " something like this, you say something very simple, like, I have a value d of s in every", "tokens": [51456, 746, 411, 341, 11, 291, 584, 746, 588, 2199, 11, 411, 11, 286, 362, 257, 2158, 274, 295, 262, 294, 633, 51712], "temperature": 0.0, "avg_logprob": -0.09712011004806659, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.19173173606395721}, {"id": 1393, "seek": 807376, "start": 8073.76, "end": 8078.400000000001, "text": " single one of my nodes, which is initially infinity everywhere and zero in the source", "tokens": [50364, 2167, 472, 295, 452, 13891, 11, 597, 307, 9105, 13202, 5315, 293, 4018, 294, 264, 4009, 50596], "temperature": 0.0, "avg_logprob": -0.07314610708327521, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0022869135718792677}, {"id": 1394, "seek": 807376, "start": 8078.400000000001, "end": 8085.76, "text": " vertex. And then at every point, I say, the distance of my particular node is the minimum", "tokens": [50596, 28162, 13, 400, 550, 412, 633, 935, 11, 286, 584, 11, 264, 4560, 295, 452, 1729, 9984, 307, 264, 7285, 50964], "temperature": 0.0, "avg_logprob": -0.07314610708327521, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0022869135718792677}, {"id": 1395, "seek": 807376, "start": 8085.76, "end": 8092.16, "text": " of all the distances of my neighbors plus the edge weight, right. And this kind of function is", "tokens": [50964, 295, 439, 264, 22182, 295, 452, 12512, 1804, 264, 4691, 3364, 11, 558, 13, 400, 341, 733, 295, 2445, 307, 51284], "temperature": 0.0, "avg_logprob": -0.07314610708327521, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0022869135718792677}, {"id": 1396, "seek": 807376, "start": 8092.96, "end": 8098.64, "text": " generally more graceful than than taking an argmax. And you can also think of, for example,", "tokens": [51324, 5101, 544, 10042, 906, 813, 813, 1940, 364, 3882, 41167, 13, 400, 291, 393, 611, 519, 295, 11, 337, 1365, 11, 51608], "temperature": 0.0, "avg_logprob": -0.07314610708327521, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0022869135718792677}, {"id": 1397, "seek": 807376, "start": 8098.64, "end": 8102.56, "text": " if you have to compute expected values or something like this, using dynamic programming,", "tokens": [51608, 498, 291, 362, 281, 14722, 5176, 4190, 420, 746, 411, 341, 11, 1228, 8546, 9410, 11, 51804], "temperature": 0.0, "avg_logprob": -0.07314610708327521, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0022869135718792677}, {"id": 1398, "seek": 810256, "start": 8102.56, "end": 8107.200000000001, "text": " that's also one example where actually summing is what you need to do across all of your neighbors", "tokens": [50364, 300, 311, 611, 472, 1365, 689, 767, 2408, 2810, 307, 437, 291, 643, 281, 360, 2108, 439, 295, 428, 12512, 50596], "temperature": 0.0, "avg_logprob": -0.03943928371776234, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0030273734591901302}, {"id": 1399, "seek": 810256, "start": 8107.200000000001, "end": 8113.4400000000005, "text": " or something like this. So yeah, it is true that like, across individual steps, you may be doing", "tokens": [50596, 420, 746, 411, 341, 13, 407, 1338, 11, 309, 307, 2074, 300, 411, 11, 2108, 2609, 4439, 11, 291, 815, 312, 884, 50908], "temperature": 0.0, "avg_logprob": -0.03943928371776234, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0030273734591901302}, {"id": 1400, "seek": 810256, "start": 8113.4400000000005, "end": 8120.64, "text": " like discrete optimization steps. But usually, it's propelled by some kind of continuous", "tokens": [50908, 411, 27706, 19618, 4439, 13, 583, 2673, 11, 309, 311, 25577, 5929, 538, 512, 733, 295, 10957, 51268], "temperature": 0.0, "avg_logprob": -0.03943928371776234, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0030273734591901302}, {"id": 1401, "seek": 810256, "start": 8120.64, "end": 8124.88, "text": " computation under the hood. So that's the part that the graph neural network actually simulates.", "tokens": [51268, 24903, 833, 264, 13376, 13, 407, 300, 311, 264, 644, 300, 264, 4295, 18161, 3209, 767, 1034, 26192, 13, 51480], "temperature": 0.0, "avg_logprob": -0.03943928371776234, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0030273734591901302}, {"id": 1402, "seek": 810256, "start": 8124.88, "end": 8128.4800000000005, "text": " And then the part which does the argmax would be some kind of classifier that you stitch on", "tokens": [51480, 400, 550, 264, 644, 597, 775, 264, 3882, 41167, 576, 312, 512, 733, 295, 1508, 9902, 300, 291, 5635, 322, 51660], "temperature": 0.0, "avg_logprob": -0.03943928371776234, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0030273734591901302}, {"id": 1403, "seek": 812848, "start": 8128.48, "end": 8134.08, "text": " top of that. So in principle, it's not, yeah, it's not too challenging to massage it into a", "tokens": [50364, 1192, 295, 300, 13, 407, 294, 8665, 11, 309, 311, 406, 11, 1338, 11, 309, 311, 406, 886, 7595, 281, 16145, 309, 666, 257, 50644], "temperature": 0.0, "avg_logprob": -0.09678181096127159, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.012411537580192089}, {"id": 1404, "seek": 812848, "start": 8134.08, "end": 8139.759999999999, "text": " neural network framework. So one of the, I think one of you mentioned this before, brought up", "tokens": [50644, 18161, 3209, 8388, 13, 407, 472, 295, 264, 11, 286, 519, 472, 295, 291, 2835, 341, 949, 11, 3038, 493, 50928], "temperature": 0.0, "avg_logprob": -0.09678181096127159, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.012411537580192089}, {"id": 1405, "seek": 812848, "start": 8139.759999999999, "end": 8146.32, "text": " transformers. And, you know, in recent years, we've had, I think about 10 different papers", "tokens": [50928, 4088, 433, 13, 400, 11, 291, 458, 11, 294, 5162, 924, 11, 321, 600, 632, 11, 286, 519, 466, 1266, 819, 10577, 51256], "temperature": 0.0, "avg_logprob": -0.09678181096127159, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.012411537580192089}, {"id": 1406, "seek": 812848, "start": 8146.32, "end": 8152.959999999999, "text": " saying transformers are something there is transformers are RNNs, transformers are Hopfield", "tokens": [51256, 1566, 4088, 433, 366, 746, 456, 307, 4088, 433, 366, 45702, 45, 82, 11, 4088, 433, 366, 13438, 7610, 51588], "temperature": 0.0, "avg_logprob": -0.09678181096127159, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.012411537580192089}, {"id": 1407, "seek": 815296, "start": 8152.96, "end": 8159.84, "text": " networks. And also transformers are graph neural networks or compute some kind of", "tokens": [50364, 9590, 13, 400, 611, 4088, 433, 366, 4295, 18161, 9590, 420, 14722, 512, 733, 295, 50708], "temperature": 0.0, "avg_logprob": -0.12358785257106875, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.007343328092247248}, {"id": 1408, "seek": 815296, "start": 8159.84, "end": 8165.44, "text": " graph neural networks. Can you maybe speak a bit to that? Are transformers specifically", "tokens": [50708, 4295, 18161, 9590, 13, 1664, 291, 1310, 1710, 257, 857, 281, 300, 30, 2014, 4088, 433, 4682, 50988], "temperature": 0.0, "avg_logprob": -0.12358785257106875, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.007343328092247248}, {"id": 1409, "seek": 815296, "start": 8165.44, "end": 8172.0, "text": " graph neural networks? Or are they just so general that you can also formulate a graph problem in", "tokens": [50988, 4295, 18161, 9590, 30, 1610, 366, 436, 445, 370, 2674, 300, 291, 393, 611, 47881, 257, 4295, 1154, 294, 51316], "temperature": 0.0, "avg_logprob": -0.12358785257106875, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.007343328092247248}, {"id": 1410, "seek": 815296, "start": 8172.0, "end": 8180.08, "text": " terms of a transformer? Okay, that's a very good question. I would start off by saying,", "tokens": [51316, 2115, 295, 257, 31782, 30, 1033, 11, 300, 311, 257, 588, 665, 1168, 13, 286, 576, 722, 766, 538, 1566, 11, 51720], "temperature": 0.0, "avg_logprob": -0.12358785257106875, "compression_ratio": 1.6985645933014355, "no_speech_prob": 0.007343328092247248}, {"id": 1411, "seek": 818008, "start": 8180.96, "end": 8185.28, "text": " like, I don't want to start this discussion just by saying, yes, transformers are graph", "tokens": [50408, 411, 11, 286, 500, 380, 528, 281, 722, 341, 5017, 445, 538, 1566, 11, 2086, 11, 4088, 433, 366, 4295, 50624], "temperature": 0.0, "avg_logprob": -0.06546490171314341, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.006191696971654892}, {"id": 1412, "seek": 818008, "start": 8185.28, "end": 8189.5199999999995, "text": " neural networks. This is why end of story, because I feel like, you know, that doesn't", "tokens": [50624, 18161, 9590, 13, 639, 307, 983, 917, 295, 1657, 11, 570, 286, 841, 411, 11, 291, 458, 11, 300, 1177, 380, 50836], "temperature": 0.0, "avg_logprob": -0.06546490171314341, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.006191696971654892}, {"id": 1413, "seek": 818008, "start": 8189.5199999999995, "end": 8194.0, "text": " touch upon the whole picture. So let's let's look at this from a natural language processing", "tokens": [50836, 2557, 3564, 264, 1379, 3036, 13, 407, 718, 311, 718, 311, 574, 412, 341, 490, 257, 3303, 2856, 9007, 51060], "temperature": 0.0, "avg_logprob": -0.06546490171314341, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.006191696971654892}, {"id": 1414, "seek": 818008, "start": 8194.0, "end": 8199.76, "text": " angle, which is how most people have come to know about transformers. So imagine that you have a task", "tokens": [51060, 5802, 11, 597, 307, 577, 881, 561, 362, 808, 281, 458, 466, 4088, 433, 13, 407, 3811, 300, 291, 362, 257, 5633, 51348], "temperature": 0.0, "avg_logprob": -0.06546490171314341, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.006191696971654892}, {"id": 1415, "seek": 818008, "start": 8199.76, "end": 8206.64, "text": " which is specified on a sentence. And you want to exploit the fact that words in the sentence", "tokens": [51348, 597, 307, 22206, 322, 257, 8174, 13, 400, 291, 528, 281, 25924, 264, 1186, 300, 2283, 294, 264, 8174, 51692], "temperature": 0.0, "avg_logprob": -0.06546490171314341, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.006191696971654892}, {"id": 1416, "seek": 820664, "start": 8206.64, "end": 8210.88, "text": " interact, right? It's not just a bag of words. There is there's some interesting structure", "tokens": [50364, 4648, 11, 558, 30, 467, 311, 406, 445, 257, 3411, 295, 2283, 13, 821, 307, 456, 311, 512, 1880, 3877, 50576], "temperature": 0.0, "avg_logprob": -0.07681705240617719, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0378597192466259}, {"id": 1417, "seek": 820664, "start": 8210.88, "end": 8216.24, "text": " inside this bunch of words that you might want to exploit. When we were using recurrent neural", "tokens": [50576, 1854, 341, 3840, 295, 2283, 300, 291, 1062, 528, 281, 25924, 13, 1133, 321, 645, 1228, 18680, 1753, 18161, 50844], "temperature": 0.0, "avg_logprob": -0.07681705240617719, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0378597192466259}, {"id": 1418, "seek": 820664, "start": 8216.24, "end": 8220.48, "text": " networks, we assume that the structure between the words was a line graph. So basically,", "tokens": [50844, 9590, 11, 321, 6552, 300, 264, 3877, 1296, 264, 2283, 390, 257, 1622, 4295, 13, 407, 1936, 11, 51056], "temperature": 0.0, "avg_logprob": -0.07681705240617719, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0378597192466259}, {"id": 1419, "seek": 820664, "start": 8221.199999999999, "end": 8226.96, "text": " every word is preceding another word and so on and so forth. And you kind of just linearly", "tokens": [51092, 633, 1349, 307, 16969, 278, 1071, 1349, 293, 370, 322, 293, 370, 5220, 13, 400, 291, 733, 295, 445, 43586, 51380], "temperature": 0.0, "avg_logprob": -0.07681705240617719, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0378597192466259}, {"id": 1420, "seek": 820664, "start": 8226.96, "end": 8234.48, "text": " process them with a model like LSTM or something. But, you know, basically line graphs, as we know,", "tokens": [51380, 1399, 552, 365, 257, 2316, 411, 441, 6840, 44, 420, 746, 13, 583, 11, 291, 458, 11, 1936, 1622, 24877, 11, 382, 321, 458, 11, 51756], "temperature": 0.0, "avg_logprob": -0.07681705240617719, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0378597192466259}, {"id": 1421, "seek": 823448, "start": 8234.48, "end": 8239.92, "text": " are not the way language is actually structured. There can be super long range interactions", "tokens": [50364, 366, 406, 264, 636, 2856, 307, 767, 18519, 13, 821, 393, 312, 1687, 938, 3613, 13280, 50636], "temperature": 0.0, "avg_logprob": -0.05653051229623648, "compression_ratio": 1.75, "no_speech_prob": 0.005300954915583134}, {"id": 1422, "seek": 823448, "start": 8239.92, "end": 8245.119999999999, "text": " inside language. So subjects and objects in the same sentence could appear miles away from each", "tokens": [50636, 1854, 2856, 13, 407, 13066, 293, 6565, 294, 264, 912, 8174, 727, 4204, 6193, 1314, 490, 1184, 50896], "temperature": 0.0, "avg_logprob": -0.05653051229623648, "compression_ratio": 1.75, "no_speech_prob": 0.005300954915583134}, {"id": 1423, "seek": 823448, "start": 8245.119999999999, "end": 8250.08, "text": " other. So using the line graph is not the most optimal way of getting that information in the", "tokens": [50896, 661, 13, 407, 1228, 264, 1622, 4295, 307, 406, 264, 881, 16252, 636, 295, 1242, 300, 1589, 294, 264, 51144], "temperature": 0.0, "avg_logprob": -0.05653051229623648, "compression_ratio": 1.75, "no_speech_prob": 0.005300954915583134}, {"id": 1424, "seek": 823448, "start": 8250.08, "end": 8255.52, "text": " fastest possible in the fastest possible way. So, okay, there's clearly some kind of non trivial", "tokens": [51144, 14573, 1944, 294, 264, 14573, 1944, 636, 13, 407, 11, 1392, 11, 456, 311, 4448, 512, 733, 295, 2107, 26703, 51416], "temperature": 0.0, "avg_logprob": -0.05653051229623648, "compression_ratio": 1.75, "no_speech_prob": 0.005300954915583134}, {"id": 1425, "seek": 823448, "start": 8255.52, "end": 8261.279999999999, "text": " graph structure. What is it? Well, it turns out that people cannot really agree what this optimal", "tokens": [51416, 4295, 3877, 13, 708, 307, 309, 30, 1042, 11, 309, 4523, 484, 300, 561, 2644, 534, 3986, 437, 341, 16252, 51704], "temperature": 0.0, "avg_logprob": -0.05653051229623648, "compression_ratio": 1.75, "no_speech_prob": 0.005300954915583134}, {"id": 1426, "seek": 826128, "start": 8261.28, "end": 8266.480000000001, "text": " graph structure is, and it may well be task dependent, actually. So just consider syntax", "tokens": [50364, 4295, 3877, 307, 11, 293, 309, 815, 731, 312, 5633, 12334, 11, 767, 13, 407, 445, 1949, 28431, 50624], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1427, "seek": 826128, "start": 8266.480000000001, "end": 8272.0, "text": " trees, for example, like there's not always a unique way of decomposing a sentence into a syntax", "tokens": [50624, 5852, 11, 337, 1365, 11, 411, 456, 311, 406, 1009, 257, 3845, 636, 295, 22867, 6110, 257, 8174, 666, 257, 28431, 50900], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1428, "seek": 826128, "start": 8272.0, "end": 8276.480000000001, "text": " tree. And the exact kind of tree you might wish to use to represent a sentence may be different", "tokens": [50900, 4230, 13, 400, 264, 1900, 733, 295, 4230, 291, 1062, 3172, 281, 764, 281, 2906, 257, 8174, 815, 312, 819, 51124], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1429, "seek": 826128, "start": 8276.480000000001, "end": 8281.6, "text": " depending on what is the actual thing that you're solving. So, okay, we have a situation where we", "tokens": [51124, 5413, 322, 437, 307, 264, 3539, 551, 300, 291, 434, 12606, 13, 407, 11, 1392, 11, 321, 362, 257, 2590, 689, 321, 51380], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1430, "seek": 826128, "start": 8281.6, "end": 8286.08, "text": " know that there's some connectivity between the words, but we don't know what that connectivity is.", "tokens": [51380, 458, 300, 456, 311, 512, 21095, 1296, 264, 2283, 11, 457, 321, 500, 380, 458, 437, 300, 21095, 307, 13, 51604], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1431, "seek": 826128, "start": 8286.08, "end": 8290.24, "text": " So in graph representation learning, what we typically do when we don't know the graph,", "tokens": [51604, 407, 294, 4295, 10290, 2539, 11, 437, 321, 5850, 360, 562, 321, 500, 380, 458, 264, 4295, 11, 51812], "temperature": 0.0, "avg_logprob": -0.06476254570753054, "compression_ratio": 1.8115015974440896, "no_speech_prob": 0.005729212425649166}, {"id": 1432, "seek": 829024, "start": 8290.24, "end": 8294.4, "text": " as long as the number of objects is not huge, is to assume a complete graph and let the graph", "tokens": [50364, 382, 938, 382, 264, 1230, 295, 6565, 307, 406, 2603, 11, 307, 281, 6552, 257, 3566, 4295, 293, 718, 264, 4295, 50572], "temperature": 0.0, "avg_logprob": -0.0713040875453575, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.004680210724473}, {"id": 1433, "seek": 829024, "start": 8294.4, "end": 8300.32, "text": " neural network figure out by itself what the important connections are. And if I now stitch", "tokens": [50572, 18161, 3209, 2573, 484, 538, 2564, 437, 264, 1021, 9271, 366, 13, 400, 498, 286, 586, 5635, 50868], "temperature": 0.0, "avg_logprob": -0.0713040875453575, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.004680210724473}, {"id": 1434, "seek": 829024, "start": 8300.32, "end": 8305.84, "text": " an attentional message passing mechanism onto this graph neural network, I have effectively", "tokens": [50868, 364, 3202, 304, 3636, 8437, 7513, 3911, 341, 4295, 18161, 3209, 11, 286, 362, 8659, 51144], "temperature": 0.0, "avg_logprob": -0.0713040875453575, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.004680210724473}, {"id": 1435, "seek": 829024, "start": 8305.84, "end": 8314.4, "text": " rederived the transformer model equation without ever like using this specific transformer lingo.", "tokens": [51144, 2182, 260, 3194, 264, 31782, 2316, 5367, 1553, 1562, 411, 1228, 341, 2685, 31782, 287, 18459, 13, 51572], "temperature": 0.0, "avg_logprob": -0.0713040875453575, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.004680210724473}, {"id": 1436, "seek": 829024, "start": 8314.4, "end": 8320.0, "text": " So from this kind of angle, the fact that it's a model that operates over a complete graph", "tokens": [51572, 407, 490, 341, 733, 295, 5802, 11, 264, 1186, 300, 309, 311, 257, 2316, 300, 22577, 670, 257, 3566, 4295, 51852], "temperature": 0.0, "avg_logprob": -0.0713040875453575, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.004680210724473}, {"id": 1437, "seek": 832000, "start": 8320.56, "end": 8326.4, "text": " individual words, in a way that you know, once you've put all the embeddings to them is permutation", "tokens": [50392, 2609, 2283, 11, 294, 257, 636, 300, 291, 458, 11, 1564, 291, 600, 829, 439, 264, 12240, 29432, 281, 552, 307, 4784, 11380, 50684], "temperature": 0.0, "avg_logprob": -0.08091517611666843, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.003705826820805669}, {"id": 1438, "seek": 832000, "start": 8326.4, "end": 8332.08, "text": " equivalent, this describes the central equations of self attention that the transformer uses.", "tokens": [50684, 10344, 11, 341, 15626, 264, 5777, 11787, 295, 2698, 3202, 300, 264, 31782, 4960, 13, 50968], "temperature": 0.0, "avg_logprob": -0.08091517611666843, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.003705826820805669}, {"id": 1439, "seek": 832000, "start": 8332.08, "end": 8337.76, "text": " The part which I think causes a bit of a divide here is the fact that transformers like the model", "tokens": [50968, 440, 644, 597, 286, 519, 7700, 257, 857, 295, 257, 9845, 510, 307, 264, 1186, 300, 4088, 433, 411, 264, 2316, 51252], "temperature": 0.0, "avg_logprob": -0.08091517611666843, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.003705826820805669}, {"id": 1440, "seek": 832000, "start": 8337.76, "end": 8342.64, "text": " that was originally presented are not just the equations of a transformer, they're also the", "tokens": [51252, 300, 390, 7993, 8212, 366, 406, 445, 264, 11787, 295, 257, 31782, 11, 436, 434, 611, 264, 51496], "temperature": 0.0, "avg_logprob": -0.08091517611666843, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.003705826820805669}, {"id": 1441, "seek": 832000, "start": 8342.64, "end": 8347.2, "text": " positional embeddings of a transformer. And that's the part that sort of gives it a bit more of a", "tokens": [51496, 2535, 304, 12240, 29432, 295, 257, 31782, 13, 400, 300, 311, 264, 644, 300, 1333, 295, 2709, 309, 257, 857, 544, 295, 257, 51724], "temperature": 0.0, "avg_logprob": -0.08091517611666843, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.003705826820805669}, {"id": 1442, "seek": 834720, "start": 8347.28, "end": 8354.800000000001, "text": " central structure. Well, actually, if you look at these sine and cosine waves that get attached to", "tokens": [50368, 5777, 3877, 13, 1042, 11, 767, 11, 498, 291, 574, 412, 613, 18609, 293, 23565, 9417, 300, 483, 8570, 281, 50744], "temperature": 0.0, "avg_logprob": -0.0910251113190048, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.002631275448948145}, {"id": 1443, "seek": 834720, "start": 8354.800000000001, "end": 8360.240000000002, "text": " the individual words in an input to a transformer, you will see that you can you can actually derive", "tokens": [50744, 264, 2609, 2283, 294, 364, 4846, 281, 257, 31782, 11, 291, 486, 536, 300, 291, 393, 291, 393, 767, 28446, 51016], "temperature": 0.0, "avg_logprob": -0.0910251113190048, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.002631275448948145}, {"id": 1444, "seek": 834720, "start": 8360.240000000002, "end": 8365.2, "text": " a pretty good connection between them and the discrete Fourier transform, which actually", "tokens": [51016, 257, 1238, 665, 4984, 1296, 552, 293, 264, 27706, 36810, 4088, 11, 597, 767, 51264], "temperature": 0.0, "avg_logprob": -0.0910251113190048, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.002631275448948145}, {"id": 1445, "seek": 834720, "start": 8365.2, "end": 8371.92, "text": " turned out to be the eigenvectors of a graph Laplacian for a line graph. So essentially these", "tokens": [51264, 3574, 484, 281, 312, 264, 10446, 303, 5547, 295, 257, 4295, 2369, 564, 326, 952, 337, 257, 1622, 4295, 13, 407, 4476, 613, 51600], "temperature": 0.0, "avg_logprob": -0.0910251113190048, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.002631275448948145}, {"id": 1446, "seek": 837192, "start": 8371.92, "end": 8377.04, "text": " positional embeddings are hinting to the model that you are the decent that these words in a", "tokens": [50364, 2535, 304, 12240, 29432, 366, 12075, 278, 281, 264, 2316, 300, 291, 366, 264, 8681, 300, 613, 2283, 294, 257, 50620], "temperature": 0.0, "avg_logprob": -0.09311632749413241, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.002434239722788334}, {"id": 1447, "seek": 837192, "start": 8377.04, "end": 8381.6, "text": " sentence are arranged in a particular way, and you can use that information. But because it's", "tokens": [50620, 8174, 366, 18721, 294, 257, 1729, 636, 11, 293, 291, 393, 764, 300, 1589, 13, 583, 570, 309, 311, 50848], "temperature": 0.0, "avg_logprob": -0.09311632749413241, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.002434239722788334}, {"id": 1448, "seek": 837192, "start": 8381.6, "end": 8386.72, "text": " fed in as features, the model doesn't have to use any of that information, like sometimes bag of words", "tokens": [50848, 4636, 294, 382, 4122, 11, 264, 2316, 1177, 380, 362, 281, 764, 604, 295, 300, 1589, 11, 411, 2171, 3411, 295, 2283, 51104], "temperature": 0.0, "avg_logprob": -0.09311632749413241, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.002434239722788334}, {"id": 1449, "seek": 837192, "start": 8386.72, "end": 8392.24, "text": " is the right thing to do, for example, right. So essentially, the transformer has a bit of a light", "tokens": [51104, 307, 264, 558, 551, 281, 360, 11, 337, 1365, 11, 558, 13, 407, 4476, 11, 264, 31782, 575, 257, 857, 295, 257, 1442, 51380], "temperature": 0.0, "avg_logprob": -0.09311632749413241, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.002434239722788334}, {"id": 1450, "seek": 837192, "start": 8392.24, "end": 8398.64, "text": " hint that there's a central structure in there in the form of a line graph. But, you know, the", "tokens": [51380, 12075, 300, 456, 311, 257, 5777, 3877, 294, 456, 294, 264, 1254, 295, 257, 1622, 4295, 13, 583, 11, 291, 458, 11, 264, 51700], "temperature": 0.0, "avg_logprob": -0.09311632749413241, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.002434239722788334}, {"id": 1451, "seek": 839864, "start": 8398.72, "end": 8403.519999999999, "text": " model itself is a permutation equivalent model over a complete graph. And from our lens of the", "tokens": [50368, 2316, 2564, 307, 257, 4784, 11380, 10344, 2316, 670, 257, 3566, 4295, 13, 400, 490, 527, 6765, 295, 264, 50608], "temperature": 0.0, "avg_logprob": -0.06954509019851685, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.001064183539710939}, {"id": 1452, "seek": 839864, "start": 8403.519999999999, "end": 8409.119999999999, "text": " geometric deep learning, it is effectively a special case of an attentional GNN. Now, I think", "tokens": [50608, 33246, 2452, 2539, 11, 309, 307, 8659, 257, 2121, 1389, 295, 364, 3202, 304, 46411, 45, 13, 823, 11, 286, 519, 50888], "temperature": 0.0, "avg_logprob": -0.06954509019851685, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.001064183539710939}, {"id": 1453, "seek": 839864, "start": 8409.119999999999, "end": 8413.92, "text": " this positional embedding aspect is a super important one. And it could hint to how we might", "tokens": [50888, 341, 2535, 304, 12240, 3584, 4171, 307, 257, 1687, 1021, 472, 13, 400, 309, 727, 12075, 281, 577, 321, 1062, 51128], "temperature": 0.0, "avg_logprob": -0.06954509019851685, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.001064183539710939}, {"id": 1454, "seek": 839864, "start": 8413.92, "end": 8419.039999999999, "text": " extend these transformers from sentences to more general structures. And I think, Michael, you", "tokens": [51128, 10101, 613, 4088, 433, 490, 16579, 281, 544, 2674, 9227, 13, 400, 286, 519, 11, 5116, 11, 291, 51384], "temperature": 0.0, "avg_logprob": -0.06954509019851685, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.001064183539710939}, {"id": 1455, "seek": 839864, "start": 8419.039999999999, "end": 8424.0, "text": " might have a lot more thoughts on that than I do. So maybe you can say a bit about that.", "tokens": [51384, 1062, 362, 257, 688, 544, 4598, 322, 300, 813, 286, 360, 13, 407, 1310, 291, 393, 584, 257, 857, 466, 300, 13, 51632], "temperature": 0.0, "avg_logprob": -0.06954509019851685, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.001064183539710939}, {"id": 1456, "seek": 842400, "start": 8424.64, "end": 8428.8, "text": " Yeah, so positional encoding has been done for graphs as well. As Petter mentioned,", "tokens": [50396, 865, 11, 370, 2535, 304, 43430, 575, 668, 1096, 337, 24877, 382, 731, 13, 1018, 10472, 391, 2835, 11, 50604], "temperature": 0.0, "avg_logprob": -0.18627196667241117, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005279639270156622}, {"id": 1457, "seek": 842400, "start": 8428.8, "end": 8435.44, "text": " in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates", "tokens": [50604, 294, 1389, 295, 257, 4295, 11, 291, 393, 15325, 356, 2674, 1125, 341, 18609, 420, 23565, 2535, 304, 21056, 50936], "temperature": 0.0, "avg_logprob": -0.18627196667241117, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005279639270156622}, {"id": 1458, "seek": 842400, "start": 8435.44, "end": 8440.0, "text": " that are used in transformers using the eigenvectors of the Laplacian. There are other", "tokens": [50936, 300, 366, 1143, 294, 4088, 433, 1228, 264, 10446, 303, 5547, 295, 264, 2369, 564, 326, 952, 13, 821, 366, 661, 51164], "temperature": 0.0, "avg_logprob": -0.18627196667241117, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005279639270156622}, {"id": 1459, "seek": 842400, "start": 8440.0, "end": 8446.0, "text": " techniques you can actually show that you can make it a message passing type neural network", "tokens": [51164, 7512, 291, 393, 767, 855, 300, 291, 393, 652, 309, 257, 3636, 8437, 2010, 18161, 3209, 51464], "temperature": 0.0, "avg_logprob": -0.18627196667241117, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005279639270156622}, {"id": 1460, "seek": 842400, "start": 8446.0, "end": 8451.52, "text": " strictly more powerful than traditional message passing. The equivalent vise for 11 graphics", "tokens": [51464, 20792, 544, 4005, 813, 5164, 3636, 8437, 13, 440, 10344, 371, 908, 337, 2975, 11837, 51740], "temperature": 0.0, "avg_logprob": -0.18627196667241117, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005279639270156622}, {"id": 1461, "seek": 845152, "start": 8451.52, "end": 8458.960000000001, "text": " or morphism test by using a special kind of structure where positional encoding, for example,", "tokens": [50364, 420, 25778, 1434, 1500, 538, 1228, 257, 2121, 733, 295, 3877, 689, 2535, 304, 43430, 11, 337, 1365, 11, 50736], "temperature": 0.0, "avg_logprob": -0.15105891227722168, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.00864375103265047}, {"id": 1462, "seek": 845152, "start": 8458.960000000001, "end": 8465.28, "text": " if you can count substructures of the graph, such as cycles or rings and so on. And this way,", "tokens": [50736, 498, 291, 393, 1207, 4594, 44513, 295, 264, 4295, 11, 1270, 382, 17796, 420, 11136, 293, 370, 322, 13, 400, 341, 636, 11, 51052], "temperature": 0.0, "avg_logprob": -0.15105891227722168, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.00864375103265047}, {"id": 1463, "seek": 845152, "start": 8465.28, "end": 8471.04, "text": " you have a message passing algorithm that is specialized to the particular position in the", "tokens": [51052, 291, 362, 257, 3636, 8437, 9284, 300, 307, 19813, 281, 264, 1729, 2535, 294, 264, 51340], "temperature": 0.0, "avg_logprob": -0.15105891227722168, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.00864375103265047}, {"id": 1464, "seek": 845152, "start": 8471.04, "end": 8476.640000000001, "text": " graph and can, for example, detect structures that the traditional message passing cannot detect.", "tokens": [51340, 4295, 293, 393, 11, 337, 1365, 11, 5531, 9227, 300, 264, 5164, 3636, 8437, 2644, 5531, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15105891227722168, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.00864375103265047}, {"id": 1465, "seek": 847664, "start": 8477.199999999999, "end": 8482.88, "text": " So it is at least not less powerful than the vise for 11 algorithm. And we can actually show", "tokens": [50392, 407, 309, 307, 412, 1935, 406, 1570, 4005, 813, 264, 371, 908, 337, 2975, 9284, 13, 400, 321, 393, 767, 855, 50676], "temperature": 0.0, "avg_logprob": -0.07788858681081612, "compression_ratio": 1.6164874551971327, "no_speech_prob": 0.002269190037623048}, {"id": 1466, "seek": 847664, "start": 8482.88, "end": 8488.56, "text": " examples on which vise for 11 algorithm or traditional message passing fails, whereas", "tokens": [50676, 5110, 322, 597, 371, 908, 337, 2975, 9284, 420, 5164, 3636, 8437, 18199, 11, 9735, 50960], "temperature": 0.0, "avg_logprob": -0.07788858681081612, "compression_ratio": 1.6164874551971327, "no_speech_prob": 0.002269190037623048}, {"id": 1467, "seek": 847664, "start": 8488.56, "end": 8494.96, "text": " this kind of approach succeeds. So the thing I've always wondered about transformers networks", "tokens": [50960, 341, 733, 295, 3109, 49263, 13, 407, 264, 551, 286, 600, 1009, 17055, 466, 4088, 433, 9590, 51280], "temperature": 0.0, "avg_logprob": -0.07788858681081612, "compression_ratio": 1.6164874551971327, "no_speech_prob": 0.002269190037623048}, {"id": 1468, "seek": 847664, "start": 8495.519999999999, "end": 8499.439999999999, "text": " are the position tokens. I really don't like them and I want them to go away", "tokens": [51308, 366, 264, 2535, 22667, 13, 286, 534, 500, 380, 411, 552, 293, 286, 528, 552, 281, 352, 1314, 51504], "temperature": 0.0, "avg_logprob": -0.07788858681081612, "compression_ratio": 1.6164874551971327, "no_speech_prob": 0.002269190037623048}, {"id": 1469, "seek": 847664, "start": 8499.439999999999, "end": 8504.72, "text": " because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher", "tokens": [51504, 570, 309, 3417, 588, 704, 540, 11, 1177, 380, 309, 30, 286, 519, 437, 321, 534, 528, 281, 1466, 307, 512, 733, 295, 2946, 51768], "temperature": 0.0, "avg_logprob": -0.07788858681081612, "compression_ratio": 1.6164874551971327, "no_speech_prob": 0.002269190037623048}, {"id": 1470, "seek": 850472, "start": 8504.8, "end": 8511.679999999998, "text": " order structure in the language. And it kind of felt like we were using the position tokens to", "tokens": [50368, 1668, 3877, 294, 264, 2856, 13, 400, 309, 733, 295, 2762, 411, 321, 645, 1228, 264, 2535, 22667, 281, 50712], "temperature": 0.0, "avg_logprob": -0.06394484899576428, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004621520638465881}, {"id": 1471, "seek": 850472, "start": 8511.679999999998, "end": 8515.84, "text": " cheat a little bit. So what I'm trying to get across here is that I think the position of a", "tokens": [50712, 17470, 257, 707, 857, 13, 407, 437, 286, 478, 1382, 281, 483, 2108, 510, 307, 300, 286, 519, 264, 2535, 295, 257, 50920], "temperature": 0.0, "avg_logprob": -0.06394484899576428, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004621520638465881}, {"id": 1472, "seek": 850472, "start": 8515.84, "end": 8521.519999999999, "text": " token in an utterance should be invariant. I mean, clearly, in different languages,", "tokens": [50920, 14862, 294, 364, 17567, 719, 820, 312, 33270, 394, 13, 286, 914, 11, 4448, 11, 294, 819, 8650, 11, 51204], "temperature": 0.0, "avg_logprob": -0.06394484899576428, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004621520638465881}, {"id": 1473, "seek": 850472, "start": 8521.519999999999, "end": 8526.96, "text": " the tokens are in different places. In Turkish, the order is completely reversed. And I would", "tokens": [51204, 264, 22667, 366, 294, 819, 3190, 13, 682, 18565, 11, 264, 1668, 307, 2584, 30563, 13, 400, 286, 576, 51476], "temperature": 0.0, "avg_logprob": -0.06394484899576428, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004621520638465881}, {"id": 1474, "seek": 850472, "start": 8526.96, "end": 8533.119999999999, "text": " like to think that our internal language representation ignores the transmission", "tokens": [51476, 411, 281, 519, 300, 527, 6920, 2856, 10290, 5335, 2706, 264, 11574, 51784], "temperature": 0.0, "avg_logprob": -0.06394484899576428, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004621520638465881}, {"id": 1475, "seek": 853312, "start": 8533.12, "end": 8538.480000000001, "text": " arrangement given the particular language and the constraint that we only communicate sequential", "tokens": [50364, 17620, 2212, 264, 1729, 2856, 293, 264, 25534, 300, 321, 787, 7890, 42881, 50632], "temperature": 0.0, "avg_logprob": -0.09025543757847378, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0168658047914505}, {"id": 1476, "seek": 853312, "start": 8538.480000000001, "end": 8544.08, "text": " streams of words. However, I do appreciate what Michael is saying above that the position", "tokens": [50632, 15842, 295, 2283, 13, 2908, 11, 286, 360, 4449, 437, 5116, 307, 1566, 3673, 300, 264, 2535, 50912], "temperature": 0.0, "avg_logprob": -0.09025543757847378, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0168658047914505}, {"id": 1477, "seek": 853312, "start": 8544.08, "end": 8550.240000000002, "text": " encodings can actually encode more powerful structures like cycles and rings. The key question", "tokens": [50912, 2058, 378, 1109, 393, 767, 2058, 1429, 544, 4005, 9227, 411, 17796, 293, 11136, 13, 440, 2141, 1168, 51220], "temperature": 0.0, "avg_logprob": -0.09025543757847378, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0168658047914505}, {"id": 1478, "seek": 853312, "start": 8550.240000000002, "end": 8556.480000000001, "text": " is, do we actually need to have these structures in natural language? I don't agree that you want", "tokens": [51220, 307, 11, 360, 321, 767, 643, 281, 362, 613, 9227, 294, 3303, 2856, 30, 286, 500, 380, 3986, 300, 291, 528, 51532], "temperature": 0.0, "avg_logprob": -0.09025543757847378, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0168658047914505}, {"id": 1479, "seek": 853312, "start": 8556.480000000001, "end": 8561.76, "text": " to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you", "tokens": [51532, 281, 483, 3973, 295, 552, 13, 407, 2535, 304, 43430, 11, 309, 311, 257, 733, 295, 6562, 295, 732, 13401, 13, 407, 498, 291, 51796], "temperature": 0.0, "avg_logprob": -0.09025543757847378, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0168658047914505}, {"id": 1480, "seek": 856176, "start": 8561.76, "end": 8567.84, "text": " consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the", "tokens": [50364, 1949, 257, 4295, 11, 550, 291, 434, 2584, 623, 77, 19634, 281, 264, 21739, 295, 264, 13891, 13, 639, 307, 472, 295, 264, 50668], "temperature": 0.0, "avg_logprob": -0.09092979098475257, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.023219235241413116}, {"id": 1481, "seek": 856176, "start": 8567.84, "end": 8572.72, "text": " really key characteristics of graphs and sets more generally that you don't have the ordering", "tokens": [50668, 534, 2141, 10891, 295, 24877, 293, 6352, 544, 5101, 300, 291, 500, 380, 362, 264, 21739, 50912], "temperature": 0.0, "avg_logprob": -0.09092979098475257, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.023219235241413116}, {"id": 1482, "seek": 856176, "start": 8572.72, "end": 8578.72, "text": " of the nodes. The situations and the problems where transformers are applied, you actually do have", "tokens": [50912, 295, 264, 13891, 13, 440, 6851, 293, 264, 2740, 689, 4088, 433, 366, 6456, 11, 291, 767, 360, 362, 51212], "temperature": 0.0, "avg_logprob": -0.09092979098475257, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.023219235241413116}, {"id": 1483, "seek": 856176, "start": 8578.72, "end": 8585.68, "text": " an order. But you use the graph as Petra described to model different long distance relations", "tokens": [51212, 364, 1668, 13, 583, 291, 764, 264, 4295, 382, 10472, 424, 7619, 281, 2316, 819, 938, 4560, 2299, 51560], "temperature": 0.0, "avg_logprob": -0.09092979098475257, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.023219235241413116}, {"id": 1484, "seek": 858568, "start": 8585.76, "end": 8592.4, "text": " between different tokens or words in a sentence. So you want to incorporate this prior knowledge", "tokens": [50368, 1296, 819, 22667, 420, 2283, 294, 257, 8174, 13, 407, 291, 528, 281, 16091, 341, 4059, 3601, 50700], "temperature": 0.0, "avg_logprob": -0.07493403585333573, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.026121726259589195}, {"id": 1485, "seek": 858568, "start": 8592.4, "end": 8599.12, "text": " that these nodes are not in arbitrary order, that they have some sentence order. And this", "tokens": [50700, 300, 613, 13891, 366, 406, 294, 23211, 1668, 11, 300, 436, 362, 512, 8174, 1668, 13, 400, 341, 51036], "temperature": 0.0, "avg_logprob": -0.07493403585333573, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.026121726259589195}, {"id": 1486, "seek": 858568, "start": 8599.12, "end": 8603.68, "text": " principle applied more generally, you can use positional encoding to tell message passing", "tokens": [51036, 8665, 6456, 544, 5101, 11, 291, 393, 764, 2535, 304, 43430, 281, 980, 3636, 8437, 51264], "temperature": 0.0, "avg_logprob": -0.07493403585333573, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.026121726259589195}, {"id": 1487, "seek": 858568, "start": 8603.68, "end": 8609.04, "text": " not to apply exactly the same function everywhere on the graph, but to make it", "tokens": [51264, 406, 281, 3079, 2293, 264, 912, 2445, 5315, 322, 264, 4295, 11, 457, 281, 652, 309, 51532], "temperature": 0.0, "avg_logprob": -0.07493403585333573, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.026121726259589195}, {"id": 1488, "seek": 858568, "start": 8609.04, "end": 8613.84, "text": " specialized for different portions of the graphs or at least make it a possibility. And then", "tokens": [51532, 19813, 337, 819, 25070, 295, 264, 24877, 420, 412, 1935, 652, 309, 257, 7959, 13, 400, 550, 51772], "temperature": 0.0, "avg_logprob": -0.07493403585333573, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.026121726259589195}, {"id": 1489, "seek": 861384, "start": 8614.56, "end": 8620.32, "text": " the training will decide whether to use this information or not or in which way.", "tokens": [50400, 264, 3097, 486, 4536, 1968, 281, 764, 341, 1589, 420, 406, 420, 294, 597, 636, 13, 50688], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1490, "seek": 861384, "start": 8621.52, "end": 8625.36, "text": " It's strange because I don't know whether the order is just a function of the communication", "tokens": [50748, 467, 311, 5861, 570, 286, 500, 380, 458, 1968, 264, 1668, 307, 445, 257, 2445, 295, 264, 6101, 50940], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1491, "seek": 861384, "start": 8625.36, "end": 8631.36, "text": " medium. So we transmit the tokens in a sequence. And could we then represent them in our brains", "tokens": [50940, 6399, 13, 407, 321, 17831, 264, 22667, 294, 257, 8310, 13, 400, 727, 321, 550, 2906, 552, 294, 527, 15442, 51240], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1492, "seek": 861384, "start": 8631.36, "end": 8634.56, "text": " in a completely different domain where the sequence is no longer relevant? Well,", "tokens": [51240, 294, 257, 2584, 819, 9274, 689, 264, 8310, 307, 572, 2854, 7340, 30, 1042, 11, 51400], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1493, "seek": 861384, "start": 8634.56, "end": 8638.16, "text": " actually a lot of neuroscientists think that our brain is a prediction machine and", "tokens": [51400, 767, 257, 688, 295, 28813, 5412, 1751, 519, 300, 527, 3567, 307, 257, 17630, 3479, 293, 51580], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1494, "seek": 861384, "start": 8638.16, "end": 8642.4, "text": " it's a sequence prediction machine. So the sequence is kind of fundamentally important.", "tokens": [51580, 309, 311, 257, 8310, 17630, 3479, 13, 407, 264, 8310, 307, 733, 295, 17879, 1021, 13, 51792], "temperature": 0.0, "avg_logprob": -0.08403825345246688, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0028814610559493303}, {"id": 1495, "seek": 864240, "start": 8643.359999999999, "end": 8647.92, "text": " Yeah, it's also a function of the specific language that you use. And as we discussed", "tokens": [50412, 865, 11, 309, 311, 611, 257, 2445, 295, 264, 2685, 2856, 300, 291, 764, 13, 400, 382, 321, 7152, 50640], "temperature": 0.0, "avg_logprob": -0.12368229952725497, "compression_ratio": 1.6217228464419475, "no_speech_prob": 0.0016165527049452066}, {"id": 1496, "seek": 864240, "start": 8647.92, "end": 8653.68, "text": " before, there are languages which convey the same meaning with a difference in structure.", "tokens": [50640, 949, 11, 456, 366, 8650, 597, 16965, 264, 912, 3620, 365, 257, 2649, 294, 3877, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12368229952725497, "compression_ratio": 1.6217228464419475, "no_speech_prob": 0.0016165527049452066}, {"id": 1497, "seek": 864240, "start": 8653.68, "end": 8659.44, "text": " I want to get a little bit into what you said about essentially what we're doing with these", "tokens": [50928, 286, 528, 281, 483, 257, 707, 857, 666, 437, 291, 848, 466, 4476, 437, 321, 434, 884, 365, 613, 51216], "temperature": 0.0, "avg_logprob": -0.12368229952725497, "compression_ratio": 1.6217228464419475, "no_speech_prob": 0.0016165527049452066}, {"id": 1498, "seek": 864240, "start": 8659.44, "end": 8664.72, "text": " positional encodings is we hint. We hint to the model that there is something here,", "tokens": [51216, 2535, 304, 2058, 378, 1109, 307, 321, 12075, 13, 492, 12075, 281, 264, 2316, 300, 456, 307, 746, 510, 11, 51480], "temperature": 0.0, "avg_logprob": -0.12368229952725497, "compression_ratio": 1.6217228464419475, "no_speech_prob": 0.0016165527049452066}, {"id": 1499, "seek": 864240, "start": 8664.72, "end": 8670.48, "text": " which is a big break from sort of the old approach, let's say, of an LSTM to say,", "tokens": [51480, 597, 307, 257, 955, 1821, 490, 1333, 295, 264, 1331, 3109, 11, 718, 311, 584, 11, 295, 364, 441, 6840, 44, 281, 584, 11, 51768], "temperature": 0.0, "avg_logprob": -0.12368229952725497, "compression_ratio": 1.6217228464419475, "no_speech_prob": 0.0016165527049452066}, {"id": 1500, "seek": 867048, "start": 8670.56, "end": 8678.48, "text": " this is the structure. So with the world of geometric deep learning, I often have the feeling", "tokens": [50368, 341, 307, 264, 3877, 13, 407, 365, 264, 1002, 295, 33246, 2452, 2539, 11, 286, 2049, 362, 264, 2633, 50764], "temperature": 0.0, "avg_logprob": -0.08488130051156749, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.0006559785106219351}, {"id": 1501, "seek": 867048, "start": 8678.48, "end": 8683.6, "text": " people talk about, they talk about symmetries and we need to exploit these symmetries that are", "tokens": [50764, 561, 751, 466, 11, 436, 751, 466, 14232, 302, 2244, 293, 321, 643, 281, 25924, 613, 14232, 302, 2244, 300, 366, 51020], "temperature": 0.0, "avg_logprob": -0.08488130051156749, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.0006559785106219351}, {"id": 1502, "seek": 867048, "start": 8683.6, "end": 8690.4, "text": " present in the world. And there's almost to me two different groups of these symmetries. So one", "tokens": [51020, 1974, 294, 264, 1002, 13, 400, 456, 311, 1920, 281, 385, 732, 819, 3935, 295, 613, 14232, 302, 2244, 13, 407, 472, 51360], "temperature": 0.0, "avg_logprob": -0.08488130051156749, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.0006559785106219351}, {"id": 1503, "seek": 867048, "start": 8690.4, "end": 8696.32, "text": " group is maybe you would call them like exact symmetries or something like this. When I think", "tokens": [51360, 1594, 307, 1310, 291, 576, 818, 552, 411, 1900, 14232, 302, 2244, 420, 746, 411, 341, 13, 1133, 286, 519, 51656], "temperature": 0.0, "avg_logprob": -0.08488130051156749, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.0006559785106219351}, {"id": 1504, "seek": 869632, "start": 8696.32, "end": 8702.64, "text": " about Alpha fold, and I think about like a protein, it doesn't, I don't care which side is up, right?", "tokens": [50364, 466, 20588, 4860, 11, 293, 286, 519, 466, 411, 257, 7944, 11, 309, 1177, 380, 11, 286, 500, 380, 1127, 597, 1252, 307, 493, 11, 558, 30, 50680], "temperature": 0.0, "avg_logprob": -0.09078194914745684, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.01938658580183983}, {"id": 1505, "seek": 869632, "start": 8702.64, "end": 8707.84, "text": " Like the protein is the same, the same protein, and there's no reason to prefer any direction", "tokens": [50680, 1743, 264, 7944, 307, 264, 912, 11, 264, 912, 7944, 11, 293, 456, 311, 572, 1778, 281, 4382, 604, 3513, 50940], "temperature": 0.0, "avg_logprob": -0.09078194914745684, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.01938658580183983}, {"id": 1506, "seek": 869632, "start": 8707.84, "end": 8714.0, "text": " over any other direction. However, if I think of like, because people have made this argument", "tokens": [50940, 670, 604, 661, 3513, 13, 2908, 11, 498, 286, 519, 295, 411, 11, 570, 561, 362, 1027, 341, 6770, 51248], "temperature": 0.0, "avg_logprob": -0.09078194914745684, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.01938658580183983}, {"id": 1507, "seek": 869632, "start": 8714.0, "end": 8718.72, "text": " for CNNs, for example, they say, well, a CNN is a good architecture because it's translation", "tokens": [51248, 337, 24859, 82, 11, 337, 1365, 11, 436, 584, 11, 731, 11, 257, 24859, 307, 257, 665, 9482, 570, 309, 311, 12853, 51484], "temperature": 0.0, "avg_logprob": -0.09078194914745684, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.01938658580183983}, {"id": 1508, "seek": 869632, "start": 8718.72, "end": 8724.64, "text": " invariant, right? And essentially, if we want to do object recognition or image classification,", "tokens": [51484, 33270, 394, 11, 558, 30, 400, 4476, 11, 498, 321, 528, 281, 360, 2657, 11150, 420, 3256, 21538, 11, 51780], "temperature": 0.0, "avg_logprob": -0.09078194914745684, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.01938658580183983}, {"id": 1509, "seek": 872464, "start": 8724.64, "end": 8730.56, "text": " translation invariance is like a given, but but it's not, right? It's not a given the pictures", "tokens": [50364, 12853, 33270, 719, 307, 411, 257, 2212, 11, 457, 457, 309, 311, 406, 11, 558, 30, 467, 311, 406, 257, 2212, 264, 5242, 50660], "temperature": 0.0, "avg_logprob": -0.10518588306747864, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.00048781922669149935}, {"id": 1510, "seek": 872464, "start": 8730.56, "end": 8736.32, "text": " that we feed to these algorithms, most often the object is in the middle, most often, you know,", "tokens": [50660, 300, 321, 3154, 281, 613, 14642, 11, 881, 2049, 264, 2657, 307, 294, 264, 2808, 11, 881, 2049, 11, 291, 458, 11, 50948], "temperature": 0.0, "avg_logprob": -0.10518588306747864, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.00048781922669149935}, {"id": 1511, "seek": 872464, "start": 8736.32, "end": 8742.56, "text": " it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera like this,", "tokens": [50948, 309, 311, 733, 295, 27405, 11, 411, 264, 5443, 307, 322, 1192, 11, 293, 264, 4123, 11, 2086, 11, 286, 393, 1797, 452, 2799, 411, 341, 11, 51260], "temperature": 0.0, "avg_logprob": -0.10518588306747864, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.00048781922669149935}, {"id": 1512, "seek": 872464, "start": 8742.56, "end": 8750.96, "text": " but I don't, right? So it, it seems to be, it seems to be in many cases better to not", "tokens": [51260, 457, 286, 500, 380, 11, 558, 30, 407, 309, 11, 309, 2544, 281, 312, 11, 309, 2544, 281, 312, 294, 867, 3331, 1101, 281, 406, 51680], "temperature": 0.0, "avg_logprob": -0.10518588306747864, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.00048781922669149935}, {"id": 1513, "seek": 875096, "start": 8750.96, "end": 8757.759999999998, "text": " put the symmetries in there until we're like, really, really, really, really sure that these are", "tokens": [50364, 829, 264, 14232, 302, 2244, 294, 456, 1826, 321, 434, 411, 11, 534, 11, 534, 11, 534, 11, 534, 988, 300, 613, 366, 50704], "temperature": 0.0, "avg_logprob": -0.12342752436155914, "compression_ratio": 1.8689320388349515, "no_speech_prob": 0.010323623195290565}, {"id": 1514, "seek": 875096, "start": 8757.759999999998, "end": 8765.439999999999, "text": " actual symmetries, because with more data, it seems the model that does not have the prior inside", "tokens": [50704, 3539, 14232, 302, 2244, 11, 570, 365, 544, 1412, 11, 309, 2544, 264, 2316, 300, 775, 406, 362, 264, 4059, 1854, 51088], "temperature": 0.0, "avg_logprob": -0.12342752436155914, "compression_ratio": 1.8689320388349515, "no_speech_prob": 0.010323623195290565}, {"id": 1515, "seek": 875096, "start": 8766.0, "end": 8772.96, "text": " becomes better than the model that does have the prior inside, if that prior doesn't exactly match", "tokens": [51116, 3643, 1101, 813, 264, 2316, 300, 775, 362, 264, 4059, 1854, 11, 498, 300, 4059, 1177, 380, 2293, 2995, 51464], "temperature": 0.0, "avg_logprob": -0.12342752436155914, "compression_ratio": 1.8689320388349515, "no_speech_prob": 0.010323623195290565}, {"id": 1516, "seek": 875096, "start": 8772.96, "end": 8779.279999999999, "text": " the world is, do you think that's a fair characterization of, for example, why transformers", "tokens": [51464, 264, 1002, 307, 11, 360, 291, 519, 300, 311, 257, 3143, 49246, 295, 11, 337, 1365, 11, 983, 4088, 433, 51780], "temperature": 0.0, "avg_logprob": -0.12342752436155914, "compression_ratio": 1.8689320388349515, "no_speech_prob": 0.010323623195290565}, {"id": 1517, "seek": 877928, "start": 8779.28, "end": 8784.960000000001, "text": " with large data all of a sudden beat classic CNN models or come close to them?", "tokens": [50364, 365, 2416, 1412, 439, 295, 257, 3990, 4224, 7230, 24859, 5245, 420, 808, 1998, 281, 552, 30, 50648], "temperature": 0.0, "avg_logprob": -0.12747750963483537, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.0038785473443567753}, {"id": 1518, "seek": 877928, "start": 8785.84, "end": 8791.76, "text": " I think it's, it's always this question of the trade off between how much your, your model and", "tokens": [50692, 286, 519, 309, 311, 11, 309, 311, 1009, 341, 1168, 295, 264, 4923, 766, 1296, 577, 709, 428, 11, 428, 2316, 293, 50988], "temperature": 0.0, "avg_logprob": -0.12747750963483537, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.0038785473443567753}, {"id": 1519, "seek": 877928, "start": 8791.76, "end": 8799.12, "text": " how much you learn and I remember when I was a student, there was this maxim that that machine", "tokens": [50988, 577, 709, 291, 1466, 293, 286, 1604, 562, 286, 390, 257, 3107, 11, 456, 390, 341, 5138, 300, 300, 3479, 51356], "temperature": 0.0, "avg_logprob": -0.12747750963483537, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.0038785473443567753}, {"id": 1520, "seek": 877928, "start": 8799.12, "end": 8804.960000000001, "text": " learning is always the second best solution. And maybe nowadays with deep learning showing", "tokens": [51356, 2539, 307, 1009, 264, 1150, 1151, 3827, 13, 400, 1310, 13434, 365, 2452, 2539, 4099, 51648], "temperature": 0.0, "avg_logprob": -0.12747750963483537, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.0038785473443567753}, {"id": 1521, "seek": 880496, "start": 8804.96, "end": 8810.16, "text": " some remarkable set of successes, I'm probably less confident in this statement, but it's probably", "tokens": [50364, 512, 12802, 992, 295, 26101, 11, 286, 478, 1391, 1570, 6679, 294, 341, 5629, 11, 457, 309, 311, 1391, 50624], "temperature": 0.0, "avg_logprob": -0.10046454456364998, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.010848874226212502}, {"id": 1522, "seek": 880496, "start": 8810.16, "end": 8815.279999999999, "text": " still quite true that the more you know about your problem, the better chances that machine", "tokens": [50624, 920, 1596, 2074, 300, 264, 544, 291, 458, 466, 428, 1154, 11, 264, 1101, 10486, 300, 3479, 50880], "temperature": 0.0, "avg_logprob": -0.10046454456364998, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.010848874226212502}, {"id": 1523, "seek": 880496, "start": 8815.279999999999, "end": 8821.279999999999, "text": " learning will work for it. To me, it makes sense to model as much as possible and learn what is", "tokens": [50880, 2539, 486, 589, 337, 309, 13, 1407, 385, 11, 309, 1669, 2020, 281, 2316, 382, 709, 382, 1944, 293, 1466, 437, 307, 51180], "temperature": 0.0, "avg_logprob": -0.10046454456364998, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.010848874226212502}, {"id": 1524, "seek": 880496, "start": 8821.279999999999, "end": 8827.519999999999, "text": " current or impossible to model. And in practice, of course, there is a spectrum of possibilities", "tokens": [51180, 2190, 420, 6243, 281, 2316, 13, 400, 294, 3124, 11, 295, 1164, 11, 456, 307, 257, 11143, 295, 12178, 51492], "temperature": 0.0, "avg_logprob": -0.10046454456364998, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.010848874226212502}, {"id": 1525, "seek": 880496, "start": 8827.519999999999, "end": 8833.759999999998, "text": " of how much of these prior assumptions are hardwired into the architecture. And usually,", "tokens": [51492, 295, 577, 709, 295, 613, 4059, 17695, 366, 1152, 86, 1824, 666, 264, 9482, 13, 400, 2673, 11, 51804], "temperature": 0.0, "avg_logprob": -0.10046454456364998, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.010848874226212502}, {"id": 1526, "seek": 883376, "start": 8833.76, "end": 8838.4, "text": " it's a trade off between the, for example, computational complexity availability of the", "tokens": [50364, 309, 311, 257, 4923, 766, 1296, 264, 11, 337, 1365, 11, 28270, 14024, 17945, 295, 264, 50596], "temperature": 0.0, "avg_logprob": -0.1470230160927286, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006137064658105373}, {"id": 1527, "seek": 883376, "start": 8838.4, "end": 8844.08, "text": " data also hardware friendliness. And if you think of what happened in computer vision,", "tokens": [50596, 1412, 611, 8837, 1277, 32268, 13, 400, 498, 291, 519, 295, 437, 2011, 294, 3820, 5201, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1470230160927286, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006137064658105373}, {"id": 1528, "seek": 883376, "start": 8844.08, "end": 8848.56, "text": " it's probably a good illustration that that convolutional networks have translational", "tokens": [50880, 309, 311, 1391, 257, 665, 22645, 300, 300, 45216, 304, 9590, 362, 5105, 1478, 51104], "temperature": 0.0, "avg_logprob": -0.1470230160927286, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006137064658105373}, {"id": 1529, "seek": 883376, "start": 8848.56, "end": 8853.28, "text": " environments, for example, as you mentioned, but in many problems, you might benefit from", "tokens": [51104, 12388, 11, 337, 1365, 11, 382, 291, 2835, 11, 457, 294, 867, 2740, 11, 291, 1062, 5121, 490, 51340], "temperature": 0.0, "avg_logprob": -0.1470230160927286, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006137064658105373}, {"id": 1530, "seek": 883376, "start": 8853.28, "end": 8857.92, "text": " other symmetries such as rotations, again, depends on the application, but imagine that you want to", "tokens": [51340, 661, 14232, 302, 2244, 1270, 382, 44796, 11, 797, 11, 5946, 322, 264, 3861, 11, 457, 3811, 300, 291, 528, 281, 51572], "temperature": 0.0, "avg_logprob": -0.1470230160927286, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006137064658105373}, {"id": 1531, "seek": 885792, "start": 8858.0, "end": 8863.76, "text": " recognize, I don't know, traffic signs, when you can also tilt your car. And you may ask why", "tokens": [50368, 5521, 11, 286, 500, 380, 458, 11, 6419, 7880, 11, 562, 291, 393, 611, 18446, 428, 1032, 13, 400, 291, 815, 1029, 983, 50656], "temperature": 0.0, "avg_logprob": -0.09598440282485064, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.009531316347420216}, {"id": 1532, "seek": 885792, "start": 8863.76, "end": 8870.16, "text": " in these applications as well CNNs are still so popular. And probably one of the reasons is that", "tokens": [50656, 294, 613, 5821, 382, 731, 24859, 82, 366, 920, 370, 3743, 13, 400, 1391, 472, 295, 264, 4112, 307, 300, 50976], "temperature": 0.0, "avg_logprob": -0.09598440282485064, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.009531316347420216}, {"id": 1533, "seek": 885792, "start": 8870.16, "end": 8875.84, "text": " they met very well to the single instruction multiple data type of hardware architectures", "tokens": [50976, 436, 1131, 588, 731, 281, 264, 2167, 10951, 3866, 1412, 2010, 295, 8837, 6331, 1303, 51260], "temperature": 0.0, "avg_logprob": -0.09598440282485064, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.009531316347420216}, {"id": 1534, "seek": 885792, "start": 8875.84, "end": 8882.72, "text": " that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry", "tokens": [51260, 300, 18407, 82, 2626, 13, 400, 291, 393, 29458, 337, 20803, 406, 19163, 337, 45420, 25440, 51604], "temperature": 0.0, "avg_logprob": -0.09598440282485064, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.009531316347420216}, {"id": 1535, "seek": 885792, "start": 8882.72, "end": 8887.44, "text": " with data augmentation, and more complex architectures and larger training sets. And", "tokens": [51604, 365, 1412, 14501, 19631, 11, 293, 544, 3997, 6331, 1303, 293, 4833, 3097, 6352, 13, 400, 51840], "temperature": 0.0, "avg_logprob": -0.09598440282485064, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.009531316347420216}, {"id": 1536, "seek": 888744, "start": 8887.44, "end": 8893.76, "text": " this is exactly what happened a decade ago, this convergence of three trends, the availability", "tokens": [50364, 341, 307, 2293, 437, 2011, 257, 10378, 2057, 11, 341, 32181, 295, 1045, 13892, 11, 264, 17945, 50680], "temperature": 0.0, "avg_logprob": -0.11024315412654433, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.002817608183249831}, {"id": 1537, "seek": 888744, "start": 8893.76, "end": 8899.12, "text": " of compute power, right, the GPUs, algorithms that map well to these computational architectures,", "tokens": [50680, 295, 14722, 1347, 11, 558, 11, 264, 18407, 82, 11, 14642, 300, 4471, 731, 281, 613, 28270, 6331, 1303, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11024315412654433, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.002817608183249831}, {"id": 1538, "seek": 888744, "start": 8899.12, "end": 8904.800000000001, "text": " and these happen to be convolutional networks, and also very large data sets that you can train", "tokens": [50948, 293, 613, 1051, 281, 312, 45216, 304, 9590, 11, 293, 611, 588, 2416, 1412, 6352, 300, 291, 393, 3847, 51232], "temperature": 0.0, "avg_logprob": -0.11024315412654433, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.002817608183249831}, {"id": 1539, "seek": 888744, "start": 8904.800000000001, "end": 8912.960000000001, "text": " these architectures on, such as ImageNet. So many of the choices that become popular in the", "tokens": [51232, 613, 6331, 1303, 322, 11, 1270, 382, 29903, 31890, 13, 407, 867, 295, 264, 7994, 300, 1813, 3743, 294, 264, 51640], "temperature": 0.0, "avg_logprob": -0.11024315412654433, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.002817608183249831}, {"id": 1540, "seek": 891296, "start": 8912.96, "end": 8919.439999999999, "text": " literature are maybe not necessarily theoretically the best ones. So I think in hardware design,", "tokens": [50364, 10394, 366, 1310, 406, 4725, 29400, 264, 1151, 2306, 13, 407, 286, 519, 294, 8837, 1715, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12290752452352773, "compression_ratio": 1.732, "no_speech_prob": 0.004895455203950405}, {"id": 1541, "seek": 891296, "start": 8919.439999999999, "end": 8924.08, "text": " there is this phenomenon that is called the hardware lottery, when it's not necessarily the", "tokens": [50688, 456, 307, 341, 14029, 300, 307, 1219, 264, 8837, 27391, 11, 562, 309, 311, 406, 4725, 264, 50920], "temperature": 0.0, "avg_logprob": -0.12290752452352773, "compression_ratio": 1.732, "no_speech_prob": 0.004895455203950405}, {"id": 1542, "seek": 891296, "start": 8924.08, "end": 8930.24, "text": " best algorithm and the best hardware that solve the problem, it's just some lucky coincidence,", "tokens": [50920, 1151, 9284, 293, 264, 1151, 8837, 300, 5039, 264, 1154, 11, 309, 311, 445, 512, 6356, 22137, 11, 51228], "temperature": 0.0, "avg_logprob": -0.12290752452352773, "compression_ratio": 1.732, "no_speech_prob": 0.004895455203950405}, {"id": 1543, "seek": 891296, "start": 8930.24, "end": 8933.439999999999, "text": " and they are happy marriage that makes them successful.", "tokens": [51228, 293, 436, 366, 2055, 7194, 300, 1669, 552, 4406, 13, 51388], "temperature": 0.0, "avg_logprob": -0.12290752452352773, "compression_ratio": 1.732, "no_speech_prob": 0.004895455203950405}, {"id": 1544, "seek": 891296, "start": 8935.119999999999, "end": 8940.4, "text": " We keep raising this point about how transformers can be seen as special cases of attentional", "tokens": [51472, 492, 1066, 11225, 341, 935, 466, 577, 4088, 433, 393, 312, 1612, 382, 2121, 3331, 295, 3202, 304, 51736], "temperature": 0.0, "avg_logprob": -0.12290752452352773, "compression_ratio": 1.732, "no_speech_prob": 0.004895455203950405}, {"id": 1545, "seek": 894040, "start": 8940.4, "end": 8947.359999999999, "text": " GNNs, but the status quo is that people will use transformers for very many tasks nowadays,", "tokens": [50364, 46411, 45, 82, 11, 457, 264, 6558, 28425, 307, 300, 561, 486, 764, 4088, 433, 337, 588, 867, 9608, 13434, 11, 50712], "temperature": 0.0, "avg_logprob": -0.07489224508696911, "compression_ratio": 1.7, "no_speech_prob": 0.03255956619977951}, {"id": 1546, "seek": 894040, "start": 8947.359999999999, "end": 8952.64, "text": " and there may well be a good argument for considering them in this completely separate", "tokens": [50712, 293, 456, 815, 731, 312, 257, 665, 6770, 337, 8079, 552, 294, 341, 2584, 4994, 50976], "temperature": 0.0, "avg_logprob": -0.07489224508696911, "compression_ratio": 1.7, "no_speech_prob": 0.03255956619977951}, {"id": 1547, "seek": 894040, "start": 8952.64, "end": 8958.08, "text": " light, and one possible explanation or justification for this is the hardware lottery, because,", "tokens": [50976, 1442, 11, 293, 472, 1944, 10835, 420, 31591, 337, 341, 307, 264, 8837, 27391, 11, 570, 11, 51248], "temperature": 0.0, "avg_logprob": -0.07489224508696911, "compression_ratio": 1.7, "no_speech_prob": 0.03255956619977951}, {"id": 1548, "seek": 894040, "start": 8958.72, "end": 8963.44, "text": " yes, sure, transformers perform permutation equivalent operations over a complete graph,", "tokens": [51280, 2086, 11, 988, 11, 4088, 433, 2042, 4784, 11380, 10344, 7705, 670, 257, 3566, 4295, 11, 51516], "temperature": 0.0, "avg_logprob": -0.07489224508696911, "compression_ratio": 1.7, "no_speech_prob": 0.03255956619977951}, {"id": 1549, "seek": 894040, "start": 8963.44, "end": 8969.52, "text": " but they do so in a way that is very, very highly amenable to the kind of matrix multiplication", "tokens": [51516, 457, 436, 360, 370, 294, 257, 636, 300, 307, 588, 11, 588, 5405, 18497, 712, 281, 264, 733, 295, 8141, 27290, 51820], "temperature": 0.0, "avg_logprob": -0.07489224508696911, "compression_ratio": 1.7, "no_speech_prob": 0.03255956619977951}, {"id": 1550, "seek": 896952, "start": 8969.6, "end": 8976.24, "text": " routines that we can support very efficiently on GPUs nowadays. So basically, they can be seen as", "tokens": [50368, 33827, 300, 321, 393, 1406, 588, 19621, 322, 18407, 82, 13434, 13, 407, 1936, 11, 436, 393, 312, 1612, 382, 50700], "temperature": 0.0, "avg_logprob": -0.07075440740010824, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.00689724599942565}, {"id": 1551, "seek": 896952, "start": 8976.24, "end": 8981.68, "text": " the graph neural network that has won the current hardware lottery, even in cases where maybe it", "tokens": [50700, 264, 4295, 18161, 3209, 300, 575, 1582, 264, 2190, 8837, 27391, 11, 754, 294, 3331, 689, 1310, 309, 50972], "temperature": 0.0, "avg_logprob": -0.07075440740010824, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.00689724599942565}, {"id": 1552, "seek": 896952, "start": 8981.68, "end": 8986.640000000001, "text": " will make more sense to consider a more constrained graph, especially if you have low data environments", "tokens": [50972, 486, 652, 544, 2020, 281, 1949, 257, 544, 38901, 4295, 11, 2318, 498, 291, 362, 2295, 1412, 12388, 51220], "temperature": 0.0, "avg_logprob": -0.07075440740010824, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.00689724599942565}, {"id": 1553, "seek": 896952, "start": 8986.640000000001, "end": 8993.92, "text": " or something like this, the potential overheads of running a full graph neural network solution", "tokens": [51220, 420, 746, 411, 341, 11, 264, 3995, 19922, 82, 295, 2614, 257, 1577, 4295, 18161, 3209, 3827, 51584], "temperature": 0.0, "avg_logprob": -0.07075440740010824, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.00689724599942565}, {"id": 1554, "seek": 899392, "start": 8993.92, "end": 9000.0, "text": " with message passing, which, given its extremely sparse nature, doesn't align that well with GPUs", "tokens": [50364, 365, 3636, 8437, 11, 597, 11, 2212, 1080, 4664, 637, 11668, 3687, 11, 1177, 380, 7975, 300, 731, 365, 18407, 82, 50668], "temperature": 0.0, "avg_logprob": -0.07835091677579013, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.11750699579715729}, {"id": 1555, "seek": 899392, "start": 9000.0, "end": 9005.6, "text": " and TPUs nowadays. Sometimes just using a complete graph neural network is the more economical option", "tokens": [50668, 293, 314, 8115, 82, 13434, 13, 4803, 445, 1228, 257, 3566, 4295, 18161, 3209, 307, 264, 544, 42473, 3614, 50948], "temperature": 0.0, "avg_logprob": -0.07835091677579013, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.11750699579715729}, {"id": 1556, "seek": 899392, "start": 9005.6, "end": 9009.84, "text": " when you take all factors into account. And that's, and also the fact that they use an attention", "tokens": [50948, 562, 291, 747, 439, 6771, 666, 2696, 13, 400, 300, 311, 11, 293, 611, 264, 1186, 300, 436, 764, 364, 3202, 51160], "temperature": 0.0, "avg_logprob": -0.07835091677579013, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.11750699579715729}, {"id": 1557, "seek": 899392, "start": 9009.84, "end": 9016.48, "text": " mechanism, which is kind of a middle ground between a simple diffusion process on a graph,", "tokens": [51160, 7513, 11, 597, 307, 733, 295, 257, 2808, 2727, 1296, 257, 2199, 25242, 1399, 322, 257, 4295, 11, 51492], "temperature": 0.0, "avg_logprob": -0.07835091677579013, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.11750699579715729}, {"id": 1558, "seek": 899392, "start": 9016.48, "end": 9020.960000000001, "text": " which we just kind of average things together based on the topology, and the full on message", "tokens": [51492, 597, 321, 445, 733, 295, 4274, 721, 1214, 2361, 322, 264, 1192, 1793, 11, 293, 264, 1577, 322, 3636, 51716], "temperature": 0.0, "avg_logprob": -0.07835091677579013, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.11750699579715729}, {"id": 1559, "seek": 902096, "start": 9020.96, "end": 9025.919999999998, "text": " passing where you actually compute a full on vector message to be sent across the edges.", "tokens": [50364, 8437, 689, 291, 767, 14722, 257, 1577, 322, 8062, 3636, 281, 312, 2279, 2108, 264, 8819, 13, 50612], "temperature": 0.0, "avg_logprob": -0.09901315074855999, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.01941419579088688}, {"id": 1560, "seek": 902096, "start": 9025.919999999998, "end": 9031.519999999999, "text": " Like it strikes a nice balance of scalability and still being able to represent a lot of functions", "tokens": [50612, 1743, 309, 16750, 257, 1481, 4772, 295, 15664, 2310, 293, 920, 885, 1075, 281, 2906, 257, 688, 295, 6828, 50892], "temperature": 0.0, "avg_logprob": -0.09901315074855999, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.01941419579088688}, {"id": 1561, "seek": 902096, "start": 9031.519999999999, "end": 9036.64, "text": " of interest, especially when your inputs are just word tokens, right? So like, you know, in a way,", "tokens": [50892, 295, 1179, 11, 2318, 562, 428, 15743, 366, 445, 1349, 22667, 11, 558, 30, 407, 411, 11, 291, 458, 11, 294, 257, 636, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09901315074855999, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.01941419579088688}, {"id": 1562, "seek": 902096, "start": 9036.64, "end": 9042.0, "text": " it's a GNN that strikes a very nice sweet spot. And that's probably the reason why it's become so", "tokens": [51148, 309, 311, 257, 46411, 45, 300, 16750, 257, 588, 1481, 3844, 4008, 13, 400, 300, 311, 1391, 264, 1778, 983, 309, 311, 1813, 370, 51416], "temperature": 0.0, "avg_logprob": -0.09901315074855999, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.01941419579088688}, {"id": 1563, "seek": 902096, "start": 9043.679999999998, "end": 9048.88, "text": " popular in current times. Now, of course, there is a chance that hardware, and there's actually a", "tokens": [51500, 3743, 294, 2190, 1413, 13, 823, 11, 295, 1164, 11, 456, 307, 257, 2931, 300, 8837, 11, 293, 456, 311, 767, 257, 51760], "temperature": 0.0, "avg_logprob": -0.09901315074855999, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.01941419579088688}, {"id": 1564, "seek": 904888, "start": 9048.88, "end": 9053.599999999999, "text": " pretty high probability that hardware will catch up to the trends in graph representation learning,", "tokens": [50364, 1238, 1090, 8482, 300, 8837, 486, 3745, 493, 281, 264, 13892, 294, 4295, 10290, 2539, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08493862900079466, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.012787297368049622}, {"id": 1565, "seek": 904888, "start": 9053.599999999999, "end": 9059.679999999998, "text": " and we will start to see a bit more graph oriented hardware. But at least for the time being, yeah,", "tokens": [50600, 293, 321, 486, 722, 281, 536, 257, 857, 544, 4295, 21841, 8837, 13, 583, 412, 1935, 337, 264, 565, 885, 11, 1338, 11, 50904], "temperature": 0.0, "avg_logprob": -0.08493862900079466, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.012787297368049622}, {"id": 1566, "seek": 904888, "start": 9059.679999999998, "end": 9063.759999999998, "text": " there's a bit of a combination of what's theoretically making the most sense for your problem,", "tokens": [50904, 456, 311, 257, 857, 295, 257, 6562, 295, 437, 311, 29400, 1455, 264, 881, 2020, 337, 428, 1154, 11, 51108], "temperature": 0.0, "avg_logprob": -0.08493862900079466, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.012787297368049622}, {"id": 1567, "seek": 904888, "start": 9064.32, "end": 9069.759999999998, "text": " and what the hardware that you have right now will support the most easily. Yeah.", "tokens": [51136, 293, 437, 264, 8837, 300, 291, 362, 558, 586, 486, 1406, 264, 881, 3612, 13, 865, 13, 51408], "temperature": 0.0, "avg_logprob": -0.08493862900079466, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.012787297368049622}, {"id": 1568, "seek": 904888, "start": 9070.32, "end": 9074.88, "text": " There is a British startup, I think they have reached recently a unicorn status", "tokens": [51436, 821, 307, 257, 6221, 18578, 11, 286, 519, 436, 362, 6488, 3938, 257, 28122, 6558, 51664], "temperature": 0.0, "avg_logprob": -0.08493862900079466, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.012787297368049622}, {"id": 1569, "seek": 907488, "start": 9074.88, "end": 9079.119999999999, "text": " called Graphcore. And you can already hear from the name of the company,", "tokens": [50364, 1219, 21884, 12352, 13, 400, 291, 393, 1217, 1568, 490, 264, 1315, 295, 264, 2237, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1570, "seek": 907488, "start": 9079.119999999999, "end": 9084.08, "text": " that there is a graph inside, they try to develop hardware that goes beyond the traditional", "tokens": [50576, 300, 456, 307, 257, 4295, 1854, 11, 436, 853, 281, 1499, 8837, 300, 1709, 4399, 264, 5164, 50824], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1571, "seek": 907488, "start": 9084.08, "end": 9088.64, "text": " paradigm. Yeah, I'm really interested in the hardware lottery. We had Sarah Hooker on that massive", "tokens": [50824, 24709, 13, 865, 11, 286, 478, 534, 3102, 294, 264, 8837, 27391, 13, 492, 632, 9519, 33132, 260, 322, 300, 5994, 51052], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1572, "seek": 907488, "start": 9088.64, "end": 9095.039999999999, "text": " shout out to Sarah. And Janik made a video on the hardware lottery paper as well. I mean,", "tokens": [51052, 8043, 484, 281, 9519, 13, 400, 4956, 1035, 1027, 257, 960, 322, 264, 8837, 27391, 3035, 382, 731, 13, 286, 914, 11, 51372], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1573, "seek": 907488, "start": 9095.039999999999, "end": 9099.92, "text": " I'd push back a little bit. I think there's also a bit of an optimization and an algorithmic lottery", "tokens": [51372, 286, 1116, 2944, 646, 257, 707, 857, 13, 286, 519, 456, 311, 611, 257, 857, 295, 364, 19618, 293, 364, 9284, 299, 27391, 51616], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1574, "seek": 907488, "start": 9099.92, "end": 9104.16, "text": " going on. I think there's something very, very interesting about stochastic gradient descent", "tokens": [51616, 516, 322, 13, 286, 519, 456, 311, 746, 588, 11, 588, 1880, 466, 342, 8997, 2750, 16235, 23475, 51828], "temperature": 0.0, "avg_logprob": -0.12418831395738908, "compression_ratio": 1.7365079365079366, "no_speech_prob": 0.015030101872980595}, {"id": 1575, "seek": 910416, "start": 9104.24, "end": 9108.32, "text": " and the kind of data that we're working with. But this actually gets to my next question,", "tokens": [50368, 293, 264, 733, 295, 1412, 300, 321, 434, 1364, 365, 13, 583, 341, 767, 2170, 281, 452, 958, 1168, 11, 50572], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1576, "seek": 910416, "start": 9108.32, "end": 9113.36, "text": " which is about why exactly geometry is a good prior and how principled it is. So", "tokens": [50572, 597, 307, 466, 983, 2293, 18426, 307, 257, 665, 4059, 293, 577, 3681, 15551, 309, 307, 13, 407, 50824], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1577, "seek": 910416, "start": 9113.36, "end": 9118.56, "text": " it seems like these geometric priors are principled. And they have utility because they are", "tokens": [50824, 309, 2544, 411, 613, 33246, 1790, 830, 366, 3681, 15551, 13, 400, 436, 362, 14877, 570, 436, 366, 51084], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1578, "seek": 910416, "start": 9118.56, "end": 9123.6, "text": " low level primitives, right? They're ubiquitous and natural data. But why exactly is it a", "tokens": [51084, 2295, 1496, 2886, 38970, 11, 558, 30, 814, 434, 43868, 39831, 293, 3303, 1412, 13, 583, 983, 2293, 307, 309, 257, 51336], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1579, "seek": 910416, "start": 9123.6, "end": 9127.92, "text": " principled approach to start with things we know, which is to say geometric primitives,", "tokens": [51336, 3681, 15551, 3109, 281, 722, 365, 721, 321, 458, 11, 597, 307, 281, 584, 33246, 2886, 38970, 11, 51552], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1580, "seek": 910416, "start": 9127.92, "end": 9132.24, "text": " and to work upwards from there? You know, what would it look like if we went top down instead?", "tokens": [51552, 293, 281, 589, 22167, 490, 456, 30, 509, 458, 11, 437, 576, 309, 574, 411, 498, 321, 1437, 1192, 760, 2602, 30, 51768], "temperature": 0.0, "avg_logprob": -0.08383169974989564, "compression_ratio": 1.7656765676567656, "no_speech_prob": 0.0048933993093669415}, {"id": 1581, "seek": 913224, "start": 9132.24, "end": 9136.8, "text": " And what makes a good prior? I mean, one way to think about it is the actual function space", "tokens": [50364, 400, 437, 1669, 257, 665, 4059, 30, 286, 914, 11, 472, 636, 281, 519, 466, 309, 307, 264, 3539, 2445, 1901, 50592], "temperature": 0.0, "avg_logprob": -0.08421003709145643, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.0033654537983238697}, {"id": 1582, "seek": 913224, "start": 9137.44, "end": 9141.84, "text": " that you're searching through, you know, this hypothesis space, it's not just about being able", "tokens": [50624, 300, 291, 434, 10808, 807, 11, 291, 458, 11, 341, 17291, 1901, 11, 309, 311, 406, 445, 466, 885, 1075, 50844], "temperature": 0.0, "avg_logprob": -0.08421003709145643, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.0033654537983238697}, {"id": 1583, "seek": 913224, "start": 9141.84, "end": 9146.72, "text": " to find the function easily in that space, or the simplicity of the function you find.", "tokens": [50844, 281, 915, 264, 2445, 3612, 294, 300, 1901, 11, 420, 264, 25632, 295, 264, 2445, 291, 915, 13, 51088], "temperature": 0.0, "avg_logprob": -0.08421003709145643, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.0033654537983238697}, {"id": 1584, "seek": 913224, "start": 9146.72, "end": 9151.6, "text": " Chalet would say it's the information conversion ratio of that function. So, you know, can you", "tokens": [51088, 761, 49744, 576, 584, 309, 311, 264, 1589, 14298, 8509, 295, 300, 2445, 13, 407, 11, 291, 458, 11, 393, 291, 51332], "temperature": 0.0, "avg_logprob": -0.08421003709145643, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.0033654537983238697}, {"id": 1585, "seek": 913224, "start": 9151.6, "end": 9156.64, "text": " use this function that you found to convert a very small piece of information and experience space", "tokens": [51332, 764, 341, 2445, 300, 291, 1352, 281, 7620, 257, 588, 1359, 2522, 295, 1589, 293, 1752, 1901, 51584], "temperature": 0.0, "avg_logprob": -0.08421003709145643, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.0033654537983238697}, {"id": 1586, "seek": 915664, "start": 9156.64, "end": 9162.16, "text": " into new knowledge or a new ability? But how do you find these functions?", "tokens": [50364, 666, 777, 3601, 420, 257, 777, 3485, 30, 583, 577, 360, 291, 915, 613, 6828, 30, 50640], "temperature": 0.0, "avg_logprob": -0.10029388055568789, "compression_ratio": 1.6875, "no_speech_prob": 0.0034664624836295843}, {"id": 1587, "seek": 915664, "start": 9163.439999999999, "end": 9170.48, "text": " One of the points that we try to make in the book is the separation between the domain and", "tokens": [50704, 1485, 295, 264, 2793, 300, 321, 853, 281, 652, 294, 264, 1446, 307, 264, 14634, 1296, 264, 9274, 293, 51056], "temperature": 0.0, "avg_logprob": -0.10029388055568789, "compression_ratio": 1.6875, "no_speech_prob": 0.0034664624836295843}, {"id": 1588, "seek": 915664, "start": 9170.48, "end": 9175.92, "text": " the group that you assume on the domain, the symmetry group. So you might have the same domain,", "tokens": [51056, 264, 1594, 300, 291, 6552, 322, 264, 9274, 11, 264, 25440, 1594, 13, 407, 291, 1062, 362, 264, 912, 9274, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10029388055568789, "compression_ratio": 1.6875, "no_speech_prob": 0.0034664624836295843}, {"id": 1589, "seek": 915664, "start": 9175.92, "end": 9182.32, "text": " like two dimensional grid or two dimensional plane. And for example, the translation group", "tokens": [51328, 411, 732, 18795, 10748, 420, 732, 18795, 5720, 13, 400, 337, 1365, 11, 264, 12853, 1594, 51648], "temperature": 0.0, "avg_logprob": -0.10029388055568789, "compression_ratio": 1.6875, "no_speech_prob": 0.0034664624836295843}, {"id": 1590, "seek": 918232, "start": 9182.32, "end": 9188.16, "text": " or the group of rotations and translations or the group of rigid motions that also include", "tokens": [50364, 420, 264, 1594, 295, 44796, 293, 37578, 420, 264, 1594, 295, 22195, 27500, 300, 611, 4090, 50656], "temperature": 0.0, "avg_logprob": -0.08296516329743141, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.005027180537581444}, {"id": 1591, "seek": 918232, "start": 9188.72, "end": 9197.84, "text": " reflections. So these are completely separate notions. And which one to choose depends on the", "tokens": [50684, 30679, 13, 407, 613, 366, 2584, 4994, 35799, 13, 400, 597, 472, 281, 2826, 5946, 322, 264, 51140], "temperature": 0.0, "avg_logprob": -0.08296516329743141, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.005027180537581444}, {"id": 1592, "seek": 918232, "start": 9197.84, "end": 9203.36, "text": " problem. The choice of the domain really comes from the structure of your data. So if your data", "tokens": [51140, 1154, 13, 440, 3922, 295, 264, 9274, 534, 1487, 490, 264, 3877, 295, 428, 1412, 13, 407, 498, 428, 1412, 51416], "temperature": 0.0, "avg_logprob": -0.08296516329743141, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.005027180537581444}, {"id": 1593, "seek": 918232, "start": 9203.36, "end": 9208.96, "text": " comes as an image, then of course, you use a grid to represent it as the choice of the domain.", "tokens": [51416, 1487, 382, 364, 3256, 11, 550, 295, 1164, 11, 291, 764, 257, 10748, 281, 2906, 309, 382, 264, 3922, 295, 264, 9274, 13, 51696], "temperature": 0.0, "avg_logprob": -0.08296516329743141, "compression_ratio": 1.7688679245283019, "no_speech_prob": 0.005027180537581444}, {"id": 1594, "seek": 920896, "start": 9208.96, "end": 9214.8, "text": " Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying", "tokens": [50364, 823, 11, 597, 25440, 1594, 281, 764, 307, 257, 544, 13743, 935, 13, 400, 309, 534, 5946, 322, 437, 291, 434, 1382, 50656], "temperature": 0.0, "avg_logprob": -0.10246431451094778, "compression_ratio": 1.576, "no_speech_prob": 0.0032427278347313404}, {"id": 1595, "seek": 920896, "start": 9214.8, "end": 9220.56, "text": " to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road,", "tokens": [50656, 281, 4584, 13, 509, 393, 519, 295, 11, 337, 1365, 11, 6419, 1465, 11150, 13, 1133, 257, 1032, 11754, 322, 264, 3060, 11, 50944], "temperature": 0.0, "avg_logprob": -0.10246431451094778, "compression_ratio": 1.576, "no_speech_prob": 0.0032427278347313404}, {"id": 1596, "seek": 920896, "start": 9221.119999999999, "end": 9226.72, "text": " usually the signs will have certain orientation. It's very unlikely that you will see it upside", "tokens": [50972, 2673, 264, 7880, 486, 362, 1629, 14764, 13, 467, 311, 588, 17518, 300, 291, 486, 536, 309, 14119, 51252], "temperature": 0.0, "avg_logprob": -0.10246431451094778, "compression_ratio": 1.576, "no_speech_prob": 0.0032427278347313404}, {"id": 1597, "seek": 920896, "start": 9226.72, "end": 9233.439999999999, "text": " down. So really the only invariance or the only kind of symmetry you have is translation. So CNNs", "tokens": [51252, 760, 13, 407, 534, 264, 787, 33270, 719, 420, 264, 787, 733, 295, 25440, 291, 362, 307, 12853, 13, 407, 24859, 82, 51588], "temperature": 0.0, "avg_logprob": -0.10246431451094778, "compression_ratio": 1.576, "no_speech_prob": 0.0032427278347313404}, {"id": 1598, "seek": 923344, "start": 9233.44, "end": 9239.04, "text": " in this case would work perfectly well. So for example, we have histopathological samples. So", "tokens": [50364, 294, 341, 1389, 576, 589, 6239, 731, 13, 407, 337, 1365, 11, 321, 362, 1758, 27212, 4383, 10938, 13, 407, 50644], "temperature": 0.0, "avg_logprob": -0.10976057137008262, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005681545473635197}, {"id": 1599, "seek": 923344, "start": 9239.04, "end": 9244.32, "text": " you have a slice of tissue that you need to put under the microscope. So you can naturally flip", "tokens": [50644, 291, 362, 257, 13153, 295, 12404, 300, 291, 643, 281, 829, 833, 264, 29753, 13, 407, 291, 393, 8195, 7929, 50908], "temperature": 0.0, "avg_logprob": -0.10976057137008262, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005681545473635197}, {"id": 1600, "seek": 923344, "start": 9244.32, "end": 9249.6, "text": " the glass. You don't know how it is oriented. So reflections are also initial transformation.", "tokens": [50908, 264, 4276, 13, 509, 500, 380, 458, 577, 309, 307, 21841, 13, 407, 30679, 366, 611, 5883, 9887, 13, 51172], "temperature": 0.0, "avg_logprob": -0.10976057137008262, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005681545473635197}, {"id": 1601, "seek": 923344, "start": 9249.6, "end": 9254.800000000001, "text": " So in the traffic signs, of course, this is not physical unless you see your sign in the back", "tokens": [51172, 407, 294, 264, 6419, 7880, 11, 295, 1164, 11, 341, 307, 406, 4001, 5969, 291, 536, 428, 1465, 294, 264, 646, 51432], "temperature": 0.0, "avg_logprob": -0.10976057137008262, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005681545473635197}, {"id": 1602, "seek": 923344, "start": 9254.800000000001, "end": 9261.76, "text": " mirror. But in this histopathology example, it is an initial transformation. So the choice of the", "tokens": [51432, 8013, 13, 583, 294, 341, 1758, 27212, 1793, 1365, 11, 309, 307, 364, 5883, 9887, 13, 407, 264, 3922, 295, 264, 51780], "temperature": 0.0, "avg_logprob": -0.10976057137008262, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005681545473635197}, {"id": 1603, "seek": 926176, "start": 9261.76, "end": 9267.92, "text": " symmetry group and what makes a good geometric prior is really dictated by the specific problem.", "tokens": [50364, 25440, 1594, 293, 437, 1669, 257, 665, 33246, 4059, 307, 534, 12569, 770, 538, 264, 2685, 1154, 13, 50672], "temperature": 0.0, "avg_logprob": -0.11229318052857787, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.002563372952863574}, {"id": 1604, "seek": 926176, "start": 9268.48, "end": 9276.16, "text": " It's often very hard, though, to actually choose because we often don't really know, right, coming", "tokens": [50700, 467, 311, 2049, 588, 1152, 11, 1673, 11, 281, 767, 2826, 570, 321, 2049, 500, 380, 534, 458, 11, 558, 11, 1348, 51084], "temperature": 0.0, "avg_logprob": -0.11229318052857787, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.002563372952863574}, {"id": 1605, "seek": 926176, "start": 9276.16, "end": 9283.2, "text": " back to what I said before, if we actually hit the group correctly. And in fact, we've sort of seen", "tokens": [51084, 646, 281, 437, 286, 848, 949, 11, 498, 321, 767, 2045, 264, 1594, 8944, 13, 400, 294, 1186, 11, 321, 600, 1333, 295, 1612, 51436], "temperature": 0.0, "avg_logprob": -0.11229318052857787, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.002563372952863574}, {"id": 1606, "seek": 926176, "start": 9284.16, "end": 9289.84, "text": " the more successful approach. And this might be hardware specific, but it seems the more", "tokens": [51484, 264, 544, 4406, 3109, 13, 400, 341, 1062, 312, 8837, 2685, 11, 457, 309, 2544, 264, 544, 51768], "temperature": 0.0, "avg_logprob": -0.11229318052857787, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.002563372952863574}, {"id": 1607, "seek": 928984, "start": 9289.84, "end": 9297.2, "text": " successful approach is often to actually make data augmentations with respect to what we assume", "tokens": [50364, 4406, 3109, 307, 2049, 281, 767, 652, 1412, 29919, 763, 365, 3104, 281, 437, 321, 6552, 50732], "temperature": 0.0, "avg_logprob": -0.07103027835969002, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.001986633287742734}, {"id": 1608, "seek": 928984, "start": 9297.2, "end": 9304.4, "text": " are symmetries. So to know, I think of all the color distortions that we do to images, we rotate", "tokens": [50732, 366, 14232, 302, 2244, 13, 407, 281, 458, 11, 286, 519, 295, 439, 264, 2017, 37555, 626, 300, 321, 360, 281, 5267, 11, 321, 13121, 51092], "temperature": 0.0, "avg_logprob": -0.07103027835969002, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.001986633287742734}, {"id": 1609, "seek": 928984, "start": 9304.4, "end": 9310.56, "text": " them a bit, we rescale them and so on, it will be definitely possible to build architectures that", "tokens": [51092, 552, 257, 857, 11, 321, 9610, 1220, 552, 293, 370, 322, 11, 309, 486, 312, 2138, 1944, 281, 1322, 6331, 1303, 300, 51400], "temperature": 0.0, "avg_logprob": -0.07103027835969002, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.001986633287742734}, {"id": 1610, "seek": 928984, "start": 9310.56, "end": 9317.6, "text": " are just invariant to those things. However, it seems to be a more successful approach in practice", "tokens": [51400, 366, 445, 33270, 394, 281, 729, 721, 13, 2908, 11, 309, 2544, 281, 312, 257, 544, 4406, 3109, 294, 3124, 51752], "temperature": 0.0, "avg_logprob": -0.07103027835969002, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.001986633287742734}, {"id": 1611, "seek": 931760, "start": 9317.6, "end": 9326.4, "text": " to put this all into the data augmentations. What's your take on that? How do we choose between", "tokens": [50364, 281, 829, 341, 439, 666, 264, 1412, 29919, 763, 13, 708, 311, 428, 747, 322, 300, 30, 1012, 360, 321, 2826, 1296, 50804], "temperature": 0.0, "avg_logprob": -0.06456515300704772, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0025684016291052103}, {"id": 1612, "seek": 931760, "start": 9326.4, "end": 9332.4, "text": " putting prior knowledge into augmentations versus putting prior knowledge into the architecture?", "tokens": [50804, 3372, 4059, 3601, 666, 29919, 763, 5717, 3372, 4059, 3601, 666, 264, 9482, 30, 51104], "temperature": 0.0, "avg_logprob": -0.06456515300704772, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0025684016291052103}, {"id": 1613, "seek": 931760, "start": 9333.52, "end": 9338.800000000001, "text": " It's not a binary choice. It's not either your model or your augment with data.", "tokens": [51160, 467, 311, 406, 257, 17434, 3922, 13, 467, 311, 406, 2139, 428, 2316, 420, 428, 29919, 365, 1412, 13, 51424], "temperature": 0.0, "avg_logprob": -0.06456515300704772, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0025684016291052103}, {"id": 1614, "seek": 931760, "start": 9339.76, "end": 9344.16, "text": " One of the key principles that we also emphasize in the book is that, of course,", "tokens": [51472, 1485, 295, 264, 2141, 9156, 300, 321, 611, 16078, 294, 264, 1446, 307, 300, 11, 295, 1164, 11, 51692], "temperature": 0.0, "avg_logprob": -0.06456515300704772, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0025684016291052103}, {"id": 1615, "seek": 934416, "start": 9344.16, "end": 9350.56, "text": " this perfect invariance or equivariance is a wishful thinking. In many cases, you want to get", "tokens": [50364, 341, 2176, 33270, 719, 420, 48726, 3504, 719, 307, 257, 3172, 906, 1953, 13, 682, 867, 3331, 11, 291, 528, 281, 483, 50684], "temperature": 0.0, "avg_logprob": -0.10132928496425592, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.004500233568251133}, {"id": 1616, "seek": 934416, "start": 9350.56, "end": 9356.8, "text": " the property that we call geometric stability. You have some transformation that is approximately", "tokens": [50684, 264, 4707, 300, 321, 818, 33246, 11826, 13, 509, 362, 512, 9887, 300, 307, 10447, 50996], "temperature": 0.0, "avg_logprob": -0.10132928496425592, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.004500233568251133}, {"id": 1617, "seek": 934416, "start": 9356.8, "end": 9361.44, "text": " a group. Or imagine that you have a video where two objects are moving, let's say one car moves", "tokens": [50996, 257, 1594, 13, 1610, 3811, 300, 291, 362, 257, 960, 689, 732, 6565, 366, 2684, 11, 718, 311, 584, 472, 1032, 6067, 51228], "temperature": 0.0, "avg_logprob": -0.10132928496425592, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.004500233568251133}, {"id": 1618, "seek": 934416, "start": 9361.44, "end": 9366.4, "text": " left and another car moves right. So there is no global translation that describes the relation", "tokens": [51228, 1411, 293, 1071, 1032, 6067, 558, 13, 407, 456, 307, 572, 4338, 12853, 300, 15626, 264, 9721, 51476], "temperature": 0.0, "avg_logprob": -0.10132928496425592, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.004500233568251133}, {"id": 1619, "seek": 934416, "start": 9366.4, "end": 9371.039999999999, "text": " between the two frames in this video. The geometric stability principle tells you that", "tokens": [51476, 1296, 264, 732, 12083, 294, 341, 960, 13, 440, 33246, 11826, 8665, 5112, 291, 300, 51708], "temperature": 0.0, "avg_logprob": -0.10132928496425592, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.004500233568251133}, {"id": 1620, "seek": 937104, "start": 9372.0, "end": 9378.480000000001, "text": " if you are close enough to an element of the group, if you can describe these transformations", "tokens": [50412, 498, 291, 366, 1998, 1547, 281, 364, 4478, 295, 264, 1594, 11, 498, 291, 393, 6786, 613, 34852, 50736], "temperature": 0.0, "avg_logprob": -0.19601903378384786, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00256994366645813}, {"id": 1621, "seek": 937104, "start": 9378.480000000001, "end": 9382.560000000001, "text": " as an approximate translation, then you will be approximately invariant or approximately", "tokens": [50736, 382, 364, 30874, 12853, 11, 550, 291, 486, 312, 10447, 33270, 394, 420, 10447, 50940], "temperature": 0.0, "avg_logprob": -0.19601903378384786, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00256994366645813}, {"id": 1622, "seek": 937104, "start": 9382.560000000001, "end": 9389.28, "text": " equivalent. This is actually what happens in CNN. This was shown by Joan. They use this", "tokens": [50940, 10344, 13, 639, 307, 767, 437, 2314, 294, 24859, 13, 639, 390, 4898, 538, 25748, 13, 814, 764, 341, 51276], "temperature": 0.0, "avg_logprob": -0.19601903378384786, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00256994366645813}, {"id": 1623, "seek": 937104, "start": 9389.28, "end": 9394.480000000001, "text": " motivation to explain why convolutional neural networks are so powerful. Roughly speaking,", "tokens": [51276, 12335, 281, 2903, 983, 45216, 304, 18161, 9590, 366, 370, 4005, 13, 42791, 356, 4124, 11, 51536], "temperature": 0.0, "avg_logprob": -0.19601903378384786, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00256994366645813}, {"id": 1624, "seek": 937104, "start": 9394.480000000001, "end": 9399.52, "text": " if I don't have a translation, but for example, if I have an MNIS digit and you have different", "tokens": [51536, 498, 286, 500, 380, 362, 257, 12853, 11, 457, 337, 1365, 11, 498, 286, 362, 364, 376, 45, 2343, 14293, 293, 291, 362, 819, 51788], "temperature": 0.0, "avg_logprob": -0.19601903378384786, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00256994366645813}, {"id": 1625, "seek": 939952, "start": 9399.76, "end": 9405.6, "text": " styles of the digits, you can think of them as warping of some canonical digits. So in this case,", "tokens": [50376, 13273, 295, 264, 27011, 11, 291, 393, 519, 295, 552, 382, 1516, 3381, 295, 512, 46491, 27011, 13, 407, 294, 341, 1389, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11643869900009007, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0034018424339592457}, {"id": 1626, "seek": 939952, "start": 9405.6, "end": 9410.08, "text": " even though it's not described as a translation and the neural network will not be invariant or", "tokens": [50668, 754, 1673, 309, 311, 406, 7619, 382, 257, 12853, 293, 264, 18161, 3209, 486, 406, 312, 33270, 394, 420, 50892], "temperature": 0.0, "avg_logprob": -0.11643869900009007, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0034018424339592457}, {"id": 1627, "seek": 939952, "start": 9410.08, "end": 9414.800000000001, "text": " equivariant to this kind of transformation, it will be stable under these transformations.", "tokens": [50892, 48726, 3504, 394, 281, 341, 733, 295, 9887, 11, 309, 486, 312, 8351, 833, 613, 34852, 13, 51128], "temperature": 0.0, "avg_logprob": -0.11643869900009007, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0034018424339592457}, {"id": 1628, "seek": 939952, "start": 9414.800000000001, "end": 9420.16, "text": " And that's why data augmentation works in some sense that you're extending your", "tokens": [51128, 400, 300, 311, 983, 1412, 14501, 19631, 1985, 294, 512, 2020, 300, 291, 434, 24360, 428, 51396], "temperature": 0.0, "avg_logprob": -0.11643869900009007, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0034018424339592457}, {"id": 1629, "seek": 939952, "start": 9420.16, "end": 9423.44, "text": " invariance or equivariance class to approximate invariance and equivariance.", "tokens": [51396, 33270, 719, 420, 48726, 3504, 719, 1508, 281, 30874, 33270, 719, 293, 48726, 3504, 719, 13, 51560], "temperature": 0.0, "avg_logprob": -0.11643869900009007, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0034018424339592457}, {"id": 1630, "seek": 942344, "start": 9424.24, "end": 9428.4, "text": " Taco also had a few interesting things to say about data augmentations versus", "tokens": [50404, 37992, 611, 632, 257, 1326, 1880, 721, 281, 584, 466, 1412, 29919, 763, 5717, 50612], "temperature": 0.0, "avg_logprob": -0.15211099233382788, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.011815718375146389}, {"id": 1631, "seek": 942344, "start": 9429.44, "end": 9435.76, "text": " building the inductive priors into the model. This is Taco. Is your preference towards...", "tokens": [50664, 2390, 264, 31612, 488, 1790, 830, 666, 264, 2316, 13, 639, 307, 37992, 13, 1119, 428, 17502, 3030, 485, 50980], "temperature": 0.0, "avg_logprob": -0.15211099233382788, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.011815718375146389}, {"id": 1632, "seek": 942344, "start": 9436.880000000001, "end": 9442.32, "text": " I assume it is towards creating inductive priors in the architecture around geometry", "tokens": [51036, 286, 6552, 309, 307, 3030, 4084, 31612, 488, 1790, 830, 294, 264, 9482, 926, 18426, 51308], "temperature": 0.0, "avg_logprob": -0.15211099233382788, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.011815718375146389}, {"id": 1633, "seek": 942344, "start": 9442.32, "end": 9449.84, "text": " instead of data augmentation? Oh, that's a good question. I think it depends on the", "tokens": [51308, 2602, 295, 1412, 14501, 19631, 30, 876, 11, 300, 311, 257, 665, 1168, 13, 286, 519, 309, 5946, 322, 264, 51684], "temperature": 0.0, "avg_logprob": -0.15211099233382788, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.011815718375146389}, {"id": 1634, "seek": 944984, "start": 9450.8, "end": 9457.2, "text": " on the problem. Like in some cases, you don't have a choice. So graphs are a great example.", "tokens": [50412, 322, 264, 1154, 13, 1743, 294, 512, 3331, 11, 291, 500, 380, 362, 257, 3922, 13, 407, 24877, 366, 257, 869, 1365, 13, 50732], "temperature": 0.0, "avg_logprob": -0.10482574738178056, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.009265574626624584}, {"id": 1635, "seek": 944984, "start": 9457.2, "end": 9462.32, "text": " The group of permutations is n factorial elements. If n is large, you have 1000 nodes,", "tokens": [50732, 440, 1594, 295, 4784, 325, 763, 307, 297, 36916, 4959, 13, 759, 297, 307, 2416, 11, 291, 362, 9714, 13891, 11, 50988], "temperature": 0.0, "avg_logprob": -0.10482574738178056, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.009265574626624584}, {"id": 1636, "seek": 944984, "start": 9462.32, "end": 9468.16, "text": " you're never ever going to be able to exhaustively sample that group. And so it's better to just", "tokens": [50988, 291, 434, 1128, 1562, 516, 281, 312, 1075, 281, 14687, 3413, 6889, 300, 1594, 13, 400, 370, 309, 311, 1101, 281, 445, 51280], "temperature": 0.0, "avg_logprob": -0.10482574738178056, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.009265574626624584}, {"id": 1637, "seek": 944984, "start": 9468.16, "end": 9475.04, "text": " build it in. And that's also why no graph neural net doesn't respect the symmetry. Nobody's", "tokens": [51280, 1322, 309, 294, 13, 400, 300, 311, 611, 983, 572, 4295, 18161, 2533, 1177, 380, 3104, 264, 25440, 13, 9297, 311, 51624], "temperature": 0.0, "avg_logprob": -0.10482574738178056, "compression_ratio": 1.5040983606557377, "no_speech_prob": 0.009265574626624584}, {"id": 1638, "seek": 947504, "start": 9475.04, "end": 9481.52, "text": " suggesting you should do that by data augmentation. In some other cases, it is", "tokens": [50364, 18094, 291, 820, 360, 300, 538, 1412, 14501, 19631, 13, 682, 512, 661, 3331, 11, 309, 307, 50688], "temperature": 0.0, "avg_logprob": -0.14631530694794237, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.003272000467404723}, {"id": 1639, "seek": 947504, "start": 9484.080000000002, "end": 9490.480000000001, "text": " somewhat possible to sample a reasonably dense grid of transformations in your group.", "tokens": [50816, 8344, 1944, 281, 6889, 257, 23551, 18011, 10748, 295, 34852, 294, 428, 1594, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14631530694794237, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.003272000467404723}, {"id": 1640, "seek": 947504, "start": 9492.400000000001, "end": 9501.44, "text": " And indeed, augmentation is turning out to be very important in unsupervised learning and", "tokens": [51232, 400, 6451, 11, 14501, 19631, 307, 6246, 484, 281, 312, 588, 1021, 294, 2693, 12879, 24420, 2539, 293, 51684], "temperature": 0.0, "avg_logprob": -0.14631530694794237, "compression_ratio": 1.5029585798816567, "no_speech_prob": 0.003272000467404723}, {"id": 1641, "seek": 950144, "start": 9501.44, "end": 9509.2, "text": " self-supervised learning techniques. So I am actually... I look very positively towards that.", "tokens": [50364, 2698, 12, 48172, 24420, 2539, 7512, 13, 407, 286, 669, 767, 485, 286, 574, 588, 25795, 3030, 300, 13, 50752], "temperature": 0.0, "avg_logprob": -0.11104746787778792, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.010984672233462334}, {"id": 1642, "seek": 950144, "start": 9509.2, "end": 9515.68, "text": " I don't think it's wrong to put in this knowledge using data augmentation. But in some cases,", "tokens": [50752, 286, 500, 380, 519, 309, 311, 2085, 281, 829, 294, 341, 3601, 1228, 1412, 14501, 19631, 13, 583, 294, 512, 3331, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11104746787778792, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.010984672233462334}, {"id": 1643, "seek": 950144, "start": 9515.68, "end": 9522.880000000001, "text": " like let's say when you're on classifying medical data like cells in a dish, a histopathology image", "tokens": [51076, 411, 718, 311, 584, 562, 291, 434, 322, 1508, 5489, 4625, 1412, 411, 5438, 294, 257, 5025, 11, 257, 1758, 27212, 1793, 3256, 51436], "temperature": 0.0, "avg_logprob": -0.11104746787778792, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.010984672233462334}, {"id": 1644, "seek": 950144, "start": 9522.880000000001, "end": 9528.24, "text": " or something, you just know for sure there's translation and rotation symmetries. The cells", "tokens": [51436, 420, 746, 11, 291, 445, 458, 337, 988, 456, 311, 12853, 293, 12447, 14232, 302, 2244, 13, 440, 5438, 51704], "temperature": 0.0, "avg_logprob": -0.11104746787778792, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.010984672233462334}, {"id": 1645, "seek": 952824, "start": 9528.24, "end": 9533.76, "text": " don't have a natural orientation. And in those cases, I do think it makes sense to build it", "tokens": [50364, 500, 380, 362, 257, 3303, 14764, 13, 400, 294, 729, 3331, 11, 286, 360, 519, 309, 1669, 2020, 281, 1322, 309, 50640], "temperature": 0.0, "avg_logprob": -0.06958953072043027, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.002050603972747922}, {"id": 1646, "seek": 952824, "start": 9533.76, "end": 9541.6, "text": " into the architecture for the simple reason that if you build it into the architecture,", "tokens": [50640, 666, 264, 9482, 337, 264, 2199, 1778, 300, 498, 291, 1322, 309, 666, 264, 9482, 11, 51032], "temperature": 0.0, "avg_logprob": -0.06958953072043027, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.002050603972747922}, {"id": 1647, "seek": 952824, "start": 9541.6, "end": 9546.64, "text": " you're guaranteed that the network will be equivariant, not just at the training data,", "tokens": [51032, 291, 434, 18031, 300, 264, 3209, 486, 312, 48726, 3504, 394, 11, 406, 445, 412, 264, 3097, 1412, 11, 51284], "temperature": 0.0, "avg_logprob": -0.06958953072043027, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.002050603972747922}, {"id": 1648, "seek": 952824, "start": 9547.6, "end": 9553.76, "text": " but also at the test data. So you never have that the network would make the correct classification", "tokens": [51332, 457, 611, 412, 264, 1500, 1412, 13, 407, 291, 1128, 362, 300, 264, 3209, 576, 652, 264, 3006, 21538, 51640], "temperature": 0.0, "avg_logprob": -0.06958953072043027, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.002050603972747922}, {"id": 1649, "seek": 955376, "start": 9553.76, "end": 9558.64, "text": " for your test data when you have it in one orientation, but when you rotate it, it suddenly", "tokens": [50364, 337, 428, 1500, 1412, 562, 291, 362, 309, 294, 472, 14764, 11, 457, 562, 291, 13121, 309, 11, 309, 5800, 50608], "temperature": 0.0, "avg_logprob": -0.08240258693695068, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0051369075663387775}, {"id": 1650, "seek": 955376, "start": 9558.64, "end": 9566.64, "text": " does something different, which can happen if even when you present your training images in", "tokens": [50608, 775, 746, 819, 11, 597, 393, 1051, 498, 754, 562, 291, 1974, 428, 3097, 5267, 294, 51008], "temperature": 0.0, "avg_logprob": -0.08240258693695068, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0051369075663387775}, {"id": 1651, "seek": 955376, "start": 9566.64, "end": 9573.68, "text": " all possible orientations. So I think for that reason, equivariance does tend to work better", "tokens": [51008, 439, 1944, 8579, 763, 13, 407, 286, 519, 337, 300, 1778, 11, 48726, 3504, 719, 775, 3928, 281, 589, 1101, 51360], "temperature": 0.0, "avg_logprob": -0.08240258693695068, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0051369075663387775}, {"id": 1652, "seek": 955376, "start": 9573.68, "end": 9579.28, "text": " in those cases where there's an exact symmetry in the data. We have actually demonstrated that", "tokens": [51360, 294, 729, 3331, 689, 456, 311, 364, 1900, 25440, 294, 264, 1412, 13, 492, 362, 767, 18772, 300, 51640], "temperature": 0.0, "avg_logprob": -0.08240258693695068, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0051369075663387775}, {"id": 1653, "seek": 957928, "start": 9579.28, "end": 9586.400000000001, "text": " empirically, where for example, in a medical imaging problem of detecting lung nodules in", "tokens": [50364, 25790, 984, 11, 689, 337, 1365, 11, 294, 257, 4625, 25036, 1154, 295, 40237, 16730, 15224, 3473, 294, 50720], "temperature": 0.0, "avg_logprob": -0.13705964421117028, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.006093704607337713}, {"id": 1654, "seek": 957928, "start": 9586.400000000001, "end": 9595.92, "text": " three dimensional CT scans, we started off with a convolutional network with a data augmentation", "tokens": [50720, 1045, 18795, 19529, 35116, 11, 321, 1409, 766, 365, 257, 45216, 304, 3209, 365, 257, 1412, 14501, 19631, 51196], "temperature": 0.0, "avg_logprob": -0.13705964421117028, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.006093704607337713}, {"id": 1655, "seek": 957928, "start": 9595.92, "end": 9601.92, "text": " pipeline, which completely tuned what state-of-the-art method at the time. And we simply replaced", "tokens": [51196, 15517, 11, 597, 2584, 10870, 437, 1785, 12, 2670, 12, 3322, 12, 446, 3170, 412, 264, 565, 13, 400, 321, 2935, 10772, 51496], "temperature": 0.0, "avg_logprob": -0.13705964421117028, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.006093704607337713}, {"id": 1656, "seek": 957928, "start": 9601.92, "end": 9606.640000000001, "text": " all the convolutions by group convolutions that respect to rotational symmetries as well as", "tokens": [51496, 439, 264, 3754, 15892, 538, 1594, 3754, 15892, 300, 3104, 281, 45420, 14232, 302, 2244, 382, 731, 382, 51732], "temperature": 0.0, "avg_logprob": -0.13705964421117028, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.006093704607337713}, {"id": 1657, "seek": 960664, "start": 9606.64, "end": 9613.76, "text": " the translations. And we get very significant improvement in performance. So that goes to show", "tokens": [50364, 264, 37578, 13, 400, 321, 483, 588, 4776, 10444, 294, 3389, 13, 407, 300, 1709, 281, 855, 50720], "temperature": 0.0, "avg_logprob": -0.11429881035013402, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0054654646664857864}, {"id": 1658, "seek": 960664, "start": 9613.76, "end": 9620.32, "text": " that in practice, often data augmentation can't get you all the way. So for the cases where there's", "tokens": [50720, 300, 294, 3124, 11, 2049, 1412, 14501, 19631, 393, 380, 483, 291, 439, 264, 636, 13, 407, 337, 264, 3331, 689, 456, 311, 51048], "temperature": 0.0, "avg_logprob": -0.11429881035013402, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0054654646664857864}, {"id": 1659, "seek": 960664, "start": 9620.32, "end": 9627.68, "text": " an exact symmetry, or where the group of symmetries is very large, I think building it into the network", "tokens": [51048, 364, 1900, 25440, 11, 420, 689, 264, 1594, 295, 14232, 302, 2244, 307, 588, 2416, 11, 286, 519, 2390, 309, 666, 264, 3209, 51416], "temperature": 0.0, "avg_logprob": -0.11429881035013402, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0054654646664857864}, {"id": 1660, "seek": 960664, "start": 9627.68, "end": 9633.68, "text": " is the way to go for the foreseeable future. But there are many cases where augmentation is also", "tokens": [51416, 307, 264, 636, 281, 352, 337, 264, 38736, 712, 2027, 13, 583, 456, 366, 867, 3331, 689, 14501, 19631, 307, 611, 51716], "temperature": 0.0, "avg_logprob": -0.11429881035013402, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0054654646664857864}, {"id": 1661, "seek": 963368, "start": 9634.64, "end": 9636.48, "text": " well, where augmentation is the way to go.", "tokens": [50412, 731, 11, 689, 14501, 19631, 307, 264, 636, 281, 352, 13, 50504], "temperature": 0.0, "avg_logprob": -0.19092274620419458, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.001904460834339261}, {"id": 1662, "seek": 963368, "start": 9637.44, "end": 9643.2, "text": " Fascinating. I'm really interested in this notion that could these symmetries actually be", "tokens": [50552, 49098, 8205, 13, 286, 478, 534, 3102, 294, 341, 10710, 300, 727, 613, 14232, 302, 2244, 767, 312, 50840], "temperature": 0.0, "avg_logprob": -0.19092274620419458, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.001904460834339261}, {"id": 1663, "seek": 963368, "start": 9643.2, "end": 9647.68, "text": " harmful? I know Joanne, for example, spoke about the three sources of error in machine learning", "tokens": [50840, 19727, 30, 286, 458, 3139, 12674, 11, 337, 1365, 11, 7179, 466, 264, 1045, 7139, 295, 6713, 294, 3479, 2539, 51064], "temperature": 0.0, "avg_logprob": -0.19092274620419458, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.001904460834339261}, {"id": 1664, "seek": 963368, "start": 9647.68, "end": 9652.880000000001, "text": " models. And one of them is the approximation error. And normally when we get signals, they come to us", "tokens": [51064, 5245, 13, 400, 472, 295, 552, 307, 264, 28023, 6713, 13, 400, 5646, 562, 321, 483, 12354, 11, 436, 808, 281, 505, 51324], "temperature": 0.0, "avg_logprob": -0.19092274620419458, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.001904460834339261}, {"id": 1665, "seek": 963368, "start": 9652.880000000001, "end": 9658.08, "text": " in a contrived form, don't know, they might be projected onto a planar manifold. And", "tokens": [51324, 294, 257, 660, 470, 937, 1254, 11, 500, 380, 458, 11, 436, 1062, 312, 26231, 3911, 257, 1393, 289, 47138, 13, 400, 51584], "temperature": 0.0, "avg_logprob": -0.19092274620419458, "compression_ratio": 1.5779467680608366, "no_speech_prob": 0.001904460834339261}, {"id": 1666, "seek": 965808, "start": 9658.08, "end": 9666.24, "text": " you know, the sky is always up, for example. Does it really help us having these geometrics", "tokens": [50364, 291, 458, 11, 264, 5443, 307, 1009, 493, 11, 337, 1365, 13, 4402, 309, 534, 854, 505, 1419, 613, 12956, 10716, 50772], "temperature": 0.0, "avg_logprob": -0.1428615883605121, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0017794425366446376}, {"id": 1667, "seek": 965808, "start": 9666.24, "end": 9672.96, "text": " symmetries as primitives in the model? Yeah, that's a good question. The simple answer is,", "tokens": [50772, 14232, 302, 2244, 382, 2886, 38970, 294, 264, 2316, 30, 865, 11, 300, 311, 257, 665, 1168, 13, 440, 2199, 1867, 307, 11, 51108], "temperature": 0.0, "avg_logprob": -0.1428615883605121, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0017794425366446376}, {"id": 1668, "seek": 965808, "start": 9672.96, "end": 9680.72, "text": " if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data,", "tokens": [51108, 498, 428, 1154, 1177, 380, 362, 264, 1900, 25440, 11, 550, 412, 1935, 294, 264, 4948, 295, 1419, 13785, 1412, 11, 51496], "temperature": 0.0, "avg_logprob": -0.1428615883605121, "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.0017794425366446376}, {"id": 1669, "seek": 968072, "start": 9680.96, "end": 9687.92, "text": " building in a covariance is going to be harmful. And it's better to learn the true structure of", "tokens": [50376, 2390, 294, 257, 49851, 719, 307, 516, 281, 312, 19727, 13, 400, 309, 311, 1101, 281, 1466, 264, 2074, 3877, 295, 50724], "temperature": 0.0, "avg_logprob": -0.14833751944608467, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.04267936572432518}, {"id": 1670, "seek": 968072, "start": 9687.92, "end": 9692.32, "text": " the data, this approximate symmetry, which you should be able to pick up from the data alone.", "tokens": [50724, 264, 1412, 11, 341, 30874, 25440, 11, 597, 291, 820, 312, 1075, 281, 1888, 493, 490, 264, 1412, 3312, 13, 50944], "temperature": 0.0, "avg_logprob": -0.14833751944608467, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.04267936572432518}, {"id": 1671, "seek": 968072, "start": 9694.0, "end": 9699.599999999999, "text": " So that's one thing that still means in a low data regime, it can be very useful,", "tokens": [51028, 407, 300, 311, 472, 551, 300, 920, 1355, 294, 257, 2295, 1412, 13120, 11, 309, 393, 312, 588, 4420, 11, 51308], "temperature": 0.0, "avg_logprob": -0.14833751944608467, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.04267936572432518}, {"id": 1672, "seek": 968072, "start": 9699.599999999999, "end": 9706.0, "text": " even when the symmetry is approximate. But I would also say that sometimes,", "tokens": [51308, 754, 562, 264, 25440, 307, 30874, 13, 583, 286, 576, 611, 584, 300, 2171, 11, 51628], "temperature": 0.0, "avg_logprob": -0.14833751944608467, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.04267936572432518}, {"id": 1673, "seek": 970600, "start": 9706.56, "end": 9712.48, "text": " in machine learning, we have a tendency to put too much faith into the", "tokens": [50392, 294, 3479, 2539, 11, 321, 362, 257, 18187, 281, 829, 886, 709, 4522, 666, 264, 50688], "temperature": 0.0, "avg_logprob": -0.1005037481134588, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.03903480991721153}, {"id": 1674, "seek": 970600, "start": 9713.44, "end": 9719.92, "text": " evaluation metrics and data sets. So we say, we want to solve computer vision. And what we mean", "tokens": [50736, 13344, 16367, 293, 1412, 6352, 13, 407, 321, 584, 11, 321, 528, 281, 5039, 3820, 5201, 13, 400, 437, 321, 914, 51060], "temperature": 0.0, "avg_logprob": -0.1005037481134588, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.03903480991721153}, {"id": 1675, "seek": 970600, "start": 9719.92, "end": 9725.76, "text": " is we want to get a high score on ImageNet. And certainly it's true in ImageNet, the images tend", "tokens": [51060, 307, 321, 528, 281, 483, 257, 1090, 6175, 322, 29903, 31890, 13, 400, 3297, 309, 311, 2074, 294, 29903, 31890, 11, 264, 5267, 3928, 51352], "temperature": 0.0, "avg_logprob": -0.1005037481134588, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.03903480991721153}, {"id": 1676, "seek": 970600, "start": 9725.76, "end": 9732.0, "text": " to appear in upright position, and they are photographed by humans. So the key objects are", "tokens": [51352, 281, 4204, 294, 27405, 2535, 11, 293, 436, 366, 45067, 538, 6255, 13, 407, 264, 2141, 6565, 366, 51664], "temperature": 0.0, "avg_logprob": -0.1005037481134588, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.03903480991721153}, {"id": 1677, "seek": 973200, "start": 9732.32, "end": 9736.72, "text": " in the center most of the time, etc. So these are biases that you could exploit,", "tokens": [50380, 294, 264, 3056, 881, 295, 264, 565, 11, 5183, 13, 407, 613, 366, 32152, 300, 291, 727, 25924, 11, 50600], "temperature": 0.0, "avg_logprob": -0.13895008529441943, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.04020655155181885}, {"id": 1678, "seek": 973200, "start": 9737.28, "end": 9748.0, "text": " and you might stop yourself from exploiting them if you build in the symmetry. But that's only a", "tokens": [50628, 293, 291, 1062, 1590, 1803, 490, 12382, 1748, 552, 498, 291, 1322, 294, 264, 25440, 13, 583, 300, 311, 787, 257, 51164], "temperature": 0.0, "avg_logprob": -0.13895008529441943, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.04020655155181885}, {"id": 1679, "seek": 973200, "start": 9748.0, "end": 9755.52, "text": " problem if you put on your blinders and you say ImageNet accuracy is the only thing that counts.", "tokens": [51164, 1154, 498, 291, 829, 322, 428, 6865, 433, 293, 291, 584, 29903, 31890, 14170, 307, 264, 787, 551, 300, 14893, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13895008529441943, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.04020655155181885}, {"id": 1680, "seek": 975552, "start": 9756.16, "end": 9761.04, "text": " You might very well think, if you want to build a very general vision engine,", "tokens": [50396, 509, 1062, 588, 731, 519, 11, 498, 291, 528, 281, 1322, 257, 588, 2674, 5201, 2848, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08398924555097308, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.37676581740379333}, {"id": 1681, "seek": 975552, "start": 9761.04, "end": 9766.720000000001, "text": " it is useful that it still works if suddenly the robot falls over and has to look at the", "tokens": [50640, 309, 307, 4420, 300, 309, 920, 1985, 498, 5800, 264, 7881, 8804, 670, 293, 575, 281, 574, 412, 264, 50924], "temperature": 0.0, "avg_logprob": -0.08398924555097308, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.37676581740379333}, {"id": 1682, "seek": 975552, "start": 9766.720000000001, "end": 9773.12, "text": " world upside down. So the symmetry can still be there in principle, even if it's not there in", "tokens": [50924, 1002, 14119, 760, 13, 407, 264, 25440, 393, 920, 312, 456, 294, 8665, 11, 754, 498, 309, 311, 406, 456, 294, 51244], "temperature": 0.0, "avg_logprob": -0.08398924555097308, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.37676581740379333}, {"id": 1683, "seek": 975552, "start": 9773.12, "end": 9780.800000000001, "text": " practice in your data set. And then there's maybe a robustness versus computational efficiency", "tokens": [51244, 3124, 294, 428, 1412, 992, 13, 400, 550, 456, 311, 1310, 257, 13956, 1287, 5717, 28270, 10493, 51628], "temperature": 0.0, "avg_logprob": -0.08398924555097308, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.37676581740379333}, {"id": 1684, "seek": 978080, "start": 9780.8, "end": 9786.08, "text": " tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still", "tokens": [50364, 4923, 4506, 13, 407, 2086, 11, 1310, 291, 434, 4950, 281, 10692, 498, 428, 7881, 8804, 670, 11, 291, 920, 50628], "temperature": 0.0, "avg_logprob": -0.11409217735816693, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.08501750230789185}, {"id": 1685, "seek": 978080, "start": 9786.08, "end": 9791.92, "text": " want it to work. So you want that rotation equivariance. But then again, if we don't have", "tokens": [50628, 528, 309, 281, 589, 13, 407, 291, 528, 300, 12447, 48726, 3504, 719, 13, 583, 550, 797, 11, 498, 321, 500, 380, 362, 50920], "temperature": 0.0, "avg_logprob": -0.11409217735816693, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.08501750230789185}, {"id": 1686, "seek": 978080, "start": 9791.92, "end": 9798.56, "text": " to process the images upside down, in the 99% of cases where the images are upright,", "tokens": [50920, 281, 1399, 264, 5267, 14119, 760, 11, 294, 264, 11803, 4, 295, 3331, 689, 264, 5267, 366, 27405, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11409217735816693, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.08501750230789185}, {"id": 1687, "seek": 978080, "start": 9799.359999999999, "end": 9804.8, "text": " we gain some computational efficiency. So there's a tradeoff there. And yeah, I don't", "tokens": [51292, 321, 6052, 512, 28270, 10493, 13, 407, 456, 311, 257, 4923, 4506, 456, 13, 400, 1338, 11, 286, 500, 380, 51564], "temperature": 0.0, "avg_logprob": -0.11409217735816693, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.08501750230789185}, {"id": 1688, "seek": 978080, "start": 9804.8, "end": 9809.92, "text": " think there's a right or wrong answer. It's something you have to look at on a case by case", "tokens": [51564, 519, 456, 311, 257, 558, 420, 2085, 1867, 13, 467, 311, 746, 291, 362, 281, 574, 412, 322, 257, 1389, 538, 1389, 51820], "temperature": 0.0, "avg_logprob": -0.11409217735816693, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.08501750230789185}, {"id": 1689, "seek": 980992, "start": 9809.92, "end": 9815.12, "text": " basis. When I put this to Professor Bronstein as well, he also said that you folks were looking at", "tokens": [50364, 5143, 13, 1133, 286, 829, 341, 281, 8419, 19544, 9089, 382, 731, 11, 415, 611, 848, 300, 291, 4024, 645, 1237, 412, 50624], "temperature": 0.0, "avg_logprob": -0.10157481755051657, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.016839396208524704}, {"id": 1690, "seek": 980992, "start": 9815.12, "end": 9818.56, "text": " trying to remember how he described it. I think he said there was a kind of representational", "tokens": [50624, 1382, 281, 1604, 577, 415, 7619, 309, 13, 286, 519, 415, 848, 456, 390, 257, 733, 295, 2906, 1478, 50796], "temperature": 0.0, "avg_logprob": -0.10157481755051657, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.016839396208524704}, {"id": 1691, "seek": 980992, "start": 9818.56, "end": 9823.84, "text": " stability which allowed for approximate symmetries. So it's not necessarily that you're going for", "tokens": [50796, 11826, 597, 4350, 337, 30874, 14232, 302, 2244, 13, 407, 309, 311, 406, 4725, 300, 291, 434, 516, 337, 51060], "temperature": 0.0, "avg_logprob": -0.10157481755051657, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.016839396208524704}, {"id": 1692, "seek": 980992, "start": 9823.84, "end": 9829.36, "text": " these precise symmetries, you're actually looking for a little sort of margin of robustness going", "tokens": [51060, 613, 13600, 14232, 302, 2244, 11, 291, 434, 767, 1237, 337, 257, 707, 1333, 295, 10270, 295, 13956, 1287, 516, 51336], "temperature": 0.0, "avg_logprob": -0.10157481755051657, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.016839396208524704}, {"id": 1693, "seek": 980992, "start": 9829.36, "end": 9832.960000000001, "text": " to moving into approximate symmetries that you might not have explicitly captured.", "tokens": [51336, 281, 2684, 666, 30874, 14232, 302, 2244, 300, 291, 1062, 406, 362, 20803, 11828, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10157481755051657, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.016839396208524704}, {"id": 1694, "seek": 983296, "start": 9833.279999999999, "end": 9840.96, "text": " I agree. I think that's also a very important philosophy and approach. To take the group,", "tokens": [50380, 286, 3986, 13, 286, 519, 300, 311, 611, 257, 588, 1021, 10675, 293, 3109, 13, 1407, 747, 264, 1594, 11, 50764], "temperature": 0.0, "avg_logprob": -0.17484721508654918, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.011154387146234512}, {"id": 1695, "seek": 983296, "start": 9840.96, "end": 9847.039999999999, "text": " think of it as somehow embedded in a larger group, like most of the geometrical symmetries", "tokens": [50764, 519, 295, 309, 382, 6063, 16741, 294, 257, 4833, 1594, 11, 411, 881, 295, 264, 12956, 15888, 14232, 302, 2244, 51068], "temperature": 0.0, "avg_logprob": -0.17484721508654918, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.011154387146234512}, {"id": 1696, "seek": 983296, "start": 9847.039999999999, "end": 9854.56, "text": " that we think about are somehow a subgroup of diffeomorphisms. And then if you say,", "tokens": [51068, 300, 321, 519, 466, 366, 6063, 257, 1422, 17377, 295, 679, 2106, 32702, 13539, 13, 400, 550, 498, 291, 584, 11, 51444], "temperature": 0.0, "avg_logprob": -0.17484721508654918, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.011154387146234512}, {"id": 1697, "seek": 983296, "start": 9854.56, "end": 9859.519999999999, "text": " I don't want invariance or equivariance, but some kind of stability or smoothness to", "tokens": [51444, 286, 500, 380, 528, 33270, 719, 420, 48726, 3504, 719, 11, 457, 512, 733, 295, 11826, 420, 5508, 1287, 281, 51692], "temperature": 0.0, "avg_logprob": -0.17484721508654918, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.011154387146234512}, {"id": 1698, "seek": 985952, "start": 9860.48, "end": 9868.880000000001, "text": " elements in the group plus small diffeomorphisms, for instance, you might get some of the", "tokens": [50412, 4959, 294, 264, 1594, 1804, 1359, 679, 2106, 32702, 13539, 11, 337, 5197, 11, 291, 1062, 483, 512, 295, 264, 50832], "temperature": 0.0, "avg_logprob": -0.17270792684247416, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.0036354116164147854}, {"id": 1699, "seek": 985952, "start": 9868.880000000001, "end": 9876.24, "text": " generalization benefit without unduly limiting the capacity of your model.", "tokens": [50832, 2674, 2144, 5121, 1553, 674, 3540, 22083, 264, 6042, 295, 428, 2316, 13, 51200], "temperature": 0.0, "avg_logprob": -0.17270792684247416, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.0036354116164147854}, {"id": 1700, "seek": 985952, "start": 9877.12, "end": 9884.16, "text": " Is there a hope though that we can get this approximate? Because I see your point, I think,", "tokens": [51244, 1119, 456, 257, 1454, 1673, 300, 321, 393, 483, 341, 30874, 30, 1436, 286, 536, 428, 935, 11, 286, 519, 11, 51596], "temperature": 0.0, "avg_logprob": -0.17270792684247416, "compression_ratio": 1.4463276836158192, "no_speech_prob": 0.0036354116164147854}, {"id": 1701, "seek": 988416, "start": 9884.24, "end": 9891.039999999999, "text": " is that, or one of the points is, I think, is that if we program like a symmetry into the", "tokens": [50368, 307, 300, 11, 420, 472, 295, 264, 2793, 307, 11, 286, 519, 11, 307, 300, 498, 321, 1461, 411, 257, 25440, 666, 264, 50708], "temperature": 0.0, "avg_logprob": -0.128591885076505, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.11550218611955643}, {"id": 1702, "seek": 988416, "start": 9891.039999999999, "end": 9895.6, "text": " architecture, it will be rather fixed, right? We make an architecture translation invariant,", "tokens": [50708, 9482, 11, 309, 486, 312, 2831, 6806, 11, 558, 30, 492, 652, 364, 9482, 12853, 33270, 394, 11, 50936], "temperature": 0.0, "avg_logprob": -0.128591885076505, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.11550218611955643}, {"id": 1703, "seek": 988416, "start": 9895.6, "end": 9901.76, "text": " it's going to be fully translation invariant. Are there good ways to bring approximate", "tokens": [50936, 309, 311, 516, 281, 312, 4498, 12853, 33270, 394, 13, 2014, 456, 665, 2098, 281, 1565, 30874, 51244], "temperature": 0.0, "avg_logprob": -0.128591885076505, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.11550218611955643}, {"id": 1704, "seek": 988416, "start": 9901.76, "end": 9906.24, "text": " invariances into the space of the architectures that we work with?", "tokens": [51244, 33270, 2676, 666, 264, 1901, 295, 264, 6331, 1303, 300, 321, 589, 365, 30, 51468], "temperature": 0.0, "avg_logprob": -0.128591885076505, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.11550218611955643}, {"id": 1705, "seek": 988416, "start": 9906.24, "end": 9912.72, "text": " Yeah, I mean, I have a very quick answer, maybe not too satisfying, but one very simple one,", "tokens": [51468, 865, 11, 286, 914, 11, 286, 362, 257, 588, 1702, 1867, 11, 1310, 406, 886, 18348, 11, 457, 472, 588, 2199, 472, 11, 51792], "temperature": 0.0, "avg_logprob": -0.128591885076505, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.11550218611955643}, {"id": 1706, "seek": 991272, "start": 9912.72, "end": 9917.519999999999, "text": " if you think of neural network blocks as like components that implement different symmetries,", "tokens": [50364, 498, 291, 519, 295, 18161, 3209, 8474, 382, 411, 6677, 300, 4445, 819, 14232, 302, 2244, 11, 50604], "temperature": 0.0, "avg_logprob": -0.076782039567536, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0037062368355691433}, {"id": 1707, "seek": 991272, "start": 9917.519999999999, "end": 9921.199999999999, "text": " and then you think of like a calculus of these blocks as you know, building your deep learning", "tokens": [50604, 293, 550, 291, 519, 295, 411, 257, 33400, 295, 613, 8474, 382, 291, 458, 11, 2390, 428, 2452, 2539, 50788], "temperature": 0.0, "avg_logprob": -0.076782039567536, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0037062368355691433}, {"id": 1708, "seek": 991272, "start": 9921.199999999999, "end": 9928.4, "text": " architecture. One very simple representational tool that we can use to allow the model to use", "tokens": [50788, 9482, 13, 1485, 588, 2199, 2906, 1478, 2290, 300, 321, 393, 764, 281, 2089, 264, 2316, 281, 764, 51148], "temperature": 0.0, "avg_logprob": -0.076782039567536, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0037062368355691433}, {"id": 1709, "seek": 991272, "start": 9928.4, "end": 9934.72, "text": " the symmetry, but also not use the symmetry is the skip connection. So essentially, you could have", "tokens": [51148, 264, 25440, 11, 457, 611, 406, 764, 264, 25440, 307, 264, 10023, 4984, 13, 407, 4476, 11, 291, 727, 362, 51464], "temperature": 0.0, "avg_logprob": -0.076782039567536, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0037062368355691433}, {"id": 1710, "seek": 991272, "start": 9934.72, "end": 9939.519999999999, "text": " a model that processes, for example, your graph data using a particular connectivity structure", "tokens": [51464, 257, 2316, 300, 7555, 11, 337, 1365, 11, 428, 4295, 1412, 1228, 257, 1729, 21095, 3877, 51704], "temperature": 0.0, "avg_logprob": -0.076782039567536, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0037062368355691433}, {"id": 1711, "seek": 993952, "start": 9939.52, "end": 9944.16, "text": " that you want to be invariant to. And you can also use say a transformer that processes things", "tokens": [50364, 300, 291, 528, 281, 312, 33270, 394, 281, 13, 400, 291, 393, 611, 764, 584, 257, 31782, 300, 7555, 721, 50596], "temperature": 0.0, "avg_logprob": -0.06988834079943206, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.025166386738419533}, {"id": 1712, "seek": 993952, "start": 9944.16, "end": 9949.28, "text": " in a completely permutation invariant way over the complete graph. And you can just shortcut the", "tokens": [50596, 294, 257, 2584, 4784, 11380, 33270, 394, 636, 670, 264, 3566, 4295, 13, 400, 291, 393, 445, 24822, 264, 50852], "temperature": 0.0, "avg_logprob": -0.06988834079943206, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.025166386738419533}, {"id": 1713, "seek": 993952, "start": 9949.28, "end": 9954.560000000001, "text": " results of one model over the other model, if you want to give also the model the choice to ignore", "tokens": [50852, 3542, 295, 472, 2316, 670, 264, 661, 2316, 11, 498, 291, 528, 281, 976, 611, 264, 2316, 264, 3922, 281, 11200, 51116], "temperature": 0.0, "avg_logprob": -0.06988834079943206, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.025166386738419533}, {"id": 1714, "seek": 993952, "start": 9954.560000000001, "end": 9961.36, "text": " the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one", "tokens": [51116, 264, 3894, 472, 13, 407, 1310, 406, 257, 588, 11, 291, 458, 11, 9942, 293, 18348, 1867, 11, 457, 300, 311, 472, 51456], "temperature": 0.0, "avg_logprob": -0.06988834079943206, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.025166386738419533}, {"id": 1715, "seek": 993952, "start": 9961.36, "end": 9965.76, "text": " simple way in which we could do something like this. And in some of our more recent algorithmic", "tokens": [51456, 2199, 636, 294, 597, 321, 727, 360, 746, 411, 341, 13, 400, 294, 512, 295, 527, 544, 5162, 9284, 299, 51676], "temperature": 0.0, "avg_logprob": -0.06988834079943206, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.025166386738419533}, {"id": 1716, "seek": 996576, "start": 9965.76, "end": 9970.880000000001, "text": " reasoning blueprint papers, we do exactly this, because one very important thing that we're trying", "tokens": [50364, 21577, 35868, 10577, 11, 321, 360, 2293, 341, 11, 570, 472, 588, 1021, 551, 300, 321, 434, 1382, 50620], "temperature": 0.0, "avg_logprob": -0.0625430907843248, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.015658803284168243}, {"id": 1717, "seek": 996576, "start": 9970.880000000001, "end": 9977.28, "text": " to solve is apply classical algorithms to problems that would need them. But the data is super rich,", "tokens": [50620, 281, 5039, 307, 3079, 13735, 14642, 281, 2740, 300, 576, 643, 552, 13, 583, 264, 1412, 307, 1687, 4593, 11, 50940], "temperature": 0.0, "avg_logprob": -0.0625430907843248, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.015658803284168243}, {"id": 1718, "seek": 996576, "start": 9977.28, "end": 9982.0, "text": " and it's really hard to, you know, massage it into the abstractified form that the algorithm needs.", "tokens": [50940, 293, 309, 311, 534, 1152, 281, 11, 291, 458, 11, 16145, 309, 666, 264, 12649, 2587, 1254, 300, 264, 9284, 2203, 13, 51176], "temperature": 0.0, "avg_logprob": -0.0625430907843248, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.015658803284168243}, {"id": 1719, "seek": 996576, "start": 9982.0, "end": 9986.800000000001, "text": " For example, you want to find shortest paths in a road network, a real world road network,", "tokens": [51176, 1171, 1365, 11, 291, 528, 281, 915, 31875, 14518, 294, 257, 3060, 3209, 11, 257, 957, 1002, 3060, 3209, 11, 51416], "temperature": 0.0, "avg_logprob": -0.0625430907843248, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.015658803284168243}, {"id": 1720, "seek": 996576, "start": 9986.800000000001, "end": 9991.76, "text": " you cannot just take all the complexity of changing weather conditions, changing diffusion", "tokens": [51416, 291, 2644, 445, 747, 439, 264, 14024, 295, 4473, 5503, 4487, 11, 4473, 25242, 51664], "temperature": 0.0, "avg_logprob": -0.0625430907843248, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.015658803284168243}, {"id": 1721, "seek": 999176, "start": 9991.76, "end": 9996.48, "text": " patterns on the on the roads and the roadblocks and all these kinds of things and turn that into", "tokens": [50364, 8294, 322, 264, 322, 264, 11344, 293, 264, 3060, 15962, 2761, 293, 439, 613, 3685, 295, 721, 293, 1261, 300, 666, 50600], "temperature": 0.0, "avg_logprob": -0.09042543225583777, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.016653627157211304}, {"id": 1722, "seek": 999176, "start": 9996.48, "end": 10001.04, "text": " this abstractified graph with exactly one scalar per each edge. So you can apply dykstra or something", "tokens": [50600, 341, 12649, 2587, 4295, 365, 2293, 472, 39684, 680, 1184, 4691, 13, 407, 291, 393, 3079, 14584, 74, 19639, 420, 746, 50828], "temperature": 0.0, "avg_logprob": -0.09042543225583777, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.016653627157211304}, {"id": 1723, "seek": 999176, "start": 10001.04, "end": 10006.16, "text": " like this, like, it's just not feasible without losing a ton of information. So what we're doing", "tokens": [50828, 411, 341, 11, 411, 11, 309, 311, 445, 406, 26648, 1553, 7027, 257, 2952, 295, 1589, 13, 407, 437, 321, 434, 884, 51084], "temperature": 0.0, "avg_logprob": -0.09042543225583777, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.016653627157211304}, {"id": 1724, "seek": 999176, "start": 10006.16, "end": 10011.28, "text": " here is we make this high dimensional neural network component that simulates the effects of", "tokens": [51084, 510, 307, 321, 652, 341, 1090, 18795, 18161, 3209, 6542, 300, 1034, 26192, 264, 5065, 295, 51340], "temperature": 0.0, "avg_logprob": -0.09042543225583777, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.016653627157211304}, {"id": 1725, "seek": 999176, "start": 10011.28, "end": 10017.52, "text": " dykstra. But we're also mindful of the fact that to compute say the expected travel time,", "tokens": [51340, 14584, 74, 19639, 13, 583, 321, 434, 611, 14618, 295, 264, 1186, 300, 281, 14722, 584, 264, 5176, 3147, 565, 11, 51652], "temperature": 0.0, "avg_logprob": -0.09042543225583777, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.016653627157211304}, {"id": 1726, "seek": 1001752, "start": 10017.52, "end": 10022.16, "text": " there's more factors at play than just the output of a shortest path algorithm, right?", "tokens": [50364, 456, 311, 544, 6771, 412, 862, 813, 445, 264, 5598, 295, 257, 31875, 3100, 9284, 11, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.06662789380775308, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.016650531440973282}, {"id": 1727, "seek": 1001752, "start": 10022.16, "end": 10027.52, "text": " There could well also be some flow related elements, maybe just some elements related to the", "tokens": [50596, 821, 727, 731, 611, 312, 512, 3095, 4077, 4959, 11, 1310, 445, 512, 4959, 4077, 281, 264, 50864], "temperature": 0.0, "avg_logprob": -0.06662789380775308, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.016650531440973282}, {"id": 1728, "seek": 1001752, "start": 10027.52, "end": 10033.76, "text": " current time of day, human psychology, whatnot, right? So we start off by assuming the algorithm", "tokens": [50864, 2190, 565, 295, 786, 11, 1952, 15105, 11, 25882, 11, 558, 30, 407, 321, 722, 766, 538, 11926, 264, 9284, 51176], "temperature": 0.0, "avg_logprob": -0.06662789380775308, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.016650531440973282}, {"id": 1729, "seek": 1001752, "start": 10033.76, "end": 10041.04, "text": " does not give the complete picture in this high dimensional noisy world. So we always, as default,", "tokens": [51176, 775, 406, 976, 264, 3566, 3036, 294, 341, 1090, 18795, 24518, 1002, 13, 407, 321, 1009, 11, 382, 7576, 11, 51540], "temperature": 0.0, "avg_logprob": -0.06662789380775308, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.016650531440973282}, {"id": 1730, "seek": 1001752, "start": 10041.04, "end": 10045.84, "text": " as part of our architecture, incorporate a skip connection from just, you know, a raw neural", "tokens": [51540, 382, 644, 295, 527, 9482, 11, 16091, 257, 10023, 4984, 490, 445, 11, 291, 458, 11, 257, 8936, 18161, 51780], "temperature": 0.0, "avg_logprob": -0.06662789380775308, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.016650531440973282}, {"id": 1731, "seek": 1004584, "start": 10045.84, "end": 10051.28, "text": " network encoder over the algorithm. So in case there's any model free information that you want", "tokens": [50364, 3209, 2058, 19866, 670, 264, 9284, 13, 407, 294, 1389, 456, 311, 604, 2316, 1737, 1589, 300, 291, 528, 50636], "temperature": 0.0, "avg_logprob": -0.1038883396836578, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0014321634080260992}, {"id": 1732, "seek": 1004584, "start": 10051.28, "end": 10056.24, "text": " to extract without looking at what the algorithm tells you, you can do that. So maybe I don't", "tokens": [50636, 281, 8947, 1553, 1237, 412, 437, 264, 9284, 5112, 291, 11, 291, 393, 360, 300, 13, 407, 1310, 286, 500, 380, 50884], "temperature": 0.0, "avg_logprob": -0.1038883396836578, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0014321634080260992}, {"id": 1733, "seek": 1004584, "start": 10056.24, "end": 10060.880000000001, "text": " know, Yannick, if that answers your question about approximate symmetries, but that's, that's the", "tokens": [50884, 458, 11, 398, 969, 618, 11, 498, 300, 6338, 428, 1168, 466, 30874, 14232, 302, 2244, 11, 457, 300, 311, 11, 300, 311, 264, 51116], "temperature": 0.0, "avg_logprob": -0.1038883396836578, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0014321634080260992}, {"id": 1734, "seek": 1004584, "start": 10060.880000000001, "end": 10065.44, "text": " kind of divide by God's when I heard the question. I mean, that's a very, that's a very practical", "tokens": [51116, 733, 295, 9845, 538, 1265, 311, 562, 286, 2198, 264, 1168, 13, 286, 914, 11, 300, 311, 257, 588, 11, 300, 311, 257, 588, 8496, 51344], "temperature": 0.0, "avg_logprob": -0.1038883396836578, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0014321634080260992}, {"id": 1735, "seek": 1004584, "start": 10066.0, "end": 10071.92, "text": " answer for sure that that, you know, you can actually get out there. It even opens the possibility", "tokens": [51372, 1867, 337, 988, 300, 300, 11, 291, 458, 11, 291, 393, 767, 483, 484, 456, 13, 467, 754, 9870, 264, 7959, 51668], "temperature": 0.0, "avg_logprob": -0.1038883396836578, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0014321634080260992}, {"id": 1736, "seek": 1007192, "start": 10071.92, "end": 10079.76, "text": " to having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your", "tokens": [50364, 281, 1419, 1310, 3866, 3685, 295, 10023, 9271, 293, 25882, 11, 291, 458, 11, 1419, 26764, 493, 428, 50756], "temperature": 0.0, "avg_logprob": -0.07229124957864935, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.004328811075538397}, {"id": 1737, "seek": 1007192, "start": 10079.76, "end": 10087.12, "text": " symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe", "tokens": [50756, 14232, 302, 2244, 666, 2609, 8474, 300, 366, 1190, 294, 8952, 11, 1310, 13, 583, 300, 5607, 385, 281, 1310, 51124], "temperature": 0.0, "avg_logprob": -0.07229124957864935, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.004328811075538397}, {"id": 1738, "seek": 1007192, "start": 10087.12, "end": 10091.44, "text": " another thing we've talked about, you know, there are symmetries, you want to incorporate them into", "tokens": [51124, 1071, 551, 321, 600, 2825, 466, 11, 291, 458, 11, 456, 366, 14232, 302, 2244, 11, 291, 528, 281, 16091, 552, 666, 51340], "temperature": 0.0, "avg_logprob": -0.07229124957864935, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.004328811075538397}, {"id": 1739, "seek": 1007192, "start": 10091.44, "end": 10097.36, "text": " your problem and so on. And we've also talked about the symbolic regression beforehand to maybe", "tokens": [51340, 428, 1154, 293, 370, 322, 13, 400, 321, 600, 611, 2825, 466, 264, 25755, 24590, 22893, 281, 1310, 51636], "temperature": 0.0, "avg_logprob": -0.07229124957864935, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.004328811075538397}, {"id": 1740, "seek": 1009736, "start": 10098.32, "end": 10104.880000000001, "text": " parse out symmetries of the underlying problem. What are the current best approaches if we don't", "tokens": [50412, 48377, 484, 14232, 302, 2244, 295, 264, 14217, 1154, 13, 708, 366, 264, 2190, 1151, 11587, 498, 321, 500, 380, 50740], "temperature": 0.0, "avg_logprob": -0.09827252438193873, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004330295603722334}, {"id": 1741, "seek": 1009736, "start": 10104.880000000001, "end": 10110.960000000001, "text": " know the symmetries? So we have a bunch of data, we suspect there must be some kind of", "tokens": [50740, 458, 264, 14232, 302, 2244, 30, 407, 321, 362, 257, 3840, 295, 1412, 11, 321, 9091, 456, 1633, 312, 512, 733, 295, 51044], "temperature": 0.0, "avg_logprob": -0.09827252438193873, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004330295603722334}, {"id": 1742, "seek": 1009736, "start": 10110.960000000001, "end": 10117.36, "text": " symmetries at play because they're usually are in the world, right? And they, if we knew them,", "tokens": [51044, 14232, 302, 2244, 412, 862, 570, 436, 434, 2673, 366, 294, 264, 1002, 11, 558, 30, 400, 436, 11, 498, 321, 2586, 552, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09827252438193873, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004330295603722334}, {"id": 1743, "seek": 1009736, "start": 10117.36, "end": 10123.44, "text": " we could describe our problems in very compact forms and solve them very efficiently, but we don't", "tokens": [51364, 321, 727, 6786, 527, 2740, 294, 588, 14679, 6422, 293, 5039, 552, 588, 19621, 11, 457, 321, 500, 380, 51668], "temperature": 0.0, "avg_logprob": -0.09827252438193873, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004330295603722334}, {"id": 1744, "seek": 1012344, "start": 10123.44, "end": 10130.32, "text": " often know. So what are the current state of the art? When I don't know the symmetries, how do I", "tokens": [50364, 2049, 458, 13, 407, 437, 366, 264, 2190, 1785, 295, 264, 1523, 30, 1133, 286, 500, 380, 458, 264, 14232, 302, 2244, 11, 577, 360, 286, 50708], "temperature": 0.0, "avg_logprob": -0.07245164005844681, "compression_ratio": 1.6594982078853047, "no_speech_prob": 0.009088188409805298}, {"id": 1745, "seek": 1012344, "start": 10130.32, "end": 10135.92, "text": " discover what group structure is at play in a particular problem? I don't think that there is", "tokens": [50708, 4411, 437, 1594, 3877, 307, 412, 862, 294, 257, 1729, 1154, 30, 286, 500, 380, 519, 300, 456, 307, 50988], "temperature": 0.0, "avg_logprob": -0.07245164005844681, "compression_ratio": 1.6594982078853047, "no_speech_prob": 0.009088188409805298}, {"id": 1746, "seek": 1012344, "start": 10135.92, "end": 10141.84, "text": " a single approach that solves this problem in a satisfactory manner. And one of the reasons why", "tokens": [50988, 257, 2167, 3109, 300, 39890, 341, 1154, 294, 257, 48614, 9060, 13, 400, 472, 295, 264, 4112, 983, 51284], "temperature": 0.0, "avg_logprob": -0.07245164005844681, "compression_ratio": 1.6594982078853047, "no_speech_prob": 0.009088188409805298}, {"id": 1747, "seek": 1012344, "start": 10141.84, "end": 10146.880000000001, "text": " because the problem is ambiguous. So maybe an example, think of objects mostly translate", "tokens": [51284, 570, 264, 1154, 307, 39465, 13, 407, 1310, 364, 1365, 11, 519, 295, 6565, 5240, 13799, 51536], "temperature": 0.0, "avg_logprob": -0.07245164005844681, "compression_ratio": 1.6594982078853047, "no_speech_prob": 0.009088188409805298}, {"id": 1748, "seek": 1012344, "start": 10146.880000000001, "end": 10152.480000000001, "text": " horizontally, but you also have a little bit of vertical translation. What is the right", "tokens": [51536, 33796, 11, 457, 291, 611, 362, 257, 707, 857, 295, 9429, 12853, 13, 708, 307, 264, 558, 51816], "temperature": 0.0, "avg_logprob": -0.07245164005844681, "compression_ratio": 1.6594982078853047, "no_speech_prob": 0.009088188409805298}, {"id": 1749, "seek": 1015248, "start": 10152.48, "end": 10156.24, "text": " symmetry structure to model? Is it a one dimensional translation group or a two dimensional", "tokens": [50364, 25440, 3877, 281, 2316, 30, 1119, 309, 257, 472, 18795, 12853, 1594, 420, 257, 732, 18795, 50552], "temperature": 0.0, "avg_logprob": -0.09934232018210672, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.006070288829505444}, {"id": 1750, "seek": 1015248, "start": 10156.24, "end": 10163.52, "text": " translation group? Do we want to absorb the vertical, the slight vertical motions as the noise", "tokens": [50552, 12853, 1594, 30, 1144, 321, 528, 281, 15631, 264, 9429, 11, 264, 4036, 9429, 27500, 382, 264, 5658, 50916], "temperature": 0.0, "avg_logprob": -0.09934232018210672, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.006070288829505444}, {"id": 1751, "seek": 1015248, "start": 10163.52, "end": 10168.96, "text": " and deal with it as a data augmentation, or you want to describe it in the structure of the group", "tokens": [50916, 293, 2028, 365, 309, 382, 257, 1412, 14501, 19631, 11, 420, 291, 528, 281, 6786, 309, 294, 264, 3877, 295, 264, 1594, 51188], "temperature": 0.0, "avg_logprob": -0.09934232018210672, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.006070288829505444}, {"id": 1752, "seek": 1015248, "start": 10168.96, "end": 10174.96, "text": " that you discover? So there is no single answer. So you cannot say that one is correct and another", "tokens": [51188, 300, 291, 4411, 30, 407, 456, 307, 572, 2167, 1867, 13, 407, 291, 2644, 584, 300, 472, 307, 3006, 293, 1071, 51488], "temperature": 0.0, "avg_logprob": -0.09934232018210672, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.006070288829505444}, {"id": 1753, "seek": 1015248, "start": 10174.96, "end": 10180.24, "text": " one is wrong. Yeah, I think this was kind of where I was going with the question of how", "tokens": [51488, 472, 307, 2085, 13, 865, 11, 286, 519, 341, 390, 733, 295, 689, 286, 390, 516, 365, 264, 1168, 295, 577, 51752], "temperature": 0.0, "avg_logprob": -0.09934232018210672, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.006070288829505444}, {"id": 1754, "seek": 1018024, "start": 10180.24, "end": 10186.08, "text": " principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that", "tokens": [50364, 3681, 15551, 366, 264, 14232, 302, 2244, 30, 400, 264, 14232, 302, 2244, 1643, 281, 312, 35250, 804, 445, 294, 264, 912, 636, 300, 50656], "temperature": 0.0, "avg_logprob": -0.08704822510480881, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.004987930413335562}, {"id": 1755, "seek": 1018024, "start": 10186.08, "end": 10192.32, "text": " geometries are hierarchical. You were saying that, for example, the projective geometry is kind of", "tokens": [50656, 12956, 2244, 366, 35250, 804, 13, 509, 645, 1566, 300, 11, 337, 1365, 11, 264, 1716, 488, 18426, 307, 733, 295, 50968], "temperature": 0.0, "avg_logprob": -0.08704822510480881, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.004987930413335562}, {"id": 1756, "seek": 1018024, "start": 10192.32, "end": 10197.36, "text": " subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large", "tokens": [50968, 2090, 10018, 462, 1311, 31264, 282, 18426, 11, 457, 286, 632, 257, 707, 1194, 5120, 13, 407, 3811, 286, 2729, 291, 257, 2416, 51220], "temperature": 0.0, "avg_logprob": -0.08704822510480881, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.004987930413335562}, {"id": 1757, "seek": 1018024, "start": 10197.36, "end": 10203.44, "text": " data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers,", "tokens": [51220, 1412, 992, 7126, 538, 257, 20560, 488, 17948, 304, 5102, 13, 823, 3687, 307, 1577, 295, 17948, 1124, 11, 5852, 11, 18361, 11, 51524], "temperature": 0.0, "avg_logprob": -0.08704822510480881, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.004987930413335562}, {"id": 1758, "seek": 1018024, "start": 10203.44, "end": 10209.76, "text": " coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the", "tokens": [51524, 8684, 11045, 11, 10233, 11, 12193, 11, 22535, 21288, 82, 11, 293, 754, 48026, 13, 407, 718, 311, 584, 286, 994, 380, 980, 291, 264, 51840], "temperature": 0.0, "avg_logprob": -0.08704822510480881, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.004987930413335562}, {"id": 1759, "seek": 1020976, "start": 10209.76, "end": 10214.48, "text": " simple rule which produced this pattern. Now what kind of regularities would you look for in the", "tokens": [50364, 2199, 4978, 597, 7126, 341, 5102, 13, 823, 437, 733, 295, 3890, 1088, 576, 291, 574, 337, 294, 264, 50600], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1760, "seek": 1020976, "start": 10214.48, "end": 10219.12, "text": " model that you built? I mean, it seems obvious that there would be an expanding scale symmetry,", "tokens": [50600, 2316, 300, 291, 3094, 30, 286, 914, 11, 309, 2544, 6322, 300, 456, 576, 312, 364, 14702, 4373, 25440, 11, 50832], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1761, "seek": 1020976, "start": 10219.12, "end": 10224.32, "text": " which might resemble the original rule. But it feels like there'd be plenty of other emergent,", "tokens": [50832, 597, 1062, 36870, 264, 3380, 4978, 13, 583, 309, 3417, 411, 456, 1116, 312, 7140, 295, 661, 4345, 6930, 11, 51092], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1762, "seek": 1020976, "start": 10224.32, "end": 10228.64, "text": " abstract symmetries which are not obviously related to the simple rule which produced the pattern.", "tokens": [51092, 12649, 14232, 302, 2244, 597, 366, 406, 2745, 4077, 281, 264, 2199, 4978, 597, 7126, 264, 5102, 13, 51308], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1763, "seek": 1020976, "start": 10228.64, "end": 10233.92, "text": " I mean, Janik was just saying, when you look at computer vision, you see a kind of regularity", "tokens": [51308, 286, 914, 11, 4956, 1035, 390, 445, 1566, 11, 562, 291, 574, 412, 3820, 5201, 11, 291, 536, 257, 733, 295, 3890, 507, 51572], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1764, "seek": 1020976, "start": 10233.92, "end": 10238.960000000001, "text": " or invariance to color shifts, for example. So our fractals are good analogy for physical", "tokens": [51572, 420, 33270, 719, 281, 2017, 19201, 11, 337, 1365, 13, 407, 527, 17948, 1124, 366, 665, 21663, 337, 4001, 51824], "temperature": 0.0, "avg_logprob": -0.09086467628192185, "compression_ratio": 1.78125, "no_speech_prob": 0.0011505753500387073}, {"id": 1765, "seek": 1023896, "start": 10238.96, "end": 10243.679999999998, "text": " reality. And should we be looking for the low-level primitive regularities which I think you're", "tokens": [50364, 4103, 13, 400, 820, 321, 312, 1237, 337, 264, 2295, 12, 12418, 28540, 3890, 1088, 597, 286, 519, 291, 434, 50600], "temperature": 0.0, "avg_logprob": -0.09898821512858073, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.002205394208431244}, {"id": 1766, "seek": 1023896, "start": 10243.679999999998, "end": 10248.08, "text": " advocating for? Or should we be looking at more abstract emergent symmetries which appear?", "tokens": [50600, 32050, 337, 30, 1610, 820, 321, 312, 1237, 412, 544, 12649, 4345, 6930, 14232, 302, 2244, 597, 4204, 30, 50820], "temperature": 0.0, "avg_logprob": -0.09898821512858073, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.002205394208431244}, {"id": 1767, "seek": 1023896, "start": 10249.039999999999, "end": 10256.48, "text": " In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claimed really", "tokens": [50868, 682, 264, 4289, 82, 11, 456, 390, 257, 4618, 3035, 538, 5116, 4156, 3474, 320, 322, 17948, 304, 17720, 13, 400, 436, 12941, 534, 51240], "temperature": 0.0, "avg_logprob": -0.09898821512858073, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.002205394208431244}, {"id": 1768, "seek": 1023896, "start": 10256.48, "end": 10262.24, "text": " unbelievable compression ratios for natural images. And the way it worked was to try to", "tokens": [51240, 16605, 19355, 32435, 337, 3303, 5267, 13, 400, 264, 636, 309, 2732, 390, 281, 853, 281, 51528], "temperature": 0.0, "avg_logprob": -0.09898821512858073, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.002205394208431244}, {"id": 1769, "seek": 1023896, "start": 10262.24, "end": 10267.359999999999, "text": " reassemble the image from parts of itself. And possibly, of course, you can take parts and", "tokens": [51528, 319, 37319, 264, 3256, 490, 3166, 295, 2564, 13, 400, 6264, 11, 295, 1164, 11, 291, 393, 747, 3166, 293, 51784], "temperature": 0.0, "avg_logprob": -0.09898821512858073, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.002205394208431244}, {"id": 1770, "seek": 1026736, "start": 10267.36, "end": 10271.76, "text": " subject them to some geometric transformation. So roughly, if you have a page of pixels,", "tokens": [50364, 3983, 552, 281, 512, 33246, 9887, 13, 407, 9810, 11, 498, 291, 362, 257, 3028, 295, 18668, 11, 50584], "temperature": 0.0, "avg_logprob": -0.12412899017333984, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.007513930089771748}, {"id": 1771, "seek": 1026736, "start": 10272.400000000001, "end": 10277.76, "text": " you can approximate it as another page taken from somewhere else in the image that you translate,", "tokens": [50616, 291, 393, 30874, 309, 382, 1071, 3028, 2726, 490, 4079, 1646, 294, 264, 3256, 300, 291, 13799, 11, 50884], "temperature": 0.0, "avg_logprob": -0.12412899017333984, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.007513930089771748}, {"id": 1772, "seek": 1026736, "start": 10278.720000000001, "end": 10284.16, "text": " rotate, and scale. And then the image was represented as an operator that makes such a", "tokens": [50932, 13121, 11, 293, 4373, 13, 400, 550, 264, 3256, 390, 10379, 382, 364, 12973, 300, 1669, 1270, 257, 51204], "temperature": 0.0, "avg_logprob": -0.12412899017333984, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.007513930089771748}, {"id": 1773, "seek": 1026736, "start": 10284.16, "end": 10289.28, "text": " decomposition. And this operator was constructed in a special way to be", "tokens": [51204, 48356, 13, 400, 341, 12973, 390, 17083, 294, 257, 2121, 636, 281, 312, 51460], "temperature": 0.0, "avg_logprob": -0.12412899017333984, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.007513930089771748}, {"id": 1774, "seek": 1026736, "start": 10289.28, "end": 10295.2, "text": " contractive. And then they used the Banach-Fix point theorem that you can apply this operator", "tokens": [51460, 4364, 488, 13, 400, 550, 436, 1143, 264, 13850, 608, 12, 37, 970, 935, 20904, 300, 291, 393, 3079, 341, 12973, 51756], "temperature": 0.0, "avg_logprob": -0.12412899017333984, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.007513930089771748}, {"id": 1775, "seek": 1029520, "start": 10295.2, "end": 10300.16, "text": " to any image. So you can start with the noise for example, completely random image. And you have", "tokens": [50364, 281, 604, 3256, 13, 407, 291, 393, 722, 365, 264, 5658, 337, 1365, 11, 2584, 4974, 3256, 13, 400, 291, 362, 50612], "temperature": 0.0, "avg_logprob": -0.13977422800150002, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.006348226685076952}, {"id": 1776, "seek": 1029520, "start": 10300.16, "end": 10306.960000000001, "text": " the target image emerge after a few iterations. So that this iterative scheme will converge to the", "tokens": [50612, 264, 3779, 3256, 21511, 934, 257, 1326, 36540, 13, 407, 300, 341, 17138, 1166, 12232, 486, 41881, 281, 264, 50952], "temperature": 0.0, "avg_logprob": -0.13977422800150002, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.006348226685076952}, {"id": 1777, "seek": 1029520, "start": 10306.960000000001, "end": 10312.560000000001, "text": " fixed point of the operator, which is the image itself. And it was actually used in the industry,", "tokens": [50952, 6806, 935, 295, 264, 12973, 11, 597, 307, 264, 3256, 2564, 13, 400, 309, 390, 767, 1143, 294, 264, 3518, 11, 51232], "temperature": 0.0, "avg_logprob": -0.13977422800150002, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.006348226685076952}, {"id": 1778, "seek": 1029520, "start": 10312.560000000001, "end": 10319.44, "text": " well, Microsoft and CARTA encyclopedia. I don't know how many viewers are old enough to remember it.", "tokens": [51232, 731, 11, 8116, 293, 15939, 8241, 465, 34080, 47795, 13, 286, 500, 380, 458, 577, 867, 8499, 366, 1331, 1547, 281, 1604, 309, 13, 51576], "temperature": 0.0, "avg_logprob": -0.13977422800150002, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.006348226685076952}, {"id": 1779, "seek": 1029520, "start": 10320.480000000001, "end": 10324.960000000001, "text": " But the main issue was the difficulty to build such operations. The compression", "tokens": [51628, 583, 264, 2135, 2734, 390, 264, 10360, 281, 1322, 1270, 7705, 13, 440, 19355, 51852], "temperature": 0.0, "avg_logprob": -0.13977422800150002, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.006348226685076952}, {"id": 1780, "seek": 1032496, "start": 10324.96, "end": 10331.359999999999, "text": " was very asymmetric. It was very easy to decode. You just take any image and apply this operator", "tokens": [50364, 390, 588, 37277, 17475, 13, 467, 390, 588, 1858, 281, 979, 1429, 13, 509, 445, 747, 604, 3256, 293, 3079, 341, 12973, 50684], "temperature": 0.0, "avg_logprob": -0.11678289293168902, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.02296454645693302}, {"id": 1781, "seek": 1032496, "start": 10331.359999999999, "end": 10336.96, "text": " multiple times. But it was really very hard to encode. And in fact, some of these constructions", "tokens": [50684, 3866, 1413, 13, 583, 309, 390, 534, 588, 1152, 281, 2058, 1429, 13, 400, 294, 1186, 11, 512, 295, 613, 7690, 626, 50964], "temperature": 0.0, "avg_logprob": -0.11678289293168902, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.02296454645693302}, {"id": 1782, "seek": 1032496, "start": 10336.96, "end": 10343.119999999999, "text": " that showed remarkable compression ratios were constructed semi by hand. I should say that in", "tokens": [50964, 300, 4712, 12802, 19355, 32435, 645, 17083, 12909, 538, 1011, 13, 286, 820, 584, 300, 294, 51272], "temperature": 0.0, "avg_logprob": -0.11678289293168902, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.02296454645693302}, {"id": 1783, "seek": 1032496, "start": 10343.119999999999, "end": 10347.199999999999, "text": " more recent times in computer vision, for example, the group of Michali Rani from the Weizmann", "tokens": [51272, 544, 5162, 1413, 294, 3820, 5201, 11, 337, 1365, 11, 264, 1594, 295, 3392, 5103, 497, 3782, 490, 264, 492, 590, 14912, 51476], "temperature": 0.0, "avg_logprob": -0.11678289293168902, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.02296454645693302}, {"id": 1784, "seek": 1032496, "start": 10347.199999999999, "end": 10353.119999999999, "text": " Institute in Israel used similar ideas for super resolution and image denoising where you can", "tokens": [51476, 9446, 294, 5674, 1143, 2531, 3487, 337, 1687, 8669, 293, 3256, 1441, 78, 3436, 689, 291, 393, 51772], "temperature": 0.0, "avg_logprob": -0.11678289293168902, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.02296454645693302}, {"id": 1785, "seek": 1035312, "start": 10353.12, "end": 10360.240000000002, "text": " build the clean or higher resolution image from bits and pieces of the image itself. So it's a", "tokens": [50364, 1322, 264, 2541, 420, 2946, 8669, 3256, 490, 9239, 293, 3755, 295, 264, 3256, 2564, 13, 407, 309, 311, 257, 50720], "temperature": 0.0, "avg_logprob": -0.11367917615313862, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00112279097083956}, {"id": 1786, "seek": 1035312, "start": 10360.240000000002, "end": 10368.08, "text": " single image denoising or super resolution. But what you do is you try to use similarities across", "tokens": [50720, 2167, 3256, 1441, 78, 3436, 420, 1687, 8669, 13, 583, 437, 291, 360, 307, 291, 853, 281, 764, 24197, 2108, 51112], "temperature": 0.0, "avg_logprob": -0.11367917615313862, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00112279097083956}, {"id": 1787, "seek": 1035312, "start": 10368.08, "end": 10374.400000000001, "text": " different positions and scales. That's absolutely fascinating. I mean, I spend a lot of time thinking", "tokens": [51112, 819, 8432, 293, 17408, 13, 663, 311, 3122, 10343, 13, 286, 914, 11, 286, 3496, 257, 688, 295, 565, 1953, 51428], "temperature": 0.0, "avg_logprob": -0.11367917615313862, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00112279097083956}, {"id": 1788, "seek": 1035312, "start": 10374.400000000001, "end": 10379.2, "text": " about this because my intuition is that deep learning works quite well because of the strict", "tokens": [51428, 466, 341, 570, 452, 24002, 307, 300, 2452, 2539, 1985, 1596, 731, 570, 295, 264, 10910, 51668], "temperature": 0.0, "avg_logprob": -0.11367917615313862, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.00112279097083956}, {"id": 1789, "seek": 1037920, "start": 10379.2, "end": 10384.16, "text": " structural limitations of the data which is produced by our physical world, right? And", "tokens": [50364, 15067, 15705, 295, 264, 1412, 597, 307, 7126, 538, 527, 4001, 1002, 11, 558, 30, 400, 50612], "temperature": 0.0, "avg_logprob": -0.08503551483154297, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.006212478969246149}, {"id": 1790, "seek": 1037920, "start": 10384.16, "end": 10390.240000000002, "text": " would you say that physical reality is highly dimensional or not? If it's highly dimensional,", "tokens": [50612, 576, 291, 584, 300, 4001, 4103, 307, 5405, 18795, 420, 406, 30, 759, 309, 311, 5405, 18795, 11, 50916], "temperature": 0.0, "avg_logprob": -0.08503551483154297, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.006212478969246149}, {"id": 1791, "seek": 1037920, "start": 10390.240000000002, "end": 10394.720000000001, "text": " is it because it emerged from a simple set of rules or relations like we were just talking", "tokens": [50916, 307, 309, 570, 309, 20178, 490, 257, 2199, 992, 295, 4474, 420, 2299, 411, 321, 645, 445, 1417, 51140], "temperature": 0.0, "avg_logprob": -0.08503551483154297, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.006212478969246149}, {"id": 1792, "seek": 1037920, "start": 10394.720000000001, "end": 10399.84, "text": " about? Because I think what you're arguing for is that it could be collapsible in some sense.", "tokens": [51140, 466, 30, 1436, 286, 519, 437, 291, 434, 19697, 337, 307, 300, 309, 727, 312, 16567, 964, 294, 512, 2020, 13, 51396], "temperature": 0.0, "avg_logprob": -0.08503551483154297, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.006212478969246149}, {"id": 1793, "seek": 1037920, "start": 10400.800000000001, "end": 10406.400000000001, "text": " Probably the term dimension is a bit frivolously used here. But I would say that it's", "tokens": [51444, 9210, 264, 1433, 10139, 307, 257, 857, 431, 21356, 5098, 1143, 510, 13, 583, 286, 576, 584, 300, 309, 311, 51724], "temperature": 0.0, "avg_logprob": -0.08503551483154297, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.006212478969246149}, {"id": 1794, "seek": 1040640, "start": 10407.359999999999, "end": 10412.4, "text": " probably fair to say that at some scale, many physical systems can be described with a small", "tokens": [50412, 1391, 3143, 281, 584, 300, 412, 512, 4373, 11, 867, 4001, 3652, 393, 312, 7619, 365, 257, 1359, 50664], "temperature": 0.0, "avg_logprob": -0.1267357378934337, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0027295679319649935}, {"id": 1795, "seek": 1040640, "start": 10412.4, "end": 10418.8, "text": " number of degrees of freedom, parametres that capture the system. And as we are talking,", "tokens": [50664, 1230, 295, 5310, 295, 5645, 11, 6220, 302, 495, 300, 7983, 264, 1185, 13, 400, 382, 321, 366, 1417, 11, 50984], "temperature": 0.0, "avg_logprob": -0.1267357378934337, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0027295679319649935}, {"id": 1796, "seek": 1040640, "start": 10418.8, "end": 10423.76, "text": " I'm sitting in a room, I'm surrounded by probably a quadrillion of gas molecules in the air that", "tokens": [50984, 286, 478, 3798, 294, 257, 1808, 11, 286, 478, 13221, 538, 1391, 257, 10787, 81, 11836, 295, 4211, 13093, 294, 264, 1988, 300, 51232], "temperature": 0.0, "avg_logprob": -0.1267357378934337, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0027295679319649935}, {"id": 1797, "seek": 1040640, "start": 10423.76, "end": 10427.68, "text": " fly through the room and collide with each other and the walls of the room. So at the", "tokens": [51232, 3603, 807, 264, 1808, 293, 49093, 365, 1184, 661, 293, 264, 7920, 295, 264, 1808, 13, 407, 412, 264, 51428], "temperature": 0.0, "avg_logprob": -0.1267357378934337, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0027295679319649935}, {"id": 1798, "seek": 1040640, "start": 10428.8, "end": 10435.199999999999, "text": " microscopic level, the dimension is very high. So it's absolutely intractable if I were to model", "tokens": [51484, 47897, 1496, 11, 264, 10139, 307, 588, 1090, 13, 407, 309, 311, 3122, 560, 1897, 712, 498, 286, 645, 281, 2316, 51804], "temperature": 0.0, "avg_logprob": -0.1267357378934337, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0027295679319649935}, {"id": 1799, "seek": 1043520, "start": 10435.2, "end": 10440.960000000001, "text": " each molecule and how it collides, I will have a huge number of degrees of freedom. And yet if we", "tokens": [50364, 1184, 15582, 293, 577, 309, 1263, 1875, 11, 286, 486, 362, 257, 2603, 1230, 295, 5310, 295, 5645, 13, 400, 1939, 498, 321, 50652], "temperature": 0.0, "avg_logprob": -0.09783131735665458, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0038989221211522818}, {"id": 1800, "seek": 1043520, "start": 10440.960000000001, "end": 10446.800000000001, "text": " zoom out, we can model the system statistically, and that's exactly the main idea of thermodynamics", "tokens": [50652, 8863, 484, 11, 321, 393, 2316, 264, 1185, 36478, 11, 293, 300, 311, 2293, 264, 2135, 1558, 295, 8810, 35483, 50944], "temperature": 0.0, "avg_logprob": -0.09783131735665458, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0038989221211522818}, {"id": 1801, "seek": 1043520, "start": 10446.800000000001, "end": 10454.0, "text": " and statistical mechanics. And this macroscopic system is surprisingly simple. It can be described", "tokens": [50944, 293, 22820, 12939, 13, 400, 341, 7912, 38006, 299, 1185, 307, 17600, 2199, 13, 467, 393, 312, 7619, 51304], "temperature": 0.0, "avg_logprob": -0.09783131735665458, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0038989221211522818}, {"id": 1802, "seek": 1043520, "start": 10454.0, "end": 10459.92, "text": " by just a few parameters such as temperature. And the example of fractals that you brought up before", "tokens": [51304, 538, 445, 257, 1326, 9834, 1270, 382, 4292, 13, 400, 264, 1365, 295, 17948, 1124, 300, 291, 3038, 493, 949, 51600], "temperature": 0.0, "avg_logprob": -0.09783131735665458, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0038989221211522818}, {"id": 1803, "seek": 1045992, "start": 10460.8, "end": 10465.92, "text": " essentially show that you can create very complex patterns with very simple rules that", "tokens": [50408, 4476, 855, 300, 291, 393, 1884, 588, 3997, 8294, 365, 588, 2199, 4474, 300, 50664], "temperature": 0.0, "avg_logprob": -0.09174937293643043, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.009569174610078335}, {"id": 1804, "seek": 1045992, "start": 10465.92, "end": 10472.64, "text": " apply locally in a repeated way. This might be a question for you, Peter. The geometric blueprint", "tokens": [50664, 3079, 16143, 294, 257, 10477, 636, 13, 639, 1062, 312, 257, 1168, 337, 291, 11, 6508, 13, 440, 33246, 35868, 51000], "temperature": 0.0, "avg_logprob": -0.09174937293643043, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.009569174610078335}, {"id": 1805, "seek": 1045992, "start": 10472.64, "end": 10478.24, "text": " works brilliantly in the ideal world where we can compute all of the possible group actions. But", "tokens": [51000, 1985, 8695, 42580, 294, 264, 7157, 1002, 689, 321, 393, 14722, 439, 295, 264, 1944, 1594, 5909, 13, 583, 51280], "temperature": 0.0, "avg_logprob": -0.09174937293643043, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.009569174610078335}, {"id": 1806, "seek": 1045992, "start": 10478.88, "end": 10483.52, "text": " graph neural networks, for example, you know, the permutation group is factorial in size,", "tokens": [51312, 4295, 18161, 9590, 11, 337, 1365, 11, 291, 458, 11, 264, 4784, 11380, 1594, 307, 36916, 294, 2744, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09174937293643043, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.009569174610078335}, {"id": 1807, "seek": 1045992, "start": 10483.52, "end": 10488.8, "text": " which means we need to rely on heuristics like graph convolutions. So how much better would graph", "tokens": [51544, 597, 1355, 321, 643, 281, 10687, 322, 415, 374, 6006, 411, 4295, 3754, 15892, 13, 407, 577, 709, 1101, 576, 4295, 51808], "temperature": 0.0, "avg_logprob": -0.09174937293643043, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.009569174610078335}, {"id": 1808, "seek": 1048880, "start": 10488.88, "end": 10493.599999999999, "text": " neural networks be if we could compute all of the permutations? I mean, are you happy with these", "tokens": [50368, 18161, 9590, 312, 498, 321, 727, 14722, 439, 295, 264, 4784, 325, 763, 30, 286, 914, 11, 366, 291, 2055, 365, 613, 50604], "temperature": 0.0, "avg_logprob": -0.04747283458709717, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.007456012070178986}, {"id": 1809, "seek": 1048880, "start": 10493.599999999999, "end": 10501.359999999999, "text": " heuristics in general? So that is a very good question. And yes, so let's just start from stating", "tokens": [50604, 415, 374, 6006, 294, 2674, 30, 407, 300, 307, 257, 588, 665, 1168, 13, 400, 2086, 11, 370, 718, 311, 445, 722, 490, 26688, 50992], "temperature": 0.0, "avg_logprob": -0.04747283458709717, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.007456012070178986}, {"id": 1810, "seek": 1048880, "start": 10501.359999999999, "end": 10507.759999999998, "text": " the obvious. If you want to explicitly express every possible operation that properly commutes", "tokens": [50992, 264, 6322, 13, 759, 291, 528, 281, 20803, 5109, 633, 1944, 6916, 300, 6108, 800, 1819, 51312], "temperature": 0.0, "avg_logprob": -0.04747283458709717, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.007456012070178986}, {"id": 1811, "seek": 1048880, "start": 10507.759999999998, "end": 10513.84, "text": " with the graph structure, and in that sense is a graph convolution, you would not be able to", "tokens": [51312, 365, 264, 4295, 3877, 11, 293, 294, 300, 2020, 307, 257, 4295, 45216, 11, 291, 576, 406, 312, 1075, 281, 51616], "temperature": 0.0, "avg_logprob": -0.04747283458709717, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.007456012070178986}, {"id": 1812, "seek": 1051384, "start": 10513.84, "end": 10518.880000000001, "text": " represent that properly as a neural network operation because you have to store in principle", "tokens": [50364, 2906, 300, 6108, 382, 257, 18161, 3209, 6916, 570, 291, 362, 281, 3531, 294, 8665, 50616], "temperature": 0.0, "avg_logprob": -0.050083194059484144, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0037648191209882498}, {"id": 1813, "seek": 1051384, "start": 10518.880000000001, "end": 10524.32, "text": " a vector for every single element of the permutation group. So unless your graph is super", "tokens": [50616, 257, 8062, 337, 633, 2167, 4478, 295, 264, 4784, 11380, 1594, 13, 407, 5969, 428, 4295, 307, 1687, 50888], "temperature": 0.0, "avg_logprob": -0.050083194059484144, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0037648191209882498}, {"id": 1814, "seek": 1051384, "start": 10524.32, "end": 10532.08, "text": " tiny, that is just not going to work. So on one hand, this is a potentially annoying result.", "tokens": [50888, 5870, 11, 300, 307, 445, 406, 516, 281, 589, 13, 407, 322, 472, 1011, 11, 341, 307, 257, 7263, 11304, 1874, 13, 51276], "temperature": 0.0, "avg_logprob": -0.050083194059484144, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0037648191209882498}, {"id": 1815, "seek": 1051384, "start": 10532.08, "end": 10539.36, "text": " On another hand, it is also exciting because we know that even though we ended up like doing most", "tokens": [51276, 1282, 1071, 1011, 11, 309, 307, 611, 4670, 570, 321, 458, 300, 754, 1673, 321, 4590, 493, 411, 884, 881, 51640], "temperature": 0.0, "avg_logprob": -0.050083194059484144, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0037648191209882498}, {"id": 1816, "seek": 1053936, "start": 10539.36, "end": 10544.400000000001, "text": " of our graph neural network research in this very restricted regime of I'm going to define a", "tokens": [50364, 295, 527, 4295, 18161, 3209, 2132, 294, 341, 588, 20608, 13120, 295, 286, 478, 516, 281, 6964, 257, 50616], "temperature": 0.0, "avg_logprob": -0.07625159740447998, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.011679103597998619}, {"id": 1817, "seek": 1053936, "start": 10544.400000000001, "end": 10549.92, "text": " permutation invariant function over my immediate neighbors, and that will as a result translate", "tokens": [50616, 4784, 11380, 33270, 394, 2445, 670, 452, 11629, 12512, 11, 293, 300, 486, 382, 257, 1874, 13799, 50892], "temperature": 0.0, "avg_logprob": -0.07625159740447998, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.011679103597998619}, {"id": 1818, "seek": 1053936, "start": 10549.92, "end": 10555.04, "text": " into a permutation equivalent function over the whole graph. Even though most of our research has", "tokens": [50892, 666, 257, 4784, 11380, 10344, 2445, 670, 264, 1379, 4295, 13, 2754, 1673, 881, 295, 527, 2132, 575, 51148], "temperature": 0.0, "avg_logprob": -0.07625159740447998, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.011679103597998619}, {"id": 1819, "seek": 1053936, "start": 10555.04, "end": 10561.44, "text": " happened in that particular area, we know from this result that there actually exists a huge wealth", "tokens": [51148, 2011, 294, 300, 1729, 1859, 11, 321, 458, 490, 341, 1874, 300, 456, 767, 8198, 257, 2603, 7203, 51468], "temperature": 0.0, "avg_logprob": -0.07625159740447998, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.011679103597998619}, {"id": 1820, "seek": 1053936, "start": 10561.44, "end": 10569.2, "text": " of very interesting architectures beyond that. And I think one of the potentially like earliest", "tokens": [51468, 295, 588, 1880, 6331, 1303, 4399, 300, 13, 400, 286, 519, 472, 295, 264, 7263, 411, 20573, 51856], "temperature": 0.0, "avg_logprob": -0.07625159740447998, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.011679103597998619}, {"id": 1821, "seek": 1056920, "start": 10569.2, "end": 10573.84, "text": " examples that have demonstrated that there exists this wealth of space is actually one of", "tokens": [50364, 5110, 300, 362, 18772, 300, 456, 8198, 341, 7203, 295, 1901, 307, 767, 472, 295, 50596], "temperature": 0.0, "avg_logprob": -0.12000529373748393, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0038812074344605207}, {"id": 1822, "seek": 1056920, "start": 10574.720000000001, "end": 10581.28, "text": " Jean Bruno's earlier papers on the graph Fourier transform that, you know, analyzing from a pure", "tokens": [50640, 13854, 23046, 311, 3071, 10577, 322, 264, 4295, 36810, 4088, 300, 11, 291, 458, 11, 23663, 490, 257, 6075, 50968], "temperature": 0.0, "avg_logprob": -0.12000529373748393, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0038812074344605207}, {"id": 1823, "seek": 1056920, "start": 10581.28, "end": 10586.880000000001, "text": " signal processing angle, they have shown that you can represent basically every proper graph", "tokens": [50968, 6358, 9007, 5802, 11, 436, 362, 4898, 300, 291, 393, 2906, 1936, 633, 2296, 4295, 51248], "temperature": 0.0, "avg_logprob": -0.12000529373748393, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0038812074344605207}, {"id": 1824, "seek": 1056920, "start": 10586.880000000001, "end": 10594.16, "text": " convolution as just, you know, parameterizing its eigenvalues with respect to the eigenvectors of", "tokens": [51248, 45216, 382, 445, 11, 291, 458, 11, 13075, 3319, 1080, 10446, 46033, 365, 3104, 281, 264, 10446, 303, 5547, 295, 51612], "temperature": 0.0, "avg_logprob": -0.12000529373748393, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0038812074344605207}, {"id": 1825, "seek": 1059416, "start": 10594.16, "end": 10600.56, "text": " the graph Laplacian. So, but the big issue that kind of limits us from going further in this", "tokens": [50364, 264, 4295, 2369, 564, 326, 952, 13, 407, 11, 457, 264, 955, 2734, 300, 733, 295, 10406, 505, 490, 516, 3052, 294, 341, 50684], "temperature": 0.0, "avg_logprob": -0.06636236374636731, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0033762750681489706}, {"id": 1826, "seek": 1059416, "start": 10600.56, "end": 10606.0, "text": " direction right now is the issue that Michael highlighted of geometric stability. So basically,", "tokens": [50684, 3513, 558, 586, 307, 264, 2734, 300, 5116, 17173, 295, 33246, 11826, 13, 407, 1936, 11, 50956], "temperature": 0.0, "avg_logprob": -0.06636236374636731, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0033762750681489706}, {"id": 1827, "seek": 1059416, "start": 10606.0, "end": 10611.28, "text": " a lot of these additional graph neural networks that do something more interesting than one", "tokens": [50956, 257, 688, 295, 613, 4497, 4295, 18161, 9590, 300, 360, 746, 544, 1880, 813, 472, 51220], "temperature": 0.0, "avg_logprob": -0.06636236374636731, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0033762750681489706}, {"id": 1828, "seek": 1059416, "start": 10611.28, "end": 10618.32, "text": " hop spatial message passing pay the price in being very geometrically unstable. So the graph Fourier", "tokens": [51220, 3818, 23598, 3636, 8437, 1689, 264, 3218, 294, 885, 588, 12956, 81, 984, 23742, 13, 407, 264, 4295, 36810, 51572], "temperature": 0.0, "avg_logprob": -0.06636236374636731, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0033762750681489706}, {"id": 1829, "seek": 1061832, "start": 10618.32, "end": 10625.6, "text": " transform in its most generic form will have every single node in the graph be updated based", "tokens": [50364, 4088, 294, 1080, 881, 19577, 1254, 486, 362, 633, 2167, 9984, 294, 264, 4295, 312, 10588, 2361, 50728], "temperature": 0.0, "avg_logprob": -0.08571164267403739, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.04812238737940788}, {"id": 1830, "seek": 1061832, "start": 10625.6, "end": 10631.199999999999, "text": " on whatever is located in any other node in the graph, very conditional on the graph topology. So", "tokens": [50728, 322, 2035, 307, 6870, 294, 604, 661, 9984, 294, 264, 4295, 11, 588, 27708, 322, 264, 4295, 1192, 1793, 13, 407, 51008], "temperature": 0.0, "avg_logprob": -0.08571164267403739, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.04812238737940788}, {"id": 1831, "seek": 1061832, "start": 10631.199999999999, "end": 10636.8, "text": " if you imagine any kind of approximate symmetry, any kind of perturbation either in the node features", "tokens": [51008, 498, 291, 3811, 604, 733, 295, 30874, 25440, 11, 604, 733, 295, 40468, 399, 2139, 294, 264, 9984, 4122, 51288], "temperature": 0.0, "avg_logprob": -0.08571164267403739, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.04812238737940788}, {"id": 1832, "seek": 1061832, "start": 10636.8, "end": 10642.24, "text": " or the edge structure of the graph, this, you basically don't have any protection against", "tokens": [51288, 420, 264, 4691, 3877, 295, 264, 4295, 11, 341, 11, 291, 1936, 500, 380, 362, 604, 6334, 1970, 51560], "temperature": 0.0, "avg_logprob": -0.08571164267403739, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.04812238737940788}, {"id": 1833, "seek": 1061832, "start": 10642.24, "end": 10646.88, "text": " that like that error is going to immediately propagate everywhere. And as a result, you'll", "tokens": [51560, 300, 411, 300, 6713, 307, 516, 281, 4258, 48256, 5315, 13, 400, 382, 257, 1874, 11, 291, 603, 51792], "temperature": 0.0, "avg_logprob": -0.08571164267403739, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.04812238737940788}, {"id": 1834, "seek": 1064688, "start": 10646.88, "end": 10652.16, "text": " end up with a layer that theoretically works really well. But in practice is very unstable to", "tokens": [50364, 917, 493, 365, 257, 4583, 300, 29400, 1985, 534, 731, 13, 583, 294, 3124, 307, 588, 23742, 281, 50628], "temperature": 0.0, "avg_logprob": -0.05977056262729404, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.005300181917846203}, {"id": 1835, "seek": 1064688, "start": 10652.16, "end": 10657.279999999999, "text": " these kinds of numerical or inaccuracy issues. One thing that's also very important to note is that", "tokens": [50628, 613, 3685, 295, 29054, 420, 37957, 374, 2551, 2663, 13, 1485, 551, 300, 311, 611, 588, 1021, 281, 3637, 307, 300, 50884], "temperature": 0.0, "avg_logprob": -0.05977056262729404, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.005300181917846203}, {"id": 1836, "seek": 1064688, "start": 10657.279999999999, "end": 10661.679999999998, "text": " often in graph neural networks, we have this subtle assumption of we have the graph and we're", "tokens": [50884, 2049, 294, 4295, 18161, 9590, 11, 321, 362, 341, 13743, 15302, 295, 321, 362, 264, 4295, 293, 321, 434, 51104], "temperature": 0.0, "avg_logprob": -0.05977056262729404, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.005300181917846203}, {"id": 1837, "seek": 1064688, "start": 10661.679999999998, "end": 10665.439999999999, "text": " using this graph that's given to us. But who guarantees that the graph that's given to you is", "tokens": [51104, 1228, 341, 4295, 300, 311, 2212, 281, 505, 13, 583, 567, 32567, 300, 264, 4295, 300, 311, 2212, 281, 291, 307, 51292], "temperature": 0.0, "avg_logprob": -0.05977056262729404, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.005300181917846203}, {"id": 1838, "seek": 1064688, "start": 10665.439999999999, "end": 10671.599999999999, "text": " the correct one actually very often, we estimate these graphs based on very, very weird heuristics", "tokens": [51292, 264, 3006, 472, 767, 588, 2049, 11, 321, 12539, 613, 24877, 2361, 322, 588, 11, 588, 3657, 415, 374, 6006, 51600], "temperature": 0.0, "avg_logprob": -0.05977056262729404, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.005300181917846203}, {"id": 1839, "seek": 1067160, "start": 10671.6, "end": 10677.6, "text": " ourselves. So basically, all of these kinds of perfection assumptions are what might limit the", "tokens": [50364, 4175, 13, 407, 1936, 11, 439, 295, 613, 3685, 295, 19708, 17695, 366, 437, 1062, 4948, 264, 50664], "temperature": 0.0, "avg_logprob": -0.052574504926366714, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.015185581520199776}, {"id": 1840, "seek": 1067160, "start": 10677.6, "end": 10682.32, "text": " applicability of these kinds of layers. But that being said, I find it comforting that these layers", "tokens": [50664, 2580, 2310, 295, 613, 3685, 295, 7914, 13, 583, 300, 885, 848, 11, 286, 915, 309, 38439, 300, 613, 7914, 50900], "temperature": 0.0, "avg_logprob": -0.052574504926366714, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.015185581520199776}, {"id": 1841, "seek": 1067160, "start": 10682.32, "end": 10687.36, "text": " exist, which means that there are meaningful ways to push our research forward to potentially", "tokens": [50900, 2514, 11, 597, 1355, 300, 456, 366, 10995, 2098, 281, 2944, 527, 2132, 2128, 281, 7263, 51152], "temperature": 0.0, "avg_logprob": -0.052574504926366714, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.015185581520199776}, {"id": 1842, "seek": 1067160, "start": 10687.36, "end": 10693.68, "text": " discover new, you know, wonderful basins of geometric stability inside these different,", "tokens": [51152, 4411, 777, 11, 291, 458, 11, 3715, 987, 1292, 295, 33246, 11826, 1854, 613, 819, 11, 51468], "temperature": 0.0, "avg_logprob": -0.052574504926366714, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.015185581520199776}, {"id": 1843, "seek": 1067160, "start": 10693.68, "end": 10698.16, "text": " you know, layers that may not just do one hop message passing. So that's my take on this,", "tokens": [51468, 291, 458, 11, 7914, 300, 815, 406, 445, 360, 472, 3818, 3636, 8437, 13, 407, 300, 311, 452, 747, 322, 341, 11, 51692], "temperature": 0.0, "avg_logprob": -0.052574504926366714, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.015185581520199776}, {"id": 1844, "seek": 1069816, "start": 10698.16, "end": 10702.4, "text": " like it gives me, it gives me faith that there's more interesting things to be discovered.", "tokens": [50364, 411, 309, 2709, 385, 11, 309, 2709, 385, 4522, 300, 456, 311, 544, 1880, 721, 281, 312, 6941, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11268159578431328, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.003992509562522173}, {"id": 1845, "seek": 1069816, "start": 10702.96, "end": 10707.76, "text": " That being said, it is pretty tricky to find stable layers in that vast landscape.", "tokens": [50604, 663, 885, 848, 11, 309, 307, 1238, 12414, 281, 915, 8351, 7914, 294, 300, 8369, 9661, 13, 50844], "temperature": 0.0, "avg_logprob": -0.11268159578431328, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.003992509562522173}, {"id": 1846, "seek": 1069816, "start": 10709.039999999999, "end": 10712.72, "text": " Michael, do you have some thoughts on this as well? I know you've worked quite a bit on these", "tokens": [50908, 5116, 11, 360, 291, 362, 512, 4598, 322, 341, 382, 731, 30, 286, 458, 291, 600, 2732, 1596, 257, 857, 322, 613, 51092], "temperature": 0.0, "avg_logprob": -0.11268159578431328, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.003992509562522173}, {"id": 1847, "seek": 1069816, "start": 10712.72, "end": 10720.4, "text": " geometric stability aspects. I just wanted to add one thought about it that essentially,", "tokens": [51092, 33246, 11826, 7270, 13, 286, 445, 1415, 281, 909, 472, 1194, 466, 309, 300, 4476, 11, 51476], "temperature": 0.0, "avg_logprob": -0.11268159578431328, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.003992509562522173}, {"id": 1848, "seek": 1069816, "start": 10720.4, "end": 10726.24, "text": " locality is a feature, not a bug in many situations. And in convolutional neural networks,", "tokens": [51476, 1628, 1860, 307, 257, 4111, 11, 406, 257, 7426, 294, 867, 6851, 13, 400, 294, 45216, 304, 18161, 9590, 11, 51768], "temperature": 0.0, "avg_logprob": -0.11268159578431328, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.003992509562522173}, {"id": 1849, "seek": 1072624, "start": 10726.24, "end": 10731.6, "text": " actually, if you look again, historically, the early architectures like AlexNet, they started", "tokens": [50364, 767, 11, 498, 291, 574, 797, 11, 16180, 11, 264, 2440, 6331, 1303, 411, 5202, 31890, 11, 436, 1409, 50632], "temperature": 0.0, "avg_logprob": -0.11243499574207125, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00729396753013134}, {"id": 1850, "seek": 1072624, "start": 10731.6, "end": 10735.68, "text": " with very large filters and the few layers or relatively few layers, I think something like", "tokens": [50632, 365, 588, 2416, 15995, 293, 264, 1326, 7914, 420, 7226, 1326, 7914, 11, 286, 519, 746, 411, 50836], "temperature": 0.0, "avg_logprob": -0.11243499574207125, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00729396753013134}, {"id": 1851, "seek": 1072624, "start": 10735.68, "end": 10741.68, "text": " five or six. And nowadays, what you see is very small filters and hundreds of layers.", "tokens": [50836, 1732, 420, 2309, 13, 400, 13434, 11, 437, 291, 536, 307, 588, 1359, 15995, 293, 6779, 295, 7914, 13, 51136], "temperature": 0.0, "avg_logprob": -0.11243499574207125, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00729396753013134}, {"id": 1852, "seek": 1072624, "start": 10741.68, "end": 10746.4, "text": " One of the reasons why you can do it is because of compositionality properties. So you can,", "tokens": [51136, 1485, 295, 264, 4112, 983, 291, 393, 360, 309, 307, 570, 295, 12686, 1860, 7221, 13, 407, 291, 393, 11, 51372], "temperature": 0.0, "avg_logprob": -0.11243499574207125, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00729396753013134}, {"id": 1853, "seek": 1072624, "start": 10746.4, "end": 10752.96, "text": " you can create complex features from, from simple primitives. So in some other cases,", "tokens": [51372, 291, 393, 1884, 3997, 4122, 490, 11, 490, 2199, 2886, 38970, 13, 407, 294, 512, 661, 3331, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11243499574207125, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00729396753013134}, {"id": 1854, "seek": 1075296, "start": 10752.96, "end": 10759.919999999998, "text": " like, like manifolds, there are deeper geometric considerations why you must be local, so related", "tokens": [50364, 411, 11, 411, 8173, 31518, 11, 456, 366, 7731, 33246, 24070, 983, 291, 1633, 312, 2654, 11, 370, 4077, 50712], "temperature": 0.0, "avg_logprob": -0.12291975130980042, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00472599919885397}, {"id": 1855, "seek": 1075296, "start": 10759.919999999998, "end": 10766.32, "text": " to what is called the injectivity radius of the manifold. On graphs, well, maybe we like a little", "tokens": [50712, 281, 437, 307, 1219, 264, 10711, 4253, 15845, 295, 264, 47138, 13, 1282, 24877, 11, 731, 11, 1310, 321, 411, 257, 707, 51032], "temperature": 0.0, "avg_logprob": -0.12291975130980042, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00472599919885397}, {"id": 1856, "seek": 1075296, "start": 10766.32, "end": 10772.88, "text": " bit the theoretical necessity to be local, besides, of course, the computational complexity. But", "tokens": [51032, 857, 264, 20864, 24217, 281, 312, 2654, 11, 11868, 11, 295, 1164, 11, 264, 28270, 14024, 13, 583, 51360], "temperature": 0.0, "avg_logprob": -0.12291975130980042, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00472599919885397}, {"id": 1857, "seek": 1075296, "start": 10773.519999999999, "end": 10778.96, "text": " in many cases, it is actually a good property because many problems do not really depend on", "tokens": [51392, 294, 867, 3331, 11, 309, 307, 767, 257, 665, 4707, 570, 867, 2740, 360, 406, 534, 5672, 322, 51664], "temperature": 0.0, "avg_logprob": -0.12291975130980042, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.00472599919885397}, {"id": 1858, "seek": 1077896, "start": 10778.96, "end": 10784.8, "text": " distant interactions. So if you think of social networks, probably most of the information comes", "tokens": [50364, 17275, 13280, 13, 407, 498, 291, 519, 295, 2093, 9590, 11, 1391, 881, 295, 264, 1589, 1487, 50656], "temperature": 0.0, "avg_logprob": -0.11303563117980957, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.008284672163426876}, {"id": 1859, "seek": 1077896, "start": 10784.8, "end": 10790.88, "text": " from your immediate neighbors. Is there some sort of, let's assume I, you know, I have a graph,", "tokens": [50656, 490, 428, 11629, 12512, 13, 1119, 456, 512, 1333, 295, 11, 718, 311, 6552, 286, 11, 291, 458, 11, 286, 362, 257, 4295, 11, 50960], "temperature": 0.0, "avg_logprob": -0.11303563117980957, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.008284672163426876}, {"id": 1860, "seek": 1077896, "start": 10791.679999999998, "end": 10800.64, "text": " and I have my, my symmetries, my groups that I suspect there are in the problem, or I want to", "tokens": [51000, 293, 286, 362, 452, 11, 452, 14232, 302, 2244, 11, 452, 3935, 300, 286, 9091, 456, 366, 294, 264, 1154, 11, 420, 286, 528, 281, 51448], "temperature": 0.0, "avg_logprob": -0.11303563117980957, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.008284672163426876}, {"id": 1861, "seek": 1077896, "start": 10800.64, "end": 10807.679999999998, "text": " be invariant to, is there like, can you give us a bit of a practical blueprint of how would I build", "tokens": [51448, 312, 33270, 394, 281, 11, 307, 456, 411, 11, 393, 291, 976, 505, 257, 857, 295, 257, 8496, 35868, 295, 577, 576, 286, 1322, 51800], "temperature": 0.0, "avg_logprob": -0.11303563117980957, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.008284672163426876}, {"id": 1862, "seek": 1080768, "start": 10807.76, "end": 10815.44, "text": " a network that, you know, takes this as an input and applies this? How would you go about this,", "tokens": [50368, 257, 3209, 300, 11, 291, 458, 11, 2516, 341, 382, 364, 4846, 293, 13165, 341, 30, 1012, 576, 291, 352, 466, 341, 11, 50752], "temperature": 0.0, "avg_logprob": -0.13029646240504442, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.002351026516407728}, {"id": 1863, "seek": 1080768, "start": 10815.44, "end": 10820.56, "text": " you know, what would be the building blocks that you choose, the orders and so on? Is there", "tokens": [50752, 291, 458, 11, 437, 576, 312, 264, 2390, 8474, 300, 291, 2826, 11, 264, 9470, 293, 370, 322, 30, 1119, 456, 51008], "temperature": 0.0, "avg_logprob": -0.13029646240504442, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.002351026516407728}, {"id": 1864, "seek": 1080768, "start": 10820.56, "end": 10825.76, "text": " overarching principles in, in how to do? I don't think that there is really a general", "tokens": [51008, 45501, 9156, 294, 11, 294, 577, 281, 360, 30, 286, 500, 380, 519, 300, 456, 307, 534, 257, 2674, 51268], "temperature": 0.0, "avg_logprob": -0.13029646240504442, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.002351026516407728}, {"id": 1865, "seek": 1080768, "start": 10825.76, "end": 10831.36, "text": " recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic", "tokens": [51268, 6782, 13, 407, 309, 311, 1154, 12334, 13, 583, 1310, 472, 1365, 307, 5821, 294, 12558, 13, 440, 3875, 51548], "temperature": 0.0, "avg_logprob": -0.13029646240504442, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.002351026516407728}, {"id": 1866, "seek": 1080768, "start": 10831.36, "end": 10835.92, "text": " structure that they have in graph is a privatization invariance. This has to do with the", "tokens": [51548, 3877, 300, 436, 362, 294, 4295, 307, 257, 31856, 2144, 33270, 719, 13, 639, 575, 281, 360, 365, 264, 51776], "temperature": 0.0, "avg_logprob": -0.13029646240504442, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.002351026516407728}, {"id": 1867, "seek": 1083592, "start": 10835.92, "end": 10840.24, "text": " structure of the graph itself. It says nothing about the structure of the features.", "tokens": [50364, 3877, 295, 264, 4295, 2564, 13, 467, 1619, 1825, 466, 264, 3877, 295, 264, 4122, 13, 50580], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1868, "seek": 1083592, "start": 10840.24, "end": 10845.76, "text": " You might also have some secondary symmetry structure in the feature space. In case of molecules,", "tokens": [50580, 509, 1062, 611, 362, 512, 11396, 25440, 3877, 294, 264, 4111, 1901, 13, 682, 1389, 295, 13093, 11, 50856], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1869, "seek": 1083592, "start": 10845.76, "end": 10850.8, "text": " for example, you might have a combination of features. Some of them are geometric. So it's", "tokens": [50856, 337, 1365, 11, 291, 1062, 362, 257, 6562, 295, 4122, 13, 2188, 295, 552, 366, 33246, 13, 407, 309, 311, 51108], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1870, "seek": 1083592, "start": 10850.8, "end": 10855.92, "text": " actually not an abstract topological graph. It's a geometric graph. A molecule is a graph that", "tokens": [51108, 767, 406, 364, 12649, 1192, 4383, 4295, 13, 467, 311, 257, 33246, 4295, 13, 316, 15582, 307, 257, 4295, 300, 51364], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1871, "seek": 1083592, "start": 10855.92, "end": 10861.2, "text": " lives in three dimensional space. And so the features are the positional coordinates of the", "tokens": [51364, 2909, 294, 1045, 18795, 1901, 13, 400, 370, 264, 4122, 366, 264, 2535, 304, 21056, 295, 264, 51628], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1872, "seek": 1083592, "start": 10861.2, "end": 10865.76, "text": " nodes, as well as some chemical properties such as atomic numbers. Now, when you deal with the", "tokens": [51628, 13891, 11, 382, 731, 382, 512, 7313, 7221, 1270, 382, 22275, 3547, 13, 823, 11, 562, 291, 2028, 365, 264, 51856], "temperature": 0.0, "avg_logprob": -0.10920877075195312, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.006077509839087725}, {"id": 1873, "seek": 1086576, "start": 10865.76, "end": 10870.16, "text": " molecule, you usually don't care about how it is positioned in space. It wants to be", "tokens": [50364, 15582, 11, 291, 2673, 500, 380, 1127, 466, 577, 309, 307, 24889, 294, 1901, 13, 467, 2738, 281, 312, 50584], "temperature": 0.0, "avg_logprob": -0.11415590154062404, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0012954453704878688}, {"id": 1874, "seek": 1086576, "start": 10870.16, "end": 10876.16, "text": " equivariant to rigid transformations. And therefore you need to treat accordingly the", "tokens": [50584, 48726, 3504, 394, 281, 22195, 34852, 13, 400, 4412, 291, 643, 281, 2387, 19717, 264, 50884], "temperature": 0.0, "avg_logprob": -0.11415590154062404, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0012954453704878688}, {"id": 1875, "seek": 1086576, "start": 10876.16, "end": 10880.48, "text": " geometric coordinates of the nodes of this graph. And this is actually what has been successfully", "tokens": [50884, 33246, 21056, 295, 264, 13891, 295, 341, 4295, 13, 400, 341, 307, 767, 437, 575, 668, 10727, 51100], "temperature": 0.0, "avg_logprob": -0.11415590154062404, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0012954453704878688}, {"id": 1876, "seek": 1086576, "start": 10880.48, "end": 10886.0, "text": " done. So when you do, for example, virtual drag screening, architectures that do message passing,", "tokens": [51100, 1096, 13, 407, 562, 291, 360, 11, 337, 1365, 11, 6374, 5286, 17732, 11, 6331, 1303, 300, 360, 3636, 8437, 11, 51376], "temperature": 0.0, "avg_logprob": -0.11415590154062404, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0012954453704878688}, {"id": 1877, "seek": 1086576, "start": 10886.0, "end": 10891.04, "text": " but in a way that is equivariant to these rigid transformations actually are more successful", "tokens": [51376, 457, 294, 257, 636, 300, 307, 48726, 3504, 394, 281, 613, 22195, 34852, 767, 366, 544, 4406, 51628], "temperature": 0.0, "avg_logprob": -0.11415590154062404, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.0012954453704878688}, {"id": 1878, "seek": 1089104, "start": 10891.04, "end": 10896.320000000002, "text": " than generic graph neural networks. Also, this principle was exploited in the recent version", "tokens": [50364, 813, 19577, 4295, 18161, 9590, 13, 2743, 11, 341, 8665, 390, 40918, 294, 264, 5162, 3037, 50628], "temperature": 0.0, "avg_logprob": -0.1273978177238913, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0031650166492909193}, {"id": 1879, "seek": 1089104, "start": 10896.320000000002, "end": 10902.720000000001, "text": " of AlphaFold, where I think they call it point invariant attention, which is essentially a form", "tokens": [50628, 295, 20588, 37, 2641, 11, 689, 286, 519, 436, 818, 309, 935, 33270, 394, 3202, 11, 597, 307, 4476, 257, 1254, 50948], "temperature": 0.0, "avg_logprob": -0.1273978177238913, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0031650166492909193}, {"id": 1880, "seek": 1089104, "start": 10902.720000000001, "end": 10910.240000000002, "text": " of a latent graph neural network or a transformer architecture with equivariant message passing.", "tokens": [50948, 295, 257, 48994, 4295, 18161, 3209, 420, 257, 31782, 9482, 365, 48726, 3504, 394, 3636, 8437, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1273978177238913, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0031650166492909193}, {"id": 1881, "seek": 1089104, "start": 10910.240000000002, "end": 10915.2, "text": " Yeah, I'd just like to add one more point to this conversation, which is maybe a bit more", "tokens": [51324, 865, 11, 286, 1116, 445, 411, 281, 909, 472, 544, 935, 281, 341, 3761, 11, 597, 307, 1310, 257, 857, 544, 51572], "temperature": 0.0, "avg_logprob": -0.1273978177238913, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0031650166492909193}, {"id": 1882, "seek": 1091520, "start": 10915.2, "end": 10922.320000000002, "text": " philosophical, but it relates to this aspect of building the overarching symmetry discovering", "tokens": [50364, 25066, 11, 457, 309, 16155, 281, 341, 4171, 295, 2390, 264, 45501, 25440, 24773, 50720], "temperature": 0.0, "avg_logprob": -0.07705192680818489, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.201569065451622}, {"id": 1883, "seek": 1091520, "start": 10922.960000000001, "end": 10928.720000000001, "text": " procedures, which I think would be a really fantastic thing to have in general. And I hope that", "tokens": [50752, 13846, 11, 597, 286, 519, 576, 312, 257, 534, 5456, 551, 281, 362, 294, 2674, 13, 400, 286, 1454, 300, 51040], "temperature": 0.0, "avg_logprob": -0.07705192680818489, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.201569065451622}, {"id": 1884, "seek": 1091520, "start": 10929.44, "end": 10936.400000000001, "text": " some component of a true AGI is going to be figuring out, making sense of the data you're", "tokens": [51076, 512, 6542, 295, 257, 2074, 316, 26252, 307, 516, 281, 312, 15213, 484, 11, 1455, 2020, 295, 264, 1412, 291, 434, 51424], "temperature": 0.0, "avg_logprob": -0.07705192680818489, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.201569065451622}, {"id": 1885, "seek": 1091520, "start": 10936.400000000001, "end": 10942.880000000001, "text": " receiving and figuring out what's the right symmetry to bake into it. I don't necessarily", "tokens": [51424, 10040, 293, 15213, 484, 437, 311, 264, 558, 25440, 281, 16562, 666, 309, 13, 286, 500, 380, 4725, 51748], "temperature": 0.0, "avg_logprob": -0.07705192680818489, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.201569065451622}, {"id": 1886, "seek": 1094288, "start": 10942.88, "end": 10949.92, "text": " have a good opinion on what this model might look like. But what I do say is just looking at the", "tokens": [50364, 362, 257, 665, 4800, 322, 437, 341, 2316, 1062, 574, 411, 13, 583, 437, 286, 360, 584, 307, 445, 1237, 412, 264, 50716], "temperature": 0.0, "avg_logprob": -0.06884517439876695, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.021267293021082878}, {"id": 1887, "seek": 1094288, "start": 10949.92, "end": 10956.08, "text": " immediate utility of the geometric deep learning blueprint, we are like, I think very strictly", "tokens": [50716, 11629, 14877, 295, 264, 33246, 2452, 2539, 35868, 11, 321, 366, 411, 11, 286, 519, 588, 20792, 51024], "temperature": 0.0, "avg_logprob": -0.06884517439876695, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.021267293021082878}, {"id": 1888, "seek": 1094288, "start": 10956.08, "end": 10962.08, "text": " saying that we don't want to use this blueprint to propose, you know, the one true architecture.", "tokens": [51024, 1566, 300, 321, 500, 380, 528, 281, 764, 341, 35868, 281, 17421, 11, 291, 458, 11, 264, 472, 2074, 9482, 13, 51324], "temperature": 0.0, "avg_logprob": -0.06884517439876695, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.021267293021082878}, {"id": 1889, "seek": 1094288, "start": 10963.519999999999, "end": 10969.119999999999, "text": " Rather, we make the argument that different problems require different specifications,", "tokens": [51396, 16571, 11, 321, 652, 264, 6770, 300, 819, 2740, 3651, 819, 29448, 11, 51676], "temperature": 0.0, "avg_logprob": -0.06884517439876695, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.021267293021082878}, {"id": 1890, "seek": 1096912, "start": 10969.12, "end": 10974.560000000001, "text": " and we provide a common language that will allow, say, someone who works on primarily grid data to", "tokens": [50364, 293, 321, 2893, 257, 2689, 2856, 300, 486, 2089, 11, 584, 11, 1580, 567, 1985, 322, 10029, 10748, 1412, 281, 50636], "temperature": 0.0, "avg_logprob": -0.13054683973204415, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.006000628229230642}, {"id": 1891, "seek": 1096912, "start": 10974.560000000001, "end": 10979.04, "text": " speak with someone who works on manifold data without necessarily thinking that, you know,", "tokens": [50636, 1710, 365, 1580, 567, 1985, 322, 47138, 1412, 1553, 4725, 1953, 300, 11, 291, 458, 11, 50860], "temperature": 0.0, "avg_logprob": -0.13054683973204415, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.006000628229230642}, {"id": 1892, "seek": 1096912, "start": 10979.92, "end": 10984.480000000001, "text": " you know, somebody might say, commonets are the ultimate architecture. Someone else might say", "tokens": [50904, 291, 458, 11, 2618, 1062, 584, 11, 2689, 1385, 366, 264, 9705, 9482, 13, 8734, 1646, 1062, 584, 51132], "temperature": 0.0, "avg_logprob": -0.13054683973204415, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.006000628229230642}, {"id": 1893, "seek": 1096912, "start": 10984.480000000001, "end": 10988.880000000001, "text": " GNNs are the ultimate architecture. And in some ways, they could both be right and they could", "tokens": [51132, 46411, 45, 82, 366, 264, 9705, 9482, 13, 400, 294, 512, 2098, 11, 436, 727, 1293, 312, 558, 293, 436, 727, 51352], "temperature": 0.0, "avg_logprob": -0.13054683973204415, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.006000628229230642}, {"id": 1894, "seek": 1096912, "start": 10988.880000000001, "end": 10994.960000000001, "text": " both be wrong. But this blueprint kind of just provides a clear delimiting aspect to these", "tokens": [51352, 1293, 312, 2085, 13, 583, 341, 35868, 733, 295, 445, 6417, 257, 1850, 1103, 332, 1748, 4171, 281, 613, 51656], "temperature": 0.0, "avg_logprob": -0.13054683973204415, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.006000628229230642}, {"id": 1895, "seek": 1099496, "start": 10994.96, "end": 11000.88, "text": " things, just like in the 1800s, you had all these different types of geometries that basically lived", "tokens": [50364, 721, 11, 445, 411, 294, 264, 24327, 82, 11, 291, 632, 439, 613, 819, 3467, 295, 12956, 2244, 300, 1936, 5152, 50660], "temperature": 0.0, "avg_logprob": -0.08984803765769897, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.016392016783356667}, {"id": 1896, "seek": 1099496, "start": 11000.88, "end": 11007.519999999999, "text": " on completely different kinds of geometric objects, right? So hyperbolic, elliptic, and so on and so", "tokens": [50660, 322, 2584, 819, 3685, 295, 33246, 6565, 11, 558, 30, 407, 9848, 65, 7940, 11, 8284, 22439, 299, 11, 293, 370, 322, 293, 370, 50992], "temperature": 0.0, "avg_logprob": -0.08984803765769897, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.016392016783356667}, {"id": 1897, "seek": 1099496, "start": 11007.519999999999, "end": 11013.759999999998, "text": " forth. And what Klein-Zerlangen program allowed us to do was, among other things, reason about", "tokens": [50992, 5220, 13, 400, 437, 33327, 12, 57, 260, 75, 10784, 1461, 4350, 505, 281, 360, 390, 11, 3654, 661, 721, 11, 1778, 466, 51304], "temperature": 0.0, "avg_logprob": -0.08984803765769897, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.016392016783356667}, {"id": 1898, "seek": 1099496, "start": 11013.759999999998, "end": 11018.64, "text": " all of these geometries using the same language of group invariance and symmetries, right? But in", "tokens": [51304, 439, 295, 613, 12956, 2244, 1228, 264, 912, 2856, 295, 1594, 33270, 719, 293, 14232, 302, 2244, 11, 558, 30, 583, 294, 51548], "temperature": 0.0, "avg_logprob": -0.08984803765769897, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.016392016783356667}, {"id": 1899, "seek": 1099496, "start": 11018.64, "end": 11024.56, "text": " principle, the specifics of whether you want to use a hyperbolic geometry or whether you want to use", "tokens": [51548, 8665, 11, 264, 28454, 295, 1968, 291, 528, 281, 764, 257, 9848, 65, 7940, 18426, 420, 1968, 291, 528, 281, 764, 51844], "temperature": 0.0, "avg_logprob": -0.08984803765769897, "compression_ratio": 1.7553191489361701, "no_speech_prob": 0.016392016783356667}, {"id": 1900, "seek": 1102456, "start": 11024.56, "end": 11029.84, "text": " an elliptic geometry, partly rests on your assumption what the main do you actually live in,", "tokens": [50364, 364, 8284, 22439, 299, 18426, 11, 17031, 39755, 322, 428, 15302, 437, 264, 2135, 360, 291, 767, 1621, 294, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11872475722740436, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0038797850720584393}, {"id": 1901, "seek": 1102456, "start": 11029.84, "end": 11036.16, "text": " right? When you're doing these computations. So I think just generally speaking, I think that", "tokens": [50628, 558, 30, 1133, 291, 434, 884, 613, 2807, 763, 13, 407, 286, 519, 445, 5101, 4124, 11, 286, 519, 300, 50944], "temperature": 0.0, "avg_logprob": -0.11872475722740436, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0038797850720584393}, {"id": 1902, "seek": 1102456, "start": 11036.16, "end": 11041.279999999999, "text": " having a divide is a potentially useful thing, as long as you have a language that you can use", "tokens": [50944, 1419, 257, 9845, 307, 257, 7263, 4420, 551, 11, 382, 938, 382, 291, 362, 257, 2856, 300, 291, 393, 764, 51200], "temperature": 0.0, "avg_logprob": -0.11872475722740436, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0038797850720584393}, {"id": 1903, "seek": 1102456, "start": 11041.279999999999, "end": 11048.24, "text": " to index into this divide, if that makes sense. It does make sense. But I have a feeling that some", "tokens": [51200, 281, 8186, 666, 341, 9845, 11, 498, 300, 1669, 2020, 13, 467, 775, 652, 2020, 13, 583, 286, 362, 257, 2633, 300, 512, 51548], "temperature": 0.0, "avg_logprob": -0.11872475722740436, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0038797850720584393}, {"id": 1904, "seek": 1102456, "start": 11048.24, "end": 11054.0, "text": " people could benefit from geometric deep learning in their runaways. I mean, I don't want you guys", "tokens": [51548, 561, 727, 5121, 490, 33246, 2452, 2539, 294, 641, 1190, 27545, 13, 286, 914, 11, 286, 500, 380, 528, 291, 1074, 51836], "temperature": 0.0, "avg_logprob": -0.11872475722740436, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0038797850720584393}, {"id": 1905, "seek": 1105400, "start": 11054.0, "end": 11058.8, "text": " to motivate geometric deep learning in general, because I think, you know, a lot of deep learning", "tokens": [50364, 281, 28497, 33246, 2452, 2539, 294, 2674, 11, 570, 286, 519, 11, 291, 458, 11, 257, 688, 295, 2452, 2539, 50604], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1906, "seek": 1105400, "start": 11058.8, "end": 11063.12, "text": " with a structured prior is already geometric deep learning is as you folks demonstrated in", "tokens": [50604, 365, 257, 18519, 4059, 307, 1217, 33246, 2452, 2539, 307, 382, 291, 4024, 18772, 294, 50820], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1907, "seek": 1105400, "start": 11063.12, "end": 11067.52, "text": " your blueprint, you know, like RNNs and CNNs, for example. So, you know, like it or not, we're already", "tokens": [50820, 428, 35868, 11, 291, 458, 11, 411, 45702, 45, 82, 293, 24859, 82, 11, 337, 1365, 13, 407, 11, 291, 458, 11, 411, 309, 420, 406, 11, 321, 434, 1217, 51040], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1908, "seek": 1105400, "start": 11067.52, "end": 11072.16, "text": " all using geometric deep learning. But some of the esoteric flavors of geometric deep learning,", "tokens": [51040, 439, 1228, 33246, 2452, 2539, 13, 583, 512, 295, 264, 785, 21585, 299, 16303, 295, 33246, 2452, 2539, 11, 51272], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1909, "seek": 1105400, "start": 11072.16, "end": 11076.72, "text": " particularly on irregular meshes, they seem a little bit out there, don't they? I mean, it's", "tokens": [51272, 4098, 322, 29349, 3813, 8076, 11, 436, 1643, 257, 707, 857, 484, 456, 11, 500, 380, 436, 30, 286, 914, 11, 309, 311, 51500], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1910, "seek": 1105400, "start": 11076.72, "end": 11080.64, "text": " possible that many people could benefit from this, but they just don't know about it yet. I was", "tokens": [51500, 1944, 300, 867, 561, 727, 5121, 490, 341, 11, 457, 436, 445, 500, 380, 458, 466, 309, 1939, 13, 286, 390, 51696], "temperature": 0.0, "avg_logprob": -0.0686322400267695, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.00578449247404933}, {"id": 1911, "seek": 1108064, "start": 11080.64, "end": 11085.76, "text": " thinking that, for example, if I had a LiDAR scanner on my phone, and the result is a point cloud,", "tokens": [50364, 1953, 300, 11, 337, 1365, 11, 498, 286, 632, 257, 8349, 35, 1899, 30211, 322, 452, 2593, 11, 293, 264, 1874, 307, 257, 935, 4588, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09107859929402669, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0011498908279463649}, {"id": 1912, "seek": 1108064, "start": 11085.76, "end": 11091.359999999999, "text": " which is not particularly useful. But I would presumably transform it into a mesh, which would", "tokens": [50620, 597, 307, 406, 4098, 4420, 13, 583, 286, 576, 26742, 4088, 309, 666, 257, 17407, 11, 597, 576, 50900], "temperature": 0.0, "avg_logprob": -0.09107859929402669, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0011498908279463649}, {"id": 1913, "seek": 1108064, "start": 11091.359999999999, "end": 11095.519999999999, "text": " be more useful. But is it possible that loads of data scientists out there are sitting on data sets", "tokens": [50900, 312, 544, 4420, 13, 583, 307, 309, 1944, 300, 12668, 295, 1412, 7708, 484, 456, 366, 3798, 322, 1412, 6352, 51108], "temperature": 0.0, "avg_logprob": -0.09107859929402669, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0011498908279463649}, {"id": 1914, "seek": 1108064, "start": 11095.519999999999, "end": 11099.92, "text": " that they could be thinking about geometrically, but they're not? Many folks are exotic. It's", "tokens": [51108, 300, 436, 727, 312, 1953, 466, 12956, 81, 984, 11, 457, 436, 434, 406, 30, 5126, 4024, 366, 27063, 13, 467, 311, 51328], "temperature": 0.0, "avg_logprob": -0.09107859929402669, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0011498908279463649}, {"id": 1915, "seek": 1108064, "start": 11099.92, "end": 11104.56, "text": " probably in the eyes of the beholder. And well, in machine learning, probably they are, to some", "tokens": [51328, 1391, 294, 264, 2575, 295, 264, 312, 20480, 13, 400, 731, 11, 294, 3479, 2539, 11, 1391, 436, 366, 11, 281, 512, 51560], "temperature": 0.0, "avg_logprob": -0.09107859929402669, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0011498908279463649}, {"id": 1916, "seek": 1110456, "start": 11104.56, "end": 11111.119999999999, "text": " extent, exotic. But joking apart, many folks are a convenient model for all sorts of data.", "tokens": [50364, 8396, 11, 27063, 13, 583, 17396, 4936, 11, 867, 4024, 366, 257, 10851, 2316, 337, 439, 7527, 295, 1412, 13, 50692], "temperature": 0.0, "avg_logprob": -0.12661788660451906, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.02787543460726738}, {"id": 1917, "seek": 1110456, "start": 11111.68, "end": 11116.4, "text": " And the data might be a high dimensional, but still have a low intrinsic dimensionality or", "tokens": [50720, 400, 264, 1412, 1062, 312, 257, 1090, 18795, 11, 457, 920, 362, 257, 2295, 35698, 10139, 1860, 420, 50956], "temperature": 0.0, "avg_logprob": -0.12661788660451906, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.02787543460726738}, {"id": 1918, "seek": 1110456, "start": 11116.4, "end": 11121.439999999999, "text": " can be explained by a small number of parameters or degrees of freedom. And this is really the premise", "tokens": [50956, 393, 312, 8825, 538, 257, 1359, 1230, 295, 9834, 420, 5310, 295, 5645, 13, 400, 341, 307, 534, 264, 22045, 51208], "temperature": 0.0, "avg_logprob": -0.12661788660451906, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.02787543460726738}, {"id": 1919, "seek": 1110456, "start": 11121.439999999999, "end": 11127.359999999999, "text": " of nonlinear dimensionality reduction. And for example, the reasons why data visualization", "tokens": [51208, 295, 2107, 28263, 10139, 1860, 11004, 13, 400, 337, 1365, 11, 264, 4112, 983, 1412, 25801, 51504], "temperature": 0.0, "avg_logprob": -0.12661788660451906, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.02787543460726738}, {"id": 1920, "seek": 1110456, "start": 11127.359999999999, "end": 11133.039999999999, "text": " techniques such as TSE and E at all work. And maybe the key question, as you're asking is,", "tokens": [51504, 7512, 1270, 382, 314, 5879, 293, 462, 412, 439, 589, 13, 400, 1310, 264, 2141, 1168, 11, 382, 291, 434, 3365, 307, 11, 51788], "temperature": 0.0, "avg_logprob": -0.12661788660451906, "compression_ratio": 1.646643109540636, "no_speech_prob": 0.02787543460726738}, {"id": 1921, "seek": 1113304, "start": 11133.12, "end": 11138.320000000002, "text": " how much of the manifold structure of the continuous manifold you actually leverage? And", "tokens": [50368, 577, 709, 295, 264, 47138, 3877, 295, 264, 10957, 47138, 291, 767, 13982, 30, 400, 50628], "temperature": 0.0, "avg_logprob": -0.0964308684726931, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005682364106178284}, {"id": 1922, "seek": 1113304, "start": 11138.320000000002, "end": 11143.2, "text": " in the TSE example, the only structure that you really use is local distances.", "tokens": [50628, 294, 264, 314, 5879, 1365, 11, 264, 787, 3877, 300, 291, 534, 764, 307, 2654, 22182, 13, 50872], "temperature": 0.0, "avg_logprob": -0.0964308684726931, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005682364106178284}, {"id": 1923, "seek": 1113304, "start": 11144.160000000002, "end": 11150.0, "text": " So if you think of a point cloud, of course, you can deal with it as a set. But if you assume", "tokens": [50920, 407, 498, 291, 519, 295, 257, 935, 4588, 11, 295, 1164, 11, 291, 393, 2028, 365, 309, 382, 257, 992, 13, 583, 498, 291, 6552, 51212], "temperature": 0.0, "avg_logprob": -0.0964308684726931, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005682364106178284}, {"id": 1924, "seek": 1113304, "start": 11150.0, "end": 11154.560000000001, "text": " that it comes from sampling of some continuous surface, you can probably say more. And this is", "tokens": [51212, 300, 309, 1487, 490, 21179, 295, 512, 10957, 3753, 11, 291, 393, 1391, 584, 544, 13, 400, 341, 307, 51440], "temperature": 0.0, "avg_logprob": -0.0964308684726931, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005682364106178284}, {"id": 1925, "seek": 1113304, "start": 11154.560000000001, "end": 11160.480000000001, "text": " forgivable what we tried to do in some of our works on geometric deep learning in applications", "tokens": [51440, 3667, 34376, 437, 321, 3031, 281, 360, 294, 512, 295, 527, 1985, 322, 33246, 2452, 2539, 294, 5821, 51736], "temperature": 0.0, "avg_logprob": -0.0964308684726931, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005682364106178284}, {"id": 1926, "seek": 1116048, "start": 11160.48, "end": 11166.88, "text": " in computer vision and graphics. And measures are one way of thinking of them is as graphs and", "tokens": [50364, 294, 3820, 5201, 293, 11837, 13, 400, 8000, 366, 472, 636, 295, 1953, 295, 552, 307, 382, 24877, 293, 50684], "temperature": 0.0, "avg_logprob": -0.13901256561279296, "compression_ratio": 1.7519685039370079, "no_speech_prob": 0.006055319216102362}, {"id": 1927, "seek": 1116048, "start": 11166.88, "end": 11171.52, "text": " steroids, where we have additional structure to leverage. So it's not only nodes and edges,", "tokens": [50684, 45717, 11, 689, 321, 362, 4497, 3877, 281, 13982, 13, 407, 309, 311, 406, 787, 13891, 293, 8819, 11, 50916], "temperature": 0.0, "avg_logprob": -0.13901256561279296, "compression_ratio": 1.7519685039370079, "no_speech_prob": 0.006055319216102362}, {"id": 1928, "seek": 1116048, "start": 11171.52, "end": 11176.16, "text": " but also faces. And in fact, measures are what is called simplicial complexes.", "tokens": [50916, 457, 611, 8475, 13, 400, 294, 1186, 11, 8000, 366, 437, 307, 1219, 1034, 4770, 831, 43676, 13, 51148], "temperature": 0.0, "avg_logprob": -0.13901256561279296, "compression_ratio": 1.7519685039370079, "no_speech_prob": 0.006055319216102362}, {"id": 1929, "seek": 1116048, "start": 11176.8, "end": 11182.48, "text": " As to the practical usefulness, computer vision and graphics are obviously the two fields where", "tokens": [51180, 1018, 281, 264, 8496, 4420, 1287, 11, 3820, 5201, 293, 11837, 366, 2745, 264, 732, 7909, 689, 51464], "temperature": 0.0, "avg_logprob": -0.13901256561279296, "compression_ratio": 1.7519685039370079, "no_speech_prob": 0.006055319216102362}, {"id": 1930, "seek": 1116048, "start": 11183.039999999999, "end": 11187.76, "text": " geometric deep learning on measures is important. And just to give a recent example", "tokens": [51492, 33246, 2452, 2539, 322, 8000, 307, 1021, 13, 400, 445, 281, 976, 257, 5162, 1365, 51728], "temperature": 0.0, "avg_logprob": -0.13901256561279296, "compression_ratio": 1.7519685039370079, "no_speech_prob": 0.006055319216102362}, {"id": 1931, "seek": 1118776, "start": 11187.76, "end": 11194.56, "text": " of a commercial success. There was a British startup called the AI. It was founded by", "tokens": [50364, 295, 257, 6841, 2245, 13, 821, 390, 257, 6221, 18578, 1219, 264, 7318, 13, 467, 390, 13234, 538, 50704], "temperature": 0.0, "avg_logprob": -0.19665196884510128, "compression_ratio": 1.5101214574898785, "no_speech_prob": 0.011849112808704376}, {"id": 1932, "seek": 1118776, "start": 11194.56, "end": 11199.36, "text": " my colleague and friend, Yasunos Kokinos. I was also one of the investors. And we had a", "tokens": [50704, 452, 13532, 293, 1277, 11, 30557, 409, 329, 36915, 15220, 13, 286, 390, 611, 472, 295, 264, 11519, 13, 400, 321, 632, 257, 50944], "temperature": 0.0, "avg_logprob": -0.19665196884510128, "compression_ratio": 1.5101214574898785, "no_speech_prob": 0.011849112808704376}, {"id": 1933, "seek": 1118776, "start": 11199.36, "end": 11204.72, "text": " collaboration on three different reconstruction using geometric neural networks. And the company", "tokens": [50944, 9363, 322, 1045, 819, 31565, 1228, 33246, 18161, 9590, 13, 400, 264, 2237, 51212], "temperature": 0.0, "avg_logprob": -0.19665196884510128, "compression_ratio": 1.5101214574898785, "no_speech_prob": 0.011849112808704376}, {"id": 1934, "seek": 1118776, "start": 11204.72, "end": 11210.960000000001, "text": " was acquired last year by SNAP and these technologies already now part of SNAP products. So you see it", "tokens": [51212, 390, 17554, 1036, 1064, 538, 13955, 4715, 293, 613, 7943, 1217, 586, 644, 295, 13955, 4715, 3383, 13, 407, 291, 536, 309, 51524], "temperature": 0.0, "avg_logprob": -0.19665196884510128, "compression_ratio": 1.5101214574898785, "no_speech_prob": 0.011849112808704376}, {"id": 1935, "seek": 1121096, "start": 11211.039999999999, "end": 11218.4, "text": " in the form of some 3D avatars or virtual and augmented reality applications that SNAP is", "tokens": [50368, 294, 264, 1254, 295, 512, 805, 35, 1305, 267, 685, 420, 6374, 293, 36155, 4103, 5821, 300, 13955, 4715, 307, 50736], "temperature": 0.0, "avg_logprob": -0.14240980999810354, "compression_ratio": 1.6006825938566553, "no_speech_prob": 0.010973501019179821}, {"id": 1936, "seek": 1121096, "start": 11218.4, "end": 11223.759999999998, "text": " developed. Professor Bronstein, I saw that you were doing some really cool stuff with the higher", "tokens": [50736, 4743, 13, 8419, 19544, 9089, 11, 286, 1866, 300, 291, 645, 884, 512, 534, 1627, 1507, 365, 264, 2946, 51004], "temperature": 0.0, "avg_logprob": -0.14240980999810354, "compression_ratio": 1.6006825938566553, "no_speech_prob": 0.010973501019179821}, {"id": 1937, "seek": 1121096, "start": 11223.759999999998, "end": 11229.199999999999, "text": " order simplicial coverings in graphs. And actually, I was going to call out your recent work on", "tokens": [51004, 1668, 1034, 4770, 831, 2060, 1109, 294, 24877, 13, 400, 767, 11, 286, 390, 516, 281, 818, 484, 428, 5162, 589, 322, 51276], "temperature": 0.0, "avg_logprob": -0.14240980999810354, "compression_ratio": 1.6006825938566553, "no_speech_prob": 0.010973501019179821}, {"id": 1938, "seek": 1121096, "start": 11229.199999999999, "end": 11235.119999999999, "text": " diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to", "tokens": [51276, 25242, 19077, 293, 4295, 319, 86, 5057, 13, 821, 366, 370, 867, 1627, 721, 300, 321, 393, 360, 281, 24877, 281, 51572], "temperature": 0.0, "avg_logprob": -0.14240980999810354, "compression_ratio": 1.6006825938566553, "no_speech_prob": 0.010973501019179821}, {"id": 1939, "seek": 1121096, "start": 11235.119999999999, "end": 11240.0, "text": " actually enable a little of this analysis. But there was a question from my good friend,", "tokens": [51572, 767, 9528, 257, 707, 295, 341, 5215, 13, 583, 456, 390, 257, 1168, 490, 452, 665, 1277, 11, 51816], "temperature": 0.0, "avg_logprob": -0.14240980999810354, "compression_ratio": 1.6006825938566553, "no_speech_prob": 0.010973501019179821}, {"id": 1940, "seek": 1124000, "start": 11240.08, "end": 11245.68, "text": " Zach Jost, who's one of our staff members here. And he says, what do you think is the most important", "tokens": [50368, 21028, 508, 555, 11, 567, 311, 472, 295, 527, 3525, 2679, 510, 13, 400, 415, 1619, 11, 437, 360, 291, 519, 307, 264, 881, 1021, 50648], "temperature": 0.0, "avg_logprob": -0.10860394985876351, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.004566723946481943}, {"id": 1941, "seek": 1124000, "start": 11245.68, "end": 11249.92, "text": " problem to solve with message passing graph neural networks? And what's the most promising path", "tokens": [50648, 1154, 281, 5039, 365, 3636, 8437, 4295, 18161, 9590, 30, 400, 437, 311, 264, 881, 20257, 3100, 50860], "temperature": 0.0, "avg_logprob": -0.10860394985876351, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.004566723946481943}, {"id": 1942, "seek": 1124000, "start": 11249.92, "end": 11255.12, "text": " forward? Probably we first need to agree about terminology. And to me, message passing and", "tokens": [50860, 2128, 30, 9210, 321, 700, 643, 281, 3986, 466, 27575, 13, 400, 281, 385, 11, 3636, 8437, 293, 51120], "temperature": 0.0, "avg_logprob": -0.10860394985876351, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.004566723946481943}, {"id": 1943, "seek": 1124000, "start": 11255.12, "end": 11262.56, "text": " here I agree with Petra is just a very generic mechanism for propagating information on the", "tokens": [51120, 510, 286, 3986, 365, 10472, 424, 307, 445, 257, 588, 19577, 7513, 337, 12425, 990, 1589, 322, 264, 51492], "temperature": 0.0, "avg_logprob": -0.10860394985876351, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.004566723946481943}, {"id": 1944, "seek": 1124000, "start": 11262.56, "end": 11267.76, "text": " graph. Now, traditional message passing that is used in graph neural networks uses the input", "tokens": [51492, 4295, 13, 823, 11, 5164, 3636, 8437, 300, 307, 1143, 294, 4295, 18161, 9590, 4960, 264, 4846, 51752], "temperature": 0.0, "avg_logprob": -0.10860394985876351, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.004566723946481943}, {"id": 1945, "seek": 1126776, "start": 11267.76, "end": 11272.800000000001, "text": " graph for this propagation. And we know, right, as we discussed that it is equivalent to device", "tokens": [50364, 4295, 337, 341, 38377, 13, 400, 321, 458, 11, 558, 11, 382, 321, 7152, 300, 309, 307, 10344, 281, 4302, 50616], "temperature": 0.0, "avg_logprob": -0.09150588953936542, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005504343658685684}, {"id": 1946, "seek": 1126776, "start": 11272.800000000001, "end": 11279.2, "text": " for a lemon graph isomorphism test that has limitations in the kinds of structures it can", "tokens": [50616, 337, 257, 11356, 4295, 307, 32702, 1434, 1500, 300, 575, 15705, 294, 264, 3685, 295, 9227, 309, 393, 50936], "temperature": 0.0, "avg_logprob": -0.09150588953936542, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005504343658685684}, {"id": 1947, "seek": 1126776, "start": 11279.2, "end": 11285.12, "text": " detect. Now, there exists topological constructions that go beyond graphs, such as simplicial and", "tokens": [50936, 5531, 13, 823, 11, 456, 8198, 1192, 4383, 7690, 626, 300, 352, 4399, 24877, 11, 1270, 382, 1034, 4770, 831, 293, 51232], "temperature": 0.0, "avg_logprob": -0.09150588953936542, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005504343658685684}, {"id": 1948, "seek": 1126776, "start": 11285.12, "end": 11290.72, "text": " cell complexes that you mentioned. And what we did in our recent works is developing a message", "tokens": [51232, 2815, 43676, 300, 291, 2835, 13, 400, 437, 321, 630, 294, 527, 5162, 1985, 307, 6416, 257, 3636, 51512], "temperature": 0.0, "avg_logprob": -0.09150588953936542, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005504343658685684}, {"id": 1949, "seek": 1126776, "start": 11290.72, "end": 11296.32, "text": " passing mechanism that is able to work on such structures. And of course, you may ask whether", "tokens": [51512, 8437, 7513, 300, 307, 1075, 281, 589, 322, 1270, 9227, 13, 400, 295, 1164, 11, 291, 815, 1029, 1968, 51792], "temperature": 0.0, "avg_logprob": -0.09150588953936542, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005504343658685684}, {"id": 1950, "seek": 1129632, "start": 11296.32, "end": 11302.72, "text": " we do encounter such structures in real life. So first of all, we do the measures that I mentioned", "tokens": [50364, 321, 360, 8593, 1270, 9227, 294, 957, 993, 13, 407, 700, 295, 439, 11, 321, 360, 264, 8000, 300, 286, 2835, 50684], "temperature": 0.0, "avg_logprob": -0.09942724054509944, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0057917931117117405}, {"id": 1951, "seek": 1129632, "start": 11302.72, "end": 11308.32, "text": " before are in fact, simplicial complexes. But secondly, what we show in the paper is that we", "tokens": [50684, 949, 366, 294, 1186, 11, 1034, 4770, 831, 43676, 13, 583, 26246, 11, 437, 321, 855, 294, 264, 3035, 307, 300, 321, 50964], "temperature": 0.0, "avg_logprob": -0.09942724054509944, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0057917931117117405}, {"id": 1952, "seek": 1129632, "start": 11308.32, "end": 11312.96, "text": " can take a traditional graph and lift it into a cell or simplicial complex. And probably a good", "tokens": [50964, 393, 747, 257, 5164, 4295, 293, 5533, 309, 666, 257, 2815, 420, 1034, 4770, 831, 3997, 13, 400, 1391, 257, 665, 51196], "temperature": 0.0, "avg_logprob": -0.09942724054509944, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0057917931117117405}, {"id": 1953, "seek": 1129632, "start": 11312.96, "end": 11318.64, "text": " example here is from the domain of computational chemistry, the graph neural network that you", "tokens": [51196, 1365, 510, 307, 490, 264, 9274, 295, 28270, 12558, 11, 264, 4295, 18161, 3209, 300, 291, 51480], "temperature": 0.0, "avg_logprob": -0.09942724054509944, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0057917931117117405}, {"id": 1954, "seek": 1129632, "start": 11318.64, "end": 11323.039999999999, "text": " apply to a molecular graph would consider a molecule just as a collection of nodes and edges,", "tokens": [51480, 3079, 281, 257, 19046, 4295, 576, 1949, 257, 15582, 445, 382, 257, 5765, 295, 13891, 293, 8819, 11, 51700], "temperature": 0.0, "avg_logprob": -0.09942724054509944, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0057917931117117405}, {"id": 1955, "seek": 1132304, "start": 11323.04, "end": 11328.080000000002, "text": " atoms, and chemical bonds between them. But this is not how chemists think of molecules.", "tokens": [50364, 16871, 11, 293, 7313, 14713, 1296, 552, 13, 583, 341, 307, 406, 577, 4771, 1751, 519, 295, 13093, 13, 50616], "temperature": 0.0, "avg_logprob": -0.10378984647376516, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.01820170320570469}, {"id": 1956, "seek": 1132304, "start": 11328.080000000002, "end": 11333.76, "text": " They think of them as structures such as, for example, aromatic rings. And with our approach,", "tokens": [50616, 814, 519, 295, 552, 382, 9227, 1270, 382, 11, 337, 1365, 11, 45831, 11136, 13, 400, 365, 527, 3109, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10378984647376516, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.01820170320570469}, {"id": 1957, "seek": 1132304, "start": 11333.76, "end": 11339.44, "text": " we can regard the rings as cells. So we have a special new object, and we can do a different", "tokens": [50900, 321, 393, 3843, 264, 11136, 382, 5438, 13, 407, 321, 362, 257, 2121, 777, 2657, 11, 293, 321, 393, 360, 257, 819, 51184], "temperature": 0.0, "avg_logprob": -0.10378984647376516, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.01820170320570469}, {"id": 1958, "seek": 1132304, "start": 11339.44, "end": 11343.92, "text": " form of message passing on them. And we can also show from the theoretical perspective that", "tokens": [51184, 1254, 295, 3636, 8437, 322, 552, 13, 400, 321, 393, 611, 855, 490, 264, 20864, 4585, 300, 51408], "temperature": 0.0, "avg_logprob": -0.10378984647376516, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.01820170320570469}, {"id": 1959, "seek": 1132304, "start": 11344.640000000001, "end": 11348.640000000001, "text": " this kind of message passing is strictly more powerful than the vice for a lemon algorithm.", "tokens": [51444, 341, 733, 295, 3636, 8437, 307, 20792, 544, 4005, 813, 264, 11964, 337, 257, 11356, 9284, 13, 51644], "temperature": 0.0, "avg_logprob": -0.10378984647376516, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.01820170320570469}, {"id": 1960, "seek": 1134864, "start": 11349.279999999999, "end": 11356.32, "text": " Do you see entirely new problems opening up that we wouldn't even have, let's say,", "tokens": [50396, 1144, 291, 536, 7696, 777, 2740, 5193, 493, 300, 321, 2759, 380, 754, 362, 11, 718, 311, 584, 11, 50748], "temperature": 0.0, "avg_logprob": -0.08537109038409064, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.013633111491799355}, {"id": 1961, "seek": 1134864, "start": 11356.32, "end": 11362.96, "text": " we wouldn't even have dared to touch before, you know, in, let's say, we simply have our", "tokens": [50748, 321, 2759, 380, 754, 362, 44564, 281, 2557, 949, 11, 291, 458, 11, 294, 11, 718, 311, 584, 11, 321, 2935, 362, 527, 51080], "temperature": 0.0, "avg_logprob": -0.08537109038409064, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.013633111491799355}, {"id": 1962, "seek": 1134864, "start": 11362.96, "end": 11367.92, "text": " classic neural networks or whatnot, or even our classic graph message passing algorithms.", "tokens": [51080, 7230, 18161, 9590, 420, 25882, 11, 420, 754, 527, 7230, 4295, 3636, 8437, 14642, 13, 51328], "temperature": 0.0, "avg_logprob": -0.08537109038409064, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.013633111491799355}, {"id": 1963, "seek": 1134864, "start": 11367.92, "end": 11375.119999999999, "text": " Do you see new problems that are now in reach that previously with none of these methods were", "tokens": [51328, 1144, 291, 536, 777, 2740, 300, 366, 586, 294, 2524, 300, 8046, 365, 6022, 295, 613, 7150, 645, 51688], "temperature": 0.0, "avg_logprob": -0.08537109038409064, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.013633111491799355}, {"id": 1964, "seek": 1137512, "start": 11375.12, "end": 11381.68, "text": " really, let's say, better than random guessing? It's a very interesting question that I think", "tokens": [50364, 534, 11, 718, 311, 584, 11, 1101, 813, 4974, 17939, 30, 467, 311, 257, 588, 1880, 1168, 300, 286, 519, 50692], "temperature": 0.0, "avg_logprob": -0.0912478703718919, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.003480936400592327}, {"id": 1965, "seek": 1137512, "start": 11381.68, "end": 11387.04, "text": " I'll answer from two angles, because you could, like, there could be like some", "tokens": [50692, 286, 603, 1867, 490, 732, 14708, 11, 570, 291, 727, 11, 411, 11, 456, 727, 312, 411, 512, 50960], "temperature": 0.0, "avg_logprob": -0.0912478703718919, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.003480936400592327}, {"id": 1966, "seek": 1137512, "start": 11387.04, "end": 11392.560000000001, "text": " longstanding problem that you knew about and wouldn't dare to attack. And now maybe you feel", "tokens": [50960, 938, 8618, 1154, 300, 291, 2586, 466, 293, 2759, 380, 8955, 281, 2690, 13, 400, 586, 1310, 291, 841, 51236], "temperature": 0.0, "avg_logprob": -0.0912478703718919, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.003480936400592327}, {"id": 1967, "seek": 1137512, "start": 11392.560000000001, "end": 11397.12, "text": " a bit more confident to attack it. There's also the aspect of uncovering a problem,", "tokens": [51236, 257, 857, 544, 6679, 281, 2690, 309, 13, 821, 311, 611, 264, 4171, 295, 21694, 278, 257, 1154, 11, 51464], "temperature": 0.0, "avg_logprob": -0.0912478703718919, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.003480936400592327}, {"id": 1968, "seek": 1137512, "start": 11397.12, "end": 11401.68, "text": " because when you start thinking about things in this particular way, you might realize,", "tokens": [51464, 570, 562, 291, 722, 1953, 466, 721, 294, 341, 1729, 636, 11, 291, 1062, 4325, 11, 51692], "temperature": 0.0, "avg_logprob": -0.0912478703718919, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.003480936400592327}, {"id": 1969, "seek": 1140168, "start": 11401.68, "end": 11405.84, "text": " hang on, to make this work, I made some assumptions. And those assumptions actually", "tokens": [50364, 3967, 322, 11, 281, 652, 341, 589, 11, 286, 1027, 512, 17695, 13, 400, 729, 17695, 767, 50572], "temperature": 0.0, "avg_logprob": -0.06541367999294348, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.010981938801705837}, {"id": 1970, "seek": 1140168, "start": 11405.84, "end": 11410.16, "text": " don't really hold at all in principle. So how do I make things, you know, a little bit better?", "tokens": [50572, 500, 380, 534, 1797, 412, 439, 294, 8665, 13, 407, 577, 360, 286, 652, 721, 11, 291, 458, 11, 257, 707, 857, 1101, 30, 50788], "temperature": 0.0, "avg_logprob": -0.06541367999294348, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.010981938801705837}, {"id": 1971, "seek": 1140168, "start": 11410.16, "end": 11416.960000000001, "text": " So I'll try to give an example for both of those. So in terms of a problem that previously,", "tokens": [50788, 407, 286, 603, 853, 281, 976, 364, 1365, 337, 1293, 295, 729, 13, 407, 294, 2115, 295, 257, 1154, 300, 8046, 11, 51128], "temperature": 0.0, "avg_logprob": -0.06541367999294348, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.010981938801705837}, {"id": 1972, "seek": 1140168, "start": 11416.960000000001, "end": 11423.04, "text": " I don't think was very easy to attack. And now we might have some tools that could help us attack", "tokens": [51128, 286, 500, 380, 519, 390, 588, 1858, 281, 2690, 13, 400, 586, 321, 1062, 362, 512, 3873, 300, 727, 854, 505, 2690, 51432], "temperature": 0.0, "avg_logprob": -0.06541367999294348, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.010981938801705837}, {"id": 1973, "seek": 1140168, "start": 11423.04, "end": 11430.08, "text": " it better. I have a longstanding interest in reinforcement learning. Actually, when I", "tokens": [51432, 309, 1101, 13, 286, 362, 257, 938, 8618, 1179, 294, 29280, 2539, 13, 5135, 11, 562, 286, 51784], "temperature": 0.0, "avg_logprob": -0.06541367999294348, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.010981938801705837}, {"id": 1974, "seek": 1143008, "start": 11430.16, "end": 11436.0, "text": " started my PhD, I spent six months attacking a reinforcement learning problem with one super", "tokens": [50368, 1409, 452, 14476, 11, 286, 4418, 2309, 2493, 15010, 257, 29280, 2539, 1154, 365, 472, 1687, 50660], "temperature": 0.0, "avg_logprob": -0.06344067255655925, "compression_ratio": 1.671280276816609, "no_speech_prob": 0.007932673208415508}, {"id": 1975, "seek": 1143008, "start": 11436.0, "end": 11442.32, "text": " tiny GPU. And that was, at that time, a massive time sink. Actually, DeepMind ended up scooping", "tokens": [50660, 5870, 18407, 13, 400, 300, 390, 11, 412, 300, 565, 11, 257, 5994, 565, 9500, 13, 5135, 11, 14895, 44, 471, 4590, 493, 19555, 278, 50976], "temperature": 0.0, "avg_logprob": -0.06344067255655925, "compression_ratio": 1.671280276816609, "no_speech_prob": 0.007932673208415508}, {"id": 1976, "seek": 1143008, "start": 11442.32, "end": 11447.92, "text": " my work sometime after that. And I quickly moved to things that were more, you know, doable with", "tokens": [50976, 452, 589, 15053, 934, 300, 13, 400, 286, 2661, 4259, 281, 721, 300, 645, 544, 11, 291, 458, 11, 41183, 365, 51256], "temperature": 0.0, "avg_logprob": -0.06344067255655925, "compression_ratio": 1.671280276816609, "no_speech_prob": 0.007932673208415508}, {"id": 1977, "seek": 1143008, "start": 11447.92, "end": 11452.8, "text": " the kind of hardware that I had at the time. But, you know, I always had a big interest in this area.", "tokens": [51256, 264, 733, 295, 8837, 300, 286, 632, 412, 264, 565, 13, 583, 11, 291, 458, 11, 286, 1009, 632, 257, 955, 1179, 294, 341, 1859, 13, 51500], "temperature": 0.0, "avg_logprob": -0.06344067255655925, "compression_ratio": 1.671280276816609, "no_speech_prob": 0.007932673208415508}, {"id": 1978, "seek": 1143008, "start": 11452.8, "end": 11458.32, "text": " And after joining DeepMind, I started to contribute to these kinds of directions more and more.", "tokens": [51500, 400, 934, 5549, 14895, 44, 471, 11, 286, 1409, 281, 10586, 281, 613, 3685, 295, 11095, 544, 293, 544, 13, 51776], "temperature": 0.0, "avg_logprob": -0.06344067255655925, "compression_ratio": 1.671280276816609, "no_speech_prob": 0.007932673208415508}, {"id": 1979, "seek": 1145832, "start": 11458.96, "end": 11467.76, "text": " And I think that basically, there are a lot of problems in reinforcement learning", "tokens": [50396, 400, 286, 519, 300, 1936, 11, 456, 366, 257, 688, 295, 2740, 294, 29280, 2539, 50836], "temperature": 0.0, "avg_logprob": -0.08843563748644544, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0019871785771101713}, {"id": 1980, "seek": 1145832, "start": 11468.56, "end": 11475.92, "text": " concerning data efficiency. So when you have to learn how to meaningfully act and do stuff,", "tokens": [50876, 18087, 1412, 10493, 13, 407, 562, 291, 362, 281, 1466, 577, 281, 3620, 2277, 605, 293, 360, 1507, 11, 51244], "temperature": 0.0, "avg_logprob": -0.08843563748644544, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0019871785771101713}, {"id": 1981, "seek": 1145832, "start": 11475.92, "end": 11481.199999999999, "text": " which is actually a fairly like low dimensional signal compared to the potential richness of", "tokens": [51244, 597, 307, 767, 257, 6457, 411, 2295, 18795, 6358, 5347, 281, 264, 3995, 44506, 295, 51508], "temperature": 0.0, "avg_logprob": -0.08843563748644544, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0019871785771101713}, {"id": 1982, "seek": 1145832, "start": 11481.199999999999, "end": 11485.76, "text": " the trajectories that you have to go through before you get that useful signal, like long", "tokens": [51508, 264, 18257, 2083, 300, 291, 362, 281, 352, 807, 949, 291, 483, 300, 4420, 6358, 11, 411, 938, 51736], "temperature": 0.0, "avg_logprob": -0.08843563748644544, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0019871785771101713}, {"id": 1983, "seek": 1148576, "start": 11485.76, "end": 11491.76, "text": " term credit assignment, all these kinds of problems, I feel like we can start to get more", "tokens": [50364, 1433, 5397, 15187, 11, 439, 613, 3685, 295, 2740, 11, 286, 841, 411, 321, 393, 722, 281, 483, 544, 50664], "temperature": 0.0, "avg_logprob": -0.09724580505747854, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0040676952339708805}, {"id": 1984, "seek": 1148576, "start": 11491.76, "end": 11497.52, "text": " data efficient reinforcement learning architectures by leveraging geometric concepts and also", "tokens": [50664, 1412, 7148, 29280, 2539, 6331, 1303, 538, 32666, 33246, 10392, 293, 611, 50952], "temperature": 0.0, "avg_logprob": -0.09724580505747854, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0040676952339708805}, {"id": 1985, "seek": 1148576, "start": 11497.52, "end": 11504.72, "text": " algorithmic concepts. So to give you one example of this, we have some months ago put out a paper", "tokens": [50952, 9284, 299, 10392, 13, 407, 281, 976, 291, 472, 1365, 295, 341, 11, 321, 362, 512, 2493, 2057, 829, 484, 257, 3035, 51312], "temperature": 0.0, "avg_logprob": -0.09724580505747854, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0040676952339708805}, {"id": 1986, "seek": 1148576, "start": 11504.72, "end": 11510.880000000001, "text": " on the archive called the executed latent value iteration network or x selvin, where we have", "tokens": [51312, 322, 264, 23507, 1219, 264, 17577, 48994, 2158, 24784, 3209, 420, 2031, 5851, 4796, 11, 689, 321, 362, 51620], "temperature": 0.0, "avg_logprob": -0.09724580505747854, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0040676952339708805}, {"id": 1987, "seek": 1151088, "start": 11511.199999999999, "end": 11518.64, "text": " captured the essence of an algorithm in RL, which perfectly solves the RL problem. So the value", "tokens": [50380, 11828, 264, 12801, 295, 364, 9284, 294, 497, 43, 11, 597, 6239, 39890, 264, 497, 43, 1154, 13, 407, 264, 2158, 50752], "temperature": 0.0, "avg_logprob": -0.10465916565486363, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0026722669135779142}, {"id": 1988, "seek": 1151088, "start": 11518.64, "end": 11523.039999999999, "text": " iteration algorithm, assuming you give it a Markov decision process will give you the perfect", "tokens": [50752, 24784, 9284, 11, 11926, 291, 976, 309, 257, 3934, 5179, 3537, 1399, 486, 976, 291, 264, 2176, 50972], "temperature": 0.0, "avg_logprob": -0.10465916565486363, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0026722669135779142}, {"id": 1989, "seek": 1151088, "start": 11523.039999999999, "end": 11527.599999999999, "text": " policy for that Markov decision process. So it's a super attractive algorithm to think about when", "tokens": [50972, 3897, 337, 300, 3934, 5179, 3537, 1399, 13, 407, 309, 311, 257, 1687, 12609, 9284, 281, 519, 466, 562, 51200], "temperature": 0.0, "avg_logprob": -0.10465916565486363, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0026722669135779142}, {"id": 1990, "seek": 1151088, "start": 11527.599999999999, "end": 11533.359999999999, "text": " you do RL, big caveat, right? You need to know all the dynamics about your environment, and you need", "tokens": [51200, 291, 360, 497, 43, 11, 955, 43012, 11, 558, 30, 509, 643, 281, 458, 439, 264, 15679, 466, 428, 2823, 11, 293, 291, 643, 51488], "temperature": 0.0, "avg_logprob": -0.10465916565486363, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0026722669135779142}, {"id": 1991, "seek": 1151088, "start": 11533.359999999999, "end": 11538.32, "text": " to know all the reward models of the environment before you can apply the algorithm. So this obviously", "tokens": [51488, 281, 458, 439, 264, 7782, 5245, 295, 264, 2823, 949, 291, 393, 3079, 264, 9284, 13, 407, 341, 2745, 51736], "temperature": 0.0, "avg_logprob": -0.10465916565486363, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.0026722669135779142}, {"id": 1992, "seek": 1153832, "start": 11538.4, "end": 11543.76, "text": " limits its use in the more generic deep reinforcement learning setting. But now with the knowledge of", "tokens": [50368, 10406, 1080, 764, 294, 264, 544, 19577, 2452, 29280, 2539, 3287, 13, 583, 586, 365, 264, 3601, 295, 50636], "temperature": 0.0, "avg_logprob": -0.08305130913144067, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.002933455165475607}, {"id": 1993, "seek": 1153832, "start": 11544.32, "end": 11549.119999999999, "text": " the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning", "tokens": [50664, 264, 14217, 18426, 295, 264, 4295, 295, 4368, 300, 264, 376, 11373, 13716, 887, 293, 264, 9284, 299, 21577, 50904], "temperature": 0.0, "avg_logprob": -0.08305130913144067, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.002933455165475607}, {"id": 1994, "seek": 1153832, "start": 11549.119999999999, "end": 11553.36, "text": " blueprint, we actually taught the graph neural network, which aligns super well with value", "tokens": [50904, 35868, 11, 321, 767, 5928, 264, 4295, 18161, 3209, 11, 597, 7975, 82, 1687, 731, 365, 2158, 51116], "temperature": 0.0, "avg_logprob": -0.08305130913144067, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.002933455165475607}, {"id": 1995, "seek": 1153832, "start": 11553.36, "end": 11559.199999999999, "text": " iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way", "tokens": [51116, 24784, 11, 767, 11, 321, 5928, 309, 281, 294, 257, 9594, 48224, 990, 294, 257, 23551, 48224, 990, 636, 51408], "temperature": 0.0, "avg_logprob": -0.08305130913144067, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.002933455165475607}, {"id": 1996, "seek": 1153832, "start": 11559.84, "end": 11565.279999999999, "text": " on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation,", "tokens": [51440, 322, 257, 3840, 295, 16979, 3247, 15551, 376, 11373, 82, 11, 1466, 264, 12801, 295, 264, 2158, 24784, 24903, 11, 51712], "temperature": 0.0, "avg_logprob": -0.08305130913144067, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.002933455165475607}, {"id": 1997, "seek": 1156528, "start": 11565.36, "end": 11569.92, "text": " and then we stitched it into a planning algorithm in a deep reinforcement learning setting.", "tokens": [50368, 293, 550, 321, 48992, 309, 666, 257, 5038, 9284, 294, 257, 2452, 29280, 2539, 3287, 13, 50596], "temperature": 0.0, "avg_logprob": -0.08677922617090811, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0019868300296366215}, {"id": 1998, "seek": 1156528, "start": 11569.92, "end": 11574.880000000001, "text": " And just by like training this pipeline end to end with a model free loss, we were able to", "tokens": [50596, 400, 445, 538, 411, 3097, 341, 15517, 917, 281, 917, 365, 257, 2316, 1737, 4470, 11, 321, 645, 1075, 281, 50844], "temperature": 0.0, "avg_logprob": -0.08677922617090811, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0019868300296366215}, {"id": 1999, "seek": 1156528, "start": 11575.52, "end": 11581.6, "text": " get interesting returns in Atari games much sooner than some of the competing approaches. So", "tokens": [50876, 483, 1880, 11247, 294, 41381, 2813, 709, 15324, 813, 512, 295, 264, 15439, 11587, 13, 407, 51180], "temperature": 0.0, "avg_logprob": -0.08677922617090811, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0019868300296366215}, {"id": 2000, "seek": 1156528, "start": 11581.6, "end": 11586.640000000001, "text": " it's a very small step. It still requires, you know, 100,000 200,000 iterations of", "tokens": [51180, 309, 311, 257, 588, 1359, 1823, 13, 467, 920, 7029, 11, 291, 458, 11, 2319, 11, 1360, 2331, 11, 1360, 36540, 295, 51432], "temperature": 0.0, "avg_logprob": -0.08677922617090811, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0019868300296366215}, {"id": 2001, "seek": 1156528, "start": 11587.36, "end": 11591.44, "text": " playing before you get meaning, some meaningful behaviors start to come out. But", "tokens": [51468, 2433, 949, 291, 483, 3620, 11, 512, 10995, 15501, 722, 281, 808, 484, 13, 583, 51672], "temperature": 0.0, "avg_logprob": -0.08677922617090811, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0019868300296366215}, {"id": 2002, "seek": 1159144, "start": 11592.08, "end": 11597.12, "text": " it's a sign that we might be able to move the needle a bit backwards and not require, you know,", "tokens": [50396, 309, 311, 257, 1465, 300, 321, 1062, 312, 1075, 281, 1286, 264, 11037, 257, 857, 12204, 293, 406, 3651, 11, 291, 458, 11, 50648], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2003, "seek": 1159144, "start": 11597.12, "end": 11602.16, "text": " billions and billions of transitions before we start to see meaningful behavior emerge. And I", "tokens": [50648, 17375, 293, 17375, 295, 23767, 949, 321, 722, 281, 536, 10995, 5223, 21511, 13, 400, 286, 50900], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2004, "seek": 1159144, "start": 11602.16, "end": 11606.16, "text": " think that's very important because in most real world applications of reinforcement learning,", "tokens": [50900, 519, 300, 311, 588, 1021, 570, 294, 881, 957, 1002, 5821, 295, 29280, 2539, 11, 51100], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2005, "seek": 1159144, "start": 11606.16, "end": 11611.04, "text": " you don't have a budget for billions of interactions before you have to already learn a", "tokens": [51100, 291, 500, 380, 362, 257, 4706, 337, 17375, 295, 13280, 949, 291, 362, 281, 1217, 1466, 257, 51344], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2006, "seek": 1159144, "start": 11611.04, "end": 11615.44, "text": " meaningful policy. So that's one side, I think. And just generally in reinforcement learning,", "tokens": [51344, 10995, 3897, 13, 407, 300, 311, 472, 1252, 11, 286, 519, 13, 400, 445, 5101, 294, 29280, 2539, 11, 51564], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2007, "seek": 1159144, "start": 11615.44, "end": 11620.560000000001, "text": " graphs appear left, right and center, not just in the algorithms, but also in the structure of the", "tokens": [51564, 24877, 4204, 1411, 11, 558, 293, 3056, 11, 406, 445, 294, 264, 14642, 11, 457, 611, 294, 264, 3877, 295, 264, 51820], "temperature": 0.0, "avg_logprob": -0.06346805572509766, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0011693573324009776}, {"id": 2008, "seek": 1162056, "start": 11620.56, "end": 11624.72, "text": " environment and these kinds of things. So I think that's one area where geometric deep learning", "tokens": [50364, 2823, 293, 613, 3685, 295, 721, 13, 407, 286, 519, 300, 311, 472, 1859, 689, 33246, 2452, 2539, 50572], "temperature": 0.0, "avg_logprob": -0.05969611122494652, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.001809805165976286}, {"id": 2009, "seek": 1162056, "start": 11624.72, "end": 11629.68, "text": " could really help us, you know, get better behaviors faster, not necessarily solve it", "tokens": [50572, 727, 534, 854, 505, 11, 291, 458, 11, 483, 1101, 15501, 4663, 11, 406, 4725, 5039, 309, 50820], "temperature": 0.0, "avg_logprob": -0.05969611122494652, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.001809805165976286}, {"id": 2010, "seek": 1162056, "start": 11629.68, "end": 11634.56, "text": " better than the standard deep RL, but, you know, get the better behaviors and fewer interactions.", "tokens": [50820, 1101, 813, 264, 3832, 2452, 497, 43, 11, 457, 11, 291, 458, 11, 483, 264, 1101, 15501, 293, 13366, 13280, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05969611122494652, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.001809805165976286}, {"id": 2011, "seek": 1162056, "start": 11634.56, "end": 11640.24, "text": " And as for one problem that we have uncovered through this kind of observational lens,", "tokens": [51064, 400, 382, 337, 472, 1154, 300, 321, 362, 37729, 807, 341, 733, 295, 9951, 1478, 6765, 11, 51348], "temperature": 0.0, "avg_logprob": -0.05969611122494652, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.001809805165976286}, {"id": 2012, "seek": 1162056, "start": 11641.279999999999, "end": 11645.6, "text": " you know, as I said, often in graph representation learning, we assume innocently that the graph", "tokens": [51400, 291, 458, 11, 382, 286, 848, 11, 2049, 294, 4295, 10290, 2539, 11, 321, 6552, 10843, 2276, 300, 264, 4295, 51616], "temperature": 0.0, "avg_logprob": -0.05969611122494652, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.001809805165976286}, {"id": 2013, "seek": 1164560, "start": 11645.6, "end": 11653.28, "text": " is given to us, whereas very often this is not the case. So this divide has brought about this new", "tokens": [50364, 307, 2212, 281, 505, 11, 9735, 588, 2049, 341, 307, 406, 264, 1389, 13, 407, 341, 9845, 575, 3038, 466, 341, 777, 50748], "temperature": 0.0, "avg_logprob": -0.04831881706531231, "compression_ratio": 1.7087719298245614, "no_speech_prob": 0.039027564227581024}, {"id": 2014, "seek": 1164560, "start": 11653.28, "end": 11658.48, "text": " emerging area of latent graph learning or latent graph inference, where the objective is to learn", "tokens": [50748, 14989, 1859, 295, 48994, 4295, 2539, 420, 48994, 4295, 38253, 11, 689, 264, 10024, 307, 281, 1466, 51008], "temperature": 0.0, "avg_logprob": -0.04831881706531231, "compression_ratio": 1.7087719298245614, "no_speech_prob": 0.039027564227581024}, {"id": 2015, "seek": 1164560, "start": 11658.48, "end": 11664.800000000001, "text": " the graph simultaneously with using it for your underlying decision problem. And this is a big", "tokens": [51008, 264, 4295, 16561, 365, 1228, 309, 337, 428, 14217, 3537, 1154, 13, 400, 341, 307, 257, 955, 51324], "temperature": 0.0, "avg_logprob": -0.04831881706531231, "compression_ratio": 1.7087719298245614, "no_speech_prob": 0.039027564227581024}, {"id": 2016, "seek": 1164560, "start": 11664.800000000001, "end": 11668.800000000001, "text": " issue for neural network optimization, because you're fundamentally making a discrete decision in", "tokens": [51324, 2734, 337, 18161, 3209, 19618, 11, 570, 291, 434, 17879, 1455, 257, 27706, 3537, 294, 51524], "temperature": 0.0, "avg_logprob": -0.04831881706531231, "compression_ratio": 1.7087719298245614, "no_speech_prob": 0.039027564227581024}, {"id": 2017, "seek": 1164560, "start": 11668.800000000001, "end": 11673.84, "text": " there. And if the number of nodes is huge, you cannot afford to start with the n squared approach", "tokens": [51524, 456, 13, 400, 498, 264, 1230, 295, 13891, 307, 2603, 11, 291, 2644, 6157, 281, 722, 365, 264, 297, 8889, 3109, 51776], "temperature": 0.0, "avg_logprob": -0.04831881706531231, "compression_ratio": 1.7087719298245614, "no_speech_prob": 0.039027564227581024}, {"id": 2018, "seek": 1167384, "start": 11673.92, "end": 11679.28, "text": " and then gradually refine it. So currently, the state of the art in many regards of what we have", "tokens": [50368, 293, 550, 13145, 33906, 309, 13, 407, 4362, 11, 264, 1785, 295, 264, 1523, 294, 867, 14258, 295, 437, 321, 362, 50636], "temperature": 0.0, "avg_logprob": -0.1002942313832685, "compression_ratio": 1.71875, "no_speech_prob": 0.002216639695689082}, {"id": 2019, "seek": 1167384, "start": 11679.28, "end": 11684.72, "text": " here is to do a K nearest neighbor graph in the feature space, and just hope that that gets us", "tokens": [50636, 510, 307, 281, 360, 257, 591, 23831, 5987, 4295, 294, 264, 4111, 1901, 11, 293, 445, 1454, 300, 300, 2170, 505, 50908], "temperature": 0.0, "avg_logprob": -0.1002942313832685, "compression_ratio": 1.71875, "no_speech_prob": 0.002216639695689082}, {"id": 2020, "seek": 1167384, "start": 11684.72, "end": 11690.0, "text": " most of the way there. And usually this works quite well for getting, you know, interesting answers,", "tokens": [50908, 881, 295, 264, 636, 456, 13, 400, 2673, 341, 1985, 1596, 731, 337, 1242, 11, 291, 458, 11, 1880, 6338, 11, 51172], "temperature": 0.0, "avg_logprob": -0.1002942313832685, "compression_ratio": 1.71875, "no_speech_prob": 0.002216639695689082}, {"id": 2021, "seek": 1167384, "start": 11690.0, "end": 11694.4, "text": " because, you know, if you have a decent ish enough KNN graph, you will cover everything that you need", "tokens": [51172, 570, 11, 291, 458, 11, 498, 291, 362, 257, 8681, 307, 71, 1547, 26967, 45, 4295, 11, 291, 486, 2060, 1203, 300, 291, 643, 51392], "temperature": 0.0, "avg_logprob": -0.1002942313832685, "compression_ratio": 1.71875, "no_speech_prob": 0.002216639695689082}, {"id": 2022, "seek": 1167384, "start": 11694.4, "end": 11700.32, "text": " reasonably quickly. But, you know, then there that raises the issue of what if the graph itself is a", "tokens": [51392, 23551, 2661, 13, 583, 11, 291, 458, 11, 550, 456, 300, 19658, 264, 2734, 295, 437, 498, 264, 4295, 2564, 307, 257, 51688], "temperature": 0.0, "avg_logprob": -0.1002942313832685, "compression_ratio": 1.71875, "no_speech_prob": 0.002216639695689082}, {"id": 2023, "seek": 1170032, "start": 11700.32, "end": 11704.64, "text": " meaningful output of your problem, what if you're a causality researcher that wants to figure out", "tokens": [50364, 10995, 5598, 295, 428, 1154, 11, 437, 498, 291, 434, 257, 3302, 1860, 21751, 300, 2738, 281, 2573, 484, 50580], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2024, "seek": 1170032, "start": 11704.64, "end": 11709.039999999999, "text": " how different, you know, parts of information interact to them, they probably wouldn't be", "tokens": [50580, 577, 819, 11, 291, 458, 11, 3166, 295, 1589, 4648, 281, 552, 11, 436, 1391, 2759, 380, 312, 50800], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2025, "seek": 1170032, "start": 11709.039999999999, "end": 11713.92, "text": " satisfied with a K nearest neighbor graph as an output of the system. So yeah, I feel like there's", "tokens": [50800, 11239, 365, 257, 591, 23831, 5987, 4295, 382, 364, 5598, 295, 264, 1185, 13, 407, 1338, 11, 286, 841, 411, 456, 311, 51044], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2026, "seek": 1170032, "start": 11713.92, "end": 11719.92, "text": " a lot of work to be done to actually scalably and usefully do something like this. And I don't", "tokens": [51044, 257, 688, 295, 589, 281, 312, 1096, 281, 767, 15664, 1188, 293, 764, 2277, 360, 746, 411, 341, 13, 400, 286, 500, 380, 51344], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2027, "seek": 1170032, "start": 11719.92, "end": 11724.24, "text": " have a better answer than what I just said is the state of the art. So a potential open problem", "tokens": [51344, 362, 257, 1101, 1867, 813, 437, 286, 445, 848, 307, 264, 1785, 295, 264, 1523, 13, 407, 257, 3995, 1269, 1154, 51560], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2028, "seek": 1170032, "start": 11724.24, "end": 11728.16, "text": " for everybody in the audience today. Absolutely. And you touch on some really interesting things", "tokens": [51560, 337, 2201, 294, 264, 4034, 965, 13, 7021, 13, 400, 291, 2557, 322, 512, 534, 1880, 721, 51756], "temperature": 0.0, "avg_logprob": -0.08907080408352525, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.008173889480531216}, {"id": 2029, "seek": 1172816, "start": 11728.16, "end": 11733.119999999999, "text": " that I think causality is a huge area that we could be looking at graphs on. And also we had", "tokens": [50364, 300, 286, 519, 3302, 1860, 307, 257, 2603, 1859, 300, 321, 727, 312, 1237, 412, 24877, 322, 13, 400, 611, 321, 632, 50612], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2030, "seek": 1172816, "start": 11733.119999999999, "end": 11738.48, "text": " Dr. Tom Zahavi from DeepMind, one of your colleagues, and he said that, you know, he looks at", "tokens": [50612, 2491, 13, 5041, 1176, 545, 18442, 490, 14895, 44, 471, 11, 472, 295, 428, 7734, 11, 293, 415, 848, 300, 11, 291, 458, 11, 415, 1542, 412, 50880], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2031, "seek": 1172816, "start": 11738.48, "end": 11743.28, "text": " meta learning and also diversity preservation in in agent based learning. But he thinks that", "tokens": [50880, 19616, 2539, 293, 611, 8811, 27257, 294, 294, 9461, 2361, 2539, 13, 583, 415, 7309, 300, 51120], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2032, "seek": 1172816, "start": 11743.28, "end": 11747.44, "text": " reinforcement learning is just about to have its image net moment where we can discover", "tokens": [51120, 29280, 2539, 307, 445, 466, 281, 362, 1080, 3256, 2533, 1623, 689, 321, 393, 4411, 51328], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2033, "seek": 1172816, "start": 11747.44, "end": 11751.76, "text": " a lot of the structure in these problems, which is fascinating. I would like to bring up", "tokens": [51328, 257, 688, 295, 264, 3877, 294, 613, 2740, 11, 597, 307, 10343, 13, 286, 576, 411, 281, 1565, 493, 51544], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2034, "seek": 1172816, "start": 11752.8, "end": 11757.68, "text": " one application where maybe quantitative improvement that is afforded by a genetic", "tokens": [51596, 472, 3861, 689, 1310, 27778, 10444, 300, 307, 6157, 292, 538, 257, 12462, 51840], "temperature": 0.0, "avg_logprob": -0.15541344597226098, "compression_ratio": 1.6635802469135803, "no_speech_prob": 0.016171757131814957}, {"id": 2035, "seek": 1175768, "start": 11757.68, "end": 11762.48, "text": " deep learning can lead to a qualitative breakthrough. And this is a problem of", "tokens": [50364, 2452, 2539, 393, 1477, 281, 257, 31312, 22397, 13, 400, 341, 307, 257, 1154, 295, 50604], "temperature": 0.0, "avg_logprob": -0.16489092151770432, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003961518406867981}, {"id": 2036, "seek": 1175768, "start": 11763.12, "end": 11768.56, "text": " structural biology. Alpha fold is one such example for correctly geometrically modeling", "tokens": [50636, 15067, 14956, 13, 20588, 4860, 307, 472, 1270, 1365, 337, 8944, 12956, 81, 984, 15983, 50908], "temperature": 0.0, "avg_logprob": -0.16489092151770432, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003961518406867981}, {"id": 2037, "seek": 1175768, "start": 11769.2, "end": 11774.48, "text": " the problem you get a breakthrough in the performance. So it's indeed an image net", "tokens": [50940, 264, 1154, 291, 483, 257, 22397, 294, 264, 3389, 13, 407, 309, 311, 6451, 364, 3256, 2533, 51204], "temperature": 0.0, "avg_logprob": -0.16489092151770432, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003961518406867981}, {"id": 2038, "seek": 1175768, "start": 11774.48, "end": 11779.84, "text": " moment that happened in this field. And now once you have sufficiently accurate", "tokens": [51204, 1623, 300, 2011, 294, 341, 2519, 13, 400, 586, 1564, 291, 362, 31868, 8559, 51472], "temperature": 0.0, "avg_logprob": -0.16489092151770432, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003961518406867981}, {"id": 2039, "seek": 1175768, "start": 11780.720000000001, "end": 11785.2, "text": " prediction of 3D structure of proteins, it suddenly enables a lot of interesting applications", "tokens": [51516, 17630, 295, 805, 35, 3877, 295, 15577, 11, 309, 5800, 17077, 257, 688, 295, 1880, 5821, 51740], "temperature": 0.0, "avg_logprob": -0.16489092151770432, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003961518406867981}, {"id": 2040, "seek": 1178520, "start": 11785.2, "end": 11790.08, "text": " for example, in the field of drug design. So potentially entire pharmaceutical", "tokens": [50364, 337, 1365, 11, 294, 264, 2519, 295, 4110, 1715, 13, 407, 7263, 2302, 27130, 50608], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2041, "seek": 1178520, "start": 11790.08, "end": 11796.08, "text": " pipelines we invented with the use of this technology. And the impact can be extraordinary.", "tokens": [50608, 40168, 321, 14479, 365, 264, 764, 295, 341, 2899, 13, 400, 264, 2712, 393, 312, 10581, 13, 50908], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2042, "seek": 1178520, "start": 11796.800000000001, "end": 11800.720000000001, "text": " It's so true. I mean, Professor Bronstein, when I was watching your lecture series,", "tokens": [50944, 467, 311, 370, 2074, 13, 286, 914, 11, 8419, 19544, 9089, 11, 562, 286, 390, 1976, 428, 7991, 2638, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2043, "seek": 1178520, "start": 11800.720000000001, "end": 11804.800000000001, "text": " it blew me away when you were talking about all of the applications. I think Yannick said a minute", "tokens": [51140, 309, 19075, 385, 1314, 562, 291, 645, 1417, 466, 439, 295, 264, 5821, 13, 286, 519, 398, 969, 618, 848, 257, 3456, 51344], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2044, "seek": 1178520, "start": 11804.800000000001, "end": 11810.960000000001, "text": " ago that it's almost as if some of these applications are just so ambitious that the thought", "tokens": [51344, 2057, 300, 309, 311, 1920, 382, 498, 512, 295, 613, 5821, 366, 445, 370, 20239, 300, 264, 1194, 51652], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2045, "seek": 1178520, "start": 11810.960000000001, "end": 11814.560000000001, "text": " wouldn't even have crossed our mind that we might be able to do it before, like for example,", "tokens": [51652, 2759, 380, 754, 362, 14622, 527, 1575, 300, 321, 1062, 312, 1075, 281, 360, 309, 949, 11, 411, 337, 1365, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1282772488064236, "compression_ratio": 1.6283987915407856, "no_speech_prob": 0.0028751352801918983}, {"id": 2046, "seek": 1181456, "start": 11814.56, "end": 11820.32, "text": " being able to predict facial geometry from a DNA sequence. So we might be able to look at an old", "tokens": [50364, 885, 1075, 281, 6069, 15642, 18426, 490, 257, 8272, 8310, 13, 407, 321, 1062, 312, 1075, 281, 574, 412, 364, 1331, 50652], "temperature": 0.0, "avg_logprob": -0.10478066061144677, "compression_ratio": 1.5625, "no_speech_prob": 0.0037172376178205013}, {"id": 2047, "seek": 1181456, "start": 11820.32, "end": 11825.119999999999, "text": " DNA sequence and actually see what that person looked like. That would have been unimaginable", "tokens": [50652, 8272, 8310, 293, 767, 536, 437, 300, 954, 2956, 411, 13, 663, 576, 362, 668, 517, 44976, 712, 50892], "temperature": 0.0, "avg_logprob": -0.10478066061144677, "compression_ratio": 1.5625, "no_speech_prob": 0.0037172376178205013}, {"id": 2048, "seek": 1181456, "start": 11825.119999999999, "end": 11830.48, "text": " just a few years ago. So that's incredible. I'm really interested in your definition", "tokens": [50892, 445, 257, 1326, 924, 2057, 13, 407, 300, 311, 4651, 13, 286, 478, 534, 3102, 294, 428, 7123, 51160], "temperature": 0.0, "avg_logprob": -0.10478066061144677, "compression_ratio": 1.5625, "no_speech_prob": 0.0037172376178205013}, {"id": 2049, "seek": 1181456, "start": 11830.48, "end": 11834.0, "text": " of intelligence, right? And whether you think neural networks could ever be intelligent.", "tokens": [51160, 295, 7599, 11, 558, 30, 400, 1968, 291, 519, 18161, 9590, 727, 1562, 312, 13232, 13, 51336], "temperature": 0.0, "avg_logprob": -0.10478066061144677, "compression_ratio": 1.5625, "no_speech_prob": 0.0037172376178205013}, {"id": 2050, "seek": 1181456, "start": 11834.96, "end": 11839.439999999999, "text": " Douglas Hofstadter, for example, he wrote the famous book Godel Escher Bach. It was a", "tokens": [51384, 23010, 37379, 48299, 391, 11, 337, 1365, 11, 415, 4114, 264, 4618, 1446, 1265, 338, 2313, 6759, 30920, 13, 467, 390, 257, 51608], "temperature": 0.0, "avg_logprob": -0.10478066061144677, "compression_ratio": 1.5625, "no_speech_prob": 0.0037172376178205013}, {"id": 2051, "seek": 1183944, "start": 11839.44, "end": 11845.36, "text": " Pulitzer Prize winning book in the 1970s, but he made the argument that analogy is the core of", "tokens": [50364, 35568, 16845, 22604, 8224, 1446, 294, 264, 14577, 82, 11, 457, 415, 1027, 264, 6770, 300, 21663, 307, 264, 4965, 295, 50660], "temperature": 0.0, "avg_logprob": -0.09695079637610395, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.03589150309562683}, {"id": 2052, "seek": 1183944, "start": 11845.36, "end": 11850.880000000001, "text": " cognition. He said that analogies are a bit like the interstate freeway of cognition.", "tokens": [50660, 46905, 13, 634, 848, 300, 16660, 530, 366, 257, 857, 411, 264, 728, 15406, 1737, 676, 295, 46905, 13, 50936], "temperature": 0.0, "avg_logprob": -0.09695079637610395, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.03589150309562683}, {"id": 2053, "seek": 1183944, "start": 11850.880000000001, "end": 11855.84, "text": " They're not little modules on the side or something like that. And I think that in a way,", "tokens": [50936, 814, 434, 406, 707, 16679, 322, 264, 1252, 420, 746, 411, 300, 13, 400, 286, 519, 300, 294, 257, 636, 11, 51184], "temperature": 0.0, "avg_logprob": -0.09695079637610395, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.03589150309562683}, {"id": 2054, "seek": 1183944, "start": 11855.84, "end": 11862.08, "text": " analogies are also symmetries, right? So when I say that someone is firewalling a person,", "tokens": [51184, 16660, 530, 366, 611, 14232, 302, 2244, 11, 558, 30, 407, 562, 286, 584, 300, 1580, 307, 36109, 278, 257, 954, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09695079637610395, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.03589150309562683}, {"id": 2055, "seek": 1183944, "start": 11862.720000000001, "end": 11867.2, "text": " it means that they don't want to talk with that person. It's a symmetry between the abstract", "tokens": [51528, 309, 1355, 300, 436, 500, 380, 528, 281, 751, 365, 300, 954, 13, 467, 311, 257, 25440, 1296, 264, 12649, 51752], "temperature": 0.0, "avg_logprob": -0.09695079637610395, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.03589150309562683}, {"id": 2056, "seek": 1186720, "start": 11867.28, "end": 11872.560000000001, "text": " representation of a real network firewall and an abstract social category.", "tokens": [50368, 10290, 295, 257, 957, 3209, 36109, 293, 364, 12649, 2093, 7719, 13, 50632], "temperature": 0.0, "avg_logprob": -0.10116216114589147, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0064758798107504845}, {"id": 2057, "seek": 1186720, "start": 11872.560000000001, "end": 11878.640000000001, "text": " So does this require a different neural network architecture or could geometric deep learning", "tokens": [50632, 407, 775, 341, 3651, 257, 819, 18161, 3209, 9482, 420, 727, 33246, 2452, 2539, 50936], "temperature": 0.0, "avg_logprob": -0.10116216114589147, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0064758798107504845}, {"id": 2058, "seek": 1186720, "start": 11878.640000000001, "end": 11882.800000000001, "text": " already deliver the goods, right? It's almost as if it's just a representation problem.", "tokens": [50936, 1217, 4239, 264, 10179, 11, 558, 30, 467, 311, 1920, 382, 498, 309, 311, 445, 257, 10290, 1154, 13, 51144], "temperature": 0.0, "avg_logprob": -0.10116216114589147, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0064758798107504845}, {"id": 2059, "seek": 1186720, "start": 11882.800000000001, "end": 11889.12, "text": " I think it's a very important question, one which I cannot claim to have the right answer to. And", "tokens": [51144, 286, 519, 309, 311, 257, 588, 1021, 1168, 11, 472, 597, 286, 2644, 3932, 281, 362, 264, 558, 1867, 281, 13, 400, 51460], "temperature": 0.0, "avg_logprob": -0.10116216114589147, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0064758798107504845}, {"id": 2060, "seek": 1186720, "start": 11889.12, "end": 11895.2, "text": " my definition is I guess a little bit skewed by the specific research that I do and the", "tokens": [51460, 452, 7123, 307, 286, 2041, 257, 707, 857, 8756, 26896, 538, 264, 2685, 2132, 300, 286, 360, 293, 264, 51764], "temperature": 0.0, "avg_logprob": -0.10116216114589147, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0064758798107504845}, {"id": 2061, "seek": 1189520, "start": 11895.2, "end": 11901.2, "text": " engineering approaches that I do. But I think in large, I agree with the idea of analogy making,", "tokens": [50364, 7043, 11587, 300, 286, 360, 13, 583, 286, 519, 294, 2416, 11, 286, 3986, 365, 264, 1558, 295, 21663, 1455, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07045230325662864, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.008975916542112827}, {"id": 2062, "seek": 1189520, "start": 11901.2, "end": 11906.800000000001, "text": " and maybe I would take it a step further, right? Where you have a particular set of", "tokens": [50664, 293, 1310, 286, 576, 747, 309, 257, 1823, 3052, 11, 558, 30, 2305, 291, 362, 257, 1729, 992, 295, 50944], "temperature": 0.0, "avg_logprob": -0.07045230325662864, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.008975916542112827}, {"id": 2063, "seek": 1189520, "start": 11906.800000000001, "end": 11912.560000000001, "text": " knowledge and conclusions that you've derived so far, a set of primitives that you can use once", "tokens": [50944, 3601, 293, 22865, 300, 291, 600, 18949, 370, 1400, 11, 257, 992, 295, 2886, 38970, 300, 291, 393, 764, 1564, 51232], "temperature": 0.0, "avg_logprob": -0.07045230325662864, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.008975916542112827}, {"id": 2064, "seek": 1189520, "start": 11912.560000000001, "end": 11917.28, "text": " your information comes in to figure out how to recompose them and either discover new analogies", "tokens": [51232, 428, 1589, 1487, 294, 281, 2573, 484, 577, 281, 48000, 541, 552, 293, 2139, 4411, 777, 16660, 530, 51468], "temperature": 0.0, "avg_logprob": -0.07045230325662864, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.008975916542112827}, {"id": 2065, "seek": 1189520, "start": 11917.28, "end": 11921.92, "text": " or just discover new conclusions that you can use in the next step of reasoning.", "tokens": [51468, 420, 445, 4411, 777, 22865, 300, 291, 393, 764, 294, 264, 958, 1823, 295, 21577, 13, 51700], "temperature": 0.0, "avg_logprob": -0.07045230325662864, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.008975916542112827}, {"id": 2066, "seek": 1192192, "start": 11922.56, "end": 11930.32, "text": " And it just feels really amenable to a kind of synergy of, as Daniel Kahneman puts it,", "tokens": [50396, 400, 309, 445, 3417, 534, 18497, 712, 281, 257, 733, 295, 50163, 295, 11, 382, 8033, 591, 12140, 15023, 8137, 309, 11, 50784], "temperature": 0.0, "avg_logprob": -0.09300532451895781, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.009853930212557316}, {"id": 2067, "seek": 1192192, "start": 11930.32, "end": 11935.68, "text": " System 1 and System 2, right? You have the perceptive component that feeds in the raw", "tokens": [50784, 8910, 502, 293, 8910, 568, 11, 558, 30, 509, 362, 264, 43276, 488, 6542, 300, 23712, 294, 264, 8936, 51052], "temperature": 0.0, "avg_logprob": -0.09300532451895781, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.009853930212557316}, {"id": 2068, "seek": 1192192, "start": 11935.68, "end": 11941.04, "text": " information that you receive as your input data, transforms it into some kind of abstract", "tokens": [51052, 1589, 300, 291, 4774, 382, 428, 4846, 1412, 11, 35592, 309, 666, 512, 733, 295, 12649, 51320], "temperature": 0.0, "avg_logprob": -0.09300532451895781, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.009853930212557316}, {"id": 2069, "seek": 1192192, "start": 11941.04, "end": 11947.28, "text": " conceptual information. And then in the System 2 land, you have access to this kind of reasoning", "tokens": [51320, 24106, 1589, 13, 400, 550, 294, 264, 8910, 568, 2117, 11, 291, 362, 2105, 281, 341, 733, 295, 21577, 51632], "temperature": 0.0, "avg_logprob": -0.09300532451895781, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.009853930212557316}, {"id": 2070, "seek": 1194728, "start": 11947.28, "end": 11953.28, "text": " procedures that are able to take all of these concepts and derive new ones from hopefully a", "tokens": [50364, 13846, 300, 366, 1075, 281, 747, 439, 295, 613, 10392, 293, 28446, 777, 2306, 490, 4696, 257, 50664], "temperature": 0.0, "avg_logprob": -0.081881959097726, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.041420891880989075}, {"id": 2071, "seek": 1194728, "start": 11953.28, "end": 11960.320000000002, "text": " nice and not very high dimensional set of roles. And this is why I believe that if we, that in", "tokens": [50664, 1481, 293, 406, 588, 1090, 18795, 992, 295, 9604, 13, 400, 341, 307, 983, 286, 1697, 300, 498, 321, 11, 300, 294, 51016], "temperature": 0.0, "avg_logprob": -0.081881959097726, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.041420891880989075}, {"id": 2072, "seek": 1194728, "start": 11960.320000000002, "end": 11966.24, "text": " terms of like moving towards the, an architecture that supports something like this, I think we", "tokens": [51016, 2115, 295, 411, 2684, 3030, 264, 11, 364, 9482, 300, 9346, 746, 411, 341, 11, 286, 519, 321, 51312], "temperature": 0.0, "avg_logprob": -0.081881959097726, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.041420891880989075}, {"id": 2073, "seek": 1194728, "start": 11966.24, "end": 11970.0, "text": " have a lot of the building blocks in place with geometric deep learning, especially if we're", "tokens": [51312, 362, 257, 688, 295, 264, 2390, 8474, 294, 1081, 365, 33246, 2452, 2539, 11, 2318, 498, 321, 434, 51500], "temperature": 0.0, "avg_logprob": -0.081881959097726, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.041420891880989075}, {"id": 2074, "seek": 1194728, "start": 11970.0, "end": 11974.08, "text": " willing to, as I said, kind of broaden the definition of geometric deep learning to also", "tokens": [51500, 4950, 281, 11, 382, 286, 848, 11, 733, 295, 47045, 264, 7123, 295, 33246, 2452, 2539, 281, 611, 51704], "temperature": 0.0, "avg_logprob": -0.081881959097726, "compression_ratio": 1.681159420289855, "no_speech_prob": 0.041420891880989075}, {"id": 2075, "seek": 1197408, "start": 11974.08, "end": 11979.2, "text": " include category theory concepts because that might allow us to reconcile algorithmic computation", "tokens": [50364, 4090, 7719, 5261, 10392, 570, 300, 1062, 2089, 505, 281, 41059, 9284, 299, 24903, 50620], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2076, "seek": 1197408, "start": 11979.2, "end": 11984.8, "text": " as well into the blueprint. So the idea is, you know, you have this, I mean, there's no need to", "tokens": [50620, 382, 731, 666, 264, 35868, 13, 407, 264, 1558, 307, 11, 291, 458, 11, 291, 362, 341, 11, 286, 914, 11, 456, 311, 572, 643, 281, 50900], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2077, "seek": 1197408, "start": 11984.8, "end": 11989.76, "text": " talk at length about all these great perceptive architectures. So I think we're already at a", "tokens": [50900, 751, 412, 4641, 466, 439, 613, 869, 43276, 488, 6331, 1303, 13, 407, 286, 519, 321, 434, 1217, 412, 257, 51148], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2078, "seek": 1197408, "start": 11989.76, "end": 11995.84, "text": " point of, if we show our AGI lots and lots of data, it's going to be able to pick up on a lot of", "tokens": [51148, 935, 295, 11, 498, 321, 855, 527, 316, 26252, 3195, 293, 3195, 295, 1412, 11, 309, 311, 516, 281, 312, 1075, 281, 1888, 493, 322, 257, 688, 295, 51452], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2079, "seek": 1197408, "start": 11995.84, "end": 11999.28, "text": " the interesting things just by observing, like, you know, self-supervised learning,", "tokens": [51452, 264, 1880, 721, 445, 538, 22107, 11, 411, 11, 291, 458, 11, 2698, 12, 48172, 24420, 2539, 11, 51624], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2080, "seek": 1197408, "start": 11999.28, "end": 12003.68, "text": " unsupervised learning is already showing a lot of promise there. But then the question is,", "tokens": [51624, 2693, 12879, 24420, 2539, 307, 1217, 4099, 257, 688, 295, 6228, 456, 13, 583, 550, 264, 1168, 307, 11, 51844], "temperature": 0.0, "avg_logprob": -0.06847900280849539, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.008441573940217495}, {"id": 2081, "seek": 1200368, "start": 12003.68, "end": 12008.4, "text": " where I think we still have quite a bit of work to do is once we have these concepts,", "tokens": [50364, 689, 286, 519, 321, 920, 362, 1596, 257, 857, 295, 589, 281, 360, 307, 1564, 321, 362, 613, 10392, 11, 50600], "temperature": 0.0, "avg_logprob": -0.0741423826951247, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.011153502389788628}, {"id": 2082, "seek": 1200368, "start": 12008.4, "end": 12013.04, "text": " let's even assume that they're perfect. What do we do with them? How do we meaningfully use them?", "tokens": [50600, 718, 311, 754, 6552, 300, 436, 434, 2176, 13, 708, 360, 321, 360, 365, 552, 30, 1012, 360, 321, 3620, 2277, 764, 552, 30, 50832], "temperature": 0.0, "avg_logprob": -0.0741423826951247, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.011153502389788628}, {"id": 2083, "seek": 1200368, "start": 12013.04, "end": 12017.76, "text": " And I think the reason why I believe there's a lot of work to be done there is because one of the", "tokens": [50832, 400, 286, 519, 264, 1778, 983, 286, 1697, 456, 311, 257, 688, 295, 589, 281, 312, 1096, 456, 307, 570, 472, 295, 264, 51068], "temperature": 0.0, "avg_logprob": -0.0741423826951247, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.011153502389788628}, {"id": 2084, "seek": 1200368, "start": 12018.32, "end": 12023.44, "text": " very key things that I do on a day to day basis is teach graph neural networks to imitate algorithms", "tokens": [51096, 588, 2141, 721, 300, 286, 360, 322, 257, 786, 281, 786, 5143, 307, 2924, 4295, 18161, 9590, 281, 35556, 14642, 51352], "temperature": 0.0, "avg_logprob": -0.0741423826951247, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.011153502389788628}, {"id": 2085, "seek": 1200368, "start": 12023.44, "end": 12028.32, "text": " from perfect data. So I give them exactly the abstract input that the algorithm would expect.", "tokens": [51352, 490, 2176, 1412, 13, 407, 286, 976, 552, 2293, 264, 12649, 4846, 300, 264, 9284, 576, 2066, 13, 51596], "temperature": 0.0, "avg_logprob": -0.0741423826951247, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.011153502389788628}, {"id": 2086, "seek": 1202832, "start": 12028.32, "end": 12034.8, "text": " And I ask them, hey, simulate this algorithm for me, please. And it turns out that that is super,", "tokens": [50364, 400, 286, 1029, 552, 11, 4177, 11, 27817, 341, 9284, 337, 385, 11, 1767, 13, 400, 309, 4523, 484, 300, 300, 307, 1687, 11, 50688], "temperature": 0.0, "avg_logprob": -0.06615800216418355, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0510578453540802}, {"id": 2087, "seek": 1202832, "start": 12034.8, "end": 12039.279999999999, "text": " super hard. Well, it's super easy to do it in distribution, but you're not algorithmic if you", "tokens": [50688, 1687, 1152, 13, 1042, 11, 309, 311, 1687, 1858, 281, 360, 309, 294, 7316, 11, 457, 291, 434, 406, 9284, 299, 498, 291, 50912], "temperature": 0.0, "avg_logprob": -0.06615800216418355, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0510578453540802}, {"id": 2088, "seek": 1202832, "start": 12039.279999999999, "end": 12044.4, "text": " don't extrapolate. And that's, I think, one of the big challenges that we need to work towards", "tokens": [50912, 500, 380, 48224, 473, 13, 400, 300, 311, 11, 286, 519, 11, 472, 295, 264, 955, 4759, 300, 321, 643, 281, 589, 3030, 51168], "temperature": 0.0, "avg_logprob": -0.06615800216418355, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0510578453540802}, {"id": 2089, "seek": 1202832, "start": 12044.4, "end": 12050.16, "text": " addressing. Will geometric deep learning be enough to encompass the ultimate solution? I have a", "tokens": [51168, 14329, 13, 3099, 33246, 2452, 2539, 312, 1547, 281, 28268, 264, 9705, 3827, 30, 286, 362, 257, 51456], "temperature": 0.0, "avg_logprob": -0.06615800216418355, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0510578453540802}, {"id": 2090, "seek": 1202832, "start": 12050.16, "end": 12055.92, "text": " feeling that it will. But, you know, I don't necessarily just based on the empirical evidence", "tokens": [51456, 2633, 300, 309, 486, 13, 583, 11, 291, 458, 11, 286, 500, 380, 4725, 445, 2361, 322, 264, 31886, 4467, 51744], "temperature": 0.0, "avg_logprob": -0.06615800216418355, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0510578453540802}, {"id": 2091, "seek": 1205592, "start": 12055.92, "end": 12062.08, "text": " we've been seeing in the recent papers that we've put out. But yeah, I don't I don't have a very", "tokens": [50364, 321, 600, 668, 2577, 294, 264, 5162, 10577, 300, 321, 600, 829, 484, 13, 583, 1338, 11, 286, 500, 380, 286, 500, 380, 362, 257, 588, 50672], "temperature": 0.0, "avg_logprob": -0.08893531911513385, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.00797568541020155}, {"id": 2092, "seek": 1205592, "start": 12062.08, "end": 12068.960000000001, "text": " strong theoretical reason why I think it's going to be enough. Yeah, I'm fascinated by this notion", "tokens": [50672, 2068, 20864, 1778, 983, 286, 519, 309, 311, 516, 281, 312, 1547, 13, 865, 11, 286, 478, 24597, 538, 341, 10710, 51016], "temperature": 0.0, "avg_logprob": -0.08893531911513385, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.00797568541020155}, {"id": 2093, "seek": 1205592, "start": 12068.960000000001, "end": 12075.12, "text": " that intelligence isn't mysterious as we might think it is. It's a it's a receding horizon. And", "tokens": [51016, 300, 7599, 1943, 380, 13831, 382, 321, 1062, 519, 309, 307, 13, 467, 311, 257, 309, 311, 257, 850, 9794, 18046, 13, 400, 51324], "temperature": 0.0, "avg_logprob": -0.08893531911513385, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.00797568541020155}, {"id": 2094, "seek": 1205592, "start": 12075.12, "end": 12082.48, "text": " it might in the end be disappointingly simple to mechanize. Actually, if you take the term literally,", "tokens": [51324, 309, 1062, 294, 264, 917, 312, 8505, 12163, 2199, 281, 4236, 1125, 13, 5135, 11, 498, 291, 747, 264, 1433, 3736, 11, 51692], "temperature": 0.0, "avg_logprob": -0.08893531911513385, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.00797568541020155}, {"id": 2095, "seek": 1208248, "start": 12082.48, "end": 12088.16, "text": " intelligence come from the Latin word that means to understand. And I think what is meant by", "tokens": [50364, 7599, 808, 490, 264, 10803, 1349, 300, 1355, 281, 1223, 13, 400, 286, 519, 437, 307, 4140, 538, 50648], "temperature": 0.0, "avg_logprob": -0.11245066511864756, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.004279168788343668}, {"id": 2096, "seek": 1208248, "start": 12088.16, "end": 12092.16, "text": " understanding is really a very vague question. And probably if you ask different people, they", "tokens": [50648, 3701, 307, 534, 257, 588, 24247, 1168, 13, 400, 1391, 498, 291, 1029, 819, 561, 11, 436, 50848], "temperature": 0.0, "avg_logprob": -0.11245066511864756, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.004279168788343668}, {"id": 2097, "seek": 1208248, "start": 12092.16, "end": 12098.16, "text": " will give you different definitions. I will define it as the faculty to abstract information.", "tokens": [50848, 486, 976, 291, 819, 21988, 13, 286, 486, 6964, 309, 382, 264, 6389, 281, 12649, 1589, 13, 51148], "temperature": 0.0, "avg_logprob": -0.11245066511864756, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.004279168788343668}, {"id": 2098, "seek": 1208248, "start": 12098.8, "end": 12103.279999999999, "text": " And in particular, information that is obtained in one context, the ability to use it in other", "tokens": [51180, 400, 294, 1729, 11, 1589, 300, 307, 14879, 294, 472, 4319, 11, 264, 3485, 281, 764, 309, 294, 661, 51404], "temperature": 0.0, "avg_logprob": -0.11245066511864756, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.004279168788343668}, {"id": 2099, "seek": 1208248, "start": 12103.279999999999, "end": 12109.119999999999, "text": " contexts, this is what we usually call learning. The way that this information is abstracted and", "tokens": [51404, 30628, 11, 341, 307, 437, 321, 2673, 818, 2539, 13, 440, 636, 300, 341, 1589, 307, 12649, 292, 293, 51696], "temperature": 0.0, "avg_logprob": -0.11245066511864756, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.004279168788343668}, {"id": 2100, "seek": 1210912, "start": 12109.12, "end": 12114.0, "text": " represented might be very different in a biological neural network in our brain versus an", "tokens": [50364, 10379, 1062, 312, 588, 819, 294, 257, 13910, 18161, 3209, 294, 527, 3567, 5717, 364, 50608], "temperature": 0.0, "avg_logprob": -0.07243387198742525, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.010350617580115795}, {"id": 2101, "seek": 1210912, "start": 12114.0, "end": 12121.52, "text": " artificial intelligence system. So if you hear some people saying that that the brain probably", "tokens": [50608, 11677, 7599, 1185, 13, 407, 498, 291, 1568, 512, 561, 1566, 300, 300, 264, 3567, 1391, 50984], "temperature": 0.0, "avg_logprob": -0.07243387198742525, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.010350617580115795}, {"id": 2102, "seek": 1210912, "start": 12121.52, "end": 12128.640000000001, "text": " doesn't really do geometric computations, my answer to that would be that we don't necessarily need", "tokens": [50984, 1177, 380, 534, 360, 33246, 2807, 763, 11, 452, 1867, 281, 300, 576, 312, 300, 321, 500, 380, 4725, 643, 51340], "temperature": 0.0, "avg_logprob": -0.07243387198742525, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.010350617580115795}, {"id": 2103, "seek": 1210912, "start": 12128.640000000001, "end": 12135.76, "text": " to imitate exactly the way that the brain works. We just need probably to try to achieve this high", "tokens": [51340, 281, 35556, 2293, 264, 636, 300, 264, 3567, 1985, 13, 492, 445, 643, 1391, 281, 853, 281, 4584, 341, 1090, 51696], "temperature": 0.0, "avg_logprob": -0.07243387198742525, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.010350617580115795}, {"id": 2104, "seek": 1213576, "start": 12135.76, "end": 12141.6, "text": " level mechanism that is able to abstract information and applied as knowledge to different", "tokens": [50364, 1496, 7513, 300, 307, 1075, 281, 12649, 1589, 293, 6456, 382, 3601, 281, 819, 50656], "temperature": 0.0, "avg_logprob": -0.14526377916336058, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.007024866994470358}, {"id": 2105, "seek": 1213576, "start": 12141.6, "end": 12148.0, "text": " problems. The definitions of artificial intelligence that are being used, like the famous Turing test,", "tokens": [50656, 2740, 13, 440, 21988, 295, 11677, 7599, 300, 366, 885, 1143, 11, 411, 264, 4618, 314, 1345, 1500, 11, 50976], "temperature": 0.0, "avg_logprob": -0.14526377916336058, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.007024866994470358}, {"id": 2106, "seek": 1213576, "start": 12148.0, "end": 12154.56, "text": " I find is very disturbing that it's very anthropocentric. And it is actually probably very", "tokens": [50976, 286, 915, 307, 588, 21903, 300, 309, 311, 588, 22727, 905, 32939, 13, 400, 309, 307, 767, 1391, 588, 51304], "temperature": 0.0, "avg_logprob": -0.14526377916336058, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.007024866994470358}, {"id": 2107, "seek": 1213576, "start": 12154.56, "end": 12162.24, "text": " characteristic of the human species more broadly. And this way, by judging what is intelligent,", "tokens": [51304, 16282, 295, 264, 1952, 6172, 544, 19511, 13, 400, 341, 636, 11, 538, 23587, 437, 307, 13232, 11, 51688], "temperature": 0.0, "avg_logprob": -0.14526377916336058, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.007024866994470358}, {"id": 2108, "seek": 1216224, "start": 12162.64, "end": 12167.76, "text": " what is not, we might potentially rule out other intelligent species because they are very different", "tokens": [50384, 437, 307, 406, 11, 321, 1062, 7263, 4978, 484, 661, 13232, 6172, 570, 436, 366, 588, 819, 50640], "temperature": 0.0, "avg_logprob": -0.11431305749075753, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.008278364315629005}, {"id": 2109, "seek": 1216224, "start": 12167.76, "end": 12173.039999999999, "text": " from us. I may be obsessed with sperm whales because I'm working on studying their communication.", "tokens": [50640, 490, 505, 13, 286, 815, 312, 16923, 365, 32899, 32403, 570, 286, 478, 1364, 322, 7601, 641, 6101, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11431305749075753, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.008278364315629005}, {"id": 2110, "seek": 1216224, "start": 12173.76, "end": 12177.36, "text": " They are definitely intelligent creatures, but would they pass the Turing test?", "tokens": [50940, 814, 366, 2138, 13232, 12281, 11, 457, 576, 436, 1320, 264, 314, 1345, 1500, 30, 51120], "temperature": 0.0, "avg_logprob": -0.11431305749075753, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.008278364315629005}, {"id": 2111, "seek": 1216224, "start": 12177.92, "end": 12184.48, "text": " Probably not. It's like subjecting a cat to a swimming test or a fish to climbing on a tree.", "tokens": [51148, 9210, 406, 13, 467, 311, 411, 3983, 278, 257, 3857, 281, 257, 11989, 1500, 420, 257, 3506, 281, 14780, 322, 257, 4230, 13, 51476], "temperature": 0.0, "avg_logprob": -0.11431305749075753, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.008278364315629005}, {"id": 2112, "seek": 1218448, "start": 12185.359999999999, "end": 12192.08, "text": " So I would just like to add to Michael's great answer, one quote that I think is very popular", "tokens": [50408, 407, 286, 576, 445, 411, 281, 909, 281, 5116, 311, 869, 1867, 11, 472, 6513, 300, 286, 519, 307, 588, 3743, 50744], "temperature": 0.0, "avg_logprob": -0.14296814703172253, "compression_ratio": 1.625, "no_speech_prob": 0.22184215486049652}, {"id": 2113, "seek": 1218448, "start": 12192.08, "end": 12197.359999999999, "text": " and applies really well in this setting. The question of whether computers can think is about", "tokens": [50744, 293, 13165, 534, 731, 294, 341, 3287, 13, 440, 1168, 295, 1968, 10807, 393, 519, 307, 466, 51008], "temperature": 0.0, "avg_logprob": -0.14296814703172253, "compression_ratio": 1.625, "no_speech_prob": 0.22184215486049652}, {"id": 2114, "seek": 1218448, "start": 12197.359999999999, "end": 12203.359999999999, "text": " as relevant as whether submarines can swim. When you built submarines, you weren't necessarily", "tokens": [51008, 382, 7340, 382, 1968, 48138, 393, 7110, 13, 1133, 291, 3094, 48138, 11, 291, 4999, 380, 4725, 51308], "temperature": 0.0, "avg_logprob": -0.14296814703172253, "compression_ratio": 1.625, "no_speech_prob": 0.22184215486049652}, {"id": 2115, "seek": 1218448, "start": 12203.359999999999, "end": 12207.68, "text": " trying to copy fish. You were solving a problem that was fundamentally slightly different.", "tokens": [51308, 1382, 281, 5055, 3506, 13, 509, 645, 12606, 257, 1154, 300, 390, 17879, 4748, 819, 13, 51524], "temperature": 0.0, "avg_logprob": -0.14296814703172253, "compression_ratio": 1.625, "no_speech_prob": 0.22184215486049652}, {"id": 2116, "seek": 1218448, "start": 12208.32, "end": 12210.96, "text": " So could be relevant in this case as well.", "tokens": [51556, 407, 727, 312, 7340, 294, 341, 1389, 382, 731, 13, 51688], "temperature": 0.0, "avg_logprob": -0.14296814703172253, "compression_ratio": 1.625, "no_speech_prob": 0.22184215486049652}, {"id": 2117, "seek": 1221096, "start": 12211.759999999998, "end": 12215.359999999999, "text": " We spend quite a lot of time on this podcast talking about whether we should have an", "tokens": [50404, 492, 3496, 1596, 257, 688, 295, 565, 322, 341, 7367, 1417, 466, 1968, 321, 820, 362, 364, 50584], "temperature": 0.0, "avg_logprob": -0.1204765300558071, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.004628427792340517}, {"id": 2118, "seek": 1221096, "start": 12215.359999999999, "end": 12220.32, "text": " anthropocentric conception of intelligence. A corporation is intelligent.", "tokens": [50584, 22727, 905, 32939, 30698, 295, 7599, 13, 316, 22197, 307, 13232, 13, 50832], "temperature": 0.0, "avg_logprob": -0.1204765300558071, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.004628427792340517}, {"id": 2119, "seek": 1221096, "start": 12222.08, "end": 12225.839999999998, "text": " I'm starting to come around to the view of embodiment and thinking that there is something", "tokens": [50920, 286, 478, 2891, 281, 808, 926, 281, 264, 1910, 295, 28935, 2328, 293, 1953, 300, 456, 307, 746, 51108], "temperature": 0.0, "avg_logprob": -0.1204765300558071, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.004628427792340517}, {"id": 2120, "seek": 1221096, "start": 12225.839999999998, "end": 12230.8, "text": " very human like about our particular flavour of intelligence. But maybe there is a kind of pure", "tokens": [51108, 588, 1952, 411, 466, 527, 1729, 22190, 295, 7599, 13, 583, 1310, 456, 307, 257, 733, 295, 6075, 51356], "temperature": 0.0, "avg_logprob": -0.1204765300558071, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.004628427792340517}, {"id": 2121, "seek": 1221096, "start": 12230.8, "end": 12236.32, "text": " intelligence as well. And this was the end of my conversation with Tako Kohen. One of the really", "tokens": [51356, 7599, 382, 731, 13, 400, 341, 390, 264, 917, 295, 452, 3761, 365, 9118, 78, 30861, 268, 13, 1485, 295, 264, 534, 51632], "temperature": 0.0, "avg_logprob": -0.1204765300558071, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.004628427792340517}, {"id": 2122, "seek": 1223632, "start": 12236.4, "end": 12243.84, "text": " interesting things is you're getting on to some of the work that you've done are being able to", "tokens": [50368, 1880, 721, 307, 291, 434, 1242, 322, 281, 512, 295, 264, 589, 300, 291, 600, 1096, 366, 885, 1075, 281, 50740], "temperature": 0.0, "avg_logprob": -0.12815112845842228, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.02168574184179306}, {"id": 2123, "seek": 1223632, "start": 12243.84, "end": 12250.8, "text": " think of group convolutions on homogeneous objects like spheres, for example, but also you moved on", "tokens": [50740, 519, 295, 1594, 3754, 15892, 322, 42632, 6565, 411, 41225, 11, 337, 1365, 11, 457, 611, 291, 4259, 322, 51088], "temperature": 0.0, "avg_logprob": -0.12815112845842228, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.02168574184179306}, {"id": 2124, "seek": 1223632, "start": 12250.8, "end": 12260.08, "text": " to irregular objects like any mesh and you looked into things like fibre bundles and local convolutions.", "tokens": [51088, 281, 29349, 6565, 411, 604, 17407, 293, 291, 2956, 666, 721, 411, 36738, 13882, 904, 293, 2654, 3754, 15892, 13, 51552], "temperature": 0.0, "avg_logprob": -0.12815112845842228, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.02168574184179306}, {"id": 2125, "seek": 1223632, "start": 12260.08, "end": 12264.08, "text": " Because these are objects, I think you said a homogeneous object is where you can't", "tokens": [51552, 1436, 613, 366, 6565, 11, 286, 519, 291, 848, 257, 42632, 2657, 307, 689, 291, 393, 380, 51752], "temperature": 0.0, "avg_logprob": -0.12815112845842228, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.02168574184179306}, {"id": 2126, "seek": 1226408, "start": 12264.08, "end": 12268.24, "text": " perform some transformation to get from one place of the object to the other part of the object.", "tokens": [50364, 2042, 512, 9887, 281, 483, 490, 472, 1081, 295, 264, 2657, 281, 264, 661, 644, 295, 264, 2657, 13, 50572], "temperature": 0.0, "avg_logprob": -0.11380078028706671, "compression_ratio": 1.68, "no_speech_prob": 0.015156522393226624}, {"id": 2127, "seek": 1226408, "start": 12268.24, "end": 12270.88, "text": " So what work did you do there on those irregular objects?", "tokens": [50572, 407, 437, 589, 630, 291, 360, 456, 322, 729, 29349, 6565, 30, 50704], "temperature": 0.0, "avg_logprob": -0.11380078028706671, "compression_ratio": 1.68, "no_speech_prob": 0.015156522393226624}, {"id": 2128, "seek": 1226408, "start": 12272.0, "end": 12276.88, "text": " Yeah, that's a great question. So when we think of convolution, we're sort of", "tokens": [50760, 865, 11, 300, 311, 257, 869, 1168, 13, 407, 562, 321, 519, 295, 45216, 11, 321, 434, 1333, 295, 51004], "temperature": 0.0, "avg_logprob": -0.11380078028706671, "compression_ratio": 1.68, "no_speech_prob": 0.015156522393226624}, {"id": 2129, "seek": 1226408, "start": 12276.88, "end": 12283.2, "text": " putting together a whole bunch of things, namely this idea of locality. So typically our filters", "tokens": [51004, 3372, 1214, 257, 1379, 3840, 295, 721, 11, 20926, 341, 1558, 295, 1628, 1860, 13, 407, 5850, 527, 15995, 51320], "temperature": 0.0, "avg_logprob": -0.11380078028706671, "compression_ratio": 1.68, "no_speech_prob": 0.015156522393226624}, {"id": 2130, "seek": 1226408, "start": 12283.2, "end": 12290.0, "text": " are local, but that's a choice ultimately. Convolution doesn't have to use a local filter,", "tokens": [51320, 366, 2654, 11, 457, 300, 311, 257, 3922, 6284, 13, 2656, 85, 3386, 1177, 380, 362, 281, 764, 257, 2654, 6608, 11, 51660], "temperature": 0.0, "avg_logprob": -0.11380078028706671, "compression_ratio": 1.68, "no_speech_prob": 0.015156522393226624}, {"id": 2131, "seek": 1229000, "start": 12290.08, "end": 12296.24, "text": " though in practice we know that works very well. And there's the idea of weight sharing between", "tokens": [50368, 1673, 294, 3124, 321, 458, 300, 1985, 588, 731, 13, 400, 456, 311, 264, 1558, 295, 3364, 5414, 1296, 50676], "temperature": 0.0, "avg_logprob": -0.10167227293315687, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.05093827471137047}, {"id": 2132, "seek": 1229000, "start": 12296.24, "end": 12300.96, "text": " different positions and potentially also between different orientations of the filter.", "tokens": [50676, 819, 8432, 293, 7263, 611, 1296, 819, 8579, 763, 295, 264, 6608, 13, 50912], "temperature": 0.0, "avg_logprob": -0.10167227293315687, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.05093827471137047}, {"id": 2133, "seek": 1229000, "start": 12301.68, "end": 12308.8, "text": " And as I mentioned before, this weight sharing really comes from the symmetry.", "tokens": [50948, 400, 382, 286, 2835, 949, 11, 341, 3364, 5414, 534, 1487, 490, 264, 25440, 13, 51304], "temperature": 0.0, "avg_logprob": -0.10167227293315687, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.05093827471137047}, {"id": 2134, "seek": 1229000, "start": 12309.92, "end": 12315.68, "text": " So the fact that you use the same filter at each position in your image when you're doing", "tokens": [51360, 407, 264, 1186, 300, 291, 764, 264, 912, 6608, 412, 1184, 2535, 294, 428, 3256, 562, 291, 434, 884, 51648], "temperature": 0.0, "avg_logprob": -0.10167227293315687, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.05093827471137047}, {"id": 2135, "seek": 1231568, "start": 12315.68, "end": 12322.4, "text": " a two-dimensional convolution, that's because you want to respect the translation symmetry acting", "tokens": [50364, 257, 732, 12, 18759, 45216, 11, 300, 311, 570, 291, 528, 281, 3104, 264, 12853, 25440, 6577, 50700], "temperature": 0.0, "avg_logprob": -0.10429777724019597, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0156523697078228}, {"id": 2136, "seek": 1231568, "start": 12322.4, "end": 12330.56, "text": " on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry.", "tokens": [50700, 322, 264, 5720, 13, 682, 264, 1389, 295, 257, 2674, 47138, 420, 17407, 11, 291, 5850, 406, 362, 257, 4338, 25440, 13, 51108], "temperature": 0.0, "avg_logprob": -0.10429777724019597, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0156523697078228}, {"id": 2137, "seek": 1231568, "start": 12331.2, "end": 12337.84, "text": " If you think of a mesh representing a human figure, or let's say it's some complicated protein", "tokens": [51140, 759, 291, 519, 295, 257, 17407, 13460, 257, 1952, 2573, 11, 420, 718, 311, 584, 309, 311, 512, 6179, 7944, 51472], "temperature": 0.0, "avg_logprob": -0.10429777724019597, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0156523697078228}, {"id": 2138, "seek": 1231568, "start": 12337.84, "end": 12345.2, "text": " structure, there may not be a global symmetry. Or sometimes in the case of say a molecule might", "tokens": [51472, 3877, 11, 456, 815, 406, 312, 257, 4338, 25440, 13, 1610, 2171, 294, 264, 1389, 295, 584, 257, 15582, 1062, 51840], "temperature": 0.0, "avg_logprob": -0.10429777724019597, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0156523697078228}, {"id": 2139, "seek": 1234520, "start": 12345.2, "end": 12351.84, "text": " have some six-fold rotational symmetry, this symmetry may not be transitive as it's called,", "tokens": [50364, 362, 512, 2309, 12, 18353, 45420, 25440, 11, 341, 25440, 815, 406, 312, 1145, 2187, 382, 309, 311, 1219, 11, 50696], "temperature": 0.0, "avg_logprob": -0.10457653246427837, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0029346125666052103}, {"id": 2140, "seek": 1234520, "start": 12351.84, "end": 12357.84, "text": " meaning you cannot take any two points on your manifold and map one to the other using a symmetry.", "tokens": [50696, 3620, 291, 2644, 747, 604, 732, 2793, 322, 428, 47138, 293, 4471, 472, 281, 264, 661, 1228, 257, 25440, 13, 50996], "temperature": 0.0, "avg_logprob": -0.10457653246427837, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0029346125666052103}, {"id": 2141, "seek": 1234520, "start": 12357.84, "end": 12362.640000000001, "text": " In the case of a sphere, you can do that. Any two points in the sphere are related by rotation.", "tokens": [50996, 682, 264, 1389, 295, 257, 16687, 11, 291, 393, 360, 300, 13, 2639, 732, 2793, 294, 264, 16687, 366, 4077, 538, 12447, 13, 51236], "temperature": 0.0, "avg_logprob": -0.10457653246427837, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0029346125666052103}, {"id": 2142, "seek": 1234520, "start": 12362.640000000001, "end": 12371.2, "text": " So we say a sphere is a homogeneous space, but these let's say this protein shape is not homogeneous,", "tokens": [51236, 407, 321, 584, 257, 16687, 307, 257, 42632, 1901, 11, 457, 613, 718, 311, 584, 341, 7944, 3909, 307, 406, 42632, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10457653246427837, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0029346125666052103}, {"id": 2143, "seek": 1237120, "start": 12371.2, "end": 12378.08, "text": " even if it has some kind of symmetry. So in that case, if you try to motivate the weight sharing", "tokens": [50364, 754, 498, 309, 575, 512, 733, 295, 25440, 13, 407, 294, 300, 1389, 11, 498, 291, 853, 281, 28497, 264, 3364, 5414, 50708], "temperature": 0.0, "avg_logprob": -0.08140850067138672, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.007812850177288055}, {"id": 2144, "seek": 1237120, "start": 12378.08, "end": 12382.240000000002, "text": " via symmetry, you try to take your filter, put it in one position, move it around to a different", "tokens": [50708, 5766, 25440, 11, 291, 853, 281, 747, 428, 6608, 11, 829, 309, 294, 472, 2535, 11, 1286, 309, 926, 281, 257, 819, 50916], "temperature": 0.0, "avg_logprob": -0.08140850067138672, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.007812850177288055}, {"id": 2145, "seek": 1237120, "start": 12382.240000000002, "end": 12386.16, "text": " position using a symmetry, you're not going to be able to hit all positions. So you're not going to", "tokens": [50916, 2535, 1228, 257, 25440, 11, 291, 434, 406, 516, 281, 312, 1075, 281, 2045, 439, 8432, 13, 407, 291, 434, 406, 516, 281, 51112], "temperature": 0.0, "avg_logprob": -0.08140850067138672, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.007812850177288055}, {"id": 2146, "seek": 1237120, "start": 12386.16, "end": 12394.720000000001, "text": " get weight sharing globally. And that just is what it is. If you say I have a signal on this manifold", "tokens": [51112, 483, 3364, 5414, 18958, 13, 400, 300, 445, 307, 437, 309, 307, 13, 759, 291, 584, 286, 362, 257, 6358, 322, 341, 47138, 51540], "temperature": 0.0, "avg_logprob": -0.08140850067138672, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.007812850177288055}, {"id": 2147, "seek": 1239472, "start": 12395.679999999998, "end": 12402.24, "text": " and I want to respect the symmetry, well, if there are no global symmetries, there's nothing to", "tokens": [50412, 293, 286, 528, 281, 3104, 264, 25440, 11, 731, 11, 498, 456, 366, 572, 4338, 14232, 302, 2244, 11, 456, 311, 1825, 281, 50740], "temperature": 0.0, "avg_logprob": -0.1464401391836313, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.014058724045753479}, {"id": 2148, "seek": 1239472, "start": 12402.24, "end": 12406.32, "text": " respect. So you get no code strain. So you can just use arbitrary linear map.", "tokens": [50740, 3104, 13, 407, 291, 483, 572, 3089, 14249, 13, 407, 291, 393, 445, 764, 23211, 8213, 4471, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1464401391836313, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.014058724045753479}, {"id": 2149, "seek": 1239472, "start": 12408.8, "end": 12414.56, "text": " Now, it turns out there are certain other kinds of symmetries called gauge symmetries that you", "tokens": [51068, 823, 11, 309, 4523, 484, 456, 366, 1629, 661, 3685, 295, 14232, 302, 2244, 1219, 17924, 14232, 302, 2244, 300, 291, 51356], "temperature": 0.0, "avg_logprob": -0.1464401391836313, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.014058724045753479}, {"id": 2150, "seek": 1239472, "start": 12414.56, "end": 12424.08, "text": " might still want to respect. And in practice, what respecting gauge symmetry will do is we'll", "tokens": [51356, 1062, 920, 528, 281, 3104, 13, 400, 294, 3124, 11, 437, 41968, 17924, 25440, 486, 360, 307, 321, 603, 51832], "temperature": 0.0, "avg_logprob": -0.1464401391836313, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.014058724045753479}, {"id": 2151, "seek": 1242408, "start": 12424.08, "end": 12430.88, "text": " put some constraints on the filter at a particular position. So for example, that might have to be", "tokens": [50364, 829, 512, 18491, 322, 264, 6608, 412, 257, 1729, 2535, 13, 407, 337, 1365, 11, 300, 1062, 362, 281, 312, 50704], "temperature": 0.0, "avg_logprob": -0.09611236784193251, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.004395500756800175}, {"id": 2152, "seek": 1242408, "start": 12430.88, "end": 12436.56, "text": " a rotationally equivariant filter, but it doesn't tie the weights of filters at different positions.", "tokens": [50704, 257, 12447, 379, 48726, 3504, 394, 6608, 11, 457, 309, 1177, 380, 7582, 264, 17443, 295, 15995, 412, 819, 8432, 13, 50988], "temperature": 0.0, "avg_logprob": -0.09611236784193251, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.004395500756800175}, {"id": 2153, "seek": 1242408, "start": 12438.16, "end": 12445.44, "text": " So if you want that as well, then you can maybe motivate it via some kind of notion of local", "tokens": [51068, 407, 498, 291, 528, 300, 382, 731, 11, 550, 291, 393, 1310, 28497, 309, 5766, 512, 733, 295, 10710, 295, 2654, 51432], "temperature": 0.0, "avg_logprob": -0.09611236784193251, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.004395500756800175}, {"id": 2154, "seek": 1242408, "start": 12445.44, "end": 12450.4, "text": " symmetry. I have something on a local symmetry group point to motivate that in my thesis.", "tokens": [51432, 25440, 13, 286, 362, 746, 322, 257, 2654, 25440, 1594, 935, 281, 28497, 300, 294, 452, 22288, 13, 51680], "temperature": 0.0, "avg_logprob": -0.09611236784193251, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.004395500756800175}, {"id": 2155, "seek": 1245040, "start": 12451.359999999999, "end": 12464.24, "text": " But there isn't a very principled way to motivate weight sharing on general manifolds", "tokens": [50412, 583, 456, 1943, 380, 257, 588, 3681, 15551, 636, 281, 28497, 3364, 5414, 322, 2674, 8173, 31518, 51056], "temperature": 0.0, "avg_logprob": -0.13633578947220726, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024187995586544275}, {"id": 2156, "seek": 1245040, "start": 12464.24, "end": 12469.199999999999, "text": " between different locations. Yeah, I'm just trying to get my head around this. So you're saying,", "tokens": [51056, 1296, 819, 9253, 13, 865, 11, 286, 478, 445, 1382, 281, 483, 452, 1378, 926, 341, 13, 407, 291, 434, 1566, 11, 51304], "temperature": 0.0, "avg_logprob": -0.13633578947220726, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024187995586544275}, {"id": 2157, "seek": 1245040, "start": 12469.199999999999, "end": 12475.52, "text": " because the whole point that we're trying to achieve here is to have a parameter-efficient", "tokens": [51304, 570, 264, 1379, 935, 300, 321, 434, 1382, 281, 4584, 510, 307, 281, 362, 257, 13075, 12, 68, 7816, 51620], "temperature": 0.0, "avg_logprob": -0.13633578947220726, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024187995586544275}, {"id": 2158, "seek": 1245040, "start": 12475.52, "end": 12480.08, "text": " neural network that uses local connectivity and weight sharing, as we do with, let's say,", "tokens": [51620, 18161, 3209, 300, 4960, 2654, 21095, 293, 3364, 5414, 11, 382, 321, 360, 365, 11, 718, 311, 584, 11, 51848], "temperature": 0.0, "avg_logprob": -0.13633578947220726, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024187995586544275}, {"id": 2159, "seek": 1248008, "start": 12480.08, "end": 12484.0, "text": " plain RCNN, whereas when you have an irregular object, it's very, very difficult to do that.", "tokens": [50364, 11121, 28987, 45, 45, 11, 9735, 562, 291, 362, 364, 29349, 2657, 11, 309, 311, 588, 11, 588, 2252, 281, 360, 300, 13, 50560], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2160, "seek": 1248008, "start": 12484.0, "end": 12487.76, "text": " So you're saying, in some restricted domain, you can do it. Let's say if you have a,", "tokens": [50560, 407, 291, 434, 1566, 11, 294, 512, 20608, 9274, 11, 291, 393, 360, 309, 13, 961, 311, 584, 498, 291, 362, 257, 11, 50748], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2161, "seek": 1248008, "start": 12488.64, "end": 12493.6, "text": " let's say, rotation equivariance, but you can't do the other forms of weight sharing.", "tokens": [50792, 718, 311, 584, 11, 12447, 48726, 3504, 719, 11, 457, 291, 393, 380, 360, 264, 661, 6422, 295, 3364, 5414, 13, 51040], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2162, "seek": 1248008, "start": 12494.16, "end": 12497.92, "text": " I'm just trying to get my head around this, because with a graph convolution on your network,", "tokens": [51068, 286, 478, 445, 1382, 281, 483, 452, 1378, 926, 341, 11, 570, 365, 257, 4295, 45216, 322, 428, 3209, 11, 51256], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2163, "seek": 1248008, "start": 12497.92, "end": 12504.08, "text": " for example, it seems like you can abstract the local neighborhood. This node is connected to", "tokens": [51256, 337, 1365, 11, 309, 2544, 411, 291, 393, 12649, 264, 2654, 7630, 13, 639, 9984, 307, 4582, 281, 51564], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2164, "seek": 1248008, "start": 12504.08, "end": 12509.039999999999, "text": " these other nodes. And potentially, that could translate to a different part of the irregular", "tokens": [51564, 613, 661, 13891, 13, 400, 7263, 11, 300, 727, 13799, 281, 257, 819, 644, 295, 264, 29349, 51812], "temperature": 0.0, "avg_logprob": -0.09824578201069552, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.001047669560648501}, {"id": 2165, "seek": 1250904, "start": 12509.12, "end": 12516.400000000001, "text": " mesh. So why can't you do it more than you suggested? I think if you want to be precise,", "tokens": [50368, 17407, 13, 407, 983, 393, 380, 291, 360, 309, 544, 813, 291, 10945, 30, 286, 519, 498, 291, 528, 281, 312, 13600, 11, 50732], "temperature": 0.0, "avg_logprob": -0.10677611097997548, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.005906832404434681}, {"id": 2166, "seek": 1250904, "start": 12516.400000000001, "end": 12522.720000000001, "text": " you just have to say, what are the symmetries that we're talking about here? And in a graph,", "tokens": [50732, 291, 445, 362, 281, 584, 11, 437, 366, 264, 14232, 302, 2244, 300, 321, 434, 1417, 466, 510, 30, 400, 294, 257, 4295, 11, 51048], "temperature": 0.0, "avg_logprob": -0.10677611097997548, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.005906832404434681}, {"id": 2167, "seek": 1250904, "start": 12522.720000000001, "end": 12529.6, "text": " the most obvious one is the global permutation symmetry. So you can reorder the nodes of a graph", "tokens": [51048, 264, 881, 6322, 472, 307, 264, 4338, 4784, 11380, 25440, 13, 407, 291, 393, 319, 4687, 264, 13891, 295, 257, 4295, 51392], "temperature": 0.0, "avg_logprob": -0.10677611097997548, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.005906832404434681}, {"id": 2168, "seek": 1250904, "start": 12530.240000000002, "end": 12536.0, "text": " and really any graph neural net, whether they're coming from an equivariance perspective or not,", "tokens": [51424, 293, 534, 604, 4295, 18161, 2533, 11, 1968, 436, 434, 1348, 490, 364, 48726, 3504, 719, 4585, 420, 406, 11, 51712], "temperature": 0.0, "avg_logprob": -0.10677611097997548, "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.005906832404434681}, {"id": 2169, "seek": 1253600, "start": 12536.0, "end": 12541.6, "text": " all graph neural nets in existence that have been proposed, they respect this permutation", "tokens": [50364, 439, 4295, 18161, 36170, 294, 9123, 300, 362, 668, 10348, 11, 436, 3104, 341, 4784, 11380, 50644], "temperature": 0.0, "avg_logprob": -0.15237896659157493, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0024338497314602137}, {"id": 2170, "seek": 1253600, "start": 12541.6, "end": 12548.64, "text": " symmetry. And typically, this happens through, let's say, in the most simple kind of graph", "tokens": [50644, 25440, 13, 400, 5850, 11, 341, 2314, 807, 11, 718, 311, 584, 11, 294, 264, 881, 2199, 733, 295, 4295, 50996], "temperature": 0.0, "avg_logprob": -0.15237896659157493, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0024338497314602137}, {"id": 2171, "seek": 1253600, "start": 12548.64, "end": 12554.16, "text": " convolutional nets, like the ones by Kip van Belling, for example, there's a sum operation,", "tokens": [50996, 45216, 304, 36170, 11, 411, 264, 2306, 538, 591, 647, 3161, 879, 2669, 11, 337, 1365, 11, 456, 311, 257, 2408, 6916, 11, 51272], "temperature": 0.0, "avg_logprob": -0.15237896659157493, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0024338497314602137}, {"id": 2172, "seek": 1253600, "start": 12554.16, "end": 12560.72, "text": " some messages from your neighbors. And some operation does depend on the border of the summands.", "tokens": [51272, 512, 7897, 490, 428, 12512, 13, 400, 512, 6916, 775, 5672, 322, 264, 7838, 295, 264, 8367, 2967, 13, 51600], "temperature": 0.0, "avg_logprob": -0.15237896659157493, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0024338497314602137}, {"id": 2173, "seek": 1253600, "start": 12560.72, "end": 12564.4, "text": " So it doesn't depend on the border of the neighbors. And that's why the whole thing becomes", "tokens": [51600, 407, 309, 1177, 380, 5672, 322, 264, 7838, 295, 264, 12512, 13, 400, 300, 311, 983, 264, 1379, 551, 3643, 51784], "temperature": 0.0, "avg_logprob": -0.15237896659157493, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.0024338497314602137}, {"id": 2174, "seek": 1256440, "start": 12564.4, "end": 12571.039999999999, "text": " every variant. So that's a global symmetry that all graph networks respect.", "tokens": [50364, 633, 17501, 13, 407, 300, 311, 257, 4338, 25440, 300, 439, 4295, 9590, 3104, 13, 50696], "temperature": 0.0, "avg_logprob": -0.1458006440923455, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.005054194945842028}, {"id": 2175, "seek": 1256440, "start": 12571.039999999999, "end": 12577.92, "text": " On that, though, could you not create a local, let's say if there was a local graph isomorphism,", "tokens": [50696, 1282, 300, 11, 1673, 11, 727, 291, 406, 1884, 257, 2654, 11, 718, 311, 584, 498, 456, 390, 257, 2654, 4295, 307, 32702, 1434, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1458006440923455, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.005054194945842028}, {"id": 2176, "seek": 1256440, "start": 12577.92, "end": 12583.52, "text": " and so I have an irregular object, but it has a local isomorphism, could I not use something like", "tokens": [51040, 293, 370, 286, 362, 364, 29349, 2657, 11, 457, 309, 575, 257, 2654, 307, 32702, 1434, 11, 727, 286, 406, 764, 746, 411, 51320], "temperature": 0.0, "avg_logprob": -0.1458006440923455, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.005054194945842028}, {"id": 2177, "seek": 1256440, "start": 12583.52, "end": 12586.96, "text": " a GCN, a local version of it to capture that isomorphism?", "tokens": [51320, 257, 29435, 45, 11, 257, 2654, 3037, 295, 309, 281, 7983, 300, 307, 32702, 1434, 30, 51492], "temperature": 0.0, "avg_logprob": -0.1458006440923455, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.005054194945842028}, {"id": 2178, "seek": 1258696, "start": 12587.679999999998, "end": 12596.32, "text": " Yeah. So actually, this was something we proposed to do in our paper, Natural Graph Networks.", "tokens": [50400, 865, 13, 407, 767, 11, 341, 390, 746, 321, 10348, 281, 360, 294, 527, 3035, 11, 20137, 21884, 12640, 82, 13, 50832], "temperature": 0.0, "avg_logprob": -0.11002118388811748, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.012428649701178074}, {"id": 2179, "seek": 1258696, "start": 12596.32, "end": 12603.039999999999, "text": " So this paper really has two aspects to it. One is the naturality as a generalization of", "tokens": [50832, 407, 341, 3035, 534, 575, 732, 7270, 281, 309, 13, 1485, 307, 264, 3303, 507, 382, 257, 2674, 2144, 295, 51168], "temperature": 0.0, "avg_logprob": -0.11002118388811748, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.012428649701178074}, {"id": 2180, "seek": 1258696, "start": 12603.039999999999, "end": 12610.24, "text": " equivariance, I can talk about that. But another key point was that we can not just develop a", "tokens": [51168, 48726, 3504, 719, 11, 286, 393, 751, 466, 300, 13, 583, 1071, 2141, 935, 390, 300, 321, 393, 406, 445, 1499, 257, 51528], "temperature": 0.0, "avg_logprob": -0.11002118388811748, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.012428649701178074}, {"id": 2181, "seek": 1258696, "start": 12610.24, "end": 12614.88, "text": " global natural graph network, as we call it, but also a local one. And what the local one will do", "tokens": [51528, 4338, 3303, 4295, 3209, 11, 382, 321, 818, 309, 11, 457, 611, 257, 2654, 472, 13, 400, 437, 264, 2654, 472, 486, 360, 51760], "temperature": 0.0, "avg_logprob": -0.11002118388811748, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.012428649701178074}, {"id": 2182, "seek": 1261488, "start": 12615.519999999999, "end": 12624.8, "text": " is it will look at certain local motifs. So maybe if you're analyzing molecular graphs,", "tokens": [50396, 307, 309, 486, 574, 412, 1629, 2654, 2184, 18290, 13, 407, 1310, 498, 291, 434, 23663, 19046, 24877, 11, 50860], "temperature": 0.0, "avg_logprob": -0.10984248667955399, "compression_ratio": 1.4910179640718564, "no_speech_prob": 0.01047310046851635}, {"id": 2183, "seek": 1261488, "start": 12624.8, "end": 12631.439999999999, "text": " one motif that you often find is, you know, aromatic ring or something, some ring with,", "tokens": [50860, 472, 39478, 300, 291, 2049, 915, 307, 11, 291, 458, 11, 45831, 4875, 420, 746, 11, 512, 4875, 365, 11, 51192], "temperature": 0.0, "avg_logprob": -0.10984248667955399, "compression_ratio": 1.4910179640718564, "no_speech_prob": 0.01047310046851635}, {"id": 2184, "seek": 1261488, "start": 12631.439999999999, "end": 12638.96, "text": " let's say, six corners, various other kinds of little small graph motifs.", "tokens": [51192, 718, 311, 584, 11, 2309, 12413, 11, 3683, 661, 3685, 295, 707, 1359, 4295, 2184, 18290, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10984248667955399, "compression_ratio": 1.4910179640718564, "no_speech_prob": 0.01047310046851635}, {"id": 2185, "seek": 1263896, "start": 12639.679999999998, "end": 12647.119999999999, "text": " And these motifs might appear multiple times in the same molecule or across different molecules.", "tokens": [50400, 400, 613, 2184, 18290, 1062, 4204, 3866, 1413, 294, 264, 912, 15582, 420, 2108, 819, 13093, 13, 50772], "temperature": 0.0, "avg_logprob": -0.11347357062406319, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.03566865250468254}, {"id": 2186, "seek": 1263896, "start": 12647.119999999999, "end": 12652.08, "text": " And so what this method is doing is it's essentially finding those using some kind", "tokens": [50772, 400, 370, 437, 341, 3170, 307, 884, 307, 309, 311, 4476, 5006, 729, 1228, 512, 733, 51020], "temperature": 0.0, "avg_logprob": -0.11347357062406319, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.03566865250468254}, {"id": 2187, "seek": 1263896, "start": 12652.08, "end": 12660.56, "text": " of graph isomorph, local graph isomorphism, and then making sure that whenever we encounter", "tokens": [51020, 295, 4295, 307, 32702, 11, 2654, 4295, 307, 32702, 1434, 11, 293, 550, 1455, 988, 300, 5699, 321, 8593, 51444], "temperature": 0.0, "avg_logprob": -0.11347357062406319, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.03566865250468254}, {"id": 2188, "seek": 1263896, "start": 12660.56, "end": 12668.56, "text": " this particular motif, we process it in the same way, i.e. using the same weights. And if the", "tokens": [51444, 341, 1729, 39478, 11, 321, 1399, 309, 294, 264, 912, 636, 11, 741, 13, 68, 13, 1228, 264, 912, 17443, 13, 400, 498, 264, 51844], "temperature": 0.0, "avg_logprob": -0.11347357062406319, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.03566865250468254}, {"id": 2189, "seek": 1266856, "start": 12668.56, "end": 12674.4, "text": " local motif has some kind of symmetry, like this aromatic ring, you can rotate it six times,", "tokens": [50364, 2654, 39478, 575, 512, 733, 295, 25440, 11, 411, 341, 45831, 4875, 11, 291, 393, 13121, 309, 2309, 1413, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1010622311663884, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0038229625206440687}, {"id": 2190, "seek": 1266856, "start": 12674.4, "end": 12679.439999999999, "text": " and it gets back to the origin or you can flip it over. So that's the symmetry of this graph", "tokens": [50656, 293, 309, 2170, 646, 281, 264, 4957, 420, 291, 393, 7929, 309, 670, 13, 407, 300, 311, 264, 25440, 295, 341, 4295, 50908], "temperature": 0.0, "avg_logprob": -0.1010622311663884, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0038229625206440687}, {"id": 2191, "seek": 1266856, "start": 12679.439999999999, "end": 12685.199999999999, "text": " structure or an automorphism of this graph. And then the weights will also be constrained by this", "tokens": [50908, 3877, 420, 364, 3553, 18191, 1434, 295, 341, 4295, 13, 400, 550, 264, 17443, 486, 611, 312, 38901, 538, 341, 51196], "temperature": 0.0, "avg_logprob": -0.1010622311663884, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0038229625206440687}, {"id": 2192, "seek": 1266856, "start": 12685.199999999999, "end": 12691.84, "text": " automorphism group, this group of symmetries of the local motif. And various other authors also", "tokens": [51196, 3553, 18191, 1434, 1594, 11, 341, 1594, 295, 14232, 302, 2244, 295, 264, 2654, 39478, 13, 400, 3683, 661, 16552, 611, 51528], "temperature": 0.0, "avg_logprob": -0.1010622311663884, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0038229625206440687}, {"id": 2193, "seek": 1269184, "start": 12691.92, "end": 12698.64, "text": " have, I think, even Michael Bronstein and students have developed methods based on", "tokens": [50368, 362, 11, 286, 519, 11, 754, 5116, 19544, 9089, 293, 1731, 362, 4743, 7150, 2361, 322, 50704], "temperature": 0.0, "avg_logprob": -0.17306144084405461, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0314599834382534}, {"id": 2194, "seek": 1269184, "start": 12698.64, "end": 12704.8, "text": " similar ideas. Awesome. Taiko, it's been such an honor having you on the show. And actually,", "tokens": [50704, 2531, 3487, 13, 10391, 13, 6551, 10770, 11, 309, 311, 668, 1270, 364, 5968, 1419, 291, 322, 264, 855, 13, 400, 767, 11, 51012], "temperature": 0.0, "avg_logprob": -0.17306144084405461, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0314599834382534}, {"id": 2195, "seek": 1269184, "start": 12704.8, "end": 12708.960000000001, "text": " you're coming back on the show in a few weeks' time, so we don't want to spoil the surprise.", "tokens": [51012, 291, 434, 1348, 646, 322, 264, 855, 294, 257, 1326, 3259, 6, 565, 11, 370, 321, 500, 380, 528, 281, 18630, 264, 6365, 13, 51220], "temperature": 0.0, "avg_logprob": -0.17306144084405461, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0314599834382534}, {"id": 2196, "seek": 1269184, "start": 12708.960000000001, "end": 12714.08, "text": " But looking on this proto book that you've written with the other folks, what's the main", "tokens": [51220, 583, 1237, 322, 341, 47896, 1446, 300, 291, 600, 3720, 365, 264, 661, 4024, 11, 437, 311, 264, 2135, 51476], "temperature": 0.0, "avg_logprob": -0.17306144084405461, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0314599834382534}, {"id": 2197, "seek": 1269184, "start": 12714.08, "end": 12716.8, "text": " thing that sticks out to you as being the coolest thing in the book?", "tokens": [51476, 551, 300, 12518, 484, 281, 291, 382, 885, 264, 22013, 551, 294, 264, 1446, 30, 51612], "temperature": 0.0, "avg_logprob": -0.17306144084405461, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0314599834382534}, {"id": 2198, "seek": 1271680, "start": 12717.679999999998, "end": 12729.759999999998, "text": " I think there's any one particular thing. What excites me is to put some order to the chaos", "tokens": [50408, 286, 519, 456, 311, 604, 472, 1729, 551, 13, 708, 1624, 3324, 385, 307, 281, 829, 512, 1668, 281, 264, 14158, 51012], "temperature": 0.0, "avg_logprob": -0.11709628786359515, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.007674206979572773}, {"id": 2199, "seek": 1271680, "start": 12730.48, "end": 12737.92, "text": " of the zoo of architectures and to see, actually, that there is something that they all have in", "tokens": [51048, 295, 264, 25347, 295, 6331, 1303, 293, 281, 536, 11, 767, 11, 300, 456, 307, 746, 300, 436, 439, 362, 294, 51420], "temperature": 0.0, "avg_logprob": -0.11709628786359515, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.007674206979572773}, {"id": 2200, "seek": 1271680, "start": 12737.92, "end": 12745.679999999998, "text": " common. And I really think this can help new people who are new to the field to learn more", "tokens": [51420, 2689, 13, 400, 286, 534, 519, 341, 393, 854, 777, 561, 567, 366, 777, 281, 264, 2519, 281, 1466, 544, 51808], "temperature": 0.0, "avg_logprob": -0.11709628786359515, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.007674206979572773}, {"id": 2201, "seek": 1274568, "start": 12745.68, "end": 12753.68, "text": " quickly, to get an overview of all the things that are out there. And I also think that this is", "tokens": [50364, 2661, 11, 281, 483, 364, 12492, 295, 439, 264, 721, 300, 366, 484, 456, 13, 400, 286, 611, 519, 300, 341, 307, 50764], "temperature": 0.0, "avg_logprob": -0.09315793854849679, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.02833038941025734}, {"id": 2202, "seek": 1274568, "start": 12753.68, "end": 12763.44, "text": " the start of at least one way in which we can take the black box of deep learning, which often is", "tokens": [50764, 264, 722, 295, 412, 1935, 472, 636, 294, 597, 321, 393, 747, 264, 2211, 2424, 295, 2452, 2539, 11, 597, 2049, 307, 51252], "temperature": 0.0, "avg_logprob": -0.09315793854849679, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.02833038941025734}, {"id": 2203, "seek": 1274568, "start": 12763.44, "end": 12769.28, "text": " viewed as completely inscrutable and actually start to open it and start to understand how the", "tokens": [51252, 19174, 382, 2584, 1028, 10757, 32148, 293, 767, 722, 281, 1269, 309, 293, 722, 281, 1223, 577, 264, 51544], "temperature": 0.0, "avg_logprob": -0.09315793854849679, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.02833038941025734}, {"id": 2204, "seek": 1276928, "start": 12769.28, "end": 12777.12, "text": " pieces connect, which can then perhaps inform future developments that are guided by both", "tokens": [50364, 3755, 1745, 11, 597, 393, 550, 4317, 1356, 2027, 20862, 300, 366, 19663, 538, 1293, 50756], "temperature": 0.0, "avg_logprob": -0.14521955239652384, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.10217004269361496}, {"id": 2205, "seek": 1276928, "start": 12777.12, "end": 12783.92, "text": " empirical results and an understanding of what's going on. Amazing. Thanks so much, Taiko.", "tokens": [50756, 31886, 3542, 293, 364, 3701, 295, 437, 311, 516, 322, 13, 14165, 13, 2561, 370, 709, 11, 6551, 10770, 13, 51096], "temperature": 0.0, "avg_logprob": -0.14521955239652384, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.10217004269361496}, {"id": 2206, "seek": 1276928, "start": 12784.960000000001, "end": 12786.480000000001, "text": " Thanks for having me. It's been a pleasure.", "tokens": [51148, 2561, 337, 1419, 385, 13, 467, 311, 668, 257, 6834, 13, 51224], "temperature": 0.0, "avg_logprob": -0.14521955239652384, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.10217004269361496}, {"id": 2207, "seek": 1276928, "start": 12787.12, "end": 12789.84, "text": " Joan, thank you so much for joining us. This has been amazing.", "tokens": [51256, 3139, 282, 11, 1309, 291, 370, 709, 337, 5549, 505, 13, 639, 575, 668, 2243, 13, 51392], "temperature": 0.0, "avg_logprob": -0.14521955239652384, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.10217004269361496}, {"id": 2208, "seek": 1276928, "start": 12790.400000000001, "end": 12795.04, "text": " Okay, no, thank you so much, Tim. It was very fun and best of luck. And I think I'm", "tokens": [51420, 1033, 11, 572, 11, 1309, 291, 370, 709, 11, 7172, 13, 467, 390, 588, 1019, 293, 1151, 295, 3668, 13, 400, 286, 519, 286, 478, 51652], "temperature": 0.0, "avg_logprob": -0.14521955239652384, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.10217004269361496}, {"id": 2209, "seek": 1279504, "start": 12795.04, "end": 12800.560000000001, "text": " let's maybe get in touch. Thank you very much. It's very nice to be talking to you today about", "tokens": [50364, 718, 311, 1310, 483, 294, 2557, 13, 1044, 291, 588, 709, 13, 467, 311, 588, 1481, 281, 312, 1417, 281, 291, 965, 466, 50640], "temperature": 0.0, "avg_logprob": -0.2927894047328404, "compression_ratio": 1.233644859813084, "no_speech_prob": 0.013526453636586666}, {"id": 2210, "seek": 1279504, "start": 12800.560000000001, "end": 12802.480000000001, "text": " these completely random topics. Yeah.", "tokens": [50640, 613, 2584, 4974, 8378, 13, 865, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2927894047328404, "compression_ratio": 1.233644859813084, "no_speech_prob": 0.013526453636586666}], "language": "en"}