1
00:00:00,000 --> 00:00:06,000
I had the great pleasure of catching up with Professor Larissa Soldatova from Goldsmiths University.

2
00:00:06,000 --> 00:00:10,000
She's really interested in automating the process of science.

3
00:00:10,000 --> 00:00:17,000
And in order to do that, you need to know about logic, abduction, knowledge and semantics.

4
00:00:17,000 --> 00:00:24,000
Biology is so complex, we still scratch the surface trying to understand.

5
00:00:24,000 --> 00:00:30,000
And there are not enough scientists in the world to do it.

6
00:00:30,000 --> 00:00:36,000
And if we can have some AI help, that would be probably a good thing.

7
00:00:36,000 --> 00:00:45,000
So my presentation about reflecting on automation of science, history of it, where we are with it now,

8
00:00:45,000 --> 00:00:52,000
how much we can automate now and where we're probably heading towards.

9
00:00:52,000 --> 00:00:56,000
So I'm really interested in the philosophy of science, which is to say,

10
00:00:56,000 --> 00:01:00,000
what is the meta process that scientists go through when they do science?

11
00:01:00,000 --> 00:01:09,000
So science is about generating theories, explanations about how the world is this way and also how it's not that way.

12
00:01:09,000 --> 00:01:19,000
But science is based on empiricism, which is basically following the data, doing experiments and trying to discount hypotheses.

13
00:01:19,000 --> 00:01:24,000
But we still have this chicken and egg problem, which is where do the hypotheses come from?

14
00:01:24,000 --> 00:01:28,000
And this is very related to the bias variance trade-off in machine learning.

15
00:01:28,000 --> 00:01:33,000
Generally, we can't start from a blank slate. We can't start with nothing.

16
00:01:33,000 --> 00:01:38,000
We need to have some world knowledge to structure our search process.

17
00:01:38,000 --> 00:01:45,000
And then we run into trouble. How much should we seed the system with priors and world knowledge?

18
00:01:45,000 --> 00:01:50,000
There are so many things in our physical world that it just feels like we should encode it into our systems.

19
00:01:50,000 --> 00:01:56,000
It'd be a waste of time not to. But then there are some things that we feel that the system should learn.

20
00:01:56,000 --> 00:02:02,000
But if we bias the system too much, then we might accrue a kind of approximation error

21
00:02:02,000 --> 00:02:07,000
because there are things that our system just won't be able to search because we've biased it too much.

22
00:02:07,000 --> 00:02:14,000
So there's always this trade-off between having the flexibility to find new things versus cutting down the search base.

23
00:02:14,000 --> 00:02:22,000
The next big step will be when we will be able to consume more knowledge.

24
00:02:22,000 --> 00:02:30,000
So not just data, but if we start using what we know.

25
00:02:30,000 --> 00:02:40,000
Specific knowledge is a major source of machine and human intelligence and will suffice even with simple inference.

26
00:02:40,000 --> 00:02:46,000
So if you take an AI system, it's mostly taking lots of data.

27
00:02:46,000 --> 00:02:57,000
In the best case, it will take lots of different data, put it together, and then it outputs some predictive model or something else or piece of art.

28
00:02:57,000 --> 00:03:00,000
But knowledge is still hugely underused.

29
00:03:00,000 --> 00:03:05,000
No, one reason there is not much available in forms that computers can consume.

30
00:03:05,000 --> 00:03:14,000
But just imagine if even these new models were immediately fed back and could be an input, so it's called closed loop systems,

31
00:03:14,000 --> 00:03:17,000
then probably it would be more intelligent.

32
00:03:17,000 --> 00:03:18,000
So this is quite interesting.

33
00:03:18,000 --> 00:03:25,000
So going back to the 80s, the modus operandi in artificial intelligence was encoding knowledge into expert systems

34
00:03:25,000 --> 00:03:32,000
because we knew that if you could encode knowledge, you can deduce new knowledge and new facts from existing knowledge.

35
00:03:32,000 --> 00:03:38,000
So knowledge isn't just data, knowledge is one level of abstraction higher where you actually know the relations

36
00:03:38,000 --> 00:03:42,000
and you can apply some kind of functions between data.

37
00:03:42,000 --> 00:03:44,000
The problem is it's very brittle.

38
00:03:44,000 --> 00:03:49,000
Famously, Doug Lenitz created this project called The Psych Project,

39
00:03:49,000 --> 00:03:53,000
which had millions and millions of facts and relations and things in it.

40
00:03:53,000 --> 00:03:56,000
And it was almost a complete waste of time.

41
00:03:56,000 --> 00:03:59,000
It was too brittle, it wasn't particularly useful.

42
00:03:59,000 --> 00:04:02,000
But empirically, though, we seem to be wasting our time with this blank slate approach

43
00:04:02,000 --> 00:04:06,000
because machine learning moved away from really any explicit domain knowledge

44
00:04:06,000 --> 00:04:09,000
and it just became these meta priors.

45
00:04:09,000 --> 00:04:17,000
So things like symmetries and, I don't know, learning schedules and weight decay and stuff like that.

46
00:04:17,000 --> 00:04:23,000
And the models only had very abstract priors in the sense of how to learn things statistically.

47
00:04:23,000 --> 00:04:25,000
They didn't have any domain knowledge at all.

48
00:04:25,000 --> 00:04:29,000
But there are so many things in our physical world that we just understand

49
00:04:29,000 --> 00:04:32,000
and we should be able to explicitly code into our system.

50
00:04:32,000 --> 00:04:38,000
But we have this paradox which is that every time we try and encode high level knowledge into systems,

51
00:04:38,000 --> 00:04:43,000
it just introduces brittleness and this is what Rich Sutton warned against in his bitter lesson essay.

52
00:04:43,000 --> 00:04:47,000
What's the difference for you between data and knowledge?

53
00:04:47,000 --> 00:04:55,000
Yes, it's a good question and in different textbooks you can actually find different definitions.

54
00:04:55,000 --> 00:05:00,000
But people who are working in knowledge representations, they have very clear ideas.

55
00:05:00,000 --> 00:05:02,000
So data is facts.

56
00:05:02,000 --> 00:05:05,000
Knowledge is when you go level up.

57
00:05:05,000 --> 00:05:11,000
It's like rules, relations, ultimately better if you have models.

58
00:05:11,000 --> 00:05:14,000
Better if you have executable models.

59
00:05:14,000 --> 00:05:19,000
So you can put something in, get something out.

60
00:05:19,000 --> 00:05:21,000
So this is knowledge.

61
00:05:21,000 --> 00:05:24,000
A lot of knowledge you can discover directly from data.

62
00:05:24,000 --> 00:05:29,000
If you have a lot of data, you can discover these rules, you can construct models.

63
00:05:29,000 --> 00:05:35,000
But why don't you represent it immediately as knowledge if you already know it?

64
00:05:35,000 --> 00:05:38,000
Why? It's very inefficient way.

65
00:05:38,000 --> 00:05:43,000
It takes a lot of energy, these neural networks, they are destroying the planet.

66
00:05:43,000 --> 00:05:51,000
You actually can encode a lot what we know in quite compact way and you can reuse it.

67
00:05:51,000 --> 00:05:54,000
A lot what we know actually is not in the data.

68
00:05:54,000 --> 00:06:00,000
Just an example, my collaborator gave this example.

69
00:06:00,000 --> 00:06:04,000
They were modeling ecological systems and you can have a lot of data

70
00:06:04,000 --> 00:06:09,000
and you can discover from data that bear has between two and five cups

71
00:06:09,000 --> 00:06:11,000
and if it's more north it's less cup.

72
00:06:11,000 --> 00:06:17,000
But why bother if we know you just encode it as a knowledge so you can extract it

73
00:06:17,000 --> 00:06:22,000
but you also can just encode immediately and you can encode a lot additionally.

74
00:06:22,000 --> 00:06:24,000
What is not in the data?

75
00:06:24,000 --> 00:06:27,000
That is why it's more powerful and promising.

76
00:06:27,000 --> 00:06:35,000
Larissa spoke about a project about 60 years ago to automate the scientific discovery of chemical compounds.

77
00:06:35,000 --> 00:06:40,000
Not too dissimilar actually to Alpha Fold which came out recently from DeepMind

78
00:06:40,000 --> 00:06:45,000
but anyway she's kind of saying that if you look at how these systems work today

79
00:06:45,000 --> 00:06:48,000
it's not dramatically different than it was 60 years ago.

80
00:06:48,000 --> 00:06:50,000
What chemists would do?

81
00:06:50,000 --> 00:06:55,000
And they put in place a whole pipeline, knowledge acquisition

82
00:06:55,000 --> 00:07:04,000
also giving output in forms that is good for users in this case chemists.

83
00:07:04,000 --> 00:07:09,000
It really produced very interesting promising chemical structure.

84
00:07:09,000 --> 00:07:12,000
Moreover this system actually was multi-agent.

85
00:07:12,000 --> 00:07:16,000
It was heuristic dendral, metadendral and it was closed loop.

86
00:07:16,000 --> 00:07:23,000
So whatever it managed to discover it immediately was going back and improved over time.

87
00:07:23,000 --> 00:07:30,000
So it was 60 years ago and I would claim we haven't progressed much further.

88
00:07:30,000 --> 00:07:34,000
Of course there were other fantastic systems

89
00:07:34,000 --> 00:07:41,000
but the underlying architecture and principle how it operates is still the same.

90
00:07:42,000 --> 00:07:51,000
So it's called the robot scientist and this was introduced again 15-20 years ago now.

91
00:07:51,000 --> 00:08:01,000
So the idea is to develop a system, AI system that is capable of generating hypothesis, designing experiments

92
00:08:01,000 --> 00:08:09,000
having real robotic labs to carry out these experiments, analyze results, do it in cycle and hopefully discover something new.

93
00:08:09,000 --> 00:08:15,000
So it's a lot of data, external data, whatever data sets are relevant,

94
00:08:15,000 --> 00:08:20,000
plus internally produced because it's a robotic lab, very powerful,

95
00:08:20,000 --> 00:08:26,000
it can run in parallel, like 1000 biologists working in parallel in lab.

96
00:08:26,000 --> 00:08:30,000
But it also has this knowledge, formalized knowledge.

97
00:08:30,000 --> 00:08:36,000
It has domain knowledge, the first system worked with yeast biology

98
00:08:36,000 --> 00:08:44,000
and it's also had this knowledge about what scientific discovery is, what key elements, how they're connected,

99
00:08:44,000 --> 00:08:49,000
what you need to do, just like our experimental practices, how to design experiments.

100
00:08:49,000 --> 00:08:52,000
You need to explain to machine at all.

101
00:08:52,000 --> 00:08:55,000
So in science we need to do abduction.

102
00:08:55,000 --> 00:09:00,000
This is absolutely delicious because you've heard us speak about abduction ad nauseam on MLST

103
00:09:00,000 --> 00:09:04,000
and Larissa is out there talking about it, which is wonderful.

104
00:09:04,000 --> 00:09:10,000
But yeah, you have to start with a hypothesis or a set of hypotheses in science before you do inference.

105
00:09:10,000 --> 00:09:15,000
So inference is when you kind of learn mappings from hypotheses,

106
00:09:15,000 --> 00:09:20,000
but abduction is how you first select the relevant hypotheses,

107
00:09:20,000 --> 00:09:26,000
hypotheses that could generate powerful explanations for some phenomena.

108
00:09:26,000 --> 00:09:29,000
And it's very creative, it's very mysterious.

109
00:09:29,000 --> 00:09:32,000
So what is the logician 101 definition of abduction?

110
00:09:32,000 --> 00:09:36,000
Well, it's reasoning to the best explanation.

111
00:09:36,000 --> 00:09:38,000
Is that helpful?

112
00:09:38,000 --> 00:09:40,000
Maybe, maybe.

113
00:09:40,000 --> 00:09:45,000
I love the kind of cognitive science view on things, which is, you know, an explanation.

114
00:09:45,000 --> 00:09:50,000
It's a causal model, which helps us understand the world because it gives explanatory power.

115
00:09:50,000 --> 00:09:53,000
It carves the world up by the joints.

116
00:09:53,000 --> 00:09:54,000
Why is the world this way?

117
00:09:54,000 --> 00:09:56,000
Why is the world not that way?

118
00:09:56,000 --> 00:10:02,000
And the raw building materials of these theories are cognitive priors,

119
00:10:02,000 --> 00:10:06,000
and they're the same kind of priors that we imbue into machine learning models.

120
00:10:06,000 --> 00:10:10,000
Noam Chomsky says that some of these priors are built into our brains.

121
00:10:10,000 --> 00:10:14,000
I'm a big fan that many of the priors are socially embedded.

122
00:10:14,000 --> 00:10:17,000
They're like software that float around mimetically.

123
00:10:17,000 --> 00:10:22,000
But anyway, abduction is about you have a bunch of priors in the system,

124
00:10:22,000 --> 00:10:27,000
and we as humans have this magical ability to grab the relevant priors,

125
00:10:27,000 --> 00:10:32,000
stick them together into small models that give explanatory power,

126
00:10:32,000 --> 00:10:37,000
and selecting a few of those models, and that's what we do when we do abduction.

127
00:10:37,000 --> 00:10:42,000
So the key question is basically, can we automate that in a machine?

128
00:10:42,000 --> 00:10:47,000
I will focus on hypothesis because in philosophy of science for a long time,

129
00:10:47,000 --> 00:10:53,000
we considered that it's something that you cannot automate, that only humans can do it.

130
00:10:53,000 --> 00:10:58,000
And to enable these hypotheses, this generation,

131
00:10:58,000 --> 00:11:02,000
so you need to go out of the system, out of what you already know.

132
00:11:02,000 --> 00:11:06,000
So you cannot use conventional logic like abduction.

133
00:11:06,000 --> 00:11:08,000
You need to use something else.

134
00:11:08,000 --> 00:11:11,000
And I'm used abduction, so there are other logic that you can use,

135
00:11:11,000 --> 00:11:16,000
but you need to go outside of the system to have these guesses,

136
00:11:16,000 --> 00:11:21,000
and then try to see if some of it makes sense or not.

137
00:11:21,000 --> 00:11:22,000
It's not trivial.

138
00:11:22,000 --> 00:11:31,000
Focusing on hypothesis, like you want to test hypothesis that this particular gene has that function,

139
00:11:31,000 --> 00:11:36,000
but you actually cannot put gene into well, or you cannot measure function.

140
00:11:36,000 --> 00:11:39,000
So you start doing some inferences.

141
00:11:39,000 --> 00:11:42,000
What if we replace it to use some proxy?

142
00:11:42,000 --> 00:11:48,000
If we use something with this gene knocked out, and if we put metabolites,

143
00:11:48,000 --> 00:11:51,000
and so you go deeper, deeper, deeper.

144
00:11:51,000 --> 00:11:55,000
So biologists do it just normal practices,

145
00:11:55,000 --> 00:12:00,000
but when you try to explain it to computer how to do it, you really need to think.

146
00:12:00,000 --> 00:12:03,000
So what steps we're taking,

147
00:12:03,000 --> 00:12:08,000
we actually very rarely experiment with something that we make conclusions about.

148
00:12:08,000 --> 00:12:12,000
So our conclusions are not always accurate, so then you have to go back.

149
00:12:12,000 --> 00:12:16,000
So it was very interesting for us to try to automate it,

150
00:12:16,000 --> 00:12:21,000
and then also reflect on how humans are doing science, especially experimental science.

151
00:12:21,000 --> 00:12:24,000
There are many levels to this as well.

152
00:12:24,000 --> 00:12:26,000
So if you want to go and get a job at Google now,

153
00:12:26,000 --> 00:12:32,000
they already have a bit of a meritocracy which is based on kind of what we're talking about here.

154
00:12:32,000 --> 00:12:36,000
To be very senior at Google, you need to find people,

155
00:12:36,000 --> 00:12:40,000
and to be one step below that, you need to find the areas,

156
00:12:40,000 --> 00:12:42,000
and below that you need to solve abstract problems,

157
00:12:42,000 --> 00:12:45,000
and then below that you need to solve specific problems.

158
00:12:45,000 --> 00:12:51,000
So there's a kind of hierarchy in the amount of creativity and ambiguity that you can tolerate,

159
00:12:51,000 --> 00:12:55,000
and that's what scientists do, so they're always traversing these levels.

160
00:12:55,000 --> 00:12:58,000
Abduction is about finding the areas,

161
00:12:58,000 --> 00:13:02,000
and then once you perform inference on these hypotheses that you have abducted,

162
00:13:02,000 --> 00:13:05,000
then that's solving a much more specific problem,

163
00:13:05,000 --> 00:13:07,000
and it gets more and more specific.

164
00:13:07,000 --> 00:13:13,000
The key point is that science is combinatorially, exponentially expensive,

165
00:13:13,000 --> 00:13:15,000
just to run experiments.

166
00:13:15,000 --> 00:13:19,000
And there isn't enough time in the world, there aren't enough scientists in the world.

167
00:13:19,000 --> 00:13:22,000
Yes, having AI scientists can help us,

168
00:13:22,000 --> 00:13:27,000
but we still need to always reflexively and abstractly go one level up.

169
00:13:27,000 --> 00:13:30,000
We always need to be asking ourselves the question, can we do this better?

170
00:13:30,000 --> 00:13:34,000
If we had a different perspective or a different view, could we make this more efficient?

171
00:13:34,000 --> 00:13:37,000
And then could we make this more efficient?

172
00:13:37,000 --> 00:13:39,000
And that's what we humans can do,

173
00:13:39,000 --> 00:13:42,000
and that's what we need to imbue into AI robot scientists.

174
00:13:42,000 --> 00:13:48,000
Well, you need to test like five, six drugs, work on these combinations.

175
00:13:48,000 --> 00:13:51,000
So yes, it's combinatorical problem.

176
00:13:51,000 --> 00:13:58,000
If you do it in a lab, we just don't have enough cells in our blood to run all these experiments.

177
00:13:58,000 --> 00:14:02,000
So this is a justification when you need such system.

178
00:14:02,000 --> 00:14:06,000
You first do reasoning based on all what we know,

179
00:14:06,000 --> 00:14:11,000
with all these gaps, contradictions, but the best what you can do.

180
00:14:11,000 --> 00:14:16,000
Output some plausible hypothesis is what should work in this situation.

181
00:14:16,000 --> 00:14:21,000
Then you can do some simulation, again, the computational,

182
00:14:21,000 --> 00:14:26,000
come up with some hundreds, several hundred hypothesis,

183
00:14:26,000 --> 00:14:33,000
that then you can automatically test so that there's a cheaper way of doing it using robotics.

184
00:14:33,000 --> 00:14:37,000
Yes, so I'm really interested in this idea of human cyborgs,

185
00:14:37,000 --> 00:14:40,000
basically where humans and AIs work together.

186
00:14:40,000 --> 00:14:48,000
The problem is it's not easy to say that combining humans and AIs together produces more productivity.

187
00:14:48,000 --> 00:14:51,000
It probably does, but we don't know for sure.

188
00:14:51,000 --> 00:14:56,000
There was a piece out by ThoughtWorks recently where they were talking about generative AI or co-pilot,

189
00:14:56,000 --> 00:14:59,000
making developers two times more productive.

190
00:14:59,000 --> 00:15:01,000
How do they measure that?

191
00:15:01,000 --> 00:15:03,000
Software engineering is not a reducible activity.

192
00:15:03,000 --> 00:15:05,000
It's a very complex phenomenon.

193
00:15:05,000 --> 00:15:09,000
People have tried to measure the productivity of software engineers for many years.

194
00:15:09,000 --> 00:15:15,000
How many lines of code do they write and the storyboard and issue tracking and stuff like that.

195
00:15:15,000 --> 00:15:20,000
It's all rubbish, and the reason for that is it's a very complex, irreducible phenomenon.

196
00:15:20,000 --> 00:15:22,000
Therein lies the problem.

197
00:15:22,000 --> 00:15:27,000
There's a meme going around on LinkedIn at the moment talking about how with co-pilot,

198
00:15:27,000 --> 00:15:33,000
you can generate the code ten times faster, but you then spend ten times more time debugging the code,

199
00:15:33,000 --> 00:15:38,000
and that's because there's a technical debt, a new type of technical debt, understanding debt.

200
00:15:38,000 --> 00:15:44,000
You generated all of this code and it works, and then in order to fix a problem, you need to actually understand it.

201
00:15:44,000 --> 00:15:47,000
The mental model is not in the code.

202
00:15:47,000 --> 00:15:51,000
Actually, in any software engineering project, the mental model is a memetic social thing.

203
00:15:51,000 --> 00:15:56,000
It's in the brains of the developers, and it kind of floats around in the ether.

204
00:15:56,000 --> 00:15:58,000
It's not in the code.

205
00:15:58,000 --> 00:16:00,000
So, yeah, it's a similar thing here.

206
00:16:00,000 --> 00:16:05,000
If we had an AI robotic scientist, how would we know that it's making us go faster?

207
00:16:05,000 --> 00:16:09,000
I don't think such systems will replace humans.

208
00:16:09,000 --> 00:16:20,000
As you see, at the best, they can automate some science, some experimental-driven science,

209
00:16:20,000 --> 00:16:28,000
but it's an important part of science, and if it can be automated, it will be of great help.

210
00:16:28,000 --> 00:16:34,000
Humans still have distinct advantages in many areas.

211
00:16:34,000 --> 00:16:36,000
Hopefully, we will retain it.

212
00:16:36,000 --> 00:16:44,000
Ultimately, the best way is to work together, take advantage, and really, because it's him.

213
00:16:44,000 --> 00:16:49,000
Then maybe we can make a faster progress.

214
00:16:49,000 --> 00:16:50,000
Welcome to MLST.

215
00:16:50,000 --> 00:16:53,000
We are here with Professor Larissa Soldatova.

216
00:16:53,000 --> 00:17:01,000
Larissa joined Goldsmiths in November 2017 as director of the Online Masters in Data Science program.

217
00:17:01,000 --> 00:17:05,000
She's an internationally recognized expert in artificial intelligence,

218
00:17:05,000 --> 00:17:12,000
particularly in discovery science, reasoning, knowledge representation, and semantic technologies.

219
00:17:12,000 --> 00:17:16,000
Larissa has also been working on the Robot Scientist project,

220
00:17:16,000 --> 00:17:20,000
which investigates which processes of scientific discovery can be automated

221
00:17:20,000 --> 00:17:24,000
and how robotic and human scientists can work together.

222
00:17:24,000 --> 00:17:31,000
The Robot Scientist, Adam, was the first system which made an autonomous scientific discovery.

223
00:17:31,000 --> 00:17:36,000
She's also involved in a number of international projects in the development of semantic standards,

224
00:17:36,000 --> 00:17:41,000
for example, the machine learning schema, the robotics task ontology standard,

225
00:17:41,000 --> 00:17:45,000
and also the laboratory protocols exact.

226
00:17:45,000 --> 00:17:48,000
It's a pleasure to have you here, Larissa.

227
00:17:48,000 --> 00:17:52,000
Maybe you could just start off by telling us about the events and your talk today.

228
00:17:52,000 --> 00:17:58,000
My talk was about where we are with automating science.

229
00:17:58,000 --> 00:18:04,000
Yes, it's very creative endeavor to do science,

230
00:18:04,000 --> 00:18:11,000
but there are parts of it that scientists try to automate for a long time.

231
00:18:11,000 --> 00:18:19,000
So the history of scientific discovery as a subject of computer science

232
00:18:19,000 --> 00:18:23,000
is actually more than half a century old.

233
00:18:23,000 --> 00:18:31,000
The first systems were developed in the 1960s in Stanford.

234
00:18:31,000 --> 00:18:38,000
The first system was inspired by the need to go to other planets,

235
00:18:38,000 --> 00:18:43,000
collect samples, like if you go to Mars.

236
00:18:43,000 --> 00:18:48,000
You cannot send many samples back.

237
00:18:48,000 --> 00:18:53,000
If you send data through what you managed to analyze,

238
00:18:53,000 --> 00:19:00,000
it will take so long to send it to human scientists to get some instructions what to do next.

239
00:19:00,000 --> 00:19:06,000
So that would stimulate the development of autonomous intelligence systems

240
00:19:06,000 --> 00:19:14,000
that need to collect something, do something, decide what to do next, what it means.

241
00:19:15,000 --> 00:19:21,000
Since then, it was a slow but steady progress.

242
00:19:21,000 --> 00:19:26,000
So the systems that we are working on, it's already over 20 years.

243
00:19:26,000 --> 00:19:28,000
Yes, it's called Robot Scientist.

244
00:19:28,000 --> 00:19:30,000
And yes, the first was Adam.

245
00:19:30,000 --> 00:19:38,000
It's actually stand for adaptive machines, but yeah, Adam, because it was the first.

246
00:19:38,000 --> 00:19:43,000
Not to be confused with the other Adam for training neural networks?

247
00:19:43,000 --> 00:19:45,000
Yes, it's a good point.

248
00:19:45,000 --> 00:19:51,000
So the principle of such system is that it's very knowledge-intensive.

249
00:19:51,000 --> 00:19:59,000
So we hear a lot about progress in AI, but it's mostly data-driven.

250
00:19:59,000 --> 00:20:03,000
So now we have so much data and also computational power,

251
00:20:03,000 --> 00:20:07,000
so it can be processed and something useful output.

252
00:20:07,000 --> 00:20:14,000
And I believe that the next step will be when a lot of knowledge will be available

253
00:20:14,000 --> 00:20:16,000
in the form that computers can take.

254
00:20:16,000 --> 00:20:18,000
So right now they are taking data.

255
00:20:18,000 --> 00:20:27,000
If we can give our knowledge and let them to reason with it, so that will be just...

256
00:20:27,000 --> 00:20:31,000
And to do scientific discovery, of course, you need to have knowledge.

257
00:20:31,000 --> 00:20:41,000
Scientists still insure supply because it takes like 25 years to produce.

258
00:20:41,000 --> 00:20:43,000
Wow, you need to educate.

259
00:20:43,000 --> 00:20:46,000
It's many, many years of studying.

260
00:20:46,000 --> 00:20:56,000
And if we can create systems and also knowledge models starting from some domain knowledge

261
00:20:56,000 --> 00:21:00,000
in some particular areas, so we are working with biological systems.

262
00:21:00,000 --> 00:21:07,000
So if you can encode in machine-processable form, what we know,

263
00:21:07,000 --> 00:21:11,000
and also encode how we do science.

264
00:21:11,000 --> 00:21:14,000
So how we formulate hypothesis.

265
00:21:14,000 --> 00:21:17,000
Of course, not any, but at least some.

266
00:21:17,000 --> 00:21:20,000
It's actually quite algorithmic.

267
00:21:20,000 --> 00:21:22,000
Yes, I'd love to know more about that.

268
00:21:22,000 --> 00:21:24,000
So you said some really interesting things.

269
00:21:24,000 --> 00:21:27,000
I mean, first of all, as you say, if we want to learn about Mars,

270
00:21:27,000 --> 00:21:29,000
it's very sample inefficient because it's very expensive.

271
00:21:29,000 --> 00:21:32,000
And then we get to the purpose of science.

272
00:21:32,000 --> 00:21:38,000
And in my opinion, the purpose of science is about generating intelligible explanations.

273
00:21:38,000 --> 00:21:40,000
And then we get to knowledge.

274
00:21:40,000 --> 00:21:45,000
And I think you were just alluding to data and information is not knowledge.

275
00:21:45,000 --> 00:21:48,000
And I know you're an expert in knowledge representation.

276
00:21:48,000 --> 00:21:56,000
So perhaps we could just touch on, does knowledge have some primacy over just normal information?

277
00:21:57,000 --> 00:22:03,000
Okay, so it's very true that different textbooks will give different definition.

278
00:22:03,000 --> 00:22:06,000
And sometimes there is no distinction between data and information.

279
00:22:06,000 --> 00:22:08,000
It's like synonym.

280
00:22:08,000 --> 00:22:13,000
But yes, in my area of research, we are very clear about that.

281
00:22:13,000 --> 00:22:14,000
Naturally.

282
00:22:14,000 --> 00:22:17,000
So what we can see the data is facts.

283
00:22:17,000 --> 00:22:18,000
Yes.

284
00:22:18,000 --> 00:22:21,000
So this gene has this function.

285
00:22:21,000 --> 00:22:25,000
And duck is birds.

286
00:22:25,000 --> 00:22:29,000
Knowledge is when you go further.

287
00:22:29,000 --> 00:22:38,000
So when you have rules, when you have, like, even connections or links between facts,

288
00:22:38,000 --> 00:22:40,000
you can go further.

289
00:22:40,000 --> 00:22:43,000
Yes, more knowledge model.

290
00:22:44,000 --> 00:22:55,000
If you have a lot of data, how's prices in this area, how it was affected by flooding,

291
00:22:55,000 --> 00:23:00,000
or so they dropped how much, then you can detect this patterns.

292
00:23:00,000 --> 00:23:02,000
So this is knowledge.

293
00:23:02,000 --> 00:23:07,000
So if there is flooding in the area, prices will drop.

294
00:23:07,000 --> 00:23:09,000
So this is the rule.

295
00:23:09,000 --> 00:23:14,000
And you can represent it in various forms.

296
00:23:14,000 --> 00:23:22,000
So if we can represent it in forms that you can put it into memory of computers,

297
00:23:22,000 --> 00:23:24,000
then it can be used.

298
00:23:24,000 --> 00:23:29,000
And if you can be used together with data, if you have all these components,

299
00:23:29,000 --> 00:23:31,000
it will be even more intelligent.

300
00:23:31,000 --> 00:23:32,000
Interesting.

301
00:23:32,000 --> 00:23:38,000
I guess where I was going with the question a little bit was also, can knowledge be probabilistic?

302
00:23:38,000 --> 00:23:39,000
Of course.

303
00:23:39,000 --> 00:23:41,000
All our knowledge is probabilistic.

304
00:23:41,000 --> 00:23:44,000
We are not certain about anything.

305
00:23:44,000 --> 00:23:48,000
Even if we think, yes, this is the state of the art.

306
00:23:48,000 --> 00:23:53,000
So much knowledge was changed, was refuted.

307
00:23:53,000 --> 00:23:57,000
So we always walk in this probabilistic framework.

308
00:23:57,000 --> 00:24:00,000
We not always reflect on it.

309
00:24:00,000 --> 00:24:06,000
But if you want to build a system that really outputs something,

310
00:24:06,000 --> 00:24:10,000
it is underlying.

311
00:24:10,000 --> 00:24:14,000
So the other side of working on such a system,

312
00:24:14,000 --> 00:24:20,000
it gives you a chance to understand better about what science is.

313
00:24:20,000 --> 00:24:24,000
There are no guaranteed truths.

314
00:24:24,000 --> 00:24:27,000
It's just what we believe in at this point,

315
00:24:27,000 --> 00:24:31,000
given all data we have and the other series,

316
00:24:31,000 --> 00:24:33,000
we just walk together.

317
00:24:33,000 --> 00:24:35,000
But it can change in the future.

318
00:24:35,000 --> 00:24:37,000
There are a couple of things.

319
00:24:37,000 --> 00:24:41,000
Rationalists think that there is only certainty.

320
00:24:41,000 --> 00:24:43,000
You either know it or you don't.

321
00:24:43,000 --> 00:24:51,000
I believe that Bayesian reasoning is an extension of logical reasoning in the domain of uncertainty.

322
00:24:51,000 --> 00:24:53,000
Absolutely correct.

323
00:24:53,000 --> 00:24:55,000
And it's important to remember about it.

324
00:24:55,000 --> 00:25:02,000
It's important to remember even when we read newspapers.

325
00:25:02,000 --> 00:25:06,000
But even scientific articles,

326
00:25:06,000 --> 00:25:11,000
scientists actually tend to overgeneralize.

327
00:25:11,000 --> 00:25:15,000
They are very excited about what they discovered.

328
00:25:15,000 --> 00:25:19,000
It's important to step back and think that,

329
00:25:19,000 --> 00:25:21,000
yes, it actually can be refuted.

330
00:25:21,000 --> 00:25:24,000
It can be only one explanation.

331
00:25:24,000 --> 00:25:29,000
And if you look at history of science,

332
00:25:29,000 --> 00:25:33,000
in Newtonian physics,

333
00:25:33,000 --> 00:25:38,000
it looked like the ultimate truth and said no.

334
00:25:38,000 --> 00:25:42,000
The theory of relativity turned it all upside down.

335
00:25:42,000 --> 00:25:45,000
It's only some fraction of it.

336
00:25:45,000 --> 00:25:46,000
Yes.

337
00:25:46,000 --> 00:25:51,000
To what extent should knowledge be grounded in the physical world?

338
00:25:51,000 --> 00:25:57,000
So, the whole science, it has to be verifiable.

339
00:25:57,000 --> 00:26:04,000
You can imagine many things plausible,

340
00:26:04,000 --> 00:26:11,000
but unless you really show that this corresponds to reality,

341
00:26:11,000 --> 00:26:13,000
we cannot trust it.

342
00:26:13,000 --> 00:26:17,000
Of course, there can be maybe another explanation,

343
00:26:17,000 --> 00:26:21,000
but at least this is connected to what we observe or even better

344
00:26:21,000 --> 00:26:26,000
if we can design an experiment to show there is something behind it.

345
00:26:26,000 --> 00:26:30,000
So, this is how science always worked.

346
00:26:30,000 --> 00:26:34,000
Of course, I referred to experimental science

347
00:26:34,000 --> 00:26:37,000
like biology, physics, where you actually can choose.

348
00:26:37,000 --> 00:26:41,000
There are sort of experiments where we will not go astray

349
00:26:41,000 --> 00:26:47,000
as we cannot build an automated system to do that.

350
00:26:47,000 --> 00:26:48,000
Interesting.

351
00:26:48,000 --> 00:26:51,000
I wondered whether you would define yourself as a neat or a scruffy

352
00:26:51,000 --> 00:26:53,000
when it comes to knowledge representation.

353
00:26:53,000 --> 00:26:55,000
I mean, for example, if I define...

354
00:26:55,000 --> 00:26:58,000
And by the way, a neat is like a puritanical,

355
00:26:58,000 --> 00:27:01,000
simple underlying principle, parsimony.

356
00:27:01,000 --> 00:27:03,000
So, if I wanted to define a chair,

357
00:27:03,000 --> 00:27:08,000
I could think of thousands of different analogies to describe a chair.

358
00:27:08,000 --> 00:27:12,000
Is it collapsible or is it just very complicated?

359
00:27:12,000 --> 00:27:17,000
I am a very neat and I can give you a perfect example

360
00:27:17,000 --> 00:27:19,000
how to model a chair.

361
00:27:19,000 --> 00:27:20,000
Go on.

362
00:27:20,000 --> 00:27:24,000
We are very neat because we work with these reasoning systems

363
00:27:24,000 --> 00:27:27,000
and they need sharp, clean logic.

364
00:27:27,000 --> 00:27:30,000
And if you complicate matters, it complicates reasoning.

365
00:27:30,000 --> 00:27:33,000
A chance at something will be really discovered

366
00:27:33,000 --> 00:27:36,000
and we want to connect different bits of knowledge, different data.

367
00:27:36,000 --> 00:27:39,000
So, the more streamlined it is, the better.

368
00:27:39,000 --> 00:27:44,000
And if you use like minimum of relations

369
00:27:44,000 --> 00:27:49,000
and so over years, I just learn to think that way

370
00:27:49,000 --> 00:27:51,000
and about chair.

371
00:27:51,000 --> 00:27:54,000
It's actually examples that I used in my lectures

372
00:27:54,000 --> 00:27:56,000
when I was explaining to students.

373
00:27:56,000 --> 00:27:57,000
Wonderful.

374
00:27:57,000 --> 00:27:59,000
So, I was showing them just examples.

375
00:27:59,000 --> 00:28:01,000
So, these are chairs.

376
00:28:01,000 --> 00:28:03,000
They can look very differently.

377
00:28:03,000 --> 00:28:05,000
Some don't have any legs.

378
00:28:05,000 --> 00:28:07,000
Some have three legs.

379
00:28:07,000 --> 00:28:09,000
So, how to explain computers?

380
00:28:09,000 --> 00:28:10,000
This is a chair.

381
00:28:10,000 --> 00:28:11,000
Yes.

382
00:28:11,000 --> 00:28:12,000
So, you can...

383
00:28:12,000 --> 00:28:15,000
Yes, you can represent many, many, many ways.

384
00:28:15,000 --> 00:28:19,000
But a good knowledge model

385
00:28:19,000 --> 00:28:23,000
will focus what we call intrinsic property.

386
00:28:23,000 --> 00:28:25,000
What makes it a chair?

387
00:28:28,000 --> 00:28:29,000
Give me an example.

388
00:28:29,000 --> 00:28:31,000
What makes it a chair?

389
00:28:31,000 --> 00:28:34,000
It's function to be seated on.

390
00:28:34,000 --> 00:28:37,000
So, it's a relational model.

391
00:28:37,000 --> 00:28:40,000
So, it was designed.

392
00:28:40,000 --> 00:28:43,000
It was produced for it.

393
00:28:43,000 --> 00:28:44,000
Yeah.

394
00:28:44,000 --> 00:28:45,000
This is so interesting.

395
00:28:45,000 --> 00:28:47,000
It can take many shapes, many colors,

396
00:28:47,000 --> 00:28:50,000
but this is what defines a chair.

397
00:28:50,000 --> 00:28:52,000
And because we produced it human,

398
00:28:52,000 --> 00:28:57,000
so we can say this is function.

399
00:28:57,000 --> 00:28:59,000
And all other...

400
00:28:59,000 --> 00:29:01,000
Yes, it can have other properties

401
00:29:01,000 --> 00:29:03,000
like what color it is, what shape.

402
00:29:03,000 --> 00:29:06,000
But ultimately, this would make it a chair.

403
00:29:06,000 --> 00:29:07,000
Yes.

404
00:29:07,000 --> 00:29:10,000
Because when I hear philosophers talking about semantics,

405
00:29:10,000 --> 00:29:12,000
they have to come up with some kind of a relational model.

406
00:29:12,000 --> 00:29:15,000
And they will choose the view of function

407
00:29:15,000 --> 00:29:18,000
or purpose is another good one.

408
00:29:18,000 --> 00:29:22,000
And I guess what I'm saying is that it's a little bit anthropocentric

409
00:29:22,000 --> 00:29:26,000
and like function.

410
00:29:26,000 --> 00:29:28,000
For example, a hospital.

411
00:29:28,000 --> 00:29:30,000
That building used to be a hospital,

412
00:29:30,000 --> 00:29:32,000
but it's not a hospital anymore.

413
00:29:32,000 --> 00:29:38,000
But again, why it is so centric?

414
00:29:38,000 --> 00:29:41,000
Because it's us who made it.

415
00:29:41,000 --> 00:29:43,000
So we have control over it.

416
00:29:43,000 --> 00:29:46,000
Yes, it was hospital, now it's no more hospital.

417
00:29:46,000 --> 00:29:49,000
Some social convention, some decision was made,

418
00:29:49,000 --> 00:29:51,000
it was repurposed.

419
00:29:51,000 --> 00:29:53,000
Now it is not hospital, it's something else.

420
00:29:53,000 --> 00:29:57,000
But if you take biological systems like gene function,

421
00:29:57,000 --> 00:30:00,000
we have nothing to do with that.

422
00:30:00,000 --> 00:30:02,000
It evolved to be there.

423
00:30:02,000 --> 00:30:05,000
It's still intrinsic property of it,

424
00:30:05,000 --> 00:30:08,000
what function it has, what it's for.

425
00:30:08,000 --> 00:30:11,000
Otherwise you can be very far.

426
00:30:11,000 --> 00:30:14,000
Again, going back to a chair,

427
00:30:14,000 --> 00:30:16,000
I can sit on a table.

428
00:30:16,000 --> 00:30:18,000
It doesn't make it a chair

429
00:30:18,000 --> 00:30:20,000
because it's not primarily function.

430
00:30:20,000 --> 00:30:23,000
This is not what this was made for.

431
00:30:23,000 --> 00:30:24,000
Yes, yes.

432
00:30:24,000 --> 00:30:26,000
So if you agreed on it,

433
00:30:26,000 --> 00:30:29,000
you need to have this clarity,

434
00:30:29,000 --> 00:30:32,000
also agreement that this is how you model things.

435
00:30:32,000 --> 00:30:34,000
Then you start to have consistency.

436
00:30:34,000 --> 00:30:36,000
What is a chair? What is a table?

437
00:30:36,000 --> 00:30:39,000
Because if you want to have intelligences,

438
00:30:39,000 --> 00:30:42,000
then to design your office, for example.

439
00:30:42,000 --> 00:30:44,000
So how will it do?

440
00:30:44,000 --> 00:30:47,000
How it will distinguish what is what?

441
00:30:47,000 --> 00:30:51,000
So this is quite similar in principle to Wittgenstein

442
00:30:51,000 --> 00:30:54,000
said that the meaning of a word is in its use.

443
00:30:54,000 --> 00:30:57,000
And I'm friends with a colleague of yours, Mark Bishop,

444
00:30:57,000 --> 00:31:00,000
who says the meaning of computation is in its use.

445
00:31:00,000 --> 00:31:03,000
But that's a very kind of relativistic,

446
00:31:03,000 --> 00:31:06,000
social emergentist kind of view.

447
00:31:06,000 --> 00:31:10,000
I guess what I'm saying is do you think there's a platonic meaning?

448
00:31:10,000 --> 00:31:14,000
So for me, it's very easy to answer on these questions

449
00:31:14,000 --> 00:31:18,000
because I actually work in the development of real systems

450
00:31:18,000 --> 00:31:21,000
that need to take it, reason with it and produce something.

451
00:31:21,000 --> 00:31:25,000
Justification of what we are doing and how we are doing it works.

452
00:31:25,000 --> 00:31:28,000
You can do it differently.

453
00:31:28,000 --> 00:31:30,000
You can take different points of view.

454
00:31:30,000 --> 00:31:32,000
Let's design a different system based on it

455
00:31:32,000 --> 00:31:34,000
and then compare what is better.

456
00:31:34,000 --> 00:31:36,000
No one has done it.

457
00:31:36,000 --> 00:31:40,000
So this is what is done, how it's done, it's working.

458
00:31:40,000 --> 00:31:42,000
Okay, so that's very pragmatic.

459
00:31:42,000 --> 00:31:47,000
Yes, I would say it's not relativistic.

460
00:31:47,000 --> 00:31:53,000
It's a pragmatic approach to producing knowledge

461
00:31:53,000 --> 00:31:55,000
in machine-consumable form.

462
00:31:55,000 --> 00:31:57,000
It's not optimal, it's not the best,

463
00:31:57,000 --> 00:32:02,000
but it's something good that computers can work with

464
00:32:02,000 --> 00:32:04,000
and produce something.

465
00:32:04,000 --> 00:32:06,000
I think you might be a closet scruffy.

466
00:32:06,000 --> 00:32:14,000
Okay, are you familiar with the Psyche project from Doug Lennet?

467
00:32:14,000 --> 00:32:16,000
I think so.

468
00:32:16,000 --> 00:32:20,000
It's almost like preaching to the choir.

469
00:32:20,000 --> 00:32:25,000
What kind of retrospective comments do you have about that project?

470
00:32:25,000 --> 00:32:30,000
I think they just attempted to do too much.

471
00:32:30,000 --> 00:32:33,000
So it's like in AI.

472
00:32:33,000 --> 00:32:44,000
So this was a constant genetic AI versus something practical, fragmented.

473
00:32:44,000 --> 00:32:48,000
So of course it's wonderful to have a system that can solve any task

474
00:32:48,000 --> 00:32:51,000
for that you need such knowledge models.

475
00:32:51,000 --> 00:32:53,000
But just not say yet.

476
00:32:54,000 --> 00:32:56,000
Maybe in the future.

477
00:32:56,000 --> 00:33:02,000
Right now we're trying to model some fragments, some domains of knowledge.

478
00:33:02,000 --> 00:33:07,000
And if we have enough such fragments, hopefully we can combine.

479
00:33:07,000 --> 00:33:14,000
But ultimately, if you want to have general AI, you will need such models.

480
00:33:14,000 --> 00:33:16,000
Interesting.

481
00:33:16,000 --> 00:33:21,000
One contrast people make is, I have friends who are in the domain of certainty.

482
00:33:21,000 --> 00:33:24,000
So I think knowledge is either you know it or you didn't.

483
00:33:24,000 --> 00:33:27,000
And the reason that they give and the reason why they don't like language models

484
00:33:27,000 --> 00:33:31,000
is because they say, well, when you have actual knowledge, so you actually know,

485
00:33:31,000 --> 00:33:34,000
you can deduce new facts and new knowledge.

486
00:33:34,000 --> 00:33:37,000
So you can create more knowledge.

487
00:33:37,000 --> 00:33:41,000
And they would say with a language model, because it's only pretending to reason,

488
00:33:41,000 --> 00:33:44,000
you can't create any new knowledge with it.

489
00:33:44,000 --> 00:33:50,000
It's actually an advantage we already started using it to generate hypothesis.

490
00:33:50,000 --> 00:33:58,000
Because it's not rigid deduction, it is how you can actually make new guesses.

491
00:33:58,000 --> 00:34:00,000
And some of them might be true.

492
00:34:00,000 --> 00:34:03,000
You just need to experiment to show it.

493
00:34:03,000 --> 00:34:06,000
So we already started.

494
00:34:06,000 --> 00:34:11,000
We generated some very interesting hypothesis.

495
00:34:11,000 --> 00:34:14,000
We're already organizing experiments to test them.

496
00:34:14,000 --> 00:34:16,000
Yes.

497
00:34:16,000 --> 00:34:20,000
I believe that science is about abduction.

498
00:34:20,000 --> 00:34:23,000
So like creating this plausible set of hypotheses.

499
00:34:23,000 --> 00:34:26,000
And we're at an event all about creativity.

500
00:34:26,000 --> 00:34:29,000
And humans have that spark of inspiration, don't they?

501
00:34:29,000 --> 00:34:33,000
We just have an idea about plausible hypotheses.

502
00:34:33,000 --> 00:34:39,000
And maybe the world is quite sclerotic and predictable and boring.

503
00:34:39,000 --> 00:34:44,000
But do you think that there's some spark that we won't get from Ayo?

504
00:34:44,000 --> 00:34:46,000
I completely agree with you.

505
00:34:46,000 --> 00:34:48,000
And it's exactly what we use.

506
00:34:48,000 --> 00:34:53,000
We use abduction in our reasoning system to generate hypothesis.

507
00:34:53,000 --> 00:34:54,000
Yes.

508
00:34:54,000 --> 00:35:00,000
So in some simple application areas like in drug design, you use abduction.

509
00:35:00,000 --> 00:35:06,000
You just have many, many, many examples of chemical structures, short activities.

510
00:35:06,000 --> 00:35:10,000
So you can, if you see similar structure, you induce it.

511
00:35:10,000 --> 00:35:13,000
It might show similar activity again.

512
00:35:13,000 --> 00:35:14,000
It's not always.

513
00:35:14,000 --> 00:35:18,000
So still, you cannot infer ultimate truth.

514
00:35:18,000 --> 00:35:20,000
That is why you need experiments.

515
00:35:20,000 --> 00:35:26,000
But abduction, yes, this is something that takes you outside of what you already know.

516
00:35:26,000 --> 00:35:27,000
Yes.

517
00:35:27,000 --> 00:35:28,000
I completely agree.

518
00:35:28,000 --> 00:35:33,000
I think it's the most exciting part.

519
00:35:33,000 --> 00:35:40,000
But again, this is what is technological.

520
00:35:40,000 --> 00:35:44,000
So there are reasoners that can do abduction.

521
00:35:44,000 --> 00:35:49,000
There are languages that can handle it.

522
00:35:49,000 --> 00:35:53,000
So we have many building blocks to build such system.

523
00:35:53,000 --> 00:35:54,000
Yes.

524
00:35:54,000 --> 00:35:55,000
Yes.

525
00:35:55,000 --> 00:35:59,000
And are you excited about the prospect of neurosymbolic architectures?

526
00:35:59,000 --> 00:36:06,000
Or are you still very old school and you like these explicit knowledge representation?

527
00:36:06,000 --> 00:36:10,000
That is a difficult question.

528
00:36:10,000 --> 00:36:14,000
Yes, I'm very excited about what is happening.

529
00:36:14,000 --> 00:36:21,000
I was skeptical about all these natural language processing.

530
00:36:21,000 --> 00:36:24,000
We tried to work with NLP people.

531
00:36:24,000 --> 00:36:30,000
And the results were quite disappointing how much useful information they actually could extract.

532
00:36:30,000 --> 00:36:34,000
And then such progress.

533
00:36:34,000 --> 00:36:36,000
So of course, it is exciting.

534
00:36:36,000 --> 00:36:42,000
And we already started using it in our scientific work.

535
00:36:42,000 --> 00:36:59,000
So at the same time, I believe there is a place for this very clean, crisp model just because you need something as simple as possible just to enable reasoning.

536
00:36:59,000 --> 00:37:07,000
Just because all these reasoning engines are powerful.

537
00:37:07,000 --> 00:37:15,000
But if they can reason over simpler structures, there is more chances they will do it better.

538
00:37:15,000 --> 00:37:25,000
So for practical purposes, from all my experiences, the cleaner you can get, the better it works.

539
00:37:25,000 --> 00:37:36,000
And ultimately, when we have more technological progress, we probably will be able to more reflect how it is in our brain.

540
00:37:36,000 --> 00:37:37,000
Yes.

541
00:37:37,000 --> 00:37:40,000
But there is also robustness and reliability and trustworthiness.

542
00:37:40,000 --> 00:37:41,000
Precisely.

543
00:37:41,000 --> 00:37:47,000
So you actually then can explain how it was done through this explainability.

544
00:37:47,000 --> 00:37:49,000
It's all trustworthiness.

545
00:37:49,000 --> 00:37:53,000
So if people understand how it works, if we can explain, this is how it happened.

546
00:37:53,000 --> 00:37:56,000
This is what was taken as input.

547
00:37:56,000 --> 00:37:58,000
This is how it was done.

548
00:37:58,000 --> 00:38:00,000
And people will trust it more.

549
00:38:00,000 --> 00:38:06,000
And especially like systems that we are working on, applications for cancer research.

550
00:38:06,000 --> 00:38:16,000
What doctor would recommend these drugs if they don't understand how this system came up with the suggestions?

551
00:38:16,000 --> 00:38:17,000
Yes.

552
00:38:17,000 --> 00:38:23,000
And again, the cleaner your representation, the easier to explain it.

553
00:38:23,000 --> 00:38:24,000
I know.

554
00:38:24,000 --> 00:38:25,000
I really agree with you.

555
00:38:25,000 --> 00:38:39,000
The only intuition I have is that if we're designing a novel molecule, for example, even if we knew or could understand how it was created, would we really understand it?

556
00:38:39,000 --> 00:38:41,000
It seems so complicated, doesn't it?

557
00:38:41,000 --> 00:38:43,000
It is.

558
00:38:43,000 --> 00:38:45,000
But again, there are different logics.

559
00:38:45,000 --> 00:38:50,000
So for example, physiologic, you can build system around that.

560
00:38:50,000 --> 00:39:01,000
So ultimately, let's try to build more such systems and then you just force to understand.

561
00:39:01,000 --> 00:39:04,000
So if you can build it, then you understand how it's working.

562
00:39:04,000 --> 00:39:05,000
Yes.

563
00:39:05,000 --> 00:39:09,000
Inevitably.

564
00:39:09,000 --> 00:39:16,000
So I still believe that some portion and put in portion of this can be modeled.

565
00:39:16,000 --> 00:39:20,000
Something probably not in any near future.

566
00:39:20,000 --> 00:39:24,000
It's just too complex, too fast.

567
00:39:24,000 --> 00:39:26,000
Louisa has been an absolute honor.

568
00:39:26,000 --> 00:39:30,000
How can people find out more about you and what you're up to?

569
00:39:30,000 --> 00:39:36,000
We're actually very bad with telling about what we are doing.

570
00:39:36,000 --> 00:39:40,000
That is why I'm so glad that you invited me to talk about it.

571
00:39:40,000 --> 00:39:47,000
We just buried in our work and the best we will publish our scientific papers and it is there with top.

572
00:39:47,000 --> 00:39:52,000
Yes, you teach students, you tell students, it's based on your research, you use examples.

573
00:39:52,000 --> 00:40:00,000
But apart from that, we don't do much and we are really quite keen to change this.

574
00:40:00,000 --> 00:40:09,000
I think a lot of people are intimidated by papers and science and if only people knew how many fascinating ideas there are in there,

575
00:40:09,000 --> 00:40:13,000
I think they just need a hook and it's so interesting.

576
00:40:13,000 --> 00:40:15,000
No, I blame scientists.

577
00:40:15,000 --> 00:40:20,000
They keep this complicated language even if sometimes they don't need it.

578
00:40:20,000 --> 00:40:25,000
So they just keep people away from it.

579
00:40:25,000 --> 00:40:26,000
Yes, they do.

580
00:40:26,000 --> 00:40:32,000
No, I think scientists should do more about explaining their work, how they are doing it.

581
00:40:32,000 --> 00:40:38,000
I always thought that we are doing our work, someone else will.

582
00:40:38,000 --> 00:40:51,000
It's so full that AI has created problems, but no, it's actually us who have to do it and explain and tell and also warn.

583
00:40:51,000 --> 00:40:54,000
Yeah, well now people can ask GPT for.

584
00:40:54,000 --> 00:40:59,000
But what it generates, it's not all of us correct.

585
00:40:59,000 --> 00:41:02,000
That's true, it's not, but it's quite good.

586
00:41:02,000 --> 00:41:04,000
It's good, it's very impressive.

587
00:41:04,000 --> 00:41:06,000
Yeah, it's quite good.

588
00:41:06,000 --> 00:41:07,000
Larissa has been an honor.

589
00:41:07,000 --> 00:41:08,000
Thank you so much.

590
00:41:08,000 --> 00:41:09,000
Thank you.

591
00:41:09,000 --> 00:41:10,000
Thank you very much.

592
00:41:10,000 --> 00:41:11,000
My pleasure.

593
00:41:11,000 --> 00:41:15,000
I'm so impressed by how much you know and understand.

594
00:41:15,000 --> 00:41:16,000
Oh really?

595
00:41:16,000 --> 00:41:17,000
Wow.

596
00:41:17,000 --> 00:41:20,000
So many people around know abduction.

597
00:41:20,000 --> 00:41:21,000
Come on.

598
00:41:21,000 --> 00:41:23,000
We are a computer science podcast.

599
00:41:23,000 --> 00:41:26,000
I think it's one of the most fundamental things.

600
00:41:26,000 --> 00:41:32,000
Because you know, going back to Aristotle, you know, with the weak syllogisms and the strong syllogisms.

601
00:41:32,000 --> 00:41:39,000
And, you know, I think of induction is actually quite interesting because most people use it to include abduction.

602
00:41:39,000 --> 00:41:46,000
So not only creating the map to the hypothesis space, but actually, you know, abduction is just creating the plausible.

603
00:41:46,000 --> 00:41:53,000
I completely agree because if you clear about it, so you clear about logic, so you can have fuzzy concept.

604
00:41:53,000 --> 00:41:57,000
All this probabilistic, but underlying logic must be crisp.

605
00:41:57,000 --> 00:41:58,000
You need to understand.

606
00:41:58,000 --> 00:42:04,000
And if it's abduction, precisely, it's only with some probabilities that it's correct.

