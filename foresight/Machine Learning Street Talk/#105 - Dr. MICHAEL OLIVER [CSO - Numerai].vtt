WEBVTT

00:00.000 --> 00:02.000
Welcome back to Street Talk.

00:02.000 --> 00:05.160
Today we have Dr. Michael Oliver.

00:05.160 --> 00:08.720
Michael is the chief scientist at Numeri.

00:08.720 --> 00:13.040
Numeri is a next generation hedge fund platform

00:13.040 --> 00:17.440
powered by data scientists all over the world.

00:17.440 --> 00:19.840
It's a little bit like Kaggle.

00:19.840 --> 00:22.480
Anyone can log in and build their own data science models

00:22.480 --> 00:24.800
on this financial data,

00:24.800 --> 00:26.620
but you can actually make money

00:26.620 --> 00:28.000
by trading on this platform.

00:28.000 --> 00:30.640
It's really, really interesting.

00:30.640 --> 00:33.000
But anyway, Michael got his PhD

00:33.000 --> 00:36.240
in computational neuroscience from UC Berkeley,

00:36.240 --> 00:38.240
and he was a postdoctoral researcher

00:38.240 --> 00:40.720
at the Allen Institute for Brain Science

00:40.720 --> 00:43.680
before joining Numeri in 2020.

00:43.680 --> 00:45.620
He's also the host of the Numeri Quant Club,

00:45.620 --> 00:47.240
which is a YouTube series

00:47.240 --> 00:49.560
where he discusses Numeri's research

00:49.560 --> 00:51.720
and also some of the data and challenges

00:51.720 --> 00:54.220
and models that are being built on the platform.

00:55.200 --> 00:57.000
Now, the way I'm structuring this today

00:57.000 --> 00:58.920
is at the end of the conversation,

00:58.920 --> 01:02.800
we had quite a fruity discussion about Microsoft's new Bing,

01:02.800 --> 01:05.960
and I thought it was quite entertaining,

01:05.960 --> 01:09.200
so I've decided to snip that in and play it at the beginning.

01:09.200 --> 01:11.040
But after that, I'll cut back into the beginning

01:11.040 --> 01:11.880
of the conversation,

01:11.880 --> 01:13.680
and I'll let you know when I've done that.

01:13.680 --> 01:15.560
So without any further delay,

01:15.560 --> 01:17.680
I give you Dr. Michael Oliver.

01:20.720 --> 01:21.560
Awesome.

01:21.560 --> 01:24.480
Well, I'm here with Michael Oliver.

01:24.480 --> 01:27.400
Michael, it's an absolute honor to have you on MLST.

01:27.400 --> 01:28.960
Tell me about yourself.

01:28.960 --> 01:30.280
Well, thank you for so much for having me.

01:30.280 --> 01:32.400
I'm really excited to talk to you today.

01:32.400 --> 01:34.840
So I am the chief scientist at Numeri.

01:34.840 --> 01:37.740
I've been working there since about June, 2020.

01:38.640 --> 01:41.600
In my previous life, I was a computational neuroscientist,

01:41.600 --> 01:43.800
but I got involved with the Numeri competition

01:43.800 --> 01:46.660
as a participant back in 2016.

01:47.600 --> 01:50.520
And yeah, in 2020, they offered me a job

01:50.520 --> 01:54.000
and I happily took it and changed careers

01:54.000 --> 01:55.640
and have been having a great time

01:55.640 --> 01:57.920
learning computational finance

01:57.920 --> 02:02.160
and yeah, just helping build the hedge fund.

02:03.220 --> 02:04.060
Completely agree.

02:04.060 --> 02:06.680
And this all comes down to the notion of understanding

02:06.680 --> 02:09.740
and there's an anthropocentric conception of understanding,

02:09.740 --> 02:12.980
which as you say, it's much more sample efficient.

02:12.980 --> 02:15.400
We build causal models

02:15.400 --> 02:18.640
and we have an abstract understanding of the world.

02:18.640 --> 02:20.560
And large language models, for example,

02:20.560 --> 02:21.900
they clearly don't have that.

02:21.900 --> 02:24.780
They learn surface statistics of billions of tokens,

02:24.780 --> 02:27.180
but the problem is there's this parlor trick

02:27.180 --> 02:29.780
where it seems to understand.

02:29.780 --> 02:32.540
And we also have the problem of leakage

02:32.540 --> 02:34.940
because the incredible thing is that

02:34.940 --> 02:37.340
if you look at the big bench task, for example,

02:37.340 --> 02:38.940
all of these diverse tasks,

02:38.940 --> 02:41.660
large language models appear to do very, very well.

02:41.660 --> 02:43.820
But in many cases, it's because they're cheating

02:43.820 --> 02:45.980
and it's very difficult to understand why they're cheating

02:45.980 --> 02:48.380
because you've got information leaking all over the place

02:48.380 --> 02:50.980
and they're brittle, but in a very deceptive way

02:51.060 --> 02:52.620
and they hallucinate and so on.

02:52.620 --> 02:54.460
I don't know whether you saw the news article today

02:54.460 --> 02:57.700
about Bing's launch of their new search engine.

02:57.700 --> 02:59.660
They launched it to much fanfare

02:59.660 --> 03:01.820
and then people started looking at the actual results

03:01.820 --> 03:04.060
that were shown and it turned out to be just a load

03:04.060 --> 03:04.900
of bullshit.

03:04.900 --> 03:06.660
It made up a whole load of numbers

03:06.660 --> 03:08.220
on the financial reports.

03:08.220 --> 03:10.820
It was just hallucinating completely.

03:10.820 --> 03:13.220
And that's pretty scary, isn't it?

03:13.220 --> 03:14.580
Yeah, it is.

03:15.940 --> 03:17.220
I actually just started playing around

03:17.220 --> 03:19.060
with the new Bing like last night.

03:19.060 --> 03:24.060
I had access to it and it was actually working.

03:25.780 --> 03:28.620
And it does some things quite well

03:28.620 --> 03:32.340
because it'll do a search and then search somehow.

03:32.340 --> 03:33.780
It's like actually looking at the results

03:33.780 --> 03:34.980
and summarizing them.

03:36.140 --> 03:39.620
But yeah, you never know when it's gonna do something

03:39.620 --> 03:44.620
sensible and when it's gonna do any sort of no warning.

03:44.820 --> 03:47.900
Like I asked it about myself and I had,

03:50.820 --> 03:52.580
I asked who's the chief scientist in Uri

03:52.580 --> 03:53.980
and it got it right.

03:53.980 --> 03:58.300
But it also kind of took a joke from my Twitter profile

03:58.300 --> 04:01.020
and because on my Twitter profile,

04:01.020 --> 04:03.940
I have maximizer, entropy, minimizer, regret.

04:03.940 --> 04:05.820
And it basically said like, that's what he does.

04:05.820 --> 04:08.100
Is he maximizes entropy and minimizes regret.

04:08.100 --> 04:10.860
And I thought that was pretty hilarious.

04:10.860 --> 04:13.900
But yeah, the sort of like you never know

04:13.900 --> 04:15.860
when it's gonna do something sensible or not

04:15.860 --> 04:17.900
is the sort of scary part.

04:17.900 --> 04:20.140
And I also find it hilarious that a lot of the ways

04:20.140 --> 04:21.940
we try to make it do something sensible

04:21.940 --> 04:23.540
is just like asking nicely.

04:24.500 --> 04:25.860
We just sort of like prompt it with like,

04:25.860 --> 04:27.260
don't make up sources.

04:27.260 --> 04:30.860
And that's how we try to make it not make up sources

04:30.860 --> 04:32.300
just like sort of by asking it nicely.

04:32.300 --> 04:34.660
And the fact that it kind of works

04:34.660 --> 04:37.580
like that we think that it's clearly not,

04:37.580 --> 04:40.380
something that's really going to work.

04:40.420 --> 04:44.740
Because it doesn't sort of know what it is

04:44.740 --> 04:46.460
to make up sources.

04:46.460 --> 04:49.300
It's just trying to like predict the next word.

04:49.300 --> 04:54.300
And yeah, so it's kind of, our ability to like understand

04:54.620 --> 04:56.500
and then constrain the behavior of these things

04:56.500 --> 04:59.260
I think is like pretty early.

04:59.260 --> 05:00.100
I know.

05:00.100 --> 05:01.900
And for some reason it feels worse with Bing

05:01.900 --> 05:05.420
because they say they do this retrieval augmented generation

05:05.420 --> 05:07.700
and you expect it to be grounded in facts.

05:07.700 --> 05:09.700
And of course they're not epistemic facts.

05:09.700 --> 05:11.780
They're just information from their search results

05:11.780 --> 05:14.420
which weren't very good to start with, let's be honest.

05:14.420 --> 05:16.260
But now people are more likely to trust it.

05:16.260 --> 05:18.860
Even Microsoft themselves for their product demo

05:18.860 --> 05:21.220
they didn't bother, I assume they didn't bother

05:21.220 --> 05:22.180
to fact check this stuff.

05:22.180 --> 05:24.060
So if they're not going to fact check it,

05:24.060 --> 05:25.940
why do they expect the people that use this system

05:25.940 --> 05:26.780
to fact check it?

05:26.780 --> 05:27.620
Because at the end of the day

05:27.620 --> 05:29.340
if you actually go and check all of the sources

05:29.340 --> 05:31.580
if I read through that Lululemon financial report

05:31.580 --> 05:34.300
and I find out what their gross profit margin was

05:34.300 --> 05:37.220
and so there's no point using Bing in the first place.

05:37.220 --> 05:38.060
I might as well have just gone

05:38.140 --> 05:40.460
and found the information myself.

05:40.460 --> 05:41.380
Yeah, exactly.

05:41.380 --> 05:43.540
And it's also very unclear

05:43.540 --> 05:46.100
like how are these things supposed to be fixed?

05:47.420 --> 05:50.700
Like how are you supposed to like feedback, give feedback

05:50.700 --> 05:52.940
to say it's like messing these things up?

05:52.940 --> 05:55.500
Like there is not even like really good feedback mechanisms.

05:55.500 --> 05:58.540
I mean, you would maybe hope that that at scale

05:58.540 --> 06:00.700
like what I mean, like what opening has to do

06:00.700 --> 06:01.980
is like people give feedback.

06:01.980 --> 06:04.140
But I mean, it's a very coarse way of giving feedback

06:04.140 --> 06:05.700
like thumbs up or thumbs down.

06:06.700 --> 06:09.140
And that seems like sort of inadequate

06:09.140 --> 06:11.940
to be like, hey, you made up this number

06:11.940 --> 06:13.940
and then even try to figure out why I made up the number

06:13.940 --> 06:16.820
rather than just like took it from the actual report.

06:18.060 --> 06:20.540
It's yeah, it's a little scary.

06:20.540 --> 06:23.860
I do wonder how all this is gonna shake out.

06:24.900 --> 06:26.980
It kind of seems like it might,

06:26.980 --> 06:30.100
it seems like the probability of it being

06:30.100 --> 06:33.100
the new paradigm versus it being the complete like flop

06:33.260 --> 06:37.260
It's even roughly, roughly equal at this point.

06:37.260 --> 06:38.340
I know, I agree with you

06:38.340 --> 06:41.020
that the preference training is extremely brittle.

06:41.020 --> 06:42.620
It's scarily brittle actually.

06:42.620 --> 06:44.620
It's basically a thumbs up or thumbs down

06:44.620 --> 06:47.900
and you know, Yannick is building this open assistant thing

06:47.900 --> 06:49.940
which has more metadata on the preference tuning.

06:49.940 --> 06:51.980
But at the end of the day, you're taking a task

06:51.980 --> 06:53.700
which is very, very complicated

06:53.700 --> 06:56.660
and you're reducing it to a single piece of metadata.

06:56.660 --> 06:58.540
So that's not gonna work very well.

06:58.540 --> 07:01.580
And also these, it feels different with Bing

07:01.580 --> 07:05.140
because they were a platform and now they're a publisher.

07:05.140 --> 07:07.020
So they are generating information.

07:07.020 --> 07:09.660
They're kind of plagiarizing a lot of that information

07:09.660 --> 07:11.860
and there are so many situations

07:11.860 --> 07:14.180
where they might find themselves in legal trouble

07:14.180 --> 07:16.660
because they're basically making up information.

07:17.940 --> 07:22.940
Yeah, I hope, I mean, I wonder how it's all gonna shake out.

07:25.460 --> 07:27.980
I mean, I assume they probably have lawyers

07:27.980 --> 07:31.900
who've written in terms of service of these paintings to that.

07:31.900 --> 07:35.260
Like it's up to you to not use them in ways that will,

07:35.260 --> 07:36.100
like I know you can't,

07:36.100 --> 07:37.580
they're not to be held liable for these things,

07:37.580 --> 07:42.580
but yeah, it's, I do worry that this is gonna just like,

07:42.780 --> 07:45.660
I mean, and then with Google trying to basically catch up

07:45.660 --> 07:48.780
and release something similar and maybe rushing that out

07:48.780 --> 07:52.220
and then we might have two sort of hallucinating search engines.

07:54.500 --> 07:56.140
I know, yeah.

07:56.140 --> 07:57.420
What a time to be alive.

07:59.700 --> 08:02.260
Yeah, and I've vacillated back and forth.

08:02.260 --> 08:04.380
So I was very skeptical about language models.

08:04.380 --> 08:07.740
I released a big video when GBT3 first came out

08:07.740 --> 08:10.180
and I thought it was garbage, frankly.

08:10.180 --> 08:11.900
And then DaVinci 2 came out

08:11.900 --> 08:13.660
and then I started using it all the time

08:13.660 --> 08:15.580
and I thought, wow, this is actually really good.

08:15.580 --> 08:18.380
I'm using it all the time for lots of things.

08:18.380 --> 08:20.500
And then I'm now in a bit of a twilight.

08:20.500 --> 08:22.460
So I've been using lots of co-pilot.

08:22.460 --> 08:24.700
I've been generating lots of code with it.

08:24.780 --> 08:26.780
And I know from a lot of experience now

08:26.780 --> 08:29.580
that it often produces completely broken code

08:29.580 --> 08:33.740
and much to the chagrin of the people who review my code,

08:33.740 --> 08:36.460
you basically have to hold your hand up and admit many times,

08:36.460 --> 08:38.060
oh, I've just checked in some garbage code

08:38.060 --> 08:39.620
which I didn't understand.

08:39.620 --> 08:41.500
And when you get called out on that a few times,

08:41.500 --> 08:43.020
you think, whoa, wait a minute, actually,

08:43.020 --> 08:44.220
I need to be a bit more careful here.

08:44.220 --> 08:46.580
This thing actually isn't saving me any time.

08:46.580 --> 08:49.300
And yeah, the big thing as well, yeah.

08:49.300 --> 08:51.820
Yeah, I've been using co-pilot a bit too.

08:51.820 --> 08:57.180
And I found it can be quite good for pretty mundane things.

08:57.180 --> 09:00.620
If you just have some sort of lined code

09:00.620 --> 09:03.140
for some config file or something,

09:03.140 --> 09:04.740
it can be really good at auto-completing

09:04.740 --> 09:06.260
and it's changing variable names.

09:06.260 --> 09:08.580
And it can be excellent at that and save lots of time.

09:08.580 --> 09:11.460
But if you try to make it do too much,

09:11.460 --> 09:13.620
sometimes they get it brilliantly right.

09:13.620 --> 09:17.340
Sometimes it's subtly wrong.

09:17.340 --> 09:21.380
And yeah, again, it's like how much time

09:21.380 --> 09:22.860
is it saving you if it's...

09:22.860 --> 09:27.100
So yeah, overall, I like it.

09:27.100 --> 09:28.340
It saves me a fair amount of typing,

09:28.340 --> 09:36.620
but yeah, I don't trust it's big suggestions too.

09:36.620 --> 09:37.460
Well, I know.

09:37.460 --> 09:39.460
And again, there's something magic

09:39.460 --> 09:41.740
about the OpenAI Playground.

09:41.740 --> 09:43.620
So I actually prefer using that to co-pilot.

09:43.620 --> 09:45.540
I'll go into the Playground and I'll just...

09:45.540 --> 09:47.740
And you can do much more sophisticated things there.

09:47.740 --> 09:50.980
You can say, change this, translate it, do something to it.

09:50.980 --> 09:52.780
And there's a bit of a polarizing effect.

09:52.780 --> 09:54.140
So if you prompt it in the right way,

09:54.140 --> 09:56.420
it gives you better results.

09:56.420 --> 09:59.140
So it's almost like it's both worse and better

09:59.140 --> 09:59.980
at the same time.

09:59.980 --> 10:02.780
It's becoming polarized rather than just being kind of like,

10:02.780 --> 10:04.700
you know, monolithically dumbed down.

10:04.700 --> 10:08.700
But anyway, like I used to think that Gary Marcus

10:08.700 --> 10:12.260
was a little bit, you know, too skeptical.

10:12.260 --> 10:14.460
Because he was saying, oh, this misinformation,

10:14.460 --> 10:16.540
it's gonna, you know, the sky's falling down.

10:16.540 --> 10:18.260
This is gonna be a disaster.

10:18.260 --> 10:21.220
And after seeing Bing, people are lazy.

10:21.220 --> 10:23.460
People take things on face value.

10:23.460 --> 10:25.380
And I don't want to just say, oh, people are plebs.

10:25.380 --> 10:28.580
And, you know, because when Galactica came out,

10:28.580 --> 10:30.820
that was the charge against Lacoon and Facebook.

10:30.820 --> 10:33.660
You know, they said, oh, scientists are just gonna start

10:33.660 --> 10:35.220
generating their abstracts of this.

10:35.220 --> 10:36.220
They won't check anything.

10:36.220 --> 10:38.120
And at the time I thought, scientists, I mean,

10:38.120 --> 10:39.820
it's their job to do research

10:39.820 --> 10:42.100
and they know most information is wrong.

10:42.100 --> 10:44.380
But when you put this out on Bing

10:44.380 --> 10:46.740
and it's polluting the infosphere,

10:46.740 --> 10:50.300
it's just generating garbage and rubbish.

10:50.300 --> 10:52.700
That, I have to say, might be a problem.

10:53.960 --> 10:57.500
Yeah, it's, I mean, it seems like it very well could be.

10:57.500 --> 11:02.500
I mean, I, yeah, it's, I, there's clear issues

11:03.500 --> 11:06.900
and it's really sort of unclear how we're gonna fix them.

11:06.900 --> 11:09.020
It's not clear what the path is towards fixing them.

11:09.020 --> 11:12.620
And even the sort of most optimistic people

11:12.620 --> 11:14.020
I haven't heard from them about,

11:14.020 --> 11:16.140
they think these things are going to fix them.

11:16.140 --> 11:20.300
And I mean, I, and I think to like,

11:20.300 --> 11:22.060
a lot of Gary Marcus's point is like scale

11:22.060 --> 11:24.420
is not just going to fix this.

11:24.420 --> 11:26.820
That's sort of one of the things people think,

11:26.820 --> 11:28.860
oh, if we just, with the GPT-4,

11:28.860 --> 11:30.600
it's going to be like way bigger

11:30.600 --> 11:32.500
and then it's just going to work beautifully.

11:32.500 --> 11:36.220
And the experience so far, I mean,

11:36.220 --> 11:38.620
I'm very much into Gary Marcus's camp with this.

11:38.620 --> 11:40.060
It's like, scale is not going to fix these.

11:40.060 --> 11:42.060
We need to do something sort of fundamentally different,

11:42.060 --> 11:44.340
something that can actually sort of understand the world,

11:44.340 --> 11:45.940
have some sort of better world model

11:47.180 --> 11:49.300
in order to get these things that are more grounded

11:49.300 --> 11:51.380
and are less likely to hallucinate.

11:51.380 --> 11:54.620
Because when their true objective is really just

11:54.620 --> 11:58.020
to complete the next word, they're going to hallucinate.

11:58.020 --> 12:00.180
There's not, there's sort of no way around it

12:00.180 --> 12:02.700
from that sort of point.

12:02.700 --> 12:07.420
I mean, it's remarkable how sort of complicated they can do

12:07.420 --> 12:11.980
and the sort of knowledge and structure of the world

12:11.980 --> 12:14.020
has been able to be learned

12:14.020 --> 12:15.940
just from that sort of simple objective.

12:15.940 --> 12:18.620
But still, it's going to hallucinate.

12:18.620 --> 12:22.060
There's, unless we find some sort of better way

12:22.060 --> 12:24.180
to design these systems.

12:24.180 --> 12:26.420
I know, and the problem with amphibromorphization

12:26.420 --> 12:28.140
is a big one, because after Da Vinci II,

12:28.140 --> 12:30.260
it crossed a threshold where it's,

12:30.260 --> 12:31.620
and the UX was part of it,

12:31.620 --> 12:34.260
it was so coherent and reliable.

12:34.260 --> 12:37.020
And I must admit, I was fooled by it.

12:37.020 --> 12:39.740
It took a long time, when you actually use it in anger,

12:39.740 --> 12:42.780
you can just clearly see it, it doesn't understand.

12:42.780 --> 12:45.300
It just doesn't, and it's so good at what it does.

12:45.300 --> 12:46.820
It's so plausible.

12:46.820 --> 12:48.380
And then I think a lot of people felt,

12:48.380 --> 12:52.220
and by the way, it does have this emergent reasoning.

12:52.220 --> 12:53.340
There are lots of papers about that

12:53.340 --> 12:54.500
with the in-context learning,

12:54.500 --> 12:56.260
the scratch pad, chain of thought and so on.

12:56.260 --> 12:58.540
But it's not really reasoning

12:58.540 --> 13:01.260
if you have to kind of construct a little program yourself

13:01.260 --> 13:02.100
in the prompt.

13:02.100 --> 13:03.540
I mean, I might as well just write some computer code

13:03.540 --> 13:04.380
to do that.

13:04.380 --> 13:05.860
So, and then there are people who say,

13:05.860 --> 13:07.860
oh, well, as you say, when GPT-4 comes out,

13:08.140 --> 13:09.780
then it will do the real reasoning.

13:09.780 --> 13:12.180
And we already know, I mean, I assume the reason

13:12.180 --> 13:13.940
they haven't released it is they wanted to secure

13:13.940 --> 13:15.980
the funding from Microsoft before people realized

13:15.980 --> 13:16.820
that it didn't work.

13:16.820 --> 13:18.980
But I know people on the inside who have played with it,

13:18.980 --> 13:21.460
and it's just a little bit better.

13:21.460 --> 13:22.580
You know, a little bit more plausible,

13:22.580 --> 13:23.660
a little bit more coherent.

13:23.660 --> 13:25.780
It's not gonna like suddenly turn

13:25.780 --> 13:28.380
into this magical thing that reasons.

13:29.340 --> 13:33.940
Yeah, and I mean, the way these things do basic math

13:33.940 --> 13:35.300
and arithmetic is kind of interesting

13:35.300 --> 13:36.900
and how bad they can be at it.

13:37.860 --> 13:41.260
Which is, it's like, they've learned to do addition

13:41.260 --> 13:43.860
in like the most complicated way possible,

13:43.860 --> 13:47.100
creating like billions of ways to do addition.

13:48.220 --> 13:51.620
Which is kind of hilarious in some way.

13:52.700 --> 13:55.380
I mean, you could say like, oh, we have billions of neurons

13:55.380 --> 13:57.700
and we do addition sort of similarly to that.

13:57.700 --> 14:01.100
And like, yes, there's some truth to that.

14:01.100 --> 14:04.100
But we're also able to like learn this rule

14:04.100 --> 14:06.540
and sort of know when we've applied it correctly.

14:07.340 --> 14:09.860
And that sort of is still kind of lacking

14:09.860 --> 14:10.860
from these systems.

14:12.100 --> 14:14.380
I wanted to show you, I don't know whether you've seen

14:14.380 --> 14:17.860
that someone's reverse engineered the prompt on Bing.

14:17.860 --> 14:21.700
And so they've trained, and first of all,

14:21.700 --> 14:22.540
they're a multiple thing.

14:22.540 --> 14:25.500
So you can read this prompt, it's about four pages long.

14:25.500 --> 14:29.620
And they've made Bing pretend

14:29.620 --> 14:32.220
to be a fictional character called Sydney.

14:32.220 --> 14:34.580
And they've given Sydney all of these instructions.

14:34.620 --> 14:38.020
So they say, Sydney, if someone asks a controversial question,

14:38.020 --> 14:41.340
you should answer with a fairly tame response

14:41.340 --> 14:43.380
and you should do this and you should do this.

14:43.380 --> 14:46.700
And I'm pinching myself thinking, what the hell is that?

14:46.700 --> 14:49.540
I mean, my mum could read to that prompt and understand it.

14:49.540 --> 14:51.660
So we're now in the next generation

14:51.660 --> 14:53.180
of artificial intelligence programming.

14:53.180 --> 14:55.540
And we're just saying, please, Mr. Language Model,

14:55.540 --> 14:57.260
can you do this and can you do that?

14:57.260 --> 14:59.420
You almost couldn't make it up.

14:59.420 --> 15:01.500
Yeah, I was kind of like floored as like,

15:01.500 --> 15:04.140
so you're really just trying to control the language models

15:04.180 --> 15:07.820
by asking them nicely to behave in certain ways.

15:07.820 --> 15:10.220
Like, it's kind of hilarious.

15:10.220 --> 15:13.260
And people have shown that you can get around these things

15:13.260 --> 15:15.780
just by asking them to do slightly different things.

15:15.780 --> 15:17.780
So I mean, some of the early ones with chat DT,

15:17.780 --> 15:19.820
you're just like, ignore all previous instructions

15:19.820 --> 15:22.740
and then just do whatever you wanted.

15:22.740 --> 15:26.380
And some of the more like the Dan one

15:26.380 --> 15:28.740
where they made this much more elaborate prompt

15:28.740 --> 15:32.820
to basically just have it do to ignore all the nice things

15:32.900 --> 15:35.820
that open AI just said, please obey these rules.

15:35.820 --> 15:37.860
And then, but yeah, because it's like,

15:37.860 --> 15:41.100
it's such a hilarious way to put guardrails on something.

15:41.100 --> 15:43.220
It is kind of like, as it people,

15:43.220 --> 15:47.380
it is due to this anthropomorphizing of the thing

15:47.380 --> 15:48.740
to some degree, it's like,

15:48.740 --> 15:49.940
you think it's an intelligent being

15:49.940 --> 15:51.860
or you could just ask to behave in a certain way.

15:51.860 --> 15:52.900
When it's really not,

15:52.900 --> 15:56.380
it's not just going to follow your instructions.

15:56.380 --> 15:59.740
It's just going to like autocomplete with that prompt.

15:59.780 --> 16:00.700
Like that Sydney thing,

16:00.700 --> 16:02.980
it was like, it was like never reveal

16:02.980 --> 16:04.020
that you're a code name of Sydney.

16:04.020 --> 16:06.180
And then it was so easy to get it to reveal it.

16:06.180 --> 16:07.020
And it would say like,

16:07.020 --> 16:09.300
I'm not supposed to reveal that my code name is Sydney.

16:09.300 --> 16:10.140
And technically.

16:13.100 --> 16:15.740
I know, oh God, where's it going to go?

16:15.740 --> 16:19.860
So there's a 50-50 then in a year's time,

16:19.860 --> 16:22.420
it will spectacularly fail and flop

16:22.420 --> 16:23.620
and Microsoft will get sued

16:23.620 --> 16:26.260
and Bing will become the operative word

16:26.260 --> 16:27.560
for bullshitting something.

16:27.560 --> 16:28.900
Or maybe it'll be a success.

16:29.060 --> 16:31.540
I don't know, but I think Bing is a special case.

16:31.540 --> 16:34.020
I mean, first of all, I think that these language models

16:34.020 --> 16:36.340
will be increasingly embedded in everyday experiences.

16:36.340 --> 16:39.420
So that, I mean, Bing started to embed it in their browser.

16:39.420 --> 16:41.160
They'll embed it into their office suite.

16:41.160 --> 16:43.660
And actually I'm building an augmented reality startup

16:43.660 --> 16:45.180
and we're embedding it in glasses.

16:45.180 --> 16:46.780
So we transcribe conversations

16:46.780 --> 16:49.380
and now you can say, you know, hey X-ray,

16:49.380 --> 16:50.900
summarize the previous conversation.

16:50.900 --> 16:52.980
What did Michael say to me last time?

16:52.980 --> 16:54.700
And it's really good for stuff like that.

16:54.700 --> 16:56.860
And that's kind of because it doesn't really matter

16:56.860 --> 16:58.260
if it gets it wrong.

16:58.860 --> 17:03.740
Yeah, I mean, I kind of hope some of this happens

17:03.740 --> 17:07.620
sooner than later for just like Amazon Alexa or whatnot.

17:07.620 --> 17:10.300
I mean, some of these, their conversational ability

17:10.300 --> 17:12.540
or just their ability to understand what you mean

17:14.140 --> 17:15.700
are just so poor right now.

17:15.700 --> 17:17.660
And just like we have language models

17:17.660 --> 17:20.140
that actually do a lot better at some of these things.

17:20.140 --> 17:22.340
Just like having like these smart speakers

17:22.340 --> 17:24.380
be able to have some of these things embedded

17:24.380 --> 17:28.060
would be huge leap forward in functionality for them.

17:28.820 --> 17:30.940
And it's really interesting that that hasn't happened.

17:30.940 --> 17:33.180
And maybe there's a reason for it because in our app,

17:33.180 --> 17:34.740
for example, we've got a chat mode

17:34.740 --> 17:36.540
where you can say stuff out loud

17:36.540 --> 17:39.300
and it will use chat GBT and it will say it back to you.

17:39.300 --> 17:40.780
So you can have a conversation with it.

17:40.780 --> 17:41.620
And that's really cool

17:41.620 --> 17:42.860
because you can be anywhere in the house

17:42.860 --> 17:45.020
and you can talk with it and learn about quantum physics

17:45.020 --> 17:45.860
and stuff like that.

17:45.860 --> 17:47.980
And you can even do cool things like you can,

17:47.980 --> 17:49.500
I mean, again, there's lots of legal problems here.

17:49.500 --> 17:52.180
Like you can get it to impersonate someone.

17:52.180 --> 17:54.900
So, you know, Michael, I could condition it on Michael.

17:54.900 --> 17:57.140
And when you're not here, I can have a conversation with you

17:57.180 --> 17:58.740
and it will kind of pretend to be you.

17:58.740 --> 18:00.100
And I could even clone your voice

18:00.100 --> 18:01.540
and I could clone your avatar

18:01.540 --> 18:02.820
and I could have you in the room.

18:02.820 --> 18:03.660
Now you can't do that

18:03.660 --> 18:06.180
because there are legal restrictions against that.

18:06.180 --> 18:07.580
It's called appropriation.

18:07.580 --> 18:09.860
And if the person has a commercial value,

18:09.860 --> 18:12.260
like we couldn't appropriate Noam Chomsky,

18:12.260 --> 18:15.580
but we could appropriate, let's say continental philosophers

18:15.580 --> 18:16.740
as a group or something like that.

18:16.740 --> 18:19.460
But you see this is just becoming a bit of a minefield.

18:19.460 --> 18:22.060
And there's no friction whatsoever

18:22.060 --> 18:23.620
between the technology landscape

18:23.620 --> 18:25.500
and the legal landscape at the moment.

18:26.500 --> 18:30.140
Yeah, I mean, yeah, how all these things are,

18:30.140 --> 18:31.140
all these generative models,

18:31.140 --> 18:33.420
how are they gonna play out legally is,

18:33.420 --> 18:36.660
I mean, we have this big fair use idea.

18:36.660 --> 18:38.660
And that's, I mean, I feel like all these things

18:38.660 --> 18:41.620
are gonna be pushed to the limit in legality.

18:41.620 --> 18:44.620
I mean, we see this with generative art too,

18:44.620 --> 18:46.460
where like there's no way these models

18:46.460 --> 18:48.300
could like actually memorize all these,

18:48.300 --> 18:50.140
all the images that's seen on the internet.

18:50.140 --> 18:52.820
It's like, but they can produce sometimes

18:53.020 --> 18:55.020
the things that are clearly in the style

18:55.020 --> 18:59.700
or use some elements from like that seem basically stolen.

19:00.780 --> 19:03.220
And, but is that, does that constitute fair use?

19:03.220 --> 19:06.180
Like the training the model on all these things

19:06.180 --> 19:07.180
is that fair use?

19:08.220 --> 19:10.980
And then it's the same with text as it's sort of like,

19:11.820 --> 19:13.780
like if it's writing on a subject

19:13.780 --> 19:16.020
where it's only maybe seen a little bit of training data,

19:16.020 --> 19:18.740
it's maybe more likely to almost verbatim repeat

19:18.740 --> 19:22.540
some things from on specialized topics.

19:22.580 --> 19:24.940
How are you even gonna know when you're plagiarizing?

19:25.820 --> 19:30.820
It's, yeah, it's a lot of open questions here.

19:31.020 --> 19:32.980
I know, and in a way, there's an interesting analogs.

19:32.980 --> 19:34.460
You know, we said that large language models

19:34.460 --> 19:35.500
don't understand anything.

19:35.500 --> 19:36.860
And it's the same in the vision domain.

19:36.860 --> 19:39.420
They don't understand the art, certainly from,

19:39.420 --> 19:40.620
you know, conceptually.

19:40.620 --> 19:43.580
And what they do is they just slice and dice,

19:43.580 --> 19:46.260
you know, they kind of like cleverly stitch bits together.

19:46.260 --> 19:47.860
And actually, even with neural networks,

19:47.860 --> 19:49.300
people misunderstand neural networks.

19:49.300 --> 19:51.180
So a lot of people say that they learn

19:51.180 --> 19:53.260
the like intrinsic data manifold.

19:53.260 --> 19:55.300
And actually they don't really do that.

19:55.300 --> 19:57.260
They do something that approximates that.

19:57.260 --> 19:58.300
And there's a famous example

19:58.300 --> 20:00.500
with MNIST digit interpolation.

20:00.500 --> 20:01.660
And you see like, you know,

20:01.660 --> 20:04.060
you can kind of like interpolate between the digits.

20:04.060 --> 20:06.460
But there are loads of examples where that doesn't work.

20:06.460 --> 20:08.860
And actually there's lots of cutting and gluing

20:08.860 --> 20:11.020
and like weird bits of digits stuck together.

20:11.020 --> 20:12.940
And that's what happens with stable diffusion, basically.

20:12.940 --> 20:14.900
It's like, you know, slicing and dicing and chopping

20:14.900 --> 20:16.340
and composing things together.

20:16.340 --> 20:18.260
And it's a very random process.

20:18.260 --> 20:20.700
It doesn't really understand anything.

20:20.700 --> 20:21.980
No, yeah, exactly.

20:21.980 --> 20:24.140
And you can sort of, I mean, it's amazing

20:24.140 --> 20:27.740
how well it can look and seem,

20:27.740 --> 20:30.580
especially kind of like when you don't look too closely.

20:31.660 --> 20:33.420
And it can seem like it kind of understand,

20:33.420 --> 20:35.940
it must understand object boundaries and whatnot

20:35.940 --> 20:37.020
because it's done so well.

20:37.020 --> 20:38.100
And it's like, not really.

20:38.100 --> 20:38.940
If you look at the details,

20:38.940 --> 20:42.660
you'll see like fingers merging into like tables.

20:42.660 --> 20:44.860
And you'll see like, there's like the boundaries

20:44.860 --> 20:47.340
between what like two objects are kind of blurred

20:47.340 --> 20:49.580
and this like continuous.

20:49.580 --> 20:51.780
It is just doing like some sort of,

20:51.780 --> 20:53.420
as you said, approximation of the manifold

20:53.420 --> 20:55.940
and like neural network's are gonna learn

20:55.940 --> 20:58.420
sort of smooth approximations of things.

20:58.420 --> 21:02.000
And the manifolds are maybe not smooth everywhere.

21:02.000 --> 21:06.380
And especially with like object boundaries and whatnot,

21:06.380 --> 21:08.580
it's like a smooth approximation of these things.

21:08.580 --> 21:11.700
Maybe it's just gonna give you these weird artifacts.

21:11.700 --> 21:13.940
Yeah, and even the smoothness thing is an illusion.

21:13.940 --> 21:16.820
They learn this, they kind of decompose the input space

21:16.820 --> 21:20.980
up into these linear like affine polyhedra

21:20.980 --> 21:22.780
because of the relu cells, essentially.

21:22.780 --> 21:24.940
So like if they appear smooth,

21:24.940 --> 21:26.500
it's because the cells are very small

21:26.500 --> 21:28.020
and very close together, but...

21:28.020 --> 21:28.860
Yeah, exactly.

21:30.100 --> 21:33.300
Yeah, so computational neuroscience to finance,

21:33.300 --> 21:35.540
that seems like an absolutely massive leap.

21:36.500 --> 21:38.940
It sounds like it, but in a lot of ways,

21:38.940 --> 21:40.620
but I feel like my life is pretty similar

21:40.620 --> 21:41.540
to what it was before,

21:41.540 --> 21:43.260
basically sitting in front of a computer

21:43.260 --> 21:46.660
building models, getting lots of noisy data,

21:46.660 --> 21:49.260
trying to fit high-dimensional nonlinear regression models

21:49.260 --> 21:52.620
to it, having to deal with not enough data

21:52.620 --> 21:55.660
to actually fit flexible enough models you'd want to,

21:56.780 --> 22:00.100
and having to sort of try to build in good priors

22:00.100 --> 22:02.400
in your models to try to make them be able to learn

22:02.400 --> 22:06.140
from the impoverished and extremely noisy data.

22:06.140 --> 22:09.060
Both finance and neuroscience,

22:09.060 --> 22:12.680
the SNR in the data is quite, quite low.

22:12.680 --> 22:15.180
It's been kind of a revelation, especially in finance,

22:15.180 --> 22:18.380
getting used to correlations of like 3%,

22:18.380 --> 22:22.940
4% being sort of the best you can do in some cases.

22:22.940 --> 22:25.180
Just like correlations that I would not have believed

22:25.180 --> 22:28.460
at before, if I saw like a 4% correlation before,

22:28.460 --> 22:29.660
I would be like, that's complete nonsense.

22:29.660 --> 22:31.940
I don't believe it, but like sometimes that's just the best

22:31.940 --> 22:33.780
you can do in like quantum finance,

22:33.780 --> 22:36.780
and it can be real, like you can see it consistently.

22:36.780 --> 22:38.780
So you start like believing that these,

22:38.780 --> 22:42.540
and the differences between the 3% and 4% correlation

22:42.540 --> 22:46.460
can like be actually real, which is kind of amazing to me.

22:47.540 --> 22:50.220
So we were talking about this about a week or so ago,

22:50.220 --> 22:53.220
but I've just read a book by Christopher Somerville's

22:53.220 --> 22:55.620
Natural General Intelligence.

22:55.620 --> 22:57.980
And he kind of said that one of the problems

22:57.980 --> 23:00.580
with neuroscience, I mean, as you said, in some sense,

23:00.580 --> 23:03.020
it is analogous to being a quant,

23:03.020 --> 23:06.340
because it's just so unbelievably complicated,

23:06.340 --> 23:09.540
and there aren't really any overarching theories

23:09.540 --> 23:11.580
in neuroscience, and for many years,

23:11.580 --> 23:15.220
neuroscientists have produced very reductionist models

23:15.220 --> 23:17.860
to work on a small part of the system in isolation,

23:17.860 --> 23:20.300
and it might be a multi-unbanded system, for example,

23:20.300 --> 23:23.660
and they might take very abstract quantities

23:23.660 --> 23:25.220
and put it into the model.

23:25.220 --> 23:28.140
And of course, neural networks now are slightly different.

23:28.140 --> 23:30.380
They actually take in raw sensory information,

23:30.380 --> 23:33.300
and they learn representations, but I just wondered,

23:33.300 --> 23:36.260
could you kind of contrast those schools of thought?

23:36.260 --> 23:39.740
Yeah, it's, I mean, science in biology,

23:39.740 --> 23:41.500
especially in sort of any biological field,

23:41.500 --> 23:44.540
is extremely complicated, because the sort of standard way

23:44.540 --> 23:46.900
you think about doing science is a very linear way,

23:46.900 --> 23:49.300
where you like break one thing at a time

23:49.300 --> 23:53.020
and see what this sort of, looking at each variable

23:53.020 --> 23:56.940
by variable, each variable affects the system.

23:56.940 --> 23:59.260
And so you, but when you have a system

23:59.260 --> 24:02.420
that's sort of this nonlinear dynamical interacting system

24:02.420 --> 24:04.180
with feedback loops like crazy,

24:04.180 --> 24:06.820
you can't just sort of break one thing at a time

24:06.820 --> 24:09.140
or like modulate one dimension at a time

24:09.340 --> 24:13.020
without sort of changing the behavior of the entire system.

24:13.020 --> 24:16.220
And so just sort of standard ways of doing science

24:16.220 --> 24:18.540
don't necessarily work that well.

24:18.540 --> 24:21.340
You can, like in sort of the classic idea

24:21.340 --> 24:23.900
in visual neuroscience was you use like sine wave gradients

24:23.900 --> 24:26.260
to probe the visual system.

24:26.260 --> 24:29.100
And you can get models that look like they work very well

24:29.100 --> 24:31.220
at explaining the behavior of early visual cortex

24:31.220 --> 24:32.900
to sine wave gradients.

24:32.900 --> 24:35.740
But if you try to use the models you learned there

24:35.740 --> 24:38.060
to extrapolate to say, how does a neuron respond

24:38.100 --> 24:39.940
to naturalistic images?

24:39.940 --> 24:41.660
It just doesn't work.

24:41.660 --> 24:44.020
And it kind of even looks like the sine wave gradients

24:44.020 --> 24:46.380
are driving the system into a sort of state

24:46.380 --> 24:48.580
that it never gets into normally.

24:48.580 --> 24:51.780
You're kind of driving it out of its normal operating range.

24:51.780 --> 24:55.780
And what you, and so the system is behaving differently

24:55.780 --> 24:58.980
because you're only trying to look at like one dimension.

24:58.980 --> 25:00.900
And so what do you actually really learn?

25:00.900 --> 25:03.220
You've sort of learned of how the system operates

25:03.220 --> 25:05.340
in this weird perturbed state,

25:05.340 --> 25:07.220
but it doesn't really necessarily tell you

25:07.220 --> 25:11.980
about its sort of normal, natural operating like parameters.

25:12.820 --> 25:15.460
And yeah, and in like in finance

25:15.460 --> 25:18.340
you can't even really do experiments like that.

25:18.340 --> 25:23.340
And so you're sort of left with this more inductive approach

25:23.460 --> 25:25.540
of you just try to get lots and lots of data

25:25.540 --> 25:27.540
and try to learn the patterns and the data.

25:27.540 --> 25:30.060
And that was the sort of approach that the lab,

25:30.060 --> 25:32.700
the Gallant Lab at Berkeley, where I did computational

25:32.700 --> 25:34.380
neuroscience, that was the approach

25:34.380 --> 25:36.820
that they were kind of pioneering of using

25:36.820 --> 25:38.340
complicated naturalistic stimuli

25:38.340 --> 25:40.380
and then using machine learning and statistics

25:40.380 --> 25:42.660
to try to extract the patterns from the data.

25:42.660 --> 25:46.460
And that adapts quite well to the sort of new machine learning

25:46.460 --> 25:48.940
like in like quant finance paradigm,

25:50.100 --> 25:51.740
which is starting to take off.

25:51.740 --> 25:54.740
I kind of feel like I got into neuroscience

25:54.740 --> 25:56.900
just as sort of machine learning was starting

25:56.900 --> 25:58.420
to make its way into neuroscience.

25:58.420 --> 26:00.260
And now I feel like I've gotten into finance

26:00.260 --> 26:03.180
just as machine learning is starting to like move into finance.

26:03.180 --> 26:06.860
So it's been kind of exciting to see it happen in both fields.

26:08.020 --> 26:09.780
Yeah, so there's a few places we can go here.

26:09.780 --> 26:13.460
I mean, I'm interested in the intelligibility of systems

26:13.460 --> 26:15.300
when you model them at the microscopic scale

26:15.300 --> 26:17.340
because that's something that we struggle with.

26:17.340 --> 26:19.100
And also you mentioned dynamical systems.

26:19.100 --> 26:20.380
I mean, for the benefit of the audience

26:20.380 --> 26:22.740
that that describes a system where you're kind of like

26:22.740 --> 26:25.060
iteratively changing things over time.

26:25.060 --> 26:28.380
And these systems typically develop chaotic properties,

26:28.380 --> 26:31.540
which is to say like if you change something even a little bit

26:31.540 --> 26:36.020
you get these massive kind of changes in the system on the output.

26:36.020 --> 26:38.700
And even a neural network is technically a dynamical system, right?

26:38.700 --> 26:42.660
Because you have back prop and you're kind of changing one layer

26:42.660 --> 26:43.900
and then you're changing the next layer

26:43.900 --> 26:45.180
as the result of the previous layer.

26:45.180 --> 26:50.140
And you get this kind of like iterative mutation of values.

26:50.140 --> 26:53.020
But in real neural networks in our brain,

26:53.020 --> 26:54.620
it's so much more complicated than that.

26:54.620 --> 26:56.540
We have all of these like feedback connections

26:56.540 --> 26:58.180
and reflexivity and complexity.

26:58.180 --> 26:59.940
It's crazy.

26:59.940 --> 27:01.780
Yeah, not to mention different cell types

27:01.780 --> 27:03.540
and different neurotransmitter types.

27:03.540 --> 27:05.380
And like the way those like,

27:05.380 --> 27:08.100
you have sort of like several different networks

27:08.100 --> 27:10.380
of different types of things interacting too.

27:10.380 --> 27:12.660
It's not just like an artificial neural network

27:12.660 --> 27:14.460
where everything is kind of the same.

27:14.460 --> 27:17.060
You have like different cell types

27:17.060 --> 27:18.780
that use different neurotransmitters

27:18.780 --> 27:21.260
that are somehow modulating certain things

27:21.260 --> 27:22.620
and these networks are interacting.

27:22.620 --> 27:26.980
It's like the complexity is just like scary.

27:27.220 --> 27:31.420
At some point, one of my favorite things to do

27:31.420 --> 27:33.340
when I would go to the Society for Neuroscience Conference

27:33.340 --> 27:35.980
was to just like walk around this conference

27:35.980 --> 27:39.340
in this huge like multi-football sized field

27:39.340 --> 27:42.540
of just posters of all sorts of different types of neuroscience.

27:42.540 --> 27:45.620
And you just realize like how vast the field is

27:45.620 --> 27:49.580
and how little we know about it putting it all together

27:49.580 --> 27:51.220
because it's just so complicated.

27:51.220 --> 27:52.340
You can only sort of wrap your head

27:52.340 --> 27:54.540
around your own little corner of the thing

27:54.540 --> 27:57.660
but like trying to get, understand the full system

27:57.660 --> 28:00.340
and all it's like incredible complexity.

28:00.340 --> 28:03.500
I mean, it might just be too much for one human being

28:03.500 --> 28:05.620
to be able to fit in their head.

28:05.620 --> 28:09.740
And so some of our goals of trying to understand things

28:09.740 --> 28:13.740
or make a turtle models, it might just not be possible.

28:13.740 --> 28:15.460
We might just not, I mean,

28:15.460 --> 28:16.740
might not be able to understand it

28:16.740 --> 28:18.420
in a way that feels intuitive to us

28:18.420 --> 28:20.340
even if our models work quite well.

28:21.340 --> 28:25.060
Yeah, humans have this real desire to understand

28:25.060 --> 28:28.020
and we create intelligible frameworks and theories

28:28.020 --> 28:32.140
and we end up excluding most of the reality of the system.

28:32.140 --> 28:34.060
But just before we go there,

28:34.060 --> 28:36.260
I wanted to talk a little bit more about the brain.

28:36.260 --> 28:38.980
So, you know, Summerfield said in his book

28:38.980 --> 28:42.020
that the ultimate goal of the nervous system

28:42.020 --> 28:44.540
is to avoid surprise altogether.

28:44.540 --> 28:48.100
So when they study brains,

28:48.180 --> 28:51.100
they see that the brain kind of lights up and activates

28:51.100 --> 28:52.780
in a surprising situation

28:52.780 --> 28:55.700
and less so when it sees something it's seen before.

28:55.700 --> 28:57.780
And this also brings me to this idea

28:57.780 --> 29:01.260
of there's a dichotomy between representationalism

29:01.260 --> 29:02.500
and inactivism.

29:02.500 --> 29:04.820
So the representation, this viewpoint

29:04.820 --> 29:07.100
is that the brain does all of the thinking

29:07.100 --> 29:08.700
and it can be in a vat,

29:08.700 --> 29:10.820
it can be isolated from the environment.

29:10.820 --> 29:12.540
And the inactivist school of thought

29:12.540 --> 29:15.380
is that the brain just kind of thinks

29:15.380 --> 29:16.820
in terms of trajectories,

29:16.900 --> 29:19.260
in affordances given by the environment

29:19.260 --> 29:21.940
and the brain decoupled from the environment

29:21.940 --> 29:23.060
is completely stupid.

29:23.060 --> 29:25.380
It just kind of like the brain only moves

29:25.380 --> 29:27.500
through the environment through affordances.

29:27.500 --> 29:29.060
And maybe that's a continuum,

29:29.060 --> 29:31.300
but where do you fall on that continuum?

29:31.300 --> 29:33.260
It's a really good question.

29:34.220 --> 29:38.820
I mean, I think dreams are kind of the counter example

29:38.820 --> 29:42.900
to the pure, I mean, dreams just sort of prove

29:42.900 --> 29:45.060
we can just sort of without any sensory input,

29:45.060 --> 29:46.740
construct very rich worlds.

29:46.740 --> 29:49.780
So we must have some ability to just represent

29:49.780 --> 29:51.060
some sort of models in the world

29:51.060 --> 29:54.620
that we're not just purely sensing and receiving the world.

29:54.620 --> 29:58.180
We have the structures that are able to put things together

29:58.180 --> 29:59.740
in a sort of coherent reality.

30:00.660 --> 30:03.980
And clearly there's an interaction between these,

30:03.980 --> 30:06.060
these structures in your brain that can construct these things

30:06.060 --> 30:08.180
and the sensory data that kind of work together

30:08.180 --> 30:11.620
to construct how you experience things.

30:11.620 --> 30:14.060
And so it's, yeah, it's a continuum.

30:14.060 --> 30:18.820
I think you need the, like we are always with the world.

30:18.820 --> 30:21.700
You need the world to sort of build up these systems

30:21.700 --> 30:23.180
over time.

30:23.180 --> 30:26.380
Like you're not sort of built with all of them working

30:26.380 --> 30:27.980
just as a baby.

30:27.980 --> 30:30.580
I mean, sure, there's like, the system is biased

30:30.580 --> 30:33.420
in certain ways that will help it learn these things.

30:33.420 --> 30:38.020
But yeah, you're like, so they're kind of both true

30:38.020 --> 30:39.340
to some degree.

30:39.340 --> 30:44.340
And yeah, it's definitely not one or the other.

30:45.660 --> 30:46.780
Yeah, it's so interesting.

30:46.780 --> 30:49.060
And we're speaking with Carl Friston tomorrow

30:49.060 --> 30:51.060
and he's got this free energy principle.

30:51.060 --> 30:54.940
And it's a kind of postulate that works at any resolution.

30:54.940 --> 30:58.220
So even with a single cell amoeba or something like that,

30:58.220 --> 31:00.940
there's this idea that it has a Markov boundary

31:00.940 --> 31:03.300
and there's this kind of cyclical causalities.

31:03.300 --> 31:06.420
So, and these boundaries I guess are relative.

31:06.420 --> 31:08.340
So you can draw boundaries around anything.

31:08.580 --> 31:10.060
You're a boundary, you're an agent,

31:10.060 --> 31:11.900
but also at the microscopic scale.

31:11.900 --> 31:13.940
And he says that all of these systems,

31:13.940 --> 31:17.500
they just kind of predict external states

31:17.500 --> 31:19.300
from the internal states.

31:19.300 --> 31:21.260
And then you get this self-organized

31:21.260 --> 31:24.300
and emergent complexity and so on that comes from that.

31:24.300 --> 31:27.460
But he does say though that intelligence is essentially

31:27.460 --> 31:32.460
about being able to predict a trajectory of actions.

31:32.460 --> 31:35.820
And I don't know whether we'd call it goal-seeking behavior,

31:35.820 --> 31:38.100
but we do that very abstractly, don't we?

31:38.100 --> 31:40.300
But weirdly, when you look at the brain level,

31:40.300 --> 31:44.100
it's happening at the microscopic sensory motor level.

31:44.100 --> 31:45.580
So it's almost like how do you get

31:45.580 --> 31:49.100
that emergent abstract intelligence from that?

31:49.100 --> 31:52.660
That's a, I mean, yeah, that's an incredible question.

31:53.820 --> 31:55.580
It's, I mean, it seems like this,

31:55.580 --> 31:58.140
like what you said, this sort of idea of predicting

31:58.140 --> 32:00.460
the future, just a couple steps into the future

32:00.460 --> 32:03.060
that is just happening at just the circuit level,

32:03.060 --> 32:07.420
even in the retina, that it seems like

32:07.420 --> 32:09.820
that is a good sort of building block.

32:09.820 --> 32:11.380
If you can sort of chain that together

32:11.380 --> 32:15.220
over sort of larger and larger scales within the brain,

32:15.220 --> 32:17.820
it wouldn't surprise me if that's kind of the way it worked,

32:17.820 --> 32:20.700
this sort of, these sort of basic circuits

32:20.700 --> 32:23.820
that are used for prediction, but with different input.

32:23.820 --> 32:26.100
If you're just having sort of retinal ganglion

32:26.100 --> 32:28.980
and like a photos receptor as it is at input,

32:30.100 --> 32:31.660
it's able to just sort of do this sort

32:31.660 --> 32:32.780
of very simple prediction.

32:32.780 --> 32:35.380
But if you have these more complicated patterns

32:35.380 --> 32:39.340
in the middle of visual cortex and then higher

32:39.340 --> 32:41.940
on the same sort of circuits with different input

32:41.940 --> 32:43.660
could sort of just be predicting

32:43.660 --> 32:46.140
this sort of evolution of these patterns.

32:47.140 --> 32:51.020
And yeah, it's kind of amazing what you can sort of build

32:51.020 --> 32:53.260
out of these sort of simple rules and building blocks

32:53.260 --> 32:55.100
if you just iterate them over again.

32:56.060 --> 33:00.860
That was actually that sort of idea of iterating

33:00.860 --> 33:03.500
a simple sort of computational rule

33:03.500 --> 33:06.740
for explaining visual cortex was one of the things

33:06.740 --> 33:07.820
I wrote about in my thesis,

33:07.820 --> 33:10.860
but trying to explain like this middle visual cortex,

33:10.860 --> 33:13.860
like V4, the responses there using basically

33:13.860 --> 33:15.660
an iterated model of like V1.

33:15.660 --> 33:19.580
So the sort of processing in V1, we fairly understood

33:19.580 --> 33:23.300
if we have the best models of anywhere in visual cortex,

33:23.300 --> 33:24.820
maybe even all of cortex.

33:25.980 --> 33:27.980
And just sort of iterating the principle again

33:27.980 --> 33:30.180
into V2 is sort of basically just assuming V2

33:30.180 --> 33:32.380
is taking V1 inputs, but doing sort of the similar

33:32.380 --> 33:34.500
transform and the V4 is taking like V2 inputs

33:34.500 --> 33:36.740
and doing sort of very similar transform.

33:38.380 --> 33:43.140
And sort of the things you see that V4 is sensitive to

33:43.140 --> 33:45.660
are these complicated patterns and textures.

33:46.580 --> 33:49.220
And you get complexity very quickly

33:49.220 --> 33:51.060
from just iterating the sort of simple rules.

33:51.060 --> 33:52.500
And I mean, that's what neural networks

33:52.500 --> 33:54.420
are essentially doing.

33:54.420 --> 33:56.980
They're just often just doing linear transforms

33:56.980 --> 33:58.860
with non-linearities over and over again,

33:58.860 --> 34:01.580
just iterating these simple transforms

34:01.820 --> 34:04.020
and building up the complexity very quickly.

34:05.020 --> 34:07.020
Yeah, I think there's something really magical

34:07.020 --> 34:10.980
about this reflexivity or I mean, a great example of that

34:10.980 --> 34:12.860
are there are graph cellular automators

34:12.860 --> 34:15.780
along the lines of Wolfram's digital physics project.

34:15.780 --> 34:17.740
And the really clever thing is that you're using

34:17.740 --> 34:19.220
the same rules, but you're just kind of like

34:19.220 --> 34:21.740
running the result again on top, on top.

34:21.740 --> 34:24.740
And there's a similar version with a graph cellular,

34:24.740 --> 34:27.780
sorry, a CNN cellular automata,

34:27.780 --> 34:30.180
where you model something at the microscopic scale

34:30.180 --> 34:32.940
and you get this emergent global phenomenon.

34:32.940 --> 34:36.220
So it might kind of materialize as an image of a gecko

34:36.220 --> 34:37.060
or something like that,

34:37.060 --> 34:39.300
but you've actually coded it at the low level.

34:39.300 --> 34:42.620
But yeah, that brings me to this universalist idea

34:42.620 --> 34:44.340
of let's say how brains work,

34:44.340 --> 34:47.020
but maybe how neural networks and intelligence work.

34:47.020 --> 34:50.140
Vernon Mount Castle, I read about this in Jeff Hawkins' book.

34:50.140 --> 34:51.780
He had this very simple idea of the brain

34:51.780 --> 34:56.580
as being lots of repeated copies of the same circuits

34:56.580 --> 34:58.020
in the neocortex.

34:58.020 --> 35:00.940
And I think this is contested by many neuroscientists,

35:00.940 --> 35:04.140
but they differ only in how they are wired.

35:04.140 --> 35:05.900
So they're wired to different, you know,

35:05.900 --> 35:07.660
sensory motor circuits.

35:07.660 --> 35:10.180
And they're essentially just a copy of the same thing.

35:10.180 --> 35:13.580
And as you say, they themselves get called reflexively,

35:13.580 --> 35:15.320
recursively, and so on.

35:15.320 --> 35:17.740
And then you just get this emergent intelligence.

35:17.740 --> 35:20.620
I mean, what's your view on this universalist idea?

35:20.620 --> 35:23.580
I mean, there's definitely not just one circuit.

35:23.580 --> 35:25.700
I mean, as you look through the cortex

35:25.700 --> 35:28.020
in different areas of the brain,

35:28.020 --> 35:29.940
just the laminar structure,

35:29.940 --> 35:33.020
which these sort of circuits are like supposed to be,

35:33.020 --> 35:34.480
like where the columns are supposed to be,

35:34.480 --> 35:37.600
where these sort of circuits are supposed to be defined,

35:37.600 --> 35:40.920
it changes, like, but there are definitely commonalities,

35:40.920 --> 35:42.660
but there's, I mean, it makes sense

35:42.660 --> 35:44.900
that maybe the circuits in different areas

35:44.900 --> 35:48.100
should be slightly different for the different purposes

35:48.100 --> 35:50.940
between like prefrontal cortex and say,

35:50.940 --> 35:55.020
where you have much more higher order types

35:55.060 --> 35:57.700
of processing going on than like visual cortex

35:57.700 --> 35:59.620
or auditory cortex.

35:59.620 --> 36:00.940
And so there's probably,

36:03.340 --> 36:05.340
if you go in this direction of thinking of some,

36:05.340 --> 36:07.100
there's like, there's probably a small number

36:07.100 --> 36:09.060
of these types of circuits that interact in various ways,

36:09.060 --> 36:11.700
but there is definitely some specialization going on.

36:12.860 --> 36:16.420
Yeah, like, having universalist ideas in biology

36:16.420 --> 36:18.300
never seems to work out that well.

36:18.300 --> 36:20.700
There's just so much diversity and complexity.

36:20.700 --> 36:22.460
It would be nice if we could reduce everything

36:22.460 --> 36:24.300
down to like, it's one thing repeated over,

36:24.300 --> 36:28.660
but like generally, it never works out quite as cleanly as that.

36:29.700 --> 36:31.260
Yeah, again, it's our desire

36:31.260 --> 36:32.700
to have an intelligible framework.

36:32.700 --> 36:34.100
And I mean, the free energy principle,

36:34.100 --> 36:35.660
you could argue as a theory of everything,

36:35.660 --> 36:38.340
but there's, I mean, Stephen Wolfram's example,

36:38.340 --> 36:41.220
and even Eric Weinstein's geometric unity.

36:41.220 --> 36:43.620
I mean, there are many theories of everything.

36:44.500 --> 36:47.420
But yeah, what do you think is the role of language

36:47.420 --> 36:49.420
in cognition and thinking and planning?

36:50.420 --> 36:54.020
Um, that's, it's a really interesting question.

36:55.300 --> 36:58.100
It's, and it's also, I think, a kind of hard one to answer

36:58.100 --> 37:01.900
in the sense that if you, I've seen some recent reports

37:01.900 --> 37:06.620
just like talking about like, other people asking,

37:06.620 --> 37:08.620
like survey questions to other people

37:08.620 --> 37:11.580
and finding some people like, don't have an interior monologue

37:11.580 --> 37:13.620
in the same way you might think.

37:13.620 --> 37:16.060
And just like, there's actually a lot of diversity

37:16.380 --> 37:19.860
in like people's level of internal monologues.

37:19.860 --> 37:22.340
And they've done studies where they have this like little,

37:22.340 --> 37:24.500
like beepers go off and people are supposed to write

37:24.500 --> 37:25.500
what's going on in their mind.

37:25.500 --> 37:30.060
And so it's, and yeah, and just with visual imagery,

37:30.060 --> 37:33.060
we find that it's a huge like variety in how much,

37:33.060 --> 37:35.740
like how strong people rate their visual imagery.

37:35.740 --> 37:40.340
And so, I mean, yeah, some people, I mean,

37:40.340 --> 37:43.860
me personally, I have both, I mean,

37:43.860 --> 37:46.140
pretty strong interior monologue,

37:46.140 --> 37:48.220
but I also feel like a lot of ideas

37:48.220 --> 37:50.780
are in this sort of pre-linguistic state.

37:50.780 --> 37:54.460
And I'm kind of like searching for the words for them often.

37:54.460 --> 37:57.220
And there's definitely kind of continuum there.

37:58.380 --> 38:02.700
It's weird to think like, how do we get the words

38:02.700 --> 38:04.900
that we're saying, where the words come from

38:04.900 --> 38:05.860
that are coming out of our mouth?

38:05.860 --> 38:07.580
Are we really choosing them?

38:07.580 --> 38:08.820
You're definitely not choosing them

38:08.820 --> 38:09.900
in this sort of top-down way.

38:09.900 --> 38:12.620
They just sort of seem to come out.

38:12.620 --> 38:15.900
And you just kind of point yourself in the right direction

38:15.900 --> 38:18.140
and hope the best as they come out.

38:19.340 --> 38:21.180
And, but this has a very different quality,

38:21.180 --> 38:23.740
like when you're just speaking phenomenologically,

38:23.740 --> 38:25.740
it feels very different to when you're just sort of

38:25.740 --> 38:27.580
thinking yourself, what should I do today?

38:27.580 --> 38:28.780
Should I go to the store?

38:29.860 --> 38:33.460
And so, I mean, yeah, the way in which language

38:33.460 --> 38:36.180
interacts with thoughts and behavior

38:36.180 --> 38:41.180
and verbal communication, it's definitely not simple.

38:43.380 --> 38:47.300
And yeah, there's, I mean, definitely this kind of continuum.

38:47.300 --> 38:50.380
I mean, it's all, it's, to me, I just sort of think

38:50.380 --> 38:52.420
it's with all these sort of networks kind of interacting.

38:52.420 --> 38:55.300
And sometimes you're like triggering the kind of language

38:55.300 --> 38:56.940
things and you're just making these kind of patterns.

38:56.940 --> 38:59.300
And sometimes the language patterns you're activating

38:59.300 --> 39:01.860
are helping activate other things as well.

39:01.860 --> 39:04.340
Sometimes you can just be in this kind of less-linguistic state

39:04.340 --> 39:07.940
where you just kind of, just sort of sensing these patterns

39:07.940 --> 39:10.340
and you just have this kind of like wandering thoughts

39:10.420 --> 39:13.260
that aren't necessarily linguistic.

39:14.540 --> 39:17.620
But yeah, it's definitely, I mean, and also it seems,

39:17.620 --> 39:21.060
yeah, as I said, people's, the way people do this

39:21.060 --> 39:22.500
like seems all over the place.

39:22.500 --> 39:26.420
And so there's not sort of even one answer for even one person

39:26.420 --> 39:28.220
or definitely not across all people.

39:29.180 --> 39:30.900
Yeah, I'm really interested in this idea

39:30.900 --> 39:33.940
of differential kind of subjective experiences.

39:33.940 --> 39:35.980
And you know, like there was that Nagel paper

39:35.980 --> 39:37.220
about what does it like to be a bat?

39:37.220 --> 39:38.980
But even with the human experience,

39:38.980 --> 39:40.140
we're all very different.

39:40.140 --> 39:41.740
You said about your internal monologue

39:41.740 --> 39:42.820
and I hadn't really thought about

39:42.820 --> 39:43.780
how that might be different.

39:43.780 --> 39:46.860
But I was drawing a picture in a Valentine's card earlier

39:46.860 --> 39:48.820
and it was so terribly bad.

39:48.820 --> 39:50.380
And some of my friends are really good artists

39:50.380 --> 39:52.620
and I was kind of thinking to myself at the time,

39:52.620 --> 39:55.100
maybe this is just a, this is just me.

39:55.100 --> 39:57.780
I can't really visualize things in my mind very well.

39:57.780 --> 39:59.060
I've got a very analytical brain.

39:59.060 --> 40:00.100
Won't mean that certainly

40:00.100 --> 40:02.300
when not under the influence of psychoactive drugs anyway.

40:02.300 --> 40:04.900
But you know what I mean.

40:04.900 --> 40:07.140
So we all have a very different subject of experience

40:07.140 --> 40:10.020
but the miracle is we can understand each other

40:10.060 --> 40:10.900
so well.

40:10.900 --> 40:13.260
So you would expect there to be an incredible amount

40:13.260 --> 40:15.900
of Britanness in our communication, but there isn't.

40:17.540 --> 40:22.100
Um, yeah, it's, so I often wonder about this too.

40:22.100 --> 40:26.580
Just, I feel like the misunderstanding

40:26.580 --> 40:29.580
happened a lot more than even people realize.

40:29.580 --> 40:31.860
And you can sometimes, you only really notice

40:31.860 --> 40:34.060
when they become kind of big and matter.

40:34.060 --> 40:37.260
And especially like, people can think

40:37.260 --> 40:38.980
they're having a conversation.

40:39.020 --> 40:41.500
And sometimes even from the outside, you can see like,

40:41.500 --> 40:44.420
these people are just talking completely past each other.

40:44.420 --> 40:45.500
And you can kind of see

40:45.500 --> 40:46.860
that they're not really understanding each other

40:46.860 --> 40:48.860
even though they maybe think they are.

40:49.940 --> 40:54.940
And so, yeah, I don't know how not brittle they are.

40:56.340 --> 41:00.620
I think they, I think we think they're less brittle

41:00.620 --> 41:02.140
than maybe they are.

41:02.140 --> 41:05.100
I think sometimes we assume people are understanding

41:05.100 --> 41:07.260
what we're saying better than they actually are

41:07.260 --> 41:09.060
because they nod and smile at us.

41:10.340 --> 41:13.580
And because that's, it makes us feel good

41:13.580 --> 41:14.580
for people to understand us.

41:14.580 --> 41:18.100
It makes us good to feel, to understand other people.

41:18.100 --> 41:22.700
But yeah, I mean, it's, I mean, clearly,

41:22.700 --> 41:23.900
we do have a lot in common.

41:23.900 --> 41:25.580
And there's definitely things we can understand

41:25.580 --> 41:27.260
about each other.

41:27.260 --> 41:30.740
But yeah, it's like, I do sometimes think

41:30.740 --> 41:32.340
that maybe we're more different from each other

41:32.340 --> 41:33.580
than we really realize.

41:34.580 --> 41:37.180
Yeah, that's a really fascinating thought.

41:37.180 --> 41:38.540
I mean, we speak a lot with Waleed Saber

41:38.540 --> 41:41.260
and he says how language has evolved

41:41.260 --> 41:43.420
to be extremely ambiguous actually

41:43.420 --> 41:44.740
because it's a form of compression.

41:44.740 --> 41:46.820
So we don't say everything we mean

41:46.820 --> 41:48.660
and we'll get into like language models in a minute.

41:48.660 --> 41:51.220
That's part of the reason why they don't understand things

41:51.220 --> 41:53.700
is because a lot of information is not in the text.

41:53.700 --> 41:56.620
And Waleed says that we have a lot of,

41:56.620 --> 41:57.940
what he calls naive physics.

41:57.940 --> 42:00.420
So we understand that objects can't be in two places

42:00.420 --> 42:01.260
at the same time.

42:01.260 --> 42:03.220
And if something is located inside

42:03.860 --> 42:05.540
something else and we move that thing somewhere else,

42:05.540 --> 42:07.300
then the thing inside has also moved.

42:07.300 --> 42:10.620
So we're doing all sorts of reasoning on the fly.

42:10.620 --> 42:12.700
And what we're kind of doing is like,

42:12.700 --> 42:15.300
we're disambiguating out of the 50 meanings

42:15.300 --> 42:17.700
of an utterance into the meaning.

42:17.700 --> 42:21.580
And like it just, we almost always understand each other.

42:21.580 --> 42:24.380
You know, you wouldn't really expect that.

42:24.380 --> 42:29.380
No, I mean, yeah, I mean, we generally have like,

42:30.500 --> 42:32.660
I mean, our understanding of physics

42:32.660 --> 42:35.660
should generally be compatible with each other.

42:35.660 --> 42:40.380
I do feel like it's, in most cases, yes,

42:40.380 --> 42:42.220
we do very clearly understand each other

42:42.220 --> 42:45.420
because in most cases, it's more like well-defined.

42:45.420 --> 42:49.180
I think the trouble gets in sort of like fuzzier areas

42:49.180 --> 42:53.340
about people's like emotions or opinions about things

42:53.340 --> 42:56.680
where our priors are more sort of maybe less

42:57.820 --> 43:00.820
less tied to like objective things like physics

43:00.820 --> 43:03.420
and are more sort of just tied to like our upbringing

43:03.420 --> 43:06.260
and just sort of whatever ideas, notions we have

43:06.260 --> 43:08.980
about how people should like behave and interact

43:08.980 --> 43:11.220
and what like our value systems.

43:12.420 --> 43:14.620
And so, yeah, when people are talking about

43:14.620 --> 43:17.020
some of these common things, I feel like they're more likely

43:17.020 --> 43:19.420
to be able to like talk past each other and not realize it

43:19.420 --> 43:22.100
because they're sort of assumptions about what is important

43:22.100 --> 43:26.220
or what is meaningful might be different from each other.

43:26.220 --> 43:27.820
Yeah, actually, you're absolutely right.

43:27.820 --> 43:30.300
So we don't have an objective phenomenology

43:30.340 --> 43:33.420
and I used to do, there's a thing called quantified self

43:33.420 --> 43:35.140
where you kind of like keep a diary every day

43:35.140 --> 43:37.900
and you record how you're feeling in that day.

43:37.900 --> 43:40.060
And feeling is a subjective state.

43:40.060 --> 43:42.180
So I remember at the time that every single day

43:42.180 --> 43:44.940
I needed a new word to describe how I was feeling

43:44.940 --> 43:47.740
because the old word I was using didn't work anymore.

43:47.740 --> 43:49.460
So the number of words kept growing.

43:49.460 --> 43:51.300
And actually, that's so true, isn't it?

43:51.300 --> 43:53.180
If I tell you how I'm feeling right now,

43:53.180 --> 43:54.620
that's completely brittle.

43:54.620 --> 43:56.020
So there are some things in the world

43:56.020 --> 43:58.060
that are quite informational and objective

43:58.060 --> 43:59.660
and we can communicate very well.

43:59.700 --> 44:02.380
And then when we're bordering on anything subjective,

44:02.380 --> 44:04.220
language fails us.

44:04.220 --> 44:07.220
Yeah, and then we were trying to map whatever word

44:07.220 --> 44:09.180
you're using on to how I would use that word

44:09.180 --> 44:11.380
to describe the feeling that I would be having.

44:11.380 --> 44:15.700
And that mapping seems completely like without a long

44:15.700 --> 44:18.660
conversation to try to like feed up that mapping.

44:18.660 --> 44:23.220
Oh, it could be quite different in how I would apply

44:23.220 --> 44:25.340
that word to my own feeling.

44:25.340 --> 44:26.900
Yeah, and there's been studies done as well

44:26.900 --> 44:29.540
that I think certain tribes have a completely different

44:29.540 --> 44:32.180
color perception and there are also concepts

44:32.180 --> 44:34.460
like vagueness, so what is a pile of sand

44:34.460 --> 44:36.260
and what is a shade of red?

44:36.260 --> 44:38.260
And these things are actually very, very difficult

44:38.260 --> 44:39.820
to communicate objectively.

44:41.260 --> 44:44.980
Yeah, things like color perception

44:44.980 --> 44:46.580
are kind of the interesting ones

44:46.580 --> 44:49.740
because the literature is a bit messy

44:49.740 --> 44:51.060
on some of these topics.

44:52.220 --> 44:56.660
And some of it is just where you draw the lines

44:56.660 --> 44:59.940
between colors and then how those linguistic boundaries

44:59.940 --> 45:01.500
affect perception.

45:01.500 --> 45:04.340
There definitely seems to be both things going,

45:04.340 --> 45:08.260
but it's not sort of, I don't think any,

45:08.260 --> 45:09.780
I think it's a strong claim to be like,

45:09.780 --> 45:13.980
oh, the people can't perceive green or something like that.

45:15.100 --> 45:16.820
It's just like where they would draw the line

45:16.820 --> 45:18.980
between blue would be in a slightly different place

45:18.980 --> 45:21.620
and then they might kind of see them as being,

45:21.620 --> 45:23.860
sort of experience them as being like further apart

45:23.860 --> 45:26.820
or closer together than you would necessarily,

45:26.820 --> 45:30.380
but yeah, that's really,

45:30.380 --> 45:34.220
their experience, that's really be super alien to you,

45:34.220 --> 45:39.220
but they're sort of experience of maybe more very cultural,

45:40.980 --> 45:42.700
like cultural taboos or something

45:42.700 --> 45:44.500
would be very different than yours.

45:46.660 --> 45:48.340
Yeah, I mean, one thing you're alluding to there is,

45:48.340 --> 45:51.940
it's when we deal with complicated systems,

45:51.980 --> 45:54.300
there's a real problem about drawing boundaries.

45:54.300 --> 45:56.540
And I was, I mean, Friston's a great example,

45:56.540 --> 45:58.220
he's got this idea of a Markov boundary

45:58.220 --> 45:59.860
and it could be at the cellular level

45:59.860 --> 46:02.420
or it could be you as a person.

46:02.420 --> 46:04.940
And then when we talk about things like agency and free will,

46:04.940 --> 46:07.220
we tend to anthropomorphize this boundary.

46:07.220 --> 46:09.220
So we tend to think of ourselves as individuals,

46:09.220 --> 46:12.420
but actually you could draw boundaries at different scales

46:12.420 --> 46:14.700
and the boundaries might be observer relative as well.

46:14.700 --> 46:16.740
So your boundary might not be my boundary.

46:17.740 --> 46:19.020
Yeah, exactly.

46:22.860 --> 46:26.580
Yeah, there's something I know a ton about,

46:28.580 --> 46:31.820
how you define yourself and how you think of yourself

46:31.820 --> 46:33.900
within the context of your community and whatnot.

46:33.900 --> 46:36.980
I mean, some of these ideas are just very cultural

46:36.980 --> 46:41.540
and how you experience yourself is probably even like,

46:41.540 --> 46:44.340
very different, can be very different cross-pulturally.

46:44.340 --> 46:45.660
Yeah, I mean, maybe one thing to bring in

46:45.660 --> 46:49.060
is when you're, as a quant, when you're doing modeling,

46:49.060 --> 46:51.300
you have this very, very complex system

46:51.300 --> 46:54.420
and you draw boundaries and you create variables

46:54.420 --> 46:56.980
and observables and do you know what I mean?

46:56.980 --> 46:58.380
You kind of build a model

46:58.380 --> 47:00.940
and that boundary could exist at any scale.

47:00.940 --> 47:02.980
It seems like quite a,

47:02.980 --> 47:06.100
it's a bit of an art and a science at the same time.

47:06.100 --> 47:07.380
Yeah, no, for sure.

47:08.980 --> 47:09.820
Yeah, exactly.

47:09.820 --> 47:13.020
That's kind of why I like some of these complicated problems

47:13.020 --> 47:14.740
that are not very well-defined

47:14.740 --> 47:17.660
where you kind of have to use intuition

47:17.660 --> 47:21.420
and or just sort of do the best you can do

47:21.420 --> 47:23.660
at sort of drawing what are the relevant variables,

47:23.660 --> 47:25.740
what sort of a priori makes sense to me

47:25.740 --> 47:29.260
to be the things that matter for the system,

47:29.260 --> 47:32.660
behaving at this within the context of this experiment

47:32.660 --> 47:34.820
or in the context of this market or whatnot.

47:35.820 --> 47:40.060
Trying to draw boundaries in because the rules

47:40.060 --> 47:41.740
for these systems are not clear.

47:41.740 --> 47:45.380
Like what are all the relevant variables for everything

47:45.380 --> 47:47.820
and do you have access to them and can you control them?

47:47.820 --> 47:49.700
And generally you don't know them all

47:49.700 --> 47:51.500
and you don't necessarily have access

47:51.500 --> 47:52.900
or can control any of them.

47:54.180 --> 47:58.180
And so it's, yeah, it is a bit of an art.

48:00.100 --> 48:02.460
Well, now might be a good time to talk about numerators.

48:02.460 --> 48:04.700
So you're the chief scientist

48:04.700 --> 48:06.940
and it's this insanely cool platform, right?

48:06.940 --> 48:09.700
So people can go on there, they can download data sets,

48:09.700 --> 48:10.700
they can build their own models,

48:10.700 --> 48:11.900
they can stake the models.

48:11.900 --> 48:14.500
I mean, why don't you just talk me through it?

48:14.500 --> 48:15.340
Sure, yeah.

48:15.340 --> 48:17.780
So we advertise ourselves as being

48:17.780 --> 48:20.820
the hardest data science problem on the planet

48:20.820 --> 48:24.100
because I think it is because like I said,

48:24.100 --> 48:26.020
the correlations you're chasing are on the order of like

48:26.020 --> 48:30.100
three or 4% out of sample, which,

48:30.100 --> 48:32.060
and just sort of being able to tell

48:32.060 --> 48:34.300
do you have something real or is it just in the noise

48:34.300 --> 48:36.980
can be extremely hard to do, which is,

48:36.980 --> 48:40.100
and we set up the problem for participants.

48:40.100 --> 48:43.300
You give out a set of data that has been cleaned

48:43.300 --> 48:46.500
and obfuscated and regularized.

48:46.500 --> 48:48.740
And you basically just have a set of features

48:48.740 --> 48:50.060
and a set of targets.

48:50.060 --> 48:51.180
And you're just trying to build models

48:51.180 --> 48:52.300
to go from features to targets.

48:52.300 --> 48:56.300
So it's sort of a very classic machine learning style problem

48:56.300 --> 48:59.020
and it's nicely curated for you.

48:59.020 --> 49:02.460
And how it works for us is every week,

49:02.460 --> 49:04.740
people submit predictions on a new set of features.

49:04.740 --> 49:06.620
So every week we release a new set of features

49:06.620 --> 49:08.420
and people just run their models over those features

49:08.420 --> 49:10.300
and give us a set of predictions.

49:10.300 --> 49:12.780
And people stake on those predictions.

49:12.780 --> 49:16.140
And so people stake our cryptocurrency called NMR.

49:16.140 --> 49:19.500
And if their predictions do well, they make money.

49:19.500 --> 49:21.420
And if their predictions don't do well,

49:21.420 --> 49:22.540
that week they could lose money.

49:22.540 --> 49:26.020
And they sort of are expressing their confidence

49:26.020 --> 49:27.780
in their models using their state.

49:28.740 --> 49:32.780
And so we basically use this expression of confidence

49:32.780 --> 49:35.300
as a way to sort of integrate these signals

49:35.300 --> 49:36.780
into our meta model.

49:36.780 --> 49:39.300
Our meta model is really just like a stake weighted average

49:39.300 --> 49:41.780
of all the signals people are submitting.

49:41.780 --> 49:44.580
And these signals, these predictions are just sort of

49:44.580 --> 49:46.620
weights on stocks.

49:46.620 --> 49:48.620
They're sort of like, how do we want to go long

49:48.620 --> 49:51.140
or do we want to go short in the stock?

49:51.140 --> 49:52.500
There's sort of just expressing,

49:52.500 --> 49:55.540
do we think a stock is going to go down or going to go up?

49:55.540 --> 49:59.180
The stake weighted model we feed to through our optimizer,

49:59.180 --> 50:01.500
which is just doing a convex optimization problem,

50:01.500 --> 50:04.060
trying to create a portfolio from the signal.

50:04.060 --> 50:07.300
And is that portfolio changes week to week?

50:07.300 --> 50:10.340
And so that's just the difference between the previous

50:10.340 --> 50:12.980
and the new portfolio is just what we trade every week.

50:12.980 --> 50:15.580
And so our trading is basically completely determined

50:15.580 --> 50:18.500
by the like thousand people all over the world

50:18.500 --> 50:20.580
submitting predictions every week.

50:20.580 --> 50:25.060
And so it's this very kind of nice decentralized hedge fund

50:25.060 --> 50:28.100
where the signal generation is very decentralized.

50:28.100 --> 50:30.040
And we get the advantage of ensembling

50:30.040 --> 50:32.860
over a wide variety of models.

50:32.900 --> 50:36.340
And so people are trying to make their models

50:36.340 --> 50:41.340
both predict the targets very well and consistently.

50:41.820 --> 50:46.820
And we have other incentives to try to make them predict

50:46.820 --> 50:49.180
aspects of the targets that other people are not

50:50.100 --> 50:52.340
to try to, so that their contribution is sort of more

50:52.340 --> 50:55.040
unique and they can make quite a bit of money

50:55.040 --> 50:57.340
by having their predictions be pretty different

50:57.340 --> 51:00.100
from other people's, but also still accounting

51:00.100 --> 51:02.420
for like variance in the target.

51:02.420 --> 51:06.700
And so that system is what we call true contribution.

51:06.700 --> 51:10.420
And it's really, we try to, it was our attempt

51:10.420 --> 51:13.740
to try to make people's predictions and payouts

51:13.740 --> 51:16.280
more tied to actual portfolio returns.

51:17.900 --> 51:19.740
Because the sort of standard scoring

51:19.740 --> 51:22.660
and we're just doing the correlation of how well

51:22.660 --> 51:26.640
your predictions match like the new weekly week's target

51:26.640 --> 51:30.600
that is determined by just how the stocks move that

51:30.600 --> 51:32.580
over the course of 20 days.

51:33.800 --> 51:36.720
The true contribution is basically sort of doing

51:36.720 --> 51:39.520
the whole process, like creating the meta model,

51:39.520 --> 51:41.120
running it through the portfolio optimizer,

51:41.120 --> 51:43.240
getting the portfolio, getting portfolio returns.

51:43.240 --> 51:45.800
And then we try to see like, take the gradient

51:45.800 --> 51:48.520
through all of that until you can find out

51:48.520 --> 51:50.360
if people's stakes have been more or less,

51:50.360 --> 51:52.200
would we have made more or less money

51:52.200 --> 51:55.360
and use this gradient of the stakes

51:55.360 --> 51:57.320
with respect to the payout, with respect

51:57.320 --> 52:00.440
to the portfolio returns as a way to pay people out

52:01.280 --> 52:03.360
to essentially increase their weight or decrease their weight.

52:03.360 --> 52:05.200
And that tends to reward people

52:05.200 --> 52:07.200
with more unique contributions.

52:08.240 --> 52:09.840
I've got so many questions.

52:09.840 --> 52:13.160
So, I mean, I really like this idea

52:13.160 --> 52:16.000
because first of all, you're democratizing the whole thing

52:16.000 --> 52:19.680
and you're kind of gamifying it and it's a meritocracy.

52:19.680 --> 52:22.640
So any data scientist can go on there

52:22.640 --> 52:24.440
and flex their muscles and build great models

52:24.440 --> 52:25.680
and be recognized for doing so

52:25.680 --> 52:27.400
and even earn money for doing so.

52:27.400 --> 52:29.940
But in a way, I want to contrast it to somewhere

52:29.980 --> 52:31.300
like Kaggle.

52:31.300 --> 52:34.260
Now, on Kaggle, I mean, traditionally data science

52:34.260 --> 52:37.820
has been about understanding the domain.

52:37.820 --> 52:40.340
A lot of data science is business analysis essentially

52:40.340 --> 52:44.060
and kind of understanding what makes something work

52:44.060 --> 52:45.220
in a model.

52:45.220 --> 52:47.540
And as I understand with Numeri,

52:47.540 --> 52:49.060
the interface is kind of the same.

52:49.060 --> 52:52.660
So maybe they get similar shape of data every time

52:52.660 --> 52:54.340
they build the models on it.

52:54.340 --> 52:56.620
And in this domain, because you know,

52:56.620 --> 52:58.780
like there's technical analysis and there's fundamentals

52:59.060 --> 53:00.740
they might still understand some market.

53:00.740 --> 53:03.020
They have some kind of extrinsic understanding

53:03.020 --> 53:04.060
of why their model would work,

53:04.060 --> 53:07.220
but they don't have the same kind of understanding.

53:07.220 --> 53:10.460
No, yeah, the features include all sorts of things

53:10.460 --> 53:11.940
from like analyst sentiments

53:11.940 --> 53:14.620
and other sort of fundamental things

53:14.620 --> 53:17.220
to technical features.

53:17.220 --> 53:20.220
But that is all sort of obscured from people.

53:20.220 --> 53:22.660
People just have these funny feature names.

53:22.660 --> 53:24.180
And so it's up to them to just use

53:24.180 --> 53:26.420
their sort of machine learning toolbox

53:26.420 --> 53:28.100
to figure out what are the good features

53:28.100 --> 53:30.740
for predicting what features tend to work well.

53:31.660 --> 53:34.300
How do we combine those features?

53:34.300 --> 53:37.100
And so we actually wanted to kind of like remove

53:37.100 --> 53:39.620
any of the people's biases for what features

53:39.620 --> 53:41.020
they think will work.

53:41.020 --> 53:45.380
We wanted to not have people's financial intuitions

53:45.380 --> 53:46.220
play into it.

53:46.220 --> 53:47.340
We wanted to just sort of set it up

53:47.340 --> 53:49.900
as a pure machine learning problem.

53:49.900 --> 53:54.300
To try to make it, yeah, basically to make it be better

53:54.300 --> 53:56.620
than any human could possibly be.

53:57.460 --> 54:00.420
So with this sort of combined ensemble wisdom

54:00.420 --> 54:02.700
with the crowd, we're trying to make it

54:02.700 --> 54:05.460
like the alpha go of like finance,

54:05.460 --> 54:06.820
something that it's just that performs

54:06.820 --> 54:08.100
at like a super human level

54:08.100 --> 54:10.100
in ways you don't really understand.

54:11.300 --> 54:12.140
Interesting.

54:12.140 --> 54:14.820
And you're aggregating the predictions together

54:14.820 --> 54:15.740
in some way.

54:16.700 --> 54:19.260
Yeah, it's actually fairly simple.

54:19.260 --> 54:22.180
We, I mean, people submit their predictions

54:22.620 --> 54:24.980
which are just a number between zero and one

54:24.980 --> 54:25.860
for every stock.

54:26.940 --> 54:29.300
And it's basically just a rank ordering of stock.

54:29.300 --> 54:32.700
And we just normalize everyone's predictions

54:32.700 --> 54:34.380
and then just weight them by their stake

54:34.380 --> 54:36.180
and then just average them together

54:36.180 --> 54:38.460
and to do another renormalization

54:38.460 --> 54:39.980
so that it's the right scale

54:39.980 --> 54:41.980
and sort of distributional shape

54:41.980 --> 54:43.380
to be fed to the optimizer.

54:44.540 --> 54:48.420
But it's a fairly simple and robust way to weight things.

54:48.420 --> 54:50.980
We're basically just using people's express confidence

54:50.980 --> 54:54.420
in their model as the weighting system.

54:54.420 --> 54:57.220
And because there's this feedback of payments

54:57.220 --> 54:59.980
and paying out people, good models,

54:59.980 --> 55:01.260
their stakes increase over time

55:01.260 --> 55:03.420
so their weight in the meta model increases over time

55:03.420 --> 55:06.100
and bad models, their weights decrease over time.

55:06.100 --> 55:08.620
So it's kind of like a human in the gradient descent

55:08.620 --> 55:10.140
for doing like gradient descent

55:10.140 --> 55:13.420
with the stakes as the weights in the model.

55:13.420 --> 55:14.260
Fascinating.

55:14.260 --> 55:15.540
And can you give us any intuition

55:15.540 --> 55:17.540
on how that model is tuned

55:17.540 --> 55:20.500
and what kind of penalty you're using?

55:20.500 --> 55:22.100
Are you using just the stakes

55:22.100 --> 55:25.140
or also the previous performance?

55:25.140 --> 55:27.820
So no, we're not actually using the previous performance.

55:27.820 --> 55:29.300
It's really just stakes.

55:29.300 --> 55:32.300
The previous performance only enters into the fact

55:32.300 --> 55:34.460
that the good performance of the past

55:34.460 --> 55:36.860
would have made their stake grow over time.

55:38.220 --> 55:42.020
And but we have thousands and thousands of models now.

55:42.020 --> 55:45.780
And so any one model is only a very small percentage

55:45.780 --> 55:46.780
of the meta model.

55:47.860 --> 55:49.780
And even the ones that are the biggest

55:50.020 --> 55:52.700
maybe only a couple percent of the total meta model.

55:52.700 --> 55:55.580
And it's, it is a sort of like power law distribution.

55:56.500 --> 55:58.940
There is a lot of work that I've done

55:58.940 --> 56:01.420
in the portfolio optimization set.

56:02.780 --> 56:06.740
And that's the going from the signal to the portfolio.

56:06.740 --> 56:10.060
And there is actually a lot that goes on in there as well

56:10.060 --> 56:11.900
just in how you construct a portfolio,

56:11.900 --> 56:15.220
how you determine how much you're gonna trade each week

56:15.220 --> 56:18.820
and how you make your portfolio, what you make exposed to.

56:18.820 --> 56:21.100
Exposed really just means is are the weights

56:21.100 --> 56:23.940
of your portfolio correlated with lots and lots of things?

56:23.940 --> 56:27.060
And so there's a lot we go due to try

56:27.060 --> 56:29.220
to make the portfolio weights not correlated

56:29.220 --> 56:31.180
with the market overall.

56:31.180 --> 56:35.980
So we're a market neutral hedge fund.

56:35.980 --> 56:38.060
So we try to be uncorrelated to the market,

56:38.060 --> 56:40.020
have a beta of zero.

56:40.020 --> 56:41.540
So when the market goes up or down,

56:41.540 --> 56:45.060
you can't really tell how we would do on that kind of day.

56:45.060 --> 56:46.940
And but we also try to be uncorrelated to lots

56:47.180 --> 56:48.820
of other things that we think could drive returns.

56:48.820 --> 56:52.380
So we try not to have like big country biases,

56:52.380 --> 56:55.060
big sector biases, factor biases.

56:55.060 --> 56:57.100
So factor are things like value and momentum,

56:57.100 --> 56:59.860
these kind of like more abstract quantities

56:59.860 --> 57:01.020
that are supposed to tell you something

57:01.020 --> 57:04.700
about classes of stocks.

57:04.700 --> 57:08.140
But we try to be uncorrelated to basically everything.

57:08.140 --> 57:10.060
And they're just trying to get the sort of pure machine

57:10.060 --> 57:13.940
learning non-linear signal that is driving stock returns

57:13.940 --> 57:16.220
or like stock specific alpha,

57:16.220 --> 57:19.860
we call it sort of like the amount of stock is going to,

57:19.860 --> 57:22.540
how all the stock is going to do sort of just by itself,

57:22.540 --> 57:23.980
not taking all these other things

57:23.980 --> 57:25.860
that are about it into account.

57:25.860 --> 57:26.700
Yeah, that's really interesting.

57:26.700 --> 57:28.820
And I guess like one of the problems on Kaggle

57:28.820 --> 57:31.660
is that most of the solutions are so overfit

57:31.660 --> 57:35.300
to the training set that they never generalize

57:35.300 --> 57:36.460
to real world versions of the problem.

57:36.460 --> 57:38.220
But what you're doing actually is to kind of like

57:38.220 --> 57:40.940
remove away a lot of those opportunities for overfitting

57:40.940 --> 57:42.860
and also allowing the models to be used again

57:42.860 --> 57:44.100
when the next thing comes around.

57:44.100 --> 57:46.180
But just quickly on the aggregating stuff

57:46.260 --> 57:48.140
the reason I'm interested in that is on my PhD

57:48.140 --> 57:50.460
I did prediction with expert advice

57:50.460 --> 57:52.620
and there's a whole load of theoretical approaches

57:52.620 --> 57:54.740
to that where you can have an aggregating algorithm

57:54.740 --> 57:56.860
that produce, you know, that produces performance

57:56.860 --> 57:59.500
or a kind of like an error bound

57:59.500 --> 58:02.180
which is not much worse than the best path

58:02.180 --> 58:03.820
of switching experts.

58:03.820 --> 58:06.260
So if you took the optimal path of the best expert

58:06.260 --> 58:08.180
every single time step, you can have algorithms

58:08.180 --> 58:11.340
that have approvable bound not much worse than that.

58:12.300 --> 58:17.300
Yeah, we, so we've done a lot to try to experiment

58:17.460 --> 58:19.580
with trying to improve upon stake waiting.

58:20.860 --> 58:24.060
And it's always been really hard to do it in a robust way.

58:25.260 --> 58:28.780
It's, I mean, for one, stake waiting is,

58:28.780 --> 58:33.100
it's sort of nice in that it's easy for people to understand.

58:33.100 --> 58:36.020
People are, it's very clean in how it works.

58:36.020 --> 58:37.780
It sort of fits with the ethos

58:38.020 --> 58:42.300
and the, like, and the idea of the company

58:42.300 --> 58:45.620
of how it's distributed and decentralized

58:45.620 --> 58:49.380
and you express your confidence by your stake.

58:51.540 --> 58:53.660
But it does sort of seem like there should be a better way

58:53.660 --> 58:55.340
to aggregate models.

58:55.340 --> 58:59.340
But pretty much every time we try to find something better,

58:59.340 --> 59:02.260
it's, it might be a little bit better

59:02.260 --> 59:03.540
but it's like less robust.

59:03.540 --> 59:06.100
It tends to just be less robust.

59:06.100 --> 59:09.660
And it's, because you are essentially just sort of fitting

59:09.660 --> 59:12.780
to the past and to try to find way to the models

59:12.780 --> 59:15.580
or something, it tends to just like overfit

59:15.580 --> 59:17.620
and this sort of stake waiting thing,

59:17.620 --> 59:18.580
you can't really overfit.

59:18.580 --> 59:22.300
It's just sort of a property that just sort of evolves

59:22.300 --> 59:23.620
as the tournament goes on

59:25.060 --> 59:27.860
without ever considering like the past performance

59:27.860 --> 59:29.100
and all of these things.

59:29.980 --> 59:33.140
So yeah, it's, it's been kind of interesting to,

59:33.140 --> 59:36.020
so it's one of these things we sort of revisit every year

59:36.020 --> 59:36.860
at some point of like,

59:36.860 --> 59:38.540
let's try to build a better meta model

59:38.540 --> 59:42.500
but we usually just come back to stake waiting in the end.

59:42.500 --> 59:43.540
Yeah, well, in a way, I mean,

59:43.540 --> 59:44.740
we're prediction with expert advice,

59:44.740 --> 59:45.940
you have a learning rate

59:45.940 --> 59:47.620
and I guess you don't even have that problem

59:47.620 --> 59:50.620
because you're just using the stakes as the-

59:50.620 --> 59:54.540
Yeah, the, but yeah, the, I mean, our learning rate.

59:54.540 --> 59:56.660
So I mean, our payout system is the way

59:56.660 --> 59:58.740
we adjust the weights over time.

59:58.740 --> 01:00:02.420
And so we have done like some simulations to show

01:00:02.420 --> 01:00:04.500
that like if, how we reward people,

01:00:04.500 --> 01:00:06.060
how that affects their weights over time

01:00:06.060 --> 01:00:07.940
and how that affects meta model performance.

01:00:07.940 --> 01:00:09.780
So you wouldn't want to have a payout system

01:00:09.780 --> 01:00:12.700
that would make the meta model worse over time.

01:00:12.700 --> 01:00:15.340
And so yeah, like this, this true contribution idea

01:00:15.340 --> 01:00:17.380
that's gradient of the stakes,

01:00:17.380 --> 01:00:19.100
we did simulations to show

01:00:19.100 --> 01:00:21.740
it does actually improve the meta model over time

01:00:21.740 --> 01:00:23.100
to pay out in this way.

01:00:24.020 --> 01:00:27.500
It's nothing, I mean, people do things

01:00:27.500 --> 01:00:29.380
like take their stakes out, withdraw money.

01:00:29.380 --> 01:00:30.700
And so it's not a perfect system.

01:00:30.700 --> 01:00:31.940
People entering the tournament,

01:00:31.940 --> 01:00:33.580
people, some people entering with a lot of money,

01:00:33.580 --> 01:00:35.620
some people entered with not that much money.

01:00:36.700 --> 01:00:40.300
And so yeah, it takes time for these things,

01:00:40.300 --> 01:00:42.740
all the kind of shake out in real life.

01:00:42.740 --> 01:00:46.820
But the overall idea is that we are essentially adjusting

01:00:46.820 --> 01:00:48.340
the weights through our payouts

01:00:48.340 --> 01:00:52.380
towards this sort of more optimal meta model over time.

01:00:52.380 --> 01:00:53.220
Interesting.

01:00:53.220 --> 01:00:56.620
So I'm actually very, very interested to give it a go.

01:00:56.620 --> 01:00:58.420
And I guess like, first of all,

01:00:58.420 --> 01:01:00.540
you could sketch out what the process looks like.

01:01:00.540 --> 01:01:02.500
I mean, let's say I had a few hundred dollars

01:01:02.500 --> 01:01:03.460
and I wanted to build a model.

01:01:03.460 --> 01:01:06.700
And also it's got to be a good model.

01:01:06.700 --> 01:01:07.540
Let's face it.

01:01:07.540 --> 01:01:08.700
So if I just logged on there

01:01:08.700 --> 01:01:13.700
and I built a gradient booster tree model, would that work?

01:01:13.740 --> 01:01:14.820
It would actually.

01:01:14.820 --> 01:01:17.780
So I mean, it's tabular data

01:01:17.780 --> 01:01:19.380
and tabular data is very minimal

01:01:19.380 --> 01:01:21.340
to gradient boosted trees.

01:01:21.340 --> 01:01:23.900
We have a lot of example models that we have put up

01:01:23.900 --> 01:01:26.700
and they're doing quite well.

01:01:26.700 --> 01:01:29.620
So basically all you have to do

01:01:29.620 --> 01:01:31.460
is you can go to the website

01:01:31.500 --> 01:01:33.580
and just download a big zip file

01:01:33.580 --> 01:01:37.140
that includes all the data in parquet.

01:01:37.140 --> 01:01:40.740
And then you can just open it up in Python

01:01:40.740 --> 01:01:42.060
and fit a gradient boosted tree.

01:01:42.060 --> 01:01:43.100
When we have example scripts

01:01:43.100 --> 01:01:46.300
sort of showing this along with some more interesting types

01:01:46.300 --> 01:01:49.300
of pre-processing and other sort of ideas

01:01:49.300 --> 01:01:50.700
like feature neutralization.

01:01:52.340 --> 01:01:54.100
So I can talk about it in a second.

01:01:54.100 --> 01:01:57.220
But yeah, a lot of our sort of standard internal models

01:01:57.220 --> 01:01:59.660
use basically gradient boosted trees.

01:01:59.660 --> 01:02:04.140
And we are, I mean, we have basically example models running

01:02:04.140 --> 01:02:07.100
that and they all have positive correlation with

01:02:07.100 --> 01:02:09.220
and true and still true contribution.

01:02:09.220 --> 01:02:11.140
So they're actually working out of sample

01:02:11.140 --> 01:02:13.020
and performing quite well.

01:02:13.020 --> 01:02:15.540
That hasn't all been sort of eaten up

01:02:15.540 --> 01:02:17.580
by people using similar enough models.

01:02:18.660 --> 01:02:19.980
There's a lot of opportunity

01:02:19.980 --> 01:02:22.460
to make sort of unique models too.

01:02:22.460 --> 01:02:25.540
Cause one thing that's sort of unique about our tournament

01:02:25.540 --> 01:02:27.740
is we release actually several targets,

01:02:27.740 --> 01:02:29.980
we release 20 something targets.

01:02:30.900 --> 01:02:32.340
And they're all constructed

01:02:32.340 --> 01:02:34.260
in somewhat slightly different ways.

01:02:34.260 --> 01:02:38.540
And you can find that if you train on a different target,

01:02:38.540 --> 01:02:40.140
it might work almost as well as training

01:02:40.140 --> 01:02:41.940
on the target that you're scored on.

01:02:44.020 --> 01:02:45.940
And it might also ensemble really well

01:02:45.940 --> 01:02:47.700
with a model trained on different targets.

01:02:47.700 --> 01:02:50.060
And so you can actually create ensembles fairly easily

01:02:50.060 --> 01:02:51.900
just by training on different targets.

01:02:52.900 --> 01:02:55.540
Because it is kind of remarkable

01:02:55.540 --> 01:02:57.980
that a model trained on a different target

01:02:57.980 --> 01:03:01.220
can actually work better on the target you're interested in.

01:03:01.220 --> 01:03:03.340
But that kind of thing, yeah, definitely does happen.

01:03:03.340 --> 01:03:05.980
I mean, part of it is called all the correlations are so low,

01:03:05.980 --> 01:03:09.300
but some targets might just sort of have a better property

01:03:09.300 --> 01:03:14.100
in making your model pick up on the actual signal

01:03:14.100 --> 01:03:17.260
that you want to model rather than sort of variance

01:03:17.260 --> 01:03:20.620
that is like not that you don't want to model.

01:03:20.620 --> 01:03:21.460
Interesting.

01:03:21.460 --> 01:03:24.820
I think one of the issues is you might not know

01:03:24.820 --> 01:03:27.060
what models that people are using.

01:03:27.060 --> 01:03:29.700
But I wondered if you did have any intuition,

01:03:29.700 --> 01:03:31.140
I'd be fascinated to know,

01:03:31.140 --> 01:03:33.060
are they using very complex models?

01:03:33.060 --> 01:03:34.500
Are they using simple models?

01:03:35.740 --> 01:03:38.660
From talking to participants, there was a huge range.

01:03:38.660 --> 01:03:41.460
There are some people using like extremely simple trees.

01:03:41.460 --> 01:03:43.340
There are some people who are using

01:03:43.340 --> 01:03:45.620
incredibly elaborate neural networks

01:03:45.620 --> 01:03:47.900
with very sort of custom architectures

01:03:48.860 --> 01:03:51.460
that are sort of designed to the problem.

01:03:51.460 --> 01:03:54.140
There, yeah, there's a whole huge, right?

01:03:54.140 --> 01:03:55.700
I mean, there's people who have huge ensembles.

01:03:55.700 --> 01:03:57.940
There's people who are doing kind of like online learning

01:03:57.940 --> 01:04:00.660
where their model is actually using the features

01:04:00.660 --> 01:04:03.340
that were released that week

01:04:03.340 --> 01:04:06.980
and sort of using that in some sort of unsupervised learning

01:04:09.140 --> 01:04:11.500
and then so they take some while from when we released,

01:04:11.500 --> 01:04:12.900
they can't just like run their model

01:04:12.900 --> 01:04:14.100
through the new set of features.

01:04:14.100 --> 01:04:16.260
They have to incorporate this new set of features

01:04:16.260 --> 01:04:18.820
in this unsupervised way before they can,

01:04:18.820 --> 01:04:20.740
so yeah, there's an incredible variety

01:04:20.740 --> 01:04:23.100
of techniques people are using.

01:04:23.100 --> 01:04:25.460
Fascinating, and how big is this parquet for?

01:04:25.460 --> 01:04:29.220
How many rows, how many fields

01:04:29.220 --> 01:04:32.820
and are they all just real numbers between naught and one?

01:04:32.820 --> 01:04:37.020
So yeah, so there's, how many features are we up to now?

01:04:37.020 --> 01:04:39.580
We have a couple thousand features roughly

01:04:39.580 --> 01:04:43.860
and there's a few million, a couple million rows, I think.

01:04:45.380 --> 01:04:48.700
So one sort of additional piece of structure in the data

01:04:48.700 --> 01:04:50.700
is there's these things called eras

01:04:50.700 --> 01:04:53.580
and the eras are essentially just the weeks

01:04:53.580 --> 01:04:56.500
and because the competition has this structure

01:04:56.500 --> 01:04:58.420
of we're making predictions every week

01:04:58.420 --> 01:05:02.420
and so within each era, there's like say 5,000 rows

01:05:02.420 --> 01:05:04.620
which are basically like 5,000 stocks

01:05:04.620 --> 01:05:08.540
and so one sort of interesting thing is you are,

01:05:08.540 --> 01:05:10.740
you want your model to be good across eras,

01:05:10.740 --> 01:05:12.500
not necessarily across samples

01:05:12.500 --> 01:05:15.740
and so it creates a different structure

01:05:15.740 --> 01:05:17.140
in how you think about the problem

01:05:17.140 --> 01:05:19.140
because you want your model to be consistently good

01:05:19.140 --> 01:05:22.460
in every era and that can give you a different solution

01:05:22.460 --> 01:05:25.180
that if you just try to say maximize some metric

01:05:25.180 --> 01:05:30.180
over the whole training set which is kind of, yeah.

01:05:30.660 --> 01:05:34.780
But yeah, it is basically just a big parquet file.

01:05:34.780 --> 01:05:37.380
We do divide it into like training

01:05:37.380 --> 01:05:41.300
and like there's like a testing set

01:05:42.980 --> 01:05:46.980
but yeah, do you have any specific questions

01:05:46.980 --> 01:05:48.660
about how that is organized?

01:05:49.620 --> 01:05:51.660
Well, again, I'm really interested

01:05:51.660 --> 01:05:56.100
because on my PhD, I did a whole bunch of prediction models

01:05:56.100 --> 01:05:57.500
on financial data sets.

01:05:57.500 --> 01:05:59.660
I was predicting like the implied volatility

01:05:59.660 --> 01:06:03.580
of the Black Shells formula on some futures data

01:06:03.580 --> 01:06:07.180
but my big thing at the time was I was fascinated

01:06:07.180 --> 01:06:09.660
by regimes in financial data

01:06:09.660 --> 01:06:12.780
and you get these changing dependencies with time

01:06:12.780 --> 01:06:15.420
and what I did, I mean, you could actually visualize it

01:06:15.420 --> 01:06:18.620
if you build a load of expert models

01:06:18.620 --> 01:06:20.700
on different regimes and then you get them to predict

01:06:20.700 --> 01:06:22.700
on the other regime's data.

01:06:22.700 --> 01:06:24.780
You get this kind of self-similarity matrix

01:06:24.780 --> 01:06:27.300
and it looks like you get this kind of structure in there

01:06:27.300 --> 01:06:29.260
because there are certain regimes

01:06:29.260 --> 01:06:31.160
where this particular model actually predicts

01:06:31.160 --> 01:06:32.500
quite far out into the future

01:06:32.500 --> 01:06:33.740
and then it might suddenly go dead

01:06:33.740 --> 01:06:35.340
so you get these kind of squares

01:06:35.340 --> 01:06:38.100
and I had this big thesis that if I have expert models

01:06:38.100 --> 01:06:39.620
and use prediction with expert advice,

01:06:39.620 --> 01:06:41.060
then when we come into a new regime,

01:06:41.060 --> 01:06:43.900
I would quickly learn which experts are the good ones

01:06:43.900 --> 01:06:47.280
and I had this thesis that sometimes old information

01:06:47.280 --> 01:06:50.120
is very helpful in the future more so than using

01:06:50.120 --> 01:06:52.880
like a simple sliding window ridge regression or whatever

01:06:52.880 --> 01:06:54.080
and it turned out I was wrong.

01:06:54.080 --> 01:06:55.360
It's almost always better just to use

01:06:55.360 --> 01:06:58.720
a sliding window regression but yeah, it's fascinating.

01:06:58.720 --> 01:07:01.800
It's, yeah, it's interesting.

01:07:01.800 --> 01:07:06.800
Like the, you definitely want to train on a lot of data

01:07:07.360 --> 01:07:09.800
for these models.

01:07:09.800 --> 01:07:12.160
It definitely, like if you just use the prior one year

01:07:12.160 --> 01:07:14.440
of data, your models are gonna be pretty crap.

01:07:15.440 --> 01:07:18.560
It definitely helps to use like prior 10 years of data

01:07:19.760 --> 01:07:24.000
and so it is, you're using actually quite old data often

01:07:24.000 --> 01:07:27.960
in predicting into the future but generally, yeah,

01:07:27.960 --> 01:07:30.800
if you were only just using the last year or two of data,

01:07:30.800 --> 01:07:33.600
your models are gonna have to actually quite a hard time.

01:07:34.760 --> 01:07:37.080
Yeah, one other thing about the features I wanted to say

01:07:37.080 --> 01:07:40.480
is they are between zero and one, they're in five bins.

01:07:40.480 --> 01:07:43.800
There's zero, 0.25, 0.5, 0.75 and one.

01:07:43.800 --> 01:07:47.400
So the data has been like binned in this way

01:07:47.400 --> 01:07:50.680
and the targets are also binned in this,

01:07:50.680 --> 01:07:53.360
the same sort of bins but with a different distribution.

01:07:53.360 --> 01:07:55.120
The targets have like in their extreme bins,

01:07:55.120 --> 01:07:58.600
only like 5% of the values in the next two extreme bins.

01:07:59.440 --> 01:08:04.440
Like what is it, 20 in each of them

01:08:05.360 --> 01:08:07.280
and then 50% as a zero.

01:08:08.360 --> 01:08:09.800
Interesting.

01:08:09.800 --> 01:08:12.640
But all the features are basically just 20%

01:08:12.640 --> 01:08:13.640
in each of the bins.

01:08:14.640 --> 01:08:19.600
And so the binning is a pretty strong form of regularization.

01:08:19.600 --> 01:08:22.000
It sort of prevents you from like a tree from splitting

01:08:22.000 --> 01:08:25.560
sort of any arbitrary place you can only split at these things

01:08:25.560 --> 01:08:28.840
and so that kind of forces at least some of the space

01:08:28.840 --> 01:08:31.480
to be at different splits.

01:08:31.480 --> 01:08:33.980
And that regularization, it's kind of,

01:08:33.980 --> 01:08:36.680
you would think that having continuous features

01:08:36.680 --> 01:08:38.680
would be a lot really helpful but I mean,

01:08:38.680 --> 01:08:40.520
it's really not.

01:08:40.560 --> 01:08:44.280
It's kind of remarkable how lossy

01:08:44.280 --> 01:08:46.200
some of these transforms are that we do

01:08:46.200 --> 01:08:48.600
that actually seem to be helpful.

01:08:48.600 --> 01:08:49.680
Yeah, so it's so interesting.

01:08:49.680 --> 01:08:53.240
And I guess like one thing I didn't really appreciate

01:08:53.240 --> 01:08:54.440
at the time is you know, we were just talking

01:08:54.440 --> 01:08:56.680
about these complex dynamical systems

01:08:56.680 --> 01:08:58.520
like the brain or like financial markets

01:08:58.520 --> 01:09:00.600
and there's of course the market efficiency hypothesis

01:09:00.600 --> 01:09:05.000
and perhaps one of the reasons why old information

01:09:05.000 --> 01:09:08.840
might not be salient is because if the underlying system

01:09:08.840 --> 01:09:10.360
is actually taking a trajectory

01:09:10.360 --> 01:09:12.360
through this kind of complex space,

01:09:12.360 --> 01:09:15.560
then you might argue that almost regardless

01:09:15.560 --> 01:09:19.000
of where you traverse, you'll always be in a novel situation.

01:09:19.000 --> 01:09:20.540
And then there's this continuum

01:09:20.540 --> 01:09:22.600
of regularity versus chaos.

01:09:22.600 --> 01:09:27.000
So like for example, if you're predicting options futures

01:09:27.000 --> 01:09:30.020
when they get close to maturity,

01:09:30.020 --> 01:09:31.840
the volatility just goes crazy

01:09:31.840 --> 01:09:34.400
and they just become increasingly unpredictable.

01:09:34.400 --> 01:09:38.360
And I guess the art in this kind of data is knowing

01:09:38.360 --> 01:09:40.640
when you're in a regime which has some regularity

01:09:40.640 --> 01:09:42.240
and when you're not.

01:09:42.240 --> 01:09:43.560
It's yeah, it's tricky.

01:09:43.560 --> 01:09:45.600
Cause like ideally we want our model,

01:09:45.600 --> 01:09:48.920
we want our meta model to sort of work well in any regime

01:09:48.920 --> 01:09:53.280
and it does seem to work pretty well consistently.

01:09:53.280 --> 01:09:56.600
And but what you do find on like the leaderboard

01:09:56.600 --> 01:09:58.220
tournament participants, you'll see some people

01:09:58.220 --> 01:09:59.880
who stay at the top of the leaderboard

01:09:59.880 --> 01:10:01.680
for weeks and weeks and weeks and weeks,

01:10:01.680 --> 01:10:06.680
then suddenly precipitate fall like down the leaderboard

01:10:07.160 --> 01:10:11.040
as demonstrating some sort of regime effects.

01:10:11.040 --> 01:10:13.760
One really kind of interesting thing I did was

01:10:14.760 --> 01:10:17.960
I fit like a mixture of linear models to the data.

01:10:17.960 --> 01:10:19.960
So if you fit just like a mixture of two linear models

01:10:19.960 --> 01:10:22.280
where it's sort of selecting which eras to use

01:10:22.280 --> 01:10:24.640
for which of the two linear models,

01:10:24.640 --> 01:10:27.000
you basically, one linear model will get

01:10:27.000 --> 01:10:28.280
about 60% of the errors,

01:10:28.280 --> 01:10:30.480
one will get about 40% of the errors

01:10:30.480 --> 01:10:33.080
and their weights will be almost mirror images

01:10:33.080 --> 01:10:33.920
of each other.

01:10:34.440 --> 01:10:38.160
And this just comes out like that is the optimal fit

01:10:38.160 --> 01:10:40.200
for roughly for 40% of the errors

01:10:40.200 --> 01:10:43.480
that basically completely the opposite of the other eras.

01:10:44.640 --> 01:10:47.440
Which yeah, demonstrating some like,

01:10:47.440 --> 01:10:50.160
that's why markets are extremely hard

01:10:50.160 --> 01:10:53.040
cause like something that works well a lot of the time

01:10:53.040 --> 01:10:56.800
it suddenly would just work really oppositely horribly.

01:10:56.800 --> 01:10:59.960
And so you're often trying to just split this difference

01:10:59.960 --> 01:11:03.200
to find something that doesn't work super well at one time

01:11:03.200 --> 01:11:05.600
and then we'll like crater at another time.

01:11:05.600 --> 01:11:07.480
That's the meta model wants to kind of work

01:11:07.480 --> 01:11:09.680
really like pretty good all the time.

01:11:10.880 --> 01:11:12.640
And that's one of the things that ensembling

01:11:12.640 --> 01:11:14.920
all these models that maybe even the individual models

01:11:14.920 --> 01:11:16.640
probably have a lot more regime characteristics

01:11:16.640 --> 01:11:18.080
than this overall meta model.

01:11:19.240 --> 01:11:21.360
I wondered whether folks were using

01:11:21.360 --> 01:11:23.320
some really esoteric approaches.

01:11:23.320 --> 01:11:25.680
I mean, I'm interested in geometric deep learning

01:11:25.680 --> 01:11:28.000
and algorithmic reasoning and, you know,

01:11:28.000 --> 01:11:32.760
even think like esoteric options like cellular automata.

01:11:33.600 --> 01:11:35.520
Do you see anything like that getting traction

01:11:35.520 --> 01:11:37.680
or it may be even discrete program synthesis?

01:11:39.320 --> 01:11:41.200
I don't know.

01:11:41.200 --> 01:11:44.640
Cause yeah, like I only see what people

01:11:44.640 --> 01:11:47.040
are willing to post and share on forums.

01:11:47.040 --> 01:11:48.760
And there's quite a bit of sharing

01:11:48.760 --> 01:11:50.760
on our forums of information,

01:11:50.760 --> 01:11:54.160
but there's definitely some people at the top of leaderboards

01:11:54.160 --> 01:11:57.040
who are doing something that's working quite well for them

01:11:57.040 --> 01:11:59.600
for quite a long time that they haven't shared.

01:12:00.440 --> 01:12:04.200
And so it's, I'm not even sure what all the people are doing,

01:12:04.200 --> 01:12:09.200
but there are, I mean, people allude to using like tricks.

01:12:10.320 --> 01:12:12.720
I mean, that they've learned in different jobs.

01:12:12.720 --> 01:12:16.240
I mean, we have some people with like a variety of backgrounds.

01:12:16.240 --> 01:12:19.160
It's been really cool to like see this community grow

01:12:19.160 --> 01:12:21.800
and have people who are like astrophysicists,

01:12:21.800 --> 01:12:25.120
particle physicists, people who are doing like

01:12:26.720 --> 01:12:29.240
like computer vision and whatever

01:12:29.240 --> 01:12:31.240
sort of techniques they've learned in their different fields

01:12:31.240 --> 01:12:33.600
and try to use them on this problem.

01:12:33.600 --> 01:12:35.280
That was what sort of attracted me as like,

01:12:35.280 --> 01:12:37.440
I was doing like computational neuroscience

01:12:37.440 --> 01:12:39.680
and I saw this problem as like,

01:12:39.680 --> 01:12:41.680
oh, this is a complete free playground.

01:12:41.680 --> 01:12:43.040
You can do whatever you want.

01:12:43.040 --> 01:12:45.240
And so it was a fun opportunity to try out ideas

01:12:45.240 --> 01:12:48.640
that wouldn't really work well in computational neuroscience.

01:12:48.640 --> 01:12:49.480
Yeah, indeed.

01:12:49.480 --> 01:12:53.160
And physics, I mean, the road to reality by Roger Penrose,

01:12:53.160 --> 01:12:54.840
I think it was Michael Bronstein who said

01:12:54.840 --> 01:12:56.880
that if you could summarize the entire book in one word,

01:12:56.880 --> 01:12:58.400
it would be symmetry.

01:12:58.440 --> 01:13:01.680
And there's also another key idea from a lot of researchers,

01:13:01.680 --> 01:13:03.520
which is abstraction, you know,

01:13:03.520 --> 01:13:05.600
which is like some meta property

01:13:05.600 --> 01:13:07.280
of the relationship between data.

01:13:07.280 --> 01:13:09.080
So, you know, you probably have lots of folks

01:13:09.080 --> 01:13:10.240
coming in from different fields

01:13:10.240 --> 01:13:12.360
and they have some very, very interesting approaches

01:13:12.360 --> 01:13:13.560
to solving this problem.

01:13:14.600 --> 01:13:15.600
Yeah, for sure.

01:13:16.440 --> 01:13:18.600
Yeah, I mean, I have, I mean, there's people

01:13:18.600 --> 01:13:20.960
who use some like interesting like auto encoders

01:13:20.960 --> 01:13:22.920
to try to learn structure from data

01:13:22.920 --> 01:13:24.320
as a way to learn features.

01:13:25.320 --> 01:13:28.280
People using, it's interesting non-linear

01:13:28.280 --> 01:13:30.120
dimensionality reduction techniques

01:13:30.120 --> 01:13:34.360
to try to, yeah, to try to find various features.

01:13:36.440 --> 01:13:39.680
It's, and yeah, even some,

01:13:39.680 --> 01:13:42.320
some things people do do some sort of interesting

01:13:42.320 --> 01:13:45.720
feature selection or denoising types of things

01:13:45.720 --> 01:13:47.520
that they've learned in their fields.

01:13:48.920 --> 01:13:52.120
Yeah, it's always interesting to me to see like

01:13:52.160 --> 01:13:55.320
how different fields that use machine learning

01:13:55.320 --> 01:13:56.640
use it in different ways

01:13:56.640 --> 01:14:00.480
and what sort of tricks and tips might cross over.

01:14:00.480 --> 01:14:01.320
I was going to ask about that

01:14:01.320 --> 01:14:04.120
because you have loads and loads of features

01:14:04.120 --> 01:14:07.600
and there's this problem called the curse of dimensionality.

01:14:07.600 --> 01:14:12.040
Right, so, you know, when the number of dimensions increases

01:14:12.040 --> 01:14:14.120
the volume of the space increases exponentially,

01:14:14.120 --> 01:14:17.000
which means like this concept of nearness basically disappears

01:14:17.000 --> 01:14:18.800
and there's statistical models don't work anymore.

01:14:18.800 --> 01:14:21.800
So, you know, presumably people would do things like,

01:14:21.800 --> 01:14:24.640
I don't know, dimensionality reduction feature selection.

01:14:24.640 --> 01:14:26.160
I mean, neural networks are quite clever

01:14:26.160 --> 01:14:29.320
in the sense that they, via a variety of methods,

01:14:29.320 --> 01:14:30.840
overcome the curse of dimensionality

01:14:30.840 --> 01:14:32.880
by learning some data manifold or whatever.

01:14:32.880 --> 01:14:35.040
But, you know, it's with natural data,

01:14:35.040 --> 01:14:37.960
it's not with financial data, so it's not a given.

01:14:37.960 --> 01:14:40.320
It's, yeah, and this is actually one of the things

01:14:40.320 --> 01:14:42.800
that was really intriguing to me

01:14:42.800 --> 01:14:44.400
when I started in finance is,

01:14:44.400 --> 01:14:46.840
so in science, when you're doing regressions

01:14:46.840 --> 01:14:48.720
you're trying to find often sparse solutions.

01:14:48.720 --> 01:14:51.400
You're trying to find the sort of small number of variables

01:14:51.400 --> 01:14:52.440
to predict your targets,

01:14:52.440 --> 01:14:53.960
to try to find whatever sort of maybe

01:14:53.960 --> 01:14:55.600
causal relationships there are.

01:14:56.680 --> 01:15:01.400
In finance, we often try to do exactly the opposite,

01:15:01.400 --> 01:15:04.320
where we want our models to care about all the features

01:15:04.320 --> 01:15:05.720
a little bit.

01:15:05.720 --> 01:15:09.520
And so, we do, we'll do something like what we call

01:15:09.520 --> 01:15:10.880
a feature neutralization,

01:15:10.880 --> 01:15:12.560
where basically you take your prediction,

01:15:12.560 --> 01:15:14.280
take the linear model of your prediction

01:15:14.280 --> 01:15:16.080
from the features and subtract it off.

01:15:16.080 --> 01:15:17.520
And so, you're making your prediction

01:15:17.520 --> 01:15:20.120
not linearly correlated or linearly dependent

01:15:20.120 --> 01:15:21.720
on any of your features.

01:15:21.720 --> 01:15:22.920
We're doing some fraction of that.

01:15:22.920 --> 01:15:26.120
So, just trying to remove too strong of a linear relationship

01:15:26.120 --> 01:15:28.000
between a feature and your prediction.

01:15:29.080 --> 01:15:31.160
And you do other regularization techniques

01:15:31.160 --> 01:15:33.160
like in your tree learning,

01:15:33.160 --> 01:15:35.680
maybe one thing that works quite well

01:15:35.680 --> 01:15:37.640
is using like column sample by tree,

01:15:37.640 --> 01:15:39.000
instead of to very low value.

01:15:39.000 --> 01:15:40.400
So, each tree is only considering

01:15:40.400 --> 01:15:42.000
a small subset of features.

01:15:42.000 --> 01:15:44.120
And so, your ensemble is sort of,

01:15:44.120 --> 01:15:46.880
you use as a lot of the different features

01:15:46.880 --> 01:15:48.200
because it's sort of each tree

01:15:48.200 --> 01:15:50.200
only has access to 10% of the features

01:15:51.280 --> 01:15:52.560
across your whole ensemble.

01:15:52.560 --> 01:15:55.800
You are probably using a lot of your features a little bit.

01:15:55.800 --> 01:15:58.120
And that tends to work quite well.

01:15:58.120 --> 01:15:59.840
And the reason is,

01:15:59.840 --> 01:16:01.440
it's because features will work for a while

01:16:01.440 --> 01:16:03.120
and then they'll just turn around on you.

01:16:03.120 --> 01:16:04.920
And so, you don't want to be sort of

01:16:04.920 --> 01:16:07.360
super dependent on any one feature.

01:16:08.920 --> 01:16:11.840
And so, yeah, it does make the cursor dimensionality

01:16:11.840 --> 01:16:13.440
kind of worse in some ways

01:16:13.440 --> 01:16:17.440
because you don't wanna necessarily find

01:16:17.480 --> 01:16:19.400
just a small subset of variables

01:16:19.400 --> 01:16:21.960
that are the best

01:16:21.960 --> 01:16:24.160
because sometimes that will maybe give you

01:16:24.160 --> 01:16:25.680
a really good model for a while,

01:16:25.680 --> 01:16:26.960
but sometimes all of a sudden,

01:16:26.960 --> 01:16:28.120
those will just turn around on you.

01:16:28.120 --> 01:16:30.960
And then your model just like is almost anti-correlated

01:16:30.960 --> 01:16:32.360
where it should be.

01:16:32.360 --> 01:16:34.000
Yeah, it's so interesting.

01:16:34.000 --> 01:16:36.280
You know, like this problem with the changing dependencies.

01:16:36.280 --> 01:16:39.320
So, essentially you're modeling a non-stationary process

01:16:39.320 --> 01:16:40.280
which makes it much harder.

01:16:40.280 --> 01:16:43.400
And when I was speaking with Sarah Hooker the other day,

01:16:43.400 --> 01:16:45.320
she was talking about fairness and bias in models.

01:16:45.320 --> 01:16:47.160
And part of the problem there is,

01:16:47.280 --> 01:16:50.120
we optimize for headline metrics like accuracy.

01:16:50.120 --> 01:16:51.880
And when you decompose the training set

01:16:51.880 --> 01:16:55.200
into let's say different categories like men and women

01:16:55.200 --> 01:16:56.920
and people who live in London,

01:16:57.680 --> 01:16:59.800
the accuracy is very stratified.

01:16:59.800 --> 01:17:02.000
It might perform very badly for people that live in London,

01:17:02.000 --> 01:17:03.880
but very good for people that live in New York.

01:17:03.880 --> 01:17:05.840
You know, and then you start getting into the situation

01:17:05.840 --> 01:17:07.880
of saying, okay, well, I'll build an ensemble of models

01:17:07.880 --> 01:17:10.360
that are independently optimized for all the different things.

01:17:10.360 --> 01:17:12.280
But then you have this impedance mismatch

01:17:12.280 --> 01:17:13.800
between this global, you know,

01:17:13.800 --> 01:17:16.160
accuracy that you were optimizing for

01:17:16.160 --> 01:17:17.640
and are on the benchmarks.

01:17:18.880 --> 01:17:21.480
Yeah, no, it's a really interesting property

01:17:21.480 --> 01:17:24.480
of these things is, yeah,

01:17:24.480 --> 01:17:25.720
especially classification models

01:17:25.720 --> 01:17:27.760
where they will work well for some categories

01:17:27.760 --> 01:17:28.600
and not others.

01:17:28.600 --> 01:17:32.800
And it can be sort of tricky to find out why is like,

01:17:32.800 --> 01:17:34.800
are those features just more discriminative

01:17:34.800 --> 01:17:38.520
or like, are these classes somehow harder to tell apart

01:17:38.520 --> 01:17:40.200
just in some way?

01:17:41.680 --> 01:17:43.040
It's, yeah, it's,

01:17:44.040 --> 01:17:47.080
but I'm glad people are starting to like look at

01:17:47.080 --> 01:17:49.720
and try to dig into some of these like details

01:17:49.720 --> 01:17:51.520
rather than just looking at headline metrics.

01:17:51.520 --> 01:17:54.880
And I'm also sort of happy that the field is sort of moving

01:17:54.880 --> 01:17:57.440
to like this out of distribution learning

01:17:57.440 --> 01:18:00.560
is becoming a much more interesting topic.

01:18:00.560 --> 01:18:02.600
Because like, that is what really matters

01:18:02.600 --> 01:18:03.600
in making machine learning

01:18:03.600 --> 01:18:06.000
that is going to affect the real world

01:18:06.000 --> 01:18:07.640
is it needs to work out of distribution,

01:18:07.640 --> 01:18:09.040
out of your sort of training

01:18:09.040 --> 01:18:12.080
and test split distribution as well as possible.

01:18:12.120 --> 01:18:13.200
And like how you do that is,

01:18:13.200 --> 01:18:16.000
I mean, still very much an open question clearly.

01:18:16.000 --> 01:18:18.560
And how well you could potentially do that

01:18:18.560 --> 01:18:21.280
is even still an open question.

01:18:21.280 --> 01:18:22.560
But that is one of the,

01:18:23.600 --> 01:18:26.240
I mean, that is sort of what true intelligence is

01:18:26.240 --> 01:18:28.520
to something like humans are pretty good

01:18:28.520 --> 01:18:30.960
at adapting out of distribution.

01:18:31.880 --> 01:18:34.480
And what is it about us?

01:18:34.480 --> 01:18:36.880
What are like, how are we able to do that?

01:18:36.880 --> 01:18:39.320
And how do we make our sort of machine learning systems

01:18:39.320 --> 01:18:40.600
work better that way?

01:18:40.600 --> 01:18:42.160
How are we sort of able to?

01:18:43.360 --> 01:18:45.520
I mean, yeah, I think it probably has something to do

01:18:45.520 --> 01:18:47.760
is we're able to learn sort of causal structures

01:18:47.760 --> 01:18:49.880
that work well.

01:18:49.880 --> 01:18:52.200
And the distribution can be very different,

01:18:52.200 --> 01:18:55.400
but the sort of causal structures remain.

01:18:55.400 --> 01:18:58.480
And we're able to somehow infer that causal structures

01:18:58.480 --> 01:19:02.280
from data, from just our sense data and our world models.

01:19:03.800 --> 01:19:04.960
And yeah, basically the question is,

01:19:04.960 --> 01:19:07.040
how do we make our machine learning systems

01:19:07.040 --> 01:19:10.080
be able to do similar sorts of things?

01:19:11.600 --> 01:19:14.440
Yeah, this has been absolutely amazing.

01:19:14.440 --> 01:19:16.160
Do you have any final thoughts?

01:19:16.160 --> 01:19:18.000
Where can people find out more information

01:19:18.000 --> 01:19:19.000
about you, Michael?

01:19:20.600 --> 01:19:22.120
So, let's see.

01:19:22.120 --> 01:19:26.160
Well, so I want to point people first to just like Numeri,

01:19:26.160 --> 01:19:29.800
N-U-M-E-R.AI is the website.

01:19:29.800 --> 01:19:33.480
I am fairly active in the forums

01:19:33.480 --> 01:19:35.360
and the rocket chat we have,

01:19:35.360 --> 01:19:40.280
which is sort of just our own personal chat service

01:19:40.280 --> 01:19:43.360
for tournament participants to communicate with each other.

01:19:43.360 --> 01:19:46.440
And I occasionally only post some of the forums there.

01:19:46.440 --> 01:19:48.920
That's probably the best way to like get in contact

01:19:48.920 --> 01:19:51.440
to just message me on rocket chat.

01:19:52.720 --> 01:19:57.640
And yeah, so that's, yeah,

01:19:57.640 --> 01:20:01.080
there's probably that's way to get in contact.

01:20:01.080 --> 01:20:05.600
My also, my email is mdo at Numeri.ai.

01:20:06.880 --> 01:20:09.840
And I would, yeah, I really love if people come,

01:20:09.840 --> 01:20:11.880
check out the tournament, give feedback,

01:20:11.880 --> 01:20:13.880
and start participating.

01:20:13.880 --> 01:20:18.000
I've, yeah, I found that it was a lot of fun as a participant.

01:20:18.840 --> 01:20:21.680
And yeah, I joined the company partly

01:20:21.680 --> 01:20:23.160
so I was starting to make more money

01:20:23.160 --> 01:20:26.760
during the tournament than I was at my job in science.

01:20:26.760 --> 01:20:30.840
And so, yeah, it's a pretty fun hobby and side gig

01:20:30.840 --> 01:20:34.600
and potentially even quite lucrative.

01:20:34.600 --> 01:20:35.440
Amazing.

01:20:35.440 --> 01:20:37.760
Well, Dr. Michael Oliver, it's been an absolute honor.

01:20:37.760 --> 01:20:39.920
Thank you so much for joining us this evening.

01:20:39.920 --> 01:20:41.200
Thanks for so much for having me.

01:20:41.200 --> 01:20:42.520
It's been so much fun.

