WEBVTT

00:00.000 --> 00:03.200
Um, Sarah, it's amazing to have you back on MLST.

00:03.200 --> 00:08.400
It's so lovely to be here. It's been a year and a half or something since our last conversation.

00:08.400 --> 00:13.600
Yes, it has. Yeah, because I think, um, we met at NeurIPS and then, and then I came in

00:13.600 --> 00:17.440
filming to be in the London office, which is really good. Um, but fans of the show, of course,

00:17.440 --> 00:20.720
will know that our first interview was about your hardware lottery paper.

00:20.720 --> 00:21.280
Yeah.

00:21.280 --> 00:23.520
And that was your first grumpy essay.

00:23.520 --> 00:28.240
That was a very grumpy essay. You know, I lead career for AI, so it's a research

00:28.240 --> 00:34.560
lab. We do a lot of fundamental research, uh, and we, a lot of my work is on efficiency, um,

00:34.560 --> 00:37.920
reliability, and building these models that scale the next generation models.

00:37.920 --> 00:41.360
So you can go to Co-Here for AI and take a look at some of our work.

00:42.160 --> 00:48.240
Sarah Hooker is VP of research at Co-Here, and she leads Co-Here for AI, a research lab,

00:48.240 --> 00:53.520
which seeks to solve complex machine learning problems. Co-Here for AI supports fundamental

00:53.520 --> 00:59.520
research, which explores the unknown. She leads a team of researchers and engineers working on

00:59.520 --> 01:05.920
making large language models more efficient, safe, and grounded. In this conversation, Sarah

01:05.920 --> 01:11.200
discusses her recent work on multilingual AI and the challenges of developing language models,

01:11.200 --> 01:16.480
which work across many different languages. She provides insights into the limitations

01:16.480 --> 01:22.720
of current approaches like RLHF, especially for low resource languages. Sarah also talks about

01:22.720 --> 01:28.320
her recent paper critiquing the use of compute thresholds as an AI governance strategy,

01:28.320 --> 01:34.240
explaining why simple measures like flops are inadequate for assessing AI capabilities and

01:34.240 --> 01:40.480
risks. Sarah emphasizes the importance of understanding the relationship between compute,

01:40.480 --> 01:47.520
data, and model architectures. She advocates for a more nuanced approach to AI development

01:47.520 --> 01:53.280
and governance, which considers the complexities of language, culture, and the representational

01:53.280 --> 01:59.120
long tail, where all the low frequency data lives, which is so often neglected in current models.

02:00.000 --> 02:05.600
Sarah's work aims to make AI more globally representative and equitable, as these technologies

02:05.600 --> 02:10.160
become increasingly integrated into society. Enjoy the show.

02:10.160 --> 02:19.040
Your most recent grumpy paper is called On the limitations of compute thresholds as a

02:19.040 --> 02:22.480
governance strategy. Can you give us the elevator pitch?

02:23.760 --> 02:33.760
So this paper is, it has a very boring title. And at face value, it's just about this kind of

02:34.480 --> 02:39.680
odd, known to not many people in the public, compute thresholds that have actually been

02:39.680 --> 02:45.360
widely adopted. They were adopted by the executive order on AI, they were adopted by the EU AI Act.

02:45.920 --> 02:51.040
And what's fascinating is that these are kind of the key policies that have come out on AI.

02:52.560 --> 02:58.320
Why did I write a paper about this very, very deep topic of compute thresholds?

02:59.440 --> 03:05.120
Because it's at the heart of really what our field is asking right now, which is that compute

03:05.120 --> 03:12.560
thresholds are based on an idea that models at a future size, so it doesn't apply to models in the

03:12.560 --> 03:19.520
well now, are going to trigger some difference in risk profile that deserves scrutiny. And this

03:19.520 --> 03:28.000
question of, does scale trigger this moment where models have these properties that are

03:28.000 --> 03:33.840
fundamentally different from models before that? It is actually very much being at the core of our

03:33.840 --> 03:39.200
field for the last two decades. Because in the last two decades, we've had this philosophy of

03:39.200 --> 03:45.360
bigger is better, we scale data and we scale model size. So this essay is really about,

03:45.360 --> 03:51.600
is that true? As we look and stand and look at the last decade, what do we know about the relationship

03:51.600 --> 03:58.000
between compute and risk? And what do we think is the feasibility of these compute thresholds

03:58.000 --> 04:01.920
actually mitigating risk? And that was the starting point.

04:01.920 --> 04:06.800
Yeah, so in the beginning, you were talking about how historically we have tried to estimate and

04:06.800 --> 04:11.920
control and respond to risk. Can you give us a couple of examples?

04:11.920 --> 04:19.280
Mostly as a society, we have tried to grapple with this idea that we want to proactively control

04:19.280 --> 04:24.320
our future for the better. And this is actually recent as well. So it's very typical of modern

04:24.320 --> 04:30.320
society that we have this notion of planning and anticipating risks and being able to mitigate.

04:30.320 --> 04:37.040
There's examples where me and you do this every day, right? We could put on sunscreen if we're

04:37.040 --> 04:43.280
knowing we're going to the sun. We avoid working in dark areas. There's also areas where governments

04:43.280 --> 04:52.000
have done this, you know, even in this modern era of the last 300, 400 years. And it requires two

04:52.000 --> 04:56.480
things to do well. One is that you have to understand where risk comes from. So you have to

04:56.480 --> 05:03.360
understand what is the kind of lever of risk. A good example of where that's failed is something

05:03.360 --> 05:10.080
like the Black Death, where for example, a lot of the protocols around the time didn't realize

05:10.080 --> 05:14.640
that rats were the main vector of the disease. And so because of that, many of the mitigation

05:14.640 --> 05:20.400
techniques were unsuccessful. But the second crucial aspect is that once you've identified the lever

05:20.400 --> 05:26.800
of risk, you have to form a proportionate response. And we also have examples where that's failed

05:26.800 --> 05:32.880
historically. So for example, the London fire is a great example where it was known that this was a

05:32.880 --> 05:38.160
risk, but the fail to curb it early on in the expansion of the fire led to the destruction of

05:38.160 --> 05:44.080
a large part of London. So these are the two challenges that policymakers face. And what

05:44.160 --> 05:50.000
compounds it for something like technology is that typically, the idea of identifying the

05:50.000 --> 05:55.440
lever of risk is very difficult, because most technology breakthroughs, by the nature of

05:55.440 --> 06:01.680
being a breakthrough, you're in a kind of rather than a proactive setting, you're in a retroactive

06:01.680 --> 06:06.400
setting. What do we do now that this is changing the world? And that's a very difficult position

06:06.400 --> 06:12.240
for someone to form a response to. Yes, exactly. I mean, you know, one of the

06:12.240 --> 06:18.080
themes of the paper is we're super bad at predicting the future. And maybe we should just

06:18.080 --> 06:23.600
linger just, you know, just for a second on the the executive order and the EU AI Act. Now,

06:23.600 --> 06:30.160
they used this notion called flops. And please explain what flops are in a second. But the

06:30.720 --> 06:35.440
in America, they set the limit to I think 10 to the 26. Is that right? And then in the EU,

06:35.440 --> 06:40.160
it was they wanted to be a little bit more strict. So they just went down to 25.

06:40.400 --> 06:46.960
Yeah. Tell me about that. So flops, by the way, is this measure by which this compute threshold

06:46.960 --> 06:51.840
is done? I think flops is just a it's a way of counting. So typically, when you train a model,

06:51.840 --> 06:56.240
you're doing many different operations, you're doing additions, subtraction, multiplication,

06:56.240 --> 07:01.360
famously matrix multiplies, dominate our modern networks. And so that can be decomposed into

07:01.360 --> 07:07.120
all these operations. So flops is just a tally, it just counts it up. And these thresholds,

07:08.080 --> 07:14.480
10 to the 26 and 10 to the 25 are this idea that at that moment, that's when you kick in scrutiny.

07:14.480 --> 07:18.800
And it's important to realize that doesn't apply to models in the wild right now.

07:19.440 --> 07:23.360
For the executive order, for the EU AI Act, when it comes into effect next year,

07:23.360 --> 07:29.360
it might hit a handful of models. But this is a for looking policy. It is not based on current

07:29.360 --> 07:34.080
risk in the wild. And so that's interesting to think about, because that creates the question of,

07:34.080 --> 07:38.320
well, are we good at predicting what risks emerge? And is that the right number to do it at? And

07:38.320 --> 07:45.120
that's where it starts to get very interesting. Yeah, yes. So they have a tally, they kind of

07:45.120 --> 07:50.480
estimate how much computation is happening in the models. And then they've set this threshold. So

07:50.480 --> 07:54.960
they don't care about anything below that number. So there's lots of real risk now that they

07:54.960 --> 07:59.360
presumably don't care about. And then they're saying above this number, there's a problem. And I

07:59.360 --> 08:03.760
think did they set the number roughly commensurate to the size of a GPT-4 model?

08:03.760 --> 08:10.240
So it's difficult because they haven't formally justified why they set the number there. But

08:11.120 --> 08:17.440
anecdotally, my understanding is that guided it. So it's interesting. And it's worth thinking about,

08:18.640 --> 08:24.800
well, it's this interesting aspect of, well, firstly, there's a notion of, is this number

08:24.880 --> 08:30.720
valid tally of risk? Like, is training compute the number that you care about if you wanted to

08:30.720 --> 08:37.840
do this tally, if you believed in this future risk? But secondly, if are we good at predicting

08:37.840 --> 08:42.560
like that number? And that's kind of interesting to think about. Yeah, I mean, to me, it was

08:42.560 --> 08:48.800
a bit crazy on its face. And what's going through my mind is, have they got anyone working in the

08:48.800 --> 08:52.960
government that actually know what they're talking about? Because presumably, if they asked you,

08:52.960 --> 08:57.440
you would have thrown this thing out straight away. And you gave some examples, actually. So you

08:57.440 --> 09:01.840
said, there are things that have a normal distribution, like the weight of babies when

09:01.840 --> 09:06.160
they're born or blood pressure or certain things like that. And then there are other things that

09:06.160 --> 09:10.400
are significantly more complex, like if I'm buying a house, what does the estate agent do? Well,

09:10.400 --> 09:14.400
they have a complex model where they look at the neighborhood, they look at various different factors.

09:15.520 --> 09:21.680
Some people have indexes and they have things that can shift over time. So having this one

09:21.680 --> 09:28.480
absolutist number just seems a bit ridiculous. I actually think I feel for policymakers because

09:28.480 --> 09:35.360
I think it will put pressure on them to continually adapt the number. So I will say there were benefits

09:35.360 --> 09:40.800
in the thinking of this number. I think it's unfortunate it got so far without scientific

09:40.800 --> 09:46.480
input. But one reason that people like Flops is, for example, it's hardware agnostic, you can measure

09:46.560 --> 09:52.480
the same way across different types of hardware. And also, it's fairly easy to measure because all

09:52.480 --> 10:00.160
it's doing is a tally of operations. So it also avoids specifying maybe what risk you care about.

10:00.160 --> 10:06.800
So it gives a degree of, I would say, flexibility there for governments to adapt over time. I would

10:06.800 --> 10:14.800
say that is probably one of the larger shortcomings is that by not specifying, you can end up with

10:14.800 --> 10:20.800
something which is evading your Flops threshold but a highly risky model. So I think that's actually

10:20.800 --> 10:26.960
one of the crucial shortcomings. But I do understand that the motivation of a lot of policymakers are

10:26.960 --> 10:32.560
what else? Like what else could I use? I would argue if you're going to stick with this measure

10:32.560 --> 10:38.400
and it has been formalized in several policies, you have to understand that this can be manipulated

10:38.400 --> 10:43.600
as a measure. And there's many ways, and I list some out in the paper, but to your point,

10:43.600 --> 10:51.120
a single number puts a lot of pressure on policymakers to constantly adjust this and have

10:51.120 --> 10:56.640
the technical information to adjust it because this is a rapidly changing distribution. The notion of

10:56.640 --> 11:02.960
compute has been highly unstable. Just looking at the last decade, we know this. And so it will quickly

11:03.760 --> 11:08.480
have an expiration date. And I would argue what you're saying is excellent as an example.

11:08.480 --> 11:14.080
One is that you need a reference class of what are you comparing against? So you mentioned the

11:15.200 --> 11:21.360
kind of real estate agent who compares the pool of houses. Each of these domains, like biology models,

11:21.360 --> 11:27.280
which are very interesting to certain researchers because of bio risk, language models, multimodal

11:27.280 --> 11:31.760
models, they have different distributions of compute requirements. And so it has to be done

11:31.760 --> 11:36.960
relative to your reference class, but also it should be done dynamically. The same way that a

11:36.960 --> 11:43.120
real estate agent does it based on a percentile of surrounding houses, the notion of a single

11:43.120 --> 11:49.520
inflection point for risk is not a viable policy tool because you just are changing things all the

11:49.520 --> 11:54.480
time. Yes. I mean, there are so many things to get into here because you went through a wonderful

11:54.480 --> 11:59.680
list of examples in your paper. But one of the elephants in the room is that it supposes that

11:59.680 --> 12:05.600
there is some kind of linear commensurate relationship between compute and capabilities.

12:05.600 --> 12:10.080
And of course, you're working in multilingual. I mean, it actually penalizes you because

12:10.080 --> 12:16.400
you need to do more compute just to have a model that works at all in many different languages.

12:16.400 --> 12:21.120
And this thing just isn't working for you. Yeah. I mean, what you're pointing out is that once you

12:21.120 --> 12:26.400
do something like multilingual, you're basically trying to learn a new distribution each time

12:26.400 --> 12:32.080
that's as vast as English. And so you typically need a lot. It's called the curse of multilinguality.

12:32.080 --> 12:38.000
And so you need more compute. There's other things there which are very tricky is that

12:38.000 --> 12:42.640
how do you flops and how does training compute account for the vast amount of

12:43.840 --> 12:49.920
change and how we optimize after training? So we talked about RLHF. There's also instruction

12:49.920 --> 12:56.160
fine-tuning. There's also things like synthetic data distillation which shortens training times.

12:56.160 --> 13:02.480
So these are all what we call inference time optimization. So you spend time after training.

13:02.480 --> 13:06.800
You pay for it in compute. Like you can do best event sampling, which is what you refer to with

13:06.800 --> 13:13.600
the Francois Chollet where you sample a lot of completions and you choose the best. That all has

13:13.600 --> 13:19.760
very pronounced benefits for models. So typically your model performance alone, just using a subset

13:19.760 --> 13:25.360
of these techniques is two to six times more powerful. And that's not reflected in flops.

13:25.360 --> 13:30.560
Yes. Because I'm really interested in when you look at the model life cycle or the predictive

13:30.560 --> 13:35.760
life cycle, there are so many places where you can spend computation. So you can do dataset

13:35.760 --> 13:40.000
generation and you can do, obviously there's the training of the model and then you can do

13:40.000 --> 13:43.920
like inference time optimization and active inference and a whole bunch of stuff like that.

13:43.920 --> 13:48.480
And they are only taking into account the model training. But then there's the further issue

13:48.480 --> 13:52.880
of training provenance. So for example, I can download a model from Huggingface and I can fine

13:52.880 --> 13:57.120
tune it and do a bunch of stuff on it. And like, how do you know, right? It's just an inscrutable

13:57.120 --> 14:02.160
bunch of weights. Like you have no idea how much training has gone into it. This is the idea of

14:02.160 --> 14:06.800
tracing flops across the life cycle. I think this is also going to be formidable because

14:06.800 --> 14:12.320
increasingly the most popular models on Huggingface, by the way, are models which haven't been

14:12.320 --> 14:16.560
instruction fine-tuned. They're base models. And why? Because people want to do continued

14:16.560 --> 14:21.040
pre-training. They want to overlay their own optimization techniques, which suggests that

14:21.040 --> 14:25.680
people are using this as one step in their optimization process. It's going to be a formidable

14:25.680 --> 14:31.520
challenge to tally it in a reasonable way, especially when sometimes the way that we

14:32.240 --> 14:36.720
measure these, think about something like mixture of experts or a classic ensemble.

14:37.520 --> 14:42.240
What counts then? Because you may have many different experts in your mixture of experts,

14:42.240 --> 14:48.800
but you're only using two at the end. Classic ensembling is even more nuanced because technically

14:48.800 --> 14:54.240
you didn't even optimize all the models together. You just show up and you ensemble them at the end

14:54.240 --> 14:58.880
and you get one model at the end. So how do you handle that? It's very interesting and it's very

14:58.880 --> 15:04.720
related to this challenge of people are likely taking some level of compute already and they're

15:04.720 --> 15:09.760
doing some changes at the end of training that make it more performant. Yeah. And then there's

15:09.760 --> 15:15.760
this matter of Good Heart's Law, which is that when a target becomes a measure, it ceases to be

15:15.760 --> 15:21.040
a good measure. And there are so many examples of this. For example, the banks have these arbitrary

15:21.040 --> 15:25.840
limits on the amount of money that you can send, which is why it's set at, let's say, $10,000 and

15:25.840 --> 15:30.720
then you see loads and loads of transactions at like $9999 because they know what the limit is.

15:30.720 --> 15:35.040
And it must be the same here, right? It's going to gamify the system. People are going to evade

15:35.040 --> 15:41.680
it in so many ways. I think that the main advice I have about this is that if policy makers have

15:41.680 --> 15:46.320
decided on this and they're going to go for it, they need to complement it with an auxiliary

15:46.320 --> 15:51.040
measure of the actual risk that they care about. It has to be an index because if you just stick

15:51.040 --> 15:57.120
with compute, it is too easy to evade because there's too many different things you can do post

15:57.120 --> 16:03.040
training to gain percentage points. And there's too many ways of essentially shortening your

16:03.040 --> 16:07.440
training time or reducing your flops while still arriving at a highly performant model.

16:08.160 --> 16:13.360
And so that's the other key recommendation I have is that you need something that is

16:13.360 --> 16:19.120
anchored to the downstream risk you actually care about. And compute is not. It just reflects

16:19.120 --> 16:28.080
our belief that more compute is better. And that simply is too simplistic of you to account for

16:28.720 --> 16:33.680
all the ways in which smaller models, if they're very targeted, can be extremely risky.

16:34.400 --> 16:42.720
Yes. So I think maybe we should bring Rich Sutton in. So he wrote this essay called The

16:42.720 --> 16:48.960
Bitter Lesson. And I'll let you bring it in. But he was partially responsible for this idea that

16:48.960 --> 16:54.960
compute is all you need. Yeah, bring that in. Yeah. And by the way, I think that's a fantastic

16:54.960 --> 17:03.280
essay. It really is this idea that history tells us in computer science in particular that all

17:03.360 --> 17:11.840
efforts to codify our expertise, to work on very fancy ways of imparting what we think is the right

17:11.840 --> 17:18.720
way to learn to a model have been particularly futile. Like he's really saying we're not very good

17:18.720 --> 17:23.600
as computer scientists. And the biggest ingredient of success that's driven things is being adding

17:23.600 --> 17:28.400
compute to the mix. And that we can do things that are algorithmic, but it has to play well

17:28.400 --> 17:34.480
with compute. So anyway, he is kind of get this idea of hardware. But it's more general. It's this

17:34.480 --> 17:38.000
idea that it's not specific to a different sort of type of hardware. It's just compute. If you

17:38.000 --> 17:43.360
play well with compute, if it scales well, it's going to be the winning variant. And yeah, go for

17:43.360 --> 17:48.000
it. Well, I mean, I just I have an intuition that he's he's right and wrong at the same time. So

17:48.000 --> 17:52.080
I mean, in terms of system one models that just memorize things better and better, he's kind

17:52.080 --> 17:55.760
of right. Because there is a commensurate relationship, you know, as we memorize more of

17:55.760 --> 18:01.040
the long tail, the models get better and better. But I still think that there might be a fundamental

18:01.040 --> 18:06.240
break between compute and capabilities when and ironically, a regulation like this might

18:06.240 --> 18:10.960
incentivize to find such a break. So you know, we might design system two models that actually do

18:10.960 --> 18:14.640
reasoning and have a, you know, Neurosymbolic architecture or whatever. And now all of a

18:14.640 --> 18:20.960
sudden we've got like really good capabilities with less compute. Yeah, I mean, you're hitting on

18:21.680 --> 18:26.560
the head, I don't disagree with you. I think where I agree with Rich said it is that for

18:26.560 --> 18:33.920
given architecture, say, transformers, you can throw more compute at it up until a certain

18:33.920 --> 18:38.960
point where it's saturated, but you're going to see all the things equal, your data sets equal,

18:38.960 --> 18:43.280
compute is better because these are greedy learners, they, they're, you know, our deep

18:43.280 --> 18:47.280
neural networks are frequency counters, you're going to see gains on the long tail performance

18:47.280 --> 18:54.880
and overall gains. Where it misses the point is that really there's a few things going on.

18:54.880 --> 19:00.080
One is that because our current representations are so inefficient, there's ways to

19:02.160 --> 19:09.840
really change the algorithm itself and bend the, the rule of compute. So, and the rate at which

19:09.840 --> 19:15.760
compute is needed to unlock gains. And deep neural networks in particular are a great example of

19:15.760 --> 19:19.440
this because they're so painfully inefficient because we have to show all the data the same

19:19.440 --> 19:23.600
amount of times because we have to do these global updates. And so we're seeing all these tricks.

19:24.240 --> 19:28.480
For example, now we care about data again, and we care about data quality. So we condition that

19:28.480 --> 19:33.840
space better to represent what we want to model downstream. That means we have to train far less

19:33.840 --> 19:38.080
because all the features in the data set, the ones we want to learn was you just train on the

19:38.080 --> 19:41.840
internet. There's a lot you don't want to learn. So you have to kind of unlearn it afterwards and

19:41.840 --> 19:47.440
spend a lot of compute just trying to find what you want within that. So that's where you bend

19:47.440 --> 19:52.240
the rule and it becomes more nuanced, which is that the other thing that that misses is that,

19:52.880 --> 19:56.480
or at least that wasn't a core part of this essay, I actually think Rich might agree with

19:56.480 --> 20:03.760
me on this, is that your rate of compute is really determined more than anything by the

20:03.760 --> 20:08.960
prior of your algorithm. So yes, if your algorithm plays well with compute, if it's scalable,

20:09.040 --> 20:14.400
compute unlocks a lot. But the rate and the saturation point is determined by the algorithm.

20:14.400 --> 20:19.600
And what do we mean by this? Convolutional neural networks are a great example. Introduced in 2012,

20:20.560 --> 20:25.280
really unlocked scalability. Why? Because convolutional filters and patches made it

20:25.280 --> 20:30.400
possible to model high dimensional images at the time. Why? Because you move your patch over your

20:30.400 --> 20:36.960
image. This takes advantage of local relationships. You can really reduce your dimensionality. Max

20:37.840 --> 20:43.360
max pooling layers, which Jeffrey Hinton is famously grumpy about, and rightly so,

20:43.360 --> 20:47.200
just throws away everything except for the max. You reduce the amount of features,

20:47.200 --> 20:52.960
you unlock the ability to model images, you have scalability up to your point. This is what we

20:52.960 --> 20:57.680
famously know about image models. Now there's been a saturation point. Everyone who switched

20:57.680 --> 21:02.880
to transformers because there's a new arc. So what I mean by this is your algorithm is kind of your

21:02.960 --> 21:12.080
most heavy prior on your search space. And it's Richard's right that what plays well with compute

21:12.080 --> 21:18.240
is the one we're going to default to. But the question becomes your scaling laws and your ability

21:18.240 --> 21:24.160
to predict the future are essentially limited to algorithm and compute. And that's what's

21:24.160 --> 21:28.000
interesting is that it means we're not very good at predicting the future because it means we're

21:28.000 --> 21:33.520
too locked in to this narrow arc of this architecture we use combined with compute.

21:33.520 --> 21:38.000
Yeah. I mean, even the CNN example is, I think an example that proves Richard wrong,

21:38.000 --> 21:43.680
because he said in the bitter lesson that any attempt to impute hand-designed priors like

21:43.680 --> 21:50.480
symmetry or CNN is a symmetry. So in the CNN, it encodes a symmetry and scale invariance.

21:50.480 --> 21:55.200
And essentially, all it's doing is a shortcut because it's still in MLP at the end. So what

21:55.200 --> 22:01.520
it's doing is it's basically it's building an MLP as if you didn't have the scale and symmetry in

22:01.520 --> 22:05.520
there. But it's just kind of like doing this thing and it's like, you know, it's embedding it

22:05.520 --> 22:09.280
all into the MLP. But it's basically still an MLP with a symmetry shortcut, which was hand-designed.

22:10.880 --> 22:17.680
So yeah, yeah, but there's still this notion though that there's, you know, connectionists think

22:17.680 --> 22:25.040
that, I mean, like Neil Nanda said to me, he's like a rationalist guide from DeepMind and now,

22:25.040 --> 22:27.920
yeah, DeepMind. And he said, these things are just smarter than you, man.

22:30.080 --> 22:34.720
How did that make you feel? Well, I mean, not great, not great, but you know, there's this whole

22:34.720 --> 22:42.480
like mech-interp thing. And I think that they genuinely believe that there's just some deep

22:42.480 --> 22:47.040
form of inscrutable intelligence going on. There was that like monosomanticity paper from

22:47.120 --> 22:50.880
Anthropic recently. And you know, there's this deep belief that there's something

22:50.880 --> 22:56.640
really interesting going on. What do you think? I think that it is, there are persuasive,

22:58.560 --> 23:05.040
here's what I will say. Language is very powerful, which is why we connect and with this technology

23:05.040 --> 23:10.240
so much, because language is how we connect with each other emotionally. It's very tied to how we,

23:10.240 --> 23:15.520
as humans, are quite tribal. And so whenever a model learns a distribution that is indistinguishable

23:15.520 --> 23:22.000
from humans, I think that it gives pause. And it is, I do think that these conversations are

23:22.000 --> 23:28.400
useful because it gives worthy pause to how this technology is used. And for example, I'm very

23:28.400 --> 23:37.200
against many of the efforts to sometimes deploy these algorithms without notifying humans that

23:37.200 --> 23:41.600
it's an algorithm. I think that you should always be aware when you're talking to an algorithm.

23:41.600 --> 23:46.800
And that's because these are quite convincing sometimes. And so it's very important that we

23:46.800 --> 23:51.920
always communicate what is the role of the model and the human. Do I think that there's a higher

23:51.920 --> 24:00.560
reasoning here? I don't. I think that in many ways, whenever you have persuasive interpolation of a

24:00.560 --> 24:06.560
space between ideas, it's going to be surprising to us. I think what delights us is the creativity

24:06.560 --> 24:13.520
and the surprise and element. But is this ability to reason? I don't think so. I think that there

24:15.280 --> 24:20.640
is a clear relationship with the type of architecture we have of a memorization

24:20.640 --> 24:26.640
relationship. And we know this. We know that when we increase scale, we learn a given architecture.

24:26.640 --> 24:31.120
Or when we do different tricks to compensate for scale, so we can go smaller and still learn

24:31.120 --> 24:34.720
things, what we're really doing is we're just trying to induce good memorization

24:35.520 --> 24:39.600
and good steering towards part of the distribution we care about. Frankly, while

24:39.600 --> 24:46.080
why all these optimization tricks have worked beyond compute to reduce compute has been

24:46.080 --> 24:49.760
we're largely training on a distribution we don't want at the end of the day. So we start by

24:49.760 --> 24:56.000
training on the internet. And we actually don't want the internet when we engage with these models.

24:56.000 --> 25:00.160
We want something that's very chatty and philosophical and wise. And so a lot of what

25:00.160 --> 25:04.320
we're doing is we're trying to steer things towards the part the tiny sliver of the distribution

25:04.320 --> 25:09.120
that training data that we care about. And that's why we have so many optimization tricks

25:09.760 --> 25:14.160
before we get to the end. But that's fascinating because what's that that's really telling you is

25:14.160 --> 25:18.160
that unlike a traditional machine learning problem where you're training to the data set is the

25:18.160 --> 25:23.440
distribution you want to learn, a lot of what we're doing with language is we're unlearning.

25:23.440 --> 25:27.600
We're just trying to steer and unlearning nor and then focus on what we want. So it's really

25:27.600 --> 25:32.400
interesting. Yeah, machine unlearning. That's fascinating. I'm also a big fan of the

25:32.400 --> 25:36.480
externalist tradition in cognitive science, you know, like for recognition. And in that sense,

25:36.480 --> 25:39.920
I think it doesn't make sense to draw a boundary around the model, because I think, you know,

25:39.920 --> 25:44.320
our sense making semantics, situated knowledge and so on, it's kind of observe a relative anyway,

25:44.320 --> 25:48.960
you know, these things are embedded in our culture. And sense making humans put prompts in there and

25:48.960 --> 25:52.720
they interpret the outputs. And actually, even the data generating process that went into building

25:52.720 --> 25:56.400
these things was, you know, originated from the universe, we're all agents and we're all in the

25:56.400 --> 26:00.400
physical world and the social world. And we, you know, we're doing the effect of computation,

26:00.400 --> 26:04.400
both in how the models are built and how they are used and interpreted and evaluated and so on.

26:04.400 --> 26:08.320
So what are you going to do? Like draw a big, should you include the flops of the universe as

26:08.320 --> 26:14.000
well? Oh, that's fascinating. Yeah, there is this interesting, yeah, it does spark something else with

26:14.640 --> 26:18.960
flops, which is that, so typically the final model that you deliver is only one of the possible

26:18.960 --> 26:25.040
models, right? So in fact, typically, even at massive scale, you train many candidate models,

26:25.040 --> 26:28.560
and then you choose the best one. And so it's interesting because these are not optimized

26:28.560 --> 26:32.960
together, but they implicitly optimized through the selection process. And it is really interesting

26:32.960 --> 26:37.280
because we kind of steer towards what we want. So yeah, it's a fascinating dynamic. I didn't think

26:37.280 --> 26:42.480
of it like that, though, that's an even bigger meta approach for thinking about this. Hardware

26:42.480 --> 26:46.640
Lottery Paper, which we talked about, and that was a really fun conversation because I remember it

26:46.640 --> 26:54.560
was when you were doing the trio kind of the group of ML Street talk, the earlier version. And

26:55.360 --> 27:00.960
I think you originally invited me onto the show because there was this idea that I wrote about,

27:00.960 --> 27:08.000
which is that most of computer science history has been driven by whether your idea works with

27:08.000 --> 27:13.840
available hardware or not. And I think that resonated with a lot of people at the time,

27:13.840 --> 27:18.800
because what it's really saying is that we may be in another hardware lottery right now,

27:18.800 --> 27:25.120
that something like Transformers, which we all use, has become increasingly locked in to GPUs

27:25.120 --> 27:29.920
and to TPUs, which have all been built to accelerate this one hardware. So it raises the

27:29.920 --> 27:35.840
question of what next and how do we make sure that the next brilliant idea isn't stuck in

27:35.840 --> 27:40.160
purgatory for decades, because that's what happened to deep neural networks. It simply

27:40.160 --> 27:49.200
didn't work until TPUs were converted from video game use, which was really not the intended

27:49.200 --> 27:53.360
purpose of how they were converted to work for machine learning workloads. And that happened

27:53.360 --> 27:57.680
over the course of a decade. It was a very slow conversion process, but that turned out to be

27:57.680 --> 28:03.280
the key for deep neural networks. What we now identify as 2012, the moment that this explosion

28:03.280 --> 28:09.360
of interest and funding and acceleration happened. People identify that with convolutional neural

28:09.360 --> 28:14.800
networks or the algorithm, but really it was both. It was the hardware making the algorithm

28:14.800 --> 28:19.840
feasible. And that's when you first had the empirical proof that deep neural networks were viable.

28:20.640 --> 28:23.360
To what extent do you think there is an algorithm lottery as well?

28:24.480 --> 28:30.080
Oh, what do you mean by that? Well, as in now, your paper was about the basin of attraction of

28:30.080 --> 28:35.840
hardware. But is there a basin of attraction of algorithms as well? Absolutely. I mean, you just

28:35.840 --> 28:43.920
have to look at optimizers to see that. So what I mean by that is an algorithm is really how you

28:43.920 --> 28:50.800
learn from data. This is the essence of an algorithm. And what we've been locked into is this idea that

28:50.800 --> 28:56.640
it has to be gradient based optimization. It's really hard to do something that's a non-differentiable

28:56.640 --> 29:03.520
objective. And what that means kind of in accessible terms is that we're stuck doing these

29:03.600 --> 29:09.200
global updates. So the way our models train is that we kind of send through shovel through data,

29:09.200 --> 29:13.920
and then the update to the weights is based on an average of all the data that's seen.

29:14.480 --> 29:19.440
Why is that tricky for a few reasons? Because why does it mean that we overfit to the average?

29:19.440 --> 29:23.760
And that's why we need so much training data. Because essentially, if you're just overfitting

29:23.760 --> 29:28.960
to the average, it takes ages to learn the rare patterns. So you train for longer, you need more

29:28.960 --> 29:35.520
data. But the other thing that's very tricky is that it means that models forget. So every time

29:35.520 --> 29:40.000
you shovel in new data, the model forgets the old data because you're updating everything at once.

29:40.000 --> 29:45.360
A nice point of contrast is that as humans, we typically have long-term memory and short-term

29:45.360 --> 29:49.120
memory. These are different ways of learning, and the rate of learning is different. And so when

29:49.120 --> 29:54.240
you process information, some are stored in your long-term memory. You may have a distinctive memory

29:54.320 --> 30:00.080
from a child that you think is like your first memory from a child. And it may be mutated over

30:00.080 --> 30:04.800
time. That's the nature of memory. But this ability to preserve two states of what you did today,

30:04.800 --> 30:10.720
what you did years ago, that's very different from gradient updates. And somehow, because we

30:10.720 --> 30:14.880
haven't found an alternative way, even though a lot of people have worked on it, we are in this

30:14.880 --> 30:20.400
algorithm based on where it's very tricky to propose an algorithm that doesn't rely on a

30:20.400 --> 30:26.560
differentiable objective. Yes. And I think we'll talk about this later. But part of the problem

30:26.560 --> 30:31.440
is people think of this paradigm as a form of general abstract pure intelligence. And the

30:31.440 --> 30:37.360
reality is that certainly with your multilingual work, that we're dealing with just this long tail

30:37.360 --> 30:43.360
of complexity, heterogeneous data sets. But maybe that's a good segue because you just released

30:44.720 --> 30:48.800
this primer paper called the AI language gap. Can you tell us about that?

30:49.680 --> 30:56.320
So it's quite fun because in some ways, what you're talking about these themes leads so nice into

30:56.320 --> 31:04.480
the AI language gap. Really, when we have built these models, we've overfitted to what is weighted

31:04.480 --> 31:08.640
most importantly to those who built it. And these models have been built in a few places.

31:09.360 --> 31:15.600
We're in London. London is a very big hub of where researchers have been. So is the US and

31:15.680 --> 31:22.000
Europe and China. And because some of the first impressive large language models were built in

31:22.000 --> 31:29.840
the US and the UK with DeepMind, and in the US with places like Coher and places like OpenAI,

31:29.840 --> 31:33.840
I think that that has necessarily reflected the nature of the researchers who built them.

31:33.840 --> 31:40.880
They wanted to work in English. The tricky thing is that when you try and make AI actually work

31:40.960 --> 31:46.400
for the world, you're talking about this vast array of different languages. So there's 7,000

31:46.400 --> 31:53.040
languages in the world. 80% of those have no text data. So it's truly not even a language problem.

31:53.040 --> 32:00.400
It's also a multimodal problem. The second part is that even with the top 101 languages, no models

32:00.400 --> 32:06.480
except for Io101 currently cover it. So there's this vast amount of the world that simply isn't

32:06.480 --> 32:13.680
reflected in the way that AI works and who AI serves. The primer about the language gap is

32:13.680 --> 32:17.920
really calling attention to this. But at the root of this problem and what you're getting at with

32:17.920 --> 32:23.360
this theme of how does models work with the long tail is that the fundamental issue is

32:24.000 --> 32:30.880
our models really overfit to high frequency patterns. And so the key difficulty with the

32:30.880 --> 32:37.440
language gap is that, one, these languages typically are underserved by available data on

32:37.440 --> 32:42.400
the internet. The internet kind of reflects early patterns of adoption, not necessarily humanity

32:42.400 --> 32:47.280
as it is. So that means that there's way more English on the internet than there is people who

32:47.280 --> 32:53.840
speak English. So 5% of homes speak English, but 50% of the internet is in English. In contrast,

32:53.840 --> 32:59.360
something like Yoruba, spoken by 40 million people, is really underserved. And so it's a long tail

32:59.360 --> 33:04.320
problem. But here's the other thing, it's a pattern where the rich get richer and the poor get poorer

33:04.320 --> 33:10.320
because we're now in a synthetic data era. So as models get much better at generating English and

33:10.320 --> 33:14.880
Chinese in particular, these are the two high resource languages that are well served, you're

33:14.880 --> 33:20.720
going to see more content in those two languages. And that makes it even harder if you're relying on

33:20.720 --> 33:26.480
large data to properly represent the languages that are currently underserved. Yeah, so interesting

33:26.480 --> 33:32.240
because we're moving away from the material world into the information world. And right now in the

33:32.240 --> 33:36.160
material world, there is a kind of a commensurate relationship between the number of people who

33:36.160 --> 33:40.000
speak English and the amount of data on the internet. And as you say, we're now moving to this place

33:40.000 --> 33:45.920
where we are generating data of language and the polarization is going to increase. So you're talking

33:45.920 --> 33:52.240
about there's this kind of North American tech based inequality, which is getting worse. And you

33:52.240 --> 33:56.560
said that there are safety implications for this. I was interested in this word safety, we spoke

33:56.560 --> 34:03.600
about this last night. Because when I think of AI safety, I think of X risk in Silicon Valley and

34:03.600 --> 34:10.160
stuff like that. And I've noticed over the years, the conflation of the two communities in terms of

34:10.160 --> 34:18.480
ethics and existential risk. And how do you feel about that? I think it's, I mean, I feel grumpy

34:18.560 --> 34:23.040
about that. But here's the thing. So, you know, subfields are always like this. I think there's

34:23.040 --> 34:29.360
always this notion of subfields, which are extremely, you know, actually people caring about the same

34:29.360 --> 34:35.920
objectives, trying to distinguish themselves over time. AI safety encompasses a large array of

34:35.920 --> 34:43.600
perspectives and expertise and people who care about different things. I think that this shift

34:43.680 --> 34:48.800
towards talking about from response for AI to AI safety is a fascinating one,

34:48.800 --> 34:54.640
because it's been a bit intentional from communities who want to maybe suggest that

34:55.520 --> 35:01.680
response for AI is distinct from what they're doing. And instead saying AI safety is about

35:01.680 --> 35:07.360
these profound risks, these like fundamental issues of our time. And response for AI is,

35:08.080 --> 35:12.400
okay, great, you're doing that, but keep going. And so I do think there's a very interesting

35:12.400 --> 35:17.760
thing with how we name things and how we really have precision in our conversations.

35:19.120 --> 35:23.760
Increasingly, I think AI safety encompasses both of these, and you need more precise

35:23.760 --> 35:30.160
language with both. And I actually think my main ask is, we need to be precise about what our

35:30.160 --> 35:38.400
objective is with AI safety. Because it can be, it is in many ways the same goals as response for AI.

35:39.280 --> 35:44.320
But the degree of precision when this is articulated is a sign of accountability for

35:44.320 --> 35:47.600
the objective. And I think sometimes the use of that word lacks accountability.

35:48.320 --> 35:55.120
Yes, exactly. And when I hear some ex-risk folks talk about AI, it feels to be in the abstract.

35:55.120 --> 36:01.840
And what I mean by that is they are just thinking about, if we scale this technology up,

36:01.840 --> 36:07.360
it learns these abstract representations, which work in any situation, and it's just a matter of

36:07.360 --> 36:12.720
scale. And it feels unmoored from the research, because when I read your work about multilingual

36:12.720 --> 36:17.920
models, you're clearly pointing out that when we have what they call low resource languages,

36:17.920 --> 36:22.000
the models don't work very well. They're not learning these abstractions that just

36:22.000 --> 36:27.600
automatically work in other languages. There's a specificity to it. That seems to be the difference

36:27.600 --> 36:33.040
to me. Yeah, there's this big question right now, what you're getting at is there's this idea of,

36:33.920 --> 36:38.240
there's a mystique that some people are attributing to scale. It's been called different

36:38.240 --> 36:42.640
things. It's this question of, are there emergent properties? Are there properties that appear from

36:42.640 --> 36:47.360
nowhere that we unlock with scale? By the way, multilingual is originally proposed as one of

36:47.360 --> 36:51.360
them. Like in the first paper about emergent properties, multilingual was there. It's like,

36:51.360 --> 36:56.800
wow, how did this appear? We didn't even have this in our training data. But it's very interesting.

36:56.800 --> 37:01.440
Now there's been subsequent work which is shown. It was there all along. It just wasn't documented

37:01.440 --> 37:06.080
in the training data. So scale is just really learning your long tail. It's learning the low

37:06.080 --> 37:11.520
frequency. We just get surprised because I think there's a big disconnect between what we think

37:11.520 --> 37:16.320
we know about the vast amounts of data that we train on and what's actually in that mix.

37:16.320 --> 37:21.440
And so there is often certain properties where it takes scale to unlock because it's very

37:21.440 --> 37:26.480
relate to this question of memorization. I think how this conversation has become a bigger theme

37:26.560 --> 37:32.320
beyond this scientific question of when do properties emerge and what to scale and lock,

37:32.320 --> 37:38.640
it's become this thing of kind of creating a myth around these models. That there's a lack of

37:39.920 --> 37:45.920
ability to understand what scale gives. And then that is used to kind of impart a degree of anxiety

37:45.920 --> 37:50.640
that because we don't know precisely when this property will emerge, there should be anxiety

37:50.640 --> 37:56.000
about this. And there should be a sense of real danger about the use of these models. And I would

37:56.000 --> 38:04.480
say that that is actually the wrong framing for this. The right framing is that one is the notion

38:04.480 --> 38:10.480
that we're just going to keep on scaling I think is flawed. I think there's very clear evidence that

38:10.480 --> 38:15.440
you know bigger is not always better that we're kind of reaching the limits of how we scale with

38:15.440 --> 38:19.280
something like transformers and it's very architecture bound. But the second thing that I

38:19.280 --> 38:28.160
would say is it really kind of ignores the mounting evidence that these kind of properties

38:28.160 --> 38:31.360
are surprising only because we're not going to predict in what emerges at scale.

38:32.160 --> 38:38.480
Yes, yes. I spoke with David Chalmers recently and he bemoans the fact that whenever we have

38:38.480 --> 38:45.600
a complex system, we say, oh, it's emergent. And there is something interesting going on

38:45.600 --> 38:50.480
as you say that when you memorize more and more of the long tail, you do see this qualitative

38:50.480 --> 38:55.360
increase in capabilities. And it's quite easy as an observer just to say, oh, you know, it's an

38:55.360 --> 39:02.160
emergent property. And people ascribe things like you know, divergent intentionality and reasoning

39:02.160 --> 39:06.320
and all of these kind of anthropomorphic qualities to the models even though they probably don't

39:06.320 --> 39:11.760
really exist. But one interesting thing though is that, you know, when you memorize all of these

39:11.760 --> 39:17.120
surface statistics at scale, you can use the language model as an idea generator. And like on

39:17.120 --> 39:22.160
Francois Chalet's arc challenge, you know, Ryan Greenblatt generated about 30,000 completions for

39:22.160 --> 39:27.280
all of the tasks. And the remarkable thing is in terms of sensitivity, the correct answer is in

39:27.280 --> 39:32.560
those completions. And then you can do some neuro symbolic evaluation and selection and you can

39:32.560 --> 39:36.160
pull the thing out, you know, so you can build an architecture that does really well. But I think

39:36.160 --> 39:41.360
people underestimate the amount of human selection kind of like smoothing out the brittleness.

39:42.080 --> 39:47.040
Yeah, well, right now, I agree, there's a huge amount of creativity that's unlocked for these

39:47.040 --> 39:52.000
models. So this iteration, and actually, by the way, this idea of like, you can create a lot of

39:52.000 --> 39:56.400
different options, and then you can verify which are correct, you see this in a lot of different

39:57.760 --> 40:01.840
kind of states of progress right now, that's how code is currently done, like you can

40:01.840 --> 40:07.840
create a really nice code data set by running code and seeing which one passed the test and kind

40:07.920 --> 40:13.600
of do formal verification of which ones passed. So it's not that these models are not capable of

40:13.600 --> 40:18.560
generating insensible answers is just that the probability on every single turn consistency

40:18.560 --> 40:23.760
is what you're putting out consistency is sometimes not there. And I also think part of what is

40:23.760 --> 40:28.320
beautiful from the creativity perspective, iteration of ideas is that sometimes you actually

40:28.320 --> 40:32.560
don't want consistency. So the objective may be different in different settings. So for example,

40:32.560 --> 40:37.520
for code, we always want code that passes. So that's a good example where we sample a lot just

40:37.520 --> 40:43.280
to get the subset. But sometimes I've talked to people who use it as a way to seed ideas or

40:43.280 --> 40:47.440
things like that. And actually there the diversity is the important part and gain very different

40:47.440 --> 40:52.640
responses each time. And so I think over time, we'll actually have different models for different

40:52.640 --> 40:57.840
things and be able to this is the core of the challenge of steerability of control, which right

40:57.840 --> 41:02.880
now is not good, frankly, like why do we have prompt engineering and why does everyone love it?

41:02.880 --> 41:07.840
Like this is a this is a symptom of a problem, not a symptom of a solution. The fact that we

41:07.840 --> 41:12.320
spend so much time prompt engineering the perfect thing to steer. So hopefully we have better tools

41:12.320 --> 41:16.560
in the future. But I see that as one key thing that will change is that we'll be able to steer

41:16.560 --> 41:21.360
towards the mode we want to use. Do we want consistency? Do we want exploration? And how

41:21.360 --> 41:25.120
does it fit into our iteration pattern? We won't spend too long on this because I asked everyone

41:25.120 --> 41:29.120
about this. But you know, where are the sources of creativity? So as we memorize more of the

41:29.120 --> 41:33.520
long tail, and the models can extrapolate, and the human prompters can, you know, mix novel

41:33.520 --> 41:38.240
combinations of things together. So there's this potential extrapolative space and whatnot that's

41:39.440 --> 41:45.120
how creative can they be? Yeah, I've been so one of the recent papers that we released was a paper

41:45.120 --> 41:53.120
about what we call active inheritance. It's this idea that we can start to steer how we sample data

41:53.120 --> 41:58.240
to sampling different parts of the distributions from different models. So so far the paradigm

41:58.240 --> 42:03.440
of like sampling data, either for human or for another model, has been very much like there's

42:03.440 --> 42:07.920
a single teacher, you're the student, or there's another student or your co creators with a single

42:07.920 --> 42:13.680
model. But if you think about it, one, that's a kind of passive inheritance, you're just trying a

42:13.680 --> 42:19.920
single prompt, you're not really kind of enforcing any criteria. Active inheritance is where you

42:19.920 --> 42:24.560
sample different parts of the problem you want to solve from a variety of different models.

42:24.560 --> 42:29.840
And that diversity actually spurs really interesting patterns where you increase

42:29.840 --> 42:35.760
the realm of what's possible and kind of spur higher quality that transcends the quality of

42:35.760 --> 42:41.280
any one model. And I see that as a very important step that we're building a lot of work on,

42:41.280 --> 42:47.200
including a multilingual, but also in this fundamental area of we actually used it to

42:47.200 --> 42:51.440
in the paper that we just released, we used it to steer towards non-differentiable objectives.

42:51.440 --> 42:54.800
So you know, going back to what you were talking about the algorithm basin,

42:54.800 --> 42:58.640
this idea, and I was saying everything is dependent on gradient descent, it's very hard

42:58.640 --> 43:03.680
to steer towards non-differentiable objectives. Before deep neural networks, there was decades

43:03.680 --> 43:09.360
of research on just these non-differentiable objectives. There are things like, how do you

43:09.360 --> 43:16.080
compute the perplexity of like a given, like, what is the reading grade level of a given sentence?

43:16.080 --> 43:20.560
So there's these scores that are kind of codified, but you can't really use it because they're not

43:20.560 --> 43:24.800
differentiable. And we actually show that you can use that as part of active inheritance where

43:24.800 --> 43:29.840
you steer towards models that are better at a reading grade level. And then you use that to

43:29.840 --> 43:35.040
kind of form your basis of your data set. So I think that's fascinating. And I think that's

43:35.040 --> 43:39.840
really going to spur creativity beyond just this more static notion of you just sample from a single

43:39.840 --> 43:44.000
teacher. Yeah, that's fascinating because there's so much of your research has been on the tyranny

43:44.000 --> 43:47.600
of forgetting the long tail or not paying attention to it. And of course, you can solve that with

43:47.600 --> 43:52.560
better optimization and, you know, federated learning and a gentile, you know, kind of

43:52.560 --> 43:57.600
multimodal systems that share information and query and almost like an adversary or setup.

43:57.600 --> 44:01.680
Yeah, it's a more dynamic pool. And so it's this idea that you can actually, and actually

44:02.320 --> 44:07.520
the long tail is a perfect example of where I find active inheritance most promising is that

44:07.520 --> 44:12.080
because the long pool, the long tail, you typically have many weak teachers. No one's very good at

44:12.080 --> 44:17.600
the long tail. But sampling effectively and doing this active inheritance rather than passive of

44:17.600 --> 44:22.640
just choosing a single teacher, but choosing a variety of teachers and then comparing and optimizing,

44:22.640 --> 44:26.320
this is fascinating. And I suspect it will benefit most the long tail.

44:27.040 --> 44:31.520
So you said in your language gap paper that language models are going to become integral to

44:31.520 --> 44:38.640
modern societies. How do you see that panning out? It's already happening in different ways.

44:38.720 --> 44:44.960
Like I would call it the high low way. So we can talk about high level themes, which is there'll be

44:44.960 --> 44:52.160
a ability to communicate much more easily. And so you'll just see much more proliferation of

44:52.160 --> 44:58.960
things like art or people writing or kind of taking away some of the difficult parts of

44:58.960 --> 45:04.000
how we communicate. I think the low way is just the more granular ways that you're using it right

45:04.000 --> 45:11.440
now, which is I use it typically for very basic things throughout my day. We write a lot of papers,

45:11.440 --> 45:16.800
so I'll do my citation reformatting using a language model. So there's both the mundane,

45:16.800 --> 45:22.400
but there's also the profound. I think the profound is that it changes the ease of communication.

45:22.400 --> 45:27.760
And so it changes the rate of inflammation flow. And this can be really powerful. It can mean that

45:27.760 --> 45:34.560
we can be more creative and experiment more the space. It can also bring new risks. And so

45:34.560 --> 45:40.720
I think this is also important to think about. Interesting. And you said that this North American

45:40.720 --> 45:47.280
bias in language model training, you said that it affects the design, the outputs and the behavior

45:47.280 --> 45:55.120
of the models. What did you mean by that? Well, there's two things. I mean, when I say design

45:55.200 --> 46:02.000
outputs and the behavior of the models, I think that there's optimization bias in the models

46:02.000 --> 46:07.120
itself against different languages. So tokenizes is a great example. So Roman scripts are things like

46:08.080 --> 46:15.600
French, Italian. We also have a Latin based scripts. This is also English. Whenever you deviate

46:15.600 --> 46:22.960
from Latin based scripts, you have something like Hindi, Korean, and these do not play well

46:22.960 --> 46:28.480
with tokenizers. So there's a lot of work which shows not only do tokenizers not work very well

46:28.480 --> 46:34.320
for these languages, but also it ends up being at this double tax because not only does the models

46:34.320 --> 46:38.800
perform worse, it also takes more tokens to represent these languages. So it's higher latency,

46:38.800 --> 46:44.400
higher cost for users outside of English to use APIs right now. So that's an example of like an

46:44.400 --> 46:52.160
optimization bias. The other, frankly, the issue is that whenever you're trying to have a model

46:52.160 --> 46:58.400
that represents many different parts of a distribution, typically our solution right now is

46:58.400 --> 47:03.120
we've got to give it more capacity. So I1 and 1 was an interesting example of this. We released

47:03.120 --> 47:08.000
I1 and 1. It represented 101 languages, and you can start to think about how many that is when you

47:08.000 --> 47:13.600
try and list more than 20. So you'll probably get to 10, and then you'll start struggling. And 101

47:13.600 --> 47:21.200
is nuts. It includes things like we had Welsh, we had Irish, but we also had Telegu, we had many

47:21.200 --> 47:29.840
African languages, and we had very much these underrepresented like Haitian things, the variety

47:29.840 --> 47:36.000
and the complexity as well as dialect. So 101 is probably like preparing for the space race. It's

47:36.000 --> 47:41.440
like at the most extreme of the problem. And what's interesting is everything you learn there

47:42.160 --> 47:47.920
trickles down to less severe settings. But one of the things that we learned there is that you

47:48.000 --> 47:52.640
have to be very careful about how you use capacity because we had this 13 billion parameter model,

47:53.280 --> 47:58.560
and we were stuck with it because there was no pre-training data that covered 101. So this model

47:58.560 --> 48:04.720
was actually from 2019, which is crazy given how much has happened since then. But because of that,

48:04.720 --> 48:10.080
we were stuck with this model, and it meant that everything we had to do was try and make the best

48:10.080 --> 48:15.680
user capacity. We had to wait properly. We had to do data processing, data cleaning, but also we

48:15.680 --> 48:20.720
had to do a lot of work with synthetic data and the manipulation of how we did the optimization time.

48:21.360 --> 48:27.840
So you can do this two ways. We could have even increased it to 103 billion parameter model,

48:27.840 --> 48:31.440
and then we would have to retrain because right now models, unless they're trained with the day

48:31.440 --> 48:37.520
from the beginning, you can't just add it at the end. But also there's a secondary way, which is we

48:37.520 --> 48:42.640
get much more clever about the optimization and the data creation. And so this is really the issue

48:42.720 --> 48:46.400
is that when you go multilingual, all your problems in a given language

48:47.520 --> 48:50.960
are kind of multiplied out. And so you have to be very careful about all the details.

48:52.160 --> 48:56.480
I wonder what's the relationship between language and capabilities? And the reason I asked this is

48:56.480 --> 49:00.960
there was a great book I read called The Language Game by Morton Christensen and Nick Chater.

49:00.960 --> 49:06.400
And that very much led me to this idea of situated knowledge, I guess. So actually a lot of our

49:06.400 --> 49:13.280
cognition and thinking is quite specific to the culture and the language that we are in.

49:13.280 --> 49:17.520
And that seems to go against the grain of the idea that these things are learning

49:17.520 --> 49:22.880
general patterns of reasoning across languages. So then it rather kind of leads you to this

49:22.880 --> 49:29.040
conclusion that you actually need to be within the language and the culture in order to do the

49:29.040 --> 49:33.920
kind of thinking that they do inside that culture. So how does that work then when you're mixing all

49:33.920 --> 49:40.080
of these together into one language model? I think it doesn't work that well right now. So I would

49:40.080 --> 49:43.920
say this is like one of the core problems because you're precisely right. So we actually, so there's

49:43.920 --> 49:48.720
a few things I would say here. One is that we already see this with things like dialect. So the

49:48.720 --> 49:54.080
notion of dialect, which isn't really a counter for any models, including Aya, I think that we all go

49:54.080 --> 49:59.760
as mainly just to be the first next step in state of art. But even ours doesn't do this nuance of

49:59.760 --> 50:04.400
dialect. We do have various dialects of Arabic and some other dialects, but take something like

50:04.400 --> 50:10.480
Portuguese for example. Portuguese is spoken in many different places of the world. I spent part

50:10.480 --> 50:15.280
of my childhood in Mozambique. The Mozambique Portuguese is very different from, you know,

50:15.280 --> 50:20.240
I guess the most extreme would be Brazilian Portuguese. But also Portuguese in Portugal

50:20.240 --> 50:25.440
has its own nuances. And actually when we did Aya, we had researchers all over the world who were

50:25.440 --> 50:31.040
part of this project. And we would frequently have these little riffs between the Portuguese

50:31.040 --> 50:36.240
contributors in Brazil and the Portuguese contributors in Portugal because they were asked

50:36.240 --> 50:41.520
to review within a single pool. And so because the Brazilians outnumbered the Portuguese in Portugal,

50:41.520 --> 50:47.120
they would all correct their submissions to Brazilian Portuguese. This is a very interesting

50:47.120 --> 50:52.160
concept. And this is just on the notion of dialect. But your wider point is this idea that

50:52.960 --> 50:59.360
language is a tool for communication. And there's actually this very interesting concept about

50:59.360 --> 51:05.120
whether we even use language to think or if we use it as a utilitarian tool. Why is that relevant

51:05.120 --> 51:10.720
here? Because the way that we achieve an objective is going to depend upon where we are in the world.

51:10.720 --> 51:14.880
And the way that technology should serve us is going to depend on where we are with the world.

51:14.880 --> 51:19.200
This has come out recently. We just released a paper which I'm quite proud of, which is thinking

51:19.200 --> 51:26.000
about this idea of local versus global harms. At any one moment, we have multiple facets of our

51:26.000 --> 51:34.320
identity. So there's notions of what is insensitive to us as part of a notion of being global citizens.

51:34.320 --> 51:38.880
And that probably gets to things like there's a universal agreement that some types of harms,

51:38.880 --> 51:45.200
like harms to world children are particularly egregious. And most of, almost universally,

51:45.200 --> 51:50.320
our legal systems reflect this. But there's also notions of very particular harms which are

51:51.040 --> 51:56.240
cultural and very specific to how we live. And that is reflected in things like wording. So

51:56.960 --> 52:02.320
we just released this paper, which I think is important for safety, but also part of this

52:02.320 --> 52:08.160
broader move and in the field, which is that most of our models right now are trained with a

52:08.160 --> 52:13.120
single objective, a single decision boundary. What that means is all the data gets flattened

52:13.120 --> 52:18.880
to this one decision boundary. I'm very interested in multi-objective optimization. And this changes

52:18.880 --> 52:24.400
it so that you can hold multiple objectives at once. And that perhaps you can even adapt

52:24.400 --> 52:29.440
these objectives on the fly, which is very interesting. Yeah, a couple of things. I mean,

52:29.440 --> 52:34.640
you're talking, I guess, about the interplay between having a relativistic worldview and

52:34.640 --> 52:40.480
having some global norms. And in general, the way we do model alignment with our LHF and so on,

52:40.480 --> 52:47.360
it tends to de-complexify the reality of the world that we live in. And in ethical frameworks,

52:47.360 --> 52:51.600
there are deontology people who think there are just guiding principles and there are

52:51.600 --> 52:55.760
virtue ethics people who think there are certain virtues that we should emphasize. And there's

52:55.760 --> 53:00.160
consequentialism that there are certain consequences that are bad. And as you were just

53:00.160 --> 53:05.680
pointing to, it's very, very difficult to have a hybrid ethical framework that encapsulates

53:05.680 --> 53:10.240
all of these things together. What kind of work are people doing and what are you thinking about

53:11.040 --> 53:16.160
it? Well, we recently, the paper we just released is this really, this paper called,

53:16.160 --> 53:22.480
we call the Multilingual Prism, which is this idea that for safety, we collected both local

53:22.480 --> 53:29.280
examples of red teaming safety with really this very nuanced collection process across multiple

53:29.280 --> 53:35.040
languages, as well as harms that were considered global. From there, you can go into something

53:35.040 --> 53:39.920
like our LHF and you can change the notion of a single reward model. So this is an area I'm

53:39.920 --> 53:44.080
quite interested in. Like, how do you have multiple reward models? And then how do you

53:44.080 --> 53:47.680
balance them? This is the crux of the problem. And that's what you're getting at. So how do

53:47.680 --> 53:52.720
you have these two things in unison? And I suspect what we're going to see there is

53:53.600 --> 53:59.200
this notion of adaptation of our models in a more nimble way than previously. So typically,

53:59.200 --> 54:06.800
in a production setting, you spend months doing this model, you release it, cool, thumbs up,

54:06.880 --> 54:13.760
enjoy, and it's not as dynamic, but a true production model is refreshed and is more nimble

54:13.760 --> 54:17.600
and is deployed in different ways to different places. Like Netflix famously does this with

54:17.600 --> 54:23.200
its recommendation systems. I think here, this is actually a much more profound way of doing this

54:23.200 --> 54:30.320
because you can have these models, which essentially the way that they're steered is adapted. And this

54:30.320 --> 54:35.600
is both interesting as well as profoundly challenging because the tricky thing is,

54:36.240 --> 54:42.320
is you want to be sensitive to how the preferences of users change around the world,

54:42.320 --> 54:47.200
but you cannot overfit to too granular a preference because this is a philosophical

54:47.200 --> 54:51.200
tension you're actually getting at, which is that, you know, a libertarian view would say

54:51.920 --> 54:57.360
every person here has a list of preferences and those should be respected in their rank order.

54:57.440 --> 55:03.360
But as a society, we typically say we have this group of preferences, but we also adhere and kind

55:03.360 --> 55:09.360
of subsume some of our preferences for the common grid. And so there's this notion as well, when

55:09.360 --> 55:15.440
you articulate that as an algorithm, how do you get that balance somewhere in the middle, like where

55:15.440 --> 55:21.520
you are basically not adhering completely to a societal view. I think that's one of the concerns

55:21.520 --> 55:26.480
about algorithms and tokenizers being used in certain states where there's a state influence

55:26.480 --> 55:32.320
on how algorithms are deployed, but also not being used a total libertarian view where we don't want

55:32.320 --> 55:38.880
objectives that essentially amplify how a person thinks about the world without balancing and

55:38.880 --> 55:42.400
introducing different viewpoints. Yeah, it's so fascinating because, you know, even things like

55:42.400 --> 55:46.880
polarization on their face seem like an incredibly bad thing, but some kind of diversity preservation

55:46.880 --> 55:52.640
might actually lead to a pluralistic society that, you know, gains information and, you know,

55:52.640 --> 55:56.800
like a degree of health actually that we need. But with safetyism in general though, there's

55:56.800 --> 56:02.480
always this notion of, I think we probably agree on this a little bit, that if you leave people

56:02.480 --> 56:08.560
to their own devices, then that can be bad, but you also need a little bit of that because otherwise

56:08.560 --> 56:14.080
the society might become quite sclerotic. And these decisions presumably need to be baked into

56:14.080 --> 56:19.600
the way that we build these models in some way. Yeah, and currently then not. I would say currently

56:19.600 --> 56:25.360
the way that we approach safety, it's quite, we have this notion of refusals. So when you've

56:25.360 --> 56:30.720
engaged with a model, you typically will see refusals for certain what I would call the more

56:30.720 --> 56:35.760
black and white type cases. There's this really interesting opportunity I see in the evolution

56:35.760 --> 56:40.400
of how we think about safety, which is that instead of just saying I can't answer this to kind of

56:41.520 --> 56:47.520
provide more nuance or provide links to additional support. And I think that's very

56:47.520 --> 56:52.560
interesting because there's a different type of discussion. But I would say your perspective,

56:52.560 --> 56:57.760
what you're talking about, which is really this part in the middle where you have some values as

56:57.760 --> 57:02.400
like how you build your algorithm, but also you realize that this is someone who's engaging with

57:02.400 --> 57:08.000
an algorithm and like this is, you know, the algorithm should not reflect perfectly a single

57:08.000 --> 57:14.320
view of the world. You need more ways I also think within the UI for the person to influence and

57:14.320 --> 57:19.120
provide feedback and to something as course hallucinations is really interesting because

57:19.120 --> 57:24.480
hallucinations is not, you can't, I'm very skeptical we're going to eliminate hallucinations

57:24.480 --> 57:28.880
because they're also what we really like about these models. It's the creativity. So for me,

57:28.880 --> 57:33.440
this is not just, we often fixate a lot on the model in these conversations. The model has to

57:33.440 --> 57:37.920
solve this, but I think there's also a notion of the system. And I think that some things that will

57:37.920 --> 57:44.640
be interesting to play within the system is how does the user express when they think that

57:44.640 --> 57:50.240
steering isn't aligned with what they think is reasonable? A good example, this is for example

57:50.800 --> 57:55.520
a question about sexual health. There's valid reasons to ask those questions. There's valid

57:55.520 --> 58:01.520
reasons to want to understand like parts of your biology or things like that. Wikipedia has whole

58:01.520 --> 58:06.400
pages about sexual health. And so it's very interesting that a lot of systems refuse to

58:06.400 --> 58:12.000
answer this right now. So there's this nuance where we need to make sure that we are updating these

58:12.000 --> 58:16.320
binary decision boundaries where it's outright refusal and move towards something which is

58:16.320 --> 58:22.160
instead steering towards resources. Yeah, and I think so much of this is about when you fix something

58:22.160 --> 58:26.320
as it is now, it can go both ways as well. So maybe you can explain to the model, no in this

58:26.320 --> 58:31.600
situation I think you really should tell me. And likewise the model can say no, actually I think

58:31.600 --> 58:35.040
the reason I'm not allowing you to do this is because of this and maybe you should shift your

58:35.040 --> 58:40.080
viewpoint a little bit. It has to be done subtly because people don't like re-education. So that

58:40.080 --> 58:44.320
actually creates, they say the road to hell is paved with good intentions, it creates an equal

58:44.320 --> 58:49.200
and opposite reaction when you try to re-educate people. But coming on to RLHF, I mean we've

58:49.200 --> 58:54.880
spoken about this for years, you've always been a bit grumpy about RLHF and I read your paper,

58:54.880 --> 58:58.240
unfortunately I don't have an internet connection so I'm doing this from memory. Can you just remind

58:58.240 --> 59:02.720
me the multilingual paper that you've just released where you're trying to remove translation

59:02.720 --> 59:09.920
artifacts? Oh yes, RLHF speaks many languages. So this is a really nice paper, it was led by

59:09.920 --> 59:15.600
John. This idea that we were the first to extend a lot of the RLHF techniques from many different

59:15.600 --> 59:22.880
languages. So I think actually there's a wider view of RLHF and I have been grumpy about it,

59:22.880 --> 59:27.200
but you go first. Yeah, what were you going to... Well I mean even in this paper you were saying

59:27.200 --> 59:31.840
that, I mean obviously the broader conversation we've just had is that we need perhaps you know

59:31.840 --> 59:37.440
some kind of a more systems approach where we have a multitude of different models and optimizers

59:37.440 --> 59:42.160
and datasets and all that good stuff. But even within RLHF you are saying that it's hideously

59:42.880 --> 59:47.280
complex and inefficient and you have to have this separate reward model and it can't be optimized

59:47.280 --> 59:52.240
very well and you are saying that sometimes using DPO or even just basic reinforce is better.

59:52.240 --> 59:57.280
Yeah, so there's an excellent paper which we also recently released back to basics where we actually

59:57.280 --> 01:00:01.360
do a much more profound question of this, not even specific to multilingual, we take a step back.

01:00:02.000 --> 01:00:11.840
And we say okay, it's really interesting. All the kind of most cited papers originally on RLHF

01:00:12.400 --> 01:00:19.600
are papers which really take this canonical method within RL, PPO, and apply it to the

01:00:19.600 --> 01:00:28.640
language setting. And PPO really evolved in the RL, traditional RL space to address and mitigate

01:00:28.640 --> 01:00:34.480
a lot of the issues with traditional RL. RL is typically over a large expansive search space,

01:00:34.480 --> 01:00:41.040
incredibly noisy, and the trickiest part right is that your errors compound. So it's almost like

01:00:41.040 --> 01:00:45.920
thinking about well what if I bet incorrectly at a game table and then tried to bet again and

01:00:45.920 --> 01:00:53.360
did it incorrectly and your losses just compound the more that your estimates are off. So PPO is

01:00:53.440 --> 01:01:02.240
heavily what I would call kind of regularized or conditioned to limit the impact of an incorrect

01:01:02.240 --> 01:01:07.520
estimate. What that means is that often it's quite memory intensive, you kind of have four

01:01:07.520 --> 01:01:13.680
models and play at any one time, and it also means that it's quite sensitive. So typically PPO to

01:01:13.680 --> 01:01:21.600
train, it takes longer. And showed the language setting. And so the initial adoption of PPO and

01:01:21.600 --> 01:01:27.280
the success of it was taken at least value. This is incredible, let's go with this. But

01:01:28.080 --> 01:01:31.840
the language space is also an enormous search space because if you think about it, you're trying to

01:01:31.840 --> 01:01:37.440
predict the next token, how many tokens, how many possible tokens are there in the world to represent

01:01:37.440 --> 01:01:43.840
language. But by the time you have a trained model, and by the time you've done all this pre-training,

01:01:43.840 --> 01:01:48.960
the search space is much narrower. And actually it's quite interesting because the likelihood

01:01:48.960 --> 01:01:53.520
and the probability of what the next token will be is actually very concentrated.

01:01:53.520 --> 01:01:58.080
And when you have this pre-trained base, it's only going to be a few different tokens that you

01:01:58.080 --> 01:02:03.200
would likely predict, which means that this was overkill for the setting. And what we show

01:02:03.200 --> 01:02:09.440
convincingly and back to basics is that you can strip a lot, a lot of the components of PPO out.

01:02:10.080 --> 01:02:16.880
You can propose something like RLU, which is still an RL method, but that works effectively and even

01:02:16.880 --> 01:02:24.400
surpasses it. And RLU is also what we used in the RLHF speaks many languages. And we showed that

01:02:24.400 --> 01:02:30.080
this is very impactful. And because it's online, it does beat things like DPO, which is offline.

01:02:30.080 --> 01:02:37.360
So RLU is still an RL method. But what it's really saying is that we are in a well-conditioned

01:02:37.360 --> 01:02:42.400
search space. And because of that, we can be a lot more nimble about how we explore it.

01:02:42.400 --> 01:02:48.400
Yeah. Well, in the RLHF on many languages one, because obviously you've had this huge focus on

01:02:48.400 --> 01:02:53.440
multilingual. And I suppose there's the problem of getting diverse data, because this is super

01:02:53.440 --> 01:02:58.480
heterogeneous data when we're doing multilingual language training. And of course, even the

01:02:58.480 --> 01:03:04.080
preference completions, they needed to be generated as well. And I think you generated some with

01:03:04.080 --> 01:03:08.480
translations and then you had a strong model and you had a setup there. Can you tell us about that?

01:03:08.480 --> 01:03:13.200
That's fun, because it's part of this wider issue where multilingual relies a lot traditionally on

01:03:13.200 --> 01:03:17.760
translations. You don't have data, so you translate your good English data, your gold standard data,

01:03:17.760 --> 01:03:23.680
or your good Mandarin Chinese data into many different languages. Here is where it gets

01:03:23.680 --> 01:03:28.800
interesting. Translation models typically have what we call translation ease. There's these weird

01:03:28.800 --> 01:03:34.880
artifacts that pop up. So you might have like, odd enumeration where instead of like the one,

01:03:34.880 --> 01:03:39.680
two, three, it spells out one, two, three. So it's just, and it's very annoying for people who

01:03:39.680 --> 01:03:45.280
have to experience it because it gets imparted to the model, the downstream model. So we did

01:03:45.280 --> 01:03:51.520
something which I think is very fun with this paper where we said, well, the whole goal of RLHF

01:03:51.520 --> 01:03:57.520
is to steer away from certain parts of the distribution, steer towards other parts. And so

01:03:57.520 --> 01:04:02.240
what we did for our preference pairs, so let's think about the normal way preference pairs are

01:04:02.240 --> 01:04:08.160
done. It's quite expensive and time intensive. You have to go get annotators and you're asking

01:04:08.160 --> 01:04:15.440
humans which one do you prefer. We did this really fun, I would say, trick here where we said, well,

01:04:15.440 --> 01:04:20.880
we know we have translation pairs. We generate synthetic pair, the other pair, with what is

01:04:20.880 --> 01:04:26.480
a very high-performance model. In this case, we use Command R+, which is super-performance,

01:04:26.480 --> 01:04:31.440
does very well in many different languages. And then we compare the two and we ask an

01:04:31.440 --> 01:04:37.520
Alem is a judge, which is better, the translated English or the sampled in the other language.

01:04:37.520 --> 01:04:42.160
And what we found was this actually helped with translation artifacts because it steered the model

01:04:42.160 --> 01:04:50.400
away from the bad, translated and towards the more versatile, fluid Command R+, generation. So

01:04:50.400 --> 01:04:55.280
really interesting. And there were some percentage of time where the translated was better. And so

01:04:55.280 --> 01:05:00.560
you got that nuance too. So very, very interesting. Yeah, amazing. And then that removed a lot of

01:05:00.560 --> 01:05:06.880
the translation artifacts. Yeah. Amazing. Sarah, this has been incredible. Where would you like

01:05:06.880 --> 01:05:12.880
to point people to as well for your later stuff? Feel free. So, you know, I lead Co-Here4AI. So

01:05:12.880 --> 01:05:18.720
it's a research that we do a lot of fundamental research. And we, a lot of my work is on efficiency,

01:05:19.920 --> 01:05:24.080
reliability and building these models that scale the next generation models. So you can go to

01:05:24.080 --> 01:05:28.720
Co-Here4AI and take a look at some of our work and just a lovely being here again. It's really

01:05:28.720 --> 01:05:41.440
nice catching up. Amazing. Sarah, thank you so much. Yeah, thank you.

