{"text": " Past a certain level of complexity, every system starts looking like a living organism. In order to build a general intelligence, you need to be optimising for generality itself. We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness of patterns from a tiny little bit of information. Generalisation is the ability to mine previous experience to make sense of future novel situations. Generalisation describes a knowledge differential. It characterises the ratio between known information and the space of possible future situations. To what extent can we analyse the knowledge that we already have into simulacrums that apply widely across experienced space? So intelligence, which is to say generalisation power, is literally sensitivity to abstract analysis, and that's in fact all there is to it. In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years. I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops and hackathons. I used to travel around the world and I always had a copy of Francois's book Under My Arm. It never left my side. I used to force everyone to read the first four chapters of that book and of course the chapter on the limitations of deep learning before we did anything. Francois has a clarity of thought, which is unparalleled I think in any other human being on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has no trouble at all finding holes in some of our guests' work, had this to say while prepping for the show. I'm working on it. It turned out to be a little bit more difficult than I thought. Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me doing that. But anyway Chalet is extremely controversial to some people actually, but he's not controversial to us. Our discussion today lies at the intersection of machine learning and reasoning. Now Chalet has made his vision completely clear about what he thinks the future of machine learning is. Make no mistake, what you should take from today's episode is that the future of artificial intelligence is going to be discrete as well as continuous. Actually the two are going to be enmeshed. The future of AI will almost certainly involve a large degree of program synthesis. Deep learning has its limits. You can use deep learning for continuous problems where the data is interpolative and has a learnable manifold and where you have a dense sampling across the entire surface of the manifold between which you need to make predictions. For Chalet, generalization itself is by far the most important feature of intelligence and of developing strong AI. He describes a spectrum of generalization starting with, for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed. The machine learning we have today confers some adaptation within a known domain of tasks. For example, being able to recognize dogs or cats within a variety of different poses and lighting conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the door of this now with GPT-3, where the subtask, if you like, is given at test time. Although Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3 knows was learned on the vast amounts of training data that we trained it on, the poet algorithm from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training process, which is very, very interesting. It's creating new problems and new solutions as part of the training process. But broadly speaking in the machine learning space at the moment, the task that we are doing is fixed and not generalizable. The other thing is that the real world does not have a static distribution. We need systems that can adapt dynamically. Intelligence requires that you adapt to novelty without the help of the engineer who helped you write the system. Chalet has come up with a formalism of intelligence that balances the task skill, the difficulty, the knowledge, and experience to effectively quantify and normalise an algorithmic information conversion ratio. It's the ability to convert experience into future skill that is Chalet's measure of intelligence. At the end of his measure of intelligence paper, Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware generalisation. Any model that we have needs to generalise to tasks that the developer was unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to represent the kind of priors that us humans have. Now machine learning algorithms are completely ineffective against the ARC challenge because it's so challenging to generalise from a few examples. The only solutions that were effective in the ARC challenge were programme synthesis. The manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space. There are both theoretical and experimental reasons to believe this is true. If you believe this, then the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds. The only way deep learning models can generalise is via interpolation. Most perception problems in particular, according to Francois, are interpolative. Neural networks not only have to represent the manifold of the data that they're learning, the manifold also needs to be learnable. And that's an even tougher constraint. Gradient descent will not learn data that has challenging discontinuities in its manifold. It'll just resort to memorising the data. Deep learning allows you to represent complex programmes that you couldn't write by hand, but on the other side of the coin it also fails to represent very simple programmes that you could write by hand. Discrete programmes. So there are some problems where deep learning is a great fit and there are other problems where deep learning is a disaster. And the reason for that is that they are not interpolative in nature. These tend to be algorithmic reasoning problems. Francois thinks that 99% of software written today, with code, is not interpolative in nature and therefore it's a bad fit for deep learning. The only answer to these problems is discrete programme search. To use deep learning for these problems requires a lot of data. It's hard to train and the representation will be glitchy. It'll be brittle. Neural networks cannot even extrapolate the scalar identity function, f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation. So is this similar to interpolation? Well, I mean, all of machine learning is similar to interpolation if you want, right? When you train a linear regression on scalar values, you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the best values of A and B for y equals A x plus B that minimizes the square error of the prediction of a line to all of the points, right? That's linear regression. That's interpolation. All of machine learning is interpolation. In a high dimensional space, there is essentially no such thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right? So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have a million samples, you're only covering a tiny portion of the dimensions of that space, right? Those images are in a tiny sliver of surface among the space of all possible combinations of values of pixels. So when you show the system a new image, it's very unlikely that this image is a linear combination of previous images. What you're doing is extrapolation, not interpolation, okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard. I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out. It's certainly true that interpolation on the native data domain is useless, right? We need to pull some useful information out of the data and the model architecture and training method matter a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation in the original space of the data, right? Chalet is quite clear that neural networks only generalize through interpolation. You might argue that you can go a tiny step outside of the convex hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue that if the manifold doesn't give you any useful information outside of the training range, then it wouldn't be any better than finding your nearest training example and just adding a bit of random noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly, you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on might be shaped like a football or a sphere, which means if you go outside of the training range, you actually have some information about those data points. The scalar identity function might seem like a contrived example, but it's a really interesting one. When you go outside of the training range, nothing about the manifold is known, right? Think about the manifold. It's just a string that goes on forever. We don't know anything about that manifold outside of the training range. This is not true for most perceptual problems in deep learning. And this is why image models, for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous problems versus discrete problems that we're going to be exploring in the show today. It's very interesting that brittleness works both ways, depending on the discreteness of the problem. Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist, and deep learning would be extremely brittle predicting the digits of pi or prime numbers or sorting a list. So brittleness here means the overall fit of your model or your program, so accuracy and robustness. Imagine if every single bug you experienced with computer software was entirely unique to you and the development team wouldn't even be able to reproduce it. This is what would happen if software was written entirely with neural networks. It would be more, not less brittle. Sholey thinks that motivated thinking is the primary obstacle to getting people to wake up to the fact that neural networks are poorly suited to discrete problems. The people who are good enough at deep learning to realize its limitations are too invested in its success to say so. Sholey fundamentally thinks that there are two types of thinking, type one and type two. He thinks that every single thought in our minds is not simply one or the other, rather it's a combination of both types. Type one and type two, they are enmeshed together in everything you think and in everything you do. Even our reasoning is guided by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization and the way we perform abstraction is different in continuous versus discrete space. We need to find analogies and those analogies will be found differently in both of those different spaces. Program search allows us to generalize broadly from just a few examples. It marks a significant deviation from traditional machine learning. Rather than trying to interpolate between the examples you have, you're constructing an entire search space from scratch and testing if it fits our training data. It all started with the flash fill feature in Microsoft Excel. Do you remember that? You give a few examples of some transformation that you want to perform and it will generate a piece of programming code for you, which means it can generalize that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around for about 20 years actually, but what's really making it work now is the idea of using neural networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new algorithms on the fly. It's already learned continuous and often glitchy representations of existing tasks during its training. It's completely ineffective against his arc challenge tasks. People often claim that neural networks are turing complete. No, they're not. A model has a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded space or unbounded time. For example, could you train a neural network to predict the nth digit of pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural network to do it. A simple turing machine program can do just that and that is because a turing machine can access unbounded memory and time. The best thing that neural networks can do is approximate unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural network to approximately multiply integers together. Yet, even when learning to multiply fixed-width integers, practically-sized neural networks introduce errors occasionally, and for a fixed-sized neural network, these errors grow more common as the size of the input grows. That said, neural networks are finite state machines, and just as finite state machines can be augmented with unbounded memory and iteration to yield a turing machine, neural networks can also be automated in the same way to produce a turing-complete computational model. If you want to see a concrete example of the kind of discrete program search that Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video about it. So yeah, it feels like today is the culmination of a year of really hard work and passion from the MLST team. We've worked with so many fascinating people. We've had so many amazing guests on. It really means a lot to us. Today is a very, very special episode. It was my dream from the beginning to get Chalet on the show. I know that Chalet is going to say lots of interesting things that will trigger some people and inspire others, and please take to the comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out. Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher. Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in the machine learning space who's caused a massive stir in my thinking, the only other notable one actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was always to get Francois on the show, and I can't believe that it's actually happened. We actually have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show, but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually than anyone else by a wide margin, and it's because his work is very thought-provoking and disruptive. I spent many weeks actually studying his measure of intelligence paper last year, and of course his recent New York's workshop was fascinating as well. Almost every single word in my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that. Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general intelligence. All intelligence is specialized. Critically, he thinks that generalization, the ability to deal with novelty and uncertainty is the most important concept in intelligence. He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep learning only works for problems where the manifold hypothesis applies. For example, problems which are interpolative in nature and when a sufficiently dense sampling of your distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize, but it cannot always generalize. And in his recent New York's presentation, he introduced the concept of program-centric and value-centric generalization, which we'll get into in the show today. But I wanted to move straight on to this concept of deep learning being a hash table, because this is what Francois thinks. He says that a deep learning model is a high-dimensional curve with some constraints on its structure given by inductive priors, and that curve has enough parameters that it could fit almost anything. If you train your model for long enough, it'll simply memorize your data. And because of SGD, your manifold fit is found progressively, and at some point, the manifold will approximate the natural manifold between underfitting and overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating on that manifold. So the power of the model to generalize is actually a consequence of the structure of the data and the gradual process of SGD, according to Francois, rather than any property of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a rather different view, because one school of thought is that deep learning models are kind of like searching for a space of possible programs, and advocates of GPT-3 make this argument quite strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is interpolating between mathematical conjectures, assuming that interpolation space would actually give us new information about mathematics, if he thought that that space wasn't interpolatable. What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation is the origin of generalization in deep learning models, and that's very much by construction, by nature, right? Like a deep learning model is a very large, differentiable, parametric model, trained with gradient descent. And so the only way it's ever going to be generalizing is your interpolation. This is literally, this is what it is, this is what it does. So I think the question, you know, are all deep learning models, interpolators or not, is not a super interesting question, because it's not an open question. We know they are. But the more interesting question, I think, is what can you actually achieve with the stored of interpolation on this very complex, very high-dimensional manifold, that they're deep learning models and implementing. We're telling you the properties of this generalization, the tasks for which it will perform well, the tasks for which it will not perform well. I guess one example I could give you is encoding data with the Fourier transform, like you know about the Fourier transform. And maybe, you know, some people will play around with it and they will be like, hey, you know, actually the Fourier transform can draw much more than curves. Look, I made a square with it, right? And then you would have to point out that, no, actually the square, you've made it by supposing lots of tiny curves. And it's not, in fact, a perfect square, right? Because it is made of this, with the other supposition of lots of tiny curves. And that's really, this is true by nature, by construction. This is where the Fourier transform starts, right? And the more interesting question is, you know, what sort of data is a good fit for encoding the Fourier transform? And what sort of data is not a good fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves, then it's going to be a very, very efficient encoding at a good idea. And deep learning is very much like that. We should ask, you know, what are its strong points, what are its weak points? Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there, I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table with some amount of generalization power, because they have some notion of distance between parts. They're capable of comparing points by measuring the distance between them, right? And this, this is what would enable this kind of hash table to actually generalize, as opposed to the classic kind of hash table, which is just memorizing the data. It's very interesting that you allude to the fact that, you know, what kind of data is the model good for, and so on. And now, deep learning models being essentially like really, as Tim said, like big interpolators of arbitrary manifolds, do you think there is something common across the types of data we choose deep learning for? Or, you know, could we in fact use deep learning for most kinds of manifold dish data? Or do you think there is some kind of specialness about natural signals that makes deep learning very attuned to them? So I think most things are to some extent interpolative, which is why you can actually do lots of things with deep learning models. Doesn't necessarily mean it's always a good idea, but it's going to kind of work, right? You know, when people hear the word interpolation, they tend to think about linear interpolation, that's what pops up in their mind. That's not always what deep learning models are doing, right? They're interpolating on this very complex, very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior. And in practice, it's always possible to an arbitrary discrete algorithm in a continuous manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory. So for any program, you can imagine, you can ask, you know, is there a deep learning model that will encode some kind of approximation of it? And the answer is always yes, right? Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But there are, if you try to do that, actually, there are some issues with that. There are very much, you know, some problems for which deep learning is good fit, some problems for which deep learning is not a good fit. In the limit, the extreme point is a space that is not interpolative at all, which is quite right, actually. You know, most spaces, even very discrete kind of spaces, do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance, trying to train a deep learning model to predict the next prime number, right? Or to tell whether a number is a prime number. As you cannot actually do that, the best you can do is memorize the train data point, because the space of prime numbers is not interpreted at all. So your deep learning model will always have zero generalization power. But that's actually quite rare. This is kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily mean that it's a good idea to try to solve, you know, such problems with deep learning models for deep learning to be a good idea. You need a very, you need very much the manifold level as it is to apply. So it works best for perception problems. Any problem that humans can solve via pure intuition or perception is probably a good fit for deep learning. But any problem where you need, you know, high level explicit step by step reasoning is probably a bad fit for deep learning. And, you know, 99% of what today software engineers solve, the writing code is going to be a bad fit for deep learning. That doesn't mean that there wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm in a smooth manifold. This is always possible to some extent, right? But there are very significant issues with attempting to do this. I like just because something is theoretically possible doesn't mean you should actually do it. I think we might be not being careful enough when we say what we mean by program. Because, for example, if I take program to be the universal sense like a program is something that can run on a Turing machine, for example, because of the fact that that type of program actually has access to unbounded time and memory computation. It's impossible in the general sense to encode that in any finite neural network, like I can write a very short piece of code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I think that's like a big source of confusion often time with these statements that like, you know, oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural Turing machine, which is a neural network that's the finite state machine piece of a Turing machine, that can be Turing complete. But in the general case, you know, finite neural networks, which is what everyone means by neural networks, are not Turing complete. And it actually has practical effects, right? This is why we see this sort of explosion and the number of parameters to kind of, you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only interested in realistic programs, like the sort of programs that start to engineer with right, for instance. And we're only interested in realistic neural networks. And by the way, the constraints that we have on neural networks are actually much stronger than asking, given this program that I have, is there a neural network that could embed it in a continuous manifold? The constraint is actually, is there a neural network that could not only represent it, but that could learn this embedding of the program from there. And this is a several orders of magnitude harder, right? Learnability is a big problem because you're fitting your manifold via gradient descent, right? And if the structure you're trying to fit is too discrete, with too big discontinuities, gradient descent will not work at all. And the best you can do is, again, just memorize the train data. So I can maybe give you a concrete example to kind of ground our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do something pretty cool, which actually became a cool example on the Keras website. He used a LSTM model to multiply numbers, but not like numbers multiplied by value, but the input of the model would be strings, like two strings, strings of digits. And the LSTM will actually learn the multiplication algorithm for like multiplying three digits and three digits numbers, kind of the sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably, that worked, right? It works just fine. So you can train a deep learning model to learn this algorithm. And you could, of course, train a transformer model to do the same. It will actually be probably significantly more efficient. So that works. That comes with a number of downsides. So first, in order to train that algorithm, which is very simple, you're going to need thousands and thousands of examples of different strategic numbers. And once you've trained your algorithm, because the actual algorithm was embedded in the neural network, it does generalize to never see before digits, right? So it's actually learning the algorithm. It's not just learning, I'm just not memorizing the data. But the thing is, because the embedding of an algorithm, the embedding of a discrete structure in the continuous space, is not the same thing as the original discrete object. There are glitches in your deep learning network, unless that's something you could have found via program synthesis, for instance, it's not going to be correct 100% of the time, it's going to be correct 95% of the time. In much the same way that if you try to encode a very discrete object via the Fourier transform, it's not going to be correct. 100% of the time is going to be an approximation and around sharp angles, it's actually going to be wrong. And very importantly, and this is really like the algorithm that you've painstakingly embedded into your deep learning model via exposure to data, does only, it does not generalize very well, it only does local generalization, meaning that if you train it with three, to multiply three digit numbers, and then you send it a five digit number, is it going to work? No, absolutely not. And not only is it not going to work, but you could not in fact, few shots fine tune your algorithm to learn to handle five digits, seven digits and so on. If you want to fine tune your algorithm, you're going to need thousands, maybe millions of examples, right? So it's all local generalization. And lastly, it's super inefficient, like I think we can all agree with this, that multiplication is not like it's not a clever use of an LSTM, it's you're burning tons of resources for something that's actually super easy. And you can compare that, like since we are talking about pros and cons of deep learning, you can compare that to what you could get with a program synthesis engine. Like I don't want to compare to what you could get with a human written algorithm, because kind of the point of deep learning is that it enables you to develop programs that you could not otherwise write by hand. So the right point of comparison is actually what you could do with deep learning versus what you could do with discrete program synthesis based on discrete search and the DSL. And if you were to use a program synthesis to solve the multiplication problem, so you would find a solution, even a very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to find it, it can find it with a handful of examples, you're not going to need thousands of examples, like in the deep learning case, you're going to need maybe five. And the program you get out of it is going to be exact, because it is the exact discrete algorithm, it is not a continuous embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight, so it will be very efficient, you know, and like the LCM or transformer model, and crucially, it's going to generalize. So if you develop it only from three digit numbers, maybe there will be something inside it that will hardcore the assumption that they're dealing with three digit numbers. But even if that's the case, you can take it and automatically learn a generalized form of it if you just start giving it seven digit numbers. Very easy because it's just modifying probably a couple lines of code. So it is capable of strong generalization. So here you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on a person at a time, it's going to be pretty long. It's only going to be capable of hardcore generalization, right? Because again, there is a huge difference in representational flexibility between your very simple, discrete algorithm and some kind of very complex, high dimensional continuous embedding. And then there's also the efficiency consideration. So clearly for if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's perception problem, where you have data points that fit on a nice and smooth manifold, then deep learning is actually the right answer. And if you tried to train a discrete program to develop your program synthesis, an actual algorithm to classify MNIST digits, for instance. Everything I just said would be true, but in reverse, your program would be brittle. The deep learning model would be robust, and so on. So there are really problems where deep learning is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying. It is possible to sort a list of deep learning with some hacky inductive priors and probably memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots of problems that fall in the middle, where there is a semi continuous structure and some regularity, but it's still a discrete problem. And you're saying in that situation, we should still use program search, but maybe we can use deep learning, maybe something about the shape of the manifold, even though it's semi continuous, could actually tell us about how to do that program search more efficiently. But it seems to me that if there are problems out there, let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here, it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily paying a lot of attention to the nature of deep learning, why it works, why it doesn't work. I also think the people, they are basically two categories of people. They are like laypeople, and they are people with deep expertise. And the big problem we have here is that the people with a lot of expertise are going to be a lot of the time driven by motivated thinking. Because like I do, they work in the field of deep learning, and so they're going to have this vested interest in deep learning being potentially more powerful, more general at the nature is. I think if you want to think clearly, the primary obstacle is motivated thinking. It's fighting against what you want to be true. So I tend to have super boring opinions in that sense, because I do my best to try to forget kind of what I would like the world to be in my best interest and try to look at it as it really is. And that will tend to actually diminish the importance of my own work. So yeah, but you know, I've been doing deep learning for almost a decade. Of course, I would want it to be like this incredible world changing thing that leads to human level intelligence, right off the bat, that would be that would be awesome, that would be amazing, and that would be right in the middle of it. But that's not that's not actually what's going on. You said you tend to be what was the word, not not controversial ideas or something because you try to stick to the way the world is rather than the way you want the world to be. But we just had Yannick produce an interesting video about how if you think that machine learning models essentially attempt to do the same thing, right? I mean, they're not human beings, they don't really have wants per se, they're just modeling reality as it is. It turns out reality itself really annoys a lot of people, like they just don't like reality, and they don't like the way the world is, and they wish it was something different. And that infects like every mode of their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend. But I have no exception. Most people have opinions not because they've seen evidence in support of their opinion, but because it's in their interest for this opinion to be true, or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3 and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim that the pre-trained language model could perform few short generalizations. I thought that's super fascinating. I'm always super excited if I hear about something that's really challenging my initial kind of mental model of how the world works, you know, it's like a few years back, and there was this claim that a neutrino was measured going faster than speed of flight. I mean, that's exciting, right? That's like new physics, you want it to be true, at least you want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something, something novel, and that would really challenge what they thought to be true by deep learning models. And I regret to say that everything I've seen close has actually confirmed. In my view, that basically deep learning models, they can learn to embed algorithms given sufficient exposure to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC, for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually learned anything from it. It hasn't expanded my view of the world, which is too bad. Like, I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model is exposed to many patterns. You could call them algorithms, for instance, in many different contexts. And so it has memorized these patterns. And now it's able to take these patterns and apply them to new data in much the same way that the multiplication algorithm we are talking about. Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms like that. But the model is not synthesizing these algorithms on the fly. They are in the model already. And if you try to apply GPT-3 to something for which a new algorithm would need to be produced, like an ARC task, for instance, it has just completed anything. It seems to all build up what you're saying, because there is this strong generalization versus local generalization. And then you make a case that in order to do strong generalization, we need maybe something like program synthesis approach. So deep learning can't necessarily get us there in most problems. And you make an interesting case that something like graph isomorphism search could play a core role in that. Could you briefly connect all of these terms together of the case you're making there? Because it's super interesting. So going back to it, Tim was saying it's rarely the case that you have problems that are fully interpretive or fully discrete. There are definitely such problems. In fact, most perception problems are almost entirely interpretive. And most programs, the kind of programs that you write there, they're largely discrete, not interpretive. But most tasks actually are best solved via a combination of both. And I actually believe that's true for the way humans think. You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every thought you have and everything you do with your mind is not one or the other. It's a combination of both. That type 1 and type 2 are really unmatched into each other in everything you think and everything you do. Like, for instance, perception. That looks like something very instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot of reasoning that's embedded into perception. And the reverse is true, for instance. If you look at a mathematician, for instance, proving a theorem, where they're writing down on the sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided by high-level intuition, which is very much interpreted. They know where they're going, without having to derive the exact sequence of steps to get there. So they have this high-level kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions because you are driving on network frauds. But if you have a bird, a GPS, for instance, you can kind of see the direction in which you are going, which is interpolated. If you're talking about direction, you're talking about distances, you're talking about geometric spaces. And everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same time. If you go back to first principles, intelligence is about abstraction. So intelligence is about the ability to face the future, given things you've seen in the past. And the way you do that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template, maybe it's an algorithm that will actually be effective in terms of explaining the future. And that's why it makes it abstract, is that it can handle multiple instances of some kind of thing, that thing is an abstraction. And if it's abstract enough, it can actually handle instances you've never seen before, right? It does generalization power. And all abstraction is worn from analogy. Abstraction starts when you make an analogy between two things. Like you say, time is like a river, if you want to get philosophical or something. But in general, you can just say this apple looks similar to this other apple. So there is such a thing as the concept of an apple, for instance. And the part that is shared between the two things that you're relating to each other, the subject of the analogy that that's the part that can be said to be abstract, that is the part that will help you make sense of the future, like you encounter a third apple in the future, you know, it's an apple. Because you don't even need to relate this to the apple should have memorized, you just need to, you just need to relate it to the template, the abstract template of an apple that you've formed by from exposure to different kinds of apples in the past. And if you think about what's what's an analogy, really, like how do you find an analogy, it's a way to compare two things to each other. And there are only really two ways to compare things. You can, you can basically ask how similar are they in terms of distance, like you can say implicitly, there's you're looking at the space of points, there's a distance between any two points. That's, that's the type one, a subject analogy that leads to type one abstractions, which leads to a type one thinking, right? So a type one analogy is like your things, you say to what degree they're similar to each other. So you read them by distance, you, so implicitly, it means you put your things on in a geometric space, right? And type one abstraction is going to be a template. It's like you're going to have clusters of things, you can take the average and say everything that is within a certain distance of that template belongs to this category. That's that's type one. It's very much the way deep learning models work. And then you and then you start adding perception and intuition on top of that, which is very much the type one thing. And the other way you can compare two things is the discrete way, right? You can say these two things are exactly the same. They have exactly the same structure. Or maybe the structure of this thing is a subset of the structure of this bigger thing. So this creates topology grounded comparisons. So you have the geometry grounded comparison. It's all about distances and templates. And then you have the topology grounded way of comparing things. That's all about exact comparison or finding a sub graph isomorphism. So in the first case, your objects are very much points in geometric spaces. So they are vectors. And deep learning is always a great fit for this sort of stuff. And in the second case, your objects are going to be graphs, right? And you're and you're really looking at the structure of these graphs and substructure and so on. And you're doing always you're doing exact comparisons. And in practice, most thinking is actually kind of some some combination of these two atoms, right? Of these two poles. You're very rarely just going to say, yeah, this airport is exactly this close to my template of an airport. So it's an airport. You're going to have basically layers upon layers of thinking. And some of them are going to be intuitive. Some of them are going to be more about, you know, comparing structures and so on. What you're saying is really interesting, right? Because you invoke the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information, just like in a kaleidoscope, could be represented widely across experience space. So you say that intelligence is literally having some kind of sensitivity to abstract analogies. So the intelligence is about being able to face the future unknown future, given your past experience. And that's fundamentally requires the future to share some commonalities with the past. And that's that's the idea of the kaleidoscope hypothesis that the universe and our lives are made of lots of repeated atoms of structure. And in fact, if you look at the source, there are very few things that are that are unique that are kind of like the grains of sand that are at the origin of all the different kinds of moving patterns you can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see two things in the universe that look similar to each other or that share some commonalities, a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is going to be is going to be an abstraction. We'll be one of these grains of sand in your in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering the universe to get back to this source of intrinsic complexity in the universe to get back to these abstractions. I think the heart of this conversation goes back thousands of years because what we're talking about right now is a lot of say, Platonism, right? Which is that there are these ideal abstract structures. And of course, they they really thought of them as actually existing in some universe. But you know, even if they don't exist in some reality, they at least exist in concept. And it strikes at the heart of this duality that's always been a very that's been one of the central mystery, really, of a lot of human thinking, which is particle versus wave, you know, discrete versus continuous abstract versus the real versus the messy. And you know, I think you pointed out, you definitely pointed this out in this call. But I think also in some of your papers that in your view, you know, let's say the ultimate solution or whatever of creating artificial intelligence or synthetic intelligence or whatever is a hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers. And, you know, I'm kind of curious, where is the state of the art now with actually implementing hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the topological neural networks that we talked about? Where where lies the direction of some type of a hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you will? Yeah, that's a great question. So I think this is definitely an active field of research, but I think the most promising direction right now is going to be discrete search very much. So a system that is discrete search centric that has a DSA and so on. And that's one of the it's basically just problems in this engine. But it is getting lots of help from deep learning models. And there are two ways in which you can incorporate this type one sort of thinking into a phenomenally type two centric system. So one way is so basically, you want to apply deep learning to any sorts of data sets where you have an abundance of data, and your data is interpreted. One example would be being able to easily play models to generate a sort of like perception DSL that your discrete search process can build upon. So look at art, art tasks, for instance, a human that is looking at art tasks, the very first layer through which they're approaching the art task is by applying basically perception primitives to the grid they're looking at. They are not actually analyzing the grid in a in a discrete way like cell by cell, object by object, they're approaching it holistically, like what do they see? And these outputs can be discrete concepts. And then you can start you can start applying the script reasoning to them. So generating the DSL. And by the way, the reason it's possible is because humans have access to tons of visual data and these different frames share lots of commonalities, right? So it is an interpolative space where deep learning is relevant, where intuition and perception are relevant. And the other way, which is is is much more difficult and much, much more subtle thing is basically being able to provide guidance to the discrete search process, basically, because even though one single program, so learning one single problem, for instance, for an art task is not a good fit for deep learning model at all, because you only have a handful of examples to learn from. And the program is super discrete. It's not really easily embeddable in this movement. However, here's the thing, the space of all possible programs, for instance, the space of all possible art tasks and all possible programs that solve art tasks is actually very likely going to be interpolative, at least to some extent. And so you can imagine a deep learning model that has enough experience with with these problems and the algorithmic solution that it can it can start providing directions to the search to the discrete system. So basically, you're in a kind of like you have, yeah, you have like layers of of learning the lowest layer is going to be perceptive. It's going to be learned across many different tasks and many different environments. It's going to be type type one, then you're going to have the context specific on the fly problem solving system that's going to be type two. And the reason is going to be possible and efficient is because it's going to be guided by this upper layer, which is going to be type one, which is also going to be trained from a very, very long experience across many different problems and tasks. And it is able to do interpolation between different tasks. So can I challenge you a little bit maybe because you say maybe, you know, all of these problems and what humans do is a bit of an interpolate like an interpolation between the interpolative systems and the discrete systems. And I see that going for, you know, something like an arc task or or if you really write code. But if you really come to let's say, let's say the highest levels of human intelligence, which to me seems to be navigating social situations, which is is is ultimately is super complex. And I can imagine something like the graph structure you're referring to be that being, let's say I come into a room and I see the graphs as, you know, what kind of social dynamics exist in this room, you know, this is the father of this person, and that person's kind of angry at me. And so I need to, you know, do something. And my question is, how often is that really a disk like how often can you really map this in a discrete way to another graph? Isn't isn't every situation going to be a little bit different, even in terms of its graph structure? And, you know, even if in an arc task, a line is just like a little bit squiggled, any program synthesis approach would have a hard time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction is to erase the irrelevant differences between different instances of the thing and focus on the commonalities that matter. So like if the squiggled in your line is not relevant, then the proper abstraction for a line should abstract it away. I was going to pick up on that because your main point basically is that program based abstraction is more powerful than geometric based abstraction, because topology is robust to small perturbations, but it's more than that. It comes back to these analogies, right? So we actually have functions and abstractions in our mind that as you say, will take away all of the relevant differences, but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you think the type one and type two reasoning are really different or is there also a continuum between them? Like you say we need we need hybrid systems, but is there something, right? Because they're both they're both in the brain, they're both on the same neurons, like is there a continuum? So right, so yes and no, I do believe they are they are very qualitatively different. These are the two poles of cognition, but there are there are, you know, most most things we do with our mind are a combination of both. That doesn't mean it's it lies somewhere in between. It means it's a direct combination of one pole with the other, kind of like what I described with with the arc solver with three layers, with two layers of that type one and one layer in the middle of type two. But in very much the same way that you can embed discrete programs in a smooth manifold, you can also do the reverse. And when you're meaning you can basically encode an approximation of a geometric space using discrete constructs. In fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing, you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors of seemingly constant new numbers, you can compute a distance between two vectors and so all of this is an approximation that's actually grounded in discrete programs. So you can you can actually kind of merge the two together. It's not necessarily always a good idea. In particular, I think it's often not a good idea to try to embed an overly complex or overly discreet program in a constant new space. As I was mentioning earlier, the reverse is actually usually way more tractable. And by the way, my I think this is something that came up before in our conversation, but my kind of subjective totally not backed by any evidence opinion of how the brain works is that fundamentally it's doing type one on type two, using a discrete system, because it's actually much easier to do to type one via an approximation of a geometric space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can, if I just for the benefit of the reader, the listeners, if I can give some other examples, you know, for example, and mixed integer optimization, it's often the case that you take that problem. And instead of having these discrete values, you project it into a continuous space, do a continuous optimization. And then as you get sort of close to a good optimization, you discretize it back over into the, the discrete variables, you know, to, to kind of, you know, flesh out the most optimal path within that discrete space, or an example to is the gamma function, you know, which is a continuous generalization of the factorial, right? And it kind of provides some cool and interesting behavior in between those, those poles that show up very clearly on the graph as these discrete points. And this is this bizarre duality between the continuous and the discrete that we see like throughout the universe. And it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what some of the transformers folks must be saying now, because Max Welling, we had him on and folks have done topological applications using transformers or using graph neural networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms, right? It was looking at different types of equivariance in topological space. Is it a naive thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess, I guess the question is, is there like one approach that's going to end up being universal? And it's, it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind of, it's kind of the question. And the answer is no, actually, like, by, by construction, do, due to the very nature of what deep learning is, it's like parametric continuous parametric models in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never actually going to be a good fit for most discrete programs. So, and, and the reverse is true as well. I don't think, so you have basically two engines that you can use to learn problems. You have quite undecent and you have discrete search. And I think the reverse is also true that discrete search is not going to be this universal approach that's going to beat everything. I truly believe that the AIs of the future will be truly hybrid in the sense that they will have these two engines inside them, they will be able to do this, they will be able to do this quick search. Right. And then, and then, and they will set, you, is that appropriate? You said, by the way, in your measure of intelligence paper that there are three types of priors, right? Low level, sensory motor priors and meta learning priors. That's the interesting one. I think that's got intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you said in your presentation last year, the two winning folks on that Kaggle challenge, one was doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that was just doing a brute force, you know, selecting combinations of, of operations on this DSL. So this absolutely fascinates me. So at the moment, that seems like a horrific solution, but clearly no one could do it using deep learning. So, but, but this is what you're advocating for. So you're saying for these discrete problems, get, get a DSL. Now, all the stuff you're talking about, presumably they haven't done yet, you're saying, well, software engineering, the beauty of software engineering is being able to modularize things into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But he said, the reason why software engineering is so good is if I ask you, how long will it take you to build the game of Tetris? You will say not long at all. And if you look at the number of state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple of weeks is because you know that you can modularize it into, into blocks, you can't say the same for deep learning, right? But they don't appear to have done that on the arc challenge yet. Yeah, so the, the solutions we've seen on the accident so far have been incredibly, incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%. It's very primitive solutions. I think you can, even with today's technology, you can go much further. Like the, what I was describing before about learning a DSL that is perceptive and then guiding discrete program search. Yeah, intuition about program space. This is already something that you can try today. So there's one approach that I was very excited about. And that I thought was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks. So check it out if you, if you have incidents, it's very good. I think that they're trying to play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into this engine? And I think that's really to me, that that is the sort of direction that is the most promising to that. So you have a paper that's fairly long on, it's called on the measure of intelligence. And you make the case that intelligence is something like the efficiency with which we transform prior information and experience into task solutions, as, as you have said before. And in that same paper, the arc challenge is presented. So, you know, a naive reader like me assumes there is some connection between, you know, what you say about intelligence and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge. Is it immediately intelligent? Or what would you ask of that system for, for you to say, yes, that's intelligent, or it's, it's intelligent is, is high or something like this. So you, you, you would be able to make that, that conclusion, if and only if arc was a, was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc, are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the thing. So the thing you need to really understand about arc is that it's not kind of the end state of the intelligence benchmark. It is very much a work in progress. And there will be new iterations, especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches although there were some cool stuff we still learned about and so on. But mostly we learned about the flows of arc. So there will be future additions and so on. So I will tell you this, if you solve the specific test set of arc as it exists today, you're not necessarily intelligent because it is not perfect because it has its laws. But if more generally speaking, you give me a system that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow, for instance, I give them to your system. If it's always solving them, I will say, yeah, it's looking like you've got a system that's, that's got, you know, pretty close to human level fluid intelligence. This is one of the things that, look, and I like the paper a lot, I think, I think it serves as a really good, you know, foundation for us to think differently about how to build intelligence. But, but I have some, some issues with it too as well. And one of them is this sort of necessity that it requires kind of white box analysis of things in order to figure out whether or not they're intelligent. Because for example, suppose time travel is actually possible. And you know, somebody like 100 years from now looks back on your arc thing and writes an algorithm that, that solves all, all them in there because it actually knows about them already and then ships it back into the past and we enter it into the competition. And no matter what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know, this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper, it's actually just encoded, you know, prior knowledge from the future. So we have to, we always have to kind of be able to look into the box, right, in order to evaluate intelligence in the way that you define in the paper. And so my question is one, isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box measure of intelligence? So basically, the fundamental issue is that if intelligence is this conversion ratio, then computing it requires knowing where you start from. And you don't really have a way around it. So the thing to keep in mind is that the under measure of intelligence stuff is not so much meant to provide like a sort of like golden measure of tape to measure anyone's intelligence or anything's intelligence. It is more meant as a sort of cognitive device to help you think about what the actual challenges are to help you kind of kind of reframe AI because they think they have been pretty deep and longstanding conceptual misunderstandings. So that is really being, that's being holding the feedback. So it's very much meant as a cognitive device. If you take a step back and you ask, why are we even trying to define intelligence and measure intelligence in the first place, why is it useful at all? I think it's useful to the extent that it is actionable, right, a good definition and a good measure should be actionable. So meaning it should help you think, it should help you find solutions and it should help you make progress. In particular, a good definition is a definition that will highlight the key challenges and help you think about it. And I think that's what the paper does. And a good measure is a measure that gives you an actionable feedback signal towards building the right kind of system, right in the sense that it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying to achieve. And the way it's trying to control for priors and experience is by assuming a fixed set of priors. And you're going to see, you know, every test taker can have such priors. This is the core knowledge priors. And then it controls for experience by only giving you a very small number of input examples. And also by making sure the tasks are sufficiently novel and surprising that you're unlikely to have seen a very similar instance before. So now, of course, it's super flawed. So this is not 100% true, of course, but this is kind of like the the planning ideal that we're trying to get to. So that for the record, that's a fascinating point to me is that you view this more as a cognitive device to help guide us to produce better, better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence and all we need to do is solve ARC. This is not at all the point. It's like it's one. Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would mean I was intelligent. But another interesting point, because Keith and I were looking at the paper again yesterday, because it's been, I haven't properly studied it since last year. But we were starting to talk about an alien that comes in from outer space. And, you know, we don't know the priors and the experience. And then I was thinking in a way, it might be a kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat someone with a higher elo than me, then only really tells me that I'm better, you know, as good as that person that I just beat. And similarly, this measure of intelligence, it only gives you a reading in the situation when you know what the conversion was. So if they are not converting anything, then you don't know. And another interesting byproduct of this is the more experienced you get, the less intelligent you get. So I would push back against that last claim that the measure of intelligence as I define it is dependent on how much experience you have. Because the amount of initial experience you have does not actually change at the conversion ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks, then yes, it does affect it. But if you're able to renew your set of tasks and come up with styles that are orthogonal to the experience that you have, then it's not the actual effect, the definition. So, but yeah, you're definitely right that if you take a pure black box approach, and all you're looking at, the only thing you can really measure is the behavior of a system. And unless you know how that behavior is achieved, you can't really tell immediately how much intelligence was involved in producing this behavior. If you look at an insect, they're capable of super complex behavior. Are they crazy intelligent? Well, actually, you know, probably not. And the way you can really tell is by putting these systems out of their comfort zone, getting them to face novel situations and see how they adapt. And that's the measure of intelligence. It's adaptability, the ability to deal with novel and unknown situations. But in order to give your system a novel and unknown situation, you need to have this white box understanding of what it already knows about. And that's not really something you can work on. So can I ask about the generalization difficulty? Because I sort of had some difficulty intuitively with some of, let's say, it's limiting cases. So for example, you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic complexity will be greatest when that's just a random mapping, like I just assigned zero and one randomly to every single integer. And if I go to look at that generalization difficulty, it's going to be super high, because the length of the program for any set is basically going to be, you'd have to encode the entire set as a hash table, right? So how does like this measure account for or help us avoid problems where we're confusing generalization difficulty with just increasing random, you know, randomness? Well, I mean, increasing randomness is a part of the realization difficulty, right? Generalization is really the ability to deal with the stuff you don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is a part of it. But you're right that if you just add randomness to a system, you're increasing the generalization difficulty, but you're not increasing it in a very interesting way, right? Because you're increasing it in a way that's kind of orthogonal to an integration system's ability to deal with it, right? The best you can do is modify the system to be more robust to very much randomness. But that's not super interesting. What's really interesting is to test the system's sensitivity to subtle analogies, is to make the system face novel and unexpected situations that are actually derived from the past, but in interesting ways, right? Not just random ways. You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable, like playing them against each other and so on. And also, we know that something like markets can be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks and other agents solving ARC tasks, and they're going some kind of money around and so on. And this could be kind of a powerful engine for research teams to research anything like this. And, you know, given that you have, I don't know how much, but you do have the backing of Google with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google around this kind of project. But, yeah, so it would be super interesting to have this kind of two-part system where one part is generating the task and one part is learning to solve them. And you could get them to do some kind of curriculum optimization, like the task generator network would not just be trying to generate tasks that look like ARC tasks. It would be trying to generate tasks that correspond to level of generalization, difficulty and complexity that is right below the limits of the student system that's trying to solve them. Kind of like, you know, the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be. They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because that's how you get the most growth. So it's actually a system that's described at the very end of the paper on the measure of contagions. And I think one thing I point out in the paper is kind of like the pitfall you should avoid falling into is that this system is circular, right? And the complexity you're going to see in your task, it needs to come from somewhere, right? It's like conservation of complexity. So the system, this two-part system needs to have a source of intrinsic complexity. It needs to be grounded in the real world. And one way we can achieve that grounding, and I've been thinking about it, is I think we should, you know, like ARC tasks, as they are today, they're made by me and this is not a good setup because it's going to be biased. It's going to be very bottlenecked as well. I think we should start crowdsourcing our task. There should definitely be, you know, a filtering system so that we make sure that we're only keeping our tasks that are interesting, that are not too easy, that are not difficult, and that are only grounded in core knowledge priors. But if we have, like, this stream of novel ARC tasks that contain intrinsic complexity and novel information, because they come from the real world, they come from human brains, that have experienced the real world, and you use that as a way to ground your task generator, then you're starting to get a very interesting three-part system, right? So I would love to actually get that started, to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks that will be crowdsourced, and maybe something that will take the form of a continuous challenge where you have an API where you can draw a new ARC task, and every time you draw a task, it's actually a different one because you have so many of them. Gamify it, that'll make a fun game on a mobile app. There are actually a few people who have created, because ARC is open source, and they're totally free licensed, there are a few people who have created mobile apps where users sort of ARC tasks, and apparently it's popular. So there's also the other angle you mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right now, which is that, okay, let's start thinking about how to map ARC performance to psychometric, you know, classic kind of psychometric tests. Are there any efforts that you're aware of underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a workshop at AAAI the other day, and there were two presentations about efforts that teams of people, so there are people who do neuropsychology, and they're using ARC in very interesting ways. So there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of course, you know, other than the measurement of intelligence, you are also famous for a small library you wrote once in a while called Keras. And I wish I wrote it, and then that was that. No, I yeah, it's been very much an ongoing project for the past six years. It was because I remember, you know, the days of TensorFlow one and and Theano, and things like this. And Keras was just, I think, so helpful to a lot of people, because it just simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to so many people. And now with the development of, you know, things like PyTorch and TensorFlow two, it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras. And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going? Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we are still growing very nicely, both inside Google, like one more teams that Google are moving away from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there, and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very different and much, much bigger set of features and things you can do it. So evolving, definitely. And there are so several, so you, I think you asked, you know, about, yeah, like, Keras is getting kind of merged into TensorFlow, does it mean it's like failing away? So definitely not. So merging with TensorFlow was a good idea because it starts enabling a spectrum of workflows from the very high level, like scikit-learn like, to the very low level, numpy like, and everything in between. In the early days, because Keras had to interact with multiple backends via backend interface, it means you had this kind of like a barrier where as long as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy, very proactive, very fast. But if you wanted more customization, at some point, you would hit that backend barrier. And you had to reverse to TensorFlow base or piano base workflow, that was low level, but when, where you couldn't really leverage Keras effectively, by removing the backend thing and just saying the flow together in one spectrum, then you get really this progressive disclosure of complexity when you can start out with the very high level thing, but then you need to customize your training step. You have an API for that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the level of abstraction that you want. Very, very easy high level or very, very low level full flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting type one with type two reason. Yeah, why not? I was just about to do that. At least Francois has great form for this, because not only does he talk about having powerful and useful interfaces and abstractions in deep learning, he's been playing this game in the library world for quite some time. But I wanted to touch on this quickly. We had a couple of people in our community asking you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end? We've also had a couple of questions about PyTorch as well. Is there anything on the roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the developers have really done a very, very interesting and very good job with it. And lots of people, I like Jax actually. So that said, adoption is not super high. I think Google is probably the company where it's the most adopted, where you will find the most users. And even then, it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project, it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away from this like interface back end kind of model. So precisely for the reason I was describing, because you want to achieve this spectrum of workflows, with that, I think this cliff where you go, you fall from the high level down to the low level. We don't want the cliffs. We don't because cliffs create silos of users where you have the high level users. You want a gradient. Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening and that will still follow the Keras API spec. It would still be the same thing, but on top of Jax. That said, so I would love to see something like this. This is also a very low priority for us because we have the actual current Keras, which I wish we need to work on, which has lots of users. So we don't really have time to do this. But in theory, would it be cool? Yeah, sure. I would love to see something like this. So if I had tons of free time, I would probably build it, but in practice, I don't. Fantastic. Well, we've got another question from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book. Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me. And he talks a lot about objectives leading to deception. So sometimes following an objective monotonically sends you in the wrong direction. And his solution to that is either quality, diversity, or more recently, open-endedness, which is that if you have an infinitude of objectives, in a sense, the system has no objective. And you can also with diversity, preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes to your description of the problem's objectives, I completely agree that one thing I mentioned in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one objective, you're going to achieve it. But the thing is, you're going to take every shortcut along the way for things that we are not actually incorporated in your objective. And this leads to systems that are not actually doing what you wanted them to do. Like for instance, we built chess playing systems, because we hoped that a system that could play chess would have to be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays chess. That's what it does. The same is true with challenges and Kaggle. The winning systems, they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the expense of everything else that you might care about the system. Like, is the code base readable? No. Is it computationally efficient? No, it's actually terrible. You could never put it in production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth says as well. I love what you said about shortcuts. You said in your New York's presentation that if you optimize for a specific metric, then you'll take shortcuts on every other dimension, not captured by your metric. And you said in a machine learning context, it's similar to overfitting, right? Because on task specific skills, you actually lose generalization if you get good at a particular task. So it's completely orthogonal to what you want. I know you're very well known for your skepticism of the intelligence explosion. And what I love about your conception of intelligence is that you think of it as a system or as a process, you say that intelligence is embodied, right? So you have a brain in a body acting in an environment. And in that context, it makes sense that you would think that there are environmental kind of rate limiting steps to any kind of super intelligence, right? But I spoke to someone the other day who is of the other persuasion, shall we say, and this person was saying, Well, what if you had a super, super smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out that IQ only helps up to about 125. And then it stops helping you. But these people would say, Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is just making better decisions, they would consistently make better decisions and science would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent in human terms, that would in fact accelerate science. But it would not really like accelerate science in a linear fashion and very much not in an exponential fashion. So I guess the main conceptual differences I have with these folks is that they tend to credit everything humans can do to the human brain. And they have this vision of intelligence as you know, a brain in a jar kind of thing. And if you tweak the brain, it gets more intelligent and intelligence is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can do more things, you can solve more problems and so on. And in particular, you can build a better brain. And by the way, there is not really any practical evidence that this is true. But I view intelligence here more as this holistic thing that okay, you have the brain, but actually the brain is in a body which gives it access to a certain set of actions it can do and set up a perception primitives. And this body is an environment which gives it access to a set of experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just, it's not so much a problem solving algorithm, like a problem center descending, as it is a big sport. And you put it in an environment to absorb experiences from that environment. And one thing that's super important to understand if you're on issue, if you really think deeply about intelligence, is that most of our expressed intelligence does not come from here, it is externalized intelligence. So externalized intelligence can be can be many things. If I look up something online, that's externalized intelligence, Google is part of my brain. If I write a Python script to test some idea, that's externalized intelligence, my laptop is part of my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition is crystallized, the crystallized output of someone, someone else's thinking. And the process through which we get access to all these accumulated outputs of people's thinking is civilization, right? And like 99% of the things you think are the behaviors you act, the behaviors you execute, you did not invent them. You did not solve the underlying problem yourself. You're just copying a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands after you went outside. And that's a very smart behavior. But did you invent it? Did you come up with that? No, actually, other people came up with that. You did not also come up with the infrastructure that enables you to do it in the first place. And so, and this is true, you know, for even the most intimate of your thoughts, you're thinking with words that you did not invent, you're thinking with concepts that you did not invent or that you did not derive from your own experience. They really come from other people, from this accumulation of past generations. And if you want to enhance the expressed intelligence of people, then this is actually the system you need to tweak and improve, not the human brain, but civilization, right? In a way, that seems like a contradiction, because you're talking about the externalization of knowledge, not intelligence. So by your own definition, isn't that the opposite of intelligence? That's a great point. So I'm relating expressed intelligence. So I was specifically saying expressed intelligence as opposed to fluid intelligence. And what expressed intelligence means in this context is something very different from what we talk about in the measure of intelligence. It means intelligence behavior. And in particular, I think the ability to solve problems that you encounter as an individual. Typically, when you solve a problem as an individual, you're actually using a solution you found somewhere else. There are not that many problems that as an individual, you solve from scratch in your own lifetime. But here's the thing is that if you're able to actually solve something novel yourself, you have the ability to write about it, you have the ability to communicate it, and then the next generation can benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're reading a novel about, I don't know, a kind of planet of the apes or something, which was a planet that had a life form similar to ours, but with a significantly lower IQ. And a human being shows up there one day, and these things start writing about this, hey, this weird alien just showed up here, and we captured it, we ran some tests on it, and we figured out it's really intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen when 100 of them show up instead of just this initial explorer. And some other of these guys were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what they are is kind of outside of their brain. So I'm not really worried about it. We would be reading that with trepidation, right, because we know that when this more intelligent species with more fluid intelligence, more externalized intelligence, better technology, all this kind of stuff shows up, those guys are going to get wiped out. And it's actually happened like many times throughout human history, not that humans were more fluid intelligence showed up and killed off, you know, other people, but humans that had more externalized intelligence or more, you know, represented intelligence and technology certainly showed up and dominated. Absolutely. You're saying it yourself that when it has happened in history, it was not fundamentally about one people having smarter brains, but one people having higher technology. But that that is not something that is attributable to intelligence itself, right? There's a connection there. If you did have a group of species or whatever, that was much more intelligent, they will have advanced technologically much faster and further in any given amount of time, all else being equal, right? It depends on many factors. And that's kind of my point is that is your brain a factor? Yes, absolutely, it is. But there are other factors like we are just talking about the development of technology. So in that case, the critical factor was not the brain, but the superstructure in particular communication and environmental constraints around it. The direction in which civilization develops is a direct function of the specific challenges it encounters that come from its environment, that comes from its surrounding enemies, and so on. And technological development advances the fastest when you have a civilization that are dealing with very harsh challenges, but that are not quite fortunate to work them out. Because that's what forces them to develop as fast as it can survive. So this is actually a very good example where the critical factor was the superstructure that guided the development civilization was not actually the brain. But of course, yeah, if a one is smaller, then civilization will advance faster. But my point is that there are many factors and that by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately some other factor will be the bottleneck. There are civilizations that have not actually advanced very much at all because they simply did not face any changes. And did they have worse brains? No, actually, they had exactly the same brain. But somehow the outcome was different because something else, then the brain turned out to be the bottleneck like lack of environmental change. I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and when you have role fragmentation and lots of different businesses and lots of different organization or structures, some people might decide to structure themselves based on data domain or based on organization or based on something else. And you can think of it topologically. And I think human society is very similar to this. And I'm not sure whether evolution would lead itself to one particular topology. But the environmental structures and the ways that we organize ourselves can create incredible bottlenecks. And that seems to be where the real interesting stuff goes on rather than the individuals. And I think you would agree with that, Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ is like 15 points higher, but it has a terrible organizational structure and terrible incentives and the promo process is super broken or something. And that company is actually going to perform worse than the more progressive innovation encouraging company that has a very nice organizational structure and where people are actually more mediocre. Maybe they have on average 15 points less in IQ, but they're actually going to do a better job because they have the better superstructure. Yeah, it's fascinating that the problem is in most corporations, you can't actually design the information architecture to be more efficient, because everything is so decentralized and fractionated, you can only do it in pockets. And if you try and fix something in one part of the organization, everyone else will say, well, my requirements are different. I'm not going to wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing to do well. To sum up the whole like intelligence explosion thing, the point is really that it's a system you have to look at holistically to get it holistically. And just by tweaking one factor, which is the intelligence of an individual human brain, then what this means is this factor starts being the bottleneck. But that means some other factor in the system, because there's an infinite factor that will become the bottleneck. And by just focusing on one factor, you're not going to actually lift all the votes. Yeah, and I actually agree with you. However, I do want to say, I think we just don't know. I think both sides of the intelligence, quote unquote, explosion really can't say for certain that it will or will not pose a mortal threat to humanity. I think we have to accept that it's at least a risk factor. And we have to be very careful about, in the future, when we start embodying, if we find general intelligence, we need to be cautious. If we come up with something that looks like general intelligence, there is absolutely some risk potential around it. However, I've never seen anything coming anywhere close to that. In fact, the systems that we have today, they fit your almost no intelligence whatsoever. So I think it's a bit early to start banning them. And even if we get into that conversation, I think Francois would say that intelligence must be specialized, right, because of the no free lunch theorems. If you define intelligence as your ability to solve problems, then yeah, it's going to be specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch theorem is saying is basically, if you want to learn something from data, you have to make assumptions about it. Which is why you know a convent, for instance, is a great fit for image data. It's not really a great fit for natural language processing. And because it makes different assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly certain that whatever the first AGI that gets created, it's going to be highly specialized for killing other people, because it's going to be a military, you know, secret project, probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't have anything coming close to AGI. It's probably going to be actually a system that just displays you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the first AGI is probably just going to like write, not only display, but write the perfect ad for you on the fly. You know, it knows what you ate and you know, I know you're joking with actually think on the, on the more serious, I think that's highly unlikely because of the short code of the story because of the short patrol. I don't think a general intelligence is going to be created by the military is not going to be created by a system that's trying to show you ads because these are specific goals. And so if you try to optimize those specific goals, you're going to end up with a very specialized system in order to build a general intelligence, you need to be optimizing for generality itself. So it's going to come from, if it comes from the applied, either it's going to come from the academic side, where you have researchers who are actually optimizing for generality itself, who said generality as they are going. Or if it's come from the applied side, it's going to come from people who have problems where they have to deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads, it's not going to be the military. I don't know where this is going to be. One of the things that interested me about Kenneth Stanley was that he says the reason we can't monotonically optimize on objectives is because of deception, which means sometimes you need to get a lot worse before you get better. His original conception was quality diversity, which basically means if you optimize for novelty, that's something that you can optimize on monotonically. And also, if you look at evolution, where there is a cacophony of problems and solutions divergently being generated, then as an information accumulator, you can optimize on that monotonically. And your conception of intelligence is generality. And that also appears to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show. Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you. Thanks for having me on the podcast. It's really my pleasure. This was super fun. Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful. We're going to jump straight into the post-show analysis. Okay, well, I'm going to mention you did really well, Tim, that trickle sweat that this was running down your face the whole time. Not very noticeable. So I think you can relax. That was fun. I think it went pretty well. Yeah, it was a dream come true. I was actually I was very pleasantly kind of interested in how he he framed, you know, the measure of intelligence paper like, look, it's not really about the measure per se. It's just that this is this is a cognitive framework, a cognitive tool for thinking about where to go and a guidepost for building more generalizable or more general intelligences say like that, I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like, here's a framework to help us think more in the direction we need to be thinking. Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because you know, he self admits that it's flawed, right? Because he like, he makes the tasks. And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks he makes, you know, in the public set, you would think that not someone will come up with an intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many people investigating it. Or whether it's really actually a hard problem. And if it is a problem, you know, well, it's fascinating too, because if he if he achieves what he wanted, which was getting it more outsourced, right, like getting all the intelligent people all around the world contributing to arc problems and refining them over time, I think actually that community project would help the core knowledge people in that line of research and figuring out, okay, what, what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these prime thoughts, because we would, we would play these brain teasers all the time. And we realized that there were patterns, right? Like, well, this brain teaser requires the concept of coloring, like with a red black tree, where you add an additional variable that kind of lets you solve the problem. And if we could really have a nice catalog of, here's all the core knowledge, here's all the like problem solving techniques, I think that would be really powerful. I mean, well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about six core knowledge systems, right? And that and the arc challenge uses four of them. So objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three, numbers, counting, quantitative comparisons. So the two that weren't in there are places and social partners. Now, the thing is, I think we may discover new ones. Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it, imagine if you just guessed the classification on ImageNet when you've got 1000 classes, 20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc, right? And what's interesting as well is that all of those different tasks that have been created by Francois, they all tie back to just four priors, right? Which means, I don't know whether it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM. Yeah, there's, there's two things. So first, I would have thought that if someone, if someone came up with something that solves more than 5%, it's going to be like immediately at 95%. Like just because they've sort of cracked the problem. And then, you know, there might be a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising. And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I do think is that the space of these priors is still way too large. Like, so if you just think about something like object, because in, in these arc tasks, there are, I feel so many more priors than just the core knowledge things. Because so one of them is like, you have the, you have like this thing, and then you have this thing. And the solution is like, it goes, right? It could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize like this is a wall or something, but there is no, there's no, no prior to says like a wall needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact that this is much more core knowledge, right? Like in, you know, we build stuff out of straight walls. And I think, I think I agree with you, which is I think, I think what you're getting at, correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified right now is vague, right? There's a vagueness to it. And I think if we actually start to try and codify that more and some type of a mathematical language, Tim, I think it's going to expand like the scope of that, we're going to end up with more core knowledge concepts really than, than just six, we'll need to make them finer grained. And I'm really excited, you know, to see that develop because this has been for me a long wonder, right, which is what are the in, in a rigorously defined way? What are these core concepts, these core bits of knowledge that make human cognition so powerful? Yeah. And there's also, because Yannick made the point about brittleness, right, even in topological space, you still have brittleness, but, but the solution was to create powerful abstractions, right? But how would that work with the priors? Because if you think about it, you can recombine many of the priors to come up with powerful abstractions. And you might find that it doesn't actually filter down to, to that many. But the question is, how many things are there? Remember when we spoke to Walid Saber, and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained concepts are more important. I think probably stems from a lot of the computer science education that I had where, where when we were devising algorithms to do one thing or another, you get these little hints that kind of like clever bits of core knowledge that was used to solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going to randomly choose an element. Well, random selection is kind of a bit of core knowledge. And then I'm just going to partition by that, and then repeat, you know, or things like, I don't know how to balance this tree the way it is. But if I color stuff, like add in red, black nodes, I can now overlay a computation that, you know, so there's all these little bits, you know, that's what's fascinating about computer programming is it, is it really strikes at the heart of this cognition and this core knowledge and how to read, and you have to do it rigorously, right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a bit amazed by people who have just kind of sort of learned programming, because it's, it's almost like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not fully. Yeah. But then on the other side of the coin to that. So when I was working in, you know, quantitative trading, right, we had these these massive globally integrated, automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks, but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually work in reality. You know, you sit there and you look at them and go, when I first went in there, as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to figure out some continuous equation that, you know, fits this piecewise linear thing, and it's going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better. It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're like, Oh, that's, well, there should be some continuous like thing in there. Like all these weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete work. And, and that's weird. It was weird to me and still weird to me. Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay. So one, intelligence is generalization. I think that's super powerful to his idea that deep learning is really good for value centric abstraction. And because of the manifold hypothesis, lots of natural data has some kind of manifold, which you can interpolate on, but lots of discrete problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But he's saying that it doesn't exist for discrete problems. The manifold might be there or it might only be there in parts. So that was interesting. And then the third thing that fascinated me about Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be thinking about individual brains, we should be thinking about the externalization of knowledge. Yeah. And the way he described this, what he thinks like a hybrid system should look like, which is sort of you have a perception layer and then a discrete search layer. And then on top of that kind of another fuzzy layer that guides the search that can be deep learning again. And I think we're like halfway there on the top with the top very much looks like alpha zero, right, which is kind of a discrete search that is guided by a neural networks. And the bottom layer we have to because that's just our, you know, regular neural networks. I think we have big trouble in how to connect the two in a in a single unified way such that we can learn them, right? Because the best we can do right now is is right, we can, we can plug a pre train network onto alpha zero or something like this, but we can't really, we don't really have it figured out yet how to connect the all the stuff. A good example of that is the neural Turing machines, like how it's so hard to to optimize them, right? And I think not only do we need these kind of three components that that nicely integrate and are optimal, we have to be able to modularize and componentize and connect multiple instances of those things together. And some, you know, weird topological network to really achieve like kind of the capsule network kind of vision where each of the capsules is maybe one of these units. And then they're part of it's like a fractal, you know, kind of these fractal layers of those pieces. I don't know whether I was misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning models and the way you search is quite opinionated. What you're always talking about is have a very basic DSL and in that topological space, you just search and you start to modularize and you start to create functions and abstractions. And you have from a software engineering point of view, you start to build a library of functions that have been written in code that do certain things, right? And that's that's different, isn't it to alpha zero? Well, the alpha zero is made specifically to search over actions in some kind of RL space. Yeah, I mean, what he describes is certainly much more abstract in that you search over applications of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by these lower level neural networks. But I mean, in S, I just, that just came to my mind when he described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because that's essentially neural network guided search is something we, we already, already do though. Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because what you do as a, as a human, there's also some part of hierarchical system to it, in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like, okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub problem. And there is also, you can do it at will too, by the way, like you can, you can scan an image, and you get this type one that sort of finds a bunch of objects, and then you do this type two thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark is objects and bugs and reason about so you have this ability to transcend the process and tune it and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's even like, apart from intelligence, you have the ability to, to introspect the whole thing. And that probably is a big part of intelligence. I mean, I guess you could have intelligence without consciousness, but you know, there is an argument to be made that the fact that you can introspect your own processes contributes in big part to the furthering of intelligence. Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on his newest presentation was when he said intelligence is literally sensitivity to abstract analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that there is so much repetition in the universe. Right, but it's repetition in this funny way where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies, kind of resembles, you know, but, but then there are these little weird differences, these asymmetries, and you know, like the universe is a fascinating place. And I don't know, something, yeah. Right, that's not what when you say you have to make analogies, which is I can, I can absolutely see, you know, this and me, I think my question was formulated a bit dumb where I said, you know, if the line is squiggly, what I more meant is that, you know, in that case, it's not a line, it's a squiggly line. And the same with the social situations, you know, that is like, okay, that that person over there kind of doesn't like me. But then in the next social situation, it's kind of a person that doesn't like you and has a gun, or something like this. I almost feel like or a group of people that you don't consider as a single single sure they are similar in some way, but it's never the exact same thing. So this reasoning by analogy does work, but you always do your little modifications on top specific to the situation. And I'm sure there there's a place in his framework for this, but it's it's just, again, it's it's like a lot more complex than yeah. I think that's what he call I think that's abstraction, at least that, you know, that with prior to today, my my concept of abstraction was similar to that, which it's removing the insignificant details. So you're able you're you're able to take whatever, you know, some, you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't matter for whatever your purpose is, that's abstraction. And, you know, I think one of the weird things is that, and this is kind of the unreasonable effectiveness of mathematics, right, is that abstracting actually produces things that are useful, you know, that abstraction, I think the fact that abstraction helps with generalization is a very not well understood kind of mystery in a sense, like, why should abstraction help generalize, but it does, like in the real world, that's what happens. Though the yet abstraction in though abstraction has to be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only if, you know, you're looking for food or non food, but when it comes to this fear, if you want to shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in common with this orange, right, so that even the way how you abstract, it's not like, it's not like we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's our abstraction, but the how you abstract is also incredibly specific to what you want to do. Yeah, and that's what, and I agree with Saba that this is an empirical question, right, you know, like he's kind of like these concepts or whatever, it's an empirical question, and Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give us lots of data to start thinking about this empirically, and it's going to be really fascinating. I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can, you can probably, like you can probably even zero, you can zero knowledge prove that you can solve a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't even have to show your solution, and if they're, you know, people would put up arc problems, and they, you know, if you want to try them, you will have to put up some money, and if you can solve it, you know, the creator of the challenge gives you some money or something like this, like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic, like arc, right, or like you don't even, you somehow, like you're saying, you can just prove you can solve the problem without ever you having seen the problem, but just an encryption of it. Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence. And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks? So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin, can't we just say people should have to pay through Bitcoin, if they, if somebody wins the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer aware generalization, which means the developer could not have conceived of the task. But if all of the tasks are representing four human priors, then how is that developer aware of generalization? Because the developer would be aware of all of those priors. Of the priors, right? But not, not of the task, right? That's the control is the control, like that's what he said, you have to know the start of where your, your white box analyzing from. And the start is not clean slate, but the start here is these four priors. So it's, it's kind of the diff between you give the developer those four priors, what can the developer come up with, just from that, right? Yeah, because I think, I think there's a lot of information leakage there. And you implicitly said the same thing, because you said, once you solve it, you know, once you solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's, but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it? I guess. We need to figure out something cooler. Like, no art coin. Okay. I don't care enough to grab it. Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah, thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional. We've recently reached 10k subscribers actually. So yeah, thank you very much. We're still going to continue the show now that we've had Shaleo. Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean, to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye. I really hope you've enjoyed the episode today. Remember to like, comment and subscribe. We love reading your comments and we'll see you back next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 20.64, "text": " Past a certain level of complexity, every system starts looking like a living organism.", "tokens": [50364, 18408, 257, 1629, 1496, 295, 14024, 11, 633, 1185, 3719, 1237, 411, 257, 2647, 24128, 13, 51396], "temperature": 0.0, "avg_logprob": -0.22944273948669433, "compression_ratio": 1.0740740740740742, "no_speech_prob": 0.04261752590537071}, {"id": 1, "seek": 2064, "start": 20.64, "end": 27.28, "text": " In order to build a general intelligence, you need to be optimising for generality itself.", "tokens": [50364, 682, 1668, 281, 1322, 257, 2674, 7599, 11, 291, 643, 281, 312, 5028, 3436, 337, 1337, 1860, 2564, 13, 50696], "temperature": 0.0, "avg_logprob": -0.15666907383845402, "compression_ratio": 1.5, "no_speech_prob": 0.03292352706193924}, {"id": 2, "seek": 2064, "start": 30.240000000000002, "end": 37.92, "text": " We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness", "tokens": [50844, 492, 366, 13221, 538, 307, 32702, 13539, 11, 445, 411, 257, 34699, 7895, 13960, 13, 467, 7829, 257, 12802, 44506, 51228], "temperature": 0.0, "avg_logprob": -0.15666907383845402, "compression_ratio": 1.5, "no_speech_prob": 0.03292352706193924}, {"id": 3, "seek": 2064, "start": 37.92, "end": 45.28, "text": " of patterns from a tiny little bit of information. Generalisation is the ability to mine previous", "tokens": [51228, 295, 8294, 490, 257, 5870, 707, 857, 295, 1589, 13, 6996, 7623, 307, 264, 3485, 281, 3892, 3894, 51596], "temperature": 0.0, "avg_logprob": -0.15666907383845402, "compression_ratio": 1.5, "no_speech_prob": 0.03292352706193924}, {"id": 4, "seek": 4528, "start": 45.28, "end": 52.64, "text": " experience to make sense of future novel situations. Generalisation describes a knowledge", "tokens": [50364, 1752, 281, 652, 2020, 295, 2027, 7613, 6851, 13, 6996, 7623, 15626, 257, 3601, 50732], "temperature": 0.0, "avg_logprob": -0.09985860462846427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.08940892666578293}, {"id": 5, "seek": 4528, "start": 52.64, "end": 60.160000000000004, "text": " differential. It characterises the ratio between known information and the space of possible future", "tokens": [50732, 15756, 13, 467, 2517, 3598, 264, 8509, 1296, 2570, 1589, 293, 264, 1901, 295, 1944, 2027, 51108], "temperature": 0.0, "avg_logprob": -0.09985860462846427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.08940892666578293}, {"id": 6, "seek": 4528, "start": 60.160000000000004, "end": 67.44, "text": " situations. To what extent can we analyse the knowledge that we already have into simulacrums", "tokens": [51108, 6851, 13, 1407, 437, 8396, 393, 321, 37840, 264, 3601, 300, 321, 1217, 362, 666, 1034, 425, 326, 6247, 82, 51472], "temperature": 0.0, "avg_logprob": -0.09985860462846427, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.08940892666578293}, {"id": 7, "seek": 6744, "start": 67.44, "end": 74.88, "text": " that apply widely across experienced space? So intelligence, which is to say generalisation", "tokens": [50364, 300, 3079, 13371, 2108, 6751, 1901, 30, 407, 7599, 11, 597, 307, 281, 584, 2674, 7623, 50736], "temperature": 0.0, "avg_logprob": -0.2056162875631581, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.014039065688848495}, {"id": 8, "seek": 6744, "start": 74.88, "end": 82.88, "text": " power, is literally sensitivity to abstract analysis, and that's in fact all there is to it.", "tokens": [50736, 1347, 11, 307, 3736, 19392, 281, 12649, 5215, 11, 293, 300, 311, 294, 1186, 439, 456, 307, 281, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2056162875631581, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.014039065688848495}, {"id": 9, "seek": 6744, "start": 82.88, "end": 90.08, "text": " In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years.", "tokens": [51136, 682, 965, 311, 855, 321, 366, 6869, 538, 34695, 271, 761, 1833, 302, 13, 286, 362, 668, 1228, 264, 591, 6985, 12806, 337, 867, 924, 13, 51496], "temperature": 0.0, "avg_logprob": -0.2056162875631581, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.014039065688848495}, {"id": 10, "seek": 6744, "start": 90.08, "end": 96.32, "text": " I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy", "tokens": [51496, 286, 611, 1401, 702, 14895, 15205, 365, 15329, 1446, 11, 597, 390, 15883, 11, 293, 286, 6941, 702, 367, 2551, 51808], "temperature": 0.0, "avg_logprob": -0.2056162875631581, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.014039065688848495}, {"id": 11, "seek": 9632, "start": 96.32, "end": 101.52, "text": " Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops", "tokens": [50364, 5794, 3154, 13, 1133, 286, 2732, 337, 8116, 286, 1143, 281, 1190, 3479, 2539, 43112, 293, 19162, 50624], "temperature": 0.0, "avg_logprob": -0.0760000811682807, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.008579292334616184}, {"id": 12, "seek": 9632, "start": 101.52, "end": 108.08, "text": " and hackathons. I used to travel around the world and I always had a copy of Francois's book", "tokens": [50624, 293, 10339, 998, 892, 13, 286, 1143, 281, 3147, 926, 264, 1002, 293, 286, 1009, 632, 257, 5055, 295, 34695, 271, 311, 1446, 50952], "temperature": 0.0, "avg_logprob": -0.0760000811682807, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.008579292334616184}, {"id": 13, "seek": 9632, "start": 108.08, "end": 114.72, "text": " Under My Arm. It never left my side. I used to force everyone to read the first four chapters", "tokens": [50952, 6974, 1222, 11893, 13, 467, 1128, 1411, 452, 1252, 13, 286, 1143, 281, 3464, 1518, 281, 1401, 264, 700, 1451, 20013, 51284], "temperature": 0.0, "avg_logprob": -0.0760000811682807, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.008579292334616184}, {"id": 14, "seek": 9632, "start": 114.72, "end": 119.67999999999999, "text": " of that book and of course the chapter on the limitations of deep learning before we did anything.", "tokens": [51284, 295, 300, 1446, 293, 295, 1164, 264, 7187, 322, 264, 15705, 295, 2452, 2539, 949, 321, 630, 1340, 13, 51532], "temperature": 0.0, "avg_logprob": -0.0760000811682807, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.008579292334616184}, {"id": 15, "seek": 11968, "start": 120.32000000000001, "end": 127.76, "text": " Francois has a clarity of thought, which is unparalleled I think in any other human being", "tokens": [50396, 34695, 271, 575, 257, 16992, 295, 1194, 11, 597, 307, 517, 2181, 336, 31689, 286, 519, 294, 604, 661, 1952, 885, 50768], "temperature": 0.0, "avg_logprob": -0.09982753522468335, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.1141570508480072}, {"id": 16, "seek": 11968, "start": 127.76, "end": 134.32, "text": " on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has", "tokens": [50768, 322, 264, 5054, 13, 467, 311, 534, 1596, 4651, 13, 15061, 754, 527, 1065, 2491, 13, 413, 697, 2976, 11, 567, 5646, 575, 51096], "temperature": 0.0, "avg_logprob": -0.09982753522468335, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.1141570508480072}, {"id": 17, "seek": 11968, "start": 135.12, "end": 140.72, "text": " no trouble at all finding holes in some of our guests' work, had this to say while prepping", "tokens": [51136, 572, 5253, 412, 439, 5006, 8118, 294, 512, 295, 527, 9804, 6, 589, 11, 632, 341, 281, 584, 1339, 659, 3759, 51416], "temperature": 0.0, "avg_logprob": -0.09982753522468335, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.1141570508480072}, {"id": 18, "seek": 11968, "start": 140.72, "end": 146.32, "text": " for the show. I'm working on it. It turned out to be a little bit more difficult than I thought.", "tokens": [51416, 337, 264, 855, 13, 286, 478, 1364, 322, 309, 13, 467, 3574, 484, 281, 312, 257, 707, 857, 544, 2252, 813, 286, 1194, 13, 51696], "temperature": 0.0, "avg_logprob": -0.09982753522468335, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.1141570508480072}, {"id": 19, "seek": 14632, "start": 146.95999999999998, "end": 152.4, "text": " Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me", "tokens": [50396, 761, 49744, 307, 257, 707, 857, 886, 10585, 13, 865, 11, 360, 291, 411, 452, 413, 697, 2976, 11982, 30, 634, 576, 2103, 385, 50668], "temperature": 0.0, "avg_logprob": -0.0984395642146886, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.03205568343400955}, {"id": 20, "seek": 14632, "start": 152.4, "end": 157.12, "text": " doing that. But anyway Chalet is extremely controversial to some people actually, but", "tokens": [50668, 884, 300, 13, 583, 4033, 761, 49744, 307, 4664, 17323, 281, 512, 561, 767, 11, 457, 50904], "temperature": 0.0, "avg_logprob": -0.0984395642146886, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.03205568343400955}, {"id": 21, "seek": 14632, "start": 157.12, "end": 163.35999999999999, "text": " he's not controversial to us. Our discussion today lies at the intersection of machine learning", "tokens": [50904, 415, 311, 406, 17323, 281, 505, 13, 2621, 5017, 965, 9134, 412, 264, 15236, 295, 3479, 2539, 51216], "temperature": 0.0, "avg_logprob": -0.0984395642146886, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.03205568343400955}, {"id": 22, "seek": 14632, "start": 163.35999999999999, "end": 169.04, "text": " and reasoning. Now Chalet has made his vision completely clear about what he thinks the future", "tokens": [51216, 293, 21577, 13, 823, 761, 49744, 575, 1027, 702, 5201, 2584, 1850, 466, 437, 415, 7309, 264, 2027, 51500], "temperature": 0.0, "avg_logprob": -0.0984395642146886, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.03205568343400955}, {"id": 23, "seek": 14632, "start": 169.04, "end": 174.32, "text": " of machine learning is. Make no mistake, what you should take from today's episode is that the", "tokens": [51500, 295, 3479, 2539, 307, 13, 4387, 572, 6146, 11, 437, 291, 820, 747, 490, 965, 311, 3500, 307, 300, 264, 51764], "temperature": 0.0, "avg_logprob": -0.0984395642146886, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.03205568343400955}, {"id": 24, "seek": 17432, "start": 174.32, "end": 180.64, "text": " future of artificial intelligence is going to be discrete as well as continuous. Actually the two", "tokens": [50364, 2027, 295, 11677, 7599, 307, 516, 281, 312, 27706, 382, 731, 382, 10957, 13, 5135, 264, 732, 50680], "temperature": 0.0, "avg_logprob": -0.06082565028492997, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0004440789925865829}, {"id": 25, "seek": 17432, "start": 180.64, "end": 188.64, "text": " are going to be enmeshed. The future of AI will almost certainly involve a large degree of program", "tokens": [50680, 366, 516, 281, 312, 465, 5814, 27096, 13, 440, 2027, 295, 7318, 486, 1920, 3297, 9494, 257, 2416, 4314, 295, 1461, 51080], "temperature": 0.0, "avg_logprob": -0.06082565028492997, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0004440789925865829}, {"id": 26, "seek": 17432, "start": 188.64, "end": 194.88, "text": " synthesis. Deep learning has its limits. You can use deep learning for continuous problems", "tokens": [51080, 30252, 13, 14895, 2539, 575, 1080, 10406, 13, 509, 393, 764, 2452, 2539, 337, 10957, 2740, 51392], "temperature": 0.0, "avg_logprob": -0.06082565028492997, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0004440789925865829}, {"id": 27, "seek": 17432, "start": 194.88, "end": 200.79999999999998, "text": " where the data is interpolative and has a learnable manifold and where you have a dense", "tokens": [51392, 689, 264, 1412, 307, 44902, 1166, 293, 575, 257, 1466, 712, 47138, 293, 689, 291, 362, 257, 18011, 51688], "temperature": 0.0, "avg_logprob": -0.06082565028492997, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0004440789925865829}, {"id": 28, "seek": 20080, "start": 200.8, "end": 207.20000000000002, "text": " sampling across the entire surface of the manifold between which you need to make predictions.", "tokens": [50364, 21179, 2108, 264, 2302, 3753, 295, 264, 47138, 1296, 597, 291, 643, 281, 652, 21264, 13, 50684], "temperature": 0.0, "avg_logprob": -0.07048617737202705, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.001597368041984737}, {"id": 29, "seek": 20080, "start": 207.76000000000002, "end": 215.20000000000002, "text": " For Chalet, generalization itself is by far the most important feature of intelligence", "tokens": [50712, 1171, 761, 49744, 11, 2674, 2144, 2564, 307, 538, 1400, 264, 881, 1021, 4111, 295, 7599, 51084], "temperature": 0.0, "avg_logprob": -0.07048617737202705, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.001597368041984737}, {"id": 30, "seek": 20080, "start": 215.20000000000002, "end": 222.24, "text": " and of developing strong AI. He describes a spectrum of generalization starting with,", "tokens": [51084, 293, 295, 6416, 2068, 7318, 13, 634, 15626, 257, 11143, 295, 2674, 2144, 2891, 365, 11, 51436], "temperature": 0.0, "avg_logprob": -0.07048617737202705, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.001597368041984737}, {"id": 31, "seek": 20080, "start": 222.24, "end": 229.36, "text": " for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed.", "tokens": [51436, 337, 1365, 11, 257, 24122, 9284, 689, 456, 307, 572, 44805, 281, 6231, 281, 17076, 13, 440, 5633, 307, 6806, 13, 51792], "temperature": 0.0, "avg_logprob": -0.07048617737202705, "compression_ratio": 1.5355648535564854, "no_speech_prob": 0.001597368041984737}, {"id": 32, "seek": 22936, "start": 230.08, "end": 237.20000000000002, "text": " The machine learning we have today confers some adaptation within a known domain of tasks. For", "tokens": [50400, 440, 3479, 2539, 321, 362, 965, 1497, 433, 512, 21549, 1951, 257, 2570, 9274, 295, 9608, 13, 1171, 50756], "temperature": 0.0, "avg_logprob": -0.07721933504430259, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00026116729713976383}, {"id": 33, "seek": 22936, "start": 237.20000000000002, "end": 244.4, "text": " example, being able to recognize dogs or cats within a variety of different poses and lighting", "tokens": [50756, 1365, 11, 885, 1075, 281, 5521, 7197, 420, 11111, 1951, 257, 5673, 295, 819, 26059, 293, 9577, 51116], "temperature": 0.0, "avg_logprob": -0.07721933504430259, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00026116729713976383}, {"id": 34, "seek": 22936, "start": 244.4, "end": 252.4, "text": " conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to", "tokens": [51116, 4487, 13, 708, 311, 406, 668, 13956, 356, 18772, 370, 1400, 307, 4152, 2674, 2144, 11, 21549, 281, 51516], "temperature": 0.0, "avg_logprob": -0.07721933504430259, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00026116729713976383}, {"id": 35, "seek": 22936, "start": 252.4, "end": 258.96000000000004, "text": " unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the", "tokens": [51516, 9841, 46048, 1951, 257, 2570, 457, 4152, 9274, 13, 467, 311, 3297, 2074, 300, 321, 434, 24085, 322, 264, 51844], "temperature": 0.0, "avg_logprob": -0.07721933504430259, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00026116729713976383}, {"id": 36, "seek": 25896, "start": 258.96, "end": 265.28, "text": " door of this now with GPT-3, where the subtask, if you like, is given at test time. Although", "tokens": [50364, 2853, 295, 341, 586, 365, 26039, 51, 12, 18, 11, 689, 264, 7257, 3863, 11, 498, 291, 411, 11, 307, 2212, 412, 1500, 565, 13, 5780, 50680], "temperature": 0.0, "avg_logprob": -0.09316339331158137, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.004369064699858427}, {"id": 37, "seek": 25896, "start": 265.28, "end": 270.96, "text": " Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3", "tokens": [50680, 761, 49744, 576, 652, 264, 6770, 300, 264, 7257, 3863, 1943, 380, 3264, 412, 1500, 565, 11, 1203, 300, 26039, 51, 12, 18, 50964], "temperature": 0.0, "avg_logprob": -0.09316339331158137, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.004369064699858427}, {"id": 38, "seek": 25896, "start": 270.96, "end": 276.32, "text": " knows was learned on the vast amounts of training data that we trained it on, the poet algorithm", "tokens": [50964, 3255, 390, 3264, 322, 264, 8369, 11663, 295, 3097, 1412, 300, 321, 8895, 309, 322, 11, 264, 20874, 9284, 51232], "temperature": 0.0, "avg_logprob": -0.09316339331158137, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.004369064699858427}, {"id": 39, "seek": 25896, "start": 276.32, "end": 281.84, "text": " from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training", "tokens": [51232, 490, 33735, 28329, 1030, 419, 13, 663, 7038, 281, 312, 19616, 12, 47204, 9608, 382, 644, 295, 264, 3097, 51508], "temperature": 0.0, "avg_logprob": -0.09316339331158137, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.004369064699858427}, {"id": 40, "seek": 25896, "start": 281.84, "end": 286.4, "text": " process, which is very, very interesting. It's creating new problems and new solutions as part", "tokens": [51508, 1399, 11, 597, 307, 588, 11, 588, 1880, 13, 467, 311, 4084, 777, 2740, 293, 777, 6547, 382, 644, 51736], "temperature": 0.0, "avg_logprob": -0.09316339331158137, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.004369064699858427}, {"id": 41, "seek": 28640, "start": 286.4, "end": 291.2, "text": " of the training process. But broadly speaking in the machine learning space at the moment,", "tokens": [50364, 295, 264, 3097, 1399, 13, 583, 19511, 4124, 294, 264, 3479, 2539, 1901, 412, 264, 1623, 11, 50604], "temperature": 0.0, "avg_logprob": -0.05175936698913574, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.0099986232817173}, {"id": 42, "seek": 28640, "start": 291.2, "end": 297.12, "text": " the task that we are doing is fixed and not generalizable. The other thing is that the real", "tokens": [50604, 264, 5633, 300, 321, 366, 884, 307, 6806, 293, 406, 2674, 22395, 13, 440, 661, 551, 307, 300, 264, 957, 50900], "temperature": 0.0, "avg_logprob": -0.05175936698913574, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.0099986232817173}, {"id": 43, "seek": 28640, "start": 297.12, "end": 303.67999999999995, "text": " world does not have a static distribution. We need systems that can adapt dynamically.", "tokens": [50900, 1002, 775, 406, 362, 257, 13437, 7316, 13, 492, 643, 3652, 300, 393, 6231, 43492, 13, 51228], "temperature": 0.0, "avg_logprob": -0.05175936698913574, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.0099986232817173}, {"id": 44, "seek": 28640, "start": 303.67999999999995, "end": 309.35999999999996, "text": " Intelligence requires that you adapt to novelty without the help of the engineer who helped you", "tokens": [51228, 27274, 7029, 300, 291, 6231, 281, 44805, 1553, 264, 854, 295, 264, 11403, 567, 4254, 291, 51512], "temperature": 0.0, "avg_logprob": -0.05175936698913574, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.0099986232817173}, {"id": 45, "seek": 28640, "start": 309.35999999999996, "end": 314.88, "text": " write the system. Chalet has come up with a formalism of intelligence that balances the task", "tokens": [51512, 2464, 264, 1185, 13, 761, 49744, 575, 808, 493, 365, 257, 9860, 1434, 295, 7599, 300, 33993, 264, 5633, 51788], "temperature": 0.0, "avg_logprob": -0.05175936698913574, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.0099986232817173}, {"id": 46, "seek": 31488, "start": 314.88, "end": 321.84, "text": " skill, the difficulty, the knowledge, and experience to effectively quantify and normalise", "tokens": [50364, 5389, 11, 264, 10360, 11, 264, 3601, 11, 293, 1752, 281, 8659, 40421, 293, 2710, 908, 50712], "temperature": 0.0, "avg_logprob": -0.11538334590632741, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.006253203377127647}, {"id": 47, "seek": 31488, "start": 321.84, "end": 328.56, "text": " an algorithmic information conversion ratio. It's the ability to convert experience into future skill", "tokens": [50712, 364, 9284, 299, 1589, 14298, 8509, 13, 467, 311, 264, 3485, 281, 7620, 1752, 666, 2027, 5389, 51048], "temperature": 0.0, "avg_logprob": -0.11538334590632741, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.006253203377127647}, {"id": 48, "seek": 31488, "start": 328.56, "end": 333.04, "text": " that is Chalet's measure of intelligence. At the end of his measure of intelligence paper,", "tokens": [51048, 300, 307, 761, 49744, 311, 3481, 295, 7599, 13, 1711, 264, 917, 295, 702, 3481, 295, 7599, 3035, 11, 51272], "temperature": 0.0, "avg_logprob": -0.11538334590632741, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.006253203377127647}, {"id": 49, "seek": 31488, "start": 333.04, "end": 338.56, "text": " Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a", "tokens": [51272, 34695, 271, 7268, 264, 8943, 34, 3430, 13, 467, 3062, 257, 48751, 22631, 6211, 382, 731, 293, 309, 7268, 257, 51548], "temperature": 0.0, "avg_logprob": -0.11538334590632741, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.006253203377127647}, {"id": 50, "seek": 33856, "start": 338.64, "end": 344.64, "text": " massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware", "tokens": [50368, 5994, 8811, 295, 9608, 13, 440, 1778, 321, 362, 257, 8811, 295, 9608, 307, 337, 10754, 12, 17074, 50668], "temperature": 0.0, "avg_logprob": -0.08657167542655513, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.05179082974791527}, {"id": 51, "seek": 33856, "start": 344.64, "end": 350.24, "text": " generalisation. Any model that we have needs to generalise to tasks that the developer was", "tokens": [50668, 2674, 7623, 13, 2639, 2316, 300, 321, 362, 2203, 281, 2674, 908, 281, 9608, 300, 264, 10754, 390, 50948], "temperature": 0.0, "avg_logprob": -0.08657167542655513, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.05179082974791527}, {"id": 52, "seek": 33856, "start": 350.24, "end": 356.24, "text": " unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or", "tokens": [50948, 32065, 295, 13, 400, 761, 49744, 7309, 300, 7599, 307, 2121, 2640, 13, 467, 2203, 281, 312, 1952, 12, 45300, 420, 51248], "temperature": 0.0, "avg_logprob": -0.08657167542655513, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.05179082974791527}, {"id": 53, "seek": 33856, "start": 356.24, "end": 362.48, "text": " anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to", "tokens": [51248, 22727, 905, 32939, 13, 407, 264, 733, 295, 1790, 830, 300, 291, 643, 281, 5039, 613, 7599, 9608, 643, 281, 51560], "temperature": 0.0, "avg_logprob": -0.08657167542655513, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.05179082974791527}, {"id": 54, "seek": 33856, "start": 362.48, "end": 367.36, "text": " represent the kind of priors that us humans have. Now machine learning algorithms are completely", "tokens": [51560, 2906, 264, 733, 295, 1790, 830, 300, 505, 6255, 362, 13, 823, 3479, 2539, 14642, 366, 2584, 51804], "temperature": 0.0, "avg_logprob": -0.08657167542655513, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.05179082974791527}, {"id": 55, "seek": 36736, "start": 367.36, "end": 372.48, "text": " ineffective against the ARC challenge because it's so challenging to generalise from a few", "tokens": [50364, 48836, 1970, 264, 8943, 34, 3430, 570, 309, 311, 370, 7595, 281, 2674, 908, 490, 257, 1326, 50620], "temperature": 0.0, "avg_logprob": -0.05464245443758757, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0007927159313112497}, {"id": 56, "seek": 36736, "start": 372.48, "end": 378.64, "text": " examples. The only solutions that were effective in the ARC challenge were programme synthesis.", "tokens": [50620, 5110, 13, 440, 787, 6547, 300, 645, 4942, 294, 264, 8943, 34, 3430, 645, 14001, 30252, 13, 50928], "temperature": 0.0, "avg_logprob": -0.05464245443758757, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0007927159313112497}, {"id": 57, "seek": 36736, "start": 378.64, "end": 383.28000000000003, "text": " The manifold hypothesis is that natural data forms lower-dimensional manifolds", "tokens": [50928, 440, 47138, 17291, 307, 300, 3303, 1412, 6422, 3126, 12, 18759, 8173, 31518, 51160], "temperature": 0.0, "avg_logprob": -0.05464245443758757, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0007927159313112497}, {"id": 58, "seek": 36736, "start": 383.28000000000003, "end": 389.92, "text": " in its embedding space. There are both theoretical and experimental reasons to believe this is true.", "tokens": [51160, 294, 1080, 12240, 3584, 1901, 13, 821, 366, 1293, 20864, 293, 17069, 4112, 281, 1697, 341, 307, 2074, 13, 51492], "temperature": 0.0, "avg_logprob": -0.05464245443758757, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0007927159313112497}, {"id": 59, "seek": 36736, "start": 389.92, "end": 394.64, "text": " If you believe this, then the task of a classification algorithm is fundamentally", "tokens": [51492, 759, 291, 1697, 341, 11, 550, 264, 5633, 295, 257, 21538, 9284, 307, 17879, 51728], "temperature": 0.0, "avg_logprob": -0.05464245443758757, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.0007927159313112497}, {"id": 60, "seek": 39464, "start": 394.64, "end": 400.47999999999996, "text": " to separate a bunch of tangled manifolds. The only way deep learning models can generalise", "tokens": [50364, 281, 4994, 257, 3840, 295, 47192, 8173, 31518, 13, 440, 787, 636, 2452, 2539, 5245, 393, 2674, 908, 50656], "temperature": 0.0, "avg_logprob": -0.05241193490869859, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003796877572312951}, {"id": 61, "seek": 39464, "start": 400.47999999999996, "end": 406.96, "text": " is via interpolation. Most perception problems in particular, according to Francois, are", "tokens": [50656, 307, 5766, 44902, 399, 13, 4534, 12860, 2740, 294, 1729, 11, 4650, 281, 34695, 271, 11, 366, 50980], "temperature": 0.0, "avg_logprob": -0.05241193490869859, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003796877572312951}, {"id": 62, "seek": 39464, "start": 406.96, "end": 413.12, "text": " interpolative. Neural networks not only have to represent the manifold of the data that they're", "tokens": [50980, 44902, 1166, 13, 1734, 1807, 9590, 406, 787, 362, 281, 2906, 264, 47138, 295, 264, 1412, 300, 436, 434, 51288], "temperature": 0.0, "avg_logprob": -0.05241193490869859, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003796877572312951}, {"id": 63, "seek": 39464, "start": 413.12, "end": 418.64, "text": " learning, the manifold also needs to be learnable. And that's an even tougher constraint.", "tokens": [51288, 2539, 11, 264, 47138, 611, 2203, 281, 312, 1466, 712, 13, 400, 300, 311, 364, 754, 30298, 25534, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05241193490869859, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003796877572312951}, {"id": 64, "seek": 39464, "start": 418.64, "end": 423.76, "text": " Gradient descent will not learn data that has challenging discontinuities in its manifold.", "tokens": [51564, 16710, 1196, 23475, 486, 406, 1466, 1412, 300, 575, 7595, 31420, 84, 1088, 294, 1080, 47138, 13, 51820], "temperature": 0.0, "avg_logprob": -0.05241193490869859, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003796877572312951}, {"id": 65, "seek": 42376, "start": 423.76, "end": 430.08, "text": " It'll just resort to memorising the data. Deep learning allows you to represent complex programmes", "tokens": [50364, 467, 603, 445, 19606, 281, 10560, 3436, 264, 1412, 13, 14895, 2539, 4045, 291, 281, 2906, 3997, 31097, 50680], "temperature": 0.0, "avg_logprob": -0.061833777517642616, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0005694119026884437}, {"id": 66, "seek": 42376, "start": 430.08, "end": 435.92, "text": " that you couldn't write by hand, but on the other side of the coin it also fails to represent", "tokens": [50680, 300, 291, 2809, 380, 2464, 538, 1011, 11, 457, 322, 264, 661, 1252, 295, 264, 11464, 309, 611, 18199, 281, 2906, 50972], "temperature": 0.0, "avg_logprob": -0.061833777517642616, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0005694119026884437}, {"id": 67, "seek": 42376, "start": 435.92, "end": 442.0, "text": " very simple programmes that you could write by hand. Discrete programmes. So there are some", "tokens": [50972, 588, 2199, 31097, 300, 291, 727, 2464, 538, 1011, 13, 19839, 7600, 31097, 13, 407, 456, 366, 512, 51276], "temperature": 0.0, "avg_logprob": -0.061833777517642616, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0005694119026884437}, {"id": 68, "seek": 42376, "start": 442.0, "end": 446.4, "text": " problems where deep learning is a great fit and there are other problems where deep learning", "tokens": [51276, 2740, 689, 2452, 2539, 307, 257, 869, 3318, 293, 456, 366, 661, 2740, 689, 2452, 2539, 51496], "temperature": 0.0, "avg_logprob": -0.061833777517642616, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0005694119026884437}, {"id": 69, "seek": 42376, "start": 446.4, "end": 451.76, "text": " is a disaster. And the reason for that is that they are not interpolative in nature. These tend", "tokens": [51496, 307, 257, 11293, 13, 400, 264, 1778, 337, 300, 307, 300, 436, 366, 406, 44902, 1166, 294, 3687, 13, 1981, 3928, 51764], "temperature": 0.0, "avg_logprob": -0.061833777517642616, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0005694119026884437}, {"id": 70, "seek": 45176, "start": 451.76, "end": 457.68, "text": " to be algorithmic reasoning problems. Francois thinks that 99% of software written today,", "tokens": [50364, 281, 312, 9284, 299, 21577, 2740, 13, 34695, 271, 7309, 300, 11803, 4, 295, 4722, 3720, 965, 11, 50660], "temperature": 0.0, "avg_logprob": -0.07878322784717266, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.009800560772418976}, {"id": 71, "seek": 45176, "start": 457.68, "end": 463.12, "text": " with code, is not interpolative in nature and therefore it's a bad fit for deep learning.", "tokens": [50660, 365, 3089, 11, 307, 406, 44902, 1166, 294, 3687, 293, 4412, 309, 311, 257, 1578, 3318, 337, 2452, 2539, 13, 50932], "temperature": 0.0, "avg_logprob": -0.07878322784717266, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.009800560772418976}, {"id": 72, "seek": 45176, "start": 463.12, "end": 468.15999999999997, "text": " The only answer to these problems is discrete programme search. To use deep learning for these", "tokens": [50932, 440, 787, 1867, 281, 613, 2740, 307, 27706, 14001, 3164, 13, 1407, 764, 2452, 2539, 337, 613, 51184], "temperature": 0.0, "avg_logprob": -0.07878322784717266, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.009800560772418976}, {"id": 73, "seek": 45176, "start": 468.15999999999997, "end": 473.84, "text": " problems requires a lot of data. It's hard to train and the representation will be glitchy.", "tokens": [51184, 2740, 7029, 257, 688, 295, 1412, 13, 467, 311, 1152, 281, 3847, 293, 264, 10290, 486, 312, 23552, 88, 13, 51468], "temperature": 0.0, "avg_logprob": -0.07878322784717266, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.009800560772418976}, {"id": 74, "seek": 45176, "start": 473.84, "end": 479.84, "text": " It'll be brittle. Neural networks cannot even extrapolate the scalar identity function,", "tokens": [51468, 467, 603, 312, 49325, 13, 1734, 1807, 9590, 2644, 754, 48224, 473, 264, 39684, 6575, 2445, 11, 51768], "temperature": 0.0, "avg_logprob": -0.07878322784717266, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.009800560772418976}, {"id": 75, "seek": 47984, "start": 479.91999999999996, "end": 487.2, "text": " f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent", "tokens": [50368, 283, 295, 2031, 6915, 2031, 13, 814, 393, 787, 44902, 473, 2212, 264, 9123, 295, 257, 5508, 47138, 294, 264, 48994, 50732], "temperature": 0.0, "avg_logprob": -0.1163248104995556, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0017244629561901093}, {"id": 76, "seek": 47984, "start": 487.2, "end": 494.0, "text": " space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.", "tokens": [50732, 1901, 13, 4956, 40113, 2613, 3938, 848, 281, 28327, 78, 300, 439, 1090, 18795, 3479, 2539, 307, 48224, 399, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1163248104995556, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0017244629561901093}, {"id": 77, "seek": 47984, "start": 494.56, "end": 499.44, "text": " So is this similar to interpolation? Well, I mean, all of machine learning is similar to", "tokens": [51100, 407, 307, 341, 2531, 281, 44902, 399, 30, 1042, 11, 286, 914, 11, 439, 295, 3479, 2539, 307, 2531, 281, 51344], "temperature": 0.0, "avg_logprob": -0.1163248104995556, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0017244629561901093}, {"id": 78, "seek": 47984, "start": 499.44, "end": 504.88, "text": " interpolation if you want, right? When you train a linear regression on scalar values,", "tokens": [51344, 44902, 399, 498, 291, 528, 11, 558, 30, 1133, 291, 3847, 257, 8213, 24590, 322, 39684, 4190, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1163248104995556, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0017244629561901093}, {"id": 79, "seek": 50488, "start": 504.88, "end": 510.0, "text": " you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the", "tokens": [50364, 291, 434, 3097, 257, 2316, 11, 558, 30, 509, 434, 2902, 257, 3840, 295, 15494, 2031, 293, 288, 13, 509, 434, 3365, 437, 366, 264, 50620], "temperature": 0.0, "avg_logprob": -0.11263640852999096, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.009829332120716572}, {"id": 80, "seek": 50488, "start": 510.0, "end": 516.96, "text": " best values of A and B for y equals A x plus B that minimizes the square error of the prediction", "tokens": [50620, 1151, 4190, 295, 316, 293, 363, 337, 288, 6915, 316, 2031, 1804, 363, 300, 4464, 5660, 264, 3732, 6713, 295, 264, 17630, 50968], "temperature": 0.0, "avg_logprob": -0.11263640852999096, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.009829332120716572}, {"id": 81, "seek": 50488, "start": 516.96, "end": 521.04, "text": " of a line to all of the points, right? That's linear regression. That's interpolation.", "tokens": [50968, 295, 257, 1622, 281, 439, 295, 264, 2793, 11, 558, 30, 663, 311, 8213, 24590, 13, 663, 311, 44902, 399, 13, 51172], "temperature": 0.0, "avg_logprob": -0.11263640852999096, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.009829332120716572}, {"id": 82, "seek": 50488, "start": 521.6, "end": 526.32, "text": " All of machine learning is interpolation. In a high dimensional space, there is essentially no such", "tokens": [51200, 1057, 295, 3479, 2539, 307, 44902, 399, 13, 682, 257, 1090, 18795, 1901, 11, 456, 307, 4476, 572, 1270, 51436], "temperature": 0.0, "avg_logprob": -0.11263640852999096, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.009829332120716572}, {"id": 83, "seek": 50488, "start": 526.32, "end": 532.24, "text": " thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right?", "tokens": [51436, 551, 382, 44902, 399, 13, 5471, 307, 48224, 399, 13, 407, 3811, 291, 366, 294, 257, 1901, 295, 5267, 11, 558, 30, 51732], "temperature": 0.0, "avg_logprob": -0.11263640852999096, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.009829332120716572}, {"id": 84, "seek": 53224, "start": 532.24, "end": 538.24, "text": " So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have", "tokens": [50364, 407, 291, 362, 257, 4965, 5267, 38882, 538, 38882, 13, 407, 309, 311, 2331, 11, 1360, 18795, 4846, 1901, 13, 2754, 498, 291, 362, 50664], "temperature": 0.0, "avg_logprob": -0.09113365091303344, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004896583966910839}, {"id": 85, "seek": 53224, "start": 538.24, "end": 544.5600000000001, "text": " a million samples, you're only covering a tiny portion of the dimensions of that space, right?", "tokens": [50664, 257, 2459, 10938, 11, 291, 434, 787, 10322, 257, 5870, 8044, 295, 264, 12819, 295, 300, 1901, 11, 558, 30, 50980], "temperature": 0.0, "avg_logprob": -0.09113365091303344, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004896583966910839}, {"id": 86, "seek": 53224, "start": 544.5600000000001, "end": 552.48, "text": " Those images are in a tiny sliver of surface among the space of all possible combinations", "tokens": [50980, 3950, 5267, 366, 294, 257, 5870, 1061, 1837, 295, 3753, 3654, 264, 1901, 295, 439, 1944, 21267, 51376], "temperature": 0.0, "avg_logprob": -0.09113365091303344, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004896583966910839}, {"id": 87, "seek": 53224, "start": 552.48, "end": 558.4, "text": " of values of pixels. So when you show the system a new image, it's very unlikely that this image", "tokens": [51376, 295, 4190, 295, 18668, 13, 407, 562, 291, 855, 264, 1185, 257, 777, 3256, 11, 309, 311, 588, 17518, 300, 341, 3256, 51672], "temperature": 0.0, "avg_logprob": -0.09113365091303344, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.004896583966910839}, {"id": 88, "seek": 55840, "start": 558.4, "end": 563.36, "text": " is a linear combination of previous images. What you're doing is extrapolation, not interpolation,", "tokens": [50364, 307, 257, 8213, 6562, 295, 3894, 5267, 13, 708, 291, 434, 884, 307, 48224, 399, 11, 406, 44902, 399, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10410132578441075, "compression_ratio": 1.7, "no_speech_prob": 0.0025106777902692556}, {"id": 89, "seek": 55840, "start": 563.36, "end": 567.84, "text": " okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard.", "tokens": [50612, 1392, 30, 400, 294, 1090, 10139, 11, 439, 295, 3479, 2539, 307, 48224, 399, 11, 597, 307, 983, 309, 311, 1152, 13, 50836], "temperature": 0.0, "avg_logprob": -0.10410132578441075, "compression_ratio": 1.7, "no_speech_prob": 0.0025106777902692556}, {"id": 90, "seek": 55840, "start": 567.84, "end": 573.76, "text": " I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out.", "tokens": [50836, 286, 478, 885, 12653, 5141, 484, 4956, 40113, 4106, 11, 264, 3044, 11541, 295, 2452, 2539, 11, 457, 1568, 385, 484, 13, 51132], "temperature": 0.0, "avg_logprob": -0.10410132578441075, "compression_ratio": 1.7, "no_speech_prob": 0.0025106777902692556}, {"id": 91, "seek": 55840, "start": 573.76, "end": 580.48, "text": " It's certainly true that interpolation on the native data domain is useless, right? We need to", "tokens": [51132, 467, 311, 3297, 2074, 300, 44902, 399, 322, 264, 8470, 1412, 9274, 307, 14115, 11, 558, 30, 492, 643, 281, 51468], "temperature": 0.0, "avg_logprob": -0.10410132578441075, "compression_ratio": 1.7, "no_speech_prob": 0.0025106777902692556}, {"id": 92, "seek": 55840, "start": 580.48, "end": 585.68, "text": " pull some useful information out of the data and the model architecture and training method matter", "tokens": [51468, 2235, 512, 4420, 1589, 484, 295, 264, 1412, 293, 264, 2316, 9482, 293, 3097, 3170, 1871, 51728], "temperature": 0.0, "avg_logprob": -0.10410132578441075, "compression_ratio": 1.7, "no_speech_prob": 0.0025106777902692556}, {"id": 93, "seek": 58568, "start": 585.68, "end": 592.3199999999999, "text": " a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation", "tokens": [50364, 257, 688, 510, 13, 492, 393, 439, 3986, 300, 44902, 399, 322, 264, 3264, 47138, 576, 1643, 411, 48224, 399, 50696], "temperature": 0.0, "avg_logprob": -0.08692465546310589, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.005553689319640398}, {"id": 94, "seek": 58568, "start": 592.3199999999999, "end": 598.0, "text": " in the original space of the data, right? Chalet is quite clear that neural networks only", "tokens": [50696, 294, 264, 3380, 1901, 295, 264, 1412, 11, 558, 30, 761, 49744, 307, 1596, 1850, 300, 18161, 9590, 787, 50980], "temperature": 0.0, "avg_logprob": -0.08692465546310589, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.005553689319640398}, {"id": 95, "seek": 58568, "start": 598.88, "end": 605.04, "text": " generalize through interpolation. You might argue that you can go a tiny step outside of the convex", "tokens": [51024, 2674, 1125, 807, 44902, 399, 13, 509, 1062, 9695, 300, 291, 393, 352, 257, 5870, 1823, 2380, 295, 264, 42432, 51332], "temperature": 0.0, "avg_logprob": -0.08692465546310589, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.005553689319640398}, {"id": 96, "seek": 58568, "start": 605.04, "end": 611.28, "text": " hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue", "tokens": [51332, 32335, 295, 428, 1412, 11, 754, 538, 257, 5870, 707, 857, 11, 293, 291, 393, 12120, 48224, 473, 13, 1042, 11, 286, 576, 9695, 51644], "temperature": 0.0, "avg_logprob": -0.08692465546310589, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.005553689319640398}, {"id": 97, "seek": 61128, "start": 611.28, "end": 616.24, "text": " that if the manifold doesn't give you any useful information outside of the training range, then", "tokens": [50364, 300, 498, 264, 47138, 1177, 380, 976, 291, 604, 4420, 1589, 2380, 295, 264, 3097, 3613, 11, 550, 50612], "temperature": 0.0, "avg_logprob": -0.05126814715630185, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.004754609428346157}, {"id": 98, "seek": 61128, "start": 616.24, "end": 620.88, "text": " it wouldn't be any better than finding your nearest training example and just adding a bit of random", "tokens": [50612, 309, 2759, 380, 312, 604, 1101, 813, 5006, 428, 23831, 3097, 1365, 293, 445, 5127, 257, 857, 295, 4974, 50844], "temperature": 0.0, "avg_logprob": -0.05126814715630185, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.004754609428346157}, {"id": 99, "seek": 61128, "start": 620.88, "end": 628.3199999999999, "text": " noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly,", "tokens": [50844, 5658, 13, 759, 291, 3847, 797, 11, 337, 1365, 11, 291, 393, 44902, 473, 322, 264, 48994, 47138, 11, 457, 25873, 11, 51216], "temperature": 0.0, "avg_logprob": -0.05126814715630185, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.004754609428346157}, {"id": 100, "seek": 61128, "start": 628.3199999999999, "end": 634.88, "text": " you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on", "tokens": [51216, 291, 393, 48224, 473, 13, 583, 264, 1778, 337, 300, 307, 264, 3303, 47138, 300, 264, 1412, 295, 8475, 12696, 322, 51544], "temperature": 0.0, "avg_logprob": -0.05126814715630185, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.004754609428346157}, {"id": 101, "seek": 61128, "start": 634.88, "end": 640.8, "text": " might be shaped like a football or a sphere, which means if you go outside of the training range,", "tokens": [51544, 1062, 312, 13475, 411, 257, 7346, 420, 257, 16687, 11, 597, 1355, 498, 291, 352, 2380, 295, 264, 3097, 3613, 11, 51840], "temperature": 0.0, "avg_logprob": -0.05126814715630185, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.004754609428346157}, {"id": 102, "seek": 64080, "start": 640.8, "end": 646.7199999999999, "text": " you actually have some information about those data points. The scalar identity function might seem", "tokens": [50364, 291, 767, 362, 512, 1589, 466, 729, 1412, 2793, 13, 440, 39684, 6575, 2445, 1062, 1643, 50660], "temperature": 0.0, "avg_logprob": -0.05532356175509366, "compression_ratio": 1.7275985663082438, "no_speech_prob": 6.013773599988781e-05}, {"id": 103, "seek": 64080, "start": 646.7199999999999, "end": 651.28, "text": " like a contrived example, but it's a really interesting one. When you go outside of the", "tokens": [50660, 411, 257, 660, 470, 937, 1365, 11, 457, 309, 311, 257, 534, 1880, 472, 13, 1133, 291, 352, 2380, 295, 264, 50888], "temperature": 0.0, "avg_logprob": -0.05532356175509366, "compression_ratio": 1.7275985663082438, "no_speech_prob": 6.013773599988781e-05}, {"id": 104, "seek": 64080, "start": 651.28, "end": 657.76, "text": " training range, nothing about the manifold is known, right? Think about the manifold. It's just a", "tokens": [50888, 3097, 3613, 11, 1825, 466, 264, 47138, 307, 2570, 11, 558, 30, 6557, 466, 264, 47138, 13, 467, 311, 445, 257, 51212], "temperature": 0.0, "avg_logprob": -0.05532356175509366, "compression_ratio": 1.7275985663082438, "no_speech_prob": 6.013773599988781e-05}, {"id": 105, "seek": 64080, "start": 657.76, "end": 663.92, "text": " string that goes on forever. We don't know anything about that manifold outside of the training range.", "tokens": [51212, 6798, 300, 1709, 322, 5680, 13, 492, 500, 380, 458, 1340, 466, 300, 47138, 2380, 295, 264, 3097, 3613, 13, 51520], "temperature": 0.0, "avg_logprob": -0.05532356175509366, "compression_ratio": 1.7275985663082438, "no_speech_prob": 6.013773599988781e-05}, {"id": 106, "seek": 64080, "start": 663.92, "end": 668.8, "text": " This is not true for most perceptual problems in deep learning. And this is why image models,", "tokens": [51520, 639, 307, 406, 2074, 337, 881, 43276, 901, 2740, 294, 2452, 2539, 13, 400, 341, 307, 983, 3256, 5245, 11, 51764], "temperature": 0.0, "avg_logprob": -0.05532356175509366, "compression_ratio": 1.7275985663082438, "no_speech_prob": 6.013773599988781e-05}, {"id": 107, "seek": 66880, "start": 668.88, "end": 674.0, "text": " for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you", "tokens": [50368, 337, 1365, 11, 9753, 14147, 6316, 2997, 3876, 13, 708, 366, 428, 4598, 466, 341, 30, 1545, 500, 380, 291, 50624], "temperature": 0.0, "avg_logprob": -0.09025943062522195, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.003272168803960085}, {"id": 108, "seek": 66880, "start": 674.0, "end": 679.12, "text": " let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous", "tokens": [50624, 718, 505, 458, 294, 264, 3053, 3541, 322, 3088, 30, 407, 456, 311, 257, 957, 1880, 10390, 310, 8488, 295, 10957, 50880], "temperature": 0.0, "avg_logprob": -0.09025943062522195, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.003272168803960085}, {"id": 109, "seek": 66880, "start": 679.12, "end": 684.0799999999999, "text": " problems versus discrete problems that we're going to be exploring in the show today. It's very", "tokens": [50880, 2740, 5717, 27706, 2740, 300, 321, 434, 516, 281, 312, 12736, 294, 264, 855, 965, 13, 467, 311, 588, 51128], "temperature": 0.0, "avg_logprob": -0.09025943062522195, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.003272168803960085}, {"id": 110, "seek": 66880, "start": 684.0799999999999, "end": 689.1999999999999, "text": " interesting that brittleness works both ways, depending on the discreteness of the problem.", "tokens": [51128, 1880, 300, 738, 593, 45887, 1985, 1293, 2098, 11, 5413, 322, 264, 2983, 35383, 442, 295, 264, 1154, 13, 51384], "temperature": 0.0, "avg_logprob": -0.09025943062522195, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.003272168803960085}, {"id": 111, "seek": 66880, "start": 689.1999999999999, "end": 696.3199999999999, "text": " Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,", "tokens": [51384, 8338, 30252, 576, 312, 4664, 49325, 294, 1508, 5489, 11111, 5717, 7197, 420, 754, 376, 12, 77, 468, 11, 51740], "temperature": 0.0, "avg_logprob": -0.09025943062522195, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.003272168803960085}, {"id": 112, "seek": 69632, "start": 696.32, "end": 702.5600000000001, "text": " and deep learning would be extremely brittle predicting the digits of pi or prime numbers", "tokens": [50364, 293, 2452, 2539, 576, 312, 4664, 49325, 32884, 264, 27011, 295, 3895, 420, 5835, 3547, 50676], "temperature": 0.0, "avg_logprob": -0.05150433381398519, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.00051065010484308}, {"id": 113, "seek": 69632, "start": 702.5600000000001, "end": 708.48, "text": " or sorting a list. So brittleness here means the overall fit of your model or your program,", "tokens": [50676, 420, 32411, 257, 1329, 13, 407, 738, 593, 45887, 510, 1355, 264, 4787, 3318, 295, 428, 2316, 420, 428, 1461, 11, 50972], "temperature": 0.0, "avg_logprob": -0.05150433381398519, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.00051065010484308}, {"id": 114, "seek": 69632, "start": 708.48, "end": 715.2, "text": " so accuracy and robustness. Imagine if every single bug you experienced with computer software", "tokens": [50972, 370, 14170, 293, 13956, 1287, 13, 11739, 498, 633, 2167, 7426, 291, 6751, 365, 3820, 4722, 51308], "temperature": 0.0, "avg_logprob": -0.05150433381398519, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.00051065010484308}, {"id": 115, "seek": 69632, "start": 715.2, "end": 720.72, "text": " was entirely unique to you and the development team wouldn't even be able to reproduce it.", "tokens": [51308, 390, 7696, 3845, 281, 291, 293, 264, 3250, 1469, 2759, 380, 754, 312, 1075, 281, 29501, 309, 13, 51584], "temperature": 0.0, "avg_logprob": -0.05150433381398519, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.00051065010484308}, {"id": 116, "seek": 69632, "start": 720.72, "end": 724.8800000000001, "text": " This is what would happen if software was written entirely with neural networks.", "tokens": [51584, 639, 307, 437, 576, 1051, 498, 4722, 390, 3720, 7696, 365, 18161, 9590, 13, 51792], "temperature": 0.0, "avg_logprob": -0.05150433381398519, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.00051065010484308}, {"id": 117, "seek": 72488, "start": 724.88, "end": 730.56, "text": " It would be more, not less brittle. Sholey thinks that motivated thinking is the primary", "tokens": [50364, 467, 576, 312, 544, 11, 406, 1570, 49325, 13, 31404, 3420, 7309, 300, 14515, 1953, 307, 264, 6194, 50648], "temperature": 0.0, "avg_logprob": -0.069769612065068, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0011673986446112394}, {"id": 118, "seek": 72488, "start": 730.56, "end": 735.84, "text": " obstacle to getting people to wake up to the fact that neural networks are poorly suited", "tokens": [50648, 23112, 281, 1242, 561, 281, 6634, 493, 281, 264, 1186, 300, 18161, 9590, 366, 22271, 24736, 50912], "temperature": 0.0, "avg_logprob": -0.069769612065068, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0011673986446112394}, {"id": 119, "seek": 72488, "start": 735.84, "end": 741.6, "text": " to discrete problems. The people who are good enough at deep learning to realize its limitations", "tokens": [50912, 281, 27706, 2740, 13, 440, 561, 567, 366, 665, 1547, 412, 2452, 2539, 281, 4325, 1080, 15705, 51200], "temperature": 0.0, "avg_logprob": -0.069769612065068, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0011673986446112394}, {"id": 120, "seek": 72488, "start": 741.6, "end": 747.6, "text": " are too invested in its success to say so. Sholey fundamentally thinks that there are two types of", "tokens": [51200, 366, 886, 13104, 294, 1080, 2245, 281, 584, 370, 13, 31404, 3420, 17879, 7309, 300, 456, 366, 732, 3467, 295, 51500], "temperature": 0.0, "avg_logprob": -0.069769612065068, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0011673986446112394}, {"id": 121, "seek": 74760, "start": 747.6, "end": 754.88, "text": " thinking, type one and type two. He thinks that every single thought in our minds is not simply", "tokens": [50364, 1953, 11, 2010, 472, 293, 2010, 732, 13, 634, 7309, 300, 633, 2167, 1194, 294, 527, 9634, 307, 406, 2935, 50728], "temperature": 0.0, "avg_logprob": -0.08149407566457555, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.0395989865064621}, {"id": 122, "seek": 74760, "start": 754.88, "end": 762.24, "text": " one or the other, rather it's a combination of both types. Type one and type two, they are", "tokens": [50728, 472, 420, 264, 661, 11, 2831, 309, 311, 257, 6562, 295, 1293, 3467, 13, 15576, 472, 293, 2010, 732, 11, 436, 366, 51096], "temperature": 0.0, "avg_logprob": -0.08149407566457555, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.0395989865064621}, {"id": 123, "seek": 74760, "start": 762.24, "end": 770.5600000000001, "text": " enmeshed together in everything you think and in everything you do. Even our reasoning is guided", "tokens": [51096, 465, 5814, 27096, 1214, 294, 1203, 291, 519, 293, 294, 1203, 291, 360, 13, 2754, 527, 21577, 307, 19663, 51512], "temperature": 0.0, "avg_logprob": -0.08149407566457555, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.0395989865064621}, {"id": 124, "seek": 77056, "start": 770.56, "end": 778.56, "text": " by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization", "tokens": [50364, 538, 24002, 11, 597, 307, 44902, 1166, 294, 3687, 13, 31404, 3420, 7309, 300, 37765, 307, 2141, 281, 2674, 2144, 50764], "temperature": 0.0, "avg_logprob": -0.07033100472875388, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.10635791718959808}, {"id": 125, "seek": 77056, "start": 778.56, "end": 785.92, "text": " and the way we perform abstraction is different in continuous versus discrete space. We need to", "tokens": [50764, 293, 264, 636, 321, 2042, 37765, 307, 819, 294, 10957, 5717, 27706, 1901, 13, 492, 643, 281, 51132], "temperature": 0.0, "avg_logprob": -0.07033100472875388, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.10635791718959808}, {"id": 126, "seek": 77056, "start": 785.92, "end": 791.68, "text": " find analogies and those analogies will be found differently in both of those different spaces.", "tokens": [51132, 915, 16660, 530, 293, 729, 16660, 530, 486, 312, 1352, 7614, 294, 1293, 295, 729, 819, 7673, 13, 51420], "temperature": 0.0, "avg_logprob": -0.07033100472875388, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.10635791718959808}, {"id": 127, "seek": 77056, "start": 791.68, "end": 797.8399999999999, "text": " Program search allows us to generalize broadly from just a few examples. It marks a significant", "tokens": [51420, 8338, 3164, 4045, 505, 281, 2674, 1125, 19511, 490, 445, 257, 1326, 5110, 13, 467, 10640, 257, 4776, 51728], "temperature": 0.0, "avg_logprob": -0.07033100472875388, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.10635791718959808}, {"id": 128, "seek": 79784, "start": 797.9200000000001, "end": 802.64, "text": " deviation from traditional machine learning. Rather than trying to interpolate between the", "tokens": [50368, 25163, 490, 5164, 3479, 2539, 13, 16571, 813, 1382, 281, 44902, 473, 1296, 264, 50604], "temperature": 0.0, "avg_logprob": -0.049870053927103676, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.002631361596286297}, {"id": 129, "seek": 79784, "start": 802.64, "end": 808.72, "text": " examples you have, you're constructing an entire search space from scratch and testing", "tokens": [50604, 5110, 291, 362, 11, 291, 434, 39969, 364, 2302, 3164, 1901, 490, 8459, 293, 4997, 50908], "temperature": 0.0, "avg_logprob": -0.049870053927103676, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.002631361596286297}, {"id": 130, "seek": 79784, "start": 808.72, "end": 814.64, "text": " if it fits our training data. It all started with the flash fill feature in Microsoft Excel.", "tokens": [50908, 498, 309, 9001, 527, 3097, 1412, 13, 467, 439, 1409, 365, 264, 7319, 2836, 4111, 294, 8116, 19060, 13, 51204], "temperature": 0.0, "avg_logprob": -0.049870053927103676, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.002631361596286297}, {"id": 131, "seek": 79784, "start": 814.64, "end": 819.6, "text": " Do you remember that? You give a few examples of some transformation that you want to perform", "tokens": [51204, 1144, 291, 1604, 300, 30, 509, 976, 257, 1326, 5110, 295, 512, 9887, 300, 291, 528, 281, 2042, 51452], "temperature": 0.0, "avg_logprob": -0.049870053927103676, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.002631361596286297}, {"id": 132, "seek": 79784, "start": 819.6, "end": 824.4000000000001, "text": " and it will generate a piece of programming code for you, which means it can generalize", "tokens": [51452, 293, 309, 486, 8460, 257, 2522, 295, 9410, 3089, 337, 291, 11, 597, 1355, 309, 393, 2674, 1125, 51692], "temperature": 0.0, "avg_logprob": -0.049870053927103676, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.002631361596286297}, {"id": 133, "seek": 82440, "start": 824.4, "end": 830.0, "text": " that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around", "tokens": [50364, 300, 9887, 2108, 364, 2302, 27733, 13, 467, 311, 1596, 257, 22687, 1558, 13, 467, 311, 668, 926, 50644], "temperature": 0.0, "avg_logprob": -0.051106587052345276, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0040673441253602505}, {"id": 134, "seek": 82440, "start": 830.0, "end": 835.68, "text": " for about 20 years actually, but what's really making it work now is the idea of using neural", "tokens": [50644, 337, 466, 945, 924, 767, 11, 457, 437, 311, 534, 1455, 309, 589, 586, 307, 264, 1558, 295, 1228, 18161, 50928], "temperature": 0.0, "avg_logprob": -0.051106587052345276, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0040673441253602505}, {"id": 135, "seek": 82440, "start": 835.68, "end": 843.04, "text": " networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks", "tokens": [50928, 9590, 420, 257, 18161, 2848, 281, 5934, 264, 27706, 1461, 3164, 13, 492, 7179, 466, 26039, 51, 12, 18, 13, 634, 7309, 51296], "temperature": 0.0, "avg_logprob": -0.051106587052345276, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0040673441253602505}, {"id": 136, "seek": 82440, "start": 843.04, "end": 849.36, "text": " that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new", "tokens": [51296, 300, 26039, 51, 12, 18, 6132, 380, 14342, 702, 3601, 295, 264, 1002, 13, 634, 1619, 300, 26039, 51, 12, 18, 307, 406, 2539, 604, 777, 51612], "temperature": 0.0, "avg_logprob": -0.051106587052345276, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0040673441253602505}, {"id": 137, "seek": 84936, "start": 849.36, "end": 855.52, "text": " algorithms on the fly. It's already learned continuous and often glitchy representations", "tokens": [50364, 14642, 322, 264, 3603, 13, 467, 311, 1217, 3264, 10957, 293, 2049, 23552, 88, 33358, 50672], "temperature": 0.0, "avg_logprob": -0.07319134253042715, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.011330313049256802}, {"id": 138, "seek": 84936, "start": 855.52, "end": 860.8000000000001, "text": " of existing tasks during its training. It's completely ineffective against his arc challenge", "tokens": [50672, 295, 6741, 9608, 1830, 1080, 3097, 13, 467, 311, 2584, 48836, 1970, 702, 10346, 3430, 50936], "temperature": 0.0, "avg_logprob": -0.07319134253042715, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.011330313049256802}, {"id": 139, "seek": 84936, "start": 860.8000000000001, "end": 869.36, "text": " tasks. People often claim that neural networks are turing complete. No, they're not. A model has", "tokens": [50936, 9608, 13, 3432, 2049, 3932, 300, 18161, 9590, 366, 256, 1345, 3566, 13, 883, 11, 436, 434, 406, 13, 316, 2316, 575, 51364], "temperature": 0.0, "avg_logprob": -0.07319134253042715, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.011330313049256802}, {"id": 140, "seek": 84936, "start": 869.36, "end": 875.52, "text": " a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded", "tokens": [51364, 257, 37498, 1230, 295, 13891, 293, 257, 37498, 34474, 13, 467, 2644, 14483, 14642, 300, 3651, 517, 18767, 292, 51672], "temperature": 0.0, "avg_logprob": -0.07319134253042715, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.011330313049256802}, {"id": 141, "seek": 87552, "start": 875.52, "end": 882.4, "text": " space or unbounded time. For example, could you train a neural network to predict the nth digit of", "tokens": [50364, 1901, 420, 517, 18767, 292, 565, 13, 1171, 1365, 11, 727, 291, 3847, 257, 18161, 3209, 281, 6069, 264, 297, 392, 14293, 295, 50708], "temperature": 0.0, "avg_logprob": -0.05611324787139892, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0023229580838233232}, {"id": 142, "seek": 87552, "start": 882.4, "end": 888.0, "text": " pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural", "tokens": [50708, 3895, 30, 883, 11, 291, 2809, 380, 13, 509, 727, 2464, 257, 3820, 1461, 281, 360, 309, 11, 457, 291, 2809, 380, 3847, 257, 18161, 50988], "temperature": 0.0, "avg_logprob": -0.05611324787139892, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0023229580838233232}, {"id": 143, "seek": 87552, "start": 888.0, "end": 893.1999999999999, "text": " network to do it. A simple turing machine program can do just that and that is because a turing machine", "tokens": [50988, 3209, 281, 360, 309, 13, 316, 2199, 256, 1345, 3479, 1461, 393, 360, 445, 300, 293, 300, 307, 570, 257, 256, 1345, 3479, 51248], "temperature": 0.0, "avg_logprob": -0.05611324787139892, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0023229580838233232}, {"id": 144, "seek": 87552, "start": 893.1999999999999, "end": 898.96, "text": " can access unbounded memory and time. The best thing that neural networks can do is approximate", "tokens": [51248, 393, 2105, 517, 18767, 292, 4675, 293, 565, 13, 440, 1151, 551, 300, 18161, 9590, 393, 360, 307, 30874, 51536], "temperature": 0.0, "avg_logprob": -0.05611324787139892, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0023229580838233232}, {"id": 145, "seek": 89896, "start": 898.96, "end": 905.2, "text": " unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural", "tokens": [50364, 517, 18767, 292, 14642, 11, 457, 884, 370, 486, 5366, 23552, 279, 13, 1171, 1365, 11, 472, 393, 3847, 257, 18161, 50676], "temperature": 0.0, "avg_logprob": -0.08508321974012586, "compression_ratio": 1.8, "no_speech_prob": 0.022278131917119026}, {"id": 146, "seek": 89896, "start": 905.2, "end": 910.64, "text": " network to approximately multiply integers together. Yet, even when learning to multiply", "tokens": [50676, 3209, 281, 10447, 12972, 41674, 1214, 13, 10890, 11, 754, 562, 2539, 281, 12972, 50948], "temperature": 0.0, "avg_logprob": -0.08508321974012586, "compression_ratio": 1.8, "no_speech_prob": 0.022278131917119026}, {"id": 147, "seek": 89896, "start": 910.64, "end": 915.9200000000001, "text": " fixed-width integers, practically-sized neural networks introduce errors occasionally,", "tokens": [50948, 6806, 12, 21271, 41674, 11, 15667, 12, 20614, 18161, 9590, 5366, 13603, 16895, 11, 51212], "temperature": 0.0, "avg_logprob": -0.08508321974012586, "compression_ratio": 1.8, "no_speech_prob": 0.022278131917119026}, {"id": 148, "seek": 89896, "start": 915.9200000000001, "end": 921.76, "text": " and for a fixed-sized neural network, these errors grow more common as the size of the input grows.", "tokens": [51212, 293, 337, 257, 6806, 12, 20614, 18161, 3209, 11, 613, 13603, 1852, 544, 2689, 382, 264, 2744, 295, 264, 4846, 13156, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08508321974012586, "compression_ratio": 1.8, "no_speech_prob": 0.022278131917119026}, {"id": 149, "seek": 89896, "start": 921.76, "end": 927.2800000000001, "text": " That said, neural networks are finite state machines, and just as finite state machines", "tokens": [51504, 663, 848, 11, 18161, 9590, 366, 19362, 1785, 8379, 11, 293, 445, 382, 19362, 1785, 8379, 51780], "temperature": 0.0, "avg_logprob": -0.08508321974012586, "compression_ratio": 1.8, "no_speech_prob": 0.022278131917119026}, {"id": 150, "seek": 92728, "start": 927.28, "end": 932.64, "text": " can be augmented with unbounded memory and iteration to yield a turing machine, neural", "tokens": [50364, 393, 312, 36155, 365, 517, 18767, 292, 4675, 293, 24784, 281, 11257, 257, 256, 1345, 3479, 11, 18161, 50632], "temperature": 0.0, "avg_logprob": -0.07049170987946647, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0003051782841794193}, {"id": 151, "seek": 92728, "start": 932.64, "end": 938.4, "text": " networks can also be automated in the same way to produce a turing-complete computational model.", "tokens": [50632, 9590, 393, 611, 312, 18473, 294, 264, 912, 636, 281, 5258, 257, 256, 1345, 12, 1112, 17220, 28270, 2316, 13, 50920], "temperature": 0.0, "avg_logprob": -0.07049170987946647, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0003051782841794193}, {"id": 152, "seek": 92728, "start": 938.4, "end": 943.28, "text": " If you want to see a concrete example of the kind of discrete program search that", "tokens": [50920, 759, 291, 528, 281, 536, 257, 9859, 1365, 295, 264, 733, 295, 27706, 1461, 3164, 300, 51164], "temperature": 0.0, "avg_logprob": -0.07049170987946647, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0003051782841794193}, {"id": 153, "seek": 92728, "start": 943.28, "end": 949.36, "text": " Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video", "tokens": [51164, 761, 49744, 307, 1417, 466, 11, 574, 572, 3052, 813, 264, 5162, 12105, 34, 19866, 3035, 13, 398, 969, 618, 445, 1027, 257, 960, 51468], "temperature": 0.0, "avg_logprob": -0.07049170987946647, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0003051782841794193}, {"id": 154, "seek": 92728, "start": 949.36, "end": 954.48, "text": " about it. So yeah, it feels like today is the culmination of a year of really hard work and", "tokens": [51468, 466, 309, 13, 407, 1338, 11, 309, 3417, 411, 965, 307, 264, 28583, 399, 295, 257, 1064, 295, 534, 1152, 589, 293, 51724], "temperature": 0.0, "avg_logprob": -0.07049170987946647, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0003051782841794193}, {"id": 155, "seek": 95448, "start": 954.48, "end": 959.84, "text": " passion from the MLST team. We've worked with so many fascinating people. We've had so many", "tokens": [50364, 5418, 490, 264, 21601, 6840, 1469, 13, 492, 600, 2732, 365, 370, 867, 10343, 561, 13, 492, 600, 632, 370, 867, 50632], "temperature": 0.0, "avg_logprob": -0.08396979173024495, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.5114259719848633}, {"id": 156, "seek": 95448, "start": 959.84, "end": 965.6, "text": " amazing guests on. It really means a lot to us. Today is a very, very special episode. It was", "tokens": [50632, 2243, 9804, 322, 13, 467, 534, 1355, 257, 688, 281, 505, 13, 2692, 307, 257, 588, 11, 588, 2121, 3500, 13, 467, 390, 50920], "temperature": 0.0, "avg_logprob": -0.08396979173024495, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.5114259719848633}, {"id": 157, "seek": 95448, "start": 965.6, "end": 970.5600000000001, "text": " my dream from the beginning to get Chalet on the show. I know that Chalet is going to say", "tokens": [50920, 452, 3055, 490, 264, 2863, 281, 483, 761, 49744, 322, 264, 855, 13, 286, 458, 300, 761, 49744, 307, 516, 281, 584, 51168], "temperature": 0.0, "avg_logprob": -0.08396979173024495, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.5114259719848633}, {"id": 158, "seek": 95448, "start": 970.5600000000001, "end": 975.84, "text": " lots of interesting things that will trigger some people and inspire others, and please take to the", "tokens": [51168, 3195, 295, 1880, 721, 300, 486, 7875, 512, 561, 293, 15638, 2357, 11, 293, 1767, 747, 281, 264, 51432], "temperature": 0.0, "avg_logprob": -0.08396979173024495, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.5114259719848633}, {"id": 159, "seek": 95448, "start": 975.84, "end": 982.48, "text": " comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out.", "tokens": [51432, 2871, 3541, 293, 980, 505, 2293, 437, 291, 519, 13, 5684, 11, 2103, 264, 855, 13, 3008, 291, 958, 1243, 13, 13204, 484, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08396979173024495, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.5114259719848633}, {"id": 160, "seek": 98248, "start": 982.5600000000001, "end": 986.32, "text": " Welcome back to the Machine Learning Street Talk YouTube channel and podcast", "tokens": [50368, 4027, 646, 281, 264, 22155, 15205, 7638, 8780, 3088, 2269, 293, 7367, 50556], "temperature": 0.0, "avg_logprob": -0.11921978848321098, "compression_ratio": 1.5432525951557095, "no_speech_prob": 0.23707087337970734}, {"id": 161, "seek": 98248, "start": 986.32, "end": 992.96, "text": " with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.", "tokens": [50556, 365, 452, 732, 715, 345, 495, 11, 13100, 11, 14476, 11, 2491, 13, 20613, 413, 697, 2976, 293, 398, 969, 618, 38226, 494, 292, 591, 2352, 6759, 13, 50888], "temperature": 0.0, "avg_logprob": -0.11921978848321098, "compression_ratio": 1.5432525951557095, "no_speech_prob": 0.23707087337970734}, {"id": 162, "seek": 98248, "start": 993.6, "end": 999.9200000000001, "text": " Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in", "tokens": [50920, 823, 965, 321, 362, 257, 588, 2121, 8341, 11, 34695, 271, 761, 49744, 13, 34695, 271, 307, 472, 295, 264, 1326, 3523, 294, 51236], "temperature": 0.0, "avg_logprob": -0.11921978848321098, "compression_ratio": 1.5432525951557095, "no_speech_prob": 0.23707087337970734}, {"id": 163, "seek": 98248, "start": 999.9200000000001, "end": 1004.8000000000001, "text": " the machine learning space who's caused a massive stir in my thinking, the only other notable one", "tokens": [51236, 264, 3479, 2539, 1901, 567, 311, 7008, 257, 5994, 8946, 294, 452, 1953, 11, 264, 787, 661, 22556, 472, 51480], "temperature": 0.0, "avg_logprob": -0.11921978848321098, "compression_ratio": 1.5432525951557095, "no_speech_prob": 0.23707087337970734}, {"id": 164, "seek": 98248, "start": 1004.8000000000001, "end": 1008.88, "text": " actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was", "tokens": [51480, 767, 885, 33735, 28329, 11, 567, 321, 632, 322, 3938, 13, 1222, 9705, 3387, 365, 7638, 8780, 390, 51684], "temperature": 0.0, "avg_logprob": -0.11921978848321098, "compression_ratio": 1.5432525951557095, "no_speech_prob": 0.23707087337970734}, {"id": 165, "seek": 100888, "start": 1008.88, "end": 1013.68, "text": " always to get Francois on the show, and I can't believe that it's actually happened. We actually", "tokens": [50364, 1009, 281, 483, 34695, 271, 322, 264, 855, 11, 293, 286, 393, 380, 1697, 300, 309, 311, 767, 2011, 13, 492, 767, 50604], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 166, "seek": 100888, "start": 1013.68, "end": 1017.92, "text": " have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show,", "tokens": [50604, 362, 257, 4978, 11, 538, 264, 636, 11, 300, 286, 478, 787, 4350, 281, 41117, 34695, 271, 311, 1315, 466, 1564, 680, 855, 11, 50816], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 167, "seek": 100888, "start": 1017.92, "end": 1024.16, "text": " but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually", "tokens": [50816, 457, 300, 4978, 486, 406, 3079, 965, 13, 398, 969, 618, 293, 286, 362, 1027, 544, 2701, 322, 34695, 271, 761, 49744, 767, 51128], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 168, "seek": 100888, "start": 1024.16, "end": 1028.16, "text": " than anyone else by a wide margin, and it's because his work is very thought-provoking", "tokens": [51128, 813, 2878, 1646, 538, 257, 4874, 10270, 11, 293, 309, 311, 570, 702, 589, 307, 588, 1194, 12, 49911, 5953, 51328], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 169, "seek": 100888, "start": 1028.16, "end": 1033.36, "text": " and disruptive. I spent many weeks actually studying his measure of intelligence paper last year,", "tokens": [51328, 293, 37865, 13, 286, 4418, 867, 3259, 767, 7601, 702, 3481, 295, 7599, 3035, 1036, 1064, 11, 51588], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 170, "seek": 100888, "start": 1033.36, "end": 1038.0, "text": " and of course his recent New York's workshop was fascinating as well. Almost every single word in", "tokens": [51588, 293, 295, 1164, 702, 5162, 1873, 3609, 311, 13541, 390, 10343, 382, 731, 13, 12627, 633, 2167, 1349, 294, 51820], "temperature": 0.0, "avg_logprob": -0.08149225934803915, "compression_ratio": 1.718562874251497, "no_speech_prob": 0.09240178018808365}, {"id": 171, "seek": 103800, "start": 1038.0, "end": 1042.4, "text": " my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that.", "tokens": [50364, 452, 4800, 300, 1487, 484, 295, 34695, 271, 311, 4525, 17037, 29882, 2979, 11, 293, 286, 6638, 914, 300, 13, 50584], "temperature": 0.0, "avg_logprob": -0.06853679890902538, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.006912741344422102}, {"id": 172, "seek": 103800, "start": 1043.44, "end": 1048.96, "text": " Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's", "tokens": [50636, 34695, 271, 7309, 300, 7599, 307, 42046, 11, 309, 311, 257, 1399, 11, 293, 309, 311, 406, 445, 257, 3567, 13, 634, 311, 50912], "temperature": 0.0, "avg_logprob": -0.06853679890902538, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.006912741344422102}, {"id": 173, "seek": 103800, "start": 1048.96, "end": 1053.68, "text": " skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general", "tokens": [50912, 28601, 295, 264, 370, 12, 11880, 7599, 15673, 11, 293, 415, 7309, 456, 311, 572, 1270, 551, 382, 2674, 51148], "temperature": 0.0, "avg_logprob": -0.06853679890902538, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.006912741344422102}, {"id": 174, "seek": 103800, "start": 1053.68, "end": 1060.08, "text": " intelligence. All intelligence is specialized. Critically, he thinks that generalization,", "tokens": [51148, 7599, 13, 1057, 7599, 307, 19813, 13, 23202, 984, 11, 415, 7309, 300, 2674, 2144, 11, 51468], "temperature": 0.0, "avg_logprob": -0.06853679890902538, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.006912741344422102}, {"id": 175, "seek": 103800, "start": 1060.08, "end": 1066.96, "text": " the ability to deal with novelty and uncertainty is the most important concept in intelligence.", "tokens": [51468, 264, 3485, 281, 2028, 365, 44805, 293, 15697, 307, 264, 881, 1021, 3410, 294, 7599, 13, 51812], "temperature": 0.0, "avg_logprob": -0.06853679890902538, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.006912741344422102}, {"id": 176, "seek": 106696, "start": 1066.96, "end": 1072.8, "text": " He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep", "tokens": [50364, 634, 7309, 300, 5633, 12, 29258, 3942, 5112, 291, 1825, 466, 7599, 13, 634, 7309, 300, 2452, 50656], "temperature": 0.0, "avg_logprob": -0.0639048417409261, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0014971478376537561}, {"id": 177, "seek": 106696, "start": 1072.8, "end": 1077.92, "text": " learning only works for problems where the manifold hypothesis applies. For example,", "tokens": [50656, 2539, 787, 1985, 337, 2740, 689, 264, 47138, 17291, 13165, 13, 1171, 1365, 11, 50912], "temperature": 0.0, "avg_logprob": -0.0639048417409261, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0014971478376537561}, {"id": 178, "seek": 106696, "start": 1077.92, "end": 1083.52, "text": " problems which are interpolative in nature and when a sufficiently dense sampling of your", "tokens": [50912, 2740, 597, 366, 44902, 1166, 294, 3687, 293, 562, 257, 31868, 18011, 21179, 295, 428, 51192], "temperature": 0.0, "avg_logprob": -0.0639048417409261, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0014971478376537561}, {"id": 179, "seek": 106696, "start": 1083.52, "end": 1091.04, "text": " distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize,", "tokens": [51192, 7316, 307, 14879, 13, 10328, 11, 2452, 2539, 2644, 2674, 1125, 13, 14895, 2539, 393, 787, 27478, 11, 51568], "temperature": 0.0, "avg_logprob": -0.0639048417409261, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0014971478376537561}, {"id": 180, "seek": 106696, "start": 1091.04, "end": 1096.4, "text": " but it cannot always generalize. And in his recent New York's presentation, he introduced the concept", "tokens": [51568, 457, 309, 2644, 1009, 2674, 1125, 13, 400, 294, 702, 5162, 1873, 3609, 311, 5860, 11, 415, 7268, 264, 3410, 51836], "temperature": 0.0, "avg_logprob": -0.0639048417409261, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0014971478376537561}, {"id": 181, "seek": 109640, "start": 1096.4, "end": 1101.1200000000001, "text": " of program-centric and value-centric generalization, which we'll get into in the show today.", "tokens": [50364, 295, 1461, 12, 45300, 293, 2158, 12, 45300, 2674, 2144, 11, 597, 321, 603, 483, 666, 294, 264, 855, 965, 13, 50600], "temperature": 0.0, "avg_logprob": -0.08286244401307864, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.003922341391444206}, {"id": 182, "seek": 109640, "start": 1101.1200000000001, "end": 1106.8000000000002, "text": " But I wanted to move straight on to this concept of deep learning being a hash table,", "tokens": [50600, 583, 286, 1415, 281, 1286, 2997, 322, 281, 341, 3410, 295, 2452, 2539, 885, 257, 22019, 3199, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08286244401307864, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.003922341391444206}, {"id": 183, "seek": 109640, "start": 1106.8000000000002, "end": 1113.2, "text": " because this is what Francois thinks. He says that a deep learning model is a high-dimensional", "tokens": [50884, 570, 341, 307, 437, 34695, 271, 7309, 13, 634, 1619, 300, 257, 2452, 2539, 2316, 307, 257, 1090, 12, 18759, 51204], "temperature": 0.0, "avg_logprob": -0.08286244401307864, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.003922341391444206}, {"id": 184, "seek": 109640, "start": 1113.2, "end": 1119.2800000000002, "text": " curve with some constraints on its structure given by inductive priors, and that curve has", "tokens": [51204, 7605, 365, 512, 18491, 322, 1080, 3877, 2212, 538, 31612, 488, 1790, 830, 11, 293, 300, 7605, 575, 51508], "temperature": 0.0, "avg_logprob": -0.08286244401307864, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.003922341391444206}, {"id": 185, "seek": 109640, "start": 1119.2800000000002, "end": 1124.0, "text": " enough parameters that it could fit almost anything. If you train your model for long enough,", "tokens": [51508, 1547, 9834, 300, 309, 727, 3318, 1920, 1340, 13, 759, 291, 3847, 428, 2316, 337, 938, 1547, 11, 51744], "temperature": 0.0, "avg_logprob": -0.08286244401307864, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.003922341391444206}, {"id": 186, "seek": 112400, "start": 1124.0, "end": 1130.08, "text": " it'll simply memorize your data. And because of SGD, your manifold fit is found progressively,", "tokens": [50364, 309, 603, 2935, 27478, 428, 1412, 13, 400, 570, 295, 34520, 35, 11, 428, 47138, 3318, 307, 1352, 46667, 11, 50668], "temperature": 0.0, "avg_logprob": -0.0695063015362164, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0096569349989295}, {"id": 187, "seek": 112400, "start": 1130.08, "end": 1134.56, "text": " and at some point, the manifold will approximate the natural manifold between underfitting and", "tokens": [50668, 293, 412, 512, 935, 11, 264, 47138, 486, 30874, 264, 3303, 47138, 1296, 833, 69, 2414, 293, 50892], "temperature": 0.0, "avg_logprob": -0.0695063015362164, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0096569349989295}, {"id": 188, "seek": 112400, "start": 1134.56, "end": 1140.64, "text": " overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating", "tokens": [50892, 670, 69, 2414, 13, 400, 412, 341, 935, 11, 291, 603, 312, 1075, 281, 652, 2020, 295, 7613, 15743, 538, 44902, 990, 51196], "temperature": 0.0, "avg_logprob": -0.0695063015362164, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0096569349989295}, {"id": 189, "seek": 112400, "start": 1140.64, "end": 1145.68, "text": " on that manifold. So the power of the model to generalize is actually a consequence of the", "tokens": [51196, 322, 300, 47138, 13, 407, 264, 1347, 295, 264, 2316, 281, 2674, 1125, 307, 767, 257, 18326, 295, 264, 51448], "temperature": 0.0, "avg_logprob": -0.0695063015362164, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0096569349989295}, {"id": 190, "seek": 112400, "start": 1145.68, "end": 1151.68, "text": " structure of the data and the gradual process of SGD, according to Francois, rather than any property", "tokens": [51448, 3877, 295, 264, 1412, 293, 264, 32890, 1399, 295, 34520, 35, 11, 4650, 281, 34695, 271, 11, 2831, 813, 604, 4707, 51748], "temperature": 0.0, "avg_logprob": -0.0695063015362164, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0096569349989295}, {"id": 191, "seek": 115168, "start": 1151.68, "end": 1157.6000000000001, "text": " of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a", "tokens": [50364, 295, 264, 2316, 2564, 13, 5264, 1243, 11, 34695, 271, 11, 321, 645, 1417, 281, 5778, 18885, 12716, 11, 293, 415, 2516, 257, 50660], "temperature": 0.0, "avg_logprob": -0.08922941596419723, "compression_ratio": 1.6026936026936027, "no_speech_prob": 0.029707064852118492}, {"id": 192, "seek": 115168, "start": 1157.6000000000001, "end": 1161.68, "text": " rather different view, because one school of thought is that deep learning models are kind of like", "tokens": [50660, 2831, 819, 1910, 11, 570, 472, 1395, 295, 1194, 307, 300, 2452, 2539, 5245, 366, 733, 295, 411, 50864], "temperature": 0.0, "avg_logprob": -0.08922941596419723, "compression_ratio": 1.6026936026936027, "no_speech_prob": 0.029707064852118492}, {"id": 193, "seek": 115168, "start": 1162.4, "end": 1168.0, "text": " searching for a space of possible programs, and advocates of GPT-3 make this argument quite", "tokens": [50900, 10808, 337, 257, 1901, 295, 1944, 4268, 11, 293, 25160, 295, 26039, 51, 12, 18, 652, 341, 6770, 1596, 51180], "temperature": 0.0, "avg_logprob": -0.08922941596419723, "compression_ratio": 1.6026936026936027, "no_speech_prob": 0.029707064852118492}, {"id": 194, "seek": 115168, "start": 1168.0, "end": 1172.5600000000002, "text": " strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is", "tokens": [51180, 10613, 13, 400, 26742, 11, 5778, 18885, 12716, 11, 415, 2759, 380, 312, 884, 437, 415, 311, 884, 11, 597, 307, 51408], "temperature": 0.0, "avg_logprob": -0.08922941596419723, "compression_ratio": 1.6026936026936027, "no_speech_prob": 0.029707064852118492}, {"id": 195, "seek": 115168, "start": 1173.68, "end": 1178.96, "text": " interpolating between mathematical conjectures, assuming that interpolation space would actually", "tokens": [51464, 44902, 990, 1296, 18894, 416, 1020, 1303, 11, 11926, 300, 44902, 399, 1901, 576, 767, 51728], "temperature": 0.0, "avg_logprob": -0.08922941596419723, "compression_ratio": 1.6026936026936027, "no_speech_prob": 0.029707064852118492}, {"id": 196, "seek": 117896, "start": 1178.96, "end": 1184.48, "text": " give us new information about mathematics, if he thought that that space wasn't interpolatable.", "tokens": [50364, 976, 505, 777, 1589, 466, 18666, 11, 498, 415, 1194, 300, 300, 1901, 2067, 380, 44902, 31415, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1516216066148546, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.006567998323589563}, {"id": 197, "seek": 117896, "start": 1184.48, "end": 1191.44, "text": " What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation", "tokens": [50640, 708, 360, 291, 519, 34695, 271, 30, 1779, 11, 286, 519, 291, 600, 1217, 14611, 1602, 309, 11, 534, 13, 865, 11, 370, 44902, 399, 50988], "temperature": 0.0, "avg_logprob": -0.1516216066148546, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.006567998323589563}, {"id": 198, "seek": 117896, "start": 1191.44, "end": 1197.6000000000001, "text": " is the origin of generalization in deep learning models, and that's very much by construction,", "tokens": [50988, 307, 264, 4957, 295, 2674, 2144, 294, 2452, 2539, 5245, 11, 293, 300, 311, 588, 709, 538, 6435, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1516216066148546, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.006567998323589563}, {"id": 199, "seek": 117896, "start": 1197.6000000000001, "end": 1203.3600000000001, "text": " by nature, right? Like a deep learning model is a very large, differentiable,", "tokens": [51296, 538, 3687, 11, 558, 30, 1743, 257, 2452, 2539, 2316, 307, 257, 588, 2416, 11, 819, 9364, 11, 51584], "temperature": 0.0, "avg_logprob": -0.1516216066148546, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.006567998323589563}, {"id": 200, "seek": 120336, "start": 1203.36, "end": 1209.12, "text": " parametric model, trained with gradient descent. And so the only way it's ever going to be", "tokens": [50364, 6220, 17475, 2316, 11, 8895, 365, 16235, 23475, 13, 400, 370, 264, 787, 636, 309, 311, 1562, 516, 281, 312, 50652], "temperature": 0.0, "avg_logprob": -0.15782068456922257, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.016912730410695076}, {"id": 201, "seek": 120336, "start": 1209.12, "end": 1215.04, "text": " generalizing is your interpolation. This is literally, this is what it is, this is what it does.", "tokens": [50652, 2674, 3319, 307, 428, 44902, 399, 13, 639, 307, 3736, 11, 341, 307, 437, 309, 307, 11, 341, 307, 437, 309, 775, 13, 50948], "temperature": 0.0, "avg_logprob": -0.15782068456922257, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.016912730410695076}, {"id": 202, "seek": 120336, "start": 1215.04, "end": 1220.24, "text": " So I think the question, you know, are all deep learning models, interpolators or not,", "tokens": [50948, 407, 286, 519, 264, 1168, 11, 291, 458, 11, 366, 439, 2452, 2539, 5245, 11, 44902, 3391, 420, 406, 11, 51208], "temperature": 0.0, "avg_logprob": -0.15782068456922257, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.016912730410695076}, {"id": 203, "seek": 120336, "start": 1220.24, "end": 1224.1599999999999, "text": " is not a super interesting question, because it's not an open question. We know they are.", "tokens": [51208, 307, 406, 257, 1687, 1880, 1168, 11, 570, 309, 311, 406, 364, 1269, 1168, 13, 492, 458, 436, 366, 13, 51404], "temperature": 0.0, "avg_logprob": -0.15782068456922257, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.016912730410695076}, {"id": 204, "seek": 120336, "start": 1224.1599999999999, "end": 1228.8799999999999, "text": " But the more interesting question, I think, is what can you actually achieve with the", "tokens": [51404, 583, 264, 544, 1880, 1168, 11, 286, 519, 11, 307, 437, 393, 291, 767, 4584, 365, 264, 51640], "temperature": 0.0, "avg_logprob": -0.15782068456922257, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.016912730410695076}, {"id": 205, "seek": 122888, "start": 1228.88, "end": 1233.68, "text": " stored of interpolation on this very complex, very high-dimensional manifold,", "tokens": [50364, 12187, 295, 44902, 399, 322, 341, 588, 3997, 11, 588, 1090, 12, 18759, 47138, 11, 50604], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 206, "seek": 122888, "start": 1233.68, "end": 1238.0, "text": " that they're deep learning models and implementing. We're telling you the properties of this generalization,", "tokens": [50604, 300, 436, 434, 2452, 2539, 5245, 293, 18114, 13, 492, 434, 3585, 291, 264, 7221, 295, 341, 2674, 2144, 11, 50820], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 207, "seek": 122888, "start": 1239.1200000000001, "end": 1242.72, "text": " the tasks for which it will perform well, the tasks for which it will not perform well.", "tokens": [50876, 264, 9608, 337, 597, 309, 486, 2042, 731, 11, 264, 9608, 337, 597, 309, 486, 406, 2042, 731, 13, 51056], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 208, "seek": 122888, "start": 1242.72, "end": 1248.72, "text": " I guess one example I could give you is encoding data with the Fourier transform,", "tokens": [51056, 286, 2041, 472, 1365, 286, 727, 976, 291, 307, 43430, 1412, 365, 264, 36810, 4088, 11, 51356], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 209, "seek": 122888, "start": 1248.72, "end": 1253.1200000000001, "text": " like you know about the Fourier transform. And maybe, you know, some people will play around", "tokens": [51356, 411, 291, 458, 466, 264, 36810, 4088, 13, 400, 1310, 11, 291, 458, 11, 512, 561, 486, 862, 926, 51576], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 210, "seek": 122888, "start": 1253.1200000000001, "end": 1257.8400000000001, "text": " with it and they will be like, hey, you know, actually the Fourier transform can draw much", "tokens": [51576, 365, 309, 293, 436, 486, 312, 411, 11, 4177, 11, 291, 458, 11, 767, 264, 36810, 4088, 393, 2642, 709, 51812], "temperature": 0.0, "avg_logprob": -0.20650493032563993, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.002246550051495433}, {"id": 211, "seek": 125784, "start": 1257.84, "end": 1262.1599999999999, "text": " more than curves. Look, I made a square with it, right? And then you would have to point out that,", "tokens": [50364, 544, 813, 19490, 13, 2053, 11, 286, 1027, 257, 3732, 365, 309, 11, 558, 30, 400, 550, 291, 576, 362, 281, 935, 484, 300, 11, 50580], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 212, "seek": 125784, "start": 1262.1599999999999, "end": 1266.6399999999999, "text": " no, actually the square, you've made it by supposing lots of tiny curves. And it's not,", "tokens": [50580, 572, 11, 767, 264, 3732, 11, 291, 600, 1027, 309, 538, 1003, 6110, 3195, 295, 5870, 19490, 13, 400, 309, 311, 406, 11, 50804], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 213, "seek": 125784, "start": 1266.6399999999999, "end": 1270.8799999999999, "text": " in fact, a perfect square, right? Because it is made of this, with the other supposition of lots", "tokens": [50804, 294, 1186, 11, 257, 2176, 3732, 11, 558, 30, 1436, 309, 307, 1027, 295, 341, 11, 365, 264, 661, 1003, 5830, 295, 3195, 51016], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 214, "seek": 125784, "start": 1270.8799999999999, "end": 1276.32, "text": " of tiny curves. And that's really, this is true by nature, by construction. This is where the", "tokens": [51016, 295, 5870, 19490, 13, 400, 300, 311, 534, 11, 341, 307, 2074, 538, 3687, 11, 538, 6435, 13, 639, 307, 689, 264, 51288], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 215, "seek": 125784, "start": 1276.32, "end": 1281.6, "text": " Fourier transform starts, right? And the more interesting question is, you know, what sort of", "tokens": [51288, 36810, 4088, 3719, 11, 558, 30, 400, 264, 544, 1880, 1168, 307, 11, 291, 458, 11, 437, 1333, 295, 51552], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 216, "seek": 125784, "start": 1281.6, "end": 1286.8799999999999, "text": " data is a good fit for encoding the Fourier transform? And what sort of data is not a good", "tokens": [51552, 1412, 307, 257, 665, 3318, 337, 43430, 264, 36810, 4088, 30, 400, 437, 1333, 295, 1412, 307, 406, 257, 665, 51816], "temperature": 0.0, "avg_logprob": -0.17048638487515383, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.001859332900494337}, {"id": 217, "seek": 128688, "start": 1286.88, "end": 1291.7600000000002, "text": " fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have", "tokens": [50364, 3318, 30, 1743, 498, 291, 853, 281, 2058, 1429, 264, 256, 12, 33292, 543, 17948, 304, 365, 264, 36810, 4088, 11, 291, 434, 516, 281, 362, 50608], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 218, "seek": 128688, "start": 1291.7600000000002, "end": 1297.3600000000001, "text": " a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves,", "tokens": [50608, 257, 1578, 565, 13, 400, 498, 291, 853, 281, 2058, 1429, 264, 6316, 11, 300, 311, 5240, 445, 11, 291, 458, 11, 1481, 11, 5508, 19490, 11, 50888], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 219, "seek": 128688, "start": 1297.3600000000001, "end": 1302.0, "text": " then it's going to be a very, very efficient encoding at a good idea. And deep learning is", "tokens": [50888, 550, 309, 311, 516, 281, 312, 257, 588, 11, 588, 7148, 43430, 412, 257, 665, 1558, 13, 400, 2452, 2539, 307, 51120], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 220, "seek": 128688, "start": 1302.0, "end": 1306.16, "text": " very much like that. We should ask, you know, what are its strong points, what are its weak points?", "tokens": [51120, 588, 709, 411, 300, 13, 492, 820, 1029, 11, 291, 458, 11, 437, 366, 1080, 2068, 2793, 11, 437, 366, 1080, 5336, 2793, 30, 51328], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 221, "seek": 128688, "start": 1306.16, "end": 1310.88, "text": " Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there,", "tokens": [51328, 865, 11, 370, 286, 11, 538, 264, 636, 11, 370, 286, 500, 380, 1697, 300, 2452, 2539, 5245, 366, 22019, 8020, 11, 1804, 456, 11, 51564], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 222, "seek": 128688, "start": 1310.88, "end": 1315.8400000000001, "text": " I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table", "tokens": [51564, 286, 2673, 584, 456, 11, 2654, 1088, 366, 9477, 22019, 8020, 11, 3620, 300, 733, 295, 411, 257, 22019, 3199, 51812], "temperature": 0.0, "avg_logprob": -0.15275109649463825, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.002035650657489896}, {"id": 223, "seek": 131584, "start": 1315.84, "end": 1320.9599999999998, "text": " with some amount of generalization power, because they have some notion of distance", "tokens": [50364, 365, 512, 2372, 295, 2674, 2144, 1347, 11, 570, 436, 362, 512, 10710, 295, 4560, 50620], "temperature": 0.0, "avg_logprob": -0.12331977257361779, "compression_ratio": 1.772, "no_speech_prob": 0.0022120652720332146}, {"id": 224, "seek": 131584, "start": 1320.9599999999998, "end": 1326.1599999999999, "text": " between parts. They're capable of comparing points by measuring the distance between them, right?", "tokens": [50620, 1296, 3166, 13, 814, 434, 8189, 295, 15763, 2793, 538, 13389, 264, 4560, 1296, 552, 11, 558, 30, 50880], "temperature": 0.0, "avg_logprob": -0.12331977257361779, "compression_ratio": 1.772, "no_speech_prob": 0.0022120652720332146}, {"id": 225, "seek": 131584, "start": 1326.1599999999999, "end": 1331.12, "text": " And this, this is what would enable this kind of hash table to actually generalize, as opposed to", "tokens": [50880, 400, 341, 11, 341, 307, 437, 576, 9528, 341, 733, 295, 22019, 3199, 281, 767, 2674, 1125, 11, 382, 8851, 281, 51128], "temperature": 0.0, "avg_logprob": -0.12331977257361779, "compression_ratio": 1.772, "no_speech_prob": 0.0022120652720332146}, {"id": 226, "seek": 131584, "start": 1331.12, "end": 1334.56, "text": " the classic kind of hash table, which is just memorizing the data.", "tokens": [51128, 264, 7230, 733, 295, 22019, 3199, 11, 597, 307, 445, 10560, 3319, 264, 1412, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12331977257361779, "compression_ratio": 1.772, "no_speech_prob": 0.0022120652720332146}, {"id": 227, "seek": 131584, "start": 1334.56, "end": 1339.76, "text": " It's very interesting that you allude to the fact that, you know, what kind of data is the model", "tokens": [51300, 467, 311, 588, 1880, 300, 291, 439, 2303, 281, 264, 1186, 300, 11, 291, 458, 11, 437, 733, 295, 1412, 307, 264, 2316, 51560], "temperature": 0.0, "avg_logprob": -0.12331977257361779, "compression_ratio": 1.772, "no_speech_prob": 0.0022120652720332146}, {"id": 228, "seek": 133976, "start": 1339.76, "end": 1346.72, "text": " good for, and so on. And now, deep learning models being essentially like really, as Tim said,", "tokens": [50364, 665, 337, 11, 293, 370, 322, 13, 400, 586, 11, 2452, 2539, 5245, 885, 4476, 411, 534, 11, 382, 7172, 848, 11, 50712], "temperature": 0.0, "avg_logprob": -0.102115967056968, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.03066255711019039}, {"id": 229, "seek": 133976, "start": 1346.72, "end": 1351.04, "text": " like big interpolators of arbitrary manifolds, do you think there is something", "tokens": [50712, 411, 955, 44902, 3391, 295, 23211, 8173, 31518, 11, 360, 291, 519, 456, 307, 746, 50928], "temperature": 0.0, "avg_logprob": -0.102115967056968, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.03066255711019039}, {"id": 230, "seek": 133976, "start": 1351.68, "end": 1358.48, "text": " common across the types of data we choose deep learning for? Or, you know, could we in fact", "tokens": [50960, 2689, 2108, 264, 3467, 295, 1412, 321, 2826, 2452, 2539, 337, 30, 1610, 11, 291, 458, 11, 727, 321, 294, 1186, 51300], "temperature": 0.0, "avg_logprob": -0.102115967056968, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.03066255711019039}, {"id": 231, "seek": 133976, "start": 1358.48, "end": 1365.28, "text": " use deep learning for most kinds of manifold dish data? Or do you think there is some kind of", "tokens": [51300, 764, 2452, 2539, 337, 881, 3685, 295, 47138, 5025, 1412, 30, 1610, 360, 291, 519, 456, 307, 512, 733, 295, 51640], "temperature": 0.0, "avg_logprob": -0.102115967056968, "compression_ratio": 1.7598039215686274, "no_speech_prob": 0.03066255711019039}, {"id": 232, "seek": 136528, "start": 1365.36, "end": 1370.72, "text": " specialness about natural signals that makes deep learning very attuned to them?", "tokens": [50368, 2121, 1287, 466, 3303, 12354, 300, 1669, 2452, 2539, 588, 951, 43703, 281, 552, 30, 50636], "temperature": 0.0, "avg_logprob": -0.1126399040222168, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005110122147016227}, {"id": 233, "seek": 136528, "start": 1371.44, "end": 1377.04, "text": " So I think most things are to some extent interpolative, which is why you can actually do", "tokens": [50672, 407, 286, 519, 881, 721, 366, 281, 512, 8396, 44902, 1166, 11, 597, 307, 983, 291, 393, 767, 360, 50952], "temperature": 0.0, "avg_logprob": -0.1126399040222168, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005110122147016227}, {"id": 234, "seek": 136528, "start": 1377.04, "end": 1380.96, "text": " lots of things with deep learning models. Doesn't necessarily mean it's always a good idea,", "tokens": [50952, 3195, 295, 721, 365, 2452, 2539, 5245, 13, 12955, 380, 4725, 914, 309, 311, 1009, 257, 665, 1558, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1126399040222168, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005110122147016227}, {"id": 235, "seek": 136528, "start": 1380.96, "end": 1385.2, "text": " but it's going to kind of work, right? You know, when people hear the word interpolation,", "tokens": [51148, 457, 309, 311, 516, 281, 733, 295, 589, 11, 558, 30, 509, 458, 11, 562, 561, 1568, 264, 1349, 44902, 399, 11, 51360], "temperature": 0.0, "avg_logprob": -0.1126399040222168, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005110122147016227}, {"id": 236, "seek": 136528, "start": 1385.2, "end": 1390.72, "text": " they tend to think about linear interpolation, that's what pops up in their mind. That's not", "tokens": [51360, 436, 3928, 281, 519, 466, 8213, 44902, 399, 11, 300, 311, 437, 16795, 493, 294, 641, 1575, 13, 663, 311, 406, 51636], "temperature": 0.0, "avg_logprob": -0.1126399040222168, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005110122147016227}, {"id": 237, "seek": 139072, "start": 1390.88, "end": 1395.44, "text": " always what deep learning models are doing, right? They're interpolating on this very complex,", "tokens": [50372, 1009, 437, 2452, 2539, 5245, 366, 884, 11, 558, 30, 814, 434, 44902, 990, 322, 341, 588, 3997, 11, 50600], "temperature": 0.0, "avg_logprob": -0.13668471843272717, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.003365502692759037}, {"id": 238, "seek": 139072, "start": 1395.44, "end": 1401.68, "text": " very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior.", "tokens": [50600, 588, 1090, 18795, 47138, 13, 400, 341, 17077, 588, 11, 291, 458, 11, 19071, 3289, 3997, 5223, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13668471843272717, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.003365502692759037}, {"id": 239, "seek": 139072, "start": 1402.4, "end": 1408.96, "text": " And in practice, it's always possible to an arbitrary discrete algorithm in a continuous", "tokens": [50948, 400, 294, 3124, 11, 309, 311, 1009, 1944, 281, 364, 23211, 27706, 9284, 294, 257, 10957, 51276], "temperature": 0.0, "avg_logprob": -0.13668471843272717, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.003365502692759037}, {"id": 240, "seek": 139072, "start": 1408.96, "end": 1414.08, "text": " manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory.", "tokens": [51276, 47138, 11, 558, 30, 467, 311, 406, 4725, 257, 665, 1558, 11, 457, 309, 311, 1009, 1944, 11, 412, 1935, 294, 5261, 13, 51532], "temperature": 0.0, "avg_logprob": -0.13668471843272717, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.003365502692759037}, {"id": 241, "seek": 139072, "start": 1414.08, "end": 1419.68, "text": " So for any program, you can imagine, you can ask, you know, is there a deep learning model that", "tokens": [51532, 407, 337, 604, 1461, 11, 291, 393, 3811, 11, 291, 393, 1029, 11, 291, 458, 11, 307, 456, 257, 2452, 2539, 2316, 300, 51812], "temperature": 0.0, "avg_logprob": -0.13668471843272717, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.003365502692759037}, {"id": 242, "seek": 141968, "start": 1419.68, "end": 1424.48, "text": " will encode some kind of approximation of it? And the answer is always yes, right?", "tokens": [50364, 486, 2058, 1429, 512, 733, 295, 28023, 295, 309, 30, 400, 264, 1867, 307, 1009, 2086, 11, 558, 30, 50604], "temperature": 0.0, "avg_logprob": -0.12554334167741302, "compression_ratio": 1.8146718146718146, "no_speech_prob": 0.0006941332831047475}, {"id": 243, "seek": 141968, "start": 1425.1200000000001, "end": 1431.04, "text": " Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But", "tokens": [50636, 10905, 281, 577, 291, 393, 1009, 2058, 1429, 364, 23211, 3909, 365, 264, 36810, 4088, 11, 558, 30, 583, 50932], "temperature": 0.0, "avg_logprob": -0.12554334167741302, "compression_ratio": 1.8146718146718146, "no_speech_prob": 0.0006941332831047475}, {"id": 244, "seek": 141968, "start": 1431.04, "end": 1435.1200000000001, "text": " there are, if you try to do that, actually, there are some issues with that. There are very much,", "tokens": [50932, 456, 366, 11, 498, 291, 853, 281, 360, 300, 11, 767, 11, 456, 366, 512, 2663, 365, 300, 13, 821, 366, 588, 709, 11, 51136], "temperature": 0.0, "avg_logprob": -0.12554334167741302, "compression_ratio": 1.8146718146718146, "no_speech_prob": 0.0006941332831047475}, {"id": 245, "seek": 141968, "start": 1435.1200000000001, "end": 1439.52, "text": " you know, some problems for which deep learning is good fit, some problems for which deep learning", "tokens": [51136, 291, 458, 11, 512, 2740, 337, 597, 2452, 2539, 307, 665, 3318, 11, 512, 2740, 337, 597, 2452, 2539, 51356], "temperature": 0.0, "avg_logprob": -0.12554334167741302, "compression_ratio": 1.8146718146718146, "no_speech_prob": 0.0006941332831047475}, {"id": 246, "seek": 141968, "start": 1439.52, "end": 1446.4, "text": " is not a good fit. In the limit, the extreme point is a space that is not interpolative at all,", "tokens": [51356, 307, 406, 257, 665, 3318, 13, 682, 264, 4948, 11, 264, 8084, 935, 307, 257, 1901, 300, 307, 406, 44902, 1166, 412, 439, 11, 51700], "temperature": 0.0, "avg_logprob": -0.12554334167741302, "compression_ratio": 1.8146718146718146, "no_speech_prob": 0.0006941332831047475}, {"id": 247, "seek": 144640, "start": 1446.4, "end": 1450.4, "text": " which is quite right, actually. You know, most spaces, even very discrete kind of spaces,", "tokens": [50364, 597, 307, 1596, 558, 11, 767, 13, 509, 458, 11, 881, 7673, 11, 754, 588, 27706, 733, 295, 7673, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13551315818865275, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.0045205047354102135}, {"id": 248, "seek": 144640, "start": 1450.4, "end": 1456.96, "text": " do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance,", "tokens": [50564, 360, 362, 11, 291, 458, 11, 512, 11663, 295, 44902, 267, 8477, 13, 407, 411, 11, 457, 472, 1365, 576, 312, 11, 337, 5197, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13551315818865275, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.0045205047354102135}, {"id": 249, "seek": 144640, "start": 1456.96, "end": 1464.4, "text": " trying to train a deep learning model to predict the next prime number, right? Or to tell whether", "tokens": [50892, 1382, 281, 3847, 257, 2452, 2539, 2316, 281, 6069, 264, 958, 5835, 1230, 11, 558, 30, 1610, 281, 980, 1968, 51264], "temperature": 0.0, "avg_logprob": -0.13551315818865275, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.0045205047354102135}, {"id": 250, "seek": 144640, "start": 1464.4, "end": 1470.48, "text": " a number is a prime number. As you cannot actually do that, the best you can do is memorize the", "tokens": [51264, 257, 1230, 307, 257, 5835, 1230, 13, 1018, 291, 2644, 767, 360, 300, 11, 264, 1151, 291, 393, 360, 307, 27478, 264, 51568], "temperature": 0.0, "avg_logprob": -0.13551315818865275, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.0045205047354102135}, {"id": 251, "seek": 147048, "start": 1470.48, "end": 1476.8, "text": " train data point, because the space of prime numbers is not interpreted at all. So your deep", "tokens": [50364, 3847, 1412, 935, 11, 570, 264, 1901, 295, 5835, 3547, 307, 406, 26749, 412, 439, 13, 407, 428, 2452, 50680], "temperature": 0.0, "avg_logprob": -0.10941683394568306, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0020813560113310814}, {"id": 252, "seek": 147048, "start": 1476.8, "end": 1483.2, "text": " learning model will always have zero generalization power. But that's actually quite rare. This is", "tokens": [50680, 2539, 2316, 486, 1009, 362, 4018, 2674, 2144, 1347, 13, 583, 300, 311, 767, 1596, 5892, 13, 639, 307, 51000], "temperature": 0.0, "avg_logprob": -0.10941683394568306, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0020813560113310814}, {"id": 253, "seek": 147048, "start": 1483.2, "end": 1489.68, "text": " kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic", "tokens": [51000, 733, 295, 364, 8084, 1389, 13, 4534, 2740, 11, 754, 2740, 300, 366, 17434, 11, 27706, 11, 9284, 299, 51324], "temperature": 0.0, "avg_logprob": -0.10941683394568306, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0020813560113310814}, {"id": 254, "seek": 147048, "start": 1489.68, "end": 1494.24, "text": " problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily", "tokens": [51324, 2740, 11, 456, 486, 312, 512, 2372, 295, 44902, 399, 300, 291, 393, 360, 11, 558, 30, 583, 300, 1177, 380, 4725, 51552], "temperature": 0.0, "avg_logprob": -0.10941683394568306, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0020813560113310814}, {"id": 255, "seek": 147048, "start": 1494.24, "end": 1499.1200000000001, "text": " mean that it's a good idea to try to solve, you know, such problems with deep learning models", "tokens": [51552, 914, 300, 309, 311, 257, 665, 1558, 281, 853, 281, 5039, 11, 291, 458, 11, 1270, 2740, 365, 2452, 2539, 5245, 51796], "temperature": 0.0, "avg_logprob": -0.10941683394568306, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0020813560113310814}, {"id": 256, "seek": 149912, "start": 1499.12, "end": 1504.0, "text": " for deep learning to be a good idea. You need a very, you need very much the manifold", "tokens": [50364, 337, 2452, 2539, 281, 312, 257, 665, 1558, 13, 509, 643, 257, 588, 11, 291, 643, 588, 709, 264, 47138, 50608], "temperature": 0.0, "avg_logprob": -0.19820434356404243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 0.000763538118917495}, {"id": 257, "seek": 149912, "start": 1504.0, "end": 1509.36, "text": " level as it is to apply. So it works best for perception problems. Any problem that humans", "tokens": [50608, 1496, 382, 309, 307, 281, 3079, 13, 407, 309, 1985, 1151, 337, 12860, 2740, 13, 2639, 1154, 300, 6255, 50876], "temperature": 0.0, "avg_logprob": -0.19820434356404243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 0.000763538118917495}, {"id": 258, "seek": 149912, "start": 1509.36, "end": 1514.32, "text": " can solve via pure intuition or perception is probably a good fit for deep learning. But any", "tokens": [50876, 393, 5039, 5766, 6075, 24002, 420, 12860, 307, 1391, 257, 665, 3318, 337, 2452, 2539, 13, 583, 604, 51124], "temperature": 0.0, "avg_logprob": -0.19820434356404243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 0.000763538118917495}, {"id": 259, "seek": 149912, "start": 1514.32, "end": 1519.6, "text": " problem where you need, you know, high level explicit step by step reasoning is probably", "tokens": [51124, 1154, 689, 291, 643, 11, 291, 458, 11, 1090, 1496, 13691, 1823, 538, 1823, 21577, 307, 1391, 51388], "temperature": 0.0, "avg_logprob": -0.19820434356404243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 0.000763538118917495}, {"id": 260, "seek": 149912, "start": 1519.6, "end": 1524.2399999999998, "text": " a bad fit for deep learning. And, you know, 99% of what today software engineers solve,", "tokens": [51388, 257, 1578, 3318, 337, 2452, 2539, 13, 400, 11, 291, 458, 11, 11803, 4, 295, 437, 965, 4722, 11955, 5039, 11, 51620], "temperature": 0.0, "avg_logprob": -0.19820434356404243, "compression_ratio": 1.8506224066390042, "no_speech_prob": 0.000763538118917495}, {"id": 261, "seek": 152424, "start": 1524.96, "end": 1530.0, "text": " the writing code is going to be a bad fit for deep learning. That doesn't mean that there", "tokens": [50400, 264, 3579, 3089, 307, 516, 281, 312, 257, 1578, 3318, 337, 2452, 2539, 13, 663, 1177, 380, 914, 300, 456, 50652], "temperature": 0.0, "avg_logprob": -0.11539873070673111, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.006471056025475264}, {"id": 262, "seek": 152424, "start": 1530.0, "end": 1535.44, "text": " wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm", "tokens": [50652, 2759, 380, 312, 11, 291, 458, 11, 29400, 11, 257, 2452, 2539, 2316, 300, 393, 12240, 264, 912, 9284, 50924], "temperature": 0.0, "avg_logprob": -0.11539873070673111, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.006471056025475264}, {"id": 263, "seek": 152424, "start": 1535.44, "end": 1541.04, "text": " in a smooth manifold. This is always possible to some extent, right? But there are very significant", "tokens": [50924, 294, 257, 5508, 47138, 13, 639, 307, 1009, 1944, 281, 512, 8396, 11, 558, 30, 583, 456, 366, 588, 4776, 51204], "temperature": 0.0, "avg_logprob": -0.11539873070673111, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.006471056025475264}, {"id": 264, "seek": 152424, "start": 1541.04, "end": 1546.08, "text": " issues with attempting to do this. I like just because something is theoretically possible doesn't", "tokens": [51204, 2663, 365, 22001, 281, 360, 341, 13, 286, 411, 445, 570, 746, 307, 29400, 1944, 1177, 380, 51456], "temperature": 0.0, "avg_logprob": -0.11539873070673111, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.006471056025475264}, {"id": 265, "seek": 152424, "start": 1546.08, "end": 1552.24, "text": " mean you should actually do it. I think we might be not being careful enough when we say what we", "tokens": [51456, 914, 291, 820, 767, 360, 309, 13, 286, 519, 321, 1062, 312, 406, 885, 5026, 1547, 562, 321, 584, 437, 321, 51764], "temperature": 0.0, "avg_logprob": -0.11539873070673111, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.006471056025475264}, {"id": 266, "seek": 155224, "start": 1552.24, "end": 1559.2, "text": " mean by program. Because, for example, if I take program to be the universal sense like a program", "tokens": [50364, 914, 538, 1461, 13, 1436, 11, 337, 1365, 11, 498, 286, 747, 1461, 281, 312, 264, 11455, 2020, 411, 257, 1461, 50712], "temperature": 0.0, "avg_logprob": -0.0706613944924396, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0032725536730140448}, {"id": 267, "seek": 155224, "start": 1559.2, "end": 1564.24, "text": " is something that can run on a Turing machine, for example, because of the fact that that type", "tokens": [50712, 307, 746, 300, 393, 1190, 322, 257, 314, 1345, 3479, 11, 337, 1365, 11, 570, 295, 264, 1186, 300, 300, 2010, 50964], "temperature": 0.0, "avg_logprob": -0.0706613944924396, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0032725536730140448}, {"id": 268, "seek": 155224, "start": 1564.24, "end": 1571.2, "text": " of program actually has access to unbounded time and memory computation. It's impossible in the", "tokens": [50964, 295, 1461, 767, 575, 2105, 281, 517, 18767, 292, 565, 293, 4675, 24903, 13, 467, 311, 6243, 294, 264, 51312], "temperature": 0.0, "avg_logprob": -0.0706613944924396, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0032725536730140448}, {"id": 269, "seek": 155224, "start": 1571.2, "end": 1577.28, "text": " general sense to encode that in any finite neural network, like I can write a very short piece of", "tokens": [51312, 2674, 2020, 281, 2058, 1429, 300, 294, 604, 19362, 18161, 3209, 11, 411, 286, 393, 2464, 257, 588, 2099, 2522, 295, 51616], "temperature": 0.0, "avg_logprob": -0.0706613944924396, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0032725536730140448}, {"id": 270, "seek": 157728, "start": 1577.28, "end": 1582.96, "text": " code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do", "tokens": [50364, 3089, 20864, 314, 1345, 3479, 393, 5598, 11, 291, 458, 11, 264, 297, 392, 14293, 295, 17741, 13, 467, 311, 6243, 281, 360, 50648], "temperature": 0.0, "avg_logprob": -0.14231520551976148, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.008576271124184132}, {"id": 271, "seek": 157728, "start": 1582.96, "end": 1587.6, "text": " that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I", "tokens": [50648, 300, 365, 604, 19362, 18161, 3209, 13, 6068, 291, 3986, 30, 865, 11, 3122, 13, 7021, 13, 1033, 11, 570, 286, 50880], "temperature": 0.0, "avg_logprob": -0.14231520551976148, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.008576271124184132}, {"id": 272, "seek": 157728, "start": 1587.6, "end": 1591.36, "text": " think that's like a big source of confusion often time with these statements that like, you know,", "tokens": [50880, 519, 300, 311, 411, 257, 955, 4009, 295, 15075, 2049, 565, 365, 613, 12363, 300, 411, 11, 291, 458, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14231520551976148, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.008576271124184132}, {"id": 273, "seek": 157728, "start": 1591.92, "end": 1596.48, "text": " oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural", "tokens": [51096, 1954, 11, 18161, 9590, 366, 314, 1345, 3566, 13, 1042, 11, 572, 11, 436, 434, 406, 13, 509, 458, 11, 498, 291, 362, 257, 18161, 51324], "temperature": 0.0, "avg_logprob": -0.14231520551976148, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.008576271124184132}, {"id": 274, "seek": 157728, "start": 1596.48, "end": 1602.48, "text": " Turing machine, which is a neural network that's the finite state machine piece of a Turing machine,", "tokens": [51324, 314, 1345, 3479, 11, 597, 307, 257, 18161, 3209, 300, 311, 264, 19362, 1785, 3479, 2522, 295, 257, 314, 1345, 3479, 11, 51624], "temperature": 0.0, "avg_logprob": -0.14231520551976148, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.008576271124184132}, {"id": 275, "seek": 160248, "start": 1603.04, "end": 1607.6, "text": " that can be Turing complete. But in the general case, you know, finite neural networks, which is", "tokens": [50392, 300, 393, 312, 314, 1345, 3566, 13, 583, 294, 264, 2674, 1389, 11, 291, 458, 11, 19362, 18161, 9590, 11, 597, 307, 50620], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 276, "seek": 160248, "start": 1607.6, "end": 1612.32, "text": " what everyone means by neural networks, are not Turing complete. And it actually has practical", "tokens": [50620, 437, 1518, 1355, 538, 18161, 9590, 11, 366, 406, 314, 1345, 3566, 13, 400, 309, 767, 575, 8496, 50856], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 277, "seek": 160248, "start": 1612.32, "end": 1617.84, "text": " effects, right? This is why we see this sort of explosion and the number of parameters to kind of,", "tokens": [50856, 5065, 11, 558, 30, 639, 307, 983, 321, 536, 341, 1333, 295, 15673, 293, 264, 1230, 295, 9834, 281, 733, 295, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 278, "seek": 160248, "start": 1617.84, "end": 1623.52, "text": " you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only", "tokens": [51132, 291, 458, 11, 722, 281, 9021, 13, 865, 11, 3122, 13, 2319, 6856, 509, 434, 7696, 558, 13, 407, 321, 434, 787, 51416], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 279, "seek": 160248, "start": 1623.52, "end": 1627.3600000000001, "text": " interested in realistic programs, like the sort of programs that start to engineer with right,", "tokens": [51416, 3102, 294, 12465, 4268, 11, 411, 264, 1333, 295, 4268, 300, 722, 281, 11403, 365, 558, 11, 51608], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 280, "seek": 160248, "start": 1627.3600000000001, "end": 1631.3600000000001, "text": " for instance. And we're only interested in realistic neural networks. And by the way,", "tokens": [51608, 337, 5197, 13, 400, 321, 434, 787, 3102, 294, 12465, 18161, 9590, 13, 400, 538, 264, 636, 11, 51808], "temperature": 0.0, "avg_logprob": -0.1121984352742819, "compression_ratio": 1.8398692810457515, "no_speech_prob": 0.0018965043127536774}, {"id": 281, "seek": 163136, "start": 1631.4399999999998, "end": 1635.36, "text": " the constraints that we have on neural networks are actually much stronger than asking,", "tokens": [50368, 264, 18491, 300, 321, 362, 322, 18161, 9590, 366, 767, 709, 7249, 813, 3365, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13377773877486443, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0007197980303317308}, {"id": 282, "seek": 163136, "start": 1635.36, "end": 1641.28, "text": " given this program that I have, is there a neural network that could embed it in a continuous", "tokens": [50564, 2212, 341, 1461, 300, 286, 362, 11, 307, 456, 257, 18161, 3209, 300, 727, 12240, 309, 294, 257, 10957, 50860], "temperature": 0.0, "avg_logprob": -0.13377773877486443, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0007197980303317308}, {"id": 283, "seek": 163136, "start": 1641.28, "end": 1646.24, "text": " manifold? The constraint is actually, is there a neural network that could not only represent it,", "tokens": [50860, 47138, 30, 440, 25534, 307, 767, 11, 307, 456, 257, 18161, 3209, 300, 727, 406, 787, 2906, 309, 11, 51108], "temperature": 0.0, "avg_logprob": -0.13377773877486443, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0007197980303317308}, {"id": 284, "seek": 163136, "start": 1646.24, "end": 1652.4799999999998, "text": " but that could learn this embedding of the program from there. And this is a several orders of", "tokens": [51108, 457, 300, 727, 1466, 341, 12240, 3584, 295, 264, 1461, 490, 456, 13, 400, 341, 307, 257, 2940, 9470, 295, 51420], "temperature": 0.0, "avg_logprob": -0.13377773877486443, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0007197980303317308}, {"id": 285, "seek": 163136, "start": 1652.4799999999998, "end": 1659.36, "text": " magnitude harder, right? Learnability is a big problem because you're fitting your manifold via", "tokens": [51420, 15668, 6081, 11, 558, 30, 17216, 2310, 307, 257, 955, 1154, 570, 291, 434, 15669, 428, 47138, 5766, 51764], "temperature": 0.0, "avg_logprob": -0.13377773877486443, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0007197980303317308}, {"id": 286, "seek": 165936, "start": 1659.36, "end": 1667.36, "text": " gradient descent, right? And if the structure you're trying to fit is too discrete, with too", "tokens": [50364, 16235, 23475, 11, 558, 30, 400, 498, 264, 3877, 291, 434, 1382, 281, 3318, 307, 886, 27706, 11, 365, 886, 50764], "temperature": 0.0, "avg_logprob": -0.14615575472513834, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.002209312282502651}, {"id": 287, "seek": 165936, "start": 1667.36, "end": 1673.84, "text": " big discontinuities, gradient descent will not work at all. And the best you can do is, again,", "tokens": [50764, 955, 31420, 84, 1088, 11, 16235, 23475, 486, 406, 589, 412, 439, 13, 400, 264, 1151, 291, 393, 360, 307, 11, 797, 11, 51088], "temperature": 0.0, "avg_logprob": -0.14615575472513834, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.002209312282502651}, {"id": 288, "seek": 165936, "start": 1673.84, "end": 1680.9599999999998, "text": " just memorize the train data. So I can maybe give you a concrete example to kind of ground", "tokens": [51088, 445, 27478, 264, 3847, 1412, 13, 407, 286, 393, 1310, 976, 291, 257, 9859, 1365, 281, 733, 295, 2727, 51444], "temperature": 0.0, "avg_logprob": -0.14615575472513834, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.002209312282502651}, {"id": 289, "seek": 165936, "start": 1680.9599999999998, "end": 1688.4799999999998, "text": " our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do", "tokens": [51444, 527, 5017, 510, 13, 407, 294, 7546, 11, 512, 1277, 295, 3892, 11, 370, 702, 1315, 307, 11, 415, 1143, 591, 6985, 281, 360, 51820], "temperature": 0.0, "avg_logprob": -0.14615575472513834, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.002209312282502651}, {"id": 290, "seek": 168848, "start": 1688.48, "end": 1694.16, "text": " something pretty cool, which actually became a cool example on the Keras website. He used a LSTM", "tokens": [50364, 746, 1238, 1627, 11, 597, 767, 3062, 257, 1627, 1365, 322, 264, 591, 6985, 3144, 13, 634, 1143, 257, 441, 6840, 44, 50648], "temperature": 0.0, "avg_logprob": -0.1511599752638075, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0017129749758169055}, {"id": 291, "seek": 168848, "start": 1694.16, "end": 1702.16, "text": " model to multiply numbers, but not like numbers multiplied by value, but the input of the model", "tokens": [50648, 2316, 281, 12972, 3547, 11, 457, 406, 411, 3547, 17207, 538, 2158, 11, 457, 264, 4846, 295, 264, 2316, 51048], "temperature": 0.0, "avg_logprob": -0.1511599752638075, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0017129749758169055}, {"id": 292, "seek": 168848, "start": 1702.16, "end": 1707.3600000000001, "text": " would be strings, like two strings, strings of digits. And the LSTM will actually learn the", "tokens": [51048, 576, 312, 13985, 11, 411, 732, 13985, 11, 13985, 295, 27011, 13, 400, 264, 441, 6840, 44, 486, 767, 1466, 264, 51308], "temperature": 0.0, "avg_logprob": -0.1511599752638075, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0017129749758169055}, {"id": 293, "seek": 168848, "start": 1707.3600000000001, "end": 1712.24, "text": " multiplication algorithm for like multiplying three digits and three digits numbers, kind of the", "tokens": [51308, 27290, 9284, 337, 411, 30955, 1045, 27011, 293, 1045, 27011, 3547, 11, 733, 295, 264, 51552], "temperature": 0.0, "avg_logprob": -0.1511599752638075, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0017129749758169055}, {"id": 294, "seek": 168848, "start": 1712.24, "end": 1718.0, "text": " sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably,", "tokens": [51552, 1333, 295, 9284, 321, 576, 1466, 294, 6194, 1395, 11, 558, 11, 281, 360, 27290, 13, 400, 37381, 11, 51840], "temperature": 0.0, "avg_logprob": -0.1511599752638075, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0017129749758169055}, {"id": 295, "seek": 171800, "start": 1718.08, "end": 1722.8, "text": " that worked, right? It works just fine. So you can train a deep learning model to learn this", "tokens": [50368, 300, 2732, 11, 558, 30, 467, 1985, 445, 2489, 13, 407, 291, 393, 3847, 257, 2452, 2539, 2316, 281, 1466, 341, 50604], "temperature": 0.0, "avg_logprob": -0.12519424333484894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0007074412424117327}, {"id": 296, "seek": 171800, "start": 1722.8, "end": 1726.88, "text": " algorithm. And you could, of course, train a transformer model to do the same. It will actually", "tokens": [50604, 9284, 13, 400, 291, 727, 11, 295, 1164, 11, 3847, 257, 31782, 2316, 281, 360, 264, 912, 13, 467, 486, 767, 50808], "temperature": 0.0, "avg_logprob": -0.12519424333484894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0007074412424117327}, {"id": 297, "seek": 171800, "start": 1726.88, "end": 1732.88, "text": " be probably significantly more efficient. So that works. That comes with a number of downsides.", "tokens": [50808, 312, 1391, 10591, 544, 7148, 13, 407, 300, 1985, 13, 663, 1487, 365, 257, 1230, 295, 21554, 1875, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12519424333484894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0007074412424117327}, {"id": 298, "seek": 171800, "start": 1732.88, "end": 1736.88, "text": " So first, in order to train that algorithm, which is very simple, you're going to need", "tokens": [51108, 407, 700, 11, 294, 1668, 281, 3847, 300, 9284, 11, 597, 307, 588, 2199, 11, 291, 434, 516, 281, 643, 51308], "temperature": 0.0, "avg_logprob": -0.12519424333484894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0007074412424117327}, {"id": 299, "seek": 171800, "start": 1736.88, "end": 1743.28, "text": " thousands and thousands of examples of different strategic numbers. And once you've trained your", "tokens": [51308, 5383, 293, 5383, 295, 5110, 295, 819, 10924, 3547, 13, 400, 1564, 291, 600, 8895, 428, 51628], "temperature": 0.0, "avg_logprob": -0.12519424333484894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0007074412424117327}, {"id": 300, "seek": 174328, "start": 1743.28, "end": 1750.3999999999999, "text": " algorithm, because the actual algorithm was embedded in the neural network, it does generalize to", "tokens": [50364, 9284, 11, 570, 264, 3539, 9284, 390, 16741, 294, 264, 18161, 3209, 11, 309, 775, 2674, 1125, 281, 50720], "temperature": 0.0, "avg_logprob": -0.11024978811090642, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.001204732689075172}, {"id": 301, "seek": 174328, "start": 1750.3999999999999, "end": 1755.6, "text": " never see before digits, right? So it's actually learning the algorithm. It's not just learning,", "tokens": [50720, 1128, 536, 949, 27011, 11, 558, 30, 407, 309, 311, 767, 2539, 264, 9284, 13, 467, 311, 406, 445, 2539, 11, 50980], "temperature": 0.0, "avg_logprob": -0.11024978811090642, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.001204732689075172}, {"id": 302, "seek": 174328, "start": 1755.6, "end": 1761.52, "text": " I'm just not memorizing the data. But the thing is, because the embedding of an algorithm,", "tokens": [50980, 286, 478, 445, 406, 10560, 3319, 264, 1412, 13, 583, 264, 551, 307, 11, 570, 264, 12240, 3584, 295, 364, 9284, 11, 51276], "temperature": 0.0, "avg_logprob": -0.11024978811090642, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.001204732689075172}, {"id": 303, "seek": 174328, "start": 1761.52, "end": 1765.84, "text": " the embedding of a discrete structure in the continuous space, is not the same thing as the", "tokens": [51276, 264, 12240, 3584, 295, 257, 27706, 3877, 294, 264, 10957, 1901, 11, 307, 406, 264, 912, 551, 382, 264, 51492], "temperature": 0.0, "avg_logprob": -0.11024978811090642, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.001204732689075172}, {"id": 304, "seek": 174328, "start": 1765.84, "end": 1771.12, "text": " original discrete object. There are glitches in your deep learning network, unless that's something", "tokens": [51492, 3380, 27706, 2657, 13, 821, 366, 23552, 279, 294, 428, 2452, 2539, 3209, 11, 5969, 300, 311, 746, 51756], "temperature": 0.0, "avg_logprob": -0.11024978811090642, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.001204732689075172}, {"id": 305, "seek": 177112, "start": 1771.12, "end": 1775.52, "text": " you could have found via program synthesis, for instance, it's not going to be correct 100% of", "tokens": [50364, 291, 727, 362, 1352, 5766, 1461, 30252, 11, 337, 5197, 11, 309, 311, 406, 516, 281, 312, 3006, 2319, 4, 295, 50584], "temperature": 0.0, "avg_logprob": -0.12690727650618353, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0010315149556845427}, {"id": 306, "seek": 177112, "start": 1775.52, "end": 1781.52, "text": " the time, it's going to be correct 95% of the time. In much the same way that if you try to", "tokens": [50584, 264, 565, 11, 309, 311, 516, 281, 312, 3006, 13420, 4, 295, 264, 565, 13, 682, 709, 264, 912, 636, 300, 498, 291, 853, 281, 50884], "temperature": 0.0, "avg_logprob": -0.12690727650618353, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0010315149556845427}, {"id": 307, "seek": 177112, "start": 1781.52, "end": 1786.56, "text": " encode a very discrete object via the Fourier transform, it's not going to be correct. 100%", "tokens": [50884, 2058, 1429, 257, 588, 27706, 2657, 5766, 264, 36810, 4088, 11, 309, 311, 406, 516, 281, 312, 3006, 13, 2319, 4, 51136], "temperature": 0.0, "avg_logprob": -0.12690727650618353, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0010315149556845427}, {"id": 308, "seek": 177112, "start": 1786.56, "end": 1790.7199999999998, "text": " of the time is going to be an approximation and around sharp angles, it's actually going to be", "tokens": [51136, 295, 264, 565, 307, 516, 281, 312, 364, 28023, 293, 926, 8199, 14708, 11, 309, 311, 767, 516, 281, 312, 51344], "temperature": 0.0, "avg_logprob": -0.12690727650618353, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0010315149556845427}, {"id": 309, "seek": 177112, "start": 1790.7199999999998, "end": 1796.0, "text": " wrong. And very importantly, and this is really like the algorithm that you've painstakingly", "tokens": [51344, 2085, 13, 400, 588, 8906, 11, 293, 341, 307, 534, 411, 264, 9284, 300, 291, 600, 1822, 372, 2456, 356, 51608], "temperature": 0.0, "avg_logprob": -0.12690727650618353, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0010315149556845427}, {"id": 310, "seek": 179600, "start": 1796.08, "end": 1802.8, "text": " embedded into your deep learning model via exposure to data, does only, it does not generalize very", "tokens": [50368, 16741, 666, 428, 2452, 2539, 2316, 5766, 10420, 281, 1412, 11, 775, 787, 11, 309, 775, 406, 2674, 1125, 588, 50704], "temperature": 0.0, "avg_logprob": -0.1450500825865079, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.001981583423912525}, {"id": 311, "seek": 179600, "start": 1802.8, "end": 1808.0, "text": " well, it only does local generalization, meaning that if you train it with three, to multiply", "tokens": [50704, 731, 11, 309, 787, 775, 2654, 2674, 2144, 11, 3620, 300, 498, 291, 3847, 309, 365, 1045, 11, 281, 12972, 50964], "temperature": 0.0, "avg_logprob": -0.1450500825865079, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.001981583423912525}, {"id": 312, "seek": 179600, "start": 1808.0, "end": 1812.48, "text": " three digit numbers, and then you send it a five digit number, is it going to work? No,", "tokens": [50964, 1045, 14293, 3547, 11, 293, 550, 291, 2845, 309, 257, 1732, 14293, 1230, 11, 307, 309, 516, 281, 589, 30, 883, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1450500825865079, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.001981583423912525}, {"id": 313, "seek": 179600, "start": 1812.48, "end": 1816.4, "text": " absolutely not. And not only is it not going to work, but you could not in fact,", "tokens": [51188, 3122, 406, 13, 400, 406, 787, 307, 309, 406, 516, 281, 589, 11, 457, 291, 727, 406, 294, 1186, 11, 51384], "temperature": 0.0, "avg_logprob": -0.1450500825865079, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.001981583423912525}, {"id": 314, "seek": 179600, "start": 1818.24, "end": 1823.76, "text": " few shots fine tune your algorithm to learn to handle five digits, seven digits and so on.", "tokens": [51476, 1326, 8305, 2489, 10864, 428, 9284, 281, 1466, 281, 4813, 1732, 27011, 11, 3407, 27011, 293, 370, 322, 13, 51752], "temperature": 0.0, "avg_logprob": -0.1450500825865079, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.001981583423912525}, {"id": 315, "seek": 182376, "start": 1823.76, "end": 1827.76, "text": " If you want to fine tune your algorithm, you're going to need thousands, maybe millions", "tokens": [50364, 759, 291, 528, 281, 2489, 10864, 428, 9284, 11, 291, 434, 516, 281, 643, 5383, 11, 1310, 6803, 50564], "temperature": 0.0, "avg_logprob": -0.1600674654530213, "compression_ratio": 1.6389891696750902, "no_speech_prob": 0.0010763396276161075}, {"id": 316, "seek": 182376, "start": 1828.64, "end": 1835.2, "text": " of examples, right? So it's all local generalization. And lastly, it's super inefficient,", "tokens": [50608, 295, 5110, 11, 558, 30, 407, 309, 311, 439, 2654, 2674, 2144, 13, 400, 16386, 11, 309, 311, 1687, 43495, 11, 50936], "temperature": 0.0, "avg_logprob": -0.1600674654530213, "compression_ratio": 1.6389891696750902, "no_speech_prob": 0.0010763396276161075}, {"id": 317, "seek": 182376, "start": 1835.2, "end": 1840.08, "text": " like I think we can all agree with this, that multiplication is not like it's not", "tokens": [50936, 411, 286, 519, 321, 393, 439, 3986, 365, 341, 11, 300, 27290, 307, 406, 411, 309, 311, 406, 51180], "temperature": 0.0, "avg_logprob": -0.1600674654530213, "compression_ratio": 1.6389891696750902, "no_speech_prob": 0.0010763396276161075}, {"id": 318, "seek": 182376, "start": 1841.44, "end": 1846.4, "text": " a clever use of an LSTM, it's you're burning tons of resources for something that's actually", "tokens": [51248, 257, 13494, 764, 295, 364, 441, 6840, 44, 11, 309, 311, 291, 434, 9488, 9131, 295, 3593, 337, 746, 300, 311, 767, 51496], "temperature": 0.0, "avg_logprob": -0.1600674654530213, "compression_ratio": 1.6389891696750902, "no_speech_prob": 0.0010763396276161075}, {"id": 319, "seek": 182376, "start": 1846.4, "end": 1851.92, "text": " super easy. And you can compare that, like since we are talking about pros and cons of deep learning,", "tokens": [51496, 1687, 1858, 13, 400, 291, 393, 6794, 300, 11, 411, 1670, 321, 366, 1417, 466, 6267, 293, 1014, 295, 2452, 2539, 11, 51772], "temperature": 0.0, "avg_logprob": -0.1600674654530213, "compression_ratio": 1.6389891696750902, "no_speech_prob": 0.0010763396276161075}, {"id": 320, "seek": 185192, "start": 1851.92, "end": 1856.48, "text": " you can compare that to what you could get with a program synthesis engine. Like I don't want to", "tokens": [50364, 291, 393, 6794, 300, 281, 437, 291, 727, 483, 365, 257, 1461, 30252, 2848, 13, 1743, 286, 500, 380, 528, 281, 50592], "temperature": 0.0, "avg_logprob": -0.10664455978958695, "compression_ratio": 1.936, "no_speech_prob": 0.002269656164571643}, {"id": 321, "seek": 185192, "start": 1856.48, "end": 1860.48, "text": " compare to what you could get with a human written algorithm, because kind of the point of deep", "tokens": [50592, 6794, 281, 437, 291, 727, 483, 365, 257, 1952, 3720, 9284, 11, 570, 733, 295, 264, 935, 295, 2452, 50792], "temperature": 0.0, "avg_logprob": -0.10664455978958695, "compression_ratio": 1.936, "no_speech_prob": 0.002269656164571643}, {"id": 322, "seek": 185192, "start": 1860.48, "end": 1865.92, "text": " learning is that it enables you to develop programs that you could not otherwise write by hand.", "tokens": [50792, 2539, 307, 300, 309, 17077, 291, 281, 1499, 4268, 300, 291, 727, 406, 5911, 2464, 538, 1011, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10664455978958695, "compression_ratio": 1.936, "no_speech_prob": 0.002269656164571643}, {"id": 323, "seek": 185192, "start": 1865.92, "end": 1870.4, "text": " So the right point of comparison is actually what you could do with deep learning versus what you", "tokens": [51064, 407, 264, 558, 935, 295, 9660, 307, 767, 437, 291, 727, 360, 365, 2452, 2539, 5717, 437, 291, 51288], "temperature": 0.0, "avg_logprob": -0.10664455978958695, "compression_ratio": 1.936, "no_speech_prob": 0.002269656164571643}, {"id": 324, "seek": 185192, "start": 1870.4, "end": 1877.28, "text": " could do with discrete program synthesis based on discrete search and the DSL. And if you were to", "tokens": [51288, 727, 360, 365, 27706, 1461, 30252, 2361, 322, 27706, 3164, 293, 264, 15816, 43, 13, 400, 498, 291, 645, 281, 51632], "temperature": 0.0, "avg_logprob": -0.10664455978958695, "compression_ratio": 1.936, "no_speech_prob": 0.002269656164571643}, {"id": 325, "seek": 187728, "start": 1877.28, "end": 1883.76, "text": " use a program synthesis to solve the multiplication problem, so you would find a solution, even a", "tokens": [50364, 764, 257, 1461, 30252, 281, 5039, 264, 27290, 1154, 11, 370, 291, 576, 915, 257, 3827, 11, 754, 257, 50688], "temperature": 0.0, "avg_logprob": -0.14606615274894139, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0016439032042399049}, {"id": 326, "seek": 187728, "start": 1883.76, "end": 1890.24, "text": " very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to", "tokens": [50688, 588, 10654, 2848, 300, 775, 445, 411, 1310, 257, 1804, 6916, 11, 1310, 257, 6367, 13, 400, 341, 15816, 43, 307, 516, 281, 51012], "temperature": 0.0, "avg_logprob": -0.14606615274894139, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0016439032042399049}, {"id": 327, "seek": 187728, "start": 1890.24, "end": 1895.12, "text": " find it, it can find it with a handful of examples, you're not going to need thousands of examples,", "tokens": [51012, 915, 309, 11, 309, 393, 915, 309, 365, 257, 16458, 295, 5110, 11, 291, 434, 406, 516, 281, 643, 5383, 295, 5110, 11, 51256], "temperature": 0.0, "avg_logprob": -0.14606615274894139, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0016439032042399049}, {"id": 328, "seek": 187728, "start": 1895.12, "end": 1899.28, "text": " like in the deep learning case, you're going to need maybe five. And the program you get out of it", "tokens": [51256, 411, 294, 264, 2452, 2539, 1389, 11, 291, 434, 516, 281, 643, 1310, 1732, 13, 400, 264, 1461, 291, 483, 484, 295, 309, 51464], "temperature": 0.0, "avg_logprob": -0.14606615274894139, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0016439032042399049}, {"id": 329, "seek": 187728, "start": 1899.28, "end": 1905.04, "text": " is going to be exact, because it is the exact discrete algorithm, it is not a continuous", "tokens": [51464, 307, 516, 281, 312, 1900, 11, 570, 309, 307, 264, 1900, 27706, 9284, 11, 309, 307, 406, 257, 10957, 51752], "temperature": 0.0, "avg_logprob": -0.14606615274894139, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0016439032042399049}, {"id": 330, "seek": 190504, "start": 1905.04, "end": 1909.76, "text": " embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight,", "tokens": [50364, 12240, 3584, 295, 309, 13, 407, 309, 775, 406, 362, 23552, 279, 11, 309, 23930, 264, 3006, 1867, 13, 467, 486, 312, 22052, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 331, "seek": 190504, "start": 1909.76, "end": 1913.68, "text": " so it will be very efficient, you know, and like the LCM or transformer model,", "tokens": [50600, 370, 309, 486, 312, 588, 7148, 11, 291, 458, 11, 293, 411, 264, 42198, 44, 420, 31782, 2316, 11, 50796], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 332, "seek": 190504, "start": 1913.68, "end": 1919.28, "text": " and crucially, it's going to generalize. So if you develop it only from three digit numbers,", "tokens": [50796, 293, 5140, 1909, 11, 309, 311, 516, 281, 2674, 1125, 13, 407, 498, 291, 1499, 309, 787, 490, 1045, 14293, 3547, 11, 51076], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 333, "seek": 190504, "start": 1919.28, "end": 1923.04, "text": " maybe there will be something inside it that will hardcore the assumption that they're dealing with", "tokens": [51076, 1310, 456, 486, 312, 746, 1854, 309, 300, 486, 28196, 264, 15302, 300, 436, 434, 6260, 365, 51264], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 334, "seek": 190504, "start": 1923.04, "end": 1928.24, "text": " three digit numbers. But even if that's the case, you can take it and automatically learn", "tokens": [51264, 1045, 14293, 3547, 13, 583, 754, 498, 300, 311, 264, 1389, 11, 291, 393, 747, 309, 293, 6772, 1466, 51524], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 335, "seek": 190504, "start": 1929.2, "end": 1933.84, "text": " a generalized form of it if you just start giving it seven digit numbers. Very easy because it's", "tokens": [51572, 257, 44498, 1254, 295, 309, 498, 291, 445, 722, 2902, 309, 3407, 14293, 3547, 13, 4372, 1858, 570, 309, 311, 51804], "temperature": 0.0, "avg_logprob": -0.1585168061433015, "compression_ratio": 1.7610062893081762, "no_speech_prob": 0.002927596215158701}, {"id": 336, "seek": 193384, "start": 1933.84, "end": 1938.8799999999999, "text": " just modifying probably a couple lines of code. So it is capable of strong generalization. So here", "tokens": [50364, 445, 42626, 1391, 257, 1916, 3876, 295, 3089, 13, 407, 309, 307, 8189, 295, 2068, 2674, 2144, 13, 407, 510, 50616], "temperature": 0.0, "avg_logprob": -0.12261944803698309, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.001981785288080573}, {"id": 337, "seek": 193384, "start": 1938.8799999999999, "end": 1943.84, "text": " you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning", "tokens": [50616, 291, 722, 2577, 577, 337, 257, 1154, 300, 311, 17879, 257, 27706, 9284, 299, 21577, 50864], "temperature": 0.0, "avg_logprob": -0.12261944803698309, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.001981785288080573}, {"id": 338, "seek": 193384, "start": 1943.84, "end": 1950.1599999999999, "text": " problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with", "tokens": [50864, 1154, 11, 27706, 3164, 307, 264, 3006, 1867, 13, 14895, 2539, 11, 309, 311, 1944, 11, 309, 1985, 11, 457, 365, 51180], "temperature": 0.0, "avg_logprob": -0.12261944803698309, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.001981785288080573}, {"id": 339, "seek": 193384, "start": 1950.1599999999999, "end": 1956.32, "text": " extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting", "tokens": [51180, 4664, 17417, 15705, 11, 558, 30, 467, 311, 588, 1152, 281, 3847, 309, 11, 291, 643, 9131, 295, 1412, 13, 440, 16505, 51488], "temperature": 0.0, "avg_logprob": -0.12261944803698309, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.001981785288080573}, {"id": 340, "seek": 193384, "start": 1956.32, "end": 1960.6399999999999, "text": " embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on", "tokens": [51488, 12240, 3584, 11, 570, 309, 311, 406, 11, 309, 311, 406, 27706, 11, 321, 603, 362, 23552, 279, 13, 467, 311, 406, 516, 281, 589, 322, 51704], "temperature": 0.0, "avg_logprob": -0.12261944803698309, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.001981785288080573}, {"id": 341, "seek": 196064, "start": 1960.64, "end": 1964.64, "text": " a person at a time, it's going to be pretty long. It's only going to be capable of hardcore", "tokens": [50364, 257, 954, 412, 257, 565, 11, 309, 311, 516, 281, 312, 1238, 938, 13, 467, 311, 787, 516, 281, 312, 8189, 295, 28196, 50564], "temperature": 0.0, "avg_logprob": -0.24111903155291523, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.002043261658400297}, {"id": 342, "seek": 196064, "start": 1964.64, "end": 1973.5200000000002, "text": " generalization, right? Because again, there is a huge difference in representational flexibility", "tokens": [50564, 2674, 2144, 11, 558, 30, 1436, 797, 11, 456, 307, 257, 2603, 2649, 294, 2906, 1478, 12635, 51008], "temperature": 0.0, "avg_logprob": -0.24111903155291523, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.002043261658400297}, {"id": 343, "seek": 196064, "start": 1973.5200000000002, "end": 1979.0400000000002, "text": " between your very simple, discrete algorithm and some kind of very complex, high dimensional", "tokens": [51008, 1296, 428, 588, 2199, 11, 27706, 9284, 293, 512, 733, 295, 588, 3997, 11, 1090, 18795, 51284], "temperature": 0.0, "avg_logprob": -0.24111903155291523, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.002043261658400297}, {"id": 344, "seek": 196064, "start": 1979.0400000000002, "end": 1984.4, "text": " continuous embedding. And then there's also the efficiency consideration. So clearly for", "tokens": [51284, 10957, 12240, 3584, 13, 400, 550, 456, 311, 611, 264, 10493, 12381, 13, 407, 4448, 337, 51552], "temperature": 0.0, "avg_logprob": -0.24111903155291523, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.002043261658400297}, {"id": 345, "seek": 196064, "start": 1984.4, "end": 1989.6000000000001, "text": " if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's", "tokens": [51552, 498, 291, 434, 6260, 11, 293, 264, 9943, 307, 611, 2074, 11, 558, 30, 1743, 498, 291, 434, 6260, 365, 257, 1154, 300, 311, 51812], "temperature": 0.0, "avg_logprob": -0.24111903155291523, "compression_ratio": 1.6297577854671281, "no_speech_prob": 0.002043261658400297}, {"id": 346, "seek": 198960, "start": 1990.1599999999999, "end": 1996.8, "text": " perception problem, where you have data points that fit on a nice and smooth manifold, then", "tokens": [50392, 12860, 1154, 11, 689, 291, 362, 1412, 2793, 300, 3318, 322, 257, 1481, 293, 5508, 47138, 11, 550, 50724], "temperature": 0.0, "avg_logprob": -0.20485595335443335, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0005581403966061771}, {"id": 347, "seek": 198960, "start": 1996.8, "end": 2004.6399999999999, "text": " deep learning is actually the right answer. And if you tried to train a discrete program to develop", "tokens": [50724, 2452, 2539, 307, 767, 264, 558, 1867, 13, 400, 498, 291, 3031, 281, 3847, 257, 27706, 1461, 281, 1499, 51116], "temperature": 0.0, "avg_logprob": -0.20485595335443335, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0005581403966061771}, {"id": 348, "seek": 198960, "start": 2004.6399999999999, "end": 2009.36, "text": " your program synthesis, an actual algorithm to classify MNIST digits, for instance.", "tokens": [51116, 428, 1461, 30252, 11, 364, 3539, 9284, 281, 33872, 376, 45, 19756, 27011, 11, 337, 5197, 13, 51352], "temperature": 0.0, "avg_logprob": -0.20485595335443335, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0005581403966061771}, {"id": 349, "seek": 198960, "start": 2010.8, "end": 2016.24, "text": " Everything I just said would be true, but in reverse, your program would be brittle. The", "tokens": [51424, 5471, 286, 445, 848, 576, 312, 2074, 11, 457, 294, 9943, 11, 428, 1461, 576, 312, 49325, 13, 440, 51696], "temperature": 0.0, "avg_logprob": -0.20485595335443335, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0005581403966061771}, {"id": 350, "seek": 201624, "start": 2016.24, "end": 2020.96, "text": " deep learning model would be robust, and so on. So there are really problems where deep learning", "tokens": [50364, 2452, 2539, 2316, 576, 312, 13956, 11, 293, 370, 322, 13, 407, 456, 366, 534, 2740, 689, 2452, 2539, 50600], "temperature": 0.0, "avg_logprob": -0.15990211486816405, "compression_ratio": 1.7517985611510791, "no_speech_prob": 0.0022363332100212574}, {"id": 351, "seek": 201624, "start": 2020.96, "end": 2025.1200000000001, "text": " is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list", "tokens": [50600, 307, 257, 1238, 1558, 13, 467, 311, 257, 869, 3318, 13, 11676, 82, 689, 309, 311, 257, 6237, 1558, 13, 1743, 853, 32411, 257, 1329, 50808], "temperature": 0.0, "avg_logprob": -0.15990211486816405, "compression_ratio": 1.7517985611510791, "no_speech_prob": 0.0022363332100212574}, {"id": 352, "seek": 201624, "start": 2025.1200000000001, "end": 2030.64, "text": " with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying.", "tokens": [50808, 365, 2452, 2539, 2316, 13, 1664, 309, 312, 1096, 30, 1079, 11, 767, 309, 393, 13, 583, 365, 439, 613, 11730, 1720, 9275, 13, 51084], "temperature": 0.0, "avg_logprob": -0.15990211486816405, "compression_ratio": 1.7517985611510791, "no_speech_prob": 0.0022363332100212574}, {"id": 353, "seek": 201624, "start": 2031.2, "end": 2036.72, "text": " It is possible to sort a list of deep learning with some hacky inductive priors and probably", "tokens": [51112, 467, 307, 1944, 281, 1333, 257, 1329, 295, 2452, 2539, 365, 512, 10339, 88, 31612, 488, 1790, 830, 293, 1391, 51388], "temperature": 0.0, "avg_logprob": -0.15990211486816405, "compression_ratio": 1.7517985611510791, "no_speech_prob": 0.0022363332100212574}, {"id": 354, "seek": 201624, "start": 2036.72, "end": 2041.44, "text": " memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots", "tokens": [51388, 10560, 3319, 881, 295, 264, 3097, 1412, 13, 400, 309, 311, 406, 257, 17434, 11, 307, 309, 30, 509, 848, 1803, 11, 456, 311, 3195, 51624], "temperature": 0.0, "avg_logprob": -0.15990211486816405, "compression_ratio": 1.7517985611510791, "no_speech_prob": 0.0022363332100212574}, {"id": 355, "seek": 204144, "start": 2041.44, "end": 2046.88, "text": " of problems that fall in the middle, where there is a semi continuous structure and some", "tokens": [50364, 295, 2740, 300, 2100, 294, 264, 2808, 11, 689, 456, 307, 257, 12909, 10957, 3877, 293, 512, 50636], "temperature": 0.0, "avg_logprob": -0.05410365338595408, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03216075524687767}, {"id": 356, "seek": 204144, "start": 2046.88, "end": 2052.0, "text": " regularity, but it's still a discrete problem. And you're saying in that situation, we should", "tokens": [50636, 3890, 507, 11, 457, 309, 311, 920, 257, 27706, 1154, 13, 400, 291, 434, 1566, 294, 300, 2590, 11, 321, 820, 50892], "temperature": 0.0, "avg_logprob": -0.05410365338595408, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03216075524687767}, {"id": 357, "seek": 204144, "start": 2052.0, "end": 2056.4, "text": " still use program search, but maybe we can use deep learning, maybe something about the shape", "tokens": [50892, 920, 764, 1461, 3164, 11, 457, 1310, 321, 393, 764, 2452, 2539, 11, 1310, 746, 466, 264, 3909, 51112], "temperature": 0.0, "avg_logprob": -0.05410365338595408, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03216075524687767}, {"id": 358, "seek": 204144, "start": 2056.4, "end": 2060.7200000000003, "text": " of the manifold, even though it's semi continuous, could actually tell us about how to do that", "tokens": [51112, 295, 264, 47138, 11, 754, 1673, 309, 311, 12909, 10957, 11, 727, 767, 980, 505, 466, 577, 281, 360, 300, 51328], "temperature": 0.0, "avg_logprob": -0.05410365338595408, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03216075524687767}, {"id": 359, "seek": 204144, "start": 2060.7200000000003, "end": 2066.32, "text": " program search more efficiently. But it seems to me that if there are problems out there,", "tokens": [51328, 1461, 3164, 544, 19621, 13, 583, 309, 2544, 281, 385, 300, 498, 456, 366, 2740, 484, 456, 11, 51608], "temperature": 0.0, "avg_logprob": -0.05410365338595408, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03216075524687767}, {"id": 360, "seek": 206632, "start": 2066.32, "end": 2072.32, "text": " let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here,", "tokens": [50364, 718, 311, 584, 5127, 3547, 493, 294, 26809, 51, 18, 11, 562, 286, 1401, 264, 1507, 300, 291, 600, 668, 1417, 466, 510, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11452337106068929, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.006987558677792549}, {"id": 361, "seek": 206632, "start": 2072.32, "end": 2078.88, "text": " it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily", "tokens": [50664, 309, 2544, 6322, 281, 385, 13, 1545, 366, 561, 406, 8867, 493, 322, 341, 30, 286, 519, 881, 561, 366, 406, 4725, 50992], "temperature": 0.0, "avg_logprob": -0.11452337106068929, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.006987558677792549}, {"id": 362, "seek": 206632, "start": 2078.88, "end": 2084.0, "text": " paying a lot of attention to the nature of deep learning, why it works, why it doesn't work.", "tokens": [50992, 6229, 257, 688, 295, 3202, 281, 264, 3687, 295, 2452, 2539, 11, 983, 309, 1985, 11, 983, 309, 1177, 380, 589, 13, 51248], "temperature": 0.0, "avg_logprob": -0.11452337106068929, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.006987558677792549}, {"id": 363, "seek": 206632, "start": 2084.0, "end": 2089.52, "text": " I also think the people, they are basically two categories of people. They are like laypeople,", "tokens": [51248, 286, 611, 519, 264, 561, 11, 436, 366, 1936, 732, 10479, 295, 561, 13, 814, 366, 411, 2360, 21123, 11, 51524], "temperature": 0.0, "avg_logprob": -0.11452337106068929, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.006987558677792549}, {"id": 364, "seek": 206632, "start": 2089.52, "end": 2095.2000000000003, "text": " and they are people with deep expertise. And the big problem we have here is that the people with", "tokens": [51524, 293, 436, 366, 561, 365, 2452, 11769, 13, 400, 264, 955, 1154, 321, 362, 510, 307, 300, 264, 561, 365, 51808], "temperature": 0.0, "avg_logprob": -0.11452337106068929, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.006987558677792549}, {"id": 365, "seek": 209520, "start": 2095.2, "end": 2101.7599999999998, "text": " a lot of expertise are going to be a lot of the time driven by motivated thinking. Because", "tokens": [50364, 257, 688, 295, 11769, 366, 516, 281, 312, 257, 688, 295, 264, 565, 9555, 538, 14515, 1953, 13, 1436, 50692], "temperature": 0.0, "avg_logprob": -0.14444665021674577, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.003456683596596122}, {"id": 366, "seek": 209520, "start": 2103.68, "end": 2107.3599999999997, "text": " like I do, they work in the field of deep learning, and so they're going to have this vested", "tokens": [50788, 411, 286, 360, 11, 436, 589, 294, 264, 2519, 295, 2452, 2539, 11, 293, 370, 436, 434, 516, 281, 362, 341, 49317, 50972], "temperature": 0.0, "avg_logprob": -0.14444665021674577, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.003456683596596122}, {"id": 367, "seek": 209520, "start": 2107.3599999999997, "end": 2112.96, "text": " interest in deep learning being potentially more powerful, more general at the nature is. I think", "tokens": [50972, 1179, 294, 2452, 2539, 885, 7263, 544, 4005, 11, 544, 2674, 412, 264, 3687, 307, 13, 286, 519, 51252], "temperature": 0.0, "avg_logprob": -0.14444665021674577, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.003456683596596122}, {"id": 368, "seek": 209520, "start": 2112.96, "end": 2119.4399999999996, "text": " if you want to think clearly, the primary obstacle is motivated thinking. It's fighting", "tokens": [51252, 498, 291, 528, 281, 519, 4448, 11, 264, 6194, 23112, 307, 14515, 1953, 13, 467, 311, 5237, 51576], "temperature": 0.0, "avg_logprob": -0.14444665021674577, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.003456683596596122}, {"id": 369, "seek": 211944, "start": 2119.52, "end": 2126.16, "text": " against what you want to be true. So I tend to have super boring opinions in that sense,", "tokens": [50368, 1970, 437, 291, 528, 281, 312, 2074, 13, 407, 286, 3928, 281, 362, 1687, 9989, 11819, 294, 300, 2020, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1216421127319336, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0556170754134655}, {"id": 370, "seek": 211944, "start": 2126.16, "end": 2131.44, "text": " because I do my best to try to forget kind of what I would like the world to be in my", "tokens": [50700, 570, 286, 360, 452, 1151, 281, 853, 281, 2870, 733, 295, 437, 286, 576, 411, 264, 1002, 281, 312, 294, 452, 50964], "temperature": 0.0, "avg_logprob": -0.1216421127319336, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0556170754134655}, {"id": 371, "seek": 211944, "start": 2131.44, "end": 2136.2400000000002, "text": " best interest and try to look at it as it really is. And that will tend to actually diminish", "tokens": [50964, 1151, 1179, 293, 853, 281, 574, 412, 309, 382, 309, 534, 307, 13, 400, 300, 486, 3928, 281, 767, 48696, 51204], "temperature": 0.0, "avg_logprob": -0.1216421127319336, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0556170754134655}, {"id": 372, "seek": 211944, "start": 2136.2400000000002, "end": 2141.84, "text": " the importance of my own work. So yeah, but you know, I've been doing deep learning for", "tokens": [51204, 264, 7379, 295, 452, 1065, 589, 13, 407, 1338, 11, 457, 291, 458, 11, 286, 600, 668, 884, 2452, 2539, 337, 51484], "temperature": 0.0, "avg_logprob": -0.1216421127319336, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0556170754134655}, {"id": 373, "seek": 211944, "start": 2141.84, "end": 2147.36, "text": " almost a decade. Of course, I would want it to be like this incredible world changing thing that", "tokens": [51484, 1920, 257, 10378, 13, 2720, 1164, 11, 286, 576, 528, 309, 281, 312, 411, 341, 4651, 1002, 4473, 551, 300, 51760], "temperature": 0.0, "avg_logprob": -0.1216421127319336, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0556170754134655}, {"id": 374, "seek": 214736, "start": 2147.36, "end": 2151.04, "text": " leads to human level intelligence, right off the bat, that would be that would be awesome,", "tokens": [50364, 6689, 281, 1952, 1496, 7599, 11, 558, 766, 264, 7362, 11, 300, 576, 312, 300, 576, 312, 3476, 11, 50548], "temperature": 0.0, "avg_logprob": -0.12985315816155796, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0038831117562949657}, {"id": 375, "seek": 214736, "start": 2151.04, "end": 2156.2400000000002, "text": " that would be amazing, and that would be right in the middle of it. But that's not that's not", "tokens": [50548, 300, 576, 312, 2243, 11, 293, 300, 576, 312, 558, 294, 264, 2808, 295, 309, 13, 583, 300, 311, 406, 300, 311, 406, 50808], "temperature": 0.0, "avg_logprob": -0.12985315816155796, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0038831117562949657}, {"id": 376, "seek": 214736, "start": 2156.2400000000002, "end": 2162.96, "text": " actually what's going on. You said you tend to be what was the word, not not controversial ideas", "tokens": [50808, 767, 437, 311, 516, 322, 13, 509, 848, 291, 3928, 281, 312, 437, 390, 264, 1349, 11, 406, 406, 17323, 3487, 51144], "temperature": 0.0, "avg_logprob": -0.12985315816155796, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0038831117562949657}, {"id": 377, "seek": 214736, "start": 2162.96, "end": 2167.6, "text": " or something because you try to stick to the way the world is rather than the way you want the", "tokens": [51144, 420, 746, 570, 291, 853, 281, 2897, 281, 264, 636, 264, 1002, 307, 2831, 813, 264, 636, 291, 528, 264, 51376], "temperature": 0.0, "avg_logprob": -0.12985315816155796, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0038831117562949657}, {"id": 378, "seek": 214736, "start": 2167.6, "end": 2173.2000000000003, "text": " world to be. But we just had Yannick produce an interesting video about how if you think that", "tokens": [51376, 1002, 281, 312, 13, 583, 321, 445, 632, 398, 969, 618, 5258, 364, 1880, 960, 466, 577, 498, 291, 519, 300, 51656], "temperature": 0.0, "avg_logprob": -0.12985315816155796, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0038831117562949657}, {"id": 379, "seek": 217320, "start": 2173.2799999999997, "end": 2177.3599999999997, "text": " machine learning models essentially attempt to do the same thing, right? I mean, they're not human", "tokens": [50368, 3479, 2539, 5245, 4476, 5217, 281, 360, 264, 912, 551, 11, 558, 30, 286, 914, 11, 436, 434, 406, 1952, 50572], "temperature": 0.0, "avg_logprob": -0.09731903383808752, "compression_ratio": 1.7870036101083033, "no_speech_prob": 0.011315488256514072}, {"id": 380, "seek": 217320, "start": 2177.3599999999997, "end": 2182.16, "text": " beings, they don't really have wants per se, they're just modeling reality as it is. It turns out", "tokens": [50572, 8958, 11, 436, 500, 380, 534, 362, 2738, 680, 369, 11, 436, 434, 445, 15983, 4103, 382, 309, 307, 13, 467, 4523, 484, 50812], "temperature": 0.0, "avg_logprob": -0.09731903383808752, "compression_ratio": 1.7870036101083033, "no_speech_prob": 0.011315488256514072}, {"id": 381, "seek": 217320, "start": 2182.16, "end": 2188.64, "text": " reality itself really annoys a lot of people, like they just don't like reality, and they don't like", "tokens": [50812, 4103, 2564, 534, 46277, 749, 257, 688, 295, 561, 11, 411, 436, 445, 500, 380, 411, 4103, 11, 293, 436, 500, 380, 411, 51136], "temperature": 0.0, "avg_logprob": -0.09731903383808752, "compression_ratio": 1.7870036101083033, "no_speech_prob": 0.011315488256514072}, {"id": 382, "seek": 217320, "start": 2188.64, "end": 2193.2799999999997, "text": " the way the world is, and they wish it was something different. And that infects like every mode of", "tokens": [51136, 264, 636, 264, 1002, 307, 11, 293, 436, 3172, 309, 390, 746, 819, 13, 400, 300, 5888, 82, 411, 633, 4391, 295, 51368], "temperature": 0.0, "avg_logprob": -0.09731903383808752, "compression_ratio": 1.7870036101083033, "no_speech_prob": 0.011315488256514072}, {"id": 383, "seek": 217320, "start": 2193.2799999999997, "end": 2197.8399999999997, "text": " their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for", "tokens": [51368, 641, 1953, 11, 767, 13, 865, 11, 572, 11, 3122, 13, 4534, 561, 11, 291, 458, 11, 293, 300, 311, 300, 311, 2074, 337, 51596], "temperature": 0.0, "avg_logprob": -0.09731903383808752, "compression_ratio": 1.7870036101083033, "no_speech_prob": 0.011315488256514072}, {"id": 384, "seek": 219784, "start": 2197.84, "end": 2205.6000000000004, "text": " me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend.", "tokens": [50364, 385, 382, 731, 13, 286, 478, 406, 1566, 286, 478, 364, 11183, 11, 286, 478, 1382, 281, 360, 452, 1151, 281, 4597, 341, 6028, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1382862595487232, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.0300251804292202}, {"id": 385, "seek": 219784, "start": 2205.6000000000004, "end": 2211.28, "text": " But I have no exception. Most people have opinions not because they've seen evidence in", "tokens": [50752, 583, 286, 362, 572, 11183, 13, 4534, 561, 362, 11819, 406, 570, 436, 600, 1612, 4467, 294, 51036], "temperature": 0.0, "avg_logprob": -0.1382862595487232, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.0300251804292202}, {"id": 386, "seek": 219784, "start": 2211.28, "end": 2216.4, "text": " support of their opinion, but because it's in their interest for this opinion to be true,", "tokens": [51036, 1406, 295, 641, 4800, 11, 457, 570, 309, 311, 294, 641, 1179, 337, 341, 4800, 281, 312, 2074, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1382862595487232, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.0300251804292202}, {"id": 387, "seek": 219784, "start": 2216.4, "end": 2221.36, "text": " or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3", "tokens": [51292, 420, 436, 445, 528, 309, 281, 312, 2074, 13, 286, 2041, 472, 1365, 307, 11, 291, 458, 11, 321, 645, 18315, 26039, 51, 12, 18, 51540], "temperature": 0.0, "avg_logprob": -0.1382862595487232, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.0300251804292202}, {"id": 388, "seek": 219784, "start": 2221.36, "end": 2227.1200000000003, "text": " and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim", "tokens": [51540, 293, 370, 322, 293, 2365, 40496, 295, 26039, 51, 12, 18, 13, 286, 390, 767, 1687, 2919, 562, 286, 9105, 1866, 264, 3932, 51828], "temperature": 0.0, "avg_logprob": -0.1382862595487232, "compression_ratio": 1.6642335766423357, "no_speech_prob": 0.0300251804292202}, {"id": 389, "seek": 222712, "start": 2227.12, "end": 2231.68, "text": " that the pre-trained language model could perform few short generalizations. I thought that's", "tokens": [50364, 300, 264, 659, 12, 17227, 2001, 2856, 2316, 727, 2042, 1326, 2099, 2674, 14455, 13, 286, 1194, 300, 311, 50592], "temperature": 0.0, "avg_logprob": -0.18062670189037658, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.003578206291422248}, {"id": 390, "seek": 222712, "start": 2231.68, "end": 2237.6, "text": " super fascinating. I'm always super excited if I hear about something that's really challenging", "tokens": [50592, 1687, 10343, 13, 286, 478, 1009, 1687, 2919, 498, 286, 1568, 466, 746, 300, 311, 534, 7595, 50888], "temperature": 0.0, "avg_logprob": -0.18062670189037658, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.003578206291422248}, {"id": 391, "seek": 222712, "start": 2238.3199999999997, "end": 2245.12, "text": " my initial kind of mental model of how the world works, you know, it's like a few years back,", "tokens": [50924, 452, 5883, 733, 295, 4973, 2316, 295, 577, 264, 1002, 1985, 11, 291, 458, 11, 309, 311, 411, 257, 1326, 924, 646, 11, 51264], "temperature": 0.0, "avg_logprob": -0.18062670189037658, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.003578206291422248}, {"id": 392, "seek": 222712, "start": 2245.12, "end": 2249.92, "text": " and there was this claim that a neutrino was measured going faster than speed of flight.", "tokens": [51264, 293, 456, 390, 341, 3932, 300, 257, 39913, 2982, 390, 12690, 516, 4663, 813, 3073, 295, 7018, 13, 51504], "temperature": 0.0, "avg_logprob": -0.18062670189037658, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.003578206291422248}, {"id": 393, "seek": 222712, "start": 2249.92, "end": 2254.96, "text": " I mean, that's exciting, right? That's like new physics, you want it to be true, at least you", "tokens": [51504, 286, 914, 11, 300, 311, 4670, 11, 558, 30, 663, 311, 411, 777, 10649, 11, 291, 528, 309, 281, 312, 2074, 11, 412, 1935, 291, 51756], "temperature": 0.0, "avg_logprob": -0.18062670189037658, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.003578206291422248}, {"id": 394, "seek": 225496, "start": 2254.96, "end": 2259.28, "text": " want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's", "tokens": [50364, 528, 281, 483, 281, 264, 2767, 295, 309, 13, 400, 550, 309, 3574, 484, 281, 312, 257, 13160, 6713, 11, 558, 30, 407, 300, 311, 50580], "temperature": 0.0, "avg_logprob": -0.12209989298944889, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0016728424234315753}, {"id": 395, "seek": 225496, "start": 2259.28, "end": 2264.2400000000002, "text": " disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something,", "tokens": [50580, 25054, 13, 407, 286, 519, 26039, 51, 12, 18, 307, 733, 295, 264, 912, 337, 385, 13, 286, 534, 1415, 309, 281, 312, 746, 11, 50828], "temperature": 0.0, "avg_logprob": -0.12209989298944889, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0016728424234315753}, {"id": 396, "seek": 225496, "start": 2264.2400000000002, "end": 2268.4, "text": " something novel, and that would really challenge what they thought to be true by deep learning", "tokens": [50828, 746, 7613, 11, 293, 300, 576, 534, 3430, 437, 436, 1194, 281, 312, 2074, 538, 2452, 2539, 51036], "temperature": 0.0, "avg_logprob": -0.12209989298944889, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0016728424234315753}, {"id": 397, "seek": 225496, "start": 2268.4, "end": 2274.96, "text": " models. And I regret to say that everything I've seen close has actually confirmed. In my view,", "tokens": [51036, 5245, 13, 400, 286, 10879, 281, 584, 300, 1203, 286, 600, 1612, 1998, 575, 767, 11341, 13, 682, 452, 1910, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12209989298944889, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0016728424234315753}, {"id": 398, "seek": 225496, "start": 2274.96, "end": 2280.7200000000003, "text": " that basically deep learning models, they can learn to embed algorithms given sufficient exposure", "tokens": [51364, 300, 1936, 2452, 2539, 5245, 11, 436, 393, 1466, 281, 12240, 14642, 2212, 11563, 10420, 51652], "temperature": 0.0, "avg_logprob": -0.12209989298944889, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0016728424234315753}, {"id": 399, "seek": 228072, "start": 2280.72, "end": 2288.08, "text": " to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern", "tokens": [50364, 281, 1412, 11, 457, 436, 2644, 534, 11, 411, 11, 1326, 2099, 26617, 1125, 7613, 14642, 300, 2906, 257, 5102, 50732], "temperature": 0.0, "avg_logprob": -0.14616754622686476, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020101817790418863}, {"id": 400, "seek": 228072, "start": 2288.08, "end": 2293.12, "text": " they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC,", "tokens": [50732, 436, 2378, 380, 1612, 294, 257, 3847, 1939, 11, 597, 307, 983, 11, 538, 264, 636, 11, 26039, 51, 12, 18, 307, 7696, 48836, 322, 8943, 34, 11, 50984], "temperature": 0.0, "avg_logprob": -0.14616754622686476, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020101817790418863}, {"id": 401, "seek": 228072, "start": 2293.12, "end": 2300.9599999999996, "text": " for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually", "tokens": [50984, 337, 5197, 13, 400, 300, 311, 733, 295, 4227, 281, 385, 13, 286, 733, 295, 10879, 309, 11, 570, 309, 1355, 286, 2378, 380, 767, 51376], "temperature": 0.0, "avg_logprob": -0.14616754622686476, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020101817790418863}, {"id": 402, "seek": 228072, "start": 2300.9599999999996, "end": 2308.24, "text": " learned anything from it. It hasn't expanded my view of the world, which is too bad. Like,", "tokens": [51376, 3264, 1340, 490, 309, 13, 467, 6132, 380, 14342, 452, 1910, 295, 264, 1002, 11, 597, 307, 886, 1578, 13, 1743, 11, 51740], "temperature": 0.0, "avg_logprob": -0.14616754622686476, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020101817790418863}, {"id": 403, "seek": 230824, "start": 2308.24, "end": 2314.56, "text": " I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model", "tokens": [50364, 286, 3172, 309, 630, 13, 286, 3172, 309, 630, 13, 407, 294, 264, 1389, 295, 26039, 51, 12, 18, 11, 437, 311, 534, 516, 322, 307, 300, 264, 2316, 50680], "temperature": 0.0, "avg_logprob": -0.11949479880453158, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0002867231669370085}, {"id": 404, "seek": 230824, "start": 2314.56, "end": 2320.56, "text": " is exposed to many patterns. You could call them algorithms, for instance, in many different contexts.", "tokens": [50680, 307, 9495, 281, 867, 8294, 13, 509, 727, 818, 552, 14642, 11, 337, 5197, 11, 294, 867, 819, 30628, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11949479880453158, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0002867231669370085}, {"id": 405, "seek": 230824, "start": 2320.56, "end": 2325.4399999999996, "text": " And so it has memorized these patterns. And now it's able to take these patterns and apply them", "tokens": [50980, 400, 370, 309, 575, 46677, 613, 8294, 13, 400, 586, 309, 311, 1075, 281, 747, 613, 8294, 293, 3079, 552, 51224], "temperature": 0.0, "avg_logprob": -0.11949479880453158, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0002867231669370085}, {"id": 406, "seek": 230824, "start": 2325.4399999999996, "end": 2329.68, "text": " to new data in much the same way that the multiplication algorithm we are talking about.", "tokens": [51224, 281, 777, 1412, 294, 709, 264, 912, 636, 300, 264, 27290, 9284, 321, 366, 1417, 466, 13, 51436], "temperature": 0.0, "avg_logprob": -0.11949479880453158, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0002867231669370085}, {"id": 407, "seek": 230824, "start": 2329.68, "end": 2334.7999999999997, "text": " Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits", "tokens": [51436, 1436, 309, 311, 364, 3539, 9284, 11, 309, 393, 1399, 777, 27011, 13, 467, 311, 406, 445, 10560, 3319, 264, 27011, 51692], "temperature": 0.0, "avg_logprob": -0.11949479880453158, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0002867231669370085}, {"id": 408, "seek": 233480, "start": 2334.8, "end": 2340.5600000000004, "text": " in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms", "tokens": [50364, 294, 264, 3847, 13, 467, 311, 364, 3539, 9284, 13, 682, 264, 912, 636, 11, 26039, 51, 12, 18, 8306, 9131, 295, 1359, 14642, 50652], "temperature": 0.0, "avg_logprob": -0.11903522325598676, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002548500197008252}, {"id": 409, "seek": 233480, "start": 2340.5600000000004, "end": 2346.5600000000004, "text": " like that. But the model is not synthesizing these algorithms on the fly. They are in the model already.", "tokens": [50652, 411, 300, 13, 583, 264, 2316, 307, 406, 26617, 3319, 613, 14642, 322, 264, 3603, 13, 814, 366, 294, 264, 2316, 1217, 13, 50952], "temperature": 0.0, "avg_logprob": -0.11903522325598676, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002548500197008252}, {"id": 410, "seek": 233480, "start": 2347.28, "end": 2352.88, "text": " And if you try to apply GPT-3 to something for which a new algorithm would need to be produced,", "tokens": [50988, 400, 498, 291, 853, 281, 3079, 26039, 51, 12, 18, 281, 746, 337, 597, 257, 777, 9284, 576, 643, 281, 312, 7126, 11, 51268], "temperature": 0.0, "avg_logprob": -0.11903522325598676, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002548500197008252}, {"id": 411, "seek": 233480, "start": 2352.88, "end": 2356.0, "text": " like an ARC task, for instance, it has just completed anything.", "tokens": [51268, 411, 364, 8943, 34, 5633, 11, 337, 5197, 11, 309, 575, 445, 7365, 1340, 13, 51424], "temperature": 0.0, "avg_logprob": -0.11903522325598676, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002548500197008252}, {"id": 412, "seek": 233480, "start": 2356.0, "end": 2361.28, "text": " It seems to all build up what you're saying, because there is this strong generalization", "tokens": [51424, 467, 2544, 281, 439, 1322, 493, 437, 291, 434, 1566, 11, 570, 456, 307, 341, 2068, 2674, 2144, 51688], "temperature": 0.0, "avg_logprob": -0.11903522325598676, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.002548500197008252}, {"id": 413, "seek": 236128, "start": 2361.28, "end": 2367.76, "text": " versus local generalization. And then you make a case that in order to do strong generalization,", "tokens": [50364, 5717, 2654, 2674, 2144, 13, 400, 550, 291, 652, 257, 1389, 300, 294, 1668, 281, 360, 2068, 2674, 2144, 11, 50688], "temperature": 0.0, "avg_logprob": -0.06666048662162122, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.023290637880563736}, {"id": 414, "seek": 236128, "start": 2367.76, "end": 2373.36, "text": " we need maybe something like program synthesis approach. So deep learning can't necessarily", "tokens": [50688, 321, 643, 1310, 746, 411, 1461, 30252, 3109, 13, 407, 2452, 2539, 393, 380, 4725, 50968], "temperature": 0.0, "avg_logprob": -0.06666048662162122, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.023290637880563736}, {"id": 415, "seek": 236128, "start": 2373.36, "end": 2379.6800000000003, "text": " get us there in most problems. And you make an interesting case that something like graph", "tokens": [50968, 483, 505, 456, 294, 881, 2740, 13, 400, 291, 652, 364, 1880, 1389, 300, 746, 411, 4295, 51284], "temperature": 0.0, "avg_logprob": -0.06666048662162122, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.023290637880563736}, {"id": 416, "seek": 236128, "start": 2379.6800000000003, "end": 2387.44, "text": " isomorphism search could play a core role in that. Could you briefly connect all of these", "tokens": [51284, 307, 32702, 1434, 3164, 727, 862, 257, 4965, 3090, 294, 300, 13, 7497, 291, 10515, 1745, 439, 295, 613, 51672], "temperature": 0.0, "avg_logprob": -0.06666048662162122, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.023290637880563736}, {"id": 417, "seek": 238744, "start": 2387.52, "end": 2391.68, "text": " terms together of the case you're making there? Because it's super interesting.", "tokens": [50368, 2115, 1214, 295, 264, 1389, 291, 434, 1455, 456, 30, 1436, 309, 311, 1687, 1880, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11330435011121961, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.01795353926718235}, {"id": 418, "seek": 238744, "start": 2391.68, "end": 2400.2400000000002, "text": " So going back to it, Tim was saying it's rarely the case that you have problems that are fully", "tokens": [50576, 407, 516, 646, 281, 309, 11, 7172, 390, 1566, 309, 311, 13752, 264, 1389, 300, 291, 362, 2740, 300, 366, 4498, 51004], "temperature": 0.0, "avg_logprob": -0.11330435011121961, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.01795353926718235}, {"id": 419, "seek": 238744, "start": 2400.2400000000002, "end": 2404.56, "text": " interpretive or fully discrete. There are definitely such problems. In fact, most perception", "tokens": [51004, 7302, 488, 420, 4498, 27706, 13, 821, 366, 2138, 1270, 2740, 13, 682, 1186, 11, 881, 12860, 51220], "temperature": 0.0, "avg_logprob": -0.11330435011121961, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.01795353926718235}, {"id": 420, "seek": 238744, "start": 2404.56, "end": 2410.16, "text": " problems are almost entirely interpretive. And most programs, the kind of programs that you", "tokens": [51220, 2740, 366, 1920, 7696, 7302, 488, 13, 400, 881, 4268, 11, 264, 733, 295, 4268, 300, 291, 51500], "temperature": 0.0, "avg_logprob": -0.11330435011121961, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.01795353926718235}, {"id": 421, "seek": 241016, "start": 2410.72, "end": 2416.3999999999996, "text": " write there, they're largely discrete, not interpretive. But most tasks actually are best", "tokens": [50392, 2464, 456, 11, 436, 434, 11611, 27706, 11, 406, 7302, 488, 13, 583, 881, 9608, 767, 366, 1151, 50676], "temperature": 0.0, "avg_logprob": -0.19072096998041327, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.042873870581388474}, {"id": 422, "seek": 241016, "start": 2416.3999999999996, "end": 2422.96, "text": " solved via a combination of both. And I actually believe that's true for the way humans think.", "tokens": [50676, 13041, 5766, 257, 6562, 295, 1293, 13, 400, 286, 767, 1697, 300, 311, 2074, 337, 264, 636, 6255, 519, 13, 51004], "temperature": 0.0, "avg_logprob": -0.19072096998041327, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.042873870581388474}, {"id": 423, "seek": 241016, "start": 2422.96, "end": 2428.56, "text": " You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every", "tokens": [51004, 509, 458, 11, 456, 311, 2010, 502, 1953, 293, 2010, 568, 1953, 13, 286, 10613, 1697, 300, 1920, 633, 51284], "temperature": 0.0, "avg_logprob": -0.19072096998041327, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.042873870581388474}, {"id": 424, "seek": 241016, "start": 2428.56, "end": 2436.56, "text": " thought you have and everything you do with your mind is not one or the other. It's a combination", "tokens": [51284, 1194, 291, 362, 293, 1203, 291, 360, 365, 428, 1575, 307, 406, 472, 420, 264, 661, 13, 467, 311, 257, 6562, 51684], "temperature": 0.0, "avg_logprob": -0.19072096998041327, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.042873870581388474}, {"id": 425, "seek": 243656, "start": 2436.7999999999997, "end": 2442.24, "text": " of both. That type 1 and type 2 are really unmatched into each other in everything you", "tokens": [50376, 295, 1293, 13, 663, 2010, 502, 293, 2010, 568, 366, 534, 19334, 24102, 666, 1184, 661, 294, 1203, 291, 50648], "temperature": 0.0, "avg_logprob": -0.173999998304579, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.0019524313975125551}, {"id": 426, "seek": 243656, "start": 2442.24, "end": 2448.72, "text": " think and everything you do. Like, for instance, perception. That looks like something very", "tokens": [50648, 519, 293, 1203, 291, 360, 13, 1743, 11, 337, 5197, 11, 12860, 13, 663, 1542, 411, 746, 588, 50972], "temperature": 0.0, "avg_logprob": -0.173999998304579, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.0019524313975125551}, {"id": 427, "seek": 243656, "start": 2448.72, "end": 2453.84, "text": " instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot", "tokens": [50972, 9836, 13, 407, 588, 709, 264, 1333, 295, 10957, 11, 44902, 1166, 551, 13, 682, 1186, 11, 456, 311, 257, 688, 51228], "temperature": 0.0, "avg_logprob": -0.173999998304579, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.0019524313975125551}, {"id": 428, "seek": 243656, "start": 2453.84, "end": 2458.96, "text": " of reasoning that's embedded into perception. And the reverse is true, for instance. If you", "tokens": [51228, 295, 21577, 300, 311, 16741, 666, 12860, 13, 400, 264, 9943, 307, 2074, 11, 337, 5197, 13, 759, 291, 51484], "temperature": 0.0, "avg_logprob": -0.173999998304579, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.0019524313975125551}, {"id": 429, "seek": 243656, "start": 2458.96, "end": 2463.7599999999998, "text": " look at a mathematician, for instance, proving a theorem, where they're writing down on the", "tokens": [51484, 574, 412, 257, 48281, 11, 337, 5197, 11, 27221, 257, 20904, 11, 689, 436, 434, 3579, 760, 322, 264, 51724], "temperature": 0.0, "avg_logprob": -0.173999998304579, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.0019524313975125551}, {"id": 430, "seek": 246376, "start": 2463.76, "end": 2469.6000000000004, "text": " sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided", "tokens": [50364, 8193, 295, 3035, 307, 534, 1823, 12, 2322, 12, 16792, 11, 27706, 21577, 2010, 551, 13, 583, 309, 311, 588, 709, 19663, 50656], "temperature": 0.0, "avg_logprob": -0.13271643972804403, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0026623434387147427}, {"id": 431, "seek": 246376, "start": 2469.6000000000004, "end": 2474.8, "text": " by high-level intuition, which is very much interpreted. They know where they're going,", "tokens": [50656, 538, 1090, 12, 12418, 24002, 11, 597, 307, 588, 709, 26749, 13, 814, 458, 689, 436, 434, 516, 11, 50916], "temperature": 0.0, "avg_logprob": -0.13271643972804403, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0026623434387147427}, {"id": 432, "seek": 246376, "start": 2474.8, "end": 2482.2400000000002, "text": " without having to derive the exact sequence of steps to get there. So they have this high-level", "tokens": [50916, 1553, 1419, 281, 28446, 264, 1900, 8310, 295, 4439, 281, 483, 456, 13, 407, 436, 362, 341, 1090, 12, 12418, 51288], "temperature": 0.0, "avg_logprob": -0.13271643972804403, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0026623434387147427}, {"id": 433, "seek": 246376, "start": 2482.2400000000002, "end": 2486.96, "text": " kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions", "tokens": [51288, 733, 295, 1910, 13, 9242, 295, 411, 11, 291, 458, 11, 498, 291, 434, 4840, 11, 291, 362, 281, 652, 27706, 5327, 51524], "temperature": 0.0, "avg_logprob": -0.13271643972804403, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0026623434387147427}, {"id": 434, "seek": 246376, "start": 2486.96, "end": 2493.28, "text": " because you are driving on network frauds. But if you have a bird, a GPS, for instance,", "tokens": [51524, 570, 291, 366, 4840, 322, 3209, 14560, 82, 13, 583, 498, 291, 362, 257, 5255, 11, 257, 19462, 11, 337, 5197, 11, 51840], "temperature": 0.0, "avg_logprob": -0.13271643972804403, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0026623434387147427}, {"id": 435, "seek": 249328, "start": 2493.28, "end": 2497.76, "text": " you can kind of see the direction in which you are going, which is interpolated. If you're talking", "tokens": [50364, 291, 393, 733, 295, 536, 264, 3513, 294, 597, 291, 366, 516, 11, 597, 307, 44902, 770, 13, 759, 291, 434, 1417, 50588], "temperature": 0.0, "avg_logprob": -0.08231897132341252, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.0016778897261247039}, {"id": 436, "seek": 249328, "start": 2497.76, "end": 2502.0800000000004, "text": " about direction, you're talking about distances, you're talking about geometric spaces. And", "tokens": [50588, 466, 3513, 11, 291, 434, 1417, 466, 22182, 11, 291, 434, 1417, 466, 33246, 7673, 13, 400, 50804], "temperature": 0.0, "avg_logprob": -0.08231897132341252, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.0016778897261247039}, {"id": 437, "seek": 249328, "start": 2502.0800000000004, "end": 2509.0400000000004, "text": " everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same", "tokens": [50804, 1203, 294, 264, 1952, 1575, 733, 295, 10002, 341, 2316, 295, 2010, 502, 293, 2010, 568, 1953, 412, 264, 912, 51152], "temperature": 0.0, "avg_logprob": -0.08231897132341252, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.0016778897261247039}, {"id": 438, "seek": 249328, "start": 2509.0400000000004, "end": 2515.92, "text": " time. If you go back to first principles, intelligence is about abstraction. So intelligence", "tokens": [51152, 565, 13, 759, 291, 352, 646, 281, 700, 9156, 11, 7599, 307, 466, 37765, 13, 407, 7599, 51496], "temperature": 0.0, "avg_logprob": -0.08231897132341252, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.0016778897261247039}, {"id": 439, "seek": 251592, "start": 2515.92, "end": 2525.36, "text": " is about the ability to face the future, given things you've seen in the past. And the way you do", "tokens": [50364, 307, 466, 264, 3485, 281, 1851, 264, 2027, 11, 2212, 721, 291, 600, 1612, 294, 264, 1791, 13, 400, 264, 636, 291, 360, 50836], "temperature": 0.0, "avg_logprob": -0.11577453408189999, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.03145407512784004}, {"id": 440, "seek": 251592, "start": 2525.36, "end": 2532.2400000000002, "text": " that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template,", "tokens": [50836, 300, 307, 11, 1338, 11, 37765, 13, 509, 8947, 490, 264, 1791, 512, 7690, 13, 2704, 309, 311, 257, 12379, 11, 51180], "temperature": 0.0, "avg_logprob": -0.11577453408189999, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.03145407512784004}, {"id": 441, "seek": 251592, "start": 2532.2400000000002, "end": 2537.12, "text": " maybe it's an algorithm that will actually be effective in terms of explaining the future. And", "tokens": [51180, 1310, 309, 311, 364, 9284, 300, 486, 767, 312, 4942, 294, 2115, 295, 13468, 264, 2027, 13, 400, 51424], "temperature": 0.0, "avg_logprob": -0.11577453408189999, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.03145407512784004}, {"id": 442, "seek": 251592, "start": 2537.12, "end": 2543.76, "text": " that's why it makes it abstract, is that it can handle multiple instances of some kind of thing,", "tokens": [51424, 300, 311, 983, 309, 1669, 309, 12649, 11, 307, 300, 309, 393, 4813, 3866, 14519, 295, 512, 733, 295, 551, 11, 51756], "temperature": 0.0, "avg_logprob": -0.11577453408189999, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.03145407512784004}, {"id": 443, "seek": 254376, "start": 2544.0, "end": 2548.96, "text": " that thing is an abstraction. And if it's abstract enough, it can actually handle instances", "tokens": [50376, 300, 551, 307, 364, 37765, 13, 400, 498, 309, 311, 12649, 1547, 11, 309, 393, 767, 4813, 14519, 50624], "temperature": 0.0, "avg_logprob": -0.11859408940110251, "compression_ratio": 1.6605166051660516, "no_speech_prob": 0.0014980707783252}, {"id": 444, "seek": 254376, "start": 2548.96, "end": 2556.0800000000004, "text": " you've never seen before, right? It does generalization power. And all abstraction is worn", "tokens": [50624, 291, 600, 1128, 1612, 949, 11, 558, 30, 467, 775, 2674, 2144, 1347, 13, 400, 439, 37765, 307, 15254, 50980], "temperature": 0.0, "avg_logprob": -0.11859408940110251, "compression_ratio": 1.6605166051660516, "no_speech_prob": 0.0014980707783252}, {"id": 445, "seek": 254376, "start": 2556.0800000000004, "end": 2563.2000000000003, "text": " from analogy. Abstraction starts when you make an analogy between two things. Like you say,", "tokens": [50980, 490, 21663, 13, 46853, 26766, 3719, 562, 291, 652, 364, 21663, 1296, 732, 721, 13, 1743, 291, 584, 11, 51336], "temperature": 0.0, "avg_logprob": -0.11859408940110251, "compression_ratio": 1.6605166051660516, "no_speech_prob": 0.0014980707783252}, {"id": 446, "seek": 254376, "start": 2563.2000000000003, "end": 2567.5200000000004, "text": " time is like a river, if you want to get philosophical or something. But in general,", "tokens": [51336, 565, 307, 411, 257, 6810, 11, 498, 291, 528, 281, 483, 25066, 420, 746, 13, 583, 294, 2674, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11859408940110251, "compression_ratio": 1.6605166051660516, "no_speech_prob": 0.0014980707783252}, {"id": 447, "seek": 254376, "start": 2567.5200000000004, "end": 2571.5200000000004, "text": " you can just say this apple looks similar to this other apple. So there is such a thing as", "tokens": [51552, 291, 393, 445, 584, 341, 10606, 1542, 2531, 281, 341, 661, 10606, 13, 407, 456, 307, 1270, 257, 551, 382, 51752], "temperature": 0.0, "avg_logprob": -0.11859408940110251, "compression_ratio": 1.6605166051660516, "no_speech_prob": 0.0014980707783252}, {"id": 448, "seek": 257152, "start": 2571.52, "end": 2577.44, "text": " the concept of an apple, for instance. And the part that is shared between the two things that", "tokens": [50364, 264, 3410, 295, 364, 10606, 11, 337, 5197, 13, 400, 264, 644, 300, 307, 5507, 1296, 264, 732, 721, 300, 50660], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 449, "seek": 257152, "start": 2577.44, "end": 2582.96, "text": " you're relating to each other, the subject of the analogy that that's the part that can be said to", "tokens": [50660, 291, 434, 23968, 281, 1184, 661, 11, 264, 3983, 295, 264, 21663, 300, 300, 311, 264, 644, 300, 393, 312, 848, 281, 50936], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 450, "seek": 257152, "start": 2582.96, "end": 2587.7599999999998, "text": " be abstract, that is the part that will help you make sense of the future, like you encounter a", "tokens": [50936, 312, 12649, 11, 300, 307, 264, 644, 300, 486, 854, 291, 652, 2020, 295, 264, 2027, 11, 411, 291, 8593, 257, 51176], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 451, "seek": 257152, "start": 2587.7599999999998, "end": 2592.4, "text": " third apple in the future, you know, it's an apple. Because you don't even need to relate this to", "tokens": [51176, 2636, 10606, 294, 264, 2027, 11, 291, 458, 11, 309, 311, 364, 10606, 13, 1436, 291, 500, 380, 754, 643, 281, 10961, 341, 281, 51408], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 452, "seek": 257152, "start": 2592.4, "end": 2596.64, "text": " the apple should have memorized, you just need to, you just need to relate it to the template,", "tokens": [51408, 264, 10606, 820, 362, 46677, 11, 291, 445, 643, 281, 11, 291, 445, 643, 281, 10961, 309, 281, 264, 12379, 11, 51620], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 453, "seek": 257152, "start": 2596.64, "end": 2601.12, "text": " the abstract template of an apple that you've formed by from exposure to different kinds of", "tokens": [51620, 264, 12649, 12379, 295, 364, 10606, 300, 291, 600, 8693, 538, 490, 10420, 281, 819, 3685, 295, 51844], "temperature": 0.0, "avg_logprob": -0.15765765073488083, "compression_ratio": 1.9590443686006827, "no_speech_prob": 0.0012768578017130494}, {"id": 454, "seek": 260112, "start": 2601.12, "end": 2606.7999999999997, "text": " apples in the past. And if you think about what's what's an analogy, really, like how do you find", "tokens": [50364, 16814, 294, 264, 1791, 13, 400, 498, 291, 519, 466, 437, 311, 437, 311, 364, 21663, 11, 534, 11, 411, 577, 360, 291, 915, 50648], "temperature": 0.0, "avg_logprob": -0.152471483970175, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0010138248326256871}, {"id": 455, "seek": 260112, "start": 2606.7999999999997, "end": 2613.44, "text": " an analogy, it's a way to compare two things to each other. And there are only really two ways", "tokens": [50648, 364, 21663, 11, 309, 311, 257, 636, 281, 6794, 732, 721, 281, 1184, 661, 13, 400, 456, 366, 787, 534, 732, 2098, 50980], "temperature": 0.0, "avg_logprob": -0.152471483970175, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0010138248326256871}, {"id": 456, "seek": 260112, "start": 2613.44, "end": 2623.12, "text": " to compare things. You can, you can basically ask how similar are they in terms of distance,", "tokens": [50980, 281, 6794, 721, 13, 509, 393, 11, 291, 393, 1936, 1029, 577, 2531, 366, 436, 294, 2115, 295, 4560, 11, 51464], "temperature": 0.0, "avg_logprob": -0.152471483970175, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0010138248326256871}, {"id": 457, "seek": 260112, "start": 2623.12, "end": 2628.64, "text": " like you can say implicitly, there's you're looking at the space of points, there's a distance", "tokens": [51464, 411, 291, 393, 584, 26947, 356, 11, 456, 311, 291, 434, 1237, 412, 264, 1901, 295, 2793, 11, 456, 311, 257, 4560, 51740], "temperature": 0.0, "avg_logprob": -0.152471483970175, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0010138248326256871}, {"id": 458, "seek": 262864, "start": 2628.64, "end": 2634.0, "text": " between any two points. That's, that's the type one, a subject analogy that leads to type one", "tokens": [50364, 1296, 604, 732, 2793, 13, 663, 311, 11, 300, 311, 264, 2010, 472, 11, 257, 3983, 21663, 300, 6689, 281, 2010, 472, 50632], "temperature": 0.0, "avg_logprob": -0.1770154122383364, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.0005860975361429155}, {"id": 459, "seek": 262864, "start": 2634.0, "end": 2640.8799999999997, "text": " abstractions, which leads to a type one thinking, right? So a type one analogy is like your things,", "tokens": [50632, 12649, 626, 11, 597, 6689, 281, 257, 2010, 472, 1953, 11, 558, 30, 407, 257, 2010, 472, 21663, 307, 411, 428, 721, 11, 50976], "temperature": 0.0, "avg_logprob": -0.1770154122383364, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.0005860975361429155}, {"id": 460, "seek": 262864, "start": 2640.8799999999997, "end": 2646.4, "text": " you say to what degree they're similar to each other. So you read them by distance, you, so", "tokens": [50976, 291, 584, 281, 437, 4314, 436, 434, 2531, 281, 1184, 661, 13, 407, 291, 1401, 552, 538, 4560, 11, 291, 11, 370, 51252], "temperature": 0.0, "avg_logprob": -0.1770154122383364, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.0005860975361429155}, {"id": 461, "seek": 262864, "start": 2646.4, "end": 2653.12, "text": " implicitly, it means you put your things on in a geometric space, right? And type one abstraction", "tokens": [51252, 26947, 356, 11, 309, 1355, 291, 829, 428, 721, 322, 294, 257, 33246, 1901, 11, 558, 30, 400, 2010, 472, 37765, 51588], "temperature": 0.0, "avg_logprob": -0.1770154122383364, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.0005860975361429155}, {"id": 462, "seek": 262864, "start": 2653.12, "end": 2657.7599999999998, "text": " is going to be a template. It's like you're going to have clusters of things, you can take the average", "tokens": [51588, 307, 516, 281, 312, 257, 12379, 13, 467, 311, 411, 291, 434, 516, 281, 362, 23313, 295, 721, 11, 291, 393, 747, 264, 4274, 51820], "temperature": 0.0, "avg_logprob": -0.1770154122383364, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.0005860975361429155}, {"id": 463, "seek": 265776, "start": 2657.76, "end": 2663.36, "text": " and say everything that is within a certain distance of that template belongs to this category.", "tokens": [50364, 293, 584, 1203, 300, 307, 1951, 257, 1629, 4560, 295, 300, 12379, 12953, 281, 341, 7719, 13, 50644], "temperature": 0.0, "avg_logprob": -0.12591964857918875, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0012812138302251697}, {"id": 464, "seek": 265776, "start": 2663.36, "end": 2669.1200000000003, "text": " That's that's type one. It's very much the way deep learning models work. And then you and then", "tokens": [50644, 663, 311, 300, 311, 2010, 472, 13, 467, 311, 588, 709, 264, 636, 2452, 2539, 5245, 589, 13, 400, 550, 291, 293, 550, 50932], "temperature": 0.0, "avg_logprob": -0.12591964857918875, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0012812138302251697}, {"id": 465, "seek": 265776, "start": 2669.1200000000003, "end": 2673.6800000000003, "text": " you start adding perception and intuition on top of that, which is very much the type one thing.", "tokens": [50932, 291, 722, 5127, 12860, 293, 24002, 322, 1192, 295, 300, 11, 597, 307, 588, 709, 264, 2010, 472, 551, 13, 51160], "temperature": 0.0, "avg_logprob": -0.12591964857918875, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0012812138302251697}, {"id": 466, "seek": 265776, "start": 2673.6800000000003, "end": 2680.0800000000004, "text": " And the other way you can compare two things is the discrete way, right? You can say these two", "tokens": [51160, 400, 264, 661, 636, 291, 393, 6794, 732, 721, 307, 264, 27706, 636, 11, 558, 30, 509, 393, 584, 613, 732, 51480], "temperature": 0.0, "avg_logprob": -0.12591964857918875, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0012812138302251697}, {"id": 467, "seek": 265776, "start": 2680.0800000000004, "end": 2685.92, "text": " things are exactly the same. They have exactly the same structure. Or maybe the structure of this", "tokens": [51480, 721, 366, 2293, 264, 912, 13, 814, 362, 2293, 264, 912, 3877, 13, 1610, 1310, 264, 3877, 295, 341, 51772], "temperature": 0.0, "avg_logprob": -0.12591964857918875, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0012812138302251697}, {"id": 468, "seek": 268592, "start": 2685.92, "end": 2691.28, "text": " thing is a subset of the structure of this bigger thing. So this creates topology grounded", "tokens": [50364, 551, 307, 257, 25993, 295, 264, 3877, 295, 341, 3801, 551, 13, 407, 341, 7829, 1192, 1793, 23535, 50632], "temperature": 0.0, "avg_logprob": -0.14212519208961558, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0008671980467624962}, {"id": 469, "seek": 268592, "start": 2691.28, "end": 2696.88, "text": " comparisons. So you have the geometry grounded comparison. It's all about distances and templates.", "tokens": [50632, 33157, 13, 407, 291, 362, 264, 18426, 23535, 9660, 13, 467, 311, 439, 466, 22182, 293, 21165, 13, 50912], "temperature": 0.0, "avg_logprob": -0.14212519208961558, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0008671980467624962}, {"id": 470, "seek": 268592, "start": 2696.88, "end": 2702.7200000000003, "text": " And then you have the topology grounded way of comparing things. That's all about exact comparison", "tokens": [50912, 400, 550, 291, 362, 264, 1192, 1793, 23535, 636, 295, 15763, 721, 13, 663, 311, 439, 466, 1900, 9660, 51204], "temperature": 0.0, "avg_logprob": -0.14212519208961558, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0008671980467624962}, {"id": 471, "seek": 268592, "start": 2702.7200000000003, "end": 2709.76, "text": " or finding a sub graph isomorphism. So in the first case, your objects are very much", "tokens": [51204, 420, 5006, 257, 1422, 4295, 307, 32702, 1434, 13, 407, 294, 264, 700, 1389, 11, 428, 6565, 366, 588, 709, 51556], "temperature": 0.0, "avg_logprob": -0.14212519208961558, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0008671980467624962}, {"id": 472, "seek": 268592, "start": 2709.76, "end": 2714.7200000000003, "text": " points in geometric spaces. So they are vectors. And deep learning is always a great fit for this", "tokens": [51556, 2793, 294, 33246, 7673, 13, 407, 436, 366, 18875, 13, 400, 2452, 2539, 307, 1009, 257, 869, 3318, 337, 341, 51804], "temperature": 0.0, "avg_logprob": -0.14212519208961558, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0008671980467624962}, {"id": 473, "seek": 271472, "start": 2714.72, "end": 2719.4399999999996, "text": " sort of stuff. And in the second case, your objects are going to be graphs, right? And you're", "tokens": [50364, 1333, 295, 1507, 13, 400, 294, 264, 1150, 1389, 11, 428, 6565, 366, 516, 281, 312, 24877, 11, 558, 30, 400, 291, 434, 50600], "temperature": 0.0, "avg_logprob": -0.19320501974963267, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.001673373975791037}, {"id": 474, "seek": 271472, "start": 2719.4399999999996, "end": 2722.64, "text": " and you're really looking at the structure of these graphs and substructure and so on.", "tokens": [50600, 293, 291, 434, 534, 1237, 412, 264, 3877, 295, 613, 24877, 293, 4594, 2885, 293, 370, 322, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19320501974963267, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.001673373975791037}, {"id": 475, "seek": 271472, "start": 2722.64, "end": 2730.16, "text": " And you're doing always you're doing exact comparisons. And in practice, most thinking is", "tokens": [50760, 400, 291, 434, 884, 1009, 291, 434, 884, 1900, 33157, 13, 400, 294, 3124, 11, 881, 1953, 307, 51136], "temperature": 0.0, "avg_logprob": -0.19320501974963267, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.001673373975791037}, {"id": 476, "seek": 271472, "start": 2730.16, "end": 2735.4399999999996, "text": " actually kind of some some combination of these two atoms, right? Of these two poles.", "tokens": [51136, 767, 733, 295, 512, 512, 6562, 295, 613, 732, 16871, 11, 558, 30, 2720, 613, 732, 24760, 13, 51400], "temperature": 0.0, "avg_logprob": -0.19320501974963267, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.001673373975791037}, {"id": 477, "seek": 271472, "start": 2736.24, "end": 2742.3199999999997, "text": " You're very rarely just going to say, yeah, this airport is exactly this close to my template", "tokens": [51440, 509, 434, 588, 13752, 445, 516, 281, 584, 11, 1338, 11, 341, 10155, 307, 2293, 341, 1998, 281, 452, 12379, 51744], "temperature": 0.0, "avg_logprob": -0.19320501974963267, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.001673373975791037}, {"id": 478, "seek": 274232, "start": 2742.32, "end": 2746.6400000000003, "text": " of an airport. So it's an airport. You're going to have basically layers upon layers of thinking.", "tokens": [50364, 295, 364, 10155, 13, 407, 309, 311, 364, 10155, 13, 509, 434, 516, 281, 362, 1936, 7914, 3564, 7914, 295, 1953, 13, 50580], "temperature": 0.0, "avg_logprob": -0.12391361823448768, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.0031785843893885612}, {"id": 479, "seek": 274232, "start": 2746.6400000000003, "end": 2750.88, "text": " And some of them are going to be intuitive. Some of them are going to be more about, you know,", "tokens": [50580, 400, 512, 295, 552, 366, 516, 281, 312, 21769, 13, 2188, 295, 552, 366, 516, 281, 312, 544, 466, 11, 291, 458, 11, 50792], "temperature": 0.0, "avg_logprob": -0.12391361823448768, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.0031785843893885612}, {"id": 480, "seek": 274232, "start": 2750.88, "end": 2755.44, "text": " comparing structures and so on. What you're saying is really interesting, right? Because you invoke", "tokens": [50792, 15763, 9227, 293, 370, 322, 13, 708, 291, 434, 1566, 307, 534, 1880, 11, 558, 30, 1436, 291, 41117, 51020], "temperature": 0.0, "avg_logprob": -0.12391361823448768, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.0031785843893885612}, {"id": 481, "seek": 274232, "start": 2755.44, "end": 2762.1600000000003, "text": " the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information,", "tokens": [51020, 264, 34699, 7895, 13960, 17291, 294, 428, 3035, 13, 400, 264, 1558, 456, 307, 300, 257, 5870, 857, 295, 1589, 11, 51356], "temperature": 0.0, "avg_logprob": -0.12391361823448768, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.0031785843893885612}, {"id": 482, "seek": 274232, "start": 2762.1600000000003, "end": 2770.1600000000003, "text": " just like in a kaleidoscope, could be represented widely across experience space. So you say that", "tokens": [51356, 445, 411, 294, 257, 34699, 7895, 13960, 11, 727, 312, 10379, 13371, 2108, 1752, 1901, 13, 407, 291, 584, 300, 51756], "temperature": 0.0, "avg_logprob": -0.12391361823448768, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.0031785843893885612}, {"id": 483, "seek": 277016, "start": 2770.16, "end": 2776.0, "text": " intelligence is literally having some kind of sensitivity to abstract analogies.", "tokens": [50364, 7599, 307, 3736, 1419, 512, 733, 295, 19392, 281, 12649, 16660, 530, 13, 50656], "temperature": 0.0, "avg_logprob": -0.12876220802208047, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001452034804970026}, {"id": 484, "seek": 277016, "start": 2776.0, "end": 2783.2799999999997, "text": " So the intelligence is about being able to face the future unknown future, given your past experience.", "tokens": [50656, 407, 264, 7599, 307, 466, 885, 1075, 281, 1851, 264, 2027, 9841, 2027, 11, 2212, 428, 1791, 1752, 13, 51020], "temperature": 0.0, "avg_logprob": -0.12876220802208047, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001452034804970026}, {"id": 485, "seek": 277016, "start": 2783.2799999999997, "end": 2789.2, "text": " And that's fundamentally requires the future to share some commonalities with the past. And", "tokens": [51020, 400, 300, 311, 17879, 7029, 264, 2027, 281, 2073, 512, 2689, 16110, 365, 264, 1791, 13, 400, 51316], "temperature": 0.0, "avg_logprob": -0.12876220802208047, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001452034804970026}, {"id": 486, "seek": 277016, "start": 2789.2, "end": 2794.7999999999997, "text": " that's that's the idea of the kaleidoscope hypothesis that the universe and our lives", "tokens": [51316, 300, 311, 300, 311, 264, 1558, 295, 264, 34699, 7895, 13960, 17291, 300, 264, 6445, 293, 527, 2909, 51596], "temperature": 0.0, "avg_logprob": -0.12876220802208047, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001452034804970026}, {"id": 487, "seek": 279480, "start": 2794.88, "end": 2801.28, "text": " are made of lots of repeated atoms of structure. And in fact, if you look at the source,", "tokens": [50368, 366, 1027, 295, 3195, 295, 10477, 16871, 295, 3877, 13, 400, 294, 1186, 11, 498, 291, 574, 412, 264, 4009, 11, 50688], "temperature": 0.0, "avg_logprob": -0.08748781358873522, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.014389125630259514}, {"id": 488, "seek": 279480, "start": 2801.28, "end": 2804.6400000000003, "text": " there are very few things that are that are unique that are kind of like", "tokens": [50688, 456, 366, 588, 1326, 721, 300, 366, 300, 366, 3845, 300, 366, 733, 295, 411, 50856], "temperature": 0.0, "avg_logprob": -0.08748781358873522, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.014389125630259514}, {"id": 489, "seek": 279480, "start": 2804.6400000000003, "end": 2809.36, "text": " the grains of sand that are at the origin of all the different kinds of moving patterns you", "tokens": [50856, 264, 22908, 295, 4932, 300, 366, 412, 264, 4957, 295, 439, 264, 819, 3685, 295, 2684, 8294, 291, 51092], "temperature": 0.0, "avg_logprob": -0.08748781358873522, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.014389125630259514}, {"id": 490, "seek": 279480, "start": 2809.36, "end": 2814.0800000000004, "text": " can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe", "tokens": [51092, 393, 536, 294, 264, 34699, 7895, 13960, 11, 558, 30, 407, 264, 733, 295, 411, 35698, 3877, 16212, 294, 264, 6445, 51328], "temperature": 0.0, "avg_logprob": -0.08748781358873522, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.014389125630259514}, {"id": 491, "seek": 279480, "start": 2814.0800000000004, "end": 2821.92, "text": " is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see", "tokens": [51328, 307, 588, 1359, 11, 457, 309, 307, 10477, 294, 439, 3685, 295, 21669, 11, 558, 30, 400, 264, 1558, 307, 300, 498, 291, 536, 51720], "temperature": 0.0, "avg_logprob": -0.08748781358873522, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.014389125630259514}, {"id": 492, "seek": 282192, "start": 2821.92, "end": 2827.36, "text": " two things in the universe that look similar to each other or that share some commonalities,", "tokens": [50364, 732, 721, 294, 264, 6445, 300, 574, 2531, 281, 1184, 661, 420, 300, 2073, 512, 2689, 16110, 11, 50636], "temperature": 0.0, "avg_logprob": -0.13773105300475505, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0013382061151787639}, {"id": 493, "seek": 282192, "start": 2827.36, "end": 2833.36, "text": " a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is", "tokens": [50636, 257, 1422, 34091, 11, 1310, 11, 309, 17879, 1355, 300, 436, 808, 490, 264, 912, 551, 13, 400, 300, 551, 307, 50936], "temperature": 0.0, "avg_logprob": -0.13773105300475505, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0013382061151787639}, {"id": 494, "seek": 282192, "start": 2833.36, "end": 2837.28, "text": " going to be is going to be an abstraction. We'll be one of these grains of sand in your", "tokens": [50936, 516, 281, 312, 307, 516, 281, 312, 364, 37765, 13, 492, 603, 312, 472, 295, 613, 22908, 295, 4932, 294, 428, 51132], "temperature": 0.0, "avg_logprob": -0.13773105300475505, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0013382061151787639}, {"id": 495, "seek": 282192, "start": 2837.28, "end": 2844.8, "text": " in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering", "tokens": [51132, 294, 428, 34699, 7895, 13960, 420, 22908, 295, 4276, 11, 767, 13, 400, 7599, 307, 439, 466, 9943, 7043, 51508], "temperature": 0.0, "avg_logprob": -0.13773105300475505, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0013382061151787639}, {"id": 496, "seek": 282192, "start": 2844.8, "end": 2850.4, "text": " the universe to get back to this source of intrinsic complexity in the universe to get", "tokens": [51508, 264, 6445, 281, 483, 646, 281, 341, 4009, 295, 35698, 14024, 294, 264, 6445, 281, 483, 51788], "temperature": 0.0, "avg_logprob": -0.13773105300475505, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.0013382061151787639}, {"id": 497, "seek": 285040, "start": 2850.4, "end": 2855.36, "text": " back to these abstractions. I think the heart of this conversation goes back thousands of years", "tokens": [50364, 646, 281, 613, 12649, 626, 13, 286, 519, 264, 1917, 295, 341, 3761, 1709, 646, 5383, 295, 924, 50612], "temperature": 0.0, "avg_logprob": -0.09839322274191338, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0015008695190772414}, {"id": 498, "seek": 285040, "start": 2855.36, "end": 2859.12, "text": " because what we're talking about right now is a lot of say, Platonism, right? Which is that there", "tokens": [50612, 570, 437, 321, 434, 1417, 466, 558, 586, 307, 257, 688, 295, 584, 11, 17461, 266, 1434, 11, 558, 30, 3013, 307, 300, 456, 50800], "temperature": 0.0, "avg_logprob": -0.09839322274191338, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0015008695190772414}, {"id": 499, "seek": 285040, "start": 2859.12, "end": 2864.4, "text": " are these ideal abstract structures. And of course, they they really thought of them as actually", "tokens": [50800, 366, 613, 7157, 12649, 9227, 13, 400, 295, 1164, 11, 436, 436, 534, 1194, 295, 552, 382, 767, 51064], "temperature": 0.0, "avg_logprob": -0.09839322274191338, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0015008695190772414}, {"id": 500, "seek": 285040, "start": 2864.4, "end": 2870.7200000000003, "text": " existing in some universe. But you know, even if they don't exist in some reality, they at least", "tokens": [51064, 6741, 294, 512, 6445, 13, 583, 291, 458, 11, 754, 498, 436, 500, 380, 2514, 294, 512, 4103, 11, 436, 412, 1935, 51380], "temperature": 0.0, "avg_logprob": -0.09839322274191338, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0015008695190772414}, {"id": 501, "seek": 285040, "start": 2870.7200000000003, "end": 2876.1600000000003, "text": " exist in concept. And it strikes at the heart of this duality that's always been a very", "tokens": [51380, 2514, 294, 3410, 13, 400, 309, 16750, 412, 264, 1917, 295, 341, 11848, 507, 300, 311, 1009, 668, 257, 588, 51652], "temperature": 0.0, "avg_logprob": -0.09839322274191338, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0015008695190772414}, {"id": 502, "seek": 287616, "start": 2876.8799999999997, "end": 2880.96, "text": " that's been one of the central mystery, really, of a lot of human thinking, which is", "tokens": [50400, 300, 311, 668, 472, 295, 264, 5777, 11422, 11, 534, 11, 295, 257, 688, 295, 1952, 1953, 11, 597, 307, 50604], "temperature": 0.0, "avg_logprob": -0.09111933026994978, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.0003799548139795661}, {"id": 503, "seek": 287616, "start": 2881.6, "end": 2887.12, "text": " particle versus wave, you know, discrete versus continuous abstract versus the real versus the", "tokens": [50636, 12359, 5717, 5772, 11, 291, 458, 11, 27706, 5717, 10957, 12649, 5717, 264, 957, 5717, 264, 50912], "temperature": 0.0, "avg_logprob": -0.09111933026994978, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.0003799548139795661}, {"id": 504, "seek": 287616, "start": 2887.12, "end": 2892.16, "text": " messy. And you know, I think you pointed out, you definitely pointed this out in this call. But", "tokens": [50912, 16191, 13, 400, 291, 458, 11, 286, 519, 291, 10932, 484, 11, 291, 2138, 10932, 341, 484, 294, 341, 818, 13, 583, 51164], "temperature": 0.0, "avg_logprob": -0.09111933026994978, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.0003799548139795661}, {"id": 505, "seek": 287616, "start": 2892.16, "end": 2897.04, "text": " I think also in some of your papers that in your view, you know, let's say the ultimate solution", "tokens": [51164, 286, 519, 611, 294, 512, 295, 428, 10577, 300, 294, 428, 1910, 11, 291, 458, 11, 718, 311, 584, 264, 9705, 3827, 51408], "temperature": 0.0, "avg_logprob": -0.09111933026994978, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.0003799548139795661}, {"id": 506, "seek": 287616, "start": 2897.04, "end": 2903.12, "text": " or whatever of creating artificial intelligence or synthetic intelligence or whatever is a", "tokens": [51408, 420, 2035, 295, 4084, 11677, 7599, 420, 23420, 7599, 420, 2035, 307, 257, 51712], "temperature": 0.0, "avg_logprob": -0.09111933026994978, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.0003799548139795661}, {"id": 507, "seek": 290312, "start": 2903.2, "end": 2908.4, "text": " hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers.", "tokens": [50368, 13051, 1185, 300, 393, 360, 1293, 295, 613, 3467, 295, 21577, 11, 1310, 294, 733, 295, 3866, 7914, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10117091303286345, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0035932185128331184}, {"id": 508, "seek": 290312, "start": 2909.04, "end": 2914.88, "text": " And, you know, I'm kind of curious, where is the state of the art now with actually implementing", "tokens": [50660, 400, 11, 291, 458, 11, 286, 478, 733, 295, 6369, 11, 689, 307, 264, 1785, 295, 264, 1523, 586, 365, 767, 18114, 50952], "temperature": 0.0, "avg_logprob": -0.10117091303286345, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0035932185128331184}, {"id": 509, "seek": 290312, "start": 2915.68, "end": 2919.7599999999998, "text": " hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the", "tokens": [50992, 13051, 3652, 11, 291, 458, 11, 746, 411, 11, 286, 500, 380, 458, 11, 307, 309, 29247, 9590, 30, 1119, 309, 264, 51196], "temperature": 0.0, "avg_logprob": -0.10117091303286345, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0035932185128331184}, {"id": 510, "seek": 290312, "start": 2919.7599999999998, "end": 2925.8399999999997, "text": " topological neural networks that we talked about? Where where lies the direction of some type of a", "tokens": [51196, 1192, 4383, 18161, 9590, 300, 321, 2825, 466, 30, 2305, 689, 9134, 264, 3513, 295, 512, 2010, 295, 257, 51500], "temperature": 0.0, "avg_logprob": -0.10117091303286345, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0035932185128331184}, {"id": 511, "seek": 292584, "start": 2925.84, "end": 2933.6000000000004, "text": " hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you", "tokens": [50364, 13051, 1185, 300, 294, 257, 26787, 636, 307, 8189, 295, 884, 1293, 295, 613, 14068, 295, 21577, 11, 498, 291, 50752], "temperature": 0.0, "avg_logprob": -0.13876366108021837, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004980293568223715}, {"id": 512, "seek": 292584, "start": 2933.6000000000004, "end": 2939.28, "text": " will? Yeah, that's a great question. So I think this is definitely an active field of research,", "tokens": [50752, 486, 30, 865, 11, 300, 311, 257, 869, 1168, 13, 407, 286, 519, 341, 307, 2138, 364, 4967, 2519, 295, 2132, 11, 51036], "temperature": 0.0, "avg_logprob": -0.13876366108021837, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004980293568223715}, {"id": 513, "seek": 292584, "start": 2939.28, "end": 2945.36, "text": " but I think the most promising direction right now is going to be discrete search very much. So", "tokens": [51036, 457, 286, 519, 264, 881, 20257, 3513, 558, 586, 307, 516, 281, 312, 27706, 3164, 588, 709, 13, 407, 51340], "temperature": 0.0, "avg_logprob": -0.13876366108021837, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004980293568223715}, {"id": 514, "seek": 292584, "start": 2945.36, "end": 2949.76, "text": " a system that is discrete search centric that has a DSA and so on. And that's one of the", "tokens": [51340, 257, 1185, 300, 307, 27706, 3164, 1489, 1341, 300, 575, 257, 413, 8886, 293, 370, 322, 13, 400, 300, 311, 472, 295, 264, 51560], "temperature": 0.0, "avg_logprob": -0.13876366108021837, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.004980293568223715}, {"id": 515, "seek": 294976, "start": 2949.76, "end": 2955.36, "text": " it's basically just problems in this engine. But it is getting lots of help from deep learning", "tokens": [50364, 309, 311, 1936, 445, 2740, 294, 341, 2848, 13, 583, 309, 307, 1242, 3195, 295, 854, 490, 2452, 2539, 50644], "temperature": 0.0, "avg_logprob": -0.16056819491916233, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.008181690238416195}, {"id": 516, "seek": 294976, "start": 2955.36, "end": 2961.6800000000003, "text": " models. And there are two ways in which you can incorporate this type one sort of thinking into", "tokens": [50644, 5245, 13, 400, 456, 366, 732, 2098, 294, 597, 291, 393, 16091, 341, 2010, 472, 1333, 295, 1953, 666, 50960], "temperature": 0.0, "avg_logprob": -0.16056819491916233, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.008181690238416195}, {"id": 517, "seek": 294976, "start": 2961.6800000000003, "end": 2970.0800000000004, "text": " a phenomenally type two centric system. So one way is so basically, you want to apply deep learning", "tokens": [50960, 257, 9388, 379, 2010, 732, 1489, 1341, 1185, 13, 407, 472, 636, 307, 370, 1936, 11, 291, 528, 281, 3079, 2452, 2539, 51380], "temperature": 0.0, "avg_logprob": -0.16056819491916233, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.008181690238416195}, {"id": 518, "seek": 294976, "start": 2970.0800000000004, "end": 2976.96, "text": " to any sorts of data sets where you have an abundance of data, and your data is interpreted.", "tokens": [51380, 281, 604, 7527, 295, 1412, 6352, 689, 291, 362, 364, 23391, 295, 1412, 11, 293, 428, 1412, 307, 26749, 13, 51724], "temperature": 0.0, "avg_logprob": -0.16056819491916233, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.008181690238416195}, {"id": 519, "seek": 297696, "start": 2976.96, "end": 2981.84, "text": " One example would be being able to easily play models to generate a sort of like perception", "tokens": [50364, 1485, 1365, 576, 312, 885, 1075, 281, 3612, 862, 5245, 281, 8460, 257, 1333, 295, 411, 12860, 50608], "temperature": 0.0, "avg_logprob": -0.18244559611749211, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.002243851311504841}, {"id": 520, "seek": 297696, "start": 2981.84, "end": 2990.08, "text": " DSL that your discrete search process can build upon. So look at art, art tasks, for instance,", "tokens": [50608, 15816, 43, 300, 428, 27706, 3164, 1399, 393, 1322, 3564, 13, 407, 574, 412, 1523, 11, 1523, 9608, 11, 337, 5197, 11, 51020], "temperature": 0.0, "avg_logprob": -0.18244559611749211, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.002243851311504841}, {"id": 521, "seek": 297696, "start": 2990.08, "end": 2994.16, "text": " a human that is looking at art tasks, the very first layer through which they're approaching", "tokens": [51020, 257, 1952, 300, 307, 1237, 412, 1523, 9608, 11, 264, 588, 700, 4583, 807, 597, 436, 434, 14908, 51224], "temperature": 0.0, "avg_logprob": -0.18244559611749211, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.002243851311504841}, {"id": 522, "seek": 297696, "start": 2994.16, "end": 2999.6, "text": " the art task is by applying basically perception primitives to the grid they're looking at. They", "tokens": [51224, 264, 1523, 5633, 307, 538, 9275, 1936, 12860, 2886, 38970, 281, 264, 10748, 436, 434, 1237, 412, 13, 814, 51496], "temperature": 0.0, "avg_logprob": -0.18244559611749211, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.002243851311504841}, {"id": 523, "seek": 297696, "start": 2999.6, "end": 3005.04, "text": " are not actually analyzing the grid in a in a discrete way like cell by cell, object by object,", "tokens": [51496, 366, 406, 767, 23663, 264, 10748, 294, 257, 294, 257, 27706, 636, 411, 2815, 538, 2815, 11, 2657, 538, 2657, 11, 51768], "temperature": 0.0, "avg_logprob": -0.18244559611749211, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.002243851311504841}, {"id": 524, "seek": 300504, "start": 3005.2799999999997, "end": 3010.32, "text": " they're approaching it holistically, like what do they see? And these outputs can be discrete", "tokens": [50376, 436, 434, 14908, 309, 4091, 20458, 11, 411, 437, 360, 436, 536, 30, 400, 613, 23930, 393, 312, 27706, 50628], "temperature": 0.0, "avg_logprob": -0.2092924294648347, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0006355183431878686}, {"id": 525, "seek": 300504, "start": 3010.32, "end": 3014.56, "text": " concepts. And then you can start you can start applying the script reasoning to them. So generating", "tokens": [50628, 10392, 13, 400, 550, 291, 393, 722, 291, 393, 722, 9275, 264, 5755, 21577, 281, 552, 13, 407, 17746, 50840], "temperature": 0.0, "avg_logprob": -0.2092924294648347, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0006355183431878686}, {"id": 526, "seek": 300504, "start": 3014.56, "end": 3020.16, "text": " the DSL. And by the way, the reason it's possible is because humans have access to tons of visual", "tokens": [50840, 264, 15816, 43, 13, 400, 538, 264, 636, 11, 264, 1778, 309, 311, 1944, 307, 570, 6255, 362, 2105, 281, 9131, 295, 5056, 51120], "temperature": 0.0, "avg_logprob": -0.2092924294648347, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0006355183431878686}, {"id": 527, "seek": 300504, "start": 3020.16, "end": 3025.84, "text": " data and these different frames share lots of commonalities, right? So it is an interpolative", "tokens": [51120, 1412, 293, 613, 819, 12083, 2073, 3195, 295, 2689, 16110, 11, 558, 30, 407, 309, 307, 364, 44902, 1166, 51404], "temperature": 0.0, "avg_logprob": -0.2092924294648347, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0006355183431878686}, {"id": 528, "seek": 300504, "start": 3025.84, "end": 3029.68, "text": " space where deep learning is relevant, where intuition and perception are relevant. And the", "tokens": [51404, 1901, 689, 2452, 2539, 307, 7340, 11, 689, 24002, 293, 12860, 366, 7340, 13, 400, 264, 51596], "temperature": 0.0, "avg_logprob": -0.2092924294648347, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0006355183431878686}, {"id": 529, "seek": 302968, "start": 3029.68, "end": 3035.9199999999996, "text": " other way, which is is is much more difficult and much, much more subtle thing is basically being", "tokens": [50364, 661, 636, 11, 597, 307, 307, 307, 709, 544, 2252, 293, 709, 11, 709, 544, 13743, 551, 307, 1936, 885, 50676], "temperature": 0.0, "avg_logprob": -0.20016304282254951, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001980829518288374}, {"id": 530, "seek": 302968, "start": 3035.9199999999996, "end": 3043.04, "text": " able to provide guidance to the discrete search process, basically, because even though one single", "tokens": [50676, 1075, 281, 2893, 10056, 281, 264, 27706, 3164, 1399, 11, 1936, 11, 570, 754, 1673, 472, 2167, 51032], "temperature": 0.0, "avg_logprob": -0.20016304282254951, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001980829518288374}, {"id": 531, "seek": 302968, "start": 3043.04, "end": 3047.6, "text": " program, so learning one single problem, for instance, for an art task is not a good fit", "tokens": [51032, 1461, 11, 370, 2539, 472, 2167, 1154, 11, 337, 5197, 11, 337, 364, 1523, 5633, 307, 406, 257, 665, 3318, 51260], "temperature": 0.0, "avg_logprob": -0.20016304282254951, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001980829518288374}, {"id": 532, "seek": 302968, "start": 3047.6, "end": 3052.56, "text": " for deep learning model at all, because you only have a handful of examples to learn from.", "tokens": [51260, 337, 2452, 2539, 2316, 412, 439, 11, 570, 291, 787, 362, 257, 16458, 295, 5110, 281, 1466, 490, 13, 51508], "temperature": 0.0, "avg_logprob": -0.20016304282254951, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.001980829518288374}, {"id": 533, "seek": 305256, "start": 3053.36, "end": 3059.92, "text": " And the program is super discrete. It's not really easily embeddable in this movement.", "tokens": [50404, 400, 264, 1461, 307, 1687, 27706, 13, 467, 311, 406, 534, 3612, 12240, 67, 712, 294, 341, 3963, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1929925031001025, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.001301275216974318}, {"id": 534, "seek": 305256, "start": 3059.92, "end": 3064.7999999999997, "text": " However, here's the thing, the space of all possible programs, for instance, the space of", "tokens": [50732, 2908, 11, 510, 311, 264, 551, 11, 264, 1901, 295, 439, 1944, 4268, 11, 337, 5197, 11, 264, 1901, 295, 50976], "temperature": 0.0, "avg_logprob": -0.1929925031001025, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.001301275216974318}, {"id": 535, "seek": 305256, "start": 3064.7999999999997, "end": 3069.12, "text": " all possible art tasks and all possible programs that solve art tasks is actually", "tokens": [50976, 439, 1944, 1523, 9608, 293, 439, 1944, 4268, 300, 5039, 1523, 9608, 307, 767, 51192], "temperature": 0.0, "avg_logprob": -0.1929925031001025, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.001301275216974318}, {"id": 536, "seek": 305256, "start": 3069.12, "end": 3074.7999999999997, "text": " very likely going to be interpolative, at least to some extent. And so you can imagine a deep", "tokens": [51192, 588, 3700, 516, 281, 312, 44902, 1166, 11, 412, 1935, 281, 512, 8396, 13, 400, 370, 291, 393, 3811, 257, 2452, 51476], "temperature": 0.0, "avg_logprob": -0.1929925031001025, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.001301275216974318}, {"id": 537, "seek": 305256, "start": 3074.7999999999997, "end": 3081.04, "text": " learning model that has enough experience with with these problems and the algorithmic solution", "tokens": [51476, 2539, 2316, 300, 575, 1547, 1752, 365, 365, 613, 2740, 293, 264, 9284, 299, 3827, 51788], "temperature": 0.0, "avg_logprob": -0.1929925031001025, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.001301275216974318}, {"id": 538, "seek": 308104, "start": 3081.04, "end": 3086.24, "text": " that it can it can start providing directions to the search to the discrete system. So", "tokens": [50364, 300, 309, 393, 309, 393, 722, 6530, 11095, 281, 264, 3164, 281, 264, 27706, 1185, 13, 407, 50624], "temperature": 0.0, "avg_logprob": -0.18120837661455264, "compression_ratio": 1.897872340425532, "no_speech_prob": 0.001446991111151874}, {"id": 539, "seek": 308104, "start": 3087.6, "end": 3092.88, "text": " basically, you're in a kind of like you have, yeah, you have like layers of", "tokens": [50692, 1936, 11, 291, 434, 294, 257, 733, 295, 411, 291, 362, 11, 1338, 11, 291, 362, 411, 7914, 295, 50956], "temperature": 0.0, "avg_logprob": -0.18120837661455264, "compression_ratio": 1.897872340425532, "no_speech_prob": 0.001446991111151874}, {"id": 540, "seek": 308104, "start": 3094.0, "end": 3099.12, "text": " of learning the lowest layer is going to be perceptive. It's going to be learned across many", "tokens": [51012, 295, 2539, 264, 12437, 4583, 307, 516, 281, 312, 43276, 488, 13, 467, 311, 516, 281, 312, 3264, 2108, 867, 51268], "temperature": 0.0, "avg_logprob": -0.18120837661455264, "compression_ratio": 1.897872340425532, "no_speech_prob": 0.001446991111151874}, {"id": 541, "seek": 308104, "start": 3099.12, "end": 3104.08, "text": " different tasks and many different environments. It's going to be type type one, then you're going", "tokens": [51268, 819, 9608, 293, 867, 819, 12388, 13, 467, 311, 516, 281, 312, 2010, 2010, 472, 11, 550, 291, 434, 516, 51516], "temperature": 0.0, "avg_logprob": -0.18120837661455264, "compression_ratio": 1.897872340425532, "no_speech_prob": 0.001446991111151874}, {"id": 542, "seek": 308104, "start": 3104.08, "end": 3110.48, "text": " to have the context specific on the fly problem solving system that's going to be type two.", "tokens": [51516, 281, 362, 264, 4319, 2685, 322, 264, 3603, 1154, 12606, 1185, 300, 311, 516, 281, 312, 2010, 732, 13, 51836], "temperature": 0.0, "avg_logprob": -0.18120837661455264, "compression_ratio": 1.897872340425532, "no_speech_prob": 0.001446991111151874}, {"id": 543, "seek": 311104, "start": 3111.2, "end": 3115.2799999999997, "text": " And the reason is going to be possible and efficient is because it's going to be guided", "tokens": [50372, 400, 264, 1778, 307, 516, 281, 312, 1944, 293, 7148, 307, 570, 309, 311, 516, 281, 312, 19663, 50576], "temperature": 0.0, "avg_logprob": -0.09715839668556496, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.0006055759149603546}, {"id": 544, "seek": 311104, "start": 3115.2799999999997, "end": 3119.84, "text": " by this upper layer, which is going to be type one, which is also going to be trained", "tokens": [50576, 538, 341, 6597, 4583, 11, 597, 307, 516, 281, 312, 2010, 472, 11, 597, 307, 611, 516, 281, 312, 8895, 50804], "temperature": 0.0, "avg_logprob": -0.09715839668556496, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.0006055759149603546}, {"id": 545, "seek": 311104, "start": 3119.84, "end": 3124.96, "text": " from a very, very long experience across many different problems and tasks. And it is able", "tokens": [50804, 490, 257, 588, 11, 588, 938, 1752, 2108, 867, 819, 2740, 293, 9608, 13, 400, 309, 307, 1075, 51060], "temperature": 0.0, "avg_logprob": -0.09715839668556496, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.0006055759149603546}, {"id": 546, "seek": 311104, "start": 3124.96, "end": 3132.32, "text": " to do interpolation between different tasks. So can I challenge you a little bit maybe because", "tokens": [51060, 281, 360, 44902, 399, 1296, 819, 9608, 13, 407, 393, 286, 3430, 291, 257, 707, 857, 1310, 570, 51428], "temperature": 0.0, "avg_logprob": -0.09715839668556496, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.0006055759149603546}, {"id": 547, "seek": 311104, "start": 3132.32, "end": 3137.7599999999998, "text": " you say maybe, you know, all of these problems and what humans do is a bit of an interpolate", "tokens": [51428, 291, 584, 1310, 11, 291, 458, 11, 439, 295, 613, 2740, 293, 437, 6255, 360, 307, 257, 857, 295, 364, 44902, 473, 51700], "temperature": 0.0, "avg_logprob": -0.09715839668556496, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.0006055759149603546}, {"id": 548, "seek": 313776, "start": 3137.76, "end": 3145.2000000000003, "text": " like an interpolation between the interpolative systems and the discrete systems. And I see that", "tokens": [50364, 411, 364, 44902, 399, 1296, 264, 44902, 1166, 3652, 293, 264, 27706, 3652, 13, 400, 286, 536, 300, 50736], "temperature": 0.0, "avg_logprob": -0.10296632978651259, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0042630210518836975}, {"id": 549, "seek": 313776, "start": 3145.2000000000003, "end": 3152.0, "text": " going for, you know, something like an arc task or or if you really write code. But if you really", "tokens": [50736, 516, 337, 11, 291, 458, 11, 746, 411, 364, 10346, 5633, 420, 420, 498, 291, 534, 2464, 3089, 13, 583, 498, 291, 534, 51076], "temperature": 0.0, "avg_logprob": -0.10296632978651259, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0042630210518836975}, {"id": 550, "seek": 313776, "start": 3152.0, "end": 3158.0, "text": " come to let's say, let's say the highest levels of human intelligence, which to me seems to be", "tokens": [51076, 808, 281, 718, 311, 584, 11, 718, 311, 584, 264, 6343, 4358, 295, 1952, 7599, 11, 597, 281, 385, 2544, 281, 312, 51376], "temperature": 0.0, "avg_logprob": -0.10296632978651259, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0042630210518836975}, {"id": 551, "seek": 313776, "start": 3158.0, "end": 3166.88, "text": " navigating social situations, which is is is ultimately is super complex. And I can imagine", "tokens": [51376, 32054, 2093, 6851, 11, 597, 307, 307, 307, 6284, 307, 1687, 3997, 13, 400, 286, 393, 3811, 51820], "temperature": 0.0, "avg_logprob": -0.10296632978651259, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0042630210518836975}, {"id": 552, "seek": 316688, "start": 3166.96, "end": 3173.84, "text": " something like the graph structure you're referring to be that being, let's say I come into a room", "tokens": [50368, 746, 411, 264, 4295, 3877, 291, 434, 13761, 281, 312, 300, 885, 11, 718, 311, 584, 286, 808, 666, 257, 1808, 50712], "temperature": 0.0, "avg_logprob": -0.11561647415161133, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0057216305285692215}, {"id": 553, "seek": 316688, "start": 3173.84, "end": 3181.44, "text": " and I see the graphs as, you know, what kind of social dynamics exist in this room, you know,", "tokens": [50712, 293, 286, 536, 264, 24877, 382, 11, 291, 458, 11, 437, 733, 295, 2093, 15679, 2514, 294, 341, 1808, 11, 291, 458, 11, 51092], "temperature": 0.0, "avg_logprob": -0.11561647415161133, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0057216305285692215}, {"id": 554, "seek": 316688, "start": 3181.44, "end": 3187.2000000000003, "text": " this is the father of this person, and that person's kind of angry at me. And so I need to,", "tokens": [51092, 341, 307, 264, 3086, 295, 341, 954, 11, 293, 300, 954, 311, 733, 295, 6884, 412, 385, 13, 400, 370, 286, 643, 281, 11, 51380], "temperature": 0.0, "avg_logprob": -0.11561647415161133, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0057216305285692215}, {"id": 555, "seek": 316688, "start": 3187.2000000000003, "end": 3195.36, "text": " you know, do something. And my question is, how often is that really a disk like how often can", "tokens": [51380, 291, 458, 11, 360, 746, 13, 400, 452, 1168, 307, 11, 577, 2049, 307, 300, 534, 257, 12355, 411, 577, 2049, 393, 51788], "temperature": 0.0, "avg_logprob": -0.11561647415161133, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0057216305285692215}, {"id": 556, "seek": 319536, "start": 3195.36, "end": 3202.0, "text": " you really map this in a discrete way to another graph? Isn't isn't every situation going to be", "tokens": [50364, 291, 534, 4471, 341, 294, 257, 27706, 636, 281, 1071, 4295, 30, 6998, 380, 1943, 380, 633, 2590, 516, 281, 312, 50696], "temperature": 0.0, "avg_logprob": -0.10625617672698666, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.008175690658390522}, {"id": 557, "seek": 319536, "start": 3202.0, "end": 3209.1200000000003, "text": " a little bit different, even in terms of its graph structure? And, you know, even if in an arc task,", "tokens": [50696, 257, 707, 857, 819, 11, 754, 294, 2115, 295, 1080, 4295, 3877, 30, 400, 11, 291, 458, 11, 754, 498, 294, 364, 10346, 5633, 11, 51052], "temperature": 0.0, "avg_logprob": -0.10625617672698666, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.008175690658390522}, {"id": 558, "seek": 319536, "start": 3209.1200000000003, "end": 3216.6400000000003, "text": " a line is just like a little bit squiggled, any program synthesis approach would have a hard", "tokens": [51052, 257, 1622, 307, 445, 411, 257, 707, 857, 2339, 6249, 1493, 11, 604, 1461, 30252, 3109, 576, 362, 257, 1152, 51428], "temperature": 0.0, "avg_logprob": -0.10625617672698666, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.008175690658390522}, {"id": 559, "seek": 319536, "start": 3216.6400000000003, "end": 3222.1600000000003, "text": " time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like", "tokens": [51428, 565, 365, 309, 11, 286, 841, 11, 420, 360, 291, 519, 11, 420, 360, 291, 519, 286, 478, 29227, 746, 510, 30, 1743, 51704], "temperature": 0.0, "avg_logprob": -0.10625617672698666, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.008175690658390522}, {"id": 560, "seek": 322216, "start": 3222.64, "end": 3229.8399999999997, "text": " how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction", "tokens": [50388, 577, 27706, 307, 534, 27706, 30, 663, 311, 264, 4334, 295, 37765, 13, 440, 4334, 295, 37765, 50748], "temperature": 0.0, "avg_logprob": -0.13911925292596583, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.009693549014627934}, {"id": 561, "seek": 322216, "start": 3229.8399999999997, "end": 3236.8799999999997, "text": " is to erase the irrelevant differences between different instances of the thing and focus on", "tokens": [50748, 307, 281, 23525, 264, 28682, 7300, 1296, 819, 14519, 295, 264, 551, 293, 1879, 322, 51100], "temperature": 0.0, "avg_logprob": -0.13911925292596583, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.009693549014627934}, {"id": 562, "seek": 322216, "start": 3237.92, "end": 3243.7599999999998, "text": " the commonalities that matter. So like if the squiggled in your line is not relevant,", "tokens": [51152, 264, 2689, 16110, 300, 1871, 13, 407, 411, 498, 264, 2339, 6249, 1493, 294, 428, 1622, 307, 406, 7340, 11, 51444], "temperature": 0.0, "avg_logprob": -0.13911925292596583, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.009693549014627934}, {"id": 563, "seek": 322216, "start": 3243.7599999999998, "end": 3248.24, "text": " then the proper abstraction for a line should abstract it away. I was going to pick up on that", "tokens": [51444, 550, 264, 2296, 37765, 337, 257, 1622, 820, 12649, 309, 1314, 13, 286, 390, 516, 281, 1888, 493, 322, 300, 51668], "temperature": 0.0, "avg_logprob": -0.13911925292596583, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.009693549014627934}, {"id": 564, "seek": 324824, "start": 3248.24, "end": 3253.3599999999997, "text": " because your main point basically is that program based abstraction is more powerful", "tokens": [50364, 570, 428, 2135, 935, 1936, 307, 300, 1461, 2361, 37765, 307, 544, 4005, 50620], "temperature": 0.0, "avg_logprob": -0.09852859708997938, "compression_ratio": 1.7, "no_speech_prob": 0.010950162075459957}, {"id": 565, "seek": 324824, "start": 3253.3599999999997, "end": 3256.8799999999997, "text": " than geometric based abstraction, because topology is robust to small perturbations,", "tokens": [50620, 813, 33246, 2361, 37765, 11, 570, 1192, 1793, 307, 13956, 281, 1359, 40468, 763, 11, 50796], "temperature": 0.0, "avg_logprob": -0.09852859708997938, "compression_ratio": 1.7, "no_speech_prob": 0.010950162075459957}, {"id": 566, "seek": 324824, "start": 3256.8799999999997, "end": 3261.8399999999997, "text": " but it's more than that. It comes back to these analogies, right? So we actually have functions", "tokens": [50796, 457, 309, 311, 544, 813, 300, 13, 467, 1487, 646, 281, 613, 16660, 530, 11, 558, 30, 407, 321, 767, 362, 6828, 51044], "temperature": 0.0, "avg_logprob": -0.09852859708997938, "compression_ratio": 1.7, "no_speech_prob": 0.010950162075459957}, {"id": 567, "seek": 324824, "start": 3261.8399999999997, "end": 3267.4399999999996, "text": " and abstractions in our mind that as you say, will take away all of the relevant differences,", "tokens": [51044, 293, 12649, 626, 294, 527, 1575, 300, 382, 291, 584, 11, 486, 747, 1314, 439, 295, 264, 7340, 7300, 11, 51324], "temperature": 0.0, "avg_logprob": -0.09852859708997938, "compression_ratio": 1.7, "no_speech_prob": 0.010950162075459957}, {"id": 568, "seek": 324824, "start": 3267.4399999999996, "end": 3274.3199999999997, "text": " but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you", "tokens": [51324, 457, 1879, 322, 437, 311, 1845, 1196, 293, 437, 311, 2674, 22395, 13, 865, 11, 2293, 13, 407, 294, 294, 264, 955, 2020, 11, 360, 291, 51668], "temperature": 0.0, "avg_logprob": -0.09852859708997938, "compression_ratio": 1.7, "no_speech_prob": 0.010950162075459957}, {"id": 569, "seek": 327432, "start": 3274.32, "end": 3281.04, "text": " think the type one and type two reasoning are really different or is there also a continuum", "tokens": [50364, 519, 264, 2010, 472, 293, 2010, 732, 21577, 366, 534, 819, 420, 307, 456, 611, 257, 36120, 50700], "temperature": 0.0, "avg_logprob": -0.15342593837428736, "compression_ratio": 1.8252032520325203, "no_speech_prob": 0.01902134343981743}, {"id": 570, "seek": 327432, "start": 3281.04, "end": 3286.4, "text": " between them? Like you say we need we need hybrid systems, but is there something,", "tokens": [50700, 1296, 552, 30, 1743, 291, 584, 321, 643, 321, 643, 13051, 3652, 11, 457, 307, 456, 746, 11, 50968], "temperature": 0.0, "avg_logprob": -0.15342593837428736, "compression_ratio": 1.8252032520325203, "no_speech_prob": 0.01902134343981743}, {"id": 571, "seek": 327432, "start": 3287.6800000000003, "end": 3291.28, "text": " right? Because they're both they're both in the brain, they're both on the same neurons,", "tokens": [51032, 558, 30, 1436, 436, 434, 1293, 436, 434, 1293, 294, 264, 3567, 11, 436, 434, 1293, 322, 264, 912, 22027, 11, 51212], "temperature": 0.0, "avg_logprob": -0.15342593837428736, "compression_ratio": 1.8252032520325203, "no_speech_prob": 0.01902134343981743}, {"id": 572, "seek": 327432, "start": 3291.28, "end": 3297.36, "text": " like is there a continuum? So right, so yes and no, I do believe they are they are very", "tokens": [51212, 411, 307, 456, 257, 36120, 30, 407, 558, 11, 370, 2086, 293, 572, 11, 286, 360, 1697, 436, 366, 436, 366, 588, 51516], "temperature": 0.0, "avg_logprob": -0.15342593837428736, "compression_ratio": 1.8252032520325203, "no_speech_prob": 0.01902134343981743}, {"id": 573, "seek": 327432, "start": 3297.36, "end": 3302.2400000000002, "text": " qualitatively different. These are the two poles of cognition, but there are there are, you know,", "tokens": [51516, 31312, 356, 819, 13, 1981, 366, 264, 732, 24760, 295, 46905, 11, 457, 456, 366, 456, 366, 11, 291, 458, 11, 51760], "temperature": 0.0, "avg_logprob": -0.15342593837428736, "compression_ratio": 1.8252032520325203, "no_speech_prob": 0.01902134343981743}, {"id": 574, "seek": 330224, "start": 3302.24, "end": 3306.7999999999997, "text": " most most things we do with our mind are a combination of both. That doesn't mean it's", "tokens": [50364, 881, 881, 721, 321, 360, 365, 527, 1575, 366, 257, 6562, 295, 1293, 13, 663, 1177, 380, 914, 309, 311, 50592], "temperature": 0.0, "avg_logprob": -0.18746822758724815, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.0013616681098937988}, {"id": 575, "seek": 330224, "start": 3306.7999999999997, "end": 3310.72, "text": " it lies somewhere in between. It means it's a direct combination of one pole with the other,", "tokens": [50592, 309, 9134, 4079, 294, 1296, 13, 467, 1355, 309, 311, 257, 2047, 6562, 295, 472, 13208, 365, 264, 661, 11, 50788], "temperature": 0.0, "avg_logprob": -0.18746822758724815, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.0013616681098937988}, {"id": 576, "seek": 330224, "start": 3310.72, "end": 3316.16, "text": " kind of like what I described with with the arc solver with three layers, with two layers of", "tokens": [50788, 733, 295, 411, 437, 286, 7619, 365, 365, 264, 10346, 1404, 331, 365, 1045, 7914, 11, 365, 732, 7914, 295, 51060], "temperature": 0.0, "avg_logprob": -0.18746822758724815, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.0013616681098937988}, {"id": 577, "seek": 330224, "start": 3316.16, "end": 3321.9199999999996, "text": " that type one and one layer in the middle of type two. But in very much the same way that you can", "tokens": [51060, 300, 2010, 472, 293, 472, 4583, 294, 264, 2808, 295, 2010, 732, 13, 583, 294, 588, 709, 264, 912, 636, 300, 291, 393, 51348], "temperature": 0.0, "avg_logprob": -0.18746822758724815, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.0013616681098937988}, {"id": 578, "seek": 330224, "start": 3322.56, "end": 3329.3599999999997, "text": " embed discrete programs in a smooth manifold, you can also do the reverse. And when you're", "tokens": [51380, 12240, 27706, 4268, 294, 257, 5508, 47138, 11, 291, 393, 611, 360, 264, 9943, 13, 400, 562, 291, 434, 51720], "temperature": 0.0, "avg_logprob": -0.18746822758724815, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.0013616681098937988}, {"id": 579, "seek": 332936, "start": 3329.36, "end": 3335.36, "text": " meaning you can basically encode an approximation of a geometric space using discrete constructs. In", "tokens": [50364, 3620, 291, 393, 1936, 2058, 1429, 364, 28023, 295, 257, 33246, 1901, 1228, 27706, 7690, 82, 13, 682, 50664], "temperature": 0.0, "avg_logprob": -0.12849089732536903, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0015005904715508223}, {"id": 580, "seek": 332936, "start": 3335.36, "end": 3339.76, "text": " fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing,", "tokens": [50664, 1186, 11, 498, 291, 600, 1096, 604, 1333, 295, 8213, 21989, 322, 257, 3820, 11, 300, 311, 2293, 437, 291, 434, 884, 11, 50884], "temperature": 0.0, "avg_logprob": -0.12849089732536903, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0015005904715508223}, {"id": 581, "seek": 332936, "start": 3339.76, "end": 3344.8, "text": " you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors", "tokens": [50884, 291, 434, 767, 40805, 2306, 293, 35193, 13, 583, 6063, 6063, 291, 434, 1075, 281, 362, 18875, 51136], "temperature": 0.0, "avg_logprob": -0.12849089732536903, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0015005904715508223}, {"id": 582, "seek": 332936, "start": 3344.8, "end": 3349.44, "text": " of seemingly constant new numbers, you can compute a distance between two vectors and so", "tokens": [51136, 295, 18709, 5754, 777, 3547, 11, 291, 393, 14722, 257, 4560, 1296, 732, 18875, 293, 370, 51368], "temperature": 0.0, "avg_logprob": -0.12849089732536903, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0015005904715508223}, {"id": 583, "seek": 332936, "start": 3349.44, "end": 3354.6400000000003, "text": " all of this is an approximation that's actually grounded in discrete programs. So you can you", "tokens": [51368, 439, 295, 341, 307, 364, 28023, 300, 311, 767, 23535, 294, 27706, 4268, 13, 407, 291, 393, 291, 51628], "temperature": 0.0, "avg_logprob": -0.12849089732536903, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0015005904715508223}, {"id": 584, "seek": 335464, "start": 3354.64, "end": 3359.2799999999997, "text": " can actually kind of merge the two together. It's not necessarily always a good idea. In", "tokens": [50364, 393, 767, 733, 295, 22183, 264, 732, 1214, 13, 467, 311, 406, 4725, 1009, 257, 665, 1558, 13, 682, 50596], "temperature": 0.0, "avg_logprob": -0.16777269928543656, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0006066273781470954}, {"id": 585, "seek": 335464, "start": 3359.2799999999997, "end": 3365.12, "text": " particular, I think it's often not a good idea to try to embed an overly complex or overly", "tokens": [50596, 1729, 11, 286, 519, 309, 311, 2049, 406, 257, 665, 1558, 281, 853, 281, 12240, 364, 24324, 3997, 420, 24324, 50888], "temperature": 0.0, "avg_logprob": -0.16777269928543656, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0006066273781470954}, {"id": 586, "seek": 335464, "start": 3365.12, "end": 3372.4, "text": " discreet program in a constant new space. As I was mentioning earlier, the reverse is actually", "tokens": [50888, 2983, 4751, 1461, 294, 257, 5754, 777, 1901, 13, 1018, 286, 390, 18315, 3071, 11, 264, 9943, 307, 767, 51252], "temperature": 0.0, "avg_logprob": -0.16777269928543656, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0006066273781470954}, {"id": 587, "seek": 335464, "start": 3372.4, "end": 3377.92, "text": " usually way more tractable. And by the way, my I think this is something that came up before", "tokens": [51252, 2673, 636, 544, 24207, 712, 13, 400, 538, 264, 636, 11, 452, 286, 519, 341, 307, 746, 300, 1361, 493, 949, 51528], "temperature": 0.0, "avg_logprob": -0.16777269928543656, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0006066273781470954}, {"id": 588, "seek": 335464, "start": 3377.92, "end": 3383.92, "text": " in our conversation, but my kind of subjective totally not backed by any evidence opinion of", "tokens": [51528, 294, 527, 3761, 11, 457, 452, 733, 295, 25972, 3879, 406, 20391, 538, 604, 4467, 4800, 295, 51828], "temperature": 0.0, "avg_logprob": -0.16777269928543656, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.0006066273781470954}, {"id": 589, "seek": 338392, "start": 3384.0, "end": 3390.2400000000002, "text": " how the brain works is that fundamentally it's doing type one on type two, using a discrete", "tokens": [50368, 577, 264, 3567, 1985, 307, 300, 17879, 309, 311, 884, 2010, 472, 322, 2010, 732, 11, 1228, 257, 27706, 50680], "temperature": 0.0, "avg_logprob": -0.16879898678939953, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.0003919926821254194}, {"id": 590, "seek": 338392, "start": 3390.2400000000002, "end": 3395.92, "text": " system, because it's actually much easier to do to type one via an approximation of a geometric", "tokens": [50680, 1185, 11, 570, 309, 311, 767, 709, 3571, 281, 360, 281, 2010, 472, 5766, 364, 28023, 295, 257, 33246, 50964], "temperature": 0.0, "avg_logprob": -0.16879898678939953, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.0003919926821254194}, {"id": 591, "seek": 338392, "start": 3395.92, "end": 3399.84, "text": " space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can,", "tokens": [50964, 1901, 300, 311, 2058, 12340, 294, 257, 6566, 3877, 813, 309, 307, 281, 360, 264, 9943, 13, 865, 11, 293, 498, 286, 393, 11, 51160], "temperature": 0.0, "avg_logprob": -0.16879898678939953, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.0003919926821254194}, {"id": 592, "seek": 338392, "start": 3400.4, "end": 3404.64, "text": " if I just for the benefit of the reader, the listeners, if I can give some other examples,", "tokens": [51188, 498, 286, 445, 337, 264, 5121, 295, 264, 15149, 11, 264, 23274, 11, 498, 286, 393, 976, 512, 661, 5110, 11, 51400], "temperature": 0.0, "avg_logprob": -0.16879898678939953, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.0003919926821254194}, {"id": 593, "seek": 338392, "start": 3404.64, "end": 3409.6, "text": " you know, for example, and mixed integer optimization, it's often the case that you", "tokens": [51400, 291, 458, 11, 337, 1365, 11, 293, 7467, 24922, 19618, 11, 309, 311, 2049, 264, 1389, 300, 291, 51648], "temperature": 0.0, "avg_logprob": -0.16879898678939953, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.0003919926821254194}, {"id": 594, "seek": 340960, "start": 3409.6, "end": 3414.48, "text": " take that problem. And instead of having these discrete values, you project it into a continuous", "tokens": [50364, 747, 300, 1154, 13, 400, 2602, 295, 1419, 613, 27706, 4190, 11, 291, 1716, 309, 666, 257, 10957, 50608], "temperature": 0.0, "avg_logprob": -0.0849505244074641, "compression_ratio": 1.8902439024390243, "no_speech_prob": 0.001098676584661007}, {"id": 595, "seek": 340960, "start": 3414.48, "end": 3420.16, "text": " space, do a continuous optimization. And then as you get sort of close to a good optimization,", "tokens": [50608, 1901, 11, 360, 257, 10957, 19618, 13, 400, 550, 382, 291, 483, 1333, 295, 1998, 281, 257, 665, 19618, 11, 50892], "temperature": 0.0, "avg_logprob": -0.0849505244074641, "compression_ratio": 1.8902439024390243, "no_speech_prob": 0.001098676584661007}, {"id": 596, "seek": 340960, "start": 3420.16, "end": 3425.52, "text": " you discretize it back over into the, the discrete variables, you know, to, to kind of,", "tokens": [50892, 291, 25656, 1125, 309, 646, 670, 666, 264, 11, 264, 27706, 9102, 11, 291, 458, 11, 281, 11, 281, 733, 295, 11, 51160], "temperature": 0.0, "avg_logprob": -0.0849505244074641, "compression_ratio": 1.8902439024390243, "no_speech_prob": 0.001098676584661007}, {"id": 597, "seek": 340960, "start": 3425.52, "end": 3430.96, "text": " you know, flesh out the most optimal path within that discrete space, or an example to is the", "tokens": [51160, 291, 458, 11, 12497, 484, 264, 881, 16252, 3100, 1951, 300, 27706, 1901, 11, 420, 364, 1365, 281, 307, 264, 51432], "temperature": 0.0, "avg_logprob": -0.0849505244074641, "compression_ratio": 1.8902439024390243, "no_speech_prob": 0.001098676584661007}, {"id": 598, "seek": 340960, "start": 3430.96, "end": 3435.92, "text": " gamma function, you know, which is a continuous generalization of the factorial, right? And", "tokens": [51432, 15546, 2445, 11, 291, 458, 11, 597, 307, 257, 10957, 2674, 2144, 295, 264, 36916, 11, 558, 30, 400, 51680], "temperature": 0.0, "avg_logprob": -0.0849505244074641, "compression_ratio": 1.8902439024390243, "no_speech_prob": 0.001098676584661007}, {"id": 599, "seek": 343592, "start": 3435.92, "end": 3441.76, "text": " it kind of provides some cool and interesting behavior in between those, those poles that", "tokens": [50364, 309, 733, 295, 6417, 512, 1627, 293, 1880, 5223, 294, 1296, 729, 11, 729, 24760, 300, 50656], "temperature": 0.0, "avg_logprob": -0.09478749070212106, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.00035113561898469925}, {"id": 600, "seek": 343592, "start": 3441.76, "end": 3446.4, "text": " show up very clearly on the graph as these discrete points. And this is this bizarre", "tokens": [50656, 855, 493, 588, 4448, 322, 264, 4295, 382, 613, 27706, 2793, 13, 400, 341, 307, 341, 18265, 50888], "temperature": 0.0, "avg_logprob": -0.09478749070212106, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.00035113561898469925}, {"id": 601, "seek": 343592, "start": 3447.04, "end": 3451.44, "text": " duality between the continuous and the discrete that we see like throughout the universe. And", "tokens": [50920, 11848, 507, 1296, 264, 10957, 293, 264, 27706, 300, 321, 536, 411, 3710, 264, 6445, 13, 400, 51140], "temperature": 0.0, "avg_logprob": -0.09478749070212106, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.00035113561898469925}, {"id": 602, "seek": 343592, "start": 3451.44, "end": 3456.96, "text": " it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what", "tokens": [51140, 309, 311, 733, 295, 472, 295, 264, 24404, 377, 721, 321, 362, 281, 2028, 365, 13, 865, 11, 2293, 13, 286, 445, 2441, 437, 51416], "temperature": 0.0, "avg_logprob": -0.09478749070212106, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.00035113561898469925}, {"id": 603, "seek": 343592, "start": 3456.96, "end": 3460.88, "text": " some of the transformers folks must be saying now, because Max Welling, we had him on and", "tokens": [51416, 512, 295, 264, 4088, 433, 4024, 1633, 312, 1566, 586, 11, 570, 7402, 1042, 278, 11, 321, 632, 796, 322, 293, 51612], "temperature": 0.0, "avg_logprob": -0.09478749070212106, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.00035113561898469925}, {"id": 604, "seek": 346088, "start": 3461.6, "end": 3466.6400000000003, "text": " folks have done topological applications using transformers or using graph neural", "tokens": [50400, 4024, 362, 1096, 1192, 4383, 5821, 1228, 4088, 433, 420, 1228, 4295, 18161, 50652], "temperature": 0.0, "avg_logprob": -0.14723540924407624, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.005038017872720957}, {"id": 605, "seek": 346088, "start": 3466.6400000000003, "end": 3472.96, "text": " networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,", "tokens": [50652, 9590, 293, 264, 8961, 4860, 11, 264, 551, 490, 14895, 44, 471, 11, 300, 390, 1237, 412, 4295, 307, 32702, 13539, 11, 50968], "temperature": 0.0, "avg_logprob": -0.14723540924407624, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.005038017872720957}, {"id": 606, "seek": 346088, "start": 3472.96, "end": 3478.88, "text": " right? It was looking at different types of equivariance in topological space. Is it a naive", "tokens": [50968, 558, 30, 467, 390, 1237, 412, 819, 3467, 295, 1267, 592, 3504, 719, 294, 1192, 4383, 1901, 13, 1119, 309, 257, 29052, 51264], "temperature": 0.0, "avg_logprob": -0.14723540924407624, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.005038017872720957}, {"id": 607, "seek": 346088, "start": 3478.88, "end": 3485.44, "text": " thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess,", "tokens": [51264, 551, 281, 584, 300, 321, 727, 652, 309, 10957, 420, 366, 321, 322, 257, 10596, 281, 1825, 30, 1779, 13, 407, 286, 2041, 11, 51592], "temperature": 0.0, "avg_logprob": -0.14723540924407624, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.005038017872720957}, {"id": 608, "seek": 348544, "start": 3485.52, "end": 3492.08, "text": " I guess the question is, is there like one approach that's going to end up being universal? And it's,", "tokens": [50368, 286, 2041, 264, 1168, 307, 11, 307, 456, 411, 472, 3109, 300, 311, 516, 281, 917, 493, 885, 11455, 30, 400, 309, 311, 11, 50696], "temperature": 0.0, "avg_logprob": -0.16501182016700205, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0025471211411058903}, {"id": 609, "seek": 348544, "start": 3492.08, "end": 3497.2000000000003, "text": " it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind", "tokens": [50696, 309, 311, 411, 11, 393, 291, 767, 4373, 2452, 2539, 281, 4813, 23211, 6566, 4268, 30, 467, 311, 733, 50952], "temperature": 0.0, "avg_logprob": -0.16501182016700205, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0025471211411058903}, {"id": 610, "seek": 348544, "start": 3497.2000000000003, "end": 3503.36, "text": " of, it's kind of the question. And the answer is no, actually, like, by, by construction, do,", "tokens": [50952, 295, 11, 309, 311, 733, 295, 264, 1168, 13, 400, 264, 1867, 307, 572, 11, 767, 11, 411, 11, 538, 11, 538, 6435, 11, 360, 11, 51260], "temperature": 0.0, "avg_logprob": -0.16501182016700205, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0025471211411058903}, {"id": 611, "seek": 348544, "start": 3503.36, "end": 3509.6, "text": " due to the very nature of what deep learning is, it's like parametric continuous parametric models", "tokens": [51260, 3462, 281, 264, 588, 3687, 295, 437, 2452, 2539, 307, 11, 309, 311, 411, 6220, 17475, 10957, 6220, 17475, 5245, 51572], "temperature": 0.0, "avg_logprob": -0.16501182016700205, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0025471211411058903}, {"id": 612, "seek": 350960, "start": 3509.8399999999997, "end": 3515.2, "text": " in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never", "tokens": [50376, 294, 1186, 11, 5508, 82, 11, 570, 436, 434, 819, 9364, 11, 988, 13, 400, 309, 311, 1596, 674, 3045, 317, 13, 663, 307, 1128, 50644], "temperature": 0.0, "avg_logprob": -0.2289263512477402, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.0029754142742604017}, {"id": 613, "seek": 350960, "start": 3515.2, "end": 3521.7599999999998, "text": " actually going to be a good fit for most discrete programs. So, and, and the reverse is true as", "tokens": [50644, 767, 516, 281, 312, 257, 665, 3318, 337, 881, 27706, 4268, 13, 407, 11, 293, 11, 293, 264, 9943, 307, 2074, 382, 50972], "temperature": 0.0, "avg_logprob": -0.2289263512477402, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.0029754142742604017}, {"id": 614, "seek": 350960, "start": 3521.7599999999998, "end": 3526.3199999999997, "text": " well. I don't think, so you have basically two engines that you can use to learn problems. You", "tokens": [50972, 731, 13, 286, 500, 380, 519, 11, 370, 291, 362, 1936, 732, 12982, 300, 291, 393, 764, 281, 1466, 2740, 13, 509, 51200], "temperature": 0.0, "avg_logprob": -0.2289263512477402, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.0029754142742604017}, {"id": 615, "seek": 350960, "start": 3526.3199999999997, "end": 3531.68, "text": " have quite undecent and you have discrete search. And I think the reverse is also true that discrete", "tokens": [51200, 362, 1596, 674, 3045, 317, 293, 291, 362, 27706, 3164, 13, 400, 286, 519, 264, 9943, 307, 611, 2074, 300, 27706, 51468], "temperature": 0.0, "avg_logprob": -0.2289263512477402, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.0029754142742604017}, {"id": 616, "seek": 350960, "start": 3531.68, "end": 3537.2, "text": " search is not going to be this universal approach that's going to beat everything. I truly believe", "tokens": [51468, 3164, 307, 406, 516, 281, 312, 341, 11455, 3109, 300, 311, 516, 281, 4224, 1203, 13, 286, 4908, 1697, 51744], "temperature": 0.0, "avg_logprob": -0.2289263512477402, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.0029754142742604017}, {"id": 617, "seek": 353720, "start": 3537.2799999999997, "end": 3542.64, "text": " that the AIs of the future will be truly hybrid in the sense that they will have these two engines", "tokens": [50368, 300, 264, 316, 6802, 295, 264, 2027, 486, 312, 4908, 13051, 294, 264, 2020, 300, 436, 486, 362, 613, 732, 12982, 50636], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 618, "seek": 353720, "start": 3542.64, "end": 3546.72, "text": " inside them, they will be able to do this, they will be able to do this quick search.", "tokens": [50636, 1854, 552, 11, 436, 486, 312, 1075, 281, 360, 341, 11, 436, 486, 312, 1075, 281, 360, 341, 1702, 3164, 13, 50840], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 619, "seek": 353720, "start": 3546.72, "end": 3550.56, "text": " Right. And then, and then, and they will set, you, is that appropriate? You said, by the way,", "tokens": [50840, 1779, 13, 400, 550, 11, 293, 550, 11, 293, 436, 486, 992, 11, 291, 11, 307, 300, 6854, 30, 509, 848, 11, 538, 264, 636, 11, 51032], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 620, "seek": 353720, "start": 3550.56, "end": 3554.64, "text": " in your measure of intelligence paper that there are three types of priors, right? Low level,", "tokens": [51032, 294, 428, 3481, 295, 7599, 3035, 300, 456, 366, 1045, 3467, 295, 1790, 830, 11, 558, 30, 17078, 1496, 11, 51236], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 621, "seek": 353720, "start": 3554.64, "end": 3558.96, "text": " sensory motor priors and meta learning priors. That's the interesting one. I think that's got", "tokens": [51236, 27233, 5932, 1790, 830, 293, 19616, 2539, 1790, 830, 13, 663, 311, 264, 1880, 472, 13, 286, 519, 300, 311, 658, 51452], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 622, "seek": 353720, "start": 3558.96, "end": 3563.8399999999997, "text": " intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you", "tokens": [51452, 5613, 2667, 293, 1090, 1496, 3601, 13, 400, 550, 321, 483, 670, 281, 264, 8943, 34, 3430, 293, 11, 293, 382, 291, 51696], "temperature": 0.0, "avg_logprob": -0.25548095703125, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0029685248155146837}, {"id": 623, "seek": 356384, "start": 3563.84, "end": 3568.7200000000003, "text": " said in your presentation last year, the two winning folks on that Kaggle challenge, one was", "tokens": [50364, 848, 294, 428, 5860, 1036, 1064, 11, 264, 732, 8224, 4024, 322, 300, 48751, 22631, 3430, 11, 472, 390, 50608], "temperature": 0.0, "avg_logprob": -0.10045917217548077, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.04430270940065384}, {"id": 624, "seek": 356384, "start": 3568.7200000000003, "end": 3573.2000000000003, "text": " doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program", "tokens": [50608, 884, 257, 12462, 9284, 670, 257, 15816, 43, 13, 407, 884, 437, 291, 434, 1417, 466, 11, 257, 733, 295, 1461, 50832], "temperature": 0.0, "avg_logprob": -0.10045917217548077, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.04430270940065384}, {"id": 625, "seek": 356384, "start": 3573.2000000000003, "end": 3579.1200000000003, "text": " search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that", "tokens": [50832, 3164, 11, 293, 767, 264, 8507, 567, 658, 466, 945, 4, 14170, 13, 400, 300, 390, 11, 300, 390, 445, 11, 1338, 11, 300, 51128], "temperature": 0.0, "avg_logprob": -0.10045917217548077, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.04430270940065384}, {"id": 626, "seek": 356384, "start": 3579.1200000000003, "end": 3585.2000000000003, "text": " was just doing a brute force, you know, selecting combinations of, of operations on this DSL.", "tokens": [51128, 390, 445, 884, 257, 47909, 3464, 11, 291, 458, 11, 18182, 21267, 295, 11, 295, 7705, 322, 341, 15816, 43, 13, 51432], "temperature": 0.0, "avg_logprob": -0.10045917217548077, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.04430270940065384}, {"id": 627, "seek": 356384, "start": 3586.0, "end": 3591.2000000000003, "text": " So this absolutely fascinates me. So at the moment, that seems like a horrific solution,", "tokens": [51472, 407, 341, 3122, 7184, 259, 1024, 385, 13, 407, 412, 264, 1623, 11, 300, 2544, 411, 257, 29248, 3827, 11, 51732], "temperature": 0.0, "avg_logprob": -0.10045917217548077, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.04430270940065384}, {"id": 628, "seek": 359120, "start": 3591.2, "end": 3595.52, "text": " but clearly no one could do it using deep learning. So, but, but this is what you're", "tokens": [50364, 457, 4448, 572, 472, 727, 360, 309, 1228, 2452, 2539, 13, 407, 11, 457, 11, 457, 341, 307, 437, 291, 434, 50580], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 629, "seek": 359120, "start": 3595.52, "end": 3599.8399999999997, "text": " advocating for. So you're saying for these discrete problems, get, get a DSL. Now,", "tokens": [50580, 32050, 337, 13, 407, 291, 434, 1566, 337, 613, 27706, 2740, 11, 483, 11, 483, 257, 15816, 43, 13, 823, 11, 50796], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 630, "seek": 359120, "start": 3599.8399999999997, "end": 3603.2799999999997, "text": " all the stuff you're talking about, presumably they haven't done yet, you're saying, well,", "tokens": [50796, 439, 264, 1507, 291, 434, 1417, 466, 11, 26742, 436, 2378, 380, 1096, 1939, 11, 291, 434, 1566, 11, 731, 11, 50968], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 631, "seek": 359120, "start": 3603.2799999999997, "end": 3607.8399999999997, "text": " software engineering, the beauty of software engineering is being able to modularize things", "tokens": [50968, 4722, 7043, 11, 264, 6643, 295, 4722, 7043, 307, 885, 1075, 281, 31111, 1125, 721, 51196], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 632, "seek": 359120, "start": 3607.8399999999997, "end": 3612.56, "text": " into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But", "tokens": [51196, 666, 2390, 8474, 13, 400, 294, 1186, 11, 286, 959, 48749, 341, 551, 767, 490, 4379, 21299, 3998, 21491, 13, 583, 51432], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 633, "seek": 359120, "start": 3612.56, "end": 3617.2799999999997, "text": " he said, the reason why software engineering is so good is if I ask you, how long will it take", "tokens": [51432, 415, 848, 11, 264, 1778, 983, 4722, 7043, 307, 370, 665, 307, 498, 286, 1029, 291, 11, 577, 938, 486, 309, 747, 51668], "temperature": 0.0, "avg_logprob": -0.1296305055971499, "compression_ratio": 1.7252396166134185, "no_speech_prob": 0.02238522842526436}, {"id": 634, "seek": 361728, "start": 3617.28, "end": 3622.96, "text": " you to build the game of Tetris? You will say not long at all. And if you look at the number of", "tokens": [50364, 291, 281, 1322, 264, 1216, 295, 31580, 5714, 30, 509, 486, 584, 406, 938, 412, 439, 13, 400, 498, 291, 574, 412, 264, 1230, 295, 50648], "temperature": 0.0, "avg_logprob": -0.10327607109433129, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.02956053987145424}, {"id": 635, "seek": 361728, "start": 3622.96, "end": 3628.6400000000003, "text": " state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple", "tokens": [50648, 1785, 7673, 294, 31580, 5714, 11, 309, 311, 11, 309, 311, 2603, 13, 583, 264, 1778, 291, 603, 312, 6679, 281, 1322, 309, 294, 257, 1916, 50932], "temperature": 0.0, "avg_logprob": -0.10327607109433129, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.02956053987145424}, {"id": 636, "seek": 361728, "start": 3628.6400000000003, "end": 3634.0, "text": " of weeks is because you know that you can modularize it into, into blocks, you can't say the same for", "tokens": [50932, 295, 3259, 307, 570, 291, 458, 300, 291, 393, 31111, 1125, 309, 666, 11, 666, 8474, 11, 291, 393, 380, 584, 264, 912, 337, 51200], "temperature": 0.0, "avg_logprob": -0.10327607109433129, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.02956053987145424}, {"id": 637, "seek": 361728, "start": 3634.0, "end": 3638.1600000000003, "text": " deep learning, right? But they don't appear to have done that on the arc challenge yet.", "tokens": [51200, 2452, 2539, 11, 558, 30, 583, 436, 500, 380, 4204, 281, 362, 1096, 300, 322, 264, 10346, 3430, 1939, 13, 51408], "temperature": 0.0, "avg_logprob": -0.10327607109433129, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.02956053987145424}, {"id": 638, "seek": 361728, "start": 3638.88, "end": 3643.44, "text": " Yeah, so the, the solutions we've seen on the accident so far have been incredibly,", "tokens": [51444, 865, 11, 370, 264, 11, 264, 6547, 321, 600, 1612, 322, 264, 6398, 370, 1400, 362, 668, 6252, 11, 51672], "temperature": 0.0, "avg_logprob": -0.10327607109433129, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.02956053987145424}, {"id": 639, "seek": 364344, "start": 3643.44, "end": 3648.32, "text": " incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%.", "tokens": [50364, 6252, 28540, 13, 400, 370, 309, 311, 11, 309, 311, 767, 1596, 1880, 300, 291, 393, 483, 281, 945, 6856, 50608], "temperature": 0.0, "avg_logprob": -0.1670891387122018, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.00637438427656889}, {"id": 640, "seek": 364344, "start": 3649.36, "end": 3654.64, "text": " It's very primitive solutions. I think you can, even with today's technology, you can go much", "tokens": [50660, 467, 311, 588, 28540, 6547, 13, 286, 519, 291, 393, 11, 754, 365, 965, 311, 2899, 11, 291, 393, 352, 709, 50924], "temperature": 0.0, "avg_logprob": -0.1670891387122018, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.00637438427656889}, {"id": 641, "seek": 364344, "start": 3654.64, "end": 3660.56, "text": " further. Like the, what I was describing before about learning a DSL that is perceptive and then", "tokens": [50924, 3052, 13, 1743, 264, 11, 437, 286, 390, 16141, 949, 466, 2539, 257, 15816, 43, 300, 307, 43276, 488, 293, 550, 51220], "temperature": 0.0, "avg_logprob": -0.1670891387122018, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.00637438427656889}, {"id": 642, "seek": 364344, "start": 3660.56, "end": 3665.84, "text": " guiding discrete program search. Yeah, intuition about program space. This is already something", "tokens": [51220, 25061, 27706, 1461, 3164, 13, 865, 11, 24002, 466, 1461, 1901, 13, 639, 307, 1217, 746, 51484], "temperature": 0.0, "avg_logprob": -0.1670891387122018, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.00637438427656889}, {"id": 643, "seek": 364344, "start": 3665.84, "end": 3670.64, "text": " that you can try today. So there's one approach that I was very excited about. And that I thought", "tokens": [51484, 300, 291, 393, 853, 965, 13, 407, 456, 311, 472, 3109, 300, 286, 390, 588, 2919, 466, 13, 400, 300, 286, 1194, 51724], "temperature": 0.0, "avg_logprob": -0.1670891387122018, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.00637438427656889}, {"id": 644, "seek": 367064, "start": 3670.96, "end": 3676.8799999999997, "text": " was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks.", "tokens": [50380, 390, 588, 1627, 13, 400, 286, 534, 411, 309, 311, 11, 309, 311, 1219, 12105, 66, 19866, 538, 2491, 13, 9954, 38171, 293, 11, 293, 4024, 13, 50676], "temperature": 0.0, "avg_logprob": -0.20606005469033883, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.01299112569540739}, {"id": 645, "seek": 367064, "start": 3677.92, "end": 3681.7599999999998, "text": " So check it out if you, if you have incidents, it's very good. I think that they're trying to", "tokens": [50728, 407, 1520, 309, 484, 498, 291, 11, 498, 291, 362, 21139, 11, 309, 311, 588, 665, 13, 286, 519, 300, 436, 434, 1382, 281, 50920], "temperature": 0.0, "avg_logprob": -0.20606005469033883, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.01299112569540739}, {"id": 646, "seek": 367064, "start": 3681.7599999999998, "end": 3687.68, "text": " play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into", "tokens": [50920, 862, 281, 10346, 586, 11, 457, 309, 311, 5101, 411, 11, 307, 341, 733, 295, 411, 13051, 2452, 2539, 4268, 666, 51216], "temperature": 0.0, "avg_logprob": -0.20606005469033883, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.01299112569540739}, {"id": 647, "seek": 367064, "start": 3687.68, "end": 3693.44, "text": " this engine? And I think that's really to me, that that is the sort of direction that is the most", "tokens": [51216, 341, 2848, 30, 400, 286, 519, 300, 311, 534, 281, 385, 11, 300, 300, 307, 264, 1333, 295, 3513, 300, 307, 264, 881, 51504], "temperature": 0.0, "avg_logprob": -0.20606005469033883, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.01299112569540739}, {"id": 648, "seek": 367064, "start": 3693.44, "end": 3699.3599999999997, "text": " promising to that. So you have a paper that's fairly long on, it's called on the measure of", "tokens": [51504, 20257, 281, 300, 13, 407, 291, 362, 257, 3035, 300, 311, 6457, 938, 322, 11, 309, 311, 1219, 322, 264, 3481, 295, 51800], "temperature": 0.0, "avg_logprob": -0.20606005469033883, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.01299112569540739}, {"id": 649, "seek": 369936, "start": 3699.36, "end": 3705.1200000000003, "text": " intelligence. And you make the case that intelligence is something like the efficiency", "tokens": [50364, 7599, 13, 400, 291, 652, 264, 1389, 300, 7599, 307, 746, 411, 264, 10493, 50652], "temperature": 0.0, "avg_logprob": -0.07190667847056448, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0013865305809304118}, {"id": 650, "seek": 369936, "start": 3705.1200000000003, "end": 3711.44, "text": " with which we transform prior information and experience into task solutions, as, as you have", "tokens": [50652, 365, 597, 321, 4088, 4059, 1589, 293, 1752, 666, 5633, 6547, 11, 382, 11, 382, 291, 362, 50968], "temperature": 0.0, "avg_logprob": -0.07190667847056448, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0013865305809304118}, {"id": 651, "seek": 369936, "start": 3711.44, "end": 3718.2400000000002, "text": " said before. And in that same paper, the arc challenge is presented. So, you know, a naive", "tokens": [50968, 848, 949, 13, 400, 294, 300, 912, 3035, 11, 264, 10346, 3430, 307, 8212, 13, 407, 11, 291, 458, 11, 257, 29052, 51308], "temperature": 0.0, "avg_logprob": -0.07190667847056448, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0013865305809304118}, {"id": 652, "seek": 369936, "start": 3718.2400000000002, "end": 3724.1600000000003, "text": " reader like me assumes there is some connection between, you know, what you say about intelligence", "tokens": [51308, 15149, 411, 385, 37808, 456, 307, 512, 4984, 1296, 11, 291, 458, 11, 437, 291, 584, 466, 7599, 51604], "temperature": 0.0, "avg_logprob": -0.07190667847056448, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0013865305809304118}, {"id": 653, "seek": 372416, "start": 3724.24, "end": 3731.2799999999997, "text": " and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and", "tokens": [50368, 293, 12606, 341, 10346, 3430, 13, 407, 452, 1168, 307, 11, 498, 4153, 11, 291, 458, 11, 257, 777, 1469, 1487, 293, 50720], "temperature": 0.0, "avg_logprob": -0.12256181240081787, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02551766112446785}, {"id": 654, "seek": 372416, "start": 3731.2799999999997, "end": 3737.44, "text": " gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge.", "tokens": [50720, 2709, 291, 257, 3827, 11, 291, 13059, 309, 11, 309, 2170, 2035, 13420, 4, 3006, 11, 309, 39890, 264, 10346, 3430, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12256181240081787, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02551766112446785}, {"id": 655, "seek": 372416, "start": 3738.16, "end": 3745.8399999999997, "text": " Is it immediately intelligent? Or what would you ask of that system for, for you to say,", "tokens": [51064, 1119, 309, 4258, 13232, 30, 1610, 437, 576, 291, 1029, 295, 300, 1185, 337, 11, 337, 291, 281, 584, 11, 51448], "temperature": 0.0, "avg_logprob": -0.12256181240081787, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02551766112446785}, {"id": 656, "seek": 372416, "start": 3745.8399999999997, "end": 3750.7999999999997, "text": " yes, that's intelligent, or it's, it's intelligent is, is high or something like this.", "tokens": [51448, 2086, 11, 300, 311, 13232, 11, 420, 309, 311, 11, 309, 311, 13232, 307, 11, 307, 1090, 420, 746, 411, 341, 13, 51696], "temperature": 0.0, "avg_logprob": -0.12256181240081787, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.02551766112446785}, {"id": 657, "seek": 375080, "start": 3750.88, "end": 3758.0, "text": " So you, you, you would be able to make that, that conclusion, if and only if arc was a,", "tokens": [50368, 407, 291, 11, 291, 11, 291, 576, 312, 1075, 281, 652, 300, 11, 300, 10063, 11, 498, 293, 787, 498, 10346, 390, 257, 11, 50724], "temperature": 0.0, "avg_logprob": -0.11842904371373794, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0007899809279479086}, {"id": 658, "seek": 375080, "start": 3758.0, "end": 3763.28, "text": " was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc,", "tokens": [50724, 390, 257, 2176, 18927, 11, 457, 309, 311, 406, 11, 309, 311, 767, 588, 709, 38823, 13, 407, 498, 291, 5039, 10346, 11, 50988], "temperature": 0.0, "avg_logprob": -0.11842904371373794, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0007899809279479086}, {"id": 659, "seek": 375080, "start": 3763.28, "end": 3770.0800000000004, "text": " are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the", "tokens": [50988, 366, 291, 11, 366, 291, 13232, 30, 1042, 11, 572, 11, 570, 10346, 307, 7263, 38823, 13, 663, 311, 11, 300, 311, 264, 51328], "temperature": 0.0, "avg_logprob": -0.11842904371373794, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0007899809279479086}, {"id": 660, "seek": 375080, "start": 3770.0800000000004, "end": 3776.5600000000004, "text": " thing. So the thing you need to really understand about arc is that it's not kind of the end state", "tokens": [51328, 551, 13, 407, 264, 551, 291, 643, 281, 534, 1223, 466, 10346, 307, 300, 309, 311, 406, 733, 295, 264, 917, 1785, 51652], "temperature": 0.0, "avg_logprob": -0.11842904371373794, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0007899809279479086}, {"id": 661, "seek": 377656, "start": 3776.56, "end": 3782.24, "text": " of the intelligence benchmark. It is very much a work in progress. And there will be new iterations,", "tokens": [50364, 295, 264, 7599, 18927, 13, 467, 307, 588, 709, 257, 589, 294, 4205, 13, 400, 456, 486, 312, 777, 36540, 11, 50648], "temperature": 0.0, "avg_logprob": -0.16548126884128736, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002278411528095603}, {"id": 662, "seek": 377656, "start": 3782.24, "end": 3787.68, "text": " especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle", "tokens": [50648, 2318, 382, 321, 1466, 544, 466, 264, 12867, 13, 400, 538, 264, 636, 11, 370, 1036, 1064, 11, 321, 5872, 257, 48751, 22631, 50920], "temperature": 0.0, "avg_logprob": -0.16548126884128736, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002278411528095603}, {"id": 663, "seek": 377656, "start": 3787.68, "end": 3793.2, "text": " challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches", "tokens": [50920, 3430, 322, 10346, 11, 293, 321, 3264, 257, 2952, 11, 406, 4725, 257, 2952, 466, 1461, 30252, 11587, 51196], "temperature": 0.0, "avg_logprob": -0.16548126884128736, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002278411528095603}, {"id": 664, "seek": 377656, "start": 3793.2, "end": 3798.16, "text": " although there were some cool stuff we still learned about and so on. But mostly we learned about", "tokens": [51196, 4878, 456, 645, 512, 1627, 1507, 321, 920, 3264, 466, 293, 370, 322, 13, 583, 5240, 321, 3264, 466, 51444], "temperature": 0.0, "avg_logprob": -0.16548126884128736, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002278411528095603}, {"id": 665, "seek": 377656, "start": 3798.16, "end": 3803.2, "text": " the flows of arc. So there will be future additions and so on. So I will tell you this,", "tokens": [51444, 264, 12867, 295, 10346, 13, 407, 456, 486, 312, 2027, 35113, 293, 370, 322, 13, 407, 286, 486, 980, 291, 341, 11, 51696], "temperature": 0.0, "avg_logprob": -0.16548126884128736, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002278411528095603}, {"id": 666, "seek": 380320, "start": 3804.08, "end": 3809.6, "text": " if you solve the specific test set of arc as it exists today, you're not necessarily intelligent", "tokens": [50408, 498, 291, 5039, 264, 2685, 1500, 992, 295, 10346, 382, 309, 8198, 965, 11, 291, 434, 406, 4725, 13232, 50684], "temperature": 0.0, "avg_logprob": -0.11661761369162459, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.006181549280881882}, {"id": 667, "seek": 380320, "start": 3809.6, "end": 3816.0, "text": " because it is not perfect because it has its laws. But if more generally speaking, you give me a system", "tokens": [50684, 570, 309, 307, 406, 2176, 570, 309, 575, 1080, 6064, 13, 583, 498, 544, 5101, 4124, 11, 291, 976, 385, 257, 1185, 51004], "temperature": 0.0, "avg_logprob": -0.11661761369162459, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.006181549280881882}, {"id": 668, "seek": 380320, "start": 3816.0, "end": 3823.2799999999997, "text": " that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow,", "tokens": [51004, 300, 307, 1270, 300, 604, 777, 10346, 5633, 286, 3507, 412, 309, 11, 411, 286, 393, 11, 286, 393, 652, 512, 777, 2306, 4153, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11661761369162459, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.006181549280881882}, {"id": 669, "seek": 380320, "start": 3823.2799999999997, "end": 3827.52, "text": " for instance, I give them to your system. If it's always solving them, I will say,", "tokens": [51368, 337, 5197, 11, 286, 976, 552, 281, 428, 1185, 13, 759, 309, 311, 1009, 12606, 552, 11, 286, 486, 584, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11661761369162459, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.006181549280881882}, {"id": 670, "seek": 380320, "start": 3827.52, "end": 3831.2799999999997, "text": " yeah, it's looking like you've got a system that's, that's got, you know, pretty close to", "tokens": [51580, 1338, 11, 309, 311, 1237, 411, 291, 600, 658, 257, 1185, 300, 311, 11, 300, 311, 658, 11, 291, 458, 11, 1238, 1998, 281, 51768], "temperature": 0.0, "avg_logprob": -0.11661761369162459, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.006181549280881882}, {"id": 671, "seek": 383128, "start": 3831.28, "end": 3837.76, "text": " human level fluid intelligence. This is one of the things that, look, and I like the paper a lot,", "tokens": [50364, 1952, 1496, 9113, 7599, 13, 639, 307, 472, 295, 264, 721, 300, 11, 574, 11, 293, 286, 411, 264, 3035, 257, 688, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09443477104450095, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0017224439652636647}, {"id": 672, "seek": 383128, "start": 3837.76, "end": 3843.52, "text": " I think, I think it serves as a really good, you know, foundation for us to think differently", "tokens": [50688, 286, 519, 11, 286, 519, 309, 13451, 382, 257, 534, 665, 11, 291, 458, 11, 7030, 337, 505, 281, 519, 7614, 50976], "temperature": 0.0, "avg_logprob": -0.09443477104450095, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0017224439652636647}, {"id": 673, "seek": 383128, "start": 3843.52, "end": 3848.0, "text": " about how to build intelligence. But, but I have some, some issues with it too as well. And one", "tokens": [50976, 466, 577, 281, 1322, 7599, 13, 583, 11, 457, 286, 362, 512, 11, 512, 2663, 365, 309, 886, 382, 731, 13, 400, 472, 51200], "temperature": 0.0, "avg_logprob": -0.09443477104450095, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0017224439652636647}, {"id": 674, "seek": 383128, "start": 3848.0, "end": 3854.0800000000004, "text": " of them is this sort of necessity that it requires kind of white box analysis of things in order to", "tokens": [51200, 295, 552, 307, 341, 1333, 295, 24217, 300, 309, 7029, 733, 295, 2418, 2424, 5215, 295, 721, 294, 1668, 281, 51504], "temperature": 0.0, "avg_logprob": -0.09443477104450095, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0017224439652636647}, {"id": 675, "seek": 383128, "start": 3854.0800000000004, "end": 3857.92, "text": " figure out whether or not they're intelligent. Because for example, suppose time travel is", "tokens": [51504, 2573, 484, 1968, 420, 406, 436, 434, 13232, 13, 1436, 337, 1365, 11, 7297, 565, 3147, 307, 51696], "temperature": 0.0, "avg_logprob": -0.09443477104450095, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0017224439652636647}, {"id": 676, "seek": 385792, "start": 3858.0, "end": 3863.92, "text": " actually possible. And you know, somebody like 100 years from now looks back on your arc thing and", "tokens": [50368, 767, 1944, 13, 400, 291, 458, 11, 2618, 411, 2319, 924, 490, 586, 1542, 646, 322, 428, 10346, 551, 293, 50664], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 677, "seek": 385792, "start": 3863.92, "end": 3868.48, "text": " writes an algorithm that, that solves all, all them in there because it actually knows about them", "tokens": [50664, 13657, 364, 9284, 300, 11, 300, 39890, 439, 11, 439, 552, 294, 456, 570, 309, 767, 3255, 466, 552, 50892], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 678, "seek": 385792, "start": 3868.48, "end": 3873.04, "text": " already and then ships it back into the past and we enter it into the competition. And no matter", "tokens": [50892, 1217, 293, 550, 11434, 309, 646, 666, 264, 1791, 293, 321, 3242, 309, 666, 264, 6211, 13, 400, 572, 1871, 51120], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 679, "seek": 385792, "start": 3873.04, "end": 3877.2000000000003, "text": " what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know,", "tokens": [51120, 437, 777, 10346, 551, 291, 3507, 412, 309, 11, 309, 1333, 295, 775, 731, 13, 400, 291, 584, 11, 731, 11, 1338, 11, 291, 458, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 680, "seek": 385792, "start": 3877.2000000000003, "end": 3882.16, "text": " this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper,", "tokens": [51328, 341, 551, 311, 411, 733, 295, 13232, 11, 457, 11, 457, 321, 1116, 312, 2085, 570, 294, 264, 2020, 294, 264, 3035, 11, 51576], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 681, "seek": 385792, "start": 3882.16, "end": 3886.2400000000002, "text": " it's actually just encoded, you know, prior knowledge from the future. So we have to,", "tokens": [51576, 309, 311, 767, 445, 2058, 12340, 11, 291, 458, 11, 4059, 3601, 490, 264, 2027, 13, 407, 321, 362, 281, 11, 51780], "temperature": 0.0, "avg_logprob": -0.08230180673665934, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.004609114024788141}, {"id": 682, "seek": 388624, "start": 3886.24, "end": 3890.16, "text": " we always have to kind of be able to look into the box, right, in order to evaluate", "tokens": [50364, 321, 1009, 362, 281, 733, 295, 312, 1075, 281, 574, 666, 264, 2424, 11, 558, 11, 294, 1668, 281, 13059, 50560], "temperature": 0.0, "avg_logprob": -0.09273280547215389, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.00043049626401625574}, {"id": 683, "seek": 388624, "start": 3890.72, "end": 3894.3999999999996, "text": " intelligence in the way that you define in the paper. And so my question is one,", "tokens": [50588, 7599, 294, 264, 636, 300, 291, 6964, 294, 264, 3035, 13, 400, 370, 452, 1168, 307, 472, 11, 50772], "temperature": 0.0, "avg_logprob": -0.09273280547215389, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.00043049626401625574}, {"id": 684, "seek": 388624, "start": 3894.9599999999996, "end": 3901.2, "text": " isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box", "tokens": [50800, 1943, 380, 300, 257, 857, 295, 257, 45667, 21493, 4111, 30, 400, 732, 11, 360, 291, 362, 604, 13681, 337, 257, 544, 2211, 2424, 51112], "temperature": 0.0, "avg_logprob": -0.09273280547215389, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.00043049626401625574}, {"id": 685, "seek": 388624, "start": 3901.2, "end": 3906.16, "text": " measure of intelligence? So basically, the fundamental issue is that if intelligence", "tokens": [51112, 3481, 295, 7599, 30, 407, 1936, 11, 264, 8088, 2734, 307, 300, 498, 7599, 51360], "temperature": 0.0, "avg_logprob": -0.09273280547215389, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.00043049626401625574}, {"id": 686, "seek": 388624, "start": 3906.16, "end": 3913.04, "text": " is this conversion ratio, then computing it requires knowing where you start from. And", "tokens": [51360, 307, 341, 14298, 8509, 11, 550, 15866, 309, 7029, 5276, 689, 291, 722, 490, 13, 400, 51704], "temperature": 0.0, "avg_logprob": -0.09273280547215389, "compression_ratio": 1.6705426356589148, "no_speech_prob": 0.00043049626401625574}, {"id": 687, "seek": 391304, "start": 3913.04, "end": 3917.6, "text": " you don't really have a way around it. So the thing to keep in mind is that the", "tokens": [50364, 291, 500, 380, 534, 362, 257, 636, 926, 309, 13, 407, 264, 551, 281, 1066, 294, 1575, 307, 300, 264, 50592], "temperature": 0.0, "avg_logprob": -0.14640211591533586, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0006161134806461632}, {"id": 688, "seek": 391304, "start": 3917.6, "end": 3923.2799999999997, "text": " under measure of intelligence stuff is not so much meant to provide like a sort of like", "tokens": [50592, 833, 3481, 295, 7599, 1507, 307, 406, 370, 709, 4140, 281, 2893, 411, 257, 1333, 295, 411, 50876], "temperature": 0.0, "avg_logprob": -0.14640211591533586, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0006161134806461632}, {"id": 689, "seek": 391304, "start": 3923.84, "end": 3928.48, "text": " golden measure of tape to measure anyone's intelligence or anything's intelligence.", "tokens": [50904, 9729, 3481, 295, 7314, 281, 3481, 2878, 311, 7599, 420, 1340, 311, 7599, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14640211591533586, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0006161134806461632}, {"id": 690, "seek": 391304, "start": 3928.48, "end": 3936.08, "text": " It is more meant as a sort of cognitive device to help you think about what the actual challenges are", "tokens": [51136, 467, 307, 544, 4140, 382, 257, 1333, 295, 15605, 4302, 281, 854, 291, 519, 466, 437, 264, 3539, 4759, 366, 51516], "temperature": 0.0, "avg_logprob": -0.14640211591533586, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0006161134806461632}, {"id": 691, "seek": 391304, "start": 3936.8, "end": 3941.68, "text": " to help you kind of kind of reframe AI because they think they have been pretty deep and", "tokens": [51552, 281, 854, 291, 733, 295, 733, 295, 13334, 529, 7318, 570, 436, 519, 436, 362, 668, 1238, 2452, 293, 51796], "temperature": 0.0, "avg_logprob": -0.14640211591533586, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0006161134806461632}, {"id": 692, "seek": 394168, "start": 3941.68, "end": 3946.3199999999997, "text": " longstanding conceptual misunderstandings. So that is really being, that's being holding the", "tokens": [50364, 938, 8618, 24106, 35736, 1109, 13, 407, 300, 307, 534, 885, 11, 300, 311, 885, 5061, 264, 50596], "temperature": 0.0, "avg_logprob": -0.15211777375123212, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.00040421707672066987}, {"id": 693, "seek": 394168, "start": 3946.3199999999997, "end": 3954.3999999999996, "text": " feedback. So it's very much meant as a cognitive device. If you take a step back and you ask,", "tokens": [50596, 5824, 13, 407, 309, 311, 588, 709, 4140, 382, 257, 15605, 4302, 13, 759, 291, 747, 257, 1823, 646, 293, 291, 1029, 11, 51000], "temperature": 0.0, "avg_logprob": -0.15211777375123212, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.00040421707672066987}, {"id": 694, "seek": 394168, "start": 3954.3999999999996, "end": 3959.2, "text": " why are we even trying to define intelligence and measure intelligence in the first place,", "tokens": [51000, 983, 366, 321, 754, 1382, 281, 6964, 7599, 293, 3481, 7599, 294, 264, 700, 1081, 11, 51240], "temperature": 0.0, "avg_logprob": -0.15211777375123212, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.00040421707672066987}, {"id": 695, "seek": 394168, "start": 3959.2, "end": 3965.2799999999997, "text": " why is it useful at all? I think it's useful to the extent that it is actionable, right,", "tokens": [51240, 983, 307, 309, 4420, 412, 439, 30, 286, 519, 309, 311, 4420, 281, 264, 8396, 300, 309, 307, 45098, 11, 558, 11, 51544], "temperature": 0.0, "avg_logprob": -0.15211777375123212, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.00040421707672066987}, {"id": 696, "seek": 394168, "start": 3965.2799999999997, "end": 3970.7999999999997, "text": " a good definition and a good measure should be actionable. So meaning it should help you", "tokens": [51544, 257, 665, 7123, 293, 257, 665, 3481, 820, 312, 45098, 13, 407, 3620, 309, 820, 854, 291, 51820], "temperature": 0.0, "avg_logprob": -0.15211777375123212, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.00040421707672066987}, {"id": 697, "seek": 397080, "start": 3971.6000000000004, "end": 3977.28, "text": " think, it should help you find solutions and it should help you make progress. In particular,", "tokens": [50404, 519, 11, 309, 820, 854, 291, 915, 6547, 293, 309, 820, 854, 291, 652, 4205, 13, 682, 1729, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12078097083351828, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002004842273890972}, {"id": 698, "seek": 397080, "start": 3977.28, "end": 3982.6400000000003, "text": " a good definition is a definition that will highlight the key challenges and help you think", "tokens": [50688, 257, 665, 7123, 307, 257, 7123, 300, 486, 5078, 264, 2141, 4759, 293, 854, 291, 519, 50956], "temperature": 0.0, "avg_logprob": -0.12078097083351828, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002004842273890972}, {"id": 699, "seek": 397080, "start": 3982.6400000000003, "end": 3987.2000000000003, "text": " about it. And I think that's what the paper does. And a good measure is a measure that gives you an", "tokens": [50956, 466, 309, 13, 400, 286, 519, 300, 311, 437, 264, 3035, 775, 13, 400, 257, 665, 3481, 307, 257, 3481, 300, 2709, 291, 364, 51184], "temperature": 0.0, "avg_logprob": -0.12078097083351828, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002004842273890972}, {"id": 700, "seek": 397080, "start": 3987.2000000000003, "end": 3993.76, "text": " actionable feedback signal towards building the right kind of system, right in the sense that", "tokens": [51184, 45098, 5824, 6358, 3030, 2390, 264, 558, 733, 295, 1185, 11, 558, 294, 264, 2020, 300, 51512], "temperature": 0.0, "avg_logprob": -0.12078097083351828, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002004842273890972}, {"id": 701, "seek": 397080, "start": 3993.76, "end": 3999.84, "text": " it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying", "tokens": [51512, 309, 486, 312, 8189, 295, 884, 544, 13, 400, 370, 300, 311, 644, 295, 264, 5824, 6358, 307, 437, 8943, 34, 307, 1382, 51816], "temperature": 0.0, "avg_logprob": -0.12078097083351828, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002004842273890972}, {"id": 702, "seek": 399984, "start": 3999.84, "end": 4008.7200000000003, "text": " to achieve. And the way it's trying to control for priors and experience is by assuming a fixed", "tokens": [50364, 281, 4584, 13, 400, 264, 636, 309, 311, 1382, 281, 1969, 337, 1790, 830, 293, 1752, 307, 538, 11926, 257, 6806, 50808], "temperature": 0.0, "avg_logprob": -0.16662491008799563, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0026180774439126253}, {"id": 703, "seek": 399984, "start": 4008.7200000000003, "end": 4013.6800000000003, "text": " set of priors. And you're going to see, you know, every test taker can have such priors.", "tokens": [50808, 992, 295, 1790, 830, 13, 400, 291, 434, 516, 281, 536, 11, 291, 458, 11, 633, 1500, 991, 260, 393, 362, 1270, 1790, 830, 13, 51056], "temperature": 0.0, "avg_logprob": -0.16662491008799563, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0026180774439126253}, {"id": 704, "seek": 399984, "start": 4013.6800000000003, "end": 4018.8, "text": " This is the core knowledge priors. And then it controls for experience by only giving you a very", "tokens": [51056, 639, 307, 264, 4965, 3601, 1790, 830, 13, 400, 550, 309, 9003, 337, 1752, 538, 787, 2902, 291, 257, 588, 51312], "temperature": 0.0, "avg_logprob": -0.16662491008799563, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0026180774439126253}, {"id": 705, "seek": 399984, "start": 4018.8, "end": 4025.36, "text": " small number of input examples. And also by making sure the tasks are sufficiently novel and", "tokens": [51312, 1359, 1230, 295, 4846, 5110, 13, 400, 611, 538, 1455, 988, 264, 9608, 366, 31868, 7613, 293, 51640], "temperature": 0.0, "avg_logprob": -0.16662491008799563, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0026180774439126253}, {"id": 706, "seek": 402536, "start": 4025.36, "end": 4031.44, "text": " surprising that you're unlikely to have seen a very similar instance before. So now, of course,", "tokens": [50364, 8830, 300, 291, 434, 17518, 281, 362, 1612, 257, 588, 2531, 5197, 949, 13, 407, 586, 11, 295, 1164, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1255377841596844, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0049752467311918736}, {"id": 707, "seek": 402536, "start": 4031.44, "end": 4036.2400000000002, "text": " it's super flawed. So this is not 100% true, of course, but this is kind of like the the", "tokens": [50668, 309, 311, 1687, 38823, 13, 407, 341, 307, 406, 2319, 4, 2074, 11, 295, 1164, 11, 457, 341, 307, 733, 295, 411, 264, 264, 50908], "temperature": 0.0, "avg_logprob": -0.1255377841596844, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0049752467311918736}, {"id": 708, "seek": 402536, "start": 4036.2400000000002, "end": 4040.88, "text": " planning ideal that we're trying to get to. So that for the record, that's a fascinating point to", "tokens": [50908, 5038, 7157, 300, 321, 434, 1382, 281, 483, 281, 13, 407, 300, 337, 264, 2136, 11, 300, 311, 257, 10343, 935, 281, 51140], "temperature": 0.0, "avg_logprob": -0.1255377841596844, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0049752467311918736}, {"id": 709, "seek": 402536, "start": 4040.88, "end": 4045.2000000000003, "text": " me is that you view this more as a cognitive device to help guide us to produce better,", "tokens": [51140, 385, 307, 300, 291, 1910, 341, 544, 382, 257, 15605, 4302, 281, 854, 5934, 505, 281, 5258, 1101, 11, 51356], "temperature": 0.0, "avg_logprob": -0.1255377841596844, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0049752467311918736}, {"id": 710, "seek": 402536, "start": 4046.0, "end": 4052.88, "text": " better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence", "tokens": [51396, 1101, 13232, 12554, 13, 467, 307, 406, 364, 4846, 13, 467, 311, 406, 411, 8943, 34, 307, 411, 264, 3481, 295, 7599, 51740], "temperature": 0.0, "avg_logprob": -0.1255377841596844, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0049752467311918736}, {"id": 711, "seek": 405288, "start": 4052.88, "end": 4059.2000000000003, "text": " and all we need to do is solve ARC. This is not at all the point. It's like it's one.", "tokens": [50364, 293, 439, 321, 643, 281, 360, 307, 5039, 8943, 34, 13, 639, 307, 406, 412, 439, 264, 935, 13, 467, 311, 411, 309, 311, 472, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 712, "seek": 405288, "start": 4059.2000000000003, "end": 4063.28, "text": " Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would", "tokens": [50680, 876, 11, 29063, 11, 570, 286, 390, 884, 1238, 731, 322, 512, 295, 264, 5110, 13, 286, 390, 7159, 300, 576, 50884], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 713, "seek": 405288, "start": 4063.28, "end": 4067.28, "text": " mean I was intelligent. But another interesting point, because Keith and I were looking at the", "tokens": [50884, 914, 286, 390, 13232, 13, 583, 1071, 1880, 935, 11, 570, 20613, 293, 286, 645, 1237, 412, 264, 51084], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 714, "seek": 405288, "start": 4067.28, "end": 4071.84, "text": " paper again yesterday, because it's been, I haven't properly studied it since last year. But", "tokens": [51084, 3035, 797, 5186, 11, 570, 309, 311, 668, 11, 286, 2378, 380, 6108, 9454, 309, 1670, 1036, 1064, 13, 583, 51312], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 715, "seek": 405288, "start": 4072.88, "end": 4076.32, "text": " we were starting to talk about an alien that comes in from outer space. And, you know,", "tokens": [51364, 321, 645, 2891, 281, 751, 466, 364, 12319, 300, 1487, 294, 490, 10847, 1901, 13, 400, 11, 291, 458, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 716, "seek": 405288, "start": 4076.96, "end": 4082.6400000000003, "text": " we don't know the priors and the experience. And then I was thinking in a way, it might be a", "tokens": [51568, 321, 500, 380, 458, 264, 1790, 830, 293, 264, 1752, 13, 400, 550, 286, 390, 1953, 294, 257, 636, 11, 309, 1062, 312, 257, 51852], "temperature": 0.0, "avg_logprob": -0.1479751480209244, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.0070422859862446785}, {"id": 717, "seek": 408264, "start": 4082.64, "end": 4088.24, "text": " kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat", "tokens": [50364, 733, 295, 3126, 5472, 322, 7599, 11, 558, 30, 1436, 11, 291, 458, 11, 498, 286, 862, 24122, 11, 293, 498, 286, 4224, 50644], "temperature": 0.0, "avg_logprob": -0.07018219491709833, "compression_ratio": 1.7323420074349443, "no_speech_prob": 0.0028604534454643726}, {"id": 718, "seek": 408264, "start": 4088.24, "end": 4092.72, "text": " someone with a higher elo than me, then only really tells me that I'm better, you know, as", "tokens": [50644, 1580, 365, 257, 2946, 38682, 813, 385, 11, 550, 787, 534, 5112, 385, 300, 286, 478, 1101, 11, 291, 458, 11, 382, 50868], "temperature": 0.0, "avg_logprob": -0.07018219491709833, "compression_ratio": 1.7323420074349443, "no_speech_prob": 0.0028604534454643726}, {"id": 719, "seek": 408264, "start": 4092.72, "end": 4098.32, "text": " good as that person that I just beat. And similarly, this measure of intelligence, it only gives you", "tokens": [50868, 665, 382, 300, 954, 300, 286, 445, 4224, 13, 400, 14138, 11, 341, 3481, 295, 7599, 11, 309, 787, 2709, 291, 51148], "temperature": 0.0, "avg_logprob": -0.07018219491709833, "compression_ratio": 1.7323420074349443, "no_speech_prob": 0.0028604534454643726}, {"id": 720, "seek": 408264, "start": 4098.32, "end": 4103.599999999999, "text": " a reading in the situation when you know what the conversion was. So if they are not converting", "tokens": [51148, 257, 3760, 294, 264, 2590, 562, 291, 458, 437, 264, 14298, 390, 13, 407, 498, 436, 366, 406, 29942, 51412], "temperature": 0.0, "avg_logprob": -0.07018219491709833, "compression_ratio": 1.7323420074349443, "no_speech_prob": 0.0028604534454643726}, {"id": 721, "seek": 408264, "start": 4103.599999999999, "end": 4109.12, "text": " anything, then you don't know. And another interesting byproduct of this is the more", "tokens": [51412, 1340, 11, 550, 291, 500, 380, 458, 13, 400, 1071, 1880, 538, 33244, 295, 341, 307, 264, 544, 51688], "temperature": 0.0, "avg_logprob": -0.07018219491709833, "compression_ratio": 1.7323420074349443, "no_speech_prob": 0.0028604534454643726}, {"id": 722, "seek": 410912, "start": 4109.12, "end": 4116.48, "text": " experienced you get, the less intelligent you get. So I would push back against that last claim", "tokens": [50364, 6751, 291, 483, 11, 264, 1570, 13232, 291, 483, 13, 407, 286, 576, 2944, 646, 1970, 300, 1036, 3932, 50732], "temperature": 0.0, "avg_logprob": -0.12728464192357555, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.012019988149404526}, {"id": 723, "seek": 410912, "start": 4116.48, "end": 4121.5199999999995, "text": " that the measure of intelligence as I define it is dependent on how much experience you have.", "tokens": [50732, 300, 264, 3481, 295, 7599, 382, 286, 6964, 309, 307, 12334, 322, 577, 709, 1752, 291, 362, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12728464192357555, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.012019988149404526}, {"id": 724, "seek": 410912, "start": 4123.36, "end": 4127.12, "text": " Because the amount of initial experience you have does not actually change at the conversion", "tokens": [51076, 1436, 264, 2372, 295, 5883, 1752, 291, 362, 775, 406, 767, 1319, 412, 264, 14298, 51264], "temperature": 0.0, "avg_logprob": -0.12728464192357555, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.012019988149404526}, {"id": 725, "seek": 410912, "start": 4127.12, "end": 4134.24, "text": " ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks,", "tokens": [51264, 8509, 498, 291, 3481, 309, 5766, 264, 558, 5633, 13, 407, 291, 1062, 643, 11, 370, 498, 291, 362, 257, 6806, 992, 295, 9608, 11, 51620], "temperature": 0.0, "avg_logprob": -0.12728464192357555, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.012019988149404526}, {"id": 726, "seek": 413424, "start": 4134.24, "end": 4139.5199999999995, "text": " then yes, it does affect it. But if you're able to renew your set of tasks and come up with", "tokens": [50364, 550, 2086, 11, 309, 775, 3345, 309, 13, 583, 498, 291, 434, 1075, 281, 10162, 428, 992, 295, 9608, 293, 808, 493, 365, 50628], "temperature": 0.0, "avg_logprob": -0.10596242415166535, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0021466827020049095}, {"id": 727, "seek": 413424, "start": 4139.5199999999995, "end": 4144.0, "text": " styles that are orthogonal to the experience that you have, then it's not the actual effect,", "tokens": [50628, 13273, 300, 366, 41488, 281, 264, 1752, 300, 291, 362, 11, 550, 309, 311, 406, 264, 3539, 1802, 11, 50852], "temperature": 0.0, "avg_logprob": -0.10596242415166535, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0021466827020049095}, {"id": 728, "seek": 413424, "start": 4144.0, "end": 4151.28, "text": " the definition. So, but yeah, you're definitely right that if you take a pure black box approach,", "tokens": [50852, 264, 7123, 13, 407, 11, 457, 1338, 11, 291, 434, 2138, 558, 300, 498, 291, 747, 257, 6075, 2211, 2424, 3109, 11, 51216], "temperature": 0.0, "avg_logprob": -0.10596242415166535, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0021466827020049095}, {"id": 729, "seek": 413424, "start": 4151.28, "end": 4157.599999999999, "text": " and all you're looking at, the only thing you can really measure is the behavior of a system.", "tokens": [51216, 293, 439, 291, 434, 1237, 412, 11, 264, 787, 551, 291, 393, 534, 3481, 307, 264, 5223, 295, 257, 1185, 13, 51532], "temperature": 0.0, "avg_logprob": -0.10596242415166535, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0021466827020049095}, {"id": 730, "seek": 413424, "start": 4157.599999999999, "end": 4163.5199999999995, "text": " And unless you know how that behavior is achieved, you can't really tell immediately", "tokens": [51532, 400, 5969, 291, 458, 577, 300, 5223, 307, 11042, 11, 291, 393, 380, 534, 980, 4258, 51828], "temperature": 0.0, "avg_logprob": -0.10596242415166535, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0021466827020049095}, {"id": 731, "seek": 416352, "start": 4163.52, "end": 4169.040000000001, "text": " how much intelligence was involved in producing this behavior. If you look at an insect,", "tokens": [50364, 577, 709, 7599, 390, 3288, 294, 10501, 341, 5223, 13, 759, 291, 574, 412, 364, 13261, 11, 50640], "temperature": 0.0, "avg_logprob": -0.0936571510092726, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0031581339426338673}, {"id": 732, "seek": 416352, "start": 4169.040000000001, "end": 4173.92, "text": " they're capable of super complex behavior. Are they crazy intelligent? Well, actually,", "tokens": [50640, 436, 434, 8189, 295, 1687, 3997, 5223, 13, 2014, 436, 3219, 13232, 30, 1042, 11, 767, 11, 50884], "temperature": 0.0, "avg_logprob": -0.0936571510092726, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0031581339426338673}, {"id": 733, "seek": 416352, "start": 4173.92, "end": 4179.360000000001, "text": " you know, probably not. And the way you can really tell is by putting these systems out of", "tokens": [50884, 291, 458, 11, 1391, 406, 13, 400, 264, 636, 291, 393, 534, 980, 307, 538, 3372, 613, 3652, 484, 295, 51156], "temperature": 0.0, "avg_logprob": -0.0936571510092726, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0031581339426338673}, {"id": 734, "seek": 416352, "start": 4179.360000000001, "end": 4184.8, "text": " their comfort zone, getting them to face novel situations and see how they adapt. And that's", "tokens": [51156, 641, 3400, 6668, 11, 1242, 552, 281, 1851, 7613, 6851, 293, 536, 577, 436, 6231, 13, 400, 300, 311, 51428], "temperature": 0.0, "avg_logprob": -0.0936571510092726, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0031581339426338673}, {"id": 735, "seek": 416352, "start": 4184.8, "end": 4192.160000000001, "text": " the measure of intelligence. It's adaptability, the ability to deal with novel and unknown", "tokens": [51428, 264, 3481, 295, 7599, 13, 467, 311, 6231, 2310, 11, 264, 3485, 281, 2028, 365, 7613, 293, 9841, 51796], "temperature": 0.0, "avg_logprob": -0.0936571510092726, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0031581339426338673}, {"id": 736, "seek": 419216, "start": 4192.16, "end": 4198.96, "text": " situations. But in order to give your system a novel and unknown situation, you need to have", "tokens": [50364, 6851, 13, 583, 294, 1668, 281, 976, 428, 1185, 257, 7613, 293, 9841, 2590, 11, 291, 643, 281, 362, 50704], "temperature": 0.0, "avg_logprob": -0.12923339355823604, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002233929408248514}, {"id": 737, "seek": 419216, "start": 4198.96, "end": 4205.28, "text": " this white box understanding of what it already knows about. And that's not really something", "tokens": [50704, 341, 2418, 2424, 3701, 295, 437, 309, 1217, 3255, 466, 13, 400, 300, 311, 406, 534, 746, 51020], "temperature": 0.0, "avg_logprob": -0.12923339355823604, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002233929408248514}, {"id": 738, "seek": 419216, "start": 4205.28, "end": 4212.4, "text": " you can work on. So can I ask about the generalization difficulty? Because I sort of had", "tokens": [51020, 291, 393, 589, 322, 13, 407, 393, 286, 1029, 466, 264, 2674, 2144, 10360, 30, 1436, 286, 1333, 295, 632, 51376], "temperature": 0.0, "avg_logprob": -0.12923339355823604, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002233929408248514}, {"id": 739, "seek": 419216, "start": 4212.4, "end": 4217.76, "text": " some difficulty intuitively with some of, let's say, it's limiting cases. So for example,", "tokens": [51376, 512, 10360, 46506, 365, 512, 295, 11, 718, 311, 584, 11, 309, 311, 22083, 3331, 13, 407, 337, 1365, 11, 51644], "temperature": 0.0, "avg_logprob": -0.12923339355823604, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002233929408248514}, {"id": 740, "seek": 421776, "start": 4218.72, "end": 4223.76, "text": " you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems", "tokens": [50412, 291, 458, 11, 264, 9284, 299, 14024, 307, 6343, 13, 961, 311, 445, 7297, 321, 434, 6260, 365, 2740, 50664], "temperature": 0.0, "avg_logprob": -0.1095123984596946, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.000570305623114109}, {"id": 741, "seek": 421776, "start": 4223.76, "end": 4230.320000000001, "text": " tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic", "tokens": [50664, 9608, 689, 321, 362, 2035, 6352, 295, 41674, 33318, 281, 4018, 11, 472, 4190, 11, 291, 458, 11, 264, 9284, 299, 50992], "temperature": 0.0, "avg_logprob": -0.1095123984596946, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.000570305623114109}, {"id": 742, "seek": 421776, "start": 4230.320000000001, "end": 4236.08, "text": " complexity will be greatest when that's just a random mapping, like I just assigned zero and one", "tokens": [50992, 14024, 486, 312, 6636, 562, 300, 311, 445, 257, 4974, 18350, 11, 411, 286, 445, 13279, 4018, 293, 472, 51280], "temperature": 0.0, "avg_logprob": -0.1095123984596946, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.000570305623114109}, {"id": 743, "seek": 421776, "start": 4236.08, "end": 4241.280000000001, "text": " randomly to every single integer. And if I go to look at that generalization difficulty,", "tokens": [51280, 16979, 281, 633, 2167, 24922, 13, 400, 498, 286, 352, 281, 574, 412, 300, 2674, 2144, 10360, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1095123984596946, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.000570305623114109}, {"id": 744, "seek": 421776, "start": 4241.280000000001, "end": 4246.320000000001, "text": " it's going to be super high, because the length of the program for any set is basically going to", "tokens": [51540, 309, 311, 516, 281, 312, 1687, 1090, 11, 570, 264, 4641, 295, 264, 1461, 337, 604, 992, 307, 1936, 516, 281, 51792], "temperature": 0.0, "avg_logprob": -0.1095123984596946, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.000570305623114109}, {"id": 745, "seek": 424632, "start": 4246.719999999999, "end": 4252.32, "text": " be, you'd have to encode the entire set as a hash table, right? So how does like this measure", "tokens": [50384, 312, 11, 291, 1116, 362, 281, 2058, 1429, 264, 2302, 992, 382, 257, 22019, 3199, 11, 558, 30, 407, 577, 775, 411, 341, 3481, 50664], "temperature": 0.0, "avg_logprob": -0.13567320505777994, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034810686483979225}, {"id": 746, "seek": 424632, "start": 4252.32, "end": 4259.12, "text": " account for or help us avoid problems where we're confusing generalization difficulty with just", "tokens": [50664, 2696, 337, 420, 854, 505, 5042, 2740, 689, 321, 434, 13181, 2674, 2144, 10360, 365, 445, 51004], "temperature": 0.0, "avg_logprob": -0.13567320505777994, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034810686483979225}, {"id": 747, "seek": 424632, "start": 4259.12, "end": 4264.88, "text": " increasing random, you know, randomness? Well, I mean, increasing randomness is a part of", "tokens": [51004, 5662, 4974, 11, 291, 458, 11, 4974, 1287, 30, 1042, 11, 286, 914, 11, 5662, 4974, 1287, 307, 257, 644, 295, 51292], "temperature": 0.0, "avg_logprob": -0.13567320505777994, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034810686483979225}, {"id": 748, "seek": 424632, "start": 4264.88, "end": 4270.799999999999, "text": " the realization difficulty, right? Generalization is really the ability to deal with the stuff you", "tokens": [51292, 264, 25138, 10360, 11, 558, 30, 6996, 2144, 307, 534, 264, 3485, 281, 2028, 365, 264, 1507, 291, 51588], "temperature": 0.0, "avg_logprob": -0.13567320505777994, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034810686483979225}, {"id": 749, "seek": 424632, "start": 4270.799999999999, "end": 4275.84, "text": " don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is", "tokens": [51588, 500, 380, 458, 466, 264, 1507, 291, 500, 380, 2066, 11, 264, 1507, 291, 2378, 380, 1612, 949, 13, 400, 4974, 1287, 307, 51840], "temperature": 0.0, "avg_logprob": -0.13567320505777994, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034810686483979225}, {"id": 750, "seek": 427584, "start": 4275.92, "end": 4280.400000000001, "text": " a part of it. But you're right that if you just add randomness to a system, you're increasing", "tokens": [50368, 257, 644, 295, 309, 13, 583, 291, 434, 558, 300, 498, 291, 445, 909, 4974, 1287, 281, 257, 1185, 11, 291, 434, 5662, 50592], "temperature": 0.0, "avg_logprob": -0.1147248416111387, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0025104887317866087}, {"id": 751, "seek": 427584, "start": 4280.400000000001, "end": 4285.68, "text": " the generalization difficulty, but you're not increasing it in a very interesting way, right?", "tokens": [50592, 264, 2674, 2144, 10360, 11, 457, 291, 434, 406, 5662, 309, 294, 257, 588, 1880, 636, 11, 558, 30, 50856], "temperature": 0.0, "avg_logprob": -0.1147248416111387, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0025104887317866087}, {"id": 752, "seek": 427584, "start": 4285.68, "end": 4291.4400000000005, "text": " Because you're increasing it in a way that's kind of orthogonal to an integration system's ability", "tokens": [50856, 1436, 291, 434, 5662, 309, 294, 257, 636, 300, 311, 733, 295, 41488, 281, 364, 10980, 1185, 311, 3485, 51144], "temperature": 0.0, "avg_logprob": -0.1147248416111387, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0025104887317866087}, {"id": 753, "seek": 427584, "start": 4291.4400000000005, "end": 4297.52, "text": " to deal with it, right? The best you can do is modify the system to be more robust to very much", "tokens": [51144, 281, 2028, 365, 309, 11, 558, 30, 440, 1151, 291, 393, 360, 307, 16927, 264, 1185, 281, 312, 544, 13956, 281, 588, 709, 51448], "temperature": 0.0, "avg_logprob": -0.1147248416111387, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0025104887317866087}, {"id": 754, "seek": 427584, "start": 4297.52, "end": 4303.12, "text": " randomness. But that's not super interesting. What's really interesting is to test the system's", "tokens": [51448, 4974, 1287, 13, 583, 300, 311, 406, 1687, 1880, 13, 708, 311, 534, 1880, 307, 281, 1500, 264, 1185, 311, 51728], "temperature": 0.0, "avg_logprob": -0.1147248416111387, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0025104887317866087}, {"id": 755, "seek": 430312, "start": 4303.12, "end": 4310.32, "text": " sensitivity to subtle analogies, is to make the system face novel and unexpected situations that", "tokens": [50364, 19392, 281, 13743, 16660, 530, 11, 307, 281, 652, 264, 1185, 1851, 7613, 293, 13106, 6851, 300, 50724], "temperature": 0.0, "avg_logprob": -0.16631813049316407, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.001366511220112443}, {"id": 756, "seek": 430312, "start": 4310.32, "end": 4316.16, "text": " are actually derived from the past, but in interesting ways, right? Not just random ways.", "tokens": [50724, 366, 767, 18949, 490, 264, 1791, 11, 457, 294, 1880, 2098, 11, 558, 30, 1726, 445, 4974, 2098, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16631813049316407, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.001366511220112443}, {"id": 757, "seek": 430312, "start": 4316.16, "end": 4324.72, "text": " You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and", "tokens": [51016, 509, 600, 1190, 341, 48751, 22631, 3430, 322, 8943, 34, 13, 400, 11, 291, 458, 11, 321, 458, 490, 3652, 1270, 382, 20588, 1037, 293, 51444], "temperature": 0.0, "avg_logprob": -0.16631813049316407, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.001366511220112443}, {"id": 758, "seek": 430312, "start": 4324.72, "end": 4332.0, "text": " so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable,", "tokens": [51444, 370, 322, 300, 11450, 19639, 3759, 13232, 11, 411, 11450, 19639, 3759, 7318, 3652, 393, 312, 588, 8263, 11, 51808], "temperature": 0.0, "avg_logprob": -0.16631813049316407, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.001366511220112443}, {"id": 759, "seek": 433200, "start": 4332.0, "end": 4338.64, "text": " like playing them against each other and so on. And also, we know that something like markets can", "tokens": [50364, 411, 2433, 552, 1970, 1184, 661, 293, 370, 322, 13, 400, 611, 11, 321, 458, 300, 746, 411, 8383, 393, 50696], "temperature": 0.0, "avg_logprob": -0.06474901585096723, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003706118790432811}, {"id": 760, "seek": 433200, "start": 4338.64, "end": 4349.2, "text": " be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks", "tokens": [50696, 312, 588, 7148, 293, 8263, 13, 400, 286, 3811, 257, 1185, 689, 291, 1116, 362, 12554, 4084, 8943, 34, 9608, 51224], "temperature": 0.0, "avg_logprob": -0.06474901585096723, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003706118790432811}, {"id": 761, "seek": 433200, "start": 4349.2, "end": 4355.04, "text": " and other agents solving ARC tasks, and they're going some kind of money around and so on. And", "tokens": [51224, 293, 661, 12554, 12606, 8943, 34, 9608, 11, 293, 436, 434, 516, 512, 733, 295, 1460, 926, 293, 370, 322, 13, 400, 51516], "temperature": 0.0, "avg_logprob": -0.06474901585096723, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003706118790432811}, {"id": 762, "seek": 433200, "start": 4355.04, "end": 4361.12, "text": " this could be kind of a powerful engine for research teams to research anything like this.", "tokens": [51516, 341, 727, 312, 733, 295, 257, 4005, 2848, 337, 2132, 5491, 281, 2132, 1340, 411, 341, 13, 51820], "temperature": 0.0, "avg_logprob": -0.06474901585096723, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003706118790432811}, {"id": 763, "seek": 436112, "start": 4361.12, "end": 4367.599999999999, "text": " And, you know, given that you have, I don't know how much, but you do have the backing of Google", "tokens": [50364, 400, 11, 291, 458, 11, 2212, 300, 291, 362, 11, 286, 500, 380, 458, 577, 709, 11, 457, 291, 360, 362, 264, 19373, 295, 3329, 50688], "temperature": 0.0, "avg_logprob": -0.13535093959373765, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004385269712656736}, {"id": 764, "seek": 436112, "start": 4367.599999999999, "end": 4378.16, "text": " with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is", "tokens": [50688, 365, 257, 857, 295, 4238, 294, 1011, 13, 7497, 291, 3811, 456, 885, 257, 2944, 337, 341, 733, 295, 551, 30, 1610, 307, 51216], "temperature": 0.0, "avg_logprob": -0.13535093959373765, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004385269712656736}, {"id": 765, "seek": 436112, "start": 4378.16, "end": 4388.88, "text": " it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google", "tokens": [51216, 309, 11, 382, 295, 586, 11, 364, 12576, 18769, 30, 865, 11, 370, 286, 500, 380, 362, 300, 709, 19373, 291, 490, 3329, 51752], "temperature": 0.0, "avg_logprob": -0.13535093959373765, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.004385269712656736}, {"id": 766, "seek": 438888, "start": 4388.88, "end": 4395.4400000000005, "text": " around this kind of project. But, yeah, so it would be super interesting to have this kind of", "tokens": [50364, 926, 341, 733, 295, 1716, 13, 583, 11, 1338, 11, 370, 309, 576, 312, 1687, 1880, 281, 362, 341, 733, 295, 50692], "temperature": 0.0, "avg_logprob": -0.12728679314088287, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.006698341574519873}, {"id": 767, "seek": 438888, "start": 4395.4400000000005, "end": 4400.64, "text": " two-part system where one part is generating the task and one part is learning to solve them.", "tokens": [50692, 732, 12, 6971, 1185, 689, 472, 644, 307, 17746, 264, 5633, 293, 472, 644, 307, 2539, 281, 5039, 552, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12728679314088287, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.006698341574519873}, {"id": 768, "seek": 438888, "start": 4400.64, "end": 4406.72, "text": " And you could get them to do some kind of curriculum optimization, like the task generator network", "tokens": [50952, 400, 291, 727, 483, 552, 281, 360, 512, 733, 295, 14302, 19618, 11, 411, 264, 5633, 19265, 3209, 51256], "temperature": 0.0, "avg_logprob": -0.12728679314088287, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.006698341574519873}, {"id": 769, "seek": 438888, "start": 4406.72, "end": 4413.04, "text": " would not just be trying to generate tasks that look like ARC tasks. It would be trying to", "tokens": [51256, 576, 406, 445, 312, 1382, 281, 8460, 9608, 300, 574, 411, 8943, 34, 9608, 13, 467, 576, 312, 1382, 281, 51572], "temperature": 0.0, "avg_logprob": -0.12728679314088287, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.006698341574519873}, {"id": 770, "seek": 441304, "start": 4413.04, "end": 4419.12, "text": " generate tasks that correspond to level of generalization, difficulty and complexity that is", "tokens": [50364, 8460, 9608, 300, 6805, 281, 1496, 295, 2674, 2144, 11, 10360, 293, 14024, 300, 307, 50668], "temperature": 0.0, "avg_logprob": -0.11425808993252841, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.012266227975487709}, {"id": 771, "seek": 441304, "start": 4420.08, "end": 4425.44, "text": " right below the limits of the student system that's trying to solve them. Kind of like, you know,", "tokens": [50716, 558, 2507, 264, 10406, 295, 264, 3107, 1185, 300, 311, 1382, 281, 5039, 552, 13, 9242, 295, 411, 11, 291, 458, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11425808993252841, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.012266227975487709}, {"id": 772, "seek": 441304, "start": 4425.44, "end": 4432.72, "text": " the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be.", "tokens": [50984, 264, 636, 257, 5027, 576, 2893, 11900, 300, 366, 1404, 17915, 11, 457, 7595, 13, 814, 4659, 380, 312, 13, 51348], "temperature": 0.0, "avg_logprob": -0.11425808993252841, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.012266227975487709}, {"id": 773, "seek": 441304, "start": 4432.72, "end": 4436.72, "text": " They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because", "tokens": [51348, 814, 4659, 380, 312, 1858, 13, 814, 4659, 380, 312, 6243, 13, 814, 820, 312, 1404, 17915, 13, 1436, 51548], "temperature": 0.0, "avg_logprob": -0.11425808993252841, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.012266227975487709}, {"id": 774, "seek": 441304, "start": 4436.72, "end": 4441.76, "text": " that's how you get the most growth. So it's actually a system that's described at the very end", "tokens": [51548, 300, 311, 577, 291, 483, 264, 881, 4599, 13, 407, 309, 311, 767, 257, 1185, 300, 311, 7619, 412, 264, 588, 917, 51800], "temperature": 0.0, "avg_logprob": -0.11425808993252841, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.012266227975487709}, {"id": 775, "seek": 444176, "start": 4442.4800000000005, "end": 4447.68, "text": " of the paper on the measure of contagions. And I think one thing I point out in the paper is", "tokens": [50400, 295, 264, 3035, 322, 264, 3481, 295, 28525, 626, 13, 400, 286, 519, 472, 551, 286, 935, 484, 294, 264, 3035, 307, 50660], "temperature": 0.0, "avg_logprob": -0.18622589111328125, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006370278540998697}, {"id": 776, "seek": 444176, "start": 4447.68, "end": 4453.360000000001, "text": " kind of like the pitfall you should avoid falling into is that this system is circular,", "tokens": [50660, 733, 295, 411, 264, 10147, 6691, 291, 820, 5042, 7440, 666, 307, 300, 341, 1185, 307, 16476, 11, 50944], "temperature": 0.0, "avg_logprob": -0.18622589111328125, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006370278540998697}, {"id": 777, "seek": 444176, "start": 4454.0, "end": 4460.0, "text": " right? And the complexity you're going to see in your task, it needs to come from somewhere, right?", "tokens": [50976, 558, 30, 400, 264, 14024, 291, 434, 516, 281, 536, 294, 428, 5633, 11, 309, 2203, 281, 808, 490, 4079, 11, 558, 30, 51276], "temperature": 0.0, "avg_logprob": -0.18622589111328125, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006370278540998697}, {"id": 778, "seek": 444176, "start": 4461.2, "end": 4466.72, "text": " It's like conservation of complexity. So the system, this two-part system needs to have", "tokens": [51336, 467, 311, 411, 16185, 295, 14024, 13, 407, 264, 1185, 11, 341, 732, 12, 6971, 1185, 2203, 281, 362, 51612], "temperature": 0.0, "avg_logprob": -0.18622589111328125, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.006370278540998697}, {"id": 779, "seek": 446672, "start": 4467.68, "end": 4473.84, "text": " a source of intrinsic complexity. It needs to be grounded in the real world.", "tokens": [50412, 257, 4009, 295, 35698, 14024, 13, 467, 2203, 281, 312, 23535, 294, 264, 957, 1002, 13, 50720], "temperature": 0.0, "avg_logprob": -0.13723604903261885, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.0023480416275560856}, {"id": 780, "seek": 446672, "start": 4473.84, "end": 4479.360000000001, "text": " And one way we can achieve that grounding, and I've been thinking about it, is I think we should,", "tokens": [50720, 400, 472, 636, 321, 393, 4584, 300, 46727, 11, 293, 286, 600, 668, 1953, 466, 309, 11, 307, 286, 519, 321, 820, 11, 50996], "temperature": 0.0, "avg_logprob": -0.13723604903261885, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.0023480416275560856}, {"id": 781, "seek": 446672, "start": 4479.360000000001, "end": 4485.12, "text": " you know, like ARC tasks, as they are today, they're made by me and this is not a good setup", "tokens": [50996, 291, 458, 11, 411, 8943, 34, 9608, 11, 382, 436, 366, 965, 11, 436, 434, 1027, 538, 385, 293, 341, 307, 406, 257, 665, 8657, 51284], "temperature": 0.0, "avg_logprob": -0.13723604903261885, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.0023480416275560856}, {"id": 782, "seek": 446672, "start": 4485.12, "end": 4490.400000000001, "text": " because it's going to be biased. It's going to be very bottlenecked as well. I think we should", "tokens": [51284, 570, 309, 311, 516, 281, 312, 28035, 13, 467, 311, 516, 281, 312, 588, 44641, 44118, 382, 731, 13, 286, 519, 321, 820, 51548], "temperature": 0.0, "avg_logprob": -0.13723604903261885, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.0023480416275560856}, {"id": 783, "seek": 446672, "start": 4490.400000000001, "end": 4495.92, "text": " start crowdsourcing our task. There should definitely be, you know, a filtering system so", "tokens": [51548, 722, 26070, 41849, 527, 5633, 13, 821, 820, 2138, 312, 11, 291, 458, 11, 257, 30822, 1185, 370, 51824], "temperature": 0.0, "avg_logprob": -0.13723604903261885, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.0023480416275560856}, {"id": 784, "seek": 449592, "start": 4495.92, "end": 4500.0, "text": " that we make sure that we're only keeping our tasks that are interesting, that are not too easy,", "tokens": [50364, 300, 321, 652, 988, 300, 321, 434, 787, 5145, 527, 9608, 300, 366, 1880, 11, 300, 366, 406, 886, 1858, 11, 50568], "temperature": 0.0, "avg_logprob": -0.13972780725977443, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.0002337323094252497}, {"id": 785, "seek": 449592, "start": 4500.0, "end": 4506.4800000000005, "text": " that are not difficult, and that are only grounded in core knowledge priors. But if we have, like,", "tokens": [50568, 300, 366, 406, 2252, 11, 293, 300, 366, 787, 23535, 294, 4965, 3601, 1790, 830, 13, 583, 498, 321, 362, 11, 411, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13972780725977443, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.0002337323094252497}, {"id": 786, "seek": 449592, "start": 4506.4800000000005, "end": 4512.0, "text": " this stream of novel ARC tasks that contain intrinsic complexity and novel information,", "tokens": [50892, 341, 4309, 295, 7613, 8943, 34, 9608, 300, 5304, 35698, 14024, 293, 7613, 1589, 11, 51168], "temperature": 0.0, "avg_logprob": -0.13972780725977443, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.0002337323094252497}, {"id": 787, "seek": 449592, "start": 4512.0, "end": 4517.04, "text": " because they come from the real world, they come from human brains, that have experienced the", "tokens": [51168, 570, 436, 808, 490, 264, 957, 1002, 11, 436, 808, 490, 1952, 15442, 11, 300, 362, 6751, 264, 51420], "temperature": 0.0, "avg_logprob": -0.13972780725977443, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.0002337323094252497}, {"id": 788, "seek": 449592, "start": 4517.04, "end": 4523.12, "text": " real world, and you use that as a way to ground your task generator, then you're starting to get", "tokens": [51420, 957, 1002, 11, 293, 291, 764, 300, 382, 257, 636, 281, 2727, 428, 5633, 19265, 11, 550, 291, 434, 2891, 281, 483, 51724], "temperature": 0.0, "avg_logprob": -0.13972780725977443, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.0002337323094252497}, {"id": 789, "seek": 452312, "start": 4523.12, "end": 4529.44, "text": " a very interesting three-part system, right? So I would love to actually get that started,", "tokens": [50364, 257, 588, 1880, 1045, 12, 6971, 1185, 11, 558, 30, 407, 286, 576, 959, 281, 767, 483, 300, 1409, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1590986019227563, "compression_ratio": 1.625, "no_speech_prob": 0.004002440720796585}, {"id": 790, "seek": 452312, "start": 4529.44, "end": 4536.24, "text": " to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks", "tokens": [50680, 281, 767, 5258, 257, 691, 17, 295, 8943, 34, 382, 2321, 382, 1944, 11, 718, 311, 4090, 11, 291, 458, 11, 1266, 87, 544, 9608, 51020], "temperature": 0.0, "avg_logprob": -0.1590986019227563, "compression_ratio": 1.625, "no_speech_prob": 0.004002440720796585}, {"id": 791, "seek": 452312, "start": 4536.24, "end": 4541.44, "text": " that will be crowdsourced, and maybe something that will take the form of a continuous challenge", "tokens": [51020, 300, 486, 312, 26070, 396, 1232, 11, 293, 1310, 746, 300, 486, 747, 264, 1254, 295, 257, 10957, 3430, 51280], "temperature": 0.0, "avg_logprob": -0.1590986019227563, "compression_ratio": 1.625, "no_speech_prob": 0.004002440720796585}, {"id": 792, "seek": 452312, "start": 4541.44, "end": 4546.24, "text": " where you have an API where you can draw a new ARC task, and every time you draw a task, it's", "tokens": [51280, 689, 291, 362, 364, 9362, 689, 291, 393, 2642, 257, 777, 8943, 34, 5633, 11, 293, 633, 565, 291, 2642, 257, 5633, 11, 309, 311, 51520], "temperature": 0.0, "avg_logprob": -0.1590986019227563, "compression_ratio": 1.625, "no_speech_prob": 0.004002440720796585}, {"id": 793, "seek": 452312, "start": 4546.24, "end": 4552.4, "text": " actually a different one because you have so many of them. Gamify it, that'll make a fun game", "tokens": [51520, 767, 257, 819, 472, 570, 291, 362, 370, 867, 295, 552, 13, 24723, 2505, 309, 11, 300, 603, 652, 257, 1019, 1216, 51828], "temperature": 0.0, "avg_logprob": -0.1590986019227563, "compression_ratio": 1.625, "no_speech_prob": 0.004002440720796585}, {"id": 794, "seek": 455240, "start": 4552.48, "end": 4557.44, "text": " on a mobile app. There are actually a few people who have created, because ARC is open source,", "tokens": [50368, 322, 257, 6013, 724, 13, 821, 366, 767, 257, 1326, 561, 567, 362, 2942, 11, 570, 8943, 34, 307, 1269, 4009, 11, 50616], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 795, "seek": 455240, "start": 4557.44, "end": 4561.12, "text": " and they're totally free licensed, there are a few people who have created mobile apps where", "tokens": [50616, 293, 436, 434, 3879, 1737, 25225, 11, 456, 366, 257, 1326, 561, 567, 362, 2942, 6013, 7733, 689, 50800], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 796, "seek": 455240, "start": 4561.12, "end": 4565.12, "text": " users sort of ARC tasks, and apparently it's popular. So there's also the other angle you", "tokens": [50800, 5022, 1333, 295, 8943, 34, 9608, 11, 293, 7970, 309, 311, 3743, 13, 407, 456, 311, 611, 264, 661, 5802, 291, 51000], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 797, "seek": 455240, "start": 4565.12, "end": 4568.96, "text": " mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right", "tokens": [51000, 2835, 294, 264, 3035, 11, 597, 390, 11, 597, 307, 1238, 10343, 11, 291, 434, 1417, 466, 309, 1920, 558, 51192], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 798, "seek": 455240, "start": 4568.96, "end": 4577.44, "text": " now, which is that, okay, let's start thinking about how to map ARC performance to psychometric,", "tokens": [51192, 586, 11, 597, 307, 300, 11, 1392, 11, 718, 311, 722, 1953, 466, 577, 281, 4471, 8943, 34, 3389, 281, 4681, 29470, 11, 51616], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 799, "seek": 455240, "start": 4577.44, "end": 4581.679999999999, "text": " you know, classic kind of psychometric tests. Are there any efforts that you're aware of", "tokens": [51616, 291, 458, 11, 7230, 733, 295, 4681, 29470, 6921, 13, 2014, 456, 604, 6484, 300, 291, 434, 3650, 295, 51828], "temperature": 0.0, "avg_logprob": -0.15285860883058422, "compression_ratio": 1.8311688311688312, "no_speech_prob": 0.010648668743669987}, {"id": 800, "seek": 458168, "start": 4581.68, "end": 4590.0, "text": " underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a", "tokens": [50364, 27534, 558, 586, 281, 360, 300, 30, 2014, 291, 3288, 294, 604, 462, 8241, 82, 30, 865, 11, 462, 8241, 82, 11, 286, 478, 406, 988, 13, 407, 321, 630, 257, 50780], "temperature": 0.0, "avg_logprob": -0.16565754983277448, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.004902360495179892}, {"id": 801, "seek": 458168, "start": 4590.0, "end": 4595.6, "text": " workshop at AAAI the other day, and there were two presentations about efforts that teams of people,", "tokens": [50780, 13541, 412, 34347, 40, 264, 661, 786, 11, 293, 456, 645, 732, 18964, 466, 6484, 300, 5491, 295, 561, 11, 51060], "temperature": 0.0, "avg_logprob": -0.16565754983277448, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.004902360495179892}, {"id": 802, "seek": 458168, "start": 4595.6, "end": 4601.280000000001, "text": " so there are people who do neuropsychology, and they're using ARC in very interesting ways. So", "tokens": [51060, 370, 456, 366, 561, 567, 360, 22510, 1513, 4202, 1793, 11, 293, 436, 434, 1228, 8943, 34, 294, 588, 1880, 2098, 13, 407, 51344], "temperature": 0.0, "avg_logprob": -0.16565754983277448, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.004902360495179892}, {"id": 803, "seek": 458168, "start": 4601.280000000001, "end": 4606.96, "text": " there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology", "tokens": [51344, 456, 311, 257, 1594, 412, 42682, 11, 293, 456, 311, 257, 1594, 412, 13100, 11, 293, 1338, 11, 370, 436, 434, 1228, 8943, 34, 337, 22510, 1513, 4202, 1793, 51628], "temperature": 0.0, "avg_logprob": -0.16565754983277448, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.004902360495179892}, {"id": 804, "seek": 460696, "start": 4606.96, "end": 4613.28, "text": " experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of", "tokens": [50364, 12050, 11, 293, 309, 311, 309, 311, 1687, 1627, 13, 14165, 13, 286, 528, 281, 3679, 670, 257, 707, 857, 11, 570, 295, 50680], "temperature": 0.0, "avg_logprob": -0.14898624420166015, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.08846335858106613}, {"id": 805, "seek": 460696, "start": 4613.28, "end": 4618.64, "text": " course, you know, other than the measurement of intelligence, you are also famous for a small", "tokens": [50680, 1164, 11, 291, 458, 11, 661, 813, 264, 13160, 295, 7599, 11, 291, 366, 611, 4618, 337, 257, 1359, 50948], "temperature": 0.0, "avg_logprob": -0.14898624420166015, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.08846335858106613}, {"id": 806, "seek": 460696, "start": 4618.64, "end": 4627.28, "text": " library you wrote once in a while called Keras. And I wish I wrote it, and then that was that.", "tokens": [50948, 6405, 291, 4114, 1564, 294, 257, 1339, 1219, 591, 6985, 13, 400, 286, 3172, 286, 4114, 309, 11, 293, 550, 300, 390, 300, 13, 51380], "temperature": 0.0, "avg_logprob": -0.14898624420166015, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.08846335858106613}, {"id": 807, "seek": 460696, "start": 4628.64, "end": 4632.56, "text": " No, I yeah, it's been very much an ongoing project for the past six years.", "tokens": [51448, 883, 11, 286, 1338, 11, 309, 311, 668, 588, 709, 364, 10452, 1716, 337, 264, 1791, 2309, 924, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14898624420166015, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.08846335858106613}, {"id": 808, "seek": 463256, "start": 4633.52, "end": 4639.04, "text": " It was because I remember, you know, the days of TensorFlow one and and Theano,", "tokens": [50412, 467, 390, 570, 286, 1604, 11, 291, 458, 11, 264, 1708, 295, 37624, 472, 293, 293, 440, 3730, 11, 50688], "temperature": 0.0, "avg_logprob": -0.18867273613958074, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001572290202602744}, {"id": 809, "seek": 463256, "start": 4639.84, "end": 4646.320000000001, "text": " and things like this. And Keras was just, I think, so helpful to a lot of people, because it just", "tokens": [50728, 293, 721, 411, 341, 13, 400, 591, 6985, 390, 445, 11, 286, 519, 11, 370, 4961, 281, 257, 688, 295, 561, 11, 570, 309, 445, 51052], "temperature": 0.0, "avg_logprob": -0.18867273613958074, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001572290202602744}, {"id": 810, "seek": 463256, "start": 4646.96, "end": 4653.360000000001, "text": " simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to", "tokens": [51084, 26335, 439, 295, 341, 11, 291, 458, 11, 4295, 6435, 11, 25882, 11, 293, 370, 322, 13, 467, 445, 1027, 309, 9515, 281, 51404], "temperature": 0.0, "avg_logprob": -0.18867273613958074, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001572290202602744}, {"id": 811, "seek": 463256, "start": 4653.360000000001, "end": 4660.240000000001, "text": " so many people. And now with the development of, you know, things like PyTorch and TensorFlow two,", "tokens": [51404, 370, 867, 561, 13, 400, 586, 365, 264, 3250, 295, 11, 291, 458, 11, 721, 411, 9953, 51, 284, 339, 293, 37624, 732, 11, 51748], "temperature": 0.0, "avg_logprob": -0.18867273613958074, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001572290202602744}, {"id": 812, "seek": 466024, "start": 4660.32, "end": 4667.76, "text": " it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras.", "tokens": [50368, 309, 1920, 2544, 411, 591, 6985, 307, 309, 575, 668, 733, 295, 20799, 538, 37624, 732, 11, 558, 11, 456, 307, 40964, 13, 42, 6985, 13, 50740], "temperature": 0.0, "avg_logprob": -0.13376733108803077, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.003026543417945504}, {"id": 813, "seek": 466024, "start": 4667.76, "end": 4674.24, "text": " And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see", "tokens": [50740, 400, 586, 286, 519, 264, 17569, 21445, 366, 754, 1333, 295, 3161, 3807, 300, 257, 707, 857, 13, 1144, 291, 360, 291, 536, 51064], "temperature": 0.0, "avg_logprob": -0.13376733108803077, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.003026543417945504}, {"id": 814, "seek": 466024, "start": 4675.28, "end": 4680.719999999999, "text": " Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going?", "tokens": [51116, 591, 6985, 516, 1314, 30, 1144, 291, 536, 309, 4473, 30, 2305, 360, 291, 536, 309, 30, 2305, 360, 291, 536, 591, 6985, 516, 30, 51388], "temperature": 0.0, "avg_logprob": -0.13376733108803077, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.003026543417945504}, {"id": 815, "seek": 466024, "start": 4681.36, "end": 4686.88, "text": " Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we", "tokens": [51420, 865, 11, 370, 516, 1314, 11, 2138, 406, 13, 286, 914, 11, 321, 362, 321, 362, 544, 5022, 813, 1562, 949, 13, 400, 321, 51696], "temperature": 0.0, "avg_logprob": -0.13376733108803077, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.003026543417945504}, {"id": 816, "seek": 468688, "start": 4686.88, "end": 4691.76, "text": " are still growing very nicely, both inside Google, like one more teams that Google are moving away", "tokens": [50364, 366, 920, 4194, 588, 9594, 11, 1293, 1854, 3329, 11, 411, 472, 544, 5491, 300, 3329, 366, 2684, 1314, 50608], "temperature": 0.0, "avg_logprob": -0.18163577845839204, "compression_ratio": 1.65993265993266, "no_speech_prob": 0.0021106007043272257}, {"id": 817, "seek": 468688, "start": 4691.76, "end": 4697.04, "text": " from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there,", "tokens": [50608, 490, 37624, 472, 293, 32328, 591, 6985, 293, 2380, 3329, 382, 731, 13, 467, 311, 257, 955, 2142, 484, 456, 11, 50872], "temperature": 0.0, "avg_logprob": -0.18163577845839204, "compression_ratio": 1.65993265993266, "no_speech_prob": 0.0021106007043272257}, {"id": 818, "seek": 468688, "start": 4697.04, "end": 4703.76, "text": " and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly", "tokens": [50872, 293, 456, 311, 2138, 1808, 337, 3866, 29834, 13, 5689, 401, 798, 3122, 11, 286, 914, 11, 591, 6985, 307, 6460, 51208], "temperature": 0.0, "avg_logprob": -0.18163577845839204, "compression_ratio": 1.65993265993266, "no_speech_prob": 0.0021106007043272257}, {"id": 819, "seek": 468688, "start": 4703.76, "end": 4710.08, "text": " evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at", "tokens": [51208, 21085, 11, 457, 21085, 365, 23807, 13, 1743, 498, 291, 574, 412, 591, 6985, 490, 6549, 420, 7546, 11, 291, 574, 412, 51524], "temperature": 0.0, "avg_logprob": -0.18163577845839204, "compression_ratio": 1.65993265993266, "no_speech_prob": 0.0021106007043272257}, {"id": 820, "seek": 468688, "start": 4710.08, "end": 4716.24, "text": " Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very", "tokens": [51524, 591, 6985, 586, 11, 291, 5521, 11, 307, 309, 264, 912, 551, 30, 400, 309, 311, 264, 912, 9362, 13, 400, 1939, 309, 311, 767, 257, 588, 51832], "temperature": 0.0, "avg_logprob": -0.18163577845839204, "compression_ratio": 1.65993265993266, "no_speech_prob": 0.0021106007043272257}, {"id": 821, "seek": 471624, "start": 4716.24, "end": 4722.5599999999995, "text": " different and much, much bigger set of features and things you can do it. So evolving, definitely.", "tokens": [50364, 819, 293, 709, 11, 709, 3801, 992, 295, 4122, 293, 721, 291, 393, 360, 309, 13, 407, 21085, 11, 2138, 13, 50680], "temperature": 0.0, "avg_logprob": -0.166382543156656, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00102997908834368}, {"id": 822, "seek": 471624, "start": 4722.5599999999995, "end": 4729.44, "text": " And there are so several, so you, I think you asked, you know, about, yeah, like,", "tokens": [50680, 400, 456, 366, 370, 2940, 11, 370, 291, 11, 286, 519, 291, 2351, 11, 291, 458, 11, 466, 11, 1338, 11, 411, 11, 51024], "temperature": 0.0, "avg_logprob": -0.166382543156656, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00102997908834368}, {"id": 823, "seek": 471624, "start": 4729.44, "end": 4733.5199999999995, "text": " Keras is getting kind of merged into TensorFlow, does it mean it's like failing away?", "tokens": [51024, 591, 6985, 307, 1242, 733, 295, 36427, 666, 37624, 11, 775, 309, 914, 309, 311, 411, 18223, 1314, 30, 51228], "temperature": 0.0, "avg_logprob": -0.166382543156656, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00102997908834368}, {"id": 824, "seek": 471624, "start": 4733.5199999999995, "end": 4739.84, "text": " So definitely not. So merging with TensorFlow was a good idea because it starts enabling", "tokens": [51228, 407, 2138, 406, 13, 407, 44559, 365, 37624, 390, 257, 665, 1558, 570, 309, 3719, 23148, 51544], "temperature": 0.0, "avg_logprob": -0.166382543156656, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00102997908834368}, {"id": 825, "seek": 473984, "start": 4739.92, "end": 4746.4800000000005, "text": " a spectrum of workflows from the very high level, like scikit-learn like, to the very low level,", "tokens": [50368, 257, 11143, 295, 43461, 490, 264, 588, 1090, 1496, 11, 411, 2180, 22681, 12, 306, 1083, 411, 11, 281, 264, 588, 2295, 1496, 11, 50696], "temperature": 0.0, "avg_logprob": -0.18531011826921218, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.05100023001432419}, {"id": 826, "seek": 473984, "start": 4746.4800000000005, "end": 4753.76, "text": " numpy like, and everything in between. In the early days, because Keras had to interact with", "tokens": [50696, 1031, 8200, 411, 11, 293, 1203, 294, 1296, 13, 682, 264, 2440, 1708, 11, 570, 591, 6985, 632, 281, 4648, 365, 51060], "temperature": 0.0, "avg_logprob": -0.18531011826921218, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.05100023001432419}, {"id": 827, "seek": 473984, "start": 4753.76, "end": 4760.08, "text": " multiple backends via backend interface, it means you had this kind of like a barrier where as long", "tokens": [51060, 3866, 646, 2581, 5766, 38087, 9226, 11, 309, 1355, 291, 632, 341, 733, 295, 411, 257, 13357, 689, 382, 938, 51376], "temperature": 0.0, "avg_logprob": -0.18531011826921218, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.05100023001432419}, {"id": 828, "seek": 473984, "start": 4760.08, "end": 4765.52, "text": " as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy,", "tokens": [51376, 382, 291, 764, 264, 591, 6985, 21445, 11, 1203, 390, 1687, 2199, 13, 467, 390, 2180, 22681, 12, 306, 1083, 411, 11, 370, 588, 1858, 11, 51648], "temperature": 0.0, "avg_logprob": -0.18531011826921218, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.05100023001432419}, {"id": 829, "seek": 476552, "start": 4765.52, "end": 4771.52, "text": " very proactive, very fast. But if you wanted more customization, at some point, you would hit", "tokens": [50364, 588, 28028, 11, 588, 2370, 13, 583, 498, 291, 1415, 544, 39387, 11, 412, 512, 935, 11, 291, 576, 2045, 50664], "temperature": 0.0, "avg_logprob": -0.17833680522685147, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.0060457405634224415}, {"id": 830, "seek": 476552, "start": 4771.52, "end": 4777.4400000000005, "text": " that backend barrier. And you had to reverse to TensorFlow base or piano base workflow,", "tokens": [50664, 300, 38087, 13357, 13, 400, 291, 632, 281, 9943, 281, 37624, 3096, 420, 9211, 3096, 20993, 11, 50960], "temperature": 0.0, "avg_logprob": -0.17833680522685147, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.0060457405634224415}, {"id": 831, "seek": 476552, "start": 4777.4400000000005, "end": 4782.64, "text": " that was low level, but when, where you couldn't really leverage Keras effectively,", "tokens": [50960, 300, 390, 2295, 1496, 11, 457, 562, 11, 689, 291, 2809, 380, 534, 13982, 591, 6985, 8659, 11, 51220], "temperature": 0.0, "avg_logprob": -0.17833680522685147, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.0060457405634224415}, {"id": 832, "seek": 476552, "start": 4782.64, "end": 4787.92, "text": " by removing the backend thing and just saying the flow together in one spectrum,", "tokens": [51220, 538, 12720, 264, 38087, 551, 293, 445, 1566, 264, 3095, 1214, 294, 472, 11143, 11, 51484], "temperature": 0.0, "avg_logprob": -0.17833680522685147, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.0060457405634224415}, {"id": 833, "seek": 476552, "start": 4787.92, "end": 4793.68, "text": " then you get really this progressive disclosure of complexity when you can start out with the", "tokens": [51484, 550, 291, 483, 534, 341, 16131, 30392, 295, 14024, 562, 291, 393, 722, 484, 365, 264, 51772], "temperature": 0.0, "avg_logprob": -0.17833680522685147, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.0060457405634224415}, {"id": 834, "seek": 479368, "start": 4793.84, "end": 4798.88, "text": " very high level thing, but then you need to customize your training step. You have an API for", "tokens": [50372, 588, 1090, 1496, 551, 11, 457, 550, 291, 643, 281, 19734, 428, 3097, 1823, 13, 509, 362, 364, 9362, 337, 50624], "temperature": 0.0, "avg_logprob": -0.164030412657071, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001954160165041685}, {"id": 835, "seek": 479368, "start": 4798.88, "end": 4805.52, "text": " that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high", "tokens": [50624, 300, 13, 400, 291, 393, 445, 2890, 293, 2995, 38083, 264, 2295, 1496, 37624, 1507, 365, 264, 1090, 50956], "temperature": 0.0, "avg_logprob": -0.164030412657071, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001954160165041685}, {"id": 836, "seek": 479368, "start": 4805.52, "end": 4810.0, "text": " level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the", "tokens": [50956, 1496, 591, 6985, 1823, 13, 400, 300, 636, 291, 393, 4584, 604, 11, 393, 589, 365, 591, 6985, 293, 37624, 412, 264, 51180], "temperature": 0.0, "avg_logprob": -0.164030412657071, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001954160165041685}, {"id": 837, "seek": 479368, "start": 4810.0, "end": 4815.68, "text": " level of abstraction that you want. Very, very easy high level or very, very low level full", "tokens": [51180, 1496, 295, 37765, 300, 291, 528, 13, 4372, 11, 588, 1858, 1090, 1496, 420, 588, 11, 588, 2295, 1496, 1577, 51464], "temperature": 0.0, "avg_logprob": -0.164030412657071, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001954160165041685}, {"id": 838, "seek": 479368, "start": 4815.68, "end": 4821.52, "text": " flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting", "tokens": [51464, 12635, 13, 467, 311, 493, 281, 291, 13, 286, 478, 516, 281, 935, 484, 264, 30423, 510, 281, 16660, 1125, 11015, 51756], "temperature": 0.0, "avg_logprob": -0.164030412657071, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001954160165041685}, {"id": 839, "seek": 482152, "start": 4821.52, "end": 4827.84, "text": " type one with type two reason. Yeah, why not? I was just about to do that. At least Francois", "tokens": [50364, 2010, 472, 365, 2010, 732, 1778, 13, 865, 11, 983, 406, 30, 286, 390, 445, 466, 281, 360, 300, 13, 1711, 1935, 34695, 271, 50680], "temperature": 0.0, "avg_logprob": -0.16254056620801616, "compression_ratio": 1.570967741935484, "no_speech_prob": 0.015509805642068386}, {"id": 840, "seek": 482152, "start": 4827.84, "end": 4832.88, "text": " has great form for this, because not only does he talk about having powerful and useful interfaces", "tokens": [50680, 575, 869, 1254, 337, 341, 11, 570, 406, 787, 775, 415, 751, 466, 1419, 4005, 293, 4420, 28416, 50932], "temperature": 0.0, "avg_logprob": -0.16254056620801616, "compression_ratio": 1.570967741935484, "no_speech_prob": 0.015509805642068386}, {"id": 841, "seek": 482152, "start": 4832.88, "end": 4838.160000000001, "text": " and abstractions in deep learning, he's been playing this game in the library world for quite", "tokens": [50932, 293, 12649, 626, 294, 2452, 2539, 11, 415, 311, 668, 2433, 341, 1216, 294, 264, 6405, 1002, 337, 1596, 51196], "temperature": 0.0, "avg_logprob": -0.16254056620801616, "compression_ratio": 1.570967741935484, "no_speech_prob": 0.015509805642068386}, {"id": 842, "seek": 482152, "start": 4838.160000000001, "end": 4842.96, "text": " some time. But I wanted to touch on this quickly. We had a couple of people in our community asking", "tokens": [51196, 512, 565, 13, 583, 286, 1415, 281, 2557, 322, 341, 2661, 13, 492, 632, 257, 1916, 295, 561, 294, 527, 1768, 3365, 51436], "temperature": 0.0, "avg_logprob": -0.16254056620801616, "compression_ratio": 1.570967741935484, "no_speech_prob": 0.015509805642068386}, {"id": 843, "seek": 482152, "start": 4842.96, "end": 4850.56, "text": " you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned", "tokens": [51436, 291, 466, 591, 6985, 11, 767, 13, 400, 7977, 441, 933, 293, 28893, 3773, 8903, 848, 300, 7970, 440, 3730, 575, 8752, 51816], "temperature": 0.0, "avg_logprob": -0.16254056620801616, "compression_ratio": 1.570967741935484, "no_speech_prob": 0.015509805642068386}, {"id": 844, "seek": 485056, "start": 4850.56, "end": 4855.120000000001, "text": " with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end", "tokens": [50364, 365, 508, 2797, 293, 1783, 11435, 7223, 293, 415, 2738, 281, 458, 366, 456, 604, 5482, 281, 909, 309, 382, 257, 591, 6985, 646, 917, 50592], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 845, "seek": 485056, "start": 4855.120000000001, "end": 4860.0, "text": " and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end?", "tokens": [50592, 293, 7977, 441, 933, 611, 1619, 11, 291, 458, 11, 445, 508, 2797, 322, 1080, 1065, 13, 6068, 291, 909, 300, 382, 257, 646, 917, 30, 50836], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 846, "seek": 485056, "start": 4860.0, "end": 4863.200000000001, "text": " We've also had a couple of questions about PyTorch as well. Is there anything on the", "tokens": [50836, 492, 600, 611, 632, 257, 1916, 295, 1651, 466, 9953, 51, 284, 339, 382, 731, 13, 1119, 456, 1340, 322, 264, 50996], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 847, "seek": 485056, "start": 4863.200000000001, "end": 4867.92, "text": " roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the", "tokens": [50996, 35738, 337, 300, 30, 1033, 11, 370, 718, 311, 751, 466, 508, 2797, 13, 286, 519, 508, 2797, 307, 364, 3476, 1716, 293, 264, 51232], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 848, "seek": 485056, "start": 4867.92, "end": 4872.88, "text": " developers have really done a very, very interesting and very good job with it. And lots of people,", "tokens": [51232, 8849, 362, 534, 1096, 257, 588, 11, 588, 1880, 293, 588, 665, 1691, 365, 309, 13, 400, 3195, 295, 561, 11, 51480], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 849, "seek": 485056, "start": 4872.88, "end": 4878.240000000001, "text": " I like Jax actually. So that said, adoption is not super high. I think Google is probably the", "tokens": [51480, 286, 411, 508, 2797, 767, 13, 407, 300, 848, 11, 19215, 307, 406, 1687, 1090, 13, 286, 519, 3329, 307, 1391, 264, 51748], "temperature": 0.0, "avg_logprob": -0.09944933224347682, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.02077716588973999}, {"id": 850, "seek": 487824, "start": 4878.24, "end": 4882.88, "text": " company where it's the most adopted, where you will find the most users. And even then,", "tokens": [50364, 2237, 689, 309, 311, 264, 881, 12175, 11, 689, 291, 486, 915, 264, 881, 5022, 13, 400, 754, 550, 11, 50596], "temperature": 0.0, "avg_logprob": -0.14638229586043447, "compression_ratio": 1.6, "no_speech_prob": 0.002048001391813159}, {"id": 851, "seek": 487824, "start": 4882.88, "end": 4888.639999999999, "text": " it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project,", "tokens": [50596, 309, 311, 411, 257, 5870, 11, 5870, 11, 5870, 14135, 295, 3217, 3479, 14924, 412, 3329, 13, 583, 286, 519, 382, 257, 1716, 11, 50884], "temperature": 0.0, "avg_logprob": -0.14638229586043447, "compression_ratio": 1.6, "no_speech_prob": 0.002048001391813159}, {"id": 852, "seek": 487824, "start": 4888.639999999999, "end": 4895.04, "text": " it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add", "tokens": [50884, 309, 311, 257, 2238, 1716, 13, 467, 311, 21117, 13, 467, 311, 4005, 13, 467, 311, 869, 13, 407, 576, 286, 411, 281, 909, 51204], "temperature": 0.0, "avg_logprob": -0.14638229586043447, "compression_ratio": 1.6, "no_speech_prob": 0.002048001391813159}, {"id": 853, "seek": 487824, "start": 4895.04, "end": 4901.2, "text": " Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away", "tokens": [51204, 508, 2797, 646, 917, 281, 591, 6985, 420, 9953, 51, 284, 339, 646, 917, 281, 591, 6985, 30, 407, 286, 528, 281, 584, 321, 600, 534, 4259, 1314, 51512], "temperature": 0.0, "avg_logprob": -0.14638229586043447, "compression_ratio": 1.6, "no_speech_prob": 0.002048001391813159}, {"id": 854, "seek": 490120, "start": 4901.2, "end": 4909.679999999999, "text": " from this like interface back end kind of model. So precisely for the reason I was describing,", "tokens": [50364, 490, 341, 411, 9226, 646, 917, 733, 295, 2316, 13, 407, 13402, 337, 264, 1778, 286, 390, 16141, 11, 50788], "temperature": 0.0, "avg_logprob": -0.1588127037574505, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.2138834446668625}, {"id": 855, "seek": 490120, "start": 4909.679999999999, "end": 4913.679999999999, "text": " because you want to achieve this spectrum of workflows, with that, I think this cliff where", "tokens": [50788, 570, 291, 528, 281, 4584, 341, 11143, 295, 43461, 11, 365, 300, 11, 286, 519, 341, 22316, 689, 50988], "temperature": 0.0, "avg_logprob": -0.1588127037574505, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.2138834446668625}, {"id": 856, "seek": 490120, "start": 4913.679999999999, "end": 4919.28, "text": " you go, you fall from the high level down to the low level. We don't want the cliffs. We don't", "tokens": [50988, 291, 352, 11, 291, 2100, 490, 264, 1090, 1496, 760, 281, 264, 2295, 1496, 13, 492, 500, 380, 528, 264, 50039, 13, 492, 500, 380, 51268], "temperature": 0.0, "avg_logprob": -0.1588127037574505, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.2138834446668625}, {"id": 857, "seek": 490120, "start": 4919.28, "end": 4923.5199999999995, "text": " because cliffs create silos of users where you have the high level users. You want a gradient.", "tokens": [51268, 570, 50039, 1884, 48893, 295, 5022, 689, 291, 362, 264, 1090, 1496, 5022, 13, 509, 528, 257, 16235, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1588127037574505, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.2138834446668625}, {"id": 858, "seek": 490120, "start": 4923.5199999999995, "end": 4929.04, "text": " Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a", "tokens": [51480, 865, 11, 291, 528, 264, 16235, 13, 7587, 13, 407, 300, 848, 11, 286, 519, 309, 576, 312, 1687, 1627, 281, 362, 257, 51756], "temperature": 0.0, "avg_logprob": -0.1588127037574505, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.2138834446668625}, {"id": 859, "seek": 492904, "start": 4929.04, "end": 4935.04, "text": " sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening", "tokens": [50364, 1333, 295, 411, 319, 12, 332, 781, 19631, 295, 264, 591, 6985, 9362, 322, 1192, 295, 508, 2797, 300, 486, 611, 4584, 341, 17732, 50664], "temperature": 0.0, "avg_logprob": -0.12233984470367432, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.004726928658783436}, {"id": 860, "seek": 492904, "start": 4935.6, "end": 4939.92, "text": " and that will still follow the Keras API spec. It would still be the same thing,", "tokens": [50692, 293, 300, 486, 920, 1524, 264, 591, 6985, 9362, 1608, 13, 467, 576, 920, 312, 264, 912, 551, 11, 50908], "temperature": 0.0, "avg_logprob": -0.12233984470367432, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.004726928658783436}, {"id": 861, "seek": 492904, "start": 4941.6, "end": 4945.84, "text": " but on top of Jax. That said, so I would love to see something like this. This is also a very", "tokens": [50992, 457, 322, 1192, 295, 508, 2797, 13, 663, 848, 11, 370, 286, 576, 959, 281, 536, 746, 411, 341, 13, 639, 307, 611, 257, 588, 51204], "temperature": 0.0, "avg_logprob": -0.12233984470367432, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.004726928658783436}, {"id": 862, "seek": 492904, "start": 4945.84, "end": 4951.12, "text": " low priority for us because we have the actual current Keras, which I wish we need to work on,", "tokens": [51204, 2295, 9365, 337, 505, 570, 321, 362, 264, 3539, 2190, 591, 6985, 11, 597, 286, 3172, 321, 643, 281, 589, 322, 11, 51468], "temperature": 0.0, "avg_logprob": -0.12233984470367432, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.004726928658783436}, {"id": 863, "seek": 492904, "start": 4951.12, "end": 4955.84, "text": " which has lots of users. So we don't really have time to do this. But in theory, would it be cool?", "tokens": [51468, 597, 575, 3195, 295, 5022, 13, 407, 321, 500, 380, 534, 362, 565, 281, 360, 341, 13, 583, 294, 5261, 11, 576, 309, 312, 1627, 30, 51704], "temperature": 0.0, "avg_logprob": -0.12233984470367432, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.004726928658783436}, {"id": 864, "seek": 495584, "start": 4955.84, "end": 4960.400000000001, "text": " Yeah, sure. I would love to see something like this. So if I had tons of free time, I would", "tokens": [50364, 865, 11, 988, 13, 286, 576, 959, 281, 536, 746, 411, 341, 13, 407, 498, 286, 632, 9131, 295, 1737, 565, 11, 286, 576, 50592], "temperature": 0.0, "avg_logprob": -0.1113642095550289, "compression_ratio": 1.6366782006920415, "no_speech_prob": 0.008330250158905983}, {"id": 865, "seek": 495584, "start": 4960.400000000001, "end": 4963.76, "text": " probably build it, but in practice, I don't. Fantastic. Well, we've got another question", "tokens": [50592, 1391, 1322, 309, 11, 457, 294, 3124, 11, 286, 500, 380, 13, 21320, 13, 1042, 11, 321, 600, 658, 1071, 1168, 50760], "temperature": 0.0, "avg_logprob": -0.1113642095550289, "compression_ratio": 1.6366782006920415, "no_speech_prob": 0.008330250158905983}, {"id": 866, "seek": 495584, "start": 4963.76, "end": 4968.400000000001, "text": " from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth", "tokens": [50760, 490, 47089, 35832, 767, 13, 634, 1619, 11, 437, 775, 34695, 271, 519, 295, 2491, 13, 33735, 28329, 311, 1446, 322, 264, 9474, 50992], "temperature": 0.0, "avg_logprob": -0.1113642095550289, "compression_ratio": 1.6366782006920415, "no_speech_prob": 0.008330250158905983}, {"id": 867, "seek": 495584, "start": 4968.400000000001, "end": 4972.64, "text": " of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives", "tokens": [50992, 295, 264, 10024, 30, 2014, 291, 4963, 365, 33735, 28329, 311, 589, 466, 264, 41108, 11612, 295, 15961, 51204], "temperature": 0.0, "avg_logprob": -0.1113642095550289, "compression_ratio": 1.6366782006920415, "no_speech_prob": 0.008330250158905983}, {"id": 868, "seek": 495584, "start": 4972.64, "end": 4980.400000000001, "text": " and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book.", "tokens": [51204, 293, 1269, 12, 3502, 1287, 30, 407, 286, 478, 13501, 48863, 4963, 365, 264, 1315, 13, 286, 478, 406, 534, 4963, 365, 264, 1446, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1113642095550289, "compression_ratio": 1.6366782006920415, "no_speech_prob": 0.008330250158905983}, {"id": 869, "seek": 498040, "start": 4981.12, "end": 4986.5599999999995, "text": " Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me.", "tokens": [50400, 876, 11, 1392, 13, 1042, 11, 2597, 11, 406, 281, 3292, 11, 457, 309, 311, 33735, 575, 668, 257, 2603, 10249, 337, 385, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14777449874190596, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.008199397474527359}, {"id": 870, "seek": 498040, "start": 4986.5599999999995, "end": 4994.5599999999995, "text": " And he talks a lot about objectives leading to deception. So sometimes following an objective", "tokens": [50672, 400, 415, 6686, 257, 688, 466, 15961, 5775, 281, 40451, 13, 407, 2171, 3480, 364, 10024, 51072], "temperature": 0.0, "avg_logprob": -0.14777449874190596, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.008199397474527359}, {"id": 871, "seek": 498040, "start": 4994.5599999999995, "end": 4999.5199999999995, "text": " monotonically sends you in the wrong direction. And his solution to that is either quality,", "tokens": [51072, 1108, 27794, 984, 14790, 291, 294, 264, 2085, 3513, 13, 400, 702, 3827, 281, 300, 307, 2139, 3125, 11, 51320], "temperature": 0.0, "avg_logprob": -0.14777449874190596, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.008199397474527359}, {"id": 872, "seek": 498040, "start": 4999.5199999999995, "end": 5004.0, "text": " diversity, or more recently, open-endedness, which is that if you have an infinitude of", "tokens": [51320, 8811, 11, 420, 544, 3938, 11, 1269, 12, 3502, 1287, 11, 597, 307, 300, 498, 291, 362, 364, 7193, 4377, 295, 51544], "temperature": 0.0, "avg_logprob": -0.14777449874190596, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.008199397474527359}, {"id": 873, "seek": 498040, "start": 5004.0, "end": 5009.28, "text": " objectives, in a sense, the system has no objective. And you can also with diversity,", "tokens": [51544, 15961, 11, 294, 257, 2020, 11, 264, 1185, 575, 572, 10024, 13, 400, 291, 393, 611, 365, 8811, 11, 51808], "temperature": 0.0, "avg_logprob": -0.14777449874190596, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.008199397474527359}, {"id": 874, "seek": 500928, "start": 5009.28, "end": 5013.5199999999995, "text": " preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the", "tokens": [50364, 27257, 11, 291, 393, 10473, 368, 1336, 488, 3164, 7673, 13, 583, 1338, 11, 291, 1062, 362, 2198, 295, 264, 50576], "temperature": 0.0, "avg_logprob": -0.1443206754505125, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.005690739024430513}, {"id": 875, "seek": 500928, "start": 5013.5199999999995, "end": 5018.48, "text": " poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes", "tokens": [50576, 20874, 9284, 11, 597, 415, 390, 3288, 294, 13, 865, 11, 3122, 13, 883, 11, 286, 478, 3650, 13, 400, 370, 562, 309, 1487, 50824], "temperature": 0.0, "avg_logprob": -0.1443206754505125, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.005690739024430513}, {"id": 876, "seek": 500928, "start": 5018.48, "end": 5024.719999999999, "text": " to your description of the problem's objectives, I completely agree that one thing I mentioned", "tokens": [50824, 281, 428, 3855, 295, 264, 1154, 311, 15961, 11, 286, 2584, 3986, 300, 472, 551, 286, 2835, 51136], "temperature": 0.0, "avg_logprob": -0.1443206754505125, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.005690739024430513}, {"id": 877, "seek": 500928, "start": 5024.719999999999, "end": 5031.28, "text": " in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one", "tokens": [51136, 294, 264, 3035, 11, 309, 311, 411, 264, 24822, 4978, 11, 597, 307, 300, 498, 291, 853, 281, 4584, 472, 551, 11, 472, 51464], "temperature": 0.0, "avg_logprob": -0.1443206754505125, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.005690739024430513}, {"id": 878, "seek": 500928, "start": 5031.28, "end": 5035.679999999999, "text": " objective, you're going to achieve it. But the thing is, you're going to take every shortcut", "tokens": [51464, 10024, 11, 291, 434, 516, 281, 4584, 309, 13, 583, 264, 551, 307, 11, 291, 434, 516, 281, 747, 633, 24822, 51684], "temperature": 0.0, "avg_logprob": -0.1443206754505125, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.005690739024430513}, {"id": 879, "seek": 503568, "start": 5035.68, "end": 5039.92, "text": " along the way for things that we are not actually incorporated in your objective.", "tokens": [50364, 2051, 264, 636, 337, 721, 300, 321, 366, 406, 767, 21654, 294, 428, 10024, 13, 50576], "temperature": 0.0, "avg_logprob": -0.12409960895503333, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.02465018443763256}, {"id": 880, "seek": 503568, "start": 5039.92, "end": 5044.56, "text": " And this leads to systems that are not actually doing what you wanted them to do. Like for instance,", "tokens": [50576, 400, 341, 6689, 281, 3652, 300, 366, 406, 767, 884, 437, 291, 1415, 552, 281, 360, 13, 1743, 337, 5197, 11, 50808], "temperature": 0.0, "avg_logprob": -0.12409960895503333, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.02465018443763256}, {"id": 881, "seek": 503568, "start": 5044.56, "end": 5050.88, "text": " we built chess playing systems, because we hoped that a system that could play chess would have to", "tokens": [50808, 321, 3094, 24122, 2433, 3652, 11, 570, 321, 19737, 300, 257, 1185, 300, 727, 862, 24122, 576, 362, 281, 51124], "temperature": 0.0, "avg_logprob": -0.12409960895503333, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.02465018443763256}, {"id": 882, "seek": 503568, "start": 5050.88, "end": 5056.64, "text": " be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays", "tokens": [51124, 312, 1075, 281, 4111, 21577, 11, 1446, 2539, 11, 12915, 11, 293, 370, 322, 13, 29524, 484, 309, 445, 5749, 51412], "temperature": 0.0, "avg_logprob": -0.12409960895503333, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.02465018443763256}, {"id": 883, "seek": 503568, "start": 5056.64, "end": 5063.200000000001, "text": " chess. That's what it does. The same is true with challenges and Kaggle. The winning systems,", "tokens": [51412, 24122, 13, 663, 311, 437, 309, 775, 13, 440, 912, 307, 2074, 365, 4759, 293, 48751, 22631, 13, 440, 8224, 3652, 11, 51740], "temperature": 0.0, "avg_logprob": -0.12409960895503333, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.02465018443763256}, {"id": 884, "seek": 506320, "start": 5063.2, "end": 5068.16, "text": " they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the", "tokens": [50364, 436, 445, 19719, 337, 264, 5263, 3787, 17833, 293, 436, 4584, 309, 13, 583, 436, 4584, 309, 412, 264, 50612], "temperature": 0.0, "avg_logprob": -0.1120697577794393, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.016068097203969955}, {"id": 885, "seek": 506320, "start": 5068.16, "end": 5074.32, "text": " expense of everything else that you might care about the system. Like, is the code base readable?", "tokens": [50612, 18406, 295, 1203, 1646, 300, 291, 1062, 1127, 466, 264, 1185, 13, 1743, 11, 307, 264, 3089, 3096, 49857, 30, 50920], "temperature": 0.0, "avg_logprob": -0.1120697577794393, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.016068097203969955}, {"id": 886, "seek": 506320, "start": 5074.32, "end": 5079.5199999999995, "text": " No. Is it computationally efficient? No, it's actually terrible. You could never put it in", "tokens": [50920, 883, 13, 1119, 309, 24903, 379, 7148, 30, 883, 11, 309, 311, 767, 6237, 13, 509, 727, 1128, 829, 309, 294, 51180], "temperature": 0.0, "avg_logprob": -0.1120697577794393, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.016068097203969955}, {"id": 887, "seek": 506320, "start": 5079.5199999999995, "end": 5084.08, "text": " production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for", "tokens": [51180, 4265, 13, 1119, 309, 2903, 712, 30, 883, 11, 293, 370, 322, 13, 865, 11, 370, 309, 311, 411, 11, 498, 291, 498, 291, 19719, 337, 51408], "temperature": 0.0, "avg_logprob": -0.1120697577794393, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.016068097203969955}, {"id": 888, "seek": 506320, "start": 5084.08, "end": 5089.28, "text": " something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth", "tokens": [51408, 746, 11, 291, 483, 309, 11, 457, 291, 747, 34620, 13, 865, 11, 2293, 13, 400, 300, 311, 588, 709, 437, 33735, 51668], "temperature": 0.0, "avg_logprob": -0.1120697577794393, "compression_ratio": 1.6978417266187051, "no_speech_prob": 0.016068097203969955}, {"id": 889, "seek": 508928, "start": 5089.28, "end": 5093.44, "text": " says as well. I love what you said about shortcuts. You said in your New York's presentation that if", "tokens": [50364, 1619, 382, 731, 13, 286, 959, 437, 291, 848, 466, 34620, 13, 509, 848, 294, 428, 1873, 3609, 311, 5860, 300, 498, 50572], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 890, "seek": 508928, "start": 5093.44, "end": 5098.32, "text": " you optimize for a specific metric, then you'll take shortcuts on every other dimension, not", "tokens": [50572, 291, 19719, 337, 257, 2685, 20678, 11, 550, 291, 603, 747, 34620, 322, 633, 661, 10139, 11, 406, 50816], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 891, "seek": 508928, "start": 5098.32, "end": 5102.8, "text": " captured by your metric. And you said in a machine learning context, it's similar to overfitting,", "tokens": [50816, 11828, 538, 428, 20678, 13, 400, 291, 848, 294, 257, 3479, 2539, 4319, 11, 309, 311, 2531, 281, 670, 69, 2414, 11, 51040], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 892, "seek": 508928, "start": 5102.8, "end": 5107.759999999999, "text": " right? Because on task specific skills, you actually lose generalization if you get good at", "tokens": [51040, 558, 30, 1436, 322, 5633, 2685, 3942, 11, 291, 767, 3624, 2674, 2144, 498, 291, 483, 665, 412, 51288], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 893, "seek": 508928, "start": 5107.759999999999, "end": 5112.08, "text": " a particular task. So it's completely orthogonal to what you want. I know you're very well known", "tokens": [51288, 257, 1729, 5633, 13, 407, 309, 311, 2584, 41488, 281, 437, 291, 528, 13, 286, 458, 291, 434, 588, 731, 2570, 51504], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 894, "seek": 508928, "start": 5112.08, "end": 5116.639999999999, "text": " for your skepticism of the intelligence explosion. And what I love about your conception of", "tokens": [51504, 337, 428, 19128, 26356, 295, 264, 7599, 15673, 13, 400, 437, 286, 959, 466, 428, 30698, 295, 51732], "temperature": 0.0, "avg_logprob": -0.07311669262972745, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.08341247588396072}, {"id": 895, "seek": 511664, "start": 5116.64, "end": 5121.84, "text": " intelligence is that you think of it as a system or as a process, you say that intelligence is", "tokens": [50364, 7599, 307, 300, 291, 519, 295, 309, 382, 257, 1185, 420, 382, 257, 1399, 11, 291, 584, 300, 7599, 307, 50624], "temperature": 0.0, "avg_logprob": -0.07515621185302734, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0059568751603364944}, {"id": 896, "seek": 511664, "start": 5121.84, "end": 5128.240000000001, "text": " embodied, right? So you have a brain in a body acting in an environment. And in that context,", "tokens": [50624, 42046, 11, 558, 30, 407, 291, 362, 257, 3567, 294, 257, 1772, 6577, 294, 364, 2823, 13, 400, 294, 300, 4319, 11, 50944], "temperature": 0.0, "avg_logprob": -0.07515621185302734, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0059568751603364944}, {"id": 897, "seek": 511664, "start": 5128.240000000001, "end": 5132.88, "text": " it makes sense that you would think that there are environmental kind of rate limiting steps to", "tokens": [50944, 309, 1669, 2020, 300, 291, 576, 519, 300, 456, 366, 8303, 733, 295, 3314, 22083, 4439, 281, 51176], "temperature": 0.0, "avg_logprob": -0.07515621185302734, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0059568751603364944}, {"id": 898, "seek": 511664, "start": 5132.88, "end": 5137.92, "text": " any kind of super intelligence, right? But I spoke to someone the other day who is of the other", "tokens": [51176, 604, 733, 295, 1687, 7599, 11, 558, 30, 583, 286, 7179, 281, 1580, 264, 661, 786, 567, 307, 295, 264, 661, 51428], "temperature": 0.0, "avg_logprob": -0.07515621185302734, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0059568751603364944}, {"id": 899, "seek": 511664, "start": 5137.92, "end": 5142.88, "text": " persuasion, shall we say, and this person was saying, Well, what if you had a super, super", "tokens": [51428, 16336, 6822, 11, 4393, 321, 584, 11, 293, 341, 954, 390, 1566, 11, 1042, 11, 437, 498, 291, 632, 257, 1687, 11, 1687, 51676], "temperature": 0.0, "avg_logprob": -0.07515621185302734, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0059568751603364944}, {"id": 900, "seek": 514288, "start": 5142.88, "end": 5149.12, "text": " smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a", "tokens": [50364, 4069, 3840, 295, 7708, 30, 286, 458, 291, 848, 294, 428, 319, 5955, 32831, 300, 498, 291, 574, 412, 264, 28921, 295, 257, 50676], "temperature": 0.0, "avg_logprob": -0.10719185906487542, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.06576158851385117}, {"id": 901, "seek": 514288, "start": 5149.12, "end": 5155.6, "text": " scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out", "tokens": [50676, 12662, 567, 307, 9809, 46530, 77, 1601, 11, 337, 1365, 11, 264, 912, 28921, 382, 257, 45415, 12662, 11, 4523, 484, 51000], "temperature": 0.0, "avg_logprob": -0.10719185906487542, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.06576158851385117}, {"id": 902, "seek": 514288, "start": 5155.6, "end": 5161.04, "text": " that IQ only helps up to about 125. And then it stops helping you. But these people would say,", "tokens": [51000, 300, 28921, 787, 3665, 493, 281, 466, 25276, 13, 400, 550, 309, 10094, 4315, 291, 13, 583, 613, 561, 576, 584, 11, 51272], "temperature": 0.0, "avg_logprob": -0.10719185906487542, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.06576158851385117}, {"id": 903, "seek": 514288, "start": 5161.04, "end": 5165.28, "text": " Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is", "tokens": [51272, 876, 11, 731, 11, 291, 458, 11, 437, 498, 437, 498, 633, 2167, 12662, 390, 364, 23486, 293, 7599, 307, 51484], "temperature": 0.0, "avg_logprob": -0.10719185906487542, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.06576158851385117}, {"id": 904, "seek": 514288, "start": 5165.28, "end": 5169.36, "text": " just making better decisions, they would consistently make better decisions and science", "tokens": [51484, 445, 1455, 1101, 5327, 11, 436, 576, 14961, 652, 1101, 5327, 293, 3497, 51688], "temperature": 0.0, "avg_logprob": -0.10719185906487542, "compression_ratio": 1.6836363636363636, "no_speech_prob": 0.06576158851385117}, {"id": 905, "seek": 516936, "start": 5169.36, "end": 5174.719999999999, "text": " would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a", "tokens": [50364, 576, 21341, 13, 316, 417, 8814, 1177, 380, 1223, 577, 665, 257, 1952, 307, 13, 407, 577, 576, 321, 1223, 437, 257, 50632], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 906, "seek": 516936, "start": 5174.719999999999, "end": 5178.48, "text": " super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into", "tokens": [50632, 1687, 13232, 954, 576, 360, 30, 509, 458, 11, 436, 1116, 7962, 14067, 1370, 339, 11, 436, 1116, 6580, 2969, 666, 50820], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 907, "seek": 516936, "start": 5178.48, "end": 5182.96, "text": " the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you", "tokens": [50820, 264, 8141, 11, 436, 1116, 360, 439, 295, 341, 1507, 11, 293, 6063, 436, 576, 30686, 25038, 10473, 13, 1144, 291, 51044], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 908, "seek": 516936, "start": 5182.96, "end": 5187.44, "text": " know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent", "tokens": [51044, 458, 437, 286, 914, 30, 1012, 576, 291, 4196, 281, 300, 30, 865, 11, 498, 633, 12662, 390, 1687, 13232, 51268], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 909, "seek": 516936, "start": 5187.44, "end": 5192.4, "text": " in human terms, that would in fact accelerate science. But it would not really like accelerate", "tokens": [51268, 294, 1952, 2115, 11, 300, 576, 294, 1186, 21341, 3497, 13, 583, 309, 576, 406, 534, 411, 21341, 51516], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 910, "seek": 516936, "start": 5192.4, "end": 5199.2, "text": " science in a linear fashion and very much not in an exponential fashion. So I guess the main", "tokens": [51516, 3497, 294, 257, 8213, 6700, 293, 588, 709, 406, 294, 364, 21510, 6700, 13, 407, 286, 2041, 264, 2135, 51856], "temperature": 0.0, "avg_logprob": -0.11811494115573257, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.005230488255620003}, {"id": 911, "seek": 519936, "start": 5199.36, "end": 5206.08, "text": " conceptual differences I have with these folks is that they tend to credit everything humans can do", "tokens": [50364, 24106, 7300, 286, 362, 365, 613, 4024, 307, 300, 436, 3928, 281, 5397, 1203, 6255, 393, 360, 50700], "temperature": 0.0, "avg_logprob": -0.10192045159296158, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0069994390942156315}, {"id": 912, "seek": 519936, "start": 5206.08, "end": 5212.0, "text": " to the human brain. And they have this vision of intelligence as you know, a brain in a jar", "tokens": [50700, 281, 264, 1952, 3567, 13, 400, 436, 362, 341, 5201, 295, 7599, 382, 291, 458, 11, 257, 3567, 294, 257, 15181, 50996], "temperature": 0.0, "avg_logprob": -0.10192045159296158, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0069994390942156315}, {"id": 913, "seek": 519936, "start": 5212.0, "end": 5215.92, "text": " kind of thing. And if you tweak the brain, it gets more intelligent and intelligence", "tokens": [50996, 733, 295, 551, 13, 400, 498, 291, 29879, 264, 3567, 11, 309, 2170, 544, 13232, 293, 7599, 51192], "temperature": 0.0, "avg_logprob": -0.10192045159296158, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0069994390942156315}, {"id": 914, "seek": 519936, "start": 5216.5599999999995, "end": 5222.08, "text": " is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can", "tokens": [51224, 307, 3838, 12675, 382, 1347, 13, 759, 291, 434, 544, 13232, 11, 498, 291, 362, 257, 22333, 11, 291, 393, 51500], "temperature": 0.0, "avg_logprob": -0.10192045159296158, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0069994390942156315}, {"id": 915, "seek": 519936, "start": 5222.08, "end": 5227.04, "text": " do more things, you can solve more problems and so on. And in particular, you can build a better", "tokens": [51500, 360, 544, 721, 11, 291, 393, 5039, 544, 2740, 293, 370, 322, 13, 400, 294, 1729, 11, 291, 393, 1322, 257, 1101, 51748], "temperature": 0.0, "avg_logprob": -0.10192045159296158, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0069994390942156315}, {"id": 916, "seek": 522704, "start": 5227.12, "end": 5232.56, "text": " brain. And by the way, there is not really any practical evidence that this is true. But", "tokens": [50368, 3567, 13, 400, 538, 264, 636, 11, 456, 307, 406, 534, 604, 8496, 4467, 300, 341, 307, 2074, 13, 583, 50640], "temperature": 0.0, "avg_logprob": -0.18480362362331815, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.002178272930905223}, {"id": 917, "seek": 522704, "start": 5232.56, "end": 5238.16, "text": " I view intelligence here more as this holistic thing that okay, you have the brain, but actually", "tokens": [50640, 286, 1910, 7599, 510, 544, 382, 341, 30334, 551, 300, 1392, 11, 291, 362, 264, 3567, 11, 457, 767, 50920], "temperature": 0.0, "avg_logprob": -0.18480362362331815, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.002178272930905223}, {"id": 918, "seek": 522704, "start": 5238.16, "end": 5244.64, "text": " the brain is in a body which gives it access to a certain set of actions it can do and set", "tokens": [50920, 264, 3567, 307, 294, 257, 1772, 597, 2709, 309, 2105, 281, 257, 1629, 992, 295, 5909, 309, 393, 360, 293, 992, 51244], "temperature": 0.0, "avg_logprob": -0.18480362362331815, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.002178272930905223}, {"id": 919, "seek": 522704, "start": 5244.64, "end": 5250.32, "text": " up a perception primitives. And this body is an environment which gives it access to a set of", "tokens": [51244, 493, 257, 12860, 2886, 38970, 13, 400, 341, 1772, 307, 364, 2823, 597, 2709, 309, 2105, 281, 257, 992, 295, 51528], "temperature": 0.0, "avg_logprob": -0.18480362362331815, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.002178272930905223}, {"id": 920, "seek": 525032, "start": 5250.32, "end": 5258.16, "text": " experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just,", "tokens": [50364, 5235, 11, 257, 992, 295, 2740, 309, 393, 5039, 13, 400, 281, 257, 588, 2416, 8396, 11, 291, 458, 11, 264, 3567, 307, 445, 11, 50756], "temperature": 0.0, "avg_logprob": -0.21382709216046078, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0020156456157565117}, {"id": 921, "seek": 525032, "start": 5258.16, "end": 5264.4, "text": " it's not so much a problem solving algorithm, like a problem center descending, as it is a", "tokens": [50756, 309, 311, 406, 370, 709, 257, 1154, 12606, 9284, 11, 411, 257, 1154, 3056, 40182, 11, 382, 309, 307, 257, 51068], "temperature": 0.0, "avg_logprob": -0.21382709216046078, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0020156456157565117}, {"id": 922, "seek": 525032, "start": 5264.4, "end": 5270.08, "text": " big sport. And you put it in an environment to absorb experiences from that environment. And", "tokens": [51068, 955, 7282, 13, 400, 291, 829, 309, 294, 364, 2823, 281, 15631, 5235, 490, 300, 2823, 13, 400, 51352], "temperature": 0.0, "avg_logprob": -0.21382709216046078, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0020156456157565117}, {"id": 923, "seek": 525032, "start": 5271.28, "end": 5276.0, "text": " one thing that's super important to understand if you're on issue, if you really think deeply about", "tokens": [51412, 472, 551, 300, 311, 1687, 1021, 281, 1223, 498, 291, 434, 322, 2734, 11, 498, 291, 534, 519, 8760, 466, 51648], "temperature": 0.0, "avg_logprob": -0.21382709216046078, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.0020156456157565117}, {"id": 924, "seek": 527600, "start": 5276.08, "end": 5282.88, "text": " intelligence, is that most of our expressed intelligence does not come from here, it is", "tokens": [50368, 7599, 11, 307, 300, 881, 295, 527, 12675, 7599, 775, 406, 808, 490, 510, 11, 309, 307, 50708], "temperature": 0.0, "avg_logprob": -0.16318086215427943, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.0031678909435868263}, {"id": 925, "seek": 527600, "start": 5282.88, "end": 5286.8, "text": " externalized intelligence. So externalized intelligence can be can be many things.", "tokens": [50708, 8320, 1602, 7599, 13, 407, 8320, 1602, 7599, 393, 312, 393, 312, 867, 721, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16318086215427943, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.0031678909435868263}, {"id": 926, "seek": 527600, "start": 5288.48, "end": 5294.4, "text": " If I look up something online, that's externalized intelligence, Google is part of my brain. If I", "tokens": [50988, 759, 286, 574, 493, 746, 2950, 11, 300, 311, 8320, 1602, 7599, 11, 3329, 307, 644, 295, 452, 3567, 13, 759, 286, 51284], "temperature": 0.0, "avg_logprob": -0.16318086215427943, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.0031678909435868263}, {"id": 927, "seek": 527600, "start": 5294.4, "end": 5300.24, "text": " write a Python script to test some idea, that's externalized intelligence, my laptop is part of", "tokens": [51284, 2464, 257, 15329, 5755, 281, 1500, 512, 1558, 11, 300, 311, 8320, 1602, 7599, 11, 452, 10732, 307, 644, 295, 51576], "temperature": 0.0, "avg_logprob": -0.16318086215427943, "compression_ratio": 1.956989247311828, "no_speech_prob": 0.0031678909435868263}, {"id": 928, "seek": 530024, "start": 5300.24, "end": 5307.599999999999, "text": " my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition", "tokens": [50364, 452, 46905, 11, 293, 370, 322, 13, 583, 309, 311, 767, 11, 309, 1709, 709, 3052, 813, 300, 13, 4534, 295, 527, 46905, 50732], "temperature": 0.0, "avg_logprob": -0.14698814278218283, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0096907839179039}, {"id": 929, "seek": 530024, "start": 5307.599999999999, "end": 5315.36, "text": " is crystallized, the crystallized output of someone, someone else's thinking. And the process", "tokens": [50732, 307, 31924, 1602, 11, 264, 31924, 1602, 5598, 295, 1580, 11, 1580, 1646, 311, 1953, 13, 400, 264, 1399, 51120], "temperature": 0.0, "avg_logprob": -0.14698814278218283, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0096907839179039}, {"id": 930, "seek": 530024, "start": 5315.36, "end": 5320.88, "text": " through which we get access to all these accumulated outputs of people's thinking is civilization,", "tokens": [51120, 807, 597, 321, 483, 2105, 281, 439, 613, 31346, 23930, 295, 561, 311, 1953, 307, 18036, 11, 51396], "temperature": 0.0, "avg_logprob": -0.14698814278218283, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0096907839179039}, {"id": 931, "seek": 532088, "start": 5320.88, "end": 5330.72, "text": " right? And like 99% of the things you think are the behaviors you act, the behaviors you execute,", "tokens": [50364, 558, 30, 400, 411, 11803, 4, 295, 264, 721, 291, 519, 366, 264, 15501, 291, 605, 11, 264, 15501, 291, 14483, 11, 50856], "temperature": 0.0, "avg_logprob": -0.15952874223391214, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.02328481897711754}, {"id": 932, "seek": 532088, "start": 5330.72, "end": 5337.84, "text": " you did not invent them. You did not solve the underlying problem yourself. You're just copying", "tokens": [50856, 291, 630, 406, 7962, 552, 13, 509, 630, 406, 5039, 264, 14217, 1154, 1803, 13, 509, 434, 445, 27976, 51212], "temperature": 0.0, "avg_logprob": -0.15952874223391214, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.02328481897711754}, {"id": 933, "seek": 532088, "start": 5338.88, "end": 5344.400000000001, "text": " a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands", "tokens": [51264, 257, 3827, 13, 509, 600, 1612, 411, 11, 321, 434, 294, 264, 2808, 295, 257, 5388, 11, 291, 434, 1391, 13836, 428, 2377, 51540], "temperature": 0.0, "avg_logprob": -0.15952874223391214, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.02328481897711754}, {"id": 934, "seek": 532088, "start": 5344.400000000001, "end": 5349.84, "text": " after you went outside. And that's a very smart behavior. But did you invent it? Did you come", "tokens": [51540, 934, 291, 1437, 2380, 13, 400, 300, 311, 257, 588, 4069, 5223, 13, 583, 630, 291, 7962, 309, 30, 2589, 291, 808, 51812], "temperature": 0.0, "avg_logprob": -0.15952874223391214, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.02328481897711754}, {"id": 935, "seek": 534984, "start": 5349.84, "end": 5355.68, "text": " up with that? No, actually, other people came up with that. You did not also come up with the", "tokens": [50364, 493, 365, 300, 30, 883, 11, 767, 11, 661, 561, 1361, 493, 365, 300, 13, 509, 630, 406, 611, 808, 493, 365, 264, 50656], "temperature": 0.0, "avg_logprob": -0.07320611351414731, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.004730841610580683}, {"id": 936, "seek": 534984, "start": 5355.68, "end": 5360.4800000000005, "text": " infrastructure that enables you to do it in the first place. And so, and this is true, you know,", "tokens": [50656, 6896, 300, 17077, 291, 281, 360, 309, 294, 264, 700, 1081, 13, 400, 370, 11, 293, 341, 307, 2074, 11, 291, 458, 11, 50896], "temperature": 0.0, "avg_logprob": -0.07320611351414731, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.004730841610580683}, {"id": 937, "seek": 534984, "start": 5360.4800000000005, "end": 5367.92, "text": " for even the most intimate of your thoughts, you're thinking with words that you did not invent,", "tokens": [50896, 337, 754, 264, 881, 20215, 295, 428, 4598, 11, 291, 434, 1953, 365, 2283, 300, 291, 630, 406, 7962, 11, 51268], "temperature": 0.0, "avg_logprob": -0.07320611351414731, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.004730841610580683}, {"id": 938, "seek": 534984, "start": 5367.92, "end": 5374.400000000001, "text": " you're thinking with concepts that you did not invent or that you did not derive from your own", "tokens": [51268, 291, 434, 1953, 365, 10392, 300, 291, 630, 406, 7962, 420, 300, 291, 630, 406, 28446, 490, 428, 1065, 51592], "temperature": 0.0, "avg_logprob": -0.07320611351414731, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.004730841610580683}, {"id": 939, "seek": 537440, "start": 5374.48, "end": 5381.5199999999995, "text": " experience. They really come from other people, from this accumulation of past generations.", "tokens": [50368, 1752, 13, 814, 534, 808, 490, 661, 561, 11, 490, 341, 35647, 295, 1791, 10593, 13, 50720], "temperature": 0.0, "avg_logprob": -0.09750876194093286, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.011495986953377724}, {"id": 940, "seek": 537440, "start": 5381.5199999999995, "end": 5389.04, "text": " And if you want to enhance the expressed intelligence of people, then this is actually the", "tokens": [50720, 400, 498, 291, 528, 281, 11985, 264, 12675, 7599, 295, 561, 11, 550, 341, 307, 767, 264, 51096], "temperature": 0.0, "avg_logprob": -0.09750876194093286, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.011495986953377724}, {"id": 941, "seek": 537440, "start": 5389.04, "end": 5394.879999999999, "text": " system you need to tweak and improve, not the human brain, but civilization, right?", "tokens": [51096, 1185, 291, 643, 281, 29879, 293, 3470, 11, 406, 264, 1952, 3567, 11, 457, 18036, 11, 558, 30, 51388], "temperature": 0.0, "avg_logprob": -0.09750876194093286, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.011495986953377724}, {"id": 942, "seek": 537440, "start": 5394.879999999999, "end": 5400.879999999999, "text": " In a way, that seems like a contradiction, because you're talking about the externalization of knowledge,", "tokens": [51388, 682, 257, 636, 11, 300, 2544, 411, 257, 34937, 11, 570, 291, 434, 1417, 466, 264, 8320, 2144, 295, 3601, 11, 51688], "temperature": 0.0, "avg_logprob": -0.09750876194093286, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.011495986953377724}, {"id": 943, "seek": 540088, "start": 5401.4400000000005, "end": 5407.76, "text": " not intelligence. So by your own definition, isn't that the opposite of intelligence?", "tokens": [50392, 406, 7599, 13, 407, 538, 428, 1065, 7123, 11, 1943, 380, 300, 264, 6182, 295, 7599, 30, 50708], "temperature": 0.0, "avg_logprob": -0.10803231885356288, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.00528146093711257}, {"id": 944, "seek": 540088, "start": 5408.400000000001, "end": 5414.0, "text": " That's a great point. So I'm relating expressed intelligence. So I was specifically saying", "tokens": [50740, 663, 311, 257, 869, 935, 13, 407, 286, 478, 23968, 12675, 7599, 13, 407, 286, 390, 4682, 1566, 51020], "temperature": 0.0, "avg_logprob": -0.10803231885356288, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.00528146093711257}, {"id": 945, "seek": 540088, "start": 5414.0, "end": 5418.0, "text": " expressed intelligence as opposed to fluid intelligence. And what expressed intelligence", "tokens": [51020, 12675, 7599, 382, 8851, 281, 9113, 7599, 13, 400, 437, 12675, 7599, 51220], "temperature": 0.0, "avg_logprob": -0.10803231885356288, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.00528146093711257}, {"id": 946, "seek": 540088, "start": 5418.72, "end": 5422.72, "text": " means in this context is something very different from what we talk about in the measure of", "tokens": [51256, 1355, 294, 341, 4319, 307, 746, 588, 819, 490, 437, 321, 751, 466, 294, 264, 3481, 295, 51456], "temperature": 0.0, "avg_logprob": -0.10803231885356288, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.00528146093711257}, {"id": 947, "seek": 540088, "start": 5422.72, "end": 5428.64, "text": " intelligence. It means intelligence behavior. And in particular, I think the ability to solve", "tokens": [51456, 7599, 13, 467, 1355, 7599, 5223, 13, 400, 294, 1729, 11, 286, 519, 264, 3485, 281, 5039, 51752], "temperature": 0.0, "avg_logprob": -0.10803231885356288, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.00528146093711257}, {"id": 948, "seek": 542864, "start": 5428.72, "end": 5433.12, "text": " problems that you encounter as an individual. Typically, when you solve a problem as an", "tokens": [50368, 2740, 300, 291, 8593, 382, 364, 2609, 13, 23129, 11, 562, 291, 5039, 257, 1154, 382, 364, 50588], "temperature": 0.0, "avg_logprob": -0.10717476224436344, "compression_ratio": 1.9246861924686192, "no_speech_prob": 0.009405197575688362}, {"id": 949, "seek": 542864, "start": 5433.12, "end": 5438.64, "text": " individual, you're actually using a solution you found somewhere else. There are not that many", "tokens": [50588, 2609, 11, 291, 434, 767, 1228, 257, 3827, 291, 1352, 4079, 1646, 13, 821, 366, 406, 300, 867, 50864], "temperature": 0.0, "avg_logprob": -0.10717476224436344, "compression_ratio": 1.9246861924686192, "no_speech_prob": 0.009405197575688362}, {"id": 950, "seek": 542864, "start": 5438.64, "end": 5445.04, "text": " problems that as an individual, you solve from scratch in your own lifetime. But here's the", "tokens": [50864, 2740, 300, 382, 364, 2609, 11, 291, 5039, 490, 8459, 294, 428, 1065, 11364, 13, 583, 510, 311, 264, 51184], "temperature": 0.0, "avg_logprob": -0.10717476224436344, "compression_ratio": 1.9246861924686192, "no_speech_prob": 0.009405197575688362}, {"id": 951, "seek": 542864, "start": 5445.04, "end": 5450.400000000001, "text": " thing is that if you're able to actually solve something novel yourself, you have the ability", "tokens": [51184, 551, 307, 300, 498, 291, 434, 1075, 281, 767, 5039, 746, 7613, 1803, 11, 291, 362, 264, 3485, 51452], "temperature": 0.0, "avg_logprob": -0.10717476224436344, "compression_ratio": 1.9246861924686192, "no_speech_prob": 0.009405197575688362}, {"id": 952, "seek": 542864, "start": 5450.400000000001, "end": 5455.12, "text": " to write about it, you have the ability to communicate it, and then the next generation can", "tokens": [51452, 281, 2464, 466, 309, 11, 291, 362, 264, 3485, 281, 7890, 309, 11, 293, 550, 264, 958, 5125, 393, 51688], "temperature": 0.0, "avg_logprob": -0.10717476224436344, "compression_ratio": 1.9246861924686192, "no_speech_prob": 0.009405197575688362}, {"id": 953, "seek": 545512, "start": 5455.12, "end": 5462.5599999999995, "text": " benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're", "tokens": [50364, 5121, 490, 309, 13, 407, 718, 385, 445, 10774, 257, 733, 295, 257, 5682, 6770, 281, 341, 13, 407, 7297, 291, 434, 50736], "temperature": 0.0, "avg_logprob": -0.13044127238165473, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0033757199998944998}, {"id": 954, "seek": 545512, "start": 5462.5599999999995, "end": 5468.16, "text": " reading a novel about, I don't know, a kind of planet of the apes or something, which was a", "tokens": [50736, 3760, 257, 7613, 466, 11, 286, 500, 380, 458, 11, 257, 733, 295, 5054, 295, 264, 1882, 279, 420, 746, 11, 597, 390, 257, 51016], "temperature": 0.0, "avg_logprob": -0.13044127238165473, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0033757199998944998}, {"id": 955, "seek": 545512, "start": 5468.16, "end": 5476.24, "text": " planet that had a life form similar to ours, but with a significantly lower IQ. And a human being", "tokens": [51016, 5054, 300, 632, 257, 993, 1254, 2531, 281, 11896, 11, 457, 365, 257, 10591, 3126, 28921, 13, 400, 257, 1952, 885, 51420], "temperature": 0.0, "avg_logprob": -0.13044127238165473, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0033757199998944998}, {"id": 956, "seek": 545512, "start": 5476.24, "end": 5481.36, "text": " shows up there one day, and these things start writing about this, hey, this weird alien just", "tokens": [51420, 3110, 493, 456, 472, 786, 11, 293, 613, 721, 722, 3579, 466, 341, 11, 4177, 11, 341, 3657, 12319, 445, 51676], "temperature": 0.0, "avg_logprob": -0.13044127238165473, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0033757199998944998}, {"id": 957, "seek": 548136, "start": 5481.36, "end": 5486.48, "text": " showed up here, and we captured it, we ran some tests on it, and we figured out it's really", "tokens": [50364, 4712, 493, 510, 11, 293, 321, 11828, 309, 11, 321, 5872, 512, 6921, 322, 309, 11, 293, 321, 8932, 484, 309, 311, 534, 50620], "temperature": 0.0, "avg_logprob": -0.10115661159638435, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.03408712521195412}, {"id": 958, "seek": 548136, "start": 5486.48, "end": 5492.719999999999, "text": " intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen", "tokens": [50620, 13232, 13, 467, 311, 709, 544, 13232, 813, 604, 295, 505, 366, 13, 400, 321, 434, 5804, 437, 311, 516, 281, 1051, 50932], "temperature": 0.0, "avg_logprob": -0.10115661159638435, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.03408712521195412}, {"id": 959, "seek": 548136, "start": 5492.719999999999, "end": 5499.04, "text": " when 100 of them show up instead of just this initial explorer. And some other of these guys", "tokens": [50932, 562, 2319, 295, 552, 855, 493, 2602, 295, 445, 341, 5883, 39680, 13, 400, 512, 661, 295, 613, 1074, 51248], "temperature": 0.0, "avg_logprob": -0.10115661159638435, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.03408712521195412}, {"id": 960, "seek": 548136, "start": 5499.04, "end": 5504.32, "text": " were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what", "tokens": [51248, 645, 411, 11, 3716, 11, 500, 380, 3292, 466, 309, 13, 814, 600, 658, 732, 5668, 293, 732, 5812, 411, 505, 11, 293, 881, 295, 437, 51512], "temperature": 0.0, "avg_logprob": -0.10115661159638435, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.03408712521195412}, {"id": 961, "seek": 548136, "start": 5504.32, "end": 5510.48, "text": " they are is kind of outside of their brain. So I'm not really worried about it. We would be", "tokens": [51512, 436, 366, 307, 733, 295, 2380, 295, 641, 3567, 13, 407, 286, 478, 406, 534, 5804, 466, 309, 13, 492, 576, 312, 51820], "temperature": 0.0, "avg_logprob": -0.10115661159638435, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.03408712521195412}, {"id": 962, "seek": 551048, "start": 5510.48, "end": 5516.639999999999, "text": " reading that with trepidation, right, because we know that when this more intelligent species", "tokens": [50364, 3760, 300, 365, 2192, 79, 327, 399, 11, 558, 11, 570, 321, 458, 300, 562, 341, 544, 13232, 6172, 50672], "temperature": 0.0, "avg_logprob": -0.07723039051271835, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.002714733825996518}, {"id": 963, "seek": 551048, "start": 5516.639999999999, "end": 5521.919999999999, "text": " with more fluid intelligence, more externalized intelligence, better technology, all this kind", "tokens": [50672, 365, 544, 9113, 7599, 11, 544, 8320, 1602, 7599, 11, 1101, 2899, 11, 439, 341, 733, 50936], "temperature": 0.0, "avg_logprob": -0.07723039051271835, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.002714733825996518}, {"id": 964, "seek": 551048, "start": 5521.919999999999, "end": 5527.04, "text": " of stuff shows up, those guys are going to get wiped out. And it's actually happened like many", "tokens": [50936, 295, 1507, 3110, 493, 11, 729, 1074, 366, 516, 281, 483, 26879, 484, 13, 400, 309, 311, 767, 2011, 411, 867, 51192], "temperature": 0.0, "avg_logprob": -0.07723039051271835, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.002714733825996518}, {"id": 965, "seek": 551048, "start": 5527.04, "end": 5532.639999999999, "text": " times throughout human history, not that humans were more fluid intelligence showed up and killed", "tokens": [51192, 1413, 3710, 1952, 2503, 11, 406, 300, 6255, 645, 544, 9113, 7599, 4712, 493, 293, 4652, 51472], "temperature": 0.0, "avg_logprob": -0.07723039051271835, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.002714733825996518}, {"id": 966, "seek": 551048, "start": 5532.639999999999, "end": 5538.5599999999995, "text": " off, you know, other people, but humans that had more externalized intelligence or more, you know,", "tokens": [51472, 766, 11, 291, 458, 11, 661, 561, 11, 457, 6255, 300, 632, 544, 8320, 1602, 7599, 420, 544, 11, 291, 458, 11, 51768], "temperature": 0.0, "avg_logprob": -0.07723039051271835, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.002714733825996518}, {"id": 967, "seek": 553856, "start": 5538.64, "end": 5542.0, "text": " represented intelligence and technology certainly showed up and dominated.", "tokens": [50368, 10379, 7599, 293, 2899, 3297, 4712, 493, 293, 23755, 13, 50536], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 968, "seek": 553856, "start": 5542.0, "end": 5547.120000000001, "text": " Absolutely. You're saying it yourself that when it has happened in history, it was not", "tokens": [50536, 7021, 13, 509, 434, 1566, 309, 1803, 300, 562, 309, 575, 2011, 294, 2503, 11, 309, 390, 406, 50792], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 969, "seek": 553856, "start": 5547.120000000001, "end": 5554.160000000001, "text": " fundamentally about one people having smarter brains, but one people having higher technology.", "tokens": [50792, 17879, 466, 472, 561, 1419, 20294, 15442, 11, 457, 472, 561, 1419, 2946, 2899, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 970, "seek": 553856, "start": 5554.160000000001, "end": 5558.8, "text": " But that that is not something that is attributable to intelligence itself, right?", "tokens": [51144, 583, 300, 300, 307, 406, 746, 300, 307, 9080, 32148, 281, 7599, 2564, 11, 558, 30, 51376], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 971, "seek": 553856, "start": 5558.8, "end": 5562.320000000001, "text": " There's a connection there. If you did have a group of species or whatever,", "tokens": [51376, 821, 311, 257, 4984, 456, 13, 759, 291, 630, 362, 257, 1594, 295, 6172, 420, 2035, 11, 51552], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 972, "seek": 553856, "start": 5562.320000000001, "end": 5567.4400000000005, "text": " that was much more intelligent, they will have advanced technologically much faster and further", "tokens": [51552, 300, 390, 709, 544, 13232, 11, 436, 486, 362, 7339, 1537, 17157, 709, 4663, 293, 3052, 51808], "temperature": 0.0, "avg_logprob": -0.12526192755069374, "compression_ratio": 1.7322033898305085, "no_speech_prob": 0.0027570149395614862}, {"id": 973, "seek": 556744, "start": 5567.5199999999995, "end": 5570.719999999999, "text": " in any given amount of time, all else being equal, right?", "tokens": [50368, 294, 604, 2212, 2372, 295, 565, 11, 439, 1646, 885, 2681, 11, 558, 30, 50528], "temperature": 0.0, "avg_logprob": -0.12690577507019044, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.0009396713576279581}, {"id": 974, "seek": 556744, "start": 5571.5199999999995, "end": 5576.96, "text": " It depends on many factors. And that's kind of my point is that is your brain a factor? Yes,", "tokens": [50568, 467, 5946, 322, 867, 6771, 13, 400, 300, 311, 733, 295, 452, 935, 307, 300, 307, 428, 3567, 257, 5952, 30, 1079, 11, 50840], "temperature": 0.0, "avg_logprob": -0.12690577507019044, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.0009396713576279581}, {"id": 975, "seek": 556744, "start": 5577.599999999999, "end": 5582.48, "text": " absolutely, it is. But there are other factors like we are just talking about the development", "tokens": [50872, 3122, 11, 309, 307, 13, 583, 456, 366, 661, 6771, 411, 321, 366, 445, 1417, 466, 264, 3250, 51116], "temperature": 0.0, "avg_logprob": -0.12690577507019044, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.0009396713576279581}, {"id": 976, "seek": 556744, "start": 5582.48, "end": 5588.879999999999, "text": " of technology. So in that case, the critical factor was not the brain, but the superstructure", "tokens": [51116, 295, 2899, 13, 407, 294, 300, 1389, 11, 264, 4924, 5952, 390, 406, 264, 3567, 11, 457, 264, 29423, 2885, 51436], "temperature": 0.0, "avg_logprob": -0.12690577507019044, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.0009396713576279581}, {"id": 977, "seek": 556744, "start": 5588.879999999999, "end": 5593.5199999999995, "text": " in particular communication and environmental constraints around it. The direction in which", "tokens": [51436, 294, 1729, 6101, 293, 8303, 18491, 926, 309, 13, 440, 3513, 294, 597, 51668], "temperature": 0.0, "avg_logprob": -0.12690577507019044, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.0009396713576279581}, {"id": 978, "seek": 559352, "start": 5593.84, "end": 5600.72, "text": " civilization develops is a direct function of the specific challenges it encounters that", "tokens": [50380, 18036, 25453, 307, 257, 2047, 2445, 295, 264, 2685, 4759, 309, 26310, 300, 50724], "temperature": 0.0, "avg_logprob": -0.2697143001832824, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.0047357771545648575}, {"id": 979, "seek": 559352, "start": 5600.72, "end": 5605.280000000001, "text": " come from its environment, that comes from its surrounding enemies, and so on. And", "tokens": [50724, 808, 490, 1080, 2823, 11, 300, 1487, 490, 1080, 11498, 7805, 11, 293, 370, 322, 13, 400, 50952], "temperature": 0.0, "avg_logprob": -0.2697143001832824, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.0047357771545648575}, {"id": 980, "seek": 559352, "start": 5605.92, "end": 5612.72, "text": " technological development advances the fastest when you have a civilization that are dealing with", "tokens": [50984, 18439, 3250, 25297, 264, 14573, 562, 291, 362, 257, 18036, 300, 366, 6260, 365, 51324], "temperature": 0.0, "avg_logprob": -0.2697143001832824, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.0047357771545648575}, {"id": 981, "seek": 559352, "start": 5612.72, "end": 5617.040000000001, "text": " very harsh challenges, but that are not quite fortunate to work them out.", "tokens": [51324, 588, 14897, 4759, 11, 457, 300, 366, 406, 1596, 14096, 281, 589, 552, 484, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2697143001832824, "compression_ratio": 1.673170731707317, "no_speech_prob": 0.0047357771545648575}, {"id": 982, "seek": 561704, "start": 5618.0, "end": 5624.4, "text": " Because that's what forces them to develop as fast as it can survive. So this is actually a", "tokens": [50412, 1436, 300, 311, 437, 5874, 552, 281, 1499, 382, 2370, 382, 309, 393, 7867, 13, 407, 341, 307, 767, 257, 50732], "temperature": 0.0, "avg_logprob": -0.1998043531253014, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.004055379424244165}, {"id": 983, "seek": 561704, "start": 5624.4, "end": 5629.68, "text": " very good example where the critical factor was the superstructure that guided the development", "tokens": [50732, 588, 665, 1365, 689, 264, 4924, 5952, 390, 264, 29423, 2885, 300, 19663, 264, 3250, 50996], "temperature": 0.0, "avg_logprob": -0.1998043531253014, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.004055379424244165}, {"id": 984, "seek": 561704, "start": 5629.68, "end": 5634.0, "text": " civilization was not actually the brain. But of course, yeah, if a one is smaller,", "tokens": [50996, 18036, 390, 406, 767, 264, 3567, 13, 583, 295, 1164, 11, 1338, 11, 498, 257, 472, 307, 4356, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1998043531253014, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.004055379424244165}, {"id": 985, "seek": 561704, "start": 5634.0, "end": 5641.5199999999995, "text": " then civilization will advance faster. But my point is that there are many factors and that", "tokens": [51212, 550, 18036, 486, 7295, 4663, 13, 583, 452, 935, 307, 300, 456, 366, 867, 6771, 293, 300, 51588], "temperature": 0.0, "avg_logprob": -0.1998043531253014, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.004055379424244165}, {"id": 986, "seek": 564152, "start": 5641.6, "end": 5647.200000000001, "text": " by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately", "tokens": [50368, 538, 6986, 2456, 472, 5952, 11, 264, 3567, 11, 498, 264, 3567, 10094, 885, 264, 44641, 547, 11, 550, 4258, 50648], "temperature": 0.0, "avg_logprob": -0.1129271338967716, "compression_ratio": 1.6875, "no_speech_prob": 0.0018300812225788832}, {"id": 987, "seek": 564152, "start": 5647.200000000001, "end": 5654.240000000001, "text": " some other factor will be the bottleneck. There are civilizations that have not actually advanced", "tokens": [50648, 512, 661, 5952, 486, 312, 264, 44641, 547, 13, 821, 366, 40749, 300, 362, 406, 767, 7339, 51000], "temperature": 0.0, "avg_logprob": -0.1129271338967716, "compression_ratio": 1.6875, "no_speech_prob": 0.0018300812225788832}, {"id": 988, "seek": 564152, "start": 5654.240000000001, "end": 5660.56, "text": " very much at all because they simply did not face any changes. And did they have worse brains? No,", "tokens": [51000, 588, 709, 412, 439, 570, 436, 2935, 630, 406, 1851, 604, 2962, 13, 400, 630, 436, 362, 5324, 15442, 30, 883, 11, 51316], "temperature": 0.0, "avg_logprob": -0.1129271338967716, "compression_ratio": 1.6875, "no_speech_prob": 0.0018300812225788832}, {"id": 989, "seek": 564152, "start": 5660.56, "end": 5665.68, "text": " actually, they had exactly the same brain. But somehow the outcome was different because", "tokens": [51316, 767, 11, 436, 632, 2293, 264, 912, 3567, 13, 583, 6063, 264, 9700, 390, 819, 570, 51572], "temperature": 0.0, "avg_logprob": -0.1129271338967716, "compression_ratio": 1.6875, "no_speech_prob": 0.0018300812225788832}, {"id": 990, "seek": 566568, "start": 5665.68, "end": 5671.92, "text": " something else, then the brain turned out to be the bottleneck like lack of environmental change.", "tokens": [50364, 746, 1646, 11, 550, 264, 3567, 3574, 484, 281, 312, 264, 44641, 547, 411, 5011, 295, 8303, 1319, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12784117698669434, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.005833270028233528}, {"id": 991, "seek": 566568, "start": 5672.96, "end": 5678.08, "text": " I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and", "tokens": [50728, 286, 478, 24597, 538, 4373, 293, 44641, 2761, 294, 3652, 13, 5135, 11, 286, 589, 294, 257, 2416, 22197, 293, 50984], "temperature": 0.0, "avg_logprob": -0.12784117698669434, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.005833270028233528}, {"id": 992, "seek": 566568, "start": 5678.08, "end": 5682.88, "text": " when you have role fragmentation and lots of different businesses and lots of different", "tokens": [50984, 562, 291, 362, 3090, 9241, 19631, 293, 3195, 295, 819, 6011, 293, 3195, 295, 819, 51224], "temperature": 0.0, "avg_logprob": -0.12784117698669434, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.005833270028233528}, {"id": 993, "seek": 566568, "start": 5682.88, "end": 5688.72, "text": " organization or structures, some people might decide to structure themselves based on data", "tokens": [51224, 4475, 420, 9227, 11, 512, 561, 1062, 4536, 281, 3877, 2969, 2361, 322, 1412, 51516], "temperature": 0.0, "avg_logprob": -0.12784117698669434, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.005833270028233528}, {"id": 994, "seek": 566568, "start": 5688.72, "end": 5694.4800000000005, "text": " domain or based on organization or based on something else. And you can think of it topologically.", "tokens": [51516, 9274, 420, 2361, 322, 4475, 420, 2361, 322, 746, 1646, 13, 400, 291, 393, 519, 295, 309, 1192, 17157, 13, 51804], "temperature": 0.0, "avg_logprob": -0.12784117698669434, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.005833270028233528}, {"id": 995, "seek": 569448, "start": 5694.48, "end": 5700.16, "text": " And I think human society is very similar to this. And I'm not sure whether evolution", "tokens": [50364, 400, 286, 519, 1952, 4086, 307, 588, 2531, 281, 341, 13, 400, 286, 478, 406, 988, 1968, 9303, 50648], "temperature": 0.0, "avg_logprob": -0.08625960118562273, "compression_ratio": 1.6219081272084805, "no_speech_prob": 0.008583921939134598}, {"id": 996, "seek": 569448, "start": 5700.16, "end": 5705.2, "text": " would lead itself to one particular topology. But the environmental structures and the ways", "tokens": [50648, 576, 1477, 2564, 281, 472, 1729, 1192, 1793, 13, 583, 264, 8303, 9227, 293, 264, 2098, 50900], "temperature": 0.0, "avg_logprob": -0.08625960118562273, "compression_ratio": 1.6219081272084805, "no_speech_prob": 0.008583921939134598}, {"id": 997, "seek": 569448, "start": 5705.2, "end": 5711.679999999999, "text": " that we organize ourselves can create incredible bottlenecks. And that seems to be where the real", "tokens": [50900, 300, 321, 13859, 4175, 393, 1884, 4651, 44641, 2761, 13, 400, 300, 2544, 281, 312, 689, 264, 957, 51224], "temperature": 0.0, "avg_logprob": -0.08625960118562273, "compression_ratio": 1.6219081272084805, "no_speech_prob": 0.008583921939134598}, {"id": 998, "seek": 569448, "start": 5711.679999999999, "end": 5717.12, "text": " interesting stuff goes on rather than the individuals. And I think you would agree with that,", "tokens": [51224, 1880, 1507, 1709, 322, 2831, 813, 264, 5346, 13, 400, 286, 519, 291, 576, 3986, 365, 300, 11, 51496], "temperature": 0.0, "avg_logprob": -0.08625960118562273, "compression_ratio": 1.6219081272084805, "no_speech_prob": 0.008583921939134598}, {"id": 999, "seek": 569448, "start": 5717.12, "end": 5722.799999999999, "text": " Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ", "tokens": [51496, 34695, 271, 13, 865, 11, 3122, 13, 759, 291, 747, 732, 3431, 11, 293, 294, 472, 2237, 11, 264, 4274, 28921, 51780], "temperature": 0.0, "avg_logprob": -0.08625960118562273, "compression_ratio": 1.6219081272084805, "no_speech_prob": 0.008583921939134598}, {"id": 1000, "seek": 572280, "start": 5722.8, "end": 5728.320000000001, "text": " is like 15 points higher, but it has a terrible organizational structure and terrible incentives", "tokens": [50364, 307, 411, 2119, 2793, 2946, 11, 457, 309, 575, 257, 6237, 24730, 3877, 293, 6237, 23374, 50640], "temperature": 0.0, "avg_logprob": -0.1577332457717584, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.00830799899995327}, {"id": 1001, "seek": 572280, "start": 5728.320000000001, "end": 5734.16, "text": " and the promo process is super broken or something. And that company is actually going to perform worse", "tokens": [50640, 293, 264, 26750, 1399, 307, 1687, 5463, 420, 746, 13, 400, 300, 2237, 307, 767, 516, 281, 2042, 5324, 50932], "temperature": 0.0, "avg_logprob": -0.1577332457717584, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.00830799899995327}, {"id": 1002, "seek": 572280, "start": 5734.16, "end": 5739.360000000001, "text": " than the more progressive innovation encouraging company that has a very nice organizational", "tokens": [50932, 813, 264, 544, 16131, 8504, 14580, 2237, 300, 575, 257, 588, 1481, 24730, 51192], "temperature": 0.0, "avg_logprob": -0.1577332457717584, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.00830799899995327}, {"id": 1003, "seek": 572280, "start": 5739.360000000001, "end": 5745.28, "text": " structure and where people are actually more mediocre. Maybe they have on average 15 points", "tokens": [51192, 3877, 293, 689, 561, 366, 767, 544, 45415, 13, 2704, 436, 362, 322, 4274, 2119, 2793, 51488], "temperature": 0.0, "avg_logprob": -0.1577332457717584, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.00830799899995327}, {"id": 1004, "seek": 572280, "start": 5745.28, "end": 5750.0, "text": " less in IQ, but they're actually going to do a better job because they have the better superstructure.", "tokens": [51488, 1570, 294, 28921, 11, 457, 436, 434, 767, 516, 281, 360, 257, 1101, 1691, 570, 436, 362, 264, 1101, 29423, 2885, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1577332457717584, "compression_ratio": 1.8345864661654134, "no_speech_prob": 0.00830799899995327}, {"id": 1005, "seek": 575000, "start": 5750.64, "end": 5755.68, "text": " Yeah, it's fascinating that the problem is in most corporations, you can't actually design the", "tokens": [50396, 865, 11, 309, 311, 10343, 300, 264, 1154, 307, 294, 881, 17676, 11, 291, 393, 380, 767, 1715, 264, 50648], "temperature": 0.0, "avg_logprob": -0.08754221309315074, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.003933453932404518}, {"id": 1006, "seek": 575000, "start": 5755.68, "end": 5761.6, "text": " information architecture to be more efficient, because everything is so decentralized and", "tokens": [50648, 1589, 9482, 281, 312, 544, 7148, 11, 570, 1203, 307, 370, 32870, 293, 50944], "temperature": 0.0, "avg_logprob": -0.08754221309315074, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.003933453932404518}, {"id": 1007, "seek": 575000, "start": 5761.6, "end": 5766.72, "text": " fractionated, you can only do it in pockets. And if you try and fix something in one part", "tokens": [50944, 14135, 770, 11, 291, 393, 787, 360, 309, 294, 16491, 13, 400, 498, 291, 853, 293, 3191, 746, 294, 472, 644, 51200], "temperature": 0.0, "avg_logprob": -0.08754221309315074, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.003933453932404518}, {"id": 1008, "seek": 575000, "start": 5766.72, "end": 5770.16, "text": " of the organization, everyone else will say, well, my requirements are different. I'm not going to", "tokens": [51200, 295, 264, 4475, 11, 1518, 1646, 486, 584, 11, 731, 11, 452, 7728, 366, 819, 13, 286, 478, 406, 516, 281, 51372], "temperature": 0.0, "avg_logprob": -0.08754221309315074, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.003933453932404518}, {"id": 1009, "seek": 575000, "start": 5770.16, "end": 5774.24, "text": " wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing", "tokens": [51372, 1699, 337, 291, 13, 286, 478, 516, 281, 360, 309, 452, 1065, 636, 13, 400, 309, 311, 767, 257, 534, 11, 534, 2252, 551, 51576], "temperature": 0.0, "avg_logprob": -0.08754221309315074, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.003933453932404518}, {"id": 1010, "seek": 577424, "start": 5774.24, "end": 5780.639999999999, "text": " to do well. To sum up the whole like intelligence explosion thing, the point is really that it's", "tokens": [50364, 281, 360, 731, 13, 1407, 2408, 493, 264, 1379, 411, 7599, 15673, 551, 11, 264, 935, 307, 534, 300, 309, 311, 50684], "temperature": 0.0, "avg_logprob": -0.128899798185929, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.08958099782466888}, {"id": 1011, "seek": 577424, "start": 5780.639999999999, "end": 5786.0, "text": " a system you have to look at holistically to get it holistically. And just by tweaking one factor,", "tokens": [50684, 257, 1185, 291, 362, 281, 574, 412, 4091, 20458, 281, 483, 309, 4091, 20458, 13, 400, 445, 538, 6986, 2456, 472, 5952, 11, 50952], "temperature": 0.0, "avg_logprob": -0.128899798185929, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.08958099782466888}, {"id": 1012, "seek": 577424, "start": 5786.0, "end": 5791.28, "text": " which is the intelligence of an individual human brain, then what this means is this factor starts", "tokens": [50952, 597, 307, 264, 7599, 295, 364, 2609, 1952, 3567, 11, 550, 437, 341, 1355, 307, 341, 5952, 3719, 51216], "temperature": 0.0, "avg_logprob": -0.128899798185929, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.08958099782466888}, {"id": 1013, "seek": 577424, "start": 5791.28, "end": 5795.679999999999, "text": " being the bottleneck. But that means some other factor in the system, because there's an infinite", "tokens": [51216, 885, 264, 44641, 547, 13, 583, 300, 1355, 512, 661, 5952, 294, 264, 1185, 11, 570, 456, 311, 364, 13785, 51436], "temperature": 0.0, "avg_logprob": -0.128899798185929, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.08958099782466888}, {"id": 1014, "seek": 577424, "start": 5795.679999999999, "end": 5800.48, "text": " factor that will become the bottleneck. And by just focusing on one factor, you're not going to", "tokens": [51436, 5952, 300, 486, 1813, 264, 44641, 547, 13, 400, 538, 445, 8416, 322, 472, 5952, 11, 291, 434, 406, 516, 281, 51676], "temperature": 0.0, "avg_logprob": -0.128899798185929, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.08958099782466888}, {"id": 1015, "seek": 580048, "start": 5800.48, "end": 5806.0, "text": " actually lift all the votes. Yeah, and I actually agree with you. However,", "tokens": [50364, 767, 5533, 439, 264, 12068, 13, 865, 11, 293, 286, 767, 3986, 365, 291, 13, 2908, 11, 50640], "temperature": 0.0, "avg_logprob": -0.12485873071770918, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.005136485211551189}, {"id": 1016, "seek": 580048, "start": 5806.959999999999, "end": 5811.04, "text": " I do want to say, I think we just don't know. I think both sides of the intelligence,", "tokens": [50688, 286, 360, 528, 281, 584, 11, 286, 519, 321, 445, 500, 380, 458, 13, 286, 519, 1293, 4881, 295, 264, 7599, 11, 50892], "temperature": 0.0, "avg_logprob": -0.12485873071770918, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.005136485211551189}, {"id": 1017, "seek": 580048, "start": 5811.04, "end": 5817.36, "text": " quote unquote, explosion really can't say for certain that it will or will not pose a mortal", "tokens": [50892, 6513, 37557, 11, 15673, 534, 393, 380, 584, 337, 1629, 300, 309, 486, 420, 486, 406, 10774, 257, 27624, 51208], "temperature": 0.0, "avg_logprob": -0.12485873071770918, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.005136485211551189}, {"id": 1018, "seek": 580048, "start": 5817.36, "end": 5822.16, "text": " threat to humanity. I think we have to accept that it's at least a risk factor. And we have", "tokens": [51208, 4734, 281, 10243, 13, 286, 519, 321, 362, 281, 3241, 300, 309, 311, 412, 1935, 257, 3148, 5952, 13, 400, 321, 362, 51448], "temperature": 0.0, "avg_logprob": -0.12485873071770918, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.005136485211551189}, {"id": 1019, "seek": 580048, "start": 5822.16, "end": 5829.28, "text": " to be very careful about, in the future, when we start embodying, if we find general intelligence,", "tokens": [51448, 281, 312, 588, 5026, 466, 11, 294, 264, 2027, 11, 562, 321, 722, 42575, 278, 11, 498, 321, 915, 2674, 7599, 11, 51804], "temperature": 0.0, "avg_logprob": -0.12485873071770918, "compression_ratio": 1.6754716981132076, "no_speech_prob": 0.005136485211551189}, {"id": 1020, "seek": 582928, "start": 5829.28, "end": 5833.759999999999, "text": " we need to be cautious. If we come up with something that looks like general intelligence,", "tokens": [50364, 321, 643, 281, 312, 25278, 13, 759, 321, 808, 493, 365, 746, 300, 1542, 411, 2674, 7599, 11, 50588], "temperature": 0.0, "avg_logprob": -0.11918684515622582, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008947260677814484}, {"id": 1021, "seek": 582928, "start": 5833.759999999999, "end": 5839.599999999999, "text": " there is absolutely some risk potential around it. However, I've never seen anything coming", "tokens": [50588, 456, 307, 3122, 512, 3148, 3995, 926, 309, 13, 2908, 11, 286, 600, 1128, 1612, 1340, 1348, 50880], "temperature": 0.0, "avg_logprob": -0.11918684515622582, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008947260677814484}, {"id": 1022, "seek": 582928, "start": 5839.599999999999, "end": 5845.5199999999995, "text": " anywhere close to that. In fact, the systems that we have today, they fit your almost no", "tokens": [50880, 4992, 1998, 281, 300, 13, 682, 1186, 11, 264, 3652, 300, 321, 362, 965, 11, 436, 3318, 428, 1920, 572, 51176], "temperature": 0.0, "avg_logprob": -0.11918684515622582, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008947260677814484}, {"id": 1023, "seek": 582928, "start": 5845.5199999999995, "end": 5849.92, "text": " intelligence whatsoever. So I think it's a bit early to start banning them.", "tokens": [51176, 7599, 17076, 13, 407, 286, 519, 309, 311, 257, 857, 2440, 281, 722, 5643, 773, 552, 13, 51396], "temperature": 0.0, "avg_logprob": -0.11918684515622582, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008947260677814484}, {"id": 1024, "seek": 582928, "start": 5849.92, "end": 5853.84, "text": " And even if we get into that conversation, I think Francois would say that intelligence", "tokens": [51396, 400, 754, 498, 321, 483, 666, 300, 3761, 11, 286, 519, 34695, 271, 576, 584, 300, 7599, 51592], "temperature": 0.0, "avg_logprob": -0.11918684515622582, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008947260677814484}, {"id": 1025, "seek": 585384, "start": 5853.84, "end": 5857.360000000001, "text": " must be specialized, right, because of the no free lunch theorems.", "tokens": [50364, 1633, 312, 19813, 11, 558, 11, 570, 295, 264, 572, 1737, 6349, 10299, 2592, 13, 50540], "temperature": 0.0, "avg_logprob": -0.16234537985472552, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.022515611723065376}, {"id": 1026, "seek": 585384, "start": 5857.360000000001, "end": 5862.56, "text": " If you define intelligence as your ability to solve problems, then yeah, it's going to be", "tokens": [50540, 759, 291, 6964, 7599, 382, 428, 3485, 281, 5039, 2740, 11, 550, 1338, 11, 309, 311, 516, 281, 312, 50800], "temperature": 0.0, "avg_logprob": -0.16234537985472552, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.022515611723065376}, {"id": 1027, "seek": 585384, "start": 5862.56, "end": 5869.68, "text": " specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch", "tokens": [50800, 2685, 281, 257, 11923, 295, 2740, 11, 257, 733, 295, 2740, 13, 400, 411, 11, 1338, 11, 437, 264, 572, 1737, 6349, 51156], "temperature": 0.0, "avg_logprob": -0.16234537985472552, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.022515611723065376}, {"id": 1028, "seek": 585384, "start": 5869.68, "end": 5875.2, "text": " theorem is saying is basically, if you want to learn something from data, you have to make assumptions", "tokens": [51156, 20904, 307, 1566, 307, 1936, 11, 498, 291, 528, 281, 1466, 746, 490, 1412, 11, 291, 362, 281, 652, 17695, 51432], "temperature": 0.0, "avg_logprob": -0.16234537985472552, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.022515611723065376}, {"id": 1029, "seek": 585384, "start": 5875.2, "end": 5880.0, "text": " about it. Which is why you know a convent, for instance, is a great fit for image data. It's", "tokens": [51432, 466, 309, 13, 3013, 307, 983, 291, 458, 257, 416, 2475, 11, 337, 5197, 11, 307, 257, 869, 3318, 337, 3256, 1412, 13, 467, 311, 51672], "temperature": 0.0, "avg_logprob": -0.16234537985472552, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.022515611723065376}, {"id": 1030, "seek": 588000, "start": 5880.0, "end": 5884.88, "text": " not really a great fit for natural language processing. And because it makes different", "tokens": [50364, 406, 534, 257, 869, 3318, 337, 3303, 2856, 9007, 13, 400, 570, 309, 1669, 819, 50608], "temperature": 0.0, "avg_logprob": -0.11908854425480936, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008977678604424}, {"id": 1031, "seek": 588000, "start": 5884.88, "end": 5889.44, "text": " assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly", "tokens": [50608, 17695, 466, 13563, 13, 467, 1177, 380, 976, 385, 257, 688, 295, 3400, 11, 1673, 11, 570, 286, 478, 6457, 50836], "temperature": 0.0, "avg_logprob": -0.11908854425480936, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008977678604424}, {"id": 1032, "seek": 588000, "start": 5889.44, "end": 5894.56, "text": " certain that whatever the first AGI that gets created, it's going to be highly specialized", "tokens": [50836, 1629, 300, 2035, 264, 700, 316, 26252, 300, 2170, 2942, 11, 309, 311, 516, 281, 312, 5405, 19813, 51092], "temperature": 0.0, "avg_logprob": -0.11908854425480936, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008977678604424}, {"id": 1033, "seek": 588000, "start": 5894.56, "end": 5899.92, "text": " for killing other people, because it's going to be a military, you know, secret project,", "tokens": [51092, 337, 8011, 661, 561, 11, 570, 309, 311, 516, 281, 312, 257, 4632, 11, 291, 458, 11, 4054, 1716, 11, 51360], "temperature": 0.0, "avg_logprob": -0.11908854425480936, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008977678604424}, {"id": 1034, "seek": 588000, "start": 5899.92, "end": 5906.48, "text": " probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't", "tokens": [51360, 1391, 300, 10704, 309, 13, 509, 458, 11, 309, 311, 11, 286, 500, 380, 458, 13, 583, 437, 286, 458, 307, 300, 558, 586, 11, 321, 500, 380, 51688], "temperature": 0.0, "avg_logprob": -0.11908854425480936, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008977678604424}, {"id": 1035, "seek": 590648, "start": 5906.48, "end": 5914.639999999999, "text": " have anything coming close to AGI. It's probably going to be actually a system that just displays", "tokens": [50364, 362, 1340, 1348, 1998, 281, 316, 26252, 13, 467, 311, 1391, 516, 281, 312, 767, 257, 1185, 300, 445, 20119, 50772], "temperature": 0.0, "avg_logprob": -0.16325842165479473, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.038980476558208466}, {"id": 1036, "seek": 590648, "start": 5914.639999999999, "end": 5921.2, "text": " you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the", "tokens": [50772, 291, 10342, 13, 1743, 498, 11, 411, 11, 498, 11, 291, 458, 11, 498, 291, 11, 498, 321, 536, 689, 264, 881, 1460, 307, 558, 586, 11, 264, 51100], "temperature": 0.0, "avg_logprob": -0.16325842165479473, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.038980476558208466}, {"id": 1037, "seek": 590648, "start": 5921.2, "end": 5926.959999999999, "text": " first AGI is probably just going to like write, not only display, but write the perfect ad for", "tokens": [51100, 700, 316, 26252, 307, 1391, 445, 516, 281, 411, 2464, 11, 406, 787, 4674, 11, 457, 2464, 264, 2176, 614, 337, 51388], "temperature": 0.0, "avg_logprob": -0.16325842165479473, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.038980476558208466}, {"id": 1038, "seek": 590648, "start": 5926.959999999999, "end": 5932.799999999999, "text": " you on the fly. You know, it knows what you ate and you know, I know you're joking with", "tokens": [51388, 291, 322, 264, 3603, 13, 509, 458, 11, 309, 3255, 437, 291, 8468, 293, 291, 458, 11, 286, 458, 291, 434, 17396, 365, 51680], "temperature": 0.0, "avg_logprob": -0.16325842165479473, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.038980476558208466}, {"id": 1039, "seek": 593280, "start": 5932.96, "end": 5938.24, "text": " actually think on the, on the more serious, I think that's highly unlikely because of the", "tokens": [50372, 767, 519, 322, 264, 11, 322, 264, 544, 3156, 11, 286, 519, 300, 311, 5405, 17518, 570, 295, 264, 50636], "temperature": 0.0, "avg_logprob": -0.20083674159618692, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004719081334769726}, {"id": 1040, "seek": 593280, "start": 5938.24, "end": 5943.92, "text": " short code of the story because of the short patrol. I don't think a general intelligence is", "tokens": [50636, 2099, 3089, 295, 264, 1657, 570, 295, 264, 2099, 26305, 13, 286, 500, 380, 519, 257, 2674, 7599, 307, 50920], "temperature": 0.0, "avg_logprob": -0.20083674159618692, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004719081334769726}, {"id": 1041, "seek": 593280, "start": 5943.92, "end": 5948.72, "text": " going to be created by the military is not going to be created by a system that's trying to show", "tokens": [50920, 516, 281, 312, 2942, 538, 264, 4632, 307, 406, 516, 281, 312, 2942, 538, 257, 1185, 300, 311, 1382, 281, 855, 51160], "temperature": 0.0, "avg_logprob": -0.20083674159618692, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004719081334769726}, {"id": 1042, "seek": 593280, "start": 5948.72, "end": 5954.64, "text": " you ads because these are specific goals. And so if you try to optimize those specific goals,", "tokens": [51160, 291, 10342, 570, 613, 366, 2685, 5493, 13, 400, 370, 498, 291, 853, 281, 19719, 729, 2685, 5493, 11, 51456], "temperature": 0.0, "avg_logprob": -0.20083674159618692, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004719081334769726}, {"id": 1043, "seek": 593280, "start": 5954.64, "end": 5959.92, "text": " you're going to end up with a very specialized system in order to build a general intelligence,", "tokens": [51456, 291, 434, 516, 281, 917, 493, 365, 257, 588, 19813, 1185, 294, 1668, 281, 1322, 257, 2674, 7599, 11, 51720], "temperature": 0.0, "avg_logprob": -0.20083674159618692, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004719081334769726}, {"id": 1044, "seek": 595992, "start": 5960.0, "end": 5967.36, "text": " you need to be optimizing for generality itself. So it's going to come from, if it comes from the", "tokens": [50368, 291, 643, 281, 312, 40425, 337, 1337, 1860, 2564, 13, 407, 309, 311, 516, 281, 808, 490, 11, 498, 309, 1487, 490, 264, 50736], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1045, "seek": 595992, "start": 5967.36, "end": 5971.36, "text": " applied, either it's going to come from the academic side, where you have researchers who are", "tokens": [50736, 6456, 11, 2139, 309, 311, 516, 281, 808, 490, 264, 7778, 1252, 11, 689, 291, 362, 10309, 567, 366, 50936], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1046, "seek": 595992, "start": 5971.36, "end": 5975.92, "text": " actually optimizing for generality itself, who said generality as they are going. Or if it's", "tokens": [50936, 767, 40425, 337, 1337, 1860, 2564, 11, 567, 848, 1337, 1860, 382, 436, 366, 516, 13, 1610, 498, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1047, "seek": 595992, "start": 5975.92, "end": 5980.24, "text": " come from the applied side, it's going to come from people who have problems where they have to", "tokens": [51164, 808, 490, 264, 6456, 1252, 11, 309, 311, 516, 281, 808, 490, 561, 567, 362, 2740, 689, 436, 362, 281, 51380], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1048, "seek": 595992, "start": 5980.24, "end": 5985.76, "text": " deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads,", "tokens": [51380, 2028, 365, 8084, 44805, 11, 15697, 11, 293, 28341, 2310, 13, 407, 309, 311, 406, 516, 281, 312, 10342, 11, 51656], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1049, "seek": 595992, "start": 5985.76, "end": 5988.8, "text": " it's not going to be the military. I don't know where this is going to be.", "tokens": [51656, 309, 311, 406, 516, 281, 312, 264, 4632, 13, 286, 500, 380, 458, 689, 341, 307, 516, 281, 312, 13, 51808], "temperature": 0.0, "avg_logprob": -0.18255825603709502, "compression_ratio": 2.0919540229885056, "no_speech_prob": 0.015365254133939743}, {"id": 1050, "seek": 598880, "start": 5988.88, "end": 5994.24, "text": " One of the things that interested me about Kenneth Stanley was that he says the reason we", "tokens": [50368, 1485, 295, 264, 721, 300, 3102, 385, 466, 33735, 28329, 390, 300, 415, 1619, 264, 1778, 321, 50636], "temperature": 0.0, "avg_logprob": -0.07982638222830636, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.00398359727114439}, {"id": 1051, "seek": 598880, "start": 5994.24, "end": 5999.28, "text": " can't monotonically optimize on objectives is because of deception, which means sometimes you", "tokens": [50636, 393, 380, 1108, 27794, 984, 19719, 322, 15961, 307, 570, 295, 40451, 11, 597, 1355, 2171, 291, 50888], "temperature": 0.0, "avg_logprob": -0.07982638222830636, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.00398359727114439}, {"id": 1052, "seek": 598880, "start": 5999.28, "end": 6004.16, "text": " need to get a lot worse before you get better. His original conception was quality diversity,", "tokens": [50888, 643, 281, 483, 257, 688, 5324, 949, 291, 483, 1101, 13, 2812, 3380, 30698, 390, 3125, 8811, 11, 51132], "temperature": 0.0, "avg_logprob": -0.07982638222830636, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.00398359727114439}, {"id": 1053, "seek": 598880, "start": 6004.16, "end": 6008.96, "text": " which basically means if you optimize for novelty, that's something that you can optimize on", "tokens": [51132, 597, 1936, 1355, 498, 291, 19719, 337, 44805, 11, 300, 311, 746, 300, 291, 393, 19719, 322, 51372], "temperature": 0.0, "avg_logprob": -0.07982638222830636, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.00398359727114439}, {"id": 1054, "seek": 598880, "start": 6008.96, "end": 6014.4800000000005, "text": " monotonically. And also, if you look at evolution, where there is a cacophony of problems and", "tokens": [51372, 1108, 27794, 984, 13, 400, 611, 11, 498, 291, 574, 412, 9303, 11, 689, 456, 307, 257, 269, 326, 5317, 2526, 295, 2740, 293, 51648], "temperature": 0.0, "avg_logprob": -0.07982638222830636, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.00398359727114439}, {"id": 1055, "seek": 601448, "start": 6014.48, "end": 6021.12, "text": " solutions divergently being generated, then as an information accumulator, you can optimize", "tokens": [50364, 6547, 18558, 70, 2276, 885, 10833, 11, 550, 382, 364, 1589, 12989, 16381, 11, 291, 393, 19719, 50696], "temperature": 0.0, "avg_logprob": -0.11119233516224644, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.06465800851583481}, {"id": 1056, "seek": 601448, "start": 6021.12, "end": 6026.32, "text": " on that monotonically. And your conception of intelligence is generality. And that also appears", "tokens": [50696, 322, 300, 1108, 27794, 984, 13, 400, 428, 30698, 295, 7599, 307, 1337, 1860, 13, 400, 300, 611, 7038, 50956], "temperature": 0.0, "avg_logprob": -0.11119233516224644, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.06465800851583481}, {"id": 1057, "seek": 601448, "start": 6026.32, "end": 6031.36, "text": " to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite", "tokens": [50956, 281, 312, 257, 1108, 310, 11630, 3488, 3710, 27267, 4358, 295, 7599, 13, 407, 286, 519, 300, 311, 1596, 51208], "temperature": 0.0, "avg_logprob": -0.11119233516224644, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.06465800851583481}, {"id": 1058, "seek": 601448, "start": 6031.36, "end": 6036.32, "text": " interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show.", "tokens": [51208, 1880, 13, 5684, 11, 34695, 271, 761, 1833, 302, 11, 341, 575, 668, 452, 3055, 808, 2074, 281, 362, 291, 322, 264, 855, 13, 51456], "temperature": 0.0, "avg_logprob": -0.11119233516224644, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.06465800851583481}, {"id": 1059, "seek": 601448, "start": 6036.32, "end": 6040.16, "text": " Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you.", "tokens": [51456, 1044, 291, 370, 709, 13, 467, 534, 1355, 257, 688, 281, 505, 13, 400, 1338, 11, 286, 4449, 309, 13, 1044, 291, 13, 51648], "temperature": 0.0, "avg_logprob": -0.11119233516224644, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.06465800851583481}, {"id": 1060, "seek": 604016, "start": 6040.24, "end": 6044.96, "text": " Thanks for having me on the podcast. It's really my pleasure. This was super fun.", "tokens": [50368, 2561, 337, 1419, 385, 322, 264, 7367, 13, 467, 311, 534, 452, 6834, 13, 639, 390, 1687, 1019, 13, 50604], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1061, "seek": 604016, "start": 6044.96, "end": 6048.639999999999, "text": " Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful.", "tokens": [50604, 2561, 13, 400, 1309, 291, 337, 591, 6985, 11, 538, 264, 636, 13, 2561, 13, 286, 478, 5404, 309, 311, 4420, 13, 50788], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1062, "seek": 604016, "start": 6048.639999999999, "end": 6052.08, "text": " We're going to jump straight into the post-show analysis.", "tokens": [50788, 492, 434, 516, 281, 3012, 2997, 666, 264, 2183, 12, 34436, 5215, 13, 50960], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1063, "seek": 604016, "start": 6052.08, "end": 6056.0, "text": " Okay, well, I'm going to mention you did really well, Tim, that trickle sweat", "tokens": [50960, 1033, 11, 731, 11, 286, 478, 516, 281, 2152, 291, 630, 534, 731, 11, 7172, 11, 300, 4282, 306, 11872, 51156], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1064, "seek": 604016, "start": 6056.0, "end": 6061.2, "text": " that this was running down your face the whole time. Not very noticeable. So I think you can", "tokens": [51156, 300, 341, 390, 2614, 760, 428, 1851, 264, 1379, 565, 13, 1726, 588, 26041, 13, 407, 286, 519, 291, 393, 51416], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1065, "seek": 604016, "start": 6061.2, "end": 6067.12, "text": " relax. That was fun. I think it went pretty well. Yeah, it was a dream come true.", "tokens": [51416, 5789, 13, 663, 390, 1019, 13, 286, 519, 309, 1437, 1238, 731, 13, 865, 11, 309, 390, 257, 3055, 808, 2074, 13, 51712], "temperature": 0.0, "avg_logprob": -0.17031431931715746, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.1658852994441986}, {"id": 1066, "seek": 606712, "start": 6067.68, "end": 6073.76, "text": " I was actually I was very pleasantly kind of interested in how he he framed, you know,", "tokens": [50392, 286, 390, 767, 286, 390, 588, 35122, 3627, 733, 295, 3102, 294, 577, 415, 415, 30420, 11, 291, 458, 11, 50696], "temperature": 0.0, "avg_logprob": -0.1531733057715676, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.00394489336758852}, {"id": 1067, "seek": 606712, "start": 6073.76, "end": 6077.84, "text": " the measure of intelligence paper like, look, it's not really about the measure per se. It's just", "tokens": [50696, 264, 3481, 295, 7599, 3035, 411, 11, 574, 11, 309, 311, 406, 534, 466, 264, 3481, 680, 369, 13, 467, 311, 445, 50900], "temperature": 0.0, "avg_logprob": -0.1531733057715676, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.00394489336758852}, {"id": 1068, "seek": 606712, "start": 6077.84, "end": 6085.44, "text": " that this is this is a cognitive framework, a cognitive tool for thinking about where to go", "tokens": [50900, 300, 341, 307, 341, 307, 257, 15605, 8388, 11, 257, 15605, 2290, 337, 1953, 466, 689, 281, 352, 51280], "temperature": 0.0, "avg_logprob": -0.1531733057715676, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.00394489336758852}, {"id": 1069, "seek": 606712, "start": 6085.44, "end": 6091.84, "text": " and a guidepost for building more generalizable or more general intelligences say like that,", "tokens": [51280, 293, 257, 5934, 23744, 337, 2390, 544, 2674, 22395, 420, 544, 2674, 5613, 2667, 584, 411, 300, 11, 51600], "temperature": 0.0, "avg_logprob": -0.1531733057715676, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.00394489336758852}, {"id": 1070, "seek": 609184, "start": 6091.84, "end": 6096.8, "text": " I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like,", "tokens": [50364, 286, 3879, 11, 3879, 3986, 281, 13, 400, 309, 311, 1596, 11, 291, 458, 11, 1596, 257, 10343, 3387, 11, 597, 307, 411, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1853611055484488, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.01941409707069397}, {"id": 1071, "seek": 609184, "start": 6096.8, "end": 6100.32, "text": " here's a framework to help us think more in the direction we need to be thinking.", "tokens": [50612, 510, 311, 257, 8388, 281, 854, 505, 519, 544, 294, 264, 3513, 321, 643, 281, 312, 1953, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1853611055484488, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.01941409707069397}, {"id": 1072, "seek": 609184, "start": 6100.88, "end": 6108.16, "text": " Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because", "tokens": [50816, 865, 13, 400, 309, 311, 370, 8830, 300, 411, 264, 10346, 3430, 307, 412, 411, 945, 4, 13041, 787, 570, 51180], "temperature": 0.0, "avg_logprob": -0.1853611055484488, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.01941409707069397}, {"id": 1073, "seek": 609184, "start": 6108.88, "end": 6115.360000000001, "text": " you know, he self admits that it's flawed, right? Because he like, he makes the tasks.", "tokens": [51216, 291, 458, 11, 415, 2698, 46682, 300, 309, 311, 38823, 11, 558, 30, 1436, 415, 411, 11, 415, 1669, 264, 9608, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1853611055484488, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.01941409707069397}, {"id": 1074, "seek": 609184, "start": 6115.360000000001, "end": 6120.96, "text": " And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks", "tokens": [51540, 400, 11, 291, 458, 11, 456, 311, 787, 962, 1959, 867, 293, 293, 291, 458, 11, 291, 733, 295, 291, 536, 264, 733, 295, 9608, 51820], "temperature": 0.0, "avg_logprob": -0.1853611055484488, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.01941409707069397}, {"id": 1075, "seek": 612096, "start": 6121.04, "end": 6127.6, "text": " he makes, you know, in the public set, you would think that not someone will come up with an", "tokens": [50368, 415, 1669, 11, 291, 458, 11, 294, 264, 1908, 992, 11, 291, 576, 519, 300, 406, 1580, 486, 808, 493, 365, 364, 50696], "temperature": 0.0, "avg_logprob": -0.08832788467407227, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.00911780633032322}, {"id": 1076, "seek": 612096, "start": 6127.6, "end": 6133.52, "text": " intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that", "tokens": [50696, 13232, 551, 11, 457, 1580, 486, 808, 493, 365, 411, 257, 4069, 992, 295, 34620, 281, 411, 5039, 300, 50992], "temperature": 0.0, "avg_logprob": -0.08832788467407227, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.00911780633032322}, {"id": 1077, "seek": 612096, "start": 6133.52, "end": 6140.32, "text": " sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many", "tokens": [50992, 43259, 11, 558, 30, 583, 309, 311, 920, 412, 945, 6856, 286, 500, 380, 458, 1968, 300, 311, 3462, 281, 445, 11, 291, 458, 11, 406, 886, 867, 51332], "temperature": 0.0, "avg_logprob": -0.08832788467407227, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.00911780633032322}, {"id": 1078, "seek": 612096, "start": 6140.32, "end": 6149.2, "text": " people investigating it. Or whether it's really actually a hard problem. And if it is a problem,", "tokens": [51332, 561, 22858, 309, 13, 1610, 1968, 309, 311, 534, 767, 257, 1152, 1154, 13, 400, 498, 309, 307, 257, 1154, 11, 51776], "temperature": 0.0, "avg_logprob": -0.08832788467407227, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.00911780633032322}, {"id": 1079, "seek": 614920, "start": 6149.2, "end": 6154.72, "text": " you know, well, it's fascinating too, because if he if he achieves what he wanted, which was", "tokens": [50364, 291, 458, 11, 731, 11, 309, 311, 10343, 886, 11, 570, 498, 415, 498, 415, 3538, 977, 437, 415, 1415, 11, 597, 390, 50640], "temperature": 0.0, "avg_logprob": -0.13448654372116614, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007553794421255589}, {"id": 1080, "seek": 614920, "start": 6154.72, "end": 6158.72, "text": " getting it more outsourced, right, like getting all the intelligent people all around the world", "tokens": [50640, 1242, 309, 544, 14758, 396, 1232, 11, 558, 11, 411, 1242, 439, 264, 13232, 561, 439, 926, 264, 1002, 50840], "temperature": 0.0, "avg_logprob": -0.13448654372116614, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007553794421255589}, {"id": 1081, "seek": 614920, "start": 6158.72, "end": 6165.44, "text": " contributing to arc problems and refining them over time, I think actually that community project", "tokens": [50840, 19270, 281, 10346, 2740, 293, 1895, 1760, 552, 670, 565, 11, 286, 519, 767, 300, 1768, 1716, 51176], "temperature": 0.0, "avg_logprob": -0.13448654372116614, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007553794421255589}, {"id": 1082, "seek": 614920, "start": 6165.44, "end": 6170.72, "text": " would help the core knowledge people in that line of research and figuring out, okay, what,", "tokens": [51176, 576, 854, 264, 4965, 3601, 561, 294, 300, 1622, 295, 2132, 293, 15213, 484, 11, 1392, 11, 437, 11, 51440], "temperature": 0.0, "avg_logprob": -0.13448654372116614, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007553794421255589}, {"id": 1083, "seek": 614920, "start": 6170.72, "end": 6175.84, "text": " what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these", "tokens": [51440, 437, 307, 257, 19746, 295, 439, 264, 4965, 3601, 11, 558, 30, 467, 311, 11, 797, 11, 646, 294, 1395, 11, 321, 1143, 281, 818, 613, 51696], "temperature": 0.0, "avg_logprob": -0.13448654372116614, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007553794421255589}, {"id": 1084, "seek": 617584, "start": 6175.84, "end": 6180.4800000000005, "text": " prime thoughts, because we would, we would play these brain teasers all the time. And we realized", "tokens": [50364, 5835, 4598, 11, 570, 321, 576, 11, 321, 576, 862, 613, 3567, 11488, 433, 439, 264, 565, 13, 400, 321, 5334, 50596], "temperature": 0.0, "avg_logprob": -0.11347355462808524, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.033074792474508286}, {"id": 1085, "seek": 617584, "start": 6180.4800000000005, "end": 6185.84, "text": " that there were patterns, right? Like, well, this brain teaser requires the concept of coloring,", "tokens": [50596, 300, 456, 645, 8294, 11, 558, 30, 1743, 11, 731, 11, 341, 3567, 35326, 7029, 264, 3410, 295, 23198, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11347355462808524, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.033074792474508286}, {"id": 1086, "seek": 617584, "start": 6185.84, "end": 6189.6, "text": " like with a red black tree, where you add an additional variable that kind of lets you", "tokens": [50864, 411, 365, 257, 2182, 2211, 4230, 11, 689, 291, 909, 364, 4497, 7006, 300, 733, 295, 6653, 291, 51052], "temperature": 0.0, "avg_logprob": -0.11347355462808524, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.033074792474508286}, {"id": 1087, "seek": 617584, "start": 6190.24, "end": 6195.6, "text": " solve the problem. And if we could really have a nice catalog of, here's all the core knowledge,", "tokens": [51084, 5039, 264, 1154, 13, 400, 498, 321, 727, 534, 362, 257, 1481, 19746, 295, 11, 510, 311, 439, 264, 4965, 3601, 11, 51352], "temperature": 0.0, "avg_logprob": -0.11347355462808524, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.033074792474508286}, {"id": 1088, "seek": 617584, "start": 6195.6, "end": 6200.56, "text": " here's all the like problem solving techniques, I think that would be really powerful. I mean,", "tokens": [51352, 510, 311, 439, 264, 411, 1154, 12606, 7512, 11, 286, 519, 300, 576, 312, 534, 4005, 13, 286, 914, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11347355462808524, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.033074792474508286}, {"id": 1089, "seek": 620056, "start": 6200.56, "end": 6205.4400000000005, "text": " well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about", "tokens": [50364, 731, 11, 321, 733, 295, 362, 300, 13, 407, 341, 3059, 11, 12978, 3550, 285, 330, 11, 750, 1361, 493, 365, 466, 50608], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1090, "seek": 620056, "start": 6205.4400000000005, "end": 6210.240000000001, "text": " six core knowledge systems, right? And that and the arc challenge uses four of them. So", "tokens": [50608, 2309, 4965, 3601, 3652, 11, 558, 30, 400, 300, 293, 264, 10346, 3430, 4960, 1451, 295, 552, 13, 407, 50848], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1091, "seek": 620056, "start": 6210.240000000001, "end": 6216.0, "text": " objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three,", "tokens": [50848, 2657, 1287, 293, 21769, 10649, 11, 472, 11, 9461, 1287, 281, 16429, 18426, 11, 44518, 11, 1045, 11, 51136], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1092, "seek": 620056, "start": 6216.0, "end": 6220.8, "text": " numbers, counting, quantitative comparisons. So the two that weren't in there are places", "tokens": [51136, 3547, 11, 13251, 11, 27778, 33157, 13, 407, 264, 732, 300, 4999, 380, 294, 456, 366, 3190, 51376], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1093, "seek": 620056, "start": 6220.8, "end": 6224.64, "text": " and social partners. Now, the thing is, I think we may discover new ones.", "tokens": [51376, 293, 2093, 4462, 13, 823, 11, 264, 551, 307, 11, 286, 519, 321, 815, 4411, 777, 2306, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1094, "seek": 620056, "start": 6225.200000000001, "end": 6230.4800000000005, "text": " Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it,", "tokens": [51596, 1042, 11, 321, 815, 312, 957, 11, 457, 286, 478, 6100, 300, 321, 630, 382, 731, 382, 945, 6856, 1436, 498, 291, 519, 466, 309, 11, 51860], "temperature": 0.0, "avg_logprob": -0.13953922043985395, "compression_ratio": 1.6, "no_speech_prob": 0.09471069276332855}, {"id": 1095, "seek": 623048, "start": 6230.719999999999, "end": 6236.0, "text": " imagine if you just guessed the classification on ImageNet when you've got 1000 classes,", "tokens": [50376, 3811, 498, 291, 445, 21852, 264, 21538, 322, 29903, 31890, 562, 291, 600, 658, 9714, 5359, 11, 50640], "temperature": 0.0, "avg_logprob": -0.09819676013703042, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.001233680290170014}, {"id": 1096, "seek": 623048, "start": 6236.0, "end": 6241.919999999999, "text": " 20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc,", "tokens": [50640, 945, 4, 576, 312, 2243, 11, 2759, 380, 309, 30, 400, 321, 600, 658, 257, 2531, 2372, 295, 8811, 295, 9608, 322, 10346, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09819676013703042, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.001233680290170014}, {"id": 1097, "seek": 623048, "start": 6241.919999999999, "end": 6247.44, "text": " right? And what's interesting as well is that all of those different tasks that have been created", "tokens": [50936, 558, 30, 400, 437, 311, 1880, 382, 731, 307, 300, 439, 295, 729, 819, 9608, 300, 362, 668, 2942, 51212], "temperature": 0.0, "avg_logprob": -0.09819676013703042, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.001233680290170014}, {"id": 1098, "seek": 623048, "start": 6247.44, "end": 6253.759999999999, "text": " by Francois, they all tie back to just four priors, right? Which means, I don't know whether", "tokens": [51212, 538, 34695, 271, 11, 436, 439, 7582, 646, 281, 445, 1451, 1790, 830, 11, 558, 30, 3013, 1355, 11, 286, 500, 380, 458, 1968, 51528], "temperature": 0.0, "avg_logprob": -0.09819676013703042, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.001233680290170014}, {"id": 1099, "seek": 625376, "start": 6253.76, "end": 6260.400000000001, "text": " it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM.", "tokens": [50364, 309, 311, 48806, 12631, 13, 583, 945, 4, 2544, 534, 665, 337, 445, 17939, 44663, 322, 257, 15816, 44, 13, 50696], "temperature": 0.0, "avg_logprob": -0.09449989000956217, "compression_ratio": 1.6148409893992932, "no_speech_prob": 0.2625524401664734}, {"id": 1100, "seek": 625376, "start": 6260.400000000001, "end": 6264.88, "text": " Yeah, there's, there's two things. So first, I would have thought that if someone,", "tokens": [50696, 865, 11, 456, 311, 11, 456, 311, 732, 721, 13, 407, 700, 11, 286, 576, 362, 1194, 300, 498, 1580, 11, 50920], "temperature": 0.0, "avg_logprob": -0.09449989000956217, "compression_ratio": 1.6148409893992932, "no_speech_prob": 0.2625524401664734}, {"id": 1101, "seek": 625376, "start": 6264.88, "end": 6269.84, "text": " if someone came up with something that solves more than 5%, it's going to be like immediately at", "tokens": [50920, 498, 1580, 1361, 493, 365, 746, 300, 39890, 544, 813, 1025, 8923, 309, 311, 516, 281, 312, 411, 4258, 412, 51168], "temperature": 0.0, "avg_logprob": -0.09449989000956217, "compression_ratio": 1.6148409893992932, "no_speech_prob": 0.2625524401664734}, {"id": 1102, "seek": 625376, "start": 6269.84, "end": 6275.6, "text": " 95%. Like just because they've sort of cracked the problem. And then, you know, there might be", "tokens": [51168, 13420, 6856, 1743, 445, 570, 436, 600, 1333, 295, 25140, 264, 1154, 13, 400, 550, 11, 291, 458, 11, 456, 1062, 312, 51456], "temperature": 0.0, "avg_logprob": -0.09449989000956217, "compression_ratio": 1.6148409893992932, "no_speech_prob": 0.2625524401664734}, {"id": 1103, "seek": 625376, "start": 6275.6, "end": 6280.88, "text": " a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct", "tokens": [51456, 257, 1326, 484, 23646, 13, 583, 291, 458, 11, 498, 286, 576, 2041, 300, 311, 733, 295, 257, 5633, 300, 498, 291, 2045, 264, 3006, 51720], "temperature": 0.0, "avg_logprob": -0.09449989000956217, "compression_ratio": 1.6148409893992932, "no_speech_prob": 0.2625524401664734}, {"id": 1104, "seek": 628088, "start": 6280.88, "end": 6286.32, "text": " solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising.", "tokens": [50364, 3827, 11, 309, 311, 516, 281, 312, 411, 11, 9351, 11, 291, 434, 11, 291, 434, 456, 13, 400, 300, 311, 406, 11, 597, 307, 8830, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09087412697928292, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.02161150611937046}, {"id": 1105, "seek": 628088, "start": 6286.32, "end": 6293.76, "text": " And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I", "tokens": [50636, 400, 264, 661, 551, 307, 11, 286, 11, 286, 500, 380, 11, 286, 500, 380, 841, 309, 311, 8830, 300, 456, 311, 370, 1326, 1790, 830, 13, 708, 286, 51008], "temperature": 0.0, "avg_logprob": -0.09087412697928292, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.02161150611937046}, {"id": 1106, "seek": 628088, "start": 6293.76, "end": 6300.32, "text": " do think is that the space of these priors is still way too large. Like, so if you just think", "tokens": [51008, 360, 519, 307, 300, 264, 1901, 295, 613, 1790, 830, 307, 920, 636, 886, 2416, 13, 1743, 11, 370, 498, 291, 445, 519, 51336], "temperature": 0.0, "avg_logprob": -0.09087412697928292, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.02161150611937046}, {"id": 1107, "seek": 628088, "start": 6300.32, "end": 6308.72, "text": " about something like object, because in, in these arc tasks, there are, I feel so many more priors", "tokens": [51336, 466, 746, 411, 2657, 11, 570, 294, 11, 294, 613, 10346, 9608, 11, 456, 366, 11, 286, 841, 370, 867, 544, 1790, 830, 51756], "temperature": 0.0, "avg_logprob": -0.09087412697928292, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.02161150611937046}, {"id": 1108, "seek": 630872, "start": 6308.8, "end": 6315.280000000001, "text": " than just the core knowledge things. Because so one of them is like, you have the, you have like", "tokens": [50368, 813, 445, 264, 4965, 3601, 721, 13, 1436, 370, 472, 295, 552, 307, 411, 11, 291, 362, 264, 11, 291, 362, 411, 50692], "temperature": 0.0, "avg_logprob": -0.16997783867887747, "compression_ratio": 1.869047619047619, "no_speech_prob": 0.0028436719439923763}, {"id": 1109, "seek": 630872, "start": 6316.240000000001, "end": 6320.96, "text": " this thing, and then you have this thing. And the solution is like, it goes, right? It", "tokens": [50740, 341, 551, 11, 293, 550, 291, 362, 341, 551, 13, 400, 264, 3827, 307, 411, 11, 309, 1709, 11, 558, 30, 467, 50976], "temperature": 0.0, "avg_logprob": -0.16997783867887747, "compression_ratio": 1.869047619047619, "no_speech_prob": 0.0028436719439923763}, {"id": 1110, "seek": 630872, "start": 6320.96, "end": 6326.16, "text": " could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize", "tokens": [50976, 727, 352, 11, 309, 411, 46901, 13, 583, 341, 307, 6618, 13, 865, 13, 583, 11, 457, 411, 264, 1186, 300, 321, 5521, 51236], "temperature": 0.0, "avg_logprob": -0.16997783867887747, "compression_ratio": 1.869047619047619, "no_speech_prob": 0.0028436719439923763}, {"id": 1111, "seek": 630872, "start": 6326.16, "end": 6332.0, "text": " like this is a wall or something, but there is no, there's no, no prior to says like a wall", "tokens": [51236, 411, 341, 307, 257, 2929, 420, 746, 11, 457, 456, 307, 572, 11, 456, 311, 572, 11, 572, 4059, 281, 1619, 411, 257, 2929, 51528], "temperature": 0.0, "avg_logprob": -0.16997783867887747, "compression_ratio": 1.869047619047619, "no_speech_prob": 0.0028436719439923763}, {"id": 1112, "seek": 630872, "start": 6332.0, "end": 6338.4800000000005, "text": " needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact", "tokens": [51528, 2203, 281, 312, 2997, 11, 264, 2929, 727, 312, 411, 604, 11, 291, 458, 11, 604, 1331, 11, 604, 3909, 412, 439, 13, 400, 264, 1186, 51852], "temperature": 0.0, "avg_logprob": -0.16997783867887747, "compression_ratio": 1.869047619047619, "no_speech_prob": 0.0028436719439923763}, {"id": 1113, "seek": 633848, "start": 6338.48, "end": 6344.959999999999, "text": " that this is much more core knowledge, right? Like in, you know, we build stuff out of straight", "tokens": [50364, 300, 341, 307, 709, 544, 4965, 3601, 11, 558, 30, 1743, 294, 11, 291, 458, 11, 321, 1322, 1507, 484, 295, 2997, 50688], "temperature": 0.0, "avg_logprob": -0.07750653850939847, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0003250172012485564}, {"id": 1114, "seek": 633848, "start": 6344.959999999999, "end": 6349.36, "text": " walls. And I think, I think I agree with you, which is I think, I think what you're getting at,", "tokens": [50688, 7920, 13, 400, 286, 519, 11, 286, 519, 286, 3986, 365, 291, 11, 597, 307, 286, 519, 11, 286, 519, 437, 291, 434, 1242, 412, 11, 50908], "temperature": 0.0, "avg_logprob": -0.07750653850939847, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0003250172012485564}, {"id": 1115, "seek": 633848, "start": 6349.36, "end": 6353.36, "text": " correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified", "tokens": [50908, 3006, 385, 498, 286, 478, 2085, 11, 457, 309, 311, 300, 264, 636, 294, 597, 264, 4965, 3601, 307, 733, 295, 22206, 51108], "temperature": 0.0, "avg_logprob": -0.07750653850939847, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0003250172012485564}, {"id": 1116, "seek": 633848, "start": 6353.36, "end": 6359.04, "text": " right now is vague, right? There's a vagueness to it. And I think if we actually start to try and", "tokens": [51108, 558, 586, 307, 24247, 11, 558, 30, 821, 311, 257, 13501, 7801, 442, 281, 309, 13, 400, 286, 519, 498, 321, 767, 722, 281, 853, 293, 51392], "temperature": 0.0, "avg_logprob": -0.07750653850939847, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0003250172012485564}, {"id": 1117, "seek": 633848, "start": 6359.04, "end": 6364.719999999999, "text": " codify that more and some type of a mathematical language, Tim, I think it's going to expand", "tokens": [51392, 17656, 2505, 300, 544, 293, 512, 2010, 295, 257, 18894, 2856, 11, 7172, 11, 286, 519, 309, 311, 516, 281, 5268, 51676], "temperature": 0.0, "avg_logprob": -0.07750653850939847, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0003250172012485564}, {"id": 1118, "seek": 636472, "start": 6364.72, "end": 6369.52, "text": " like the scope of that, we're going to end up with more core knowledge concepts really than,", "tokens": [50364, 411, 264, 11923, 295, 300, 11, 321, 434, 516, 281, 917, 493, 365, 544, 4965, 3601, 10392, 534, 813, 11, 50604], "temperature": 0.0, "avg_logprob": -0.14267541711980647, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.007812267169356346}, {"id": 1119, "seek": 636472, "start": 6369.52, "end": 6372.88, "text": " than just six, we'll need to make them finer grained. And I'm really excited,", "tokens": [50604, 813, 445, 2309, 11, 321, 603, 643, 281, 652, 552, 39130, 1295, 2001, 13, 400, 286, 478, 534, 2919, 11, 50772], "temperature": 0.0, "avg_logprob": -0.14267541711980647, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.007812267169356346}, {"id": 1120, "seek": 636472, "start": 6373.4400000000005, "end": 6378.320000000001, "text": " you know, to see that develop because this has been for me a long wonder, right, which is", "tokens": [50800, 291, 458, 11, 281, 536, 300, 1499, 570, 341, 575, 668, 337, 385, 257, 938, 2441, 11, 558, 11, 597, 307, 51044], "temperature": 0.0, "avg_logprob": -0.14267541711980647, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.007812267169356346}, {"id": 1121, "seek": 636472, "start": 6378.96, "end": 6385.76, "text": " what are the in, in a rigorously defined way? What are these core concepts, these core bits", "tokens": [51076, 437, 366, 264, 294, 11, 294, 257, 42191, 5098, 7642, 636, 30, 708, 366, 613, 4965, 10392, 11, 613, 4965, 9239, 51416], "temperature": 0.0, "avg_logprob": -0.14267541711980647, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.007812267169356346}, {"id": 1122, "seek": 636472, "start": 6385.76, "end": 6390.88, "text": " of knowledge that make human cognition so powerful? Yeah. And there's also,", "tokens": [51416, 295, 3601, 300, 652, 1952, 46905, 370, 4005, 30, 865, 13, 400, 456, 311, 611, 11, 51672], "temperature": 0.0, "avg_logprob": -0.14267541711980647, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.007812267169356346}, {"id": 1123, "seek": 639088, "start": 6391.4400000000005, "end": 6396.400000000001, "text": " because Yannick made the point about brittleness, right, even in topological space, you still have", "tokens": [50392, 570, 398, 969, 618, 1027, 264, 935, 466, 738, 593, 45887, 11, 558, 11, 754, 294, 1192, 4383, 1901, 11, 291, 920, 362, 50640], "temperature": 0.0, "avg_logprob": -0.15263770875476657, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.08130451291799545}, {"id": 1124, "seek": 639088, "start": 6396.400000000001, "end": 6402.88, "text": " brittleness, but, but the solution was to create powerful abstractions, right? But how would that", "tokens": [50640, 738, 593, 45887, 11, 457, 11, 457, 264, 3827, 390, 281, 1884, 4005, 12649, 626, 11, 558, 30, 583, 577, 576, 300, 50964], "temperature": 0.0, "avg_logprob": -0.15263770875476657, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.08130451291799545}, {"id": 1125, "seek": 639088, "start": 6402.88, "end": 6408.4800000000005, "text": " work with the priors? Because if you think about it, you can recombine many of the priors to come", "tokens": [50964, 589, 365, 264, 1790, 830, 30, 1436, 498, 291, 519, 466, 309, 11, 291, 393, 850, 3548, 533, 867, 295, 264, 1790, 830, 281, 808, 51244], "temperature": 0.0, "avg_logprob": -0.15263770875476657, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.08130451291799545}, {"id": 1126, "seek": 639088, "start": 6408.4800000000005, "end": 6412.72, "text": " up with powerful abstractions. And you might find that it doesn't actually filter down to, to that", "tokens": [51244, 493, 365, 4005, 12649, 626, 13, 400, 291, 1062, 915, 300, 309, 1177, 380, 767, 6608, 760, 281, 11, 281, 300, 51456], "temperature": 0.0, "avg_logprob": -0.15263770875476657, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.08130451291799545}, {"id": 1127, "seek": 639088, "start": 6412.72, "end": 6417.4400000000005, "text": " many. But the question is, how many things are there? Remember when we spoke to Walid Saber,", "tokens": [51456, 867, 13, 583, 264, 1168, 307, 11, 577, 867, 721, 366, 456, 30, 5459, 562, 321, 7179, 281, 9707, 327, 13915, 260, 11, 51692], "temperature": 0.0, "avg_logprob": -0.15263770875476657, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.08130451291799545}, {"id": 1128, "seek": 641744, "start": 6417.44, "end": 6421.759999999999, "text": " and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them", "tokens": [50364, 293, 415, 390, 1417, 466, 11, 415, 311, 658, 552, 4079, 294, 257, 25584, 9341, 11, 291, 445, 2759, 380, 976, 552, 50580], "temperature": 0.0, "avg_logprob": -0.12210370768671451, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002714850939810276}, {"id": 1129, "seek": 641744, "start": 6421.759999999999, "end": 6426.799999999999, "text": " to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained", "tokens": [50580, 281, 505, 13, 583, 291, 458, 11, 644, 295, 11, 644, 295, 983, 11, 983, 286, 3986, 365, 398, 969, 618, 300, 436, 434, 39130, 1295, 2001, 50832], "temperature": 0.0, "avg_logprob": -0.12210370768671451, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002714850939810276}, {"id": 1130, "seek": 641744, "start": 6426.799999999999, "end": 6430.719999999999, "text": " concepts are more important. I think probably stems from a lot of the computer science", "tokens": [50832, 10392, 366, 544, 1021, 13, 286, 519, 1391, 27600, 490, 257, 688, 295, 264, 3820, 3497, 51028], "temperature": 0.0, "avg_logprob": -0.12210370768671451, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002714850939810276}, {"id": 1131, "seek": 641744, "start": 6431.839999999999, "end": 6437.04, "text": " education that I had where, where when we were devising algorithms to do one thing or another,", "tokens": [51084, 3309, 300, 286, 632, 689, 11, 689, 562, 321, 645, 1905, 3436, 14642, 281, 360, 472, 551, 420, 1071, 11, 51344], "temperature": 0.0, "avg_logprob": -0.12210370768671451, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002714850939810276}, {"id": 1132, "seek": 641744, "start": 6437.04, "end": 6443.28, "text": " you get these little hints that kind of like clever bits of core knowledge that was used to", "tokens": [51344, 291, 483, 613, 707, 27271, 300, 733, 295, 411, 13494, 9239, 295, 4965, 3601, 300, 390, 1143, 281, 51656], "temperature": 0.0, "avg_logprob": -0.12210370768671451, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002714850939810276}, {"id": 1133, "seek": 644328, "start": 6443.28, "end": 6447.44, "text": " solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going", "tokens": [50364, 5039, 341, 1154, 13, 1743, 562, 291, 2979, 1702, 82, 477, 11, 293, 309, 311, 411, 11, 291, 458, 437, 11, 411, 11, 286, 478, 445, 516, 50572], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1134, "seek": 644328, "start": 6447.44, "end": 6451.84, "text": " to randomly choose an element. Well, random selection is kind of a bit of core knowledge.", "tokens": [50572, 281, 16979, 2826, 364, 4478, 13, 1042, 11, 4974, 9450, 307, 733, 295, 257, 857, 295, 4965, 3601, 13, 50792], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1135, "seek": 644328, "start": 6451.84, "end": 6455.84, "text": " And then I'm just going to partition by that, and then repeat, you know, or things like,", "tokens": [50792, 400, 550, 286, 478, 445, 516, 281, 24808, 538, 300, 11, 293, 550, 7149, 11, 291, 458, 11, 420, 721, 411, 11, 50992], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1136, "seek": 644328, "start": 6455.84, "end": 6460.0, "text": " I don't know how to balance this tree the way it is. But if I color stuff, like add in red,", "tokens": [50992, 286, 500, 380, 458, 577, 281, 4772, 341, 4230, 264, 636, 309, 307, 13, 583, 498, 286, 2017, 1507, 11, 411, 909, 294, 2182, 11, 51200], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1137, "seek": 644328, "start": 6460.0, "end": 6465.04, "text": " black nodes, I can now overlay a computation that, you know, so there's all these little bits,", "tokens": [51200, 2211, 13891, 11, 286, 393, 586, 31741, 257, 24903, 300, 11, 291, 458, 11, 370, 456, 311, 439, 613, 707, 9239, 11, 51452], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1138, "seek": 644328, "start": 6465.04, "end": 6469.599999999999, "text": " you know, that's what's fascinating about computer programming is it, is it really strikes at the", "tokens": [51452, 291, 458, 11, 300, 311, 437, 311, 10343, 466, 3820, 9410, 307, 309, 11, 307, 309, 534, 16750, 412, 264, 51680], "temperature": 0.0, "avg_logprob": -0.11495745102031119, "compression_ratio": 1.7711598746081505, "no_speech_prob": 0.015423715114593506}, {"id": 1139, "seek": 646960, "start": 6469.6, "end": 6474.64, "text": " heart of this cognition and this core knowledge and how to read, and you have to do it rigorously,", "tokens": [50364, 1917, 295, 341, 46905, 293, 341, 4965, 3601, 293, 577, 281, 1401, 11, 293, 291, 362, 281, 360, 309, 42191, 5098, 11, 50616], "temperature": 0.0, "avg_logprob": -0.132559594307237, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.014502604492008686}, {"id": 1140, "seek": 646960, "start": 6474.64, "end": 6479.120000000001, "text": " right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to", "tokens": [50616, 558, 30, 509, 393, 380, 445, 13501, 48863, 352, 11, 876, 11, 291, 458, 11, 445, 733, 295, 1333, 309, 293, 22183, 552, 13, 509, 658, 281, 50840], "temperature": 0.0, "avg_logprob": -0.132559594307237, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.014502604492008686}, {"id": 1141, "seek": 646960, "start": 6479.120000000001, "end": 6484.56, "text": " define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a", "tokens": [50840, 6964, 411, 437, 300, 1355, 13, 400, 309, 311, 10343, 8546, 9410, 13, 286, 478, 1009, 11, 286, 478, 1009, 257, 51112], "temperature": 0.0, "avg_logprob": -0.132559594307237, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.014502604492008686}, {"id": 1142, "seek": 646960, "start": 6484.56, "end": 6491.6, "text": " bit amazed by people who have just kind of sort of learned programming, because it's, it's almost", "tokens": [51112, 857, 20507, 538, 561, 567, 362, 445, 733, 295, 1333, 295, 3264, 9410, 11, 570, 309, 311, 11, 309, 311, 1920, 51464], "temperature": 0.0, "avg_logprob": -0.132559594307237, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.014502604492008686}, {"id": 1143, "seek": 646960, "start": 6491.6, "end": 6497.52, "text": " like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve", "tokens": [51464, 411, 257, 819, 1002, 294, 300, 436, 603, 11, 436, 603, 11, 436, 603, 360, 11, 309, 311, 411, 11, 876, 11, 1392, 11, 286, 643, 281, 5039, 51760], "temperature": 0.0, "avg_logprob": -0.132559594307237, "compression_ratio": 1.7357142857142858, "no_speech_prob": 0.014502604492008686}, {"id": 1144, "seek": 649752, "start": 6497.52, "end": 6503.68, "text": " this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not", "tokens": [50364, 341, 1154, 13, 1664, 286, 393, 286, 5055, 9163, 341, 3089, 510, 30, 400, 309, 1985, 411, 945, 4, 295, 264, 565, 11, 457, 406, 50672], "temperature": 0.0, "avg_logprob": -0.07860897481441498, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.009125164709985256}, {"id": 1145, "seek": 649752, "start": 6503.68, "end": 6509.120000000001, "text": " fully. Yeah. But then on the other side of the coin to that. So when I was working in,", "tokens": [50672, 4498, 13, 865, 13, 583, 550, 322, 264, 661, 1252, 295, 264, 11464, 281, 300, 13, 407, 562, 286, 390, 1364, 294, 11, 50944], "temperature": 0.0, "avg_logprob": -0.07860897481441498, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.009125164709985256}, {"id": 1146, "seek": 649752, "start": 6510.8, "end": 6515.52, "text": " you know, quantitative trading, right, we had these these massive globally integrated,", "tokens": [51028, 291, 458, 11, 27778, 9529, 11, 558, 11, 321, 632, 613, 613, 5994, 18958, 10919, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07860897481441498, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.009125164709985256}, {"id": 1147, "seek": 649752, "start": 6515.52, "end": 6521.4400000000005, "text": " automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks,", "tokens": [51264, 18473, 9529, 3652, 13, 400, 286, 914, 11, 512, 295, 264, 18265, 11, 286, 500, 380, 528, 281, 818, 552, 33617, 11, 51560], "temperature": 0.0, "avg_logprob": -0.07860897481441498, "compression_ratio": 1.5146443514644352, "no_speech_prob": 0.009125164709985256}, {"id": 1148, "seek": 652144, "start": 6521.44, "end": 6528.639999999999, "text": " but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually", "tokens": [50364, 457, 512, 295, 264, 18265, 1333, 295, 2522, 3711, 8213, 11787, 17330, 33617, 11, 2035, 300, 767, 50724], "temperature": 0.0, "avg_logprob": -0.10237648410181846, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.013636123389005661}, {"id": 1149, "seek": 652144, "start": 6528.639999999999, "end": 6533.28, "text": " work in reality. You know, you sit there and you look at them and go, when I first went in there,", "tokens": [50724, 589, 294, 4103, 13, 509, 458, 11, 291, 1394, 456, 293, 291, 574, 412, 552, 293, 352, 11, 562, 286, 700, 1437, 294, 456, 11, 50956], "temperature": 0.0, "avg_logprob": -0.10237648410181846, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.013636123389005661}, {"id": 1150, "seek": 652144, "start": 6533.28, "end": 6537.2, "text": " as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to", "tokens": [50956, 382, 4451, 484, 295, 28937, 11, 293, 286, 1409, 2577, 721, 411, 11, 876, 11, 341, 307, 12426, 11, 411, 11, 286, 478, 516, 281, 51152], "temperature": 0.0, "avg_logprob": -0.10237648410181846, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.013636123389005661}, {"id": 1151, "seek": 652144, "start": 6537.2, "end": 6541.919999999999, "text": " figure out some continuous equation that, you know, fits this piecewise linear thing, and it's", "tokens": [51152, 2573, 484, 512, 10957, 5367, 300, 11, 291, 458, 11, 9001, 341, 2522, 3711, 8213, 551, 11, 293, 309, 311, 51388], "temperature": 0.0, "avg_logprob": -0.10237648410181846, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.013636123389005661}, {"id": 1152, "seek": 652144, "start": 6541.919999999999, "end": 6546.96, "text": " going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better.", "tokens": [51388, 516, 281, 360, 1101, 13, 12172, 11, 411, 309, 994, 380, 360, 1101, 13, 286, 2809, 380, 915, 604, 10957, 551, 281, 360, 1101, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10237648410181846, "compression_ratio": 1.830827067669173, "no_speech_prob": 0.013636123389005661}, {"id": 1153, "seek": 654696, "start": 6546.96, "end": 6552.24, "text": " It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're", "tokens": [50364, 467, 311, 411, 11, 291, 458, 11, 3956, 1689, 766, 11, 558, 11, 307, 341, 341, 2522, 3711, 8213, 551, 13, 400, 11, 293, 291, 434, 50628], "temperature": 0.0, "avg_logprob": -0.18192842271592882, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.012811564840376377}, {"id": 1154, "seek": 654696, "start": 6552.24, "end": 6556.8, "text": " like, Oh, that's, well, there should be some continuous like thing in there. Like all these", "tokens": [50628, 411, 11, 876, 11, 300, 311, 11, 731, 11, 456, 820, 312, 512, 10957, 411, 551, 294, 456, 13, 1743, 439, 613, 50856], "temperature": 0.0, "avg_logprob": -0.18192842271592882, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.012811564840376377}, {"id": 1155, "seek": 654696, "start": 6556.8, "end": 6562.88, "text": " weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete", "tokens": [50856, 3657, 11, 291, 458, 11, 2522, 3711, 27706, 11, 411, 733, 295, 13051, 721, 1296, 10957, 293, 27706, 51160], "temperature": 0.0, "avg_logprob": -0.18192842271592882, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.012811564840376377}, {"id": 1156, "seek": 654696, "start": 6563.44, "end": 6567.52, "text": " work. And, and that's weird. It was weird to me and still weird to me.", "tokens": [51188, 589, 13, 400, 11, 293, 300, 311, 3657, 13, 467, 390, 3657, 281, 385, 293, 920, 3657, 281, 385, 13, 51392], "temperature": 0.0, "avg_logprob": -0.18192842271592882, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.012811564840376377}, {"id": 1157, "seek": 654696, "start": 6568.08, "end": 6575.04, "text": " Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay.", "tokens": [51420, 14711, 13, 583, 286, 600, 658, 281, 584, 11, 370, 452, 2135, 1045, 747, 7388, 490, 1160, 401, 320, 965, 13, 286, 534, 959, 1160, 401, 320, 13, 51768], "temperature": 0.0, "avg_logprob": -0.18192842271592882, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.012811564840376377}, {"id": 1158, "seek": 657504, "start": 6575.04, "end": 6582.48, "text": " So one, intelligence is generalization. I think that's super powerful to his idea that deep learning", "tokens": [50364, 407, 472, 11, 7599, 307, 2674, 2144, 13, 286, 519, 300, 311, 1687, 4005, 281, 702, 1558, 300, 2452, 2539, 50736], "temperature": 0.0, "avg_logprob": -0.10762435065375434, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0016351576196029782}, {"id": 1159, "seek": 657504, "start": 6582.48, "end": 6589.2, "text": " is really good for value centric abstraction. And because of the manifold hypothesis, lots of", "tokens": [50736, 307, 534, 665, 337, 2158, 1489, 1341, 37765, 13, 400, 570, 295, 264, 47138, 17291, 11, 3195, 295, 51072], "temperature": 0.0, "avg_logprob": -0.10762435065375434, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0016351576196029782}, {"id": 1160, "seek": 657504, "start": 6589.2, "end": 6595.2, "text": " natural data has some kind of manifold, which you can interpolate on, but lots of discrete", "tokens": [51072, 3303, 1412, 575, 512, 733, 295, 47138, 11, 597, 291, 393, 44902, 473, 322, 11, 457, 3195, 295, 27706, 51372], "temperature": 0.0, "avg_logprob": -0.10762435065375434, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0016351576196029782}, {"id": 1161, "seek": 657504, "start": 6595.2, "end": 6600.88, "text": " problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just", "tokens": [51372, 2740, 360, 406, 362, 300, 13, 1779, 13, 400, 452, 1575, 390, 1953, 11, 1042, 11, 775, 300, 914, 300, 321, 393, 445, 51656], "temperature": 0.0, "avg_logprob": -0.10762435065375434, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0016351576196029782}, {"id": 1162, "seek": 660088, "start": 6600.88, "end": 6605.52, "text": " use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But", "tokens": [50364, 764, 11, 570, 309, 311, 570, 295, 34520, 35, 11, 291, 393, 380, 754, 1466, 264, 47138, 11, 754, 498, 309, 630, 2514, 13, 583, 50596], "temperature": 0.0, "avg_logprob": -0.07884023041851752, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.021351324394345284}, {"id": 1163, "seek": 660088, "start": 6605.52, "end": 6609.36, "text": " he's saying that it doesn't exist for discrete problems. The manifold might be there or it might", "tokens": [50596, 415, 311, 1566, 300, 309, 1177, 380, 2514, 337, 27706, 2740, 13, 440, 47138, 1062, 312, 456, 420, 309, 1062, 50788], "temperature": 0.0, "avg_logprob": -0.07884023041851752, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.021351324394345284}, {"id": 1164, "seek": 660088, "start": 6609.36, "end": 6613.84, "text": " only be there in parts. So that was interesting. And then the third thing that fascinated me about", "tokens": [50788, 787, 312, 456, 294, 3166, 13, 407, 300, 390, 1880, 13, 400, 550, 264, 2636, 551, 300, 24597, 385, 466, 51012], "temperature": 0.0, "avg_logprob": -0.07884023041851752, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.021351324394345284}, {"id": 1165, "seek": 660088, "start": 6613.84, "end": 6618.88, "text": " Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be", "tokens": [51012, 1160, 401, 320, 307, 415, 6686, 466, 613, 3652, 293, 44641, 2761, 294, 3652, 13, 400, 321, 4659, 380, 312, 51264], "temperature": 0.0, "avg_logprob": -0.07884023041851752, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.021351324394345284}, {"id": 1166, "seek": 660088, "start": 6618.88, "end": 6624.16, "text": " thinking about individual brains, we should be thinking about the externalization of knowledge.", "tokens": [51264, 1953, 466, 2609, 15442, 11, 321, 820, 312, 1953, 466, 264, 8320, 2144, 295, 3601, 13, 51528], "temperature": 0.0, "avg_logprob": -0.07884023041851752, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.021351324394345284}, {"id": 1167, "seek": 662416, "start": 6624.8, "end": 6631.76, "text": " Yeah. And the way he described this, what he thinks like a hybrid system should look like,", "tokens": [50396, 865, 13, 400, 264, 636, 415, 7619, 341, 11, 437, 415, 7309, 411, 257, 13051, 1185, 820, 574, 411, 11, 50744], "temperature": 0.0, "avg_logprob": -0.1060919918856778, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.10965652763843536}, {"id": 1168, "seek": 662416, "start": 6631.76, "end": 6639.04, "text": " which is sort of you have a perception layer and then a discrete search layer. And then on top of", "tokens": [50744, 597, 307, 1333, 295, 291, 362, 257, 12860, 4583, 293, 550, 257, 27706, 3164, 4583, 13, 400, 550, 322, 1192, 295, 51108], "temperature": 0.0, "avg_logprob": -0.1060919918856778, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.10965652763843536}, {"id": 1169, "seek": 662416, "start": 6639.04, "end": 6646.4, "text": " that kind of another fuzzy layer that guides the search that can be deep learning again. And I", "tokens": [51108, 300, 733, 295, 1071, 34710, 4583, 300, 17007, 264, 3164, 300, 393, 312, 2452, 2539, 797, 13, 400, 286, 51476], "temperature": 0.0, "avg_logprob": -0.1060919918856778, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.10965652763843536}, {"id": 1170, "seek": 662416, "start": 6646.4, "end": 6652.96, "text": " think we're like halfway there on the top with the top very much looks like alpha zero, right,", "tokens": [51476, 519, 321, 434, 411, 15461, 456, 322, 264, 1192, 365, 264, 1192, 588, 709, 1542, 411, 8961, 4018, 11, 558, 11, 51804], "temperature": 0.0, "avg_logprob": -0.1060919918856778, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.10965652763843536}, {"id": 1171, "seek": 665296, "start": 6652.96, "end": 6658.72, "text": " which is kind of a discrete search that is guided by a neural networks. And the bottom", "tokens": [50364, 597, 307, 733, 295, 257, 27706, 3164, 300, 307, 19663, 538, 257, 18161, 9590, 13, 400, 264, 2767, 50652], "temperature": 0.0, "avg_logprob": -0.1297326740465666, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0015725758858025074}, {"id": 1172, "seek": 665296, "start": 6658.72, "end": 6664.96, "text": " layer we have to because that's just our, you know, regular neural networks. I think we have", "tokens": [50652, 4583, 321, 362, 281, 570, 300, 311, 445, 527, 11, 291, 458, 11, 3890, 18161, 9590, 13, 286, 519, 321, 362, 50964], "temperature": 0.0, "avg_logprob": -0.1297326740465666, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0015725758858025074}, {"id": 1173, "seek": 665296, "start": 6664.96, "end": 6671.76, "text": " big trouble in how to connect the two in a in a single unified way such that we can learn them,", "tokens": [50964, 955, 5253, 294, 577, 281, 1745, 264, 732, 294, 257, 294, 257, 2167, 26787, 636, 1270, 300, 321, 393, 1466, 552, 11, 51304], "temperature": 0.0, "avg_logprob": -0.1297326740465666, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0015725758858025074}, {"id": 1174, "seek": 665296, "start": 6671.76, "end": 6678.64, "text": " right? Because the best we can do right now is is right, we can, we can plug a pre train network", "tokens": [51304, 558, 30, 1436, 264, 1151, 321, 393, 360, 558, 586, 307, 307, 558, 11, 321, 393, 11, 321, 393, 5452, 257, 659, 3847, 3209, 51648], "temperature": 0.0, "avg_logprob": -0.1297326740465666, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0015725758858025074}, {"id": 1175, "seek": 667864, "start": 6679.6, "end": 6684.64, "text": " onto alpha zero or something like this, but we can't really, we don't really have it figured", "tokens": [50412, 3911, 8961, 4018, 420, 746, 411, 341, 11, 457, 321, 393, 380, 534, 11, 321, 500, 380, 534, 362, 309, 8932, 50664], "temperature": 0.0, "avg_logprob": -0.11460434457530146, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.005384429357945919}, {"id": 1176, "seek": 667864, "start": 6684.64, "end": 6689.360000000001, "text": " out yet how to connect the all the stuff. A good example of that is the neural Turing machines,", "tokens": [50664, 484, 1939, 577, 281, 1745, 264, 439, 264, 1507, 13, 316, 665, 1365, 295, 300, 307, 264, 18161, 314, 1345, 8379, 11, 50900], "temperature": 0.0, "avg_logprob": -0.11460434457530146, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.005384429357945919}, {"id": 1177, "seek": 667864, "start": 6689.360000000001, "end": 6694.72, "text": " like how it's so hard to to optimize them, right? And I think not only do we need these kind of", "tokens": [50900, 411, 577, 309, 311, 370, 1152, 281, 281, 19719, 552, 11, 558, 30, 400, 286, 519, 406, 787, 360, 321, 643, 613, 733, 295, 51168], "temperature": 0.0, "avg_logprob": -0.11460434457530146, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.005384429357945919}, {"id": 1178, "seek": 667864, "start": 6694.72, "end": 6700.72, "text": " three components that that nicely integrate and are optimal, we have to be able to modularize", "tokens": [51168, 1045, 6677, 300, 300, 9594, 13365, 293, 366, 16252, 11, 321, 362, 281, 312, 1075, 281, 31111, 1125, 51468], "temperature": 0.0, "avg_logprob": -0.11460434457530146, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.005384429357945919}, {"id": 1179, "seek": 667864, "start": 6700.72, "end": 6707.360000000001, "text": " and componentize and connect multiple instances of those things together. And some, you know,", "tokens": [51468, 293, 6542, 1125, 293, 1745, 3866, 14519, 295, 729, 721, 1214, 13, 400, 512, 11, 291, 458, 11, 51800], "temperature": 0.0, "avg_logprob": -0.11460434457530146, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.005384429357945919}, {"id": 1180, "seek": 670736, "start": 6707.36, "end": 6712.32, "text": " weird topological network to really achieve like kind of the capsule network kind of vision", "tokens": [50364, 3657, 1192, 4383, 3209, 281, 534, 4584, 411, 733, 295, 264, 29247, 3209, 733, 295, 5201, 50612], "temperature": 0.0, "avg_logprob": -0.10889697074890137, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.003073095576837659}, {"id": 1181, "seek": 670736, "start": 6712.32, "end": 6717.839999999999, "text": " where each of the capsules is maybe one of these units. And then they're part of it's like a fractal,", "tokens": [50612, 689, 1184, 295, 264, 13855, 3473, 307, 1310, 472, 295, 613, 6815, 13, 400, 550, 436, 434, 644, 295, 309, 311, 411, 257, 17948, 304, 11, 50888], "temperature": 0.0, "avg_logprob": -0.10889697074890137, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.003073095576837659}, {"id": 1182, "seek": 670736, "start": 6717.839999999999, "end": 6722.639999999999, "text": " you know, kind of these fractal layers of those pieces. I don't know whether I was", "tokens": [50888, 291, 458, 11, 733, 295, 613, 17948, 304, 7914, 295, 729, 3755, 13, 286, 500, 380, 458, 1968, 286, 390, 51128], "temperature": 0.0, "avg_logprob": -0.10889697074890137, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.003073095576837659}, {"id": 1183, "seek": 670736, "start": 6722.639999999999, "end": 6728.88, "text": " misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has", "tokens": [51128, 29227, 291, 949, 11, 4956, 716, 11, 457, 365, 264, 8961, 4018, 551, 11, 452, 30698, 307, 300, 575, 51440], "temperature": 0.0, "avg_logprob": -0.10889697074890137, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.003073095576837659}, {"id": 1184, "seek": 670736, "start": 6728.88, "end": 6733.839999999999, "text": " been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning", "tokens": [51440, 668, 1596, 1152, 34874, 13, 407, 291, 434, 11, 291, 434, 10808, 807, 11, 718, 311, 584, 11, 257, 3840, 295, 2452, 2539, 51688], "temperature": 0.0, "avg_logprob": -0.10889697074890137, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.003073095576837659}, {"id": 1185, "seek": 673384, "start": 6733.84, "end": 6738.88, "text": " models and the way you search is quite opinionated. What you're always talking about is have a very", "tokens": [50364, 5245, 293, 264, 636, 291, 3164, 307, 1596, 4800, 770, 13, 708, 291, 434, 1009, 1417, 466, 307, 362, 257, 588, 50616], "temperature": 0.0, "avg_logprob": -0.12334199304933902, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.026360495015978813}, {"id": 1186, "seek": 673384, "start": 6738.88, "end": 6746.08, "text": " basic DSL and in that topological space, you just search and you start to modularize and you start", "tokens": [50616, 3875, 15816, 43, 293, 294, 300, 1192, 4383, 1901, 11, 291, 445, 3164, 293, 291, 722, 281, 31111, 1125, 293, 291, 722, 50976], "temperature": 0.0, "avg_logprob": -0.12334199304933902, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.026360495015978813}, {"id": 1187, "seek": 673384, "start": 6746.08, "end": 6749.92, "text": " to create functions and abstractions. And you have from a software engineering point of view,", "tokens": [50976, 281, 1884, 6828, 293, 12649, 626, 13, 400, 291, 362, 490, 257, 4722, 7043, 935, 295, 1910, 11, 51168], "temperature": 0.0, "avg_logprob": -0.12334199304933902, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.026360495015978813}, {"id": 1188, "seek": 673384, "start": 6749.92, "end": 6755.76, "text": " you start to build a library of functions that have been written in code that do certain things,", "tokens": [51168, 291, 722, 281, 1322, 257, 6405, 295, 6828, 300, 362, 668, 3720, 294, 3089, 300, 360, 1629, 721, 11, 51460], "temperature": 0.0, "avg_logprob": -0.12334199304933902, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.026360495015978813}, {"id": 1189, "seek": 673384, "start": 6755.76, "end": 6759.04, "text": " right? And that's that's different, isn't it to alpha zero?", "tokens": [51460, 558, 30, 400, 300, 311, 300, 311, 819, 11, 1943, 380, 309, 281, 8961, 4018, 30, 51624], "temperature": 0.0, "avg_logprob": -0.12334199304933902, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.026360495015978813}, {"id": 1190, "seek": 675904, "start": 6760.0, "end": 6767.2, "text": " Well, the alpha zero is made specifically to search over actions in some kind of RL space.", "tokens": [50412, 1042, 11, 264, 8961, 4018, 307, 1027, 4682, 281, 3164, 670, 5909, 294, 512, 733, 295, 497, 43, 1901, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1367652548684014, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.0017442943062633276}, {"id": 1191, "seek": 675904, "start": 6768.0, "end": 6775.36, "text": " Yeah, I mean, what he describes is certainly much more abstract in that you search over applications", "tokens": [50812, 865, 11, 286, 914, 11, 437, 415, 15626, 307, 3297, 709, 544, 12649, 294, 300, 291, 3164, 670, 5821, 51180], "temperature": 0.0, "avg_logprob": -0.1367652548684014, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.0017442943062633276}, {"id": 1192, "seek": 675904, "start": 6775.36, "end": 6784.16, "text": " of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by", "tokens": [51180, 295, 264, 15816, 43, 13, 400, 264, 15816, 43, 2564, 307, 406, 307, 411, 257, 43276, 488, 15816, 43, 300, 294, 2564, 307, 7619, 538, 51620], "temperature": 0.0, "avg_logprob": -0.1367652548684014, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.0017442943062633276}, {"id": 1193, "seek": 678416, "start": 6784.16, "end": 6790.08, "text": " these lower level neural networks. But I mean, in S, I just, that just came to my mind when he", "tokens": [50364, 613, 3126, 1496, 18161, 9590, 13, 583, 286, 914, 11, 294, 318, 11, 286, 445, 11, 300, 445, 1361, 281, 452, 1575, 562, 415, 50660], "temperature": 0.0, "avg_logprob": -0.15078539061314852, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.024039622396230698}, {"id": 1194, "seek": 678416, "start": 6790.08, "end": 6795.84, "text": " described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because", "tokens": [50660, 7619, 264, 1185, 11, 286, 478, 411, 11, 1954, 11, 264, 1192, 644, 1542, 588, 709, 411, 11, 291, 458, 11, 8961, 4018, 11, 570, 50948], "temperature": 0.0, "avg_logprob": -0.15078539061314852, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.024039622396230698}, {"id": 1195, "seek": 678416, "start": 6795.84, "end": 6802.48, "text": " that's essentially neural network guided search is something we, we already, already do though.", "tokens": [50948, 300, 311, 4476, 18161, 3209, 19663, 3164, 307, 746, 321, 11, 321, 1217, 11, 1217, 360, 1673, 13, 51280], "temperature": 0.0, "avg_logprob": -0.15078539061314852, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.024039622396230698}, {"id": 1196, "seek": 678416, "start": 6803.28, "end": 6810.88, "text": " Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because", "tokens": [51320, 865, 11, 286, 11, 286, 519, 11, 286, 478, 406, 988, 13, 286, 519, 445, 300, 264, 4103, 307, 754, 257, 857, 544, 34710, 11, 570, 51700], "temperature": 0.0, "avg_logprob": -0.15078539061314852, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.024039622396230698}, {"id": 1197, "seek": 681088, "start": 6811.6, "end": 6817.52, "text": " what you do as a, as a human, there's also some part of hierarchical system to it,", "tokens": [50400, 437, 291, 360, 382, 257, 11, 382, 257, 1952, 11, 456, 311, 611, 512, 644, 295, 35250, 804, 1185, 281, 309, 11, 50696], "temperature": 0.0, "avg_logprob": -0.11031350465578453, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.005383538082242012}, {"id": 1198, "seek": 681088, "start": 6817.52, "end": 6823.92, "text": " in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like,", "tokens": [50696, 294, 300, 291, 393, 11, 291, 393, 360, 341, 11, 457, 291, 393, 360, 309, 35250, 984, 11, 558, 30, 509, 393, 11, 291, 393, 312, 411, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11031350465578453, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.005383538082242012}, {"id": 1199, "seek": 681088, "start": 6823.92, "end": 6830.0, "text": " okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the", "tokens": [51016, 1392, 11, 286, 478, 799, 11, 286, 362, 281, 5039, 11, 291, 458, 11, 286, 362, 341, 1090, 4583, 3164, 11, 293, 550, 1184, 295, 264, 51320], "temperature": 0.0, "avg_logprob": -0.11031350465578453, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.005383538082242012}, {"id": 1200, "seek": 681088, "start": 6830.0, "end": 6837.76, "text": " search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub", "tokens": [51320, 3164, 721, 1709, 807, 1310, 257, 34710, 551, 11, 457, 550, 291, 11, 291, 797, 11, 3164, 281, 5039, 264, 1422, 51708], "temperature": 0.0, "avg_logprob": -0.11031350465578453, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.005383538082242012}, {"id": 1201, "seek": 683776, "start": 6837.76, "end": 6844.8, "text": " problem. And there is also, you can do it at will too, by the way, like you can, you can scan an", "tokens": [50364, 1154, 13, 400, 456, 307, 611, 11, 291, 393, 360, 309, 412, 486, 886, 11, 538, 264, 636, 11, 411, 291, 393, 11, 291, 393, 11049, 364, 50716], "temperature": 0.0, "avg_logprob": -0.11124463845755307, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.016400309279561043}, {"id": 1202, "seek": 683776, "start": 6844.8, "end": 6849.6, "text": " image, and you get this type one that sort of finds a bunch of objects, and then you do this type two", "tokens": [50716, 3256, 11, 293, 291, 483, 341, 2010, 472, 300, 1333, 295, 10704, 257, 3840, 295, 6565, 11, 293, 550, 291, 360, 341, 2010, 732, 50956], "temperature": 0.0, "avg_logprob": -0.11124463845755307, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.016400309279561043}, {"id": 1203, "seek": 683776, "start": 6849.6, "end": 6853.92, "text": " thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let", "tokens": [50956, 1953, 689, 291, 722, 1778, 466, 729, 13, 400, 294, 428, 1575, 11, 291, 393, 733, 295, 8863, 294, 322, 472, 11, 718, 51172], "temperature": 0.0, "avg_logprob": -0.11124463845755307, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.016400309279561043}, {"id": 1204, "seek": 683776, "start": 6853.92, "end": 6859.2, "text": " me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark", "tokens": [51172, 385, 411, 8863, 294, 322, 300, 4230, 13, 400, 586, 411, 11, 586, 286, 600, 658, 264, 16202, 11, 291, 458, 11, 3755, 295, 264, 16202, 51436], "temperature": 0.0, "avg_logprob": -0.11124463845755307, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.016400309279561043}, {"id": 1205, "seek": 683776, "start": 6859.2, "end": 6865.12, "text": " is objects and bugs and reason about so you have this ability to transcend the process and tune it", "tokens": [51436, 307, 6565, 293, 15120, 293, 1778, 466, 370, 291, 362, 341, 3485, 281, 28535, 264, 1399, 293, 10864, 309, 51732], "temperature": 0.0, "avg_logprob": -0.11124463845755307, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.016400309279561043}, {"id": 1206, "seek": 686512, "start": 6865.12, "end": 6871.2, "text": " and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's", "tokens": [50364, 293, 1286, 309, 926, 13, 865, 11, 341, 2698, 411, 264, 11, 300, 311, 264, 1379, 10081, 4171, 11, 558, 30, 663, 311, 50668], "temperature": 0.0, "avg_logprob": -0.11628981258558191, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.007812641561031342}, {"id": 1207, "seek": 686512, "start": 6871.2, "end": 6877.28, "text": " even like, apart from intelligence, you have the ability to, to introspect the whole thing.", "tokens": [50668, 754, 411, 11, 4936, 490, 7599, 11, 291, 362, 264, 3485, 281, 11, 281, 560, 28713, 264, 1379, 551, 13, 50972], "temperature": 0.0, "avg_logprob": -0.11628981258558191, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.007812641561031342}, {"id": 1208, "seek": 686512, "start": 6878.0, "end": 6884.16, "text": " And that probably is a big part of intelligence. I mean, I guess you could have intelligence", "tokens": [51008, 400, 300, 1391, 307, 257, 955, 644, 295, 7599, 13, 286, 914, 11, 286, 2041, 291, 727, 362, 7599, 51316], "temperature": 0.0, "avg_logprob": -0.11628981258558191, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.007812641561031342}, {"id": 1209, "seek": 686512, "start": 6884.16, "end": 6889.36, "text": " without consciousness, but you know, there is an argument to be made that the fact that you can", "tokens": [51316, 1553, 10081, 11, 457, 291, 458, 11, 456, 307, 364, 6770, 281, 312, 1027, 300, 264, 1186, 300, 291, 393, 51576], "temperature": 0.0, "avg_logprob": -0.11628981258558191, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.007812641561031342}, {"id": 1210, "seek": 688936, "start": 6889.36, "end": 6895.04, "text": " introspect your own processes contributes in big part to the furthering of intelligence.", "tokens": [50364, 560, 28713, 428, 1065, 7555, 32035, 294, 955, 644, 281, 264, 3052, 278, 295, 7599, 13, 50648], "temperature": 0.0, "avg_logprob": -0.10554010440141727, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0039961799047887325}, {"id": 1211, "seek": 688936, "start": 6896.4, "end": 6902.799999999999, "text": " Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on", "tokens": [50716, 865, 11, 286, 576, 4994, 10081, 293, 7599, 11, 457, 264, 551, 300, 2045, 385, 264, 881, 322, 51036], "temperature": 0.0, "avg_logprob": -0.10554010440141727, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0039961799047887325}, {"id": 1212, "seek": 688936, "start": 6902.799999999999, "end": 6907.5199999999995, "text": " his newest presentation was when he said intelligence is literally sensitivity to abstract", "tokens": [51036, 702, 17569, 5860, 390, 562, 415, 848, 7599, 307, 3736, 19392, 281, 12649, 51272], "temperature": 0.0, "avg_logprob": -0.10554010440141727, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0039961799047887325}, {"id": 1213, "seek": 688936, "start": 6907.5199999999995, "end": 6913.2, "text": " analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that", "tokens": [51272, 16660, 530, 13, 407, 321, 645, 1417, 466, 264, 34699, 7895, 13960, 13, 440, 2135, 551, 510, 365, 7599, 307, 300, 51556], "temperature": 0.0, "avg_logprob": -0.10554010440141727, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0039961799047887325}, {"id": 1214, "seek": 691320, "start": 6913.2, "end": 6919.44, "text": " there is so much repetition in the universe. Right, but it's repetition in this funny way", "tokens": [50364, 456, 307, 370, 709, 30432, 294, 264, 6445, 13, 1779, 11, 457, 309, 311, 30432, 294, 341, 4074, 636, 50676], "temperature": 0.0, "avg_logprob": -0.16259409169681738, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.016910824924707413}, {"id": 1215, "seek": 691320, "start": 6919.44, "end": 6925.92, "text": " where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies,", "tokens": [50676, 689, 309, 311, 1333, 295, 34710, 30432, 13, 1743, 11, 1338, 11, 988, 11, 264, 7936, 1185, 733, 295, 34433, 28755, 11, 51000], "temperature": 0.0, "avg_logprob": -0.16259409169681738, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.016910824924707413}, {"id": 1216, "seek": 691320, "start": 6925.92, "end": 6931.44, "text": " kind of resembles, you know, but, but then there are these little weird differences, these asymmetries,", "tokens": [51000, 733, 295, 34433, 11, 291, 458, 11, 457, 11, 457, 550, 456, 366, 613, 707, 3657, 7300, 11, 613, 37277, 302, 2244, 11, 51276], "temperature": 0.0, "avg_logprob": -0.16259409169681738, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.016910824924707413}, {"id": 1217, "seek": 691320, "start": 6931.44, "end": 6936.8, "text": " and you know, like the universe is a fascinating place. And I don't know, something, yeah.", "tokens": [51276, 293, 291, 458, 11, 411, 264, 6445, 307, 257, 10343, 1081, 13, 400, 286, 500, 380, 458, 11, 746, 11, 1338, 13, 51544], "temperature": 0.0, "avg_logprob": -0.16259409169681738, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.016910824924707413}, {"id": 1218, "seek": 691320, "start": 6936.8, "end": 6941.84, "text": " Right, that's not what when you say you have to make analogies, which is I can, I can absolutely", "tokens": [51544, 1779, 11, 300, 311, 406, 437, 562, 291, 584, 291, 362, 281, 652, 16660, 530, 11, 597, 307, 286, 393, 11, 286, 393, 3122, 51796], "temperature": 0.0, "avg_logprob": -0.16259409169681738, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.016910824924707413}, {"id": 1219, "seek": 694184, "start": 6941.84, "end": 6946.32, "text": " see, you know, this and me, I think my question was formulated a bit dumb where I said, you know,", "tokens": [50364, 536, 11, 291, 458, 11, 341, 293, 385, 11, 286, 519, 452, 1168, 390, 48936, 257, 857, 10316, 689, 286, 848, 11, 291, 458, 11, 50588], "temperature": 0.0, "avg_logprob": -0.08242157527378627, "compression_ratio": 1.8779527559055118, "no_speech_prob": 0.02260858193039894}, {"id": 1220, "seek": 694184, "start": 6946.32, "end": 6953.4400000000005, "text": " if the line is squiggly, what I more meant is that, you know, in that case, it's not a line,", "tokens": [50588, 498, 264, 1622, 307, 2339, 46737, 11, 437, 286, 544, 4140, 307, 300, 11, 291, 458, 11, 294, 300, 1389, 11, 309, 311, 406, 257, 1622, 11, 50944], "temperature": 0.0, "avg_logprob": -0.08242157527378627, "compression_ratio": 1.8779527559055118, "no_speech_prob": 0.02260858193039894}, {"id": 1221, "seek": 694184, "start": 6953.4400000000005, "end": 6958.64, "text": " it's a squiggly line. And the same with the social situations, you know, that is like, okay,", "tokens": [50944, 309, 311, 257, 2339, 46737, 1622, 13, 400, 264, 912, 365, 264, 2093, 6851, 11, 291, 458, 11, 300, 307, 411, 11, 1392, 11, 51204], "temperature": 0.0, "avg_logprob": -0.08242157527378627, "compression_ratio": 1.8779527559055118, "no_speech_prob": 0.02260858193039894}, {"id": 1222, "seek": 694184, "start": 6958.64, "end": 6963.28, "text": " that that person over there kind of doesn't like me. But then in the next social situation, it's", "tokens": [51204, 300, 300, 954, 670, 456, 733, 295, 1177, 380, 411, 385, 13, 583, 550, 294, 264, 958, 2093, 2590, 11, 309, 311, 51436], "temperature": 0.0, "avg_logprob": -0.08242157527378627, "compression_ratio": 1.8779527559055118, "no_speech_prob": 0.02260858193039894}, {"id": 1223, "seek": 694184, "start": 6963.28, "end": 6969.52, "text": " kind of a person that doesn't like you and has a gun, or something like this. I almost feel like", "tokens": [51436, 733, 295, 257, 954, 300, 1177, 380, 411, 291, 293, 575, 257, 3874, 11, 420, 746, 411, 341, 13, 286, 1920, 841, 411, 51748], "temperature": 0.0, "avg_logprob": -0.08242157527378627, "compression_ratio": 1.8779527559055118, "no_speech_prob": 0.02260858193039894}, {"id": 1224, "seek": 696952, "start": 6969.6, "end": 6975.360000000001, "text": " or a group of people that you don't consider as a single single sure they are similar in some way,", "tokens": [50368, 420, 257, 1594, 295, 561, 300, 291, 500, 380, 1949, 382, 257, 2167, 2167, 988, 436, 366, 2531, 294, 512, 636, 11, 50656], "temperature": 0.0, "avg_logprob": -0.16900746415301068, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.004608222283422947}, {"id": 1225, "seek": 696952, "start": 6975.360000000001, "end": 6980.96, "text": " but it's never the exact same thing. So this reasoning by analogy does work, but you always", "tokens": [50656, 457, 309, 311, 1128, 264, 1900, 912, 551, 13, 407, 341, 21577, 538, 21663, 775, 589, 11, 457, 291, 1009, 50936], "temperature": 0.0, "avg_logprob": -0.16900746415301068, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.004608222283422947}, {"id": 1226, "seek": 696952, "start": 6980.96, "end": 6986.56, "text": " do your little modifications on top specific to the situation. And I'm sure there there's a place", "tokens": [50936, 360, 428, 707, 26881, 322, 1192, 2685, 281, 264, 2590, 13, 400, 286, 478, 988, 456, 456, 311, 257, 1081, 51216], "temperature": 0.0, "avg_logprob": -0.16900746415301068, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.004608222283422947}, {"id": 1227, "seek": 696952, "start": 6986.56, "end": 6993.280000000001, "text": " in his framework for this, but it's it's just, again, it's it's like a lot more complex than", "tokens": [51216, 294, 702, 8388, 337, 341, 11, 457, 309, 311, 309, 311, 445, 11, 797, 11, 309, 311, 309, 311, 411, 257, 688, 544, 3997, 813, 51552], "temperature": 0.0, "avg_logprob": -0.16900746415301068, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.004608222283422947}, {"id": 1228, "seek": 696952, "start": 6993.280000000001, "end": 6998.240000000001, "text": " yeah. I think that's what he call I think that's abstraction, at least that, you know, that with", "tokens": [51552, 1338, 13, 286, 519, 300, 311, 437, 415, 818, 286, 519, 300, 311, 37765, 11, 412, 1935, 300, 11, 291, 458, 11, 300, 365, 51800], "temperature": 0.0, "avg_logprob": -0.16900746415301068, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.004608222283422947}, {"id": 1229, "seek": 699824, "start": 6998.24, "end": 7003.28, "text": " prior to today, my my concept of abstraction was similar to that, which it's removing the", "tokens": [50364, 4059, 281, 965, 11, 452, 452, 3410, 295, 37765, 390, 2531, 281, 300, 11, 597, 309, 311, 12720, 264, 50616], "temperature": 0.0, "avg_logprob": -0.09998885068026456, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0002694585418794304}, {"id": 1230, "seek": 699824, "start": 7003.28, "end": 7008.639999999999, "text": " insignificant details. So you're able you're you're able to take whatever, you know, some,", "tokens": [50616, 43685, 4365, 13, 407, 291, 434, 1075, 291, 434, 291, 434, 1075, 281, 747, 2035, 11, 291, 458, 11, 512, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09998885068026456, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0002694585418794304}, {"id": 1231, "seek": 699824, "start": 7008.639999999999, "end": 7013.679999999999, "text": " you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't", "tokens": [50884, 291, 458, 11, 2657, 551, 2590, 1177, 380, 1871, 11, 293, 733, 295, 12828, 1314, 439, 264, 1507, 300, 1177, 380, 51136], "temperature": 0.0, "avg_logprob": -0.09998885068026456, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0002694585418794304}, {"id": 1232, "seek": 699824, "start": 7013.679999999999, "end": 7018.8, "text": " matter for whatever your purpose is, that's abstraction. And, you know, I think one of", "tokens": [51136, 1871, 337, 2035, 428, 4334, 307, 11, 300, 311, 37765, 13, 400, 11, 291, 458, 11, 286, 519, 472, 295, 51392], "temperature": 0.0, "avg_logprob": -0.09998885068026456, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0002694585418794304}, {"id": 1233, "seek": 699824, "start": 7018.8, "end": 7023.92, "text": " the weird things is that, and this is kind of the unreasonable effectiveness of mathematics,", "tokens": [51392, 264, 3657, 721, 307, 300, 11, 293, 341, 307, 733, 295, 264, 41730, 21208, 295, 18666, 11, 51648], "temperature": 0.0, "avg_logprob": -0.09998885068026456, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0002694585418794304}, {"id": 1234, "seek": 702392, "start": 7023.92, "end": 7031.04, "text": " right, is that abstracting actually produces things that are useful, you know, that abstraction,", "tokens": [50364, 558, 11, 307, 300, 12649, 278, 767, 14725, 721, 300, 366, 4420, 11, 291, 458, 11, 300, 37765, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11588925754322725, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.010646786540746689}, {"id": 1235, "seek": 702392, "start": 7031.04, "end": 7036.24, "text": " I think the fact that abstraction helps with generalization is a very not well understood", "tokens": [50720, 286, 519, 264, 1186, 300, 37765, 3665, 365, 2674, 2144, 307, 257, 588, 406, 731, 7320, 50980], "temperature": 0.0, "avg_logprob": -0.11588925754322725, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.010646786540746689}, {"id": 1236, "seek": 702392, "start": 7036.24, "end": 7041.92, "text": " kind of mystery in a sense, like, why should abstraction help generalize, but it does, like", "tokens": [50980, 733, 295, 11422, 294, 257, 2020, 11, 411, 11, 983, 820, 37765, 854, 2674, 1125, 11, 457, 309, 775, 11, 411, 51264], "temperature": 0.0, "avg_logprob": -0.11588925754322725, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.010646786540746689}, {"id": 1237, "seek": 702392, "start": 7041.92, "end": 7048.88, "text": " in the real world, that's what happens. Though the yet abstraction in though abstraction has to", "tokens": [51264, 294, 264, 957, 1002, 11, 300, 311, 437, 2314, 13, 10404, 264, 1939, 37765, 294, 1673, 37765, 575, 281, 51612], "temperature": 0.0, "avg_logprob": -0.11588925754322725, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.010646786540746689}, {"id": 1238, "seek": 704888, "start": 7048.88, "end": 7055.12, "text": " be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only", "tokens": [50364, 312, 6063, 2685, 281, 437, 437, 291, 528, 281, 360, 11, 411, 11, 411, 11, 291, 434, 558, 11, 364, 10606, 307, 364, 10606, 787, 50676], "temperature": 0.0, "avg_logprob": -0.1522001419624273, "compression_ratio": 1.9838709677419355, "no_speech_prob": 0.5692813396453857}, {"id": 1239, "seek": 704888, "start": 7055.12, "end": 7060.88, "text": " if, you know, you're looking for food or non food, but when it comes to this fear, if you want to", "tokens": [50676, 498, 11, 291, 458, 11, 291, 434, 1237, 337, 1755, 420, 2107, 1755, 11, 457, 562, 309, 1487, 281, 341, 4240, 11, 498, 291, 528, 281, 50964], "temperature": 0.0, "avg_logprob": -0.1522001419624273, "compression_ratio": 1.9838709677419355, "no_speech_prob": 0.5692813396453857}, {"id": 1240, "seek": 704888, "start": 7060.88, "end": 7066.56, "text": " shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by", "tokens": [50964, 3076, 309, 484, 295, 484, 295, 257, 7445, 393, 11, 2293, 11, 457, 562, 309, 1487, 281, 11, 291, 458, 11, 29279, 6773, 538, 51248], "temperature": 0.0, "avg_logprob": -0.1522001419624273, "compression_ratio": 1.9838709677419355, "no_speech_prob": 0.5692813396453857}, {"id": 1241, "seek": 704888, "start": 7066.56, "end": 7071.4400000000005, "text": " ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in", "tokens": [51248, 12782, 15264, 11, 550, 309, 311, 406, 364, 10606, 307, 364, 10606, 11, 550, 439, 295, 257, 3990, 11, 341, 10606, 575, 709, 544, 294, 51492], "temperature": 0.0, "avg_logprob": -0.1522001419624273, "compression_ratio": 1.9838709677419355, "no_speech_prob": 0.5692813396453857}, {"id": 1242, "seek": 704888, "start": 7071.4400000000005, "end": 7077.6, "text": " common with this orange, right, so that even the way how you abstract, it's not like, it's not like", "tokens": [51492, 2689, 365, 341, 7671, 11, 558, 11, 370, 300, 754, 264, 636, 577, 291, 12649, 11, 309, 311, 406, 411, 11, 309, 311, 406, 411, 51800], "temperature": 0.0, "avg_logprob": -0.1522001419624273, "compression_ratio": 1.9838709677419355, "no_speech_prob": 0.5692813396453857}, {"id": 1243, "seek": 707760, "start": 7077.6, "end": 7082.88, "text": " we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's", "tokens": [50364, 321, 393, 445, 11, 291, 458, 11, 5452, 294, 527, 5015, 31890, 2625, 11, 293, 550, 9351, 11, 321, 483, 364, 12240, 3584, 8062, 11, 293, 300, 311, 50628], "temperature": 0.0, "avg_logprob": -0.16243699289137317, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0005883713020011783}, {"id": 1244, "seek": 707760, "start": 7082.88, "end": 7089.84, "text": " our abstraction, but the how you abstract is also incredibly specific to what you want to do.", "tokens": [50628, 527, 37765, 11, 457, 264, 577, 291, 12649, 307, 611, 6252, 2685, 281, 437, 291, 528, 281, 360, 13, 50976], "temperature": 0.0, "avg_logprob": -0.16243699289137317, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0005883713020011783}, {"id": 1245, "seek": 707760, "start": 7090.96, "end": 7095.6, "text": " Yeah, and that's what, and I agree with Saba that this is an empirical question, right,", "tokens": [51032, 865, 11, 293, 300, 311, 437, 11, 293, 286, 3986, 365, 318, 5509, 300, 341, 307, 364, 31886, 1168, 11, 558, 11, 51264], "temperature": 0.0, "avg_logprob": -0.16243699289137317, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0005883713020011783}, {"id": 1246, "seek": 707760, "start": 7096.400000000001, "end": 7100.08, "text": " you know, like he's kind of like these concepts or whatever, it's an empirical question, and", "tokens": [51304, 291, 458, 11, 411, 415, 311, 733, 295, 411, 613, 10392, 420, 2035, 11, 309, 311, 364, 31886, 1168, 11, 293, 51488], "temperature": 0.0, "avg_logprob": -0.16243699289137317, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0005883713020011783}, {"id": 1247, "seek": 707760, "start": 7100.08, "end": 7105.92, "text": " Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give", "tokens": [51488, 42337, 311, 11, 286, 519, 264, 1523, 1716, 11, 498, 309, 1562, 3643, 341, 26070, 2948, 551, 11, 307, 516, 281, 976, 51780], "temperature": 0.0, "avg_logprob": -0.16243699289137317, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0005883713020011783}, {"id": 1248, "seek": 710592, "start": 7105.92, "end": 7110.88, "text": " us lots of data to start thinking about this empirically, and it's going to be really fascinating.", "tokens": [50364, 505, 3195, 295, 1412, 281, 722, 1953, 466, 341, 25790, 984, 11, 293, 309, 311, 516, 281, 312, 534, 10343, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1314245065053304, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.008846580982208252}, {"id": 1249, "seek": 710592, "start": 7110.88, "end": 7117.68, "text": " I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can,", "tokens": [50612, 286, 914, 11, 341, 2203, 281, 312, 322, 11, 411, 341, 307, 257, 11, 341, 307, 257, 5835, 17176, 1716, 11, 570, 291, 393, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1314245065053304, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.008846580982208252}, {"id": 1250, "seek": 710592, "start": 7117.68, "end": 7123.84, "text": " you can probably, like you can probably even zero, you can zero knowledge prove that you can solve", "tokens": [50952, 291, 393, 1391, 11, 411, 291, 393, 1391, 754, 4018, 11, 291, 393, 4018, 3601, 7081, 300, 291, 393, 5039, 51260], "temperature": 0.0, "avg_logprob": -0.1314245065053304, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.008846580982208252}, {"id": 1251, "seek": 710592, "start": 7124.88, "end": 7130.32, "text": " a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't", "tokens": [51312, 257, 2212, 992, 295, 10346, 2740, 11, 558, 11, 291, 393, 1391, 1884, 4018, 3601, 11, 370, 291, 2759, 380, 51584], "temperature": 0.0, "avg_logprob": -0.1314245065053304, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.008846580982208252}, {"id": 1252, "seek": 713032, "start": 7130.32, "end": 7136.32, "text": " even have to show your solution, and if they're, you know, people would put up arc problems,", "tokens": [50364, 754, 362, 281, 855, 428, 3827, 11, 293, 498, 436, 434, 11, 291, 458, 11, 561, 576, 829, 493, 10346, 2740, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11672586155688669, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.08628521114587784}, {"id": 1253, "seek": 713032, "start": 7136.32, "end": 7142.4, "text": " and they, you know, if you want to try them, you will have to put up some money, and if you can", "tokens": [50664, 293, 436, 11, 291, 458, 11, 498, 291, 528, 281, 853, 552, 11, 291, 486, 362, 281, 829, 493, 512, 1460, 11, 293, 498, 291, 393, 50968], "temperature": 0.0, "avg_logprob": -0.11672586155688669, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.08628521114587784}, {"id": 1254, "seek": 713032, "start": 7142.4, "end": 7148.24, "text": " solve it, you know, the creator of the challenge gives you some money or something like this,", "tokens": [50968, 5039, 309, 11, 291, 458, 11, 264, 14181, 295, 264, 3430, 2709, 291, 512, 1460, 420, 746, 411, 341, 11, 51260], "temperature": 0.0, "avg_logprob": -0.11672586155688669, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.08628521114587784}, {"id": 1255, "seek": 713032, "start": 7148.24, "end": 7153.92, "text": " like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic,", "tokens": [51260, 411, 341, 11, 341, 307, 516, 281, 312, 10343, 13, 2704, 291, 727, 360, 11, 291, 458, 11, 257, 3655, 32702, 299, 11, 51544], "temperature": 0.0, "avg_logprob": -0.11672586155688669, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.08628521114587784}, {"id": 1256, "seek": 713032, "start": 7153.92, "end": 7158.16, "text": " like arc, right, or like you don't even, you somehow, like you're saying, you can just prove", "tokens": [51544, 411, 10346, 11, 558, 11, 420, 411, 291, 500, 380, 754, 11, 291, 6063, 11, 411, 291, 434, 1566, 11, 291, 393, 445, 7081, 51756], "temperature": 0.0, "avg_logprob": -0.11672586155688669, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.08628521114587784}, {"id": 1257, "seek": 715816, "start": 7158.16, "end": 7162.48, "text": " you can solve the problem without ever you having seen the problem, but just an encryption of it.", "tokens": [50364, 291, 393, 5039, 264, 1154, 1553, 1562, 291, 1419, 1612, 264, 1154, 11, 457, 445, 364, 29575, 295, 309, 13, 50580], "temperature": 0.0, "avg_logprob": -0.16905265295204996, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.09265224635601044}, {"id": 1258, "seek": 715816, "start": 7163.36, "end": 7167.92, "text": " Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence.", "tokens": [50624, 865, 13, 865, 11, 572, 11, 5646, 3655, 32702, 299, 29575, 1487, 934, 17176, 294, 264, 912, 8174, 13, 50852], "temperature": 0.0, "avg_logprob": -0.16905265295204996, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.09265224635601044}, {"id": 1259, "seek": 715816, "start": 7170.8, "end": 7175.92, "text": " And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks?", "tokens": [50996, 400, 321, 652, 257, 297, 37177, 11, 321, 652, 257, 297, 37177, 295, 309, 13, 865, 11, 437, 1646, 393, 321, 483, 294, 456, 1266, 3259, 30, 51252], "temperature": 0.0, "avg_logprob": -0.16905265295204996, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.09265224635601044}, {"id": 1260, "seek": 715816, "start": 7175.92, "end": 7181.5199999999995, "text": " So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin,", "tokens": [51252, 407, 321, 658, 17176, 11, 3655, 32702, 299, 29575, 11, 437, 1646, 30, 708, 393, 321, 3507, 294, 456, 30, 11414, 11, 51532], "temperature": 0.0, "avg_logprob": -0.16905265295204996, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.09265224635601044}, {"id": 1261, "seek": 715816, "start": 7181.5199999999995, "end": 7186.08, "text": " can't we just say people should have to pay through Bitcoin, if they, if somebody wins", "tokens": [51532, 393, 380, 321, 445, 584, 561, 820, 362, 281, 1689, 807, 11414, 11, 498, 436, 11, 498, 2618, 10641, 51760], "temperature": 0.0, "avg_logprob": -0.16905265295204996, "compression_ratio": 1.8293650793650793, "no_speech_prob": 0.09265224635601044}, {"id": 1262, "seek": 718608, "start": 7186.08, "end": 7191.76, "text": " the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that", "tokens": [50364, 264, 3430, 322, 8943, 34, 30, 492, 603, 483, 527, 1065, 14862, 13, 8943, 34, 11, 8943, 34, 11464, 13, 876, 11, 1265, 11, 1797, 322, 11, 286, 658, 281, 483, 300, 50648], "temperature": 0.0, "avg_logprob": -0.1775891803559803, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.00805799663066864}, {"id": 1263, "seek": 718608, "start": 7191.76, "end": 7199.76, "text": " domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer", "tokens": [50648, 9274, 13, 286, 528, 281, 458, 11, 538, 264, 636, 11, 370, 264, 1379, 935, 295, 264, 10346, 11, 8811, 295, 9608, 337, 10754, 51048], "temperature": 0.0, "avg_logprob": -0.1775891803559803, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.00805799663066864}, {"id": 1264, "seek": 718608, "start": 7199.76, "end": 7205.5199999999995, "text": " aware generalization, which means the developer could not have conceived of the task. But if all", "tokens": [51048, 3650, 2674, 2144, 11, 597, 1355, 264, 10754, 727, 406, 362, 34898, 295, 264, 5633, 13, 583, 498, 439, 51336], "temperature": 0.0, "avg_logprob": -0.1775891803559803, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.00805799663066864}, {"id": 1265, "seek": 718608, "start": 7205.5199999999995, "end": 7212.72, "text": " of the tasks are representing four human priors, then how is that developer aware of generalization?", "tokens": [51336, 295, 264, 9608, 366, 13460, 1451, 1952, 1790, 830, 11, 550, 577, 307, 300, 10754, 3650, 295, 2674, 2144, 30, 51696], "temperature": 0.0, "avg_logprob": -0.1775891803559803, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.00805799663066864}, {"id": 1266, "seek": 721272, "start": 7212.8, "end": 7215.2, "text": " Because the developer would be aware of all of those priors.", "tokens": [50368, 1436, 264, 10754, 576, 312, 3650, 295, 439, 295, 729, 1790, 830, 13, 50488], "temperature": 0.0, "avg_logprob": -0.1627093563560678, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.04806168004870415}, {"id": 1267, "seek": 721272, "start": 7216.0, "end": 7222.320000000001, "text": " Of the priors, right? But not, not of the task, right? That's the control is the control, like", "tokens": [50528, 2720, 264, 1790, 830, 11, 558, 30, 583, 406, 11, 406, 295, 264, 5633, 11, 558, 30, 663, 311, 264, 1969, 307, 264, 1969, 11, 411, 50844], "temperature": 0.0, "avg_logprob": -0.1627093563560678, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.04806168004870415}, {"id": 1268, "seek": 721272, "start": 7222.320000000001, "end": 7227.76, "text": " that's what he said, you have to know the start of where your, your white box analyzing from. And", "tokens": [50844, 300, 311, 437, 415, 848, 11, 291, 362, 281, 458, 264, 722, 295, 689, 428, 11, 428, 2418, 2424, 23663, 490, 13, 400, 51116], "temperature": 0.0, "avg_logprob": -0.1627093563560678, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.04806168004870415}, {"id": 1269, "seek": 721272, "start": 7227.76, "end": 7233.6, "text": " the start is not clean slate, but the start here is these four priors. So it's, it's kind of the", "tokens": [51116, 264, 722, 307, 406, 2541, 39118, 11, 457, 264, 722, 510, 307, 613, 1451, 1790, 830, 13, 407, 309, 311, 11, 309, 311, 733, 295, 264, 51408], "temperature": 0.0, "avg_logprob": -0.1627093563560678, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.04806168004870415}, {"id": 1270, "seek": 721272, "start": 7233.6, "end": 7239.84, "text": " diff between you give the developer those four priors, what can the developer come up with,", "tokens": [51408, 7593, 1296, 291, 976, 264, 10754, 729, 1451, 1790, 830, 11, 437, 393, 264, 10754, 808, 493, 365, 11, 51720], "temperature": 0.0, "avg_logprob": -0.1627093563560678, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.04806168004870415}, {"id": 1271, "seek": 723984, "start": 7239.84, "end": 7245.6, "text": " just from that, right? Yeah, because I think, I think there's a lot of information leakage there.", "tokens": [50364, 445, 490, 300, 11, 558, 30, 865, 11, 570, 286, 519, 11, 286, 519, 456, 311, 257, 688, 295, 1589, 47799, 456, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14664594138540873, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.02517012692987919}, {"id": 1272, "seek": 723984, "start": 7245.6, "end": 7251.52, "text": " And you implicitly said the same thing, because you said, once you solve it, you know, once you", "tokens": [50652, 400, 291, 26947, 356, 848, 264, 912, 551, 11, 570, 291, 848, 11, 1564, 291, 5039, 309, 11, 291, 458, 11, 1564, 291, 50948], "temperature": 0.0, "avg_logprob": -0.14664594138540873, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.02517012692987919}, {"id": 1273, "seek": 723984, "start": 7251.52, "end": 7256.4800000000005, "text": " solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's,", "tokens": [50948, 5039, 512, 295, 552, 11, 291, 600, 13041, 439, 295, 552, 13, 1033, 11, 1523, 8562, 13, 1112, 307, 2435, 337, 264, 11, 309, 311, 11, 51196], "temperature": 0.0, "avg_logprob": -0.14664594138540873, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.02517012692987919}, {"id": 1274, "seek": 723984, "start": 7256.4800000000005, "end": 7261.2, "text": " but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it?", "tokens": [51196, 457, 309, 311, 11, 309, 311, 257, 12049, 9274, 13, 407, 309, 311, 6641, 11829, 13, 6454, 321, 483, 309, 30, 1436, 309, 575, 11464, 294, 309, 30, 51432], "temperature": 0.0, "avg_logprob": -0.14664594138540873, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.02517012692987919}, {"id": 1275, "seek": 723984, "start": 7262.16, "end": 7268.88, "text": " I guess. We need to figure out something cooler.", "tokens": [51480, 286, 2041, 13, 492, 643, 281, 2573, 484, 746, 15566, 13, 51816], "temperature": 0.0, "avg_logprob": -0.14664594138540873, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.02517012692987919}, {"id": 1276, "seek": 726888, "start": 7268.88, "end": 7274.24, "text": " Like, no art coin. Okay. I don't care enough to grab it.", "tokens": [50364, 1743, 11, 572, 1523, 11464, 13, 1033, 13, 286, 500, 380, 1127, 1547, 281, 4444, 309, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1277, "seek": 726888, "start": 7275.4400000000005, "end": 7278.24, "text": " Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah,", "tokens": [50692, 1779, 13, 5684, 11, 321, 820, 2642, 341, 281, 257, 1998, 11, 9974, 293, 11669, 13, 583, 1338, 11, 50832], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1278, "seek": 726888, "start": 7278.24, "end": 7283.04, "text": " thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional.", "tokens": [50832, 1309, 291, 588, 709, 337, 4764, 13, 7010, 13, 1044, 291, 370, 709, 13, 467, 311, 668, 11, 309, 311, 668, 6863, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1279, "seek": 726888, "start": 7283.04, "end": 7287.4400000000005, "text": " We've recently reached 10k subscribers actually. So yeah, thank you very much.", "tokens": [51072, 492, 600, 3938, 6488, 1266, 74, 11092, 767, 13, 407, 1338, 11, 1309, 291, 588, 709, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1280, "seek": 726888, "start": 7287.4400000000005, "end": 7290.8, "text": " We're still going to continue the show now that we've had Shaleo.", "tokens": [51292, 492, 434, 920, 516, 281, 2354, 264, 855, 586, 300, 321, 600, 632, 1160, 1220, 78, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1281, "seek": 726888, "start": 7292.32, "end": 7295.92, "text": " Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean,", "tokens": [51536, 876, 11, 1338, 13, 286, 1194, 341, 390, 264, 917, 13, 286, 1194, 321, 645, 516, 281, 1410, 309, 365, 855, 13, 286, 914, 11, 51716], "temperature": 0.0, "avg_logprob": -0.1894759987339829, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.009857515804469585}, {"id": 1282, "seek": 729592, "start": 7295.92, "end": 7303.76, "text": " to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye.", "tokens": [50364, 281, 312, 3245, 11, 321, 1062, 382, 731, 445, 1590, 586, 13, 5684, 11, 536, 291, 4024, 13, 2561, 11, 6085, 13, 4621, 13, 50756], "temperature": 0.0, "avg_logprob": -0.17948417058066715, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.013397783041000366}, {"id": 1283, "seek": 729592, "start": 7303.76, "end": 7308.64, "text": " I really hope you've enjoyed the episode today. Remember to like, comment and subscribe.", "tokens": [50756, 286, 534, 1454, 291, 600, 4626, 264, 3500, 965, 13, 5459, 281, 411, 11, 2871, 293, 3022, 13, 51000], "temperature": 0.0, "avg_logprob": -0.17948417058066715, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.013397783041000366}, {"id": 1284, "seek": 729592, "start": 7308.64, "end": 7312.96, "text": " We love reading your comments and we'll see you back next week.", "tokens": [51000, 492, 959, 3760, 428, 3053, 293, 321, 603, 536, 291, 646, 958, 1243, 13, 51216], "temperature": 0.0, "avg_logprob": -0.17948417058066715, "compression_ratio": 1.3815028901734103, "no_speech_prob": 0.013397783041000366}], "language": "en"}