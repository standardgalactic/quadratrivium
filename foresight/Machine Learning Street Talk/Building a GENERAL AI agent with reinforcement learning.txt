Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity
And so it's systems that essentially if you run it for longer and longer they get more and more complex
They generate more and more quote-unquote interestingness or interesting data
And so if we can actually, you know, crack this nut of how do we actually come up with a
Self-improving system in the sense that it keeps generating interesting data
We can then use that data to train further train our models
But of course you get into this perpetual data machine type of idea where obviously, you know
There's how do you generate more data?
If you know the data is ultimately coming from a model that you probably trained on previous data
How do you get net new information from that?
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function
Right or a preference function where there is outside information coming in through some sort of filtering criteria
For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic
Data that's being generated by these open-ended systems. What does waker stand for?
Right, so waker stands for a weighted acquisition of knowledge across environments for robustness
Fantastic, and what was the title of the paper?
Oh, right. Yeah reward free curricula. Oh girl. What was the title?
reward free curricula for training robust world models, that was it. Okay, so
Give us the elevator pitch. Yeah, totally. So basically like the overarching
Question that we're trying to answer with this paper is like how should we go about training like very general agents?
So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks
So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective
But also be able to perform those tasks in lots of different environments
So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen
specifically we want the robot to be able to go into like arbitrary
Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah
How do we want to create an agent that can do such a thing?
And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model
So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions
And so we argue if we have a very general world model that can lead to a very general agent
That's able to perform, you know, a variety of tasks in different environments
And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model?
And what does it mean to have a good world model that works?
While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that?
Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper
So he was doing this thing called curriculum learning and it's really related to machine teaching as well
There's quite a few things in machine learning where you say well if we had a really principled way of
Selecting the best training data and presenting it to the learner in the best possible order
Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of
Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret
Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So
So I guess we have this notion of like wanting to be
To perform well across a wide range of scenarios, right?
So scenarios in our context mean like different environments and different tasks and kind of like the most
Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance
So so how do I optimize like the expected reward across all of these different scenarios?
and a lot of the work that that munchies done as well kind of argues that
Just just optimizing for expectation isn't necessarily the best
The best objective so, you know, we can imagine in the real world
We don't really know like the distribution over possible tasks or or anything
Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be
robust instead and robust basically we can think of that as meaning like we should do
Reasonably well in every situation we could be in and that that's kind of what a robust objective is
And one of the ways that you can define a robust objective is via mini max regret
And so regret means like suboptimality like how well that I do relative to the best I could have possibly done
So that means basically the same thing as it does in normal English
And so the mini max regret objective basically says across all possible situations. I want to try and do
Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations
We should do almost as well as the best we could have possibly done
And I guess just to contrast this against the standard objective for robustness
So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance
While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario
But but the problem with kind of the maximum objective is that in some environments you just can't do anything
Let's say it's like some such situations is too hard
You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward
That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero
And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret
So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So
If I understand correctly is it a way of saying I want to have the best case
worst
Expected regret. Yes. So basically mini max regret is saying that if you assume that you know
The environment is adversarial to you in some way like when you're training or at inference time when you're actually
Testing your policy out in the real world
Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret
Over all the possible conditions of the world that this adversary could choose
What's really interesting about this paper is we are talking about the reward free
Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to
You know, let's say value based reinforcement learning where
You get this entanglement, right? So the dynamics the model of the world
It's still in there, but it's kind of in meshed with this with this value model
Whereas in model based reinforcement learning in a principal way
We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that
So we're very much in this model based domain, right? Yeah, absolutely
Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently
But yeah, like you said we you know in typical like model free reinforcement learning
We we typically aim to learn a policy and a value function and yeah
As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation
So so the bellman equation kind of propagates the information between like transition and the environments through the value function
So so the value function will like implicitly have the dynamics in it
But in model based reinforcement learning we want to very explicitly model the dynamics of the environment
And so what I mean by that is we want to be able to take some previous sequence of observations
Perhaps those are images and then also condition on the next action
We want to take in the environment and then be able to predict the distribution over the next observation or states
We're very explicitly modeling the dynamics of the environment
Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning
You don't so much care about having a model of the world
You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like
I mean just just in broader terms. What what what do we get from explicitly modeling the world?
So there are there are a few arguments for why we would want to explicitly model the environment
So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment
And the argument for this is you know the reward function might be quite sparse
And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior
That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have
It's kind of like a standard supervised or unsupervised learning problem
So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency
um, but I think like
More concrete arguments that I would argue for or that if you have a model of the environment
It's it's some kind of more general thing that you can then use to develop better decision making later on
So so if you just learn a value function
You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function
Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down
Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road
so I would think that um
You know, it's kind of a much more general way of having a powerful decision making agent
Rather than just specifying like one task and learning the optimal kind of policy for one task
and I guess another thing that I'll add to that is um
Rather than only learning like a feedforward policy like you wouldn't reinforcement learning
So something that maps directly to actions
The other thing that a world model allows you to do is also to do online planning
So you can imagine at test time we're trying to deploy it in the environment
But we can actually do a bit more further planning through the world model to then work out what the best action is
Rather than relying on just a neural network to immediately output an action
And there's kind of a lot of work showing that if you can do this like planning at test time
You can kind of get a lot of a better performance on a lot of environments, especially things that
That really rely on um search to do well things like go and like these kind of games
We do have to think explicitly ahead in the environment
And so I would think those are the main reasons you would want to consider um a learning a world model
And maybe a last point I'll just add is that I think this is kind of a um
Again, like unclear whether this is true necessarily
But but I think some people would argue that a world model will generalize better than learning a value function
So you can imagine like a world model is learning things like you know state transitions
So you can imagine if you if you're training on straight transitions
The model is kind of implicitly being forced to learn something like physics or something like that
And so if you're like very explicitly forcing the model to learn something like physics
You could argue, you know
We'll go to some new state and the rules of physics will still hold and therefore the world model will still be
Quite good at the new state potentially whereas if you learn a value function
I guess it's a little bit less clear as to whether you're putting a new situation
Will the same kind of structure of that value hold as it would a model
Anyway, so that was a bit of a long answer, but no, no, it's fascinating
I mean when I was reading the paper that one of the reads I got is um in machine learning
We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive
But even in learning the world model itself the model
Just because of the way they're trained it tends to compress the world into small little motifs and
Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways
And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections
Yeah, and I think um, you know to follow up on mark's um
Mark's point
I think it's also interesting because especially in the waker paper, uh, the world model setting
We're looking at specifically reward free world models
And so essentially there's this explicit decision to separate separate out the two components of world model
Which is essentially the dynamics function, which tells you how things transition from state to state
How does a state transition state of the world transition to the next state of the world?
Given an action that the model or the agent is taking in that world and the reward that it receives
So the slider part the reward is defined by the reward function
And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is
In this design arrangement is that you can
compositionally separate out this dynamics aspect from the reward aspect
So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting?
Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world
for example rules of physics and the agent has learned how to exploit those to accomplish, um
Navigation around that environment or reach different types of tasks
Achieve different kinds of tasks in that environment
Then you can um sort of superimpose a different reward function
That essentially defines a different task because the reward function defines what task success is
so you can essentially superimpose different tasks on top of that dynamics model and you would
You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of
navigating or manipulating different aspects of the dynamics of that world
We've been on a bit of a journey here. I think over the last few years in the literature of
um
We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment
And there there was this domain randomization and there's like unsupervised environment design
And of course your your iteration now is doing this in in the domain of
Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization?
kind of just to uh elaborate on something that mark was previously talking about which is that the typical
you know standard setup in machine learning is to
Uh, essentially optimize a model's performance uh over a uniform distribution
Over the data points and so this is really just randomly sampling data points
And we try to minimize the loss over those data points for whatever objective
We're trying to minimize or maximize in reinforcement learning. Um
We want to train agents that can perform well in lots of different
versions of the environment and so
You can think of each environment
Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can
Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize
The reward of the agent uh in that set of trajectories
So we want to specifically start to
actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for
Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward
and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment
And essentially have the agent try to maximize its performance its reward on that random sample of environments
uniformly
Sampled from, you know, the set of possible environments
And this is essentially
Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments
Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely
Which is obviously not true because some versions of the world will not be as likely as other as others
Uh, for example, like if you walk outside the sky is usually blue and not green
And so, you know, when the sky is orange, maybe that happens if you're in california
There's a wildfire, but that's not usually the case
And so instead what we can do is we can turn to decision theory and think of
Sort of more sensible approaches to what it means to act optimally
When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper
Is this idea of minimax regret where it is this idea again of
Having the agent act in a way that essentially minimizes its worst case regret
In any possible, uh, state of the world
So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling
environments during training to essentially, uh, sampling environments that maximize the agent's regret
And what this means is you're now actively sampling for those environment settings where the agents, um,
Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment?
And what did this current agent that's learning do in that environment?
And so there's this gap in performance and you want to actively find those environments where that gap is maximal
And if you view this as this adversarial game now between, you know, uh, an adversary like nature
That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a
Payoff function in that game or it's rewarded for the
Based on the regret that the agent experiences and the agent is trying to shrink that regret
So the agent you can think of as being rewarded for, you know, um, the the negative of that reward
So the agent's reward signal is you can think of as the negative of the regret
And so now you have the setting where you can essentially view this training process
this active sampling process as a two player zero sum game where the
Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the
The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games
There's always a this there's always a solution called a Nash equilibrium
and so this is an idea in game theory where basically this is
um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um
No player can do better unless the other player changes their strategy
And so you can think of this as a situation where, you know, I'm not
Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies
And so we know that all two player zero sum games have a Nash equilibrium
A set of strategies between the two players and in this case
We know there's additional theorem called the mini max theorem
Which says that when in a two player zero sum game specifically two players and zero sum when, um,
You are at the Nash equilibrium setting then each player must be playing what's called the mini max
The mini max strategy, which means that each player is minimizing the maximum
Minimizing the maximum reward for the other player
And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games
We know that, um, the agent which is, you know, receiving the payoff of negative regret
It's the min player. It must be implementing the min and max regret strategy
And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making
Rather than decision making that optimizes, um, just a uniform sample of environments
Okay, so kind of play back, um, some of those things as I understand it
So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on
And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly?
Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game
That we will reach a Nash equilibrium. So it will converge in a good place
But help me understand that why would it
You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge?
So there's no guarantees around convergence
And so I think this is an area where there's a lot of room for innovation
Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think
actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that
Around sort of this gap between the theory. I I just um explained and in practice is that
Regret itself is a pretty hard quantity to actually
Measure in practice because you know knowing regrets defined as what's optimal performance
minus my agents performance
So you kind of have to know what optimal performance is and in general you don't know the optimal behavior
Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so
In practice, we also use approximations for the regret
in order to do this kind of active sampling and so
There's a lot of deviations between theory and practice
So
There's no guarantees, you know that different forms of gradient based optimization
For rl training would actually lead to converging to Nash equilibria
A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that
the optimization algorithm is
fairly good at producing, you know an improved response to the
Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses
That the optimization algorithm is generating
Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium
But there is no mathematical guarantee that this actually happens
what we want to do is
You know build this latent dynamics
You know a predictive model which is a simulacrum of what the idealized version is
But we don't have a way of directly computing the regrets. So we kind of perform
You know, we learn a proxy for that regret. How does that work?
So we think of regret in the following way. So so there's kind of this um
Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma
and that basically says that you know
If we let's assume for now that we we have like an optimal planner
So we can give our like model of the world to this optimal planner and end some reward function
Let's say later down the road we get given some reward function
And so we give the model and the reward function to our optimal planner and we assume that this planner can return
The optimal policy in our model
So we kind of have this, you know planning oracle
And if we assume that we can do that
Then we can think about the difference between like how good the policy would be from
Our planning oracle in the model versus the truly optimal policy in the real world
And so what the simulation lemma tells us is that you know the difference between these two policies
So the one found by acting optimally in the model versus the truly optimal one
Is bounded essentially by the error between the model and the real world
Under the distribution of states that the policy would generate
So so, you know, it only it only matters that we have low error where the policy would go essentially because you know
If there are some states that are just completely irrelevant what the policy is going to do
It's not really going to matter if the if the model is not
Accurate and there
So we kind of use this result to think about the regret
So that that gives us like, you know, if we have like one
One true mdp and one model of an mdp and one reward function
The simulation lemma can tell us, you know
What would kind of be the regret if we did this optimal planning within this one model of the
Of the mdp
But then in our work, we're not really interested in the setting of like one mdp one reward function
Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions
Which we don't know in advance
And then I guess the other thing that I should say like you you alluded to like latent dynamics is
You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is
Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning
We're really interested in learning from like quite high dimensional signals. So
images or maybe
Probably images, but maybe there are the high high dimensional signals we want to reason about
And because we're just using image observations, this means that the world is like partially observable
Like we can't infer everything we need to know about the world just from one image, you know
For basically any physical task like the velocity of objects is important, but you can't infer that just from one image
Um, so in this partially observable environments, we really want to take
A sequence of observations because we need to to use those sequence of observations to infer what the state is
So, you know viewing a sequence of images will help me to infer what the um
The velocities are for example, and so we can think of this as inferring like a belief
A belief over what the state is and a partially observable mdp
Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead
What the next observation will be and that's kind of what you know, most world models are attempting to do
Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem
Um to just like just predict straight an image space
And so the most common thing to do is kind of to take your previous sequence of images
And then try and get like some compressed representation of the history of images into like the latent state
And then predict the dynamics in the latent state
So yeah, so I have my sequence of images
I kind of compress these somehow into some vector
And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action
And this now represents my prediction of the dynamics in the world
And then if I want to um, you know predict what the next observation would be an image space
Then I can also decode that back to an image
Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image
So maybe you don't want to actually decode the entire image
But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space
And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber
And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture
So there seems to be something magical about working in in the latent space
And also you were talking about um, you know partially observable markoff decision processors
And you know, that seems to be this idea that we need to have a modeling framework for the world
And I guess like the ideal situation would be is that like we just we we knew exactly what would happen
You know every single time step in every single state
Um, but we don't you know, so so we model it as a partially observable
Markov decision process and the markov bit is quite interesting as well
I mean maybe and you guys can just sort of introduce what why do we use that as a model?
So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system
um, so so in a markov decision process
We have some state and then we assume that we're able to take some actions and given some state in some action
We get some distribution over next states of the system
And then the the system will transition according to that distribution to the next state
And this is just like kind of a general framework for modeling like systems that we might want to control
So, you know, it kind of dates back to like early work and control theory
But then it's also the main framework used in reinforcement learning
um
Yeah, and the reinforcement learning setting because it's the decision process
We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair
Um, but yeah, as you said with relating to like partial observability and a lot of like systems
We we don't actually know what the true like state of the world is
So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp
We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists
But we can't we definitely can't just know that and so we usually think of it as being a partially observable system
Um, so this means that like given given the state, um, you know at each step
We'll basically get some distribution over observations and we just get to observe that observation
So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image
so I only get some
Camera image of the world that allows me to infer a bit of information about the state
Um, and because it only allows me to infer a bit of information about the state
It doesn't tell me the whole state
It really you need to keep track of all of the observations you have to be able to keep track of all the information
You have about the world
So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago
Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information
I need to have kept track of like all my previous information as well
Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones
Amazing amazing. So so minci, maybe you can um bring in this this latent idea
And and sort of contrast that to what lacuna is doing as well
Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around
You know since the inception which is learning
latent latent representations of data and one of the benefits of learning latent representation is that
You know, ideally your objective, uh, that leads to learning these latent representations
Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model
Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict
The task you're trying to predict and so um with uh with our case
Or latent space world models a lot of the benefit of working in the latent space
Is that if as opposed to working in the full image space?
for example
If your observations are images like in a video game is that there could be a lot of spurious features
Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates
Just to learn those patterns when they don't actually impact the ultimate um transition dynamics
Or reward dynamics that you need to learn in order to do well in that environment
So one example is if you have a game where, you know, maybe the background is different
Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background
doesn't really impact
How the player moves around in the environment or whether they've reached the end goal of the task and so
If you're training a model where it needs to compress a lot of this information
First into a smaller dimensional latent vector or latent representation
Um, you don't really need you would expect that latent representation not to actually capture
It would start to ignore the background color and it might only capture certain features of the environment that can
Essentially if you were to decode it back out it might only capture certain information about the environment
That's predictive of the actual task that you want to solve
Um, so maybe if the task is to say reach a coin at the end of a level
Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character
You're controlling to the coin
and so
With the jeppa related work
I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation
Um of images or videos or whatever modality we're trying to model
Um, it's a much lower dimensional computationally efficient representation. Uh that you can
You can effectively use for downstream tasks. Um
I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa
Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it
so, um, I mean
Lacune even gives the example of like, um, you know in self-driving cars
You might not be interested in the leaves on on on the road, you know
So like with increasing levels of of nesting you kind of like learn to
Ignore the things that are not relevant and focus on the things that that are relevant
But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building
And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant
and kind of like ignoring what is not relevant and
We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett
I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills
Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum
So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that
So so the curricula is is really important and then we're getting to the center of the bull's eye
Which is intelligence in in general now
I think Lacune thinks that it's specialized and and what that means is that there are there are motifs
That's statistically generalized and what that means is that
You do need
environments you need to find motifs that are present in
In as many environments as possible and those are the generalizing features. Do would you agree with that?
Yeah, definitely. I think that a lot of um
So a lot of really powerful machine learning methods, for example, uh are trained in simulation
And when you're training in simulation, there's a concept in control from control literature
Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between
Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator
Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics?
In your robotic simulator compared to those actual dynamics in the real world with a real robot
Um, and this also leads to a sim 2 real gap in terms of performance
So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit
Whatever system you're training this the model in and so it's fairly um common for
You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator
and so actually like one big area of um
Games ai is using is actually leveraging this idea where they essentially use ml models
They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically
Um, so ml systems are very good at finding exploits in whatever system you have
But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides
And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance
Uh, because that kind of defeats the purpose of then later transferring your model
That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator
Um, yeah, so yeah
Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that
It is possible to build a generalist agent
Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks
And to me that implies that at least in our physical world in any situation
You might use this agent that there are general motifs that it could have learned during free training that it could like, you know
Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um
Just the way that we should could think about like the different like latent dynamic subjective
So so I think I agree that like at least when I try and think about how I think or how people think
I think I agree that like
You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world
Like if I'm trying to like think through how to go to the airport
Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image
I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know
Um, like we said with the bgf of paper like should should be just try and like
Kind of basically model like the minimum information we need about the world to try and you know
Do the do the relevant task in the world? I think what you're saying. I think that probably is
Maybe more what we think about when we think about like human intelligence or something like that
Um, but then there's also this other way where we just say we're going to just like
Enforce the model to be able to predict ahead every single image
And so in our paper, we do actually enforce that the model has to predict the next image
um, and so um
Basically what this might mean is yeah, like maybe the model does
You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment
But it might also mean like what we're saying with like the leaves example
Like this might force the model to kind of capture a lot of irrelevant details
That don't really matter like the leaves on the ground and things like this
And so, you know, maybe that means it isn't actually capturing the underlying motifs
It's actually just getting good at image generation. Um, but then I've or image prediction I should say
Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction
You know, I you know, we think in terms of like more like these high level motifs
But people have other people would argue that you know
Kind of the machine learning machinery is there to do really good image prediction
So so if if we if we can get a model that can actually just like predict images ahead really well
Um, and not really worry so much about whether it's reasoning about these like high level features
You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context
So I think there's this kind of like
Contrasting ways of thinking about, you know, image prediction is good enough
We'll just predict like really visually good scenes and that will be good enough for decision making
Or do we want to force the model to try and reason about like more abstract features of the environment?
And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off
Yeah, yeah, I mean like it's um
Like the biggest problem in machine learning is overfitting, you know
So as you say like that, there are all of these statistically generalizing features
But they generalize within the domain and the domain might be like your your simulator or like, you know
How you're training it rather than how it's being used in in production
And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well
You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning
So that thing must be doing it wrong. Do you know what I mean?
And um, I was talking with chris bishop at msr the other day and and you know, he's um big on
Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um
The deep mind has done loads of cool stuff on on this
But it's this idea that like we know the world um has a certain geometry. It has certain physical priors
So like we can deliberately um, you know, kind of construct the approximation class in machine learning
Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there
Yeah, so I mean, I guess sort of the uh
Slight tangent I went into around the sim to real gap
I guess part of the point I wanted to make there is that um, you know
One way around the sim to real gap is you could try to train
You could try to parametrize a very large space of possible versions of reality
And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know
This is the specific task domain I care about I can parametrize the different
Uh versions of the task with a few parameters
And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world
But obviously that's not very sample efficient because that design space could be huge could be massive
And so instead we like these active sampling strategies like we were talking about earlier
around mini max regret style
Active sampling where you sample those environments that maximize your regret or some other type of objective
Maybe like uncertainty uh similar to what we do in the waker paper
But ultimately these things these active sampling process it leads to
What we like to call an auto curriculum automatic curriculum
And this is in contrast to prior curriculum learning works because here
This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is
Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at
Performing at those tasks. And so it's nice. It's an automatic curriculum
So you can think of it as almost like weaving a path through this high-dimensional design space
automatically such that if the
Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum
It'll basically be maximizing some sort of information gain objective
um
Because you know, for example regret if there's a high regret that's that means there's a high
Ceiling there's a high gap in terms of how much the agent can improve
Which implies that there's a lot more for the agent to learn in those environments
So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments
Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because
The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient
We need so much data to train them. Um, you want to train in simulation
But if you're doing the auto curriculum in the simulation design space
It could start to veer very easily and quickly into different corners or niches of the design space where
You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario
That we as human users
Uh actually care about and so kind of it would be you know
It would defeat the purpose of spending all this compute to train this model that could then help us in the real world
Because now it's veering off into parts of the design space that don't really matter for humans
It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding
How do we ground curricula? How do we align the curricula such that you know?
They can still do their exploration through this active sampling type of procedure over the environment design space
But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to
What humans care about in terms of the actual tasks they represent
I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness
And in general i've been trying to come at this problem from multiple angles
And i've been using the lens of agency because i think agency is something that happens in the real world
And that's why we have this divergent process because we have multiple agents, you know, kind of like
You know undirected following their own gradient of interestingness. So in in evolution
That's a great example that it is this divergent process, but it's also grounded
It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found
And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this
But the idea is that um to search this complex search space
We we need to have a divergent search and that's like we actually need to create the problems and the solutions
So like in the real world
The the you know the giraffes had the problem of like eating the leaves from from from the trees
And the problems and the solutions get generated in tandem
And this whole thing just kind of grows and grows and grows
And that seems to be the most important feature that is missing in current ai systems and the grounding or the
Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that
But um, i mean what mark what what what do you think about the importance of like this divergence in ai?
kind of the current paradigm of machine learning
Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning
Is kind of good enough to do like a lot of reasonable tasks that we might care about
um, you know
Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl
But it definitely seems like the next step towards like very general agents that are kind of
You know, I guess maybe I don't know if we want to use the term agi
But there's something something more along the lines of a general agent that's kind of you know
able to kind of self improve and learn in more diverse environments
Um, it definitely seems like that's kind of the next step of where machine learning will go
And if we're going to get to that point, I kind of agree with the idea that
You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge
Like it certainly seems to make sense that you know, you know, even as a human
To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more
More information or more knowledge and things like this and this is kind of what you know
Leads to this kind of I guess branching or you know, like you said like the diverse set of things um
That you might want to learn more about and so yeah
I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like
The next paradigm of how we think about these kinds of systems
But I'll I think mentally we'll have more to say about this
I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like
newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann
um, Jeff Klune, uh
Lisa Soros these a lot of these researchers, they've
They've been thinking about open-endedness and novelty based search divergent search for decades. Um
I think it's really interesting to think about why there's sort of this resurgence of these ideas now
and I think a lot of it is because
It is again, you know, it's it's sort of following the same
um
Sort of uh tailwinds that have been driving a lot of the ml industry
Which is just like much better compute much larger datasets
And I think what we're seeing now is that we know that
Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work
Um to the to their maximal capabilities. Um at some point
We're going to run out of data and a lot of people are now starting to talk about
You know this as sort of a pending issue on the horizon
Which is you know at the current rate of consuming data for training our foundation models at some point
We're going to run out of data. We're going to where are we going to get the next trillion tokens from?
Um, and so I think a lot of this uh now
points a lot of the interest to open-endedness because open-endedness is essentially, you know
We're studying systems that can generate their own data in an infinite
Capacity and so it's systems that essentially if you run it for longer and longer
They get more and more complex. They generate more and more quote-unquote interestingness or interesting data
um
And so if we can actually, you know crack this nut of how do we actually
Come up with a self-improving system in the sense that it keeps generating interesting data
We can then use that data to train further train our models
But of course you get into this perpetual data machine type of
Idea where obviously, you know, there's how do you generate more data?
If you know the data is ultimately coming from a model that you probably trained on previous data
How do you get net new information from that?
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function
Right or a preference function where there is outside information coming in through some sort of filtering criteria
For example human designers in the loop
Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic
Data that's being generated by these open-ended systems
And if we can do this kind of filtering we can essentially automatically find start to automatically find
Useful net new data net new trajectories net new even, you know, maybe
Sentences like tokens or net new content to train our models on
I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence
So in the world we live in I think that the intelligent process is is us
We are a divergent search and we are
And basically tackling a complex search space and we are building knowledge
And we are memetically sharing them in our society
We're embedding them in our language and then language models come and like acquire all of that knowledge
So the cynical take is that ai today doesn't you know generalize and
It doesn't it doesn't creatively find new knowledge
It just is a representation of the knowledge that we have found
But it's not black and white is it so the the work that you're doing is a great example of no no no
You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models
You're discovering interesting and novel combinations of those models that have not been found before
So it's creating a novel margin on something that was not there before
But I suppose the ideal future we want to get into is that we really can just
From a far deeper level generate new knowledge
Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know
The high level question is just
Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation
All of these systems they're they're amazing very impressive, you know
Like five years ago. I would not have believed that these systems could exist at this level of performance today
But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business
So I basically ask these systems a question or I give them a command and they give me an answer
Um, and so I think the next frontier of ai is really how do we design systems that don't just
Answer questions, but they actually are the ones that start to ask the questions
And I think once we can have ai systems that start to ask interesting questions
Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be
Okay, so so again really really interesting now
So we're getting into agency and and people think that oh you could give a language model agency
You just like you know run it in a loop and interesting things will happen
Well, well, that's not true because the whole point of open-endedness is to prove that
Existing systems converge so they don't diverge so they don't accumulate information
So we would need to create a kind of agent that like, you know, it would just keep running
And it would just keep doing interesting and novel things that would keep accumulating information
And I think that the reason why language models don't have agency is because they are essentially
A low entropy model and what that means is during training a lot of the the sort of like the unnecessary
Um, you know complexity was snipped off. So the models only know about relevant things in the next step
What's the next best token and it feels like we would need to have not only a higher entropy search
But we would also need to have um a diverse set of models that are actively
Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that?
Yeah, I think that
So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess
I guess the idea with that is like
You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and
Gather new data that's like good for a specific task
And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel
And things like this and so we can basically like specifically try and gather information that you know
Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal
And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal
So intrinsic meaning that the the signal for what you should get is just purely generated by the model
So it's purely intrinsic to the model
Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human
Having to define what the reward is or what the objective is or this was good data. This was bad data
Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some
External person defining what's good and what's bad?
And so yeah, I think like this the like and you also mentioned the word like creativity because I think
At least in the context of things that
I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity
So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting
rather than because
You know, it satisfies some objective
And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well
Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh
It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um
In the literature intrinsic motivations also sometimes called artificial curiosity
So this is a term that was coined by Juergen Schmidt-Huber
Could you could you explain it just what it is? Yeah, so oh, yeah
So taking a step back intrinsic motivation is essentially um in reinforcement learning
We train on reward signals and as mark was saying, um, we typically train on external reward signal by external
We mean that this is a task based reward. So this is um external in the sense that something outside of the agent
That's learning like the human system designer decided that this is what the reward signal is for the task
Uh intrinsic means that we want to we don't design directly the reward signal
But we're actually using some aspect of the model itself
In order to drive the models learning forward. And so one example of this could be prediction error
So if the model, uh has a large prediction error on a certain task like averaged over each time step
We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting
Um, how the state will transition when you act in that part of the environment. And so
Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is
Curiosity and different forms of play
Um in the psychology literature, a lot of people actually argue that, you know, different forms of play
In curiosity really they they amount to you can model these behaviors as essentially a person trying to
Engage in activities where, you know, they're not very good at predicting the outcome
And that's kind of what makes you could argue
That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen
um
You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting
If you can predict what will happen in the rest of the book just by reading the first few pages
Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world
Or experiences where it's similarly unpredictable
Stanley speaks about this this concept of deception or we call it the false compass
Which is this idea that any objective and even you you could say exploring all of the search space is an objective
So he said every every objective has deception and if you monotonically
Optimize any objective you will always lead into you know, like a
Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the
You know the the exploration. Let's just do something completely random and that doesn't seem very good
So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception?
Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out
That this notion of interestingness
Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation
Which I think is you know in practice we can get a lot of mileage out of this
And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper
it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but
Ultimately, you know, these these model based
Measures of intrinsic motivation. They are by definition based on the particular model at play and so
At some point, you know, you're you're starting to over fit to what that specific model finds interesting
And of course what that model finds interesting if your measure of interestingness is something like a prediction error
Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model
The capacity of that model to learn and so you could imagine a model where you know
At the beginning it's looking for lots of interesting parts of a particular video game environment
But at some point, you know, it might saturate what it can represent
And what it can learn and at some point it might start to find things
It's explored before interesting just because it's starting to forget those parts of the environment
You know if you have like a very rich stream of different kinds of environments that it's exploring
So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring
Parts of the environment that it finds interesting based on this prediction error
But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity
And another really famous example of this issue would be like the noisy tv
So like if your environment has you know, this this uh
Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's
You know, that's not something you can actually predict because it's just noise
And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error
It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv
It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit
So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know
Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory
You know optimization process that we can do everything we need to do
And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning
That what we should do guys is let's let's kind of pre-train a base model
So this model um understands environmental dynamics really well
And then we stick a reward in there and and we build um agents after that
So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary?
I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that
Um line of thought is basically saying that you know, we can kind of specify
You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp
So market decision process or something like that and I think our work isn't contrary to that in the sense of like
You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to
Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties
So the first one might be um, you know, how do we even specify that reward function?
so, you know, if the reward function is to um
Have a good life or something like this like there's obviously like
You know, maybe there is some like numerical way of defining that in terms of an mdp
But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards
And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem
But then you have these kind of like practical issues of how do you actually define rewards?
And how do you how do you say like?
Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um
Even with the world models lines of work. I think that's still like kind of quite a difficult issue
So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment
Which is a very useful thing for doing a lot of tasks
Um, but then if you actually want to optimize some specific task
You still have this problem of like, how do you define the reward?
And so we eventually want to get to this point of being able to like inject a reward into the world model
So we're kind of in agreement with that kind of line of thinking in a sense
We're eventually going to use a reward to derive the the the desired intelligent behavior
So I don't think there's any conflict in that sense
But we still have this kind of problem of how do we inject that reward into the the world model?
How do we define what that reward should be?
um and the case of um
You know one of the easiest things to do for example
Would just be to label each image with reward and then you can kind of
Encode that image into the latent space of the world model and then use that to define how good a certain thing is
And that's kind of the style of thinking what we think of in our work
Um, but I don't think that overcomes this like overarching issue of in general
It's you know rewards can define everything, but how do you in practice like get that function is pretty hard
Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward
um
If you know the reward function for your environment
You can essentially compute the value function, which gives you the optimal policy
um
And so reward has to be enough if you know the reward function and so
Uh, I think the more interesting question is definitely like what is enough for the reward?
What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents?
We're new models on or continue training existing models on and I think this goes back to the question of environment design
This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically
Weave through this path of possible environments of the design space of the environments
The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks
And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions
In which we can start to train a more and more general agent
But then the interesting question is again
Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space
of possible things we could be training our model or agent on and um, and that's I think
One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions?
Because really what drives humans in terms of asking further questions
Is our own implicit notion of interestingness which is informed by things like the scientific method and you know
being able to create explanations about the world
And we find things interesting when we can't actually explain some phenomenon about the world
Based on existing theories or explanations
And so I think what's really missing for a well-grounded, you know, human interpretable version of
Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories
For where there's mismatch between, you know, the their learned theory of the world
And evidence that new evidence that they find from experiences in the world
Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded
It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing
I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding
So, you know, we're we're sampling things that make sense because they're already there
That, you know, but but but obviously we can go off piece to little bit as individual agents
I I feel that that's what helps that process just coming back to Sutton
It's entirely possible that I've misunderstood Sutton, by the way
So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew
The reward function for a particular environment then it could do everything that it needed to do
But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know
General in in the kind of magical sense that it would work in in any possible situation
But if it is specialized in the way that we agreed earlier that there exists a reward function
Which would inco you know codify
Motifs and things that you know, you need to know or optimize in a particular environment or set of environments
Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah
That's that I think that aligns with my take as well where I think if you have a reward function
It's already sort of applying
Largely applies to at least the examples in that position paper about reward is enough
It seems like most of the reward functions they discuss are largely
Grounded in a specific task
And I think that if you have the reward function for a specific task
Then it definitely seems that you can have some optimization or learning algorithm
That essentially learns to optimize that reward and therefore achieve that task
So I do think sort of the open question that
Uh, I think same reward is enough
I think it kind of passes the buck up further one level to the question of where that reward comes from
And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier
Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function
So if it was like kind of baking the knowledge in into the system, um, okay
so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um
Do you think of like deep learning models as one model?
Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way
So, you know, like depending on the prompts you put into language into a language model
You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program
And that program is being run, but it's not it's not model building. It's like model
Retrieving, but we would would you agree of that?
Hmm
I guess i'm not sure about that like
Like within like subsets of a single homogeneous model
But I guess the thing that I like to think about that's I think quite related to this is this idea of like
And I think yamakun also kind of well a lot of people have laid out like a similar architecture
It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe
Like be thought of as different neural networks. And so, you know, we could have like, you know
Um, the standard notion of a policy which is like outputting actions
And maybe we also want to have the notion of like a prediction model more like a world model that predicts
What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model
And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent
You know an intelligent
Thing would have like an intelligent thing should be able to predict ahead in the world
It should also be able to output actions
It should hopefully maybe be able to infer like why other things happened and things like this
And so I guess as to whether we think that should you know be just like one homogeneous model
For which maybe you query it and maybe you know different aspects of that model are kind of um
You know handle different aspects of the query or whether we should think of those as separate components
I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah
I agree that you probably could just have like one massive model that does all these things
And I think at least from the the trend that I've been seeing
In kind of the world models literature and and also just like I guess the rl literature
Or maybe just we should call it the foundation model literature
Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model
That does the prediction observations like why not just have one massive model
That's jointly trained to predict everything you might want to query and then depending on the different query
You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language
So I think in this sense like this kind of model like you said is more like just one massive model
But it kind of has like a lots of different sub tasks that it's able to do
um
And so maybe this is actually like the more effective way of training a model because then you kind of get generalization
Across these different sub tasks as well
Well, yeah, and the reason I'm asking the question is um
It seemed I mean like you know for for an outsider coming in it looks like statistics has broken
You know in the olden days we used to talk about the no free lunch theorem used to say like you know
You need to have specialized models for different situations
And now the narrative is that we have generalist models
We have foundation models and and they are better than the specialized models in a strong sense
And you know and I like to sort of push on this a little bit and see well
When when does it break because we know that there are like these physics inspired models with inductive priors that you know
Know about invariance of you know like molecules and drug discovery and stuff like that
And surely they would be better than a language model
But no no no now they're training language models on mathematical conjecturing and like you know like
Drug formulation using tokens and and so on so you know as an outsider you might just think well
We can just use a big transformers model for everything
I I think a lot of this does come from um well, so
I think the attention-based transformer architecture is
Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions
But I think also part of it is just that we're just starting to enter this regime
When we're just training these models on an insanely large amount of data, and I think that a lot of times
We need to sort of take a step back and really consider the amazing performances on different tasks
And really think about you know how much information was actually leaked into
This task in the training data because
Right now. We're really just training
these huge models on
I think I would say that we're largely training them on the test distribution in many cases
I do there I have seen like lots of examples of
Truly impressive behaviors from these models that that do seem like
Truly novel like zero shot generalization to unseen tasks
Like there was a recent example. I saw on twitter or someone
Had like a very low resource like rare language and they gave it a few
They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language
So that does seem very impressive
But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams
I imagine a lot of that is really a function of just
Literally giving the model the test domain in terms of information during the training step
Okay, okay
So there are like two schools of thought on this when we talk about world models
You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing
I mean, obviously it's not doing navier stokes
It's not doing like fluid dynamics, but it seems to be doing something like that
So like one extreme view is that it is just a hash table
And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator
And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling
Not only, you know, just to just just the words and the language
But it's also implicitly learned to model the world and the people and all of us
So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum?
Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it
But I think I think if you can for example
You know after each kind of you know, so it's a language condition model
So if after each kind of frame you could for example put in a different language language kind of conditioning and say like
You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video
And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of
Give some new action and then you're able to see like the alternative outcome of that new action
I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of
You know, I really think like if you can predict the outcome of any action given some sequence of observations
I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works
And so I think if the model can do that
I would be kind of inclined to say that it does have a kind of world model in the sense of understanding
The underlying world but then there might also be a chance that you know
You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very
Fine grain conditioning on a slightly different outcome different like conditioning
Maybe it won't actually give you the correct kind of counterfactual prediction
And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that
To really see if it understands things well, or it is just like kind of generating some arbitrary video
Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence
It's so static that like, you know, we we um, we think of that as being intelligence
But but we're still going like we're now create we're creating knowledge right now
We're creating models because we're exploring we're doing exactly what you said Minshew
We're like we're exploring the search space and we're building models and we're combining them together
And you know, presumably we would diverge quite quickly from from from the language models
But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah
um, so just regarding the the sort of lookup analogy for these large models, I think it's
So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of
of the of this alternative take
Which is more like
There's an alternative take which is that it is kind of like a lookup table
But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function
Yeah, and a vast space of functions and france wash relay had a really good
Sort of blog post where he kind of goes more into the details of this viewpoint
But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like
A key value in a hash table
It seems more like it's these models have learned over tremendous amounts of data to compress that data
They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions
So they're approximating some kind of function
Or a vast family of functions
And I think the prompt really acts like as a key that essentially activates a particular function
And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from
Images to image net labels now like foundation model in the foundation model regime
It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions
That's basically activated selectively based on the input with prompt
Um, and I do think that you know based on this
I think it's definitely possible that with enough data from the world enough experiential data
That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works
And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on
Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models
Sure. Yeah, so yeah, these are two um separate papers. So
So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so
Basically what dreamer involves doing is like a way of training a world model
And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data
um
So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations
Um, and then be able to predict like the next a distribution of the next observation given some action
and so we also talked about how you might want to like compress this into like a um
More compressed representation of your of the previous observation
So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations
And then you map them to some compressed representation
And then could predict ahead in this latent space. Um, the next uh, latent
Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know
We can in general predict what's going to happen to condition on different actions
So now if you want to get like interesting behavior out of something like dreamer
You can then go ahead and generate a lot of synthetic data using dreamer
Or the dreamer world model and then use that to optimize behavior
And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model
So a lot of people call this like reinforcement learning and imagination
So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize
behavior in some sense
And then you could also do other things like Monte Carlo tree search
Which is like closer to like the works on on mu zero and things like this
Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud
So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before
And typically like new discovery only happens at the end of the chain not back in in the middle
Exactly and and there's also this notion that creativity happens through knowledge
So like knowledge new knowledge doesn't come from the ether. It's kind of
There's some creative component to it, but it's it's on the
The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question
But you see I mean so so when we talk about imagination through like, you know
Like reinforcement learning policies and and so on what we're saying is like, you know
I'm I'm imagining all of these like possible, you know worlds and so on
But I'm using the cognitive primitives of all of the stuff that I already know
Yeah, I think knowledge is definitely a compounding
compounding
artifact
That's basically like the culmination of everything all the experiences that we
That we encounter like throughout our whole life and through also like beyond, you know
Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models
is that they are essentially a
codification of cultural knowledge and so
Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for
You to have AI systems that generate
General AI systems and he recently added actually like as a fundamental piece of this in in his framework
This idea of building on top of foundation models
And so he says he calls it like standing on the shoulders of giant foundation models
Which is I think really just sort of the ml equivalent of building on top of cultural knowledge
There's there's a real shift recently towards talking about
um synthetic data
And as we were just saying like, you know synthetic data, it doesn't come from the ether
So we already know stuff about the world. We we build simulators and we kind of generate new
Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data
um
What what do you think about that process?
Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah
um, so so basically like the motivation of that kind of work is like
Kind of saying, you know, we might have some like previous data set or something
And we've trained our world model on that data set
But we really want to go out and like gather more data and then like improve the world model
um
By gathering more data
And so we can use things like intrinsic motivation to then give us like a reward signal within the world model
So in the sense of something like prediction error, which mentioned earlier
So now we can basically like train a policy in the world model that's now not trained for a specific task
But it's trained to go out and gather information in the world
So basically now, you know, you do this imagining in the world model to imagine ahead
But instead of imagining ahead, how do I do a task? Well, you're imagining ahead
How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore
um, and then and our um
Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular
and so basically what we're trying to say is
You know plan to explore is good for for getting an agent to go out and gather data
Um within a single environment and you know and presumably once you've gathered enough data within a single environment
Then you can generate a bunch of synthetic data in that single environment
And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment
um, but what we're really interested in is saying, you know
Let's not assume that we have like one specific environment beforehand
Let's assume that you know, there's some space of you know broad range of scenarios
Like we want a very like general agents
There might be a bunch of different environments and then within that what those different environments
We kind of want to be able to to handle absolutely any task
And so in the waker paper, we're basically saying like, you know, how should we gather the data within um
Within this like broad space of possible environments and tasks such that we can train a very good world model
And then once we have that world model, that's kind of like capable across environments and tasks
You know the assumption is that we can then use that to generate good synthetic data, which we can then
Um use to optimize behavior
And so maybe to talk a little bit about like how we formalize this problem
Um, so, you know, we mentioned earlier this idea of like the simulation lemma
So we basically say that or an existing work that says like in a single environment
We can bound the gap between the optimal policy
That's trained in the world model so trained in the synthetic data to the to the truly optimal policy
By the error in the world model and the distribution of states generated by that policy
So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it
But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand
And we also don't know what the task is beforehand
So how do we get like a good world model that can handle like all of those situations?
When we later want to go ahead and optimize some task
um
And so the way that we do this is we basically yeah
We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments
And then using the simulation lemma we can basically say now
Now the um the world model has to have low error across all environments
Under the distribution of states generated by the optimal policy for any future task
Um, so we're going to say like yeah, the world model has to be good for any environment and
Under, you know in any area that the policy might go to that's relevant to the future tasks
And then what we kind of say in the paper is, you know, if we want a truly general agent
We're not going to know what the distribution of tasks is beforehand
So we don't know we don't know what the reward function is. We don't have a set of reward functions
Um, you know, we're just going to kind of assume the agent has to do anything later down the line
And this is kind of like related to this idea of like open-endedness that we've talked a lot about
And so if we don't know what the task is going to be like later down the line
Then the best assumption we can do is say that, you know, it could be any reward function later down the line
Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of
You know, we talked about a bit about intrinsic motivation and interestingness
And if you kind of assume the task can be absolutely anything later down the line
You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it
If you do this in robotics, that might mean the task is just to do like backflips later or something like that
But you have no interest in doing that. So it's it's not clear if that's really a good assumption about
How we should think about what tasks might be interesting later, but that's the assumption we make
So we assume the task can be absolutely anything later down the line
So so now we have to get a to the point where we have the world model
Which is good for any environment and under the distribution of states generated for any task or any optimal reward function
And to do this we basically like leverage two different techniques
So to generate this state um
So to handle the aspect that we don't know what the task is later down the line. We assumed that um
We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment
And so basically if if this um
If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment
Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment
So now we have a policy that's finding like the maximum uncertainty in every environment
And then if we want to optimize this like mini max criterion across environments
We kind of need the maximum uncertainty to be low across all environments. So
So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments
um
And so
Basically, you know, what we could think like what what happened in practice is, you know
You can imagine there are a bunch of different environments
Some which are like a low complexity and some of which are high complexity
And if we just kind of naively sample from those two different environments data, you know
Our world model is going to very quickly get good at the low complexity environment
And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment
So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness
So the idea here is that we're basically going to
Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments
So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity
And then we're going to actively sample more data from the environment that has higher complexity
Such that we we bring those errors down on the higher complexity environments
And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning
You kind of assume that you have some reward function
Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing
To determine, um, you know, where the policy has more potential to learn
But because we're making this assumption that, you know, we don't know what the reward function is
We're trying to get a general agent that can kind of do any task any reward function
Um, we don't assume that we know that reward function beforehand
So we can't use reward as a metric of saying like I need more data from here or I need more data from here
But then kind of the main argument of the paper is showing that, you know
If we just think about this in terms of prediction error in the world model
Like we can actually use that as like an intrinsic motivation signal to say, you know
Does the agent need to gather more data from this environment or from this environment without access to reward function
and so we could kind of think of um
This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand
We're kind of agnostic to what the task is
And because and to kind of distill that knowledge that's that's gathered without the reward function
We use the world model as a mechanism to like distill that knowledge
Because if you just like naively have an agent gathering information with no reward function
You know, how do you how do you kind of put that knowledge into the agent?
And we kind of argue the best way of doing that is the world model
So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing
So I mean essentially you're doing a high entropy search. So you're you're leaning into
Areas of complexity and you're building a higher complexity model
Which goes against the grain of the intuition of like Occam's razor that should have simple models
So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that
And then the other interesting thing is like from from a curriculum learning point of view
I think traditionally we did explicit curriculum learning and you know, we might have some
Principles around having a monotonically increasing curriculum of complexity
Whereas here by leaning into
Environments where we do worse on so we're selecting them based on prediction error
We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well
Yeah, I guess actually it actually almost ends up being in the opposite direction
So so by leaning into the the the higher complexity environments more
We're kind of saying let's prioritize the harder environments more to begin with
So let's like gather more data in the higher complexity environments
Um, you know, because I guess intuitively if you kind of want to be good across all environments
You kind of need more data from the higher complexity environments
And we don't really explicitly think about an ordering of going first from easy to hard
Um, I guess that maybe there is a something to look into there because
You know
Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn
An initial policy that can kind of do something in the low complexity environment and then you build up the complexity
Gradually, um, but I think that that idea is most useful when you know what the task is
So you could imagine if the task is like low commotion if it's walking
You kind of want to first learn a policy that's able to walk on flat ground
And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very
Complicated terrain so it kind of makes sense to go from low to high complexity
um, but in this work we're focusing on
purely intrinsic motivation meaning that the policy is not trying to learn a specific task
It's trying to just seek out um uncertainty and like reduce uncertainty
And so we don't really have the notion of you know
You first need to be able to learn how to do something on an easy
An easy environment and then move towards harder environments because there is no specific task that we're trying to learn
And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments
So that actually, you know, we're consistently something more data from the hard environments
And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like
You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example
um
So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know
Not really a reward function in some sense. You're just trying to
Do like unsupervised prediction
And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is
Not very good at predicting a language about some certain task or something like that
And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know
What the next token is in french like we should presumably gather more data in french
And that so that kind of gives us a way of like actively gathering the appropriate data
Um, and so yeah, I think this idea of like gathering more data based on certainty
Obviously is a very general idea like the idea of like active learning
Um, but we kind of like
Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting?
It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right?
The environment complexity metrics. Yeah, we don't have the external notion of difficulty
But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um
So it kind of depended on the environment
So in some environments, you just kind of got this like very straightforward behavior of like, you know
Consistently gather more data in the more complex environment
um, but because we're we're actively trying to gather data, um
Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training
So so what happened in some of the other environments?
For example, is that initially all the environments are just like high uncertainty
Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less
To just get a rough understanding
And then you know as as the model would improve on the simplest environments
Then we would see like more and more emphasis towards sampling the highest complexity environments
So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum
But a bit different in the sense of like initially everything is uncertain
So we're just going to sample everything uniformly
Um, but then we kind of get a better understanding of which of the environments
You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data
Yeah
I mean I can see this both ways
I mean certainly from like a Bayesian optimization point of view that there's something to be said for
Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty
And uh, as you say like traditionally in curriculum learning
We are told that we need to have monotonic increasing complexity
But as you just said that's when we have a particular task in mind now neural networks
They're a little bit like a block of clay aren't they so you know, it starts off with
Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay
And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here
So like with this maximum entropy
Search, you know like high entropy search
What we're doing is is we're saying okay
Well, here are some complex models and these models must contain motifs that tell us a lot of information
It's a little bit like the elo algorithm in chess
You know, you actually get information gain when something surprising happened
So here's a big block of complexity and I'm going to try and infer
What the motifs are in that complexity that that explain the information that I'm missing
I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern
towards uh, I think that like ties a lot of these ideas around active
Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise
Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm
Um, where essentially you have the system essentially use signals. It produces itself
Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on
And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of
Tasks to train on and I think the sort of like the fundamental connecting
Uh pattern here is just that this the signal that we use to drive the training
It's always going to be based on something like, uh, an uncertainty signal or, um
Going back to the open-endedness literature something like a classic notion of interestingness
And I think there's just a lot of different possible choices for this metric and so
One for example, we talked a lot about
Minimax regret
So regret could be one of these driving signals because it measures the existence of a performance gap and therefore
Probably an information gap as well in terms of learning to master those tasks with high regret
But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within
And so there's fundamentally lots of different sort of branches of these autocurricular that you could use
Depending on this search objective that you use to drive this exploration process
Can we contrast this to you know, like, um, large language models that they are self-supervised learning
So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word
And it's a similar thing with, um, self-supervised, um image
Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information
You know with, um, let's say high entropy and that would lead to an implicit curricula
Whereas with language modeling language modeling, there is no implicit curricula
But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself
So if you give it sort of surprising and weird information, the language model might just kind of brush it off
And if you reinforce things that it already knows then it's almost like a stream of channels, you know
It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit
Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems
A lot of times like if you look at, for example, open ai's job listings
They're actually hiring specifically for experts in different domains to essentially create the next
Batch of supervised data to train or instruction tune their models on
For example, they hire biologists or they hire people with legal expertise to generate this data
And you can think of this essentially as a human steered or human driven version of this active sampling process, right?
Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks
It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point
Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process
and in a way
supervised learning is really just sort of one point in a continual learning process where, you know
Classically, we just looked at one step of this which is here's a batch of data train on that but really
Building machine learning systems, especially nowadays. Everything's in production. These are all live systems
You have to keep it up to date. You have to keep it continually generalizing to new knowledge
Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data
Train your model on that collect the next batch of data
You know continue training your model on that
And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data
It's going to overfit to that data
Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it
So you do want to selectively actively collect the data
And so I think we kind of almost explicitly already do this at a systems level
And I think the next frontier is really just having systems that self-improve in this way where they can start to guide
More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence
It's not just like you know a bunch of weights on on a on a server somewhere
And so you could argue, you know, there's this concept called graduate student descent
Which is what happens in academia or even as you just articulated with open ai
It's a little bit like an epic mechanical turk right where you know
They are monitoring the logs
They know when things go go badly and then they lean into it in the same way you are they go in higher experts
And they kind of like add more and more data in all of the holes
And eventually there are no more pockets of like abject failure
It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent
So yeah, so there's this interesting systems view of of intelligence
Yeah, it kind of starts to mimic just the scientific process in a way
Where we're sort of we were putting a lot of hope in the model to basically be able to distill
information from sort of the net news batch of data that we collect
You know that we know the model currently doesn't explain well
And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights
That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated
Scientist or an automated version of this like continual
process of creating theories and explanations about the world
But of course, you know
Humans are still much better at language models at doing this or large models at doing this
So I do think there clearly seems like a huge gap in terms of
Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories
Based on like net do new data and even seeking that out as humans do
Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency
But um, we were just sort of talking a minute ago about there being two types of ai
You know, there's there's an ai where we are the generating sources of agency
But there might potentially be another ai in the future where that that is the generating source of agency
Yeah, I so I think that um
This kind of ties into my my the framework
I personally used to think about open-ended systems as well
Where I think that you know at a high level you can you can study ai sort of in silico
You can study it in systems that you control that you design and that you try to like have the ai model self-improve within
And so you can try to build
Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem
Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space
That are not relevant to tasks you care about
Um, and so this is kind of the danger of like generating open-ended systems in silico
And I think it's very similar to potential dangers of generating agi in silico
um
And I think the alternative is really just what are existing intelligent systems
And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems?
And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch
Or do we want to build tools?
You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify
existing intelligence like human creativity human intelligence
Could you argue because if intelligence is a divergent search process?
You might be tempted to think that well if we had loads of tools to help us share
The models and help other people discover the models that i've created that that will help us generally be more intelligent
But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves
And discovering novel models. They're just going to use my model
Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know
Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models
You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the
existing expertise or
Automative capabilities of those models to then like move further beyond what they're naturally capable of doing
And really that pushes the frontier of the knowledge that we can create as a civilization
And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like
Chat gpt to help them with coding at work
They actually now are starting to match the performance of more senior engineers
Because it sort of levels the playing field
But that also translates into just like net more productivity per software engineer. And so
I think that it's more just unlocking sort of
Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value
Can discover a lot more knowledge
Than before
Okay, but I mean do you think that it creates a tendency towards boilerplate though
So we're more we're more efficient at doing things that exist
But you know like on on the frontier we might have a slowdown
There's definitely the danger that it can lock you in to certain patterns
Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it
Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already
Generated by autocomplete. So I think that that's also another really interesting realm of questions
Which is basically how do you um, how do you avoid these kinds of uh, these local optima?
When you start to train a model on its own outputs
And I think again like sort of the solution will start to look like some form of novelty search or exploration
Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and
Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very
Very clear trade-off between the two and they said they both have like fantastic things going for them
And I guess the trade-off being you know academic freedom
An academia and be able to like individually pursue ideas like purely for curiosity's sake
And um, you know, that's something I've really loved about academia
But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects
especially
You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore
You know a lot of interesting research can kind of
Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry
And so I think this kind of leads this trade-off of do you want freedom?
Or do you want to be on these like larger projects that are potentially more impactful?
And so yeah, I've really struggled with that trade-off. I think they they both
Have big pros and cons. I don't know what you think minty. Yeah, I I think that um
Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on
um exploitation and
academia is where you know in principle you should get a lot more exploration
But I do think that currently
both
Systems are kind of like entwined in the same sort of reward function at a high level where essentially
You know if if if you're if you care a lot about
Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on
sort of whatever topic is
Trendy or hyped at the current time
And so like I think you see tons of people obviously
Working on language models partly because it really is a fascinating subject
And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it
but I also think that um
A lot of it is kind of you do get this sort of rich gets richer effect around
Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have
um
And that's partly because you know like both industry and academia are at some level optimizing for a similar
Sort of reputational status or citation count sort of metric
And so I think that's an issue, but I also think that in some ways
Uh industry you could say has
Additional benefit where I do think that from like a short-term point of view industry is better poised to
make certain
Higher impact research not just because of the resources available to industry, but also partly because
um
Sort of industry, uh, you know
Rides or dies based on whether the actual research artifact you produce
Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia
Um, and then sort of on the to take the counter position
I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path
For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing
And so I think really it's about like if you want to if you want to take the bet that you can
You know play a part in discovering the next big thing
Then and that's that's suited to your taste for research then academia makes more sense
but if you know, um, you want to
You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work
Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay
And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know
Maybe we're not very intelligent
Evolution has just been running for a very very long time and we are the result of that
So in in a sense, do you think that we could make strides in intelligence?
You know just through ingenuity or are we always going to need loads of computer power?
This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning
Literature lately, which is like these kind of large scale
Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so
You know you have all these algorithms that are like, you know optimal planning and so forth
But we're kind of seeing
A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did
and so kind of reducing the problem to you know
essentially no real algorithmic
Innovation and more just like can you gather enough expert data?
And I think yeah, I guess the reason why that trend is occurring is
Is I guess like you said there's kind of been
You know the bit lesson kind of said that you know
Just being able to scale with more data and more compute is kind of the most important thing
And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up
especially like online reinforcement learning if you want to go out and like
Actually have an agent like actively collecting data in a bunch of different environments
And updating itself online like that's so much like engineering infrastructure to set up
And so I think there's this this trend towards just like the simplest algorithm possible
Which is like not even reinforcement learning not even planning just copy an expert
but I think that
That's like you kind of said earlier with like this kind of like short-term exploitation
I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because
You know, it's very easy to just train a large transformer and then gather as much data as possible
And I think in areas like robotics, we haven't really seen like how far can that go like
Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer?
And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know
You know, will will that actually hit a bottleneck or or if you just gather enough data
Will that will that kind of be sufficient?
And I guess like
You know, you could argue that I think it's probably true that there must be a better algorithm out there
That can and principle do this in a more efficient way
But I guess if it's just easier to just gather more data and just do imitation learning
I can see that there's at least a business case for trying that
So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system
But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning
Like won't get you there and so it does make sense to pursue that first
But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of
Run out of steam and everything will plateau and I think at that point
You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc
Will really come into their own
And so I guess this again relates back to like the academia industry trade-off like, you know
A lot of the projects in the industry are just going to kind of be exploiting gathering data right now
Whereas maybe there's a lot of scope to do these kind of more
Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line
I don't know what you think about this
Yeah, I definitely think that um
Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets
But um, I think again like the challenge is just at some point we will run out of tokens
We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more
I think paramount to like driving performance even further
So if we want to sort of break beyond sort of the token limit of like the data that's available now
We actually need these systems to generate their own tokens their own synthetic data
And that's that's where like the self play auto curricula exploration types of algorithms will start to
Become more and more prominent and obviously you need an environment in which to do that exploration
And that's where the world model
Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within
The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more
Um, amplify the existing data you had
Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um
It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind
You know early versions of alpha go were bootstrapped with human knowledge
And then there was the alpha zero. So it was actually doing what we're talking about
It was actually discovering knowledge on its own and um in principle. That's a great idea
But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use
The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that
We are all real agents the universe is a massive computer
We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm
And then the question is well, if you kind of just capture the thing now without the dynamics that produced it
Um, will the system be robust and could you still um, you know, kind of
Carry on as we were in the real world if that makes sense. So um, but yeah
The interesting thing with the work you've done is is that you are modeling agential systems
You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world
Like simulating much more complex systems in the real world. Yeah, I think that if you if you
So I think that just purely imitation learning alone is not really going to get you there
But I think that if you can if you can imitate
So one is sort of finding the set of tasks
I think that if you find the set of tasks or reward functions that could be relevant
Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories
So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where
If you basically have like a stock market, for example, that's a really good example
Where if you have a stock market, that's a very open-ended system
And like different traders will have different strategies that are best responses to each other
And then over time the set of strategies evolves over time in an open-ended way
Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um
They've sort of figured out those strategies. And so they won't be very competitive. And so um
I don't see an an imitation learning system being able to sort of
Um, generalize to that level of complexity just because by definition it's imitating previous
Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh
More interactive trial and error learning that allows for strategic adaptation
And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um
You you can't just purely I think learn
Uh
A model of something like the stock market just based on previous data
You really need to have more inductive biases around uh, sort of you know
What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could
um
You could learn over time, but I
But maybe not in the yeah, so this is kind of like this is not very coherent
But I feel like uh, you might need something that looks more like learning over space of programs
That starts to encompass different kinds of uh tasks
And then you can basically simulate those tasks to completion with agents that can essentially
Uh try to self-improve against other agents
The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason
Because you know like the the the the the memetic world is very ungrounded
And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know
It can go in it can go in almost any direction
And also the concept of alpha I think is really important because a trading strategy works really well today
And then when other people learn about it it no longer provides an advantage because everyone else knows about it
And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool
It was great to you know, have like a ted talk speech when it came out
And now it doesn't seem cool anymore because everyone's using it on linkedin
So it's almost like that we need to have this in like continuous
creative evolving process producing new sources of alpha
And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition
Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier
And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of
You know
Continue to improve is to kind of like filter the synthetic data for example
So you might kind of you know have the the new system and then we generate some more data
And then we kind of have some like filtering mechanism to say that you know in the current stock market
This is this is good data or what you know, whatever system we're thinking about and then we can kind of like
Use that to enable the model to improve
You know and adapt to the new system
But something I've always like why like thought about is like well
I guess one is is it really trivial to be able to like filter that you know new synthetic data
And then two it feels like if you're just relying on like filtering existing synthetic data
Like isn't that a never to go into kind of plateau and so I guess eventually
You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data
But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of
Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data
Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that
The synthetic data you do generate it'll start to sort of saturate like what's already in the model
Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more
Especially like the more likely trajectories or sequences of samples. You'll start to see that
More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data
So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um
This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks
Um and kind of yet environments where there's like varying levels of complexity
So, you know, you might have a robot pushing around a variable number of like objects
Or maybe you have different terrain that the robot might want to um
Learn to kind of you know do locomotion over and things like this
Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization
So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out
The environments that have this like higher uncertainty
Um and so basically what we show is that you know, if we do the waker approach
We still do like very well on average, but we consistently do better in terms of robustness
And so robustness by robustness. I mean here that that we do better in terms of the worst environment
That the agent is evaluated under and so this kind of means, you know
If the agent is able to do well in the worst environments that it that it is evaluated under
That kind of shows that it's able to do well across all environments because
Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property
Which we talked about in terms of like mini max regret
But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier
Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance
Which isn't something we can really know. So instead we we just show that you know
The agent performs well across all environments more so than if you just like naively sampled the environments uniformly
And in terms of decomposing the performance across the spectrum of possible environments
So like, you know the ideal situation is that we have a very simple model which just generalizes
So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious
But we've just you know, just by through some sheer magic
We found the best motif to work in all situations. Probably that's not quite true
Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail
And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution
Do you have any like way of reasoning about what that is?
um
so
Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like
Focusing more on these like long tail examples
That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that
If the model is trained on some certain subset of the tasks like that will actually generalize better
but but I think in practice, that's not something we can really um
Really know how to you know, like optimally select the best kind of set of tasks that will generalize well
And so we do focus more on on like, you know, these these kind of long tail tasks
Or like the ones that we might see rarely and therefore have high uncertainty about
um
In terms of like the the out of distribution generalization
So so we do also do some experiments like looking at how well does the model generalize out of distribution?
And basically what we show is that if we train the model in this way
And then we give it some more environments that hasn't seen at test time
Um, if the environments are more complex then then we've seen sorry hasn't seen at training time
Basically like this model then generalizes better to out of distribution environments that are like more complex
Which is kind of what you'd expect because we've kind of biased something towards more complexity
We're able to generalize better to out of distribution environments that have higher complexity
And then I guess the question is like do we care about out of distribution environments that have higher complexity?
Like what about the out of distribution environments that have lower complexity?
and I would argue that you know
Basically the lower environment out of distribution distribution environments that have lower complexity
Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know
Almost any reasonably trained model can handle the very simplest environment
So what we really care about is can we generalize out of distribution to like higher complexity environments?
And so by biasing the something towards the higher complexity environments
We do show that we're able to generalize further out of distribution to even higher complexity environments
Okay, but is there any way of knowing whether it's kind of like
Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them
Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now
Which is better evaluation benchmarks for
Generalization within different kinds of models
Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set
Which is um, which is definitely an issue that is currently happening with large language models
Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization
aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks
Um, we really need to solve this problem. How do we have these clean data sets?
That allow us to
To truly test on inputs that the model hasn't seen at training. Um, I think in the case of
reinforcement learning
That's a bit more difficult just because usually we focus on a particular task domain
And so there's always going to be some shared similarities within the task
But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled
Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between
Things that have to be accomplished in that task
But we didn't do that. Um, that I actually think would be a really interesting idea for
A new evaluation environment for rl
Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning
In general, but just just to kind of round off off the interview
I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing
I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface
Between humans and machine learning. Can you tell me about that?
Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind
Um is just that you know, I think from personally from a high level point of view
I'm very interested, you know talking about this divide sort of this fork in the road in terms of
What's the path to open studying open-endedness studying it in silico or studying it in
situ in the setting of an actual open-ended system like a user
App interaction or you know the interaction between a user and a piece of software on the web
Or potentially with many other users. There are such rich
Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans
To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying
Systems or algorithms that allow us to better steer the creativity of humans
As they are mediated by software
And basically allow us to essentially amplify
Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch
Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah
Great cool. Yeah, we're done
