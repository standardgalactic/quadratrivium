1
00:00:00,000 --> 00:00:04,080
Hello, folks. I was in New Orleans last week and I had the pleasure of interviewing Laura Ruiz,

2
00:00:04,080 --> 00:00:06,160
the primary author on this paper,

3
00:00:06,160 --> 00:00:10,080
large language models are not zero-shot communicators.

4
00:00:10,080 --> 00:00:14,880
Now, this is exploring the ability of language models to perform Implicituk,

5
00:00:14,880 --> 00:00:18,160
which I guess from a machine learning audience point of view,

6
00:00:18,160 --> 00:00:23,440
you might think of as being some kind of extrapolation or even abstractive reasoning.

7
00:00:24,080 --> 00:00:25,920
There's an example of this that we can try.

8
00:00:26,880 --> 00:00:29,680
Esther asked, can you come to my party on Friday?

9
00:00:29,680 --> 00:00:34,480
And Zhuang responded, I've got to work, which means no.

10
00:00:35,520 --> 00:00:39,840
Yeah, part of the reason I wanted to do this quick intro is since this interview,

11
00:00:39,840 --> 00:00:45,200
OpenAI has released a chat GPT, which is pretty impressive, actually.

12
00:00:45,200 --> 00:00:50,320
So we can come in here and we can say something along the lines of

13
00:00:51,840 --> 00:00:54,000
Esther asked, can you come to the party on Friday?

14
00:00:54,080 --> 00:00:55,920
Zhuang responded, I have to work.

15
00:00:57,360 --> 00:01:04,720
Does Zhuang, can Zhuang come to party?

16
00:01:09,520 --> 00:01:12,960
It looks like it has failed. It says it's not possible to say

17
00:01:12,960 --> 00:01:16,480
whether Zhuang can come to the party or not because we don't have enough information.

18
00:01:17,840 --> 00:01:22,080
Zhuang may or may not be able to come to the party depending on his work schedule and other factors.

19
00:01:24,560 --> 00:01:28,400
Yeah, so this is an example of failed implicature.

20
00:01:28,400 --> 00:01:31,360
But anyway, if we come over to Laura's Twitter,

21
00:01:32,400 --> 00:01:36,640
she posted a little thread the other day saying that loads of people have been sending her

22
00:01:36,640 --> 00:01:39,920
implicatures, which they used as examples in the paper.

23
00:01:40,560 --> 00:01:45,200
And apparently chat GPT does understand some of them, which she's very happy about,

24
00:01:45,200 --> 00:01:47,600
but she wanted to write a short thread about it.

25
00:01:47,600 --> 00:01:52,160
So she said before they started writing the paper, she would try lots of implicatures

26
00:01:52,160 --> 00:01:56,880
that she came up with on Da Vinci 2 in different wordings with moderate success.

27
00:01:56,880 --> 00:02:01,120
Some always solved and some half of the time depending on the wording,

28
00:02:01,120 --> 00:02:05,840
meaning random performance since the test is a binary, which is to say a yes or a no.

29
00:02:06,720 --> 00:02:12,320
That's why they decided to do a systematic test to figure out how good it actually was

30
00:02:12,320 --> 00:02:16,000
and how much it depended on the wording of the prompt.

31
00:02:16,560 --> 00:02:20,080
And a few months later, they had the answer that it was okay,

32
00:02:20,080 --> 00:02:21,520
but not close to humans.

33
00:02:22,080 --> 00:02:29,440
And okay means that on Da Vinci 2 and 3, the performance of zero shot implicature is roughly

34
00:02:29,440 --> 00:02:34,880
70%. Most of the other models fail, even with few shot in context prompting.

35
00:02:35,760 --> 00:02:41,280
So anyway, she said that she gets the people are excited that chat GPT is doing pragmatic

36
00:02:41,280 --> 00:02:44,960
inferences, but she felt the same with Da Vinci 2.

37
00:02:44,960 --> 00:02:46,800
It's all anecdotal, she says.

38
00:02:46,800 --> 00:02:51,520
But a more systematic test shows a significant gap with humans nonetheless,

39
00:02:51,520 --> 00:02:56,480
and it's the same for Da Vinci 3 and presumably the same for chat GPT.

40
00:02:56,480 --> 00:03:01,040
She says that once this implicature dataset gets solved, and she has no doubt that it will get

41
00:03:01,040 --> 00:03:05,200
solved relatively soon, since fine tuning with human feedback helps a lot,

42
00:03:05,200 --> 00:03:08,960
they might have some baseline pragmatics in their models.

43
00:03:08,960 --> 00:03:11,200
And that's when it will get really exciting.

44
00:03:11,200 --> 00:03:15,200
She says that she's personally blown away by chat GPT's capabilities.

45
00:03:15,200 --> 00:03:20,880
It's absolutely incredible at explaining things, compositional generalization of concepts,

46
00:03:20,880 --> 00:03:22,240
simulating a VM.

47
00:03:22,880 --> 00:03:28,240
I'm not sure what VM means, coherence, creativity, writing essays, poems and more.

48
00:03:29,200 --> 00:03:34,480
She said that the pragmatic language that they studied is part of a type of casual language

49
00:03:34,480 --> 00:03:38,320
that we're using conversation that might emerge from social interactions.

50
00:03:38,320 --> 00:03:42,320
She's personally thinking about why human feedback helps so much,

51
00:03:42,320 --> 00:03:46,720
and whether interactivity and social pressures might help even more.

52
00:03:46,720 --> 00:03:48,480
Anyway, enjoy the interview.

53
00:03:48,480 --> 00:03:49,040
Hi.

54
00:03:49,040 --> 00:03:50,080
It's lovely to meet you.

55
00:03:50,080 --> 00:03:51,120
Nice to meet you too.

56
00:03:51,120 --> 00:03:56,160
So, I was speaking with Andrew Lampanam yesterday, and he really highly recommended your paper.

57
00:03:56,160 --> 00:03:57,360
I looked it up, it's called,

58
00:03:57,360 --> 00:04:00,960
Large Language Models Are Not Zero Shot Communicators,

59
00:04:00,960 --> 00:04:04,880
and I also recognize Stella Biderman and Sarah Hooker, of course.

60
00:04:04,880 --> 00:04:06,560
Sarah's an absolute legend.

61
00:04:06,560 --> 00:04:09,360
Now, you led in the paper by saying,

62
00:04:09,360 --> 00:04:14,240
humans interpret language using beliefs and prior knowledge about the world.

63
00:04:14,240 --> 00:04:16,960
For example, we intuitively understand the response,

64
00:04:16,960 --> 00:04:21,440
I wore gloves to the question, did you leave fingerprints as meaning no?

65
00:04:22,080 --> 00:04:28,800
So, you call this implicature, but I suppose I would think of it as some kind of

66
00:04:29,920 --> 00:04:34,560
extrapolation, being able to reuse knowledge that we have about the world

67
00:04:34,560 --> 00:04:35,920
in a different situation.

68
00:04:35,920 --> 00:04:37,920
But could you talk us through that paper?

69
00:04:38,640 --> 00:04:39,440
So, yeah, thank you.

70
00:04:39,440 --> 00:04:41,040
That was a great introduction to the paper.

71
00:04:41,760 --> 00:04:47,360
Indeed, implicature is kind of the technical term that we use for this example that you gave.

72
00:04:48,720 --> 00:04:52,800
And indeed, extrapolation is a sensible way to describe this.

73
00:04:52,800 --> 00:04:58,160
What we do in this paper is kind of show that large language models are not really good at this

74
00:04:58,160 --> 00:05:02,880
aspect of communication, and we think it's a very important aspect of communication.

75
00:05:02,880 --> 00:05:06,880
So, the title says, Large Language Models Are Not Zero Shots Communicators, right?

76
00:05:07,440 --> 00:05:13,600
So, what we mean by that is to be a communicator, you have to infer the meaning of utterances,

77
00:05:14,400 --> 00:05:20,000
not only by their semantics, so not only by how words combine into some kind of meaning,

78
00:05:20,000 --> 00:05:24,480
but by interpreting the shared knowledge, our shared experience of the world.

79
00:05:25,600 --> 00:05:27,920
And that's what we look at in this paper.

80
00:05:27,920 --> 00:05:32,880
And what we find is that large language models are pretty bad at this.

81
00:05:33,840 --> 00:05:37,600
Specifically, we group them into different groups.

82
00:05:37,600 --> 00:05:41,760
So, we have base large language models like OPT and BLOOM that are just trained on

83
00:05:41,760 --> 00:05:43,280
next-word prediction.

84
00:05:43,280 --> 00:05:52,400
And we also have instructable models like Flonty5, T0, or DaVinci Instructable Models by OpenAI.

85
00:05:53,360 --> 00:05:58,160
And all models perform really bad, closer to random than to humans.

86
00:05:58,800 --> 00:06:04,880
But OpenAI's instructable models have much more promise.

87
00:06:04,880 --> 00:06:05,760
They're much better at it.

88
00:06:06,480 --> 00:06:06,960
Interesting.

89
00:06:06,960 --> 00:06:10,560
Okay, so now the zero shot thing is very interesting.

90
00:06:10,560 --> 00:06:14,000
So, we take these models, and it's kind of like self-supervised learning.

91
00:06:14,000 --> 00:06:16,560
We train them on loads of data on the Internet.

92
00:06:16,560 --> 00:06:21,600
And you're saying that zero shot is when we don't really give much information in the prompt.

93
00:06:21,600 --> 00:06:26,320
So, there's a relationship between how big the model is and how much in-context learning

94
00:06:26,400 --> 00:06:28,240
that we give to the model in the prompt.

95
00:06:28,240 --> 00:06:29,600
Yeah, that's true.

96
00:06:29,600 --> 00:06:36,000
Yeah, the zero shot case that we tested is we give the model a short instruction saying like,

97
00:06:37,600 --> 00:06:43,280
in the following exchange, someone gives a response that has some meaning beyond the utterances.

98
00:06:44,400 --> 00:06:47,600
It had the meaning is yes or no, can you resolve this?

99
00:06:47,600 --> 00:06:49,440
And then we give an example.

100
00:06:50,720 --> 00:06:55,280
And then we evaluate it on ways based on whether it can choose yes or no.

101
00:06:56,400 --> 00:06:59,840
So, that's the zero shot case, and humans, we don't give any instructions at all.

102
00:06:59,840 --> 00:07:04,080
We just said, resolve this to a yes or no.

103
00:07:04,640 --> 00:07:10,480
Okay, so, is it the case that large language models then zero shot almost irrespective of their size

104
00:07:10,480 --> 00:07:13,200
and irrespective of this human feedback alignment?

105
00:07:13,200 --> 00:07:15,440
They just don't perform very well at this implicature at all.

106
00:07:16,160 --> 00:07:21,120
The instructable models by OpenAI gets a non-trivial performance.

107
00:07:21,840 --> 00:07:26,400
I think the models like OPT and Bloom, those kind of base models,

108
00:07:27,200 --> 00:07:29,680
they really conduce this style very well at all.

109
00:07:29,680 --> 00:07:37,520
They get 10% above random, but OpenAI's models are around 70% at zero shots.

110
00:07:37,520 --> 00:07:38,000
Interesting.

111
00:07:38,000 --> 00:07:42,640
So, did you do some work looking at, okay, well, let's try some in-context learning.

112
00:07:42,640 --> 00:07:44,320
Does that improve the implicature?

113
00:07:44,960 --> 00:07:45,840
Definitely, yeah.

114
00:07:46,400 --> 00:07:48,800
Like, it's unclear, right?

115
00:07:48,800 --> 00:07:53,600
Whether zero shots is a fair comparison to humans for these models.

116
00:07:53,600 --> 00:07:56,240
Humans are primed in different ways.

117
00:07:56,240 --> 00:08:00,000
So, we also wanted to try view shots in context learning.

118
00:08:00,640 --> 00:08:05,520
And personally, I thought in this case, in context learning wouldn't help much because

119
00:08:05,520 --> 00:08:09,120
each implicature requires a completely novel type of inference.

120
00:08:09,760 --> 00:08:14,560
But in fact, we show that OpenAI's models is the only group of models that really

121
00:08:15,280 --> 00:08:17,040
benefits from this a lot.

122
00:08:17,040 --> 00:08:21,200
And they can get to up to 80% performance with roughly five examples,

123
00:08:21,760 --> 00:08:24,880
which, and afterwards, more than five examples, it's kind of plateaus.

124
00:08:26,240 --> 00:08:30,160
But they're still like a significant gap with humans.

125
00:08:30,160 --> 00:08:32,720
But it's a great improvement.

126
00:08:32,720 --> 00:08:33,520
Yeah, that's fascinating.

127
00:08:33,520 --> 00:08:37,360
So, can you give me an example of, if we were doing some in-context learning,

128
00:08:37,360 --> 00:08:41,040
let's say with DaVinci 2, what would that prompt look like?

129
00:08:42,000 --> 00:08:47,760
So, if we, I don't exactly remember the wording of the prompts,

130
00:08:47,760 --> 00:08:51,520
but there would be something like the following are examples of the task.

131
00:08:51,520 --> 00:08:55,920
And then you get a bunch of implicatures that are already resolved to a yes or no.

132
00:08:56,560 --> 00:09:01,040
And then you get the original instruct prompt that says

133
00:09:02,160 --> 00:09:05,040
resolve the following sentence to a yes or no.

134
00:09:05,040 --> 00:09:07,440
And then you get the actual example.

135
00:09:07,440 --> 00:09:10,560
And these in-context examples are all taken from a development set.

136
00:09:11,120 --> 00:09:14,720
Okay, so, can you tell us a little bit about

137
00:09:14,720 --> 00:09:18,480
how this reinforcement learning for human preferences works on language models?

138
00:09:19,200 --> 00:09:22,800
So, reinforcement learning for human preferences is a method to fine-tune

139
00:09:23,760 --> 00:09:24,800
based on our language models.

140
00:09:24,800 --> 00:09:28,880
So, the based on our language models are OPT and BLU, for example,

141
00:09:28,880 --> 00:09:30,240
that's part of the group.

142
00:09:30,240 --> 00:09:32,400
And they are just trying some next-word prediction, right?

143
00:09:32,400 --> 00:09:34,080
But they are not really aligned.

144
00:09:34,080 --> 00:09:38,000
They're sort of this alignment problem where they're trained on next-word prediction

145
00:09:38,000 --> 00:09:40,560
and that's not really what we are asking them to do.

146
00:09:41,680 --> 00:09:45,440
And then with reinforcement learning from human feedback,

147
00:09:45,440 --> 00:09:50,080
what we further do, I mean, not we, unfortunately, other people do,

148
00:09:50,080 --> 00:09:55,040
is they take some kind of human preferences from somewhere.

149
00:09:55,040 --> 00:09:59,200
Like, for example, humans are shown prompts and completions by models

150
00:09:59,200 --> 00:10:01,360
and they say, this one is better than that one.

151
00:10:01,360 --> 00:10:04,960
This completion for the text for this prompt is better than that one.

152
00:10:05,520 --> 00:10:08,800
By that, we get a sort of ranking by preference

153
00:10:08,800 --> 00:10:11,440
and we can learn a reward model on those preferences

154
00:10:12,640 --> 00:10:15,520
with an interesting trick that was published in 2017.

155
00:10:16,640 --> 00:10:19,760
And through this rewards model, we have sort of,

156
00:10:19,760 --> 00:10:26,000
we can bootstrap the preferences from humans into the based archangels model

157
00:10:26,000 --> 00:10:31,440
by fine-tuning them with regular RL on this reward model.

158
00:10:31,440 --> 00:10:32,720
Yeah, it's really interesting.

159
00:10:32,720 --> 00:10:37,520
I was speaking with Srijan Kumar who won one of the outstanding

160
00:10:37,520 --> 00:10:41,360
paper awards in Europe and he's got this work on kind of,

161
00:10:42,400 --> 00:10:45,360
we want the models to be more anthropomorphic

162
00:10:45,360 --> 00:10:48,320
and we have these priors to help us understand the world.

163
00:10:48,320 --> 00:10:53,360
And he came up with a framework of kind of like importing these priors

164
00:10:53,360 --> 00:10:57,920
from language encodings into, let's say, a discrete program synthesis model.

165
00:10:57,920 --> 00:11:00,960
But I guess what I'm saying is that there's something really interesting

166
00:11:00,960 --> 00:11:05,280
going on within context learning and it's almost like we're giving the model

167
00:11:05,280 --> 00:11:11,040
the priors to extrapolate or to do something useful in this particular situation.

168
00:11:11,040 --> 00:11:12,800
Yeah, yeah, that's really interesting.

169
00:11:12,800 --> 00:11:14,480
I don't know the paper, I should check it out.

170
00:11:14,480 --> 00:11:19,360
But the way I view it, and my thinking has been shaped this week also

171
00:11:19,360 --> 00:11:23,120
by Andrew Lampinen who wrote an interesting paper on comparing models in humans,

172
00:11:23,680 --> 00:11:28,880
is that it seems that in context learning for this specific task, implicatures,

173
00:11:28,880 --> 00:11:32,800
it's not really that they learn how to use their shared experience

174
00:11:33,600 --> 00:11:36,640
from the in-concept samples, they're primed for the task

175
00:11:36,640 --> 00:11:38,880
with a few shot examples in the context.

176
00:11:39,440 --> 00:11:42,160
And I think that's actually what's happening here.

177
00:11:42,160 --> 00:11:46,240
Like if you test the model zero shots, there's no,

178
00:11:46,240 --> 00:11:49,840
why would we expect it to do this task properly?

179
00:11:49,840 --> 00:11:51,840
There's no motivation or anything like that.

180
00:11:51,840 --> 00:11:56,480
But if you prime it with in-concept samples, it does better.

181
00:11:56,480 --> 00:12:02,560
And that would also explain why it doesn't help to add more than five examples

182
00:12:02,560 --> 00:12:06,480
because it's not using the inherent information in the examples,

183
00:12:06,480 --> 00:12:08,880
it's just being primed for this specific task.

184
00:12:08,880 --> 00:12:10,000
Yeah, that's really interesting.

185
00:12:10,000 --> 00:12:13,840
Sarah has done lots of work on interpretability in machine learning models.

186
00:12:13,840 --> 00:12:19,280
And one thing that I wrestle with a lot is whether we should try and get models

187
00:12:19,280 --> 00:12:21,040
to think the way humans do.

188
00:12:21,040 --> 00:12:25,360
And you can come at it from an intelligibility point of view,

189
00:12:25,360 --> 00:12:28,080
but you could also come at it from a generalization point of view.

190
00:12:28,080 --> 00:12:31,440
Like maybe we do symbolic generalization over cognitive priors,

191
00:12:31,440 --> 00:12:32,960
and that's how we understand the world.

192
00:12:32,960 --> 00:12:36,320
But there are people who just say large language models,

193
00:12:36,320 --> 00:12:37,920
they're just a different mode of understanding,

194
00:12:37,920 --> 00:12:39,280
and we shouldn't try and make them like us.

195
00:12:39,280 --> 00:12:40,080
Like what do you think?

196
00:12:42,240 --> 00:12:43,360
It's a good question.

197
00:12:43,360 --> 00:12:46,480
I am really a non-expert on interpretability.

198
00:12:46,480 --> 00:12:51,120
I'm like, I always come at it from a very anthropocentric view.

199
00:12:51,120 --> 00:12:53,040
Like I would love them to be more like humans

200
00:12:53,040 --> 00:12:56,400
because that would make them interesting subjects to study also

201
00:12:56,400 --> 00:12:58,960
and better to communicate with.

202
00:12:58,960 --> 00:13:01,680
But at the same time, you can take this opposite view.

203
00:13:01,680 --> 00:13:06,000
And I think Stella, the co-author on this paper often says,

204
00:13:06,880 --> 00:13:09,920
you're making a category area, you're attributing something to these models

205
00:13:09,920 --> 00:13:13,440
that they don't have knowledge, those kind of things.

206
00:13:13,440 --> 00:13:18,000
So it might also very well be that we're trying to look for pragmatics

207
00:13:18,000 --> 00:13:19,600
or semantic understanding in these models,

208
00:13:19,600 --> 00:13:22,000
but that's just not how you should think about it.

209
00:13:22,640 --> 00:13:24,400
And I completely forgot to ask you.

210
00:13:24,640 --> 00:13:28,560
So again, some of the audience don't know about

211
00:13:28,560 --> 00:13:30,480
natural language understanding in linguistics and so on.

212
00:13:30,480 --> 00:13:32,720
So what is pragmatics?

213
00:13:32,720 --> 00:13:34,000
Yeah, that's a good question.

214
00:13:34,000 --> 00:13:41,840
So pragmatics is an aspect of language, the way we study language,

215
00:13:41,840 --> 00:13:46,160
that doesn't really look at the syntax or semantics,

216
00:13:47,520 --> 00:13:52,000
which look at, for example, those kind of aspects of linguistics,

217
00:13:52,000 --> 00:13:55,920
look at what a word means and how you combine them into novel meanings.

218
00:13:55,920 --> 00:13:59,120
So those kind of areas of linguistics really look at

219
00:13:59,760 --> 00:14:03,840
when someone understands the term, the utterance as John loves Mary,

220
00:14:03,840 --> 00:14:06,080
they also understand utterance Mary loves John.

221
00:14:06,640 --> 00:14:08,720
Pragmatics goes beyond that.

222
00:14:08,720 --> 00:14:15,520
It looks at how context and our shared experience really influences meaning.

223
00:14:15,520 --> 00:14:19,360
So usually the meaning determined by pragmatic inference

224
00:14:19,360 --> 00:14:23,040
is not really directly part of the context window.

225
00:14:23,040 --> 00:14:28,080
You really have to tap into your prior knowledge.

226
00:14:29,280 --> 00:14:31,600
Yeah, so I'm a fan of Montague as well.

227
00:14:31,600 --> 00:14:36,240
So it's almost like we have the semantic potential and then we have pragmatics,

228
00:14:36,240 --> 00:14:39,760
and that's bringing some additional context.

229
00:14:39,760 --> 00:14:40,880
Yeah, yeah, exactly.

230
00:14:42,880 --> 00:14:47,840
Okay, and because that's a really great example from that extrapolative example

231
00:14:47,840 --> 00:14:49,440
from Montague about Mary loves John,

232
00:14:50,640 --> 00:14:52,880
how could a large language model realistically,

233
00:14:52,880 --> 00:14:56,160
because I think of that as being a symbolic generalization.

234
00:14:56,160 --> 00:14:58,640
So how could a language model do that kind of generalization?

235
00:14:59,200 --> 00:15:00,400
Symbolic generalization?

236
00:15:00,400 --> 00:15:01,200
Yes.

237
00:15:01,200 --> 00:15:02,160
Oh, that's a big one.

238
00:15:05,360 --> 00:15:06,160
I don't know.

239
00:15:06,160 --> 00:15:07,120
I really, really don't know.

240
00:15:08,400 --> 00:15:11,200
In my research journey, I can kind of

241
00:15:12,000 --> 00:15:14,080
come from studying compositionality in language,

242
00:15:14,080 --> 00:15:17,200
which is really more this type of thing that we're talking about now.

243
00:15:17,840 --> 00:15:22,960
And looking at more sort of neurosymbolic approaches or stronger inductive biases.

244
00:15:22,960 --> 00:15:26,720
And now these large language models really showed us that

245
00:15:27,280 --> 00:15:31,360
there is an insane amount of compositional generalization going on

246
00:15:31,360 --> 00:15:33,280
without any inductive bias for that.

247
00:15:33,840 --> 00:15:36,720
Chat GPT kind of shows us that with all these examples on Twitter,

248
00:15:36,720 --> 00:15:39,280
right, you give it two novel concepts,

249
00:15:39,280 --> 00:15:43,040
and it combines it beautifully into some kind of story.

250
00:15:44,240 --> 00:15:46,560
But yeah, to go back to your question, how can they do it?

251
00:15:46,560 --> 00:15:50,640
I don't know. Maybe Skill will get us there to the sense that

252
00:15:51,280 --> 00:15:53,840
humans are also imperfect symbolic reasoners.

253
00:15:53,840 --> 00:15:55,280
Again, to mention Andrew Lampinen,

254
00:15:55,280 --> 00:15:58,640
he did a great paper on symbolic behavior,

255
00:15:58,640 --> 00:16:00,880
where it's not really a discrete,

256
00:16:00,880 --> 00:16:04,560
I can do symbolic processing versus I cannot do symbolic processing,

257
00:16:04,560 --> 00:16:06,000
but it's more a scale.

258
00:16:07,360 --> 00:16:08,800
That's kind of shaped my thinking as well.

259
00:16:08,800 --> 00:16:10,160
I think it's a scale.

260
00:16:10,160 --> 00:16:12,880
Large language models are pretty far on that scale.

261
00:16:12,880 --> 00:16:15,760
They can do very interesting compositional generalization.

262
00:16:16,720 --> 00:16:18,800
And sort of symbolic behavior,

263
00:16:18,800 --> 00:16:22,000
but they fail in catastrophic ways as well.

264
00:16:22,880 --> 00:16:26,000
Again, an example that I think comes from Gary Marcus is

265
00:16:26,000 --> 00:16:27,760
when you ask Chat GPT,

266
00:16:30,960 --> 00:16:33,280
how do horses ride cowboys?

267
00:16:33,280 --> 00:16:37,120
And it just writes a whole story about how a horse rides a cowboy,

268
00:16:37,120 --> 00:16:39,600
even though it doesn't work.

269
00:16:40,320 --> 00:16:41,520
Yeah, it's so interesting,

270
00:16:41,520 --> 00:16:45,120
because I think it's easy to think of large language models in the binary.

271
00:16:45,840 --> 00:16:47,360
Marcus says they're bloviators,

272
00:16:48,800 --> 00:16:50,720
and Bender says they're stochastic parrots.

273
00:16:51,280 --> 00:16:54,880
And then you have the folks who think that it's emergent reasoning

274
00:16:54,880 --> 00:16:56,560
and symbolic generalization.

275
00:16:56,560 --> 00:17:00,240
And I was a skeptic, and I just can't ignore the evidence.

276
00:17:00,240 --> 00:17:03,760
They really are doing amazing things now.

277
00:17:04,720 --> 00:17:06,480
And you were just speaking to Lampinen,

278
00:17:06,480 --> 00:17:10,800
and it's a similar thing with this idea of symbolic generalization.

279
00:17:10,800 --> 00:17:12,080
It might not be a binary, right?

280
00:17:13,120 --> 00:17:13,840
Yeah, exactly.

281
00:17:14,480 --> 00:17:16,400
It might be a very great as competency.

282
00:17:17,440 --> 00:17:19,760
And humans also fail in certain cases.

283
00:17:20,720 --> 00:17:22,880
So on this in-context learning,

284
00:17:22,880 --> 00:17:24,960
because that's something that has interested me.

285
00:17:24,960 --> 00:17:28,000
So the first version of GPT-3 is zero-shot.

286
00:17:28,000 --> 00:17:29,600
We didn't really know how to prompt it.

287
00:17:29,600 --> 00:17:31,280
It looked like a bloviator.

288
00:17:31,280 --> 00:17:33,040
We then went on this intellectual journey,

289
00:17:33,040 --> 00:17:36,160
and we discovered front engineering, scratch pad,

290
00:17:36,160 --> 00:17:37,360
chain of thought reasoning.

291
00:17:38,000 --> 00:17:39,760
I spoke with Hattie Zoe the other day,

292
00:17:39,760 --> 00:17:43,040
and she's got this kind of algorithmic front in-context learning.

293
00:17:43,040 --> 00:17:45,680
And it's just remarkable what's going on there.

294
00:17:46,720 --> 00:17:47,920
Do you have any intuition?

295
00:17:48,960 --> 00:17:52,960
Is it like the prompt is some kind of a program interpreter or something?

296
00:17:55,760 --> 00:17:58,640
My intuition is rather that the prompt kind of...

297
00:17:59,360 --> 00:18:01,920
I don't know how to formalize this intuition,

298
00:18:01,920 --> 00:18:03,600
but I guess that's why it's an intuition.

299
00:18:04,640 --> 00:18:08,080
That the prompt kind of primes the model

300
00:18:08,080 --> 00:18:11,440
and puts it into a sort of area of its weight space,

301
00:18:11,440 --> 00:18:17,680
where it can better answer the actual question

302
00:18:17,680 --> 00:18:21,440
that is asked in the actual question that's asked.

303
00:18:22,000 --> 00:18:25,120
And I think certain things that point towards this

304
00:18:25,120 --> 00:18:28,160
is that there is also some research coming out

305
00:18:28,160 --> 00:18:33,520
where they permute the labels in the in-context examples,

306
00:18:33,520 --> 00:18:35,520
and they show that the performance is similarly good,

307
00:18:36,160 --> 00:18:39,520
even though at the same or like they do completely random labels

308
00:18:39,600 --> 00:18:43,280
in the in-context examples, and the performance is still pretty good.

309
00:18:43,280 --> 00:18:47,040
But there's also other work that shows that doesn't always work.

310
00:18:47,040 --> 00:18:48,800
Sometimes you do need actual labels.

311
00:18:50,560 --> 00:18:53,520
So yeah, again, the answer is basically, I don't know,

312
00:18:53,520 --> 00:18:56,800
but my intuition is rather that the model is really primed for...

313
00:18:58,480 --> 00:19:01,040
Or there's also another great way of viewing this,

314
00:19:02,400 --> 00:19:04,880
and I read that unless wrong at some point.

315
00:19:04,880 --> 00:19:07,120
I don't know how to attribute the author,

316
00:19:07,840 --> 00:19:10,480
because I forgot their name, but it's about

317
00:19:11,840 --> 00:19:14,080
that these models are good at simulating anything.

318
00:19:14,080 --> 00:19:16,880
So you have to sort of prime them to let them know

319
00:19:16,880 --> 00:19:18,080
what they're simulating right now.

320
00:19:19,440 --> 00:19:20,240
Yes, weird, isn't it?

321
00:19:20,240 --> 00:19:25,040
Because we have an anthropocentric view of the world,

322
00:19:25,040 --> 00:19:29,440
and we're agentive with individual agents.

323
00:19:29,440 --> 00:19:32,400
And a language model is everything at once.

324
00:19:32,400 --> 00:19:36,480
So it's almost like you need to give it a trajectory just to get it

325
00:19:36,480 --> 00:19:37,840
to go somewhere interesting.

326
00:19:37,840 --> 00:19:42,160
So with this in mind, we really want to make progress

327
00:19:42,160 --> 00:19:43,920
in natural language understanding.

328
00:19:43,920 --> 00:19:46,720
And what do you think are the steps we need to make

329
00:19:46,720 --> 00:19:50,000
to robustify these language models?

330
00:19:50,000 --> 00:19:51,280
Yeah, that's a good question.

331
00:19:52,000 --> 00:19:54,960
Personally, from this pragmatics paper,

332
00:19:54,960 --> 00:19:59,360
I think pragmatics is one area where they can make huge strides too.

333
00:19:59,360 --> 00:20:01,920
I think even though they have semantic failure modes,

334
00:20:01,920 --> 00:20:03,680
they're really impressive at that.

335
00:20:03,680 --> 00:20:06,400
They're really impressive at compositional generalization.

336
00:20:08,080 --> 00:20:09,600
But pragmatics might be something

337
00:20:09,600 --> 00:20:13,840
that they're simply not trained for currently.

338
00:20:13,840 --> 00:20:18,160
And one very low-hanging fruit is the RLHF that we talked about.

339
00:20:18,960 --> 00:20:21,920
I think that clearly really improves that.

340
00:20:21,920 --> 00:20:26,960
And intuitively, it seems like in the instructivity paper,

341
00:20:26,960 --> 00:20:28,880
you see that they ask the human laborers

342
00:20:28,880 --> 00:20:31,760
to really infer the human intent and the problems and write on.

343
00:20:31,760 --> 00:20:34,480
And that's very reminiscent of implicatures.

344
00:20:35,680 --> 00:20:37,520
But then on a more sort of broader scale,

345
00:20:37,520 --> 00:20:41,040
I think some kind of embodiment or interactivity

346
00:20:41,040 --> 00:20:42,800
might be really important.

347
00:20:42,800 --> 00:20:46,160
Like pragmatic inference is really a social skill that we have.

348
00:20:47,120 --> 00:20:51,360
There's a lot of pragmatic pressures that you encounter

349
00:20:51,360 --> 00:20:54,960
while just acting in the world and navigating communication

350
00:20:54,960 --> 00:20:56,960
and navigating a lot of things.

351
00:20:57,520 --> 00:21:00,000
So I think I'm currently trying to look at

352
00:21:01,680 --> 00:21:04,000
a setup in reinforcement learning

353
00:21:04,000 --> 00:21:06,960
where we're trying to make a pragmatic task

354
00:21:06,960 --> 00:21:09,600
and see when pragmatic reasoning would emerge there.

355
00:21:12,560 --> 00:21:14,720
I don't know how to consolidate that fully

356
00:21:14,720 --> 00:21:15,920
with large language models yet,

357
00:21:15,920 --> 00:21:22,320
but I think interactivity and social navigation is important.

358
00:21:23,280 --> 00:21:27,040
I'm really fascinated by the embedded tradition

359
00:21:27,040 --> 00:21:28,400
in cognitive science.

360
00:21:28,400 --> 00:21:30,080
And I think there's a lot of interesting work there.

361
00:21:30,080 --> 00:21:32,720
But I suspect you do as well a little bit.

362
00:21:33,360 --> 00:21:35,600
How do you contrast what is essentially

363
00:21:35,600 --> 00:21:36,960
the representation of this view

364
00:21:36,960 --> 00:21:39,200
where everything's in this big monolithic model

365
00:21:39,200 --> 00:21:41,200
to some kind of relational view

366
00:21:41,200 --> 00:21:44,000
where we're using essentially the world

367
00:21:44,000 --> 00:21:45,520
as its own representation?

368
00:21:45,520 --> 00:21:47,600
Yeah. Again, I don't know.

369
00:21:47,600 --> 00:21:50,640
To what extent it's also possible to express everything

370
00:21:50,720 --> 00:21:52,880
in just the representation in this view

371
00:21:52,880 --> 00:21:54,560
where you have an internal world model.

372
00:21:54,560 --> 00:21:56,640
And I don't know to what extent

373
00:21:56,640 --> 00:21:58,720
you really need an external world to learn,

374
00:21:58,720 --> 00:22:02,160
but intuitively it seems like that might be very important.

375
00:22:02,160 --> 00:22:08,240
And intuitively it also seems like the behaviors

376
00:22:08,240 --> 00:22:12,320
that can emerge are really limited by the world's models

377
00:22:12,320 --> 00:22:15,360
acting in a large language model only see sex.

378
00:22:16,320 --> 00:22:20,400
And there's basic things that just simply cannot learn,

379
00:22:20,400 --> 00:22:23,120
even though it has surprised us a lot.

380
00:22:24,080 --> 00:22:25,600
So I think, I don't know,

381
00:22:25,600 --> 00:22:28,000
it's easy to think about that it's really important

382
00:22:28,000 --> 00:22:30,240
to have some kind of external world to interact with.

383
00:22:31,440 --> 00:22:35,200
But I'm happy people are working on scaling

384
00:22:35,200 --> 00:22:41,040
and I'm not saying some type of AGI, whatever that means,

385
00:22:41,040 --> 00:22:43,920
might not emerge from simply scaling up

386
00:22:43,920 --> 00:22:45,280
basically what we're doing right now.

387
00:22:46,320 --> 00:22:49,920
Amazing. And are there any other parts of your paper

388
00:22:49,920 --> 00:22:51,120
that we haven't spoken about?

389
00:22:52,960 --> 00:22:56,000
Yeah, one thing that we found pretty interesting

390
00:22:56,000 --> 00:22:59,200
is that even though all these open AIs models

391
00:22:59,200 --> 00:23:02,560
can get really high performance, close to humans,

392
00:23:02,560 --> 00:23:05,760
6% roughly, that won't tell you much without the details.

393
00:23:05,760 --> 00:23:08,480
But it's a significant gap still, but it's really, really close.

394
00:23:08,480 --> 00:23:12,640
I don't know if a human might figure out

395
00:23:12,640 --> 00:23:16,320
whether this model is a model or a human in that case.

396
00:23:16,960 --> 00:23:21,920
But when we sort of drill down and make a taxonomy

397
00:23:21,920 --> 00:23:24,080
of the examples that are in this data set,

398
00:23:24,080 --> 00:23:27,040
we find that they are mostly benefiting

399
00:23:27,040 --> 00:23:30,720
from the simple examples where not a lot of context is needing.

400
00:23:30,720 --> 00:23:36,080
So one example is an implicator is if you ask me

401
00:23:36,640 --> 00:23:38,240
how many people came to your party,

402
00:23:38,240 --> 00:23:39,920
and then I say some people came.

403
00:23:40,560 --> 00:23:42,960
It's really the conventional meaning kind of of the word

404
00:23:42,960 --> 00:23:45,280
some that I meant not all people came.

405
00:23:45,840 --> 00:23:48,800
But it's still an implicator, but it's a very common one.

406
00:23:48,800 --> 00:23:51,120
So those kind of examples, if we isolate those

407
00:23:51,120 --> 00:23:53,680
and we look at specifically examples

408
00:23:53,680 --> 00:23:55,680
that are really context-dependent like

409
00:23:58,400 --> 00:24:01,440
are you coming to the open AI party tonight?

410
00:24:01,440 --> 00:24:03,040
I have food poisoning.

411
00:24:03,040 --> 00:24:06,080
Those need much more context to be resolved.

412
00:24:06,080 --> 00:24:07,920
And then the performance decreases again,

413
00:24:07,920 --> 00:24:13,360
and there is roughly a 9% gap, which like the best model,

414
00:24:13,440 --> 00:24:17,280
but all other based models and instructable models

415
00:24:17,280 --> 00:24:18,560
like Flancy V and stuff,

416
00:24:18,560 --> 00:24:21,920
they then again completely fail on those kind of examples.

417
00:24:21,920 --> 00:24:22,560
Fascinating.

418
00:24:23,280 --> 00:24:25,200
I'm really interested by this idea that

419
00:24:25,200 --> 00:24:27,680
understanding is a complex phenomenon,

420
00:24:27,680 --> 00:24:31,200
and it's like the parable of the blind man and the elephant.

421
00:24:31,200 --> 00:24:33,200
So we create all of these metrics,

422
00:24:33,840 --> 00:24:36,320
and the metrics exclude most of the truth.

423
00:24:36,320 --> 00:24:40,240
And the metrics for pragmatics presumably are in some sense

424
00:24:40,240 --> 00:24:41,840
even more complicated than the metrics

425
00:24:41,840 --> 00:24:43,920
that we already use in natural language understanding.

426
00:24:43,920 --> 00:24:48,960
And it just feels like is that going to be a serious problem

427
00:24:48,960 --> 00:24:52,560
for us to kind of encapsulate how well the model understands?

428
00:24:53,360 --> 00:24:57,440
Do you mean that we're sort of giving it a test

429
00:24:57,440 --> 00:24:58,560
that is couldn't really solve?

430
00:24:59,280 --> 00:25:01,520
Well, I suppose one way of looking at it is,

431
00:25:01,520 --> 00:25:04,240
in this particular test, we've come up with lots of examples

432
00:25:04,240 --> 00:25:06,960
of pragmatic inference, if you like.

433
00:25:06,960 --> 00:25:09,760
And what we're doing is we're taking a very complex phenomenon

434
00:25:09,760 --> 00:25:12,320
and we're putting pins through it,

435
00:25:12,320 --> 00:25:15,360
so we're putting like little slices through it in different angles.

436
00:25:15,360 --> 00:25:18,000
And then we've got this shortcut problem

437
00:25:18,000 --> 00:25:20,720
that if we optimize on any one of those slices,

438
00:25:20,720 --> 00:25:24,320
we might be kind of like excluding everything else that's important.

439
00:25:24,320 --> 00:25:25,280
So it feels like we've got,

440
00:25:26,800 --> 00:25:30,240
is this like a general problem in natural language understanding?

441
00:25:30,240 --> 00:25:34,240
It seems like you're getting at evaluation to some extent, right?

442
00:25:34,480 --> 00:25:39,680
I think evaluation is the single most difficult thing in NLP.

443
00:25:41,280 --> 00:25:43,760
This is just a benchmark to give us some intuition

444
00:25:43,760 --> 00:25:46,800
as to what these current failure models,

445
00:25:47,360 --> 00:25:49,120
failure modes of these models might be.

446
00:25:49,680 --> 00:25:53,520
And I think if this benchmark is at some point passed

447
00:25:53,520 --> 00:25:57,280
by a model that's in and of itself

448
00:25:57,280 --> 00:25:59,600
without trying to diminish my own paper, it doesn't tell us much.

449
00:25:59,600 --> 00:26:00,640
There's much more to be done.

450
00:26:00,640 --> 00:26:02,560
We need more different benchmarks.

451
00:26:02,560 --> 00:26:05,360
We need like human testing in a sort of Turing style maybe.

452
00:26:06,800 --> 00:26:12,000
And yeah, I think this is the most interesting problem in NLP,

453
00:26:12,000 --> 00:26:13,440
like how the property evaluates.

454
00:26:14,080 --> 00:26:14,880
Interesting.

455
00:26:14,880 --> 00:26:18,320
And do you take an interest in fairness and bias in the models as well?

456
00:26:18,880 --> 00:26:22,480
I'm very interested in it, but from a sort of spectator view as well.

457
00:26:22,480 --> 00:26:24,560
Like I haven't worked on it at all.

458
00:26:24,560 --> 00:26:27,600
Okay, yeah, because that's presumably a massive challenge.

459
00:26:27,600 --> 00:26:30,320
Yeah, yeah, definitely, yeah.

460
00:26:30,400 --> 00:26:31,280
Amazing.

461
00:26:31,280 --> 00:26:37,440
So in final question, what are you most excited about in your research trajectory

462
00:26:37,440 --> 00:26:38,240
over the next year or so?

463
00:26:40,080 --> 00:26:44,000
Well, definitely, I just feel super excited to be working on stuff like this

464
00:26:44,000 --> 00:26:47,520
currently given the capabilities these models show.

465
00:26:47,520 --> 00:26:49,520
Like they're absolutely amazing.

466
00:26:49,520 --> 00:26:52,240
And I love seeing how people interact with them.

467
00:26:52,240 --> 00:26:56,160
Like the creativity of people is really needed to get some kind of interesting

468
00:26:56,160 --> 00:26:57,680
response out of these models, right?

469
00:26:57,680 --> 00:27:01,760
And also the creativity of people is needed to find the failure modes.

470
00:27:01,760 --> 00:27:08,320
And yeah, so what I'm most interested, excited about now personally for my own

471
00:27:08,320 --> 00:27:13,920
research journey is really trying to look at, you know, an interactive setup

472
00:27:13,920 --> 00:27:20,480
and see when pragmatic inferences might emerge in what kind of environments,

473
00:27:20,480 --> 00:27:25,040
what kind of pressures do we need, and how can we translate that back to getting to be,

474
00:27:25,520 --> 00:27:30,080
to getting like language models be zero shells communicators.

475
00:27:30,080 --> 00:27:30,480
Amazing.

476
00:27:30,480 --> 00:27:32,000
And where can people find out more about you?

477
00:27:33,680 --> 00:27:38,240
They can follow me on Twitter is first name, last name, and I have a website,

478
00:27:38,240 --> 00:27:39,840
also firstname, lastname.com.

479
00:27:40,560 --> 00:27:41,280
Amazing.

480
00:27:41,280 --> 00:27:42,240
Laura, thank you so much.

481
00:27:42,240 --> 00:27:43,280
It's a pleasure to meet you.

482
00:27:43,280 --> 00:27:44,400
Thank you for having me.

483
00:27:44,400 --> 00:27:44,880
Amazing.

484
00:27:44,880 --> 00:27:45,120
Amazing.

485
00:27:45,120 --> 00:27:45,360
Right.

486
00:27:45,360 --> 00:27:45,760
Cool.

