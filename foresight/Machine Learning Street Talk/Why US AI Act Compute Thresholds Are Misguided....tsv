start	end	text
0	3200	Um, Sarah, it's amazing to have you back on MLST.
3200	8400	It's so lovely to be here. It's been a year and a half or something since our last conversation.
8400	13600	Yes, it has. Yeah, because I think, um, we met at NeurIPS and then, and then I came in
13600	17440	filming to be in the London office, which is really good. Um, but fans of the show, of course,
17440	20720	will know that our first interview was about your hardware lottery paper.
20720	21280	Yeah.
21280	23520	And that was your first grumpy essay.
23520	28240	That was a very grumpy essay. You know, I lead career for AI, so it's a research
28240	34560	lab. We do a lot of fundamental research, uh, and we, a lot of my work is on efficiency, um,
34560	37920	reliability, and building these models that scale the next generation models.
37920	41360	So you can go to Co-Here for AI and take a look at some of our work.
42160	48240	Sarah Hooker is VP of research at Co-Here, and she leads Co-Here for AI, a research lab,
48240	53520	which seeks to solve complex machine learning problems. Co-Here for AI supports fundamental
53520	59520	research, which explores the unknown. She leads a team of researchers and engineers working on
59520	65920	making large language models more efficient, safe, and grounded. In this conversation, Sarah
65920	71200	discusses her recent work on multilingual AI and the challenges of developing language models,
71200	76480	which work across many different languages. She provides insights into the limitations
76480	82720	of current approaches like RLHF, especially for low resource languages. Sarah also talks about
82720	88320	her recent paper critiquing the use of compute thresholds as an AI governance strategy,
88320	94240	explaining why simple measures like flops are inadequate for assessing AI capabilities and
94240	100480	risks. Sarah emphasizes the importance of understanding the relationship between compute,
100480	107520	data, and model architectures. She advocates for a more nuanced approach to AI development
107520	113280	and governance, which considers the complexities of language, culture, and the representational
113280	119120	long tail, where all the low frequency data lives, which is so often neglected in current models.
120000	125600	Sarah's work aims to make AI more globally representative and equitable, as these technologies
125600	130160	become increasingly integrated into society. Enjoy the show.
130160	139040	Your most recent grumpy paper is called On the limitations of compute thresholds as a
139040	142480	governance strategy. Can you give us the elevator pitch?
143760	153760	So this paper is, it has a very boring title. And at face value, it's just about this kind of
154480	159680	odd, known to not many people in the public, compute thresholds that have actually been
159680	165360	widely adopted. They were adopted by the executive order on AI, they were adopted by the EU AI Act.
165920	171040	And what's fascinating is that these are kind of the key policies that have come out on AI.
172560	178320	Why did I write a paper about this very, very deep topic of compute thresholds?
179440	185120	Because it's at the heart of really what our field is asking right now, which is that compute
185120	192560	thresholds are based on an idea that models at a future size, so it doesn't apply to models in the
192560	199520	well now, are going to trigger some difference in risk profile that deserves scrutiny. And this
199520	208000	question of, does scale trigger this moment where models have these properties that are
208000	213840	fundamentally different from models before that? It is actually very much being at the core of our
213840	219200	field for the last two decades. Because in the last two decades, we've had this philosophy of
219200	225360	bigger is better, we scale data and we scale model size. So this essay is really about,
225360	231600	is that true? As we look and stand and look at the last decade, what do we know about the relationship
231600	238000	between compute and risk? And what do we think is the feasibility of these compute thresholds
238000	241920	actually mitigating risk? And that was the starting point.
241920	246800	Yeah, so in the beginning, you were talking about how historically we have tried to estimate and
246800	251920	control and respond to risk. Can you give us a couple of examples?
251920	259280	Mostly as a society, we have tried to grapple with this idea that we want to proactively control
259280	264320	our future for the better. And this is actually recent as well. So it's very typical of modern
264320	270320	society that we have this notion of planning and anticipating risks and being able to mitigate.
270320	277040	There's examples where me and you do this every day, right? We could put on sunscreen if we're
277040	283280	knowing we're going to the sun. We avoid working in dark areas. There's also areas where governments
283280	292000	have done this, you know, even in this modern era of the last 300, 400 years. And it requires two
292000	296480	things to do well. One is that you have to understand where risk comes from. So you have to
296480	303360	understand what is the kind of lever of risk. A good example of where that's failed is something
303360	310080	like the Black Death, where for example, a lot of the protocols around the time didn't realize
310080	314640	that rats were the main vector of the disease. And so because of that, many of the mitigation
314640	320400	techniques were unsuccessful. But the second crucial aspect is that once you've identified the lever
320400	326800	of risk, you have to form a proportionate response. And we also have examples where that's failed
326800	332880	historically. So for example, the London fire is a great example where it was known that this was a
332880	338160	risk, but the fail to curb it early on in the expansion of the fire led to the destruction of
338160	344080	a large part of London. So these are the two challenges that policymakers face. And what
344160	350000	compounds it for something like technology is that typically, the idea of identifying the
350000	355440	lever of risk is very difficult, because most technology breakthroughs, by the nature of
355440	361680	being a breakthrough, you're in a kind of rather than a proactive setting, you're in a retroactive
361680	366400	setting. What do we do now that this is changing the world? And that's a very difficult position
366400	372240	for someone to form a response to. Yes, exactly. I mean, you know, one of the
372240	378080	themes of the paper is we're super bad at predicting the future. And maybe we should just
378080	383600	linger just, you know, just for a second on the the executive order and the EU AI Act. Now,
383600	390160	they used this notion called flops. And please explain what flops are in a second. But the
390720	395440	in America, they set the limit to I think 10 to the 26. Is that right? And then in the EU,
395440	400160	it was they wanted to be a little bit more strict. So they just went down to 25.
400400	406960	Yeah. Tell me about that. So flops, by the way, is this measure by which this compute threshold
406960	411840	is done? I think flops is just a it's a way of counting. So typically, when you train a model,
411840	416240	you're doing many different operations, you're doing additions, subtraction, multiplication,
416240	421360	famously matrix multiplies, dominate our modern networks. And so that can be decomposed into
421360	427120	all these operations. So flops is just a tally, it just counts it up. And these thresholds,
428080	434480	10 to the 26 and 10 to the 25 are this idea that at that moment, that's when you kick in scrutiny.
434480	438800	And it's important to realize that doesn't apply to models in the wild right now.
439440	443360	For the executive order, for the EU AI Act, when it comes into effect next year,
443360	449360	it might hit a handful of models. But this is a for looking policy. It is not based on current
449360	454080	risk in the wild. And so that's interesting to think about, because that creates the question of,
454080	458320	well, are we good at predicting what risks emerge? And is that the right number to do it at? And
458320	465120	that's where it starts to get very interesting. Yeah, yes. So they have a tally, they kind of
465120	470480	estimate how much computation is happening in the models. And then they've set this threshold. So
470480	474960	they don't care about anything below that number. So there's lots of real risk now that they
474960	479360	presumably don't care about. And then they're saying above this number, there's a problem. And I
479360	483760	think did they set the number roughly commensurate to the size of a GPT-4 model?
483760	490240	So it's difficult because they haven't formally justified why they set the number there. But
491120	497440	anecdotally, my understanding is that guided it. So it's interesting. And it's worth thinking about,
498640	504800	well, it's this interesting aspect of, well, firstly, there's a notion of, is this number
504880	510720	valid tally of risk? Like, is training compute the number that you care about if you wanted to
510720	517840	do this tally, if you believed in this future risk? But secondly, if are we good at predicting
517840	522560	like that number? And that's kind of interesting to think about. Yeah, I mean, to me, it was
522560	528800	a bit crazy on its face. And what's going through my mind is, have they got anyone working in the
528800	532960	government that actually know what they're talking about? Because presumably, if they asked you,
532960	537440	you would have thrown this thing out straight away. And you gave some examples, actually. So you
537440	541840	said, there are things that have a normal distribution, like the weight of babies when
541840	546160	they're born or blood pressure or certain things like that. And then there are other things that
546160	550400	are significantly more complex, like if I'm buying a house, what does the estate agent do? Well,
550400	554400	they have a complex model where they look at the neighborhood, they look at various different factors.
555520	561680	Some people have indexes and they have things that can shift over time. So having this one
561680	568480	absolutist number just seems a bit ridiculous. I actually think I feel for policymakers because
568480	575360	I think it will put pressure on them to continually adapt the number. So I will say there were benefits
575360	580800	in the thinking of this number. I think it's unfortunate it got so far without scientific
580800	586480	input. But one reason that people like Flops is, for example, it's hardware agnostic, you can measure
586560	592480	the same way across different types of hardware. And also, it's fairly easy to measure because all
592480	600160	it's doing is a tally of operations. So it also avoids specifying maybe what risk you care about.
600160	606800	So it gives a degree of, I would say, flexibility there for governments to adapt over time. I would
606800	614800	say that is probably one of the larger shortcomings is that by not specifying, you can end up with
614800	620800	something which is evading your Flops threshold but a highly risky model. So I think that's actually
620800	626960	one of the crucial shortcomings. But I do understand that the motivation of a lot of policymakers are
626960	632560	what else? Like what else could I use? I would argue if you're going to stick with this measure
632560	638400	and it has been formalized in several policies, you have to understand that this can be manipulated
638400	643600	as a measure. And there's many ways, and I list some out in the paper, but to your point,
643600	651120	a single number puts a lot of pressure on policymakers to constantly adjust this and have
651120	656640	the technical information to adjust it because this is a rapidly changing distribution. The notion of
656640	662960	compute has been highly unstable. Just looking at the last decade, we know this. And so it will quickly
663760	668480	have an expiration date. And I would argue what you're saying is excellent as an example.
668480	674080	One is that you need a reference class of what are you comparing against? So you mentioned the
675200	681360	kind of real estate agent who compares the pool of houses. Each of these domains, like biology models,
681360	687280	which are very interesting to certain researchers because of bio risk, language models, multimodal
687280	691760	models, they have different distributions of compute requirements. And so it has to be done
691760	696960	relative to your reference class, but also it should be done dynamically. The same way that a
696960	703120	real estate agent does it based on a percentile of surrounding houses, the notion of a single
703120	709520	inflection point for risk is not a viable policy tool because you just are changing things all the
709520	714480	time. Yes. I mean, there are so many things to get into here because you went through a wonderful
714480	719680	list of examples in your paper. But one of the elephants in the room is that it supposes that
719680	725600	there is some kind of linear commensurate relationship between compute and capabilities.
725600	730080	And of course, you're working in multilingual. I mean, it actually penalizes you because
730080	736400	you need to do more compute just to have a model that works at all in many different languages.
736400	741120	And this thing just isn't working for you. Yeah. I mean, what you're pointing out is that once you
741120	746400	do something like multilingual, you're basically trying to learn a new distribution each time
746400	752080	that's as vast as English. And so you typically need a lot. It's called the curse of multilinguality.
752080	758000	And so you need more compute. There's other things there which are very tricky is that
758000	762640	how do you flops and how does training compute account for the vast amount of
763840	769920	change and how we optimize after training? So we talked about RLHF. There's also instruction
769920	776160	fine-tuning. There's also things like synthetic data distillation which shortens training times.
776160	782480	So these are all what we call inference time optimization. So you spend time after training.
782480	786800	You pay for it in compute. Like you can do best event sampling, which is what you refer to with
786800	793600	the Francois Chollet where you sample a lot of completions and you choose the best. That all has
793600	799760	very pronounced benefits for models. So typically your model performance alone, just using a subset
799760	805360	of these techniques is two to six times more powerful. And that's not reflected in flops.
805360	810560	Yes. Because I'm really interested in when you look at the model life cycle or the predictive
810560	815760	life cycle, there are so many places where you can spend computation. So you can do dataset
815760	820000	generation and you can do, obviously there's the training of the model and then you can do
820000	823920	like inference time optimization and active inference and a whole bunch of stuff like that.
823920	828480	And they are only taking into account the model training. But then there's the further issue
828480	832880	of training provenance. So for example, I can download a model from Huggingface and I can fine
832880	837120	tune it and do a bunch of stuff on it. And like, how do you know, right? It's just an inscrutable
837120	842160	bunch of weights. Like you have no idea how much training has gone into it. This is the idea of
842160	846800	tracing flops across the life cycle. I think this is also going to be formidable because
846800	852320	increasingly the most popular models on Huggingface, by the way, are models which haven't been
852320	856560	instruction fine-tuned. They're base models. And why? Because people want to do continued
856560	861040	pre-training. They want to overlay their own optimization techniques, which suggests that
861040	865680	people are using this as one step in their optimization process. It's going to be a formidable
865680	871520	challenge to tally it in a reasonable way, especially when sometimes the way that we
872240	876720	measure these, think about something like mixture of experts or a classic ensemble.
877520	882240	What counts then? Because you may have many different experts in your mixture of experts,
882240	888800	but you're only using two at the end. Classic ensembling is even more nuanced because technically
888800	894240	you didn't even optimize all the models together. You just show up and you ensemble them at the end
894240	898880	and you get one model at the end. So how do you handle that? It's very interesting and it's very
898880	904720	related to this challenge of people are likely taking some level of compute already and they're
904720	909760	doing some changes at the end of training that make it more performant. Yeah. And then there's
909760	915760	this matter of Good Heart's Law, which is that when a target becomes a measure, it ceases to be
915760	921040	a good measure. And there are so many examples of this. For example, the banks have these arbitrary
921040	925840	limits on the amount of money that you can send, which is why it's set at, let's say, $10,000 and
925840	930720	then you see loads and loads of transactions at like $9999 because they know what the limit is.
930720	935040	And it must be the same here, right? It's going to gamify the system. People are going to evade
935040	941680	it in so many ways. I think that the main advice I have about this is that if policy makers have
941680	946320	decided on this and they're going to go for it, they need to complement it with an auxiliary
946320	951040	measure of the actual risk that they care about. It has to be an index because if you just stick
951040	957120	with compute, it is too easy to evade because there's too many different things you can do post
957120	963040	training to gain percentage points. And there's too many ways of essentially shortening your
963040	967440	training time or reducing your flops while still arriving at a highly performant model.
968160	973360	And so that's the other key recommendation I have is that you need something that is
973360	979120	anchored to the downstream risk you actually care about. And compute is not. It just reflects
979120	988080	our belief that more compute is better. And that simply is too simplistic of you to account for
988720	993680	all the ways in which smaller models, if they're very targeted, can be extremely risky.
994400	1002720	Yes. So I think maybe we should bring Rich Sutton in. So he wrote this essay called The
1002720	1008960	Bitter Lesson. And I'll let you bring it in. But he was partially responsible for this idea that
1008960	1014960	compute is all you need. Yeah, bring that in. Yeah. And by the way, I think that's a fantastic
1014960	1023280	essay. It really is this idea that history tells us in computer science in particular that all
1023360	1031840	efforts to codify our expertise, to work on very fancy ways of imparting what we think is the right
1031840	1038720	way to learn to a model have been particularly futile. Like he's really saying we're not very good
1038720	1043600	as computer scientists. And the biggest ingredient of success that's driven things is being adding
1043600	1048400	compute to the mix. And that we can do things that are algorithmic, but it has to play well
1048400	1054480	with compute. So anyway, he is kind of get this idea of hardware. But it's more general. It's this
1054480	1058000	idea that it's not specific to a different sort of type of hardware. It's just compute. If you
1058000	1063360	play well with compute, if it scales well, it's going to be the winning variant. And yeah, go for
1063360	1068000	it. Well, I mean, I just I have an intuition that he's he's right and wrong at the same time. So
1068000	1072080	I mean, in terms of system one models that just memorize things better and better, he's kind
1072080	1075760	of right. Because there is a commensurate relationship, you know, as we memorize more of
1075760	1081040	the long tail, the models get better and better. But I still think that there might be a fundamental
1081040	1086240	break between compute and capabilities when and ironically, a regulation like this might
1086240	1090960	incentivize to find such a break. So you know, we might design system two models that actually do
1090960	1094640	reasoning and have a, you know, Neurosymbolic architecture or whatever. And now all of a
1094640	1100960	sudden we've got like really good capabilities with less compute. Yeah, I mean, you're hitting on
1101680	1106560	the head, I don't disagree with you. I think where I agree with Rich said it is that for
1106560	1113920	given architecture, say, transformers, you can throw more compute at it up until a certain
1113920	1118960	point where it's saturated, but you're going to see all the things equal, your data sets equal,
1118960	1123280	compute is better because these are greedy learners, they, they're, you know, our deep
1123280	1127280	neural networks are frequency counters, you're going to see gains on the long tail performance
1127280	1134880	and overall gains. Where it misses the point is that really there's a few things going on.
1134880	1140080	One is that because our current representations are so inefficient, there's ways to
1142160	1149840	really change the algorithm itself and bend the, the rule of compute. So, and the rate at which
1149840	1155760	compute is needed to unlock gains. And deep neural networks in particular are a great example of
1155760	1159440	this because they're so painfully inefficient because we have to show all the data the same
1159440	1163600	amount of times because we have to do these global updates. And so we're seeing all these tricks.
1164240	1168480	For example, now we care about data again, and we care about data quality. So we condition that
1168480	1173840	space better to represent what we want to model downstream. That means we have to train far less
1173840	1178080	because all the features in the data set, the ones we want to learn was you just train on the
1178080	1181840	internet. There's a lot you don't want to learn. So you have to kind of unlearn it afterwards and
1181840	1187440	spend a lot of compute just trying to find what you want within that. So that's where you bend
1187440	1192240	the rule and it becomes more nuanced, which is that the other thing that that misses is that,
1192880	1196480	or at least that wasn't a core part of this essay, I actually think Rich might agree with
1196480	1203760	me on this, is that your rate of compute is really determined more than anything by the
1203760	1208960	prior of your algorithm. So yes, if your algorithm plays well with compute, if it's scalable,
1209040	1214400	compute unlocks a lot. But the rate and the saturation point is determined by the algorithm.
1214400	1219600	And what do we mean by this? Convolutional neural networks are a great example. Introduced in 2012,
1220560	1225280	really unlocked scalability. Why? Because convolutional filters and patches made it
1225280	1230400	possible to model high dimensional images at the time. Why? Because you move your patch over your
1230400	1236960	image. This takes advantage of local relationships. You can really reduce your dimensionality. Max
1237840	1243360	max pooling layers, which Jeffrey Hinton is famously grumpy about, and rightly so,
1243360	1247200	just throws away everything except for the max. You reduce the amount of features,
1247200	1252960	you unlock the ability to model images, you have scalability up to your point. This is what we
1252960	1257680	famously know about image models. Now there's been a saturation point. Everyone who switched
1257680	1262880	to transformers because there's a new arc. So what I mean by this is your algorithm is kind of your
1262960	1272080	most heavy prior on your search space. And it's Richard's right that what plays well with compute
1272080	1278240	is the one we're going to default to. But the question becomes your scaling laws and your ability
1278240	1284160	to predict the future are essentially limited to algorithm and compute. And that's what's
1284160	1288000	interesting is that it means we're not very good at predicting the future because it means we're
1288000	1293520	too locked in to this narrow arc of this architecture we use combined with compute.
1293520	1298000	Yeah. I mean, even the CNN example is, I think an example that proves Richard wrong,
1298000	1303680	because he said in the bitter lesson that any attempt to impute hand-designed priors like
1303680	1310480	symmetry or CNN is a symmetry. So in the CNN, it encodes a symmetry and scale invariance.
1310480	1315200	And essentially, all it's doing is a shortcut because it's still in MLP at the end. So what
1315200	1321520	it's doing is it's basically it's building an MLP as if you didn't have the scale and symmetry in
1321520	1325520	there. But it's just kind of like doing this thing and it's like, you know, it's embedding it
1325520	1329280	all into the MLP. But it's basically still an MLP with a symmetry shortcut, which was hand-designed.
1330880	1337680	So yeah, yeah, but there's still this notion though that there's, you know, connectionists think
1337680	1345040	that, I mean, like Neil Nanda said to me, he's like a rationalist guide from DeepMind and now,
1345040	1347920	yeah, DeepMind. And he said, these things are just smarter than you, man.
1350080	1354720	How did that make you feel? Well, I mean, not great, not great, but you know, there's this whole
1354720	1362480	like mech-interp thing. And I think that they genuinely believe that there's just some deep
1362480	1367040	form of inscrutable intelligence going on. There was that like monosomanticity paper from
1367120	1370880	Anthropic recently. And you know, there's this deep belief that there's something
1370880	1376640	really interesting going on. What do you think? I think that it is, there are persuasive,
1378560	1385040	here's what I will say. Language is very powerful, which is why we connect and with this technology
1385040	1390240	so much, because language is how we connect with each other emotionally. It's very tied to how we,
1390240	1395520	as humans, are quite tribal. And so whenever a model learns a distribution that is indistinguishable
1395520	1402000	from humans, I think that it gives pause. And it is, I do think that these conversations are
1402000	1408400	useful because it gives worthy pause to how this technology is used. And for example, I'm very
1408400	1417200	against many of the efforts to sometimes deploy these algorithms without notifying humans that
1417200	1421600	it's an algorithm. I think that you should always be aware when you're talking to an algorithm.
1421600	1426800	And that's because these are quite convincing sometimes. And so it's very important that we
1426800	1431920	always communicate what is the role of the model and the human. Do I think that there's a higher
1431920	1440560	reasoning here? I don't. I think that in many ways, whenever you have persuasive interpolation of a
1440560	1446560	space between ideas, it's going to be surprising to us. I think what delights us is the creativity
1446560	1453520	and the surprise and element. But is this ability to reason? I don't think so. I think that there
1455280	1460640	is a clear relationship with the type of architecture we have of a memorization
1460640	1466640	relationship. And we know this. We know that when we increase scale, we learn a given architecture.
1466640	1471120	Or when we do different tricks to compensate for scale, so we can go smaller and still learn
1471120	1474720	things, what we're really doing is we're just trying to induce good memorization
1475520	1479600	and good steering towards part of the distribution we care about. Frankly, while
1479600	1486080	why all these optimization tricks have worked beyond compute to reduce compute has been
1486080	1489760	we're largely training on a distribution we don't want at the end of the day. So we start by
1489760	1496000	training on the internet. And we actually don't want the internet when we engage with these models.
1496000	1500160	We want something that's very chatty and philosophical and wise. And so a lot of what
1500160	1504320	we're doing is we're trying to steer things towards the part the tiny sliver of the distribution
1504320	1509120	that training data that we care about. And that's why we have so many optimization tricks
1509760	1514160	before we get to the end. But that's fascinating because what's that that's really telling you is
1514160	1518160	that unlike a traditional machine learning problem where you're training to the data set is the
1518160	1523440	distribution you want to learn, a lot of what we're doing with language is we're unlearning.
1523440	1527600	We're just trying to steer and unlearning nor and then focus on what we want. So it's really
1527600	1532400	interesting. Yeah, machine unlearning. That's fascinating. I'm also a big fan of the
1532400	1536480	externalist tradition in cognitive science, you know, like for recognition. And in that sense,
1536480	1539920	I think it doesn't make sense to draw a boundary around the model, because I think, you know,
1539920	1544320	our sense making semantics, situated knowledge and so on, it's kind of observe a relative anyway,
1544320	1548960	you know, these things are embedded in our culture. And sense making humans put prompts in there and
1548960	1552720	they interpret the outputs. And actually, even the data generating process that went into building
1552720	1556400	these things was, you know, originated from the universe, we're all agents and we're all in the
1556400	1560400	physical world and the social world. And we, you know, we're doing the effect of computation,
1560400	1564400	both in how the models are built and how they are used and interpreted and evaluated and so on.
1564400	1568320	So what are you going to do? Like draw a big, should you include the flops of the universe as
1568320	1574000	well? Oh, that's fascinating. Yeah, there is this interesting, yeah, it does spark something else with
1574640	1578960	flops, which is that, so typically the final model that you deliver is only one of the possible
1578960	1585040	models, right? So in fact, typically, even at massive scale, you train many candidate models,
1585040	1588560	and then you choose the best one. And so it's interesting because these are not optimized
1588560	1592960	together, but they implicitly optimized through the selection process. And it is really interesting
1592960	1597280	because we kind of steer towards what we want. So yeah, it's a fascinating dynamic. I didn't think
1597280	1602480	of it like that, though, that's an even bigger meta approach for thinking about this. Hardware
1602480	1606640	Lottery Paper, which we talked about, and that was a really fun conversation because I remember it
1606640	1614560	was when you were doing the trio kind of the group of ML Street talk, the earlier version. And
1615360	1620960	I think you originally invited me onto the show because there was this idea that I wrote about,
1620960	1628000	which is that most of computer science history has been driven by whether your idea works with
1628000	1633840	available hardware or not. And I think that resonated with a lot of people at the time,
1633840	1638800	because what it's really saying is that we may be in another hardware lottery right now,
1638800	1645120	that something like Transformers, which we all use, has become increasingly locked in to GPUs
1645120	1649920	and to TPUs, which have all been built to accelerate this one hardware. So it raises the
1649920	1655840	question of what next and how do we make sure that the next brilliant idea isn't stuck in
1655840	1660160	purgatory for decades, because that's what happened to deep neural networks. It simply
1660160	1669200	didn't work until TPUs were converted from video game use, which was really not the intended
1669200	1673360	purpose of how they were converted to work for machine learning workloads. And that happened
1673360	1677680	over the course of a decade. It was a very slow conversion process, but that turned out to be
1677680	1683280	the key for deep neural networks. What we now identify as 2012, the moment that this explosion
1683280	1689360	of interest and funding and acceleration happened. People identify that with convolutional neural
1689360	1694800	networks or the algorithm, but really it was both. It was the hardware making the algorithm
1694800	1699840	feasible. And that's when you first had the empirical proof that deep neural networks were viable.
1700640	1703360	To what extent do you think there is an algorithm lottery as well?
1704480	1710080	Oh, what do you mean by that? Well, as in now, your paper was about the basin of attraction of
1710080	1715840	hardware. But is there a basin of attraction of algorithms as well? Absolutely. I mean, you just
1715840	1723920	have to look at optimizers to see that. So what I mean by that is an algorithm is really how you
1723920	1730800	learn from data. This is the essence of an algorithm. And what we've been locked into is this idea that
1730800	1736640	it has to be gradient based optimization. It's really hard to do something that's a non-differentiable
1736640	1743520	objective. And what that means kind of in accessible terms is that we're stuck doing these
1743600	1749200	global updates. So the way our models train is that we kind of send through shovel through data,
1749200	1753920	and then the update to the weights is based on an average of all the data that's seen.
1754480	1759440	Why is that tricky for a few reasons? Because why does it mean that we overfit to the average?
1759440	1763760	And that's why we need so much training data. Because essentially, if you're just overfitting
1763760	1768960	to the average, it takes ages to learn the rare patterns. So you train for longer, you need more
1768960	1775520	data. But the other thing that's very tricky is that it means that models forget. So every time
1775520	1780000	you shovel in new data, the model forgets the old data because you're updating everything at once.
1780000	1785360	A nice point of contrast is that as humans, we typically have long-term memory and short-term
1785360	1789120	memory. These are different ways of learning, and the rate of learning is different. And so when
1789120	1794240	you process information, some are stored in your long-term memory. You may have a distinctive memory
1794320	1800080	from a child that you think is like your first memory from a child. And it may be mutated over
1800080	1804800	time. That's the nature of memory. But this ability to preserve two states of what you did today,
1804800	1810720	what you did years ago, that's very different from gradient updates. And somehow, because we
1810720	1814880	haven't found an alternative way, even though a lot of people have worked on it, we are in this
1814880	1820400	algorithm based on where it's very tricky to propose an algorithm that doesn't rely on a
1820400	1826560	differentiable objective. Yes. And I think we'll talk about this later. But part of the problem
1826560	1831440	is people think of this paradigm as a form of general abstract pure intelligence. And the
1831440	1837360	reality is that certainly with your multilingual work, that we're dealing with just this long tail
1837360	1843360	of complexity, heterogeneous data sets. But maybe that's a good segue because you just released
1844720	1848800	this primer paper called the AI language gap. Can you tell us about that?
1849680	1856320	So it's quite fun because in some ways, what you're talking about these themes leads so nice into
1856320	1864480	the AI language gap. Really, when we have built these models, we've overfitted to what is weighted
1864480	1868640	most importantly to those who built it. And these models have been built in a few places.
1869360	1875600	We're in London. London is a very big hub of where researchers have been. So is the US and
1875680	1882000	Europe and China. And because some of the first impressive large language models were built in
1882000	1889840	the US and the UK with DeepMind, and in the US with places like Coher and places like OpenAI,
1889840	1893840	I think that that has necessarily reflected the nature of the researchers who built them.
1893840	1900880	They wanted to work in English. The tricky thing is that when you try and make AI actually work
1900960	1906400	for the world, you're talking about this vast array of different languages. So there's 7,000
1906400	1913040	languages in the world. 80% of those have no text data. So it's truly not even a language problem.
1913040	1920400	It's also a multimodal problem. The second part is that even with the top 101 languages, no models
1920400	1926480	except for Io101 currently cover it. So there's this vast amount of the world that simply isn't
1926480	1933680	reflected in the way that AI works and who AI serves. The primer about the language gap is
1933680	1937920	really calling attention to this. But at the root of this problem and what you're getting at with
1937920	1943360	this theme of how does models work with the long tail is that the fundamental issue is
1944000	1950880	our models really overfit to high frequency patterns. And so the key difficulty with the
1950880	1957440	language gap is that, one, these languages typically are underserved by available data on
1957440	1962400	the internet. The internet kind of reflects early patterns of adoption, not necessarily humanity
1962400	1967280	as it is. So that means that there's way more English on the internet than there is people who
1967280	1973840	speak English. So 5% of homes speak English, but 50% of the internet is in English. In contrast,
1973840	1979360	something like Yoruba, spoken by 40 million people, is really underserved. And so it's a long tail
1979360	1984320	problem. But here's the other thing, it's a pattern where the rich get richer and the poor get poorer
1984320	1990320	because we're now in a synthetic data era. So as models get much better at generating English and
1990320	1994880	Chinese in particular, these are the two high resource languages that are well served, you're
1994880	2000720	going to see more content in those two languages. And that makes it even harder if you're relying on
2000720	2006480	large data to properly represent the languages that are currently underserved. Yeah, so interesting
2006480	2012240	because we're moving away from the material world into the information world. And right now in the
2012240	2016160	material world, there is a kind of a commensurate relationship between the number of people who
2016160	2020000	speak English and the amount of data on the internet. And as you say, we're now moving to this place
2020000	2025920	where we are generating data of language and the polarization is going to increase. So you're talking
2025920	2032240	about there's this kind of North American tech based inequality, which is getting worse. And you
2032240	2036560	said that there are safety implications for this. I was interested in this word safety, we spoke
2036560	2043600	about this last night. Because when I think of AI safety, I think of X risk in Silicon Valley and
2043600	2050160	stuff like that. And I've noticed over the years, the conflation of the two communities in terms of
2050160	2058480	ethics and existential risk. And how do you feel about that? I think it's, I mean, I feel grumpy
2058560	2063040	about that. But here's the thing. So, you know, subfields are always like this. I think there's
2063040	2069360	always this notion of subfields, which are extremely, you know, actually people caring about the same
2069360	2075920	objectives, trying to distinguish themselves over time. AI safety encompasses a large array of
2075920	2083600	perspectives and expertise and people who care about different things. I think that this shift
2083680	2088800	towards talking about from response for AI to AI safety is a fascinating one,
2088800	2094640	because it's been a bit intentional from communities who want to maybe suggest that
2095520	2101680	response for AI is distinct from what they're doing. And instead saying AI safety is about
2101680	2107360	these profound risks, these like fundamental issues of our time. And response for AI is,
2108080	2112400	okay, great, you're doing that, but keep going. And so I do think there's a very interesting
2112400	2117760	thing with how we name things and how we really have precision in our conversations.
2119120	2123760	Increasingly, I think AI safety encompasses both of these, and you need more precise
2123760	2130160	language with both. And I actually think my main ask is, we need to be precise about what our
2130160	2138400	objective is with AI safety. Because it can be, it is in many ways the same goals as response for AI.
2139280	2144320	But the degree of precision when this is articulated is a sign of accountability for
2144320	2147600	the objective. And I think sometimes the use of that word lacks accountability.
2148320	2155120	Yes, exactly. And when I hear some ex-risk folks talk about AI, it feels to be in the abstract.
2155120	2161840	And what I mean by that is they are just thinking about, if we scale this technology up,
2161840	2167360	it learns these abstract representations, which work in any situation, and it's just a matter of
2167360	2172720	scale. And it feels unmoored from the research, because when I read your work about multilingual
2172720	2177920	models, you're clearly pointing out that when we have what they call low resource languages,
2177920	2182000	the models don't work very well. They're not learning these abstractions that just
2182000	2187600	automatically work in other languages. There's a specificity to it. That seems to be the difference
2187600	2193040	to me. Yeah, there's this big question right now, what you're getting at is there's this idea of,
2193920	2198240	there's a mystique that some people are attributing to scale. It's been called different
2198240	2202640	things. It's this question of, are there emergent properties? Are there properties that appear from
2202640	2207360	nowhere that we unlock with scale? By the way, multilingual is originally proposed as one of
2207360	2211360	them. Like in the first paper about emergent properties, multilingual was there. It's like,
2211360	2216800	wow, how did this appear? We didn't even have this in our training data. But it's very interesting.
2216800	2221440	Now there's been subsequent work which is shown. It was there all along. It just wasn't documented
2221440	2226080	in the training data. So scale is just really learning your long tail. It's learning the low
2226080	2231520	frequency. We just get surprised because I think there's a big disconnect between what we think
2231520	2236320	we know about the vast amounts of data that we train on and what's actually in that mix.
2236320	2241440	And so there is often certain properties where it takes scale to unlock because it's very
2241440	2246480	relate to this question of memorization. I think how this conversation has become a bigger theme
2246560	2252320	beyond this scientific question of when do properties emerge and what to scale and lock,
2252320	2258640	it's become this thing of kind of creating a myth around these models. That there's a lack of
2259920	2265920	ability to understand what scale gives. And then that is used to kind of impart a degree of anxiety
2265920	2270640	that because we don't know precisely when this property will emerge, there should be anxiety
2270640	2276000	about this. And there should be a sense of real danger about the use of these models. And I would
2276000	2284480	say that that is actually the wrong framing for this. The right framing is that one is the notion
2284480	2290480	that we're just going to keep on scaling I think is flawed. I think there's very clear evidence that
2290480	2295440	you know bigger is not always better that we're kind of reaching the limits of how we scale with
2295440	2299280	something like transformers and it's very architecture bound. But the second thing that I
2299280	2308160	would say is it really kind of ignores the mounting evidence that these kind of properties
2308160	2311360	are surprising only because we're not going to predict in what emerges at scale.
2312160	2318480	Yes, yes. I spoke with David Chalmers recently and he bemoans the fact that whenever we have
2318480	2325600	a complex system, we say, oh, it's emergent. And there is something interesting going on
2325600	2330480	as you say that when you memorize more and more of the long tail, you do see this qualitative
2330480	2335360	increase in capabilities. And it's quite easy as an observer just to say, oh, you know, it's an
2335360	2342160	emergent property. And people ascribe things like you know, divergent intentionality and reasoning
2342160	2346320	and all of these kind of anthropomorphic qualities to the models even though they probably don't
2346320	2351760	really exist. But one interesting thing though is that, you know, when you memorize all of these
2351760	2357120	surface statistics at scale, you can use the language model as an idea generator. And like on
2357120	2362160	Francois Chalet's arc challenge, you know, Ryan Greenblatt generated about 30,000 completions for
2362160	2367280	all of the tasks. And the remarkable thing is in terms of sensitivity, the correct answer is in
2367280	2372560	those completions. And then you can do some neuro symbolic evaluation and selection and you can
2372560	2376160	pull the thing out, you know, so you can build an architecture that does really well. But I think
2376160	2381360	people underestimate the amount of human selection kind of like smoothing out the brittleness.
2382080	2387040	Yeah, well, right now, I agree, there's a huge amount of creativity that's unlocked for these
2387040	2392000	models. So this iteration, and actually, by the way, this idea of like, you can create a lot of
2392000	2396400	different options, and then you can verify which are correct, you see this in a lot of different
2397760	2401840	kind of states of progress right now, that's how code is currently done, like you can
2401840	2407840	create a really nice code data set by running code and seeing which one passed the test and kind
2407920	2413600	of do formal verification of which ones passed. So it's not that these models are not capable of
2413600	2418560	generating insensible answers is just that the probability on every single turn consistency
2418560	2423760	is what you're putting out consistency is sometimes not there. And I also think part of what is
2423760	2428320	beautiful from the creativity perspective, iteration of ideas is that sometimes you actually
2428320	2432560	don't want consistency. So the objective may be different in different settings. So for example,
2432560	2437520	for code, we always want code that passes. So that's a good example where we sample a lot just
2437520	2443280	to get the subset. But sometimes I've talked to people who use it as a way to seed ideas or
2443280	2447440	things like that. And actually there the diversity is the important part and gain very different
2447440	2452640	responses each time. And so I think over time, we'll actually have different models for different
2452640	2457840	things and be able to this is the core of the challenge of steerability of control, which right
2457840	2462880	now is not good, frankly, like why do we have prompt engineering and why does everyone love it?
2462880	2467840	Like this is a this is a symptom of a problem, not a symptom of a solution. The fact that we
2467840	2472320	spend so much time prompt engineering the perfect thing to steer. So hopefully we have better tools
2472320	2476560	in the future. But I see that as one key thing that will change is that we'll be able to steer
2476560	2481360	towards the mode we want to use. Do we want consistency? Do we want exploration? And how
2481360	2485120	does it fit into our iteration pattern? We won't spend too long on this because I asked everyone
2485120	2489120	about this. But you know, where are the sources of creativity? So as we memorize more of the
2489120	2493520	long tail, and the models can extrapolate, and the human prompters can, you know, mix novel
2493520	2498240	combinations of things together. So there's this potential extrapolative space and whatnot that's
2499440	2505120	how creative can they be? Yeah, I've been so one of the recent papers that we released was a paper
2505120	2513120	about what we call active inheritance. It's this idea that we can start to steer how we sample data
2513120	2518240	to sampling different parts of the distributions from different models. So so far the paradigm
2518240	2523440	of like sampling data, either for human or for another model, has been very much like there's
2523440	2527920	a single teacher, you're the student, or there's another student or your co creators with a single
2527920	2533680	model. But if you think about it, one, that's a kind of passive inheritance, you're just trying a
2533680	2539920	single prompt, you're not really kind of enforcing any criteria. Active inheritance is where you
2539920	2544560	sample different parts of the problem you want to solve from a variety of different models.
2544560	2549840	And that diversity actually spurs really interesting patterns where you increase
2549840	2555760	the realm of what's possible and kind of spur higher quality that transcends the quality of
2555760	2561280	any one model. And I see that as a very important step that we're building a lot of work on,
2561280	2567200	including a multilingual, but also in this fundamental area of we actually used it to
2567200	2571440	in the paper that we just released, we used it to steer towards non-differentiable objectives.
2571440	2574800	So you know, going back to what you were talking about the algorithm basin,
2574800	2578640	this idea, and I was saying everything is dependent on gradient descent, it's very hard
2578640	2583680	to steer towards non-differentiable objectives. Before deep neural networks, there was decades
2583680	2589360	of research on just these non-differentiable objectives. There are things like, how do you
2589360	2596080	compute the perplexity of like a given, like, what is the reading grade level of a given sentence?
2596080	2600560	So there's these scores that are kind of codified, but you can't really use it because they're not
2600560	2604800	differentiable. And we actually show that you can use that as part of active inheritance where
2604800	2609840	you steer towards models that are better at a reading grade level. And then you use that to
2609840	2615040	kind of form your basis of your data set. So I think that's fascinating. And I think that's
2615040	2619840	really going to spur creativity beyond just this more static notion of you just sample from a single
2619840	2624000	teacher. Yeah, that's fascinating because there's so much of your research has been on the tyranny
2624000	2627600	of forgetting the long tail or not paying attention to it. And of course, you can solve that with
2627600	2632560	better optimization and, you know, federated learning and a gentile, you know, kind of
2632560	2637600	multimodal systems that share information and query and almost like an adversary or setup.
2637600	2641680	Yeah, it's a more dynamic pool. And so it's this idea that you can actually, and actually
2642320	2647520	the long tail is a perfect example of where I find active inheritance most promising is that
2647520	2652080	because the long pool, the long tail, you typically have many weak teachers. No one's very good at
2652080	2657600	the long tail. But sampling effectively and doing this active inheritance rather than passive of
2657600	2662640	just choosing a single teacher, but choosing a variety of teachers and then comparing and optimizing,
2662640	2666320	this is fascinating. And I suspect it will benefit most the long tail.
2667040	2671520	So you said in your language gap paper that language models are going to become integral to
2671520	2678640	modern societies. How do you see that panning out? It's already happening in different ways.
2678720	2684960	Like I would call it the high low way. So we can talk about high level themes, which is there'll be
2684960	2692160	a ability to communicate much more easily. And so you'll just see much more proliferation of
2692160	2698960	things like art or people writing or kind of taking away some of the difficult parts of
2698960	2704000	how we communicate. I think the low way is just the more granular ways that you're using it right
2704000	2711440	now, which is I use it typically for very basic things throughout my day. We write a lot of papers,
2711440	2716800	so I'll do my citation reformatting using a language model. So there's both the mundane,
2716800	2722400	but there's also the profound. I think the profound is that it changes the ease of communication.
2722400	2727760	And so it changes the rate of inflammation flow. And this can be really powerful. It can mean that
2727760	2734560	we can be more creative and experiment more the space. It can also bring new risks. And so
2734560	2740720	I think this is also important to think about. Interesting. And you said that this North American
2740720	2747280	bias in language model training, you said that it affects the design, the outputs and the behavior
2747280	2755120	of the models. What did you mean by that? Well, there's two things. I mean, when I say design
2755200	2762000	outputs and the behavior of the models, I think that there's optimization bias in the models
2762000	2767120	itself against different languages. So tokenizes is a great example. So Roman scripts are things like
2768080	2775600	French, Italian. We also have a Latin based scripts. This is also English. Whenever you deviate
2775600	2782960	from Latin based scripts, you have something like Hindi, Korean, and these do not play well
2782960	2788480	with tokenizers. So there's a lot of work which shows not only do tokenizers not work very well
2788480	2794320	for these languages, but also it ends up being at this double tax because not only does the models
2794320	2798800	perform worse, it also takes more tokens to represent these languages. So it's higher latency,
2798800	2804400	higher cost for users outside of English to use APIs right now. So that's an example of like an
2804400	2812160	optimization bias. The other, frankly, the issue is that whenever you're trying to have a model
2812160	2818400	that represents many different parts of a distribution, typically our solution right now is
2818400	2823120	we've got to give it more capacity. So I1 and 1 was an interesting example of this. We released
2823120	2828000	I1 and 1. It represented 101 languages, and you can start to think about how many that is when you
2828000	2833600	try and list more than 20. So you'll probably get to 10, and then you'll start struggling. And 101
2833600	2841200	is nuts. It includes things like we had Welsh, we had Irish, but we also had Telegu, we had many
2841200	2849840	African languages, and we had very much these underrepresented like Haitian things, the variety
2849840	2856000	and the complexity as well as dialect. So 101 is probably like preparing for the space race. It's
2856000	2861440	like at the most extreme of the problem. And what's interesting is everything you learn there
2862160	2867920	trickles down to less severe settings. But one of the things that we learned there is that you
2868000	2872640	have to be very careful about how you use capacity because we had this 13 billion parameter model,
2873280	2878560	and we were stuck with it because there was no pre-training data that covered 101. So this model
2878560	2884720	was actually from 2019, which is crazy given how much has happened since then. But because of that,
2884720	2890080	we were stuck with this model, and it meant that everything we had to do was try and make the best
2890080	2895680	user capacity. We had to wait properly. We had to do data processing, data cleaning, but also we
2895680	2900720	had to do a lot of work with synthetic data and the manipulation of how we did the optimization time.
2901360	2907840	So you can do this two ways. We could have even increased it to 103 billion parameter model,
2907840	2911440	and then we would have to retrain because right now models, unless they're trained with the day
2911440	2917520	from the beginning, you can't just add it at the end. But also there's a secondary way, which is we
2917520	2922640	get much more clever about the optimization and the data creation. And so this is really the issue
2922720	2926400	is that when you go multilingual, all your problems in a given language
2927520	2930960	are kind of multiplied out. And so you have to be very careful about all the details.
2932160	2936480	I wonder what's the relationship between language and capabilities? And the reason I asked this is
2936480	2940960	there was a great book I read called The Language Game by Morton Christensen and Nick Chater.
2940960	2946400	And that very much led me to this idea of situated knowledge, I guess. So actually a lot of our
2946400	2953280	cognition and thinking is quite specific to the culture and the language that we are in.
2953280	2957520	And that seems to go against the grain of the idea that these things are learning
2957520	2962880	general patterns of reasoning across languages. So then it rather kind of leads you to this
2962880	2969040	conclusion that you actually need to be within the language and the culture in order to do the
2969040	2973920	kind of thinking that they do inside that culture. So how does that work then when you're mixing all
2973920	2980080	of these together into one language model? I think it doesn't work that well right now. So I would
2980080	2983920	say this is like one of the core problems because you're precisely right. So we actually, so there's
2983920	2988720	a few things I would say here. One is that we already see this with things like dialect. So the
2988720	2994080	notion of dialect, which isn't really a counter for any models, including Aya, I think that we all go
2994080	2999760	as mainly just to be the first next step in state of art. But even ours doesn't do this nuance of
2999760	3004400	dialect. We do have various dialects of Arabic and some other dialects, but take something like
3004400	3010480	Portuguese for example. Portuguese is spoken in many different places of the world. I spent part
3010480	3015280	of my childhood in Mozambique. The Mozambique Portuguese is very different from, you know,
3015280	3020240	I guess the most extreme would be Brazilian Portuguese. But also Portuguese in Portugal
3020240	3025440	has its own nuances. And actually when we did Aya, we had researchers all over the world who were
3025440	3031040	part of this project. And we would frequently have these little riffs between the Portuguese
3031040	3036240	contributors in Brazil and the Portuguese contributors in Portugal because they were asked
3036240	3041520	to review within a single pool. And so because the Brazilians outnumbered the Portuguese in Portugal,
3041520	3047120	they would all correct their submissions to Brazilian Portuguese. This is a very interesting
3047120	3052160	concept. And this is just on the notion of dialect. But your wider point is this idea that
3052960	3059360	language is a tool for communication. And there's actually this very interesting concept about
3059360	3065120	whether we even use language to think or if we use it as a utilitarian tool. Why is that relevant
3065120	3070720	here? Because the way that we achieve an objective is going to depend upon where we are in the world.
3070720	3074880	And the way that technology should serve us is going to depend on where we are with the world.
3074880	3079200	This has come out recently. We just released a paper which I'm quite proud of, which is thinking
3079200	3086000	about this idea of local versus global harms. At any one moment, we have multiple facets of our
3086000	3094320	identity. So there's notions of what is insensitive to us as part of a notion of being global citizens.
3094320	3098880	And that probably gets to things like there's a universal agreement that some types of harms,
3098880	3105200	like harms to world children are particularly egregious. And most of, almost universally,
3105200	3110320	our legal systems reflect this. But there's also notions of very particular harms which are
3111040	3116240	cultural and very specific to how we live. And that is reflected in things like wording. So
3116960	3122320	we just released this paper, which I think is important for safety, but also part of this
3122320	3128160	broader move and in the field, which is that most of our models right now are trained with a
3128160	3133120	single objective, a single decision boundary. What that means is all the data gets flattened
3133120	3138880	to this one decision boundary. I'm very interested in multi-objective optimization. And this changes
3138880	3144400	it so that you can hold multiple objectives at once. And that perhaps you can even adapt
3144400	3149440	these objectives on the fly, which is very interesting. Yeah, a couple of things. I mean,
3149440	3154640	you're talking, I guess, about the interplay between having a relativistic worldview and
3154640	3160480	having some global norms. And in general, the way we do model alignment with our LHF and so on,
3160480	3167360	it tends to de-complexify the reality of the world that we live in. And in ethical frameworks,
3167360	3171600	there are deontology people who think there are just guiding principles and there are
3171600	3175760	virtue ethics people who think there are certain virtues that we should emphasize. And there's
3175760	3180160	consequentialism that there are certain consequences that are bad. And as you were just
3180160	3185680	pointing to, it's very, very difficult to have a hybrid ethical framework that encapsulates
3185680	3190240	all of these things together. What kind of work are people doing and what are you thinking about
3191040	3196160	it? Well, we recently, the paper we just released is this really, this paper called,
3196160	3202480	we call the Multilingual Prism, which is this idea that for safety, we collected both local
3202480	3209280	examples of red teaming safety with really this very nuanced collection process across multiple
3209280	3215040	languages, as well as harms that were considered global. From there, you can go into something
3215040	3219920	like our LHF and you can change the notion of a single reward model. So this is an area I'm
3219920	3224080	quite interested in. Like, how do you have multiple reward models? And then how do you
3224080	3227680	balance them? This is the crux of the problem. And that's what you're getting at. So how do
3227680	3232720	you have these two things in unison? And I suspect what we're going to see there is
3233600	3239200	this notion of adaptation of our models in a more nimble way than previously. So typically,
3239200	3246800	in a production setting, you spend months doing this model, you release it, cool, thumbs up,
3246880	3253760	enjoy, and it's not as dynamic, but a true production model is refreshed and is more nimble
3253760	3257600	and is deployed in different ways to different places. Like Netflix famously does this with
3257600	3263200	its recommendation systems. I think here, this is actually a much more profound way of doing this
3263200	3270320	because you can have these models, which essentially the way that they're steered is adapted. And this
3270320	3275600	is both interesting as well as profoundly challenging because the tricky thing is,
3276240	3282320	is you want to be sensitive to how the preferences of users change around the world,
3282320	3287200	but you cannot overfit to too granular a preference because this is a philosophical
3287200	3291200	tension you're actually getting at, which is that, you know, a libertarian view would say
3291920	3297360	every person here has a list of preferences and those should be respected in their rank order.
3297440	3303360	But as a society, we typically say we have this group of preferences, but we also adhere and kind
3303360	3309360	of subsume some of our preferences for the common grid. And so there's this notion as well, when
3309360	3315440	you articulate that as an algorithm, how do you get that balance somewhere in the middle, like where
3315440	3321520	you are basically not adhering completely to a societal view. I think that's one of the concerns
3321520	3326480	about algorithms and tokenizers being used in certain states where there's a state influence
3326480	3332320	on how algorithms are deployed, but also not being used a total libertarian view where we don't want
3332320	3338880	objectives that essentially amplify how a person thinks about the world without balancing and
3338880	3342400	introducing different viewpoints. Yeah, it's so fascinating because, you know, even things like
3342400	3346880	polarization on their face seem like an incredibly bad thing, but some kind of diversity preservation
3346880	3352640	might actually lead to a pluralistic society that, you know, gains information and, you know,
3352640	3356800	like a degree of health actually that we need. But with safetyism in general though, there's
3356800	3362480	always this notion of, I think we probably agree on this a little bit, that if you leave people
3362480	3368560	to their own devices, then that can be bad, but you also need a little bit of that because otherwise
3368560	3374080	the society might become quite sclerotic. And these decisions presumably need to be baked into
3374080	3379600	the way that we build these models in some way. Yeah, and currently then not. I would say currently
3379600	3385360	the way that we approach safety, it's quite, we have this notion of refusals. So when you've
3385360	3390720	engaged with a model, you typically will see refusals for certain what I would call the more
3390720	3395760	black and white type cases. There's this really interesting opportunity I see in the evolution
3395760	3400400	of how we think about safety, which is that instead of just saying I can't answer this to kind of
3401520	3407520	provide more nuance or provide links to additional support. And I think that's very
3407520	3412560	interesting because there's a different type of discussion. But I would say your perspective,
3412560	3417760	what you're talking about, which is really this part in the middle where you have some values as
3417760	3422400	like how you build your algorithm, but also you realize that this is someone who's engaging with
3422400	3428000	an algorithm and like this is, you know, the algorithm should not reflect perfectly a single
3428000	3434320	view of the world. You need more ways I also think within the UI for the person to influence and
3434320	3439120	provide feedback and to something as course hallucinations is really interesting because
3439120	3444480	hallucinations is not, you can't, I'm very skeptical we're going to eliminate hallucinations
3444480	3448880	because they're also what we really like about these models. It's the creativity. So for me,
3448880	3453440	this is not just, we often fixate a lot on the model in these conversations. The model has to
3453440	3457920	solve this, but I think there's also a notion of the system. And I think that some things that will
3457920	3464640	be interesting to play within the system is how does the user express when they think that
3464640	3470240	steering isn't aligned with what they think is reasonable? A good example, this is for example
3470800	3475520	a question about sexual health. There's valid reasons to ask those questions. There's valid
3475520	3481520	reasons to want to understand like parts of your biology or things like that. Wikipedia has whole
3481520	3486400	pages about sexual health. And so it's very interesting that a lot of systems refuse to
3486400	3492000	answer this right now. So there's this nuance where we need to make sure that we are updating these
3492000	3496320	binary decision boundaries where it's outright refusal and move towards something which is
3496320	3502160	instead steering towards resources. Yeah, and I think so much of this is about when you fix something
3502160	3506320	as it is now, it can go both ways as well. So maybe you can explain to the model, no in this
3506320	3511600	situation I think you really should tell me. And likewise the model can say no, actually I think
3511600	3515040	the reason I'm not allowing you to do this is because of this and maybe you should shift your
3515040	3520080	viewpoint a little bit. It has to be done subtly because people don't like re-education. So that
3520080	3524320	actually creates, they say the road to hell is paved with good intentions, it creates an equal
3524320	3529200	and opposite reaction when you try to re-educate people. But coming on to RLHF, I mean we've
3529200	3534880	spoken about this for years, you've always been a bit grumpy about RLHF and I read your paper,
3534880	3538240	unfortunately I don't have an internet connection so I'm doing this from memory. Can you just remind
3538240	3542720	me the multilingual paper that you've just released where you're trying to remove translation
3542720	3549920	artifacts? Oh yes, RLHF speaks many languages. So this is a really nice paper, it was led by
3549920	3555600	John. This idea that we were the first to extend a lot of the RLHF techniques from many different
3555600	3562880	languages. So I think actually there's a wider view of RLHF and I have been grumpy about it,
3562880	3567200	but you go first. Yeah, what were you going to... Well I mean even in this paper you were saying
3567200	3571840	that, I mean obviously the broader conversation we've just had is that we need perhaps you know
3571840	3577440	some kind of a more systems approach where we have a multitude of different models and optimizers
3577440	3582160	and datasets and all that good stuff. But even within RLHF you are saying that it's hideously
3582880	3587280	complex and inefficient and you have to have this separate reward model and it can't be optimized
3587280	3592240	very well and you are saying that sometimes using DPO or even just basic reinforce is better.
3592240	3597280	Yeah, so there's an excellent paper which we also recently released back to basics where we actually
3597280	3601360	do a much more profound question of this, not even specific to multilingual, we take a step back.
3602000	3611840	And we say okay, it's really interesting. All the kind of most cited papers originally on RLHF
3612400	3619600	are papers which really take this canonical method within RL, PPO, and apply it to the
3619600	3628640	language setting. And PPO really evolved in the RL, traditional RL space to address and mitigate
3628640	3634480	a lot of the issues with traditional RL. RL is typically over a large expansive search space,
3634480	3641040	incredibly noisy, and the trickiest part right is that your errors compound. So it's almost like
3641040	3645920	thinking about well what if I bet incorrectly at a game table and then tried to bet again and
3645920	3653360	did it incorrectly and your losses just compound the more that your estimates are off. So PPO is
3653440	3662240	heavily what I would call kind of regularized or conditioned to limit the impact of an incorrect
3662240	3667520	estimate. What that means is that often it's quite memory intensive, you kind of have four
3667520	3673680	models and play at any one time, and it also means that it's quite sensitive. So typically PPO to
3673680	3681600	train, it takes longer. And showed the language setting. And so the initial adoption of PPO and
3681600	3687280	the success of it was taken at least value. This is incredible, let's go with this. But
3688080	3691840	the language space is also an enormous search space because if you think about it, you're trying to
3691840	3697440	predict the next token, how many tokens, how many possible tokens are there in the world to represent
3697440	3703840	language. But by the time you have a trained model, and by the time you've done all this pre-training,
3703840	3708960	the search space is much narrower. And actually it's quite interesting because the likelihood
3708960	3713520	and the probability of what the next token will be is actually very concentrated.
3713520	3718080	And when you have this pre-trained base, it's only going to be a few different tokens that you
3718080	3723200	would likely predict, which means that this was overkill for the setting. And what we show
3723200	3729440	convincingly and back to basics is that you can strip a lot, a lot of the components of PPO out.
3730080	3736880	You can propose something like RLU, which is still an RL method, but that works effectively and even
3736880	3744400	surpasses it. And RLU is also what we used in the RLHF speaks many languages. And we showed that
3744400	3750080	this is very impactful. And because it's online, it does beat things like DPO, which is offline.
3750080	3757360	So RLU is still an RL method. But what it's really saying is that we are in a well-conditioned
3757360	3762400	search space. And because of that, we can be a lot more nimble about how we explore it.
3762400	3768400	Yeah. Well, in the RLHF on many languages one, because obviously you've had this huge focus on
3768400	3773440	multilingual. And I suppose there's the problem of getting diverse data, because this is super
3773440	3778480	heterogeneous data when we're doing multilingual language training. And of course, even the
3778480	3784080	preference completions, they needed to be generated as well. And I think you generated some with
3784080	3788480	translations and then you had a strong model and you had a setup there. Can you tell us about that?
3788480	3793200	That's fun, because it's part of this wider issue where multilingual relies a lot traditionally on
3793200	3797760	translations. You don't have data, so you translate your good English data, your gold standard data,
3797760	3803680	or your good Mandarin Chinese data into many different languages. Here is where it gets
3803680	3808800	interesting. Translation models typically have what we call translation ease. There's these weird
3808800	3814880	artifacts that pop up. So you might have like, odd enumeration where instead of like the one,
3814880	3819680	two, three, it spells out one, two, three. So it's just, and it's very annoying for people who
3819680	3825280	have to experience it because it gets imparted to the model, the downstream model. So we did
3825280	3831520	something which I think is very fun with this paper where we said, well, the whole goal of RLHF
3831520	3837520	is to steer away from certain parts of the distribution, steer towards other parts. And so
3837520	3842240	what we did for our preference pairs, so let's think about the normal way preference pairs are
3842240	3848160	done. It's quite expensive and time intensive. You have to go get annotators and you're asking
3848160	3855440	humans which one do you prefer. We did this really fun, I would say, trick here where we said, well,
3855440	3860880	we know we have translation pairs. We generate synthetic pair, the other pair, with what is
3860880	3866480	a very high-performance model. In this case, we use Command R+, which is super-performance,
3866480	3871440	does very well in many different languages. And then we compare the two and we ask an
3871440	3877520	Alem is a judge, which is better, the translated English or the sampled in the other language.
3877520	3882160	And what we found was this actually helped with translation artifacts because it steered the model
3882160	3890400	away from the bad, translated and towards the more versatile, fluid Command R+, generation. So
3890400	3895280	really interesting. And there were some percentage of time where the translated was better. And so
3895280	3900560	you got that nuance too. So very, very interesting. Yeah, amazing. And then that removed a lot of
3900560	3906880	the translation artifacts. Yeah. Amazing. Sarah, this has been incredible. Where would you like
3906880	3912880	to point people to as well for your later stuff? Feel free. So, you know, I lead Co-Here4AI. So
3912880	3918720	it's a research that we do a lot of fundamental research. And we, a lot of my work is on efficiency,
3919920	3924080	reliability and building these models that scale the next generation models. So you can go to
3924080	3928720	Co-Here4AI and take a look at some of our work and just a lovely being here again. It's really
3928720	3941440	nice catching up. Amazing. Sarah, thank you so much. Yeah, thank you.
