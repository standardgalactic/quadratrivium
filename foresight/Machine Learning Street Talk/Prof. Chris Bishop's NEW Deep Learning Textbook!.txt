Today we have the privilege of speaking with Professor Chris Bishop,
a luminary in the field of artificial intelligence
and machine learning. Chris is a technical fellow
and director at Microsoft Research, AI for Science,
in Cambridge. He's also honorary professor of computer science
at the University of Edinburgh and fellow of Darwin College,
Cambridge. Hi, nice to meet you Tim. This is the new book on
Deep Learning Foundations and Concepts published with my son Hugh.
What proper have you got? Ethanol. I don't know if I'll use it but
we're going to talk about invariance. That's wonderful, that's wonderful.
Because you ought to get a little bit techy at some point. Oh yeah, our audience loves that.
In 2004 he was elected fellow of the Royal Academy of Engineering,
in 2007 he was elected fellow of the Royal Society of Edinburgh
and in 2017 he was elected fellow of the Royal Society.
Chris was a founding member of the UK AI Council
and in 2019 he was appointed to the Prime Minister's Council
for Science and Technology. At Microsoft Research, Chris oversees a global portfolio
of industrial research and development with a strong focus on machine learning
and the natural sciences. Chris obtained a BA in physics from Oxford
and a PhD in theoretical physics from the University of Edinburgh
with a thesis on quantum field theory. Chris's contributions to the field of
machine learning have been truly remarkable. He's authored one of the main
textbooks in the field which is Pattern Recognition and Machine Learning
or PRML. It has served as an essential reference for
countless students and researchers around the world.
Chris explained in the interview how it steered the field towards a more
probabilistic perspective at the time and he also mentioned his first textbook
Neural Networks for Pattern Recognition and its role in promoting neural
networks as a powerful tool for machine learning.
So this is the new textbook, Deep Learning, Foundations and Concepts
and one of the things that we're proud of with this book is the production values.
We really worked with the publisher to ensure the book would be
produced to a high physical quality and in particular it's produced with what
are called stitched signatures. So if you look down the edge there you'll see the
pages are not simply glued in. Instead this uses an offset printing
technique where 16 pages are printed on a big sheet of paper on both sides.
Some of the pages are turned upside down and then the page of the paper is
folded and then folded and then folded again and trimmed and the resulting
set is called a signature and actually stitched in with cord. And the point
about that is it allows the book to open flat so it means that the book is easy
to read and it means it should last a long time.
What are your favourite figures in the book Chris?
Well the ones produced by my son of course are the best. I mean here's a nice
picture of the transformer architecture which is this is GPT so
you could say it's one of the most important figures in the book I suppose
and I just love the way he's done this.
How did you do the research for this?
So that's a great question. I think you know one of the big
challenges with writing a book like this is knowing what to include and
what not to include and with literally thousands of papers being
published every month it can be overwhelming for the authors never
mind the readers. So I think the value we add in the book
is trying to distill out what we think of as the core concept.
So part of this was really looking at key papers in the field
seeing what relatively recent ideas there are but also trying to focus down on
techniques and ideas that we believe will actually stand the test of time.
We don't have this book to go out of date in a year or two we want it to have
have lasting value and of course it's quite possible there'll be a breakthrough
next week and that it will turn out to be a very important
new architecture but for the most part many of the core concepts actually go
back a long way and so what we've really done is taken some
some of the foundations of the field and brought them into the modern deep learning
era but the idea of probabilities the idea of gradient based methods and so on
those have been around for decades and they're just as applicable today as
they ever were. One of the things I really like actually is the
chapter on convolutional networks. My son Hugh did a lot of this
chapter he works on using techniques like convolutional neural
nets as part of his work on autonomous vehicles and I think there's a really
nice description here of convolutional networks
really from the ground up explaining the the basic concepts and
but also motivating them not just saying this is how a convolutional network is
built but why is it built this way how do we actually motivate it so that's
one of my favorite chapters as well. Yeah it's been a very interesting career
and at this stage of the career I can now finally look back and make sense of it
but at the time it felt like a bit of a random walk
so actually when I was a teenager I went to see 2001 A Space Odyssey it was
actually very inspired by that rather abstract concept of an
artificial intelligence very different from the usual sort of Hollywood
the trail of robots so I was very interested in the idea of artificial
intelligence from a young age but I was very uninspired by the field of AI
at the time which was very much sort of rule based and and didn't seem to be on
a path through intelligence and then I did a PhD in quantum field theory which
is a very hot field at the time gauge field theory
at Edinburgh University had a wonderful time at the end of my PhD though I wanted
to do something a bit more practical a bit more useful
and so I went into the fusion program I'm a big fan of nuclear fusion
it was sort of 30 years away then and it's kind of still 30 years away now but
I'm still a big believer but I went to work on
talk about physics essentially theoretical physics of of plasmas trying to
trying to understand the instabilities and control them
so I was working very happily as a theoretical physicist having a having a
great time and after about 10 years or so as a theoretical physicist
Jeff Hinton published the backprop paper and it came to my attention
and I found that very inspiring because there I saw a very very different
approach to towards intelligence and so I started by applying neural
networks to data from the fusion program because it was big data
in its day I was I was working there to the the jet
tokamak and they had many many high resolution diagnostics I had lots of
data to play with and I became more and more fascinated by
neural networks and then I did a sort of completely crazy thing I walked away
from a very respectable career as a theoretical physicist
and went full-time into the field of neural nets which at the time was not
really a respectable field I would say it's not wasn't mainstream computer
science it certainly wasn't physics it wasn't really anything
but I just found it very inspiring and I was particularly inspired by the work
of Jeff Hinton and so I've been in that field for
you know three and a half decades now and of course
recent history suggests that was probably a good career move
and now most recently I've brought the two ends of my career together because
I'm now very excited about the impact that neural nets and machine learning
are having on the natural sciences including physics.
Hinton is a famous connectionist so he believes that knowledge is
sub-symbolic and I speaking with Nick Chater the other week he had a
book called The Mind is Flat which is talking about the inscrutability of
our brains. How do you feel that things have changed?
I mean you were talking about a convergence of these different ideas in AI.
I think one thing that's very interesting is that there has been a lot of
discussion let's say from 2012 onwards when deep
learning was clearly being very successful a lot of discussion that it was
missing the sort of symbolic approach that we somehow to find a way to combine
this connectionist approach to use that sort of probably rather dated term now
but that sort of you know that neural net approach
with the more traditional symbolic approach and I think what we've seen
with models like GPT-4 for example that it's perfectly capable of
reasoning at a more symbolic level not at the level of a human being of course
but it can do that that kind of more abstract higher level
reasoning and so I think what we're seeing with neural
nets is rather like the human brain. The human brain doesn't have a
connectionist neural net piece then some other machinery that does symbolic
reasoning that that that same substrate is capable of all of these
different kinds of reasoning and these different kinds of intelligence
and we're starting to see that emerge now with neural net so I think
I think for me the discussion of should we somehow combine symbolic reasoning
with with connectionism no that that to me that's a piece of history
it's about how can we how can we expand on the capabilities of neural nets.
Yeah that's so interesting I remember there was a paper by Polition I think it
was the the connectionist critique in 1988
and I was quite sold on this idea of you know systematicity and
productivity and so on and even now folks from that school of thought
think that our brains are Turing machines this ability to address
potential infinity and I guess what I'm getting from
what you're saying is that the distinction isn't really there anymore you
can do that kind of reasoning with neural networks.
Well I take a very simple view which is that neural nets in that since 2012 in
particular have been shown to be spectacularly capable
and there's no end in sight the rate of progress is faster now than ever
so it seems very straight nobody imagines that that machine learning and
deep learning has suddenly ended at you know whatever the time is today you
know this is this is this is the beginning of an S-curve
so the idea that we would worry so much about the limitations of neural
networks and what they can't do I think we just you put the word yet at the end
of it your neural networks can't do x y and z yet but but I don't think
any sense we've hit the buffers of of what neural nets can do and it's by
far the most successful of the most rapidly advancing technology we have
so to me you should look for the keys under the lamppost we have this powerful
technology that's getting better by the week why would we not see how far we
can push it rather than worry about its limitations.
Absolutely now Professor Bishop you are incredibly famous
for your book PRML but of course it wasn't your first book as you were just
speaking to but what was I mean could you just tell us about your your
motivations and just the thought process behind that book?
Yes so as you said it wasn't my first book my first book is published in 1995
Neural Networks for Pattern Recognition and that book had a very specific
motivation which is that I was a newcomer to the field I mentioned earlier
that I got excited about backprop and and sort of
transition from theoretical physics into machine learning.
That was my way of learning about the field you know if you're a university
professor a great way to learn about something is to teach a course on it
because it forces you to think about it very carefully you're going to get
tricky questions from smart students and you're very motivated to to really
understand it and so for me the analogue of that was was writing a book.
PRML was rather different by the time we got to published in 2006 and by then
the field was much larger its sense it was much more mature as a much more
established and respected field there were many courses on machine learning
the goal there was very different I simply wanted to write the as it were
the book that everybody would use to learn about the field so it was trying to
be comprehensive but trying to be to explain the concepts as clearly as
possible and so really that was the goal the goal was to in a sense you know
replace the earlier neural nets for pattern recognition book which was
which serves an important role in its day I think but really try to produce a
single coherent text where people could learn about the different topics in
you know with a shared notation and hopefully trying to explain things as
clearly as I could. We know in theoretical physics you know you can
you can write down an equation but solving it may be extremely difficult
you have to resort to approximations but it's still nice to have that that
north star that compass that guides you and so for me I try to think of machine
learning in similar terms there are some some foundations that that really
don't change much over time that are that are very good guiding principles and
we're dealing with data we're dealing with uncertainty we want to be
quantitative so you're led very naturally indeed uniquely into probability theory
and if you apply probability theory consistently that is the Bayesian
framework so for me the Bayesian framework is a very natural
bedrock on which you can build and think about machine learning now just as
with theoretical physics you can't often just solve things exactly and
certainly the Bayesian paradigm calls for
integration or marginalization of all possible values of the parameters in
your neural network well you always operate with a fixed
computational budget right it may be a huge one but it be always constrained
by by computational budget and should you spend that budget doing a very
thorough Bayesian marginalization over a small neural network
or should you take the same number of compute cycles and train a very much
larger network and if you have plenty of data to train the larger network then
the latter seems to be much more effective in a practical sense
so while from a practical point of view the Bayesian approach still has
certain applications in in various domains for the most part it's not the
framework we'd want to use in in sort of mainstream
machine learning today we're much more interested in scale and making point
estimates in using stagastic gradient send and so on so I still
think that students should learn the basic ideas of of Bayesian inference
because really they have to learn you have to learn about probability I don't
think you can be in machine learning and not understand probability
and then once you understand probability and you apply it uniformly that
that really is the Bayesian framework so I think it's the foundation
but then you're led to make approximations and in particular you make
point estimates so in practice you don't actually execute the full Bayesian
paradigm yeah I agree that um Bayesian reasoning is
it's beautiful and it's the continuation even of sort of propositional
logic in the domain of uncertainty it's fundamental
but there is this question of the world is a very gnarly place
and folks argue that the brain is a kind of Bayesian inference machine
but it can't it can't possibly be solving the intractable Bayesian problem
and therein lies the question so there are many hybrids or
even deep learning approaches could be seen as some kind of a continuation or
somewhere on the spectrum between maximum likelihood point estimation and
Bayesian models I mean how do you think about that
spectrum I think that's a great that's a great question I
think you're spot on there if you look back to a time when
there are a lot of competitions here's a data set we're going to hold out the
test set you've got to score as high as you can on the test set
and what approach should you use the winner always is an ensemble you
should try 10 different things preferably diverse and then combine
them suitably maybe taking an average or some smarter combination
and that ensemble will always outperform any one single model
so if you're not constrained by compute and in some of those competitions you
weren't then the ensemble always wins and you can think about that ensemble is
like as you say a sort of rough and ready approximation
to a full marginalization of all of the uncertainty in the predictions that you
might make and so I think there's a little glimmer of
sort of Bayesian approaches coming through there but again
you know in the modern era you're probably better off training one single
large model than 10 smaller ones and averaging
so it's so I think knowing about the Bayesian paradigm and understanding
where you can learn from it is still valuable today
but nevertheless it's unlikely in most applications that you're going to want
to apply the full Bayesian machinery because it's just so computational
expensive fascinating I mean just one more thing on this
do you think of large you know let's say large language models but large deep
learning models do you think of them as one model or do you think of
them as an inscrutable bundle of models because we're kind of getting into the
no-free lunch theorem here coming from the Bayesian world we
design models you know using principles and with neural networks we just
train these big black boxes so do you think of them as one model or lots of
models I certainly I always think of them as a
single model I've never thought thought of them as separate models unless you
unless you explicitly construct a mixture of experts or something like that
you have an internal an internal structure I guess
everything is sort of very distributed and somehow sort of holographic and
overlapping and you know a remarkable thing about GPT-4 is that
you know you often see people when they first they first use it they'll ask
some question how tall is the Eiffel Tower and it probably gets the right
answer and you know it's like oh that's kind of interesting and you're sort of
a little bit disappointed in this technology but it's like being given
the keys to a very expensive sports car and you notice the cup holders and you
notice that it can can support a cup rather nicely you don't realize you
need to start the engine and and drive off in it to really get the full
experience and so until you realize that actually you can you can have a
conversation it can it can write poetry it can explain jokes it can write code
it can do so many many different things and that all those capabilities
embedded in the same model and and what is I think a really interesting
lesson of the last few years is that models like GPT-4 outperform the
specialist models so for example in my lab we had a project for many years which
essentially said the following it said well you know this is Microsoft
world's biggest software company we have lots of source code we could use source
code as training data for machine learning we've added all sorts of things you
know spot bugs do water complete you know all kinds of things you could do if
you had a good model of source code and the project was reasonably successful it
was you know it worked reasonably well but what we've learned is that when you
build one gigantic model that that yes it sees source code it sees scientific
papers it sees wikipedia it sees many many different things in some way it
becomes better at writing source code than a model specifically for writing
source code and there are even even in ablation studies where people have a
model that's trained to solve maths problems and it does reasonably well
and now you give it some apparently irrelevant information let's say from
wikipedia but with anything to do with maths stripped out and you find it
actually does better at the maths so I think there are things here that we
don't really understand but the general lesson I think is fairly clear
that when you have a larger very general model it can outperform
a specific model which I think is very interesting I guess the reason I was
talking about the no free lunch theorem is it feels to me as you say that
models behave quite differently in an input sensitive way
so you ask them about this particular thing and it's almost like it's a
different model because different parts of the model get activated
and then there's this question of well is the no free lunch theorem violated
can there be such a thing as a general foundational agent
that could in robotics just do really well in any game or any environment
or do you think do you think there's still some need for specialization
another great question so I think these really open research questions honestly
I'm not sure anybody really knows but I think one of the lessons is that the
general can be more powerful than the specific so clearly one of the research
frontiers we should push on is greater and greater
and see so you know GBT4 can't ride a bicycle but if we have models that can
can do robotics should they be separate and distinct models or if we somehow
combined everything into a single model would it be more powerful
and there's a decent chance that the latter would be true that it would be
more powerful so certainly that's one research frontier
we should push on an area I'm very interested in these days is
is deep learning for for science for scientific discovery and science
amongst other things involves very precise detailed numerical calculations
now if you want to multiply some numbers together GBT4 would be a terrible way of
doing it it might give you the wrong answer and even if it gets the right
answer you're burning a tremendous amount of compute cycles to do something you
could do with the far fewer compute cycles
so there will still as far as I can see in certain domains be a role for
specialist models but even then I can see them being
integrated with things like large language models
partly to provide human interface because one of the one of the things about
language models is they they're so easy to interact with you don't have to be a
computer program you just have a a natural conversation with them
but also the other remarkable thing about the large language models
I think there are two remarkable things the first of all is that they're so good
at human language maybe that's not too surprising because they're sort of
designed to do that but by virtue of being forced to
effectively compress human language they become reasoning engines and that's a
remarkable discovery right that is a big surprise certainly to me I
think to many people perhaps to everybody in the field
that they can function as reasoning engines and so even if you're
let's say doing some specialist scientific calculations you might still
think about a large language model as as a kind of a
co-pilot for the scientist helping the scientist reason over
what increasingly consists of massive massively complex spaces
very high dimensionality many different modalities of data
it's harder and harder for humans to sort of wrap their head around this and this
is where I think a large language model can can can be valuable
but I still see it calling on specialist tools in the foreseeable future
because you were talking about statistical generalization but you could
argue that language models can't do let's say they can't compute the
nth digit of pi because they don't have an expandable memory they're not
Turing machine so that that's a computational limitation but
but they might be able to do this statistical generalization
as we were talking about even though it might in fact be a weird form of
specialization in terms of an ensemble of methods of models inside
a large language model but on the on the language thing and the reasoning this
this is fascinating so I think that language
is a bunch of memetically embedded programs
so we we play the language game and we establish cognitive categories we
embed them and share them socially and it's like
there's a little simulation out there and I'm using that to think
but the question always is to what extent
and is that that's a bunch of processing that previous humans have done
and we can use it but can the language model create
new programs like that this is I think part of a fascinating and broader
discussion so I do hear a lot of oh it can't do x y and z
often that's true and I've always put the word yet at the end of it because I
don't know any law or physics and it can't do certain there are some things
which perhaps the current architectures provably can't do
but but there's lots of exploration in different architectures there's a lot of
scope for for for expanding and generalizing neural net so I
always think of it can't do a certain thing yet
but a lot of the questions or a lot of the comments about
the limitations of models I have a have a hypothesis on this I mean
let me test this out on you I may be I may be way shorter the mark on this one
but a lot of the a lot of the critique of what models
seemingly can't do or especially when it's oh they will never be able to do this
they cannot be creative or they cannot reason or they cannot whatever
I wonder if a lot of this comes to to a much more fundamental point that's not
actually a technical one it's really to do with the human
the human journey over the last few thousand years because we've
you know a few thousand years ago I guess most humans would have perceived
humanity as the center of the universe they were the earth's center of the
universe the universe was created for the benefit of humanity
we had this very arrogant view of our own importance and what we've learned over
the centuries especially from fields like astronomy
is of course you know that the the entirety of humanity's existence
is a brief blink of the eye compared to the distance of the the whole universe
and and our physical place in the universe in terms of length scale we're on a
little speck of dust orbiting an insignificant star in a rather boring
galaxy in this colossal universe and and so I think it's natural for us as
humans to sort of continue to cling to the things that we feel make us special
and we're certainly not the fastest creatures on earth we're not the
strongest but it's our brains that seem to make us
unique we are the most intelligent creatures by far on earth
and so we think of our of our intelligence as being the very special
thing yes okay we get it that we're just living in a
sporing corner of the universe but nevertheless it's our brains that make
us special so let me tell you a little story which is
because I work for Microsoft I was very privileged to have early access to
GPT-4 and it was still a highly-tented highly-secret
project and so I was exposed to GPT-4 at a time when
I could only discuss it with a very small number of very specific colleagues
and for everybody else I couldn't couldn't even talk about it
and it was quite a shocking moment the the ability to
understand and generate language sort of didn't come as so much of a surprise
because of course I'd been following GPT-2 and GPT-3 and
you know knew this technically was getting better
but this ability to reason there was a sort of visceral reaction I had which
took me right back to that film 2001 that sense of I was engaging with something
which you know my colleague Sebastian Bubeck called it the sparks of
artificial intelligence so nobody in that nobody's claiming GPT-4 is
anywhere close to human intelligence or anything like that but there was just
the first glimpse of something it was the first time in my life that I'd
interacted with something that wasn't a human being
that had a glimmer of this this higher level of intelligence
and and realizing this may be the dawn of a of a new era that may be
even more significant than the 2012 moment of the dawn of deep learning
there was something very special going on and I wonder if part of the reaction
that we have to these models is a little bit of that sense of that threat to
the specialness that we feel as humans now maybe completely wrong this is purely
speculation but you know it's interesting that we
talk about people use phrases like stochastic
parody it's just regurgitating stuff that it that it's seen before
some people claim or you know of course it hallucinates sometimes it comes up
with stuff that's just wrong or doesn't make sense
but but think about the following imagine there was a very very smart
physics student went to you went to a top university worked really hard for
four years what would they do they would they would read books they would
read papers listen to lectures have discussions with their professors and
with other students and then they sit their final exam
and they get 95% in their final exam and they come top of the year
we don't say huh well 95% of the time there are stochastic parrot
regurgitating Einstein and Maxwell and the other 5% of the time they're
hallucinating no we say congratulations you have a
first-class honors degree you've graduated with honors this is this is a
you know a wonderful achievement so it's interesting that we do seem to view
the the capabilities of of neural nets with it with almost a different ruler
to that of humans and while nobody's suggesting that current models are
anywhere close to humans on many axes of intelligence
nevertheless i see the first sparks of of artificial intelligence
and just one final comment the term AI artificial intelligence has been very
popular for many years i used to hate it i used to always say that's machine
learning none of these systems are intelligent they're very good at
recognizing cats and images there's nothing really intelligent about this
in in in in one sense and yet now i find for the first
time i feel comfortable talking about artificial
intelligence because i think we've taken the first baby steps towards what i
think of as true artificial intelligence i still think that
agency and creativity are the distinguishing feature
not necessarily that we are and biological beings
it's more to do with we are independent agents and we are
sampling random things from our local worlds and we're combining them
together in in interesting ways and in doing so
intelligence is about the process of building models
and sharing models and embedding models in in our culture
so it feels to me that gpt was building models
at the time it was trained and and and that's all it's doing
i can imagine a world where there were lots of gpt's
we all had gpt in our pockets and maybe then it would be much more like
biomimetic intelligence i think there are lots of interesting
points that you touched on there tim so i think one thing is in terms of
creativity you know are these systems creative it's certainly true they only
exist because of humans they're created by humans
and and we should acknowledge that but i don't think it means they're
intrinsically not creative if i asked an artist to
paint me a picture of some people walking on the beach with a sunset or
whatever and they came back a few days later with some
beautiful picture i might hate it they may have used very vivid
colors i might like pale pastel colors but that's a matter of opinion
but i wouldn't deny that there was creativity there
but their expertise came because well they went they perhaps had some
intrinsic ability in some sense but they went to art school they study the
work of other artists they practice they got better
and and and that creativity owes a lot to what went before but i don't think it
diminishes that in the same way a physics student who can
explain the theory of relativity you have to say well you didn't invent the
theory of relativity you know einstein invented that you only learned it from
einstein but it doesn't diminish the the fact
that they have understanding the fact that they convey it and the fact they
can potentially think in new ways and be creative
so i'm i'm less convinced about discussions about the limitations of
of of the technology in general of where it can go i don't particularly see any
limitations the brain is a machine that uses this
a term used earlier connectionist approach it uses these fine-grained
neural nets and and so there are similarities to the
technology that we have now there are also huge differences
some of those differences point to the artificial neural nets being much more
powerful than biological neural nets and hinted made a strong
point of this lately and i think it's a very interesting
perspective so i would be the first to say
yes the technologies we have on many axes are a long way short of humans on
many axes the much better gbt4 can create text
much better than any human i mean to produce a page of coherent text that's
correctly punctuated in good grammar and so on in a few seconds there aren't
any people that can do that i think so on an increasing number of axes
systems clearly outperform humans and on others there's still a very long
way to go but i think one of the nice things about technologies like this
generative ai technologies whether it's you know saura for creating videos or
gbt4 or whatever it might be is they do rely on the prompt there is a clear
role they are co-pilots as we say they they they sit there and do nothing
and you use them as a sort of a cognitive amplifier you have an idea
sort of half-baked and then you can engage in a conversation and
sure enough it can come up with a different way of thinking say hey that's
really good i like that idea now let's take that work that back in try again
and so it becomes now a companion a co-pilot something that
enhances your your cognitive ability but the human is still very much in the
loop and playing a key part and actually initiating the process
and of course finally at the end of the day you're the one that selects the
you know the 10 video clips you pick the one that you like and so the human is
very much involved in the loop throughout so i think that's a very nice
feature of this technology i completely agree with that so
at the moment a is are embedded in the cognitive nexus of humans so we have the
agency and we drive these things and and they
help us think and also i agree with you that it doesn't
make sense to think of these things as limited forms of computation we should
think of the collective intelligence so we are touring machines and we are
driving these things and we are sharing information so when you look at
the entire system it is a new type of memetic intelligence in fact
you know to a certain extent GPT-4 isn't running on Microsoft servers
it's in all of us right and that's that's a wonderful way
um to think about it but to me the extent to which it is constraining our
agency and creativity is what i'm fascinated by so GPT says unraveling
the mysteries and you know the intricate
dance of x y z and all of these weird motifs and
constructions and maybe that's just the way that our
LHF has constrained the model or maybe it speaks to the constraining
forces in general of having these low entropy models that kind of you know
snip off a lot of the interesting pathways so
we are very creative GPT-4 resists creativity a little bit is it a problem
well i think there's some design choices there so you talked about reinforcement
learning through human feedback is part of that alignment process we ought to
create this technology in a way that does good to minimize harm
and so naturally we do constrain it so for sure it's true that our constrained
GPT-4 behave in you might say less creative ways but perhaps in more
helpful and beneficial ways and it's appropriate that we should do that
and perhaps we lose a little bit of the creativity
in the process and so there's there's a balance there's a there's a there's a
choice to be made a design choice in how we want to create the technology and
we should be very deliberate about that and not not apologetic for that i think
it's good that we are that we are making those design choices
but people sometimes have an intuition that it's not creative
and contrast that to i'm using DaVinci Resolve
and i'm using all of these nodes and i have all of these filters and
processing transforms the difference seems to be that
i'm designing the architecture so i'm using cognitive primitives
and i'm composing them together in a new way and by tweaking the parameters on
the filters i'm going off-peast a little bit i'm
doing i'm creating the structure myself whereas in neural networks the
structure is implicit i don't know what the structure is
well i think you're talking about you're contrasting two different kinds of tools
there so the video editing tool is designed so that it follows your
instructions very precisely and you prefer one tool over another
perhaps because the interface is easier to use you get the results faster but
you have in your you've done the creativity you've designed this to
video edit that you want that you want to have and now the tool is to try to get
you to that as fast as possible as accurately as possible
but sometimes we need more than that sometimes you know if you've got right
as block and you don't know where to begin
having a tool like GPT-4 could be very powerful you're not you're not
delegating the entire process to the technology you're working
with it as a as a co-pilot as an assistant
that can for sure help you with that creative process it will come up with
with crazy things and most of them you may not like but maybe one of them
you don't like it either but it causes you to think about something that you
would otherwise not have thought of and so the two working together
can surely be more creative so i think certainly as a working in unison with
humans it certainly enhances creativity so that's
certainly my experience i think there's no doubt about that but also if you
think about let's take a simple example that i think most people relate to which
is which is image generation you're giving a talk and you want some
image to illustrate the talk and you know you could go to stock images and
you know it's a fixed set and you know you can't easily adjust it or you
or you go to editing images yourself that's a sort of slow and painful process
but now you can just with a simple prompt you know you can get a a bunch of
examples and if one of those isn't quite what you like you can
alter the prompt and and fine tune it and it now becomes that
that process which is a creative process and you can still say the human is in
the driving seat but the overall creativity is certainly enhanced
and when you take a text prompt and and the machine produces this beautiful
photorealistic image i mean how many of us weren't absolutely blown away
by the incredible advances in generative AI of the last
you know the last decade why would you not call that creative if a human being
did it you would call it creative why why are we not allowing the machine
to be described as creative that's the piece that i don't that i don't quite
understand so you could argue that creativity is just pure novelty of the
artifact so it's just how much entropy is in the artifact
but you could you could think of GPT4 pros as being a kind of
category so there's a lot of variants in there
but there are also certain motifs and and now when people see the motifs they
say oh i've seen that a million times before so i did think it was novel and
interesting and now i don't and but this is the thing so now when i'm
writing blog posts and stuff like that i'm deliberately trying to do something
genuinely creative to me you know it's it's almost like the intrinsic
creativity isn't important i don't want people to think that i use GPT4
so that's driving it do you see what i mean yeah so in clearly creativity is
about novelty and novelty is you know what we desire here
but whether that novelty has value or not that's a subjective opinion in your
case it's whether it's achieving the the goals that you desire
so i think there is no doubt that it's even if you say
we're just taking existing ideas and combining them in new ways
everything that humans do or i think builds on the work of
their own previous experience and on the work of others and i think that's
absolutely fine that's a wonderful thing about the humanity is that we
from generation to generation we build upon the work of what's gone before
and the machines that we build now are heavily dependent on
the creativity and the work of humans before because they learn from humans
they're designed by humans and i think that's absolutely fine it's a
wonderful thing and they add to the sum total of human
creativity and that that's a wonderful thing
Chris you wrote a really beautiful book and you wrote it with your son
Hugh and there was a picture of Hugh i think in the introduction of of PRML
and i guess part of what i want to understand is is deep learning is a
huge field i mean what was the thought process and
how did you decide what to tackle and what not to tackle
great questions there's an interesting story behind the the new deep learning
book which is that PRML was written in 2006
it predates the deep learning revolution and what has constantly surprised
me is just how popular it's remained in spite of the fact that in one sense it's
massively out of date because it does has no mention
of the most important thing in the field of machine learning and so i've long
felt it was time to update the book produce the second edition add some
material on deep learning but life is busy and you know
anybody who's ever written a book will tell you that it takes way more effort
than you can possibly imagine if you've not actually had that experience
and so i never really got around to doing it and along came the covid pandemic
and we all went into lockdown and i feel like it was one of the very
privileged people in that lockdown we were we were locked down together as a
family in in Cambridge and you know when you're
locked down at home for several months you kind of need a project and
and i thought this would be a great time to think about a second edition of the
PRML book because you know what what else you're going to do in lockdown and it
became a project with my son because he was he was with me
by this time he he'd gained a lot of experience master's degree in machine
learning and been working in autonomous vehicle technology
and in a sense he had a lot more practical hands-on experience with deep
learning than than i did at that point and so we started this as a joint
project but we very quickly realized that what was needed was not
a couple of extra chapters on PRML but rather the whole field had changed so
much and also we didn't want to write a book we
were just accumulated more and more material it would just become a huge
a huge tome the value of a book i think is
is in the distillation is in the way it draws your attention to a subset of
specific things this is the small set of things that you really need to
understand and then you're quick to go off into the field
so what we omitted was almost as important as what we what we added
and we very quickly realized this was a this was a new book so we we we called
the book deep learning foundations and concepts
and we made a lot of progress but then of course the the lockdowns
ended i started a new team called ai for science at microsoft
hu started at wave technologies building the the core machine learning
technology for their autonomous vehicles and we were all just far too busy
and then the next thing that happened was the chat gpt moment
we're you know in a space of a few weeks a hundred million people were using
this and suddenly ai machine learning was in the
in the consciousness of the the general public
and we realized that if ever there was a time to finish this book it had to be
now and so we had a just a really big push to
to get the book finished and available for for new eurips in 2023
and we made it just you know at the last minute as you do
and the book was on display at new eurips there and
hu and i spent the week going around the conference together
talking to folks at posters and and just had a great time so it was actually a
huge privilege to to be able to write the book with my son
yeah that's fantastic um what was your favorite chapter
and i mean are there any um things that you felt were
remissions that you would have liked to do but you just had to draw a line under
it yeah in terms of uh favorite chapters i mean
of course the things the the the more recent architectures were
particularly interesting i very much enjoyed writing the the diffusion
chapter and hu had a lot of input into that chapter of course transformers as
well and just understanding how to how to
integrate the the sort of the different generative frameworks how to bring
think about gans and how to think about variational order encoders and you know
how to think about normalizing flows and so on how to think about those under
one umbrella and present them in a more coherent way
so that was that was part of the interesting free the learning experience
i always enjoy learning new things i learned things writing that book and
that and i think you did as well and so in a sense that was that was the
favorite part of the book the things where i where i learned new things or
new ways of looking at things i already knew about the real decision process is
what to put in what not to put in while keeping the size of the book under
control because i think it's something like it's thousands of papers a month
now published in machine learning uh it's overwhelming for the beginner so
really the goal of the book is to still out those few core concepts which means
there are always things oh should we have added this should we have added that
what we wanted to do was to avoid adding the latest sort of
architecture that might be very hot at the minute that could easily disappear
three months down the line so i hope we've resisted that that temptation
but there are areas where you know perhaps when we at some point if we get
around to a second edition we might think about including reinforcement
learning is something which is of growing importance and
would be lovely to have a chapter on reinforcement learning that integrates
well with the rest of the book there are books on reinforcement learning there
are review articles there's plenty of place to go learn about them
there's something that sort of integrated with the book i think could be
could be valuable so that is something we might we might visit in the future
but for the moment we've just focused on what we think are the
core principles that any any newcomer to the field whether a master student
whether they're somebody who's self-taught a practitioner coming into
the field wanting to understand the basics of the field and so the goal was
to try to keep the book as it were as short as possible but no shorter
looking back on on your last couple of books as well in in retrospect
which bits are you are you most kind of proud of and
which bits do you do you kind of feel that when you did make the decision at
the time perhaps you've you've you've mispredicted how successful something
might be very interesting so the thing i'm most proud of actually
is the very first book called neural networks for pattern recognition
and the reason is because i think that the book was quite influential in
steering the field towards a more probabilistic more statistical
perspective of machine learning it perhaps hard for people to appreciate
today but it wasn't always that way when i first went into machine learning
a lot of it was inspired by neurobiology which is which is fine
but it lacked sort of mathematical rigor it lacked any mathematical foundation
and so there was a lot of trying to learn a bit more about the brain and then
try to copy that in the algorithms and see if that worked better or not
and there was a lot of trial and error still a lot of empirical trial and error
in machine learning of course but at least we have that that sort of bedrock
of probability theory and so i think that the book was the first one to
really address machine learning and neural networks from a statistical from
a probabilistic perspective and i think in that respect the book was very
influential the field was much smaller than today we take we take that as
obvious but i think in terms of the thing i'm most proud of it's probably
the influence of that that first book back in back in 1995
in terms of things i look back on that i might do differently
i suppose when i look at if i look at prml for example and i look at the
trajectory of the field we've seen that neural networks were
were all the rage in the mid mid 1980s to mid 1990s
and then they kind of got overtaken by other techniques and then we had this
sort of Cambrian explosion of you know support vector machines and
Gaussian process and Bayesian methods and graphical models and
and all the rest of it and and i think one thing that one thing that i think
Jeff Hinton really got right is we really understood that neural networks
were the way the way forward and he really stuck to that
perspective sort of through thick and thin i got kind of distracted
particularly we talked earlier about Bayesian methods and how beautiful and
how elegant they are and a theoretical physicist it's very appealing to think
of everything from a Bayesian perspective but really what we've seen
today is that the the practical tool that's giving us these
extraordinary advances is neural networks and most of those ideas
go back to the to the mid 1980s to the idea of gradient descent and
and so on a few new a few new tweaks you know we have GPUs we have
reluers we have a few but essentially most of the ideas were
were were were still were around back in the
back in the late 1980s we didn't really understand
the incredible scale at which you need to use them but they only really work
when you have this gargantuan scale of data and compute
and of course we didn't really have GPUs or know how to use them back then
so there were some key developments that sort of unlocked this and made it
possible but i think perhaps if i did something
differently with the amazing benefit of hindsight other than sort of
investing in certain stocks and whatever and all the other things you could do
if you had perfect hindsight i think the other thing i would do is probably
just stay really focused on neural networks because eventually there
that's the technology that came good but i always come back to probability
theory it's very much a unifying idea so for let me just give you a specific
example from prml actually there were two different technologies one called
hidden markoff models that were all the rage and speech recognition back then
another technique called kalman filters that have been used for many years to
to guide spacecraft track aircraft on radar and all sorts of things
it turns out they're essentially the same algorithm
and not only are they the same algorithm but they can be derived from the most
beautifully simple principle you just take the sum and product rule of
probabilities and then you take the idea that a joint probability distribution
has a factorization described by a directed graph
and if you want to so when i was preparing prml
i looked over a bunch of books called kalman filters an introduction to
kalman filters and they become chapter after chapter at the
forward and then chapter after chapter at the reverse equations and so on it
very very complex and very very heavy going but you can derive
the kalman filter and get the hidden markoff model for free in almost a few
lines of of algebra just starting from probability theory
and this idea of factorization it's sort of deep mathematical principle that
operates there and you discover the message passing algorithm and if it's a
tree structure graph it's exact and you have two passes
it's very beautiful very elegant so i love the fact we're exploring
all these many different frontiers but i love the fact we have some at least
some compass to guide us as we as we engage in the
exploration of this combinatorially vast space
yeah it's so interesting my co-host Dr Keith Duggar he always says that he
doesn't need to remember all of the different statistical quantities
because he can re-derive them from first principles it's that nice
but we should move on to AI for science so you're leading this
initiative at Microsoft Research can you tell us about that
yes so at a personal level of course this brings back my
my earlier interest in theoretical physics and chemistry and and biology
and that brings it together with with machine learning
and what many people realized a few years ago
was that of the many areas that machine learning would impact the scientific
the area of scientific discovery would be i think in my view the most important
the reason i say that is because it's actually scientific discovery
that really has allowed humans to go on that trajectory the last few thousand
years not just understanding our place in the universe but to be much more in
control of our own destiny to double our lifespan to cure many
diseases to give us much higher standards of
living to give us a much brighter outlook for the
future than humans humans have traditionally enjoyed
and and that's come through scientific discovery and then the application
of that knowledge and understanding of the world in the form of technologies
agriculture industrial and so on and so i can't think of any more
important application for AI but what's really interesting is it's very clear
that many areas of scientific discovery are being disrupted and when i say
disrupted i'll just give you one simple example
the ability of neural nets machine learning models to
act as emulators for previously were very expensive numerical stimulators
very often gives you a factor of a thousand acceleration
you know we can forecast the weather a thousand times faster with the same
accuracy than we could a few years ago prior to the use of deep learning
now if that were the only thing that was happening
that alone would be a disruption that alone would be worth setting up a team
on AI for science i think actually it's only scratching the surface
but anytime something that's very core very important gets a thousand times
faster it means you can do things that would take
years in a few tens of hours it that really is a disruption it really is
transformational so a couple of years ago i pitched to
our chief technology officer to say look this is a really important field
i'm happy to step down from my role as the lab director of MSR
in in europe and instead i'd like to lead a new team
focusing on AI for science and met with enormous enthusiasm
and so we've been growing and building that team it's very interesting team
it's very multinational we have people on on many different continents in
different countries we've opened new labs in in in
in amsterdam and in in berlin we have teams in
in beijing and in shanghai and folks in in seattle as well
and so very very multidisciplinary very multinational
but with with one thing in common this real excitement and passion for what
machine learning and AI is going to do to really transform and accelerate our
ability to do scientific discovery you were talking about inductive
priors just a second ago and i guess i first learned about this
with the art of you know designing inductive priors and machine learning
from max welling's group they were saying that you know the
remarkable thing is that you can using principles let's say from physics
we can design these inductive priors and we can reduce the size of the
hypothesis class that that we're approximating
and because we know the target function is inside that class we are not
introducing any approximation error and we
we are kind of overcoming some of the curses in in in machine learning by
making the problem tractable which which is amazing but
that's speaking to this kind of principled approach of imbuing
domain knowledge into these systems it's really interesting actually max and i
have a similar trajectory you both did phd's in theoretical physics and then
moved into machine learning and i think we both feel there's a very
important role for inductive bias to play in the use of
machine learning in the scientific domain i think i'm sure everybody is
familiar with the the blog called the bitter lesson by rich sudden
and if any if anybody watching this is is not familiar they should
immediately after this video go and read that blog it's a very short blog
and without giving too much of a spoiler he essentially says that
every attempt by people to improve the performance of machine learning by
building in prior knowledge building in what we call inductive biases
into the models it produces some improvement and then but very quickly
it's overtaken by somebody else who just has more data
and and that indeed is a bitter lesson and and it's a wonderful blog and
people should i i've read it many times i think people should you know probably
read that once a month and and it's it's very inspiring
but i think there may be exceptions and i think the scientific domain
is one where inductive biases for the foreseeable future will be
extremely important sort of almost contrary to the bitter lesson
and a couple of reasons for this one is that the the inductive biases we have
a not not of the kind let's say let's say linguistics or something which is
any domain where which is based on human expertise acquired through experience
because a person who's had a lot of experience over a number of years and
formulated some sort of rules of thumb that guide them that's exactly what
machine learning is very good at processing very large amounts of data
and and inducing the the the rules as it were the patterns
within that data so i think that kind of inductive bias is
typically harmful and and i think the bitter lesson will certainly apply
there but in the scientific domain it's rather different first of all the
inductive biases we have are very rigorous we have
the idea of conservation of energy conservation of momentum we have
symmetries if i have a molecule in a vacuum it has a certain energy
if i rotate the molecule the representation of the coordinates of all the
atoms changes wildly in the computer but the energy is the same
so we have this very rigorous inductive bias we also know that the world at the
atomic level is described exquisitely well by by Schrodinger's equation
sprinkling a few relativistic effects and you've got an amazingly accurate
description of the world but it's way too complex to just solve it directly
or is exponentially costly in the number of electrons but nevertheless we have
this bedrock of of really understanding the laws that govern the universe
and so and so i think that's the first the first thing we have very rigorous
priors that we believe in deeply it's not that we think conservation of energy
doesn't work we know that we know that it's true
the second thing is that we're operating in a data scarce regime so large
language models are able to use very large quantities of internet scale
quantities of human created data whether it's in the form of you know
whether it's wikipedia or whether it's just scientific papers or any of the
output of humans almost is potentially material on which which large models
can feed they're in a very data rich regime and can go to scale
and and so the bitter lesson i think really kicks in there in the scientific
domain the data might come from simulations which are
computational and expensive or it might come from lab experiments which are
which are expensive and the data is is limited so we're operating
usually in a data scarce regime so we have relatively limited data
and we have very rigorous prior knowledge and so the balance between
the data and the inductive bias is very different because of course the no
free lunch theorem says you can't learn purely from data you have to have some
form of inductive bias and in the case of a transformer it's a
very lightweight form of inductive bias we believe there's a there's a deep
hierarchy there's some you know data dependent
self-attention but but really that's it and the rest is determined from the data
in science there's much more scope for bringing in these inductive biases there's
much more need to bring in the inductive biases and that also
incidentally again in my personal very biased opinion makes the
application of machine learning and ai to the sciences the most exciting
frontier of AI machine learning because it's the one that's
richest in terms of the creativity and also in terms of the need to bring in
some of that beautiful mathematics that that underpins the universe
yeah so so fascinating I mean could we just linger just just for a second on
that so rich Sutton in his bitter lesson essay he explicitly called out
symmetries as being you know he was warning against human designed
artifacts in in these models and I mean max welling as you say famously
built these gauge equivariant neural networks bringing in his
physics knowledge and so I'm just trying to understand the spectrum between
high-resolution physical priors and the kind of macroscopic human knowledge
that that we learn which is presumably brittle
is it just that we think that these physical priors are fundamental
and that's that's a that's a perfectly acceptable way to constrain
the search space but these high-level priors are brittle
yes I think I think the the the prior knowledge that comes from human
experience is is is more of that brittle kind
because the machine can see far more examples than a human can in a
lifetime and can can do a more systematic job
of looking across all of that data we're not
not subject to say recency bias and those sorts of things
so I think that kind of prior knowledge is is one where
where scale and data will will win whereas the the prior knowledge that we
have from the physical laws in a sense is much more rigorous and symmetry
is is is very powerful it's sometimes said that
physics more or less is symmetry that's almost
yes right so conservation conservation laws arise from
symmetry you know translation in variance in
spacetime gives you conservation of energy and momentum
and you know gauge symmetry of the electromagnetic field gives you charge
conservation and so on and and so these are very very rigorous laws that apply
from symmetry but you know even if you take a data-driven approach
people often use data augmentation if you know that an object doesn't depend
his identity doesn't depend on where it is in the image you might
you know make lots of random translations of your data to augment
your data so data augmentation can be a data-driven way of building in those
symmetries but now when we have very rich prior
knowledge I'll come back to Schrodinger's equation it describes the world with
exquisite precision at the atomic level but solving it is very very expensive
and so what we can do is we can cache those computations we call it the fifth
paradigm of scientific discovery which is a rather fancy term but the idea is
very simple is that instead of taking a conventional numerical
solver and using it to solve something like Schrodinger's equation or something
called density functional theory instead of solving that directly to solve your
problem instead you use that simulator to
generate training data and use that training data to train a
machine learning emulator and then that machine learning emulator
can now emulate the simulator but typically three or four
orders of magnitude faster so provided you use it a lot and you amortize the one
off cost of generating the training data and doing the training if you're going
to use it many many times overall it becomes dramatically faster dramatically
more efficient than using the simulator and that that's just one of the
breakthroughs we're seeing in this space so first of all um there's there's a
spectrum as you say of we could just train on lots of data or we could
augment the data or we could make a simulator for the data and then we can
train a machine learning model and as we were just speaking to
these inductive priors they are so high resolution
that we are not restricting the target function that that that we want to
learn and we can make quite a principled argument about that
but the one question to me is there's a kind of
I don't know whether it's best to frame it as exploration versus exploitation but
there needs to be some amount of going off-piste
so we define the structure and we we essentially build a generative model
and we can generate a whole bunch of trajectories
but could it ever be the case that we wouldn't have enough
variance to find something interesting there's a very interesting question
about the the overall scientific method of formulating hypotheses running
tests evaluating those hypotheses refining the hypotheses running more
experiments and so on that that scientific loop
I think machine learning will have an important role to play there because
data is becoming very high dimensional very high throughput humans can't
analyze this data anymore a human can't directly look at the output of the
large hadron collider with its you know petabytes a second or whatever it is
pouring off we we need machines to help us but again I think the human
rises to the level of the conductor of the orchestra as it were they no longer
have to do things by hand machines are helping to accelerate that
and and I think the machines can help accelerate the creative process
potentially by pointing to anomalies or highlighting patterns in the data and
so on but very much with the human scientist in the loop
but but even coming down from those sort of lofty more sort of philosophical
considerations just to the the practicalities when we talk about discovery
we're also interested just the very practical method of how we how do we
discover a new drug or how do we discover a new material
so scientific discovery also means that that that that that very pragmatic
near-term approach and there we're seeing really dramatic
acceleration through the the concept of this emulator
inner ability to explore the combinatorically vast space of new
molecules and new materials exploring those spaces efficiently to find
potential candidates that might be new drugs or new
new materials for batteries or other other forms of green energy
so that that alone is a very exciting frontier I think
it's so interesting so searching these space I mean drug discovery is an
interesting one I think you spoke about sustainability as well as
another application you can speak to but how do you identify an
interesting drug so the drug discovery process
starts first of all with the disease and trying to
first of all deciding we want to go tackle a particular disease
and then finding a suitable target so the the standard so-called small molecule
paradigm which is where most drugs are today
they're small synthetic organic molecules that bind with a particular
protein so pharma companies will will will
spend a lot of time identifying targets so say a protein that has a particular
region with a molecule combined can combined to
and therefore can influence the behavior of that protein switching on or
switching off some part of that disease pathway and breaking the chain of
disease so the challenge then is to find a small
molecule that first of all has the property that it binds with the target
protein that's the first step but there are many other things that it has to do
it has to be absorbed into the body it has to be metabolized and excreted it
mustn't and particularly mustn't be toxic it mustn't bind to anything
many other proteins in the body and cause bad things to happen
so what you have is a very large space of molecules usually estimated around
10 to the power 60 potential drug-like molecules
and out of that enormous space of 10 to the 60 you're trying to find
an example that meets all of these many many criteria
and so one approach is to generate a lot of candidates but in computationally
and then screen them one by one for different properties that screening
process the more that can be done in silico rather than
in a wet lab the faster it can be done and the
the larger the search space can be and therefore the bigger the fraction of
that space of possibilities you can explore hopefully thereby increasing
the chances of finding a good candidate because many attempts to find a drug for
a disease simply fail nothing nothing eventually comes of it
so increasing the probability of success increasing the speed of that
discovery process so in all of that there are many places
where machine learning could could be disruptive
so on that process of I guess you're describing
you generate candidates and then you almost discriminate interesting ones
and then you rinse and repeat in a kind of iterative process
let me give you a concrete example so we've done some work looking at
tuberculosis so tuberculosis kills something like 1.3 million people very
sadly back in 2022 which is the last year we have we have data
and I might seem surprising because we have we have antibiotics we have drugs
for tuberculosis why are so many people dying
and one core reason is that the the bacterium is evolving to develop drug
resistance and so there's a search on for new for new drugs
so maybe I'll just take a moment to explain some of the architecture and
get into a little bit of the sort of the techie details of this
so we wanted a way of finding we know what the target is we've been told what
the target protein is the target has a region called a pocket and we're
looking for molecules that are bind tightly with that pocket region on the
protein and and so the way the way we approached
this was first of all build a language model but not a language model
for human language but for the language of molecules
so we first of all take there's a representation representation called
smiles it's a way of taking a it's an acronym but just a way of
taking a molecule and describing as a one-dimensional string
and so you first of all take a large database of I don't know 10 million
molecules it represented the smile strings and you treat them like the
tokens for a for a transformer model and by getting it to predict the next token
the next element of the smile string you build a transformer based language
model that can speak the language of molecules
so it can it can run generatively and it can create new
create new molecules as output so you can think of that as kind of like a
foundation language model but speaking the language of molecules
now we want to generate molecules but not just any molecules we want molecules
which bind with a particular target protein so we have the target protein
in particular it's the pocket region that we're interested in
so we can give it the amino acid sequence of the protein as input
but we need more than that we need the geometry of the of the pocket and this
is where some of those inductive biases come in so we
we need to have representations of the geometry of the atoms in that form that
pocket but a way that represents these
equivalences and so they're encoded as input to a transformer model that learns
a representation for the protein pocket and the final piece we need as you said
we want to do this iteratively we want to take a good molecule and make it a
better molecule rather than just searching blindly across a space of
10 to the 60 possibilities and so the other thing we want to
provide as input is a molecule a descriptor of a
a known small molecule that does bind with the pocket already
and but we want to do this in a way that creates variability and we actually use
a variational autoencoder to create that representation and
that's the an encoder that trun translates the molecule into a latent
space and we can sample from that latent space
and then the this language model the smiles
language model can attend to both the output of the variational autoencoder
and the output of the protein encoder using cross-attention
and so what we've done there is I think rather tastefully combined
some elements from you know states of the arts at modern deep learning
the result then can be can be trained end to end using a database
of known other proteins that are known to bind efficiently to
small molecules and once the system is trained we can now
provide as input the known target for tuberculosis
and some known molecules that bind with this and then we can
iteratively refine those molecules at the output we get molecules that have
better binding efficiency and we're able to increase the binding
effectiveness by two orders of magnitude and so we now have states of the art
molecules in terms of binding efficiency to this
to this target protein of course we can't do the wet lab experiments ourselves
we partner with an organization called giddy the global health drug discovery
institute they've synthesized the molecules that we've we've
generated and measured their their binding efficacy
and so we're very very excited about this and of course the next stage now is to
take that as a starting point and further refine and optimize those molecules and
and try to address all those other requirements that we have for before a
drug can actually be tested on humans in terms of its
toxicity and metabolism and all and all the other things
but i think it's just a a very a very nice example of almost like a first step
in using modern deep learning architecture to accelerate the process of drug
discovery and already we have i think really quite a
spectacular success given that we're we're kind of newcomers to this
to this field partnering with experts domain experts with the wet lab
experience and the wet lab capability to me this is the beginning of a very
exciting journey that sounds incredible
is there any kind of representational transfer between the the models so for
example you're talking about this this geometric
prior model and generating tokens to go into the language model
because just using language models by the way is a fascinating approach i
spoke with christian sogeddy and he was doing mathematical conjecturing
just using language models you know just just taking mathematical
constructions and putting them into language and they used to use graph neural
networks for this and so i guess the question is
could you kind of bootstrap it with a you know with an inductive
principled model and then kind of just train using the language model afterwards
i think i think the general principle there's a very powerful one so the idea
of borrowing strength from other domains and i think we're seeing this time and
time again in deep learning that that the machine learning models are able to
extract some general patterns from even from one domain and translate them into
completely different domain we talked earlier about large language models
being getting better at writing code if they've also got exposure to
to poetry or something is seemingly quite irrelevant there's some
there's something quite deep and subtle going on there but perhaps in a less
subtle way it's clear there's a sort of a language of
molecules there's a language of materials and that by
building models that have a broader exposure to that language
they almost invariably will become better at the specific tasks that we want to
to apply them to so i think there is a general principle at heart there
yeah it's so interesting because i i used to think that
that perhaps the drawback of these inductive prior models is that
it was one inductive prior per model but this ability potentially to
bootstrap a foundational model that can do all of the things
that's really interesting i think the most powerful inductive biases and the
ones we focus on are really those very general ones where
symmetry says just very fundamental properties of the universe and we
want we want those really baked into the models i think
the the the sort of intuitions we have about more specific domains i think
they can perhaps lead us astray because they're based on
our experience of much more limited domains i think this is where the
machines can be can be much better at
processing and interpreting large volumes of data and drawing regularities
out of that out of data in a more systematic way
okay okay and just before we we leave this this is a bit of a galaxy brain
question and and that that's parlance that all the kids are using these days
by the way but how fundamental is is our physical
knowledge you know the question is like we are we're designing these inductive
priors as if they are fundamental but folks like Steven Wolfram for
example argue that there's there's a deeper
ontological reality you know might be a graph cellular automaton or something
like that and is that something you think
about the kind of the gap between our models and what reality is
so i think first of all one of the greatest scientific discoveries of all
time is the fact the universe can be described by simple laws that that is
not obvious a priori that itself is perhaps the most
profound discovery you know really going back to Newton but we found it time
and time again what we've also found is that the
our understanding of the universe as it exists today
has has it's almost like onions we're peeling way layers of onions
you know Newton if you want to navigate a spacecraft to Jupiter you still use
Newton's laws of motion and Newton's law of gravity it's just fine
it doesn't mean we believe it's exact description of nature we've now got
deeper descriptions of nature we understand relativity for example
general relativity tells us that actually Newton's second law of
motion or Newton's Newton's law of gravity rather is just an
approximation the inverse square law is a pretty good
approximation but we've got a much better description now
but but it's it's it's hard to say that we've we've found the ultimate answer
it's rather that human knowledge is or just always
stands on that that edge of what we don't understand and scientific discovery
is always about exploring the things we don't understand
working out whether you know whether the laws actually do hold
and the anomaly we see in the data is is because of some
phenomenon that we haven't yet observed I mean this is how Neptune was
discovered by by seeing that the planets were not behaving as they
should do according to Newton's laws Newton's laws were just fine there's
just another planet perturbing them or is the procession of the perihelion of
Mercury because because there's another no it's because
actually Newton's law of gravity isn't quite right we need
relativity to understand that so I think scientific
exploration as far as I can tell has no particular end in sight it's
rather that we have things that we understand and there are new frontiers
you know when I was when I was a teenager getting
excited by physics I love reading about relativity and quantum physics but it's
kind of depressing because I thought you know it's kind of born
you know 50 years too late or whatever you know all the exciting stuff happened
at the beginning of the 20th century it's kind of all been done
but now we have you know dark matter and dark energy and we realize that most of
the universe isn't sitting on the periodic table that I learned about in
schools and actually I needn't have worried you know
I think it was at Vannevar Bush who called it the endless frontier
the you know science is an endless frontier there is just there is
always more to explore and always more to learn so whether the particular ideas
you alluded to have substance I don't know at the end of the day the scientific
method will tell us if they have predictive capabilities they can predict
new phenomena that we weren't aware of before
then you know then they have they have credence as far as a scientist
is concerned but ultimately you know we still stick to the
scientific method it's about our ability to make predictions that are
testable experimentally and if they stand up to the test of
experiment then we give more weight to those to those hypotheses and
eventually they're elevated to the stages of theory
I often wonder about the horizon of our cognition
you know what we are capable of understanding and we tend to understand
things using high-level metaphors information is a great example of that
so a lot of people talk about the universe as
information this agential view is quite interesting so
modeling everything as agents and it might well be possible that the
universe is just so strange and alien that we could never possibly
understand it so there's a bit of an interplay between
our kind of intelligibility and and our models and what it is
the universe clearly is completely unintelligible in the sense of
nobody can really think about quantum physics
it completely defies our everyday intuitions that we learn at this sort
of macroscopic level so I think we have to accept already that the
universe is described mathematically that's our precise description
and then we have kind of metaphors about waves and particles and so on but
they none of them none of them really work properly they're just
crutches to lean on but ultimately it's a mathematical description
but that that is that is also very interesting the fact that the world is
described by mathematics that by making little marks on a piece of paper you
can discover a new planet that's quite incredible
shifting over to deep learning a little bit more more broadly
and we were touching on this already but the landscape is dominated by
transformers architectures what what are your broad thoughts about that
like any field I think machine learning has its sort of its fads and its waves
something works really well and then everybody latches onto that and makes
use of that and that that's all well and good
I'd be kind of surprised if the transformer is the last word in deep
learning if that's the the the the the architecture we use forever more
but it clearly works very well and we haven't reached the end of its
capabilities by any means so it makes a lot of sense to
exploit the transformer architecture in applications and see how much we can
gain from that at the same time there's clearly opportunities to think about
the limitations of transformers the computational costs can we do the same
thing you know with better scaling if you want longer context windows and all
the rest so there's plenty of interesting research I think to be done in
in new architectures as well so I think we need both
so you know here's another galaxy brain question why does deep learning work
you know because on on the face of it it shouldn't work it shouldn't train it
shouldn't generalize and they've been an absolutely
remarkable success why is that so I think first of all at one level you
could say well we understand why they work we're fitting
nonlinear functions we're kind of doing curve fitting in high-dimensional space
we need some some generalization and it comes out to no free lunch theorems of
inductive biases perhaps it's smoothness continuity perhaps it's something
more more constraining than that so at one level it's sort of not surprising I
can I can fit a polynomial to a bunch of data points and by gradient
methods and I can make good predictions for sort of intermediate points
just we're just generalizing that to more data and higher dimensions
so so one level I say no it's not at all surprising they work
at a different level of course the fact they work so well is remarkable
but the way in which they work is very interesting so one thing which
if we go back to the earlier years of machine learning and certainly back to
the world of statistics the idea that you would fit models that
have way more parameters than the number of data points would be
clearly insane to any self-respecting statistician we never would have and
perhaps that's nobody why nobody really tried it very much and yet we have
these odd phenomena whereby you know the training error goes to zero and yet the
test error continues to come down even though the training error is already
at zero something about stochastic gradient descent
the actual training process clearly is important there it's not just
here's a cost function we find the global minimum it's a property of the
global minimum no there are many many global minimum that all
have zero error some solutions will clearly overfit others generalize
well and so there's something about the training
process that we need to understand so I think there's a lot of research to be
to be done in why do they work so well I think it's an open question
we can describe the model we can say lots of things about the model we can
say because it has this and this and that number of layers
therefore the structure of the space has this and that properties and it divides
it up into such and such regions and so on
those are true I don't know whether that gives us real insights into why it's
working I think there are some some very much open questions there it strikes
me a little bit like neuroscience you know we have the human brain it does
these amazing things and we can get more and more and
richer and richer data about which neurons are firing and when and how the
firings are correlated we can learn something about the
the underlying machinery this is a bit like neuroscience except we can put a
probe in every neuron in the you know artificial brain and gather very very
rich information so again I think there's a very
interesting research frontier of getting better understanding of why are
they able to generalize so well and why do we have these
strange phenomena with these seemingly over parameterized models that don't
overfit but rather have very good generalization lots of research to be
done and just to linger on that that observation you made that you
can train a deep learning model and after the
training error has converged the test loss continues to improve I mean that
just seems it just doesn't make sense
I mean how and that there's grocking as well which is another
it's almost like we were saying with physics that outside of the
the machinations of the optimization algorithm
stuff is happening that we don't understand well you can tell stories
right you can say there's a there's a big space
each point of the space is setting for all the parameters of the model so the
sort of the weight space of the model and maybe you started off somewhere near
the origin with some little random initialization and you follow some
trajectory that's defined by stochastic gradient descent
and there are lots and lots of places in this space all of which have zero
training error so and they're connected so there's some
sort of manifold of zero training error and you're starting off at the origin
and stochastic gradient descent is somehow not taking you at a random way
maybe it's taking you to something like you know the nearest
point on this manifold or something and that maybe that's some kind of
regularization and maybe that place has
certain smoothness properties that lead to good generalization
so you can kind of tell these stories I think the challenge is to take the
stories and make them predictive so I think when we have a theory of
what's going on we'll know we have a theory because it can predict new
things not just tell stories about what we've already discovered empirically
but really become predictive I think that's still a very much an open question
so what do you think about the intelligibility of neural networks in
terms of things like bias and fairness and safety
because you could just think of these things as inscrutable
bags of neurons and but we need to have some guardrails don't we
well we absolutely need to create technology that's beneficial to humanity
there's no question about that and there are mechanisms for doing that to
align the systems whether it's through you know human feedback
whether it's external guardrails that are providing more conventional sort of
checks on how things are being used that's
clearly necessary and I find it very encouraging that so much
energy and effort is going into this and yes there'll be bumps in the
road and missteps on the way for sure but overall we seem to be heading in a
very good direction but I think the fact that there is a lot of attention
being paid to the potential risks associated with this
very powerful and very general new technology gives me hope that we will
avoid most of the the biggest risks. Can you give me a specific example of an
emulator? Yes I can so one very nice example actually
it was the final project I worked on when I was working in the fusion program so I
was using fusion as a sort of springboard to get into machine learning
and we wanted to do real-time feedback control of a fusion experiment
I think called a tokamak very high-temperature plasma we wanted to
use neural nets to do non-linear feedback so the challenge there was
to take a plasma it's like a donut shaped ring of hot plasma
and it was known that if you could change the cross-sectional shape you could
improve its performance so there's an experiment called a compass compact
assembly at Cullum in in Oxfordshire and the
experiment's designed to produce very interesting exotic cross-sectional
shapes to to explore the performance so we wanted to use a neural net to do that
feedback control. Now the good news is we had a great
piece of inductive bias I think called the grad shafranoff equation it's a
second-order elliptic partial differential equation but the point is it
describes the boundary of the plasma very accurately right so you make a bunch
of measurements from hundreds of little pickup coils around the plasma
and those are boundary conditions you solve the grad shafranoff equation you
know the shape of the plasma and the goal was to
decide ahead of the time that you wanted to create a circular plasma
and then change its shape and and and then make corrections if the shape
wasn't quite the one you wanted you would change the the big control coil
currents and an alternate shape. The problem was the grad shafranoff equation
on a state-of-the-art workstation of the day would take two or three minutes to
solve whereas we had to do feedback on a sort of 20
kilohertz frequency or something it was about something like six orders of
magnitude too slow so what we did instead was we we solved the
grad shafranoff equation many times on the on the workstation
over a period of you know days and weeks until we built up a large database
of known solutions along with their magnetic measurements
and then we trained a neural network just a simple two-layer neural network
back in the day with probably only a few thousand parameters i mean
miniscule by modern standards but it was trained to take the magnetic
measurements and predict the shape and we could put that into a standard
feedback loop and and we're in a bit of a race with
another organization that was doing a similar thing a different fusion lab
that was working on the same project and so that was very motivating and i'm
pleased to say we got there first and we did the world's first ever real-time
feedback control of a tokamak plasma using a neural network
but as a beautiful example of a of an emulator we could get five or six orders
of magnitude speed up not by solving the equation directly
to do feedback control but by using the numerical solver to generate training
data and using the training data to train the emulator
and then the emulator and even then it was still
quite demanding for the silicon of the day there was no processor fast enough so
we actually built a physical implementation of the
neural net believe it or not so it was a hybrid
analog digital system had an analog signal pathway with analog
sigmoidal units but the weights were set using digitally set
resistors so we could take the numerical output of the the emulator
downloaded into this bespoke hardware physical neural network
and do real-time feedback control so i was pretty pretty excited about that
project that's fascinating what do you think about
control now do you have any opinions on you know model predictive control and
control is a super important area different both the both the control
problem and the overall planning problem i think
despite all the remarkable advances in gbt4 the world of instantiated
ai and robotics and so on is still a very very wide open frontier
we don't we don't really have robots that can even yet drive a car through
central london that's still a a major challenge that
we're seeing some very remarkable progress recently
yeah i mean more broadly i've been speaking with some neuroscientists and
they say that we have the matrix in in our heads so we're always running
simulations and presumably in the future this will be a
principled way of building agents so the agents will run
counterfactual simulations and select trajectories which look like good ones
and then the process will will iterate i think this is this is very powerful i
mean the the idea of sort of type one and type two fast learning slow learning
the idea that we simulate the world and we compare the simulation with the
reality and we can learn from our own simulators and so on
we don't we don't quite know what best to do with that but it feels such a
powerful and compelling concept and we we think something like that is going on
in the brain that again that feels like a
an area that's ripe for exploration and i think in some form
some kind of you know model prediction and simulation of the world feels like
it will be increasingly a part of ai systems as we go forward
i mean for me the takeaway in all of this is just what an amazing time to be in
this field there are so many fascinating things to work on
professor bishop it's been an honor to have you on mlst thank you so much
well thank you i've enjoyed it thank you amazing
