start	end	text
960	5360	Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.
6240	12880	Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book
12880	19600	Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb
19600	26000	and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis
26000	33760	of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges
33760	39360	the argument made by Langreb and Smith that anything we engineer is ultimately a system
39360	45120	which can be mathematically modelled and described. He then goes on to discuss the complexity of
45120	53040	modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,
53040	59920	and constitutes systems whose behaviour affects and is affected by the environment they function in.
61920	67920	He also touches on the notion of granularity, arguing that complex systems are all the way up
68480	76400	from specific components of the mind to the mind itself and that no known mathematics can model them.
76880	82560	Dr. Saber then delves into the complexities of language and open interactive dialogues,
82560	88160	asserting that language is a prerequisite for any artificial general intelligence,
88160	94720	but that linguistic communication itself is a complex system that no mathematics can model.
95520	100800	He doesn't subscribe to the argument that interactive bots can be built in narrow domains,
101520	106880	since responses and the overall context cannot be predicted in any meaningful way.
107920	114160	Dr. Saber has two reservations as to the conclusions made by Langreb and Smith.
114160	120240	He questions their use of the word never and suggests there could be a new mathematics
120240	126480	that mental processes require that is yet to be discovered. He also doesn't believe that the fact
126560	132560	that complex behaviour cannot be mathematically modelled precludes the possibility of building
132560	139600	such systems, as evidenced by the intentional programming language LISP, and he also considers
139600	145120	the possibility of hypercomputation in validating the church-touring hypothesis.
145120	149840	We'll talk about that a bit later. Finally, Dr. Saber expresses his regret
149840	154800	that the book didn't go into further detail on the frame problem in AI.
156480	160800	He calls for further research into belief revision in complex systems.
162240	166880	Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just
166880	172480	been speaking about. Machines will never rule the world. And by the way, I think very highly
172480	178960	of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible
178960	184160	breadth of knowledge across so many fields, you know, from AI and computer science,
184160	191120	to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings
191120	198240	a very interesting contrarian view, I would say, to the current kind of modus operandi,
198240	202240	or the zeitgeist in the community at the moment. He's a breath of fresh air.
202960	208720	Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed
208720	213520	rating our podcast on your favourite podcasting platform if you happen to be listening to us.
214160	217920	Anyway, without any further delay, I give you Dr. Wallid Saber.
224400	233520	Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber,
233520	238160	but we also have Mark from our Discord community. Mark, would you like to introduce yourself?
239040	246000	Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at
246000	252960	MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur,
252960	258400	but I think this is right, Tim. I think Wallid was the first, or one of the first,
258400	265200	really big names that we had come on the show, right? I don't think so,
265200	271120	but the most controversial, let's put it this way, the one that made probably,
272400	280480	it was so predictable, what was, I mean, probably the first one that broke that
283600	289200	predictive model. Well, I just remember, I mean, I remember Tim and I being like super,
289200	294000	because we didn't know you, right, before then, and I remember us being like so excited that you
294000	298400	agreed to come on the show. You know, of course, now that we know you, we're kind of like,
298400	304160	ah, whatever, it's just Wallid. We're just through, by the way.
304160	308000	We were following all over ourselves that you were coming on the show, we're like, oh my god,
308000	313760	this is so awesome. To be honest with you, I never thought I would, I would,
315280	319680	yeah, I never thought I would have, my opinion would matter that much, to be honest with you,
319680	326480	and it all started by creating this medium blog, and I started spitting out stuff that,
327600	334080	hey, do you guys know that there's dissonance in that? And I was surprised how much it,
336400	345840	even by people that are living off and making a living, and they preach and write papers on
346560	351600	what I'm attacking, and they would say, don't mention my name, by the way, it's all private
351600	360800	messages. But apparently, I said a couple of things that touched people, but you know.
362560	367840	Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said,
367840	372720	I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides
372720	379360	a completely different perspective, because we're bred on empiricism and neural networks. And part
379360	384080	of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been
384080	388960	speaking to a lot of deep learning people recently, so we need to counteract that a little bit.
389520	398320	Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean,
398320	409040	if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean,
409040	420240	so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now.
420240	428480	Oh, go on. Yeah, I mean, look, I breaking news, breaking news.
430480	438160	Positive, although I still have my reservations as to AGI and all that stuff. But I have been
440160	447120	completely impressed with the developments in large language models. I have to admit that
448000	453920	sometimes I say, what the hell is this? Now, technically, let me tell you what's happening.
456560	459840	And I'm working on something that probably would quantify
461440	468080	where can this go? How much can you, how much will scale? The bottom line is this,
468720	474480	these guys have impressed the hell out of me, and they have proven that scale does matter.
474480	481440	I mean, now these large language models, if you take language from lexical to
482480	489200	syntactic to semantic to pragmatic levels, they have definitely mastered syntax.
490640	499200	And this is not a small feat. I mean, this is huge. They have proven that if I read
500160	506400	tons of texts written by humans, I can figure out the grammar of language,
506960	515840	and they have done that. That's huge. Okay, so and I don't like here where I don't like people.
515840	520240	I don't want to mention names, but people that supposedly are in my camp, right,
521200	529840	insisting on refusing to see the elephant in the room. No, large language models have proven
530400	538720	that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.
539360	547840	Now, okay, and here's where technically, and you have to admit, I mean, they,
549200	554560	as a matter of fact, they probably know syntax now more than many college graduates.
556480	563040	Okay, these are from data. That's a huge experiment in cognitive science that
563360	568640	no matter how, how, I mean, you can't be religious in this, you have to be scientific.
569600	576960	I see the proof that these large language models by ingesting tons of text written by humans,
576960	583840	they have figured out the syntax of language. End of story. I have, there's an existential proof.
584400	588640	Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try.
589200	597440	Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay.
598720	604240	Now, that's not the end of language understanding. There's semantics, and then there's pragmatics.
604240	611520	Wow. I mean, so I'm working on something to quantify. So now we have, I don't know,
611520	618480	we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics.
618480	623600	And semantic can be broken down to, have you guys figured out reference resolution? Have you
623600	629200	guys figured out scope resolution, prepositional phrase attachments? There are so many
631520	639760	pain points and semantic processing. Can we quantify how many more parameters we need to
639840	644160	conquer semantics? And then pragmatics, there are things like,
649840	656640	the teenager shot a policeman and he immediately fled away. Now, possibly both can,
657360	663440	the he can be the policeman. He fled away to escape further injuries. I mean, that can happen.
664240	672880	But most likely the one that's led away is the teenager. That's pragmatics because we know in
672880	680240	the world we live in, if you shoot someone, they're going to try to capture you and you try to flee,
680240	686880	right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need
686880	699360	to capture that? If you can put a, if you can come up with a rough number, I mean, it could be
699360	708400	a number that's manageable, that's doable by more scaling, which would be an interesting result.
708400	715920	But it could be that it's a number beyond the universe we live in, which means guys,
717040	721920	except that you can master syntax and a little bit of lexical semantics, you can figure out the
721920	729040	meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that
730000	737040	we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on
737040	744640	which is going to be very difficult to quantify because they have proven that scale did improve
744720	754160	syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean,
754160	760880	can you quantify how far can this go scientifically without saying, let's try with more, let's try
760880	768000	with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I
768000	773040	push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of,
773040	777200	okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of
777920	782800	proof that it does seem to have very syntactic sentences. And obviously, if you're in something
782800	788160	with whatever billion of parameters, but would you say what those rules are? Could we write them
788160	793280	down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you
793280	797680	don't have the old school kind of generative grammar approach. And also, it's not the way humans
797680	801440	have learned language. And could you reverse engineer or tweak it? We don't know what it is,
801440	809680	it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way
809680	816240	humans do learn language? I mean, I think it's more, though, it's more related to the way humans
816240	824960	learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without,
824960	830640	without knowing grammatical rules. So there is an argument that humans don't learn grammar,
830640	835040	that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add
835040	840400	vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative
840400	848160	grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years.
848160	853120	I don't think that's how we work. And we have, I mean, there's amazing competency of children
853120	857200	at two years of age, you know, with language they have, you've said, you've, you're writing,
857200	862080	you pointed this out, actually, children know, is absolutely amazing, you know, straight out of,
862080	868880	you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about
869680	876720	and we have innate stuff. But here's the change that these guys have made me
879440	885200	go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing.
885200	893200	I was told before these new results are coming out that look, we do have innate stuff, which
894160	900720	took us three, four hundred thousand years of evolution. All we're doing by ingesting all
900720	911360	this text is we're simulating, right, these 300,000 years. So give us a chance to simulate
911360	921040	this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago,
921040	928560	and I thought, come on, you're chasing infinity. What happened with the real difference in my
928560	936480	mind now is they have proven that they conquered one beast in language. Nobody can dispute that.
937120	944240	Can I have a go at disputing it? So, in the Polition and Foda connectionism critique,
944240	948720	they spoke about productivity, you know, the infinite cardinality of language. There was
948720	954160	recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well,
954160	958880	I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are
958880	966080	at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why
966080	973360	with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use,
974560	983680	languages infinite, but probably the long tail of probably 90% of ordinary language use,
984400	992560	right, can be figured out from the stuff that we write. So they will never capture all of language.
993520	1002800	Yes, but they might reach the level of a competent educated man like us in language competency.
1004080	1010240	So, all I'm saying is what they have achieved is a huge
1011200	1024320	result in terms of the big question of scale and big data. They have definitely proved that
1024960	1029520	if I see enough data, I will learn something and something that's not trivial.
1031200	1035040	Look, you know where I'm coming from. You're talking Foda and Polition. I mean,
1035040	1040640	you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,
1040640	1046800	I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.
1048240	1055120	I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody
1055120	1058720	bashed deep learning more than me, especially large language models. I mean, I'm like,
1059360	1069280	I was saying this is silly, right? But I have to say they have proven something to me at least,
1070400	1076240	which is huge because I know how difficult language is. I am impressed equally.
1077520	1081200	Wouldn't you say it's an engineering, an engineering triumph rather than a scientific?
1082160	1087040	It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.
1087840	1092800	That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to
1092800	1099520	the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know,
1099520	1105840	I know theoretically, theoretically, mathematically, you cannot understand language this way.
1105840	1113200	All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak
1113760	1124160	is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is,
1124160	1132480	is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn
1132480	1140720	something that is not trivial. That to me has been proven. The point I'm making is not the
1140800	1144240	point I'm making is not that they solve the language problem. Sorry.
1144240	1148720	Yeah, I want to jump in here a little bit. Because from my perspective, I think
1149600	1156240	part of why you're saying it's huge is because I think it was a huge step for you personally.
1156240	1160160	Because I know, you know, from the past, like talking to you, like you've had a much more
1160160	1166720	extreme view, you know, on the capabilities of large language models than, for example, myself.
1166720	1172240	Because for me, I don't see anything new here. It's kind of like, I'll give you an example
1172240	1178560	outside of syntax just for a moment. So just transcription, because that's what Tim and I
1178560	1184400	happen to be working on quite a bit right now. In other words, transcribing audio into text.
1186480	1191520	All the state-of-the-art models are pretty much sitting around each other at about 90%
1192160	1198560	you know, accuracy right of transcription. But here's the thing is that's for people speaking
1199520	1206480	relatively common languages with a relatively standard accent. Okay, as soon as you bring
1206480	1213520	someone in the room that has an accent or speaks a, you know, with maybe like some type of a
1213520	1218640	challenge, like a speech challenge, or this side of the other thing, it becomes garbage again.
1218720	1226000	And like for Tim and I, or there's noise in the background, music playing in the
1226000	1230800	background. And as Tim and I have probably hammered, you know, to death and beaten a
1230800	1236320	dead horse on our channel like so many times, we've never doubted that machine learning can learn
1236320	1242480	like the bulk of the curve, where it really, really struggles is in all these edge cases,
1242560	1249760	and the corner cases, and the periphery where it can easily, it's very brittle, right, in those
1249760	1253840	kind of areas. Like this is the point we've been making out for a long, long time. And so the fact
1253840	1260480	that like massive trillions of parameters and terabytes of data was able to learn 90% or more,
1260480	1267680	95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know,
1267680	1272320	maybe a big triumph or something. But I'm still always about that other like 5%.
1273200	1277440	And the problem with the approach of deep neural networks is to get that other 5%
1278480	1285200	is like 100 times as many more parameters, whereas like using, whereas using more generalized,
1285200	1289280	abstracted, you know, methods that we haven't yet really discovered.
1289280	1293280	You're hitting it on the nail. And that's why I'm working out on quantifying this because
1294160	1302320	now we are doing exponential growth in the number of parameters for not even linear growth in the
1302320	1313520	accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data
1313520	1318160	that we don't even have. That's what I'm working on. How far can this go because the function
1319040	1325360	is against them now? Like, I mean, we're increasing GPU power and the number of data
1325360	1332960	that we're ingesting exponentially for a minute increase in accuracy, which is,
1332960	1337600	right, that's the end of the logarithmic. And this is, this is part of the announcers that we
1337600	1346320	have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood.
1346320	1353360	All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially.
1354080	1360400	Okay, so let me, let me really be very careful in what I'm saying because now I have a following.
1360400	1368960	I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean,
1368960	1373680	science is science. And I know theoretically, I don't get into things like intentionality and
1374400	1378640	these models understand nothing about the word. I'm talking about syntax only, by the way,
1379440	1386160	syntax on and some coherence when they patch things together. The coherence is amazing.
1386960	1394080	They're not patching things together that don't relate at all. So I'm talking about syntax and,
1394080	1399040	and coherence and syntax. Okay. And a touch of semantics, right?
1399440	1404240	My point, let me repeat it so that I'm not misunderstood.
1406240	1414640	They have proven something that many cognitive scientists would never accept, never ever.
1416800	1425360	But this existential proof has told many cognitive scientists, don't dismiss learning
1425360	1430720	from data only, blind, no labeling, some aspects of language,
1432720	1437760	actually very impressive aspects of language. These guys have proven that.
1438960	1446400	And me as a cognitive scientist, I have to admit because I see it. I see from data alone,
1447040	1450720	these systems have learned non-trivial aspects of language.
1451600	1457920	Now, how do you interpret that? Where do you take it? What do you conclude from it?
1457920	1465600	We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would
1465600	1475200	see, that just ingesting text in these deep networks, you can actually figure something
1476160	1484240	not trivial about language. That has been done. I mean, you can, you can say there are pigs,
1484240	1491360	pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can.
1492080	1501760	But existential proofs are the most powerful proofs. It's an existential proof, proof by doing.
1501840	1506400	I'm showing you language competency by ingesting text on it.
1509120	1511280	So this dismissive
1517360	1527920	all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No.
1527920	1529440	Oh, Bender. Emily Bender.
1529440	1535600	Emily Bender. No, these are not stochastic parents anymore for me. I am seeing,
1536160	1542720	look, if I go through the tests on conducting, I have 20 pages of tests on every aspect.
1545280	1548720	And they get better. I mean, I am seeing things that
1550880	1553760	lexical ambiguity, they've almost resolved it like,
1554640	1564160	we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball.
1565520	1572240	I'm seeing things like, what the hell is this? And if anybody can test these systems,
1572240	1578320	I can with all humility. I'm trying my best now to make them fail, which was not the case just
1578400	1584480	a month ago. All I'm saying is I'm seeing something that I never thought I would see
1585840	1588560	as a cognitive scientist, as a computation linguist.
1590720	1597440	Let me put it this way. To see this capability now, you have to bring back Montague,
1598240	1607840	Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together
1608480	1613680	and give them a thousand bright engineers. And they will not do this.
1614880	1619520	In a minute, we're going to get onto your book review, but you are just alluding to the problem
1619520	1625040	of semantics and pragmatics. And also I want to bring in symbol grounding as being the next
1625040	1628640	potential brick walls. Could you just talk to that a little bit more?
1629360	1638400	Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.
1638400	1648480	So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to
1648480	1656880	a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based
1656880	1663440	systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers,
1663440	1670480	blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay,
1670480	1677600	you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a
1677600	1684240	dictionary to read the definition of a word. I have to know all the words. So I might go and
1684240	1690560	so it's a cyclical representational system. It's not grounded in anything in the end.
1690560	1695520	It's a closed. Basically, it's a system that defines itself, like what the hell's going on here,
1695520	1702400	right? Symbol grounding was CAT has to be associated with something real outside.
1702400	1705920	That's a real CAT. In symbolic systems, we don't have that, right?
1708720	1715120	We can get into symbol grounding. It's a huge subject on its own, like where do meanings,
1716320	1724160	where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be
1725040	1734480	can a deaf and a blind person ever understand the meaning of something? So that's a huge...
1737520	1742640	I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,
1742640	1748240	this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste
1748240	1752640	for personally, but you're very skeptical about that. Could you just sketch that out?
1752800	1758080	I don't think that's the issue grounding. I mean, people make a lot of it and like our
1758080	1764880	common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you
1764880	1770960	don't live in the environment and it's... That has never been... I don't believe so. That's why we
1770960	1775600	call it artificial intelligence, right? I mean, we're never going to have the intelligence of a
1775600	1782000	human being. We're never going to have a robot that really chokes when they see their nephew
1782000	1787040	after six years, right? I mean... And that's... That was never the one. That's why we're building
1787040	1793520	artificial intelligence, not human intelligence. So this whole argument about grounding and
1793520	1799440	embodiment and I will never understand what pain is because a robot will never really feel pain.
1799440	1804320	That to me, that's besides the point. I'm not building artificial life. I'm building an
1804320	1810160	artificially intelligent machine that will do things in a way that you would say, what the hell
1810160	1817200	was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it.
1817840	1824080	It feels pain or it doesn't feel pain or it will never know what crying is, like so.
1826320	1835760	So at least I come from this angle. I'm not into building artificial humans. I'm an engineer.
1835840	1842960	I'm into building artificial intelligence systems. Systems that can reason, right? In the environment
1842960	1849040	we live in, solve problems intelligently and problems that usually require human intelligence.
1849040	1854720	Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have
1854720	1862560	doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell
1862560	1867920	me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write
1867920	1874400	poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never
1874400	1883680	build robots that will understand love. So to me, these are arguments that
1885520	1890160	they're irrelevant. We're building artificial intelligence. When we did calculators, we never
1890160	1897040	gave a damn how we do it in the mind. And we have calculators that can beat any mathematician
1897040	1905440	in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more
1905440	1911520	where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind
1911520	1918640	right now is our conversation with Professor Chomsky where he said, yeah, these are great
1918640	1923360	feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with
1923360	1930720	science. Sure, or philosophy for that matter. I mean, I think some of these questions,
1931520	1936080	yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.
1937200	1944080	But they have a lot to do with philosophy or science or whatever. Mathematics for that matter.
1944080	1952640	This is a bridge to the book because the authors were misunderstood from their title,
1952640	1958160	and I told them that privately. No, not privately. I want to tell them that because
1959360	1965120	I got comments from people privately that the title is misleading. The title assumes they are
1965120	1970880	anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an
1970880	1977840	artificial human. We can never do that problem. Right. So all this, and this is important because
1977840	1986480	people are trivializing. I mean, you have people talking about AGI from five years ago. And all
1986480	1992880	we had was something that can do amazing pattern recognition. That's it. So it's important for
1992880	2000480	us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that
2001520	2009200	surpass human intelligence? This is not just a word you throw out, because you're impressed with
2010240	2016960	a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book
2016960	2023440	is about that. It's like, do you know what it means to have a system that can feel and
2026160	2032720	react instantly real time to changing situations around them? And do you know what you're talking
2032720	2040320	about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed
2040880	2053040	by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say,
2053040	2059680	wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at
2061280	2066000	will it be, will it be the solution for the language understanding problem? No,
2066960	2072000	because language understanding in the full sense of the word understanding,
2073040	2078080	the way we speak now, the way we were speaking now involves a lot more than mastering syntax,
2078080	2089360	but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to
2089360	2097360	make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.
2099600	2110560	And it's very impressive. So the question now becomes for AI researchers, not just engineers, is
2110880	2122000	this scalability scalable? Is this scalable? Is this approach scalable?
2125120	2132880	So much data and so much compute power, they mastered syntax more or less. I think they did
2132880	2140320	at least as much as a competent language user. So my first question is like,
2140320	2145120	do you see natural language as essentially computable as in like cheering machine computer?
2145120	2148720	Or do we need some other kind of new mathematics to describe it? For example,
2148720	2153040	hypercoputation, whatever that might be. In other words, is there a generative grammar or
2153040	2156240	algorithm or set of rules that generate our valid sentences?
2156240	2163600	When it comes to language itself, I think language is a formal language.
2164720	2173520	There is a compiler for natural language that can be built, like we have built one for Java,
2173520	2181680	C sharp. So this is Montague, yeah. Yeah, I believe Montague was right,
2181680	2188800	although Montague was was attacking the semantics part. Okay, he touched a little bit on intention
2188800	2196960	and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very
2196960	2202960	smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never
2202960	2208800	in the business of reference resolution. That's pragmatics. Montague was trying to prove
2209760	2215920	there is a formal system and algebraic system. And he used lambda calculus, strongly typed system
2216480	2223520	that I can use to compose language like I do with arithmetic or calculus or anything. There's
2223520	2229920	a logic that are mathematics for language, which is a huge thing. Montague was not a trivial
2231520	2234160	semanticist in the history of language. He was huge.
2234880	2239680	So would you say that Montague is doing for language semantics of language,
2239680	2245760	what chance he did for syntax of language? Exactly, exactly. And the common denominator
2245760	2250720	interesting between them is someone that Chomsky himself admires a lot Barbara Partee,
2251440	2257360	who was he did her PhD with Montague. She's a Montegoian, Montague semantics.
2258160	2264240	But she and she, she did say almost the same phrase. He said what Montague did for semantics
2264240	2269760	is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase.
2271520	2276720	You think he was right about semantics? Yes. So for example, there's a computable definition
2276720	2282480	of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that.
2282480	2288640	No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or
2289440	2296560	he said whatever your meaning for something is. Okay. He didn't even care. Montague never
2296560	2302080	did ontology and conceptual and like, what, how do you define the meaning of what is a book?
2303680	2309040	Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was
2309040	2314400	doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's
2314400	2320880	what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the
2320880	2330000	individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and
2330720	2334560	cognitive scientist and ontologist and disagree about the meaning of a cat.
2335520	2338880	Finally, you come to me and you say, we have a meaning for cat and it's see.
2340720	2346080	Follow me. Montague never cared about what is the nature of things outside. Right.
2346880	2354320	Whatever your meaning for these individual concepts are. Right. Here's how you make,
2354320	2361840	you get the meaning of a whole mathematically. I'll give you a simple example that will make
2361920	2371360	you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door
2372560	2378720	refers to a person. The neighbor next door that just moved from California refers to a person.
2380320	2388000	The neighbor next door that knows John very well and drives for the LTD is a person.
2388800	2396080	All of how can you have this phrase and John refer to a person and have the same semantic type
2396640	2403360	composition in a way that never fails. Like you do in arithmetic, he wanted to prove that
2403360	2410640	natural language is a formal language. He developed a semantic algebra that makes this long phrase
2410640	2416000	referred to the in the end to an object that has the same semantic type as John mathematically.
2416000	2426320	If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big
2426320	2432880	claim natural language is a formal language. Give me some time. I'll work out the full algebra.
2434080	2439040	You go then and decide what the individual meanings are. I don't care. Montague never gave a damn
2439040	2445040	about cognitive science and knowledge and he was a logician. He wanted to prove
2445680	2451440	there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning.
2452000	2456160	I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this
2456160	2462720	and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the
2462720	2477440	meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic
2477440	2483440	system behind language, like any other formal language. Like you can get an arithmetic expression
2483440	2488880	and build a tree for that, evaluate it, and get the final meaning. Natural language works the same
2488880	2496720	way. Except it's not that simple. That's all. So his project was huge and he was misunderstood.
2498480	2504320	So he was really doing semantics. That's semantics. Pragmatics is a different thing.
2505440	2511840	What do you think is the core unique property of natural human language? Would you agree with
2511840	2521440	Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity.
2524000	2533360	To me, no, it's I'm half Chomsky and half something else. To me, no, the real,
2533360	2540000	real unique thing about language. And that's why even if Montague succeeded, that's half the battle.
2540960	2549280	It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive
2549280	2555920	inference. I mean, we use induction and we use deduction and we always ignore abduction.
2555920	2564560	Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive
2564640	2570880	reasoning. They, to a certain extent, that's how they learn a few things, inductively, really.
2572720	2578640	All the lower species do inductive reasoning to a certain extent. And some of them do some
2578640	2584400	deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive
2584400	2592080	reasoning is uniquely human. And that's the part of language understanding, which means
2592080	2599120	reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively
2600000	2608080	by induction or, and not deductively, I deduced it. But I reached this conclusion because it's
2608080	2617520	possible, it can happen. And it is the best conclusion I can come up with, given everything
2617520	2624560	else I know. Abductive reasoning is the real reasoning methodology that makes us unique as
2624560	2631600	human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So
2632240	2640400	that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or
2640400	2648240	abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's
2648240	2654560	abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning
2654560	2662800	as it used to be called in the 80s, when case-based reasoning came out and expert systems who,
2663760	2670560	there was something called EBL, explanation-based learning. And it was even a learning technique,
2671120	2679360	which is really reasoning to the best explanation. Basically, I have to make a decision. Actually,
2679360	2685680	Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in
2685680	2694720	semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation
2694720	2700960	as abduction or understanding as abduction. And basically, he shows how all the difficult,
2702240	2706720	all the challenges in language understanding beyond semantics. So we're done with Montague.
2706720	2713120	Now I'm doing the final understanding of what makes sense given, because every expression has
2713120	2719600	several meanings. Even if I did the semantics perfectly, I have to choose the most plausible
2719600	2725280	meaning from all the possible meanings. That's pragmatics. And the way you do that very well
2725280	2733040	is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible
2733040	2740960	meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions.
2740960	2746160	And I'm left with three still, three possible meanings. They can all happen in the world we
2746160	2753280	live in. Which one is the most plausible? We do this abductively. Which meaning is the most
2753280	2759200	likely meaning given the context and what I know? That's the last challenge in language.
2759920	2765520	So we need to, we need to add the abductive model, which we humans do. I go back to the
2765520	2771280	teenager shot of policemen, both meanings, both interpretation can happen, right?
2772640	2780320	Either one can flee, right? But most likely it's the teenager that fled away, given what I know
2780320	2784800	and given that's abductive reasoning. But semantically both can happen.
2785760	2791280	Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very
2791360	2798000	important to mention is that there's two senses of abduction. And they differ in the
2798000	2802320	following way, which is kind of the more modern sense, which is what Wally's been talking about
2802320	2809920	like pretty much this whole time, is abduction used to justify hypotheses. But the older and
2809920	2816080	original sense of it and still an equally important one is abduction for generating
2816800	2823440	hypotheses. And this ability to generate hypotheses is something that's extremely
2823440	2829760	powerful and so far uniquely human. But generate from... Hold on, let me just finish here.
2829760	2835120	Which is this, this is like something where Einstein is just sitting there pontificating on
2836720	2841760	how the heck can light be the same no matter how the earth is moving and blah, blah, blah,
2841760	2849840	and comes up without a thin air, like this hypothesis that relativity applies, right?
2849840	2856240	That the physical laws are the same no matter what your reference frame is. So this ability
2856240	2860960	to almost... That people talk about sort of pull from thin air, this kind of intuitive
2861600	2870240	leap to something that ends up being like a grand new theory, that's also abduction.
2870240	2874080	Right. But in both cases, Keith, and I agree with you, that's
2876400	2883760	the old view of what abductive reasoning was to scientists. But in both cases, you're choosing
2883760	2890960	from possible... Oh, no, no, no, just a minute because this is where I think I probably quite
2890960	2896160	disagree with you, which is the modern sense of abduction to me is much more similar to just
2896160	2901280	inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can
2901280	2907600	tell you which hypotheses should be preferred just on the basis of marginalization and strict,
2907600	2912560	like Bayesian theory, no problem with that. It's not actually abduction, it's just inference,
2912560	2919920	right? Just rules of inference. Whereas just a minute, generating that space
2919920	2925760	in the first place is unique and very different from inference, like the ability to produce
2926880	2932400	from nothing models to consider, that's the core of abduction from my point of view.
2934160	2939680	But okay, so we're saying the same thing, but indifferent. These possibilities that you generate
2940480	2945280	are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the
2945280	2952880	inference. No, you're generating a pool of possibilities. That's the step, generating a pool
2952880	2958000	of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're
2958000	2964160	saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible
2965040	2974960	valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive
2974960	2981760	reasoning, you are, whether it's the old way or the modern way, in the end, what's common between
2981760	2990160	them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most
2990160	2995360	plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this
2995360	3000880	is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have
3000880	3005280	a bunch of possibilities, and I'm saying those possibilities have to come from somewhere,
3005280	3010560	and where they come from is abduction. Oh, okay, it depends on the domain and language. They come
3010560	3016400	from what we know is true. Okay, I see your point. Where they come from depends on the domain of
3016480	3023440	reasoning. In many cases, they come from what we know is true, or they come from evidence.
3024560	3028640	Yeah, I guess it's just important to know there are these two senses of abduction,
3029440	3032160	and don't forget about both of them, because they're both...
3032960	3040320	Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction
3040320	3045280	are both approximate. You can never have 100%, because in the end, you're assigning a score,
3045280	3052000	you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty.
3052960	3058560	So, when you're doing abductive reasoning, even in language, I make a decision as this is the right
3058560	3063920	interpretation given the context, but it's what we call... Could be wrong. You might have eaten
3063920	3068400	a ball out of this all year. Exactly, and that's why when I read further, I change my first
3068400	3077600	interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the
3077600	3087280	sense that I might override my first decision. But all of that is pragmatics, and we do this
3087280	3092880	in conversation. Two, three sentences after, I understand really fully what you said before,
3092880	3098720	because I remade the interpretation. And Waleed, do you have any thoughts on where
3099760	3104480	this came from, or basically the evolution of language, or if you like the evolution of this
3104480	3111840	abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is.
3113840	3119920	Unique to humans, definitely. I mean, animal language, animal symbolic languages have been
3119920	3126160	studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean,
3126720	3131520	language have a finite set of symbols, and they're not productive. They don't do compositions.
3133920	3137360	And this ties to... Is it animals? Your time up?
3139840	3147680	No animal, no non-human animal has a productive language. In other words, I have a set of symbols,
3148320	3154720	and if I can compose them, I can make a new symbol. Language, animals don't compose things,
3154720	3159040	because they don't decompose them when they're done. They have a finite... It's a hash table.
3159040	3163760	If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is,
3164560	3168880	because they don't have recursion, they don't have infinity, they can't deal at that level
3168880	3174960	with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same
3174960	3182720	paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John,
3182720	3189840	or the neighbor next door, or the neighbor next door that just came from California. I can
3189840	3197120	productively make a person out of three sentences, and in the end, they collapse to a John, right?
3197120	3203600	That productivity doesn't exist in any species except humans, which means compositionality,
3203600	3210640	which means systematicity, which means all of that. So, it's unique to human, definitely. This
3210640	3216720	has been established, and it came with thought. That's the if and only if. That's why we're the
3216720	3221760	only species that really reason. I mean, okay, I have people insist that animals think and they
3221760	3228480	reason. They're not really reasoning, okay? Only humans reason, and thought and language came
3228560	3234960	together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists,
3234960	3246480	and they say it looks like language was detected when tools and some basic machinery was detected
3246480	3255200	first. So, the human mind at some point had this capacity to think and language came with it. It
3255200	3261920	was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from?
3263440	3277440	Wow. I think it was the need really to express thoughts. Like at some point, we started having
3277440	3284320	thoughts that we want to communicate. So, the external artifact we see outside, whether it's
3284400	3293840	English or ancient Greek or Latin, languages evolve for societal reason and all that. But the
3293840	3301840	external artifact that we use to communicate thoughts came out of the need of the internal
3301840	3308640	language that started to develop. What Fodor calls it, the language of thought, mental ease.
3309600	3317920	And we, so we had that thing going on inside and then we had to communicate. We started with
3317920	3324480	weird sounds and then we scribbled things on the wall to communicate. And then that thing developed
3324480	3332080	until we started making symbols like, okay, if I say this, that means this. I don't know the
3332080	3340160	exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here.
3340160	3346880	So, there's a language of thought. And these external things are because linguistic research
3346880	3352640	has also shown that there are many universals in language, regardless of what the language is,
3352640	3358960	even if they are completely different systems like Asian languages and Latin-based languages.
3359520	3367040	They all have a verb, an action. They all have objects and agents of the action.
3367840	3374800	They all have events and events have duration, time and place. So, there are a set of cognitive,
3374800	3381760	I call them universal cognitive primitives, right? There's always an object there somewhere,
3381760	3386240	or an agent of an activity. Now, how you express it in different languages,
3386720	3393040	these are universals. That's the language of thought. That's the internal language,
3393040	3400240	which has to be the same. And objects have properties and all that. So, there are universal
3400240	3409120	primitives. And we instantiate them in different languages differently, but that's to me secondary.
3409840	3414640	Okay. Okay. That's great. William, could you talk a little bit about your recent overview?
3416560	3425840	A colleague that I never worked with, but a colleague in the field. To review this book,
3425840	3431040	and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.
3433280	3438880	But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,
3438880	3445920	it turned out to be not as technically involved as I thought. It's sort of,
3447200	3452400	and I'm saying that not to be negative, but it's sort of the same argument over and over.
3452400	3458960	The gist of the argument is quite simple, actually. And they try to prove it from different vantage
3458960	3463920	points than in the book, from a biological, sociological, psychological, mathematical.
3463920	3474640	But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we
3474640	3484800	can ever develop mathematically, so as to engineer it in any, in any realistic way.
3486160	3493120	They make good arguments throughout. There are many examples of the basic idea is that
3494880	3502480	all the mathematics we know, right, mathematics available to us, cannot model
3504480	3511200	not just the entire mind, but even subsystems in the mind, language being one of them.
3513360	3518800	And so it's all complex systems within complex systems in a complex environment,
3519280	3525360	the system around us that we interact with. And none of it can be modeled mathematically,
3525360	3532320	none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.
3533280	3540800	Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do
3540880	3550480	amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that,
3550480	3559120	unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus
3559120	3565280	or Newton, like we're talking about a new mathematics that we never conceived of, right?
3565600	3573200	Which they say most likely all evidence says that's not going to happen, right? So
3574560	3583440	now you can get into why. So that's their claim. And why? They say that all these systems are
3583440	3593520	complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems.
3593520	3601680	They work in a dynamic environment. They are continuously evolving and adapting, right? They
3601680	3609600	are self feeding systems. These are not systems that only take input output. These systems change
3609600	3616080	their behavior. And I gave an example from list. These systems are systems that change their behavior,
3616080	3624640	their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why
3624640	3629600	I said they, I would have liked to see a discussion on the frame problem, because the frame problem
3629600	3638480	in the AI is about this. How can I reason in a dynamic and uncertain environment and react
3639040	3645120	dynamically, although what I do in the environment might affect what I believe about the environment
3645120	3650480	in real time. And they're right. There is no mathematics we know of now. That's why we don't
3650480	3657440	have a solution for the frame problem. So this kind of cyclical cause and effect
3658640	3662640	cannot be modeled by anything we know on mathematics. And this I agree with them.
3664880	3670160	They give an example. I made just an example in language, for example. Language, we know.
3670720	3677920	If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires
3677920	3684160	having the context in mind as part of the, part of the input to the evaluation of the meaning
3684160	3692000	is the context as an extra parameter, right? Now, the context is changing based on something I
3692000	3700480	cannot predict, which is the response of some participant in the dialogue. There is no meaningful
3700480	3707360	way of predicting how someone might respond. So in other words, the context is mathematically
3707360	3712560	not defined, but I need it in the interpretation. Thus, no language understanding, no language
3712560	3718080	understanding, no AGI, because they believe language understanding is a prerequisite. So the,
3719040	3724960	their conclusion, I mean, you can question every step in this inference they come to,
3724960	3729600	but they give language as an example, but we have social behavior, I can give an example.
3729600	3735440	They have a nice example in social behavior. Here's an example of a complex system that cannot,
3736160	3743840	we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency
3744800	3751520	room. What do they call them? These ER. So, but there's a queue because they all have
3751520	3759280	emergencies, right? Now, the social behavior, then the social norm is that in the queue,
3759280	3765040	okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a
3765040	3776880	social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost
3776880	3788160	gone, right? Our social norm accepts that this person violates the queue order, right? This is
3788160	3796640	something dynamic that happens, like the queue is this way. And how can a robot update the rules
3796640	3802720	and not kill someone because they violated the order of the queue? In other words, these interactions,
3802720	3810640	these cyclical cause and effect are very complex, that no mathematical model. Or the example I said
3810640	3818000	in language, they prove this cannot be done. Context is needed to interpret everything. I cannot
3818160	3824960	predict what the context will be because I cannot predict you respond to my, so it's unpredictable,
3824960	3830320	they call it erratic, almost random. So there is no mathematics that can model it.
3832160	3836640	And there are many aspects to the mind, whether it's social reasoning, language, and then they
3836640	3846080	conclude there cannot be a system that we can model on volume and machines, because we don't
3846080	3852240	have the mathematics to model it. And these, they go into deep learning. And they give examples
3852240	3857360	even like deep learning, no matter how much data you ingest, you can never predict the future.
3858640	3866320	You're lucky if you can do a good job on the past and even forget the future. And definitely
3866320	3872880	forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I
3872880	3879680	have a couple of comments. So one is, would you agree that this is quite synonymous with,
3879680	3884960	you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the
3884960	3889600	self-referential self systems, because I mean, complex systems, a big part of them is they
3889600	3895280	usually are, they do have feedback loops. And at some scale, they become so they will involve
3895280	3901840	self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have
3901840	3907680	quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about,
3907680	3913440	with one exception, which is I'm still optimistic that we can discover a mathematics that may help
3913440	3920000	us out. And so I always think to the foundation series by Isaac Asimov, because in there, they
3920000	3926080	discover a science in a mathematics called psycho history, which at least allows them to predict
3926160	3932080	complex systems of a certain scale and larger. So in the book, it's sort of like planet scale
3932080	3937680	and larger, they're able to actually predict, you know, these complex sociological systems
3937680	3943280	and human behaviors, and how they're going to interact like beyond, beyond that scale. And
3943280	3947600	it's really fascinating. I make that point. I highly recommend that series to anybody,
3947600	3952960	because it's very fascinating because, you know, they talk a lot about sort of what if you had
3952960	3958400	the science, what might it look like, etc. And in there, there's like this little tiny microscopic
3958400	3964000	thing that's beyond the predictability of psycho history that comes in and kind of mucks up the
3964000	3969680	works and creates anomalies that they have to constantly keep combating against. So I think
3969680	3974800	if anybody wants a fictional take on a possible mathematics of this, like, I would recommend
3974800	3981440	Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems
3981440	3986960	in the sense of cyclical cause and effect that we don't have anything that can model them
3986960	3992160	intelligently. And I give an example in this, in this, I can write a program that changes itself
3992160	3997600	at runtime. Because this is intentional, I can, the whole program can be a parameter,
3998240	4003520	which I can look at it. Well, code is data. That's why I can manipulate the program itself
4003520	4007680	and go look at it after execution and see different program than the one I wrote. It's,
4007680	4013840	it's amazing list. So if I, so I can write programs in this that no one can understand
4014880	4021200	and model and do program verification. So I make the argument that, okay, I can see your point.
4022160	4029120	But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?
4029120	4035440	I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus,
4035440	4041760	why not? Which could happen. So the word never for me, it's hard to digest.
4041760	4045040	Maybe an AGI will discover the mathematics to create itself.
4046480	4052560	And the other point is, the other point is, which is another point that John McCarthy wants.
4053360	4060800	Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?
4060800	4067280	We don't. So why not build a scary intelligent machine that we don't really understand,
4067280	4073040	like my list program? So what I'm saying is I had, I had an issue with them saying,
4073760	4082320	that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us
4083040	4088640	in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent.
4089440	4094320	And we don't understand how it works. So what? This can happen. I can build something I don't
4094320	4102720	understand. So in theory, I have two issues with their book, that this never and this absolute
4102720	4106480	decision that we're done, we can never get there. No, we might build something we don't understand
4107200	4111760	by discovering some new weird mathematics. So, okay, I agree with you that it's a,
4112480	4115120	it's a complex thing that we will never understand. But so what?
4115120	4121120	But what I loved about the book is it's a sobering book. I mean,
4122240	4128400	it really is a balancing book compared to the hype and the simplicity you see out there. I mean,
4128400	4136000	you, you recommend it or highly because I mean, I didn't need that much sobering. I know that
4136000	4143280	any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements
4143280	4147680	like this, right? Although I felt I thought you might have fallen off that wagon at the beginning
4147680	4156800	of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people
4156800	4162800	that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what
4162800	4170080	you're trying to do is. Okay, guys. So before you go out and say, language understand. And the
4170080	4176240	nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.
4176960	4181200	So imagine the whole mind and the granular thing they go through it. I mean, it's all,
4181200	4184800	it's complex systems all the way down or all the way up if you want. So
4186720	4191520	language is a complex system on its own part of the mind, which is a complex system on its own
4191520	4197440	part of the human living organism, which is a complex system on its own. So and at every level,
4197440	4204240	the complexity, we don't have a mathematics for that's the gist of their argument. So people that
4204240	4211040	make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not
4211040	4217680	solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel
4217680	4225280	Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI.
4226240	4232640	And all this transferability, transferability. I mean, if you're good at chess, I know people
4232640	4239840	that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm
4239840	4247760	good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good
4247760	4254720	sobering book, mathematically speaking, philosophically speaking, so that people will tone down
4254720	4263360	what they're saying and start speaking science instead of media gibberish, right? Deep learning
4263360	4271040	will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of
4271040	4281040	despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't
4281120	4291840	like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI.
4292720	4302080	We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,
4302080	4308160	machines are now superior to us in many respects and respects even that they require intelligence,
4308160	4315360	not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us.
4315360	4326400	We have go or finding patterns and data at the scale that no human can do. So we are building
4326400	4333120	intelligent machines, but can we conquer things like language like autonomous driving was a failure.
4333120	4337680	It's a big upset for AI because they trivialize the problem that we can go.
4338560	4342880	Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is
4344240	4348080	I take these kind of sobering, these sobering things and look, I mean,
4350160	4355600	the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts,
4355600	4361520	you know, many people have had, you know, many times over the years, right? And I've recognized
4361600	4366000	that there are these limitations. But I think like part of part of why I think
4366000	4371360	books like this are actually have an optimistic kind of side to them is I hope, I hope they
4371360	4379680	encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar
4379680	4385760	you have into yet another parameter, you know, into yet another thousand or billion parameters
4385760	4390240	in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering,
4390240	4395680	but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know
4395680	4401280	Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just
4402080	4408640	go out there and try to do something crazy to find that mathematics that we need, right? Which is
4408640	4414560	let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid
4414560	4420800	systems, let's not give up on, you know, neuromorphic, you know, systems and computer,
4420800	4425840	whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just
4425840	4433680	keep going down this direction of ever larger Turing machines, like, that may not be the solution.
4434240	4441680	Actually, they make this point exactly in different ways that if anything, their goal is to let people
4441760	4447920	widen their horizon. So many aspects of this problem that guys, if we keep going,
4447920	4451680	this is not going to get us there. And that's why they argued mathematically,
4451680	4455920	philosophically. And I think they have a good argument. This is not going to take us there.
4456480	4464320	But let's explore that. So that and being so religious about this will will hinder any other
4464880	4473040	possibility. So overall, their argument is a good argument. I think everybody should read this book
4473040	4482400	that's interested in AGI as a goal. What we have cannot ever take us there. They prove this
4482400	4488640	mathematically. I mean, to me, they proved it in language on. So we need something new. And if you
4488640	4493680	want to do something new, we can't just stay in this corner and with this, that's not going to get
4493680	4501280	us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.
4502080	4507200	Well, and encouraging of more variety and more daring and more creativity and
4507760	4512640	right, they don't push too much on that. But but indirectly, the indirect
4513760	4520720	net result, if people appreciate the argument will be to look and explore other ways. So
4521280	4527440	in a way, it's more positive than negative. And by the way, stating a mathematical fact is never
4527440	4535120	negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being
4535120	4541040	canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that
4542160	4548560	what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we
4549360	4554800	need something else, we need something more. Or yours, yours, look, it could have saved us,
4554800	4561040	they use this phrase, money down the drain. Autonomous driving is a case, is a good case.
4562480	4567200	Billions, we're talking non trivial money guys, we're talking more than the budgets of some
4567200	4575360	European countries. Just imagine the scale. And those guys went bust, right? Why? Because
4575440	4580880	they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys.
4581840	4588640	It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the
4588640	4594320	fly to change to do belief revision, change its strategy, because of something new that came up.
4595040	4600160	All of that is from seeing the tree and the stop sign. It's all vision.
4600960	4605280	Yeah, so this is actually encouraging because now for all the Uber drivers out there and
4605280	4612320	long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon.
4612320	4616880	Yeah, it's amazing. And a few years back when I was still in the valley,
4618160	4625200	yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and
4625920	4632080	big AI engineers at top companies. They were so excited that we're at level four
4632080	4636640	in a year or two. And this was six years ago. I say, guys, this will not happen. They say,
4636640	4642080	why are you so negative? I said, you cannot have an autonomous agent on the road without
4642080	4646960	solving the frame problem. How do I revise everything I know, because of this new event?
4648240	4652880	And this has to happen real time. We don't have a solution for the frame problem.
4652880	4659200	All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train.
4662000	4668320	We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning,
4668960	4674080	yeah, I can have autonomous anything. We call it the railway. I mean,
4675200	4678800	we call it Amtrak. It's autonomous. You press a button and it goes.
4679760	4683200	If we're talking about reasoning in the streets of San Francisco,
4684400	4687360	you have to face the frame problem or you will kill people.
4689840	4696720	Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,
4696720	4702240	so I'm not sure about that. Probably that's the least of them. No, but the scale of money that
4702320	4709360	went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that
4709360	4715360	was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're
4715360	4723680	throwing down the drain and explore something else. Show me. That's where I'm at, too.
4724960	4731280	Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars
4731280	4737200	just because I don't want to listen to anyone else. It happened in the chatbot industry, which
4737200	4743040	I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an
4743040	4750880	explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every
4750880	4759440	website we go to, it's like, leave me alone. Nobody wants to use them because we know how
4759440	4768160	they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control
4768160	4776560	F is more effective for me than trying to interact with a chatbot. The search engines by key phrases
4776560	4780880	you put and they bring you a link and they say, read this. This is your answer. They are search
4780880	4787040	engines basically. But again, the amount of money, because I lived in that industry,
4788320	4796560	the amount of money spent on chatbots will scare the hell out of anybody. You combine that with
4796560	4804400	autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And
4804400	4810000	you talk to any one of them in the highest, in the middle of the fever. They won't listen to you.
4810000	4817840	I have people now calling me back and saying, you were right. Yeah, after $200 billion. So,
4817840	4824080	science is important. Engineering is important, but science is important too. That's where the
4824080	4831680	value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer,
4831680	4836080	you can hack your way through what we know is true. That's the space you can play with.
4836320	4845920	You cannot hack your way in a bigger set of possibilities that are. You didn't verify that
4845920	4853600	you can go there. An engineer can be creative within a Venn diagram that the scientists drew
4853600	4860400	for them. That's the difference between science and engineering. The scientist draws the Venn
4860400	4865840	diagram and that's the value of philosophers, at least the analytic philosophers, that know logic
4865840	4871920	and metaphysics and quantum mechanics and philosophers that are on the technical side.
4872720	4878400	They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here,
4879280	4885120	that's called money down the drain. Play inside the Venn diagram. Otherwise,
4885920	4890640	you're just an over enthused engineer who should go back and study computability.
4891600	4895760	It's kind of like how patent examiners can easily reject anything that comes in that
4895760	4902480	claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we
4902480	4908720	sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great
4908720	4916160	questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks,
4916160	4921760	always fun, guys. I see. Peace.
