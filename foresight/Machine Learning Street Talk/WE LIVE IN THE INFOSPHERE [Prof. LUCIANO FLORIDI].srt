1
00:00:00,000 --> 00:00:05,680
Professor Luciano Floridi is the Oxford Internet Institute's Professor of Philosophy

2
00:00:05,680 --> 00:00:11,440
and Ethics of Information at the University of Oxford, where he's also the Director of the

3
00:00:11,440 --> 00:00:17,280
Digital Ethics Lab of the Oxford Internet Institute. Still in Oxford, he's a distinguished

4
00:00:17,280 --> 00:00:22,400
research fellow of the Yuhiro Centre for Practical Ethics of the Faculty of Philosophy

5
00:00:22,400 --> 00:00:27,680
and Research Associate and Fellow in Information Policy in the Department of Computer Science.

6
00:00:28,480 --> 00:00:34,160
Outside Oxford, he's the Faculty Fellow of the Alan Turing Institute and Chair of its Data Ethics

7
00:00:34,160 --> 00:00:39,760
Group and the Distinguished Scholar in Residence of the Department of Economics at the American

8
00:00:39,760 --> 00:00:46,800
University in Washington DC. His research concerns primarily digital ethics, the philosophy of

9
00:00:46,800 --> 00:00:53,200
information, which is a discipline he founded, and the philosophy of technology itself.

10
00:00:53,200 --> 00:01:01,120
I will introduce AI as a divorce between the ability to perform a task successfully

11
00:01:01,120 --> 00:01:08,560
in view of a goal, shall we say, play chess just to be trivial, and any need to be intelligent

12
00:01:08,560 --> 00:01:17,200
in doing so. Replace that with parking a car, landing an aircraft, delivering something to your house.

13
00:01:17,920 --> 00:01:23,600
You don't have to be intelligent to do that if you are a piece of AI. That's why it works.

14
00:01:24,560 --> 00:01:31,920
If you had to wait to be intelligent, it would. My iPhone there, turn off properly, would not play

15
00:01:31,920 --> 00:01:37,840
better chess than anyone else in this room if we had to wait for their iPhone to be as intelligent

16
00:01:37,840 --> 00:01:44,160
as a rat. It is because it isn't, and we gave up on trying to make it intelligent as a rat.

17
00:01:45,040 --> 00:01:49,840
We're in the midst of an information revolution, as well as a computing revolution.

18
00:01:50,400 --> 00:01:56,560
Floridi said in his introductory book that all members of the G7 qualify as information

19
00:01:56,560 --> 00:02:03,840
societies, because in each country, at least 70% of their GDP depends on intangible goods,

20
00:02:03,840 --> 00:02:10,320
which are information related, not on material goods. Their functioning and growth requires

21
00:02:10,320 --> 00:02:18,800
and generates immense amounts of data, more data than humanity has ever seen in its entire history.

22
00:02:18,800 --> 00:02:26,880
It was estimated in 2003 that humanity had accumulated approximately 12 exabytes of data,

23
00:02:26,880 --> 00:02:32,480
and we're fast approaching the age of the zettabyte, which is 1000 exabytes.

24
00:02:33,280 --> 00:02:38,400
Sometimes people think in terms of, oh, can we regulate AI, which is a bit like, can we regulate

25
00:02:38,400 --> 00:02:44,880
electricity? Well, it's a bit vague. What do you mean? It's not true, false, correct, incorrect.

26
00:02:44,880 --> 00:02:47,840
It just doesn't yet make any friction with the real world.

27
00:02:49,360 --> 00:02:56,000
Floridi thinks that the material world is becoming redundant. Our agency as humans is being

28
00:02:56,000 --> 00:03:01,680
perniciously eroded, and that the increasing dominance of the information landscape, or the

29
00:03:02,240 --> 00:03:08,000
infosphere as Floridi calls it, is becoming the primary basis for our reality.

30
00:03:09,040 --> 00:03:15,840
Now about 95% of all of the data that we have today has been created by the current generation.

31
00:03:15,840 --> 00:03:22,240
We're not talking about Shakespeare or Dante. We're talking about lots and lots of dogs and

32
00:03:22,240 --> 00:03:29,200
cats. Floridi thinks that the infosphere is becoming so polluted with every passing day

33
00:03:29,200 --> 00:03:34,320
that the manifold of the infosphere is increasingly determined by technology

34
00:03:34,320 --> 00:03:40,000
and artificial intelligence. People are consumers. Consumers become followers.

35
00:03:40,000 --> 00:03:45,120
Followers become barcodes. Increasingly, the data is stored in a representation,

36
00:03:45,120 --> 00:03:51,200
which is primarily machine-readable, designed for consumption by other machines. Our very

37
00:03:51,200 --> 00:03:57,200
existence is being squeezed as a result. It wouldn't be great to have something instead

38
00:03:57,200 --> 00:04:04,400
of us doing it for us, better than us. Yeah, absolutely. I mean, if you read the description

39
00:04:04,400 --> 00:04:12,080
of that something that does it instead of us, better than us, in Aristotle, it's a slave.

40
00:04:13,360 --> 00:04:20,240
That's what we could do in the past. It was horrible. Luckily, it's not happening around us.

41
00:04:21,200 --> 00:04:26,480
But we were using humans as means to an end. Now we can use technology.

42
00:04:27,600 --> 00:04:33,200
Why this divorce? Why sort of pulling apart and saying, look, I want to develop something that

43
00:04:33,200 --> 00:04:38,400
is successful. I don't care whether it does it in the same way as I do it. As long as it's successful,

44
00:04:38,400 --> 00:04:43,520
that's fine. As opposed to, no, no, I want to implement human cognition. And if it's not human

45
00:04:43,520 --> 00:04:48,160
cognition, I don't care whether it's successful or not, but it has to have a little spark. Now,

46
00:04:48,160 --> 00:04:53,280
remember, AI, is it an engineering branch? Or is it a branch of colony science? Well,

47
00:04:53,280 --> 00:05:01,200
as a branch of colony science, we are at year zero. No step forward. We do not have the intelligence,

48
00:05:01,200 --> 00:05:08,960
as I said, of a rat. When it comes to engineering, that looks like magic. That looks like what has

49
00:05:08,960 --> 00:05:16,240
happened to humanity. Who has given us so much brain to do what we're doing today? And the old

50
00:05:16,240 --> 00:05:23,360
phrase like, is it worth asking whether a submarine can swim? Really? Can a computer think,

51
00:05:23,360 --> 00:05:28,240
ah, that's the wrong question. That's not the framework within which we should understand

52
00:05:28,240 --> 00:05:32,240
what a computer does. A submarine does what it does. An airplane does what it does. And it

53
00:05:32,240 --> 00:05:36,800
doesn't do it like a bird, doesn't do it like a fish. But it does it successfully. And that's

54
00:05:36,800 --> 00:05:42,880
the whole engineering point here. Success in delivering the output, given that particular input.

55
00:05:42,880 --> 00:05:46,960
But how do you know it's not the same process? Well, imagine you have two, two,

56
00:05:46,960 --> 00:05:53,040
four, and I tell you one is plus, the other one is multiplication. Which one did I use to get four

57
00:05:53,040 --> 00:05:58,480
from two and two? I don't know. Two plus two is equal to four. Two by two is four. You see, you

58
00:05:58,480 --> 00:06:05,200
can't say. So it's the same thing. No, it isn't. The way I do the dishes, the way my dishwasher

59
00:06:05,200 --> 00:06:11,600
does the dishes, two different things. Florides said something very significant and profound

60
00:06:11,600 --> 00:06:17,520
has recently happened to human self understanding. He said that in many respects,

61
00:06:17,520 --> 00:06:24,000
we're not standalone entities, but rather interconnected informational organisms or

62
00:06:24,000 --> 00:06:31,040
info orgs, sharing with biological agents and engineered artifacts, a global environment

63
00:06:31,040 --> 00:06:38,080
ultimately made of information, what Florides calls the info sphere. This is the informational

64
00:06:38,080 --> 00:06:44,880
environment constituted by all the informational processes, services and entities, thus,

65
00:06:44,880 --> 00:06:50,640
including the informational agents, as well as their properties and interactions and mutual

66
00:06:50,640 --> 00:06:56,480
relations. Now, if there was a representative scientist for the fourth revolution, this would

67
00:06:56,480 --> 00:07:03,040
definitely be allenturing. It's now critical that we equip ourselves with a viable philosophy of

68
00:07:03,040 --> 00:07:03,760
information.

69
00:07:07,680 --> 00:07:11,920
Gazillions of data, yeah, of any kind, of any sort, and they're all digital, readable, machine

70
00:07:11,920 --> 00:07:16,560
readable. When I mean machine readable, they're barcodes, not meant for our eyes, they're meant

71
00:07:16,560 --> 00:07:21,440
for the machines. If you hear anything about the natives, the natives in this space are the robots,

72
00:07:21,440 --> 00:07:29,200
not us. We scuba dive. But these machines, they are digital elements in the digital environment,

73
00:07:29,200 --> 00:07:35,680
reading digital stuff and digital processing, etc. And of course, I show you better machine

74
00:07:35,680 --> 00:07:41,040
learning, better algorithms, but also more and more internal things, things connected to other

75
00:07:41,040 --> 00:07:47,280
things. And when you go home and you see all those green and blue lights, they're not for us,

76
00:07:47,280 --> 00:07:51,840
they're talking to each other. And a lot of the data we have are generated by those machines.

77
00:07:52,000 --> 00:08:02,160
Information and communication technologies have been changing the world profoundly and irreversibly

78
00:08:02,160 --> 00:08:09,280
for more than half a century now, with breathtaking scope and at neck breaking pace. On the one hand,

79
00:08:09,280 --> 00:08:14,960
they've brought concrete and imminent opportunities of enormous benefit to people's education,

80
00:08:14,960 --> 00:08:20,240
welfare and prosperity, as well as great economic and scientific advantages.

81
00:08:22,720 --> 00:08:29,440
This merging of our existence, both analog and digital, both online and offline,

82
00:08:30,480 --> 00:08:37,120
we really ever offline? Of course not. So I'm reminded normally that if you have ever heard

83
00:08:37,120 --> 00:08:41,440
the whale singing, if you don't even know what I'm talking about, then you're very, very young.

84
00:08:42,000 --> 00:08:45,680
But if you know what I'm talking about, then you have used something called a modem.

85
00:08:48,400 --> 00:08:51,760
That is called whale singing. And if you have heard the whale singing,

86
00:08:52,320 --> 00:08:57,360
well, you're more like on my side of the divide. So we don't hear the whale singing anymore.

87
00:09:00,320 --> 00:09:06,480
Floridi thinks that the risks are the nature of reality and our knowledge of it. The organization

88
00:09:06,480 --> 00:09:13,040
of a fair society, considering the digital divide, our responsibilities and obligations to present

89
00:09:13,040 --> 00:09:19,280
and future generations, our understanding of a global world, and the scope of our potential

90
00:09:19,280 --> 00:09:25,760
interactions with the environment. And finally, conceptual implications and complexity of an

91
00:09:25,760 --> 00:09:31,840
information and communication technology landscape. So we don't hear the whale singing anymore. There

92
00:09:31,840 --> 00:09:40,640
is no terminator coming. There is no gods in the sky deciding for us. There's nobody but humanity

93
00:09:40,640 --> 00:09:45,040
responsibility for what we're going to do with this technology. And anyone disagrees,

94
00:09:45,840 --> 00:09:49,040
I hope is just naive and doesn't have an agenda.

95
00:09:53,760 --> 00:09:58,240
Floridi said that the information society has grown quickly and chaotically,

96
00:09:58,240 --> 00:10:03,040
leading to a lack of balance between technological growth and a lack of understanding

97
00:10:03,040 --> 00:10:08,240
of the implications of this growth. It also highlights the need to dig deeper into the

98
00:10:08,240 --> 00:10:14,480
nature and implications of the information age in order to better anticipate and identify

99
00:10:14,480 --> 00:10:23,760
and resolve problems. We started understanding ourselves in roughly in two ways, who I am

100
00:10:24,320 --> 00:10:32,800
and what I can do. I am, no humanity speaking, different, exceptional, special because of my

101
00:10:32,800 --> 00:10:41,840
nature and because and or what I can do compared to anything else. So one is called personal identity.

102
00:10:42,400 --> 00:10:49,680
It's going to come close to privacy and the gazillions of data out there are eroding or

103
00:10:50,240 --> 00:10:57,680
transforming, exercising pressure on that particular who I am, my nature, my data subject,

104
00:10:57,680 --> 00:11:03,440
as we are defined by the European legislation, we all data subjects here. GDPR, General Data Protection

105
00:11:03,440 --> 00:11:08,640
Regulation says so. So if you have data subject and there's a gazillion of data about you

106
00:11:08,640 --> 00:11:14,000
somewhere being manipulated, well certainly your identity is not sure we say in question.

107
00:11:14,800 --> 00:11:20,240
The other half is remember not who I am but what I can do, as opposed to anyone else,

108
00:11:20,240 --> 00:11:25,680
what I say I, and we define ourselves at least since Kant onwards in manner of the German

109
00:11:25,680 --> 00:11:32,720
philosophy in terms of autonomy. We are special because we are in control of our actions, we

110
00:11:32,720 --> 00:11:39,120
can decide, we can plan, we can choose. Well now we have autonomous agents out there and if you

111
00:11:39,120 --> 00:11:45,600
are trying to identify yourself as we can only be not the ones who play chess, well then we are

112
00:11:45,600 --> 00:11:50,800
really in trouble. You can devalue human skills, you can remove human responsibility, you can reduce

113
00:11:50,800 --> 00:11:56,640
human control. Erode human self-determination is the other side of the coin. It's not super dangerous

114
00:11:57,520 --> 00:12:07,120
but the rock who thought that their little drop of water was nothing, 18 years later has a big hole

115
00:12:07,120 --> 00:12:12,880
in it because drop after drop after drop, the drop will shape the stone will shape the rock.

116
00:12:19,840 --> 00:12:26,080
Surely we want to have a technology that makes us more capable of deciding what we want

117
00:12:27,120 --> 00:12:35,520
or prefer what we really have in front of us and I'm a bit worried that here human nature,

118
00:12:35,520 --> 00:12:40,960
no we philosophers we know something about human nature, we'll kick in, we're lazy,

119
00:12:42,800 --> 00:12:50,720
we're also very malleable and we are easily convinced to do this or that so we need to be a

120
00:12:50,720 --> 00:12:57,600
little bit more careful exposing this very fragile malleable entity to a very robust,

121
00:12:58,400 --> 00:13:07,360
quite silly, very efficient, very successful technology that will probably tend to inadvertently

122
00:13:08,080 --> 00:13:13,360
change the nature of that fragile entity. This is something that I would like to be a little

123
00:13:13,360 --> 00:13:20,880
bit more concerned about, not terminator. So until recently you would go to know this wonderful techie

124
00:13:20,960 --> 00:13:29,520
conferences and it was all about technology innovation. The average politician salivates

125
00:13:29,520 --> 00:13:34,320
immediately as soon as you talk about innovation because innovation is growth, growth is money,

126
00:13:34,320 --> 00:13:39,600
money is jobs and people happy. So innovation, innovation, yeah, absolutely and it's crucial

127
00:13:39,600 --> 00:13:43,840
but it's not real challenge. I don't think that that's the most difficult challenge we are facing

128
00:13:43,840 --> 00:13:49,200
now, not because it's easy but because something more difficult to do behind innovation. The challenge

129
00:13:49,200 --> 00:13:56,160
therefore is how you govern all this but governance is a matter of design, designing the right policies

130
00:13:56,160 --> 00:14:01,520
so that incentives and disincentives are in the right place, there are no loopholes, people tend

131
00:14:01,520 --> 00:14:06,720
to do more the right thing than the bad thing. So all of a sudden we are shifting all this into

132
00:14:06,720 --> 00:14:11,920
socio-political issues. It's a matter of design, design the kind of society we want, remember

133
00:14:11,920 --> 00:14:18,720
what's the human project here that we want to implement. So anyway, I had a very fun trip up

134
00:14:18,720 --> 00:14:24,960
to Oxford the other day and had a chat with Luciano himself. I hope you find this episode

135
00:14:24,960 --> 00:14:32,160
valuable. Remember to subscribe and if you're listening give us a rating on Apple podcasts

136
00:14:32,160 --> 00:14:43,120
or whatever you listen to the podcast on. Enjoy. So to start proceedings off with misinformation,

137
00:14:43,760 --> 00:14:50,160
the internet has put the world in our pocket, knowledge at our fingertips but the business

138
00:14:50,160 --> 00:14:56,320
of the internet is about ads and exploitation, debasement and derangement of human attention

139
00:14:56,320 --> 00:15:00,880
and some have argued that social media is a bit like the climate change of our society.

140
00:15:01,520 --> 00:15:05,600
Are you concerned that the current state of internet economics will create

141
00:15:05,600 --> 00:15:10,400
a layer of plausible yet false abstraction between us and reality?

142
00:15:10,480 --> 00:15:19,840
There is a risk of more and more content being worthless, unreliable, false, misleading,

143
00:15:20,640 --> 00:15:28,560
fake news, propaganda. It is not a new problem but the size, the immensity, the impact is new.

144
00:15:29,360 --> 00:15:34,880
It's one thing if you publish a few pamphlets with several lies and another thing if you have

145
00:15:35,760 --> 00:15:42,240
hundreds or millions of followers believing at once that something was not the case and

146
00:15:42,240 --> 00:15:48,160
something that was the case wasn't. And I'm talking about elections and very public figures.

147
00:15:50,160 --> 00:15:57,760
I wouldn't however put the whole emphasis on the negative side. The social media are also,

148
00:15:57,760 --> 00:16:03,120
if we want, the solution to the problem. Today are part of the problem, I agree,

149
00:16:03,200 --> 00:16:08,080
but they could easily become part of the solution. Precisely because then if you remember,

150
00:16:08,080 --> 00:16:16,160
and I close here, good old days when we were online and there was no web, only the internet,

151
00:16:17,520 --> 00:16:21,760
we thought that we were going to create a more informed, more civilized, more reasonable

152
00:16:23,120 --> 00:16:30,800
environment. We can still do that. It's the kind of business models and lack of political, legal

153
00:16:31,760 --> 00:16:39,840
framework that has generated this tsunami of, shall we say, rubbish, which is polluting our

154
00:16:39,840 --> 00:16:47,120
infosphere, the space of our information. So, yes, it is a problem, no, it doesn't have to be a

155
00:16:47,120 --> 00:16:52,640
problem. It could be easily a big solution. One suggestion for all and as close here, imagine

156
00:16:52,640 --> 00:16:59,600
if we were to ban, and it's a bit of a joke, advertisement online. We'd have to pay, we would

157
00:16:59,600 --> 00:17:05,040
pay for the product. That means that there will be competition for quality. We'll be probably in

158
00:17:05,040 --> 00:17:11,040
a different kind of game. Interesting. And we'll get into a minute, we'll get into the subject of how

159
00:17:11,920 --> 00:17:20,880
the structures of, let's say, markets and economic models affect us as human beings and as a society,

160
00:17:20,880 --> 00:17:29,040
but also affects our perception of reality, essentially. But before we get there, you wrote

161
00:17:29,200 --> 00:17:36,480
a paper called GPT-3, It's Nature, Scopes, Limits and Consequences. And you said that

162
00:17:37,200 --> 00:17:40,640
reversible and irreversible questions are based on mathematical logic,

163
00:17:41,760 --> 00:17:47,120
libanate law and other fields such as computing and physics. Can you explain what you meant by that?

164
00:17:48,960 --> 00:17:52,560
It's a simple idea. So, if I fail, it's my fault.

165
00:17:53,440 --> 00:18:03,120
A question that I define there as reversible means that from the answer, you can reverse to

166
00:18:03,840 --> 00:18:11,520
the actual identity of the source of the answer. An irreversible one is one that doesn't allow

167
00:18:11,520 --> 00:18:18,480
you to do that. Let me give you an irreversible answer today. Come to my house, you find clean

168
00:18:18,480 --> 00:18:24,800
dishes on the table. Who has cleaned the dishes? It's irreversible. You can't tell. It could be me

169
00:18:24,800 --> 00:18:29,440
or it could be the dishwasher. So, you cannot tell, by looking at the dishes, the output,

170
00:18:30,000 --> 00:18:33,840
whether those dishes have gone through a process of hand washing or machine washing,

171
00:18:33,840 --> 00:18:39,680
and whether the source is one or the other. A reversible one would be the sort of question

172
00:18:40,160 --> 00:18:49,760
today, and I'm talking today. We can ask to chat GPT. Use anything like Mary's mother has

173
00:18:49,760 --> 00:18:56,640
three children, tell me the name of one of them, and GPT says, I don't know. It does, I have no

174
00:18:56,640 --> 00:19:04,640
enough information. It's Mary's mother, so surely. So, that becomes immediately reversible. You know

175
00:19:04,640 --> 00:19:13,680
exactly that the answer is coming from a computer, from a large language model, and not from a human

176
00:19:13,680 --> 00:19:20,800
being who gets that immediately. Imagine the other way around. You ask for a super complex

177
00:19:20,800 --> 00:19:26,640
calculation, and boom, you get the answer in a fraction of a second. The other one may come

178
00:19:26,640 --> 00:19:33,520
snake and maybe takes longer, so who was the other side? So, increasingly in the paper, I write that,

179
00:19:34,800 --> 00:19:39,280
and it was before chat GPT coming out, but it's obvious, and it's been, you know, our progress

180
00:19:39,280 --> 00:19:46,000
since then. Increasingly, the sort of output that we are dealing with will become irreversible. We

181
00:19:46,000 --> 00:19:51,440
will be unable to tell whether the source is, for example, a human being, a group of human being,

182
00:19:51,440 --> 00:19:59,600
a human being with a computer, or chat GPT, or whatever is going to come, GPT4, etc. Imagine

183
00:19:59,600 --> 00:20:06,640
GPT10, not many years from now. I think that it will be completely irreversible. It does not mean,

184
00:20:06,640 --> 00:20:12,000
however, let me close here, that I'm like the dishwasher. If you can't tell who did the dishes,

185
00:20:12,960 --> 00:20:17,200
what difference does it make? I've done them in one way, which is completely different.

186
00:20:17,920 --> 00:20:24,640
The machine does it in a sort of mechanical, etc. So, we should be clear about what we infer from

187
00:20:25,440 --> 00:20:31,200
the reversibility, any reversibility of the output, and that seems to me where a lot of confusion is

188
00:20:31,200 --> 00:20:38,080
generated. Interesting. So, there's the provenance of the information. So, where it originated from,

189
00:20:38,080 --> 00:20:43,040
we could go down the line of saying, because it is human aligned with this reinforcement

190
00:20:43,040 --> 00:20:47,120
learning for human feedback, it's remarkably human aligned, actually, much more than the original

191
00:20:47,120 --> 00:20:55,520
GPT3. Then, you spoke to this semantic divergence, and we can talk about the pragmatic divergence

192
00:20:55,520 --> 00:21:00,720
as well. Actually, that's getting into a little bit about what John Sow talked about,

193
00:21:00,720 --> 00:21:06,800
the difference between ontology and epistemology. Is that something that you're

194
00:21:06,800 --> 00:21:12,560
seriously concerned about? Could you expand on that? I am concerned about it. I'm one of the

195
00:21:12,560 --> 00:21:20,480
few people, or I hope many more than Sims, who agree with the Chinese room and the

196
00:21:20,480 --> 00:21:25,120
philosophy behind it. Of course, you can always, among philosophers, you can always argue for

197
00:21:25,120 --> 00:21:30,240
details and sophisticated counter-argument, etc. But the bottom line, I mean, John got it

198
00:21:30,240 --> 00:21:35,280
right a long time ago, we're dealing with syntactic engines. There is no understanding,

199
00:21:35,280 --> 00:21:40,720
there is no insight, there is no emotional involvement, etc. Anything that would go

200
00:21:40,720 --> 00:21:45,440
into qualifying, characterizing human intelligence, which is also a rather fuzzy

201
00:21:45,440 --> 00:21:51,600
concept, by the way. On that front, I think we're going to see more and more of successful machines

202
00:21:52,480 --> 00:21:58,160
of the kind that we were just discussing. You mentioned the source, because that is

203
00:21:58,160 --> 00:22:02,640
the real difference. Let me give you a small example.

204
00:22:03,360 --> 00:22:14,480
Suppose you take from the shelf an amazing novel, a novel that John won the Nobel, for example.

205
00:22:16,240 --> 00:22:21,680
I would be surprised if you were to confuse that for something that's been generated by GPTX.

206
00:22:23,360 --> 00:22:28,800
The other thing is not true. Suppose you take a normal text, like an encyclopedia entry,

207
00:22:28,800 --> 00:22:35,120
or a good summary of that novel. Would you be able to tell me whether is a human being or GPTX?

208
00:22:35,120 --> 00:22:41,120
You wouldn't. So there's also a symmetry between the sort of product that we get and the sort of

209
00:22:42,240 --> 00:22:48,560
inferences that we can run. Final point, you know, technique is not everything,

210
00:22:49,200 --> 00:22:58,000
and we always fall into the same trap. You see someone reproducing on the pavement in a street,

211
00:22:58,000 --> 00:23:02,960
the Sistine Chapel. It's amazing. I mean, I wish I could do that, but it's not Michelangelo.

212
00:23:02,960 --> 00:23:09,200
Why? Well, because there is a different history, a different intention, there's a different aesthetics.

213
00:23:09,760 --> 00:23:17,280
So the mere ability of reproducing or doing like fails to get the semantic capital behind all that,

214
00:23:17,280 --> 00:23:23,120
the intention, the interpretation, the historical context, the continuity, the why did it happen

215
00:23:23,120 --> 00:23:30,960
and did not happen something else. All that is the richness of our understanding and semantics.

216
00:23:30,960 --> 00:23:37,440
We lose that if we think that all the difference that there is between Michelangelo and the artist

217
00:23:37,440 --> 00:23:42,160
in the street is just technique. It's not the case, and we should not be confused on that.

218
00:23:43,280 --> 00:23:48,560
I completely agree with you that there is a rich semantic context which is embedded,

219
00:23:49,200 --> 00:23:56,800
and this is something I'm conflicted on. So I agree with Soil in the sense that he said

220
00:23:56,800 --> 00:24:02,560
there was this rich and nagle actually, this impenetrable realm of the subject of experience,

221
00:24:02,560 --> 00:24:07,680
and that informs the meaning. You know, there's an ontological difference in understanding,

222
00:24:07,680 --> 00:24:13,920
which is what Soil said. But what's been so remarkable with chat GPT is it does seem to

223
00:24:14,880 --> 00:24:22,400
confer some semantic sort of inference, if you like. And I think the reason for that is

224
00:24:22,400 --> 00:24:31,760
going back to Pierce's triad, the symbolic version, the semantic content is embedded in

225
00:24:31,760 --> 00:24:38,400
our language. And that's why GPT3, even though it's mimicry, it often appears to, I mean,

226
00:24:38,480 --> 00:24:43,040
for example, it knows what the concept of Christmas is. So now it's becoming

227
00:24:44,080 --> 00:24:48,400
such a close reflection of us, it's becoming more difficult to distinguish.

228
00:24:50,160 --> 00:24:56,240
And at some point it would be impossible, if not already. But we should be careful not to confuse

229
00:24:56,240 --> 00:25:03,280
this with what you mentioned before, the intentionality, the realm of meaning, of existential

230
00:25:04,240 --> 00:25:12,160
experience of all this. Christmas is something that reverberates and provides meaning, for example,

231
00:25:12,160 --> 00:25:17,680
in each of us for a reason that is utterly cultural. It would have meant nothing to play

232
00:25:18,720 --> 00:25:23,280
obviously for historical reasons and so on. Let me give you an example. This would be like

233
00:25:23,280 --> 00:25:28,880
saying that when you cut the tree and you find the rings of the tree telling the age of the tree,

234
00:25:28,880 --> 00:25:36,240
the tree somehow told us its age. Well, it takes a little bit of imagination.

235
00:25:36,240 --> 00:25:42,160
No, it did not. And yet the symbolic, the representational, it is true that the tree has

236
00:25:42,720 --> 00:25:48,400
provided that information to begin with. But it's also because we can interpret that way. So

237
00:25:49,440 --> 00:25:54,480
I would like to see more work done on the hermeneutical side of all this, so that we get

238
00:25:54,720 --> 00:26:03,920
it clear on what we are consuming. Content, which is sometimes impeccable, sometimes irreversible,

239
00:26:04,880 --> 00:26:08,640
cannot be distinguished from content that could have been produced by a human being,

240
00:26:09,280 --> 00:26:14,880
but also the understanding, the interpretation, the context, the why it matters of their content.

241
00:26:14,880 --> 00:26:22,080
Now, boilerplates are exactly what they are. And if there's something in a, for example,

242
00:26:22,640 --> 00:26:29,120
encyclopedia entry or Wikipedia entry, that matters is exactly, is objectivity, is luck,

243
00:26:29,120 --> 00:26:35,440
insofar as we can do that, of interpretation, of an angle, almost like as if the author had to

244
00:26:35,440 --> 00:26:45,840
disappear. But that is exactly where tools like GPT number will excel. It would be different if

245
00:26:45,840 --> 00:26:54,000
we were talking about the way in which an artist, a writer, or anyone simply putting notes in

246
00:26:54,000 --> 00:27:00,560
her own diaries, is going through the process of verbalizing, conceptualizing the experience, etc.

247
00:27:00,560 --> 00:27:07,680
So ultimately, allow me another analogy. There's a reason why, although we could have

248
00:27:07,680 --> 00:27:13,440
the best director and the best orchestra performing that particular concert in the hall,

249
00:27:14,080 --> 00:27:19,680
or just a recording that you had to play, we don't. And we can't get a few, a bunch of kids

250
00:27:20,400 --> 00:27:26,000
in a choir and a few people who are rather amateurish here in Oxford and do a good job,

251
00:27:26,000 --> 00:27:30,880
but certainly not as good as it could have been done by professionals. Is the human experience

252
00:27:30,880 --> 00:27:34,480
that matters? It doesn't matter whether you could do that, better by having a tool.

253
00:27:35,600 --> 00:27:42,480
Given the pollution of the infosphere with data generated from large language models,

254
00:27:42,480 --> 00:27:44,720
what are the ethical implications on society?

255
00:27:46,960 --> 00:27:53,120
There are quite enormous, I mean, the responsibility on so many fronts. This responsibility,

256
00:27:53,120 --> 00:27:58,320
of course, of the source of this pollution. And by the way, we know that the actual sources are not

257
00:27:58,320 --> 00:28:06,560
many. It's not that each of us is constantly pouring, providing extra bits of misleading

258
00:28:06,560 --> 00:28:13,840
information and lies. We do inadvertently repeat and reverberate that, but the actual sources of,

259
00:28:13,840 --> 00:28:20,960
say, for example, Russian propaganda are very few and well known. And likewise, you know,

260
00:28:20,960 --> 00:28:26,640
on many other contexts. So first of all, a huge responsibility on those who are polluting the

261
00:28:26,640 --> 00:28:34,800
environment. They are few, they are powerful, and they should be curtailed, shall we say,

262
00:28:34,800 --> 00:28:40,960
if not entirely stopped. Huge responsibility on people who could legislate much better,

263
00:28:40,960 --> 00:28:48,000
and more firmly. Look at the current debate, people listening to this and watching this should

264
00:28:48,000 --> 00:28:52,560
look at the date, but look at what's happening these days to Twitter. I mean, the debate is open

265
00:28:52,560 --> 00:28:57,200
about whether it's going to be a source of information or misinformation, and who should

266
00:28:57,200 --> 00:29:01,920
be there or shouldn't, etc. How much do we want to control? How much do we want to not filter?

267
00:29:01,920 --> 00:29:07,440
But also how our responsibility as a society, we should demand more, and we should be more

268
00:29:07,440 --> 00:29:12,160
careful. So if you put together all these responsibilities between the producers, those

269
00:29:12,160 --> 00:29:18,960
who take advantage of it, the consumers, we are all in on this. No wonder, it's a mess.

270
00:29:18,960 --> 00:29:22,480
But as I said at the beginning, we could rectify it with a bit of goodwill.

271
00:29:24,240 --> 00:29:30,160
No, you wrote an article in Aion a few years back called Should We Be Afraid of AI?

272
00:29:30,160 --> 00:29:35,840
And the risk debate has been ongoing since the 1960s when Irvin John Goode prophesied a potential

273
00:29:35,840 --> 00:29:42,320
intelligence explosion that could leave humanity behind. And on one side of those who believe in

274
00:29:42,320 --> 00:29:49,040
true AI, the Church of the Singularitarians, as you wonderfully described them. And on the other

275
00:29:49,040 --> 00:29:53,600
side, you have those who do not believe in true AI, known as the Church of the Atheists,

276
00:29:53,600 --> 00:29:59,280
sort of that spell AI, which is absolutely delicious. And you said we should remain

277
00:29:59,280 --> 00:30:03,920
tolerant of both views. But the real challenge is to ensure that AI is used in a way that

278
00:30:03,920 --> 00:30:09,600
does not undermine human dignity. And you said the dogma of the Singularitarians consists of

279
00:30:09,600 --> 00:30:16,240
three beliefs. The creation of some form of AI is likely and humanity will be dominated by it.

280
00:30:16,240 --> 00:30:19,840
And it's the responsibility of the current generation to ensure it's benign.

281
00:30:21,440 --> 00:30:28,640
So you said that Singularitarianism is implausible. It relies on weak senses of the possibility

282
00:30:30,080 --> 00:30:35,760
of possibility and Moore's law, and it distracts from the real evils. It's almost an indulgent

283
00:30:35,760 --> 00:30:43,040
of the Western elite. So what can be done to ensure that Singularitarianism is not a distraction?

284
00:30:44,560 --> 00:30:51,120
I'm afraid that the only cure there will be history, meaning that as we move on,

285
00:30:52,000 --> 00:30:58,400
we will see that AI in its variety of forms, natural language processing,

286
00:30:59,600 --> 00:31:07,440
robots, even little gadgets that can help us to do this and that in our mobile phone,

287
00:31:07,440 --> 00:31:11,680
I mean, that they improve say communication or enable us to take a better picture,

288
00:31:12,240 --> 00:31:18,080
the recommended systems that you find on Netflix, et cetera, and all these things we will see as we

289
00:31:18,880 --> 00:31:27,440
move on and grow up, that what has really happened is absolutely extraordinary,

290
00:31:27,440 --> 00:31:35,600
but not extraordinary as the Singularitarian thing it is. We're actually separating and

291
00:31:35,600 --> 00:31:41,520
increasingly so the ability to do these things and to solve problems, take care of tasks

292
00:31:42,160 --> 00:31:48,480
successfully in view of some goal, some end, from any need to be intelligent in doing so,

293
00:31:48,480 --> 00:31:53,040
that is extraordinary. This divorce between agency and intelligence, that is amazing.

294
00:31:53,600 --> 00:31:57,520
And if anyone doesn't want to speculate, just think about chess playing. I mean chess playing

295
00:31:57,520 --> 00:32:02,880
today is done at zero intelligence, at least I hope people will admit that my iPhone doesn't think

296
00:32:03,760 --> 00:32:11,760
any bits, anyone I can encounter. So extraordinary ability that would require

297
00:32:11,760 --> 00:32:17,600
intelligence if I were to play that way, I wish I could, but that it can be done as zero intelligence.

298
00:32:17,600 --> 00:32:24,640
Now, as we learn more and more that there is the delta, the gap that we are building,

299
00:32:24,640 --> 00:32:29,760
and these two things will go further and further away as we move on, things in terms of what we

300
00:32:29,760 --> 00:32:35,840
just said in terms of large language models and the GPT and the other ones that are quick

301
00:32:35,840 --> 00:32:43,680
following from other companies without advertising for anyone. We will find any Singularity narrative

302
00:32:43,680 --> 00:32:50,960
science fiction, I mean entertaining at some point, frustrating at others, because as you said,

303
00:32:50,960 --> 00:32:58,400
it is also distracting. We have plenty of problems generated by this divorce. If you have an enormous

304
00:32:58,480 --> 00:33:06,000
new force or source of agency in the world, at zero intelligence, that requires intelligence,

305
00:33:06,000 --> 00:33:12,080
hours, governance, law, ethics, a sense of what is the human project you want to build,

306
00:33:12,800 --> 00:33:19,920
and meanwhile you're worried that your car would run away with your credit card to have a holiday

307
00:33:19,920 --> 00:33:25,920
on a beach, well then certainly frustration starts building up because we're not taking care of the

308
00:33:25,920 --> 00:33:31,760
real issues. Now, the discrimination, the digital divide, the amount of things that we're not doing,

309
00:33:31,760 --> 00:33:38,480
no, sort of opportunity cost, wherever we are lacking a clear framework and so on. So Singularity,

310
00:33:38,480 --> 00:33:44,400
I mean like all churches, I think they will become increasingly old and they will not die,

311
00:33:44,400 --> 00:33:50,080
I mean they will always have a few faithful ones, but hopefully will not be in the headlines,

312
00:33:50,080 --> 00:33:54,640
which is one of the reasons why people like to push forward the Singularity so that they get the

313
00:33:54,640 --> 00:34:00,800
headlines. In terms of the end game though, you might argue that they have a point, I suppose

314
00:34:00,800 --> 00:34:05,840
you're arguing that, I don't want to put words in your mouth, but you just spoke about the industrialized

315
00:34:05,840 --> 00:34:10,640
attenuation of our agency and that maybe that's analogous to saying we're becoming increasingly

316
00:34:10,640 --> 00:34:18,320
enslaved by technology and our children's generation much more so than we are. So in some sense,

317
00:34:19,200 --> 00:34:24,480
would you argue? Oh no, it would be like saying, well I mean you could read that in terms of the

318
00:34:24,480 --> 00:34:31,360
engine and wonder whether the engine and the whole industrial revolution and urbanization coming

319
00:34:31,360 --> 00:34:39,280
say from model engine and so on has been liberating, has been enslaving, has been a good thing, a bad

320
00:34:39,280 --> 00:34:46,720
thing, has all real historical and real philosophical issues. It's a mix back and you can't simply say

321
00:34:46,720 --> 00:34:54,080
oh no, I wish the car had never been invented, really like I'm not quite sure, but on the other

322
00:34:54,080 --> 00:34:58,640
hand the damage that we are caused to these environments, so same with the AI. Now one could

323
00:34:58,640 --> 00:35:04,080
say I wish people had never taken this genie out of the particular bottle, really like the kind

324
00:35:04,080 --> 00:35:10,080
of things that we can do thanks to this. Think of one case, just a simple case, like cold fusion,

325
00:35:10,080 --> 00:35:16,400
I mean today if there is even a remote chance to get there is because of machine learning.

326
00:35:17,200 --> 00:35:23,120
We will never get there without the enormous abilities of computational problem solving

327
00:35:23,120 --> 00:35:32,240
provided by AI and so forth, so this case too requires more commitment, more sort of human

328
00:35:32,240 --> 00:35:37,920
intelligence, more governance, not less. So sometimes I find that debate becomes a little bit of a

329
00:35:37,920 --> 00:35:42,960
deterministic kind of debate between things that people who think they were doomed and things

330
00:35:45,040 --> 00:35:51,040
rosy glasses think oh it's going to be a wonderful world, well honestly it just up to us. The crude

331
00:35:52,480 --> 00:35:58,000
painful sort of truth is that unless we do something about it, it will be a mess, but if we do

332
00:35:58,000 --> 00:36:02,960
something about it and we do it rightly, well then we can do an enormous amount of good things for

333
00:36:02,960 --> 00:36:09,440
this, no at last point just to have some references, just look at the impact of AI in a variety of forms

334
00:36:09,440 --> 00:36:15,920
again on the sustainable development goals has been already significant, could do so much more

335
00:36:15,920 --> 00:36:21,520
at the same time, how much energy goes into development these sort of large language models,

336
00:36:21,520 --> 00:36:27,680
huge, are we really doing the best with this sort of consumption and impact when then if we

337
00:36:27,680 --> 00:36:34,240
were to use this for example for entertainment, for advertisement, when every bit of electricity

338
00:36:34,240 --> 00:36:39,440
out there should be carefully considered, even impact on the environment, these are real issues,

339
00:36:39,440 --> 00:36:45,200
but they are philosophical issues for human beings. Is there a way that we can embrace this

340
00:36:45,200 --> 00:36:53,200
technology in a way that preserves human dignity? I think so, the question is not however to simplify

341
00:36:53,840 --> 00:37:01,760
in terms of human-centric, because what reason maybe we can explore later, but we have done

342
00:37:01,760 --> 00:37:06,880
too much of that precisely because we have been so human-centric, we have destroyed this world,

343
00:37:06,880 --> 00:37:15,760
we have killed the environment, we should be more sort of at the service of both the natural and the

344
00:37:15,840 --> 00:37:22,960
sort of man-made artificial environments as well, so to me the best way of using technology

345
00:37:22,960 --> 00:37:30,400
is by respecting as we said before human dignity, but also using technology to the benefit of

346
00:37:31,120 --> 00:37:36,000
quote-unquote the other, which could be future generations, it could be the environment,

347
00:37:36,000 --> 00:37:42,480
it could also be us taking it as humanity, but not just us, if we use technology just for us

348
00:37:42,480 --> 00:37:46,320
and our own benefit, we will ignore future generations, we will ignore the environment,

349
00:37:46,320 --> 00:37:52,560
and we will not have done a full decent job, so yes there is, but it takes a little bit more

350
00:37:52,560 --> 00:37:57,360
of a commitment that we have at the moment. What form of governance do you advocate for?

351
00:37:59,760 --> 00:38:06,080
To me the governance that we could develop would inevitably leverage this digital revolution by

352
00:38:06,960 --> 00:38:13,840
offering, not imposing, not expecting, offering more participation at the beginning of the

353
00:38:13,840 --> 00:38:19,040
decision or process, so these are two distinctions that go against the right democracy, I'm not

354
00:38:19,040 --> 00:38:26,400
advocating for the right democracy, I'm saying offering so cannot be expected or compulsory,

355
00:38:26,400 --> 00:38:33,360
but plenty of offer at the beginning, not at the end, take a referendum, a painful memory here,

356
00:38:33,520 --> 00:38:41,120
we are in the UK, a referendum is not the right democracy, why? Because you can't choose between

357
00:38:41,680 --> 00:38:48,720
A and B, if you dislike both A and B, someone has prepared the alternative for you, the real

358
00:38:49,360 --> 00:38:56,000
democracy that is co-designing of the choices, not the options at the end, but the initial

359
00:38:56,000 --> 00:39:04,320
choices is the one that says, should we consider a referendum or blah blah blah, so the point here

360
00:39:04,320 --> 00:39:11,440
is that the earlier the involvement, the better is to live in that kind of society, so the governance

361
00:39:11,440 --> 00:39:18,320
that I'm advocating is a governance that has as early as possible involvement of people in a

362
00:39:18,320 --> 00:39:27,120
co-design of the choices that we face, now that, and I was here sorry but it's a huge topic, that

363
00:39:28,240 --> 00:39:34,480
means that behind this particular sort of mechanism, there's a separation between

364
00:39:35,280 --> 00:39:44,960
those who have power, the people, and can delegate power, and those who exercise power but don't

365
00:39:45,040 --> 00:39:53,280
have it, this structural separation between having power without exercising and exercising power

366
00:39:53,280 --> 00:40:01,520
without having it, that to me is the ABC of any decent governance, governance fails, especially

367
00:40:01,520 --> 00:40:07,520
political governance, no in the democratic sense of governance, fails the moment you have those who

368
00:40:07,520 --> 00:40:12,400
have power exercise it and those who exercise it have it, it doesn't matter whether it's a majority,

369
00:40:12,400 --> 00:40:18,480
it's minority, it's an oligarchy, it's three people, it's one person, it's a family, it's a tyranny

370
00:40:18,480 --> 00:40:23,920
of a few people, maybe only one, as soon as you separate this then we start having the right

371
00:40:23,920 --> 00:40:29,440
governance, then we can talk about involvement in the decisional process as early as possible,

372
00:40:29,440 --> 00:40:35,200
then co-design, and therefore for all this the kind of technology that we have, this is doable if we

373
00:40:35,200 --> 00:40:41,280
want to implement it. One thing that concerns me is that when we talk about the architectural

374
00:40:41,280 --> 00:40:49,440
design of governance, the structure can sometimes exclude other parts of the complex world that

375
00:40:49,440 --> 00:40:54,320
we live in, and also there's this compatibility with the Demos which is to say it needs to be

376
00:40:54,320 --> 00:41:00,320
intelligible, and then you have this problem of people voting in their own self-interest,

377
00:41:00,320 --> 00:41:07,120
even if they did understand the purpose of the governance. True, and we know that self-interest

378
00:41:07,120 --> 00:41:13,680
can go only there far, we learned that from the past century, the 20th century, the 20th century

379
00:41:13,680 --> 00:41:19,280
has been the century, especially the second half, so stuff has been a disaster, the second half much

380
00:41:19,280 --> 00:41:26,080
more successful, has been the century of the citizen-consumer, and therefore the self-interest

381
00:41:26,080 --> 00:41:32,560
as a citizen voting for the best options, making a difference, and therefore competition between

382
00:41:32,560 --> 00:41:37,200
different parties etc, and the consumer having the same kind of choices, voting quote-unquote with

383
00:41:37,200 --> 00:41:41,680
his note, that what they feed so to speak, choosing their restaurant or another, their shop

384
00:41:41,680 --> 00:41:48,480
rather than another, their product or another, but this figure that was the selfish interest

385
00:41:49,680 --> 00:41:59,040
consumer slash citizen has become not the user and the follower, so today instead of having

386
00:41:59,040 --> 00:42:04,320
citizens we have sort of followers, instead of having consumers we have users, and therefore

387
00:42:04,320 --> 00:42:13,280
there has been sort of impoverishment of precisely their selfish sort of interest, the caring for

388
00:42:13,280 --> 00:42:18,400
my individual project, which is one of the two legs for democracy, so their leg has become weak,

389
00:42:18,960 --> 00:42:25,520
the other leg is absent, which is the social side, so at some point society should also invite,

390
00:42:25,520 --> 00:42:34,640
facilitate, support, so nudge gently towards more not social solutions, for the simple fact that

391
00:42:35,440 --> 00:42:41,760
if we need to be more ambitious in our goals, we need to get together, it's not enough to not pursue

392
00:42:41,760 --> 00:42:46,960
our own self-interest, we have to pursue interests that are shared, so imagine the following picture

393
00:42:46,960 --> 00:42:53,040
out of this quick analysis, two legs, one has become weaker, the selfish interest for my individual

394
00:42:53,040 --> 00:42:58,800
project, the other one which is a social project is not there for obvious reasons, when we tried in

395
00:42:58,800 --> 00:43:05,760
the first half of the last century, it was the most horrible episode in human history, the Nazi,

396
00:43:05,760 --> 00:43:12,800
the Soviet Union, no democracy going out of the window, I mean Auschwitz, I mean we have created

397
00:43:12,800 --> 00:43:22,000
the most horrific possible nightmares that humanity has ever seen, so on this not so well

398
00:43:22,160 --> 00:43:29,360
so established body of democracy which has one leg missing, at the other one we, there's a lot of

399
00:43:29,360 --> 00:43:35,760
work to be done, we can do a better job reinforcing a sense of individual projects, what we call not

400
00:43:35,760 --> 00:43:45,680
more selfish interest, yes, and no, instead of no, either or instead of weakening it, reinforcing it

401
00:43:45,680 --> 00:43:51,520
and balancing it with a social project, the idea that no, if you get together you can do so much

402
00:43:51,520 --> 00:43:56,160
more and so much better than just by yourself, finally an example, if that car, and I've used

403
00:43:56,160 --> 00:44:01,520
that more than once, forgive me, but if that car doesn't start, it's totally pointless for you to go

404
00:44:01,520 --> 00:44:06,240
and push and come back home and say I've done my duty, it has to be the five of us, and it has to be

405
00:44:06,240 --> 00:44:14,320
the five of us, one, two, three, push, coordinated, so we need to have a society that increases the

406
00:44:14,320 --> 00:44:20,640
support for individual projects and has a good liberal democracy and counterbalances that with

407
00:44:20,640 --> 00:44:26,480
social common projects, so there we have both, now normally you find this a little bit more sort

408
00:44:26,480 --> 00:44:33,360
of developed in, no, western social democracies of a Scandinavian kind, now I wonder everybody

409
00:44:33,360 --> 00:44:39,520
wants to get there, I think you would agree, I mean Chomsky spoke to this but that you know

410
00:44:39,520 --> 00:44:45,760
market forces as well as technological forces reontologize us, I want to pick up on this word

411
00:44:45,760 --> 00:44:51,280
reontologize, I think it's an amazing word, I'm just going to scroll down to this, but yeah you

412
00:44:51,280 --> 00:44:56,640
said that once digital immigrants like us are replaced by digital natives like our children,

413
00:44:56,640 --> 00:45:02,560
the immigration will become complete and future generations will increasingly feel deprived,

414
00:45:02,560 --> 00:45:07,840
excluded, handicapped or poor whenever they're disconnected from the infosphere like fish out

415
00:45:07,840 --> 00:45:12,800
of the water, you said that information and communication technologies are reontologizing

416
00:45:13,520 --> 00:45:20,080
which is an even more extreme form of re-engineering us in our society which is to say its intrinsic

417
00:45:20,080 --> 00:45:25,200
nature is being transformed, you said that we're modifying our everyday perspectives on the ultimate

418
00:45:25,200 --> 00:45:32,000
nature of reality that is our metaphysics from a materialistic one, you know in which physical

419
00:45:32,000 --> 00:45:38,000
objects and processes play a key role to an informational one, so let's I mean you know

420
00:45:38,000 --> 00:45:44,800
explain your rationale but I love this word reontologizing, yeah well this comes from a very

421
00:45:44,800 --> 00:45:52,640
Kantian anti-metaphysical perspective, I find any metaphysics of the bad kind that Kant was talking

422
00:45:52,640 --> 00:45:58,640
about beyond my comprehension, it's something that I just don't quite get, I mean how people

423
00:45:58,640 --> 00:46:04,960
can possibly believe that they have a direct line with being capital B and being told by being,

424
00:46:04,960 --> 00:46:14,480
what is like to be being is beyond me, so I'm afraid on that front I am rather deaf and I understand

425
00:46:14,480 --> 00:46:20,560
that it might be my limitation, I rather speak, sorry when I use the word metaphysics that's

426
00:46:20,560 --> 00:46:25,920
what I mean, the Kantian kind of thing that you don't do because it's meaningless to do it,

427
00:46:26,560 --> 00:46:32,320
there's also a lot of no kind of Vienna circle kind of an analytic philosophy in all this but

428
00:46:32,880 --> 00:46:41,680
ontology on the other hand is how we structure the world in the sense that we think that that's

429
00:46:41,680 --> 00:46:48,400
the way it is, so with the kind of eyes we have and the kind of light around the world

430
00:46:48,400 --> 00:46:53,600
that those are the colors we perceive, to me the colors in the world are part of the ontology of

431
00:46:53,600 --> 00:46:58,400
the world they're not, nothing to do with metaphysics, it's not the way things are in themselves,

432
00:46:58,400 --> 00:47:04,080
no woman and etc, I have no sense of what we're talking about but certainly a world full of

433
00:47:04,080 --> 00:47:10,800
colors is the world which I take it to be, the world, that's my ontology, now reontologizing

434
00:47:10,800 --> 00:47:18,080
means changing some of that particular nature, allow me a distinction so I hope it's not too

435
00:47:18,080 --> 00:47:24,480
confusing, reality in itself call it system, description of reality as we perceive it,

436
00:47:24,560 --> 00:47:30,320
enjoy it, conceptualize it, live through, model of the system, ontology to me is the ontology of

437
00:47:30,320 --> 00:47:35,440
the model, it's not the metaphysics of the system, I hope I haven't made a complete mess here,

438
00:47:35,440 --> 00:47:42,720
okay, so metaphysics no manom system, whatever the source of the data that we get, fantastic,

439
00:47:42,720 --> 00:47:47,680
the data don't speak about the source, the music of the radio is not about the radio,

440
00:47:47,680 --> 00:47:52,480
but there is a radio, of course the music is what we perceive, the music has its own ontology,

441
00:47:52,480 --> 00:47:59,360
structure etc, the model, the model is at that point what we enjoy, why the digital revolution

442
00:47:59,360 --> 00:48:05,760
has changed the nature of the world around us, not metaphysically but ontologically, so the

443
00:48:05,760 --> 00:48:11,120
reontologizing, because some of the things that we have inherited from modernity, and I really mean

444
00:48:11,120 --> 00:48:15,200
modernity in the ordinary sense, no three or four centuries depending on whether you have a

445
00:48:15,200 --> 00:48:22,160
large short medium modernity from Columbus onwards or bit shorters, first world world,

446
00:48:22,240 --> 00:48:28,640
your size, your preference, but modernity, we have been added from modernity a sense of the world

447
00:48:28,640 --> 00:48:36,320
that is now being restructured and a certain understanding of the world, so re-epistemologizing

448
00:48:36,320 --> 00:48:44,080
as well of that world, two simple examples, modernity onwards, which I know I've used in the past,

449
00:48:45,040 --> 00:48:52,560
we have grown up with the idea that law and territoriality are two sides of the same coin,

450
00:48:53,760 --> 00:48:58,640
you can't simply separate them, my place, my rules, your place, your rules,

451
00:48:58,640 --> 00:49:04,880
if from Westphalia onwards, that's the ABC of any legal system for a long long time,

452
00:49:04,880 --> 00:49:12,320
the law ends where the territory of that particular state that issues the law ends, no, at the border,

453
00:49:13,280 --> 00:49:18,160
so law and territoriality, two sides of the same coin, or two sides of the same piece of paper,

454
00:49:18,160 --> 00:49:22,320
you can't cut one without cutting the other, of course, that's the ontology of the world in which

455
00:49:22,320 --> 00:49:29,520
I live, welcome to cyberspace, right to be forgotten, our whole debate and today we know that actually

456
00:49:29,520 --> 00:49:35,200
law and territoriality are no longer two sides of the same coin, they've been completely decoupled

457
00:49:35,200 --> 00:49:40,560
and you cannot have rules that for example are passed in Europe that apply to a search engine

458
00:49:40,560 --> 00:49:45,760
that is based in the United States and yet is only one click away, so the right to be forgotten was

459
00:49:45,760 --> 00:49:51,760
no, the moment when this became obvious, we were legislating about Google and Google not complied

460
00:49:51,760 --> 00:49:57,760
in Europe, the complaint was like well but it's not changing the way google.com is working, well

461
00:49:57,760 --> 00:50:01,760
the law doesn't apply there and of course there was a debate in terms of oh but it has to apply

462
00:50:01,760 --> 00:50:09,120
everywhere in the physical space, so the decoupling of the law and the territoriality is a reontologization

463
00:50:09,120 --> 00:50:14,960
of that particular space, we live in a different world, we perceive the world differently, conceptually

464
00:50:14,960 --> 00:50:19,920
we also start thinking differently, now let me give you a small example and I close here

465
00:50:21,200 --> 00:50:29,360
of a reontologizing of something we took absolutely for granted, right, if you have a taxi you have a

466
00:50:29,360 --> 00:50:35,280
license, if you have a license you can be a taxi driver, the two things go hand in hand

467
00:50:35,840 --> 00:50:43,680
and it's illegal etc, then Uber comes and having a car being allowed to give a lift to someone

468
00:50:43,680 --> 00:50:50,960
is decoupled from having to have the license as a taxi driver, a cab driver, this decoupling

469
00:50:50,960 --> 00:50:57,840
has generated an enormous amount of profit, problems, issues, a change in game and all of a

470
00:50:57,840 --> 00:51:04,960
sudden we realize yeah those two things were not completely one side of the other, so obviously

471
00:51:04,960 --> 00:51:10,480
this uberization of the world means decoupling things that we have taken for granted as a single

472
00:51:10,480 --> 00:51:16,880
unit from modernity or gluing together things that we thought were completely independent

473
00:51:17,520 --> 00:51:23,120
in the past, last example, personal identity, for a long long time we discussed personal

474
00:51:23,120 --> 00:51:31,840
identity, I mean philosophically speaking, as you like substance, soul, body, what the incarnation

475
00:51:32,320 --> 00:51:37,600
today if you look at anything that identities our data, that's the European legislation

476
00:51:37,600 --> 00:51:44,080
describes as data subjects, I am my information, privacy, the whole privacy debate is discussed

477
00:51:44,080 --> 00:51:51,440
in terms of what constitutes me as that body of information, privacy is a matter of getting my

478
00:51:51,440 --> 00:51:56,880
body almost, so why is this important because if you look and I close here at the European

479
00:51:56,880 --> 00:52:06,480
legislation how did they cope with the decoupling of space territoriality and law by coupling personal

480
00:52:06,480 --> 00:52:14,240
identity and data, personal data so that today the legislation does not say you do as you're told

481
00:52:14,240 --> 00:52:22,320
because it's my space but it says you do as you're told because this data are the data of the individual

482
00:52:22,320 --> 00:52:29,760
who is my citizen wherever you are, so decoupling, recoupling, new legislation this is

483
00:52:29,760 --> 00:52:35,280
incomprehensible unless you have this cut and paste in mind which is the reontologizing,

484
00:52:35,280 --> 00:52:42,000
reappease demologizing of modernity through the digital evolution, it's a big deal.

485
00:52:43,200 --> 00:52:49,280
The Uber example is absolutely beautiful and I can place myself in the startup and these folks

486
00:52:49,280 --> 00:52:54,400
think they're being incredibly disruptive and what you said is interesting, we used to talk about

487
00:52:54,400 --> 00:53:01,360
the mind-body dualism but now there's almost a kind of identity dualism, there's my digital

488
00:53:01,360 --> 00:53:07,120
identity and that is becoming increasingly as you say the only thing which matters but

489
00:53:07,120 --> 00:53:13,120
to close off this reontologizing thing, I'm not sure this might be a little bit tangential but

490
00:53:13,120 --> 00:53:17,520
Heidegger conceived of technology as an intermediation between us and our environment,

491
00:53:18,320 --> 00:53:24,000
I think in a similar way and it shapes the environment as much as it shapes us back

492
00:53:24,560 --> 00:53:29,440
and in a sense reliance on technology can make us less human which is kind of what you're

493
00:53:29,440 --> 00:53:34,160
relating to, is there an instructive contrast to Heidegger's analysis of the relationship

494
00:53:34,160 --> 00:53:41,280
between humans and technology and you're right? I think Heidegger was onto something when obviously

495
00:53:41,280 --> 00:53:49,600
technology is between us and the environment but allow me to be critical, he didn't see this

496
00:53:50,480 --> 00:53:55,760
seems to me clearly enough because that description, the description of a first order,

497
00:53:55,760 --> 00:54:00,640
what I like to call first order technology is essentially the very idea that between me and the

498
00:54:00,640 --> 00:54:14,640
tree there might be a sowing mechanism or between me and that particular sort of garden

499
00:54:15,440 --> 00:54:21,120
there might be a spade but increasingly unless the industrial revolution

500
00:54:22,000 --> 00:54:25,920
instead of having humanity and nature and technology in the middle you have humanity,

501
00:54:25,920 --> 00:54:32,640
technology and technology and that's a second order technology so we increasingly use more and

502
00:54:32,640 --> 00:54:37,920
more technology to deal with other technology and that's not in Heidegger, it's a second order

503
00:54:37,920 --> 00:54:44,400
technology but what AI does is to generate a third order technology where no between A and

504
00:54:44,400 --> 00:54:48,720
B and C is technology throughout so it's a computer controlling a robot building a car

505
00:54:49,840 --> 00:54:53,760
at that point we are outside that relationship and we can just control

506
00:54:54,560 --> 00:54:57,920
check that everything goes in the right way we are on the loop or after the loop

507
00:54:58,640 --> 00:55:04,800
at that point we can reinterpret the Heideggerian negative view of technology and say that is

508
00:55:04,800 --> 00:55:12,400
exactly where you want to be, you want to be outside the me technology nature or me technology

509
00:55:12,400 --> 00:55:18,800
technology I want to be on top of technology technology technology and see whether that goes in

510
00:55:18,800 --> 00:55:27,360
the right direction now it's even more powerful even riskier but with higher risks higher rewards

511
00:55:27,360 --> 00:55:34,000
it could also mean how humanity can actually take care of the world nature and itself properly

512
00:55:34,640 --> 00:55:40,960
now we would not be able to save this planet and ourselves from ourselves on this planet

513
00:55:40,960 --> 00:55:47,040
without technology it's inconceivable that now we can see at the end of the 21st century

514
00:55:47,040 --> 00:55:53,520
as a success by abandoning technology like seriously no more embedded technology but then

515
00:55:53,520 --> 00:56:00,240
we need to understand this no third order idea if we're still stuck in a sort of 19th century 19th

516
00:56:00,240 --> 00:56:07,040
century idea that it's between me and the world we're not gonna get out this final point obviously

517
00:56:07,040 --> 00:56:13,440
being on the loop or after the loop the designer of the whole loop comes with enormous responsibilities

518
00:56:13,520 --> 00:56:19,360
that's why ultimately to me it's not digital innovation but it's the governance of the digital

519
00:56:20,000 --> 00:56:25,440
that makes the whole difference digital innovation is it's it's not easy but it's not difficult

520
00:56:25,440 --> 00:56:30,080
especially if you have deep pockets you buy the next startup and the next no three kids who come

521
00:56:30,080 --> 00:56:36,800
up with a fantastic idea knowing what to do with that that takes a genius so um you said in your

522
00:56:36,800 --> 00:56:42,800
information book that John Wheeler coined the notion of it from bit and some physicists entertain

523
00:56:42,880 --> 00:56:48,800
an information based description of reality and you said that this informational metaphysics may

524
00:56:48,800 --> 00:56:55,040
but does not have to endorse a more controversial view of the physical universe as a gigantic digital

525
00:56:55,040 --> 00:57:00,560
computer according to which dynamic processes are some kind of transitions in computational

526
00:57:00,560 --> 00:57:06,640
states in a style described similarly by Putnam and or maybe I call it Turing machine functionalism

527
00:57:07,360 --> 00:57:13,440
so you pointed out the the ontological difference between imagining a stomach as if it were a

528
00:57:13,440 --> 00:57:19,360
computer versus holding that the stomach is actually a computer you said that there's a clear

529
00:57:19,360 --> 00:57:23,760
philosophical distinction between whether the physical universe might be modelled computationally

530
00:57:25,040 --> 00:57:29,040
is a different question to whether the ultimate nature of the physical universe might actually

531
00:57:29,040 --> 00:57:35,440
be digital and computational in itself so so what's your take I think the whole problem is ultimate

532
00:57:36,800 --> 00:57:42,480
we go back to this temptation of talking about reality as if you were something that we need to

533
00:57:43,360 --> 00:57:54,880
grasp catch portray hook uh spears um when in fact the the way I prefer to understand it is as

534
00:57:56,480 --> 00:58:02,720
malleable understandable in a variety of ways um something that provides constraints

535
00:58:03,440 --> 00:58:08,320
it doesn't mean that you can interpret in any possible way but leaves room for

536
00:58:09,440 --> 00:58:14,800
different kind of interpretations so if the flow of data that come from whatever is out there

537
00:58:15,520 --> 00:58:22,720
and again I'd rather be sort of agnostic about it can be modelled in a variety of ways um one way

538
00:58:22,720 --> 00:58:28,160
is to especially 21st century given the technology we have etc to interpret that as

539
00:58:28,160 --> 00:58:34,000
no an enormous computational kind of uh environment it's perfectly fine as long as we don't think that

540
00:58:34,000 --> 00:58:41,600
there is our right metaphysics is the correct ontology for the 21st century now this is not

541
00:58:41,600 --> 00:58:48,480
relativism because on the other hand different models of the same system are comparable depending

542
00:58:48,480 --> 00:58:53,280
on why you're developing that particular model and let me give you a completely trivial exam

543
00:58:54,080 --> 00:58:56,720
suppose you ask me whether that building is the same building

544
00:58:57,920 --> 00:59:03,920
that question has no real answer because it depends on why you're asking that question

545
00:59:03,920 --> 00:59:08,160
if your question is asked because you want to have directions I'm gonna say oh yeah it's the

546
00:59:08,160 --> 00:59:13,680
same building so the same building yeah absolutely no go there turn left no traffic lights up but if

547
00:59:13,680 --> 00:59:17,840
your question is like same function as I know it's completely different building it was a school now

548
00:59:17,840 --> 00:59:26,000
is a hospital next question so is it or is it not the same that that question is the mistake

549
00:59:27,200 --> 00:59:33,760
an absolute question that provides no interface what computer scientists call level of abstraction

550
00:59:34,560 --> 00:59:40,720
chosen for one particular purpose so that I can compare whether an answer is better than another

551
00:59:40,720 --> 00:59:44,960
let me crack a joke for the philosophers who might be listening to this huh there's your ship

552
00:59:44,960 --> 00:59:51,040
is it the same or is not the same who is asking why because if it is the tax man the tax man

553
00:59:52,480 --> 00:59:57,520
you're doomed man I mean there is no way you can play any who I change every plank that you're

554
00:59:57,520 --> 01:00:04,080
gonna pay that tax is the same ship I don't care but if it is a collector that ship is worth zero

555
01:00:04,080 --> 01:00:09,280
you change all the planks you must be joking it's worthless so is it or is it not the same

556
01:00:09,840 --> 01:00:14,720
depends on why you're asking that particular question tell me why and I can give you the answer

557
01:00:15,120 --> 01:00:21,280
now why in other words no frame within which we have chosen the interface that provides the model

558
01:00:21,280 --> 01:00:27,680
of the system no potential answer so the question is like is the universe a computational

559
01:00:28,320 --> 01:00:37,520
gigantic yes or no meaningless is it worth modeling the universe as a gigantic for the purpose of

560
01:00:37,520 --> 01:00:43,920
making sense of our digital life oh yes definitely because we are informational organisms aha so

561
01:00:43,920 --> 01:00:50,080
metaphysics no I meant in the 21st century the best way of understanding human beings today

562
01:00:50,080 --> 01:00:56,800
is as information organisms last century we thought that biologically not made much more sense a lot

563
01:00:56,800 --> 01:01:06,080
of water and sprinkle all the extra and so on mechanism the car time etc so not absolute answers

564
01:01:06,080 --> 01:01:12,960
no relativistic answers but relational answers the relation between the question the purpose

565
01:01:13,040 --> 01:01:18,080
and the actual answer but it takes three not two any absolute question absolute mess

566
01:01:19,600 --> 01:01:25,040
this is so interesting I mean I spoke with David Chalmers recently and about the hard problem

567
01:01:25,040 --> 01:01:30,640
and there was a thought experiment about maybe David lives in the matrix and one of the architects

568
01:01:30,640 --> 01:01:35,840
came down to him and he said David you don't have the consciousness model installed we've

569
01:01:35,840 --> 01:01:39,760
installed it on other people but we didn't bother installing on you but you're a philosophical zombie

570
01:01:39,760 --> 01:01:44,800
or whatever and what would we need to do to to convince you that you're in a simulation and David

571
01:01:44,800 --> 01:01:48,480
said well I'm outside the Empire State Building maybe if you turned it upside down I believe you

572
01:01:48,480 --> 01:01:53,680
but but to be to be clear you're actually saying because I'm interested in this kind of

573
01:01:54,640 --> 01:01:59,840
ontological distinction you're saying that even if we did exist in a computer simulation it wouldn't

574
01:01:59,840 --> 01:02:06,160
make any difference it wouldn't it for the same reason that it doesn't matter whether we are in

575
01:02:06,160 --> 01:02:14,480
God's mind as Berkeley's subjects it makes no difference because what you have lost there is

576
01:02:14,480 --> 01:02:21,200
any traction by asking that particular question with anything that would provide any answer in other

577
01:02:21,200 --> 01:02:29,520
words these are only sophisticated ways of playing as you lose tails I win find me a solution

578
01:02:30,320 --> 01:02:36,400
you should stop engaging with these kind of questions is the trolley problem you should stop

579
01:02:36,400 --> 01:02:40,960
engaging in that particular question because there is it but philosophers love those disposals

580
01:02:40,960 --> 01:02:46,640
now they imagine the possible world in which well as soon as anyone start talking about imagine a

581
01:02:46,640 --> 01:02:52,560
possible world what are they I reach for my Kalashnikov or something else I I think we should

582
01:02:52,560 --> 01:02:56,800
really not indulge in these kind of things not because they have wronged but because they are

583
01:02:57,520 --> 01:03:04,720
misused any real philosopher and I'm talking about this no 20 25 30 classics have been using

584
01:03:05,680 --> 01:03:12,560
thought experiments logical possibilities for a purpose not in themselves you will not catch

585
01:03:12,560 --> 01:03:20,400
they can't trying to solve the demon malicious demon problem that is a tool to get somewhere

586
01:03:21,040 --> 01:03:25,760
is not something worth in itself same reason why now the trolley problem was developed to test

587
01:03:25,760 --> 01:03:31,920
the particular theory not as a puzzling itself same reason why you know people are no no in a

588
01:03:31,920 --> 01:03:38,720
matrix etc etc once again ask yourself what is the purpose for this particular question

589
01:03:39,360 --> 01:03:46,480
because the purpose is a lot of mental enjoyment chess is much better beautiful well we've got

590
01:03:46,480 --> 01:03:51,360
about two minutes left so I guess we'll have a closing statement but maybe also you could bring

591
01:03:51,360 --> 01:03:57,040
in I mean I've got a machine learning audience so they would be interested in in in digital ethics

592
01:03:57,040 --> 01:04:03,280
for example and and what they can do to learn more but also many folks are interested in getting into

593
01:04:03,280 --> 01:04:09,280
digital ethics and philosophy so what what would you say to those people to the machine to machine

594
01:04:09,280 --> 01:04:17,440
learning people I was always say like congratulations you are in the right business but secondly I

595
01:04:17,440 --> 01:04:25,600
would say please don't forget where all this is going have a sense that these are not just toys

596
01:04:25,600 --> 01:04:31,200
these are not just a matter of competition between one company another to have a larger model a faster

597
01:04:31,200 --> 01:04:38,080
sort of computer a slightly improved quantum something and on and on we can go in a variety

598
01:04:38,080 --> 01:04:41,680
of directions machine learning is one of the problems but think about the impact that they

599
01:04:41,680 --> 01:04:47,920
will have on society on the environment and on individual lives it could be a fantastic impact

600
01:04:47,920 --> 01:04:55,440
it could really make a difference in suffering in saving the environment or it could be a total

601
01:04:55,440 --> 01:05:03,760
disaster so please mind your machine learning and be a little bit more aware of the importance of

602
01:05:03,760 --> 01:05:09,760
your job don't underestimate how crucial it is what you're doing you are providing the foundations

603
01:05:09,840 --> 01:05:15,520
for the 21st century information society that is not small task we do that right we're gonna

604
01:05:15,520 --> 01:05:20,880
not be thanked by future generations you we do that wrongly future generations may not be even

605
01:05:20,880 --> 01:05:26,640
there but if they are there they would not be grateful so please not be careful what can be done

606
01:05:26,640 --> 01:05:31,920
well one of the things that no this will end up with self-advertisement a little bit but one of

607
01:05:31,920 --> 01:05:37,520
the things that we are going to do with the new center Yale is precisely opening the doors to

608
01:05:37,520 --> 01:05:43,920
all disciplines and all practitioners because research and development today is done especially

609
01:05:43,920 --> 01:05:48,720
in AI especially machine learning especially in digital context enormously more within

610
01:05:48,720 --> 01:05:56,000
industry than sometimes in even the top universities so opening up having this translational no from

611
01:05:56,000 --> 01:06:03,680
blue sky to product and multi-disciplinary open idea that it's a big table out there we need

612
01:06:03,760 --> 01:06:09,120
everybody around the table the politician the lawyer the engineer the machine learning expert

613
01:06:09,120 --> 01:06:15,440
the philosopher the ethicist the social scientist etc everybody around the table then that point

614
01:06:15,440 --> 01:06:23,040
that I made the beginning will become a little bit more reasonable doing it together to improve the

615
01:06:23,040 --> 01:06:30,160
world improve the chances of saving the environment ourselves can be done and I'm not sticking some

616
01:06:30,160 --> 01:06:38,320
on my actual so neck out so to speak on this but it's something that I truly believe it's

617
01:06:38,320 --> 01:06:45,600
entirely up to us and we can do this properly if we put ourselves into it professor Luciano

618
01:06:45,600 --> 01:06:51,600
Florida it's been an absolute honor thank you so much thanks a lot thank you so much

