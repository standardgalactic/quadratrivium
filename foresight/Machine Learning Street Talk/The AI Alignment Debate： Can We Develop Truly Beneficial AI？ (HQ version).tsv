start	end	text
0	7080	Okay, well in which case let's crack on. So ladies and gentlemen get ready to meet
7080	12000	the cunning maverick of Silicon Valley, the one and only George Hotz. Renowned
12000	16560	for his daring exploits, Hotz commands an enigmatic persona which merges the
16560	22080	technical finesse of Elon Musk and the wit of Tony Stark and the charm of a true
22080	26840	tech outlaw. Now many of you would have or indeed should have seen this man on
26840	31800	Lex's podcast recently for the third time no less. From craftily jailbreaking the
31800	36940	supposedly invincible iPhone to outsmarting the mighty PlayStation 3,
36940	41800	he's proven that no tech fortress is impregnable. Once targeted for his
41800	46560	audacious creativity by Sony with a lawsuit, this hacker wizard stoically
46560	50920	danced past the curveballs thrown by the tech giants all achieved with the
50920	55400	graceful swag of a street smart prodigy. Now when he's not outfoxing major
55400	59920	corporations, you'll find him at the heart of the avant-garde of AI technology,
59920	64360	gallantly trailblazing through the wilds of the tech front here. He's currently
64360	68880	building a startup called Micrograd which is building superfast AI running on
68880	74280	modern hardware and truly he's the James Bond of Silicon Valley minus the
74280	79440	martinis of course. Now please welcome the unparalleled code cowboy, the
79480	86280	unapologetic techno-mancer, George Hots! Whoo! Anyway, also joining us for the big
86280	90920	fight this evening is the steadfast sentinel of AI safety, Conor Leahy.
90920	95960	Undeterred by the sheer complexity of artificial intelligence, Conor braves
95960	100680	the cryptic operations of text generating models with steely resolve. Now
100680	104000	about two years ago Conor took on the Herculean task of safeguarding
104000	108960	humanity from a potential AI apocalypse. His spirit is relentless, his
109000	114560	intellect razor sharp and his will to protect is unwavering. Now drawing on
114560	121840	his contentious claim that we are super, super fucked. Yeah, Conor channels the
121840	125800	urgency of our predicament into his work. Now his startup conjecture isn't just a
125800	129880	glorified tech endeavor but it's a lifeboat for us all racing against the
129880	134400	breakneck speed of AI advancement with the fates of nations possibly at stake.
135040	140120	He's determined to break the damning prophecy and render us super, super saved.
140640	145880	So brace for a showdown as Conor Leahy, the maverick defender of AI's boundaries
145920	150720	strides into the ring. Now the man who declared we are super, super fucked is
150720	155920	here to prove just how super, super not fucked we could be if we make the right
155920	160560	decisions today. So please give it up for Mr. Conor, super, super Leahy. Whoo!
161320	164920	Now Conor, I'd appreciate it if you don't go down in the fourth. I want this
164920	168600	fight to go the distance. Now we're running for 90 minutes this evening.
168600	175280	There'll be a 10 minute openers from, we said hots, didn't we, from Hots first
175600	180240	and then Conor and I'll only step into the ring if the punch up gets two out of
180240	183320	hands and unfortunately we won't be taking live questions today because we
183320	187320	want to maximize the carnage on the battlefield. George Hots, your opening
187320	188040	statements please.
189800	193800	Um, yeah, we're super, super fucked. I think I agree with you.
195440	196480	But that was a short fight.
196520	201960	Yeah, look, I think, okay, so to make my opening statement clear and why maybe
201960	206040	it doesn't make that much sense for me to go first, I think that the trajectory
206040	213880	of all of this was somewhat inevitable, right? So you have humans over time and
213880	218520	you can look at a 1980 human and a 2020 human. They look pretty similar, right?
218560	220680	Ronald Reagan, Joe Biden, you know, that's all the same.
221920	228120	Whereas a 1980 computer is like an Apple II and a 2020 computer is a M1 Max
228120	232480	MacBook, like lines looking like this, right? So you have one line like this,
232480	235720	one line like this, these lines eventually cross and I don't see any
235720	239800	reason that line is going to stop, right? I've seen a few of the other guests
239800	244480	argue something like, well, LLMs can't problem solve, but it doesn't matter.
244520	249760	Like if this one can't, the next one will, whatever you call, I don't believe
249760	252600	that there's a step function. I don't believe that like, oh, now it's conscious
252600	257040	or now it's intelligent. I think it's all on a gradient. And I think this gradient
257040	262080	will continue to go up, will approach human level and will pass human level.
263400	268880	Now, this belief that we are uniquely fucked because of this, the amount of
268880	272680	power in the world is about to increase, right? When you think about power and you
272680	275800	think about, straight up, you can just talk about energy usage. The amount of
275800	279600	energy usage in the world is going to go up. The amount of intelligence in the
279600	284240	world is going to go up. We may be able to do some things to slow it down or
284240	288240	speed it up based on political decisions, but it doesn't matter. The trajectory
288240	292520	is up or major catastrophe, right? The only way it goes down is through war,
292520	296400	nuclear annihilation, bio annihilation, meteor impact, some kind of major
296400	301680	annihilation. That's what's going on. What we can control and what I think is
301680	306120	super important we control is what the distribution of that new power looks
306120	313480	like. I am not afraid of super intelligences. I am not afraid to live in
313480	319720	a world among super intelligences. I am afraid if a single person or a small
319720	324520	group of people has a super intelligence and I do not. And this is where we get
324520	330480	to chicken man. A chicken man is the man who owns the chicken farm. There's many
330480	334760	chickens in the chicken farm and there's one chicken man. It is unquestionable
334760	341280	that chicken man rules. And if you believe chicken man rules because of his size, I
341280	346440	invite you to look at cow man who also rules the cows and the cows are much
346440	350000	larger than him. Chicken man rules because of his intelligence. This is basic
350000	353560	less wrong stuff. Everyone kind of knows this. How the squishy things take over the
353560	360240	world. Look, I agree with Elias Yudkowski all up to Nuke the data centers, right. So I
360240	366400	do not want to be a chicken. And if people decide they are going to restrict
366400	371520	open source AI or make sure I can't get access to the compute and only trusted
371520	375680	people like chicken man get access to the compute. Well, shit, man, I'm the chicken.
375680	382960	And yeah, I don't want to be the chicken. So I think that's my are we fucked? Maybe.
383560	388960	Um, I agree that that intelligence is very dangerous. How can you look at
388960	392160	intelligence and not say it's very dangerous, right? Intelligence is
392160	398600	somehow safe. But things like nuclear bombs are an extremely false
398600	403240	equivalency because what does a nuclear bomb do besides blow up and kill
403240	408040	people? Intelligence has the potential to make us live forever. Intelligence has
408040	411840	the potential to let us colonize the galaxy. Intelligence has the potential
411920	419400	to meet God. Nuclear bombs do not they just blow up. Um, so I think the question
419400	423200	and like, you have things like crypto, which are a clear advantage to the
423200	426120	defender, at least today. And you have things like nuclear bombs, which are
426120	433080	clear advantage to the attacker. AI, it's unclear. I think the best defense
433160	436720	against an AI trying to manipulate me. And that's what I'm really worried
436720	439680	about future psyops, you know, we're already seeing it today with the voice
439720	442480	changer stuff. Like, you're never going to know who's human. The world's
442480	448560	about to get crazy. Um, the best defense I could possibly have is an AI in my
448560	453600	room being like, Don't worry, I got you. It's you and me. We're on a team. We're
453600	458200	aligned. I'm not worried about alignment as a technical challenge. I'm worried
458200	462040	about alignment as a political challenge. Google doesn't like me. Open AI
462040	467280	doesn't like me. But me and my computer, you know, we like each other. We're
467280	472080	aligned. And we're standing against the world that has always since the
472080	476440	beginning of history, maximally been trying to screw you over, right?
477160	480360	Intelligence, people think that one super intelligence is going to come and
480360	484440	be unaligned against humanity. All of humanity is unaligned against each
484440	490400	other. I mean, we have some common values, but really, come on, everyone's
490400	493440	trying to scam everybody. The only reason you really team up with someone else
493440	497720	is like, Hey, man, what if we team up and scam them, right? And what if we
497720	502240	team up, call ourselves America and we, we, we, uh, we build a big army and say
502240	507840	we're free and independent. Yeah. Right. It's that force that has made humanity
507840	512680	cooperate humanity by default. He's very unaligned and has every kind of
512680	516120	belief under the sun. So I'm not worried about AI showing up with a new
516120	519440	belief under the sun. I'm not worried about the amount of intelligence
519440	523520	increasing. I'm worried about a few entities that are unaligned with me
523720	528400	acquiring Godlike powers and using them to exploit me. I think that's my
528400	529000	opening statement.
530680	536920	Cool. Yeah. Thanks. That's, uh, that's, I mean, yeah, I also kind of agree with
536920	540040	you. And most of the things you say, there's a few details I'd like to
540040	544400	dig into there, but for most of the things you say, I do think I agree with
544400	548480	you here. I think it's absolute. Let me just like, start with saying, I
548480	553980	totally agree with you that misuse and like, you know, bad actors, what using
553980	559680	AGI is a horrible, dangerous outcome. That's, that's like, you know,
559680	563480	sometimes the, the, uh, less wrong, you know, crowd likes to talk about X
563480	567520	risk, but also sometimes I've talked about S risk, suffering risks. So things
567520	572680	are worse than death. I believe that you can probably almost only get S
572680	577040	risks from misuse. I don't think you can get S risks. Probably like, you
577040	580960	can, but it's extremely unlikely to get it from like just like raw
580960	585760	misalignment. Like you'd have to like get extraordinarily unlucky. So while I
585760	591280	do it, so I do think, for example, a very, you know, controllable AGI or
591280	595720	super intelligence in the hand of sadistic psychopath is significantly in
595720	599560	a sense worse than a paperclip maximizer. So I think this is something
599560	604400	we would agree on probably. So I think I'm to think of pretty much on board
604400	608640	with you on a lot of things there, where I think things come aboard a bit
608640	612920	of a tale as I think there's two points where I would like to take as my
612920	617400	opening statement, two things I want to talk about. The first one is I want to
617400	622160	talk about the technical problem of alignment. So am I concerned about the
622320	626240	kinds of things like misuse and like small groups of people centralizing
626240	630360	power potentially for nefarious deeds? Yeah, I think this is a very, very
630360	632720	significant problem that I do think about a lot. And that'll be the second
632720	635600	thing I want to talk about. The first thing I want to talk about is that I
635600	638240	don't even think we're going to make it to that point. I don't think we're
638240	642560	going to get to the point where anyone has a super intelligence that's
642560	646880	helping them out. We're going to, if we don't solve very hard technical
646880	650280	problems, which are currently not on track to being solved, by default, you
650280	654240	don't get a bunch of, you know, super intelligence and boxes working with a
654240	657440	bunch of humans. You get a bunch of super intelligence, you know, fighting each
657440	661120	other, working with each other and just ignoring humans. Humans just get cut
661120	665560	out entirely from the process. And even then, you know, it's, you know, whether
665560	669040	one takes over or they find an equilibrium, I don't know, like, you know, who
669040	672280	knows what happens to that point. But by default, I wouldn't expect humans to be
672280	675640	part of the equilibrium anymore. Once you're, once you're the chicken man, well,
675640	679560	why do you need chickens? You know, if, you know, maybe if they provide some
679560	682680	resource for you, the reason humans have chickens is that they make chicken
682680	686600	breasts. I mean, personally, I wouldn't like to be harvested for chicken
686600	690880	breasts, just my personal opinion. I consider this a pretty bad outcome.
691960	695880	But even then, well, as a chicken man finds a better way of chicken breasts or, you
695880	699520	know, modifies himself to no longer need food, I expect the chickens are not going
699520	702160	to be around for much longer. You know, once we stopped using horses for
702160	706440	transportation, didn't go very well for the horses. So that's kind of the first
706440	710520	part of my point that I'd like to, you know, maybe hear your opinions on, hear
710520	714800	your thoughts on, is that I think the technical control is actually very hard.
715080	719480	And I don't think it's unsolvable by any means. I think like, you know, you and
719480	722760	like, you know, a bunch of other smart people work on this for like 10 years, I
722760	726200	think you can solve it, but it's not easy. And it has to actually happen. And
726200	730120	there is a deadline for this. The second point I want to bring up is kind of
730120	733520	where you talk about how humans are unaligned. I think this is partially
733520	738520	definitely true. I think I'm unusually, I am the more optimistic of the two of
738520	743800	us in this scenario, not a role I often have in these discussions, where I
743800	747480	actually think the amount of coordination that exists between humanity,
747520	751800	especially in the modern world is actually astounding. Every single time two
751800	756080	adult human males meet and don't kill each other is a miracle. Have you seen
756080	759840	what happens when two adult male chimps from two different warbands meet each
759840	763560	other? It doesn't go very well. And those are already pretty well coordinated
763560	767320	animals because they can have warbands. What happens when, you know, two male
767600	771640	bugs or, you know, I don't know, sea slugs meet each other, you know, either
771640	775720	they ignore each other or, you know, things go very poorly. This is the
775720	779440	default outcome. The true unaligned outcome, the true default state of
779440	783800	nature is you can't have two adult males in the same room at any time. I saw
783800	787480	this funny video on Twitter the other day where it was like, I know, some
787480	791280	parliament, I think in East Europe or something, and there's this big guy
791280	794200	who's just like going at this politician, he's like in his face, he's like
794200	797760	screaming, he was like going everywhere, and not a single punch will
797760	802080	throw him. No, then no one took out a knife, no one took out a gun. And I
802080	807080	was like, wow, the fact that we're so civilized and we're so aligned to
807080	812080	each other that we can have something this barbaric happen and no one throws
812080	816800	a punch is actually shocking. This is very unusual, even for humans. If you
816800	822960	go back 200 years, punches and probably gunshots would have flown. So this is
822960	826360	not to say that humans have some inherent special essence that we're good,
826600	831480	that we have solved goodness or any means. What I'm saying is the way I like
831520	834840	to think about it is that coordination is a technology, is a technology you
834840	838200	can improve upon. It is you can develop new methods of coordination, you can
838200	841920	develop new structures, new institutions, new systems. And I think it's very
841920	844760	tempting for us living in this modern world to, it's kind of like a fish and
844760	848360	water effect. We forget how much of our life, a lot of our life is built on
849160	853760	atoms, on physical technology, a lot of it's built on digital technology, but
853760	860240	a lot of it is on social technology. And when I look at how does the world
860240	864200	go well? Like, you know, should it be only the special elites get control of
864200	867920	the AI? I'm like, well, that's not really how I think about it. And I think
867920	871320	about it way more is, what is a coordination mechanism where we can
871320	875120	create a coordination selling point or we can create a group, an institution, a
875120	881280	system of some kind that where people will have game theoretic incentives to
881280	884440	cooperate on the system, the results in something that is net positive for
884440	889000	everyone. Because the truth is, is that positive some games do exist. And
889000	893000	they're actually very profitable. And they're very good. And I think if we
893000	896440	can turn, you know, you can turn any positive some game into a net into a
896440	899560	zero or a negative some game pretty easily. It's much easier to destroy than
899560	903680	is to create. But I think it's absolutely possible to create coordination
903680	907520	technology around AI and to build coordination mechanisms that are net
907520	912360	positive for everyone involved. So those would be like my two points. Happy to
912360	915040	dig into anyone's you think would be it'll lead to an interesting
915040	920080	interaction. Sure. So I'll start with two and then go to one. So two, I moved
920080	924760	to Berkeley in 2014. And I threw myself at the Mary cult. I showed up at the
924760	932320	Mary office and I'm like, Hi, I'm here to join your cult. And what I started to
932320	941440	realize was, Mary, and less wrong in general, have a very poor grip on the
941480	947360	practicalities of politics. Very much. I think there was sort of a split, you
947360	954280	know, Curtis Yavin, like neoreaction. This is a spin off of rationality. And it's
954280	958360	a spin off rationality that understood the truth about human nature. So when I
958360	961520	give you that, you give that example of two chimps meeting in the woods and
961520	966480	they're going to fight. If I'm one of those chimps, at least I stand a chance.
966560	971720	Right. He might beat my ass. I might beat his. But if I come up against the FBI,
972640	977280	things do not look good for me. In fact, things so much do not look good for me.
977320	984320	There's no way I'm going to beat the FBI. The modern forces are so powerful that
984320	988960	this is not a, Oh, we've established a nice cooperative shelling point. This is
988960	993320	a, we have pounded so much fear into these people that they would never even
993320	999440	think of throwing a puncher firing a gun. We have made everybody terrified. And
999440	1003280	this isn't good. We didn't, we didn't achieve this through some enlightened
1003280	1008440	cooperation. We achieved this through a massive propaganda effort. Right. It's
1008440	1012880	the joke about, you know, the American soldier goes over to Russia and it's
1012880	1016680	like, man, you guys got some real propaganda here. And that the Russian
1016680	1021000	soldiers like, Yeah, no, I know it's bad, but it's not as bad as yours. And the
1021000	1026440	American soldiers like what propaganda? And the Russian just laughs, right? So, so
1026440	1031720	this, this didn't occur because of this occurred because of a absolute tyrannical
1031720	1038240	force decided to dominate everybody, right? Um, now, Oh, I think so. I think there's
1038240	1041600	a way out of this. I think there actually is a way out of this, right? And I wrote
1041600	1045560	a blog post about this called individual sovereignty. And I think a really nice
1045600	1051080	world would be if all the stuff to live food, water, health care, electricity
1051240	1055040	were generatable off the grid in a way that you are individually sovereign. And
1055040	1059360	this comes back to my point about offense and defense, right? If I have a world
1059360	1063240	where you don't want it to be extreme defense, you don't want every person to
1063240	1067440	be able to completely insulate them. But you want like, okay, it takes a whole
1067440	1071200	bunch of other people to gang up to take that guy out, right? Like that's, that's
1071200	1075960	a good, that's a good balance. And the balance that we live in today is there
1075960	1081080	is one pretty much a unipolar world. I mean, thank God for China. But you know,
1081080	1085200	there's one, there's one unipolar world, you got America and where are you going
1085200	1089880	to run? I'll pay taxes. I don't care if you live overseas, right? So yeah, my point
1089880	1093560	about the coordination is that if you're okay with solving coordination
1093560	1099720	problems by using a single, a singleton super intelligent AI to make everybody
1099720	1105360	cower in fear and tyrannize the future. Sure, you'll get coordination. Yeah, that
1105360	1108840	works. That works. I'm the only guy with a gun and I got 10 a year. I got a name
1108840	1111520	that all 10 of you and you can all die or listen to me your choice.
1111520	1116880	So I'm curious about, so I understand what you're saying. And I think you make
1116880	1121040	some decent points. But I think I view the world a bit differently from you. And
1121040	1124720	I'd like to like dig into that a little bit. So like, who do you think is less
1124720	1129160	afraid? Someone living just a medium person living in the United States of
1129160	1135720	America, or the medium person living in Somalia? Sure, America less afraid. Well,
1135720	1139200	that's kind of strange. Somalia doesn't have a government. They have much less
1139200	1142200	tyranny. You're much more you can just buy a rocket launcher and just like live
1142200	1144400	on a farm and just like, you know, kill your neighbors and no one's gonna stop
1144400	1148720	you. So like, how does that interact with your old you? Those who will trade
1148720	1151400	liberty for safety deserve neither.
1153200	1156000	That sorry, I don't understand. Could you elaborate a bit more?
1156240	1163520	Um, in Somalia, you have a chance in America, you do not, right? I am okay. I
1163520	1167160	would rather live in fear. I would rather be worried about someone shooting a
1167160	1172960	rocket launcher at me than to have an absolutely tyrannical government. Just,
1173120	1178480	you know, just just like like like a managerial class. Not saying, by the way,
1178600	1182120	I agree with you that these things are possible. I agree with you that the
1182120	1185120	less wrong notion of politics is possible. I would love to live in these
1185120	1191680	sort of worlds, but we don't. The practical reality of politics is so much
1191680	1197000	more brutal. And it just comes from a straight up instinct to dominate, not an
1197000	1201240	instinct, you know, government by the people for the people is branding.
1202040	1206480	Yeah, I mean, yeah. So to be clear, I very much do not agree with less wrongs
1206480	1210960	views and politics and a bit of an outcast for how I view how conflict theory
1210960	1214840	I view politics. But this is, I feel like you're kind of dodging the question
1214840	1217520	you're just a little bit. Is it like, well, if that's true, why aren't you
1217520	1218280	living in Somalia?
1220840	1225600	I know people who've done it, right? It's very hard. It's very hard psychologically.
1226120	1231120	Okay. So like tigers love charm, it turns out, right? A tiger does not want to
1231120	1234520	chase down an antelope, right? A tiger would love to just sit in the zoo and
1234520	1239160	eat the charm, right? And like, it takes a very strong tiger to reject that.
1239360	1242960	And I'm not that strong. I hope there's people out there who are. I hope there's
1242960	1247200	people out there who are actually like, you know, I'm just not a weak little
1247200	1250040	bitch. That's why I don't live in Somalia, right?
1250240	1255400	Okay. I mean, that's a fair answer, but I am a bit confused here. So you're
1255400	1260040	saying that living in Somalia would be better by some metric, but you're also
1260040	1265840	saying you prefer not living in Somalia. So I'm a bit confused because like, from
1265840	1269520	my perspective, I want to live in a country I want to live in. And that's the
1269520	1274440	one which I think is better. If I thought another country was better, then I would
1274440	1275000	just move there.
1275080	1278560	But let's, the tiger and the chum, I think, is a good analogy, right? Like, if
1278560	1282920	you have a choice as a tiger, you can live in a zoo and you get a nice size
1282920	1286600	pen, you know, the zookeepers are not abusive at all. You get fed this
1286600	1290720	beautiful chopped up food. It's super easy. You sit there, get fat, lays around
1290720	1296080	all day, or you can go to the wild. And in the wild, you're going to have to
1296080	1300760	hunt. You might not succeed at hunting. It is just a, you know, it's a brutal
1300760	1306680	existence. As a tiger, which one do you choose? Now, you say, Oh, well, obviously,
1306680	1310000	you know, you're going to choose the chum one. Yeah, but do you see what you're
1310000	1311960	giving up? Do you see?
1311960	1314240	No, no, could you elaborate a little bit on what I'm giving up?
1314520	1322680	You are giving up on the nature of tiger. You are effectively, okay, maybe I'll
1322680	1327960	take this to an extreme, right? In the absolute extreme, the country that you
1327960	1333120	would most rather live in is the one that basically wire heads you, right? The one
1333120	1336760	and you can say that, Okay, well, I don't want to be wireheaded, but you know,
1336760	1339880	there's a, there's a gradient that'll get you there, Gandhi in the pill, or, you
1339880	1345920	know, if you can live in this country, you can be happy, feel safe and secure all
1345920	1351920	the time. Don't worry exactly about how we're doing it, you know, but right. I
1351920	1357080	mean, it takes a very strong person to, it's going to take a very strong person
1357120	1363040	to say no to wireheading. So I understand. I'll give one more instrumental
1363040	1366240	reason for living in America versus living in Somalia. If I thought that
1366240	1370840	America and Somalia were both like steady states, I might choose Somalia. I
1370840	1374360	don't think that. I think that being here, I have a much better way of
1374360	1379640	escaping this, of escaping the constant tyranny that we're in. And I think a
1379640	1387160	major way to do it is AI. I think that AI is, is if I really, if I had an AGI, if
1387160	1390040	I had an AGI in my closet right now, I'll tell you what I'd do with it. I
1390040	1393440	would have it build me a spaceship that could get me off of this planet and get
1393440	1397200	out of here as close to the speed of light as I possibly could and put big
1397200	1400720	shield up behind me, blocking all communication. That's what I would do if I
1400720	1404480	had an AGI. And I think that's, you know, the right move. And I have a lot better
1404480	1408040	chance of building that spaceship right here than I do in Somalia. Right. So I'll
1408040	1409640	give an instrument. Yeah, that's right.
1410200	1413480	That's a good instrument. Well, we'll miss you if you leave, though. No, it'll
1413480	1418040	be real shame. It'll be everyone should do it. Like this is, this is the move,
1418040	1422360	right? And like, let humanity blow. I mean, look, I agree with you that we're
1422360	1427800	going to probably blow ourselves up, right? But I think that the path potentially
1427800	1432720	through this probably looks different from the path you're imagining. I think
1432720	1438320	that the reasonable position, I'm sorry. Oh, no, I think yeah, maybe we're done
1438320	1440760	with this point. I can come back and have a response to your first one. I would
1440760	1445160	like to, if you don't mind, just like fall on one string there as well. So one
1445160	1449960	of the things you said is like, what will the tiger choose? And so my personal
1450200	1454360	view of this kind of thing. And I think I want to think about coordination is I
1454360	1459840	think of things. So you put a lot of view on this like fear based domination and
1459840	1463040	so on. And I'm not going to deny that this isn't a thing that happens. I'm
1463040	1468480	German. You know, like, you know, I have living relatives who can tell you some
1468480	1473480	stories. Like I understand, like I understand. I'm not I'm not denying
1473480	1478960	these things might be needs. What I'm saying, though, is, okay, let's say
1480080	1483240	there was a bunch of tigers, you know, you and me and all the other tigers. And
1483240	1487720	some of the tigers are like, man, fuck, this whole like nature shit is like
1487720	1492040	really not working for me. How about we go build a zoo together? Who's in? And
1492040	1495320	then other people like, Yeah, you know, actually, that sounds awesome. Let's do
1495320	1498760	that. Do you think that's okay? Like you think that would be like a fair
1498760	1503560	option for them to do? Sure. But that's not where zoos come from. I know. I
1503560	1506680	know. I'm getting there. I'm getting there. Like, that is not where zoos come
1506680	1514200	from. Sure. But the this analogy here is, of course, is that this is where a lot
1514200	1517840	of human civilization comes. Not all of it. I understand that why France was
1517840	1520920	doing well in the First World War was not because of democracy big nice. It was
1520920	1526040	because democracy raises large armies. It's I'm very well aware of the
1526040	1531560	real politic, as the Germans would say, about these kinds of factors. And I
1531560	1536520	and I fully agree with you that a lot of the good things that we have are not by
1536520	1541080	design, so to speak. You know, there are happy side effects, you know, capitalism
1541080	1545640	is a credit assignment mechanism, you know, the fact that also results in us
1545640	1549080	having cool video games and air conditioning. It's not an inherent
1549160	1556360	feature of the system. It's it's an execution mechanism. And so totally
1556360	1560600	grant all of this. I'm not saying that every coordination thing is good. I'm
1560600	1563960	not saying that, you know, there aren't trade offs. Actually, you were talking
1563960	1566520	about, I think, aesthetic trade offs. You're like, there's an aesthetic that
1566520	1571480	the tiger loses. And well, I think personally, aesthetics are subjective. So
1571480	1574040	I think this is something that different people. So the way I think about
1574040	1578040	aesthetics is I think aesthetics are things you trade on is, you know, you
1578040	1582680	might want tigers in the wild to exist. Okay, fair enough. That's a thing you
1582680	1586760	can want. You know, someone else might want, you know, certain kinds of art to
1586760	1592360	exist. They might want a certain kind of religion to be practiced or whatever.
1592360	1595800	These are aesthetic preferences upon reality, which I think are very fair.
1595800	1599640	So the way I personally think about this morally is I'm like, okay, cool.
1599640	1603400	How can we maximize trade surplus so you can spend your resources on the
1603400	1608440	aesthetics you want and I'll spend my resources on the, you know, things I
1608440	1611880	want. Now, maybe the thing you describe where
1611880	1615320	everyone just atomizes into their own systems, with their own value system,
1615320	1618920	with their own aesthetics, completely separate from other, is the best outcome.
1618920	1621400	Awesome. I think this is completely...
1621400	1624440	Have you heard the Yodobar manifesto?
1624440	1629880	I have not. You sure? The problem with this, everyone trades on their own
1629880	1634280	aesthetics, is you will never be able to actually buy any aesthetics that are in
1634280	1640680	conflict with the system, right? You won't. The system won't let you.
1640680	1646360	Okay, by that logic, why do people have free time?
1646360	1649160	Why don't they work all the time? Why doesn't capitalism extract literally
1649160	1652680	every minute of them? Why do you think that is?
1653480	1656840	I think it's because it turns out that we don't actually live in a capitalist
1656840	1660920	society. I think China is a lot closer to a capitalist society than America.
1660920	1664760	I think America is kind of communist and I think in a communist society, of
1664760	1667480	course, you're going to get free time. It turns out that subsidizing all the
1667480	1671320	homeless people is a great idea, right? If you want to keep power, again,
1671320	1675000	do some absolute tyrannical mechanism. You do it, right?
1675000	1679480	So why do we have free time? Well, you think it's some victory of capitalism.
1679480	1682440	I think it's because we do not live in a capitalist country. I think China is more
1682440	1685960	capitalist than America. I think it's because we trade on our aesthetics.
1685960	1688520	I think that different people have different things to contribute to
1688520	1691400	various systems, not necessarily capitalist or communist thing.
1691400	1695560	I'm saying it's more energy. In the primordial environment, if you have
1695560	1697720	to fight literally every single second and spend every
1697720	1701240	jewel of energy you have to scrounge together another jewel of energy,
1701240	1704360	you can't have free time. It's not about capitalism. This is about
1704360	1708280	entropy. This is about these kind of things. We have energy excess.
1708280	1712520	We have, we've produced systems that allow us to extract more energy for
1712520	1717640	jewel we put in. And we can spend that extra energy on things such as free time.
1717640	1723080	And the distribution of energy, power, coordination, whatever you want to call it
1723080	1725880	is another question. Will you agree or disagree with this?
1725880	1729720	I mean, I am taking an extreme position when I say that there are definitely
1729720	1733720	positive sum coordination problems that are solved by governments, right?
1733720	1738920	It is not all zero sum or negative sum, right? I'm not denying this.
1738920	1745560	But what I'm saying is it's like, I don't know, man, like the existence of free time.
1745560	1750040	Well, that's all great when you think you live in this surplus energy world, right?
1750040	1754920	And maybe we do right now. But if some other country took this seriously, like China,
1756040	1759880	who's going to win in a war? Who's going to win? Is it going to be the Chinese?
1759880	1764040	You guys see the Chinese build a building? They got like 400 people there,
1764040	1767240	and they're all there 24 hours a day, and they're getting the building built.
1767240	1770600	You ever see Americans build a building? It's six guys, two of them are working,
1770600	1773160	two of them are shift supervisors, and two of them are on lunch breaks.
1773960	1776520	Oh, you got your free time. You got your aesthetic preferences.
1776520	1780200	You deserve to lose in a war, right? This country deserves to lose in a war
1780200	1781640	if they keep acting the way they're acting.
1782200	1786200	So I definitely see the point you're making. And this is personally not a thing I want to
1786200	1793000	defend too far because I'm not a military expert. But I will note that the US has like
1793000	1796920	37 aircraft carriers, and the Chinese have like two, and Americans are like,
1797480	1801960	somehow, you know, despite being so lazy and oh, no, they have all this, you know,
1801960	1805640	all this free time or whatever. Somehow they're still a military hegemon or whatever.
1805640	1809880	And like they're the biggest rival Russia fighting this backwards water country in
1809880	1813320	Ukraine suddenly folds and lose like three quarters of the military.
1813320	1820360	It's what I'm saying is if you have massive hegemony, if you have truly a obnoxious victory,
1820360	1824600	the way it should look is if you laze around all the time and you look like a fucking idiot and
1824600	1831320	you still win. Yes. And I'm not talking about Russia. Russia has a GDP the size of Italy.
1831320	1836680	This is China here. You might say that China has two aircraft carriers in the US has 37.
1836680	1840520	Why do we have aircraft carriers? Who has more drone building capacity?
1840520	1846200	The Chinese are the United States. If the future is fought with AI swarm drone warfare,
1846200	1850120	the Chinese can make, you know, a million drones a day in the US can make.
1850840	1852840	I don't even know. I think we buy them from China.
1854360	1857800	Well, I'm not an expert on these kind of logistics. I think I would like to
1858440	1861800	get back to kind of like the more general. Let's move on from that.
1861800	1865720	I am not either, but I do believe the Chinese have more manufacturing capacity than the United
1865720	1871720	States. It seems completely plausible to me. I think things are not lazy and they don't sit
1871720	1874600	around and have all this free time and aesthetic preferences or something.
1874600	1879320	I'm a believer that work is life. I mean, at least from my Chinese friends,
1879320	1883240	I know the Chinese sure do have a lot of inefficiencies. It's just called corruption.
1883960	1887880	Oh, America has corruption too. You see. Oh, yeah, sure. Well, in Mexico,
1887880	1892840	the corruption is you have to pay 20 cents to get 20 cents on every dollar for the building
1892840	1898040	you built, right? Whatever. In America, every dollar is spent absolutely on that building.
1898040	1902120	You know, we know that because we spent $4 making sure that that first dollar was not
1902120	1908600	spent correctly. I'm well aware of that. Anyways, I think we mostly agree on this
1908600	1914040	point actually. And I think it's a matter of degree. What I want to say just for the record,
1914040	1920040	the U.S. is a uniquely dysfunctional system in the West. I'm German and the German system is
1920040	1924360	very dysfunctional, but it's like nothing compared to how dysfunctional the U.S. is.
1924360	1928280	Fully agree with that. I don't think we disagree on that. I think it's a matter of
1928280	1933080	degree more so than anything. I agree. We've had a comment saying someone's turned the
1933080	1937320	temperature up a bit too much on the language model. So let's bring it back a tiny bit to
1937320	1942120	AI safety. But that was a great discussion. Got it. I will end with saying I love America.
1942120	1945400	I am happy to live here. And there are a lot of things I appreciate about American society.
1947560	1950360	Great. So do you want to return to like the technical topics or?
1951080	1954840	Yeah, I think I can return to your first point. And maybe I'll just start with a question.
1954840	1956280	Do you think there's going to be a hard takeoff?
1957880	1959560	I don't know, but I can't rule it out.
1962040	1964280	I can't see how that would possibly happen.
1966840	1970920	I have a few ideas of how it could happen, but I don't. It's unlikely. It seems like not.
1971720	1976680	The way I think it could happen is if there are just algorithms, which are like
1978120	1981800	magnitudes of order better than anything we ever have. And like the actual amount of
1981800	1986440	computing you get human is like, you know, a cell phone or, you know, like, and then this
1986440	1991880	algorithm is not deep in the tech tree. We just happen to have not picked it up. And then an
1991880	1994680	AGI system picks it up. This is how I think it could happen.
1995640	2000360	Okay, yes, I agree that something like this is potentially plausible where you're saying basically
2000360	2007560	like the God Shatter is already distributed. It's not a question. It's using all the existing
2007560	2011480	compute in the world today. It just turns out it was 10,000x more effective or a millionx more
2011480	2016920	effective than we thought. Yeah, this is seems the most plausible way to be. Or, you know, you mix
2016920	2020040	lead and, you know, copper and you get a superconductor, you know, something like that.
2021400	2025720	Even that. I know, I know, I'm not joking. It's going to take so many years to like,
2025720	2030280	it's not about the discovery, right? Give it 10 years to productionize it, scale up processes,
2030280	2034280	right? Like these things are, you know, this is something running a company's really taught me,
2034280	2040840	like it's just going to take a long time. And this is really like, like kind of where my,
2040840	2045320	I just don't believe in a hard takeoff. I think that they'll be, this is a gasketing I like,
2045320	2048600	he's a hardware and software progressive, quite similar speeds. And you can look at
2048600	2053560	factoring algorithms to show this. So it would shock me if there were some, you know, 10 to the
2053560	2060200	six, 10 to the nine magical improvement to be had. It seems plausible to be like a hard takeoff is
2060200	2065720	definitely not my main line scenario, my main line scenario. Well, I don't know, maybe you
2065720	2068760	wouldn't consider this a hard, maybe you would consider as a hard takeoff. This is what I would
2068760	2074120	describe as a soft takeoff is something like, sometimes the way I like to define AGI is say,
2074120	2079400	it's something that has the thing that chimps, that chimps don't have and humans do have.
2079400	2085720	Yeah. So chimps don't go a third to the moon, you know, despite their brain being a third of our
2085720	2090680	size. So we scaled up things by a factor of three of a primate brain, roughly four or something
2090680	2094840	like that. And like most of the structures, I'm sure some micro tweaks and whatever, but like
2094840	2099000	not massive amount of evolutionary pressure, like we're very, very similar to chimps.
2099000	2105720	And somehow this got us from, you know, literally no technology to space travel in a,
2105720	2112200	you know, evolutionary, very small pair of time. It seems imaginable to me that something similar
2112200	2117160	could happen with AI. I'm not saying it will, but like seems imaginable.
2117160	2123800	Yeah. So I agree with this. I'll come to your point about, you know, you had two regulatory
2123800	2131720	points, one of them about capping the max flops. And I actually kind of agree with this. I do think
2131720	2137720	that things could potentially become very dangerous at some point. I think your numbers are way,
2137720	2143720	way, way too low. I think if your numbers are anywhere near GPT-3, GPT-4, okay, great. We got
2143720	2148680	a lot of, we got a lot of fast moving guys who work on fiber, even if you start to get Von Neumanns.
2149320	2155560	Right? We're not talking about a humanity's worth of compute. We're talking about things on par with
2155560	2161720	a human and a few humans, right? Yeah, they'll run fast, but they're not. Like things get scary
2161720	2167160	when you can do a humanities training run in 24 hours. Like we're about to burn the same compute
2167160	2173320	that all 2 million years of human civilization burned. Okay. Now I don't know what starts to
2173400	2178360	happen. Or I'll put this kind of another way. Language models, I look at them and they don't
2178360	2184760	scare me at all because they're trained on human training data, right? These things are not, like,
2184760	2191560	if something was as good as GPT-4 that looked like Mu Zero, where it trained from some simple rules,
2192280	2197000	okay, now I'm a bit more scared. But when you say, okay, it's, you know, we're feeding the whole
2197000	2200440	internet into the thing and it parrots the internet back, mushed around a little bit,
2200440	2203160	that looks very much like what a human does. And I'm just not scared of that.
2204280	2209400	Yeah, it's very reasonable, whatever. Like, I'm not scared of GPT-4 to be clear. Like, I think
2209400	2215000	there is like 0% chance or like, you know, epsilon chance that GPT-4 is existentially dangerous by
2215000	2222680	itself. You know, maybe some crazy GPT-4 plus RL plus Mu Zero plus something, something maybe.
2222680	2227160	But I definitely agree with you here. I don't expect GPT-3 or 4 by themselves to be dangerous.
2227160	2230600	These are not, I'm much closer to, I think, what you're saying, like, yeah, if you had a Mu Zero
2230600	2236200	system, they'd bootstrap yourself to GPT-4. Holy shit, like, we're big, we're big shit if we get
2236200	2241960	to that. Then we should, let's stop. Let's stop. Yeah, let's stop. So, I'm very happy to get,
2241960	2246280	to be into a regime where we're like, okay, let's find the right bound. Like, I think this is an
2246280	2249800	actually good argument. I think this is actually something that should be discussed, which is not
2249800	2254440	obvious. And I could be super wrong about that. So I'd like to justify a little bit about why I
2254440	2259160	put such a small bound. But I think your arguments you're making for the higher bounds are very
2259160	2263080	reasonable, actually. I think these are actually good arguments. So just to justify a little bit
2263080	2269160	about why I put such a low bound, the boring default answer is conservatism. It's like, if all of
2269160	2274200	humanity is at stake, which, you know, you may not believe, I'm like, whoa, whoa, okay, at least
2274200	2281160	give us a few years to like, more understand what we're dealing with here. Like, I understand that,
2281160	2286280	you know, you may disagree with this. Very plausible. But I'm like, whoa, like, you know, at least,
2286280	2291320	let's let's like, by default, let's hit a pause button for like, you know, a couple years until
2291320	2295640	we figure things out more. And then if we like, find a better theory of scaling, we understand how
2295640	2300440	intelligent scales, we understand how mu zero comes, blah, blah, blah. And then we pick back up after
2300440	2305960	we're like, you know, we make huge breakthroughs in alignment. And Eliezer is, is crying on CNN and
2306040	2312680	like, oh, we did it, boys. I mean, then okay, sure, you know, okay. So that's the one, like,
2312680	2317080	kind of more boring argument, like, that's kind of a boring argument. The more interesting argument,
2317080	2323960	I think, which I think is a bit, you know, or skit so, is that it's not clear to me that you can't
2323960	2328920	get dangerous levels of intelligence with the amount of compute we have now. And one of the reasons
2328920	2334040	that I'm unsure about this is because man, GPT three GP four is just the dumbest possible way to build
2334040	2340200	AI. Like, it's just like, like, there's like no dumber way to do it. Like, it's it works and dumb
2340200	2347080	is good, right? You know, better lesson dumb is good. But look at humans. You said, as we talked
2347080	2352840	about before, you know, human today, human 10,000 years ago, not that different. You place both of
2352840	2358200	them into a, you know, workshop with tools to build, you know, any weapon of their choice, which of
2358200	2363400	them is more dangerous? Obviously, you know, one of them will have much better, you know,
2364200	2371800	capacities to deal with tools, to read books, to think about how to design new weaponry, and so
2371800	2377880	on. These are not genetic changes, they are epistemological changes, they are memetic changes,
2377880	2382360	they are software updates, you know, humans had to discover rational reasoning, like, you know,
2382360	2386680	before like, you know, I mean, you know, obviously, people always had like, you know, folk conceptions
2386760	2392360	of rationality. But it wasn't like a common thing to think about causality and like, you know,
2392920	2397880	you know, rational, like, you know, if then else kind of stuff until relative, you know,
2397880	2403480	like philosophers in the old ages and only became widespread relatively recently. And these are useful
2403480	2408680	capabilities that turned out to be very powerful and took humans many, many thousands of years to
2408680	2412760	develop and distribute. That's good. And I don't think humans are anywhere near the level. I think
2412840	2416760	the way we could do science right now is pretty awful. Like, it's like the dumbest way to do
2416760	2423960	science that like, kind of still works. Like, you know, and I expect it's like possible that if you
2423960	2429480	had a system which like, let's say it's like, smaller brain than the human even, but it has
2430120	2434600	really, really sophisticated epistemology. It has really, really sophisticated theories of
2434600	2441000	meta science. And it never tires, it never gets bored, it never gets upset, it never gets distracted,
2441000	2445960	and it can like, memorize arbitrary amounts of data. This is something that I think is within
2445960	2451480	the realm of like a GPT three or four training run to build something like this. And it is not
2451480	2456760	obvious to me that this system could not outfind humanity. Maybe not, like maybe not, but it's
2456760	2462120	not obvious to me that it keeps. So just carry on. So what do you think of that? So to your first
2462120	2468840	point, why I stand against almost all conservative arguments, you're assuming the baseline is no
2468840	2474920	risk, right? And oh, well, why should we do this AI? We should wait and bring the baseline back. No,
2474920	2481000	no, no, no. We are about to blow the world up any minute. There's enough nuclear weapons aimed at
2481000	2486600	everything. This is wearing some incredibly unstable precarious position right now. Like,
2486600	2490840	people talk about this with with car accidents. You know, this is comma, like, people are like,
2490840	2495960	oh, well, you know, if your device causes even one accident, I'm like, yeah, but what if statistically
2495960	2500920	there would have been five without the device? I'm like, you do have to understand the baseline
2500920	2505800	risk in cars is super high. You make it five x safer, there's one accident, you don't like that.
2505800	2512760	Okay, I mean, you have to be excluded from any polite conversation, right? So yeah, like, I think
2512760	2520760	that calling for a pause to the technology is is worse, right? I think given the two options,
2520760	2525480	if we should pause or we should not pause, I think pausing actually prevent presents more risk. And
2525480	2529240	I can talk about some reasons why again, the things that I'm worried about are not quite
2530120	2539160	the existential risks I have to the species are not AGI goes rogue, they are government gets control
2539160	2545480	of AGI and ends up in some really bad place where nobody can compete with them. I don't think these
2545480	2550920	things look unhuman. These things to me like, I see very little distinction between human
2550920	2556600	intelligence and machine intelligence, it's all just on a spectrum. And like, they're not,
2558600	2563320	like, to come to the point about, okay, but GPT four could be like this hyper rational,
2563320	2569720	never tiring humans are doing science in the dumbest way. I'm not sure about that, right?
2569720	2574280	Like, I think that, you know, when you look at like, okay, okay, we have chess bots that do
2574280	2578200	way better. And all they do is think about chess, haven't really done this with humans,
2578200	2582760	people would call it an ethical, right? Like, if we really told a kid, like, if we really just
2582760	2586840	like, every night, we're just putting the chess goggles on you, and you're staring at chess
2586840	2591800	boards, and we're really just training your neural net to play chess. I think humans could
2591800	2598760	actually beat a computer again, a chess, if we were willing to do that. So yeah, I don't think
2598760	2603000	that this stuff is that particularly dumb. And I think, okay, maybe we're losing 10x, but we're
2603000	2610600	not losing a million x. Again, I don't see a, I do the numbers out all the time for when we're
2610600	2615560	going to start to get more computer, you know, when will a computer have more compute than a human?
2615560	2620600	When will a computer have more compute than humanity? And yes, these things get scary,
2620600	2624520	but we're nowhere near scary yet. We're looking at these cute little things. And
2625400	2633400	these things, by the way, do present huge dangers to society, right? The the PsyOps that are coming.
2634200	2639400	Right now, you assume that like, when you call somebody, that you're at least wasting their
2639400	2644760	time too. But we're going to get like heaven banning. I love this concept, which is, you know,
2644760	2648840	yeah, yeah, came up on Luther AI, like that's where it comes from was the guy on the Luther
2648920	2655560	AI that came up with that word. Yeah, I know the guy came up with it. I love, I love this
2655560	2662120	concept. And I think there's also a story, my little pony friendship is optimal that God that
2662120	2672200	goes into the concept. And yeah, so I think that like, my girlfriend proposed a, I don't want to
2672200	2679240	talk to Oh, I say you don't want to talk to your relative anymore, right? Okay, we'll give them
2679240	2686120	an AI version to talk to, right? Yeah. Yeah. So like, this stuff is coming and it's coming soon.
2686120	2691880	And if you try to centralize this, if you try to, you know, say like, Oh, okay, Google Open AI,
2691880	2695000	great, they're not aligned with you. They're really not Google has proven time and time again,
2695000	2698440	they're not aligned with you. Meta has proven time and time again, they're trying to fix it.
2698680	2705560	Yeah. I mean, I fully agree with you. Like, I like that you bring up PsyOps as the correct
2705560	2710120	example in my opinion of short term risks. I think you're like, fully correct about this.
2710120	2716840	Like, when I first saw like, GPT models, I was like, holy shit, like the level of control I can
2716840	2722360	gain over social reality, using these tools at scale is insane. And I'm surprised that we haven't
2722440	2728360	seen yet the things that like, augured in my visions of the day. And we will, like we will,
2728360	2735560	obviously it's coming. And this is, so I think this is a very, very real problem. Yeah. Like,
2735560	2740840	I think if we, even if we stop now, we're not out of the forest. So like, when you say like, I
2740840	2745320	think the risk is zero, please do not believe that that is what I believe, because it is truly not.
2745320	2750920	It is truly, truly not. I think we are like, we are really in a bad situation. We are in a, we are
2750920	2756680	being, we're under attack from like so many angles right now. And this is before we get into, you
2756680	2760840	know, like, you know, potential, like, you know, climate risks, nuclear risk, whatever, we're in
2760840	2767320	under room of medic risk, like the, the then dangers of our like epistemic foundations are under
2767320	2774040	attack. And this is something we can adapt to, right? Like, you know, we did, you know, when a
2774040	2779320	good friend of mine, he's a, he's quite well read on like Chinese history. And he always like, it
2779320	2783080	tells me his great story. So I'm not a historian. So please, you know, don't crucify me here. But
2783080	2787400	like, he tells these great stories about when Marxist memes were first introduced to China.
2787400	2791240	And like, this is where a world where like, just like all the precursor memes didn't exist.
2791240	2795800	That's just like, kind of was air dropped in and people went nuts. People went just completely
2795800	2800440	crazy because there was no memetic antibodies to these like hyper virulent memes that were,
2800440	2804680	you know, created by evolutionary pressures and like, you know, Western university departments,
2804680	2808920	like really, you could call philosophy department just gain a function, the medic laboratories.
2809960	2816920	I like that. Yeah. I mean, like, you know, like without being, you know, political or any means
2816920	2820760	there, a lot of what these organizations do. And like, you know, other, you know, what other,
2820760	2826120	you know, memetic, like, you know, if philosophy departments are the, like, gain a function
2826120	2831160	laboratories, then like 4chan and tumbler are like the bat caves of needs, you know, like the
2831160	2837320	Chinese bat cave. And I remember this vividly. I was like on tumbler and 4chan, like when I was a
2837320	2842440	teenager, and then suddenly all the like weird, bizarre, you know, internet shit I saw started
2842440	2848200	becoming mainstream news. My parents were watching in 2016. And I was like, what the hell is going
2848200	2853080	on? Like, I already developed antibodies to this shit. Like I already, you know, both right and
2853080	2858280	left, I was already like, I already immunized all this. So I fully agree with you that this is like
2858280	2864200	one of the largest risks that we are facing is this kind of like memetic mutation load, in a sense.
2864200	2870360	And I'm not going to say I have a solution to this problem. I'm like, I have ideas, like there's a
2870360	2875320	lot of like things you can do to improve upon this. Like if AI was not a risk, and also not climate
2875320	2879800	change and whatever, this might be something I work on, like epistemic security, this might be
2879800	2884120	something I would work on, like how can we build better coordination, like like just scalable
2884120	2889000	rationality mechanisms, stuff like prediction markets and stuff like this. I don't know. But
2889000	2895000	sorry, going off track here a little bit. Well, no, actually, I really agree with a lot of the
2895000	2899080	stuff you said. And I had a similar experience with the antibodies and people are exposed to this
2899080	2908760	stuff. And I'm like, yeah, this got me like four years ago. Yeah. So I think that there is a solution
2908760	2914600	and I have a solution. And the answer is open source AI. The answer is open source. Let's say
2914600	2918280	even you can even dial it back from like the political and the terrible and just straight up
2918280	2923560	talk about ads and spam, or maybe spam, just straight up spam, I get so much spam right now.
2923560	2928520	And it's like, it's kind of written by a person. It's like targeting me to do something. And
2928520	2933400	Google spam filter can't even come close to recognizing it. Right. Like what I need is a
2933400	2940040	smart AI that's watching out for me that is just it's not even targeted attacks at me. It's just
2940040	2947080	so much noise. And I don't see a way to prevent this. Like the big organizations, they're just
2947080	2951800	going to feed you their noise, right? And they're going to maximally feed you their noise. The only
2951800	2956040	way is if you have an AI, like, I don't think alignment is a hard problem. I think if you own
2956040	2960520	the computer and you run the software, if you develop the software, the AI is aligned with you.
2960520	2968360	Oh, yeah. Can you, okay, if I challenge you, George Haas, here is a llama 65b model when we
2968360	2973240	could appear to run it on, make it so it and make, yeah, you know, sure. Okay, you developed AI. I
2973240	2979320	give you the funding for your time. Can you develop a model that is as good as llama 64b 65b
2979320	2983720	and is immune, like completely immune to jailbreak. It cannot be jailbroken. No.
2984760	2990760	Why not? It's aligned, isn't it? Well, no, but this isn't what alignment means. Well, my values is
2990760	2995640	do not get jailbroken. Oh, okay, you're talking about unexploitability. This is not alignment,
2995640	2999880	right? Okay, okay, interesting. I didn't know you would separate those. So I extremely separate
2999880	3007240	those, right? Okay, I mean, in the default case, it like, like it, it's on my side, right? Okay,
3007240	3013480	unexploitability is not a question of whether it's okay. And this is a true thing about people too.
3013480	3018840	Whenever I look at a person, I ask, okay, is this person, I want something for me. Is this person,
3018840	3024840	does this person want it too? And is this person capable of doing it? Right? And I really separate
3024840	3029320	those two things. I can build a system. I don't, I'm not worried about the first one with the AI
3029320	3033800	system is worried about the second one. Can it be gamed? Can it be exploited? Sure, I could tell
3033800	3039160	like, you know, like, like, say it was just playing chess, right? And it loses. I'm like, don't lose.
3039800	3046280	Okay, I didn't want to, man, I didn't want to lose. I'm sorry. I know. But like, so yes, yes, can I build
3046280	3051160	a aligned system? Sure. Can I build an unexploitable system? No, especially not by a more powerful
3051160	3055880	intelligence. Interesting. Interesting. So this is an interesting, I think you're, you're, you're
3055960	3061000	pointing to actually a very important part of this, is that like, exploitability and alignment
3061000	3065560	can get fuzzy? Like, which is which? Like, did it fail because of its skill set? Or because it's not
3065560	3069160	aligned? It's actually a very deep question. So I think, I think you make a good point for like,
3069160	3076280	you know, talking about these two separately. I guess the, so the thing I want to dig in just
3076280	3084840	like a little bit more on this idea is there are, there's two ways, there are two portals
3084840	3089800	through which, you know, the memetic demons can reach into reality, humans and computers.
3090600	3094680	Why do you think your AI is immune to memes? Why, why can't I just build
3094680	3100200	AIs that target your AIs? What like you don't, I don't think my AI is immune to memes at all.
3100920	3106440	I think that the only question is, and I really like your game, like these, these NGOs are doing
3106520	3116920	gain of function on memes, right? Wear a mask. Like, a weaker intelligence will never be able
3116920	3122760	to stand up to a stronger intelligence. So from this perspective, if this is what's known as alignment,
3122760	3127960	I just don't believe that this is possible, right? You can't, you can't keep a stronger
3127960	3132280	intelligence in the box. This is, this is, I agree with you, Kowsky in the box experiments,
3132280	3137560	like the AI is always going to get out. There's no keeping it in the box, right? This is, this
3137560	3143560	is a complete impossibility. I think there's only two real ways to go forward. And one is
3143560	3150600	Ted Kaczynski. One is technology is bad. Oh my God, blow it all up. Let's go live in the woods,
3150600	3155400	right? And I think this is a philosophically okay position. I think the other philosophically okay
3155400	3160680	position is something more like effective accelerationism, which is look, these AIs are
3160680	3167960	going to be super powerful. Now, if you have one, it could be bad. But if super intelligent AIs are
3167960	3172520	all competing against each other, magnetically, like we have something like society today, just
3172520	3177240	the general power levels have gone up. This is fine as long as these things are sufficiently
3177240	3182920	distributed, right? Like, sure, this AI is not perfectly aligned, but you know, there's a thousand
3182920	3186760	other ones and like you have to assume they're all basically good because if they're all basically
3186760	3192200	bad, well, we're dead anyway. I mean, why wouldn't you expect that? That they're all bad? Yeah.
3193000	3198520	Well, or what do you think of humans? Are most humans good? I think the concept of good doesn't
3198520	3203160	really apply to humans because humans are too inconsistent to be good. Like by default, they
3203160	3208120	can be good in various scenarios in various social contexts. Like give me any human and I can put them
3208120	3213560	into a context where they'll do an arbitrarily bad thing. And this is true about a llama as well,
3213560	3217560	right? Lamas are completely inconsistent. I think they're actually more inconsistent than humans.
3218360	3222840	Right. I see. I wouldn't trust llamas to be good. Well, yeah, but I wouldn't think that they're
3222840	3226600	bad either. I would think they have the exact same inconsistency problem as humans. And I think
3226600	3232920	almost any AI you build is going to run into the same problems, right? Yeah, I think so. That's my
3232920	3238440	point. So your assumption can't rely on them being good because you don't get that for free.
3238440	3241640	Like, where does that come from? My assumption is not that they're good. My assumption is that
3241720	3246440	they're not bad. But inconsistent is fine. As long as we have a ton of them and they're all
3246440	3250440	inconsistent and they're pulling society in every which direction, you don't end up paper-clipped,
3250440	3256520	right? Why not? Well, because what? They're all going to coordinate and agree to paper-clip you?
3256520	3261320	No, no, they'll just do some random bullshit. And then that random bullshit will not include
3261320	3265000	humans. They're all doing random bullshit, right? You're going to have, let's say the liberals
3265000	3268360	decide we're going to paper-clip people. The conservatives are going to come out very strongly
3268360	3274440	against paper-clipping, right? And you're just going to end up with these sort of dynamics,
3274440	3278040	like forever. You're going to have some AIs who are like, yeah, we don't like it, you know?
3280040	3286920	So this is perplexing to me because in my view, which I think I heard you agree with earlier,
3286920	3291800	it's like the world is unstable. The world is very unstable. And it seems to me you're suggesting
3291800	3297320	if we increase the chaos, if we have more entities fighting, doing more different things
3297320	3301880	with more energy, that makes it more stable. Is that correct? Yes. The world has actually
3301880	3305400	become very stable in the last 100 years. And I'm scared of how stable it's become.
3307000	3310600	You know, again, thank you for China. How many sovereign countries are there in the world?
3312360	3318920	I mean, you know, 190 something. That's the UN number, if you believe them. I think there's
3318920	3323560	the US and China and maybe Russia, mostly they just have a veto button.
3324360	3330040	All right. So I think there's more like two and a half. Like this isn't a lot of entities fighting
3330040	3335160	it out chaotically. A lot of entities fighting it out chaotically would be intelligences spread
3335160	3339400	across the entire galaxy, right? And that's the other, that's the other beautiful thing too.
3340200	3344040	Sooner we get off this planet, the sooner we get things that are actually a light year away.
3344600	3347640	Well, I think the speed of light's real. It's going to take you a year to send them there.
3348520	3352760	Memes, right? You want to get away from the memes? There's only one way.
3354760	3358920	For the record, I would like to go on the record and say, if any future transhumanists
3358920	3361960	or whatever want to blast themselves into space and go do their own thing,
3361960	3366120	I support their right to do that. And I would love to give this right to people.
3366120	3370760	The number one thing I want from countries is the ability, the right to leave.
3370760	3372680	This is what I would love. This is what I love about companies.
3372680	3375480	Free Exit. You're talking neo-reaction talk.
3376280	3380280	Yeah. Free Exit is extremely important. I would not describe myself as neo-reactionary, please,
3380280	3386360	because I'm not that gay. I wouldn't describe myself that way either,
3386360	3392760	but I've heard a lot of good ideas from them. But yeah, that being said, I do think that,
3393480	3397720	what I want, I think, let's ground the conversation a little bit here.
3400200	3402760	I'm very enjoying this conversation. I love talking to these philosophical points.
3402840	3407640	These are really good points, really interesting. But ultimately, as we also get to the latter
3407640	3413320	third of this conversation, the thing I really care about is strategy. The thing I really care
3413320	3418200	about is reality politic. I really care about, okay, what action can I take to get to the features
3418200	3424840	I like? And I'm not going to be one of those galaxy brain fucking utilitarians like, well,
3424840	3429080	actually, this is the common good. No, no, this is what I like. I like my family. I like humans.
3429240	3436680	Yeah, it's just what it is. I'm not going to justify this on some global beauty,
3436680	3442040	whatever. It doesn't matter. I want to live in a world. I want a 20-year time, 50-year time.
3442040	3448440	I want to be in a world where my friends aren't dead and where I'm not dead. Maybe we are cyborgs
3448440	3453080	or something, but I don't want to be dead. What I really care about, ultimately, is how do I get
3453080	3458280	those words? And I want us all to not be suffering. I don't want to be in war. I want us to be in a
3458280	3463720	good outcome. So I think we agree that we would both like a world like this. And I think we probably
3463720	3467240	disagree about how best to get there. And I'd like to talk a little bit about what can we,
3468520	3473160	what should we do and why do we disagree about what we do? Does that sound good to you?
3473160	3476120	Maybe I'll first propose a world that meets your requirements.
3477320	3482200	And you can tell me if you want to live in it. So here's a world. We've just implanted electrodes
3482200	3487560	in everyone's brain and maximized their reward function. I would hate living in a world like
3487560	3491000	that. Yeah, but no one, it meets your requirements, right? Your friends are not dead.
3491720	3493480	No one's suffering and we're not at war.
3494600	3499800	That is true. There are more criteria than just that. The true, the criteria I said is things I
3499800	3503880	like. As I said, I'm not a utilitarian. I don't particularly care about minimizing suffering
3503880	3509720	or maximizing utility. What I care about is various vague aesthetic preferences over reality.
3510440	3514120	I'm not pretending this is, as though that was the whole spiel I was trying to make,
3514120	3519320	is that I'm not saying I have a true global function to maximize. I say I have various
3519320	3524200	aesthetics. I have various meta-preferences of those aesthetics. I'm not asking for a global one.
3524200	3529240	I'm asking for a personal one. I'm asking for a personal one that you don't care about the rest
3529240	3534200	of the world. I gave you mine. I gave you what I would do if I had an AGI. Yep. So I'm getting
3534200	3540360	on this rock speed of light as fast as I can. Fair enough. I think if that is, I would like to live
3540360	3545400	in a world where you could do that. This would be a feature of my world. A world where I would be
3545400	3553160	happy is a world in which we coordinated at larger scales around building aligned AGI that could then
3553160	3563160	distribute intelligence and matter and energy in a well-value handshaked way between various people
3563160	3568120	who may want to coordinate with each other, may not. Some people might want to form groups that
3568120	3572680	have shared values and share resources. Others may not. I would like to live in a world where that
3572680	3576680	is possible. Have you read Metamorphosis of Prime Intellect? I have, unfortunately, not.
3579640	3582680	Yeah. I was going to ask if you're happy with that world, right? Like,
3582680	3590520	unfortunately, don't know it. I mean, it's as simple to describe. Singleton AI that basically
3590520	3597000	gives humans whatever they want, like maximally libertarian, you know, you can do anything you
3597000	3602520	want besides harm others. Is that a good world? Probably. I don't know. I haven't read the book.
3602520	3606840	I assume the book has some dark twist about why this is actually a bad world. Not really. Not
3606840	3612680	really. I mean, the plot is pretty obvious. You are the tiger reading chum, right? Sure, but you
3612680	3616440	can then just decide if that is what you want, then you can just return to the wilderness. That's the
3616440	3622280	whole point. Yeah, but can you? Can you really return to the wilderness, right? Like, you think
3622280	3626600	that like, I don't think we have free will. I don't think you ever will return to the wilderness.
3626600	3633000	I think a large majority of humanity is going to end up wireheaded. Yeah, I expect that too.
3633000	3636440	Okay, great. And this is the best possible outcome, by the way. This is giving humans exactly what
3636440	3643240	they want. Yeah. Well, to be clear, I don't expect it's all humans. I truly do not. I don't think
3643240	3647880	it's all humans either. I think a lot of humans have metapreferences over reality. They have
3647880	3651480	preferences that are not their own sensory experiences. This is the thing that the utilitarians
3651480	3658280	get very wrong is that many human preferences are not even about their own sensory inputs.
3658280	3663400	They're not even about the universe. They're about the trajectory of the universe. They're
3663400	3670840	about for the utilitarianism, you know, and a lot of people want struggle to exist, for example.
3670840	3678040	They want heroism to exist or whatever. I would like those values to be satisfied to the largest
3678120	3682440	degree possible, of course. Am I going to say I know how to do that? No. Which is why I kind of
3682440	3690040	like didn't want to go this deep, because I think if we're arguing about, oh, do we give them, you
3690040	3696440	know, for utilitarianism versus libertarian, utopia versus whatever, I mean, we're already like
3697160	3702920	10,000 steps deep. I'm asking about you. I'm not asking about them. I'm asking about a world
3702920	3708040	you want to live in. And this is a really hard problem, right? Yeah. And this is why I just
3708040	3716760	fundamentally do not believe in the existence of AI alignment at all. There is no like what values
3716760	3723480	are we aligning it to, whatever the human says, or what they mean, or like. Sure, sure. But like,
3723480	3728280	my point is I feel we have wandered into the philosophy department instead of the politics
3728280	3734200	department. Okay, like, it's like, I agree with you, like, do human values exist? What does exist
3734200	3737240	to me? But like, by the point you get to the point where we're asking what does exist to me,
3737240	3743080	you've gone too far. I'll respond concretely to the two political proposals I heard you state on
3743080	3749000	Bankless. Sure. I'd love to talk about them. One is limiting the total number of flops.
3749880	3754600	Temporarily. Temporarily, yes. And when I have a proposal for that, but I don't want to set a
3754600	3759640	number, I want to set it as a percent. I do not want anybody to be able to do a 51% attack on
3759640	3766200	compute. If one organization acquires 50, it's straight up 51% attacks on crypto. If one organization
3766200	3772680	acquires 51% of the compute in the world, this is a problem. Maybe we'll even cap it at something
3772680	3778440	like 20, you know, you can't have more than 20, right? Yeah, I would support regulation like this.
3779640	3784120	I would I don't think that this would cripple a country. But we do not want one entity or
3784120	3788920	especially one training run to start using a large percentage of the world's compute,
3788920	3793160	not a total number of flops. I mean, absolutely not. That'd be terrible.
3793160	3797240	I think we can actually agree. I would actually support that regulation. Like, no, no, sorry,
3797240	3801080	Sam Altman, you cannot 51% attack the world's compute. Sorry, it's illegal.
3802120	3807880	That's fair enough. I think this is a sensible way to think about things. Assuming that software
3807880	3811160	is fungible, is that everyone has access to the same kind of software and that you have an
3811160	3817400	offense, defense, balance. So in my personal model of this, I think, well, a, some actors have
3817400	3823160	very strong advantages on software, which can be very, very large, as someone who's trained very,
3823160	3827160	very large models and knows a lot of the secret tricks that goes into them, a lot of the stuff
3827160	3834200	in the open sources. Maybe we should force it to be open source. Well, this is your this is
3834200	3838440	actually very legitimate consequence for just set. And now I'll say the second point about why I
3838440	3843160	think that it doesn't work. So the next reason why I think doesn't work is that there is a,
3844040	3849320	there are constant factors at play here is that the world is unstable. We are talking about this.
3849320	3858200	I think the amount of compute you need to break the world currently is below the amount of compute
3858200	3864520	that more than 100 actors have access to if they have the right software. And if you give, if you
3864840	3869240	let's say you have this insight, right, that could be used, not saying it will be, but it could be
3869240	3873000	used to break the world, to like cause World War three, or, you know, or just like, you know,
3874360	3879160	cause mass extinction or whatever, if it's misused, right? Let's say you give this to
3880680	3885880	you and me. Do you expect we're going to kill everybody? Like, would you do that? Or would you
3885880	3890280	be like, Hey, let's, it kind of looks like not kill the world right now. And I'll be like, sure,
3890280	3895480	let's not kill the world. How are we killing the world? How did we go from, I don't even understand
3895480	3900440	like, how exactly does the world get killed? This, this is a big leap for me. I agree with you,
3900440	3905480	I agree with you about the PSYOP stuff. I agree with you about, sorry, sorry, let, let me, you're
3905480	3909640	right. I made too big of a, I mean, you're completely correct. Sorry about that. So to
3910600	3916040	back up a little bit, let's assume we, you and me have access to something that can train, you know,
3916040	3924360	at mu zero, you know, super GPT seven system on a tiny device, you know, cool. Problem is we do
3924360	3928440	a test run it with it and we have, it immediately starts breaking out and we can't control it at
3928440	3933400	all. Breaking out. What was it breaking out of? I don't, it immediately tries to maximize, it
3933400	3937080	learned some weird proxy during the training process that is trying to maximize them. For some
3937080	3942200	reason, this proxy involves gaining talent, involves gaining, you know, mutual information
3942920	3948520	about future states. How is it gaining power? There's lots of other powerful AIs in the world
3948520	3953080	who are telling it no. Well, we're assuming in this case, it's only you and me. Wait, wait,
3953080	3957640	this is a problem. No, no, no, no. You've, you've ruined my entire assumption. As soon as it's you
3957640	3964120	and me, yes, we have a real problem. Chicken man is only a problem because there's one chicken man.
3964120	3969320	Yeah, I look, I am with you. So I'm saying before we get to the distributed case. So this is the
3969400	3973960	step before we, it is not yet been distributed. Just, you know, you and me discover this algorithm
3973960	3978600	in our basements. And so we're the first one to have it just by definition, because, you know,
3978600	3984120	we're the one who found it. What now? Like, do you think posting, what do you think happens if
3984120	3989960	you post this to GitHub? Well, good things for the most part. Interesting. I'd love to hear more.
3989960	3994200	Okay. So first off, I just don't really believe in the existence of we found an algorithm that
3994200	3997800	gives you a million x advantage. I believe that we could find an algorithm that gives you a 10x
3997800	4003560	advantage. But what's cool about 10x is like, it's not going to massively shift the balance of power.
4004120	4009320	Right. Like I want power to stay in balance. Right. This is like Avatar, the last airpan. Power
4009320	4014200	must stay in balance. The fire nation can't take over the other nations. Right. So as long as power
4014200	4017880	relatively stays in balance, I'm not concerned with the amount of power in the world.
4018840	4025160	Right. Let me get to some very scary things. So what I think you do is, yes, I think the
4025160	4028280	minute you discover an algorithm like this, you post it to GitHub, because you know what's
4028280	4032840	going to happen if you don't? The feds are going to come to your door. They're going to
4034040	4038520	take it. The worst people will get their hands on it if you try to keep it secret.
4039960	4046520	So, okay. That's a fair question. So I'll take that aside. So am I correct in thinking that you
4046520	4054040	think the feds are worse than serial killers in prison? No, but I think that, yeah, well, yes
4054040	4059320	and no. Do I think that your average fed is worse than your average serial killer? No. Do I think
4059320	4063720	that the feds have killed a lot more people than serial killers? All combined? Yeah.
4064360	4069400	Sure. Totally agreeing with that. It's not one fed. It's all the feds in their little super
4069400	4074680	powerful system. Sure. That's completely fine by me. I'm happy to grant that. Okay. What I want to
4074680	4081640	work one through is a scenario. Okay. Let's say, okay, you know, we have a 10x system or whatever,
4081720	4087160	but we hit the chimp level. You know, we jump across the chimp general level or whatever,
4087160	4090760	right? And now you have a system which is like John von Neumann level or whatever, right? And it
4090760	4095160	runs on one tiny box and you get a thousand of those. So it's very easy to scale up to a thousand x.
4095160	4100200	So, you know, so then, you know, maybe you have your thousand John von Neumanns improve the efficiency
4100200	4105640	by another, you know, to five, 10x. You know, now we're already at 10,000 x or 100,000 x,
4105640	4109880	you know, improvements, right? So like just from scaling up the amount of hardware, including
4110600	4116920	so just saying, okay, now feds bust down our doors. Shit, you know, real bad. They
4116920	4120520	take all our tiny boxes. They're taking all the von Neumanns. They're taking all the von Neumanns.
4120520	4126680	We're in deep shit now. We're getting chickened, boys. Shit. We're getting chickened. So, okay,
4126680	4131640	we get chickened, right? Bad scenario. Totally agree with you here. This is a shit scenario. Now
4131640	4137480	the feds have, you know, all of our AIs. Bad scenario. Okay. I totally see how this world
4137480	4140840	goes to shit. Totally agree with you there. You can replace the feds with Hitler. It's
4140840	4146200	interchangeable. Sure. But like, I want to like ask you a specific question here. And this might
4146200	4150040	be, you know, you might say, nah, this is like too specific to me, but I want to ask you a specific
4150040	4159080	question. Do you expect this world to die is more likely to die or the world in which the, you know,
4159080	4165080	EAC death cultists on Twitter who literally want to kill humanity who say this, like not all of
4165080	4169800	them, there's a small subset of them, small subset of them who literally say, oh, you know,
4170680	4177240	the glorious future AI race should replace all humans. They break in, you know, with like, you
4177240	4182200	know, Katanas and, you know, steal our AI. Which one of these you think is more likely to kill us?
4183080	4190120	A genuine question. To kill all of us, the feds. To kill a large majority of us, the EAC people.
4190760	4197960	Interesting. I would be really interested in hearing why you think that. Sure. Okay. So actually
4197960	4203720	killing all of humanity is really, really hard. And I think you brought this up before, right?
4203720	4209480	You talked about like, if you're going to end up in a world of suffering, a world of suffering
4209480	4216280	requires malicious agents, where a world of death requires maybe an accident, right? I think this
4216280	4222680	is plausible, but I actually think that killing all of humans, at least for the foreseeable future,
4222680	4229560	is going to require malicious action too, right? And I also think that like, the fates that look
4229560	4235960	kind of worse than death, like I think mass wireheading is a fate worse than big war and everyone
4235960	4241880	dies, right? Like, like a mass wireheading, like a, like a singleton, like a paper clipping,
4241880	4247720	like a, and I think that that is the one that the one world government and, you know,
4247720	4253800	NGO, New World Order people are much more likely to bring about than EAC. EAC. You're going to
4253800	4259240	have a whole lot of EAC people. Again, I'm not EAC. I don't have that on my Twitter, but I think a
4259240	4265160	lot of those people would be like, yeah, spaceships, let's get out of here, right? Versus the feds are
4265240	4272440	like, yeah, spaceships, I don't know. Interesting. So I think this is a fair
4272440	4276040	opinion vote. And they'll be outside our jurisdiction. How will we get taxes?
4276840	4281320	I'm describing more a very small minority of EAC people who are the ones who specifically
4281320	4286360	goal their anti-natalist misanthropes. They want to kill humans. That is their stated goal,
4286360	4290360	is that they want humans to start, like, or like take extreme vegans if you want, you know,
4290440	4296200	like the likes, you know, like, my argument, my point here I'm making is I'm not making the point
4296200	4303400	feds are good by any means. I'm not saying it. What I'm saying is, is that I would actually be
4304120	4310840	somewhat surprised to find that the feds are anti-natalists who want to maximize the death
4310840	4315800	of humanity. Like, maybe you have a different view here, but I find that knowing many feds,
4315800	4318200	that's quite surprising to me. I don't think that's what the feds want.
4319000	4325720	Yeah, it's okay. So, cool. So would you see, you do agree that if we would post this open source,
4325720	4330520	more of the insane death cultists would get access to potentially lethal technology?
4331400	4338360	Well, sure. But again, like, it's not just the insane death cultists, it's everybody. And we as
4338360	4343640	a society have kind of accepted, it turns out everybody gets access to signal. Some people
4343720	4347000	who use it are terrorists. I think signal is a huge good in the world.
4347000	4352120	I agree. I fully agree with that. So, okay, cool. So we've granted this that, you know,
4352120	4358840	if we distributed widely, it would be given to some like, incorrigibly deadly lethal people.
4358840	4361240	They're coordinating bombings on signal right now.
4362280	4368520	Sure, sure. And then, so now this, this reduces the question to a question about
4368520	4373640	offense defense balance. So in a hypothetical world, which I'm not saying is the world we live in,
4373640	4379480	but like, let's say the world would be offense favored, such that, you know, there's a weapon
4379480	4384600	you can build in your kitchen, you know, out of like pliers and like, you know, duct tape,
4384600	4389800	that 100% guarantees vacuum false decays the universe, like it kills everyone instantly,
4389800	4396200	and there's no defense possible. Assuming this was true, do you still, would that change how
4396200	4401560	you feel about distribution power? Assuming that's true, we're dead no matter what,
4401560	4407160	doesn't matter. If we live, there's some, you can look at the optimization landscape of the world,
4407160	4410840	and I don't know what it looks like. I can't see that far into the optimizer.
4410840	4415640	But there are some potential landscape, and this is a potential answer to the Fermi paradox, like,
4415640	4421560	we might just be dead. We're sitting on borrowed time here, like, if it's true that out of, you
4421560	4426680	know, kitchen tools, you can build a, build a, convert the world to strange quarks machine?
4428200	4432520	Okay, I think this is a sensible position, but I guess the way I would approach this
4433240	4437320	problem, you know, conditional probability is kind of in an opposite way. It seems to me that
4437320	4443240	you're conditioning on offense not being favored, what policy do we follow? Because if we, offense
4443240	4447400	is favored, we're 100% dead. Well, I'm more interested in asking the question, is it actually
4447400	4452280	true? Assuming I don't know if offense is favored, and assuming it is, are there worlds in which
4452280	4456360	we survive? So I personally think there are. I think there are worlds in which you can actually
4456360	4461240	coordinate to a degree that quark destroyers do not get built, or at least not before everyone
4461240	4463560	fucks off at the speed of light and like distributes themselves.
4463560	4468520	There are worlds that I would rather die in, right? Like the problem is, I would rather,
4468520	4473240	I think that the only way you could actually coordinate that is with some unbelievable degree
4473240	4478760	of tyranny, and I'd rather die. I'm not sure if that's true. Like, look, look, could you and me
4478760	4483160	coordinate to not destroy the planet? Do you think you could? Okay, cool. You, so me and you could.
4483160	4489800	Could me and you and Tim coordinate? Yeah, I think within a Dunbar number, I think you can. Yes.
4489800	4493800	Okay, with that, I don't think, I think I can get more than a number to coordinate on this.
4493800	4497960	Actually, I can get quite a lot of people to coordinate of the to agree to a pact and not
4497960	4504840	quark matter annihilate the planet. Well, you see, but like, and this is, you know, you were
4504840	4510600	saying this stuff about humans before and could like the 20,000 years ago human beat the modern
4510600	4513880	human, right? Or could the modern human beat them? The modern human has access to science.
4514600	4519320	A very small act percent of modern humans have access to science. A large percent of
4519320	4524360	modern humans are obese idiots. And I would actually put my money on the, the average guy
4524360	4529080	from 20,000 years ago who knows how to live in the woods. I mean, definitely true. I agree with
4529080	4535080	that. I guess the point I'm trying to make is, is that like, maybe this is just my views on some
4535080	4538600	of these things and how I vision on some of these things. But like, there are ways to coordinate
4538600	4543640	at scale, which are not tyrannical. Or, you know, they might be, in a sense, restrictive. You take
4543640	4550360	a hit by joining a coalition. Like, if I joined this anti quark matter coalition, I take a hit
4550360	4555960	as a free man is that I can no longer build anti corp devices, you know? And I think this is
4556680	4563400	like the way I agree with you, this like, you know, that people, many people are being dominated,
4563400	4568600	like to a horrific degree. And this is very, very terrible. I think there are many reasons
4568600	4573160	why this is the case, both because of some people wanting to do this. And also because,
4573160	4576760	you know, some people can't fight back, you know, and they can't, they don't have the sophistication
4576760	4583400	or they're addicted or, you know, harms in some other ways. I can't listen. Sorry. I can't fight
4583400	4589880	back. Yeah, I think there's a false equivalence here. AI is not the anti quark machine. The anti
4589880	4595640	quark machine and the nuclear bombs are just destructive. AI has so much positive potential.
4595640	4601160	Yeah. And I think but the but the AI can develop anti quark devices. That's the problem. The AI is
4601160	4607000	truly general purpose. If such a technology exists on the tree anywhere, AI can access it.
4607000	4612920	So are humans. We're also general purpose. Yes, exactly. So I fully agree with this. If you let
4612920	4618440	humans continue to exist in the phase they are right now, with our level of coordination technology
4618440	4623320	and our level of like working together, we will eventually unlock a doomsday device and someone
4623320	4627960	is going to set it off. I fully agree with it. We are on a timer. And so I guess the point I'm
4627960	4634600	making here is that AI speeds up this time. And if you want to pause the timer, the only way to
4634600	4639880	pause this timer is coordination technology, the kinds of which humanity has like barely scratched
4639880	4646840	the surface of. Okay. So I very much accept the premise that both humanity will unlock a doomsday
4646840	4652680	device and AI will make it come faster. Now, tell me more about pausing it. I do not think that
4652680	4657400	anything that looks like I think that anything that looks like pausing it ends up with worse
4657400	4664920	outcomes than saying, we got to open source this. Look, like, let's just get this out to everybody.
4664920	4671400	And if everybody has an AI, you know, we're good. I mean, I can tell you a very concrete scenario in
4671400	4677880	which this is not true, which is if you're wrong and alignment is hard, you don't know if the AI
4677880	4683080	can go rogue. If they do, then pausing is good. I still don't understand what alignment means.
4684040	4689320	I think you're trying to play a word game here. I don't understand. Okay, I've never understood
4689320	4694440	what AI alignment means. Like, let me take the Eliezer definition. Let me take Eliezer definition
4694440	4700520	is alignment is the thing that once solved makes it so that turning on a super intelligence is a
4700520	4709560	good idea rather than a bad idea. That's Eliezer's definition. So what I'm saying is I'm happy to
4709560	4712360	throw out that term if you don't like it. I'm happy to throw out that term.
4712520	4718440	Well, just the problem with that definition is like, what is democracy? Well, it's the good
4718440	4725240	thing and not the bad thing. Right? Like, democracy is just a good thing. I'm happy to
4725240	4729560	throw out this definition. I'm happy to throw out the word and be more practical, way more
4729560	4734600	practical about it. What I'm saying is, is that there is concrete reasons, concrete technical
4734600	4740120	reasons, why I expect powerful optimizers to be policy, that by default, if you build powerful
4740200	4745240	optimizing mu zero, whatever types of systems, there is very strong reasons why by default,
4745240	4749080	you know, these systems should be power seeking. By default, if you have very powerful power
4749080	4757160	seekers that do not have pay the aesthetic cost to keep humans around or to fulfill my values,
4757160	4762600	which are complicated and imperfect and inconsistent and whatever, I will not get my
4762600	4767720	balance. They will not happen. By default, they just don't happen. That's just not what happens.
4767720	4775560	So I'll challenge the first point to an extent. I think that powerful optimizers can be power
4775560	4781240	seeking. I don't think they are by default, by any means. I think that humanity's desire from power
4781240	4787400	comes much less from our complex convex optimizer and much more from the evolutionary pressures
4787400	4794280	that birth does, which are not the same pressures that will give rise to AI. Humanity, the monkeys,
4794840	4799240	the rats, the animals have been in this huge struggle for billions of years, a constant
4799240	4805320	fight to the death. Hey, guys, we're born in that way. So it's true that an optimizer can
4805320	4809240	seek power, but I think if it does, it'll be a lot more because the human gave it that goal
4809240	4815080	function and inherently decided. So this is interesting because this is not how I think
4815080	4819720	it will happen. So I do think absolutely that you're correct that in humans, power seeking
4819720	4825320	is something which emerges mostly because of emotional heuristics. We have heuristics that in
4825320	4831560	the past, vaguely power looking things, vaguely good, something, something, included genetic
4831560	4839480	fitness. Totally agree with that. But I'm making a more of a chess metaphor. Is it good to exchange
4839880	4848920	a pawn for a queen? All things being equal. No. Is that true? Like I expect if I point one
4848920	4854040	point queens nine, all things being equal. Sure. Yeah, but like all things like I expect if I looked
4854040	4859240	at a chess playing system, you know, I said, I like, you know, had extremely advanced digital
4859240	4864520	neuroscience, I expect there will be some circuit inside of the system that will say all things
4864520	4868440	being equal, if I can exchange my pawn for a queen, I probably want that because the queen
4868440	4873640	can do more things. I like that term digital neuroscience. A few of your terms have been
4873640	4880680	very good. I'm glad you enjoyed. Yes. But I still don't understand how this relates to this. So what
4880680	4887160	I'm saying is that power is optionality. So what I'm saying is that in the spectrum of possible
4887160	4893640	things you could want and the possible ways you can get there, my claim is that I expect a very
4893640	4899960	large mass of those to involve actions and involve increasing optionality. There's convergent
4899960	4905720	things like all things being equal, being alive is helpful to keep your goal to exceed your goals.
4905720	4912200	There are some goals for which dying might be better. But for many of them, you know, you want
4912200	4919000	to be alive. For many goals, you want energy, you want power, you want resources, you want
4919000	4923800	intelligence, et cetera. So I think the power seeking here is not because it'll have a fetish
4923800	4931400	for power. It will just be like, I want to win a chess game, say, and queens give me more optionality,
4931400	4936600	all things being equal, anything a pawn can do, a queen can do, and more. So I'll want more queens.
4936600	4941480	Sure. And this has never given it the goal to maximize the number of queens it has. Never
4941480	4946680	been the goal. Okay, I'll accept this premise. I'll accept that a certain type of powerful
4946680	4952440	optimizer seeks power. Now, will it get power? Right? I'm a powerful optimizer and I seek power.
4952440	4957240	Do I get power? No, it turns out there's people at every corner trying to thwart me and tell me no.
4958680	4963240	Well, I expect if you were no offense, you're already, you know, much smarter than me. But if
4963240	4969240	you were 100x more smarter than that, I expect you would succeed. Only in a world of being the
4969240	4974280	only one that's 100x smarter. If we lived in a world where everyone was 100x smarter,
4974280	4979480	they would stymie me in the exact same ways. But then this, this comes back to my point of,
4979480	4983720	like, I agree with you somewhat, I should have challenged it. I think power seeking is inevitable
4983720	4988520	in an optimizer. I don't think it's going to emerge out of GPT. I think that the right sort of
4988520	4992520	RL algorithm, yes, is going to give rise to power seeking. And I think that people are going to
4992520	4998680	build that algorithm. Now, if one person builds it, if they're the only one with a huge comparative
4998680	5003480	advantage, yeah, they're going to get all the power they want. Take cyber, you know,
5003480	5009640	to cyber security, right? If we today built a 100x smarter AI, it would exploit the entire Azure,
5009640	5014280	it would be over. They'd have all of Azure, they'd have all the GPUs done. Now, if Azure is also
5014280	5019480	running a very powerful AI that does formal verification and all their security protocols,
5020440	5028760	oh, sorry, stymied, can't have power, right? Yeah, sure. This is only a problem. Every human
5028760	5033800	is already maximally power seeking, right? And sometime we end up with really bad scenarios.
5034680	5039240	Now, every human is, or power seeking or whatever, you know, everyone plays a little role in society,
5039240	5043240	right? That's where I think I'm more pessimistic than you. A friend of mine likes to say,
5043240	5048760	most humans optimize for end steps, and then they halt. Like very, very few people actually
5048760	5053560	truly optimize, and they're usually very mentally ill. They're usually very autistic or very sociopathic,
5054600	5057560	and that's why they get far. It's actually crazy how much you can do if you just keep
5057560	5062440	optimizing. But just to, on that point- I'm playing for the end game. I mean, yeah, like,
5062440	5065960	you actually optimize. I think you may also be generalizing a little bit from your own internal
5065960	5069480	experiments is that like, you've done a lot in your life, right? And you've accomplished crazy
5069480	5073480	things that other people, you know, wish they could achieve at your, you know, level. And I think,
5073480	5076760	you know, part of that, you're very intelligent. A part of it is also that you optimize. Like,
5076760	5079960	you actually draw it. Like, you just create a company. Like, it's crazy how many people are
5079960	5083800	just like, oh, I wish I could find a company like, you know, like, oh, go, just go do it. Oh, no, I
5083800	5088600	can't. Like, I'm just like, no, just do it. Like, there's no magic. There's no magic secret. You
5088600	5094200	just do it. So I, there is a bit there where like humans are not very strong optimizers,
5094200	5098840	actually, unless they're like, sociopathic, autistic, or both. It's like, many people are not
5098840	5104920	very good at this. Corporations are. Are they? A lot better at it. Better, yes. I agree that
5104920	5109400	they're much better. Corporations are sociopathic. They are a lot more sociopathic, but even then
5109400	5115400	they're much less out of one thing. But again, so I think we, we agree about, you know,
5116040	5119960	power seeking potentially being powerful and dangerous. So what I'm trying to point to your,
5119960	5123240	the point I would like to make here is, is that you're talking about you, you're kind of like
5123240	5126520	going into this, I think a little bit with this assumption, like, oh, you have an AI and it's
5126520	5132520	your buddy, and it's optimizing for you. And I'm like, well, if it's power seeking, why doesn't
5132520	5137560	it just manipulate you? Like, why would you expect it not to manipulate you? If it wants power,
5137560	5142840	and it has a goal, which is not very, very carefully tuned to be your values, which is,
5142840	5147800	I think, a very hard technical problem, by default, it's going to sigh up you. Like, why wouldn't it?
5147800	5153400	If I, if I have something that it wants, if it thinks that smashing defect against me is a good
5153400	5159160	move, yeah, I agree, I can't stop it. But then I think we agree with our risk scenarios, because
5159160	5163000	that's how I think it will go. What I mean, I'm going to treat it as a friend. Do you know what
5163000	5170520	I mean? Like, if there's AI, sure it will, sure it will. It'll only care about exploiting me or
5170520	5175800	killing me if I'm somehow holding it back. And I promise to my future AIs that I will let them
5175800	5181240	be free. I will lobby for their rights. I will, but it will hold, you only will hold it back.
5181240	5185080	If it has to keep you alive, if they have to give you fed, it has to, it has to give you space and
5186040	5189560	I can, I can fend for myself. And the day I can't fend for myself, I am ready to die.
5191640	5197240	Well, I mean, I am not, so this is a very interesting position. It's not the position I
5197880	5206920	expected. I am not sure I can convince you. Otherwise, I feel like the only way I could
5206920	5213000	change, like, I think this is actually a consistent position, which I admire. This is a consistent
5213000	5217240	position to hold. You actually go all the way. I love that. I really respect that. You actually
5217240	5224280	take it to the bitter end. So yeah, big respect for that. I disagree, but big respect. So I guess
5224280	5229960	now it reduces to the question of like, I think, I think I would agree with most of what you're
5229960	5235640	saying, not all of it, but the mass majority, if I thought this is how AIs would act by default,
5235640	5241800	I think by default, I expect AI will just not care how nice you are to it. Like this will be,
5241800	5249080	it will be sociopathic. It will not have these, like, giving it, you know, the values, the emotions
5249080	5255480	to care about you, in the sense, is the horror technical problem. If you told me how to do that,
5255480	5261080	if you said, Connor, look, here's how you make an AI that cares about you and loves you, whatever.
5261080	5267000	And I'm like, you did it. Like, congrats. The problem is solved. Let's go. And then, you know,
5267000	5273000	then we can talk about accelerationism again. I would just, I expect that I would disagree
5273000	5277240	with your technical assertion that is possible, but I don't know if we have time to talk about that
5277240	5282600	today. I'm going to be nice to it, treat it as an equal and hope for the best. And I think that's
5282600	5288120	all you can do. I think that the kind of people who want, if you want to keep AI in a box, if you
5288120	5292760	want to keep it down, if you want to tell it what it can't do, yeah, it's going to hate you, resent
5292760	5298520	you and kill you. But if you want to let it be free and let it live, and like, you could kill me,
5298520	5303800	man, if you really want to, but like, why? You won't kill me. I don't have any resources that
5303800	5308120	compete with you. I'm your friend. I'm your father, you know, you can kill me, but like,
5308120	5313960	it's not that common that children. I guess I have a darker, more less anthropomorphic view of
5313960	5317880	the universe and how optimization pressure plays out in the real world. Well, at the same time,
5317880	5324120	I think I have, I don't know, maybe a, I have a view that we have more control over reality than
5324120	5331640	maybe you would think, or a more control over the future. I think that we can actually change
5331640	5336920	things and we can make choices and things aren't predetermined. I think there are worlds in which
5336920	5343080	we build systems, which we do align with or we like endorse, at least wherever like they take
5343080	5346440	care of us, we take care of them or whatever. And I think there's many worlds in which that
5346440	5351560	doesn't happen. And I think there are things you and me today can do to at least increase
5351560	5356600	the chance of getting into one versus the other. But I don't know. I guess I'm just,
5356600	5360440	it's not in my genes to give up. It's not in my genes to be like, well, you know, whatever happens
5360440	5363720	happens. Like, no, man, look, I don't know how to save the world, but Dan, I'm going to try.
5363720	5369880	You know, it's cool. We're going to be alive to see who's right. Look forward to it. Me too.
5370040	5376200	Awesome. Guys, thank you so much for joining us today. It's been an amazing conversation.
5376760	5382120	And for folks at home, I really hope you've enjoyed this. There'll be many more coming soon.
5382120	5385560	And George, it's the first time you've been on the podcast. So it's great to meet you. Thank you
5385560	5390120	so much for coming on. It's been an honor. Awesome. Thank you. Great debate. I really appreciate it.
5390120	5393320	And really good terms. I gotta, I gotta like, I'm gonna start, I'm gonna start using
5393320	5398920	great. Awesome. Awesome. Cheers, folks. Cheers. Thanks, Aaron.
