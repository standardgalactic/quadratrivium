start	end	text
0	3680	Previous generations of the model have been weak reasoners,
3680	4560	but they do reason.
4560	6280	In the same way that hallucination
6280	8840	was an existential threat to this technology,
8840	11480	no, we'll never be able to trust this stuff.
11480	13120	There are hundreds of millions of people
13120	14800	using this techno, and they trust it.
14800	16000	It's actually useful for them.
16000	18360	We're making very good progress on the hallucination problem.
18360	20480	I think we'll make very good progress
20480	22360	this year and next on Reasoning.
22360	25360	Here we are again, another episode of MLST.
25360	28600	Today with Aidan Gomez, the CEO of Coheir.
28600	31360	Now, I interviewed Aidan in London a couple of weeks ago
31360	32840	just after their build event
32840	35440	and after Aidan did his presentation,
35440	38840	I sat him down for an hour and I gave him a grilling.
38840	40360	And he was such a good sport
40360	42960	for being so transparent and authentic.
42960	45000	This is the difference with Coheir.
45000	46840	They say it like it is.
46840	48160	There's no bullshit.
48160	49720	There's no digital gods.
49720	51520	There's no super intelligence.
51520	53360	They're just a bunch of folks
53360	55560	solving real world business problems.
55560	57160	Their models are incredibly good
57160	59800	for a true vlogmented generation, for multilingual.
59800	62080	But they also have some serious challenges
62080	63400	that they need to overcome.
63400	65960	We spoke about the AI risk discussion,
65960	68400	where the language models take away our agency,
68400	70120	how he's dealing with policy, right?
70120	72840	So on the one hand, he's talking with governments,
72840	75760	trying to get them to allow startups
75760	78520	to be more competitive, to innovate.
78520	81040	But at the same token, he's also very concerned
81040	83440	about some of the societal risks of AI.
83440	85000	So that's quite an interesting burden
85000	86880	and juxtaposition that he has to hold
87080	88280	in his mind.
88280	90600	We spoke a lot about the company culture at Coheir.
90600	92040	Now, I'm very impressed with Coheir.
92040	94480	All of the people I've spoken to have been very smart,
94480	96280	just really nice people.
96280	99440	And how has he cultivated that culture?
99440	101080	He was very frank and transparent
101080	104360	about some of the mistakes he made early on as a CEO.
104360	106480	So yeah, plenty to get your teeth into.
106480	108280	I hope you enjoy the conversation.
109440	111200	For Coheir, I think we're a little bit different
111200	113160	than some of the other companies in the space,
113160	116520	in the sense that we're not really here to build AGI.
116520	119280	What we're here to do is create value for the world.
119280	120840	And the way that we think we can do that
120840	125000	is by putting this tech into the hands of enterprises
125000	127240	so that they can integrate it into their products,
127240	130640	they can augment their workforce with it.
130640	133440	And so it's all about driving value
133440	135600	and really putting this technology
135600	137880	into the hands of more people
137880	141000	and driving productivity for humanity.
141840	143160	Aiden, welcome to MLSD.
143160	145160	It's an absolute honor to have you on.
145200	147200	Thank you so much. Appreciate it.
147200	148640	There's a bit of a last mile problem
148640	149800	with large language models,
149800	153680	so you folks have created this incredible general technology.
153680	156080	But when enterprises implement it,
156080	159560	they have a whole bunch of legislative constraints,
159560	161280	security constraints.
161280	165600	Yeah, yeah, I think there's loads of barriers to access.
165600	170440	Privacy to policy to just the familiarity of the teams
170440	172560	with the tech, it's brand new.
172560	174920	And so they haven't built with this technology before
174920	176960	and they're still getting up to speed
176960	180800	with the opportunity space, what they can do with it.
180800	185240	That being said, people are so excited about AI
185240	188240	and its opportunity that the motivation and the will
188240	190760	is there to overcome a lot of these hurdles.
190760	193520	So we're trying to help with that as much as we can,
193520	197320	whether it's like our LMU education course
197320	199720	to help general developers get up to speed
199720	201280	on how to build with this stuff
201280	203480	or us engaging at the policy level
203480	206120	to make sure that we have sensible policy
206120	208880	and not over-regulation or regulation
208880	213880	that hurts startups or encumbers industry in adopting it.
214040	215560	So we're trying to pull the levers that we can
215560	217280	to help accelerate the adoption
217280	220560	and make sure that it gets adopted in the right way.
220560	222960	But yeah, no, it's definitely the past two years
222960	226840	have been a push.
226840	229520	There's a lot of stuff slowing down adoption
229520	231360	that I would love to see eased.
231400	233720	I think the tools need to get better, easier to use,
233720	235720	more intuitive, more robust.
235720	237960	Prompt engineering is still a thing.
237960	239520	It shouldn't be right.
239520	242200	It shouldn't matter how you phrase something specifically.
242200	244560	It should be, the model should be smart enough
244560	246680	to generally understand your intent
246680	249520	and take action on your behalf reliably.
249520	251680	So even at the technological layer,
251680	254320	I think us as model builders, we have a lot to do
254320	256480	to bring the barriers down.
256480	260680	But I'm optimistic, like the pace of progress is fantastic.
260680	261760	Yes.
261760	265000	On that kind of prompt Britonist thing,
265000	268080	I wonder whether you think that we are on a path to have it.
268080	270000	I mean, in an ideal world,
270000	273240	the model and the application would be completely decoupled.
273240	274600	So you could swap the model out
274600	276080	or when you folks bring out a new model,
276080	278600	we can just swap it out and nothing breaks.
278600	280360	But at the moment, that's not the case.
280360	283640	But as the models become increasingly better,
283640	285960	do you think they will be robust in that way?
285960	287160	They should be, right?
288160	290840	There will always be quirks to different models
290840	292800	because we're all training on, you know,
292800	296680	we hope different data that focuses on different aspects
296680	300680	or elicits different behavior in the model.
300680	305440	So there will always be quirks to the behavior of models,
305440	307440	the personalities of models,
307440	309200	what they're good at and bad at.
309200	313440	But in general, in terms of like following an instruction,
313440	316680	we should be quite robust to that universally.
318160	319680	And so the ideal is that, yeah,
319680	322680	you can just take a prompt and drop it into any system
322680	324200	and see which one performs best
324200	326320	and then move forward with that one.
326320	329080	In reality, the status quo is a prompt
329080	330160	that works on one system,
330160	332920	fundamentally does not work on another.
332920	337920	And so there's this rift or these walls in between systems
338240	340320	that make them very, very different.
340320	342840	Hopefully that'll start to lift.
342840	346280	There's a lot of effort going into data augmentation
346280	347800	that makes these models more robust
347800	350320	to changes in prompt space.
350320	351720	We're doing a lot of work on that.
351720	355120	It's driven a lot by synthetic data
355120	359560	and finding, doing search to basically find the prompts
359560	362320	or the augmentations that changes to prompts
362320	366920	that break the model and then training to fix that break.
367880	369960	So I'm optimistic that sort of brittleness
369960	370800	is gonna go away.
370800	371800	Interesting.
371800	374400	So kind of finding problems with the model
374400	377400	and then robustifying and robustifying.
377400	379240	In doing so, how does that change
379240	381280	the characteristics of the model?
381280	384480	Does it make it less creative or less capable in some sense?
384480	386440	I mean, what do you ever feel for the trade-offs there?
386440	389240	Yeah, I mean, I don't think so.
389240	392680	I think that that's orthogonal.
392680	394280	The process of making it more robust
394280	395720	is orthogonal to creativity.
395720	397840	There are aspects of the post-training procedure
397840	401920	that do reduce creativity or, you know,
401960	404720	some people like to say like lobotomize the model.
406000	407680	So it's definitely a problem.
407680	410320	It's something that we watch for
410320	414040	and we're trying to prevent.
414040	419040	I would say that one of the most disappointing aspects
419040	420960	of the current regime of building these models
420960	424440	is that a lot of people train on synthetic data
424440	425760	from one source, right?
425760	427720	Like just from the GPT models.
427720	429680	And so all of them, all of the models
429680	431760	that are being created,
431760	433400	they sort of speak the same.
433400	435680	They kind of have the same personality
435680	438560	and it leads to this collapse
438560	441160	into a lot of different models
441160	442640	looking and feeling the same.
445160	446200	And that makes them boring
447280	451800	because like you have the same shortcomings across models
451800	454520	rather than if you have a diverse set of models
454520	456840	that have different failures in different places,
456840	459760	you can much better address, you know,
459760	462800	the preferences of much more people.
464480	468320	I've noticed due to synthetic data taking off,
469280	471600	just a total collapse in terms of
472600	477400	the different types of behavior models exhibit
477400	481280	and that cohere because our customers are enterprises.
481280	482360	Like that's who we sell to.
482360	484080	It's not consumers, it's not, you know,
484080	486240	anything other than enterprises
486240	488600	who want to adopt this tech.
488600	490040	And they're very, very sensitive
490040	491840	to what data went into the model.
491840	494800	And so we exclude other model providers,
494800	498040	data, you know, very aggressively.
498040	499320	Of course, some will slip in
499320	501120	as we're scraping the web, et cetera.
501120	503000	But we make a very concerted effort
503000	506280	to avoid other model outputs.
506280	508320	And so if you talk to our model,
508320	510280	when we release command R and R plus,
510280	513280	one of the things I kept reading on Reddit and Twitter
513280	514600	was it feels different.
514600	517680	Like something feels special about this model.
517680	520360	I don't think that's any like magic at cohere
520360	521800	other than the fact that we didn't do
521800	523240	what the other guys are doing,
523240	526360	which is training on the model outputs of OpenAI.
526360	527200	I agree with you.
527200	530040	So when people, you know,
530040	532000	see chat GPT or whatever for the first time,
532000	533760	that they're blown away by it.
533760	537200	But there are motifs that come up again and again
537200	540040	and again, unraveling the mysteries, you know,
540040	542640	delving into the intricate complexities
542640	544000	or blah, blah, blah, blah, blah.
544000	546320	And when you start to see these patterns
546320	549080	and these constructions, you just start to think,
549080	550720	oh, I don't like this very much
550720	552080	because you start to see through it.
552080	553160	It's a little bit like, you know,
553160	554800	when you start to see through someone,
554800	556520	they're not interesting anymore.
556520	558720	And I haven't seen that with cohere,
558720	561520	but I have seen it with many of the other models.
561520	564040	Now, my intuition was always,
564040	565840	I don't, I haven't really formed this very well,
565840	568400	but I thought that maybe it could come from
568400	570280	just the kind of data sets that we're using,
570280	574040	or maybe it could come from the preference fine tuning.
574400	576960	Are you saying that that monolithic effect
576960	580080	is because they're kind of eating each other's poop?
580080	581640	Yeah, no, yeah, yeah.
581640	584440	It's some sort of like human centipede effect.
584440	588960	I think, yeah, they're training on the outputs
588960	590360	of a single model.
590360	592960	And so it's all collapsing into that model's
592960	594000	output distribution.
594000	596080	And so if that output distribution has quirks
596080	599440	like saying the word delve a lot,
599440	601400	then it's gonna just pop up all over the place.
601400	603880	And people will take it for granted that,
603880	606280	oh, I guess LLMs just behave like this,
606280	607640	but they don't have to.
607640	608480	They don't have to.
608480	610760	It's interesting how subjective creativity is as well,
610760	611840	because a lot of people thought
611840	613880	that it was creative a couple of years ago.
613880	614840	And then when you see it everywhere,
614840	615800	it's not creative anymore.
615800	618280	So it needs to be novel to be creative.
618280	621160	But I mean, you folks have just released
621160	622720	the command R series of models
622720	625480	and you've blown everyone away.
625480	626360	Tell me about them.
626360	628080	But if you wouldn't mind also,
628080	630400	why did it take you a while to catch up
630400	631800	and get state of the art performance?
631840	636280	Yeah, we spent a lot of 2023 lagging.
636280	638520	I think that that is accurate to say.
639400	643880	What we were doing was sort of reorganizing internally.
643880	645200	We were rebuilding the company,
645200	648480	rebuilding the modeling team, the tech strategy
648480	652800	and preparing for the runs that led to command R.
652800	654120	It was clear to us that the process
654120	656280	that we had used to build the first command
656280	658960	and generations before that, it wasn't working.
658960	660280	It wasn't gonna scale.
660280	663520	And so we just rethought the entire pipeline.
663520	666520	And it took us a while to rebuild things,
667840	670360	run the experiments that we needed to run
670360	672440	in order to make decisions on what the design
672440	677160	of this new model building engine would look like.
679040	681040	And then it takes time to do the runs.
681040	683080	We spent a lot of last year doing that.
683080	685880	But I think the results speak for themselves.
685880	688120	And also with command R and R plus,
688120	691880	it's just the first step in a series of new models
691880	693200	that we wanna produce.
693200	697320	We're very excited to lean into specific capabilities.
697320	701720	And so while the general language model improvements,
701720	702760	they're super important.
702760	703600	They're crucial, right?
703600	704720	Like the models have to get smarter.
704720	707120	They have to get more capable.
707120	709820	And we'll continue to press on that direction.
710760	714080	We care about narrowing our focus a bit more.
714120	718360	And so for 2024, even with the command R series,
718360	720680	we focused in on rag and tool use.
720680	723080	I think you're gonna see a continuation
723080	725520	and extension of that focus
725520	727600	really making these models robust
727600	730600	at the key capabilities that enterprise cares about,
730600	732000	that will drive productivity,
732000	734720	that will automate really sophisticated processes
734720	739160	that today, as humanity knows it,
739160	741400	is only the domain of humans.
741400	742920	We really wanna go after that.
743920	748920	And give our models the ability to help in those spaces.
749120	750680	Is it fair to say that,
750680	751840	I don't know whether you feel
751840	755000	that the general large language models are saturating.
755000	756240	Maybe you could comment on that first,
756240	758000	but if you do think that,
759000	760440	does that give me a bit of a read
760440	763800	that there's a move towards specialization of the models?
763800	765560	I don't think they're saturating.
765560	766960	I think they're getting so good
766960	769720	that it's hard to see the incremental improvement.
770720	775480	But that incremental improvement is extremely important.
775480	778880	So once the models are smarter than you,
778880	782120	it's really hard to, in a domain, in medicine,
782120	784200	like Coher's model,
784200	787960	it knows more than I do about medicine, for sure.
787960	789160	Just absolutely.
789160	791280	And so I can't really effectively assess
791280	794000	whether we're improving in that dimension.
794000	795240	I can't tell anyone.
795240	796640	It's smarter than me.
796640	798600	I trust it more than I trust myself
798600	803440	to diagnose symptoms or process medical data.
803440	804720	And so I'm not equipped to do that.
804720	807040	Instead, what we need to do is create data sets
807040	810000	or go out and find people who are still better
810000	812560	than the model at those domains.
812560	815560	And they can tell me whether it's improving.
815560	818480	But for us, like the general population,
819600	823920	at some point, we kind of stop seeing improvement
823920	825040	between model versions.
825040	826640	It's harder to feel.
826960	829400	And you need to really zoom in
829400	831080	to a place that you're an expert
831080	833760	and that you know previous generations failed
833760	835160	to see the progress.
836200	838000	I think about it sometimes as like
839360	843040	painting in a canvas of knowledge.
843040	846560	And at some point, the holes in the canvas become so small.
846560	848000	You have to take out a microscope
848000	850200	to actually see it and paint it in.
850200	853640	We're sort of in that part of the space for these models.
853640	855560	And so improvement becomes much harder
855560	857440	for us, the model builders,
857440	860520	but it's much harder to feel and see for users
860520	865520	who aren't diving in very close to analyze performance.
866080	867160	I don't think it's saturating.
867160	869040	I think it's still making,
869040	872000	we're still making very significant progress.
872000	875040	I do think the past 18 months,
876280	877960	maybe a little bit less than 18 months.
877960	880760	Yeah, the past 18 months, 12 months,
880760	883920	we've been compressing.
884920	887360	So we built these massive, giant,
887360	889680	multi-trillion parameter models,
889680	893200	which were just extraordinary artifacts
893200	896240	of intelligence and capability.
896240	899960	And we realized it's impractical.
899960	902440	You can't actually put this thing into production, right?
902440	905520	Like it takes 60, a 100s to certainly,
905520	908880	it just, we could not productionize this.
908880	910760	The economics don't work out.
910760	913280	And so then we spent the year compressing
913280	916800	those massive models down into much smaller form factors.
918680	923680	There's very likely going to be a series of re-expansion
923960	928960	and scale, both on the model scale
930320	934800	in terms of parameters, but also data scale and data quality.
934800	937960	And that's being supported by much better
937960	940680	synthetic data methods that find much more useful
940680	944520	synthetic data that are quite compelling at search
944520	948560	to discover, to automatically discover weak points of models
948560	950680	and then close those gaps.
951760	954320	So I think we've, over the past year,
954320	958320	gotten very good at making models more efficient.
958320	963080	And we've created new methods that let us
963080	966320	sort of just like plug in compute and data
966320	969280	and have the model continuously improve.
969280	973240	You've used words like smart and capabilities.
973240	976760	And if you think of smart as knowledge,
976760	977720	I completely agree with you.
977720	980640	I think knowledge is a living thing.
980640	983480	We're all improvising and we are generating
983480	984760	new knowledge all the time.
984760	987400	And it just increases exponentially.
987400	989440	And there's no reason why language models
989440	991200	can't become more and more and more knowledgeable
991200	993960	because we just acquire more and more data.
993960	997520	And in that sense, a medical doctor is smarter than me
997520	999240	in that domain because they have the knowledge
999440	1000680	that I don't have.
1000680	1002920	But some people could say that intelligence
1002920	1005640	is something a little bit more abstract than that.
1005640	1008600	It might be the ability to build models.
1009640	1011200	It might be the ability to reason.
1011200	1013240	It might be the ability to plan.
1013240	1016040	And this is when we get into the kind of the age your own
1016040	1017400	thing.
1017400	1020240	So how do you demarcate those things?
1020240	1022480	Are you saying that the models are becoming more knowledgeable
1022480	1024600	but they're not necessarily becoming more intelligent
1024600	1025520	like we are?
1025520	1030520	I think reasoning is crucial to intelligence.
1033120	1036200	I think that these models can reason
1036200	1038080	and that's a controversial claim.
1038080	1040160	I think a lot of people would debate that.
1040160	1041320	A lot of people would make the claim
1041320	1042960	that the architectures we're using
1042960	1045240	or the methods we're using don't support
1046600	1047920	that sort of behavior.
1047920	1051400	I think that previous generations of the model
1051400	1054320	have been weak reasoners, but they do reason.
1055560	1058280	And it's not a discreet,
1058280	1060040	does it have this capability or not?
1060040	1062840	It's a continuum of how robust the reasoning engine
1062840	1064480	inside these models is.
1065440	1067240	We're getting much better methods
1067240	1070320	for improving reasoning generally.
1070320	1071640	We're getting much better methods
1071640	1074120	of eliciting that behavior from the models
1074120	1075920	and teaching them how to do it
1075920	1077880	and apply it to many different domains,
1077880	1081960	whether it's math, whether it's decision-making tasks,
1081960	1084860	breaking down tasks, planning how to execute them.
1086440	1089720	Those were key missing capabilities
1089720	1093080	that were quite weak in previous generations of models,
1093080	1095560	which are now starting to emerge
1096880	1099960	in a significantly more robust fashion.
1099960	1102760	And so in the same way that hallucination
1102760	1106280	was it used to be an existential threat to this technology,
1106280	1108520	no, we'll never be able to trust this stuff.
1109520	1112160	There are hundreds of millions of people using this techno
1112160	1113000	and they trust it.
1113000	1113960	It's actually useful for them.
1113960	1116280	They use it because it's useful to their job.
1116280	1118680	We're making very good progress on the hallucination problem.
1118680	1120840	I think we'll make very good progress
1120840	1123160	this year and next on reasoning.
1123160	1126640	I think it's just a capability,
1126640	1131640	a skill that the model needs to be taught.
1131800	1134560	And we're building the methods and data
1134560	1139560	and techniques to support teaching, teaching these models.
1139600	1141440	Yeah, it is interesting how you can kind of break
1141440	1143240	intelligence down to all of these things
1143240	1145240	and some you might argue are missing now,
1145240	1149680	like planning, creativity is an interesting one.
1149680	1151720	Agency is quite an interesting one.
1151720	1155000	And presumably as a thing has more understanding
1155000	1156280	and it has more autonomy,
1156280	1159000	it could in principle develop agency
1159000	1159960	at some point in the future.
1159960	1162440	But you think of these things as skills.
1162440	1165600	Could you give any hints to how you've moved the needle
1165600	1166440	on this?
1166440	1167880	So the knowledge thing, it seems to me
1167880	1169840	that you would just get more data
1169840	1171440	and curate and refine the data.
1171440	1172880	But could you give any hints
1172880	1175080	on how you've made it better at reasoning, for example?
1175080	1175920	Yeah, with knowledge,
1175920	1178720	I think it's about augmentation with RAG
1178720	1182040	and better modeling techniques, cleaner data sets
1182040	1184720	so that you remember the right stuff
1184720	1188200	and don't retain the less relevant stuff.
1189560	1192400	Those are the techniques that move the needle there.
1192400	1195360	With reasoning, there are like circuits
1195360	1197640	that you really need to bake in to the model.
1197640	1199560	You need to show it and demonstrate it,
1199600	1202160	how to break down tasks at a very low level,
1203120	1204680	think through them.
1204680	1206760	And that's stuff that's not actually
1206760	1208880	that abundant on the internet.
1208880	1212160	So it doesn't come for free using our previous techniques
1212160	1215040	of just scrape the web and train the model and scale up.
1215040	1218520	People don't usually write out their inner monologue, right?
1218520	1220720	They usually write the results of that inner monologue.
1220720	1223520	And so it's something that the model
1223520	1226560	has been missing a view into.
1226560	1228800	I think synthetic data will go a long way
1228800	1231760	in closing that gap and supporting building
1232760	1236400	multi-trillion token data sets
1236400	1240280	that actually demonstrate how to have an inner monologue,
1240280	1241480	how to reason through things,
1241480	1244440	how to think through problems, make mistakes,
1244440	1247680	identify mistakes, correct them and retry.
1247680	1250440	That sort of long thought process data
1250440	1253680	is actually extremely scarce.
1253680	1254520	It's very rare.
1254520	1255360	It's very rare.
1255360	1256200	It's really hard to find.
1256200	1257040	You can find it on the internet, of course.
1257240	1262240	Stuff like forums where people help each other
1262280	1263880	with homework and sort of break down.
1263880	1266560	This is how I arrived at this answer.
1266560	1269120	But when you look at the internet in totality,
1269120	1272560	those are like pinpricks on the surface of this thing.
1272560	1275040	And so pulling that forward, emphasizing it,
1275040	1278240	augmenting it, producing more of that data
1278240	1282240	should be a key priority if you're gonna actually
1282240	1284920	teach these models to exhibit that behavior.
1284920	1286560	Is there a trade-off between,
1286600	1289800	I mean, for example, we could use the Unreal Engine
1289800	1292320	to generate lots of visual training data
1292320	1294680	for a Vision Foundation model.
1294680	1296680	Or an alternative would be we could have
1296680	1299400	like some kind of hybrid prediction architecture
1299400	1302520	where we somehow encode naive physics
1302520	1303960	into the architecture itself,
1303960	1307320	which means rather than memorizing lots of generated data,
1307320	1310920	we just kind of build a hybrid architecture.
1310920	1313760	Is that a trade-off that you're kind of thinking about?
1313760	1316280	Like specifically with the video side of things
1316320	1318560	where physics is relevant, I think that's
1320040	1321600	a totally fine strategy.
1321600	1325080	I think that, yeah, a lot of the physics engines
1325080	1329440	that people have built are, they're flawed, right?
1329440	1332600	Like video games still don't look like reality.
1332600	1334640	They still don't behave like reality.
1334640	1337040	And so training off of that data,
1337040	1341840	I think will leave you in a really unsatisfying place.
1341880	1346480	Like there's just still some Uncanny Valley weirdness to it.
1348080	1350680	I think like we have tons of actual video data
1350680	1354120	of the real world where physics is definitely implemented
1354120	1357000	and being represented completely accurately.
1357000	1360480	And so that should be our go-to source.
1360480	1365480	I think trying to use simulators at this stage
1365920	1366760	is the wrong approach.
1366760	1368680	I think you should take as much data
1368680	1370000	from the real world as you can
1370000	1371600	and use that as a bootstrap
1371600	1373720	to then build synthetic data engines
1373720	1376320	that help you iteratively improve.
1376320	1378120	It's what happened in language as well, right?
1378120	1381640	Like we didn't go to synthetic language,
1381640	1384160	rules-based synthetic language generators
1384160	1385480	to teach our language models,
1385480	1387520	the basic principles of language
1387520	1391360	using our linguistic models that we've built.
1391360	1393200	No, we threw all that away.
1393200	1397360	We took actual language data from humans, trained on that,
1397360	1400280	and then used the models that were the output of that
1400320	1405320	to improve iteratively and via experimentation.
1407040	1409600	I think the same will be true in vision.
1409600	1412160	That's a really interesting point, actually.
1412160	1414400	Because with the SORA model from OpenAI,
1414400	1415720	it does look a bit weird.
1415720	1417160	It looks like it's always flying
1417160	1419760	and it looks very game engine-like.
1419760	1422320	And the language example is beautiful.
1422320	1424960	But what about something like mathematics?
1424960	1428560	Are there examples where rather than kind of, you know,
1428560	1431360	perturbing or mutating what already exists,
1431360	1434280	you might just start from first principles and rules?
1434280	1435280	Totally, yeah.
1435280	1440280	Like mathematics is so explicitly rule-driven
1441040	1444000	and so explicitly verifiable.
1444000	1448600	It's like the perfect example of synthetic data generation.
1448600	1453600	I think it's definitely one of the domains
1453760	1455240	that will crack first.
1455440	1459240	And on top of that, code, right?
1459240	1462280	You can completely, synthetically generate code, verify it.
1462280	1463120	Does it run?
1463120	1466560	Does it produce the outputs that you want on a test set?
1467960	1471240	So when it is that explicit and verifiable,
1471240	1473680	it's perfect for synthetic data generation,
1473680	1474880	like just ideal.
1474880	1477840	Yeah, but I guess this is kind of what I'm thinking about.
1477840	1480680	That with code, you can actually constrain it way more
1480680	1481880	than language.
1481880	1485520	So you could, rather than using an existing self-attention
1485520	1487680	transformer, you know, you might want to have something
1487680	1490080	that only works on trees or whatever.
1490080	1492960	And maybe that would work better for that particular thing.
1492960	1495240	But then I guess we'd have to have some kind of mixture
1495240	1498640	of experts and not have a single model.
1498640	1502640	Yeah, I think that's behind the scenes actually,
1502640	1504040	a lot of the strategy.
1504040	1507440	You'll likely have an MOE where one component,
1507440	1510000	one of those experts is gonna be an expert in code,
1510000	1511800	very highly specialized to that.
1511840	1514600	Heavily upsampled on synthetic code data
1514600	1517480	and real code data, math, et cetera.
1518760	1522120	And that expert will act as a general reasoning engine
1522120	1523680	and will be very good at logic
1523680	1525760	and those sorts of components.
1525760	1528320	You might have a medical expert,
1528320	1531960	which has dramatic upsampling along that axis.
1534200	1539200	Yeah, I think that's a very effective path towards
1539440	1541580	even more efficient models.
1541580	1543740	So if you're in the medical domain
1543740	1545940	or the math or code domain,
1545940	1547980	you can then pull out that expert
1547980	1549420	and use it independently.
1549420	1554420	You don't need to keep around this huge monolithic model.
1554660	1557460	You can just take out a sub-component and deploy that.
1559380	1562180	Yeah, I think that architecture already exists.
1562180	1563820	Yeah, I wonder if you can talk to that a little bit
1563820	1565180	because I'm very excited about that
1565180	1568060	because it now seems that maybe we could call
1568060	1572420	what you just described an agentic distributed AI system
1572420	1575820	where the agents can pass messages to each other
1575820	1578500	and one of them might be an expert in mathematics,
1578500	1580860	one of them might be an expert in coding or so on.
1580860	1581860	But then you've got this problem
1581860	1584300	that you kind of send a message into the nexus
1584300	1586860	and all of the models are kind of passing messages
1586860	1590460	to each other and it's kind of unbounded in runtime
1590460	1591860	as opposed to one of the great things
1591860	1593340	with the language model is
1593340	1595940	it just does a fixed amount of compute per iteration.
1596020	1598020	You just put some prompt in
1598020	1600180	and you get the answer straight back out.
1600180	1604020	So does that kind of unboundedness introduce problems?
1605060	1607940	I mean, a language model could just generate infinitely
1607940	1609580	and not produce a stop token
1609580	1611580	and you would go on forever.
1611580	1613660	So I think the problem already exists
1613660	1616020	and models are quite well-behaved in terms of,
1618700	1622020	if you train them to give up
1622020	1625220	and to say, I need to respond, they will.
1625940	1626940	They tend to.
1627820	1629900	So I'm not too concerned about like,
1629900	1633180	runaway processes that would just not be useful
1633180	1636180	as well, hugely computationally expensive.
1636180	1639060	And yeah, it seems like models can produce stop tokens.
1639060	1642700	And I think that even in a multi-agent scenario,
1644340	1647380	discourse between agents will conclude itself
1647380	1648540	in a reasonable amount of time.
1648540	1649380	Yeah, interesting.
1649380	1652340	And I think even now with your multi-step tool use,
1652340	1653340	that's basically what you've done.
1653340	1655420	You could in principle do that recursively
1655420	1657700	and you could constrain the computation graph
1657700	1659060	so that there's no cycles
1659060	1661340	and it comes back in a fixed amount of time.
1661340	1664060	Yeah, we terminate execution
1664060	1666620	after some number of failed attempts.
1666620	1668740	So it's easy to solve that way.
1668740	1671060	It's a little bit unsatisfying.
1671060	1673620	I think our multi-optimal use right now,
1673620	1675060	it's our very first pass.
1675060	1677180	It's like the negative one.
1677180	1680860	And so it's not that good at catching
1680860	1682060	when it's made mistakes.
1682060	1684200	It's not that good at correcting its mistakes,
1684200	1686320	even if it's caught that it fucked up.
1687360	1690720	And so I think we're still very early there,
1690720	1692520	but those systems are gonna start to become
1692520	1696040	extremely robust and reliable.
1696040	1698160	And I'm very excited for that.
1698160	1699000	Amazing.
1699000	1704000	So I'm interested to know from, in your own words,
1704000	1707320	how, I mean, we're just talking to you folks
1707320	1709680	who've got this forward engineering team,
1709680	1711360	your enterprise focused,
1711360	1713560	you're helping bridge the last mile problem
1713560	1715920	and really embedding yourselves into large enterprise,
1715920	1718280	which is an amazing differentiator.
1718280	1720680	But other than that, there's always the question,
1720680	1724200	lots of people say these models are just kind of interchangeable
1724200	1728000	and you're just kind of playing the token game at some point.
1728000	1730760	And I just wondered like, what's your plan there?
1730760	1732760	I agree with that sentiment.
1732760	1734880	Models are way too similar.
1734880	1737720	I think there's going to start to be differentiation
1737720	1738560	between models.
1738560	1742720	Like I was talking about before with command R and R plus,
1743920	1748480	we're going to start really focusing in on key capabilities.
1748480	1752040	The general language model game is,
1753000	1756680	there's a lot of players and it's pretty saturated.
1758880	1761480	I think people are gonna start to have to branch out.
1761480	1764240	I think that consumer language models
1764240	1767000	are going to separate away from enterprise language models.
1767000	1769320	Within enterprise, there's gonna be a lot of specialization
1769320	1771100	into specific domains.
1771660	1775500	And so for Cohere, what I want to see us do
1775500	1780500	is in product space, push into more tailored capabilities
1781340	1782740	for particular problems.
1784740	1786980	We want to drive value for enterprise
1786980	1789340	and different enterprises operated in different spaces
1789340	1790340	and they have different needs.
1790340	1792260	The tools that their models might need to use
1792260	1794460	look very different from one another.
1794460	1796180	And we want to make sure that we're serving
1796180	1800140	each of those niches particularly well or uniquely well.
1800140	1801820	And that will be our value proposition
1801820	1803380	differentiated from others.
1805100	1809180	So that notion of specialization
1809180	1812420	or enhanced capability in particular domains
1812420	1814660	is something that we definitely want to explore
1814660	1817460	at the product level and start to offer.
1817460	1820580	Because like you say, the dollar per token space,
1820580	1823100	it's super, we're not gonna stop that.
1823100	1824820	It's important for the community, right?
1824820	1826620	Like folks need to build on top of this.
1826620	1829740	They need access to good models at fair prices.
1829980	1833340	So we're gonna continue to give that to the world.
1834780	1836940	But we want to create differentiated value.
1836940	1839220	And I think that's gonna come from focusing on
1839220	1842220	the actual problems that enterprises want to tackle
1842220	1845140	and getting extremely, extremely good at them.
1845140	1845980	Interesting.
1845980	1850900	On that, are you planning kind of horizontal products
1850900	1852340	or vertical products?
1852340	1856820	And the reason I say horizontal is I know a few startups now
1856860	1860060	that are building kind of low code,
1860060	1862420	app dev platforms with large language models
1862420	1864580	and they're making it incredibly easy in the enterprise
1864580	1866740	to compose together different models
1866740	1868900	and to deploy applications on phones.
1868900	1872660	And it's really democratized because it's so much easier
1872660	1874580	now for people to do artificial intelligence.
1874580	1877820	That would be a good example of like a horizontal one, I guess.
1877820	1880820	So I think our product right now is super horizontal, right?
1880820	1882820	It's like general language models, embedding models,
1882820	1883660	re-rank models.
1883660	1886020	It's a platform that's deployable privately
1886020	1887980	on every single cloud.
1887980	1891460	You can deploy the model against any sort of data,
1891460	1895540	whether it's medical, finance, legal, it doesn't matter.
1895540	1900540	It's the most horizontal product and platform you can build.
1901660	1904020	What we're gonna start to do is more
1904020	1907820	towards verticalization and so specializing models
1907820	1911060	at particular problems or objectives
1911060	1915100	that exist in the world and offering a product
1915100	1917060	that solves that for the enterprise.
1917060	1919220	Would you ever go beyond the model
1919220	1921580	and kind of plug a little bit deeper
1921580	1923180	into the platform in the enterprise?
1923180	1926580	So for example, building operating models
1926580	1930780	or one approach would be to just fine-tune the model
1930780	1932620	on lots of data from a particular domain,
1932620	1934260	but it's still a language model.
1934260	1936180	The interface with Coheir is the same
1936180	1939900	or another one would be to let's say build
1939900	1942460	something a little bit like Databricks or Snowflake
1942500	1945820	or something like an enterprise-wide suite
1945820	1949900	that allows you to deploy, discover, create, share
1949900	1952060	artificial intelligence in an enterprise.
1952060	1955780	The only reason I say that is as you're an AWS,
1955780	1957780	they give you free credits.
1957780	1959900	They want you to get on their platform
1959900	1961780	because they know you're never leaving
1961780	1964180	because you've got something there
1964180	1966100	which is not easily replaceable.
1966100	1968540	People learn how to use it, they love it.
1968540	1969980	Would that be a potential future?
1969980	1974460	Yeah, it's definitely still going to be a platform,
1974460	1977060	customizable, something that the user,
1977060	1979220	which for us as an enterprise can adopt
1979220	1981460	and sort of bring into their environment,
1981460	1984660	hook in their data, their tools,
1984660	1987820	their whatever they want to plug in.
1987820	1990900	The verticalization is gonna come from
1990900	1992780	investing in the model to be good
1992780	1994300	within a particular domain.
1994300	1996900	That might mean fine-tuning on data within that domain.
1996900	2000500	That might mean making sure the model is very good
2000500	2003700	at using the tools that employees operating
2003700	2005140	in that domain would use.
2005140	2005980	But that's our focus.
2005980	2007660	It's starting to get more specific
2007660	2010420	and focused on the actual use cases
2010420	2011780	that enterprises care about
2011780	2014420	and not just doing, you know,
2014420	2019420	version 345 of the same general model.
2022060	2025220	So I saw you tweeted about Nick Bostrom's
2025260	2028340	Future of Humanity Institute shutting down.
2028340	2030060	Do you have any thoughts on that?
2031500	2034340	Yeah, I think it sucks to see
2034340	2039340	any sort of academic institute collapse.
2040780	2043340	To be honest, I know nothing more than what's public there.
2043340	2046220	So I don't know if there were some internal issues
2046220	2050180	that caused the philosophy department to pull funding.
2051180	2056180	But I've been a pretty vocal critic of ex-risk
2057820	2062180	and the idea that language models are going to
2062180	2064260	take over the world and kill everyone.
2065700	2068820	But despite that, I still want people thinking about that.
2068820	2070460	I still want academics thinking about that.
2070460	2074100	I don't think that regulators and policy folks
2074100	2075500	should be thinking about it yet
2075500	2077420	because it's so far away and remote
2078300	2080180	and potentially completely irrelevant.
2080180	2082540	But that's the domain of academia,
2082540	2086060	is to pursue those long horizon, high risk projects
2086060	2087060	and make progress on them.
2087060	2090220	And so I certainly don't want to see
2090220	2095220	the academic front of that effort get defunded.
2097180	2099740	That being said, I think that those organizations
2099740	2103820	have really been trying to get their hands into policy
2103820	2107100	and impact private sector, public sector
2107100	2110740	in a way that is threatening to progress,
2111900	2116900	misleading and so I think that we're starting
2119260	2120860	to have within our community,
2120860	2123300	like the AI machine learning community,
2123300	2124540	a bit of a correction.
2125460	2127580	Those people were kind of given a lot of power,
2127580	2128860	were listened to a lot
2130780	2134140	and developed what I think we all
2134140	2136180	recognize as too much influence.
2137180	2140740	And it started to produce bills
2140740	2145740	and talks about policy that would totally collapse progress
2146460	2149180	in the space, very, very prematurely
2149180	2154180	about theoretical long-term risks that might be an issue.
2154540	2155380	And so fortunately,
2155380	2157380	I think there's a cultural correction happening
2157380	2159500	where even the legislators and policymakers
2159500	2162620	are starting to say, you know, this is not,
2163620	2167300	it's not appropriate the level of influence
2167300	2169580	that this one group is having
2169580	2171620	and we should listen to a much more broad
2171620	2174860	and diverse set of opinions.
2174860	2176260	So I'm still concerned about that
2176260	2179100	and I'll continue to speak out against that when I see it,
2179100	2181420	but at the academic level,
2181420	2186420	I don't want to see, you know, professors lose their funding.
2186540	2190060	I think that they should continue to pursue those ideas.
2190060	2191580	Yeah, I think Bostrom blogged that.
2191580	2194180	He was trying to resist the entropic forces
2194180	2196180	of the philosophy department for several years
2196180	2198180	and eventually he lost.
2198180	2199980	But I'm in two minds as well.
2199980	2202020	So as you know, I've hosted many debates
2202020	2203260	with, you know, like Connolly,
2203260	2205020	he for example, and Beth Jezos
2205020	2206700	and a bunch of different people.
2206700	2209500	And one thing that strikes me is how ideological it is.
2209500	2211180	I really thought as a podcaster,
2211180	2214140	I could have an honest and open conversation
2214140	2216260	and it's never gone well.
2216260	2217780	And I've put a lot of thought
2217780	2220060	into trying to understand why that is.
2220060	2222500	And I think philosophically you can trace it back
2222500	2225860	to things like paternalism and safetyism
2225860	2228620	and utilitarianism and consequentialism
2228620	2230620	and long-termism and, you know,
2230620	2235060	these are ideologies that make one believe
2235060	2237820	that even though it's just a subjective probability
2237820	2240020	that I know better than you,
2240020	2242180	I can predict the future better than you.
2242180	2244980	And they've become much more pragmatic in recent years.
2244980	2246700	So rather than talking about
2246700	2248860	the old school Bostrom superintelligence,
2248860	2250780	we're now talking about, you know,
2250780	2253380	memetic risks and bio risks and things
2253380	2256460	that I think are designed to get more people on board with it.
2256460	2259620	And I agree with you that they've had a lot of influence.
2259620	2261580	But why is it so difficult
2261580	2263340	to have a rational conversation?
2263340	2264780	Yeah, no, I think it's what you say.
2264780	2266980	It's very ideological.
2266980	2269140	There are camps and positions
2269140	2272500	and it's, for some reason,
2272500	2275420	it's become very cult-like on both sides.
2276420	2278820	Obviously, there's the EA movement,
2278820	2282220	which formed a cult-like environment
2282220	2285100	of adherence to those principles
2285100	2289820	and their recommended behaviors and actions,
2289820	2293100	you know, what you should work on in your life.
2293100	2294860	They have dating apps for you.
2294860	2296540	Like, it's very insular.
2296540	2301140	And then there was an ironic, I think,
2301140	2303820	although it's increasingly not clear,
2303940	2306540	an ironic counter-movement, which was EAAC.
2306540	2308260	Yeah.
2308260	2309700	And that has spun out into something
2309700	2311300	that is very not ironic.
2311300	2315620	It's very libertarian, accelerationist,
2315620	2318380	which are ideals that I don't hold either.
2318380	2322980	And so both of these camps I find really unappealing.
2322980	2324700	I don't want to be associated with either of them.
2324700	2329420	Yeah, I found, you know, EA was very dominant
2329420	2330500	for a long time.
2330540	2334300	And so when EAAC came out, it was, like, refreshing.
2334300	2337540	Finally, someone's, like, calling them on their bullshit.
2337540	2339420	But at this point, it's just mind-numbing
2339420	2343700	and, like, completely not of interest to me.
2343700	2344980	Yeah, we've had Beth on the show.
2344980	2346420	He's a really nice guy, actually.
2346420	2348660	I invested in Guillaume's company.
2348660	2349700	Oh, did he? Yeah, yeah, yeah.
2349700	2350780	Oh, it's not that...
2350780	2354420	He's brilliant. Like, he's a really, really nice person.
2355620	2358700	I'm proud to see Canadians doing great things.
2359700	2361580	The best thing I found super funny.
2361580	2364260	And I think EAAC was necessary.
2364260	2368180	I now believe that both EAAC and EAAC need to be dissolved.
2368180	2373380	We've seen them through to their logical conclusion,
2373380	2375100	and now we're starting to get into territory
2375100	2378060	that's very strange.
2378060	2379420	From a philosophical perspective,
2379420	2383020	how do you kind of see the role of AI in society?
2383020	2387060	And I'm quite interested in how it's affecting our reality,
2387060	2389980	and how we interface with technology
2389980	2391740	is really dramatically changing over time.
2391740	2392900	I mean, what do you think about that?
2392900	2394940	Yeah, completely true.
2394940	2400020	I think I view it in the same way I view the computer
2400020	2401660	or the CPU.
2401660	2402620	It's a tool.
2402620	2404020	It's something that we're going to leverage,
2404020	2406580	that we're going to build on top of
2406580	2408660	and use to make our lives better,
2408660	2411060	to make us more productive,
2411060	2413700	to make things cheaper, more accessible.
2414700	2418180	I think all the good that came from the computer
2418180	2421700	and the internet is going to be dwarfed by this,
2421700	2425700	the democratization of intelligence
2425700	2429180	and having that always at your disposal at any time.
2429180	2432700	That's something that, you know, 50 years ago,
2432700	2435980	you couldn't even dream of it, right?
2435980	2438900	It's surreal, the amount of progress
2438900	2440580	that's been made in half a century.
2440580	2441940	And so I'm really excited for that.
2442380	2447380	I think it will do a lot of good and alleviate a lot of ills.
2449140	2452420	I think the human experience, our lives,
2452420	2455900	will be dramatically improved by having access
2455900	2459300	to much more intelligence in our lives.
2459300	2460380	I'm really excited as well,
2460380	2464420	but are there particular things that you are concerned about?
2464420	2467940	I mean, for example, people say that language models
2467940	2469580	might enfee us,
2469580	2473020	they might lead to mass manipulation and persuasion.
2473020	2475220	I mean, Jan Lacoon tweeted the other day,
2475220	2478140	he said, where's the mass manipulation?
2478140	2479180	Where's the persuasion?
2479180	2481580	There might be something that just happens gradually
2481580	2483140	over time, but are there things
2483140	2485260	that you do worry about?
2485260	2486580	Of course, yeah, of course.
2486580	2488180	It's a general technology,
2488180	2491340	and so it can be used in a lot of different ways,
2491340	2493500	many of which are, I think,
2493500	2495620	abhorrent and ones that we should avoid
2495620	2497340	and make very difficult to do.
2498940	2502700	I'm much more of an optimist than I am a pessimist,
2502700	2506660	but on the side of things that are risky,
2506660	2509980	I think that misinformation is high up on the list.
2509980	2512180	I think that we're already seeing
2513540	2516700	social media platforms start to build in the mitigations.
2516700	2518500	I think things like human verification
2518500	2519780	are gonna become crucial.
2520740	2524020	If I'm reading a poster talking about
2524940	2528020	whatever Canadian elections or politicians,
2528020	2531620	I wanna know that that's a voting Canadian citizen.
2531620	2534100	I wanna know, because I want to know
2534100	2537060	what my compatriots think, right?
2537060	2539820	Even if they're on the opposite side of the fence to me,
2539820	2540860	like that's fine.
2540860	2544060	I wanna hear what they think,
2545140	2548300	but I don't wanna hear what some foreign adversary
2548300	2552500	has spun up a bot to push into the discourse.
2552500	2556460	And so human verification, I think, is crucial.
2556460	2559060	That's the one that's top of mind for me.
2559060	2561140	I think some of the more remote risks,
2561140	2566060	like bio weapons and this sort of stuff,
2566060	2567500	I'm less concerned about.
2567500	2571100	In feeblement and becoming dependent on the technology,
2571100	2573340	I think folks said that about calculators
2573340	2575940	and we wouldn't learn how to do basic math.
2575940	2579660	Humans are intrinsically curious.
2579660	2580740	We want to know things,
2580740	2584100	and we can't ask the right questions of machines
2584100	2585620	without knowing things.
2585620	2588980	And so we'll continue to be really well educated,
2588980	2592140	better educated, more knowledgeable than we were before
2592140	2593260	without that technology.
2593260	2595820	Yeah, because if you look at the enfeeblement pie chart,
2595820	2597500	a calculator is a very small part
2597500	2599940	and a general AI is quite a large part,
2599940	2601940	which is a little bit concerning, I guess,
2601940	2606940	but I agree with you that maybe the jobs one has spoken about.
2606940	2608500	I've not seen a lot of evidence of that yet,
2608500	2611460	but it's so pernicious, it might happen slowly over time.
2611460	2613220	Daniel Dennett wrote an interesting article,
2613220	2615180	and rest in peace, by the way,
2615180	2617740	Daniel Dennett called Counterfeit People,
2617740	2619020	which he published in The Atlantic,
2619020	2621460	and he was kind of saying that when we have all of these bots
2621460	2624780	and generative video models and so on,
2624780	2627860	at some point they'll become indistinguishable from reality,
2627860	2630060	and that will lead to a kind of acquiescence
2630060	2632260	where we don't trust anything we see.
2632260	2635020	And I think that's quite interesting,
2635020	2637300	and I also think that these models
2637300	2640940	might kind of affect our agency in quite a weird way,
2640940	2642980	but it's so difficult to understand now
2642980	2645780	how that's going to affect society.
2645780	2648700	Yeah, I think even now,
2648700	2651420	people have been taught to be extremely skeptical
2651420	2654220	of what they read and see.
2654220	2657100	There's a very strong prior inside of us,
2657100	2658380	for any media that we consume,
2658380	2661100	that it's been skewed or manipulated
2661100	2664420	or produced to propagate an idea,
2664420	2666780	and I think it's good to have a skeptical populace.
2666780	2668500	I think it's good to be skeptical
2668500	2672020	about what you read, regardless of the medium.
2672020	2674020	And I think people will do what they've always done,
2674020	2676260	which is filter towards sources
2676260	2678140	they find trustworthy and objective.
2680220	2683820	That'll happen even with ML and the loop,
2683820	2685940	disinformation and misinformation campaigns,
2685940	2687260	manipulation campaigns,
2687260	2690500	they existed well before models existed.
2691780	2694700	And so it's not like a novel concept,
2694980	2696460	and it's always been a risk.
2697660	2700860	And the question is how much more prevalent
2700860	2703900	does the technology make that risk?
2705620	2709260	I'm optimistic that we're quite robust
2709260	2713100	and that we'll find ways to make it very hard
2713100	2715220	for bad actors to exploit the technology.
2715220	2718740	My rough take is that the more agency the AI has,
2718740	2720020	the more of a risk it is,
2720020	2722780	because if it is just doing supervised things,
2722820	2725340	then every step of the process,
2725340	2729060	it's being aligned and constrained and steered by humans.
2729060	2731420	If we ever did create a gentile AI,
2731420	2734260	then there's this kind of weird divergence
2734260	2736780	and all sorts of scary things might happen.
2736780	2738340	But I wanted to talk a little bit
2738340	2739900	about policy and regulation.
2739900	2741100	So you spoke to that earlier,
2741100	2743300	you said that potentially there are some
2743300	2746700	quite damaging policy changes being considered.
2746700	2747540	Could you speak to that?
2747540	2750180	Yeah, I've seen ideas floated.
2750180	2753380	I don't think any seriously damaging policy
2753380	2755860	has actually passed, fortunately.
2755860	2757940	But within what's being considered,
2757940	2762300	there are ideas that they will destroy innovation,
2762300	2763620	they will destroy startups.
2765020	2766860	And so you'll just entrench power
2766860	2768580	with the existing incumbents.
2769500	2773340	Some of those examples might be fines,
2774340	2777220	which if they're a $100 million fine,
2777260	2780620	that's gonna wipe out and stamp out a startup.
2780620	2784700	But for a large, you know, big tech company,
2784700	2787140	it's like 10 minutes of revenue.
2787140	2788140	It just doesn't matter.
2788140	2789780	It fundamentally is irrelevant.
2789780	2792020	And certainly a cost they're willing to take
2792020	2793180	to capture a market.
2794820	2797820	And so very disproportionate consequences
2797820	2801420	for the same punishment.
2801420	2805180	Over-regulation in that way that it's thoughtless
2805180	2807020	will have the exact opposite effect
2807020	2809860	of what I think all of us in the public
2809860	2811340	and in government want.
2811340	2812980	We want competitive markets.
2812980	2814900	We don't want oligopolies.
2815740	2818580	And we're starting to see oligopolies emerge.
2818580	2822540	And so there needs to be fairly strong action
2822540	2825500	pushing against the entrenchment of those oligopolies.
2825500	2829860	And we need to preserve the ability to self-disrupt.
2829860	2832100	Because if you can't, if you have an oligopoly
2832100	2833900	and you have entrenched incumbents,
2834620	2838500	the likelihood of self-disrupting,
2838500	2841900	of the new winner emerging within your market,
2841900	2844480	being one of your players, goes down.
2844480	2847300	And so you're gonna be disrupted from outside.
2847300	2848380	That's a huge risk.
2848380	2851820	And so you need competitive, self-disrupting markets.
2851820	2855620	And it seems like some of the policy folks
2855620	2857540	are just acting non-strategically
2857540	2860140	and not considering that.
2861100	2865980	But fortunately, what has been passed seems sensible.
2865980	2867740	Can you comment in particular
2867740	2872740	on the EU AI legislation and the Canadian?
2872820	2875180	I probably can't say anything too specific.
2875180	2880100	I think the Canadian legislation hasn't gone through yet.
2880100	2883740	The EU AI Act has, but fortunately,
2883740	2886420	it was reigned quite far back
2886420	2889260	from its initial position.
2889260	2891300	I think all of those regulators were in conversation
2891300	2895020	with all of them when you talk to the folks,
2895020	2896740	they wanna do the right thing.
2896740	2899060	They're under a lot of pressure from different parties
2899060	2902340	with conflicting interests,
2902340	2903300	but they're trying to do the right thing.
2903300	2904540	They're trying to make sure this technology
2904540	2906140	gets out into the world in a safe way,
2906140	2908140	that there's oversight,
2908140	2911300	that we don't entrench the incumbents.
2911300	2916300	And we ideally actually sort of bias towards disruption
2916460	2921460	and the creation of new value and innovation, new players.
2925060	2928740	So I think they all want that, but it's a tightrope.
2928740	2932660	It's a very difficult line to walk.
2932660	2934860	I think one of the issues is that
2934860	2937500	not a lot of people certainly in the government
2937500	2939340	understand how this technology works.
2939340	2940860	It seems like magic.
2940860	2944660	And many people, I mean, even in the AI space,
2944700	2946860	I mean, Lacoon and Hinton, for example,
2946860	2948980	people have very different opinions about it,
2948980	2950860	but I'm also interested in your views
2950860	2952940	on the kind of health of the startup scene.
2952940	2954860	So we're in a bit of a downturn at the moment.
2954860	2957580	It doesn't seem to have affected the LLM space,
2957580	2959700	but even in the LLM space, I've noticed a trend
2959700	2963060	that many people started kind of wrapper companies
2963060	2966420	where they did an LLM, but it didn't really do anything
2966420	2968460	that couldn't easily be replicated.
2968460	2970180	And what are your thoughts there?
2970180	2973020	Do you think we're gonna see a trend towards startups
2973060	2975740	doing something that is very differentiated?
2975740	2978700	Yeah, I mean, I think we're in a moment of churn.
2978700	2983700	So I think there are gonna be some players
2983700	2988700	who started a while ago who fold or go into other companies,
2988700	2991180	get acquired, that type of thing,
2991180	2993100	but there's a whole new generation emerging.
2993100	2997660	I know a bunch of people starting up, Ivan and I,
2997660	3001100	we invest in startups and we're seeing an uptick
3001100	3003500	in the number of AI startups that are coming out.
3004980	3006820	It's sort of like a reformatting.
3006820	3009500	There was a bunch of folks building at one layer,
3009500	3013240	like the LLM layer or the one layer above that,
3014500	3015820	tooling, et cetera.
3016740	3019140	The players have kind of been set in that space,
3019140	3020980	it seems, yeah, of course,
3020980	3023240	I'd be happy to see new players emerge.
3024500	3029500	But we now need a set of ideas and products and companies
3030100	3031900	building up the stack.
3031900	3036900	So more abstract concepts, stuff like end user products
3037300	3040140	and agent companies, they're all starting to pop up
3040140	3043940	and create really interesting new ideas.
3043940	3045500	And then that will sort of settle
3045500	3047820	and we'll have our players at that layer.
3047820	3050980	So it's a continuous cycle.
3052820	3055220	Yeah, I'm really excited about the AI startup space.
3055220	3057900	It feels like we're finally starting to get our feet
3057900	3058740	on the ground a little bit.
3059020	3061460	For example, with the tool use, with the RAG,
3061460	3063740	it's starting to look a lot more like traditional
3063740	3064820	software engineering.
3064820	3066980	So what we're seeing now is people kind of rolling up
3066980	3068380	their sleeves and actually building out
3068380	3070660	these very sophisticated software architectures
3070660	3073180	that compose LLMs in interesting ways.
3073180	3075460	And they're not just kind of, you know,
3075460	3078020	just building a simple LLM with a prompt on the top.
3079020	3081100	Yeah, it's definitely getting more sophisticated.
3081100	3085140	And as the tools get more robust and reliable,
3085140	3088980	it's unlocking totally new applications.
3088980	3092060	And the utility is starting to be seen and felt
3092060	3093100	in the real world.
3093100	3095420	I think last year was very much like the year
3095420	3098260	the world woke up to the technology
3098260	3100820	and got their footing with it.
3100820	3102380	So it got familiarity.
3104260	3109260	This year is when things are gonna start hitting production.
3109260	3111220	They're gonna actually start to hit our hands
3111220	3114260	and we're gonna be able to use this as part of our work,
3114260	3118340	part of our play, the products that we use.
3119380	3122260	It's gonna become a much more fundamental part
3122260	3123420	of our daily life.
3125820	3126780	So it's very gratifying.
3126780	3131020	Like we've been building Cohere for four and a half years now.
3131020	3132080	We're in our fifth year.
3135580	3137660	And I think for a long time,
3137660	3139660	we were out there sort of preaching,
3139660	3142100	this is really cool, please care about this.
3142420	3145460	Like this is gonna be an important thing.
3145460	3146780	And folks would pat us on the back
3146780	3149180	and say, nice science project.
3152180	3155260	But finally, we're actually starting to see
3155260	3157820	the fruits of all that labor.
3157820	3161540	And so it's really gratifying to see real world impact.
3161540	3163180	And I think that's what we exist for
3163180	3165100	is really trying to accelerate that
3165100	3168420	and make more of it happen faster
3168420	3172060	and in the best way possible.
3172060	3173660	And what was your biggest mistake?
3173660	3176540	I mean, do you have any advice for other startup founders?
3176540	3179860	What did you do that perhaps they should avoid?
3181100	3186100	I fucked up constantly at every stage of the company.
3189220	3192980	I think, I guess just like admitting that you've messed up
3194060	3198420	and trying not to be in denial about it
3198420	3200260	and fixing it as quickly as possible
3200300	3203620	has been the most important thing
3203620	3207740	to continue to thrive and exist.
3209020	3212340	But yeah, this is the first company I started.
3212340	3213700	Same for Nick and Ivan.
3213700	3217260	And so the whole founding team, we were fresh into it
3217260	3220040	and we made potentially every mistake
3220040	3221400	you could possibly make.
3223300	3228060	Fortunately, we were good at listening to others
3228100	3229580	who had done it before
3229580	3232180	and seen a lot more than we'd seen.
3232180	3234740	And so I'm sure we've dodged some mistakes,
3234740	3237940	but it feels like we've made them all.
3237940	3239500	Yeah, if you don't make mistakes you're not learning,
3239500	3241060	I guess, but just final question.
3241060	3244700	How do you, because it's such a large organization now
3244700	3246980	and there's this problem of vertical information flow.
3246980	3248140	So there might be a problem
3248140	3249860	that some of your folks have discovered now
3249860	3252580	and it takes a while to filter through to you.
3252580	3254700	But obviously it needs to be scalable
3254700	3255540	so you need to delegate.
3255540	3256860	How do you deal with that?
3256860	3261580	Yeah, I'm very close to like the ICs.
3261580	3262420	I'm very close.
3262420	3266580	Like I'm not someone who works through their reports
3266580	3268420	or follows the chain of command.
3268420	3272540	I just talk to the people who are actually doing the work.
3274020	3277540	And so information flows quite freely.
3279140	3282020	I'm sometimes, I think I'm mostly annoying people
3282020	3283700	at this point because I'm like pinging them every day.
3283700	3284580	How's that run going?
3284580	3286660	You know, have we tried this experiment?
3286660	3288740	I'm very deeply involved in stuff.
3289700	3291380	So we haven't had too much.
3292740	3297740	I don't feel like there's an information flow issue
3298100	3303100	echo here with scaling, collaboration between teams,
3303380	3305120	especially when you're a global company
3305120	3308120	and you're not sitting in the same office as a person.
3309860	3310980	That's very difficult.
3310980	3313220	I think remote work is really, really hard.
3313220	3314380	It's not easy.
3314380	3315220	It's not easy.
3315220	3320220	And I think that concentrating teams to geographical areas
3320820	3324900	or at least time zones is very important
3324900	3328780	and leads to a lot more productivity and effectiveness.
3328780	3331780	It's part of the reason I moved here to London
3331780	3334220	is to be closer to a good chunk
3334220	3335460	of our machine learning team.
3335460	3338180	Phil's here, Patrick is here.
3340660	3343620	Like I want to be present and involved in the ML
3343620	3346180	component of the company as much as possible.
3349340	3351140	Yeah, I think as we've scaled,
3351140	3354140	there's been systems that we've used
3354140	3356100	that have broken down at each phase,
3356100	3358940	stuff that worked for the first 10 of us,
3358940	3361220	didn't work for the next 20,
3362220	3364180	didn't work for the next 100.
3364180	3369180	Now we're pushing 350, I think.
3369740	3372260	And there are people at the company who I don't know,
3372260	3373380	which is insane.
3374620	3378780	A super weird experience.
3381060	3383260	But we've hired really fantastic people
3383260	3384740	and we continue to do so.
3384740	3388780	And I think you just trust that people will still
3388780	3390620	make the right decisions going forward
3390620	3393460	and that you've set standards high enough
3395740	3398140	that you don't need to approve every single hire.
3398140	3399740	You don't need to know what every single person
3399740	3400820	is working on.
3401820	3405180	And you just trust your colleagues.
3405180	3407900	Yeah, I can attest that you hire very well.
3407900	3409820	It's probably the best culture I've ever seen
3409820	3411700	in any company, actually.
3411700	3413020	Thank you so much.
3413020	3415020	Final question, I mean, just out of interest,
3415020	3417540	do you get like microcosms in the different offices?
3417540	3421220	I mean, do you see like different mini cultures?
3421220	3422940	Oh yeah, totally, 100%.
3422940	3426140	Like the London office compared to the Toronto office
3426140	3428980	compared to SF, New York.
3430900	3433300	The vibes are very, very different.
3433300	3434620	Like so different.
3436180	3438180	London is so nice.
3438180	3441020	It feels really tight-knit.
3441020	3442820	Like it still feels like a startup,
3444420	3445340	which we are a startup,
3445340	3447900	but it still feels like a tiny 30-person startup
3447900	3451420	where you go out for beers with your colleagues
3451420	3454700	after work at the pub like regularly.
3454700	3456620	Everyone knows what everyone else is working on
3456620	3460700	inside the office, calls on each other for help.
3460740	3461980	London is super tight-knit.
3461980	3463620	I think the culture here is like,
3465380	3466900	I can't pick favorites,
3466900	3468580	but I really like the culture here.
3470060	3472380	In Toronto, it's our biggest office,
3472380	3474940	and so it's much broader,
3476780	3478020	but the culture there is amazing too.
3478020	3480700	Super hard-working, stay late,
3480700	3483220	and like grind very passionate.
3483220	3484820	There's like different groups
3484820	3486620	that are close with each other there.
3487620	3490900	New York is new, but that city is just so much fun.
3490900	3493500	It's just like such an incredible city,
3493500	3496140	so much energy, always awake,
3496140	3497780	work hard, play hard.
3499580	3501340	SF, I spend the least time in.
3501340	3505420	I'm not a huge SF fan, to be completely honest.
3505420	3506860	It's our second HQ,
3507740	3511820	but I just haven't gotten into the city,
3511820	3513980	I think, in the way that others have.
3514820	3518620	I think SF compared to like New York, Toronto, London,
3519540	3521580	I feel like New York, Toronto, and London
3521580	3524220	are real cities.
3525220	3528580	There are artists, there are, you know,
3528580	3532780	people just doing a very diverse set of things,
3532780	3536380	and across all the different fields
3536380	3538060	that are going on there,
3538060	3540220	you have some of the best people in the world.
3541220	3544100	SF is much more, it feels more homogenous to me.
3544100	3545540	It's a lot of people doing the same stuff.
3545540	3547500	There's sort of one topic of conversation.
3547500	3549860	You don't bump into someone
3549860	3554060	with a categorically different worldview than you,
3554060	3556060	or perspective, or experience.
3558060	3559500	And so I like visiting,
3559500	3563500	because I meet like brilliant people in our field, in tech.
3565460	3568460	But to live there would be really cool.
3568460	3572420	To live there would be really difficult for me.
3572420	3577420	I would feel like I'm sacrificing whole pieces of my life.
3581340	3582420	But I love visiting.
3582420	3583260	It's a great place.
3583260	3584900	And I love the folks who are there,
3584900	3587460	and most of our investors are there.
3587460	3591300	And so it's a really cool environment,
3591300	3593700	very intense and like competitive,
3593700	3595940	and those are really good things.
3595940	3598260	It's very motivating to be there.
3598260	3600260	But I think I can get that just by visiting.
3600260	3603140	I don't need to commit myself full time.
3603980	3606300	Aidan Gomez, it's been an honor and a pleasure.
3606300	3607580	Thank you so much for joining us.
3607580	3609220	Thank you so much for having me.
