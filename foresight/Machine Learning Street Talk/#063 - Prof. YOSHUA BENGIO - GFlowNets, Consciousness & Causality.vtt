WEBVTT

00:00.000 --> 00:03.560
my pleasure. And I must say, I've been really impressed by all your

00:03.560 --> 00:08.360
questions. It showed that you did prepare and read papers and think

00:08.360 --> 00:12.760
about it. And that's very much appreciated. Thank you.

00:21.040 --> 00:24.120
Today is an incredibly special occasion. We have Professor

00:24.120 --> 00:27.960
Yoshua Benjiro on the show. Just honestly, I just can't get over

00:28.080 --> 00:31.920
it. But first of all, a little bit of housekeeping. So we've

00:31.920 --> 00:35.320
just launched a new Discord community. So please jump in

00:35.320 --> 00:39.160
there, say hello, introduce yourself. If you want to be, you

00:39.160 --> 00:42.640
know, part of the moderating community or just help us do

00:42.640 --> 00:45.320
stuff over there, we would love to talk with you. By popular

00:45.320 --> 00:47.480
demand, we've also added a couple of ways in which you can

00:47.480 --> 00:51.200
support us. So we now have a Patreon and a merch store. If

00:51.200 --> 00:53.840
you're interested in supporting some of the episodes of MLST

00:53.840 --> 00:55.440
then get in touch with us because we'd love to have a

00:55.440 --> 00:59.240
conversation with you. We're just doing so much cool stuff

00:59.240 --> 01:02.360
this year. We've already recorded about six episodes that we

01:02.360 --> 01:05.320
haven't released. And we've got some amazing people booked as

01:05.320 --> 01:08.800
well. So yeah, it's going to be incredible. As always, if you

01:08.800 --> 01:11.240
like the content here, please consider hitting the like and

01:11.240 --> 01:14.120
subscribe button and rating our podcast on iTunes because it

01:14.120 --> 01:16.640
really, really helps us out. I called it iTunes. Is it

01:16.640 --> 01:19.760
iTunes? Apple podcasts? I don't know, whatever it's called.

01:21.200 --> 01:24.520
Wait and Biases is the developer first MLS platform. And

01:24.520 --> 01:27.400
we're extremely proud today that they are sponsoring this

01:27.400 --> 01:30.840
episode. Now tracking machine learning experiments is

01:30.840 --> 01:34.840
difficult. Using the winging it methodology can only get you so

01:34.840 --> 01:38.960
far. What we need is a platform where we can compare models and

01:38.960 --> 01:42.160
visualize their performance characteristics against all of

01:42.160 --> 01:45.400
the previous runs and figure out the best hyper parameters to

01:45.400 --> 01:49.240
use. Now most importantly of all, this process needs to be

01:49.320 --> 01:52.920
reproducible. Sounds like a tool order, right? Well, this is

01:52.920 --> 01:57.320
exactly what the Wait and Biases platform does for you. Now you

01:57.320 --> 02:00.680
can even follow metrics from long running experiments in real

02:00.680 --> 02:04.240
time. I think it's really important to lean into the

02:04.240 --> 02:08.600
complex interaction between science and engineering in the

02:08.600 --> 02:12.840
ML DevOps lifecycle. Data scientists need valuable feedback

02:12.920 --> 02:15.320
and they need to communicate why they're running given

02:15.320 --> 02:18.680
experiments and they need to share their notes around the next

02:18.680 --> 02:22.880
steps. Reports keep this work well organized and connected to

02:22.880 --> 02:26.080
the Waits and Biases experiments which were run as opposed to

02:26.080 --> 02:30.080
just sharing random screenshots in Slack. It's so easy to create

02:30.080 --> 02:32.720
a report and share it with your team after you finished with

02:32.720 --> 02:35.560
your experimentation. You could just add notes for yourself as

02:35.560 --> 02:38.640
well to explore later on. You can keep a work log and you can

02:38.640 --> 02:42.280
even share your findings internally or externally. This is

02:42.280 --> 02:45.960
an absolute game changer. I'm a big believer in this kind of

02:45.960 --> 02:49.520
engineering rigor. I'm the CEO of a code review startup called

02:49.520 --> 02:53.600
Merge these days and I love how the pull request process and

02:53.600 --> 02:57.280
tooling immortalizes important collective decisions which were

02:57.280 --> 03:00.520
made during the software development lifecycle. Similarly,

03:00.720 --> 03:04.000
Waits and Biases immortalizes important decisions that were

03:04.000 --> 03:07.880
made during model development, experimentation and

03:07.880 --> 03:12.320
deployment. Remember, check out Waits and Biases today by going

03:12.320 --> 03:16.720
to 1db.com forward slash MLST and if you're interested in

03:16.720 --> 03:19.960
sponsoring future episodes, get in touch with us. Waits and

03:19.960 --> 03:23.240
Biases are currently sponsoring our premiere shows but we have

03:23.240 --> 03:25.520
lots of other content coming and opportunities for

03:25.520 --> 03:27.720
sponsorship so let us know. Cheers.

03:29.480 --> 03:32.240
Professor Yoshua Benjo has just released a bunch of papers

03:32.280 --> 03:36.160
around G-Flow Nets. Now G-Flow Nets exists squarely in the

03:36.160 --> 03:40.800
domain of active learning which is a model that can economically

03:40.800 --> 03:43.920
ask an oracle which is probably the real world for the most

03:43.920 --> 03:48.280
salient training examples to continue learning. The learner

03:48.280 --> 03:51.720
can choose or have an influence on the examples it gets and we

03:51.720 --> 03:54.600
want to learn a function which approximates the oracle

03:54.640 --> 03:58.320
efficiently. How should we pick the queries? How should we take

03:58.320 --> 04:02.400
into account not just the value of the predictor but also how

04:02.400 --> 04:05.280
certain we are about the predictors from the learning

04:05.280 --> 04:10.000
system? Areas of uncertainty or entropy are kind of like

04:10.040 --> 04:13.800
interesting candidates for us to explore further. We need to

04:13.800 --> 04:18.760
be able to imagine or invent queries to give to the oracle.

04:19.320 --> 04:21.800
Now one of the reasons that machine learning models are so

04:21.800 --> 04:25.120
sample and efficient is because of the combinatorial space of

04:25.120 --> 04:28.920
possible input examples. We can't train on everything because

04:28.920 --> 04:32.880
the space is just too large, it's vast. So you might have heard

04:32.880 --> 04:35.640
of a related concept of active learning called machine

04:35.640 --> 04:38.880
teaching which is an interactive version where the human

04:39.160 --> 04:43.400
interactively selects the most salient data to train a machine

04:43.400 --> 04:47.480
learning model maximizing the information gain in respect of

04:47.480 --> 04:53.200
the training samples. Now the reality is the function space

04:53.280 --> 04:56.760
that we're learning here is highly structured. We only really

04:56.760 --> 04:59.760
need to sample training data where most of the rich

04:59.760 --> 05:02.920
information exists in that function space. I mean, if you

05:02.920 --> 05:05.520
think about it, a machine learning model, it's just a joint

05:05.520 --> 05:09.520
probability distribution between signals and labels. And this

05:09.520 --> 05:16.280
distribution has modes or areas of density or information. And

05:16.400 --> 05:20.520
actually most of it is just areas of nothingness, which

05:20.520 --> 05:25.600
require fewer training examples to learn and to represent. Now,

05:26.040 --> 05:29.280
if you spoke to a to a Bayesian person like my friend Conor

05:29.280 --> 05:33.000
Tan at work, you know how to learn this distribution, they

05:33.000 --> 05:36.720
would bring up Markov chain Monte Carlo quicker than a whip it

05:36.720 --> 05:40.920
with a bumful of dynamite. Now Markov chain Monte Carlo is an

05:40.920 --> 05:44.200
increasingly popular sampling method for obtaining asymptotic

05:44.200 --> 05:47.160
information about unnormalised distributions or energy

05:47.160 --> 05:50.320
functions, especially for estimating the posterior

05:50.320 --> 05:53.000
distribution in Bayesian inference, which is where you've

05:53.000 --> 05:55.880
probably heard of it before. Now you can characterize a

05:55.880 --> 05:58.400
distribution without knowing all of the distributions

05:58.400 --> 06:00.600
mathematical properties. So if you don't have an analytical

06:00.600 --> 06:04.080
representation for it, just by randomly sampling values out of

06:04.080 --> 06:07.880
the distribution. Now a particular strength of Markov chain

06:07.880 --> 06:10.640
Monte Carlo is that it can be used to draw samples from

06:10.640 --> 06:14.320
distributions, even when all that is known about the

06:14.320 --> 06:17.680
distribution is how to calculate the density for different

06:17.680 --> 06:22.280
samples. Now the the Markov property of Markov chain Monte

06:22.280 --> 06:26.400
Carlo is this idea that random samples are generated by a

06:26.400 --> 06:30.520
special sequential process. And each random sample is used as a

06:30.520 --> 06:35.680
stepping stone to generate the next random sample. Now this

06:35.680 --> 06:39.200
might sound very complex, but the practical implementation is

06:39.200 --> 06:42.320
pretty simple. Markov chain Monte Carlo just starts with an

06:42.320 --> 06:46.520
initial guess, just one value that might plausibly be drawn

06:46.520 --> 06:50.040
from the distribution. And then we produce a chain of samples

06:50.040 --> 06:54.000
from this initial guess by adding random perturbations in the

06:54.000 --> 06:58.240
neighborhood of that example. And each new proposal drawn from

06:58.240 --> 07:00.960
that random perturbation distribution is either rejected

07:01.040 --> 07:04.360
or accepted. There are different flavors of this, of course, I

07:04.360 --> 07:07.960
mean, in particular, like tweaking, how the random

07:07.960 --> 07:10.920
proposals in the neighborhood are selected or whether the

07:10.920 --> 07:15.200
proposals are selected. The simplest heuristic being whether

07:15.200 --> 07:19.080
it's below the function or not. Now the idea is that Markov

07:19.080 --> 07:22.440
chain Monte Carlo methods, they capture a distribution with

07:22.440 --> 07:25.960
only a relatively small number of random samples. But the

07:25.960 --> 07:31.240
reality is anything but in high dimensions, and where the

07:31.240 --> 07:34.560
distribution has many modes spread far apart, it's actually

07:34.680 --> 07:39.200
exponentially expensive. There's a bunch of human orientated

07:39.200 --> 07:42.240
hacks to try and make this work well in specific cases. But we're

07:42.240 --> 07:46.040
missing a much more general machine learnable solution. This is

07:46.040 --> 07:48.720
the main reason why we haven't seen it used in many machine

07:48.720 --> 07:51.480
learning applications yet. Assuming that the function we want to

07:51.480 --> 07:55.480
learn has underlying structure, then we can escape the

07:55.480 --> 07:59.120
exponential time of Markov chain Monte Carlo with machine

07:59.120 --> 08:02.680
learning. And this is what Benzio calls systematic

08:02.720 --> 08:07.280
generalization, which is to say, how do we generalize far from

08:07.280 --> 08:11.680
the data in a way which is meaningful. Now G flow nets are

08:11.680 --> 08:14.600
an active learning framework, where the name of the game is to

08:14.600 --> 08:18.080
generate salient and diverse training data to augment our

08:18.080 --> 08:21.200
model in the most sample efficient way possible. For G flow

08:21.200 --> 08:24.960
nets to work, we need a reward function and a deterministic

08:24.960 --> 08:28.680
episodic environment. Does that sound familiar? Yes, just like

08:28.680 --> 08:32.520
reinforcement learning. Now a flow network is a directed graph

08:32.720 --> 08:38.080
with sources and sinks and edges carrying some amount of flow

08:38.080 --> 08:40.880
between them, you know, through intermediate nodes. So I think a

08:40.880 --> 08:44.000
good way to think about this is pipes of water. Now for our

08:44.000 --> 08:48.680
purposes, we define a flow network with a single source. So the

08:48.680 --> 08:51.280
root nodes, or you might say the sinks of the network

08:51.320 --> 08:55.000
correspond to the terminal states. Now it's designed to find

08:55.000 --> 08:59.360
the possible trajectories through our system. Okay, and just

08:59.360 --> 09:01.560
think of Alpha zero as being like a good analogy for these

09:01.560 --> 09:04.920
trajectories. Now the training objective is to make them

09:04.960 --> 09:09.120
approximately sample in proportion to the given reward

09:09.120 --> 09:12.680
function. This is in stark contrast to Alpha zero where we

09:12.680 --> 09:16.480
were sampling to maximize the expected reward. So Benzio's big

09:16.480 --> 09:19.360
idea is that we can have an interacting loop between a

09:19.360 --> 09:22.400
generative model and the real world. The real world is

09:22.400 --> 09:26.160
expensive. So why not train an imagination machine in our mind

09:26.280 --> 09:29.560
until we're ready and waiting to produce good questions to the

09:29.560 --> 09:32.920
real world, we could use imagined experiments to train our

09:32.920 --> 09:38.680
generator, then produce queries to the real world. We were

09:38.680 --> 09:43.480
thinking about a way to visualize how G flow nets work when the

09:43.480 --> 09:48.640
idea of a Galton board came to mind. A Galton board also known

09:48.640 --> 09:53.200
as a beam machine is a common prop in statistics courses,

09:53.560 --> 09:59.520
science museums, and fun gadget stores. The board has rows of

09:59.520 --> 10:05.240
interleaved pegs above a bottom row of buckets. Beads are filled

10:05.240 --> 10:09.000
into a funnel at the top of the board and then sprinkled on the

10:09.000 --> 10:13.360
top center peg. The beads bounce either to the left or to the

10:13.360 --> 10:17.520
right as they hit the pegs and eventually collect into buckets

10:17.560 --> 10:22.280
at the bottom. If the pegs are precisely and symmetrically

10:22.280 --> 10:25.640
arranged, the beads will aggregate at the bottom into a

10:25.640 --> 10:31.320
familiar binomial bell curve. Now imagine that the pegs were

10:31.320 --> 10:35.760
instead flow gates with adjustable valves that could

10:35.760 --> 10:40.400
direct the beads more to the left or more to the right to

10:40.400 --> 10:44.680
bias the flow paths. With such a machine, you could adjust the

10:44.680 --> 10:51.120
valves or flow rates to create any distribution. For example, to

10:51.120 --> 10:54.680
create a uniform distribution, we'd open up the gates flowing

10:54.720 --> 10:59.960
away from the center line of the board to drive more bead flow

11:00.080 --> 11:04.080
to the fewer number of paths leading to the edges and the

11:04.080 --> 11:10.000
corners. Or to create a multimodal distribution, we'd arrange

11:10.000 --> 11:13.920
the gates to split the flows into two or more streams that would

11:13.920 --> 11:19.920
then pile up in multiple humps or modes below. There's a lot of

11:19.920 --> 11:23.840
flexibility here. Indeed, given a distribution, there are

11:23.840 --> 11:28.720
generally multiple flow gate solutions to produce it. It'd be

11:28.720 --> 11:32.920
nice, wouldn't it? If we had an intelligent, principled way to

11:32.920 --> 11:38.320
train these gates. Enter G flow nets. G flow nets put a neural

11:38.320 --> 11:42.920
network, a brain behind the flow adjustments. A brain which can

11:42.920 --> 11:47.880
optimize the gates to match any distribution we desire. Here,

11:47.880 --> 11:51.680
we're interested in sampling a reward function in the context

11:51.680 --> 11:56.480
of reinforcement learning. In that context, this is a powerful

11:56.480 --> 12:01.320
simulation and sampling paradigm. You see, once the brain has

12:01.320 --> 12:05.240
tuned the flow weights, such a modified Galton board, or more

12:05.240 --> 12:10.400
generally, a flow network, can sample diverse paths quickly

12:10.520 --> 12:15.200
and efficiently, leading to the reward distribution. It's

12:15.200 --> 12:18.960
important to point out that the path sampling is more diverse

12:19.120 --> 12:23.640
doing it this way. Unlike classic reinforcement learning, a G

12:23.640 --> 12:27.080
flow net doesn't just fixate on a small number of high reward

12:27.080 --> 12:31.600
paths, it happens to find first. Instead, it stochastically

12:31.600 --> 12:35.440
samples a broad spectrum of paths in proportion to their reward.

12:35.880 --> 12:39.440
Sure, high reward paths will be sampled with higher weight. But

12:39.440 --> 12:43.640
the far larger population of low reward paths will get a share

12:43.680 --> 12:47.240
of the sampling as well. Why should we even bother with such

12:47.240 --> 12:52.160
paths? The answer is we need to balance exploitation or high

12:52.160 --> 12:56.160
reward with exploration or learning to better learn the

12:56.160 --> 12:59.760
reward function. This is especially important when dealing

12:59.760 --> 13:04.960
with complex real world scenarios of high uncertainty. For

13:04.960 --> 13:09.200
example, think of molecular drug discovery and design or

13:09.440 --> 13:14.120
navigating jungle terrain. In both those scenarios, we really

13:14.120 --> 13:19.080
know very little about how a particular path may play out. We

13:19.080 --> 13:23.000
might stumble into the next miracle cure or a pitfall of

13:23.000 --> 13:27.520
quicksand. To find the globally best paths, it's important to

13:27.520 --> 13:32.800
keep our options open. Beyond this sampling diversity, G

13:32.800 --> 13:36.400
flow nets also bring the full power of neural networks to

13:36.440 --> 13:41.640
discover latent structure and learn the reward function. This

13:41.680 --> 13:46.160
combined with their diverse sampling also makes G flow nets

13:46.160 --> 13:50.160
more robust when dealing with multimodal distributions, which

13:50.160 --> 13:54.360
are a common trap for greedy algorithms and Markov chain,

13:54.400 --> 13:58.760
Monte Carlo. If there is structure linking the multiple

13:58.760 --> 14:03.920
nodes, G flow nets can learn it and extrapolate to new modes

14:04.000 --> 14:08.840
and once discovered, they will by design drive the sampling to

14:08.840 --> 14:14.240
cover those modes and learn more structure. Overall, G flow

14:14.240 --> 14:19.520
nets seem to offer an intriguing new path pun intended for an

14:19.520 --> 14:24.680
intelligent sampling paradigm. So you might ask how are G

14:24.680 --> 14:27.920
flow nets different from Alpha zero? Well, the policy network

14:27.920 --> 14:31.440
and Alpha zero gives you a set of actions. Given a state, Alpha

14:31.480 --> 14:35.600
zero trains the policy network to maximize reward so that the

14:35.600 --> 14:39.640
trajectories all end up at the highest reward. Now what G

14:39.640 --> 14:43.560
flow nets do is they train so that the actions are distributed

14:43.720 --> 14:47.280
in proportion to the reward. So rather than pruning away all of

14:47.280 --> 14:50.640
the low reward trajectories, it will sample them just less

14:50.640 --> 14:56.160
often. Now there is a manifest difference between G flow nets

14:56.160 --> 14:58.880
in respect of exploration. I mean, you might argue that the

14:58.880 --> 15:02.440
Monte Carlo tree search is still doing wide exploration at the

15:02.440 --> 15:05.920
beginning. But in spite of its rapid convergence and pruning of

15:05.920 --> 15:09.280
load reward trajectories, it's still sampling from the

15:09.280 --> 15:11.400
underlying probability distribution, which has been

15:11.400 --> 15:14.720
scaled with a softmax. So that's actually not that much to

15:14.720 --> 15:18.400
explore in the first place. So in summary, G flow nets are

15:18.400 --> 15:21.600
better than Alpha zero Monte Carlo tree search in some sense,

15:21.600 --> 15:24.760
because they achieve the same goal by offloading the burning

15:24.760 --> 15:27.600
time and the stabilization time of Markov chain Monte Carlo.

15:27.760 --> 15:30.360
Remember this whole thing can be trained offline. And then

15:30.360 --> 15:33.200
when in inference mode, we can do it in a single shot. Whereas

15:33.200 --> 15:36.080
with Monte Carlo tree search, we actually had to do it in

15:36.080 --> 15:39.760
inference mode as well. The other thing is we're kind of

15:40.440 --> 15:43.320
offloading all of the human engineering required to sample

15:43.320 --> 15:46.040
efficiently from Markov chain Monte Carlo. And the other thing

15:46.040 --> 15:49.480
is diversity, baby. I mean, consider the difference between

15:49.480 --> 15:52.200
how G flow nets and Alpha zero sample the reward path

15:52.200 --> 15:54.880
distribution. If you looked at the distributions, you would

15:54.880 --> 15:59.080
see that Alpha zero has a little box around the mode. G flow

15:59.080 --> 16:02.400
nets is the whole distribution. We know very well that

16:02.400 --> 16:05.840
diversity preservation is critical in order to discover

16:05.840 --> 16:09.320
interesting stepping stones and search problems. Now finally,

16:09.320 --> 16:12.200
Benzio has published results showing the G flow nets converge

16:12.200 --> 16:15.640
exponentially faster than Markov chain Monte Carlo and PPL on

16:15.640 --> 16:19.480
some problems and finds more of the modes in the distribution

16:19.480 --> 16:22.480
function faster. Enjoy the show folks.

16:23.320 --> 16:27.920
Professor Yoshio Benzio is recognized worldwide as one of

16:27.920 --> 16:32.480
the leading experts in artificial intelligence. Indeed, a god

16:32.480 --> 16:35.600
father of deep learning. His pioneering work in deep learning

16:35.600 --> 16:38.880
earned him the Turing Award, which is the Nobel Prize of

16:38.880 --> 16:42.480
computing. He's a full professor at the University of Montreal,

16:42.680 --> 16:46.600
and the founder and scientific director of Miele, which is a

16:46.600 --> 16:50.800
prestigious community of more than 900 researchers specializing

16:50.800 --> 16:54.560
in machine learning and AI. He's one of the most cited

16:54.560 --> 16:58.280
computer scientists on the planet. And I can't even begin to

16:58.280 --> 17:01.200
articulate how honored we are today to have this conversation.

17:01.600 --> 17:05.120
Yoshio has done a lot of work recently on G flow nets, which

17:05.120 --> 17:08.040
are an active learning framework in a reinforcement learning

17:08.040 --> 17:12.160
configuration, where the name of the game is to request salient

17:12.200 --> 17:16.000
and diverse training data from the real world to augment our

17:16.000 --> 17:19.960
learned models in the most sample efficient way possible. Now

17:19.960 --> 17:22.640
we're trying to minimize the divergence between the path

17:22.640 --> 17:26.120
distribution and the reward distribution, and then sample

17:26.120 --> 17:30.040
paths according to the reward distribution. This is in stark

17:30.040 --> 17:32.800
contrast with traditional reinforcement learning, where we

17:32.800 --> 17:36.760
trying to maximize the expected reward. This approach is likely

17:36.760 --> 17:40.280
to find diverse strategies instead of being greedy and

17:40.280 --> 17:43.720
converging quickly after finding a single one. Anyway,

17:43.840 --> 17:46.840
Professor Benzio, this is amazing. Can you tell us about this

17:46.840 --> 17:48.840
exciting work in some of its applications?

17:49.520 --> 17:54.520
Yeah, I'm, I don't think I've been as excited about a new topic.

17:56.400 --> 18:00.800
At least in the last six or seven years as I'm now with G flow

18:00.800 --> 18:06.240
nets. And it's actually even much more than what you've been

18:06.240 --> 18:11.200
talking about. The way I think about G flow nets is a kind of

18:11.720 --> 18:14.120
framework for generic

18:15.920 --> 18:22.840
learnable inference for probabilistic machine learning. So

18:23.280 --> 18:28.160
one way to think about this is it's a learnable replacement for

18:28.200 --> 18:32.560
Monte Carlo Markov chain sampling. But actually, so there's

18:32.560 --> 18:37.640
that and I'll explain if you want why this is important and to use

18:37.640 --> 18:41.160
machine learning there. But but also, it can be used to

18:41.160 --> 18:44.960
estimate probabilities themselves, not just sampling, but

18:44.960 --> 18:45.920
also estimate

18:47.560 --> 18:51.600
intractable quantities like partition functions and a

18:51.600 --> 18:54.960
condition probabilities that would otherwise require summing

18:54.960 --> 19:00.120
over an intractable number of terms. So I think of this as

19:00.120 --> 19:03.640
the potentially, you know, there's still we're still at the

19:03.640 --> 19:08.280
beginnings of this has a Swiss army knife of probabilistic

19:08.280 --> 19:11.960
modeling that uses machine learning to be able to do things

19:11.960 --> 19:16.120
that look intractable, but do them efficiently thanks to

19:16.120 --> 19:18.280
generalization power of large neural nets.

19:19.520 --> 19:22.960
We've been trying to think of a way to help our listeners

19:23.120 --> 19:28.680
visualize what a what a G flow net does. And I wanted to run by

19:28.720 --> 19:31.520
a possibility to you. So I'm not sure if you've heard of

19:32.000 --> 19:36.360
Galton boards also called, you know, bean machines. And what

19:36.360 --> 19:39.680
they are is this prop that's often used by statistics

19:39.680 --> 19:43.080
professors at the start of say, an elementary introductory

19:43.080 --> 19:47.080
course to give a visual intuition. And it's a board that has

19:47.080 --> 19:51.640
these vertical buckets down at the bottom with interleaved rows

19:51.640 --> 19:57.080
of pegs above the buckets, and then beads are filled in into the

19:57.080 --> 19:59.800
top of the board, and they bounce either left or right as

19:59.800 --> 20:03.200
they hit the pegs. And they eventually collect down at the

20:03.200 --> 20:05.960
bottom. Yeah, yeah. Yeah, now now if the peg is a very good

20:05.960 --> 20:08.960
analogy, except that it's not a tree, I don't know how these

20:08.960 --> 20:12.200
things are, but you know, the ball can come to a place from

20:12.200 --> 20:14.800
two different paths or an potentially large number of

20:14.800 --> 20:18.800
paths. Right, right. And I think, given given there are some

20:18.840 --> 20:22.520
some differences, you know, the idea was that if the pegs or

20:22.520 --> 20:25.400
no, it's it's pretty close to exactly what it is. Yeah, and

20:25.400 --> 20:28.360
what we were thinking is that if the pegs on the Galton board

20:28.360 --> 20:31.720
are precisely and symmetrically arranged, you know, the beads

20:31.760 --> 20:36.200
will form a nice binomial curve at the bottom. And it seems

20:36.200 --> 20:39.520
like what G flow nets are capable of doing when they

20:39.520 --> 20:44.360
optimize the pathways. They're tweaking the pegs a little bit

20:44.360 --> 20:48.160
to the left, or a little to the right, to bias the flow of

20:48.160 --> 20:51.440
beads one way or the other. And in this way, a G flow net

20:51.440 --> 20:54.240
could arrange the pegs so that the beads could form any

20:54.240 --> 20:57.640
distribution at the bottom that we want. And for our purposes,

20:57.640 --> 21:01.800
that means the distribution that matches the reward function. So

21:01.800 --> 21:05.520
is this a good way to think about G flow nets? Yes, it is. Now,

21:05.560 --> 21:07.880
it's missing a really important aspect of it, which would be

21:07.880 --> 21:14.800
difficult to send visually, but that all of these peg weights,

21:15.400 --> 21:20.960
like the polytip that are boggles left or right, are not just

21:20.960 --> 21:23.920
like learned independently, like as a tabular machine

21:23.920 --> 21:28.480
earning, but that there's like one neural net that knows about

21:28.520 --> 21:34.560
the locations in this big board as input and tells, you know, how

21:34.560 --> 21:38.080
much relative weight should I, you know, go to go left or right

21:38.080 --> 21:42.640
at this position. So the reason this is important is because it

21:42.640 --> 21:47.000
allows for generalization. Because this board is huge, it's

21:47.000 --> 21:50.120
exponentially large. So there's no way you're going to learn, like

21:50.120 --> 21:54.280
a separate parameter for each of these choices. And so you have

21:54.280 --> 21:57.960
this neural net or potentially several neural nets, but that

21:57.960 --> 22:02.560
share allow you to share statistical strength, as we

22:02.560 --> 22:06.800
call it, share information across all the possible positions, so

22:06.800 --> 22:10.200
that you can generalize to places paths that it has never

22:10.200 --> 22:14.080
seen from a finite number of training trajectories that it

22:14.080 --> 22:16.940
sees while it's being trained. And that's crucial. Otherwise, you

22:16.940 --> 22:19.980
couldn't scale to large problems, which is really what we

22:19.980 --> 22:20.500
want to do.

22:21.220 --> 22:24.260
Professor Benjo, we spoke with Professor Carl Friston about his

22:24.260 --> 22:27.780
free energy principle, an active inference, which is pretty much

22:27.780 --> 22:30.620
a Bayesian flavored version of reinforcement learning. And he

22:30.620 --> 22:33.220
said that while we need to maintain entropy and stop

22:33.220 --> 22:36.500
models from increasing too much in complexity, we should balance

22:36.500 --> 22:38.860
entropy with accuracy in a principled way. And by the way,

22:38.860 --> 22:40.420
you can kind of think of them in just the audience think of

22:40.420 --> 22:44.500
entropy as keeping your options open. But Friston thinks that

22:44.500 --> 22:46.860
the Bellman-esque idea of reinforcement learning, which is

22:46.860 --> 22:50.020
to say maximizing expected reward is the objective is

22:50.020 --> 22:52.740
misguided. And we should instead perform inference over future

22:52.740 --> 22:56.700
paths, balancing expected reward of relative entropy. Is there a

22:56.700 --> 22:59.260
connection between these ideas? I mean, it seems like G flow

22:59.260 --> 23:01.420
nets are sampling paths proportional to the reward

23:01.420 --> 23:04.820
function, that will maintain as much entropy as the reward

23:04.820 --> 23:05.620
function itself.

23:07.580 --> 23:11.620
Yes, yes, exactly. It's a translation of the reward

23:11.620 --> 23:17.140
function into machinery that can sample, you know, the equivalent,

23:17.140 --> 23:21.340
the corresponding distribution. So yeah, I completely agree with

23:21.620 --> 23:27.660
what Carl was saying here. But as I said, what's interesting is,

23:29.140 --> 23:35.060
we can do things with G flow nets. In principle, we've done the

23:35.060 --> 23:37.780
math and some small scale experiments that we have now a

23:37.780 --> 23:41.060
number of papers, we can do things that go beyond sampling.

23:42.180 --> 23:47.660
But for example, estimate entropy itself. So entropy is

23:48.060 --> 23:57.220
notoriously difficult to estimate. And I mentioned in my talks on

23:57.220 --> 24:02.140
G flow nets that we can use the G flow net machinery to estimate

24:02.180 --> 24:06.620
entropy of say, an action distribution or a distribution

24:06.620 --> 24:09.980
over Bayesian parameters, for example, which is would be

24:09.980 --> 24:13.260
something you'd like to minimize if you're going to take an

24:13.260 --> 24:16.900
action in the world. And you have a model of the world that has

24:16.900 --> 24:20.780
uncertainty. And that connects with Carl, for instance,

24:20.780 --> 24:24.860
interest, you'd like to be able to choose an action that

24:24.860 --> 24:28.460
minimizes your uncertainty about how the world works, we know

24:28.460 --> 24:32.900
what are the latent things that may have happened. And good, you

24:32.900 --> 24:36.540
know, an important part of that is estimating the reward for

24:36.580 --> 24:40.660
the these exploratory actions, like, you know, children playing

24:40.660 --> 24:45.540
around is how much reduction in entropy of my knowledge of the

24:45.540 --> 24:48.060
world, I'm going to get through that action. So you need to be

24:48.060 --> 24:51.380
able to compute that reward. That reward word is basically an

24:51.380 --> 24:55.860
entropy over something you care about. And it turns out you can

24:55.860 --> 24:57.540
also do that with G flow nets.

24:57.700 --> 24:59.980
We're actually speaking with Friston again next week, do you

24:59.980 --> 25:02.260
have a question that you would like us to put to him?

25:03.140 --> 25:07.540
Well, he, you know, he's on the biology side of things much

25:07.540 --> 25:14.140
more than I am. And I believe there are amazing scientific

25:14.140 --> 25:22.380
opportunities to explore how the kind of machinery that G

25:22.380 --> 25:27.260
flow nets offer could be used by brains in order to do some of

25:27.260 --> 25:31.580
the things they do. Using your nets to model the probabilistic

25:31.580 --> 25:33.860
structure of the world, including uncertainty, which is

25:33.860 --> 25:38.420
something he cares about. But but also taking into consideration

25:38.420 --> 25:41.780
things like high level cognition, the global workspace theory,

25:41.780 --> 25:45.620
which is something I care a lot about, attention, they all kind

25:45.620 --> 25:53.780
of fit in the picture of G flow nets. So so I think there's a

25:53.820 --> 25:59.020
huge potential of research at the synergy of computational and

25:59.020 --> 26:03.460
theoretical neuroscience, and machine learning, probabilistic

26:03.460 --> 26:07.940
modeling of the kind that G flow nets propose to come up with a

26:08.980 --> 26:13.860
some proposals for explanatory theories about what the brain

26:13.860 --> 26:18.300
does, that's probabilistic. And, you know, I think he would be a

26:18.300 --> 26:19.900
great person to be part of that.

26:20.220 --> 26:24.220
Fascinating. Well, going a little bit further down that line,

26:24.420 --> 26:27.340
there are folks in the community who are huge advocates of

26:27.340 --> 26:29.220
biologically inspired approaches to machine

26:29.220 --> 26:32.540
intelligence. And, you know, one of the key ideas actually is

26:32.540 --> 26:35.620
diversity, discovery and preservation, both in how

26:35.620 --> 26:38.260
knowledge is acquired and represented. I mean, specifically

26:38.260 --> 26:41.300
evolutionary algorithm advocates, they differentiate

26:41.300 --> 26:44.060
themselves from gradient based single agent monolithic

26:44.060 --> 26:47.140
approaches like reinforcement learning. And they point out that

26:47.220 --> 26:50.380
their approaches overcome so called deception and search

26:50.380 --> 26:52.540
problems, you know, which is to say they don't get stuck in

26:52.540 --> 26:55.700
local minima, your approach seems to be achieving something very

26:55.700 --> 26:58.460
similar in the context of a gradient based reinforcement

26:58.460 --> 27:00.420
learning package. I mean, I don't see it as being mutually

27:00.420 --> 27:01.940
exclusive. But what's your take on this?

27:02.460 --> 27:07.260
Yeah, diversity is important when you're exploring and humans,

27:07.260 --> 27:10.220
especially young ones are exploration machines, they're

27:10.220 --> 27:12.460
trying to understand how the world works and they're acting in

27:12.460 --> 27:16.500
the world in order to get that information. Yeah, I agree that

27:16.940 --> 27:23.740
that search process needs to have a big bonus on on diversity,

27:23.740 --> 27:28.340
like on trying different ways of achieving something good, like

27:28.860 --> 27:33.900
better understanding how the world works. So it turns out that

27:33.900 --> 27:39.220
in the G flow net framework, you, you have a training objective

27:39.220 --> 27:43.420
that yields this kind of diversity and exploration, but is

27:43.420 --> 27:47.220
based on training large neural nets end to end. Now, it's a bit

27:47.220 --> 27:52.260
different from the usual end to end training, because we don't

27:52.260 --> 27:55.340
have an objective that objective we're trying to optimize

27:55.340 --> 28:00.660
is not tractable, actually. But we can sample these trajectories,

28:00.660 --> 28:04.220
which I think of like sampling thoughts, like our thought

28:04.220 --> 28:07.820
process is going through some chain of explanations, not a

28:07.820 --> 28:10.620
complete, and it doesn't represent all the explanations,

28:10.620 --> 28:14.380
but but what we found with our training objectives for G

28:14.380 --> 28:19.260
flow nets is that these sort of random randomized kind of views

28:19.260 --> 28:23.100
of the world are sufficient to give a training signal to the

28:23.100 --> 28:25.300
neural nets that do the real job.

28:26.300 --> 28:30.460
I'm curious. So this trade off between exploration versus

28:30.500 --> 28:34.820
exploitation. And this has come up in so many contexts, you know,

28:34.820 --> 28:37.580
throughout our show. And one in particular, as we talked to, you

28:37.580 --> 28:42.100
know, we've talked to multi arm banded folks, right? And G flow

28:42.100 --> 28:45.740
net seemed to capture this balance between exploration

28:45.740 --> 28:49.740
and exploitation. But the multi arm banded folks, you know, they

28:49.740 --> 28:53.700
dive deep in that research circle into this into this trade off.

28:53.700 --> 28:57.940
And I think they have some very principled ways and even very

28:57.940 --> 29:02.420
rigorous ways to analyze this fundamental trade off. To what

29:02.420 --> 29:05.620
extent do you think that that their research maybe could be

29:05.620 --> 29:10.100
applied to future G flow net variations? Like do you think

29:10.100 --> 29:14.060
maybe it might open up more options to fine tune the

29:14.060 --> 29:16.460
trade off between exploration and exploitation?

29:17.460 --> 29:22.300
Yeah, I mean, the banded research is very, very closely

29:22.300 --> 29:27.260
related to the G flow net thread. But G flow nets, as we have

29:27.260 --> 29:30.860
been using them, for example, for drug discovery, they are

29:30.860 --> 29:35.380
banded. It's just that the action space is not, you know, one

29:35.380 --> 29:40.540
out of n things. It's, it's combinatorial because you build

29:40.540 --> 29:43.860
these pieces. So the action space is not something you can

29:44.020 --> 29:48.060
enumerate. So you can't apply the typical banded algorithms, but

29:48.060 --> 29:52.300
a lot of the math is totally applicable. And in fact, what we

29:52.300 --> 29:59.260
use in the drug discovery setting is UCB upper confidence

29:59.260 --> 30:10.820
bound objective to learn a good exploration policy. So that comes

30:10.860 --> 30:16.020
out of the banded research. It what it does is it, you know, it

30:16.020 --> 30:21.980
combines the risk and reward expected reward, one of these

30:21.980 --> 30:27.100
together in a way that in theory guarantees that you will do an

30:27.100 --> 30:31.740
efficient exploration and find that where is the, you know,

30:32.140 --> 30:35.620
where's the money? Where's the reward, right? All of the

30:35.620 --> 30:37.660
possible places where you can get the reward.

30:38.140 --> 30:43.660
So in, in, in the G flow net papers, you often describe it as, you

30:43.660 --> 30:48.180
know, we want to sample not only the maximum reward path, in

30:48.180 --> 30:51.700
order to have more diversity in order to maybe figure out

30:51.700 --> 30:55.260
something that we didn't know if we were just to go to the

30:55.260 --> 30:58.740
maximum reward. And that speaks a little bit to the, like the

30:58.740 --> 31:03.820
things that we know that we don't know, right? We maybe know

31:03.820 --> 31:08.180
that, right, this seems like a lower reward trajectory might

31:08.180 --> 31:12.220
turn out to be a higher reward trajectory. However, exploration

31:12.220 --> 31:14.900
and reinforcement learning is also fundamentally addressing the

31:14.900 --> 31:18.860
things about the things that I don't know that I don't know,

31:18.940 --> 31:23.100
which is where stuff like random exploration and things like

31:23.100 --> 31:26.980
this comes in. Could you maybe comment a little bit on how you

31:26.980 --> 31:31.300
see sort of, because it seems to me that if I managed to sample

31:31.300 --> 31:35.700
according to what I think is the reward distribution, right, I

31:35.700 --> 31:39.220
still have this problem of maybe there is a deceptive rewards

31:39.220 --> 31:42.780
there are, you know, I need to take a step back, I may not know

31:42.780 --> 31:46.820
some sort of some, some area of the search space. And don't I

31:46.820 --> 31:48.980
just run into the same problems again?

31:50.340 --> 31:56.700
So, so the important trick here is you need your model of the

31:56.700 --> 32:02.300
reward distribution, or the reward function to be one that

32:02.300 --> 32:05.700
captures uncertainty, like, maybe in a Bayesian way, or, you

32:05.700 --> 32:09.540
know, whichever way, the Bayesian way, by the way, fits well

32:09.540 --> 32:15.660
with the G flow net framework, because we can consider the

32:15.940 --> 32:20.380
parameters of the reward function as latent variables, like

32:20.380 --> 32:22.500
you don't actually know the reward function, you're trying

32:22.500 --> 32:25.500
to figure it out from experiments. So the G flow

32:25.500 --> 32:32.140
net can sample, and not just like what you should be doing in

32:32.140 --> 32:37.540
order to acquire information, but also potential reward function.

32:37.540 --> 32:41.740
So, you know, we don't actually have a knowledge of how the, you

32:41.740 --> 32:43.220
know, what's going to be the rewards we're going to get in

32:43.220 --> 32:48.380
the world. Classical IRL is going, as you said, to the expected

32:48.380 --> 32:52.820
value and try to maximize that, whereas the G flow net approach

32:52.860 --> 32:56.420
is trying to acquire as much knowledge as possible about the

32:56.420 --> 32:59.380
underlying reward function. So you're trying to minimize the

32:59.380 --> 33:04.860
uncertainty. So your model with the G flow net is modeling the

33:04.860 --> 33:11.460
uncertainty, and then it can use it as a reward for the policy

33:11.460 --> 33:14.660
that is going to do action in the real world. So we're talking

33:14.660 --> 33:17.460
about different G flow nets. There's a G flow net that models

33:17.460 --> 33:20.420
the uncertainty in the reward that you're going to get from the

33:20.420 --> 33:24.660
real world. And that's like a Bayesian model. And then you have

33:24.660 --> 33:29.300
another G flow net that controls the policy that searches to

33:29.380 --> 33:33.140
and its reward is how much uncertainty reduction you're

33:33.140 --> 33:40.100
going to get by doing this or that. So, so yeah, you need to

33:40.100 --> 33:45.460
have a part of your model that is kind of aware of the fact

33:45.460 --> 33:48.060
that there are whole areas in the world that you don't know

33:48.060 --> 33:50.580
about or aspects of the world that you don't know about so

33:50.580 --> 33:52.780
that you can drive the exploration.

33:53.580 --> 33:56.140
I would love to know where some of the magic is coming from.

33:56.780 --> 33:59.740
The promise of G flow nets is that we can discover as many

33:59.740 --> 34:03.420
modes as possible in the path distribution. Traditionally in

34:03.540 --> 34:06.660
Markov chain Monte Carlo, we had to hack priors into the

34:06.660 --> 34:09.380
algorithm by hand, you know, to find new modes or areas of

34:09.380 --> 34:11.580
information efficiently, especially when they were very

34:11.580 --> 34:15.100
far apart or not very sharp. The hypothesis of G flow nets is

34:15.140 --> 34:18.380
that the structure of these modes is learnable on many

34:18.380 --> 34:21.060
problems, even in high dimensions. It's a little bit like

34:21.060 --> 34:23.100
saying we're getting a free lunch. I mean, actually, I think

34:23.100 --> 34:25.980
you used that exact phrase to describe what we're doing here.

34:26.220 --> 34:29.700
Many research avenues have tried to develop general methods to

34:29.700 --> 34:32.780
discover these structures and have failed. How do you think

34:32.780 --> 34:36.020
G flow nets will overcome this seemingly intractable curse?

34:37.020 --> 34:40.340
There is no guarantee that they will, because if there is no

34:40.340 --> 34:43.740
structure in the underlying function you're trying to

34:43.740 --> 34:47.020
discover. So let's say the reward function or the energy

34:47.020 --> 34:52.100
function that you care about, then having visited some finite

34:52.100 --> 34:58.220
number of modes like regions where your reward is high site is

34:58.220 --> 35:00.340
not going to tell you anything about what are the other good

35:00.340 --> 35:04.580
places, the other modes. So so there's no guarantee that it

35:04.580 --> 35:09.120
will work. But if if there is structure, then there is a free

35:09.120 --> 35:12.780
lunch. And we know machine learning is good at that. Like

35:13.340 --> 35:17.540
the last 10 years of deep learning and its success. What is it

35:17.540 --> 35:20.140
telling us? It's telling us that you can generalize right that

35:20.140 --> 35:23.540
these nets, I'm not saying they generalize perfectly, but they

35:23.540 --> 35:28.900
can generalize. So you can think of it like the machine

35:28.900 --> 35:34.260
learning problem is given some examples of good things, like,

35:34.300 --> 35:37.260
you know, places where you get reward, you can you generalize

35:37.260 --> 35:41.180
to other places. And the supervised learning way of

35:41.180 --> 35:44.060
thinking about it is, you know, given a candidate place, tell me

35:44.060 --> 35:47.740
how much reward I think I would get. The G for that sampler is

35:47.740 --> 35:50.900
learning the inverse function is like to sample, but it's kind of

35:50.900 --> 35:54.020
the same thing. It's just going in the other direction. Give me

35:54.020 --> 35:57.700
some, you know, sample some some good places that that you know,

35:57.700 --> 36:02.520
where the reward is high. So we now have a lot of experience in

36:02.520 --> 36:07.780
designing powerful your nets that can be leveraged to

36:07.780 --> 36:12.020
generalize in those spaces where we normally use MCMC. And if

36:12.020 --> 36:17.500
there is kind of regularities that allow to generalize, then all

36:17.500 --> 36:20.300
of that can be, you know, put to use.

36:21.100 --> 36:23.820
We mentioned earlier, reinforcement learning often

36:23.820 --> 36:27.260
being applied in a context where you have this kind of solid

36:27.260 --> 36:30.100
reward function. So let's say games, you know, playing chess.

36:30.540 --> 36:33.700
I'm really curious, what would happen hypothetically, if we

36:33.940 --> 36:39.540
applied G flow net, you know, to something like chess. So I mean,

36:39.540 --> 36:42.180
I think given the fact that reinforcement learning like say

36:42.180 --> 36:45.300
alpha zero is trained specifically to choose the best

36:45.300 --> 36:49.580
move rather than diverse moves, it seems obvious that maybe if

36:49.580 --> 36:53.860
given equal resources to both alpha zero and flow zero, alpha

36:53.860 --> 36:58.060
zero would probably beat flow zero. However, I think if flow

36:58.060 --> 37:01.620
zero were given more resources, say and trained to the same

37:01.620 --> 37:05.940
rating, say the same elo rating as alpha zero, it seems like

37:06.060 --> 37:09.740
flow zero, if you would, would play significantly more

37:09.740 --> 37:14.460
diverse and interesting games with a wider variety of styles.

37:14.460 --> 37:17.060
And I think you could even imagine also that it could be

37:17.060 --> 37:21.500
possible, even if given equal resources, but sufficiently

37:21.500 --> 37:25.660
high enough resources, that a hypothetical flow zero would

37:25.660 --> 37:29.260
consistently reach higher ratings, because it might find, you

37:29.300 --> 37:32.340
know, more interesting stepping stones that have the

37:32.340 --> 37:36.340
potential to avoid deception, because it can explore seemingly

37:36.620 --> 37:41.060
lower reward paths that ultimately develop into higher reward.

37:41.100 --> 37:42.820
More curious if you have any thoughts on that?

37:43.780 --> 37:51.100
Yeah. It's a good question. I would say where the kind of

37:51.100 --> 37:55.020
approach we've been pioneering with G flow nets might be really

37:55.020 --> 37:59.860
paying off is if you think about it from the perspective of the

37:59.860 --> 38:03.340
learner has a finite computational, you know, amount of

38:03.340 --> 38:07.700
resources, because in principle, right, if you had infinite

38:07.700 --> 38:11.940
compute, and you know the reward function, like the rules of

38:11.940 --> 38:16.620
chess or go, then you can just crank and find, you know, the

38:16.620 --> 38:21.780
policy that's best in every possible setting. Now, if you

38:21.780 --> 38:24.780
have finite resources, like, you know, you, you have a budget

38:24.780 --> 38:30.780
of compute, you'd like to use it efficiently. And so that's

38:30.780 --> 38:33.820
where the exploration exploitation trade off becomes

38:33.820 --> 38:44.340
important. And if you if you had a, say, a current policy that

38:44.340 --> 38:48.900
you're not completely sure is the right one. And, and then

38:48.900 --> 38:54.220
you're trying to say, Well, what, how should I play so that I'm

38:54.220 --> 38:57.860
going to improve my policy the most as in I'm going to reduce

38:58.220 --> 39:01.220
the uncertainty that, you know, it is the right policy, like

39:01.220 --> 39:05.620
that it picks the right things. So now we're getting closer to

39:05.620 --> 39:08.740
the kind of setting where it makes sense to use G flow nets.

39:09.420 --> 39:14.020
And then what I would expect, if we do the engineering work

39:14.020 --> 39:17.540
here, but based on the sort of much simpler problems we've

39:17.540 --> 39:22.660
looked at, is that it would converge faster. So given, if you

39:22.660 --> 39:28.860
look at on the x axis, the number of games you're playing. And

39:28.860 --> 39:32.900
on the y axis, how good is your policy measured like on other

39:32.900 --> 39:35.980
games. So that's where you would get. In other words, it's the

39:35.980 --> 39:39.580
learning curve that you might gain on asymptotically, everything

39:39.580 --> 39:43.700
is going to converge the optimal chess player, right? So the

39:43.700 --> 39:46.900
the place where it's interesting is to look at the learning

39:46.980 --> 39:50.340
curve how fast you learn. And here you want to sort of active

39:50.340 --> 39:53.580
learning thinking like, Well, I'm not just trying to win here.

39:54.220 --> 39:58.980
I'm trying to gather information so that I'll win more in the

39:58.980 --> 40:02.300
future. And it's a different objective. And that's where you

40:02.300 --> 40:05.220
need diversity and exploration and like a model of your own

40:05.220 --> 40:08.180
uncertainty and an active learning policy.

40:09.380 --> 40:14.580
How much do you think this could be part of not maybe only

40:14.580 --> 40:20.540
reward maximization things, but information collection, things

40:20.540 --> 40:25.940
like, I'm sure you're you're thinking about in, let's say the

40:25.940 --> 40:29.380
brain, there is there's sort of maybe a similar process going

40:29.380 --> 40:32.860
on and what do I still need to retrieve in order to give certain

40:32.860 --> 40:37.780
answers to questions, or maybe in our, let's say, big search

40:37.780 --> 40:42.540
engine, let's just name one for naming sake, let's Google, or

40:42.580 --> 40:47.820
so would would try to answer your query, not by just searching

40:47.820 --> 40:50.700
through their index, but by actively doing this multiple

40:50.820 --> 40:53.780
multiple things like, is this enough? Is this enough? Is this

40:53.780 --> 40:58.260
enough? Do you see connections to these types of things? Or are

40:58.260 --> 41:01.500
they inherently different? Because they might be not learning

41:01.500 --> 41:02.260
on the spot?

41:02.820 --> 41:08.060
What they're doing on the spot is acquiring information. And you

41:08.060 --> 41:10.140
want to do it in an efficient way. And that's where sort of the

41:10.140 --> 41:11.740
active learning thinking comes in.

41:12.700 --> 41:17.860
And I think it's actually a very big, practical problem in

41:17.860 --> 41:21.260
the deployment of like AI dialogue systems that are not

41:21.260 --> 41:26.260
chit chat, but they're trying to say help a user achieve, you

41:26.260 --> 41:28.900
know, get something get information or something like

41:28.900 --> 41:32.460
this. This is this is a huge need for this in, you know, the

41:32.460 --> 41:35.980
business world and search engines, and you know, it's much

41:35.980 --> 41:39.340
more than search engines. So I don't think we have the

41:39.340 --> 41:43.980
algorithms that do that right now. And it's kind of painful. The

41:43.980 --> 41:49.420
human has to know, you know, is driving. But if, if we had

41:49.420 --> 41:53.980
systems that could explicitly model their own, say,

41:53.980 --> 41:57.580
uncertainty about what the user needs or wants, or where to

41:57.580 --> 42:03.580
find information. And then, and you need like pretty powerful

42:03.580 --> 42:06.300
models of that, like it's not just galaxies, they're simple

42:06.300 --> 42:09.620
things. That's where G flow net strengths comes in, you can

42:09.620 --> 42:12.660
represent very, very complex distributions over

42:12.660 --> 42:19.140
compositional objects. It's not just a few numbers. And then I

42:19.140 --> 42:23.420
think you could get to much more efficient human machine

42:23.420 --> 42:30.420
interfaces. And the same, I believe the same methodology

42:30.420 --> 42:33.420
could be used more generally in scientific discovery. So what

42:33.460 --> 42:35.940
is scientific discovery? Like what is it that scientists do?

42:36.580 --> 42:40.500
They plan experiments that are going to allow them to reduce

42:40.500 --> 42:43.980
the uncertainty on their theories of, you know, some

42:43.980 --> 42:46.420
aspect of the world. It's the same problem. Yeah, you have a

42:46.420 --> 42:50.340
series of questions you're allowed to ask to nature. And you

42:50.340 --> 42:53.340
try to ask as few questions as possible to as quickly as

42:53.340 --> 42:55.060
possible, understand what's going on.

42:56.020 --> 42:59.220
Is there a connection fundamentally to I'm thinking of

42:59.220 --> 43:02.980
causality, which also I've seen a number of papers that you've

43:03.020 --> 43:06.420
collaborated on with people who are who are deep into

43:06.460 --> 43:10.900
causality research and so on. What do you think there is a

43:12.740 --> 43:18.180
a connection there where an agent could learn to uncover if

43:18.180 --> 43:20.700
you think about scientific discovery to uncover the

43:20.700 --> 43:24.780
fundamental causal structure of the world by asking such

43:24.780 --> 43:27.980
questions, like could there be a connection to that branch of

43:27.980 --> 43:32.340
research? And could this finally be like the unification of

43:32.620 --> 43:37.500
of something machine learning and the the world of causality?

43:38.740 --> 43:41.220
Yes, you guys are really asking all the right questions. Thank

43:41.220 --> 43:46.540
you so much. In fact, one of my main motivations for the

43:46.580 --> 43:51.260
pursuing the the G flow net research program is that I think

43:51.260 --> 43:58.100
it's the it's an ideal tool for implementing what I called in

43:58.100 --> 44:03.460
my talks, system to inductive biases. So what this means is

44:03.940 --> 44:06.580
there are lots of things we know from neuroscience and

44:06.580 --> 44:12.220
cognitive science about how we think. And we can bring that

44:12.260 --> 44:18.620
into the design of probabilistic machine learning, you know,

44:18.620 --> 44:22.940
based on deep learning is the building blocks. And one of the

44:22.940 --> 44:26.220
inductive biases, like one of the characteristics of how we

44:26.220 --> 44:29.140
think is we think causally, we're constantly asking the why

44:29.140 --> 44:34.020
questions we're trying to find explanations and so on. And, and

44:34.060 --> 44:37.940
and that connects with classical AI, like the way we think, to

44:37.940 --> 44:42.020
some extent, has also inspired classical AI, you know, rules

44:42.020 --> 44:46.620
and logic and and reasoning. And we haven't yet found the way

44:46.620 --> 44:50.380
to integrate these abilities in deep learning. And of course,

44:50.380 --> 44:54.700
lots of people are like, trying to and and that's important.

44:55.700 --> 44:59.100
But but I think the reason why G for nets give us an amazing

44:59.100 --> 45:03.940
handle on this is because they they're really good at

45:03.940 --> 45:08.300
representing distributions and sampling over graphs. And, and

45:08.300 --> 45:12.660
like a reasoning or a set of possible reasoning to explain

45:12.660 --> 45:18.540
something or to, you know, for planning. The these are graphs.

45:19.780 --> 45:24.300
And your thoughts can be seen as graphs, right? So think of like,

45:24.460 --> 45:27.140
maybe a simple version of this, think of a parse, like a

45:27.140 --> 45:32.500
semantic and syntactic parse of a sentence is a graph. But

45:32.500 --> 45:34.260
usually it's, you know, it's more than a tree, there are all

45:34.260 --> 45:37.300
sorts of semantic connections, including with knowledge graphs,

45:37.300 --> 45:42.900
right, which also graphs. So the ability to implicitly represent

45:42.940 --> 45:46.900
those distributions and sample pieces of them as thoughts is, I

45:46.900 --> 45:50.740
think, fundamental to how we think. And going back to

45:50.740 --> 45:53.020
causality, one of the hard questions that I think G for

45:53.020 --> 45:56.940
nets can help us with is causal discovery. So in other words,

45:57.060 --> 45:59.420
what is the underlying cause structure of the world, including

45:59.420 --> 46:03.980
the uncertainty about it? Given the things we observe, a lot of

46:03.980 --> 46:06.540
the research and causality has been okay, we observe these,

46:06.580 --> 46:10.900
these random variables, discover, you know, make inferences

46:10.900 --> 46:14.140
about, you know, whether what we can say about whether it goes

46:14.140 --> 46:19.500
to be and so on. But it's much harder to discover the causal

46:19.540 --> 46:24.020
graph that that, you know, in a large set of variables, and

46:24.020 --> 46:26.980
it's even harder. And really, nobody's done a real job there.

46:27.420 --> 46:31.060
To do this when what the learner sees is not the causal

46:31.060 --> 46:34.140
variables, but just like low level pixels. And you also have

46:34.140 --> 46:36.260
to figure out what are the causal variables and how they're

46:36.260 --> 46:39.220
related causal. And I think G planets can help us do that.

46:39.940 --> 46:43.060
This this opens up, this is so many avenues of questions, I

46:43.060 --> 46:46.780
think it'll probably almost be a future episode in itself. But

46:46.780 --> 46:52.140
let me just ask you about some of the basic ones, which is, as

46:52.140 --> 46:54.740
you mentioned, kind of learning the causality causality

46:54.740 --> 46:58.580
structure, much more difficult problem. And the first question

46:58.580 --> 47:01.900
is just how to represent the causality. And so you, you, you

47:01.900 --> 47:04.380
mentioned graphs, you know, graphs is one way. And of course,

47:04.740 --> 47:09.260
you can develop, you know, isomorphic ways of representing

47:09.660 --> 47:12.740
certain parts of logic as graphs, etc, depending on how, you

47:12.740 --> 47:16.500
know, how rich you make the graph structure. But there's

47:16.500 --> 47:18.860
also the other issue of, you know, when you're trying to

47:18.860 --> 47:21.620
build, and I think it's probably correct to call this a world

47:21.620 --> 47:24.220
model, right, like we're trying to build a causal

47:24.420 --> 47:27.780
that's the word I use. Okay, great. And I, and so I have one

47:27.780 --> 47:31.860
quick question about that, which is, you know, to me, to some

47:31.860 --> 47:34.860
people, world model is only the discriminative function. It's

47:34.860 --> 47:37.700
just that, you know, probability y given x, to me, it's more

47:37.700 --> 47:41.300
general. It's also the structure of x. Is that, is that also

47:41.300 --> 47:44.540
your, your view as well? Yes. Yes. Okay. And so in

47:44.580 --> 47:47.700
constructing those, those world models, some of the, let's say

47:47.700 --> 47:52.700
the pushback on on these type of generative techniques from, from

47:52.700 --> 47:55.620
folks that are more skew more towards the discriminative side

47:55.980 --> 47:58.860
is, hey, look, fine, you're going to go and try and build this

47:58.860 --> 48:01.580
generative model, it's going to be even more complicated than

48:01.580 --> 48:04.860
this discriminative model, because it also has to learn, you

48:04.860 --> 48:08.020
know, the structure on x. But I think the possible free lunch

48:08.020 --> 48:13.140
here, is that you can learn abstract structure on on x. And

48:13.140 --> 48:17.100
so if you learn these abstract world models, throwing away all

48:17.100 --> 48:19.700
the nitty gritty that doesn't really matter, you can potentially

48:19.700 --> 48:23.660
have very powerful, you know, predictive encoding, if you will,

48:23.700 --> 48:25.420
like, what's, what's your thoughts on that?

48:25.900 --> 48:30.740
Oh, that's what I've been thinking for almost 20 years. And

48:30.740 --> 48:34.140
one of the reasons why I've been interested in deep learning as

48:34.180 --> 48:39.140
a way to think of discovering abstract representations, you

48:39.140 --> 48:44.020
know, from the early days of deep learning, as in like mid like

48:44.020 --> 48:49.180
2005 or something. And, and in the paper that Jan McCarr and I

48:49.180 --> 48:53.020
wrote about, and also other papers I wrote with some of my

48:53.020 --> 48:56.540
colleagues at the University of Montreal on, you know, deep

48:56.540 --> 49:00.780
learning around 2010, they are all about that notion that we

49:00.780 --> 49:05.860
would like these unsupervised learning procedures to discover

49:05.860 --> 49:10.780
these abstract factors, as we call them. But now I think it's

49:10.780 --> 49:14.460
not just the factors like the variables, but it's also more

49:14.460 --> 49:18.100
importantly, even how they're related to each other, which in

49:18.100 --> 49:24.460
the causal language is what we call causal mechanisms. And so

49:24.500 --> 49:27.260
here's a fundamental way of thinking about this. If you

49:27.260 --> 49:31.020
don't introduce the abstract kind of structure that exists in

49:31.020 --> 49:36.900
the world, then representing p of x, the input distribution is

49:36.900 --> 49:40.740
very difficult. It's, in other words, you'll need a lot of data

49:40.740 --> 49:44.540
to learn it. And it's not going to be generalizing very well. The

49:44.540 --> 49:49.260
whole point of abstraction is that it gives you very powerful

49:49.260 --> 49:51.620
abilities to generalize to new settings, including out of

49:51.620 --> 49:53.860
distribution, which is one of the hardest topics in machine

49:53.860 --> 49:57.100
learning right now. How do we extend what we do so that it

49:57.100 --> 50:01.300
generalizes well in new settings? And thinking causally

50:01.300 --> 50:05.900
about these abstract causal dependencies, as the things that

50:05.900 --> 50:10.580
are preserved across changes in distribution, like, if I go to

50:10.580 --> 50:14.820
the moon, it's the same laws of physics, but the distribution is

50:14.820 --> 50:18.460
very different. How do I generalize, you know, across such

50:18.460 --> 50:23.980
changes in distribution? It's because the learner is us, you

50:23.980 --> 50:28.060
know, if we, if we were, if we had the right education, has

50:28.060 --> 50:31.980
figured out the underlying, at least, you know, enough of the

50:31.980 --> 50:35.740
underlying causal mechanisms, that we can be transported in a

50:35.740 --> 50:39.940
different world, but where there's the same laws of physics,

50:40.340 --> 50:44.300
and we can predict what's going to happen, even though it looks

50:44.300 --> 50:47.860
completely different from, you know, our training environment.

50:48.380 --> 50:55.820
So the, the idea of extraction is really that if you introduce

50:55.820 --> 51:01.500
abstractions, the description length of the data becomes way

51:01.500 --> 51:03.780
smaller. And that's why you get generalization.

51:05.380 --> 51:06.060
Absolutely.

51:06.220 --> 51:10.220
I'm fascinated by these abstract categories. I think it's the

51:10.220 --> 51:13.060
most exciting thing in AI. I mean, Douglas Hofstadter spoke

51:13.060 --> 51:16.860
about cognitive categories, like the concept of sour grapes,

51:16.860 --> 51:19.900
for example, to represent the certain thing. And almost

51:19.900 --> 51:23.740
magically, our brain seems to arrange these cognitive

51:23.740 --> 51:25.660
categories. And it's not entirely clear to me whether they're

51:25.660 --> 51:29.260
an emergent phenomenon, or whether it's some other process.

51:29.540 --> 51:31.980
But the modes that you're discovering in G flow nets,

51:31.980 --> 51:35.540
they're a kind of category, these cognitive categories that I

51:35.540 --> 51:38.180
just spoke about our abstractions, also things like

51:38.180 --> 51:41.060
causality and geometric deep learning that they are kinds of

51:41.060 --> 51:43.420
categories. But I've always had this intuition that deep

51:43.420 --> 51:47.700
learning doesn't learn the categories on its own, it needs

51:47.700 --> 51:52.220
humans to kind of put priors into the model, as we do with

51:52.260 --> 51:55.100
geometric deep learning. Do you think that that will always be

51:55.100 --> 51:57.380
the case? Or can we have that meta level of learning?

51:58.180 --> 52:02.820
Yes. What I really want to do is build machines that can

52:02.820 --> 52:06.140
discover their own semantic categories, abstract ones that

52:06.140 --> 52:09.740
really help them understand the world. And of course, they're

52:09.740 --> 52:13.220
going to learn, you know, better and faster if we help them just

52:13.220 --> 52:15.700
like, you know, we teach kids, we don't let them discover the

52:15.700 --> 52:21.780
world by themselves. But we do have an ability to invent new

52:21.780 --> 52:24.020
categories. That's what scientists do all the time,

52:24.020 --> 52:29.380
right? Or artists and, you know, writers and philosophers and

52:29.380 --> 52:32.700
scholars, and ordinary people who find new solutions to

52:32.700 --> 52:36.060
problems, we do that all the time, our brain is a machine

52:36.100 --> 52:39.620
discovers new abstractions. Of course, that usually it's just

52:39.620 --> 52:42.580
like one little bit on top of all the things we got from our

52:42.620 --> 52:48.180
cultural input. But but that's the ability that we don't have

52:48.180 --> 52:53.020
right now in machine learning. And that is going to, I think, be

52:53.020 --> 52:56.500
a huge advantage. So now we're not in reinforcement learning,

52:56.500 --> 52:59.620
we're not in active learning, we're talking about unsupervised

52:59.620 --> 53:04.580
learning. So we're talking about how can a machine discover

53:04.580 --> 53:14.260
these often discrete concepts that somehow help it understand.

53:14.260 --> 53:17.020
So in other words, build a compact understanding of lots of

53:17.020 --> 53:22.380
things that generalize across many settings. And yeah, that

53:22.420 --> 53:28.900
that's that the path to build that is, is becoming more and more

53:29.900 --> 53:35.860
firm in my mind, as I move forward with G flow nets. So as a

53:35.860 --> 53:39.420
clue, there was a paper we had recently, I think in Europe's

53:39.820 --> 53:43.820
on that's connected to the global workspace theory that says

53:43.820 --> 53:47.500
that it's about discrete valued neural communication, I think

53:47.500 --> 53:51.860
is a title where the one interesting intuition here is

53:51.860 --> 53:58.660
connected to this is if you if you constrain the communication

53:58.660 --> 54:01.060
between different modules, say in the brain or in machine

54:01.060 --> 54:04.540
learning system, to use as few bits as possible and discrete is

54:04.540 --> 54:08.220
the way to get the very few bits. You can get better

54:08.220 --> 54:12.180
generalization. And there are good reasons for that that we

54:12.180 --> 54:14.980
try to explain in the paper. But but that's, that's it, you

54:14.980 --> 54:19.980
know, there's a clue here that discrete concepts emerge as a

54:19.980 --> 54:21.460
way to get better generalization.

54:22.860 --> 54:27.820
You you mentioned before, and in terms of discreteness, and

54:27.860 --> 54:32.140
what you mentioned before with graphs being very fundamental, it

54:32.140 --> 54:35.620
connects a little bit back to a paper that you, I think,

54:35.620 --> 54:40.860
provocatively titled the consciousness prior, where where

54:40.860 --> 54:44.980
you connect sort of the ideas of attention, sparse factor,

54:44.980 --> 54:48.660
graphs, language, things being discreet, things being

54:48.660 --> 54:54.500
describable by language, right? And, and I find that all to be

54:54.540 --> 55:00.100
very interesting. On the topic of consciousness, we would be, it

55:00.100 --> 55:03.100
would not be appropriate for us to not put this question to you.

55:03.100 --> 55:07.060
So you're not, you're not very active on Twitter, which is

55:07.060 --> 55:10.940
probably why you're so productive. But if currently,

55:11.060 --> 55:14.580
there is a bit of a of a thing happening on Twitter, namely,

55:15.020 --> 55:21.900
Ilya Satskever of Open AI has tweeted out a seemingly innocuous

55:22.180 --> 55:27.660
tweet saying, it may be that today's large neural networks are

55:27.660 --> 55:33.580
slightly conscious, which has resulted in quite a, let's say,

55:33.580 --> 55:38.300
a storm on of people agreeing, disagreeing. Obviously, he's

55:38.300 --> 55:41.300
he's talking about maybe, you know, the large language models

55:41.300 --> 55:44.540
we have today, which do incorporate a lot of the things

55:44.540 --> 55:47.860
you talk about, they do incorporate attention mechanisms,

55:47.940 --> 55:52.140
lots of them. Presumably, it's all one needs. They do

55:52.140 --> 55:55.260
incorporate language, they do incorporate discrete things with

55:55.260 --> 55:59.140
you know, discrete tokens and so on. What do you make of a

55:59.140 --> 56:02.300
statement like this? It may be that today's large neural

56:02.300 --> 56:04.300
networks are slightly conscious.

56:05.380 --> 56:10.060
Well, this one fundamental problem with such statements,

56:11.500 --> 56:15.940
which is we don't know what consciousness really is. So I

56:15.980 --> 56:21.420
think we have to have a bit of humility here. And I can't say

56:21.460 --> 56:24.740
what Ilya is saying is true or not. I think that this is more to

56:24.740 --> 56:29.060
consciousness than what we have in these large language models by

56:29.060 --> 56:35.540
a big gap. But that being said, and you know, we do need to work

56:35.540 --> 56:39.580
with our colleagues in your science and kind of science who

56:39.580 --> 56:42.860
are trying to figure out what consciousness is from a scientific

56:43.300 --> 56:47.220
perspective and philosophers who are helping also to make sense of

56:47.220 --> 56:54.060
that landscape. So we have to be careful with the use of those

56:54.060 --> 56:56.660
words. And you know, I was a bit liberal in the title of my

56:56.780 --> 57:01.420
paper. And I learned a lot about consciousness since then,

57:02.260 --> 57:05.420
learned that there's a lot that we don't understand that at the

57:05.420 --> 57:08.500
same time, there are enough bits that we know from from

57:08.540 --> 57:14.860
cognitive neuroscience that can serve as inspiration for how we

57:14.860 --> 57:18.540
could build machine learning systems that have similar, say

57:18.540 --> 57:21.580
conscious processing machinery. Okay, let's not say consciousness

57:21.580 --> 57:23.500
but just conscious processing machine because that's less

57:23.500 --> 57:27.300
controversial. And by the way, the word consciousness has been

57:27.300 --> 57:32.260
taboo with most of science for a long time. And it has become

57:32.300 --> 57:36.540
untapped, you know, the tabooed in neuroscience, because we're

57:36.540 --> 57:39.060
starting to be able to make measurements of what's going on

57:39.100 --> 57:42.820
inside your brain, while you're doing things consciously or not

57:42.820 --> 57:46.940
and so on and distinguish the parts that you're consciously

57:46.940 --> 57:49.620
aware of and the parts that are there in your brain, but you're

57:49.620 --> 57:52.140
not conscious. So we're trying to we're starting to make a lot of

57:52.140 --> 57:55.420
progress of what it means to be conscious of something or not.

57:56.940 --> 58:02.180
And I, you know, I think this is a very exciting and important

58:02.220 --> 58:08.540
scientific question. And I would rather like work on exploring

58:08.540 --> 58:12.460
hypotheses and theories to explain our conscious abilities,

58:13.420 --> 58:16.580
rather than make bold statements about whether current neural

58:16.580 --> 58:17.900
nets are conscious or not.

58:18.620 --> 58:22.580
Professor Benjo, we've got some David Chalmers on the show next

58:22.580 --> 58:25.580
month. Do you have any questions that you had put to him?

58:26.420 --> 58:36.060
I very much like a hypothesis about consciousness that Michael

58:36.060 --> 58:45.380
Graziano has put out to help explain the qualia, the subjective

58:45.380 --> 58:51.340
experience part that Chalmers wrote might be something science

58:51.340 --> 58:58.740
can't really, you know, touch. And so what's, you know, I'd like

58:58.740 --> 59:03.060
to hear what he has to say about these kinds of approaches. And

59:03.220 --> 59:10.700
one of the basic premise here is is very grounded in things we

59:10.700 --> 59:16.460
can do scientifically. It's to say, well, let's not try to

59:16.460 --> 59:19.780
figure out what is consciousness or subjective

59:19.780 --> 59:24.940
experience more specifically, you know, from a philosopher's

59:24.940 --> 59:30.100
chairs. But let's let's consider that as a phenomenon that is

59:30.100 --> 59:32.580
happening in the brain. I mean, unless you believe in sort of

59:32.580 --> 59:35.300
supernatural things, if it is happening, something is happening

59:35.300 --> 59:38.360
in the brain, and we can report about it. And we can, we can

59:38.360 --> 59:41.780
like, measure what's going on in various parts of your brain

59:41.780 --> 59:50.540
while this is happening. And then, you know, can we then come up

59:50.540 --> 59:55.580
with theories that explain why we feel that we have subjective

59:55.580 --> 59:57.960
experience? It's not saying whether consciousness exists or

59:57.960 --> 01:00:00.940
not or subjectivity. It's not whether it exists or not in some

01:00:00.940 --> 01:00:03.540
sort of logical sense. It's whether, you know, what is it

01:00:03.540 --> 01:00:06.340
that's going down in our brain that gives us that feeling and

01:00:06.340 --> 01:00:12.540
then make us say, Well, I am, you know, I'm conscious of x, y,

01:00:12.540 --> 01:00:17.860
or z. So so that's the that's the direction I find interesting

01:00:17.860 --> 01:00:21.420
because it opens the door for a scientific investigation. And

01:00:21.420 --> 01:00:26.340
Michael Grosjean has a specific theory about that which I find

01:00:26.340 --> 01:00:32.380
compelling that is really rooted in the idea that we have a world

01:00:32.380 --> 01:00:38.180
model. And then we we because we have an attention that focuses

01:00:38.180 --> 01:00:43.980
only parts of it at a time. And we need to have like a little

01:00:44.100 --> 01:00:48.820
mini world model that controls that attention. That creates a

01:00:48.820 --> 01:00:55.860
sort of separation between the the where the real knowledge is

01:00:55.860 --> 01:01:00.580
and sort of this more abstract control and machinery that could

01:01:00.620 --> 01:01:06.020
well, give us this illusion of Cartesian dualism, which I think

01:01:06.060 --> 01:01:09.980
is an illusion, but but you know, must be grounded in some

01:01:11.860 --> 01:01:12.980
you know, biological

01:01:15.740 --> 01:01:19.580
reality. And that's I think understanding that is is a very

01:01:19.580 --> 01:01:22.580
good question to ask. And I'd like to get to know what he

01:01:22.580 --> 01:01:24.180
thinks about such a research program.

01:01:25.300 --> 01:01:26.220
Thank you very cool.

01:01:26.780 --> 01:01:30.780
Yeah, thank you. I do have one kind of nitty gritty question

01:01:30.780 --> 01:01:34.460
because and partly partly based on some of your recent work on

01:01:34.460 --> 01:01:40.580
becoming more of a fan of semi supervised learning. And you

01:01:40.580 --> 01:01:44.180
know, you had a recent paper that was on interpolation

01:01:44.180 --> 01:01:48.900
consistency training. And what I found interesting about that is

01:01:48.900 --> 01:01:52.580
that if we consider one of the biggest challenges that we face

01:01:52.580 --> 01:01:55.580
in machine learning pretty much across the board is an

01:01:55.620 --> 01:01:58.700
overcoming the various, you know, curses, if you will, the

01:01:58.700 --> 01:02:02.660
various forms of intractability that we have an empirical

01:02:02.660 --> 01:02:06.500
learning methods. And in this context of semi supervised

01:02:06.500 --> 01:02:11.260
learning, that recent paper, it found significant improvements

01:02:11.300 --> 01:02:15.900
over state of the art by forcing linearity. So in this case, it

01:02:15.900 --> 01:02:20.660
was by this mix up between the unlabeled samples and their

01:02:20.940 --> 01:02:25.740
interpolated fake labels. And in the last decade, we've also

01:02:25.740 --> 01:02:29.740
seen values come to dominance in the field of neural networks,

01:02:29.740 --> 01:02:34.900
their piecewise linear recent work by Randall Belastriero,

01:02:35.220 --> 01:02:37.980
developed an interesting frame of reference which cast

01:02:37.980 --> 01:02:41.860
multi layer perceptrons as a decomposition method, which

01:02:41.860 --> 01:02:46.700
produces a honeycomb of linear cells in the ambient space and

01:02:46.780 --> 01:02:50.660
they're activated turned off or on by input examples. So my

01:02:50.660 --> 01:02:54.260
question is, why is linearity, whether it's piecewise or

01:02:54.260 --> 01:02:58.460
otherwise, dominating the state of the art in approximation

01:02:58.460 --> 01:03:01.580
methods, it almost seems to me like we've kind of gone back to

01:03:01.580 --> 01:03:04.660
the future, if you will, sort of leaving behind attempts at more

01:03:04.940 --> 01:03:08.740
smooth nonlinear methods and gone back to newer, albeit more

01:03:08.740 --> 01:03:12.980
complicated forms of linear approximation.

01:03:13.860 --> 01:03:18.940
Right. I would say something that's roughly linear is

01:03:18.940 --> 01:03:22.820
simpler. So having a regularizer that says, oh, you want to be

01:03:22.820 --> 01:03:26.260
roughly linear or locally linear, at least to as much

01:03:26.260 --> 01:03:30.900
extent as you can is a smoothness prior. So that's going to

01:03:30.900 --> 01:03:36.300
help generalization. But it could also hurt if that is too

01:03:36.300 --> 01:03:40.180
strong. And so having these piecewise linear kind of more

01:03:40.780 --> 01:03:45.980
type of solution is a good compromise. It says as few pieces

01:03:45.980 --> 01:03:50.260
as possible, and ideally organized in a compositional way. So

01:03:50.260 --> 01:03:55.660
that it's not just like a relu, it's more like the discrete

01:03:55.700 --> 01:03:59.340
abstract logic, you know, reasoning, things sitting on

01:03:59.340 --> 01:04:04.060
top, that's controlling the pieces. But but otherwise fairly

01:04:04.060 --> 01:04:08.460
simple in each how each of the pieces are, you know, like

01:04:08.460 --> 01:04:12.420
linear, for example. So one way to look at this is, if you

01:04:12.420 --> 01:04:15.660
look at classical, the kind of rules that classical AI

01:04:15.660 --> 01:04:19.660
researchers were using, each rule is fairly simple. It's, you

01:04:19.660 --> 01:04:25.980
know, like, it's almost linear, or it's very simple logic. But

01:04:25.980 --> 01:04:29.900
it's the composition of all those rules that gives the power of

01:04:29.900 --> 01:04:32.620
expression of these systems. Of course, the problem then is that

01:04:32.620 --> 01:04:40.220
they didn't know how to train them properly. But yeah, I think

01:04:41.380 --> 01:04:54.340
we, I think we learn to come up with these discrete ways of

01:04:54.380 --> 01:05:01.020
breaking up things into simpler pieces. And that in fact, I

01:05:01.020 --> 01:05:03.900
think if you're Bayesian about it, it just comes out naturally.

01:05:04.020 --> 01:05:06.140
And they're very, very weak assumptions.

01:05:07.500 --> 01:05:11.940
So in a way, it's it's almost, it is piecewise abstraction. So

01:05:11.940 --> 01:05:15.740
we're kind of back. Yes, that's what I would lean to, rather

01:05:15.740 --> 01:05:20.020
than piecewise linear. But linear, of course, is a broad part

01:05:20.020 --> 01:05:22.580
of, you know, it's an easy way to get simple.

01:05:23.700 --> 01:05:26.740
Amazing. Professor Benjo, I'm interested in your personal

01:05:26.740 --> 01:05:29.540
journey. So we've been talking about diverse trajectories. And

01:05:29.620 --> 01:05:32.180
I wanted to know about your own trajectory of research over the

01:05:32.180 --> 01:05:35.540
last 10 years. Now, one of my mates, a psychologist and

01:05:35.540 --> 01:05:39.020
symbolist, Professor Gary Marcus, presumably one of your best

01:05:39.020 --> 01:05:42.340
friends, by the way, he pointed out in his 2012 New Yorker

01:05:42.340 --> 01:05:45.660
article that MLPs lacked ways of representing causal

01:05:45.660 --> 01:05:48.940
relationships such as between diseases and their symptoms. And

01:05:49.060 --> 01:05:50.900
I think this has been a significant focus of yours in

01:05:50.900 --> 01:05:54.540
recent years as we've discussed. And he thought at the time that

01:05:54.540 --> 01:05:58.420
you were a bit too quote system one all the way. And he spoke

01:05:58.420 --> 01:06:00.780
then about the need for heterogeneous architectures and

01:06:00.780 --> 01:06:03.700
the acquisition of abstract concepts, compositionality and

01:06:03.700 --> 01:06:07.060
extrapolation, which I think has also been a huge focus of yours

01:06:07.060 --> 01:06:09.780
in the last decade or so. We really enjoyed watching your

01:06:09.780 --> 01:06:13.340
debate with Marcus. And by the way, we would love to host V2 of

01:06:13.340 --> 01:06:15.100
that debate. So if you're interested, you just let us

01:06:15.100 --> 01:06:18.020
know we'll do that. But he's often viewed as a heretic. And,

01:06:18.140 --> 01:06:20.580
you know, just forgetting about symbols versus neural networks

01:06:20.580 --> 01:06:23.260
for a minute. Am I right in thinking that you've converged in

01:06:23.260 --> 01:06:25.500
at least some ways in your thinking? And how would you

01:06:25.500 --> 01:06:26.940
characterize that from your perspective?

01:06:27.820 --> 01:06:38.900
So, yeah, I used to be in the 90s, a, you know, pure neural net

01:06:41.380 --> 01:06:50.480
subsymbolic connectionists researcher. And I did my grad

01:06:50.480 --> 01:06:55.740
studies at a time on neural nets at a time when the dominant way

01:06:55.740 --> 01:06:59.580
of thinking was these, you know, classical AI rule based system

01:06:59.580 --> 01:07:03.220
with no learning at all, and was dominant, meaning that the

01:07:03.220 --> 01:07:06.580
little group like, you know, Jan and Jeff and I and others who

01:07:06.580 --> 01:07:13.700
were thinking otherwise, had to, you know, defend our views. And

01:07:14.740 --> 01:07:20.700
and maybe that led to a kind of, you know, us versus them, I

01:07:20.700 --> 01:07:28.460
think, unhealthy way of thinking. And of course, I matured. And

01:07:30.060 --> 01:07:33.260
one of the big, so there, I think there are several turning

01:07:33.260 --> 01:07:38.300
points on that journey. Well, one of them in the in the 2000s

01:07:38.300 --> 01:07:41.420
was the realization of the importance of abstraction. So

01:07:42.500 --> 01:07:44.800
and the way to think about this maybe more concretely, because

01:07:44.800 --> 01:07:46.900
what does it mean to be abstract? Is that I was thinking,

01:07:47.220 --> 01:07:51.820
well, what would be the right kind of representation we want to

01:07:51.820 --> 01:07:54.820
have at the top level of our unsupervised deep nets, because

01:07:54.820 --> 01:07:56.980
we were doing mostly like unsupervised deep nest, like,

01:07:56.980 --> 01:08:01.480
you know, deep boz machines and stuff in that decade. And I was

01:08:01.480 --> 01:08:04.940
thinking, well, it would be things like words, right, things

01:08:04.940 --> 01:08:08.380
like the sort of concepts that we manipulate at the top level,

01:08:08.380 --> 01:08:12.380
well, it's words or, you know, the equivalent, maybe, with

01:08:12.980 --> 01:08:19.260
disambiguated. But yeah, we, it didn't seem that we have the

01:08:19.260 --> 01:08:23.020
right tools for that. And then it remained like an objective. And

01:08:23.020 --> 01:08:32.160
then in 2014, we discovered the power of attention. And that's

01:08:32.180 --> 01:08:35.780
closely connected to abstraction, because what it does is

01:08:35.780 --> 01:08:39.140
it focuses on a few things. And of course, that's our, you

01:08:39.140 --> 01:08:43.060
know, that's very much a characteristic of how we think a

01:08:43.060 --> 01:08:46.620
thought has very few elements in it. That means we have selected

01:08:46.620 --> 01:08:49.300
those elements. And that's where attention comes in. So it's

01:08:49.300 --> 01:08:55.580
getting closer to this ideal of building machines that think like

01:08:55.580 --> 01:09:00.060
humans. And then of course, in 2017, I wrote this consciousness

01:09:00.060 --> 01:09:02.740
prior paper where, you know, I discovered all the work on global

01:09:02.740 --> 01:09:06.100
workspace theory and, and it, you know, and the momentum is

01:09:06.100 --> 01:09:12.620
built up. And of course, now, you know, humans think and they

01:09:12.620 --> 01:09:17.460
use symbols, and they understand the very abstract

01:09:17.460 --> 01:09:20.420
relationships between them. And we need to build neural nets that

01:09:20.420 --> 01:09:26.700
can do that. So I guess where I've maybe departed from Gary,

01:09:26.700 --> 01:09:31.180
but maybe he's moved to is, it's going to be neural nets that

01:09:31.180 --> 01:09:33.620
do it, right? It's just that we're going to be training them in

01:09:33.660 --> 01:09:36.420
a special way. And that's what G flow nets really aiming at.

01:09:37.660 --> 01:09:40.820
So can I just say we, we asked many guests, these, these

01:09:40.820 --> 01:09:43.860
questions about their, their evolution. And sometimes they,

01:09:44.220 --> 01:09:47.620
they tend to be spicier than others. But I have to say, from my

01:09:47.620 --> 01:09:52.060
perspective, your answer was the most informative, the most

01:09:52.060 --> 01:09:56.860
gracious and the most noble of answers that we've heard so far

01:09:56.860 --> 01:10:00.780
to similar questions. So kudos to you. That was awesome.

01:10:00.860 --> 01:10:01.300
Thanks.

01:10:02.500 --> 01:10:05.700
I just cannot believe it. And we always do a hell of a lot of

01:10:05.700 --> 01:10:08.540
preparation. But it's gone to the point now where we know that

01:10:08.540 --> 01:10:11.540
we're not going to get more than about six questions in. So we,

01:10:11.780 --> 01:10:14.380
you know, we kind of like exponentially, you know, have an

01:10:14.380 --> 01:10:16.060
exponential prior on our questions.

01:10:16.060 --> 01:10:19.020
Well, he was awesome, though, with like, you know, we asked him

01:10:19.140 --> 01:10:23.100
to give relatively sort of three minute answers. And he stuck to

01:10:23.100 --> 01:10:26.020
that, which was really cool. I mean, that's, that's very

01:10:26.020 --> 01:10:29.460
helpful to have an interesting dialogue. And I, I can't believe

01:10:29.500 --> 01:10:32.820
how proud I am, you know, that he's, that he appreciates that we

01:10:32.820 --> 01:10:36.340
put the prep time into it. And, you know, had had decent

01:10:36.340 --> 01:10:39.700
questions that were hopefully interesting for him, as well as

01:10:39.700 --> 01:10:44.460
our, as well as our audience. So Dr. Kilcher, lightspeed

01:10:44.460 --> 01:10:46.100
Kilcher, what should I take?

01:10:46.820 --> 01:10:51.700
It's cool is, I mean, his, um, yeah, I think is the thing he

01:10:51.700 --> 01:10:54.700
mentioned at the end, like his humility, it kind of shines

01:10:54.700 --> 01:10:58.740
through everything he does. And he answers, he's like, you know,

01:10:58.780 --> 01:11:03.380
here's the best answer I can give. But, you know, he seems to

01:11:03.380 --> 01:11:10.740
be very, like, open and not, not, not very, yeah, one notices

01:11:10.740 --> 01:11:14.660
he's not on Twitter. It's like, it's noticed that was a

01:11:14.660 --> 01:11:18.460
brilliant question. I think we should post that question on our

01:11:18.460 --> 01:11:21.860
Twitter. Because, you know, that there's that a bit for people

01:11:21.860 --> 01:11:23.700
watching this in a year's time, it's probably forgotten about

01:11:23.700 --> 01:11:27.260
but yeah, that ilia guy from open AI said that the models might

01:11:27.260 --> 01:11:31.180
be slightly conscious. I was exasperated by that. Because I

01:11:31.180 --> 01:11:34.500
watched his interview on Lex. And I know by saying bad things

01:11:34.500 --> 01:11:36.340
about him, he will never come on our podcast, but I don't think

01:11:36.340 --> 01:11:39.020
he would have done anyway. So it doesn't matter. But yeah, I

01:11:39.020 --> 01:11:41.700
think that it's pretty bad.

01:11:42.380 --> 01:11:47.380
What? Why? Yeah, why? It's like, it's like, you don't think

01:11:47.380 --> 01:11:52.060
it's bad? No, he says, I think, because a lot of the folks at

01:11:52.060 --> 01:11:55.620
Open AI, they are, you know, like in the rationalist community,

01:11:55.820 --> 01:12:00.420
and they seriously believe that we're an imminent threat of the AI

01:12:00.660 --> 01:12:03.940
taking over the world and us being paper clips. And I think

01:12:03.940 --> 01:12:07.420
it's next, I listened to his interview on Lex, and he sounded

01:12:07.420 --> 01:12:10.420
like a salesman, talking about Codex and how it was going to

01:12:10.420 --> 01:12:13.420
revolutionize everything. And I honestly think that there's just

01:12:13.420 --> 01:12:17.900
such a divergence between what they're saying and reality right

01:12:17.900 --> 01:12:18.140
now.

01:12:19.300 --> 01:12:23.740
Well, not to drift too far away from from our guests today.

01:12:24.100 --> 01:12:28.260
But so I thought, I thought it was just kind of a shower

01:12:28.260 --> 01:12:32.380
thought, you know, like, you know, the the large neural

01:12:32.380 --> 01:12:35.500
networks of today might be a little bit conscious, right?

01:12:35.500 --> 01:12:39.060
And, and, and you just like, yeah, well, yeah, well, shower

01:12:39.060 --> 01:12:41.460
thought, and it is a shower thought like it needs on

01:12:41.460 --> 01:12:45.500
Twitter, it's just something you tweet out. And, and it brings up

01:12:45.500 --> 01:12:48.100
interesting questions, like it brings up interesting questions,

01:12:48.100 --> 01:12:51.100
like, you know, you're a you're a ball of neurons, like you're

01:12:51.140 --> 01:12:53.740
just a slap together piece of matter, right? You have

01:12:53.740 --> 01:12:58.340
consciousness. So clearly, like something about, you know,

01:12:58.540 --> 01:13:01.900
learning systems combined with data, or maybe not even

01:13:01.900 --> 01:13:06.100
combined with data gives rise to consciousness. So why can't

01:13:06.500 --> 01:13:12.540
why can't another, you know, in silico, slap together system

01:13:12.540 --> 01:13:17.660
of neurons ingested with data be slightly conscious or have

01:13:17.700 --> 01:13:21.820
like, some properties, like, and that's that's essentially, yeah,

01:13:22.620 --> 01:13:27.780
Benjo refused to give like a humble, the humble person he is,

01:13:27.780 --> 01:13:31.820
he refused to give like, you know, the the the strong take on

01:13:31.820 --> 01:13:36.620
that, but that would have because he might just this is my

01:13:36.620 --> 01:13:40.700
opinion, not his obviously, but reading the consciousness prior

01:13:40.700 --> 01:13:45.980
paper, it is not too far off. He formulates consciousness as

01:13:45.980 --> 01:13:50.140
having these elements of, you know, I have my internal state,

01:13:50.140 --> 01:13:54.860
which is sort of everything in my brain that I could bring bring

01:13:54.860 --> 01:13:58.860
up into my forefront, then I get some input from the outside

01:13:58.860 --> 01:14:03.660
world. And through the input, I then filter, like with an

01:14:03.660 --> 01:14:09.420
attention mechanism, I do I look what in my mind, could I now

01:14:09.420 --> 01:14:14.060
bring into focus, right? And that is by use of something like

01:14:14.060 --> 01:14:19.620
an attention mechanism. And then I take that thing. And I put it

01:14:19.620 --> 01:14:26.540
into these abstract concepts I use I represent. I represent the

01:14:26.540 --> 01:14:30.980
concepts in my head as a sparse factor graph. And by focusing on

01:14:30.980 --> 01:14:34.380
parts of that, I can then make inferences in this sparse

01:14:34.380 --> 01:14:38.260
factor graph and so on. Now, obviously, something like GPT three

01:14:38.260 --> 01:14:42.820
doesn't have all of that, at least not explicitly, but some of

01:14:42.820 --> 01:14:46.780
it is there, right? It's, you know, I have a piece of input, I

01:14:46.780 --> 01:14:50.660
have giant amount of weights, I use an attention mechanism to

01:14:50.660 --> 01:14:52.700
sort of see what I can focus on.

01:14:53.660 --> 01:14:56.140
Yeah, but yeah, but I think that I think that's a very

01:14:56.140 --> 01:15:00.180
declarative description of consciousness. And at its roots,

01:15:00.180 --> 01:15:03.660
it's about the phenomenological experience. Right. And I know

01:15:03.660 --> 01:15:07.420
we discussed computationalism and panpsychism. Let's not go down

01:15:07.420 --> 01:15:11.660
that rabbit hole. But surely, they don't think that this model can

01:15:11.700 --> 01:15:13.220
feel well, but so this is

01:15:13.220 --> 01:15:17.060
consciousness is not about feeling. It's about being being

01:15:17.060 --> 01:15:22.820
like aware of of like, I don't even know what it is. I'm just

01:15:22.820 --> 01:15:27.780
saying that it sounded not too far away from what the

01:15:27.780 --> 01:15:31.700
consciousness prior paper was about. And yes, I realize it's

01:15:31.700 --> 01:15:34.940
called the consciousness prior and not consciousness. But you

01:15:34.940 --> 01:15:35.180
know,

01:15:35.980 --> 01:15:38.300
yeah, I mean, I think he answered it the way a scientist

01:15:38.380 --> 01:15:41.180
should answer it. And I was really happy with his answer,

01:15:41.180 --> 01:15:47.500
which is, okay, a consciousness has to be some activity of

01:15:47.500 --> 01:15:50.300
neurons and firings or whatever in the brain or else we're

01:15:50.300 --> 01:15:54.300
talking about magic. And that's not in the field of science. And

01:15:54.300 --> 01:15:57.200
B, you know, whatever that thing is, it's obviously quite

01:15:57.200 --> 01:16:00.940
nuanced and complicated. And we don't have we don't know yet.

01:16:00.980 --> 01:16:05.100
So we need to have some humility here, which means we

01:16:05.100 --> 01:16:08.140
shouldn't be alarmist. So we don't need to be going in, you

01:16:08.980 --> 01:16:13.020
know, burning books tomorrow because because we created a, you

01:16:13.020 --> 01:16:16.500
know, GPT, whatever, that anytime its wheel is spinning,

01:16:16.500 --> 01:16:18.860
and it's actually suffering. You know, if you ask it a

01:16:18.860 --> 01:16:21.060
question that's too hard, and it's spinning, it's because

01:16:21.060 --> 01:16:23.660
you're hurting it and it's suffering. And so we need to

01:16:23.660 --> 01:16:26.340
turn it off like right away. But wait, we can't turn it off

01:16:26.340 --> 01:16:29.300
because then we'd be like, murdering, you know, a sentient

01:16:29.300 --> 01:16:33.460
being or something, like we're way, way too, in our infantile

01:16:33.500 --> 01:16:37.960
understanding of, you know, this type of complex, complex

01:16:37.960 --> 01:16:41.180
behavior, that's the human mind and consciousness to be at that

01:16:41.180 --> 01:16:45.660
point. So from my perspective, he answered it completely 100%

01:16:45.660 --> 01:16:48.860
scientifically. And there's a lot of folks out there who are

01:16:48.860 --> 01:16:52.140
supposed to be scientists that spend a lot of time with, you

01:16:52.140 --> 01:16:57.220
know, unscientific, you know, thinking about it. Cool. Let's

01:16:57.220 --> 01:17:00.420
talk a little bit. What one of the things that I really found

01:17:00.420 --> 01:17:03.940
interesting about Benjo's ideas, other than the causality

01:17:03.940 --> 01:17:07.780
stuff and the system to stuff is this notion of diversity.

01:17:08.380 --> 01:17:10.900
We've had conversations with Kenneth Stanley all about open

01:17:10.900 --> 01:17:13.580
endedness and diversity preservation. We've also had

01:17:13.580 --> 01:17:18.340
conversations with Friston about the importance of balancing

01:17:18.340 --> 01:17:22.060
relative entropy and so on. And we have all of these curses in

01:17:22.060 --> 01:17:24.700
empirical learning, right? The statistical curses, the

01:17:24.700 --> 01:17:25.900
approximation curses.

01:17:26.740 --> 01:17:30.340
Dimensionality, we have to mention dimensionality and

01:17:30.340 --> 01:17:32.980
even, I mean, you know, we're talking about curses in the

01:17:32.980 --> 01:17:35.740
Monty, you know, the Markov chain Monte Carlo in the sense of

01:17:35.740 --> 01:17:38.660
it being a high dimensional space. And we need to assume that

01:17:38.660 --> 01:17:42.700
there's some structure around where these modes are. So all of

01:17:42.700 --> 01:17:45.900
these approaches are ways of simultaneously, and, you know,

01:17:45.900 --> 01:17:48.780
being able to explore but not being cursed. So yeah, what was

01:17:48.780 --> 01:17:49.300
your take on that?

01:17:50.260 --> 01:17:57.020
Well, any one of my take was that I like that he's so

01:17:57.060 --> 01:18:01.180
interested in abstraction, because, you know, to me, that's

01:18:01.180 --> 01:18:04.140
been not only one, you know, it's not only one of the larger

01:18:04.180 --> 01:18:07.580
mysteries, at least for me, I mean, I don't know, of the kind of

01:18:07.580 --> 01:18:12.500
the universe is abstraction, idealism, you know, platonic

01:18:12.500 --> 01:18:15.100
thinking, whatever. I mean, the whole point is just that he

01:18:15.100 --> 01:18:20.060
views abstraction as a key to pragmatically useful, you know,

01:18:20.060 --> 01:18:24.860
pass forward. And it's a hard problem, a really hard problem.

01:18:24.940 --> 01:18:29.180
And, you know, his focus right now is on kind of graph based

01:18:29.620 --> 01:18:32.540
structures. And I have to admit, you know, for me to you,

01:18:32.540 --> 01:18:35.060
they're quite seductive and appealing. I don't know if

01:18:35.060 --> 01:18:37.980
they're the the right path forward, but it's definitely

01:18:37.980 --> 01:18:41.740
cool to see a lot of research, looking into graph based, you

01:18:41.740 --> 01:18:44.940
know, methods, or, you know, hyper graph based methods,

01:18:44.940 --> 01:18:48.620
whatever they are, they seem to definitely be a promising

01:18:48.620 --> 01:18:52.180
path forward. And I think we're in for, hopefully, if we can

01:18:52.180 --> 01:18:55.380
continue to progress at a reasonable rate, you know, some

01:18:55.380 --> 01:18:57.260
some interesting decades ahead.

01:18:58.740 --> 01:19:05.340
I mean, I would, I would also postulate that maybe our most of

01:19:05.340 --> 01:19:08.820
our, let's say benchmarks that we're thinking about today aren't

01:19:08.860 --> 01:19:14.740
necessarily suited to to because his argument was by creating

01:19:14.740 --> 01:19:19.700
abstractions, it might actually, you know, help your ability to

01:19:19.740 --> 01:19:22.740
learn something, right, which is a thing that we all

01:19:22.740 --> 01:19:25.340
intuitively understand in the world, if I have good

01:19:25.340 --> 01:19:28.100
abstractions, I can transfer my knowledge from here to here and

01:19:28.100 --> 01:19:31.020
from here to here. Yeah, in something like image net

01:19:31.020 --> 01:19:34.300
classification, or whatnot, or most of the benchmarks we have

01:19:34.300 --> 01:19:38.980
today, the necessity of abstractions is probably not like

01:19:38.980 --> 01:19:42.420
the data, the hardness of the problem probably doesn't

01:19:42.420 --> 01:19:46.980
require abstractions to be introduced. And therefore, the

01:19:46.980 --> 01:19:50.460
limiting factor here might not only be the models themselves,

01:19:50.460 --> 01:19:57.140
but also, let's say, our ability to even measure the progress

01:19:57.140 --> 01:20:00.460
one could make with abstractions. And I think that's gonna change

01:20:00.460 --> 01:20:03.620
maybe in the near future, because people are going into

01:20:03.620 --> 01:20:08.300
multimodality research and so on. And there, I think the concept

01:20:08.300 --> 01:20:12.860
of sort of concepts, maybe not abstractions, but at least

01:20:12.860 --> 01:20:15.980
something like concepts is way more, more important.

01:20:17.020 --> 01:20:19.540
Yeah, there's, let me just follow real quickly there, Tim,

01:20:19.540 --> 01:20:22.380
because there's something very interesting there to Yannick,

01:20:22.380 --> 01:20:27.100
which is the lack of good tools to deal with multimodal, you

01:20:27.100 --> 01:20:30.260
know, sets of data results. And a lot of times, we're just

01:20:30.260 --> 01:20:33.820
throwing out kind of valuable, valuable sources of data, just

01:20:33.820 --> 01:20:36.820
because, you know, we don't have a good tool sets to do with

01:20:36.820 --> 01:20:39.220
them, like think about the self driving car, like the whole,

01:20:39.620 --> 01:20:43.140
should it be vision versus LiDAR debate? Why? Why isn't it

01:20:43.140 --> 01:20:46.820
both? I mean, you know, if you can for $5, you can throw on

01:20:46.820 --> 01:20:50.620
some cheap, you know, LiDAR sensors or something, maybe not

01:20:50.620 --> 01:20:52.940
something fancy, but something cheap, why wouldn't we take

01:20:52.940 --> 01:20:56.860
advantage of that data? And it's, it's really because we don't

01:20:56.860 --> 01:21:00.700
have good tools to deal with, with multimodal data.

01:21:01.340 --> 01:21:04.620
We got to a good point in the discussion where we were talking

01:21:04.620 --> 01:21:08.500
about the nature of finding abstractions. And I wonder where

01:21:08.500 --> 01:21:10.980
the neural networks can find abstractions. Now, the, the

01:21:11.020 --> 01:21:14.940
cynical view is that humans kind of create these inductive

01:21:14.940 --> 01:21:17.300
priors, and they represent the abstraction. So certainly in the

01:21:17.300 --> 01:21:19.900
case of geometric deep learning, and that's kind of what's

01:21:19.900 --> 01:21:23.220
happening, we put the, the priors in there to reduce the size of

01:21:23.220 --> 01:21:25.780
the approximation space. And Keith and I had an interesting

01:21:25.780 --> 01:21:28.140
idea yesterday that there's a kind of analogy between geometric

01:21:28.140 --> 01:21:30.620
deep learning and causal representation learning. So I

01:21:30.620 --> 01:21:32.900
think Keith, you went online and you found a really interesting

01:21:32.900 --> 01:21:35.820
definition of a causal model, which is that it's kind of

01:21:35.820 --> 01:21:40.100
immune to, let's say, adversarial examples. So what a model

01:21:40.140 --> 01:21:43.100
does right now, is it learns a relationship essentially between

01:21:43.100 --> 01:21:46.220
let's say every single pixel and something happening, right,

01:21:46.260 --> 01:21:49.060
which is why that model is vulnerable.

01:21:49.580 --> 01:21:52.620
Yeah, so that was the, and yeah, and I would love to get your

01:21:52.620 --> 01:21:56.020
comment on that. But that was, you know, this paper, and I

01:21:56.020 --> 01:21:58.420
could go dig it up, and I can get the reference right now where

01:21:58.420 --> 01:22:02.260
it said, you know, hey, what is the difference between a causal

01:22:02.580 --> 01:22:06.460
or prediction from a causal model versus a prediction from a

01:22:06.460 --> 01:22:10.940
non causal model. And the point was that, well, almost by

01:22:10.940 --> 01:22:15.740
definition, really, if you have a causal model, then if you

01:22:15.740 --> 01:22:18.460
perturbed the inputs, the prediction that you get out of

01:22:18.460 --> 01:22:22.340
it remains a valid, a valid output, because after all, if

01:22:22.340 --> 01:22:25.060
it's a causal model, and it's reflective of a sort of the

01:22:25.060 --> 01:22:28.140
causal structure of the world or whatnot, then sure, that's a

01:22:28.140 --> 01:22:32.060
valid, valid output. Whereas, if it's non causal, it has the

01:22:32.060 --> 01:22:35.580
potential to learn all these kind of spurious, spurious

01:22:35.580 --> 01:22:37.580
structures, and therefore, that's why you get the

01:22:37.580 --> 01:22:40.940
capability of these adversarial examples where you just, you

01:22:40.940 --> 01:22:44.540
know, put a little rainbow pixel somewhere, and it messes up the

01:22:44.540 --> 01:22:47.140
class because it had this spurious connection.

01:22:47.580 --> 01:22:51.020
I mean, in the same vein, you could also, the adversarial

01:22:51.020 --> 01:22:54.380
examples are there because of inaccuracies, because we don't

01:22:54.380 --> 01:22:57.740
have the perfect discriminative function, right? I could also

01:22:57.740 --> 01:23:01.500
say, well, if I just had the correct discriminative functions,

01:23:01.500 --> 01:23:04.860
it doesn't need to be causal. If I just had like the right

01:23:04.860 --> 01:23:09.780
partitioning of my input space, then, you know, I'm super not

01:23:09.780 --> 01:23:13.780
vulnerable to adversarial attacks. I guess the real question

01:23:13.780 --> 01:23:19.260
would be, would that technically amount to a causal model if I

01:23:19.260 --> 01:23:22.940
had, you know, the perfect partitioning of the input

01:23:22.940 --> 01:23:27.140
space into my classes? I don't know, that's like, is there like

01:23:27.140 --> 01:23:30.340
a mathematical equivalent from that to a causal model? Who

01:23:30.340 --> 01:23:30.820
knows?

01:23:31.180 --> 01:23:35.300
Right. Yeah, I think there's probably, it's probably, certainly,

01:23:35.740 --> 01:23:38.740
if you have the perfect discriminative function, it's

01:23:38.740 --> 01:23:41.940
probably the discriminative function that you would derive

01:23:41.940 --> 01:23:46.140
from a causal model. I'm not 100% sure you can go go in the

01:23:46.140 --> 01:23:48.780
reverse, because I imagine there probably is some some

01:23:48.780 --> 01:23:53.620
information loss going from, you know, a causal model to, you

01:23:53.620 --> 01:23:57.780
know, like, for example, I'll give you an example. In the cases

01:23:57.820 --> 01:24:00.660
of, say, production systems, you know, so, so these little

01:24:01.140 --> 01:24:05.740
rewriting rules or whatever, the definition there of a causal

01:24:06.100 --> 01:24:11.820
system is one in which all the potential graphs, all the

01:24:11.820 --> 01:24:14.780
potential transition graphs that you can get to a particular

01:24:14.780 --> 01:24:18.980
output are isomorphic. So even though you have you can have

01:24:19.340 --> 01:24:22.980
like the perfect discriminative kind of function, there may be

01:24:22.980 --> 01:24:26.140
multiple possible graphs that you could have gotten there, but

01:24:26.140 --> 01:24:29.460
they're isomorphic. So I'm not quite sure, you know, how that

01:24:29.460 --> 01:24:33.740
would translate into this, this, this point. But I think you'd

01:24:33.740 --> 01:24:37.100
be just as good for the purpose of discriminating.

01:24:38.740 --> 01:24:41.180
I think it's related to the semantics discussion we're

01:24:41.180 --> 01:24:45.140
having in NLP. So people like Walid Saber say that neural

01:24:45.140 --> 01:24:48.180
networks don't have semantics. And in the same way, as I was

01:24:48.180 --> 01:24:51.340
just saying, blue pixels, I mean, in the real world, let's say

01:24:51.340 --> 01:24:55.820
male testosterone levels is causally linked to incidents of

01:24:55.860 --> 01:24:59.500
car crashes, which means you can now take the model in in

01:24:59.500 --> 01:25:02.100
Holland in a different country. And because it's a causal

01:25:02.100 --> 01:25:05.380
factor, it will extrapolate in the same way. But neural

01:25:05.380 --> 01:25:09.140
networks models, because what a human does is we would come up

01:25:09.140 --> 01:25:12.420
with the right representational abstraction, we would build a

01:25:12.420 --> 01:25:15.660
model, which is very reductionist, a neural network models

01:25:15.660 --> 01:25:18.980
everything to everything. And the semantics are all one thing.

01:25:19.380 --> 01:25:24.220
Well, okay, I don't, I'm not, I'm not too, too keen on

01:25:24.580 --> 01:25:28.740
discussing like semantics and whatnot with with NLP people.

01:25:28.900 --> 01:25:38.860
But I don't know, you know, like, like, I don't know, it

01:25:38.860 --> 01:25:44.060
often it often veers away and veers into semantics. It's like

01:25:44.060 --> 01:25:48.860
it's a bit too, you know, I like what what I think Conor Conor

01:25:48.860 --> 01:25:53.060
Lay, he said, like, when we talked him along. Oh, he wouldn't

01:25:53.300 --> 01:25:56.260
like that. I talked him a long time ago. And I happen to agree

01:25:56.260 --> 01:25:59.220
with him there is that you sort of have to see everything from

01:25:59.220 --> 01:26:03.700
the perspective of these models. Like if I'm a GPT three, my

01:26:03.700 --> 01:26:07.580
entire world is text input, right? And people can't somehow

01:26:07.580 --> 01:26:12.260
judge GPT three by, well, you don't even have whatever a

01:26:12.260 --> 01:26:14.860
connection to the real world, you don't even know that you don't

01:26:14.860 --> 01:26:19.340
go see a doctor if your plant is sick, right? Like, how can you

01:26:19.340 --> 01:26:21.940
not know that? Like, okay, they don't live in the real world,

01:26:21.940 --> 01:26:25.820
they live in the text world of the internet. And in that world,

01:26:26.060 --> 01:26:31.140
I'm not sure if there is not a level of abstraction happening

01:26:31.260 --> 01:26:39.300
in these models. Like, it's, it's, it's, um, yeah, I'm, I don't

01:26:39.300 --> 01:26:43.860
want to, I don't want to claim that these things do not form

01:26:44.140 --> 01:26:48.180
abstract things, it might not be the same abstract classes that

01:26:48.180 --> 01:26:52.780
we form, but they definitely form some level of abstraction.

01:26:53.100 --> 01:26:56.100
And of course, they can't transfer it because we only give

01:26:56.100 --> 01:26:59.660
them the one modality, right? But they may be able to transfer

01:26:59.660 --> 01:27:05.020
it between, you know, different areas of text, which they

01:27:05.140 --> 01:27:10.500
sometimes do, right? And yeah, so that's, I just wouldn't be so

01:27:10.540 --> 01:27:13.260
conclusive with respect to these things.

01:27:13.300 --> 01:27:17.020
That that's true. I think we've gone full circle now. So after

01:27:17.060 --> 01:27:21.740
speaking with Randall Ballastriro about the splines, that almost

01:27:22.180 --> 01:27:25.100
results in such a cynical reading of MLPs that they're just

01:27:25.100 --> 01:27:27.980
hash tables, but we're not using MLPs, we're using

01:27:27.980 --> 01:27:31.140
transformers and we're using CNNs. And actually, if you think

01:27:31.140 --> 01:27:34.740
of abstraction, just as being extrapolation, I think they are

01:27:34.740 --> 01:27:37.980
basically synonymous, it's about being able to extrapolate

01:27:37.980 --> 01:27:41.820
outside of your training set. Then those inductive priors are

01:27:41.820 --> 01:27:46.460
indeed producing abstractions. But the problem is humans

01:27:46.460 --> 01:27:49.780
design those inductive priors. What we want is to learn

01:27:49.780 --> 01:27:52.020
abstractions. And that's the thing that I don't think is

01:27:52.020 --> 01:27:57.820
happening. I'm kind of I'm kind of on the same page as Yannick

01:27:57.820 --> 01:28:02.700
and Connor on the one hand, which is, hey, if an abstraction is

01:28:02.700 --> 01:28:08.220
just a compression, you know, encoding of the input space, then

01:28:08.220 --> 01:28:10.500
of course, they're learning abstractions, right? I mean,

01:28:10.500 --> 01:28:13.740
they are throwing away, you know, information and retaining

01:28:13.740 --> 01:28:17.220
some, some abstract thing. I think, but I think that just kind

01:28:17.220 --> 01:28:23.580
of devolves into somewhat like, you know, bastardization, if you

01:28:23.580 --> 01:28:27.140
will, of what people mean when they say abstractions, because

01:28:27.500 --> 01:28:31.140
the types of abstractions that traditionally we think about as

01:28:31.140 --> 01:28:34.740
abstractions are simplifications. You know, they're, they're

01:28:34.740 --> 01:28:39.780
like, simplifications of more general longer range kind of

01:28:39.780 --> 01:28:42.980
structures. Whereas we know, and I think we all know this for

01:28:42.980 --> 01:28:47.740
sure, that a lot of the quote unquote abstractions that did a

01:28:47.780 --> 01:28:52.340
neural network learns are these kind of like shortcuts, right?

01:28:52.340 --> 01:28:56.540
They're like these low level borderline spurious kinds of

01:28:56.580 --> 01:28:59.420
abstractions. And that's why they break so easy. That's why

01:28:59.420 --> 01:29:02.340
they're so brittle. And I mean, there is this vagueness here,

01:29:02.340 --> 01:29:05.020
right? Like when is an abstraction, a good abstraction,

01:29:05.020 --> 01:29:07.660
I don't know. But I think it all kind of in a way misses the

01:29:07.660 --> 01:29:12.660
point. Like, what we're talking about here is that, and this is a

01:29:12.660 --> 01:29:16.580
lot of what Benjio said, right, which is that the goal here is

01:29:16.580 --> 01:29:21.820
to figure out how to get machine learning to learn

01:29:22.380 --> 01:29:28.460
structures that by virtue of their simplification, their simple

01:29:28.460 --> 01:29:33.220
abstractions are more generalizable out of distribution.

01:29:33.980 --> 01:29:37.540
Right, like that's, that's really kind of the goal here. And I

01:29:37.540 --> 01:29:41.420
mean, so the rest of it is just semantics, pun intended. I mean,

01:29:41.420 --> 01:29:41.740
the

01:29:44.020 --> 01:29:49.900
if you look across the world, a lot of, let's say, cultures and

01:29:49.900 --> 01:29:55.980
humans and so on must have the same abstractions, right? So it

01:29:56.100 --> 01:30:00.060
must mean a little bit that it's not just something you learn

01:30:00.060 --> 01:30:04.700
during your lifetime, right? So, right? Oh, absolutely.

01:30:04.700 --> 01:30:08.180
Not correct. It's it's learned by the it's learned by evolution,

01:30:08.220 --> 01:30:10.540
by the species, by, by life itself.

01:30:10.580 --> 01:30:16.140
Exactly, right. But but is like the, the analogy to us building

01:30:16.140 --> 01:30:20.380
in the correct ones as a shortcut for just evolution doing it

01:30:20.380 --> 01:30:25.460
using essentially random search, right? That is, it might, right,

01:30:25.460 --> 01:30:28.980
it's, it's a different, it's a different quality of we want

01:30:29.020 --> 01:30:32.940
machines to learn something. Because usually we think of when

01:30:32.940 --> 01:30:35.500
we say we want machines to learn something is we want him to

01:30:35.500 --> 01:30:41.060
ingest data akin to maybe what a human does during its lifetime.

01:30:41.540 --> 01:30:45.540
But the when you know, these sort of abstractions and the

01:30:45.540 --> 01:30:48.780
ability to form abstractions, they seem to be happening on a

01:30:48.780 --> 01:30:51.020
more fundamental shared level.

01:30:51.300 --> 01:30:55.140
Yes, you just put the pin in the center of the bullseye. I think

01:30:55.140 --> 01:30:58.060
that's exactly right. You know, there's a lot to be said for

01:30:58.100 --> 01:31:01.900
me. It's an epiphenomenon. And a lot of intelligence is

01:31:01.900 --> 01:31:06.060
embodied. And I agree that there's an awful lot of stuff going

01:31:06.060 --> 01:31:10.540
on and unbeknown to us with this clearly something that most

01:31:10.540 --> 01:31:14.420
people don't have a grasp on. Maybe this is why at the

01:31:14.420 --> 01:31:17.820
population level, maybe this is why I'm frequently miscommunicating

01:31:17.820 --> 01:31:21.060
with people because I never assumed that learning was about,

01:31:21.540 --> 01:31:24.300
you know, what a human being learns and a human being's

01:31:24.340 --> 01:31:28.140
lifetime. Like it's, to me, it's always been the evolution,

01:31:28.740 --> 01:31:31.740
you know, paradigm, it's like what's encoded in your neurons,

01:31:31.740 --> 01:31:35.500
what's encoded in your DNA, you know, what was learned by

01:31:35.500 --> 01:31:38.940
bacteria a long time ago, and how did that translate into what

01:31:38.940 --> 01:31:42.460
human beings are doing. So I don't know why, like, why so

01:31:42.460 --> 01:31:46.140
many people are focused on what a human being learns in their

01:31:46.140 --> 01:31:49.380
lifetime. I mean, it's more, you know, why is that the goal?

01:31:49.420 --> 01:31:50.100
I'm not sure.

01:31:50.300 --> 01:31:53.340
I know, but we run the risk of being very reductionist because

01:31:53.380 --> 01:31:57.060
Connolly, he said that it's an open question where the humans

01:31:57.060 --> 01:32:01.100
are even intelligent. And if you go down that line, very

01:32:01.100 --> 01:32:04.060
quickly, you start saying, oh, human beings are just hash

01:32:04.060 --> 01:32:07.660
tables like GBT three, clearly humans are intelligent in some

01:32:07.660 --> 01:32:07.940
way.

01:32:08.020 --> 01:32:10.420
Well, you can just take it as a, you know, matter of

01:32:10.420 --> 01:32:13.780
definition, but it's not a binary thing. Like again, why are

01:32:13.780 --> 01:32:16.620
we always into this black and white concept, something is or

01:32:16.620 --> 01:32:19.980
is not intelligent? Like that's not how I view things. I think

01:32:19.980 --> 01:32:24.780
there's a spectrum of intelligence from like zero to, I

01:32:24.780 --> 01:32:27.740
don't know, maybe infinity or something, some really large

01:32:27.740 --> 01:32:31.540
number beyond what what human beings are. And so it's this

01:32:31.540 --> 01:32:35.300
continuum. So that's why I like chelets kind of on the measure

01:32:35.300 --> 01:32:39.420
of intelligence, because even though it doesn't actually give

01:32:39.420 --> 01:32:43.860
us a, you know, quantitative way yet to measure intelligence, it

01:32:43.860 --> 01:32:46.580
at least is thinking along the right directions, which is how

01:32:46.620 --> 01:32:51.180
do you measure intelligence? And how do you define it as a

01:32:51.180 --> 01:32:56.460
category of activity? And then we can kind of get past this

01:32:56.460 --> 01:32:58.220
black and white, you know, thinking.

01:33:00.540 --> 01:33:01.500
Well, gentlemen,

01:33:02.060 --> 01:33:05.260
always a pleasure. Absolutely. Yeah, absolutely.

