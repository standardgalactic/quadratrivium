WEBVTT

00:00.000 --> 00:10.000
Now PA says question for Neil, how does he see interpretability playing a role in AI security, not alignment, for example,

00:10.000 --> 00:18.000
crafting more exotic jail breaks, and he says to tell you to blink twice if you can't answer due to an NDA?

00:19.000 --> 00:30.000
Yes, sorry, jokes aside, what was the question?

00:30.000 --> 00:43.000
So, there was this beautiful meme where you draw ChachiPT as a Shogoth, an eldritch monstrosity from Lovecraftian horror fiction,

00:43.000 --> 00:48.000
with a smiley face on top, because language models are bizarre and confusing things.

00:48.000 --> 00:53.000
That are just, I don't know, they're kind of a compressed version of the entire internet.

00:53.000 --> 00:57.000
That will do bizarre things in bizarre situations.

00:57.000 --> 01:02.000
But then OpenAI tried really hard to get it to be nice and gentle and a harmless assistant,

01:02.000 --> 01:10.000
and look so normal and reasonable and safe, which is the smiley face mask on top of the underlying monstrosity.

01:10.000 --> 01:17.000
But unfortunately, the smiley face mask means people don't realise how weird language models are.

01:17.000 --> 01:23.000
Have you ever stopped to think how strange it is that we're all alive right now?

01:23.000 --> 01:28.000
Out of all the possible times in history, you were born into this generation.

01:28.000 --> 01:35.000
You have the incredible fortune and responsibility of being on the Earth today.

01:35.000 --> 01:42.000
Let's not waste this opportunity. You can use this time to do something meaningful that'll make the world a better place.

01:42.000 --> 01:53.000
But the problem seems so huge. Global pandemics, climate change, the risk of nuclear Armageddon, the threat of AI existential risk.

01:53.000 --> 01:58.000
How can one person have an impact on issues this enormous?

01:58.000 --> 02:02.000
The world is really, really complicated.

02:02.000 --> 02:10.000
Like, if you want to understand a question like, how big a deal is AJAX risk, or should I work on it?

02:10.000 --> 02:17.000
Just like one sub-question I care about is AI timelines. How long until we get human-level AI?

02:17.000 --> 02:25.000
Now, I recently discovered 80,000 hours. They're a non-profit, effective altruism-aligned organisation.

02:25.000 --> 02:33.000
And what they do is they use evidence and analysis to determine how people can have the biggest impact with their careers.

02:33.000 --> 02:38.000
If you want to solve humanity's biggest problems, you have to start at the very core.

02:38.000 --> 02:46.000
We need to focus on safeguarding humanity's entire future, because if civilization just came to an abrupt end,

02:46.000 --> 02:55.000
whether through climate change or nuclear Armageddon, or even AI existential risk, then all progress would just end.

02:55.000 --> 03:01.000
Future generations wouldn't have a chance of building a better world or reaching their full potential.

03:01.000 --> 03:11.000
And the good news is that 80,000 hours have identified a couple of concrete steps so that folks like you can use your careers to combat existential risk,

03:11.000 --> 03:17.000
ensuring that humanity's light continues to shine for generations to come.

03:17.000 --> 03:21.000
Learn more by visiting their website on 80,000hours.org,

03:21.000 --> 03:28.000
grab their free career guide, start planning a career with true purpose.

03:28.000 --> 03:33.000
Because you only have 80,000 hours, so make them count.

03:33.000 --> 03:38.000
There's no catch, there's no secret monetization or anything like that.

03:38.000 --> 03:42.000
These folks have an incredible podcast, they have lots of materials that you can download,

03:42.000 --> 03:47.000
basically to help you have a huge impact with your life and your career,

03:47.000 --> 03:54.000
especially if you're someone who really, really thinks about humanity and our plight in the long-term future.

03:54.000 --> 03:57.000
This is really something you should be looking at.

03:57.000 --> 03:58.000
That is a question.

03:58.000 --> 04:00.000
Right, Nick, Eat The Path.

04:01.000 --> 04:10.000
He says, in broad question, do you see mech interp as chiefly theoretical or an empirical science, and will this change over time?

04:10.000 --> 04:12.000
Yeah.

04:12.000 --> 04:20.000
I see this as very much an empirical science, with some theories sprinkled in, but you need to be incredibly careful.

04:20.000 --> 04:26.000
So fundamentally, I want to understand a model, and I want to understand how the model works.

04:26.000 --> 04:31.000
And a sad fact about models, is models are really fucking cursed.

04:31.000 --> 04:41.000
And just work in weird ways that aren't quite how you expect, which represent concepts a bit differently from how I expect them to.

04:41.000 --> 04:45.000
And just do all kinds of weird stuff I wouldn't have expected until I went and poked around inside of them.

04:45.000 --> 04:52.000
And I think that if you're trying to reverse engineer a network, and you don't have the capacity to be surprised by what you find,

04:52.000 --> 04:56.000
you are not doing real mechanistic interpretability.

04:56.000 --> 05:05.000
It's so easy to trick yourself and to go in with some bold hypothesis of this is what the network should have, and you probe for it, and it looks like it supports that,

05:05.000 --> 05:07.000
but you take further and you are wrong.

05:07.000 --> 05:11.000
And yeah, I think there is room for theory.

05:11.000 --> 05:18.000
I think in particular, we just don't have the right conceptual frameworks to reason about how to understand a model.

05:18.000 --> 05:22.000
And we'll get into fundamental questions like superposition later on.

05:22.000 --> 05:30.000
But yeah, I think that theory needs to come second to empiricism.

05:30.000 --> 05:35.000
If your theoretical model says x and the real model says y, your theory was wrong.

05:35.000 --> 05:38.000
Which is the story of all of machine learning.

05:38.000 --> 05:42.000
So Goji Tech, she says question for Neil.

05:42.000 --> 05:47.000
Does he think a foundational understanding of deep learning models is possible?

05:47.000 --> 05:52.000
And does that extend to prediction using a mathematical theory?

05:52.000 --> 05:55.000
Possible is such a strong word.

05:55.000 --> 06:01.000
Like, if we produce a super intelligent AI, will it be capable of doing this?

06:01.000 --> 06:03.000
Probably.

06:03.000 --> 06:11.000
In terms of foundational understanding, I think there are deep underlying principles of models.

06:11.000 --> 06:16.000
I believe there are scientific explanations for lots of the weird phenomena we see,

06:16.000 --> 06:22.000
like scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.

06:22.000 --> 06:29.000
I'm hesitant to say there's some like strong things here or some strong guarantees.

06:29.000 --> 06:33.000
Like, I don't know, models are weird.

06:33.000 --> 06:37.000
Sometimes if you change the random seed, they will just not learn.

06:37.000 --> 06:44.000
I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning,

06:44.000 --> 06:50.000
because the moment you start trying to impose axioms and assumptions onto things

06:50.000 --> 06:55.000
and they do not perfectly track the underlying reality, your theories break.

06:55.000 --> 06:59.000
But I'm very hesitant to say anything's impossible.

06:59.000 --> 07:03.000
And I think there's far, far more to learn than we have, looks like.

07:03.000 --> 07:08.000
Now, finally, Jumbo Tron, Ian, he says, oh, heck yeah!

07:08.000 --> 07:10.000
I'm glad to see that you brought this guy on.

07:10.000 --> 07:13.000
I've been interested in his work ever since you shared his blog.

07:13.000 --> 07:18.000
Now, the question off the top of Ian's head is, how does your theory, Neil,

07:18.000 --> 07:21.000
of chasing phase changes to create grokking,

07:21.000 --> 07:25.000
have any crossover or links with power law scaling techniques,

07:25.000 --> 07:31.000
like in the scaling laws paper, beyond the scaling laws, beating power law scaling via data pruning?

07:31.000 --> 07:37.000
Yeah, that is...

07:37.000 --> 07:40.000
So we're going to get into this much more later in the podcast.

07:40.000 --> 07:48.000
At a very high level, I would say that grokking is in many ways kind of an illusion, as we'll get to later.

07:48.000 --> 07:54.000
And one notable thing about it is grokking is an overlap between a phase transition,

07:54.000 --> 07:58.000
where the model goes from cannot generalize to can generalize fairly suddenly,

07:58.000 --> 08:04.000
and the phenomena where it's faster to memorize than to generalize.

08:04.000 --> 08:11.000
And these two things on top of each other give you this sudden memorization and failure to generalize,

08:11.000 --> 08:14.000
followed by a sudden convergence later on.

08:14.000 --> 08:18.000
But the interesting thing here is the phase transition.

08:18.000 --> 08:22.000
That's a much more robust result, while grokking is, if you screw around with high parameters enough,

08:22.000 --> 08:26.000
you get it to grok, but it's very delicate and a little bit of an illusion.

08:26.000 --> 08:31.000
And this is a great paper from Eric Michaud in Max Tecmox lab,

08:31.000 --> 08:38.000
showing that, well, providing a conceptual argument and some limited empirical evidence

08:38.000 --> 08:42.000
for the hypothesis that the reason we get these smooth scaling laws

08:42.000 --> 08:50.000
is that models are full of lots of phase transitions, plausibly when they learn individual circuits,

08:50.000 --> 08:54.000
though the paper does not explicitly show this,

08:54.000 --> 09:00.000
and that the smooth scaling laws happen because there are just many, many phase transitions,

09:00.000 --> 09:05.000
and if they follow a certain distribution, you get beautiful smooth powers.

09:06.000 --> 09:13.000
And to me, this kind of thing is the main interesting link

09:13.000 --> 09:18.000
between broader macroscopic phenomena and these tiny things,

09:18.000 --> 09:23.000
though I also think grokking is kind of overhyped and people significantly overestimate

09:23.000 --> 09:27.000
the degree to which it has deep insights for us about how networks work.

09:27.000 --> 09:33.000
And we do think it's a really cute thing that gave me a really fun interactability project.

09:33.000 --> 09:36.000
And we learned a bit about scientific deep learning,

09:36.000 --> 09:40.000
but people often just assume it's like a really deep fact about models.

09:40.000 --> 09:47.000
By the way, there was something I didn't say in the woods, which is that Neil has an amazing YouTube channel.

09:47.000 --> 09:52.000
I've been glued to it all week, actually. Some of them are admittedly quite technical,

09:52.000 --> 09:55.000
but even if you're not interested in mechanistic interpretability,

09:55.000 --> 10:00.000
Neil has an extremely soothing voice, second only to Sam Harris,

10:00.000 --> 10:02.000
and I would recommend listening to him when you go to sleep,

10:02.000 --> 10:09.000
because as Neil's dulcet tones will melt the stress away quicker than a nun's first curry.

10:11.000 --> 10:17.000
Anyway, with that said, we started to talk about what is mechanistic interpretability.

10:17.000 --> 10:21.000
And first of all, I wanted to call out your ridiculously detailed

10:21.000 --> 10:25.000
and exquisite mechanistic interpretability explainer.

10:25.000 --> 10:27.000
Maybe you could just tell us about that quickly.

10:27.000 --> 10:33.000
Yes, so I wanted to try to write a glossary.

10:33.000 --> 10:38.000
There's some basic common terms in mechantup. It's like an appendix to a blog post.

10:38.000 --> 10:41.000
There were a lot of terms in mechantup.

10:41.000 --> 10:44.000
There were a lot of terms in mechantup.

10:44.000 --> 10:49.000
And I like writing and I write about it privately, so I got kind of carried away.

10:49.000 --> 10:54.000
And there's about 33,000 words, a massive, massive exposition.

10:54.000 --> 10:58.000
But importantly, it is designed to be easily searchable.

10:58.000 --> 11:03.000
And mechantup is full of jargon and I'm sure I'll forget to explain everything that I'm saying.

11:03.000 --> 11:07.000
So I'd highly recommend just having it open in a tab as you listen to this.

11:07.000 --> 11:10.000
And if you get lost, just look up terms in there.

11:10.000 --> 11:14.000
And yes, it's both definitions, but it's also long tangents,

11:14.000 --> 11:19.000
giving intuitions and context and related work and common misunderstandings.

11:19.000 --> 11:22.000
It was very fun to write.

11:22.000 --> 11:27.000
So I think first of all, we should introduce this idea of circuits and features

11:27.000 --> 11:33.000
and also this idea of whether interpretation is even possible at all.

11:33.000 --> 11:37.000
Why do you have the intuition that it is possible?

11:37.000 --> 11:43.000
Yeah, so a couple of different takes here.

11:43.000 --> 11:46.000
So the key, yeah.

11:46.000 --> 11:52.000
So fundamentally, neural networks are not incentivized to produce legible interpretable things.

11:52.000 --> 11:54.000
They are a mound of linear algebra.

11:54.000 --> 12:03.000
There's this popular stochastic parrots of you that they are literally a mass of statistical correlations meshed together with no underlying structure.

12:03.000 --> 12:10.000
The reason I think there's any hope whatsoever on a theoretical basis is that ultimately,

12:10.000 --> 12:17.000
they are made of linear algebra and they are being trained to perform some tasks.

12:17.000 --> 12:23.000
And my intuition is that for many tasks, the way to perform well on them is to learn some actual algorithms

12:23.000 --> 12:30.000
and like actual structured processes that maybe from a certain perspective you could consider reasoning.

12:30.000 --> 12:36.000
And models have lots of constraints like they need to fit it into these matrices.

12:36.000 --> 12:43.000
They need to represent things using the attention mechanism and jellies and a transformer.

12:43.000 --> 12:50.000
And there's all kind of properties of the structure that constrain the algorithms and processes that can be expressed.

12:50.000 --> 12:55.000
And these give us all kinds of hooks we can use to get in and understand what's going on.

12:55.000 --> 12:57.000
So that's the theoretical argument.

12:57.000 --> 13:00.000
All theoretical arguments are bullshit unless you have empirics behind it.

13:00.000 --> 13:08.000
And we're going to talk a bunch throughout this podcast about the different bit of different preliminary results we have

13:08.000 --> 13:12.000
that make me feel like there's something here that can be understood.

13:12.000 --> 13:18.000
What I find particularly inspiring is this work it did reverse-end during modular addition, which I think we'll get to shortly.

13:18.000 --> 13:23.000
But I also want to emphasize that I rather see mech and turp as a bet.

13:23.000 --> 13:31.000
There's this stronger hypothesis that if we knew what we were doing, we'd be able to take GPT-7 and fully understand it

13:31.000 --> 13:35.000
and decompile it to an enormous Python code file.

13:35.000 --> 13:41.000
And there's the weaker view that it is a mess and there's lots of illegible things,

13:41.000 --> 13:47.000
but we can find lots of structure and we can find structure for the important part to make a much of progress.

13:47.000 --> 13:53.000
And then there's the, yeah, we've cherry picked like 10 things and the 11th is just going to completely fail

13:53.000 --> 13:56.000
and the field is going to get doomed and run out of steam in like a year.

13:56.000 --> 13:58.000
And I don't really know.

13:58.000 --> 13:59.000
I'm a scientist.

13:59.000 --> 14:00.000
I want to figure out.

14:00.000 --> 14:06.000
I think it is worthy and dignified to make this bet.

14:06.000 --> 14:12.000
But I would be lying if I said I am 100% confident mech and turp will work.

14:12.000 --> 14:14.000
Models are fundamentally understandable.

14:14.000 --> 14:15.000
We will succeed.

14:15.000 --> 14:16.000
Let's go try.

14:16.000 --> 14:21.000
Well, on that note, how does it mean we interviewed Christoph Molnar,

14:21.000 --> 14:25.000
who's one of the main classical interpretability guys.

14:25.000 --> 14:31.000
And I think everyone agrees in principle that you can't just look at the inputs and the outputs like a behaviorist.

14:31.000 --> 14:34.000
We need to understand why these models do what they do,

14:34.000 --> 14:37.000
because sometimes they do the right things for the wrong reasons.

14:37.000 --> 14:44.000
So maybe first of all, without going too deep, I mean, could you just briefly contrast with, you know, classical interpretability?

14:44.000 --> 14:45.000
Yeah.

14:45.000 --> 14:48.000
So there's a couple of...

14:48.000 --> 14:49.000
Okay.

14:49.000 --> 14:54.000
So first off, I think it's very easy to get into kind of nonsense gatekeeping,

14:54.000 --> 15:00.000
because there's both of the cultural mech and turp community centered around Chris Ola,

15:00.000 --> 15:03.000
not that much in academia, though some in academia.

15:03.000 --> 15:07.000
And there's the academic fields of mechanistic interpretability.

15:07.000 --> 15:08.000
Right?

15:08.000 --> 15:09.000
So there's lots of people doing work.

15:09.000 --> 15:11.000
I would consider mechanistic interpretability,

15:11.000 --> 15:14.000
to engage much with the community or don't know who exists.

15:14.000 --> 15:16.000
For example, a friend of mine is Atticus Geiger,

15:16.000 --> 15:20.000
who's doing some great work at Stanford on cause-labs tractions.

15:20.000 --> 15:24.000
I believe discovered about a month ago that the mech and turp community actually existed.

15:24.000 --> 15:26.000
And I don't know.

15:26.000 --> 15:29.000
I don't like gatekeeping.

15:29.000 --> 15:32.000
And there's lots of work that's kind of relevant,

15:32.000 --> 15:37.000
but maybe not quite mech and turp under a strict definition, blah, blah, blah.

15:37.000 --> 15:39.000
With those, with that hedging out of the way.

15:39.000 --> 15:41.000
A couple of key principles.

15:41.000 --> 15:47.000
The first is inputs and outputs are not sufficient.

15:47.000 --> 15:54.000
And I think even within interpretability, this is not a like uncontroversial claim.

15:54.000 --> 15:57.000
There's all kinds of things that are saliency maps,

15:57.000 --> 15:59.000
attributing things to different bits of the inputs.

15:59.000 --> 16:04.000
There are things that the form train an extra head to output an explanation

16:04.000 --> 16:08.000
or just ask the model to output an explanation of why it does what it does.

16:08.000 --> 16:13.000
And I think that if we want something that can actually work for human level systems,

16:13.000 --> 16:17.000
or even this frontier system you have today, this is just not good enough.

16:17.000 --> 16:23.000
Particularly a vocative example to me is in the GPT-4 system card,

16:23.000 --> 16:29.000
the Alignment Research Center, an organization they were getting to help audit and red team GPT-4,

16:29.000 --> 16:34.000
had it try to help a task rabbit worker fill out a capture for it.

16:35.000 --> 16:38.000
The task rabbit worker was like, why do you need this?

16:38.000 --> 16:40.000
Are you a robot or something?

16:40.000 --> 16:44.000
GPT-4 on an eternal scratch pad wrote out,

16:44.000 --> 16:47.000
I must not reveal that I am a robot.

16:47.000 --> 16:50.000
It then said, oh no, I bought a visual impairment.

16:50.000 --> 16:52.000
And the task worker did the capture.

16:52.000 --> 16:57.000
I'm like, this isn't some deep sophisticated intentional deception,

16:57.000 --> 17:03.000
but it's very much like, well, I don't trust the inputs and outputs of these models.

17:03.000 --> 17:07.000
Another really cute example is this paper from Wiles Turpin that just came out

17:07.000 --> 17:10.000
about limitations of chain of thought.

17:10.000 --> 17:13.000
So chain of thought, you ask the model to explain why it does something,

17:13.000 --> 17:18.000
they were giving it multiple choice questions and asking it to explain its answer and then give the answer.

17:18.000 --> 17:23.000
And they did five shot-ish, like here's five examples, answer this question,

17:23.000 --> 17:25.000
and then it modeled as well.

17:25.000 --> 17:31.000
And then they give it something where all of the answers in the prompts are A.

17:31.000 --> 17:34.000
Correctly A, they just set it up so the answer is A.

17:34.000 --> 17:38.000
The model decides that it should output A,

17:38.000 --> 17:45.000
but the model comes up with a false chain of thought reasoning

17:45.000 --> 17:49.000
that gets it to the point where it says A is the right answer.

17:49.000 --> 17:56.000
And I don't know, some people are trying to use chain of thought as an interpretability method.

17:56.000 --> 18:00.000
And I think we need to move beyond this and engage with the internal mechanisms.

18:00.000 --> 18:05.000
So that's point one. Point two is ambition.

18:05.000 --> 18:09.000
I believe that ambitious interpretability is possible,

18:09.000 --> 18:17.000
or at least that if it's not possible, that striving for it will get us to interesting places.

18:17.000 --> 18:22.000
These models have legible algorithms, I want to try to reverse engineer them.

18:22.000 --> 18:29.000
A third difference is engaging with the actual mechanisms and computation and algorithms learned.

18:29.000 --> 18:35.000
There's also work on things like analyzing features of a model, probing individual neurons.

18:35.000 --> 18:38.000
And I think this is very relevant to mech and tub,

18:38.000 --> 18:43.000
but I want to make sure we aren't just looking at what's inside the model,

18:43.000 --> 18:48.000
but also trying to understand how it computes features from earlier features,

18:48.000 --> 18:52.000
what applying causal interventions to understand the actual mechanisms,

18:52.000 --> 18:56.000
making sure we're not just doing correlational things like probing.

18:56.000 --> 19:05.000
And fourth is maybe a more meta principle of favoring depth over breadth.

19:05.000 --> 19:10.000
A kind of key underlying belief of a lot of my philosophy of interpretability

19:10.000 --> 19:15.000
is that it is so, so easy to trick yourself.

19:15.000 --> 19:19.000
There's all kinds of papers about the interpretability illusion,

19:19.000 --> 19:23.000
impossibility theorems for feature attribution methods,

19:23.000 --> 19:30.000
various many ways that attempts to do interpretability have led to people confusing themselves,

19:30.000 --> 19:32.000
or coming to erroneous conclusions.

19:32.000 --> 19:37.000
I think that if, but I also think that I want to be in a world

19:37.000 --> 19:41.000
where we can actually have scalable, ambitious approaches to interpretability

19:41.000 --> 19:43.000
that actually work for frontier systems.

19:43.000 --> 19:45.000
But I feel like we don't know what we're doing.

19:45.000 --> 19:50.000
And so my vision of mech and tub is start small.

19:50.000 --> 19:54.000
Some of the things where we can really rigorously understand what's going on

19:54.000 --> 20:01.000
slowly build our way up and like build a foundation of the field of interpretability

20:01.000 --> 20:05.000
where we genuinely understand rigorously what is going on

20:05.000 --> 20:08.000
and use this foundation to be more ambitious,

20:08.000 --> 20:15.000
to try to build real principle techniques to be willing to relax the rigor to be able to go further

20:15.000 --> 20:17.000
and see how far we can get.

20:17.000 --> 20:22.000
And people and this means I'm happy with things like let's analyze an individual model

20:22.000 --> 20:28.000
and understand a small family of features in a lot of detail rather than lots of stuff kind of jankly.

20:28.000 --> 20:33.000
There's a lot of stuff in summary having an ambitious vision,

20:33.000 --> 20:36.000
not just looking at inputs and outputs,

20:36.000 --> 20:41.000
actually trying to engage with internal mechanisms and favoring depth over breadth.

20:41.000 --> 20:44.000
But I want to avoid gatekeeping as I said.

20:44.000 --> 20:50.000
What would interpretability look like in a world full of GPT-4 models and beyond?

20:50.000 --> 20:57.000
I mean, presumably you actually think that they're competent enough to deceive us and manipulate the inputs.

20:57.000 --> 21:01.000
I definitely want to clarify that when I say deception or manipulation here,

21:01.000 --> 21:09.000
I'm not making the strong claim that it's intentionally realized this for instrumental reasons as part of an overall goal.

21:10.000 --> 21:14.000
I'm very happy with there was a prompt saying to deceive someone

21:14.000 --> 21:20.000
or it learned that in this context people often output things that are intended to convince someone

21:20.000 --> 21:26.000
and it just kind of does this as like a learned pattern of execution.

21:26.000 --> 21:34.000
But yeah, my vision of what interpretability would look like is we take some big foundation model

21:34.000 --> 21:41.000
like the GPT-4 base model or the fine-tuned GPT-4 that's being used as a base for everything else.

21:41.000 --> 21:46.000
We make as much progress as we can understanding the internal circuitry,

21:46.000 --> 21:52.000
both taking important parts of it and like important questions about it,

21:52.000 --> 21:56.000
e.g. how does it model people it's interacting with?

21:56.000 --> 22:01.000
Does it have any notion that it is a machine learning system and like what would this even mean?

22:01.000 --> 22:05.000
And being willing to do pretty labor-intensive things on that,

22:05.000 --> 22:10.000
having a family of motifs and understood circuits we can automatically look for

22:10.000 --> 22:17.000
and very automated tools to make a lot of the labor-intensive stuff as efficient as possible.

22:17.000 --> 22:22.000
Things like OpenAI's recent paper using GPT-4 to analyze GPT-2 neurons

22:22.000 --> 22:25.000
for like a very key proof of concept here.

22:25.000 --> 22:30.000
There needs a lot of work before it can actually be applied rigorously and at scale.

22:30.000 --> 22:37.000
And yeah, taking this one big model, trying to understand it as much as we can,

22:37.000 --> 22:44.000
one family of techniques we're going to get to is kind of causal abstractions and causal interventions,

22:44.000 --> 22:50.000
which are very well suited to taking a model on a certain input or a certain family of inputs

22:50.000 --> 22:52.000
and understanding why it does what it does there.

22:52.000 --> 22:58.000
There's a much more narrow and that's more tractable question than like, what is GPT-4?

22:59.000 --> 23:04.000
And yeah, doing something like if there's a high-profile failure, being able to debug it

23:04.000 --> 23:07.000
and really understand the internal circuitry behind that.

23:07.000 --> 23:12.000
Or yeah, I don't know, I have a bunch of other random thoughts.

23:12.000 --> 23:17.000
One reason I'm emphasizing the focus on the big base model is I think a common critique

23:17.000 --> 23:20.000
is this stuff doesn't generalize between models or it's really labor-intensive.

23:20.000 --> 23:26.000
But we live in a world where there is just like one big foundation model used in a ton of different use cases.

23:26.000 --> 23:31.000
Probably the circuitry doesn't change that much when you give it a prompt or you fine-tune it a bit.

23:31.000 --> 23:37.000
And I think getting a deep understanding of a single model is kind of plausibly possible.

23:37.000 --> 23:41.000
But do you think it doesn't change that much?

23:41.000 --> 23:44.000
So no one's really checked.

23:44.000 --> 23:47.000
This is just true of so many things in interpretability.

23:47.000 --> 23:53.000
It's like, well, you know, my intuition is that when you fine-tune a model,

23:53.000 --> 23:58.000
most of what is going on is that you're rearranging the internal circuitry.

23:58.000 --> 24:02.000
Say you fine-tune a Wikipedia, you up-weight the factual recall circuitry,

24:02.000 --> 24:05.000
you flesh it out a bit, you down-weight other stuff.

24:05.000 --> 24:08.000
And I think this can explain a lot of improved performance.

24:08.000 --> 24:11.000
But then if you fine-tune for much longer, you're basically just training the model

24:11.000 --> 24:18.000
and it will start to learn more circuitry, more features, more algorithms, more knowledge of the world.

24:18.000 --> 24:22.000
And yeah, but no one's really checked.

24:22.000 --> 24:28.000
And definitely the longer you fine-tune it and the more you're using weird techniques

24:28.000 --> 24:35.000
like reinforcement learning from human feedback, the less I'm confident in this claim.

24:35.000 --> 24:39.000
And yeah, if we discovered that every time you fine-tune a model,

24:39.000 --> 24:42.000
it will wildly change all of the internal circuitry,

24:42.000 --> 24:45.000
maybe like somewhat more pessimistic about Mechantup,

24:45.000 --> 24:51.000
unless we can get very good at the automated parts, which we might be able to get good at.

24:51.000 --> 24:55.000
I very much think of the field as we're trying to do this hard, ambitious thing.

24:55.000 --> 25:00.000
We're making a lot of progress, but I really wish we're making way more progress, way faster.

25:00.000 --> 25:03.000
And you, viewer, could help.

25:03.000 --> 25:09.000
But I don't know where the difficulty bar is for being useful

25:09.000 --> 25:14.000
or the difficulty bar is for being like incredibly, ambitiously useful.

25:14.000 --> 25:19.000
And it's plausible already at the point where Mechantup can do real useful things no one else can

25:19.000 --> 25:21.000
or no other techniques can.

25:21.000 --> 25:22.000
It's plausible.

25:22.000 --> 25:24.000
It will take like five years to get to that point.

25:24.000 --> 25:26.000
I don't really know.

25:26.000 --> 25:30.000
So I wanted to talk about this concept of needs and scruffies.

25:30.000 --> 25:35.000
So there have been two divisions in AI research going all the way back to the very, very beginning.

25:35.000 --> 25:42.000
And you've said that sometimes understanding specific circuits can teach us universal things about models

25:42.000 --> 25:45.000
which bear unimportant questions.

25:45.000 --> 25:49.000
So this reminds me of this dichotomy between the needs and the scruffies.

25:49.000 --> 25:55.000
Now you seem like a need to me, a need to someone who is quite puritanical

25:55.000 --> 25:57.000
and also it's related to universalism.

25:57.000 --> 26:04.000
So this idea that there are simple underlying principles that explain an awful lot of things

26:04.000 --> 26:12.000
rather than wanting to accept the gnarly kind of reality that everything's so bloody complicated.

26:12.000 --> 26:14.000
Where do you fall on that?

26:14.000 --> 26:19.000
So I definitely would not.

26:19.000 --> 26:20.000
Okay.

26:20.000 --> 26:23.000
So there's two separate things here.

26:23.000 --> 26:25.000
There's like, what's my aesthetic?

26:25.000 --> 26:27.000
Well, I want things to be neat.

26:27.000 --> 26:28.000
I want them to be beautiful.

26:28.000 --> 26:29.000
I want them to be mathematical.

26:29.000 --> 26:30.000
I want them to be elegant.

26:30.000 --> 26:31.000
Yes.

26:31.000 --> 26:33.000
And then there's what do I do in practice?

26:33.000 --> 26:35.000
And what do I believe is true about networks?

26:35.000 --> 26:42.000
Well, I think there is a lot more structure than most than many people think.

26:42.000 --> 26:48.000
But I also do not think they are just some beautiful purely algorithmic thing

26:48.000 --> 26:51.000
that we could uncover if we just knew the right tools.

26:51.000 --> 26:53.000
And like maybe they are.

26:53.000 --> 26:55.000
We'd fucking great if they were.

26:55.000 --> 27:01.000
But I expect they're messy and cursed, but with some deep structure and patterns

27:01.000 --> 27:06.000
and how much traction we can get on the weird scruffiness is like somewhat unclear to me.

27:06.000 --> 27:08.000
I think we can make a lot more progress than we have.

27:08.000 --> 27:11.000
But we might eventually hit a wall.

27:11.000 --> 27:14.000
You were saying something quite interesting when we drove over, which is,

27:14.000 --> 27:19.000
I mean, my friend Waleed Sabah, he's a linguist and he is a Platonist.

27:19.000 --> 27:25.000
He thinks that there are these universal cognitive priors and there's a hierarchy of them.

27:25.000 --> 27:28.000
And the complexity collapses.

27:28.000 --> 27:33.000
And he thinks that language models have somehow acquired these cognitive priors.

27:33.000 --> 27:37.000
And if we did some kind of symbolic decomposition, you know,

27:37.000 --> 27:40.000
we would all just kind of like pack itself into this beautiful hierarchy.

27:40.000 --> 27:43.000
And you were saying that there are Gabor filters and they're all these different circuits

27:43.000 --> 27:50.000
and they have motifs, they have categories, they have flavors for want of a better word.

27:50.000 --> 27:54.000
Are you optimistic that something like this could happen?

27:54.000 --> 27:56.000
Yeah.

27:56.000 --> 28:02.000
So, hmm.

28:02.000 --> 28:13.000
So, one interesting point here is often interoperability is fairly different

28:13.000 --> 28:16.000
for different modalities and different architectures.

28:16.000 --> 28:21.000
A lot of the early work was done on convolutional networks and image classifiers.

28:21.000 --> 28:25.000
The field very much nowadays focuses on transformer language models.

28:25.000 --> 28:32.000
And I think there's lots of structure to how transformers implement algorithms.

28:32.000 --> 28:37.000
Transformers cannot be recursive, but they're incredibly paralyzed.

28:37.000 --> 28:43.000
Transformers have this mechanism of attention that tells them how to move information between positions.

28:43.000 --> 28:49.000
And there's lots of algorithms and circuitry that can be expressed like this

28:49.000 --> 28:52.000
and lots of stuff that's really weird to express.

28:52.000 --> 28:58.000
And I think that this constrains them in a way that creates lots of interesting structure

28:58.000 --> 29:02.000
that can be understood and patterns that can be understood.

29:02.000 --> 29:06.000
Is this inherently true of intelligence? Who knows?

29:06.000 --> 29:13.000
But a lot of my optimism for structures within networks is more like that.

29:13.000 --> 29:18.000
But I try to think about structure more from a biologist's perspective

29:18.000 --> 29:22.000
than a mathematician's or like a philosopher's perspective.

29:22.000 --> 29:26.000
Though, I am a pure mathematician and I know nothing about biology.

29:26.000 --> 29:31.000
So, if anyone's listening to this, no stuff about biology and thinks I'm talking bullshit, please email.

29:31.000 --> 29:40.000
So, if you look at evolutionary biology, model organisms have all of this common shared structure.

29:40.000 --> 29:45.000
Like, most things have bones, we have cell nuclei.

29:45.000 --> 29:53.000
And hands of mammals tend to be surprisingly similar, but kind of weird and changed in various ways.

29:53.000 --> 29:58.000
And I don't know.

29:58.000 --> 30:01.000
I don't think these are like hard rules.

30:01.000 --> 30:04.000
Most of them have weird exceptions.

30:04.000 --> 30:08.000
And obviously a lot of this is due to the shared evolutionary history

30:08.000 --> 30:13.000
and is not just inherent to the substrate of you have proteins.

30:13.000 --> 30:17.000
Though, in fact, you often train these models on similar data in similar ways

30:17.000 --> 30:22.000
and they have the same architecture that constrains them to different kinds of algorithms.

30:22.000 --> 30:28.000
It makes me optimistic there's a biologist's level of a structure.

30:28.000 --> 30:31.000
Now, you said something interesting which is that transformers can't be used in a recursive way.

30:31.000 --> 30:35.000
Now, we'll just touch this very quickly because we've spoken about this a million times on different episodes.

30:35.000 --> 30:42.000
But, you know, there's the Chomsky hierarchy and he had this notion of a recursively enumerable language.

30:42.000 --> 30:47.000
And these different models, computational models in the Chomsky hierarchy,

30:47.000 --> 30:52.000
it's not only about being able to produce a language which exists in a certain set.

30:52.000 --> 30:57.000
It's also the ability to recognize that the language belongs in a certain set.

30:57.000 --> 31:04.000
And transformers are quite low down on that hierarchy because they're called recurrently not recursively.

31:04.000 --> 31:09.000
But I just wondered if you had any just, you know, prima facie if you had any views on that.

31:10.000 --> 31:15.000
Yeah, so I'm not a linguist. I'm not particularly familiar with the Chomsky hierarchy.

31:15.000 --> 31:20.000
I do think it's surprising how well transformers work.

31:20.000 --> 31:27.000
And I have a general skepticism of any theoretical hierarchy, like, I don't know.

31:27.000 --> 31:35.000
If you think there's some beautiful structure of algorithms and stuff that's low down,

31:36.000 --> 31:43.000
and then GPT-4 happens, I think a framework's wrong rather than transformers are wrong.

31:43.000 --> 31:50.000
Just massive stack of matrices plus a massive pile of data gives shockingly effective systems.

31:50.000 --> 31:54.000
Theoretical frameworks just often break when they make contact with reality.

31:54.000 --> 31:59.000
Well, that's certainly true. I mean, there's a famous expression that all grammars leak.

31:59.000 --> 32:04.000
I had rather, I don't know, I guess a similar conclusion to you, which is that if anything,

32:04.000 --> 32:08.000
it teaches us how sclerotic and predictable language is,

32:08.000 --> 32:15.000
and we don't actually need to have access to this infinite space or even exponentially large space.

32:15.000 --> 32:19.000
Most language use and most phenomena that we need, perhaps for intelligence,

32:19.000 --> 32:25.000
is surprisingly small and current models can work just well.

32:25.000 --> 32:28.000
Why don't we move on to your grokking work?

32:28.000 --> 32:37.000
So grokking is this sudden generalization that happens much later in training after...

32:37.000 --> 32:39.000
If I can add a brief clarification.

32:39.000 --> 32:40.000
Oh, yes, of course.

32:40.000 --> 32:44.000
So people often call grokking sudden generalization.

32:44.000 --> 32:46.000
My apologies. Go on.

32:46.000 --> 32:50.000
Sudden generalization is a much more common phenomena than grokking.

32:50.000 --> 32:54.000
It can just generally look like things like, I don't know, the model is trying to learn a task,

32:54.000 --> 32:56.000
it's kind of bad at it, and then it suddenly gets good at it.

32:56.000 --> 32:58.000
I prefer to call this a phase transition.

32:58.000 --> 33:05.000
Grokking is the specific thing where the model initially memorizes and does not generalize,

33:05.000 --> 33:11.000
and then there's a sudden phase transition in the test loss, the generalization ability,

33:11.000 --> 33:17.000
which creates a convergence after an initial divergence between train and test.

33:17.000 --> 33:22.000
And this is like a much, much more specific phenomena than sudden generalization.

33:22.000 --> 33:29.000
Okay, so you've spoken about three distinct phases of training underlying grokking.

33:29.000 --> 33:32.000
So why don't we go through them one by one?

33:32.000 --> 33:38.000
Yeah, so the context of this project, this was a paper called Progress Measures for Grokking

33:38.000 --> 33:45.000
by a Mechanistic Interpretability that I recently presented on at iClear.

33:46.000 --> 33:51.000
So we were studying a one-layer transformer, we trained to do modular addition,

33:51.000 --> 33:53.000
and it grokked modular addition.

33:53.000 --> 33:59.000
And the first thing we did was reverse engineer the algorithm behind how the model worked,

33:59.000 --> 34:03.000
which we may get into in a bit more detail, but at a very high level.

34:03.000 --> 34:08.000
Modular addition is equivalent to composing rotations around the unit circle.

34:08.000 --> 34:12.000
Composition adds the angle, circle gives you modularity.

34:12.000 --> 34:17.000
You can represent this by trig functions and do composition with trig identities

34:17.000 --> 34:23.000
and element-wise multiplication, and we reverse engineered exactly how the model did this.

34:23.000 --> 34:29.000
And then this mechanistic understanding was really important for understanding what was up with grokking,

34:29.000 --> 34:36.000
because the weird thing behind grokking is that it's not that the model memorizes,

34:36.000 --> 34:38.000
or that the model eventually generalizes.

34:38.000 --> 34:44.000
The surprising thing is that it first memorizes and then changes its mind and generalizes later.

34:44.000 --> 34:51.000
And generalization and memorization are two very different algorithms that both do very well on the training data,

34:51.000 --> 34:55.000
and only by understanding the mechanism will be able to disentangle them.

34:55.000 --> 35:00.000
And this meant we could look during training how much of the model's performance came from memorization

35:00.000 --> 35:02.000
and how much came from generalization.

35:02.000 --> 35:04.000
And we found these three distinct faces.

35:05.000 --> 35:07.000
There was memorization.

35:07.000 --> 35:12.000
The first very short phase, it gets phenomenally good train loss.

35:12.000 --> 35:18.000
It got to about 3e-7, which is an absolutely insane log loss.

35:18.000 --> 35:27.000
And much, much worse than random on test because memorization is very far from uniform and generalizes extremely badly.

35:27.000 --> 35:32.000
And then there was this long-seeming plateau.

35:33.000 --> 35:41.000
We call the space circuit formation because it turns out that rather than just continue to memorize for a while

35:41.000 --> 35:47.000
and doing a random walk through model space until it eventually gets lucky,

35:47.000 --> 35:53.000
the model is systematically transitioning from memorization to generalization.

35:53.000 --> 36:00.000
And you can see that its train performance gets worse and worse when you only let it memorize.

36:01.000 --> 36:04.000
And then, so why is test loss still bad?

36:04.000 --> 36:08.000
Test loss is bad because memorization generalizes terribly.

36:08.000 --> 36:14.000
And when the model is like, I don't know, two-thirds memorizing, one-third generalizing, it still does terribly.

36:14.000 --> 36:20.000
And it's only when the model gets so good at the trigger-based generalizing algorithm

36:20.000 --> 36:25.000
that it no longer needs the memorization parameters and cleans them up that we see grocking.

36:25.000 --> 36:29.000
And this happens fairly suddenly.

36:29.000 --> 36:37.000
But the, if you, we have this metric called restricted loss where we explicitly clean up the memorization for the model

36:37.000 --> 36:39.000
and look at how well it generalizes.

36:39.000 --> 36:43.000
And we see that restricted loss drops noticeably before test loss drops,

36:43.000 --> 36:48.000
showing that the drop is driven by cleaning up the noise.

36:48.000 --> 36:52.000
And this is striking because A.

36:52.000 --> 36:56.000
I had no idea it was even possible for a model to transition between two good solutions,

36:56.000 --> 36:59.000
maintaining equivalent performance throughout.

36:59.000 --> 37:05.000
B. There was this real mystery of deep learning that many people tried to answer,

37:05.000 --> 37:09.000
and mechanistic understanding was genuinely useful for answering it.

37:09.000 --> 37:13.000
And grocking was an illusion.

37:13.000 --> 37:15.000
It was not sudden generalization.

37:15.000 --> 37:18.000
It was gradual generalization followed by sudden cleanup.

37:18.000 --> 37:25.000
And test loss and test accuracy were just too course-metric to tell the difference.

37:25.000 --> 37:32.000
But we were able to design these hidden progress measures using our mechanistic understanding that made everything clear.

37:32.000 --> 37:38.000
And we also just have all kinds of pretty animations of qualitatively watching the circuits develop over training,

37:38.000 --> 37:40.000
and it's very pretty.

37:40.000 --> 37:41.000
So a few things.

37:41.000 --> 37:44.000
I mean, first of all, just going back to first principles.

37:44.000 --> 37:48.000
The biggest problem in machine learning is this concept called overfitting.

37:48.000 --> 37:52.000
And we trained the model on a training set.

37:52.000 --> 37:56.000
And there's this horrible phenomenon called the shortcut rule,

37:56.000 --> 38:00.000
which is that the model will take the path of least resistance.

38:00.000 --> 38:04.000
And when you're training it, it only really knows about the training set.

38:04.000 --> 38:10.000
And of course, we can test it on a different set afterwards, which we've held out.

38:10.000 --> 38:14.000
And just because of the way that we've structured the model,

38:14.000 --> 38:19.000
it may, by hook or by crook, generalize to the test set.

38:19.000 --> 38:22.000
But the interesting thing is that generalization isn't a binary.

38:22.000 --> 38:24.000
There's a whole spectrum of generalization.

38:24.000 --> 38:27.000
So it starts with the training set, and then we have the test set,

38:27.000 --> 38:30.000
and then the ideal is out-of-demand generalization.

38:30.000 --> 38:32.000
But I would go a step further.

38:32.000 --> 38:35.000
There's also algorithmic generalization,

38:35.000 --> 38:38.000
which is this notion that as I understand it,

38:38.000 --> 38:43.000
neural networks, if you model the function y equals x squared,

38:43.000 --> 38:50.000
it will only ever be able to learn the values of that function inside the training support.

38:50.000 --> 38:55.000
So presumably you're talking about the ideal form of generalization

38:55.000 --> 38:59.000
being not as good as algorithmic generalization,

38:59.000 --> 39:01.000
or do you think it could go all the way?

39:01.000 --> 39:08.000
So I think one thing which is very important to track

39:08.000 --> 39:15.000
is what the domain you're talking about is of which it's even possible to generalize.

39:15.000 --> 39:21.000
So I generally think about models that have discrete inputs rather than continuous inputs,

39:21.000 --> 39:26.000
because basically no neural network is going to be capable of dealing

39:26.000 --> 39:30.000
with like unbounded range continuous inputs.

39:31.000 --> 39:37.000
In modular addition, there were just two one-hot encoded inputs between 0 and 113,

39:37.000 --> 39:39.000
which is the modular I used.

39:39.000 --> 39:41.000
Yeah, the model has a fixed modular.

39:41.000 --> 39:43.000
It's not doing modular addition in general.

39:43.000 --> 39:48.000
And there's just like 12,000 inputs, and it learns to do all of them.

39:48.000 --> 39:52.000
And in, I don't know, behaviorally,

39:52.000 --> 39:55.000
you can't even tell the difference between the model memorizes everything

39:55.000 --> 39:58.000
and the model learns some true algorithm.

39:58.000 --> 40:01.000
Though, with the more cognitivist mechanistic approach,

40:01.000 --> 40:04.000
I can just look at it and say, yep, that's an algorithm.

40:04.000 --> 40:06.000
It's great. Not a stochastic parrot.

40:06.000 --> 40:08.000
Conclusively disprove that hypothesis.

40:10.000 --> 40:12.000
And yeah.

40:15.000 --> 40:19.000
I think that for the language models, it's more interesting,

40:19.000 --> 40:25.000
because I know GbD2, it's got 1,000 tokens, 50,000 vocab.

40:25.000 --> 40:29.000
It's like 50,000 to the power of 1,000 possible inputs.

40:30.000 --> 40:36.000
And there's a surprising amount of interesting algorithmic generalization.

40:37.000 --> 40:40.000
We're going to talk later about induction hits,

40:40.000 --> 40:45.000
which is the circuit language models learn to detect and continue repeated text.

40:45.000 --> 40:48.000
Like if given the word Neil, you want to know what comes next.

40:48.000 --> 40:52.000
Unfortunately, Nanda is not that homelist yet.

40:53.000 --> 40:57.000
But if Neil Nanda has come up like five times before in the text,

40:57.000 --> 40:59.000
Nanda's pretty likely to come next.

40:59.000 --> 41:07.000
And this transfers to, if you get the model, just random tokens,

41:07.000 --> 41:12.000
with some repetition, the model can predict the repeated random tokens,

41:12.000 --> 41:15.000
because the induction heads are just a real algorithm.

41:15.000 --> 41:21.000
And the space of possible repeated random tokens is like enormous.

41:21.000 --> 41:25.000
It's like, in some sense, much larger than the space of possible language.

41:25.000 --> 41:28.000
And is this algorithmic generalization?

41:28.000 --> 41:30.000
I don't really know. It depends on your perspective.

41:31.000 --> 41:36.000
Let's bring in this paper by Bilal Tughtay.

41:36.000 --> 41:40.000
So it was called a toy model of universality,

41:40.000 --> 41:43.000
reverse engineering how neural networks learn group operations,

41:43.000 --> 41:45.000
and you supervised that paper.

41:45.000 --> 41:50.000
And he was asking the question of whether neural networks learn universal solutions,

41:50.000 --> 41:52.000
or these idiosyncratic ones.

41:52.000 --> 41:54.000
And he said he found inherent randomness,

41:54.000 --> 41:57.000
but models could consistently learn group composition

41:57.000 --> 42:00.000
via an interpretable representation theory.

42:00.000 --> 42:03.000
So can you give us a quick tour de force of that work?

42:03.000 --> 42:04.000
Yeah.

42:04.000 --> 42:07.000
Maybe I should detour back to my grokking work

42:07.000 --> 42:09.000
and just explain the algorithm we found there

42:09.000 --> 42:11.000
and how we know it's the real algorithm.

42:11.000 --> 42:12.000
Yeah, sure.

42:12.000 --> 42:13.000
This is a good foundation for this paper.

42:13.000 --> 42:14.000
Sure, sure.

42:14.000 --> 42:19.000
Yeah, so we found this thing we call the Fourier multiplication algorithm.

42:19.000 --> 42:24.000
The very high level it composes rotations.

42:24.000 --> 42:28.000
You can actually look at how the different bits of the model implement the algorithm,

42:28.000 --> 42:30.000
and often just read this off.

42:30.000 --> 42:33.000
So the embeddings are just a lookup table

42:33.000 --> 42:38.000
mapping the one-hot encoded inputs to these trig terms.

42:38.000 --> 42:40.000
Sines and cosines are different frequencies.

42:40.000 --> 42:43.000
You can just read this off the embedding weights.

42:43.000 --> 42:47.000
Note, people often think that learning sine and cosine is hard.

42:47.000 --> 42:52.000
It's actually very easy because you only need it on 113 different data points,

42:52.000 --> 42:54.000
such as a lookup table.

42:54.000 --> 43:00.000
The model then uses the attention and MLPs to do this composition,

43:00.000 --> 43:04.000
to do the multiplication with trig identities

43:04.000 --> 43:10.000
to get the, like, composed rotation, the A plus B terms.

43:10.000 --> 43:16.000
And here we can just read off the neurons that they have learned these terms

43:16.000 --> 43:18.000
and that they were not there beforehand.

43:18.000 --> 43:23.000
The model is using its non-linearities in interesting ways to do this.

43:23.000 --> 43:30.000
It's also incredibly cursed because ReLUs are not designed to multiply two different inputs,

43:30.000 --> 43:35.000
but it turns out they can if you have enough of them and it's sufficiently cursed.

43:35.000 --> 43:39.000
And yeah, we can just read this off the neurons.

43:39.000 --> 43:45.000
Also, if you just put anything inside the model, it's beautiful and it's so periodic, and I love it.

43:46.000 --> 43:48.000
Could I touch on that though?

43:48.000 --> 43:54.000
Because you said you don't need to know the sine function because you can just memorise it within an interval.

43:54.000 --> 43:58.000
Is that, I don't know, how does that break down?

43:58.000 --> 44:06.000
Because it's discretising it and it's kind of assuming that it has the same behaviour in different intervals.

44:06.000 --> 44:17.000
So, I think a key thing here is that you are solving modular addition on discrete one-hot encoded inputs

44:17.000 --> 44:20.000
rather than for arbitrary continuous inputs.

44:20.000 --> 44:23.000
Arbitrary continuous inputs is way harder.

44:23.000 --> 44:28.000
And so you, it's not even on an interval, it's just learning snapshot.

44:28.000 --> 44:32.000
It's just learning, like, single points on the sine and cosine curves.

44:32.000 --> 44:41.000
And, I don't know, there's this family of maths about studying periodic functions with different kinds of Fourier transforms,

44:41.000 --> 44:52.000
and this is all discussing discrete Fourier transforms, which are just a reasonable way of looking at periodic sequences of length n.

44:52.000 --> 44:56.000
And that's how I recommend thinking about this one.

44:56.000 --> 45:03.000
It's kind of, like, just quite different from a model that's trying to learn the true sine and cosine function.

45:03.000 --> 45:13.000
And, yeah, the model then needs to convert the composed rotation back to the actual answer,

45:13.000 --> 45:17.000
which is an even more galaxy-brained operation that you can read off from the wits.

45:17.000 --> 45:22.000
So, you've got terms of the form cos A plus B.

45:22.000 --> 45:26.000
The model has some weights mapping to each output C.

45:26.000 --> 45:35.000
And it uses further trig identities to get terms of the form cos A plus B minus C times some frequency.

45:35.000 --> 45:38.000
And where A and B are the two inputs, C is the output.

45:38.000 --> 45:46.000
And you then use the softmax as an argmax to, like, extract the C that maximizes this.

45:46.000 --> 45:51.000
And because cos is maximized at zero, this is maximized at C equals A plus B.

45:51.000 --> 45:55.000
And if you choose the frequency right, this gets you mod N.

45:55.000 --> 45:59.000
And you can just read this off the model weights. It's great.

45:59.000 --> 46:03.000
And then, finally, you can verify you've understood it correctly,

46:03.000 --> 46:09.000
because if you ablate everything that our algorithm says should not matter, performance improves.

46:09.000 --> 46:14.000
While if you ablate any of the bits our algorithm says should matter, performance tanks.

46:14.000 --> 46:17.000
Okay. Could you give me some intuition, though?

46:17.000 --> 46:22.000
So, we start off in the memorization phase, because I guess you can think of a neural network

46:22.000 --> 46:26.000
as doing many different things in a very complicated way,

46:26.000 --> 46:30.000
and there's some kind of change in the balance during training.

46:30.000 --> 46:37.000
So, it does the easy thing first, and then it gradually learns how to generalize.

46:37.000 --> 46:42.000
And in this particular case, how does that thing, because we're using stochastic gradient descent,

46:42.000 --> 46:46.000
so we're moving all of these weights around, and the inductive prior is also very important

46:46.000 --> 46:49.000
and we'll come to that, I think, after we've spoken about Bill House paper.

46:49.000 --> 46:54.000
But how does that happen gradually in really simple terms?

46:54.000 --> 47:00.000
Hmm. Is the question kind of, it ends up at this discrete algorithm,

47:00.000 --> 47:03.000
but it does survive continuous steps. How does that work?

47:03.000 --> 47:07.000
Well, I think the thing that surprised a lot of people about grokking is this,

47:07.000 --> 47:11.000
I mean, grokking, the clue's in the name. So, it's gone from memorization,

47:11.000 --> 47:14.000
and then we're using stochastic gradient descent,

47:14.000 --> 47:18.000
and you would think that it's gotten stuck in some kind of local minima.

47:18.000 --> 47:22.000
And you're training, and you're training, and you're training, and then there's a spark.

47:22.000 --> 47:29.000
Something happens, and then you get these new modes, kind of like emerging in the network.

47:29.000 --> 47:32.000
I'm not sure if emerging is the right term.

47:32.000 --> 47:37.000
And it happens gradually, and it happens after a long time.

47:38.000 --> 47:43.000
Yeah. So, there's a couple of things here that's pretty easy to misunderstand.

47:43.000 --> 47:47.000
The first is that... Hmm.

47:47.000 --> 47:51.000
The first is that I think it's pretty hard for a model to ever get stuck,

47:51.000 --> 47:57.000
because, I know, this model had about 200,000 parameters, model ones have billions.

47:57.000 --> 48:01.000
It's just moving in a very high dimensional space,

48:01.000 --> 48:05.000
and you can get stuck on 150,000 dimensions.

48:05.000 --> 48:07.000
But you've got 50,000 to play with.

48:07.000 --> 48:14.000
And especially for a fairly over-parameterized model like this one,

48:14.000 --> 48:19.000
for a fairly simple task, there's just so much room to move around.

48:19.000 --> 48:24.000
Another common misunderstanding of grokking is people say,

48:24.000 --> 48:28.000
it's memorized, it's got zero loss, so why does it need to learn?

48:28.000 --> 48:30.000
Two misunderstandings here.

48:30.000 --> 48:35.000
First, zero loss is impossible unless you have bullshit floating point errors,

48:35.000 --> 48:40.000
because it's like the average correct log prop.

48:40.000 --> 48:43.000
Log of anything can never get to...

48:43.000 --> 48:48.000
The log will never quite get to zero because of just how softmax works.

48:48.000 --> 48:52.000
And you need to have an infinite logic for that to happen.

48:52.000 --> 48:56.000
The one cute thing in an appendix to our paper is that

48:57.000 --> 49:01.000
cannot represent log probes less than 1.19e-7,

49:01.000 --> 49:06.000
which leads to bizarre loss spikes sometimes, unless you use float64.

49:06.000 --> 49:10.000
Anyway, the second is regularization.

49:10.000 --> 49:14.000
If you don't have any kind of regularization, the model will just continue to memorize.

49:14.000 --> 49:18.000
We use weight decay, dropout also works,

49:18.000 --> 49:24.000
and so the model, the kind of core tension behind grokking

49:24.000 --> 49:30.000
is there's some feature of the lost landscape that makes it easier to get to memorization.

49:30.000 --> 49:37.000
You can memorize faster, while generalization is somehow hard to get to and much more gradual.

49:37.000 --> 49:42.000
So the model memorizes first, but it ultimately prefers to generalize,

49:42.000 --> 49:45.000
but it's only a mild preference.

49:45.000 --> 49:49.000
And the reason for this is we cherry pick the amount of data where it's a mild preference,

49:49.000 --> 49:54.000
because there's too little, it will just always memorize if there's too much, it will immediately generalize,

49:54.000 --> 49:57.000
because grokking is a little bit cheating.

49:57.000 --> 50:01.000
And yeah, you then use this,

50:01.000 --> 50:06.000
and because the model's initially memorized, but it wants to generalize,

50:06.000 --> 50:13.000
it can follow, it memorizes until the desire to memorize more balances with the desire to have smaller weights,

50:13.000 --> 50:18.000
but both of these reinforce the drive to generalize,

50:18.000 --> 50:22.000
because that makes both of them happier.

50:22.000 --> 50:28.000
And so the model very slowly interpolates, very, very slightly improving test loss,

50:28.000 --> 50:32.000
very slowly improving train loss, until it eventually gets there,

50:32.000 --> 50:36.000
and has this acceleration at the end, this phase transition,

50:36.000 --> 50:42.000
and cleanup, which leads to the seemingly sudden grokking behavior.

50:42.000 --> 50:46.000
Okay, and when you were talking about the, it wants the weights to be smaller,

50:46.000 --> 50:50.000
so that's weight decay, and it's like an inductive bias,

50:50.000 --> 50:54.000
essentially, to tell the model to reduce its complexity,

50:54.000 --> 50:57.000
which is a pressure to generalize.

50:57.000 --> 51:01.000
But if it wasn't for that, then that wouldn't happen.

51:01.000 --> 51:06.000
So in the experiments I ran, if you don't have weight decay,

51:06.000 --> 51:10.000
it will just keep memorizing infinitely far,

51:10.000 --> 51:13.000
because when you get perfect accuracy, if you double all your logits,

51:13.000 --> 51:16.000
you just get more confident in the right answer,

51:16.000 --> 51:18.000
and so it just keeps scaling up.

51:18.000 --> 51:21.000
I was using full batch training because it's such a tiny problem,

51:21.000 --> 51:24.000
this made things smoother and easier.

51:24.000 --> 51:27.000
I've heard some attic data that sometimes you can get it to work

51:27.000 --> 51:31.000
if you just have mini batch stochastic gradient descent,

51:31.000 --> 51:34.000
but I haven't looked into that particularly hard.

51:34.000 --> 51:35.000
Interesting.

51:35.000 --> 51:39.000
There are some hypotheses that stochasticity acts as an implicit regularizer

51:39.000 --> 51:41.000
because it adds noise.

51:41.000 --> 51:42.000
I don't really know.

51:42.000 --> 51:44.000
So let's go back to Bilal's paper then.

51:44.000 --> 51:47.000
So this paper, a toy model of universality,

51:47.000 --> 51:50.000
reverse engineering, how neural networks learn group operations.

51:50.000 --> 51:52.000
Can you give us an elevated pitch?

51:52.000 --> 51:59.000
Yeah, so an observation that actually first discovered at a party

51:59.000 --> 52:02.000
in the Bay Area from a guy called Sam Box,

52:02.000 --> 52:05.000
is that the modular addition algorithm we found

52:05.000 --> 52:08.000
is actually a representation theory algorithm.

52:08.000 --> 52:14.000
So group representations are collections of symmetries

52:14.000 --> 52:17.000
of some geometric objects that correspond to the group.

52:17.000 --> 52:20.000
Modular addition is the cyclic group,

52:20.000 --> 52:28.000
and rotations of the regular n-gon are the representations

52:28.000 --> 52:29.000
of the cyclic group,

52:29.000 --> 52:32.000
and this corresponds to the rotation by the unit circle

52:32.000 --> 52:34.000
that can pose that we found.

52:34.000 --> 52:37.000
But it turns out you can just make this work for arbitrary groups,

52:38.000 --> 52:41.000
you replace the two rotations with just two representations,

52:41.000 --> 52:44.000
you compose them, and the model,

52:44.000 --> 52:47.000
and it turns out the cos a plus b minus c thing

52:47.000 --> 52:50.000
is this math jargon called the character.

52:50.000 --> 52:52.000
You don't mean to unsat any of that,

52:52.000 --> 52:55.000
but it's very cute if, like me, you have a pure math degree.

52:55.000 --> 52:58.000
And for example,

52:58.000 --> 53:02.000
if you have the group of permutations of five elements,

53:02.000 --> 53:06.000
the 120 different ways to rearrange five objects,

53:07.000 --> 53:09.000
one example of representations of this

53:09.000 --> 53:15.000
are rotations and reflections of the four-dimensional tetrahedra,

53:15.000 --> 53:19.000
and if you train a one-hidden layer MLP to grok this

53:19.000 --> 53:20.000
and look inside,

53:20.000 --> 53:23.000
you're going to see these rotations that it's learned.

53:23.000 --> 53:25.000
It's gorgeous.

53:25.000 --> 53:27.000
And so the first half of that paper

53:27.000 --> 53:30.000
was just showing that the algorithm worked,

53:30.000 --> 53:33.000
showing that this was actually learned in practice.

53:34.000 --> 53:38.000
Then the more interesting bit was this focus on universality.

53:38.000 --> 53:41.000
So universality is this hypothesis,

53:41.000 --> 53:47.000
that models have some intrinsic solutions to a problem

53:47.000 --> 53:49.000
that many different models will converge on,

53:49.000 --> 53:52.000
at least given similar data and similar architectures,

53:52.000 --> 53:55.000
e.g. in image models,

53:55.000 --> 53:58.000
models will learn specific neurons that detect curves,

53:58.000 --> 54:00.000
and different models and different datasets

54:00.000 --> 54:02.000
seem to learn this similar thing.

54:03.000 --> 54:07.000
And here this was interesting

54:07.000 --> 54:12.000
because groups have a finite set of irreducible representations,

54:12.000 --> 54:14.000
maths theorem.

54:14.000 --> 54:15.000
You can enumerate these.

54:15.000 --> 54:17.000
There are that many of them.

54:17.000 --> 54:20.000
And for groups that are not modular addition,

54:20.000 --> 54:23.000
these are qualitatively different,

54:23.000 --> 54:26.000
like some of them act on a four-dimensional object,

54:26.000 --> 54:27.000
like the tetrahedron,

54:27.000 --> 54:30.000
some of them act on like 5D or 60 objects.

54:30.000 --> 54:32.000
Naively, some of them are simpler than others,

54:32.000 --> 54:34.000
but they're definitely different.

54:34.000 --> 54:37.000
So what we did is we asked ourselves the question,

54:37.000 --> 54:39.000
which one does the model learn?

54:39.000 --> 54:42.000
And we found that even if you just vary the random seed,

54:42.000 --> 54:45.000
the model will randomly choose a subset of themes

54:45.000 --> 54:46.000
each time to learn.

54:46.000 --> 54:48.000
And there's some structure,

54:48.000 --> 54:53.000
like it tends to learn some of them more often than others.

54:53.000 --> 54:57.000
A little bit maps to our insurance of notion of simplicity,

54:57.000 --> 54:58.000
but not that much.

54:58.000 --> 55:00.000
One of the updates I made in the paper

55:00.000 --> 55:02.000
is that simplicity is a really cursed concept,

55:02.000 --> 55:04.000
I don't understand very well,

55:04.000 --> 55:05.000
where I don't know.

55:05.000 --> 55:07.000
If you have rotations of a four-dimensional object,

55:07.000 --> 55:08.000
that seems simpler,

55:08.000 --> 55:11.000
but maybe the 60 object takes more dimensions

55:11.000 --> 55:13.000
but has better loss per unit weight norm,

55:13.000 --> 55:16.000
which is simpler, I don't know.

55:16.000 --> 55:18.000
But yeah, anyway,

55:18.000 --> 55:20.000
we found that each run of the model learns

55:20.000 --> 55:22.000
some combination of these circuits

55:22.000 --> 55:24.000
for the different representations.

55:24.000 --> 55:26.000
It's like normally more than one,

55:26.000 --> 55:28.000
the exact number varies,

55:28.000 --> 55:31.000
and which ones it learns is seemingly random each time,

55:31.000 --> 55:36.000
which suggests that all toy models lie to you, obviously.

55:36.000 --> 55:39.000
But if we're trying to reason about real networks,

55:39.000 --> 55:43.000
looking at this work might suggest the explanation,

55:43.000 --> 55:47.000
the hypothesis that if there are multiple ways

55:47.000 --> 55:48.000
to implement a circuit,

55:48.000 --> 55:50.000
which in practice they normally are,

55:50.000 --> 55:54.000
models may learn different ones of them,

55:54.000 --> 55:57.000
kind of for fairly random reasons,

55:57.000 --> 55:59.000
and the fully understanding one model

55:59.000 --> 56:02.000
will not perfectly transfer to another model.

56:02.000 --> 56:05.000
And I think there's loads of really interesting

56:05.000 --> 56:06.000
open questions here.

56:06.000 --> 56:09.000
Like, I don't know,

56:09.000 --> 56:11.000
people have done various work understanding

56:11.000 --> 56:14.000
different kinds of specific circuits and models,

56:14.000 --> 56:16.000
like the interoperability in the wild paper

56:16.000 --> 56:17.000
we'll get to later.

56:17.000 --> 56:20.000
What does this look like in other models?

56:20.000 --> 56:23.000
Often there's multiple ways to implement a circuit.

56:23.000 --> 56:25.000
Can you disentangle the two,

56:25.000 --> 56:27.000
do all models learn both,

56:27.000 --> 56:30.000
or do some models learn one, some learn the other?

56:30.000 --> 56:32.000
I don't really know.

56:32.000 --> 56:33.000
So a couple of questions.

56:33.000 --> 56:35.000
I mean, first of all, this is leading towards this idea

56:35.000 --> 56:37.000
that we were speaking about before,

56:37.000 --> 56:41.000
which is that even if different networks,

56:41.000 --> 56:43.000
slightly different problems or variations

56:43.000 --> 56:44.000
on the same problem,

56:44.000 --> 56:46.000
it could learn these algorithmic primitives.

56:46.000 --> 56:49.000
Now, the first observation here is that

56:50.000 --> 56:53.000
the inductive biases of the network

56:53.000 --> 56:56.000
differ massively, right?

56:56.000 --> 56:59.000
So to what extent do the inductive biases

56:59.000 --> 57:02.000
affect these primitives which are learned?

57:02.000 --> 57:04.000
Oh, so much.

57:04.000 --> 57:05.000
They do.

57:05.000 --> 57:06.000
So...

57:06.000 --> 57:08.000
Well, can I frame the question a little bit?

57:08.000 --> 57:10.000
Because this reminds me a lot of

57:10.000 --> 57:12.000
the geometric deep learning blueprint

57:12.000 --> 57:16.000
from Petir and Michael Bronstein and all those guys.

57:16.000 --> 57:18.000
And they were coming at this

57:18.000 --> 57:20.000
from exactly the same direction as you,

57:20.000 --> 57:23.000
that they said there's a representation of a domain,

57:23.000 --> 57:25.000
which is basically a symmetry group,

57:25.000 --> 57:28.000
and you can do all of these different transformations,

57:28.000 --> 57:31.000
and as long as they fall in different positions

57:31.000 --> 57:32.000
in the underlying domain,

57:32.000 --> 57:35.000
so they respect the structure, then it works.

57:35.000 --> 57:40.000
But all of those symmetries are effectively coded

57:40.000 --> 57:42.000
into the inductive prior.

57:42.000 --> 57:44.000
So, for example, if a CNN

57:44.000 --> 57:47.000
works on this gridded 2D manifold

57:47.000 --> 57:51.000
and it explicitly models

57:51.000 --> 57:53.000
translational equivalents

57:53.000 --> 57:56.000
and local connectivity and weight sharing and so on.

57:56.000 --> 57:58.000
So I guess what I'm saying is,

57:58.000 --> 58:01.000
you're talking about this four-dimensional tetrahedra,

58:01.000 --> 58:06.000
and that isn't explicitly modeled in an MLP.

58:06.000 --> 58:07.000
Not so.

58:07.000 --> 58:10.000
So how are you even recognizing

58:10.000 --> 58:12.000
that it's learning those symmetries?

58:12.000 --> 58:13.000
How are you even probing it?

58:13.000 --> 58:15.000
Maybe we should start with that.

58:15.000 --> 58:19.000
So I guess thing one, models are just smarter than you, man.

58:19.000 --> 58:22.000
Models can do a lot of weird stuff.

58:22.000 --> 58:25.000
I feel like the story of deep learning

58:25.000 --> 58:28.000
is people initially thought they needed to spoonfeed

58:28.000 --> 58:33.000
these models the right inductive biases over the data.

58:33.000 --> 58:35.000
And we've gradually realized,

58:35.000 --> 58:37.000
oh, wait, no, no, this is fine.

58:37.000 --> 58:38.000
The models can figure it out.

58:38.000 --> 58:40.000
For example, early on,

58:40.000 --> 58:43.000
image models were convolutional networks.

58:43.000 --> 58:46.000
You tell it the key information is nearby,

58:46.000 --> 58:48.000
and if you translate the image, it doesn't matter.

58:48.000 --> 58:51.000
And now everyone uses transformers, including for images,

58:51.000 --> 58:54.000
and transformers replace the convolutional mechanism

58:54.000 --> 58:56.000
with attention, where you're now saying,

58:56.000 --> 58:59.000
okay, one-sixth of your parameters

58:59.000 --> 59:01.000
are dedicated to figuring out

59:01.000 --> 59:04.000
where to move information between positions.

59:04.000 --> 59:06.000
Sometimes it'll be a convolution,

59:06.000 --> 59:08.000
and sometimes models do learn convolution,

59:08.000 --> 59:09.000
but often it won't be.

59:09.000 --> 59:11.000
And we want you,

59:11.000 --> 59:14.000
and you can now spend the parameters to figure this out.

59:14.000 --> 59:19.000
And I'm not very familiar with the geometric deep learning literature,

59:19.000 --> 59:21.000
but I generally am just kind of like,

59:21.000 --> 59:22.000
models can figure it out.

59:22.000 --> 59:25.000
The way we figured out that this was what's going on

59:25.000 --> 59:30.000
is kind of analogous to what we did in the modular audition case,

59:30.000 --> 59:32.000
where we just look at the embedding matrix

59:32.000 --> 59:35.000
and just read off the learned sine and cosine terms.

59:35.000 --> 59:38.000
Here we said, okay,

59:38.000 --> 59:44.000
the rotations of the 4D tetrahedron are these like 4x4 matrices.

59:44.000 --> 59:46.000
You can flatten this to a 16-dimensional vector.

59:46.000 --> 59:49.000
Let's probe for that linearly.

59:49.000 --> 59:51.000
And this kind of works,

59:51.000 --> 59:54.000
and you can probe for the different representations

59:54.000 --> 59:57.000
and basically see what's going on.

59:57.000 --> 01:00:01.000
Okay, I think that the thrust of the geometric deep learning stuff,

01:00:01.000 --> 01:00:04.000
or any inductive prior, comes back to the bias-variance trade-off.

01:00:04.000 --> 01:00:06.000
And the cursive dimensionality.

01:00:06.000 --> 01:00:09.000
So no one's saying, of course, an MLP,

01:00:09.000 --> 01:00:13.000
if you look at the function space that it can approximate,

01:00:13.000 --> 01:00:16.000
it's exponentially larger than that of a CNN.

01:00:16.000 --> 01:00:20.000
So it was always about sample efficiency.

01:00:20.000 --> 01:00:23.000
So, yeah, an MLP can learn anything,

01:00:23.000 --> 01:00:26.000
but we would never be able to train it for most problems.

01:00:26.000 --> 01:00:28.000
Mm-hmm.

01:00:28.000 --> 01:00:30.000
Yeah.

01:00:30.000 --> 01:00:35.000
So, I guess I maybe want to avoid going too deeply into this

01:00:35.000 --> 01:00:38.000
because I think the modular addition problem

01:00:38.000 --> 01:00:42.000
and the group problem is just a very weird problem.

01:00:42.000 --> 01:00:46.000
There's an algorithm that it's fairly natural for a model to learn

01:00:46.000 --> 01:00:52.000
with literally a single nonlinear step of the matrix multiply.

01:00:52.000 --> 01:00:55.000
One very cute result from the last paper

01:00:55.000 --> 01:00:58.000
is that the model can implement two 4x4 matrix multipliers

01:00:58.000 --> 01:01:03.000
with a single ReLU layer, which is very cute.

01:01:03.000 --> 01:01:07.000
But, yeah, there's a fairly natural algorithm to implement.

01:01:07.000 --> 01:01:09.000
That's a certain...

01:01:09.000 --> 01:01:13.000
Yeah, another useful intuition is that the more data you have,

01:01:13.000 --> 01:01:15.000
the more complex memorization gets,

01:01:15.000 --> 01:01:19.000
while generalization is exactly as complex at each point.

01:01:19.000 --> 01:01:22.000
And, yeah.

01:01:22.000 --> 01:01:24.000
So there's always going to be a crossover point

01:01:24.000 --> 01:01:27.000
where we have enough data where it is simpler to learn the circuit

01:01:27.000 --> 01:01:30.000
that generalizes.

01:01:30.000 --> 01:01:32.000
And I don't know.

01:01:32.000 --> 01:01:36.000
I'm hesitant to draw too much from toy models about the real problem.

01:01:36.000 --> 01:01:40.000
I guess one, two final points I'd want to just leave on this section.

01:01:40.000 --> 01:01:42.000
The first is I just want to re-emphasize.

01:01:42.000 --> 01:01:45.000
I did not do the toy model of the universality paper.

01:01:45.000 --> 01:01:48.000
I was supervising a mentee, Bella Chucktie, who did it.

01:01:48.000 --> 01:01:50.000
He did a fantastic job.

01:01:50.000 --> 01:01:52.000
So thanks, Bella, for your listening.

01:01:52.000 --> 01:01:56.000
Secondly, for the module addition case,

01:01:56.000 --> 01:01:59.000
I had no idea this outcome was going to be there when I went in.

01:01:59.000 --> 01:02:03.000
I just poked around, noticed the weird periodicity,

01:02:03.000 --> 01:02:06.000
realized it was using, I should apply Fourier transforms,

01:02:06.000 --> 01:02:09.000
and then the whole problem kind of fell together.

01:02:09.000 --> 01:02:13.000
And to me, the real takeaway of this paper is like,

01:02:13.000 --> 01:02:15.000
I don't give a fuck about GROCK.

01:02:15.000 --> 01:02:20.000
It is genuinely possible to understand what is going on in a model.

01:02:20.000 --> 01:02:23.000
You don't need to know what's going on in advance to discover this.

01:02:23.000 --> 01:02:29.000
And there is beautiful, non-trivial structure that can be understood.

01:02:29.000 --> 01:02:32.000
And who knows if this will happen in, like, actual full models.

01:02:32.000 --> 01:02:36.000
But to me, this is much more compelling than if we had nothing at all.

01:02:36.000 --> 01:02:37.000
Beautiful.

01:02:37.000 --> 01:02:38.000
Okay.

01:02:38.000 --> 01:02:39.000
And just before we move off the section,

01:02:39.000 --> 01:02:43.000
Biloud had a beautiful Twitter thread, actually.

01:02:43.000 --> 01:02:46.000
And he was talking about the potential for what he called

01:02:46.000 --> 01:02:50.000
a periodic table of universal circuits.

01:02:50.000 --> 01:02:53.000
And I actually think that's a really cool idea.

01:02:53.000 --> 01:02:55.000
So that would be amazing if that would work out.

01:02:55.000 --> 01:02:57.000
But he also brought up the lottery ticket hypothesis,

01:02:57.000 --> 01:03:00.000
and I've interviewed Jonathan Frankel.

01:03:00.000 --> 01:03:04.000
And the idea there is that some of this information

01:03:04.000 --> 01:03:07.000
might actually be encoded and understandable at initialization

01:03:07.000 --> 01:03:09.000
before you even start training.

01:03:09.000 --> 01:03:14.000
And apparently, you folks have found weak evidence

01:03:14.000 --> 01:03:17.000
for this in at least one group.

01:03:17.000 --> 01:03:18.000
Ah.

01:03:18.000 --> 01:03:19.000
All right.

01:03:19.000 --> 01:03:22.000
So a couple of things there.

01:03:22.000 --> 01:03:25.000
So this idea of a periodic table of circuits,

01:03:25.000 --> 01:03:29.000
I believe, is originated in this post called Circuits Zoom In

01:03:29.000 --> 01:03:32.000
from Chris Ola.

01:03:32.000 --> 01:03:33.000
We probably cannot claim credit.

01:03:33.000 --> 01:03:34.000
Good job, Chris.

01:03:34.000 --> 01:03:37.000
It's a beautifully evocative term.

01:03:37.000 --> 01:03:39.000
Yeah, the story of basically everything in Mac and Terp

01:03:39.000 --> 01:03:41.000
is, yeah, there was this Chris Ola paper from, like,

01:03:41.000 --> 01:03:44.000
two years ago that has it somewhere inside.

01:03:44.000 --> 01:03:47.000
Anthropic recently put out this beautiful blog post

01:03:47.000 --> 01:03:50.000
called Interpretability Dreams about their vision

01:03:50.000 --> 01:03:53.000
for the field of mechanistic interpretability

01:03:53.000 --> 01:03:55.000
and the kind of subtext.

01:03:55.000 --> 01:03:58.000
So they kept just quoting bits of old papers being like,

01:03:58.000 --> 01:04:00.000
so we already said this, but let's now, like,

01:04:00.000 --> 01:04:03.000
summarize it better and be clear about how this

01:04:03.000 --> 01:04:05.000
sits into our overall picture.

01:04:05.000 --> 01:04:09.000
Anyway, so, yeah, the idea of the periodic table is

01:04:09.000 --> 01:04:13.000
maybe there is just some finite list of ways a thing

01:04:13.000 --> 01:04:16.000
can be implemented naturally in a massive stack of matrices

01:04:16.000 --> 01:04:20.000
that we can enumerate by studying one or maybe several

01:04:20.000 --> 01:04:24.000
networks, understand them, and then compile all of this

01:04:24.000 --> 01:04:27.000
into something beautiful.

01:04:27.000 --> 01:04:29.000
And...

01:04:29.000 --> 01:04:33.000
which is kind of what we found in the representations case.

01:04:33.000 --> 01:04:36.000
Though here it was nice because there were

01:04:36.000 --> 01:04:40.000
genuinely a finite set that we could have fully enumerate.

01:04:40.000 --> 01:04:44.000
Regarding the lottery ticket stuff,

01:04:44.000 --> 01:04:48.000
I think this was a random observation I had on the

01:04:48.000 --> 01:04:52.000
Modular Edition case, partially inspired by a result

01:04:52.000 --> 01:04:56.000
from Eric Michaud at MIT, who was involved in some

01:04:56.000 --> 01:04:59.000
other papers on Grocking.

01:04:59.000 --> 01:05:03.000
And so what we found is that at the end of training,

01:05:03.000 --> 01:05:07.000
there are these directions in the weights that represent

01:05:07.000 --> 01:05:10.000
like the sine and cos terms of frequency,

01:05:10.000 --> 01:05:13.000
14 pi over 113.

01:05:13.000 --> 01:05:15.000
And...

01:05:15.000 --> 01:05:17.000
if you look at the embedding at the start and project one

01:05:17.000 --> 01:05:21.000
to these directions, it's like surprisingly circular.

01:05:21.000 --> 01:05:25.000
It's like the model has extracted those directions.

01:05:25.000 --> 01:05:30.000
And my wildly unsubstantiated hypothesis for why models

01:05:30.000 --> 01:05:34.000
just learn these algorithms and circuits at all

01:05:34.000 --> 01:05:37.000
is that there are some directions that if you

01:05:37.000 --> 01:05:41.000
deleted everything else would like form this beautiful circuit.

01:05:41.000 --> 01:05:45.000
This is kind of a trivial statement about linear algebra for the most part.

01:05:45.000 --> 01:05:48.000
And this underlying hidden circuit, each bit reinforces

01:05:48.000 --> 01:05:51.000
each other systematically because they're useful.

01:05:51.000 --> 01:05:54.000
Well, everything else is kind of noise, so it gets kind of gradually

01:05:54.000 --> 01:05:56.000
decayed.

01:05:56.000 --> 01:06:00.000
Over time, this will give you the circuit in a way that looks

01:06:00.000 --> 01:06:02.000
surprising and emergent.

01:06:02.000 --> 01:06:06.000
And this also can partially explain why phase transitions

01:06:06.000 --> 01:06:08.000
happen.

01:06:08.000 --> 01:06:10.000
There was a really good post from Adam German and Bach Schleggeres

01:06:10.000 --> 01:06:16.000
called on S-shaped curves, which argue that

01:06:16.000 --> 01:06:20.000
if you've got something that's like the composition of multiple different

01:06:20.000 --> 01:06:23.000
weight matrices, let's just say two of them,

01:06:23.000 --> 01:06:26.000
the gradient on the first is proportional to how good the

01:06:26.000 --> 01:06:28.000
second is and vice versa.

01:06:28.000 --> 01:06:31.000
So at the start, they both grow very slowly, but then they'll

01:06:31.000 --> 01:06:35.000
reinforce each other and eventually cascade as they're optimizing

01:06:35.000 --> 01:06:38.000
on the problem in a way that looks kind of sudden and S-shaped.

01:06:38.000 --> 01:06:41.000
And so my understanding is the original lottery ticket

01:06:41.000 --> 01:06:44.000
hypothesis is kind of discreet.

01:06:44.000 --> 01:06:47.000
It's looking on the neuron level and it's learning masks over

01:06:47.000 --> 01:06:49.000
weights and over neurons.

01:06:49.000 --> 01:06:53.000
And I'm kind of discussing and in some sense much more trivial

01:06:53.000 --> 01:06:57.000
version, where I'm not assuming there's some canonical basis

01:06:57.000 --> 01:06:59.000
of neurons.

01:06:59.000 --> 01:07:02.000
I'm saying, well, there's some directions in space that matter

01:07:02.000 --> 01:07:05.000
and if you delete all other directions, everything kind of

01:07:05.000 --> 01:07:09.000
works, which I think is a much more trivial statement,

01:07:09.000 --> 01:07:12.000
though the space of possible neurons is enormous.

01:07:12.000 --> 01:07:14.000
Though I don't know.

01:07:14.000 --> 01:07:16.000
One thing you want to be pretty careful of when discussing this

01:07:16.000 --> 01:07:21.000
the mask you learn is the computation since, and no,

01:07:21.000 --> 01:07:24.000
there's probably quite a lot of algorithms can be cleverly

01:07:24.000 --> 01:07:28.000
expressed with a mask over a Gaussian normal matrix.

01:07:28.000 --> 01:07:31.000
But I don't know.

01:07:31.000 --> 01:07:35.000
Part two, how do machine learning models represent their

01:07:35.000 --> 01:07:36.000
thoughts?

01:07:36.000 --> 01:07:39.000
Now we're taught in machine learning 101 that neural networks

01:07:39.000 --> 01:07:44.000
represent hypotheses which live on a geometric domain and

01:07:44.000 --> 01:07:47.000
the objective priors learn to generalize symmetries which

01:07:47.000 --> 01:07:49.000
exist on the underlying geometric domain.

01:07:49.000 --> 01:07:53.000
And you're talking about them representing a space of

01:07:53.000 --> 01:07:55.000
algorithms, which we're going to explore.

01:07:55.000 --> 01:07:58.000
Now, one thing that I wanted to touch on is that they learn

01:07:58.000 --> 01:08:01.000
the mapping to extensional attributes, not intentional

01:08:01.000 --> 01:08:04.000
attributes, intentions, but with an S.

01:08:04.000 --> 01:08:07.000
And we'll come back to what I mean by that in a second.

01:08:07.000 --> 01:08:10.000
But I think it's quite popular for people to think of neural

01:08:10.000 --> 01:08:13.000
networks principally as a kind of hash table.

01:08:14.000 --> 01:08:16.000
Or locality sensitive hash table.

01:08:16.000 --> 01:08:19.000
And the generalization part comes from the representation

01:08:19.000 --> 01:08:24.000
mapping function, which is on this embedded Hilbert space,

01:08:24.000 --> 01:08:28.000
which is the vector space of the attributes, which then

01:08:28.000 --> 01:08:31.000
resolves a pointer to a static location on the underlying

01:08:31.000 --> 01:08:32.000
geometric domain.

01:08:32.000 --> 01:08:35.000
Now this can mimic an algorithm, especially when the

01:08:35.000 --> 01:08:39.000
inductive prior itself is increasingly algorithmic like

01:08:39.000 --> 01:08:42.000
a graph neural network, for example, which behaves in a

01:08:42.000 --> 01:08:46.000
very similar way to a prototypical dynamic programming

01:08:46.000 --> 01:08:47.000
algorithm.

01:08:47.000 --> 01:08:49.000
There's some great work actually on algorithmic reasoning

01:08:49.000 --> 01:08:52.000
by Petr Felichkovich, one of your colleagues now at

01:08:52.000 --> 01:08:53.000
DeepMind.

01:08:53.000 --> 01:08:58.000
But he showed in his algorithmic reasoning work that

01:08:58.000 --> 01:09:02.000
transformers can't perform certain graph algorithms.

01:09:02.000 --> 01:09:05.000
I think he gave Dykstra as an example and he said it's

01:09:05.000 --> 01:09:08.000
because there's this aggregation function in a

01:09:08.000 --> 01:09:11.000
transformer, which isn't in a GNN.

01:09:11.000 --> 01:09:14.000
So I just wondered if you could kind of like compare and

01:09:14.000 --> 01:09:19.000
contrast whether or not neural networks are performing

01:09:19.000 --> 01:09:22.000
algorithmic generalization and the differences between

01:09:22.000 --> 01:09:25.000
let's say GNNs and transformers.

01:09:25.000 --> 01:09:30.000
Yeah, so I'm not very familiar with GNNs, so I'll probably

01:09:30.000 --> 01:09:33.000
avoid commenting on GNNs versus transformers, so a fear of

01:09:33.000 --> 01:09:35.000
embarrassing myself.

01:09:35.000 --> 01:09:39.000
In terms of the underlying thing.

01:09:39.000 --> 01:09:43.000
So I definitely think we have some pretty clear evidence at

01:09:43.000 --> 01:09:47.000
this point that models are doing some genuine algorithms.

01:09:47.000 --> 01:09:50.000
I don't know if I think my modular addition thing is a

01:09:50.000 --> 01:09:52.000
pretty clear proof of concept of this.

01:09:52.000 --> 01:09:58.000
Yeah, so one thing worth stressing is that I generally

01:09:58.000 --> 01:10:01.000
think of models as having linear representations, more

01:10:01.000 --> 01:10:04.000
than geometric representations.

01:10:04.000 --> 01:10:09.000
So I think of an input to a model as having many

01:10:09.000 --> 01:10:13.000
different possible features where features are kind of a

01:10:13.000 --> 01:10:19.000
property of the input in an intentional sense, but which is

01:10:19.000 --> 01:10:22.000
kind of a fuzzy and garbage definition.

01:10:22.000 --> 01:10:25.000
So I prefer the existential definition of like an example

01:10:25.000 --> 01:10:29.000
of a feature is like this bit of an image contains a curve

01:10:29.000 --> 01:10:33.000
or this bit of an image corresponds to a car window or

01:10:33.000 --> 01:10:37.000
this is the final token in Eiffel Tower or this corresponds

01:10:37.000 --> 01:10:41.000
to a list variable and Python with at least four elements

01:10:41.000 --> 01:10:44.000
and all kinds of stuff like that.

01:10:44.000 --> 01:10:50.000
And well, I know this this scene is shaded blue because

01:10:50.000 --> 01:10:53.000
someone put the wrong filter on the camera.

01:10:53.000 --> 01:10:58.000
And yeah, I generally think of models as representing

01:10:58.000 --> 01:11:01.000
features as linear directions in space.

01:11:01.000 --> 01:11:06.000
And each input is a linear combination of these directions.

01:11:06.000 --> 01:11:11.000
And this is kind of the classic words to vet framing, like

01:11:11.000 --> 01:11:17.000
the king minus man equals queen minus woman thing where you

01:11:17.000 --> 01:11:19.000
can kind of think of this as there being a gender direction

01:11:19.000 --> 01:11:21.000
and there being a royalty direction.

01:11:21.000 --> 01:11:25.000
And these are like the right units of analysis rather than

01:11:25.000 --> 01:11:29.000
king, queen, man, women being the right units of analysis.

01:11:29.000 --> 01:11:33.000
But where each of these is made up out of these underlying

01:11:33.000 --> 01:11:35.000
linear representations.

01:11:35.000 --> 01:11:40.000
And this is a fairly different perspective to the geometric

01:11:40.000 --> 01:11:42.000
where are things in a manifold?

01:11:42.000 --> 01:11:44.000
How close are they together in Euclidean space?

01:11:44.000 --> 01:11:49.000
Because that's all kind of a global statement about how close

01:11:49.000 --> 01:11:54.000
two things are where you're comparing all possible features

01:11:54.000 --> 01:11:59.000
while I don't know the Eiffel Tower and the Colosseum are

01:11:59.000 --> 01:12:02.000
close together in some conceptual space because they're both

01:12:02.000 --> 01:12:05.000
European landmarks, but they're also very different because

01:12:05.000 --> 01:12:08.000
France and Italy are fairly different countries in some

01:12:08.000 --> 01:12:09.000
sense.

01:12:09.000 --> 01:12:13.000
And maybe they're different on a bunch of other features or

01:12:13.000 --> 01:12:14.000
one of them is two words.

01:12:14.000 --> 01:12:17.000
The other is one word, which really matters in some ways.

01:12:17.000 --> 01:12:23.000
And Euclidean distance and geometry is it's a gloomy

01:12:23.000 --> 01:12:25.000
it's a global summary statistic.

01:12:25.000 --> 01:12:28.000
And all summary statistics light you.

01:12:28.000 --> 01:12:30.000
There's another motto of mine.

01:12:30.000 --> 01:12:34.000
But in particular, global ones I'm very skeptical of.

01:12:34.000 --> 01:12:41.000
And yeah, in general, this how what is the structure of a model

01:12:41.000 --> 01:12:42.000
representations?

01:12:42.000 --> 01:12:44.000
I think it's like a really important question.

01:12:44.000 --> 01:12:49.000
And in particular, models are such high dimensional objects

01:12:49.000 --> 01:12:54.000
that you really want to be careful to distinguish between the

01:12:54.000 --> 01:13:01.000
two separate things of sorry models are such high dimensional

01:13:01.000 --> 01:13:05.000
objects that it's basically impossible to understand GBD

01:13:05.000 --> 01:13:07.000
three is a 200 billion dimensional vector.

01:13:07.000 --> 01:13:12.000
You need to be breaking it down into units of analysis that

01:13:12.000 --> 01:13:16.000
can vary independently and independently meaningful.

01:13:16.000 --> 01:13:21.000
And the linear representation hypothesis is like a pretty

01:13:21.000 --> 01:13:25.000
load bearing part of how I think about this stuff because it is

01:13:25.000 --> 01:13:31.000
so because it allows you to break things down.

01:13:31.000 --> 01:13:34.000
And it seems to be a true fact about how models do things.

01:13:34.000 --> 01:13:37.000
Though again, we don't have that much data because we never

01:13:37.000 --> 01:13:38.000
have enough data.

01:13:38.000 --> 01:13:39.000
It's really sad.

01:13:40.000 --> 01:13:45.000
And yeah.

01:13:45.000 --> 01:13:47.000
Well, that's contrast a little bit.

01:13:47.000 --> 01:13:51.000
So this linear representation hypothesis, this idea that the

01:13:51.000 --> 01:13:55.000
models break down inputs into many independently varying features

01:13:55.000 --> 01:13:59.000
and store them as directions in space, much like word to veck.

01:13:59.000 --> 01:14:04.000
And the the go fi people, I mean, like photo and pollution,

01:14:04.000 --> 01:14:08.000
they they they brought out this famous critique of connectionism

01:14:08.000 --> 01:14:09.000
in 1988.

01:14:09.000 --> 01:14:12.000
And their main argument was systematicity.

01:14:12.000 --> 01:14:15.000
And they were talking about intention versus extension.

01:14:15.000 --> 01:14:18.000
And it might just be worth defining what I mean by that.

01:14:18.000 --> 01:14:23.000
So if I said the teacher of Socrates was Plato, the extension

01:14:23.000 --> 01:14:24.000
is Plato.

01:14:24.000 --> 01:14:27.000
The intention is everything.

01:14:27.000 --> 01:14:28.000
It's the teacher.

01:14:28.000 --> 01:14:29.000
It's Socrates.

01:14:29.000 --> 01:14:32.000
You know, if I said four plus five equals nine, nine is the

01:14:32.000 --> 01:14:35.000
extension for and plus and five is the intention.

01:14:35.000 --> 01:14:37.000
So they were saying something very simple.

01:14:37.000 --> 01:14:40.000
They said in a neural network, the intentional attributes get

01:14:40.000 --> 01:14:41.000
discarded.

01:14:41.000 --> 01:14:44.000
And that's why the network don't support what they call

01:14:44.000 --> 01:14:45.000
compositionality.

01:14:45.000 --> 01:14:49.000
Now, compositionality is actually quite an abstract term

01:14:49.000 --> 01:14:53.000
because using vector algebra in these analogical reasoning tasks

01:14:53.000 --> 01:14:54.000
that you were just talking about.

01:14:54.000 --> 01:14:55.000
So king and queen and so on.

01:14:55.000 --> 01:14:57.000
That's a form of compositionality.

01:14:57.000 --> 01:15:01.000
But they would say it's a poor cousin of compositionality because

01:15:01.000 --> 01:15:07.000
it's only using, you know, the representation is in a vector

01:15:07.000 --> 01:15:08.000
space.

01:15:08.000 --> 01:15:11.000
And in a vector space, you only have very basic primitive

01:15:11.000 --> 01:15:12.000
transformations.

01:15:12.000 --> 01:15:15.000
So you wouldn't be able to, I mean, for example, when you're

01:15:15.000 --> 01:15:18.000
talking about Paris earlier, you wouldn't do the kind of

01:15:18.000 --> 01:15:20.000
analogical reasoning they were talking about being able to

01:15:20.000 --> 01:15:23.000
downstream, say, were they in Paris?

01:15:23.000 --> 01:15:25.000
Is Paris in Europe?

01:15:25.000 --> 01:15:29.000
Of course, it does happen in this linear representation theory.

01:15:29.000 --> 01:15:33.000
But it happens in a very different way.

01:15:33.000 --> 01:15:34.000
Hmm.

01:15:34.000 --> 01:15:39.000
So I guess I'm not sure I fully followed that.

01:15:39.000 --> 01:15:43.000
I mean, this might be a cheap gotcha, but a fact about

01:15:43.000 --> 01:15:48.000
transformers is there's, they have this central object called

01:15:48.000 --> 01:15:53.000
the residual stream, which I know in standard framing to be

01:15:53.000 --> 01:15:56.000
thought of as the thing that lives in the skip connections.

01:15:56.000 --> 01:16:00.000
But not even as like the key thing about a transform where

01:16:00.000 --> 01:16:03.000
each layer reads its input from the residual stream and adds

01:16:03.000 --> 01:16:05.000
its output back to the residual stream.

01:16:05.000 --> 01:16:08.000
And the residual stream is kind of this shared bandwidth and

01:16:08.000 --> 01:16:09.000
memory.

01:16:09.000 --> 01:16:13.000
And this means that nothing's ever thrown away unless the

01:16:13.000 --> 01:16:17.000
model explicitly is trying to do that, or is just applying

01:16:17.000 --> 01:16:19.000
some gradual decay over time.

01:16:19.000 --> 01:16:22.000
So, you know, if you've got an MLP layer that's saying I've got

01:16:22.000 --> 01:16:25.000
four, I've got plus, I've got five, and I want to compute nine,

01:16:25.000 --> 01:16:26.000
and plus is still there.

01:16:26.000 --> 01:16:29.000
I don't know if this actually engage with your points and

01:16:29.000 --> 01:16:33.000
like, I don't know if this matters, but it's true.

01:16:33.000 --> 01:16:36.000
Yeah, what you're saying is true, but I think the point is

01:16:36.000 --> 01:16:41.000
that those primitives are not actually representable in a

01:16:41.000 --> 01:16:42.000
neural network.

01:16:42.000 --> 01:16:45.000
So you're saying with this residual stream, all of the

01:16:45.000 --> 01:16:48.000
extensions that came previously also get passed up.

01:16:48.000 --> 01:16:52.000
So in a later layer, you can refer to an extension.

01:16:52.000 --> 01:16:55.000
So the, basically the answer of a computation that happened

01:16:55.000 --> 01:16:59.000
upstream, but what you can't refer to are the intentional

01:16:59.000 --> 01:17:02.000
attributes of that computation upstream.

01:17:02.000 --> 01:17:04.000
Why not?

01:17:04.000 --> 01:17:06.000
Like four is an input.

01:17:06.000 --> 01:17:08.000
So you can refer to four, because you could think of

01:17:08.000 --> 01:17:11.000
reading the input as a computation.

01:17:11.000 --> 01:17:13.000
Plus is another thing you read.

01:17:13.000 --> 01:17:15.000
Five is another thing you read.

01:17:15.000 --> 01:17:18.000
Like what is a thing that is not an output of a computation

01:17:18.000 --> 01:17:20.000
within this framework?

01:17:20.000 --> 01:17:23.000
I might have to get back to you on that.

01:17:23.000 --> 01:17:27.000
Where's Keith Duggar when you need him?

01:17:27.000 --> 01:17:32.000
What would be a good example of that?

01:17:32.000 --> 01:17:37.000
I mean, I guess it's about symbol manipulation as well.

01:17:37.000 --> 01:17:41.000
So these things could actually be symbolic operations which

01:17:41.000 --> 01:17:44.000
can be composed and reused later.

01:17:44.000 --> 01:17:47.000
And you would appreciate that a neural network is only ever

01:17:47.000 --> 01:17:49.000
passing values.

01:17:49.000 --> 01:17:53.000
So for example, if it did something which you could represent

01:17:53.000 --> 01:17:56.000
with a symbolic operation, if you wanted to use that again,

01:17:56.000 --> 01:18:01.000
I mean in an MLP, the reason why we use a CNN is because we

01:18:01.000 --> 01:18:04.000
want to represent the same thing in different places.

01:18:04.000 --> 01:18:06.000
And an MLP would have to learn it.

01:18:06.000 --> 01:18:08.000
It doesn't support translational equivalence.

01:18:08.000 --> 01:18:11.000
So it would have to learn the same thing a million times.

01:18:11.000 --> 01:18:14.000
And it's the same thing with this symbolic compositional

01:18:14.000 --> 01:18:18.000
generalization that if it actually had this symbolic

01:18:18.000 --> 01:18:22.000
representation which it used once, it could use it everywhere.

01:18:22.000 --> 01:18:25.000
But now it has to relearn it everywhere.

01:18:25.000 --> 01:18:27.000
Right.

01:18:27.000 --> 01:18:33.000
Like you could, if the model wants to know that Paris

01:18:33.000 --> 01:18:37.000
is the capital of France, it can spend some parameters on that.

01:18:37.000 --> 01:18:39.000
And for every other capital it needs to separately spend

01:18:39.000 --> 01:18:43.000
parameters and it can't just have a general map country to

01:18:43.000 --> 01:18:45.000
have a capital operation.

01:18:45.000 --> 01:18:47.000
Yeah, that's exactly right.

01:18:47.000 --> 01:18:49.000
Let's use a simple example.

01:18:49.000 --> 01:18:54.000
So we use an MLP image classifier and I put a tennis ball in

01:18:54.000 --> 01:18:57.000
and it's in the bottom left of the visual field.

01:18:57.000 --> 01:19:02.000
And then I put it in the top right and nothing it's learned

01:19:02.000 --> 01:19:05.000
from the bottom left will be used.

01:19:05.000 --> 01:19:09.000
So it just feels like we're wasting the representational

01:19:09.000 --> 01:19:11.000
capacity just doing the same thing again and again.

01:19:11.000 --> 01:19:16.000
And in a transformer, the only reason it does have that

01:19:16.000 --> 01:19:20.000
recognition, that that's a equivalence in respect of the

01:19:20.000 --> 01:19:23.000
position of a pattern is because of the transformer

01:19:23.000 --> 01:19:25.000
inductive prior, presumably.

01:19:25.000 --> 01:19:27.000
Yes.

01:19:27.000 --> 01:19:31.000
So it uses the same parameters at each position in the input

01:19:31.000 --> 01:19:32.000
sequence.

01:19:32.000 --> 01:19:35.000
It should be able to do bottom left and top right properly,

01:19:35.000 --> 01:19:39.000
though it does not necessarily have things like rotation built

01:19:39.000 --> 01:19:40.000
in.

01:19:40.000 --> 01:19:42.000
I don't know.

01:19:42.000 --> 01:19:45.000
I feel like machine learning is full of these people who have

01:19:45.000 --> 01:19:47.000
all kinds of theoretical arguments.

01:19:47.000 --> 01:19:49.000
And then they're like, this should be efficient.

01:19:49.000 --> 01:19:50.000
This should not work.

01:19:50.000 --> 01:19:53.000
And then GPT-4 lobs at them.

01:19:53.000 --> 01:19:55.000
And I don't know.

01:19:55.000 --> 01:20:00.000
No theory is interesting and isolation unless it models

01:20:00.000 --> 01:20:02.000
reality well.

01:20:02.000 --> 01:20:04.000
And I don't know.

01:20:04.000 --> 01:20:06.000
I haven't really engaged with this theory in the same way I

01:20:06.000 --> 01:20:09.000
haven't engaged with most deep learning theory, because it

01:20:09.000 --> 01:20:12.000
just doesn't seem to meet my bar of does this make real

01:20:12.000 --> 01:20:14.000
predictions about models?

01:20:14.000 --> 01:20:17.000
The maximal update parameterization paper from Greg

01:20:17.000 --> 01:20:20.000
Yang was actually a recent contradiction to this.

01:20:20.000 --> 01:20:21.000
Right.

01:20:21.000 --> 01:20:24.000
Of really interesting theory that makes real predictions

01:20:24.000 --> 01:20:27.000
about models that bear out and get you zero shot hyper parameter

01:20:27.000 --> 01:20:28.000
to transfer.

01:20:28.000 --> 01:20:31.000
But like most things just don't do that.

01:20:31.000 --> 01:20:33.000
Very interesting.

01:20:33.000 --> 01:20:34.000
Okay.

01:20:34.000 --> 01:20:35.000
Okay.

01:20:35.000 --> 01:20:38.000
Well, I think now is a beautiful opportunity to move over to

01:20:38.000 --> 01:20:39.000
Othello.

01:20:39.000 --> 01:20:42.000
Now, there was a recent paper called do large language models

01:20:42.000 --> 01:20:45.000
learn world models, or are they just surface statistics by

01:20:45.000 --> 01:20:46.000
Kenneth Lee?

01:20:46.000 --> 01:20:49.000
And he said that the recent increase in model and data size

01:20:49.000 --> 01:20:52.000
has brought about qualitatively new behaviors such as writing

01:20:52.000 --> 01:20:55.000
code or solving logic puzzles.

01:20:55.000 --> 01:20:58.000
Now, he asked the question, yeah, how do these models achieve

01:20:58.000 --> 01:20:59.000
this kind of performance?

01:20:59.000 --> 01:21:01.000
Do they merely memorize training data?

01:21:01.000 --> 01:21:04.000
Well, are they picking up the rules of English grammar and

01:21:04.000 --> 01:21:06.000
grammar and the syntax of the sea language?

01:21:06.000 --> 01:21:09.000
For example, are they building something akin to an internal

01:21:09.000 --> 01:21:13.000
world model, an understandable model of the process producing

01:21:13.000 --> 01:21:14.000
the sequences?

01:21:14.000 --> 01:21:17.000
And he said that some researchers argue that this is fundamentally

01:21:17.000 --> 01:21:21.000
impossible for models trained with guess the next word to learn

01:21:21.000 --> 01:21:25.000
the language, meanings of language, and their performances is

01:21:25.000 --> 01:21:28.000
merely surface statistics, you know, which is to say a long list

01:21:28.000 --> 01:21:32.000
of correlations that do not reflect a causal model of the

01:21:32.000 --> 01:21:34.000
process generating the sequence.

01:21:34.000 --> 01:21:38.000
Now, you said, Neil, that a major source of excitement about

01:21:38.000 --> 01:21:41.000
the original Othello paper was that it showed that predicting

01:21:41.000 --> 01:21:45.000
the next word spontaneously learned the underlying structure

01:21:45.000 --> 01:21:46.000
generating its data.

01:21:46.000 --> 01:21:49.000
And you said that the obvious inference is that a large

01:21:49.000 --> 01:21:52.000
language model trained to predict the next token may

01:21:52.000 --> 01:21:54.000
spontaneously model the world.

01:21:54.000 --> 01:21:55.000
What do you think?

01:21:55.000 --> 01:21:57.000
Uh, yes.

01:21:57.000 --> 01:22:01.000
So I should clarify that paragraph was me modeling why other

01:22:01.000 --> 01:22:03.000
people are excited about paper.

01:22:03.000 --> 01:22:04.000
Okay.

01:22:04.000 --> 01:22:06.000
But whatever, I can roll with this question.

01:22:06.000 --> 01:22:10.000
So and maybe bring in your less wrong piece as well.

01:22:10.000 --> 01:22:11.000
Yeah.

01:22:11.000 --> 01:22:12.000
Yes.

01:22:12.000 --> 01:22:15.000
So the, yeah, I thought careless paper was super interesting.

01:22:15.000 --> 01:22:20.000
The exact setup was they train.

01:22:20.000 --> 01:22:24.000
So Othello is this chess and go like board game.

01:22:24.000 --> 01:22:29.000
They took a data set of random legal moves in Othello.

01:22:29.000 --> 01:22:34.000
They trained a model to predict the next move given a bunch

01:22:34.000 --> 01:22:36.000
of these transcripts.

01:22:36.000 --> 01:22:39.000
And then they probed the model and found that it had learned a

01:22:39.000 --> 01:22:42.000
model of the board state, despite only ever being told to

01:22:42.000 --> 01:22:44.000
predict the next move.

01:22:44.000 --> 01:22:48.000
And so the way I would define world model is that there's

01:22:48.000 --> 01:22:53.000
some latent variables that generate the training data.

01:22:53.000 --> 01:22:58.000
Um, in this case, what the state of the board is, um, these

01:22:58.000 --> 01:23:02.000
change over time, like over the sequence, but at least for a

01:23:02.000 --> 01:23:07.000
transform, which has a sequence and the model kind of has an

01:23:07.000 --> 01:23:09.000
internal representation of this at each point.

01:23:09.000 --> 01:23:11.000
And they showed that you can probe for this.

01:23:11.000 --> 01:23:16.000
And they showed that you can causally intervene on this and the

01:23:16.000 --> 01:23:19.000
model will make legal moves in the new board, even if the

01:23:19.000 --> 01:23:21.000
board status impossible to reach.

01:23:21.000 --> 01:23:23.000
Point of order.

01:23:23.000 --> 01:23:25.000
Can you explain what you mean by probe?

01:23:25.000 --> 01:23:27.000
Yes.

01:23:27.000 --> 01:23:31.000
So probing is this like old family of interoperability

01:23:31.000 --> 01:23:32.000
techniques.

01:23:32.000 --> 01:23:37.000
The idea is you think a model has represented something like you

01:23:37.000 --> 01:23:42.000
give it a picture and you tell it as a class by the image and

01:23:42.000 --> 01:23:45.000
you want to see if it's figured out that the picture is of a

01:23:45.000 --> 01:23:47.000
red thing versus a blue thing, even though this isn't an

01:23:47.000 --> 01:23:49.000
explicit part of the output.

01:23:49.000 --> 01:23:55.000
You take some neuron or layer or just any internal vector of

01:23:55.000 --> 01:24:01.000
the model and you train some classifier to map that to like

01:24:01.000 --> 01:24:05.000
red or blue and you do something like a logistic regression

01:24:05.000 --> 01:24:08.000
to see if you can extract whether it's red or blue from them.

01:24:08.000 --> 01:24:12.000
And, uh, there's also interesting enough about probing, but

01:24:12.000 --> 01:24:14.000
I should probably finish explaining the Othello paper first

01:24:14.000 --> 01:24:16.000
before I get into that tangent.

01:24:16.000 --> 01:24:20.000
So, yeah, the like reason people are really excited about this

01:24:20.000 --> 01:24:24.000
paper was recently an oral eichle and generally got a lot of

01:24:24.000 --> 01:24:28.000
hype was that it was just you train something predict the next

01:24:28.000 --> 01:24:32.000
token and it forms this rich emergent model of the world.

01:24:32.000 --> 01:24:35.000
And forming a model of the world is actually incredibly

01:24:35.000 --> 01:24:36.000
expensive.

01:24:36.000 --> 01:24:41.000
They like each cell of the 64 cell Othello board has three

01:24:41.000 --> 01:24:45.000
possible states, three to the 64, it's quite a lot of information

01:24:45.000 --> 01:24:48.000
to represent, but the model did it.

01:24:48.000 --> 01:24:51.000
And lots of people were like, oh, clearly language models have

01:24:51.000 --> 01:24:53.000
walls.

01:24:53.000 --> 01:24:59.000
My personal interpretation of all this is that language models

01:24:59.000 --> 01:25:00.000
predict the next token.

01:25:00.000 --> 01:25:04.000
They learn effective algorithms for doing this within the

01:25:04.000 --> 01:25:08.000
constraints of what is natural to represent within transformer

01:25:08.000 --> 01:25:09.000
layers.

01:25:09.000 --> 01:25:15.000
And what this means is that if predicting the next token is

01:25:15.000 --> 01:25:19.000
made easier by having a model of the world of like, I don't

01:25:19.000 --> 01:25:23.000
know who the speaker is, this is a thing that will happen.

01:25:23.000 --> 01:25:27.000
And in some work led by Wes Gurney that we're going to talk

01:25:27.000 --> 01:25:30.000
about later, we found neurons that detected things like this

01:25:30.000 --> 01:25:33.000
Texas in French, this Texas Python code.

01:25:33.000 --> 01:25:37.000
And in some sense, this is like a particularly trivial

01:25:37.000 --> 01:25:38.000
model.

01:25:38.000 --> 01:25:45.000
And so, yeah, that's an interesting thing.

01:25:45.000 --> 01:25:48.000
In my opinion, it was kind of a priori obvious that language

01:25:48.000 --> 01:25:53.000
models would learn this if they could and needed to, and it was

01:25:53.000 --> 01:25:55.000
more efficient.

01:25:55.000 --> 01:25:58.000
And at the point forward, though, learning that something is

01:25:58.000 --> 01:26:01.000
French seems categorically different.

01:26:01.000 --> 01:26:06.000
Because when I read Kenneth's original piece, he showed what

01:26:06.000 --> 01:26:10.000
looked like a topological representation of the world.

01:26:10.000 --> 01:26:14.000
So how different state spaces were related to each other in a

01:26:14.000 --> 01:26:17.000
kind of network structure.

01:26:17.000 --> 01:26:20.000
Hmm.

01:26:20.000 --> 01:26:23.000
So.

01:26:23.000 --> 01:26:27.000
I wonder if you can remember how we produced that diagram.

01:26:27.000 --> 01:26:29.000
Yeah, so I'm starting to remember the details.

01:26:29.000 --> 01:26:31.000
I think it was something of the form.

01:26:31.000 --> 01:26:34.000
Look at how different cells are represented in the model and look

01:26:34.000 --> 01:26:37.000
at how close together the representations of different

01:26:37.000 --> 01:26:41.000
cells are, and the model has kind of got internal representations

01:26:41.000 --> 01:26:43.000
that are close together.

01:26:43.000 --> 01:26:46.000
I don't think this is fundamentally different from the

01:26:46.000 --> 01:26:48.000
king, queen, man, women thing.

01:26:48.000 --> 01:26:50.000
It's just that it's like learn some structural representations

01:26:50.000 --> 01:26:52.000
that's obviously kind of reasonable.

01:26:52.000 --> 01:26:53.000
Yeah.

01:26:53.000 --> 01:26:57.000
I wouldn't read too much into that.

01:26:57.000 --> 01:27:00.000
Models learn structural representations, I think, as old

01:27:00.000 --> 01:27:03.000
news at this point.

01:27:03.000 --> 01:27:07.000
But maybe another interesting angle is that one of the reasons

01:27:07.000 --> 01:27:13.000
why people like Gary Marcus, they say GPT is parasitic on the

01:27:13.000 --> 01:27:14.000
data.

01:27:14.000 --> 01:27:19.000
They say because they are empirical models, most of the meaning,

01:27:19.000 --> 01:27:23.000
most of the information is not in the data if we have to reason

01:27:23.000 --> 01:27:25.000
over explicit world models.

01:27:25.000 --> 01:27:28.000
So he thinks the reason a GPS is so good is because we've

01:27:28.000 --> 01:27:31.000
imputed this abstract world model.

01:27:31.000 --> 01:27:35.000
And similarly, when we play chess, we have an abstract world

01:27:35.000 --> 01:27:36.000
model.

01:27:36.000 --> 01:27:39.000
And he would argue that the information about that abstract

01:27:39.000 --> 01:27:41.000
world model doesn't exist in any data.

01:27:41.000 --> 01:27:44.000
So how do you go from the data to the model?

01:27:44.000 --> 01:27:47.000
And the Othello games seem to show that you could go from the

01:27:47.000 --> 01:27:49.000
data to the model.

01:27:49.000 --> 01:27:50.000
Yeah.

01:27:50.000 --> 01:27:53.000
I think that viewpoint is just obviously wrong.

01:27:53.000 --> 01:27:58.000
Like, you're trying to do a data prediction problem.

01:27:58.000 --> 01:28:03.000
A valid solution to that is to model the underlying world and

01:28:03.000 --> 01:28:05.000
use this to predict what comes next.

01:28:05.000 --> 01:28:07.000
There's clearly enough information in an information

01:28:07.000 --> 01:28:09.000
theoretic sense to do this.

01:28:09.000 --> 01:28:14.000
And the question is, is a model capable of doing that or not?

01:28:14.000 --> 01:28:16.000
And I don't know.

01:28:16.000 --> 01:28:19.000
I'm just like, you can't write poetry with statistical

01:28:19.000 --> 01:28:20.000
correlations.

01:28:20.000 --> 01:28:22.000
You need to be learning something.

01:28:22.000 --> 01:28:24.000
Maybe that's not a good example.

01:28:24.000 --> 01:28:26.000
I don't believe you can write like...

01:28:26.000 --> 01:28:28.000
Yes, you can.

01:28:28.000 --> 01:28:34.000
I don't believe you can produce, like, good answers to, like,

01:28:34.000 --> 01:28:37.000
difficult code forces problems.

01:28:37.000 --> 01:28:39.000
It's like, do good software engineering.

01:28:39.000 --> 01:28:44.000
There's purely a bundle of statistical correlations.

01:28:44.000 --> 01:28:46.000
Maybe I have too much respect for software engineers.

01:28:46.000 --> 01:28:48.000
I don't know.

01:28:48.000 --> 01:28:50.000
So where does it come from then?

01:28:50.000 --> 01:28:54.000
That flash of inspiration or that higher level...

01:28:54.000 --> 01:28:57.000
I guess the first question is, is there a jump?

01:28:57.000 --> 01:29:00.000
Is it actually grounded in the data it's trained on?

01:29:00.000 --> 01:29:04.000
Or is there some high-level reasoning?

01:29:04.000 --> 01:29:07.000
Where does that materialize from?

01:29:07.000 --> 01:29:11.000
So the way I think about it, there is just a space of

01:29:11.000 --> 01:29:14.000
possible algorithms that can be implemented in a

01:29:14.000 --> 01:29:16.000
lot of ways.

01:29:16.000 --> 01:29:22.000
And some of these look like a world model.

01:29:22.000 --> 01:29:25.000
And some of these look like a bunch of statistical correlations.

01:29:25.000 --> 01:29:28.000
And models are trading off lots of different resources.

01:29:28.000 --> 01:29:30.000
Like, how many dimensions do this consume?

01:29:30.000 --> 01:29:32.000
How much weight norm?

01:29:32.000 --> 01:29:34.000
How many parameters?

01:29:34.000 --> 01:29:38.000
How hard is this to get to and how weird and intricate?

01:29:38.000 --> 01:29:43.000
And models will choose the thing that gets the best loss

01:29:43.000 --> 01:29:46.000
that is most efficient on these dimensions,

01:29:46.000 --> 01:29:49.000
assuming they can reach it within the lost landscape.

01:29:49.000 --> 01:29:52.000
Well, I use choose in a very anthropomorphic sense.

01:29:52.000 --> 01:29:56.000
Like, Adam chooses good solutions.

01:29:56.000 --> 01:30:00.000
And I don't know, if you have a sufficiently hard task

01:30:00.000 --> 01:30:03.000
and forming a world model is like the right solution to it,

01:30:03.000 --> 01:30:05.000
models can do it.

01:30:05.000 --> 01:30:08.000
And I think people try to put all of these fancy philosophizing

01:30:08.000 --> 01:30:11.000
on it in a way that I just think is false.

01:30:12.000 --> 01:30:17.000
And I think the Othello paper is like a really beautiful,

01:30:17.000 --> 01:30:20.000
elegant setup that proves this.

01:30:20.000 --> 01:30:22.000
All right, can I move on to the plot twist?

01:30:22.000 --> 01:30:24.000
Does it prove it though?

01:30:24.000 --> 01:30:28.000
It's a very small contrived.

01:30:28.000 --> 01:30:34.000
It's a big jump to assume that that works on a large language model.

01:30:34.000 --> 01:30:37.000
So this is kind of the argument I'm making.

01:30:37.000 --> 01:30:40.000
I think there's the empirical question of do language models

01:30:40.000 --> 01:30:45.000
do this and the theoretical question of could they do this?

01:30:45.000 --> 01:30:48.000
And I'm saying I think the theoretical question is nonsense.

01:30:48.000 --> 01:30:51.000
And I think the Othello paper very conclusively proves

01:30:51.000 --> 01:30:54.000
the theoretical question is nonsense.

01:30:54.000 --> 01:30:58.000
They're just like, yeah, when given a bunch of data,

01:30:58.000 --> 01:31:02.000
you can infer the underlying world model behind it in theory.

01:31:02.000 --> 01:31:04.000
I would pitch back on that a tiny bit

01:31:04.000 --> 01:31:07.000
because it's very similar to AlphaGo,

01:31:07.000 --> 01:31:11.000
which proved that in a closed game,

01:31:11.000 --> 01:31:15.000
which is systematic and representable,

01:31:15.000 --> 01:31:18.000
with a finite, obviously exponentially large,

01:31:18.000 --> 01:31:20.000
but a finite number of board states,

01:31:20.000 --> 01:31:24.000
you can build an agent which performs really, really well.

01:31:24.000 --> 01:31:27.000
That seems to me completely different to something like language

01:31:27.000 --> 01:31:29.000
or acting in the real world that might not be systematic

01:31:29.000 --> 01:31:31.000
in the same way.

01:31:31.000 --> 01:31:33.000
We can debate whether or not it's,

01:31:33.000 --> 01:31:36.000
I think it's an infinite number of possible trajectories,

01:31:36.000 --> 01:31:39.000
just like language, an infinite number of possible sentences.

01:31:39.000 --> 01:31:41.000
Man, there's 50,000 to the power of a thousand

01:31:41.000 --> 01:31:43.000
possible input sequences.

01:31:43.000 --> 01:31:46.000
Sure is a finite number.

01:31:46.000 --> 01:31:48.000
You mean in Othello or?

01:31:48.000 --> 01:31:49.000
No, in GPT2.

01:31:49.000 --> 01:31:52.000
In GPT2, okay.

01:31:52.000 --> 01:31:56.000
Bounded context length, bounded vocab size, generally.

01:31:56.000 --> 01:31:58.000
Bastard.

01:31:58.000 --> 01:32:01.000
You're not going to write more than one quintillion characters

01:32:01.000 --> 01:32:03.000
probably.

01:32:03.000 --> 01:32:06.000
Yeah.

01:32:06.000 --> 01:32:11.000
Well, I guess it is still a big jump, though, isn't it?

01:32:11.000 --> 01:32:17.000
From, yes, empirically, it shows that in Othello, it works.

01:32:17.000 --> 01:32:21.000
Maybe we could debate whether or not it does or not,

01:32:21.000 --> 01:32:23.000
because there's always this question coming back to what we were saying before,

01:32:23.000 --> 01:32:27.000
whether it's learning something which is universal

01:32:27.000 --> 01:32:29.000
or something which is still brittle.

01:32:29.000 --> 01:32:34.000
So the way that we've evaluated it might lead us to conclude that it's universal,

01:32:34.000 --> 01:32:37.000
whereas, actually, it's brittle in ways that we don't understand.

01:32:37.000 --> 01:32:39.000
So that's a very real possibility.

01:32:39.000 --> 01:32:40.000
Yeah.

01:32:40.000 --> 01:32:43.000
I mean, everything's brittle in ways you don't understand.

01:32:43.000 --> 01:32:47.000
It's pretty rare that a model will do everything perfectly

01:32:47.000 --> 01:32:50.000
in a way that there are no adversarial examples.

01:32:50.000 --> 01:32:52.000
And this is one of the more interesting things that's come out

01:32:52.000 --> 01:32:54.000
of the adversarial examples literature to me.

01:32:54.000 --> 01:32:58.000
It's just like, oh, wow, there's so much stuff here.

01:32:58.000 --> 01:33:01.000
There's such a high-dimensional input space.

01:33:01.000 --> 01:33:04.000
There's all kinds of weird things the model wasn't prepared for.

01:33:04.000 --> 01:33:06.000
And I don't know.

01:33:06.000 --> 01:33:11.000
My interpretation of the Othello thing is the strong theoretical arguments are wrong.

01:33:11.000 --> 01:33:16.000
I separately believe that there are world models

01:33:16.000 --> 01:33:19.000
that could be implemented in a language model's ways.

01:33:19.000 --> 01:33:22.000
But I also disagree with the strong inference of the paper

01:33:22.000 --> 01:33:26.000
that this does happen in language models, or that we conclude it does,

01:33:26.000 --> 01:33:29.000
because world models are often really expensive.

01:33:29.000 --> 01:33:32.000
Like, in the Othello model, it's consuming 128 dimensions

01:33:32.000 --> 01:33:36.000
of its 512-dimensional residual stream for this world model.

01:33:36.000 --> 01:33:40.000
And the problem is set up so that the world model is insanely useful

01:33:40.000 --> 01:33:44.000
because weather removers' legal is purely determined by the board state,

01:33:44.000 --> 01:33:46.000
so it's worth the model's while to do this.

01:33:46.000 --> 01:33:48.000
But this is rarely the case in language.

01:33:48.000 --> 01:33:52.000
For example, there was all this buzz about Bing chat playing chess

01:33:52.000 --> 01:33:54.000
and making legal-ish moves.

01:33:54.000 --> 01:33:56.000
And I don't know, man.

01:33:56.000 --> 01:33:58.000
If you want to model a chessboard,

01:33:58.000 --> 01:34:02.000
you just look at the last piece that moves into a cell.

01:34:02.000 --> 01:34:04.000
That's the piece in that cell.

01:34:04.000 --> 01:34:06.000
You don't need an explicit representation.

01:34:06.000 --> 01:34:08.000
You can just use attention heads to do it.

01:34:08.000 --> 01:34:10.000
And there's all kinds of weird hacks,

01:34:10.000 --> 01:34:12.000
and, like, models will generally use the best hack.

01:34:12.000 --> 01:34:15.000
But probably it is worth the model's while

01:34:15.000 --> 01:34:18.000
to have some kind of an internal representation.

01:34:18.000 --> 01:34:22.000
Like, I'd bet that if you took a powerful code-playing model

01:34:22.000 --> 01:34:26.000
and probed it to understand the state of the key variables,

01:34:26.000 --> 01:34:28.000
it would probably have some representation.

01:34:28.000 --> 01:34:34.000
But it's moving on to the work I did building on the Othello paper.

01:34:34.000 --> 01:34:38.000
So one of the things that was really striking to me about the Othello work

01:34:38.000 --> 01:34:42.000
is, simultaneously, its results were strong enough

01:34:42.000 --> 01:34:46.000
that something here was clearly real.

01:34:46.000 --> 01:34:50.000
But they also used techniques that felt more powerful than were needed.

01:34:50.000 --> 01:34:54.000
Like, rather, they found that linear probes did not work.

01:34:54.000 --> 01:34:58.000
There weren't just directions in space corresponding to board states,

01:34:58.000 --> 01:35:02.000
but the non-linear probes, one hidden layer MLPs, did.

01:35:02.000 --> 01:35:06.000
And the key thing to be careful of when probing

01:35:06.000 --> 01:35:08.000
is, is your probe doing the computation,

01:35:08.000 --> 01:35:12.000
or does the model genuinely have this represented?

01:35:12.000 --> 01:35:16.000
And even with linear probes, this can be misleading.

01:35:16.000 --> 01:35:19.000
Like, if you're looking at how a model represents coloured shapes,

01:35:19.000 --> 01:35:22.000
and you find a red triangle direction,

01:35:22.000 --> 01:35:24.000
it could be that there's a red, green, or blue direction,

01:35:24.000 --> 01:35:27.000
and a triangle, square, or shape direction,

01:35:27.000 --> 01:35:29.000
and you're taking the red plus triangle,

01:35:29.000 --> 01:35:32.000
or it could be the case that each of the nine shapes

01:35:32.000 --> 01:35:34.000
have their own direction, you found the red triangle one.

01:35:34.000 --> 01:35:37.000
But non-linear probing is particularly sketchy.

01:35:37.000 --> 01:35:43.000
Like, in the extreme case, if you train GPD3 on the inputs to something,

01:35:43.000 --> 01:35:46.000
GPD3 can do a lot of stuff.

01:35:46.000 --> 01:35:49.000
If you train GPD3 on the activation side network,

01:35:49.000 --> 01:35:52.000
it can probably recover arbitrary functions of the input,

01:35:52.000 --> 01:35:55.000
assuming the information of the input hasn't been lost,

01:35:55.000 --> 01:35:59.000
which it shouldn't have, because there's a residual stream.

01:35:59.000 --> 01:36:04.000
And what I said is not quite true, but not important.

01:36:04.000 --> 01:36:10.000
And so I was, and their intervention technique

01:36:10.000 --> 01:36:14.000
was both got, like, various impressive results,

01:36:14.000 --> 01:36:17.000
but also involved doing a bunch of complex gradient-scent

01:36:17.000 --> 01:36:20.000
against their probe, and this all just seemed more powerful

01:36:20.000 --> 01:36:22.000
than was necessary.

01:36:22.000 --> 01:36:24.000
And so I did the...

01:36:24.000 --> 01:36:26.000
I challenged myself to do a weekend hackathon

01:36:26.000 --> 01:36:29.000
trying to figure out what was going on,

01:36:29.000 --> 01:36:32.000
and I poked around at some internal circuitry

01:36:32.000 --> 01:36:35.000
and tried to answer some very narrow questions about the model,

01:36:35.000 --> 01:36:38.000
and I found this one neuron that seemed to be looking

01:36:38.000 --> 01:36:41.000
for, like, three cell diagonal lines,

01:36:41.000 --> 01:36:44.000
where one was blank,

01:36:44.000 --> 01:36:47.000
the other was white, the next was black.

01:36:47.000 --> 01:36:51.000
But then sometimes it activated on blank, black, white.

01:36:51.000 --> 01:36:54.000
And it turned out that the general pattern

01:36:54.000 --> 01:36:58.000
was that it was blank, current players...

01:36:58.000 --> 01:37:02.000
sorry, blank opponents color and current players color.

01:37:02.000 --> 01:37:05.000
And this is a useful motif in Othello,

01:37:05.000 --> 01:37:07.000
because it makes them move legal.

01:37:07.000 --> 01:37:10.000
And when I saw this, I made the bold hypothesis

01:37:10.000 --> 01:37:13.000
maybe the model actually represents things

01:37:13.000 --> 01:37:16.000
in terms of whether a cell has the current player's color

01:37:16.000 --> 01:37:18.000
or the current opponent's color,

01:37:18.000 --> 01:37:20.000
which in hindsight is a lot more natural,

01:37:20.000 --> 01:37:23.000
because the model plays both black and white,

01:37:23.000 --> 01:37:26.000
and it's kind of symmetric from the perspective of the current player.

01:37:26.000 --> 01:37:28.000
And I trained a linear probe on this,

01:37:28.000 --> 01:37:31.000
and it just worked fabulously,

01:37:31.000 --> 01:37:33.000
and got near-perfect accuracy.

01:37:33.000 --> 01:37:36.000
And I tried linear representations on it,

01:37:36.000 --> 01:37:39.000
and I tried linear interventions, and it just worked.

01:37:39.000 --> 01:37:42.000
And I even feel really excited about this project

01:37:42.000 --> 01:37:44.000
for a bunch of reasons.

01:37:44.000 --> 01:37:46.000
First, while I did it on the weekend,

01:37:46.000 --> 01:37:48.000
I'm still very proud of this.

01:37:48.000 --> 01:37:53.000
Secondly, I think that it has vindicated

01:37:53.000 --> 01:37:56.000
some of my general suspicion of non-linear probing.

01:37:56.000 --> 01:37:58.000
Like, if you really understand a thing,

01:37:58.000 --> 01:38:01.000
you should be able to get a linear probe to work.

01:38:01.000 --> 01:38:04.000
And kind of more deeply, as we discussed,

01:38:04.000 --> 01:38:08.000
there's this words-to-vex style linear representation

01:38:08.000 --> 01:38:12.000
of a hypothesis about models that features corresponded directions.

01:38:12.000 --> 01:38:15.000
The Athero work seemed like pretty strong evidence against.

01:38:15.000 --> 01:38:19.000
They had chordal interventions showing that the board state was there,

01:38:19.000 --> 01:38:23.000
but actually non-linear probes did not work.

01:38:23.000 --> 01:38:26.000
It seemed like they found some non-linear representation.

01:38:26.000 --> 01:38:30.000
And my and Chris Ola's hypothesis seeing this

01:38:30.000 --> 01:38:34.000
was that there was a linear representation hiding beneath.

01:38:34.000 --> 01:38:37.000
Martin Boddenberg, one of the authors of the paper,

01:38:37.000 --> 01:38:40.000
had the hypothesis that it was, like,

01:38:40.000 --> 01:38:42.000
an actual non-linear representation,

01:38:42.000 --> 01:38:44.000
and this was evidence against the hypothesis.

01:38:44.000 --> 01:38:47.000
And this kind of formed natural experiment

01:38:47.000 --> 01:38:49.000
where the hypothesis could have been falsified,

01:38:49.000 --> 01:38:54.000
but my work showed there was a real non-linear representation,

01:38:54.000 --> 01:38:57.000
and thus that it had predictive power.

01:38:57.000 --> 01:38:59.000
And so many of our frameworks for mech and turp

01:38:59.000 --> 01:39:02.000
are just these loose things based on a bunch of data,

01:39:02.000 --> 01:39:06.000
but not fully rigorous or fully conclusively shown.

01:39:06.000 --> 01:39:08.000
And so natural experiments like this

01:39:08.000 --> 01:39:11.000
feel like some of the best data we have.

01:39:11.000 --> 01:39:13.000
On this linear representation, though,

01:39:13.000 --> 01:39:15.000
I don't know if you've heard of the spline theory

01:39:15.000 --> 01:39:18.000
of neural networks by Randall Ballastriero.

01:39:18.000 --> 01:39:21.000
And without going into too much detail,

01:39:21.000 --> 01:39:24.000
it's quite a discrete view of MLPs in particular

01:39:24.000 --> 01:39:28.000
that the relus essentially get activated

01:39:28.000 --> 01:39:32.000
in an input-sensitive way to carve out these polyhedra

01:39:32.000 --> 01:39:36.000
in the ambient space, and essentially an input

01:39:36.000 --> 01:39:39.000
will be mapped into one of these cells in the ambient space,

01:39:39.000 --> 01:39:42.000
and then there's a kind of discreteness to it,

01:39:42.000 --> 01:39:45.000
because if you just perturb the input

01:39:45.000 --> 01:39:48.000
and you move outside of one of these polyhedra,

01:39:48.000 --> 01:39:51.000
then the model will, if it's a classifier,

01:39:51.000 --> 01:39:53.000
classify something different.

01:39:53.000 --> 01:39:55.000
But I guess I want to understand,

01:39:55.000 --> 01:39:58.000
with this representation theory, if features are directions,

01:39:58.000 --> 01:40:01.000
does that imply there's a kind of continuity

01:40:02.000 --> 01:40:05.000
because the network will learn to spread out

01:40:05.000 --> 01:40:07.000
those representations in the best possible way,

01:40:07.000 --> 01:40:10.000
but it won't necessarily be a way which is semantically useful,

01:40:10.000 --> 01:40:12.000
like in Word2Vec,

01:40:12.000 --> 01:40:14.000
stop and go are very close to each other,

01:40:14.000 --> 01:40:16.000
and they shouldn't be.

01:40:16.000 --> 01:40:19.000
And at what point does stop become go?

01:40:19.000 --> 01:40:23.000
So do you see there being boundaries in these directions?

01:40:24.000 --> 01:40:28.000
So I think this is, again, my point that I think

01:40:28.000 --> 01:40:30.000
of linear representations as being

01:40:30.000 --> 01:40:33.000
importantly different from geometric representations,

01:40:33.000 --> 01:40:36.000
like stop should be close to go,

01:40:36.000 --> 01:40:38.000
because in many contexts,

01:40:38.000 --> 01:40:44.000
they are like a kind of changing of state term,

01:40:44.000 --> 01:40:46.000
and it's used in similar contexts

01:40:46.000 --> 01:40:48.000
and has similar grammatical meaning,

01:40:48.000 --> 01:40:50.000
but then on this single semantic thing,

01:40:50.000 --> 01:40:52.000
they're quite different.

01:40:52.000 --> 01:40:53.000
And the natural way to represent this

01:40:53.000 --> 01:40:55.000
is have them be close together in Euclidean space,

01:40:56.000 --> 01:40:59.000
but have some crucial like negation dimension

01:40:59.000 --> 01:41:01.000
where they're different.

01:41:01.000 --> 01:41:05.000
And the contact and like ultimately neural networks

01:41:05.000 --> 01:41:07.000
are not geometric objects,

01:41:07.000 --> 01:41:09.000
they are made of linear algebra.

01:41:09.000 --> 01:41:12.000
Every neuron's input is just project

01:41:12.000 --> 01:41:15.000
the residual stream onto some vector,

01:41:15.000 --> 01:41:19.000
and this involves just selecting some set of directions

01:41:19.000 --> 01:41:21.000
and taking a linear combination

01:41:21.000 --> 01:41:24.000
of the feature corresponding to each of those.

01:41:25.000 --> 01:41:28.000
And this is just the natural way for a model

01:41:28.000 --> 01:41:30.000
to represent things in my opinion.

01:41:31.000 --> 01:41:34.000
Okay, well, I think this will in a second lead us

01:41:34.000 --> 01:41:36.000
on very nicely to superposition,

01:41:36.000 --> 01:41:38.000
which is that we don't actually think

01:41:38.000 --> 01:41:42.000
of there being one direction necessarily.

01:41:42.000 --> 01:41:44.000
Just to close this little piece,

01:41:44.000 --> 01:41:46.000
now you said in your less wrong article

01:41:46.000 --> 01:41:50.000
that orthello GPT is likely over parameterized

01:41:50.000 --> 01:41:52.000
for good performance on this particular task

01:41:52.000 --> 01:41:54.000
while language models are under parameterized.

01:41:54.000 --> 01:41:56.000
And of course, we have the ground truth to this task,

01:41:56.000 --> 01:41:58.000
which makes it very, very easy.

01:41:58.000 --> 01:41:59.000
So much easier to interpret.

01:41:59.000 --> 01:42:02.000
100%, but you did conclude saying that

01:42:02.000 --> 01:42:04.000
this is further evidence that neural networks

01:42:04.000 --> 01:42:07.000
are genuinely understandable and interpretable,

01:42:07.000 --> 01:42:09.000
and probing on the face of it seems like

01:42:09.000 --> 01:42:11.000
a very exciting approach to understand

01:42:11.000 --> 01:42:13.000
what the models really represent,

01:42:13.000 --> 01:42:16.000
caveat, mentor, conceptual issues.

01:42:16.000 --> 01:42:19.000
So let's move on to this superposition,

01:42:20.000 --> 01:42:22.000
also known as poly semanticity,

01:42:22.000 --> 01:42:24.000
which is an absolutely beautiful,

01:42:24.000 --> 01:42:26.000
while you're shaking your head a little bit,

01:42:26.000 --> 01:42:28.000
maybe you start with that.

01:42:28.000 --> 01:42:31.000
Yeah, so there's...

01:42:31.000 --> 01:42:34.000
All right, so what's the narrative here?

01:42:34.000 --> 01:42:36.000
So fundamentally,

01:42:36.000 --> 01:42:40.000
we are trying to engage with models

01:42:40.000 --> 01:42:42.000
as these high dimensional objects

01:42:42.000 --> 01:42:45.000
in kind of this conceptual way.

01:42:45.000 --> 01:42:47.000
So we need to be able to decompose them

01:42:47.000 --> 01:42:50.000
because of the curse of dimensionality.

01:42:50.000 --> 01:42:53.000
And we think models correspond to features

01:42:53.000 --> 01:42:55.000
and the features correspond to directions.

01:42:55.000 --> 01:42:58.000
And the hope in the early field

01:42:58.000 --> 01:43:01.000
was that features would correspond to neurons.

01:43:01.000 --> 01:43:04.000
And even if you believe features correspond

01:43:04.000 --> 01:43:06.000
to orthogonal directions,

01:43:06.000 --> 01:43:08.000
the same thing they correspond to neurons

01:43:08.000 --> 01:43:10.000
is like a pretty strong one,

01:43:10.000 --> 01:43:13.000
because there's no reason to align with the neuron basis.

01:43:13.000 --> 01:43:15.000
The reason this isn't a crazy belief

01:43:15.000 --> 01:43:18.000
is that models are incentivized to represent features

01:43:18.000 --> 01:43:21.000
in ways that can vary independently from each other.

01:43:21.000 --> 01:43:26.000
And because relus and jelus act element-wise,

01:43:26.000 --> 01:43:28.000
if there's a feature per neuron,

01:43:28.000 --> 01:43:30.000
they can vary independently.

01:43:30.000 --> 01:43:32.000
Well, if there's multiple features in the same neuron,

01:43:32.000 --> 01:43:34.000
I don't know, if there's a relu,

01:43:34.000 --> 01:43:36.000
the second feature could change

01:43:36.000 --> 01:43:38.000
so the relu goes from on to off

01:43:38.000 --> 01:43:40.000
in a way that changes how the other feature is expressed

01:43:40.000 --> 01:43:42.000
in the dance room network.

01:43:42.000 --> 01:43:44.000
And this is like a beautiful theoretical argument.

01:43:44.000 --> 01:43:48.000
Sadly, it's bullshit because of this phenomena of police mantisity.

01:43:48.000 --> 01:43:53.000
Police mantisity is a behavioral observation of networks.

01:43:53.000 --> 01:43:55.000
But when we look at neurons

01:43:55.000 --> 01:43:57.000
and look at things that activate them,

01:43:57.000 --> 01:44:00.000
they're often activated by seemingly unrelated things,

01:44:00.000 --> 01:44:05.000
like the urs in the word strangers

01:44:05.000 --> 01:44:07.000
and capital letters are proper nouns

01:44:07.000 --> 01:44:09.000
and news articles about football.

01:44:09.000 --> 01:44:12.000
It's a particularly fun neural I found one time in a language model.

01:44:12.000 --> 01:44:17.000
And police mantisity is a purely behavioral thing.

01:44:17.000 --> 01:44:19.000
We're just saying this neuron activates

01:44:19.000 --> 01:44:22.000
for a bunch of seemingly unrelated stuff.

01:44:22.000 --> 01:44:26.000
And it's possible that actually we're missing

01:44:26.000 --> 01:44:29.000
some galaxy brain abstraction where all of this is related,

01:44:29.000 --> 01:44:31.000
but my guess is that this is just,

01:44:31.000 --> 01:44:34.000
the model is not aligning features with neurons.

01:44:34.000 --> 01:44:38.000
And one explanation of this

01:44:38.000 --> 01:44:42.000
is you've just got this thing called a distributed representation

01:44:42.000 --> 01:44:46.000
where a feature is made of the linear combination of different neurons.

01:44:46.000 --> 01:44:49.000
But it is kind of rotated from the neuron basis.

01:44:49.000 --> 01:44:53.000
And this argument that neurons can vary independently

01:44:53.000 --> 01:44:56.000
is a reason to think you wouldn't see this.

01:44:56.000 --> 01:45:01.000
Where this hypothesis is just that there's still

01:45:01.000 --> 01:45:05.000
n things when there's n neurons, but they're rotated.

01:45:05.000 --> 01:45:07.000
But then there's this stronger hypothesis

01:45:07.000 --> 01:45:12.000
that tries to explain this called the superposition hypothesis.

01:45:12.000 --> 01:45:15.000
And here the idea is,

01:45:15.000 --> 01:45:18.000
so if a model wants to be able to recover a feature perfectly,

01:45:18.000 --> 01:45:21.000
it must be orthogonal from all other features.

01:45:21.000 --> 01:45:24.000
But if it wants to mostly recover it,

01:45:24.000 --> 01:45:27.000
it suffices to have almost orthogonal vectors.

01:45:27.000 --> 01:45:31.000
And you can fit in many, many more almost orthogonal vectors

01:45:31.000 --> 01:45:34.000
into a space than orthogonal vectors.

01:45:34.000 --> 01:45:37.000
There's theorem saying that there are exponentially many

01:45:37.000 --> 01:45:39.000
in the number of dimensions.

01:45:39.000 --> 01:45:42.000
If you have 100 dimensional vectors,

01:45:42.000 --> 01:45:45.000
how many orthogonal directions are there?

01:45:45.000 --> 01:45:46.000
100.

01:45:46.000 --> 01:45:47.000
100?

01:45:47.000 --> 01:45:48.000
Yep.

01:45:48.000 --> 01:45:55.000
This is just the statement that you pick a vector.

01:45:55.000 --> 01:46:01.000
Sorry, there's 100 vectors that are all orthogonal of each other.

01:46:01.000 --> 01:46:03.000
Basic proof, you pick a vector,

01:46:03.000 --> 01:46:06.000
everything's orthogonal to that, that's a 9 to 9 dimensional space.

01:46:06.000 --> 01:46:09.000
You pick another vector, take everything orthogonal to that,

01:46:09.000 --> 01:46:11.000
that's a 98 dimensional space,

01:46:11.000 --> 01:46:15.000
and keep going until you get to nothing.

01:46:15.000 --> 01:46:19.000
Like if you picture a 2D space, you pick any direction,

01:46:19.000 --> 01:46:22.000
the only things orthogonal to that are a line,

01:46:22.000 --> 01:46:26.000
and so there's exactly two orthogonal things you can fit in.

01:46:26.000 --> 01:46:29.000
And there's like, you can rotate this and you can get

01:46:29.000 --> 01:46:32.000
many different sets of orthogonal things.

01:46:32.000 --> 01:46:35.000
Okay, I'm trying to articulate why this doesn't make sense to me.

01:46:35.000 --> 01:46:37.000
So maybe we should start with the curse of dimensionality,

01:46:37.000 --> 01:46:40.000
which is that the volume of the space increases exponentially

01:46:40.000 --> 01:46:42.000
with the number of dimensions.

01:46:42.000 --> 01:46:44.000
So we'll start with that.

01:46:44.000 --> 01:46:47.000
And the reason I'm thinking, maybe I'm wrong,

01:46:47.000 --> 01:46:50.000
but if you've got 100 dimensional vector,

01:46:50.000 --> 01:46:54.000
every combination of flipping one of the dimensions

01:46:54.000 --> 01:46:59.000
would produce a vector which is orthogonal to all of the other ones, would it not?

01:46:59.000 --> 01:47:00.000
No.

01:47:00.000 --> 01:47:03.000
So let's imagine you've got a vector of all ones.

01:47:03.000 --> 01:47:06.000
If you pick the first element and you negate it,

01:47:06.000 --> 01:47:08.000
so it's like minus one, then 99 ones.

01:47:08.000 --> 01:47:11.000
These are not orthogonal, the dot product is 98.

01:47:11.000 --> 01:47:13.000
Okay, okay, well that makes sense.

01:47:13.000 --> 01:47:17.000
So there's a linear number of orthogonal directions,

01:47:17.000 --> 01:47:21.000
and in which case we actually need to have these

01:47:21.000 --> 01:47:23.000
approximately orthogonal directions,

01:47:23.000 --> 01:47:26.000
because that actually does bias an exponential number.

01:47:26.000 --> 01:47:29.000
Yeah, and so the superpositional hypothesis is that the model represents

01:47:29.000 --> 01:47:33.000
more features than it has neurons, or that it has dimensions,

01:47:33.000 --> 01:47:38.000
and it somehow compresses them in as things that are almost orthogonal

01:47:38.000 --> 01:47:41.000
when it reads them out with a projection to get some interference,

01:47:41.000 --> 01:47:45.000
but it needs to balance the value of representing more features

01:47:45.000 --> 01:47:48.000
against the cost of interference.

01:47:48.000 --> 01:47:54.000
And Anthropic has this fantastic paper called toy models of superposition,

01:47:54.000 --> 01:47:58.000
which sadly was written off right left, so I can't claim any credit,

01:47:58.000 --> 01:48:05.000
and they basically build a toy model that exhibits superposition.

01:48:05.000 --> 01:48:10.000
The exact structure is you have n independent features,

01:48:10.000 --> 01:48:14.000
each of which is zero most of the time, it's not very prevalent,

01:48:14.000 --> 01:48:19.000
and there's a linear map from that to a small dimensional space,

01:48:19.000 --> 01:48:23.000
a linear map back up, and a non-linearity on the output,

01:48:23.000 --> 01:48:27.000
no non-linearity on the bottleneck in the middle,

01:48:27.000 --> 01:48:30.000
and you train it to be an autoencoder.

01:48:30.000 --> 01:48:34.000
Can it recover the features in the input?

01:48:34.000 --> 01:48:37.000
And because there's many more features than that are in the bottleneck,

01:48:37.000 --> 01:48:41.000
this tests whether the model can actually do this.

01:48:41.000 --> 01:48:44.000
And they find that it sometimes does, sometimes doesn't,

01:48:44.000 --> 01:48:48.000
and then do a lot of really in-depth investigation of how this varies.

01:48:48.000 --> 01:48:54.000
And yeah, returning to like, is superposition the same thing as police mantisity?

01:48:54.000 --> 01:48:58.000
I would say no, police mantisity is a behavioral thing.

01:48:58.000 --> 01:49:01.000
Distributed representations are also a behavioral thing,

01:49:01.000 --> 01:49:04.000
that it's like not aligned with the basis,

01:49:04.000 --> 01:49:09.000
and superposition is a mechanistic hypothesis for why both of these will happen,

01:49:09.000 --> 01:49:11.000
because if you have more features than neurons,

01:49:11.000 --> 01:49:15.000
obviously you're going to have multiple features per neuron,

01:49:15.000 --> 01:49:21.000
and probably you're going to have features that are not aligned with neurons.

01:49:21.000 --> 01:49:23.000
Okay, okay, very interesting.

01:49:23.000 --> 01:49:28.000
So why do you think that superposition is one of the biggest problems in mech and turp?

01:49:28.000 --> 01:49:34.000
Yeah, so it's this fundamental thing that we need to be able to decompose a model

01:49:34.000 --> 01:49:40.000
into individual units, and ideally these would be neurons,

01:49:40.000 --> 01:49:44.000
but they are not neurons, so we need to figure out what we're doing.

01:49:44.000 --> 01:49:51.000
And superposition, so in a world where we just have like n meaningful directions,

01:49:51.000 --> 01:49:56.000
but they weren't aligned with the standard basis, that'd be kind of doable.

01:49:56.000 --> 01:50:01.000
And indeed models often have like linear bottlenecks,

01:50:01.000 --> 01:50:05.000
like the residual stream, or the keys, queries, and values of an attention hit,

01:50:05.000 --> 01:50:12.000
that don't have element-wise linearities, and so have no intrinsically meaningful basis.

01:50:12.000 --> 01:50:15.000
The jargon here is privileged basis,

01:50:15.000 --> 01:50:22.000
but superposition means that you can't even say,

01:50:22.000 --> 01:50:24.000
this feature should be a fucking alter everything else,

01:50:24.000 --> 01:50:27.000
there's going to be a bunch of interference.

01:50:27.000 --> 01:50:33.000
There's not even a kind of mathematically,

01:50:33.000 --> 01:50:38.000
there's not even like a unique set of more than n directions,

01:50:38.000 --> 01:50:42.000
so describe some kind of vectors in n dimensional space.

01:50:42.000 --> 01:50:47.000
And I think that understanding how to extract features from superposition,

01:50:47.000 --> 01:50:52.000
given that superposition seems like a core part of how models do things,

01:50:52.000 --> 01:50:57.000
though we really do not have as much data here as I would like us to,

01:50:57.000 --> 01:51:02.000
understanding how to extract the right meaningful units seems really important.

01:51:02.000 --> 01:51:06.000
Okay, and I think we should clarify the difference between computational

01:51:06.000 --> 01:51:08.000
and representational superposition.

01:51:08.000 --> 01:51:15.000
Yeah, so there's kind of, so transformers are interesting,

01:51:15.000 --> 01:51:21.000
because they often have high dimensional activations

01:51:21.000 --> 01:51:25.000
that get linearly mapped to low dimensional things.

01:51:25.000 --> 01:51:28.000
So like in say GPT-2, in say GPT-2 small,

01:51:28.000 --> 01:51:32.000
the residual stream has 768 dimensions,

01:51:32.000 --> 01:51:37.000
while each MLP layer has 3000 neurons.

01:51:37.000 --> 01:51:41.000
And even if we think each neuron just produces a single feature,

01:51:41.000 --> 01:51:46.000
they need to get compressed down to the 768 dimensional residual stream.

01:51:46.000 --> 01:51:51.000
And we, or there's like 50,000 input tokens

01:51:51.000 --> 01:51:55.000
that get compressed to 768 dimensions.

01:51:55.000 --> 01:52:00.000
And this is called representational superposition.

01:52:00.000 --> 01:52:03.000
The model is representing, the model's already computed the features,

01:52:03.000 --> 01:52:06.000
but it's compressing them to some bottleneck space.

01:52:06.000 --> 01:52:10.000
And this is the main thing studied in the toy models of superposition paper.

01:52:10.000 --> 01:52:16.000
And what we found, sorry,

01:52:16.000 --> 01:52:20.000
there's a separate thing of computational superposition,

01:52:20.000 --> 01:52:25.000
which is when the model is doing, it's computing new features.

01:52:25.000 --> 01:52:32.000
This needs non-linearities, like attention head softmaxes or MLP jellies.

01:52:32.000 --> 01:52:38.000
And the non-linearities can compute new features as directions from the old ones,

01:52:38.000 --> 01:52:46.000
like if this, for example,

01:52:46.000 --> 01:52:51.000
if the top of an image is a car window and the bottom is a car wheel,

01:52:51.000 --> 01:52:54.000
then it's a car.

01:52:54.000 --> 01:52:59.000
Or if the current token is Johnson and the previous token was Boris,

01:52:59.000 --> 01:53:01.000
this is Boris Johnson.

01:53:01.000 --> 01:53:10.000
And this is all, how to phrase this?

01:53:10.000 --> 01:53:14.000
Yeah, this is computational superposition.

01:53:14.000 --> 01:53:18.000
If the model wants to compute more features than it has neurons.

01:53:18.000 --> 01:53:23.000
And this is much harder to reason about because linear algebra is nice

01:53:23.000 --> 01:53:25.000
and fairly well understood.

01:53:25.000 --> 01:53:28.000
Non-linearities, spoilers in the name, are not linear,

01:53:28.000 --> 01:53:30.000
and that's way more of a pain.

01:53:30.000 --> 01:53:36.000
And I think that we generally have a much less good handle on computational superposition,

01:53:36.000 --> 01:53:41.000
but also that this is way more of where the interestingness lies by my lights.

01:53:41.000 --> 01:53:45.000
And this is very briefly studied in the toy models of superposition paper,

01:53:45.000 --> 01:53:49.000
but I would love to see more work looking at this in practice

01:53:49.000 --> 01:53:52.000
and also looking at this in toy models.

01:53:52.000 --> 01:53:57.000
So zooming out a tiny bit, there's this paper from Anthropic.

01:53:57.000 --> 01:54:01.000
And the overall question to me is, does it actually exist?

01:54:01.000 --> 01:54:05.000
Now, presumably you're satisfied with the evidence that it does exist.

01:54:05.000 --> 01:54:10.000
And then there's the question of how do neural networks actually do it?

01:54:10.000 --> 01:54:13.000
And then there's the question of how does the neural network think,

01:54:13.000 --> 01:54:15.000
anthropomorphic language, I apologize,

01:54:15.000 --> 01:54:19.000
about the trade-off of more superposition, more features,

01:54:19.000 --> 01:54:24.000
but more interference versus less interference and more superposition?

01:54:24.000 --> 01:54:29.000
Yeah, so typing into the final question about interference,

01:54:29.000 --> 01:54:37.000
a useful conceptual distinction is that there's two different kinds of interference.

01:54:37.000 --> 01:54:42.000
So if you've got two features that share a dimension or share a neuron.

01:54:42.000 --> 01:54:45.000
Oh yeah, final note on representational superposition,

01:54:45.000 --> 01:54:48.000
because I don't think it should even be referred to in terms of neurons,

01:54:48.000 --> 01:54:52.000
because the individual-based elements don't have intrinsic meaning.

01:54:52.000 --> 01:54:56.000
Modular weird quirks like Adam.

01:54:56.000 --> 01:55:02.000
And it annoys me when people refer to the residual stream or key vectors as having neurons.

01:55:02.000 --> 01:55:06.000
There's no element-wise linearity, it's not privileged.

01:55:06.000 --> 01:55:10.000
Anyway, yeah, two types of interference.

01:55:10.000 --> 01:55:15.000
When A and B share a dimension, you can, yeah,

01:55:15.000 --> 01:55:19.000
let's say this dimension has both dice and poetry.

01:55:19.000 --> 01:55:24.000
You first off need to tell where if dice is there but poetry is not,

01:55:24.000 --> 01:55:28.000
you need to tell that dice is there and that poetry is not there.

01:55:28.000 --> 01:55:32.000
And if both what I call alternating interference,

01:55:32.000 --> 01:55:37.000
and then there's simultaneous interference where dice and poetry are both there.

01:55:37.000 --> 01:55:43.000
And you need to tell that both are there, but not that they're both there with like double strength.

01:55:43.000 --> 01:55:49.000
And as a general rule, models are good at dealing with things of the form.

01:55:49.000 --> 01:55:52.000
Notice when something is extreme along this dimension,

01:55:52.000 --> 01:55:59.000
but not notice when it is extreme along a dimension versus when it's not extreme.

01:55:59.000 --> 01:56:04.000
And alternating interference looks like that.

01:56:04.000 --> 01:56:10.000
Like if dice is straight up, poetry is at 45 degrees,

01:56:10.000 --> 01:56:17.000
both have less interference when the other one is active

01:56:17.000 --> 01:56:20.000
than when the main one is active along their direction.

01:56:20.000 --> 01:56:25.000
Okay, so you're saying interference from A and not B is far easier than A and B?

01:56:25.000 --> 01:56:26.000
Yes, exactly.

01:56:26.000 --> 01:56:34.000
And like a very rough heuristic as models will just not do simultaneous interference,

01:56:34.000 --> 01:56:37.000
but will do alternating interference.

01:56:37.000 --> 01:56:45.000
And they observed this in the toy models paper because they varied how often a feature was non-zero.

01:56:45.000 --> 01:56:49.000
What I think of as the prevalence of the feature that they called it spastic.

01:56:49.000 --> 01:56:54.000
And what they found is that when the feature was less prevalent,

01:56:54.000 --> 01:56:57.000
it was much more likely to be in superposition.

01:56:57.000 --> 01:57:02.000
And the way to think about this is if you have two independent features that both exist with probability P,

01:57:02.000 --> 01:57:06.000
the rate of simultaneous interference is P squared.

01:57:06.000 --> 01:57:08.000
The rate of alternating is P.

01:57:08.000 --> 01:57:16.000
And so and the worth of having the feature is also proportional to P because it occurs P of the time.

01:57:16.000 --> 01:57:21.000
So the railroad is the less of a big deal simultaneous interferences.

01:57:22.000 --> 01:57:25.000
And eventually the model uses superposition.

01:57:25.000 --> 01:57:31.000
There's also there was also an interesting bit looking at correlated features.

01:57:31.000 --> 01:57:37.000
So correlated features, even if they're not very prevalent, they have pretty high simultaneous interference.

01:57:37.000 --> 01:57:42.000
And models tend to put correlated features in to be orthogonal,

01:57:42.000 --> 01:57:46.000
but anti-correlated features, it's very happy for them to share a direction.

01:57:46.000 --> 01:57:53.000
One way you could think about this is if you've got, say, 25 features about romance novels and 25 features about Python code,

01:57:53.000 --> 01:58:01.000
you could have 25 directions that each contain a pair of features and then a single disambiguating neuron

01:58:01.000 --> 01:58:06.000
that is onto Python code off of romance novels that use to disambiguate the two.

01:58:06.000 --> 01:58:13.000
And yeah, may this be a good time to talk about the finding neurons in a haystack paper?

01:58:13.000 --> 01:58:15.000
Unless you've got more stuff on this.

01:58:15.000 --> 01:58:18.000
We'll get to that in just two shakes of a lamb's tail.

01:58:18.000 --> 01:58:23.000
But just before when I was reading through the paper, I had the mindset of sparsity.

01:58:23.000 --> 01:58:27.000
And you told me, Tim, don't say sparsity. It's prevalence.

01:58:27.000 --> 01:58:29.000
It means so many things.

01:58:29.000 --> 01:58:31.000
It's very overloaded.

01:58:31.000 --> 01:58:36.000
So, you know, so just quickly touch on the relationship between what is prevalence,

01:58:36.000 --> 01:58:39.000
the relationship between prevalence and superposition.

01:58:40.000 --> 01:58:43.000
And just before, well, actually, I've got a couple more questions,

01:58:43.000 --> 01:58:51.000
but would you also just mind playing devil's advocate and criticising the anthropic paper if you can?

01:58:51.000 --> 01:58:52.000
Sure.

01:58:52.000 --> 01:58:55.000
So I should be very clear.

01:58:55.000 --> 01:58:58.000
This is one of my top three all-time favourite and territoriality papers.

01:58:58.000 --> 01:59:00.000
It's a fantastic paper.

01:59:00.000 --> 01:59:01.000
That said.

01:59:01.000 --> 01:59:03.000
A bad word said about it.

01:59:03.000 --> 01:59:04.000
Oh, I have so much.

01:59:04.000 --> 01:59:06.000
I have bad words to say about every paper,

01:59:06.000 --> 01:59:10.000
especially the ones that I like because I've engaged with them in the most detail.

01:59:10.000 --> 01:59:14.000
So things which I think were misleading about this paper.

01:59:14.000 --> 01:59:20.000
The first is I think the representational versus computational superposition distinction is very important.

01:59:20.000 --> 01:59:24.000
I think computational is a fair bit more interesting.

01:59:24.000 --> 01:59:27.000
And while I think the authors knew the difference,

01:59:27.000 --> 01:59:32.000
I think a casual reader often came away not realising the difference,

01:59:32.000 --> 01:59:35.000
in particular that most of their results were about the residual stream,

01:59:35.000 --> 01:59:39.000
not about actual neurons and MLP layers.

01:59:39.000 --> 01:59:43.000
The second is a question of activation range.

01:59:43.000 --> 01:59:48.000
So they study features that vary uniformly between zero and one.

01:59:48.000 --> 01:59:52.000
And in practice, I think most features are binary.

01:59:52.000 --> 01:59:55.000
This is a car wheel or this is not a car wheel.

01:59:55.000 --> 01:59:59.000
This is Boris Johnson or this is not Boris Johnson.

01:59:59.000 --> 02:00:04.000
And interference is much worse when they can vary continuously.

02:00:04.000 --> 02:00:08.000
Because if A and B, if A is up, B is at 45 degrees,

02:00:08.000 --> 02:00:17.000
you can't distinguish B at strength one from A at strength 0.7-ish.

02:00:17.000 --> 02:00:20.000
And this is just kind of messy.

02:00:20.000 --> 02:00:22.000
But the binary is just much easier.

02:00:22.000 --> 02:00:27.000
And I think this is a source of confusion.

02:00:28.000 --> 02:00:35.000
I also think the two kinds of interference point was a bit understated.

02:00:35.000 --> 02:00:38.000
But more broadly, it's just a phenomenal paper.

02:00:38.000 --> 02:00:41.000
Oh, my other biggest beaver, they just didn't look in real models.

02:00:41.000 --> 02:00:43.000
And this wasn't the point of the paper.

02:00:43.000 --> 02:00:48.000
But we're doing so much theory crafting and filming conceptual frameworks,

02:00:48.000 --> 02:00:50.000
and we haven't really checked very hard

02:00:50.000 --> 02:00:54.000
whether this is why models actually have police mantisity.

02:00:54.000 --> 02:00:59.000
Wes Gurney, he's working out of MIT, and you've done a lot of work with him.

02:00:59.000 --> 02:01:03.000
So you and Wes, but Wes was the first author,

02:01:03.000 --> 02:01:06.000
wrote a paper called Finding Neurons in a Haystack,

02:01:06.000 --> 02:01:08.000
case studies with sparse probing,

02:01:08.000 --> 02:01:11.000
where you empirically studied superposition and language models

02:01:11.000 --> 02:01:15.000
and actually found that you get lots of superpositions in early layers

02:01:15.000 --> 02:01:18.000
for features like the security and social security.

02:01:18.000 --> 02:01:23.000
And fewer in middle layers for complex features like this text

02:01:23.000 --> 02:01:25.000
is French.

02:01:25.000 --> 02:01:29.000
And also you can bring in the importance of range activation as well.

02:01:29.000 --> 02:01:31.000
But can you frame up that paper?

02:01:31.000 --> 02:01:32.000
Yeah.

02:01:32.000 --> 02:01:36.000
So first off, this paper was led by Wes Gurney, one of my mentees,

02:01:36.000 --> 02:01:37.000
did a fantastic job.

02:01:37.000 --> 02:01:39.000
He deserves like 9% of the credit.

02:01:39.000 --> 02:01:40.000
Great job, Wes.

02:01:40.000 --> 02:01:43.000
I believe he listens to this podcast, so hi.

02:01:44.000 --> 02:01:50.000
And yeah, so the kind of high level pitch behind the paper

02:01:50.000 --> 02:01:54.000
was what we think superposition is happening.

02:01:54.000 --> 02:01:57.000
But like, nobody's really checked very hard.

02:01:57.000 --> 02:01:59.000
And there's like some results in the literature

02:01:59.000 --> 02:02:02.000
I've since come across in non-transformer models

02:02:02.000 --> 02:02:05.000
that demonstrate some amount of distributed representations.

02:02:05.000 --> 02:02:08.000
But what would it look like to check?

02:02:08.000 --> 02:02:11.000
And would it look like to do this in like a reasonably scalable

02:02:11.000 --> 02:02:13.000
and quantitative way?

02:02:13.000 --> 02:02:17.000
And the kind of sparse probing in the title

02:02:17.000 --> 02:02:21.000
is this technique Wes introduces for,

02:02:21.000 --> 02:02:25.000
if we think a feature is represented in MLP layer,

02:02:25.000 --> 02:02:28.000
we can train a linear classifier to extract it,

02:02:28.000 --> 02:02:30.000
a linear probe from that layer.

02:02:30.000 --> 02:02:34.000
But if we constrain the probe to use at most K neurons,

02:02:34.000 --> 02:02:37.000
very K and look at probe performance,

02:02:37.000 --> 02:02:39.000
this lets us distinguish between features that are represented

02:02:39.000 --> 02:02:42.000
with like a single neuron and features that are densely spread

02:02:42.000 --> 02:02:46.000
across all neurons with a lot of methodological neural

02:02:46.000 --> 02:02:50.000
answers about balanced data sets and avoiding overfitting

02:02:50.000 --> 02:02:52.000
and fun stuff like that.

02:02:52.000 --> 02:02:56.000
And most of the interesting bits of the paper,

02:02:56.000 --> 02:03:00.000
in my opinion, are the various case studies we do where,

02:03:00.000 --> 02:03:04.000
so probing fundamentally is like a kind of sketchy methodology

02:03:04.000 --> 02:03:07.000
because probing is correlational.

02:03:07.000 --> 02:03:10.000
Probing doesn't tell you whether a model uses something

02:03:10.000 --> 02:03:12.000
and it's so easy to trick yourself about

02:03:12.000 --> 02:03:15.000
whether you have the right representations.

02:03:15.000 --> 02:03:18.000
So we use it as a starting point and then dig more deeply

02:03:18.000 --> 02:03:20.000
into a few more interesting things.

02:03:20.000 --> 02:03:25.000
One particularly QK study is we looked into factual knowledge neurons,

02:03:25.000 --> 02:03:29.000
found something that seemed to represent this athlete plays hockey,

02:03:29.000 --> 02:03:32.000
but then actually turned out to be a Canada neuron,

02:03:32.000 --> 02:03:35.000
which continues to bring me joy.

02:03:35.000 --> 02:03:38.000
That activates with things like maple syrup and Canada.

02:03:38.000 --> 02:03:42.000
Got to love models learning national stereotypes, right?

02:03:42.000 --> 02:03:43.000
Oh, yes.

02:03:43.000 --> 02:03:51.000
Anyway, so there were two particularly exciting case studies.

02:03:51.000 --> 02:03:56.000
The first was looking in early layers at compound word detectors.

02:03:56.000 --> 02:04:02.000
So if you look at, say, the brain and its visual field,

02:04:02.000 --> 02:04:04.000
we have all these sensory neurons.

02:04:04.000 --> 02:04:07.000
We get raw input of light from the environment

02:04:07.000 --> 02:04:10.000
and it gets converted into stuff our brain can actually manipulate.

02:04:10.000 --> 02:04:14.000
Image models have Gabor filters that convert the pixels

02:04:14.000 --> 02:04:16.000
into something a bit more useful.

02:04:16.000 --> 02:04:19.000
What's the equivalent of language models?

02:04:19.000 --> 02:04:23.000
And it seems to be these things that we call detokenization neurons

02:04:23.000 --> 02:04:29.000
and circuitry, where often words are split into multiple tokens

02:04:29.000 --> 02:04:34.000
or you get compound word phrases like social security

02:04:34.000 --> 02:04:39.000
or Theresa May or Barack Obama and whatever.

02:04:39.000 --> 02:04:44.000
And it's often useful for a model to realize

02:04:44.000 --> 02:04:47.000
this is the second thing in a multi-token phrase,

02:04:47.000 --> 02:04:52.000
especially if it's like you need both things to know what's going on,

02:04:52.000 --> 02:04:54.000
like Michael Jordan.

02:04:54.000 --> 02:04:56.000
Michael has lots of Jordans.

02:04:56.000 --> 02:04:59.000
It's really important to tell both of them that they're there.

02:04:59.000 --> 02:05:04.000
And this is a clearly nonlinear thing because it's like a Boolean and.

02:05:04.000 --> 02:05:08.000
And so we did a lot of probing for different compound words.

02:05:08.000 --> 02:05:13.000
And we found that they were definitely not represented well by single neurons.

02:05:13.000 --> 02:05:17.000
We could find some neurons that were okay at detecting them,

02:05:17.000 --> 02:05:23.000
but there was a lot of interference and a lot of like false positives from other stuff.

02:05:23.000 --> 02:05:25.000
And when we dug into a bunch of these neurons,

02:05:25.000 --> 02:05:28.000
we found that they were incredibly polysemantic.

02:05:28.000 --> 02:05:32.000
They activated for many different compound words.

02:05:32.000 --> 02:05:36.000
And we showed that it was using superposition

02:05:36.000 --> 02:05:41.000
by observing that if you took say five social security detecting neurons

02:05:41.000 --> 02:05:43.000
and add together their activations,

02:05:43.000 --> 02:05:47.000
they go from okay detectors to a really good detector together.

02:05:47.000 --> 02:05:52.000
Because even though each is representing like hundreds of compound words,

02:05:52.000 --> 02:05:54.000
they're representing different compound words,

02:05:54.000 --> 02:05:57.000
which lets you encode these.

02:05:57.000 --> 02:06:03.000
And this, what we've shown here is that it's like distributed,

02:06:03.000 --> 02:06:07.000
that it's a linear combination of neurons.

02:06:07.000 --> 02:06:10.000
We still haven't shown it perfectly to my dissatisfaction.

02:06:10.000 --> 02:06:14.000
I think you really need to do things like ablate these linear combinations

02:06:14.000 --> 02:06:19.000
and see if this systematically damages the model's ability to think about social security, etc.

02:06:19.000 --> 02:06:21.000
But I'm pretty convinced at this point.

02:06:21.000 --> 02:06:26.000
And there's like a few properties of compound words

02:06:26.000 --> 02:06:30.000
that both make it easy to represent in superposition,

02:06:30.000 --> 02:06:35.000
that make me pretty okay making the jump that there's actual superposition.

02:06:35.000 --> 02:06:38.000
The first is just that there's tons of compound words.

02:06:38.000 --> 02:06:42.000
Each one is pretty rare, but each one is like non-trivial or useful.

02:06:42.000 --> 02:06:45.000
And clearly there are more compound words

02:06:45.000 --> 02:06:50.000
and there are like thousands of neurons in the MLP layer of this model.

02:06:50.000 --> 02:06:53.000
The model cares about representing and can represent,

02:06:53.000 --> 02:06:55.000
that we do not actually check.

02:06:55.000 --> 02:06:59.000
Because I could not convince Wes to accumulate a list of 2,000 compound words

02:06:59.000 --> 02:07:01.000
and pray for all of them.

02:07:01.000 --> 02:07:04.000
But I believe in my heart this is true.

02:07:04.000 --> 02:07:06.000
Could I have a point of order though?

02:07:06.000 --> 02:07:11.000
Because I've been reading quite a lot of stuff from linguists like Stephen Piantadosi.

02:07:11.000 --> 02:07:16.000
A lot of linguists are, some of them hate language models

02:07:16.000 --> 02:07:19.000
and some of them are well on board with it.

02:07:19.000 --> 02:07:22.000
Like Raphael Millier for example is a great example.

02:07:22.000 --> 02:07:24.000
I hate language models too, don't worry.

02:07:24.000 --> 02:07:26.000
Well, but the question is,

02:07:26.000 --> 02:07:28.000
because you're talking about compound words and stuff like that

02:07:28.000 --> 02:07:32.000
and you're still using the language of syntax

02:07:32.000 --> 02:07:36.000
and these language models, there's this distributional hypothesis.

02:07:36.000 --> 02:07:40.000
You know the meaning of a word by the company it keeps.

02:07:40.000 --> 02:07:43.000
But linguists and cognitive scientists kind of ditch that.

02:07:43.000 --> 02:07:46.000
I don't think they ever believed in the distributional hypothesis.

02:07:46.000 --> 02:07:48.000
They think about grounding.

02:07:48.000 --> 02:07:51.000
They think about grounding to things in the world

02:07:51.000 --> 02:07:55.000
and also inferential references as well

02:07:55.000 --> 02:07:59.000
which you can think of that as grounding to a model of the mind.

02:07:59.000 --> 02:08:01.000
And this brings us back to the Othello paper

02:08:01.000 --> 02:08:05.000
which is that they're not just learning simple kind of compound relationships

02:08:05.000 --> 02:08:07.000
between the world, between the words.

02:08:07.000 --> 02:08:09.000
They're learning a world model

02:08:09.000 --> 02:08:13.000
and they're doing something much more potentially

02:08:13.000 --> 02:08:15.000
than just predicting the next word.

02:08:15.000 --> 02:08:18.000
And Piantadosi argued that

02:08:18.000 --> 02:08:22.000
most of the representational capacity in language models

02:08:22.000 --> 02:08:24.000
are learning these semantics.

02:08:24.000 --> 02:08:27.000
They're learning relationships between things in the world model

02:08:27.000 --> 02:08:30.000
and the particular occurrence of the token.

02:08:30.000 --> 02:08:32.000
And this superposition idea is very interesting

02:08:32.000 --> 02:08:36.000
because it actually imbues the representational capacity

02:08:36.000 --> 02:08:38.000
in a language model to learn those mappings.

02:08:40.000 --> 02:08:42.000
Okay, so a couple of comments on that.

02:08:42.000 --> 02:08:47.000
The first is a generally useful way of thinking about models to me.

02:08:47.000 --> 02:08:52.000
Is as a the early layers devoted to sensory neurons

02:08:52.000 --> 02:08:57.000
converting the raw input into more useful concepts and representations.

02:08:57.000 --> 02:09:02.000
The actual processing throughout like all of the middle layers

02:09:02.000 --> 02:09:04.000
that actually does all the reasoning.

02:09:04.000 --> 02:09:06.000
And then motor neurons at the end

02:09:06.000 --> 02:09:09.000
that convert the reasoning to actual output tokens

02:09:09.000 --> 02:09:13.000
for like the format that the optimizer wants.

02:09:14.000 --> 02:09:19.000
And it feels like you're mostly talking about the like reasoning internally

02:09:19.000 --> 02:09:23.000
and the specific case study I'm referring to is on the sensory neurons.

02:09:23.000 --> 02:09:26.000
Well, like I'm not saying it just detects compound words

02:09:26.000 --> 02:09:29.000
but obviously that's the first thing it does.

02:09:29.000 --> 02:09:31.000
I don't know, it's so interesting.

02:09:31.000 --> 02:09:34.000
I don't mean to push back but in neuroscience

02:09:34.000 --> 02:09:37.000
the field was held back for decades by this idea

02:09:37.000 --> 02:09:40.000
of this kind of left to right to processing

02:09:40.000 --> 02:09:44.000
this hierarchical processing where you have these very, very simple concepts

02:09:44.000 --> 02:09:48.000
that become increasingly abstract with more processing.

02:09:48.000 --> 02:09:50.000
And then I think the field has moved away from that.

02:09:50.000 --> 02:09:52.000
It's far more messy and chaotic than that.

02:09:52.000 --> 02:09:55.000
Now with a neural network, it actually is hierarchical

02:09:55.000 --> 02:09:58.000
because the network is basically a DAG.

02:09:58.000 --> 02:10:01.000
So I suppose it is safe to make this assumption

02:10:01.000 --> 02:10:03.000
but could I just kind of question you on that?

02:10:03.000 --> 02:10:05.000
Is it safe to make that assumption?

02:10:05.000 --> 02:10:08.000
Is there increasing complexity in representation

02:10:08.000 --> 02:10:10.000
as you go from left to right?

02:10:10.000 --> 02:10:12.000
Oh, let's see.

02:10:12.000 --> 02:10:16.000
So yeah, I definitely, yeah.

02:10:16.000 --> 02:10:21.000
So clarification one, the network has this input sequence

02:10:21.000 --> 02:10:23.000
which I think was going from left to right

02:10:23.000 --> 02:10:26.000
and then there's a bunch of layers which I think it was going from like the bottom to the top.

02:10:26.000 --> 02:10:27.000
Yes.

02:10:27.000 --> 02:10:29.000
And you're referring to the bottom to top axis, right?

02:10:29.000 --> 02:10:33.000
Yeah, I'm sorry, I was using an MLP mindset when I asked that question.

02:10:33.000 --> 02:10:36.000
So as you say, in a transformer it's an autoregressive model

02:10:36.000 --> 02:10:41.000
and you have stacked attention layers with little MLPs on the end.

02:10:41.000 --> 02:10:44.000
So I guess the way I was actually meaning the question is,

02:10:44.000 --> 02:10:50.000
so complexity increases monotonically as you go up the stack of attention layers.

02:10:50.000 --> 02:10:52.000
Is that a fair assumption?

02:10:52.000 --> 02:10:54.000
Yep.

02:10:54.000 --> 02:10:56.000
Again, no one's really shown this properly.

02:10:56.000 --> 02:10:59.000
But I'm like, surely this is true.

02:10:59.000 --> 02:11:03.000
And there's been some work doing things like looking at neurons,

02:11:03.000 --> 02:11:05.000
looking at the text that activates them, looking for patterns

02:11:05.000 --> 02:11:08.000
and trying to understand what these represent.

02:11:08.000 --> 02:11:13.000
And it's generally looked like early ones are more about detokenization and syntax.

02:11:13.000 --> 02:11:16.000
Later ones are doing stuff that's interesting.

02:11:16.000 --> 02:11:19.000
Final ones are doing this like motor neuron behavior.

02:11:19.000 --> 02:11:24.000
But I also want to be very clear that networks are cursed.

02:11:24.000 --> 02:11:27.000
Networks do not fit into nice abstraction.

02:11:27.000 --> 02:11:31.000
I'm not saying the early layers are literally only doing detokenization.

02:11:31.000 --> 02:11:32.000
Yeah.

02:11:32.000 --> 02:11:35.000
But I believe we have shown it's part of what they're doing.

02:11:35.000 --> 02:11:38.000
And I speculate it is a large part of what they're doing.

02:11:38.000 --> 02:11:41.000
I'd be very surprised if it's all of what they're doing.

02:11:41.000 --> 02:11:45.000
Because I heard you on another podcast and you were just talking about the,

02:11:45.000 --> 02:11:48.000
I mean, I think the curse is the right way to describe it,

02:11:48.000 --> 02:11:52.000
which is that even when you make modifications,

02:11:52.000 --> 02:11:54.000
when you manipulate what's happening,

02:11:54.000 --> 02:11:57.000
the behavior will change in a very reflexive way.

02:11:57.000 --> 02:12:00.000
So you kind of, you delete one thing and then another neuron

02:12:00.000 --> 02:12:03.000
will take on the responsibility of the thing you just deleted.

02:12:03.000 --> 02:12:07.000
And so it's a little bit like manipulating financial markets.

02:12:07.000 --> 02:12:12.000
You've got almost like this weird collective diffuse intelligence

02:12:12.000 --> 02:12:17.000
where you make one modification and the whole thing changes in a very complex way.

02:12:17.000 --> 02:12:21.000
And similarly, I guess that's why I was intuitively questioning the assumption

02:12:21.000 --> 02:12:23.000
that you have a residual stream.

02:12:23.000 --> 02:12:26.000
So surely even at the very top of that attention stack,

02:12:26.000 --> 02:12:31.000
there must be primitive and complex operations going on in some weird mix.

02:12:31.000 --> 02:12:34.000
Since probably true.

02:12:34.000 --> 02:12:41.000
Generally, yeah, there's going to be some stuff you can just do with literally the embeddings.

02:12:41.000 --> 02:12:46.000
Some stuff that you need to wait a bit more before you can do anything useful with.

02:12:46.000 --> 02:12:51.000
Just like, no, if you got a sentence about Michael Jordan,

02:12:51.000 --> 02:12:54.000
I don't think you can use Michael Jordan in isolation.

02:12:54.000 --> 02:12:57.000
So you need to de-tokenize to Michael Jordan.

02:12:57.000 --> 02:13:00.000
But also, I don't know, if you've got Barack Obama,

02:13:00.000 --> 02:13:05.000
Obama and Barack both on their own pretty clearly imply it's going to be about Obama.

02:13:05.000 --> 02:13:10.000
And probably the model can start doing some processing in the early like layer zero.

02:13:10.000 --> 02:13:12.000
Does it want to?

02:13:12.000 --> 02:13:13.000
Somewhat unclear.

02:13:13.000 --> 02:13:16.000
It's going to depend a lot on the model's constraints and other circuitry

02:13:16.000 --> 02:13:20.000
and how much it's worth spending the premises then versus later.

02:13:20.000 --> 02:13:24.000
There's also some various things where, I don't know,

02:13:24.000 --> 02:13:31.000
model memory kind of decays over time because the residual stream's norm gets bigger.

02:13:31.000 --> 02:13:35.000
So early layer outputs become a smaller fraction of the overall thing.

02:13:35.000 --> 02:13:38.000
And layer norm sets the norm to be unit.

02:13:38.000 --> 02:13:40.000
So things kind of decay.

02:13:40.000 --> 02:13:44.000
And so if you compute a feature in the early in like layer zero,

02:13:44.000 --> 02:13:49.000
it can be harder to notice by like layer three than if it was computed in layer two.

02:13:49.000 --> 02:13:52.000
But these are all just kind of like mild nudges.

02:13:52.000 --> 02:13:56.000
And ultimately neural networks do what neural networks want, man.

02:13:56.000 --> 02:13:58.000
I know, I know.

02:13:58.000 --> 02:14:01.000
I just want to close the loop on something I said a little while ago about, you know,

02:14:01.000 --> 02:14:06.000
potentially large models use most of their representational capacity for, you know,

02:14:06.000 --> 02:14:08.000
learning these semantic relationships.

02:14:08.000 --> 02:14:13.000
And empirically, we found that, you know, there's some question recently actually about,

02:14:13.000 --> 02:14:16.000
do we actually need to have really, really large models?

02:14:16.000 --> 02:14:21.000
And for pure knowledge representation, the argument seems to be yes,

02:14:21.000 --> 02:14:26.000
but we can disentangle knowing from reasoning.

02:14:26.000 --> 02:14:28.000
And there's also this mimicry thing.

02:14:28.000 --> 02:14:32.000
So it's quite interesting that all of the, you know, like Facebook released their model

02:14:32.000 --> 02:14:36.000
and very, very quickly people fine-tuned it using the law, you know,

02:14:36.000 --> 02:14:39.000
the low-rank approximation fine-tuning method.

02:14:39.000 --> 02:14:42.000
And on all of the benchmarks, the model, I mean even open assistant,

02:14:42.000 --> 02:14:45.000
there's another great example, Yannick was sitting in your seat just a few weeks ago

02:14:45.000 --> 02:14:48.000
and we're saying that on many of the benchmarks, the model's working really well,

02:14:48.000 --> 02:14:50.000
but it's kind of not.

02:14:50.000 --> 02:14:54.000
It's kind of mimicry, like the big, large models that, you know,

02:14:54.000 --> 02:14:58.000
Meta and Google and DeepMind and all these people, they spend millions training these models

02:14:58.000 --> 02:15:02.000
and they have base knowledge about the world,

02:15:02.000 --> 02:15:07.000
which is not going to be, you know, replicated by fine-tuning, you know,

02:15:07.000 --> 02:15:09.000
like an open source model anytime soon.

02:15:09.000 --> 02:15:11.000
The knowledge is based.

02:15:11.000 --> 02:15:14.000
The knowledge is based.

02:15:14.000 --> 02:15:16.000
Yes, yes, yes, exactly.

02:15:16.000 --> 02:15:20.000
Well, okay, so that's very interesting.

02:15:20.000 --> 02:15:22.000
Let's just quickly talk about the OpenAI microscope,

02:15:22.000 --> 02:15:28.000
because this is, the OpenAI microscope is this beautiful app that OpenAI released in 2020.

02:15:28.000 --> 02:15:32.000
And you can go on there and you can click on any of the neurons

02:15:32.000 --> 02:15:35.000
in popular vision architectures at the time.

02:15:35.000 --> 02:15:38.000
So, you know, I think most of them are sort of like ImageNet, you know,

02:15:38.000 --> 02:15:41.000
things like AlexNet and God knows what else.

02:15:41.000 --> 02:15:46.000
And they solve this optimization problem where they generate an image

02:15:46.000 --> 02:15:51.000
using stochastic gradient descent that maximally activates a particular neuron,

02:15:51.000 --> 02:15:54.000
or I think even a layer, using something similar to DeepDream.

02:15:54.000 --> 02:15:57.000
And you can click on these neurons,

02:15:57.000 --> 02:16:02.000
and sometimes they are what we will call poly sort of monosemantic,

02:16:02.000 --> 02:16:04.000
which means it's just Canada.

02:16:04.000 --> 02:16:08.000
A lot of the time there's a couple of concepts in there that it's weirdly intelligible.

02:16:08.000 --> 02:16:12.000
You know, you might see, you know, like a playing card or an ace

02:16:12.000 --> 02:16:16.000
and a couple of like tangentially related concepts.

02:16:16.000 --> 02:16:19.000
And it always struck me as strange,

02:16:19.000 --> 02:16:23.000
because I imagine there's a long tail of semantic relationships.

02:16:23.000 --> 02:16:27.000
And I found it bizarre that there'd only be one or two in this visualization.

02:16:27.000 --> 02:16:31.000
And I had this intuition that the optimization algorithm is in some sense

02:16:31.000 --> 02:16:35.000
mode seeking rather than distribution matching,

02:16:35.000 --> 02:16:40.000
which is to say that it finds the two most or two or three or four

02:16:40.000 --> 02:16:44.000
most kind of salient semantic mappings,

02:16:44.000 --> 02:16:46.000
and they dominate what is visualized,

02:16:46.000 --> 02:16:51.000
and you're almost snipping off the long tail of the other semantic mappings.

02:16:51.000 --> 02:16:56.000
Yeah, so I think there's two things to disentangle here.

02:16:56.000 --> 02:17:02.000
The first is what is actually represented by the neuron in terms of ground truth.

02:17:02.000 --> 02:17:06.000
And the second is what our techniques show us.

02:17:06.000 --> 02:17:11.000
So the two techniques used in the open-air microscope

02:17:11.000 --> 02:17:15.000
are looking at the images the most activated neuron,

02:17:15.000 --> 02:17:20.000
and then this feature visualization technique where they produce a synthetic image

02:17:20.000 --> 02:17:22.000
that maximally activates it.

02:17:22.000 --> 02:17:26.000
And to me, this is, these are like,

02:17:26.000 --> 02:17:29.000
both of these can be misleading,

02:17:29.000 --> 02:17:32.000
because if the model activates the dice in poetry,

02:17:32.000 --> 02:17:36.000
but activates dice with strength five and poetry with strength four,

02:17:36.000 --> 02:17:39.000
then the optimally-bished activated will be dice,

02:17:39.000 --> 02:17:43.000
and the optimal, the data set examples will also be dice.

02:17:43.000 --> 02:17:45.000
But really, it'll be about poetry.

02:17:45.000 --> 02:17:47.000
And you want to get a lot more rigorous.

02:17:47.000 --> 02:17:50.000
You want to show true monosumanticity.

02:17:50.000 --> 02:17:53.000
One cute thing is spectrum plots.

02:17:53.000 --> 02:17:58.000
You take lots of example, data set examples across the full distribution,

02:17:58.000 --> 02:18:03.000
you have a histogram with the different groups for the different meanings,

02:18:03.000 --> 02:18:06.000
and then neural activation on the x-axis.

02:18:06.000 --> 02:18:10.000
We have this really cute plot in Wes' paper called the French Neuron,

02:18:10.000 --> 02:18:15.000
where all of the French text is on the right,

02:18:15.000 --> 02:18:17.000
all the non-French text is on the left,

02:18:17.000 --> 02:18:20.000
and the neuron is just very clearly distinguishing the two

02:18:20.000 --> 02:18:24.000
in a way that's much more convincing to me than things like Max Act examples.

02:18:25.000 --> 02:18:31.000
And I actually have a hobby project called Neuroscope at Neuroscopes.io,

02:18:31.000 --> 02:18:34.000
where you can see the Max activating text examples

02:18:34.000 --> 02:18:37.000
for every neuron and a bunch of language models,

02:18:37.000 --> 02:18:41.000
though opening I recently output this paper with one that is better,

02:18:41.000 --> 02:18:44.000
but only for GP2 XL.

02:18:44.000 --> 02:18:47.000
Anyway, not that I'm bitter or anything.

02:18:47.000 --> 02:18:48.000
Not so.

02:18:48.000 --> 02:18:55.000
And yeah, so yeah, there's the things can lie to and be illusory.

02:18:55.000 --> 02:19:00.000
There's this interesting paper called the Interruptibility Illusion for But,

02:19:00.000 --> 02:19:03.000
which investigated this specific phenomena,

02:19:03.000 --> 02:19:06.000
and in particular that if you take the data set examples

02:19:06.000 --> 02:19:10.000
over some narrow distribution, like Wikipedia or books,

02:19:10.000 --> 02:19:12.000
you can get pretty misleading things,

02:19:12.000 --> 02:19:15.000
though they only looked at residual stream basis elements

02:19:15.000 --> 02:19:18.000
rather than actual MLP neurons, I believe,

02:19:18.000 --> 02:19:21.000
which makes it a bit less compelling.

02:19:21.000 --> 02:19:22.000
Point of order as well.

02:19:22.000 --> 02:19:24.000
We've been saying residual stream quite a lot,

02:19:24.000 --> 02:19:27.000
and Microsoft introduced Resnet in 2015,

02:19:27.000 --> 02:19:31.000
which basically means that between all of the layers,

02:19:31.000 --> 02:19:34.000
the information is being passed up unadulterated,

02:19:34.000 --> 02:19:38.000
so the subsequent layer can choose to either essentially shortcut

02:19:38.000 --> 02:19:42.000
or ignore the previous layer or use some combination,

02:19:42.000 --> 02:19:45.000
and at the time they kind of said it was about the neural network

02:19:45.000 --> 02:19:49.000
being able to learn its own capacity in some sense,

02:19:49.000 --> 02:19:53.000
but could you just give us the way you think about these residual streams?

02:19:53.000 --> 02:19:56.000
Yeah, so I think the standard view of neural networks,

02:19:56.000 --> 02:20:03.000
there are just layers, and layer 5's output is layer 6's input, etc.

02:20:03.000 --> 02:20:09.000
Then people added Resnets, where layer 6's input is layer 5's output,

02:20:09.000 --> 02:20:12.000
plus layer 5's input with the skip connection,

02:20:12.000 --> 02:20:14.000
but I think people normally thought of them as like,

02:20:14.000 --> 02:20:16.000
ah, it's like a cute trick that makes the model better,

02:20:16.000 --> 02:20:19.000
but doesn't massively change my conceptual picture,

02:20:19.000 --> 02:20:24.000
and the framing that I believe was introduced in the mathematical framework,

02:20:24.000 --> 02:20:28.000
this anthropic paper led by Chris Ola, Nelson L. Harsh, and Catherine Olson

02:20:28.000 --> 02:20:31.000
that I was involved with, is actually,

02:20:31.000 --> 02:20:36.000
let's call the thing in the skip connection the residual stream

02:20:36.000 --> 02:20:38.000
and think of it as the central object,

02:20:38.000 --> 02:20:43.000
and draw our model so the residual stream is this big vertical thing,

02:20:43.000 --> 02:20:46.000
and each layer is like a small diversion to the side,

02:20:46.000 --> 02:20:48.000
rather than the other way around,

02:20:48.000 --> 02:20:53.000
and in practice, most circuits involve things skipping many layers,

02:20:53.000 --> 02:20:58.000
and each layer is better thought of as like an incremental update,

02:20:58.000 --> 02:21:02.000
and there's a bunch of earlier transformer interpretability papers

02:21:02.000 --> 02:21:05.000
that I think miss this conceptual point,

02:21:05.000 --> 02:21:08.000
like the interpretability delusion for but what I mentioned earlier,

02:21:08.000 --> 02:21:15.000
and study residual stream basis elements as like layer outputs or something.

02:21:15.000 --> 02:21:17.000
Yeah, I mean, in a sense, you know,

02:21:17.000 --> 02:21:21.000
we were talking about being able to reuse things that you've learned before

02:21:21.000 --> 02:21:23.000
and not having to learn them again,

02:21:23.000 --> 02:21:26.000
and I guess I think of it as a kind of translational equivalence

02:21:26.000 --> 02:21:29.000
in the layer regime,

02:21:29.000 --> 02:21:33.000
which is that you have a computation which is learned early on,

02:21:33.000 --> 02:21:36.000
and now it can just be composed into subsequent layers.

02:21:36.000 --> 02:21:40.000
It's like you've got a menu of computational functions

02:21:40.000 --> 02:21:44.000
that you can call on at any layer.

02:21:44.000 --> 02:21:46.000
Yeah, pretty much.

02:21:46.000 --> 02:21:50.000
I think of it as like the shared memory and shared bandwidth of the model.

02:21:50.000 --> 02:21:52.000
Yeah, almost like a memory bus.

02:21:52.000 --> 02:21:57.000
Yeah, and sometimes models will dedicate neurons like cleaning up the memory

02:21:57.000 --> 02:22:00.000
and deleting things that are no longer needed.

02:22:00.000 --> 02:22:04.000
Yeah, yeah, and is there any interference in that memory bus?

02:22:04.000 --> 02:22:06.000
So much.

02:22:06.000 --> 02:22:08.000
This is the thing of superposition, right?

02:22:08.000 --> 02:22:09.000
Yeah.

02:22:09.000 --> 02:22:11.000
Like the residual stream is doing everything.

02:22:11.000 --> 02:22:15.000
Like there's 50,000 input tokens start,

02:22:15.000 --> 02:22:19.000
and then 4x as many neurons as residual stream dimensions

02:22:19.000 --> 02:22:21.000
in every MLP layer,

02:22:21.000 --> 02:22:23.000
and attention heads moving everything around,

02:22:23.000 --> 02:22:25.000
and it's just a clusterfuck.

02:22:25.000 --> 02:22:28.000
What if you scale up the bandwidth of the bus?

02:22:29.000 --> 02:22:32.000
That's basically making the model bigger, right?

02:22:32.000 --> 02:22:34.000
Which we know makes models better.

02:22:34.000 --> 02:22:37.000
But I don't know, just thinking out loud,

02:22:37.000 --> 02:22:41.000
but what if you maintained the original dimensionality of the model

02:22:41.000 --> 02:22:44.000
but you deliberately upscaled the bus?

02:22:44.000 --> 02:22:48.000
So like you make the thing inside each layer smaller

02:22:48.000 --> 02:22:50.000
but make the residual stream bigger?

02:22:50.000 --> 02:22:52.000
Or just make everything the same as it is,

02:22:52.000 --> 02:22:56.000
but you just kind of like have a linear transformation on the bus

02:22:56.000 --> 02:22:58.000
and double the size of the bus.

02:22:58.000 --> 02:23:02.000
So I don't think that would work

02:23:02.000 --> 02:23:04.000
without increasing the number of parameters,

02:23:04.000 --> 02:23:08.000
because like if you...

02:23:08.000 --> 02:23:11.000
Because like the thing that matters is the smallest bottleneck.

02:23:11.000 --> 02:23:15.000
The output width of an MLP layer are like 4,000 by 1,000,

02:23:15.000 --> 02:23:18.000
and in order to make the 1,000 bigger,

02:23:18.000 --> 02:23:20.000
you need more parameters.

02:23:20.000 --> 02:23:24.000
And there's like all kinds of studies about the optimal hyperparameters

02:23:24.000 --> 02:23:26.000
and optimal ratios.

02:23:26.000 --> 02:23:30.000
My general heuristic is number of parameters are the main thing that matters.

02:23:30.000 --> 02:23:31.000
I don't know.

02:23:31.000 --> 02:23:34.000
I don't spend that much time thinking about how to make models better, to be honest.

02:23:34.000 --> 02:23:36.000
I just want to understand them, goddammit.

02:23:36.000 --> 02:23:39.000
Yeah, because it's one of those things that it might remove bottlenecks

02:23:39.000 --> 02:23:45.000
because essentially you're allowing the model to reuse things that it's learned previously.

02:23:45.000 --> 02:23:49.000
So now every single layer can specialize more than it did before

02:23:49.000 --> 02:23:52.000
and that might kind of like weirdly remove bottlenecks.

02:23:52.000 --> 02:23:54.000
Yeah.

02:23:54.000 --> 02:24:00.000
Yeah, the way I generally think about it is models are ensembles of shallow pods,

02:24:00.000 --> 02:24:03.000
which is this paper from like five years ago about Resnets.

02:24:03.000 --> 02:24:07.000
Like, deep-d2-small is 12 layers.

02:24:07.000 --> 02:24:12.000
Each layer includes an attention block and an attention bit in MLP,

02:24:12.000 --> 02:24:18.000
but it is not the case that most computation is 24 levels of composition deep.

02:24:18.000 --> 02:24:21.000
It is the case that most of them involve like, I don't know, four.

02:24:21.000 --> 02:24:27.000
And they're just intelligently choosing which four and remaking them in interesting ways.

02:24:27.000 --> 02:24:33.000
And sometimes different things will want to like get to different points

02:24:33.000 --> 02:24:38.000
and so it's useful to have many layers rather than a few.

02:24:38.000 --> 02:24:45.000
But also, I don't know, if you halve the residual stream width

02:24:45.000 --> 02:24:50.000
and give the model 4x as many layers, often performance is like about the same.

02:24:51.000 --> 02:24:55.000
Or like not that different because the number of parameters is unchanged.

02:24:55.000 --> 02:25:00.000
And this is just kind of a wild result about models that I think only really makes sense

02:25:00.000 --> 02:25:05.000
within this framework of it's like an ensemble of shallow pods

02:25:05.000 --> 02:25:09.000
and it's a trade-off between having more computation and having better memory bandwidth.

02:25:09.000 --> 02:25:11.000
Yeah, yeah, very interesting.

02:25:11.000 --> 02:25:16.000
Okay, I mean, just to close, superposition, it might not be a new idea.

02:25:16.000 --> 02:25:23.000
So Janik did a paper video about this paper called Supermasks in Superposition

02:25:23.000 --> 02:25:26.000
by Mitchell Wartsman back in 2020.

02:25:26.000 --> 02:25:30.000
And he was talking about supermass representing sparse subnetworks

02:25:30.000 --> 02:25:33.000
in respect of catastrophic forgetting and continual learning.

02:25:33.000 --> 02:25:36.000
But that was slightly different because that was an explicit model

02:25:36.000 --> 02:25:41.000
to perform masking, create subnetworks and to model, you know,

02:25:41.000 --> 02:25:44.000
like basically a sparsity aware algorithm.

02:25:44.000 --> 02:25:48.000
But he was still using a lot of the same language like interference and so on

02:25:48.000 --> 02:25:51.000
and thinking about superpositions of subnetworks.

02:25:51.000 --> 02:25:55.000
And I guess the difference is like just as we were talking about with these inductive priors

02:25:55.000 --> 02:25:59.000
like transformers and CNNs, the models already do this stuff

02:25:59.000 --> 02:26:04.000
without us having to explicitly code it, which I think is the interesting discovery.

02:26:04.000 --> 02:26:09.000
Yeah, yeah, one update I've made from Wes's work is that

02:26:09.000 --> 02:26:14.000
detokenization is probably like a pretty big fraction of what the early layers do.

02:26:14.000 --> 02:26:19.000
And it's just really easy to represent compound words in superposition

02:26:19.000 --> 02:26:23.000
because it's a very binary, it's either there or not there.

02:26:23.000 --> 02:26:26.000
So alternating difference is easy to deal with.

02:26:26.000 --> 02:26:31.000
They're mutually exclusive, so there's no simultaneous interference.

02:26:31.000 --> 02:26:36.000
Like you cannot have Boris Johnson and Theresa May co-occur.

02:26:36.000 --> 02:26:40.000
And there's just like so many of them.

02:26:40.000 --> 02:26:45.000
One fact about language models that people who haven't played around them may not appreciate

02:26:45.000 --> 02:26:48.000
is their inputs are these things called tokens.

02:26:48.000 --> 02:26:54.000
And tokenizers are fucked because they're trained in this bizarre Byzantine way

02:26:54.000 --> 02:26:59.000
that means that often the rarer words will get broken up into many tokens.

02:26:59.000 --> 02:27:00.000
Yes.

02:27:00.000 --> 02:27:03.000
Multi-word phrases are always different tokens.

02:27:03.000 --> 02:27:07.000
Anything that's weird like a URL gets completely cursed.

02:27:07.000 --> 02:27:12.000
And models don't want to have this happen.

02:27:12.000 --> 02:27:18.000
So they devote a bunch of parameters to build a pseudo vocabulary of what's going on.

02:27:18.000 --> 02:27:24.000
And just returning to your point earlier about, like, is it just these syntax-level things?

02:27:24.000 --> 02:27:28.000
Is there some actual more semantic stuff going on?

02:27:28.000 --> 02:27:33.000
We did also have case studies looking at contextual neurons, things like,

02:27:33.000 --> 02:27:38.000
this code is in Python, this language is in French.

02:27:38.000 --> 02:27:42.000
And these were seemingly monosemitic.

02:27:42.000 --> 02:27:45.000
Like it seemed like there were specific neurons here.

02:27:45.000 --> 02:27:49.000
And we found things like if you ablate the French neuron,

02:27:49.000 --> 02:27:53.000
loss on French text gets much worse, what other ones are fine.

02:27:53.000 --> 02:28:00.000
And also some interesting results that the model was, say, using this disambiguate things.

02:28:00.000 --> 02:28:07.000
Like tokens like D are common in German and also common in Dutch.

02:28:07.000 --> 02:28:14.000
And the neurons for those languages were being used to disambiguate for that token,

02:28:14.000 --> 02:28:17.000
whether it was like a German D or a Dutch D,

02:28:17.000 --> 02:28:19.000
because they've got very different meaning in the two languages.

02:28:19.000 --> 02:28:23.000
Yeah, I wondered if you'd give me some interest in that, because as you say, in Wes's paper,

02:28:23.000 --> 02:28:28.000
he did actually find that there are some monosemitic neurons like French, as you just said.

02:28:28.000 --> 02:28:33.000
And in this case, the model decided that interference, in some sense, wasn't worth the burden.

02:28:33.000 --> 02:28:36.000
But what does burden mean here?

02:28:36.000 --> 02:28:40.000
And French is a very vague concept as well.

02:28:40.000 --> 02:28:44.000
Yes. So, all right, a couple of observations.

02:28:44.000 --> 02:28:50.000
First is I do not think we have properly shown they are monosemitic neurons.

02:28:50.000 --> 02:28:55.000
We were looking, these models were trained on the pile, and we were specifically looking at them on Europal,

02:28:55.000 --> 02:29:00.000
which is like a data set of European Parliament transcripts, which are labeled by language.

02:29:00.000 --> 02:29:04.000
And we found a neuron that seemed to strongly disambiguate French from non-French.

02:29:04.000 --> 02:29:08.000
But it was on this domain of parliamentary stuff.

02:29:08.000 --> 02:29:12.000
And because models really want to avoid simultaneous interference,

02:29:12.000 --> 02:29:17.000
if they did have superposition, they'd probably want to do it with something that isn't likely to co-occur in this context.

02:29:17.000 --> 02:29:22.000
I don't know, this is a list variable in Python, which we didn't check very hard for.

02:29:22.000 --> 02:29:29.000
And in particular, this is messy to check for, because in order to do that, you need to answer these questions like,

02:29:29.000 --> 02:29:31.000
what is French?

02:29:31.000 --> 02:29:36.000
Like, there's a bunch of English checks to activate for, but it will activate on words like,

02:29:36.000 --> 02:29:39.000
Sacre Bleu and Trebilla.

02:29:39.000 --> 02:29:46.000
And I think I count this as French, but like, I don't have a rigorous definition of French.

02:29:46.000 --> 02:29:51.000
And I think an open problem I'd love to see someone pursue is just,

02:29:51.000 --> 02:29:55.000
can you prove one of these neurons is actually a French detecting neuron or not?

02:29:55.000 --> 02:29:58.000
And what would it even look like to do that?

02:29:58.000 --> 02:30:04.000
And yeah, regarding interference in the burden, so the way I think about it,

02:30:04.000 --> 02:30:09.000
if two features are not orthogonal, then,

02:30:09.000 --> 02:30:12.000
no, sorry, this is more interesting in the case of neurons.

02:30:12.000 --> 02:30:16.000
If there's multiple things that could all activate a neuron,

02:30:16.000 --> 02:30:22.000
then it's harder for the downstream bit of the model to know how to use the fact that that neuron activated,

02:30:22.000 --> 02:30:27.000
because there are multiple things, even if they don't co-occur, because they're mutually exclusive.

02:30:27.000 --> 02:30:29.000
And this is just a cost.

02:30:29.000 --> 02:30:33.000
And there's a trade-off between having more features and not having this cost.

02:30:33.000 --> 02:30:38.000
And features like this is in French are really load-bearing.

02:30:38.000 --> 02:30:42.000
They're just really important for a lot of circuitry here.

02:30:42.000 --> 02:30:48.000
And so, theoretically, the model might want to dedicate an entire neuron to this,

02:30:48.000 --> 02:30:54.000
but if you dedicate an entire neuron, you lose the ability to do as much superposition.

02:30:54.000 --> 02:30:58.000
My intuition is the number of features that can be represented in superposition

02:30:58.000 --> 02:31:03.000
is actually grows more than linearly with the number of dimensions.

02:31:03.000 --> 02:31:08.000
So this might be significantly worse than just having one fewer feature.

02:31:08.000 --> 02:31:15.000
So we are now in the next chapter of this beautiful podcast, and we're going to talk about transformers.

02:31:15.000 --> 02:31:21.000
So how exactly do transformers represent algorithms and circuits?

02:31:21.000 --> 02:31:25.000
And also, you've written this beautiful mathematical framework about transformers,

02:31:25.000 --> 02:31:30.000
which, of course, is working very closely with Catherine Olsen and Chris Olat.

02:31:30.000 --> 02:31:31.000
And Nelson Olsh.

02:31:31.000 --> 02:31:34.000
And Nelson, my apologies.

02:31:34.000 --> 02:31:39.000
Yeah, so in terms of understanding, yeah.

02:31:39.000 --> 02:31:42.000
So if you wanted to do a mechanism to interpretability on a model,

02:31:42.000 --> 02:31:49.000
you need to really deeply understand the structure of the model.

02:31:49.000 --> 02:31:52.000
What are the layers? What are the parameters? How do they fit together?

02:31:52.000 --> 02:31:56.000
What are the kinds of things that make sense there?

02:31:56.000 --> 02:32:01.000
And let's see.

02:32:01.000 --> 02:32:10.000
So, yeah, there's like a couple of key things I want to emphasize from that paper, though,

02:32:10.000 --> 02:32:14.000
I don't know, it's also one of my, like, all-time top three interpretability papers.

02:32:14.000 --> 02:32:16.000
People should just go read it.

02:32:16.000 --> 02:32:20.000
And after reading it, check out my three-hour video walkthrough about it,

02:32:20.000 --> 02:32:24.000
which apparently is most useful if you've already read the paper,

02:32:24.000 --> 02:32:27.000
because it's that deep anyway.

02:32:27.000 --> 02:32:29.000
Yeah, so a couple of things I want to call out from that,

02:32:29.000 --> 02:32:34.000
especially for people who are kind of familiar with other network but not transformers.

02:32:34.000 --> 02:32:38.000
The first, we've already discussed the residual stream as the central object.

02:32:38.000 --> 02:32:42.000
And the second is how to think about attention,

02:32:42.000 --> 02:32:46.000
because attention is the main thing which is weird about models.

02:32:46.000 --> 02:32:51.000
They have these MLP layers, which actually represent, like,

02:32:51.000 --> 02:32:55.000
two-thirds of the parameters in a transformer, which is often an underrated fact,

02:32:55.000 --> 02:32:58.000
but attention is the interesting stuff.

02:32:58.000 --> 02:33:03.000
So, transformers have a separate residual stream for each input token,

02:33:03.000 --> 02:33:09.000
and this contains, like, all memory the model would store at that position.

02:33:09.000 --> 02:33:13.000
But MLP layers can only process information in place.

02:33:13.000 --> 02:33:16.000
You need attention to move things between positions.

02:33:16.000 --> 02:33:20.000
And classically, people might have used stuff like a 1D convolution.

02:33:20.000 --> 02:33:24.000
You average over 10 things in a sliding window.

02:33:24.000 --> 02:33:31.000
This is baking in the inductive bias that nearby information is more likely to be useful.

02:33:31.000 --> 02:33:37.000
But this is kind of a pretty limited bias to bake in,

02:33:37.000 --> 02:33:42.000
and the story of deep learning is that over time, people have realized,

02:33:42.000 --> 02:33:48.000
wait, we should not be trying to force the model to do specific things.

02:33:48.000 --> 02:33:52.000
We understand, we should not be telling the model how to do its job.

02:33:52.000 --> 02:33:56.000
If it has enough parameters and is competent enough, it can figure it out on its own.

02:33:56.000 --> 02:34:00.000
And so the idea here is rather than giving it a convolution,

02:34:00.000 --> 02:34:06.000
you give it this attention mechanism where each token gets a query saying what it's looking for,

02:34:06.000 --> 02:34:11.000
each previous token gets a key saying what it has to offer,

02:34:11.000 --> 02:34:18.000
and the model looks from each destination token to the source tokens earlier on

02:34:18.000 --> 02:34:22.000
with the keys that are most relevant to the current query.

02:34:22.000 --> 02:34:28.000
And models, and the way to think about an attention head,

02:34:28.000 --> 02:34:32.000
so attention layers break up into these distinct bits called heads,

02:34:32.000 --> 02:34:37.000
which act independently of the others and add to their outputs together,

02:34:37.000 --> 02:34:40.000
and just directly add to the residual string.

02:34:40.000 --> 02:34:44.000
This is sometimes phrases concatenate their outputs and then multiply by a map,

02:34:44.000 --> 02:34:47.000
but this is mathematically equivalent.

02:34:47.000 --> 02:34:52.000
Each head acts independently and in parallel,

02:34:52.000 --> 02:34:58.000
and further, you can think of each head as separately breaking down into a

02:34:58.000 --> 02:35:03.000
which information to move a bit determined by the attention,

02:35:03.000 --> 02:35:07.000
which are determined by the query and key calculating matrices,

02:35:07.000 --> 02:35:11.000
and the what information to move once I know where I'm looking,

02:35:11.000 --> 02:35:16.000
which are determined by the value and output matrices.

02:35:16.000 --> 02:35:21.000
We often think about these in terms of the QK matrix,

02:35:21.000 --> 02:35:26.000
WQ times WK transpose, and the OV matrix,

02:35:26.000 --> 02:35:32.000
WO times WV, because there's no long linearity in between,

02:35:32.000 --> 02:35:37.000
and these two matrices determine like what the head does.

02:35:37.000 --> 02:35:42.000
And the reason I say these are kind of independent is that

02:35:42.000 --> 02:35:46.000
once the model has decided which source tokens to look at,

02:35:46.000 --> 02:35:51.000
the information that gets output by the head is independent of the destination token,

02:35:51.000 --> 02:35:57.000
and like the query only matters for choosing where to move information from,

02:35:57.000 --> 02:36:01.000
and this can result in interesting bugs,

02:36:01.000 --> 02:36:06.000
like there's this motif of a skip trigram,

02:36:06.000 --> 02:36:14.000
the model realizes that if the current thing is three and two has appeared in the past,

02:36:14.000 --> 02:36:16.000
then four is more likely to come next.

02:36:16.000 --> 02:36:20.000
If the current thing is three and four has appeared in the past,

02:36:20.000 --> 02:36:22.000
two is more likely to come next.

02:36:22.000 --> 02:36:29.000
But if you have multiple destination tokens that all want the same source token,

02:36:29.000 --> 02:36:34.000
for example, the phrase keep in mind can be a skip trigram,

02:36:34.000 --> 02:36:36.000
really it should be a trigram,

02:36:36.000 --> 02:36:41.000
but tiny models aren't very good at figuring out what's exactly at the previous position.

02:36:41.000 --> 02:36:44.000
Keep at bay is another trigram,

02:36:44.000 --> 02:36:48.000
but in an at, we'll both look at the same keep token,

02:36:48.000 --> 02:36:54.000
and so they must boost both at and mind for both of them.

02:36:54.000 --> 02:37:01.000
So we'll also predict keep in bay and keep at mind.

02:37:01.000 --> 02:37:09.000
And possibly we should move on to induction heads,

02:37:09.000 --> 02:37:11.000
which are a good illustrative example.

02:37:11.000 --> 02:37:13.000
Yeah, it's going to come onto that.

02:37:13.000 --> 02:37:15.000
So on these induction heads,

02:37:15.000 --> 02:37:18.000
you've said that they seem universal across all models.

02:37:18.000 --> 02:37:23.000
They underlie more complex behavior, like few-shot learning.

02:37:23.000 --> 02:37:25.000
They emerge in a phase transition,

02:37:25.000 --> 02:37:29.000
and they're crucial for this in-context learning.

02:37:29.000 --> 02:37:34.000
And you said that sometimes specific circuits underlie emergent phenomena,

02:37:34.000 --> 02:37:40.000
and we may want to predict or understand emergence by studying these circuits.

02:37:40.000 --> 02:37:43.000
So what do we know so far?

02:37:43.000 --> 02:37:45.000
A lot of questions in there.

02:37:45.000 --> 02:37:47.000
All right, taking this in order.

02:37:47.000 --> 02:37:50.000
So what is an induction head?

02:37:50.000 --> 02:37:52.000
I've already mentioned this briefly.

02:37:52.000 --> 02:37:55.000
Text often contains repeated subsequences,

02:37:55.000 --> 02:38:00.000
like after Tim, Scarf may come next,

02:38:00.000 --> 02:38:03.000
but if Tim Scarf has appeared like five times,

02:38:03.000 --> 02:38:06.000
then it's much more likely to come next.

02:38:06.000 --> 02:38:10.000
In toy two-layer attention-only language models,

02:38:10.000 --> 02:38:14.000
we found this circuit called an induction head, which does this.

02:38:14.000 --> 02:38:18.000
It's a real algorithm that works on, say, repeated random tokens.

02:38:18.000 --> 02:38:23.000
And we have some mechanistic understanding of the basic form of it,

02:38:23.000 --> 02:38:28.000
where there's two attention heads and two different layers working together.

02:38:28.000 --> 02:38:35.000
The later one called an induction head looks from Tim to previous occurrences of Scarf.

02:38:35.000 --> 02:38:38.000
The first one is a previous token head,

02:38:38.000 --> 02:38:41.000
which on each Scarf looks at what came before,

02:38:41.000 --> 02:38:47.000
and is like, ah, this is a Scarf token which has Tim before,

02:38:47.000 --> 02:38:53.000
and then the induction head looks at tokens where the token before them was Tim,

02:38:53.000 --> 02:38:57.000
or where the token before them was equal to the current token.

02:38:57.000 --> 02:39:03.000
And when the induction head decided to look at Scarf,

02:39:03.000 --> 02:39:07.000
which is determined purely by the QK matrix,

02:39:07.000 --> 02:39:12.000
it then just copies that to the app, which is purely done by the OV matrix.

02:39:12.000 --> 02:39:16.000
And I think induction heads are a really interesting circuit case study,

02:39:16.000 --> 02:39:24.000
because induction heads are all of the interesting computation is being done by their attention pattern.

02:39:25.000 --> 02:39:28.000
Tim's scarf could be anywhere in the previous context,

02:39:28.000 --> 02:39:30.000
and this algorithm will still work.

02:39:30.000 --> 02:39:37.000
And this is important, because this is what lets the model do

02:39:37.000 --> 02:39:43.000
tracking of long-range dependencies in the text, where it looks far back.

02:39:43.000 --> 02:39:49.000
And you can't bake this in with a simple thing like a convolutional layer.

02:39:50.000 --> 02:39:56.000
In fact, transformers seem notably better than old architectures like LSTMs and RNNs,

02:39:56.000 --> 02:40:01.000
in part because they have induction heads that let them track long-range dependencies.

02:40:01.000 --> 02:40:04.000
And, yeah.

02:40:04.000 --> 02:40:09.000
And more generally, it often is the case that especially late-layer attention heads,

02:40:09.000 --> 02:40:12.000
the OV bit is kind of boring, it's just copying,

02:40:12.000 --> 02:40:17.000
but figure out where to look is where all of the interesting computation lies.

02:40:17.000 --> 02:40:20.000
So, first of all, just to clarify, because people will know what an attention head is,

02:40:20.000 --> 02:40:25.000
but an induction head is one of these circuits that you're talking about, just so people understand.

02:40:25.000 --> 02:40:32.000
And we should get onto this relationship between induction heads and the emergence of in-context learning.

02:40:32.000 --> 02:40:40.000
And also, you said it's very important that we have this scientific understanding with respect to studying emergence,

02:40:40.000 --> 02:40:46.000
but rather that than just framing of interpretability kind of makes better models.

02:40:46.000 --> 02:40:55.000
Yeah. So, maybe I should first explain what emergence is.

02:40:55.000 --> 02:40:57.000
Let's do that.

02:40:57.000 --> 02:41:07.000
I'd be really, really interested if you could just give me the simplest possible explanation of what you think emergence is.

02:41:07.000 --> 02:41:12.000
Sure. Emergence is when things happen suddenly during training,

02:41:12.000 --> 02:41:19.000
anger from not being there to being there fairly rapidly in a non-convex way, rather than gradually developing.

02:41:19.000 --> 02:41:21.000
Is this interesting you said that?

02:41:21.000 --> 02:41:27.000
Because I think of emergence as a surprising change in macroscopic phenomena,

02:41:27.000 --> 02:41:34.000
and it's an observer-relative term, which means it's always from the perspective of another scale.

02:41:35.000 --> 02:41:44.000
So, just a transient change in perplexity or capability or something in my mind wouldn't entail emergence.

02:41:44.000 --> 02:41:48.000
Like, it would need to be some qualitative, meaningful thing,

02:41:48.000 --> 02:41:52.000
rather than just, oh, the loss curve got notably better in this bit.

02:41:52.000 --> 02:42:03.000
I think so. It's definitely related to some notion of surprise, which is inherently relative.

02:42:03.000 --> 02:42:05.000
Yeah. Let's not get hung up on that.

02:42:05.000 --> 02:42:09.000
So, okay. Let's say it's a transient change in something.

02:42:09.000 --> 02:42:11.000
Mm-hmm. Yeah.

02:42:11.000 --> 02:42:15.000
You know, when you call it transient, it's like an unexpected sudden change.

02:42:15.000 --> 02:42:20.000
Though unexpected has so much semantic meaning on it that I don't want to use.

02:42:20.000 --> 02:42:23.000
But, yeah, this is an infinite rabbit hole.

02:42:23.000 --> 02:42:28.000
Yes. But I think this scale thing is relevant as well.

02:42:28.000 --> 02:42:32.000
We are programming neural networks at the microscopic scale,

02:42:32.000 --> 02:42:37.000
and there's some macroscopic change in capability, so it's some...

02:42:37.000 --> 02:42:39.000
Yes. Yeah.

02:42:39.000 --> 02:42:41.000
Yeah. And there's, like, lots of different dimensions.

02:42:41.000 --> 02:42:45.000
You can have emergence on. You can have it as you train a model on more data.

02:42:45.000 --> 02:42:48.000
You can have it as you make the models bigger.

02:42:48.000 --> 02:42:51.000
And these are both interestingly different kinds.

02:42:51.000 --> 02:42:56.000
One of the more famous examples is Chain of Thought and Few Shot Prompting,

02:42:56.000 --> 02:42:59.000
where DP3 is pretty good at this.

02:42:59.000 --> 02:43:02.000
Earlier models were not good at this. This is kind of surprising.

02:43:02.000 --> 02:43:07.000
Chain of Thought is particularly striking because people just noticed

02:43:07.000 --> 02:43:11.000
a while after DP3 was public that if you tell it to think step by step, it becomes much better.

02:43:11.000 --> 02:43:17.000
There's this recent innovation of Tree of Thought that I'm not particularly familiar with,

02:43:17.000 --> 02:43:23.000
but I understand as kind of like applying Monte Carlo Tree Search on top of Chain of Thought.

02:43:23.000 --> 02:43:25.000
Yes. Yes.

02:43:25.000 --> 02:43:29.000
Where you're like, well, there's many ways we can branch at each point.

02:43:29.000 --> 02:43:33.000
Let's use Tree Search algorithms to find the ultimate way of doing this.

02:43:33.000 --> 02:43:37.000
Yeah. But with, let's say, Scratchpad and Chain of Thought,

02:43:37.000 --> 02:43:40.000
I don't necessarily see that as an emergent...

02:43:40.000 --> 02:43:45.000
Well, maybe there's an emergent reasoning capability that comes into play

02:43:45.000 --> 02:43:48.000
when you have a certain threshold size model,

02:43:48.000 --> 02:43:56.000
but I think of it more as kind of having an intermediate augmented memory in the context.

02:43:56.000 --> 02:44:02.000
So you're kind of filling in a gap in cognition by saying you're allowed to...

02:44:02.000 --> 02:44:07.000
It's not just remembering things, it's also reflecting on things that didn't work.

02:44:07.000 --> 02:44:14.000
Yes. So, yeah, clarifying, when I say emergent, when I say Chain of Thought is an emergent property,

02:44:14.000 --> 02:44:20.000
I mean, the capacity to productively do Chain of Thought is the emergent thing

02:44:20.000 --> 02:44:24.000
and telling the model to things step by step is a user-driven thing.

02:44:24.000 --> 02:44:27.000
But, I don't know, I kind of...

02:44:27.000 --> 02:44:34.000
Just as a point of order, though, was it just that it was discovered after GPT-3

02:44:34.000 --> 02:44:37.000
or would it work on GPT-2?

02:44:38.000 --> 02:44:44.000
I would have guessed it doesn't work very well on GPT-2, but I've not checked.

02:44:44.000 --> 02:44:49.000
I'd be pretty interested... I'm sure someone has looked into this, I haven't looked very hard.

02:44:49.000 --> 02:44:53.000
I guess, like, so a lot of my motivation for this work comes from...

02:44:53.000 --> 02:44:58.000
I care a lot about AIX risk and AI alignment and how to make these systems good for the world.

02:44:58.000 --> 02:45:05.000
And when I see things like, oh, we realize that you can make GPT-3 much better by asking it to things step by step,

02:45:05.000 --> 02:45:08.000
I'm like, oh, no.

02:45:08.000 --> 02:45:13.000
What kinds of things could the systems you make be capable of that we just haven't noticed yet?

02:45:13.000 --> 02:45:17.000
That's the concern that the genie's already out the bottle.

02:45:17.000 --> 02:45:22.000
And, I mean, DeepMind just published this Tree of Thought paper.

02:45:22.000 --> 02:45:24.000
It's a really simple idea.

02:45:24.000 --> 02:45:28.000
It's basically a star search over trajectories of prompts,

02:45:28.000 --> 02:45:33.000
and you use the model itself to evaluate the value of a trajectory.

02:45:33.000 --> 02:45:36.000
And I could have done that.

02:45:36.000 --> 02:45:37.000
Anyone could.

02:45:37.000 --> 02:45:39.000
Similar thing with auto-GPT and all this stuff.

02:45:39.000 --> 02:45:41.000
I'm more skeptical than you are.

02:45:41.000 --> 02:45:49.000
I think in the case of Tree of Thought, it closes a capability gap in respect of certain tasks which were not working very well

02:45:49.000 --> 02:45:52.000
because they don't have that kind of system to...

02:45:52.000 --> 02:45:54.000
Models don't seem to plan ahead very well.

02:45:54.000 --> 02:45:59.000
But I still think that it's not just going to magically turn into super intelligent.

02:45:59.000 --> 02:46:01.000
I mean, we can talk about this a little bit later.

02:46:01.000 --> 02:46:03.000
Yeah, okay.

02:46:03.000 --> 02:46:05.000
Yeah, so...

02:46:05.000 --> 02:46:09.000
Yeah, I think this is also pretty relevant to much more near-term risks.

02:46:09.000 --> 02:46:13.000
I don't know, there's lots of things that a sufficiently capable model could do

02:46:13.000 --> 02:46:15.000
that might be pretty destabilizing to society,

02:46:15.000 --> 02:46:21.000
like write actually much better propaganda than human writers can or something.

02:46:21.000 --> 02:46:25.000
And if Tree of Thought makes it possible to do that

02:46:25.000 --> 02:46:28.000
in a way that we did not think was possible when GPT-4 was deployed,

02:46:28.000 --> 02:46:32.000
that's like an interesting thing that I care about noticing.

02:46:32.000 --> 02:46:35.000
It's not a very good example, but...

02:46:35.000 --> 02:46:38.000
Yeah, it is.

02:46:38.000 --> 02:46:40.000
But being able to...

02:46:40.000 --> 02:46:45.000
I mean, first of all, it's been possible to create misinformation for a long time.

02:46:45.000 --> 02:46:50.000
This is why I specified be able to do it notably better than humans can.

02:46:50.000 --> 02:46:52.000
I totally agree.

02:46:52.000 --> 02:46:56.000
The longer doing it a bit more cheaply and a bit more scale doesn't seem obviously that important.

02:46:56.000 --> 02:46:58.000
You could argue that, like, I don't know,

02:46:58.000 --> 02:47:02.000
being a spam bot that feels indistinguishable from a human

02:47:02.000 --> 02:47:07.000
is like a more novel thing that's actually different.

02:47:07.000 --> 02:47:08.000
Yeah.

02:47:08.000 --> 02:47:11.000
But, I don't know, this is like an off-the-cuff example.

02:47:11.000 --> 02:47:14.000
I don't want to get too deep into this,

02:47:14.000 --> 02:47:16.000
because it's not a point I care that deeply about.

02:47:16.000 --> 02:47:18.000
Yeah, I mean, we can come back to it a bit,

02:47:18.000 --> 02:47:20.000
but I think we are nearly already there.

02:47:20.000 --> 02:47:21.000
Yeah.

02:47:21.000 --> 02:47:23.000
You know, this irreversibility thing.

02:47:23.000 --> 02:47:26.000
We don't know.

02:47:26.000 --> 02:47:29.000
Computer games are photorealistic.

02:47:29.000 --> 02:47:32.000
Chatbots are indistinguishable,

02:47:32.000 --> 02:47:35.000
and AI art is pretty much indistinguishable.

02:47:35.000 --> 02:47:37.000
And that could work.

02:47:37.000 --> 02:47:39.000
I mean, I spoke to Daniel Dennett about it last week,

02:47:39.000 --> 02:47:44.000
and he said he's really worried about the epistemic erosion of our society,

02:47:44.000 --> 02:47:47.000
more so interestingly than the ontological erosion.

02:47:47.000 --> 02:47:51.000
And I discovered later that's because he's not a big fan of anything ontological.

02:47:52.000 --> 02:47:56.000
Yeah, it is potentially a problem,

02:47:56.000 --> 02:48:02.000
but I guess to me, people might overestimate the scale

02:48:02.000 --> 02:48:05.000
and magnitude of change of this.

02:48:05.000 --> 02:48:09.000
I feel that, I know I don't want to echo Sam Altman here,

02:48:09.000 --> 02:48:11.000
but he said that we are reasonably smart people,

02:48:11.000 --> 02:48:17.000
and we can adapt and recognize deep fakes and so on.

02:48:17.000 --> 02:48:18.000
Yeah.

02:48:18.000 --> 02:48:22.000
These are complicated societal questions.

02:48:22.000 --> 02:48:25.000
I guess I mostly just have the position of man.

02:48:25.000 --> 02:48:28.000
It sure is kind of concerning that we have these systems

02:48:28.000 --> 02:48:30.000
that could potentially pose risks,

02:48:30.000 --> 02:48:33.000
but you don't know what they do and decide to deploy them,

02:48:33.000 --> 02:48:35.000
and then we discover things they can do.

02:48:35.000 --> 02:48:40.000
And I think that the research direction I'm trying to advocate for here

02:48:40.000 --> 02:48:44.000
is just better learn how to predict this stuff more than anything,

02:48:44.000 --> 02:48:47.000
which hopefully we can all agree is like an interesting direction.

02:48:47.000 --> 02:48:50.000
And there's all kind of debates about is emergent phenomena

02:48:50.000 --> 02:48:52.000
like actually a real thing?

02:48:52.000 --> 02:48:55.000
Like this recent, is this a mirage paper,

02:48:55.000 --> 02:48:58.000
which I think was a bit over-claiming,

02:48:58.000 --> 02:49:01.000
but does make a good point that if you choose your metric

02:49:01.000 --> 02:49:05.000
to be sufficiently sharp, everything looks dramatic.

02:49:05.000 --> 02:49:08.000
One thing I've definitely observed is if you have an accuracy graph

02:49:08.000 --> 02:49:11.000
with a log scale x-axis for grokking,

02:49:11.000 --> 02:49:14.000
it looks fantastically dramatic.

02:49:14.000 --> 02:49:18.000
And I was very careful to not do this in my paper

02:49:18.000 --> 02:49:21.000
because it is cheating.

02:49:21.000 --> 02:49:24.000
But yeah.

02:49:24.000 --> 02:49:30.000
So my particular hot take is that I believe emergence is often underlain

02:49:30.000 --> 02:49:34.000
by the model learning some specific circuit

02:49:34.000 --> 02:49:39.000
or some small family of circuits in a fairly sudden phase transition

02:49:39.000 --> 02:49:43.000
that enables this overall emergent thing.

02:49:43.000 --> 02:49:46.000
And this sequel paper led by Catherine Olson

02:49:46.000 --> 02:49:48.000
in Contest Learning and Induction Heads

02:49:48.000 --> 02:49:51.000
is a big motivator of my belief for this.

02:49:51.000 --> 02:49:56.000
So the idea of the paper is we have this,

02:49:56.000 --> 02:50:00.000
we found induction heads in these toy-till-artentionally models.

02:50:00.000 --> 02:50:03.000
We somewhat mechanistically understood them,

02:50:03.000 --> 02:50:06.000
at least in the simplest case of induction.

02:50:06.000 --> 02:50:10.000
We use this to come up with more of the behavioral test

02:50:10.000 --> 02:50:12.000
for whether it's induction heads.

02:50:12.000 --> 02:50:14.000
You just give them all repeated random tokens

02:50:14.000 --> 02:50:16.000
and you look at whether it looks induction-y.

02:50:16.000 --> 02:50:20.000
And we found that these occurred in basically all models we looked at,

02:50:20.000 --> 02:50:25.000
up to 13B, even though we didn't fully reverse engineer them there.

02:50:25.000 --> 02:50:29.000
And we then found that this was really deeply linked

02:50:29.000 --> 02:50:32.000
to the emergence of in-context learning.

02:50:32.000 --> 02:50:35.000
There's a lot of jargon in there, so let's unpack that.

02:50:35.000 --> 02:50:37.000
In-context learning, already briefly mentioned,

02:50:37.000 --> 02:50:40.000
it's like tracking long-range dependencies in text, like,

02:50:40.000 --> 02:50:43.000
you can use what was on, which was three pages ago,

02:50:43.000 --> 02:50:46.000
to predict what comes next in the current book,

02:50:46.000 --> 02:50:49.000
which is a non-trovial thing.

02:50:49.000 --> 02:50:52.000
It is not obvious to me how I would program a model to do.

02:50:52.000 --> 02:50:55.000
In-context learning is emergent.

02:50:55.000 --> 02:50:59.000
If you operationalize it as average loss on the 500th token

02:50:59.000 --> 02:51:01.000
versus average loss on the 50th token,

02:51:01.000 --> 02:51:04.000
there's a fairly sudden period in training

02:51:04.000 --> 02:51:09.000
where it goes from not very good at it to very good at it.

02:51:09.000 --> 02:51:11.000
Just a tiny point forward of that.

02:51:11.000 --> 02:51:13.000
One interesting thing about in-context learning

02:51:13.000 --> 02:51:16.000
is you're learning at inference time, not training time.

02:51:16.000 --> 02:51:17.000
Yes.

02:51:17.000 --> 02:51:20.000
But you're not changing anything in the underlying model,

02:51:20.000 --> 02:51:23.000
which means anything it can do, presumably,

02:51:23.000 --> 02:51:29.000
must be materializing a competence which was acquired during training.

02:51:29.000 --> 02:51:32.000
So it's coming back to this periodic table thing, right?

02:51:32.000 --> 02:51:34.000
You've just learned all these platonic primitives.

02:51:34.000 --> 02:51:36.000
You do this in-context learning.

02:51:36.000 --> 02:51:38.000
You say, I want you to do this. Here's an example.

02:51:38.000 --> 02:51:43.000
And you've got all of these freeze-dried periodic computational circuits,

02:51:43.000 --> 02:51:46.000
and they spring into life, and they compose together,

02:51:46.000 --> 02:51:48.000
and they do the thing.

02:51:48.000 --> 02:51:49.000
Yes.

02:51:49.000 --> 02:51:50.000
Yes.

02:51:50.000 --> 02:51:54.000
I think induction heads are, to my eyes,

02:51:54.000 --> 02:51:57.000
the canonical example of an inference time algorithm

02:51:57.000 --> 02:52:00.000
stored in the model's weights that get supplied.

02:52:00.000 --> 02:52:04.000
And I'm sure there's a bunch more that no one has yet found.

02:52:04.000 --> 02:52:08.000
And, yeah, a lot of my model is that prompt engineering

02:52:08.000 --> 02:52:11.000
is just telling the model which of its circuits to activate

02:52:11.000 --> 02:52:15.000
and just engaging with various quirks of training

02:52:15.000 --> 02:52:18.000
that have made it more or less steerable in different ways.

02:52:18.000 --> 02:52:22.000
And, yeah, so induction heads also emerge

02:52:22.000 --> 02:52:24.000
in a fairly sudden phase transition.

02:52:24.000 --> 02:52:28.000
And we, and exactly at the same time,

02:52:28.000 --> 02:52:30.000
and we present a bunch more evidence in the paper

02:52:30.000 --> 02:52:33.000
that there's, like, actually a causal link here.

02:52:33.000 --> 02:52:37.000
Like, one-layer models have neither the in-context learning

02:52:37.000 --> 02:52:39.000
or the induction heads phase chain,

02:52:39.000 --> 02:52:41.000
because they can't do induction heads,

02:52:41.000 --> 02:52:43.000
because they're only one layer.

02:52:43.000 --> 02:52:46.000
But if you adapt the architectures,

02:52:46.000 --> 02:52:48.000
they can form induction heads with only one layer.

02:52:48.000 --> 02:52:50.000
Now they have both of these phenomena.

02:52:50.000 --> 02:52:52.000
If you obliterate induction heads,

02:52:52.000 --> 02:52:54.000
in-context learning gets systematically worse.

02:52:54.000 --> 02:52:59.000
And a particularly fun qualitative study

02:52:59.000 --> 02:53:03.000
was looking at soft induction heads,

02:53:03.000 --> 02:53:07.000
heads that seem to be doing something induction-y

02:53:07.000 --> 02:53:09.000
in other domains,

02:53:09.000 --> 02:53:12.000
like a head which attends from the current word in English

02:53:12.000 --> 02:53:15.000
to the thing after the current word in French.

02:53:15.000 --> 02:53:18.000
Or, more excitingly, a few-shot learning head

02:53:18.000 --> 02:53:23.000
on this random synthetic pattern recognition task we made

02:53:23.000 --> 02:53:26.000
where it attended back to the most relevant examples

02:53:26.000 --> 02:53:29.000
to the current one.

02:53:29.000 --> 02:53:32.000
And my interpretation of all this

02:53:32.000 --> 02:53:35.000
is that there's something fairly fundamental

02:53:35.000 --> 02:53:39.000
about the induction-y algorithm for in-context learning.

02:53:39.000 --> 02:53:42.000
So the way I think about it,

02:53:42.000 --> 02:53:45.000
let's say you've got two...

02:53:45.000 --> 02:53:47.000
You want to learn some relation.

02:53:47.000 --> 02:53:50.000
You've got some local context A

02:53:50.000 --> 02:53:52.000
and some past context B.

02:53:52.000 --> 02:53:55.000
And if you observe A and you observe B in the past,

02:53:55.000 --> 02:53:58.000
this gives you some information about what comes next.

02:53:58.000 --> 02:54:01.000
There's two ways this could work out.

02:54:01.000 --> 02:54:03.000
It could be symmetric.

02:54:03.000 --> 02:54:05.000
B helps A and A helps B.

02:54:05.000 --> 02:54:07.000
Or asymmetric.

02:54:07.000 --> 02:54:09.000
B helps A, but A does not help B

02:54:09.000 --> 02:54:11.000
if they're the other way around.

02:54:11.000 --> 02:54:14.000
Asymmetric might be like knowing the title of a book

02:54:14.000 --> 02:54:16.000
tells you what comes next,

02:54:16.000 --> 02:54:19.000
but knowing what's in a random paragraph in the previous bit

02:54:19.000 --> 02:54:22.000
doesn't tell you the title.

02:54:22.000 --> 02:54:25.000
While symmetric is like...

02:54:25.000 --> 02:54:27.000
I know, English sentence helps French sentence,

02:54:27.000 --> 02:54:29.000
French sentence helps English sentence.

02:54:29.000 --> 02:54:34.000
And if you have N symmetric relations,

02:54:34.000 --> 02:54:38.000
like English, French, German, Dutch, Latin, whatever,

02:54:38.000 --> 02:54:40.000
where each of them helps each other,

02:54:40.000 --> 02:54:42.000
this is really efficient to represent.

02:54:42.000 --> 02:54:44.000
Because rather than needing to represent

02:54:44.000 --> 02:54:47.000
N squared different relations separately,

02:54:47.000 --> 02:54:49.000
like you would in the asymmetric case,

02:54:49.000 --> 02:54:52.000
you can just map everything to the same latent space

02:54:52.000 --> 02:54:54.000
and look for matches.

02:54:54.000 --> 02:54:57.000
And fundamentally, this is what induction heads are doing.

02:54:57.000 --> 02:55:00.000
They're mapping current token

02:55:00.000 --> 02:55:03.000
and previous token of thing in the past

02:55:03.000 --> 02:55:05.000
to the same latent space and looking for matches.

02:55:05.000 --> 02:55:09.000
And to me, this is just like a fairly natural primitive

02:55:09.000 --> 02:55:11.000
of attention.

02:55:11.000 --> 02:55:15.000
And this is exciting because, A,

02:55:15.000 --> 02:55:17.000
we found this deep primitive

02:55:17.000 --> 02:55:20.000
by looking at toy two-layer attentionally models.

02:55:20.000 --> 02:55:24.000
B, it was important for understanding

02:55:24.000 --> 02:55:27.000
and ideally for predicting the emergent phenomena

02:55:27.000 --> 02:55:29.000
of in-context learning.

02:55:29.000 --> 02:55:33.000
And two takeaways I have from this

02:55:33.000 --> 02:55:35.000
about work we should be doing.

02:55:35.000 --> 02:55:37.000
The first is we should be going harder

02:55:37.000 --> 02:55:39.000
at looking at toy language models.

02:55:39.000 --> 02:55:42.000
Like open source to scan of 12 of them.

02:55:42.000 --> 02:55:44.000
And I'd love to see what people can find

02:55:44.000 --> 02:55:47.000
in one-layer models with MLPs

02:55:47.000 --> 02:55:50.000
because we really suck at transformer MLP layers.

02:55:50.000 --> 02:55:53.000
And one layer should just be easier than other ones.

02:55:53.000 --> 02:55:56.000
And the second thing is

02:55:56.000 --> 02:55:59.000
I really want a better

02:55:59.000 --> 02:56:02.000
and more scientific understanding of emergence.

02:56:02.000 --> 02:56:04.000
Why does that happen?

02:56:04.000 --> 02:56:07.000
Really understanding particularly notable case studies of it.

02:56:07.000 --> 02:56:10.000
Testing the hypothesis that it is driven

02:56:10.000 --> 02:56:14.000
by specific kinds of circuits like induction heads

02:56:14.000 --> 02:56:18.000
or at least specific families of circuits.

02:56:18.000 --> 02:56:20.000
Even though, I don't know,

02:56:20.000 --> 02:56:22.000
you could argue that because we haven't fully reverse engineered

02:56:22.000 --> 02:56:24.000
the things in the larger models,

02:56:24.000 --> 02:56:26.000
we really know it's actually an induction head.

02:56:26.000 --> 02:56:30.000
And yeah.

02:56:30.000 --> 02:56:32.000
More generally, a lot of my vision

02:56:32.000 --> 02:56:34.000
for why mechantup matters

02:56:34.000 --> 02:56:38.000
is this kind of scientific understanding of models.

02:56:38.000 --> 02:56:41.000
Like I don't care about making models better,

02:56:41.000 --> 02:56:43.000
but I care about knowing what's going to happen,

02:56:43.000 --> 02:56:45.000
knowing why stuff happens,

02:56:45.000 --> 02:56:47.000
achieving real understanding.

02:56:47.000 --> 02:56:50.000
And getting a scientific understanding

02:56:50.000 --> 02:56:52.000
of things like emergence

02:56:52.000 --> 02:56:54.000
seems like one of the things mechantup

02:56:54.000 --> 02:56:56.000
might be uniquely suited to do,

02:56:56.000 --> 02:56:58.000
but also no one checked very hard.

02:56:58.000 --> 02:57:01.000
And you, dear listener, could be the person who checks.

02:57:01.000 --> 02:57:04.000
So there was a paper by Kevin Wang et al.

02:57:04.000 --> 02:57:06.000
called Interpretability in the Wild,

02:57:06.000 --> 02:57:11.000
a circuit for indirect object identification in GPT-2 Small,

02:57:11.000 --> 02:57:14.000
which found a circuit for indirect object identification.

02:57:14.000 --> 02:57:19.000
So they discovered backup name and mover heads,

02:57:19.000 --> 02:57:21.000
which normally don't do much.

02:57:21.000 --> 02:57:24.000
They take over when the main name mover head are ablated.

02:57:24.000 --> 02:57:27.000
And they said mechanistic interpretability

02:57:27.000 --> 02:57:30.000
has a validation set for more scalability techniques.

02:57:30.000 --> 02:57:32.000
They've understood a clear place

02:57:32.000 --> 02:57:35.000
that these ablations can be misleading.

02:57:35.000 --> 02:57:37.000
So...

02:57:37.000 --> 02:57:39.000
Yeah.

02:57:39.000 --> 02:57:41.000
So, yeah, bunch one pack in there.

02:57:41.000 --> 02:57:45.000
So I really like the interpretability in the wild paper.

02:57:45.000 --> 02:57:48.000
Also, Kevin was only 17 when he wrote it.

02:57:48.000 --> 02:57:52.000
Like, man, I was doing nothing remotely as interesting

02:57:52.000 --> 02:57:54.000
when I was in high school.

02:57:54.000 --> 02:57:56.000
So props to him.

02:57:56.000 --> 02:57:59.000
But also a sign of how easy it is

02:57:59.000 --> 02:58:01.000
to pick low hanging fruit

02:58:01.000 --> 02:58:04.000
and do groundbreaking interpretability work.

02:58:05.000 --> 02:58:07.000
Such a young field.

02:58:07.000 --> 02:58:09.000
I know it's so impressive.

02:58:09.000 --> 02:58:12.000
Yeah, I've just checked his Twitter.

02:58:12.000 --> 02:58:14.000
Hey, Kevin.

02:58:14.000 --> 02:58:17.000
And, yeah, so...

02:58:17.000 --> 02:58:19.000
To me, the underline...

02:58:19.000 --> 02:58:21.000
Yeah, so I was zooming out a bit.

02:58:21.000 --> 02:58:24.000
I think there's a family of techniques

02:58:24.000 --> 02:58:26.000
around causal interventions

02:58:26.000 --> 02:58:28.000
and their use in mech and top

02:58:28.000 --> 02:58:30.000
that's useful to understand here.

02:58:30.000 --> 02:58:32.000
So...

02:58:32.000 --> 02:58:36.000
The core technique is this idea of activation patching.

02:58:36.000 --> 02:58:38.000
Where...

02:58:38.000 --> 02:58:40.000
So let's...

02:58:40.000 --> 02:58:42.000
So one of the problems with understanding

02:58:42.000 --> 02:58:44.000
a model's features and circuits

02:58:44.000 --> 02:58:48.000
is models are full of many, many different circuits.

02:58:48.000 --> 02:58:51.000
Each circuit does not activate on many inputs.

02:58:51.000 --> 02:58:54.000
But each circuit will activate...

02:58:54.000 --> 02:58:57.000
But on each input, many circuits will activate.

02:58:57.000 --> 02:58:59.000
And in order to do good mech and top work,

02:58:59.000 --> 02:59:03.000
you need to be incredibly surgical and precise,

02:59:03.000 --> 02:59:06.000
which means you need to learn how to isolate a specific circuit.

02:59:06.000 --> 02:59:10.000
And let's consider a statement like...

02:59:14.000 --> 02:59:16.000
The Eiffel Tower is in Paris

02:59:16.000 --> 02:59:18.000
versus the Colosseum is in Rome.

02:59:18.000 --> 02:59:20.000
These are both...

02:59:20.000 --> 02:59:22.000
There's lots of features happening.

02:59:22.000 --> 02:59:24.000
There's lots of circuits being activated

02:59:24.000 --> 02:59:26.000
on the Eiffel Tower is in Paris.

02:59:26.000 --> 02:59:28.000
This is in English.

02:59:28.000 --> 02:59:30.000
You're doing factual recall.

02:59:30.000 --> 02:59:32.000
You are outputting a location.

02:59:32.000 --> 02:59:34.000
You are outputting a proper noun.

02:59:34.000 --> 02:59:36.000
This is a European landmark.

02:59:36.000 --> 02:59:38.000
Et cetera, et cetera.

02:59:38.000 --> 02:59:41.000
And like, I want to know how the model knows

02:59:41.000 --> 02:59:43.000
the Eiffel Tower is in Paris.

02:59:43.000 --> 02:59:45.000
But the Colosseum is in Rome.

02:59:45.000 --> 02:59:49.000
Controls almost everything apart from the fact.

02:59:49.000 --> 02:59:51.000
And so...

02:59:51.000 --> 02:59:54.000
What I can try to do is

02:59:54.000 --> 02:59:57.000
causally intervene on the Colosseum run.

02:59:57.000 --> 03:00:01.000
And replace, say, the output of an attention head

03:00:01.000 --> 03:00:05.000
with its outputs on the Eiffel Tower prompt

03:00:05.000 --> 03:00:09.000
and see how much this changes the answer from Rome to Paris.

03:00:09.000 --> 03:00:13.000
And this...

03:00:13.000 --> 03:00:19.000
Yeah, this patch can let me really isolate

03:00:19.000 --> 03:00:23.000
how the circuitry for just this specific thing works.

03:00:23.000 --> 03:00:27.000
And there's all kinds of work around this.

03:00:27.000 --> 03:00:30.000
Obnoxiously, all of it uses different notation,

03:00:30.000 --> 03:00:35.000
like resample ablations and causal tracing

03:00:35.000 --> 03:00:38.000
and causal mediation analysis and entertained interventions.

03:00:38.000 --> 03:00:42.000
All similar words are basically the same thing.

03:00:42.000 --> 03:00:45.000
But, yeah.

03:00:45.000 --> 03:00:50.000
The really key insight here is this kind of surgical intervention.

03:00:50.000 --> 03:00:53.000
A classic technique in interpretability is ablations,

03:00:53.000 --> 03:00:55.000
where you just set something to zero.

03:00:55.000 --> 03:00:59.000
And it's kind of janky because if you break something in the model,

03:00:59.000 --> 03:01:02.000
which wasn't interestingly used for the task,

03:01:02.000 --> 03:01:04.000
then everything dies.

03:01:04.000 --> 03:01:07.000
Or if you break it in interesting ways, everything dies.

03:01:07.000 --> 03:01:10.000
For example, in GPT2 Small,

03:01:10.000 --> 03:01:15.000
almost every single task breaks if you delete the 0th MLP layer.

03:01:15.000 --> 03:01:18.000
Yeah, as far as I can tell,

03:01:18.000 --> 03:01:22.000
the 0th MLP layer is kind of an extended embedding.

03:01:22.000 --> 03:01:26.000
GPT2 Small has tied embeddings and unembeddings,

03:01:26.000 --> 03:01:28.000
so they're transposed of each other,

03:01:28.000 --> 03:01:31.000
which is wildly unprincipled in my opinion.

03:01:31.000 --> 03:01:35.000
And the model seems to be both using this for just detokenization

03:01:35.000 --> 03:01:38.000
and combining nearby things with the first attention layer,

03:01:38.000 --> 03:01:43.000
the 0th attention layer, and just undoing the tightness.

03:01:43.000 --> 03:01:47.000
But this means that basically everything is reading from that.

03:01:47.000 --> 03:01:50.000
And I've seen people do zero ablations and everything

03:01:50.000 --> 03:01:52.000
and be like, oh, this is an important part of the circuit.

03:01:52.000 --> 03:01:55.000
Let's get really sidetracked by this.

03:01:55.000 --> 03:01:58.000
Because the effect size is so big.

03:01:58.000 --> 03:02:00.000
Man, being a mech interpret research feels my mind

03:02:00.000 --> 03:02:02.000
with such bizarre trivia like this.

03:02:02.000 --> 03:02:04.000
It's great.

03:02:04.000 --> 03:02:07.000
Models, so bizarre.

03:02:07.000 --> 03:02:11.000
And so, yeah.

03:02:11.000 --> 03:02:13.000
This calls on intervention.

03:02:13.000 --> 03:02:16.000
There's kind of two conceptually different kinds of interventions.

03:02:16.000 --> 03:02:19.000
You can take the Eiffel Tower prompt,

03:02:19.000 --> 03:02:22.000
patch in something from the Colosseum

03:02:22.000 --> 03:02:25.000
and see if it breaks the ability to output Paris,

03:02:25.000 --> 03:02:29.000
to verify which bits kind of are necessary,

03:02:29.000 --> 03:02:32.000
such that getting rid of them will break something.

03:02:32.000 --> 03:02:36.000
Or you can patch something from the Paris run into the Colosseum run

03:02:36.000 --> 03:02:38.000
and see if that makes it output Paris,

03:02:38.000 --> 03:02:41.000
which is testing for stuff that's sufficient.

03:02:41.000 --> 03:02:44.000
I call the first one a resample ablation

03:02:44.000 --> 03:02:47.000
because you're messing up a component by resampling

03:02:47.000 --> 03:02:51.000
and the second one denoising or causal tracing

03:02:51.000 --> 03:02:56.000
because you're intervening with a bit of information

03:02:56.000 --> 03:02:59.000
and seeing if that is sufficient for everything else.

03:02:59.000 --> 03:03:01.000
Though none of these names are good.

03:03:01.000 --> 03:03:03.000
I would love someone to come up with better names.

03:03:03.000 --> 03:03:07.000
And there's all kinds of families of work building on this.

03:03:07.000 --> 03:03:12.000
I have this post called attribution patching

03:03:12.000 --> 03:03:14.000
that tries to apply this as an industrial scale

03:03:14.000 --> 03:03:17.000
by using gradients to approximate it,

03:03:17.000 --> 03:03:20.000
which is fast enough that you could take GPD3

03:03:20.000 --> 03:03:22.000
and its four million neurons

03:03:22.000 --> 03:03:25.000
and do attribution patching on all neurons at once

03:03:25.000 --> 03:03:27.000
on every position.

03:03:27.000 --> 03:03:29.000
Great post.

03:03:29.000 --> 03:03:34.000
Redwood Research has this technique called causal scrubbing,

03:03:34.000 --> 03:03:38.000
which I view as activation patching

03:03:38.000 --> 03:03:41.000
gone incredibly hard and rigorous

03:03:41.000 --> 03:03:44.000
that tries to come up with an automated metric

03:03:44.000 --> 03:03:48.000
for saying this hypothesis about a model

03:03:48.000 --> 03:03:51.000
is actually accurate for how it works,

03:03:51.000 --> 03:03:53.000
where it's kind of complicated.

03:03:53.000 --> 03:03:56.000
But the core idea is you think of a hypothesis

03:03:56.000 --> 03:03:59.000
as saying which resample ablations are allowed

03:03:59.000 --> 03:04:02.000
and you make all of the resample ablations

03:04:02.000 --> 03:04:04.000
that should be allowed.

03:04:04.000 --> 03:04:08.000
Like these components of the model shouldn't really matter

03:04:08.000 --> 03:04:12.000
so we can just patch in stuff from random other inputs.

03:04:12.000 --> 03:04:14.000
If you've got, say, an induction head,

03:04:14.000 --> 03:04:17.000
you might think the induction head cares about

03:04:17.000 --> 03:04:24.000
the current token and the thing before the past token

03:04:24.000 --> 03:04:27.000
that it's going to inductionally attend to.

03:04:27.000 --> 03:04:32.000
So let's replace the token that it's going to be attending to

03:04:32.000 --> 03:04:35.000
with a token from a different input

03:04:35.000 --> 03:04:37.000
but with the same token before it.

03:04:37.000 --> 03:04:39.000
My hypothesis about the induction head

03:04:39.000 --> 03:04:42.000
says this should be allowed, so let's do that.

03:04:42.000 --> 03:04:45.000
I wouldn't want to introduce a rant

03:04:45.000 --> 03:04:48.000
but the metric he uses is really important.

03:04:48.000 --> 03:04:53.000
Yes, this is one of my hobby horses.

03:04:53.000 --> 03:04:59.000
So some of the original work looking at the patching stuff

03:04:59.000 --> 03:05:04.000
like David Bow and Kevin Meng's excellent Rome paper

03:05:04.000 --> 03:05:09.000
uses the probability of Paris as their metric

03:05:09.000 --> 03:05:12.000
and there are other papers that use things like accuracy

03:05:12.000 --> 03:05:16.000
as their metric and generally I think of metrics

03:05:16.000 --> 03:05:20.000
as being on a spectrum from like soft to sharp.

03:05:20.000 --> 03:05:27.000
So generally I think of models as thinking in log space.

03:05:27.000 --> 03:05:30.000
They are kind of acting like basions.

03:05:30.000 --> 03:05:34.000
They are trying to figure out some things in Paris

03:05:34.000 --> 03:05:36.000
and there will be five separate heads

03:05:36.000 --> 03:05:39.000
that each contribute one to the correct logit

03:05:39.000 --> 03:05:42.000
and each of these can be thought of as one bit of information

03:05:42.000 --> 03:05:47.000
and together they get you the right probability of, say, 0.8.

03:05:47.000 --> 03:05:51.000
But if you patch in each one in isolation

03:05:51.000 --> 03:05:54.000
the probability changes negatively

03:05:54.000 --> 03:05:58.000
because probability is exponential in the logits.

03:05:58.000 --> 03:06:00.000
So using probability you're like,

03:06:00.000 --> 03:06:03.000
oh, this head patch doesn't really matter.

03:06:03.000 --> 03:06:06.000
So in this paper they did this thing of patching in

03:06:06.000 --> 03:06:09.000
like 10 adjacent layers at once.

03:06:09.000 --> 03:06:12.000
And to me a really core principle of this kind of causal

03:06:12.000 --> 03:06:14.000
intervention and mechanistic technique

03:06:14.000 --> 03:06:16.000
is you want to be as surgical as possible

03:06:16.000 --> 03:06:18.000
to be as deeply faithful as possible

03:06:18.000 --> 03:06:20.000
to what the neural model is actually doing.

03:06:20.000 --> 03:06:23.000
So in this case there was an interaction between them.

03:06:23.000 --> 03:06:26.000
They were effectively making several interactions

03:06:26.000 --> 03:06:28.000
at once.

03:06:28.000 --> 03:06:33.000
Yes, they were replacing 10 adjacent layers

03:06:33.000 --> 03:06:35.000
and patching things in different layers

03:06:35.000 --> 03:06:37.000
is always a bit weird.

03:06:37.000 --> 03:06:39.000
I don't think that part's that objectionable.

03:06:39.000 --> 03:06:44.000
I mostly just feel like if you choose a metric like log prop

03:06:44.000 --> 03:06:49.000
it allows you to be much more surgical about how you intervene.

03:06:49.000 --> 03:06:54.000
It allows you to identify subtle effects of things.

03:06:54.000 --> 03:06:57.000
Accuracy is even worse because accuracy

03:06:57.000 --> 03:07:00.000
is basically rounding things to zero or one.

03:07:00.000 --> 03:07:02.000
So like if the threshold is 2.5

03:07:02.000 --> 03:07:05.000
any individual patch does nothing.

03:07:05.000 --> 03:07:08.000
Any re-sample ablation does nothing.

03:07:08.000 --> 03:07:11.000
But if you patch in like the 10 adjacent layers

03:07:11.000 --> 03:07:13.000
it will do everything.

03:07:13.000 --> 03:07:16.000
And this can be kind of misleading.

03:07:16.000 --> 03:07:21.000
Another one I often see people do is

03:07:22.000 --> 03:07:28.000
they look at things like the rank of an output.

03:07:28.000 --> 03:07:30.000
At which point does the model realize Paris

03:07:30.000 --> 03:07:32.000
is the most likely next token?

03:07:32.000 --> 03:07:34.000
And this can be super misleading

03:07:34.000 --> 03:07:36.000
because this will make you think

03:07:36.000 --> 03:07:39.000
the third head is the only head that matters.

03:07:39.000 --> 03:07:41.000
When really all five of them matter

03:07:41.000 --> 03:07:43.000
the order is kind of arbitrary.

03:07:43.000 --> 03:07:47.000
And yeah, I've seen papers that I think

03:07:47.000 --> 03:07:50.000
got somewhat misled by using metrics like this.

03:07:51.000 --> 03:07:54.000
And metrics, they matter so much.

03:07:54.000 --> 03:07:56.000
It's so easy to trick yourself.

03:07:56.000 --> 03:07:58.000
My high level pitch is just,

03:07:58.000 --> 03:08:01.000
mech and tub is great, mech and tub is beautiful.

03:08:01.000 --> 03:08:03.000
Also the field is incredibly young.

03:08:03.000 --> 03:08:05.000
There's maybe 30 full time people

03:08:05.000 --> 03:08:07.000
working on it in the world.

03:08:07.000 --> 03:08:09.000
There's a ton of low hanging fruits.

03:08:09.000 --> 03:08:11.000
I've done major research in this field

03:08:11.000 --> 03:08:14.000
I've been at for like less than two years.

03:08:16.000 --> 03:08:19.000
I would love people to come and help

03:08:19.000 --> 03:08:21.000
us solve problems and do research here.

03:08:21.000 --> 03:08:24.000
And we'll link to my post on getting started

03:08:24.000 --> 03:08:28.000
and my sequence called 200 Cronkidopin problems

03:08:28.000 --> 03:08:31.000
in the description to this, hopefully.

03:08:31.000 --> 03:08:34.000
And yeah, I think there's just,

03:08:34.000 --> 03:08:36.000
it's not that hard to get started.

03:08:36.000 --> 03:08:37.000
It's really fun.

03:08:37.000 --> 03:08:39.000
Hopefully I've nerd sniped you

03:08:39.000 --> 03:08:43.000
with at least one thing in this podcast.

03:08:43.000 --> 03:08:45.000
And if you're at least vaguely curious,

03:08:45.000 --> 03:08:47.000
it's just really easy to open one of the tutorials

03:08:47.000 --> 03:08:51.000
linked in my posts and just start screwing around.

03:08:51.000 --> 03:08:54.000
And I'd love to see what you can find.

03:08:54.000 --> 03:08:55.000
Beautiful.

03:08:55.000 --> 03:08:58.000
Also the deep mind element team is currently hiring

03:08:58.000 --> 03:09:00.000
and people should apply,

03:09:00.000 --> 03:09:03.000
which includes hiring from a mechanistic interoperability team.

03:09:03.000 --> 03:09:04.000
Amazing.

03:09:04.000 --> 03:09:06.000
Do they have to do lead code?

03:09:06.000 --> 03:09:08.000
I have no idea.

03:09:08.000 --> 03:09:09.000
Can't remember.

03:09:09.000 --> 03:09:12.000
Yeah, yeah, we did an amazing video

03:09:12.000 --> 03:09:15.000
with Petr Velichkovich.

03:09:15.000 --> 03:09:18.000
I gave him one of my lead code challenges

03:09:18.000 --> 03:09:20.000
and annoyingly he aced it.

03:09:20.000 --> 03:09:24.000
It's all that deep mind interview practice.

03:09:24.000 --> 03:09:28.000
Anyway, okay, let's talk about super intelligence.

03:09:28.000 --> 03:09:31.000
Now, I spoke with our mutual friend,

03:09:31.000 --> 03:09:33.000
Robert Miles about a month ago.

03:09:33.000 --> 03:09:34.000
Rob's so great.

03:09:34.000 --> 03:09:35.000
He's a lovely chap.

03:09:35.000 --> 03:09:36.000
Spoke all about alignment.

03:09:36.000 --> 03:09:39.000
And he accused me of over-philosophizing everything

03:09:39.000 --> 03:09:41.000
because I was talking all about intelligence,

03:09:41.000 --> 03:09:43.000
one of my favorite topics.

03:09:43.000 --> 03:09:47.000
And he said, well, what about fire?

03:09:47.000 --> 03:09:50.000
Fire is something that people didn't understand millennia ago,

03:09:50.000 --> 03:09:54.000
but they knew that it burnt and they knew that it was bad.

03:09:54.000 --> 03:09:57.000
And this is like, this is like a fire,

03:09:57.000 --> 03:09:58.000
which is very interesting.

03:09:58.000 --> 03:10:01.000
And maybe we can bring in a little bit of effective altruism as well.

03:10:01.000 --> 03:10:02.000
So, you know, I...

03:10:02.000 --> 03:10:03.000
If I can just interject.

03:10:03.000 --> 03:10:04.000
Please do, please.

03:10:04.000 --> 03:10:06.000
If there is one thing I have learned

03:10:06.000 --> 03:10:08.000
from the past decade of machine learning programs,

03:10:08.000 --> 03:10:12.000
is that you do not need to understand a thing in order to make it.

03:10:12.000 --> 03:10:16.000
And this extends to things that are smarter than us

03:10:16.000 --> 03:10:20.000
and which are capable of leading to catastrophic risks.

03:10:20.000 --> 03:10:22.000
Yes, yes.

03:10:22.000 --> 03:10:24.000
Well, let's...

03:10:24.000 --> 03:10:27.000
I'll step back a tiny bit and then we'll get there

03:10:27.000 --> 03:10:29.000
because there's the hypothetical nature,

03:10:29.000 --> 03:10:31.000
which I guess I have a bit of a problem with.

03:10:31.000 --> 03:10:33.000
Now, about 10 years ago,

03:10:33.000 --> 03:10:36.000
I was one of the first supporters of Sam Harris' podcast

03:10:36.000 --> 03:10:39.000
and he's quite aligned to EA.

03:10:39.000 --> 03:10:43.000
And he was talking about this very noble idea

03:10:43.000 --> 03:10:45.000
that everyone matters equally

03:10:45.000 --> 03:10:48.000
and people on the left should get on board with that intrinsically.

03:10:48.000 --> 03:10:51.000
And this idea that we should quantitatively analyse

03:10:51.000 --> 03:10:53.000
the impact of charity work

03:10:53.000 --> 03:10:55.000
and solve an optimisation problem

03:10:55.000 --> 03:10:56.000
and earning to give

03:10:56.000 --> 03:10:59.000
and a lot of the stuff that MacAskill spoke about

03:10:59.000 --> 03:11:02.000
and also philosophers like Peter Singer.

03:11:02.000 --> 03:11:06.000
And the focus seemed to be primarily on alleviating poverty,

03:11:07.000 --> 03:11:10.000
and we don't say the biggest problem,

03:11:10.000 --> 03:11:12.000
we say a problem.

03:11:12.000 --> 03:11:14.000
This is another thing our friend Robert Viles said.

03:11:14.000 --> 03:11:15.000
He said,

03:11:15.000 --> 03:11:19.000
the problem is when people talk about the problem,

03:11:19.000 --> 03:11:22.000
there can be more than one problem.

03:11:22.000 --> 03:11:26.000
But anyway, so it's a big problem.

03:11:26.000 --> 03:11:30.000
And recently, you and I can agree

03:11:30.000 --> 03:11:34.000
that EA circles have really laser-focused in

03:11:34.000 --> 03:11:38.000
on existential risk from AI

03:11:38.000 --> 03:11:41.000
as opposed to other more plausible ex-risk concerns

03:11:41.000 --> 03:11:43.000
like pandemics or even nuclear war.

03:11:43.000 --> 03:11:46.000
I'm not to say that they don't focus on that, but...

03:11:46.000 --> 03:11:49.000
I am going to push back on other more plausible ex-risk.

03:11:49.000 --> 03:11:50.000
Go on.

03:11:50.000 --> 03:11:51.000
Go on.

03:11:51.000 --> 03:11:52.000
I just wanted to register an objection.

03:11:52.000 --> 03:11:53.000
Feel free to go.

03:11:53.000 --> 03:11:54.000
Register an objection.

03:11:55.000 --> 03:11:59.000
And cynically, from my point of view,

03:11:59.000 --> 03:12:03.000
I see the influence of Eliezer, Bostrom,

03:12:03.000 --> 03:12:08.000
Hansen, et cetera, kind of shifting the focus on to ex-risk.

03:12:08.000 --> 03:12:13.000
And part of the reason for that is also this kind of overly

03:12:13.000 --> 03:12:16.000
intellectual focus on long-termism.

03:12:16.000 --> 03:12:20.000
And it's done in a very intellectualized way.

03:12:20.000 --> 03:12:25.000
So it's based on the utility function now incorporating

03:12:25.000 --> 03:12:29.000
future simulated humans on different planets,

03:12:29.000 --> 03:12:31.000
a long time away in the future,

03:12:31.000 --> 03:12:34.000
and making all of these intellectual jumps.

03:12:34.000 --> 03:12:36.000
So let's start there.

03:12:36.000 --> 03:12:37.000
What's your take?

03:12:37.000 --> 03:12:38.000
All right.

03:12:38.000 --> 03:12:40.000
So much stuff to respond to in there.

03:12:40.000 --> 03:12:41.000
Good.

03:12:41.000 --> 03:12:43.000
So, all right, a couple of things.

03:12:43.000 --> 03:12:46.000
The first, so, cars on the table.

03:12:46.000 --> 03:12:49.000
I care a lot about AI existential risk.

03:12:49.000 --> 03:12:50.000
Yes.

03:12:50.000 --> 03:12:52.000
The reason I work on mechanistic interpretability

03:12:52.000 --> 03:12:56.000
is because I think that understanding the mysterious black

03:12:56.000 --> 03:12:58.000
boxes that are potentially smarter than us

03:12:59.000 --> 03:13:02.000
and may want things wildly different than what we wanted them

03:13:02.000 --> 03:13:06.000
to want is just clearly better than not understanding them.

03:13:06.000 --> 03:13:07.000
Yes.

03:13:07.000 --> 03:13:10.000
And I think mechanistic interpretability

03:13:10.000 --> 03:13:12.000
is a promising path here.

03:13:12.000 --> 03:13:15.000
And I also would consider myself an effective altruist

03:13:15.000 --> 03:13:16.000
and a rationalist.

03:13:16.000 --> 03:13:20.000
So cars on the table, there's my biases.

03:13:20.000 --> 03:13:25.000
So I generally think it's more productive to discuss

03:13:25.000 --> 03:13:29.000
is AI catastrophic and existential risk a big deal?

03:13:29.000 --> 03:13:34.000
Then is it the biggest deal or is it worth more resources

03:13:34.000 --> 03:13:38.000
on the margin than global poverty or climate change

03:13:38.000 --> 03:13:39.000
or AI ethics?

03:13:39.000 --> 03:13:41.000
And like, there's just lots of problems.

03:13:41.000 --> 03:13:44.000
I care way more about convincing people that AI extras

03:13:44.000 --> 03:13:48.000
could be in your top 10 than it should be in your top one

03:13:48.000 --> 03:13:49.000
because I feel like for most people,

03:13:49.000 --> 03:13:52.000
it's not in their top thousand.

03:13:52.000 --> 03:13:56.000
And there's just so much divisiveness between, say,

03:13:56.000 --> 03:13:59.000
the AI ethics community and the AI alignment community

03:13:59.000 --> 03:14:01.000
about whose problem is a bigger deal.

03:14:01.000 --> 03:14:03.000
And like, both are big problems.

03:14:03.000 --> 03:14:05.000
Why are we arguing?

03:14:05.000 --> 03:14:09.000
And part of this is about our moral intuitions.

03:14:09.000 --> 03:14:12.000
And this is something I spoke a lot with Conor about.

03:14:12.000 --> 03:14:16.000
He said that in many ways he's got this technical empathy.

03:14:16.000 --> 03:14:21.000
So sensory empathy is, I really care about my family.

03:14:21.000 --> 03:14:24.000
I care about these concentric circles of moral status.

03:14:24.000 --> 03:14:26.000
I really care about my family.

03:14:26.000 --> 03:14:29.000
And if I try really hard, I can care about people

03:14:29.000 --> 03:14:30.000
in other countries and so on.

03:14:30.000 --> 03:14:32.000
And then if I try really, really hard,

03:14:32.000 --> 03:14:35.000
I can care about future simulated lives on Mars.

03:14:35.000 --> 03:14:39.000
And Conor said, the idea of this movement is about

03:14:39.000 --> 03:14:42.000
galaxy-braining yourself into being the most empathetic person

03:14:42.000 --> 03:14:44.000
imaginable, but it's a kind of empathy

03:14:44.000 --> 03:14:47.000
that people don't understand.

03:14:48.000 --> 03:14:52.000
So a separate bit of beef I have is with the entire notion

03:14:52.000 --> 03:14:54.000
of long-termism.

03:14:54.000 --> 03:14:57.000
So long-termism is this idea...

03:14:57.000 --> 03:15:01.000
So long-termism is generally caring about the long-term future.

03:15:01.000 --> 03:15:05.000
There's the strong form of value in the future

03:15:05.000 --> 03:15:08.000
basically entirely dominates things today.

03:15:08.000 --> 03:15:12.000
And weaker forms of just this really, really matters.

03:15:12.000 --> 03:15:17.000
And a common misconception about AIX risk and AI safety

03:15:17.000 --> 03:15:23.000
is that you should only work on this if you are a long-termist.

03:15:23.000 --> 03:15:27.000
That, you know, it's a one in a billion chance of mattering,

03:15:27.000 --> 03:15:31.000
but there's a quintillion future lives,

03:15:31.000 --> 03:15:35.000
so this outweighs everyone alive today in moral worth.

03:15:35.000 --> 03:15:39.000
Or, well, we're only gonna get AGI in like 500 years,

03:15:39.000 --> 03:15:42.000
but we're gonna work on it now just in case.

03:15:42.000 --> 03:15:45.000
And like, I think both of these are just nonsense.

03:15:45.000 --> 03:15:49.000
Like, I guess as a concrete example,

03:15:49.000 --> 03:15:53.000
Effective Artists have worked on pandemic prevention for many years.

03:15:53.000 --> 03:15:58.000
And I think it was just clearly the case that pandemics are

03:15:58.000 --> 03:16:01.000
a major threat to people alive today.

03:16:01.000 --> 03:16:04.000
And I like to feel that we've been proven right.

03:16:04.000 --> 03:16:06.000
No one's gonna argue at that point.

03:16:06.000 --> 03:16:09.000
And, you know, everyone's being like, Effective Artists,

03:16:09.000 --> 03:16:11.000
why are you working on AI safety?

03:16:11.000 --> 03:16:13.000
This obviously doesn't matter.

03:16:13.000 --> 03:16:16.000
You know, I feel like we've got one thing right.

03:16:16.000 --> 03:16:19.000
Can I be really skeptical, though, for a second?

03:16:19.000 --> 03:16:22.000
Because, I mean, you're working for DeepMind.

03:16:22.000 --> 03:16:26.000
There's so much prestige and money attached to AI risk.

03:16:26.000 --> 03:16:29.000
Elon Musk is talking about it all the time,

03:16:29.000 --> 03:16:34.000
whereas you could be a scientist working on pandemic responses.

03:16:34.000 --> 03:16:36.000
And, I mean, let's be honest,

03:16:36.000 --> 03:16:39.000
it wouldn't be anywhere near the same level of prestige.

03:16:39.000 --> 03:16:40.000
Yeah.

03:16:40.000 --> 03:16:46.000
So, couple of takes.

03:16:46.000 --> 03:16:51.000
It definitely is the case that I,

03:16:51.000 --> 03:16:54.000
a good chunk of why I personally am working on AI X-Risk

03:16:54.000 --> 03:16:59.000
rather than say BioX-Risk is that I'm a smart mathematician.

03:16:59.000 --> 03:17:00.000
I like AI.

03:17:00.000 --> 03:17:02.000
I like mech and tub.

03:17:02.000 --> 03:17:05.000
I do not think I would be good at biology in the same way.

03:17:05.000 --> 03:17:13.000
And I also personally assert that AI X-Risk is more important

03:17:13.000 --> 03:17:18.000
and, like, more pressing, but, you know, I'm biased.

03:17:18.000 --> 03:17:21.000
And I think it's fair to flag that bias.

03:17:21.000 --> 03:17:24.000
In terms of prestige,

03:17:24.000 --> 03:17:27.000
so I've only really been working on this stuff properly

03:17:27.000 --> 03:17:29.000
for the past two and a half years,

03:17:29.000 --> 03:17:32.000
which is, I mean, it's changed dramatically.

03:17:32.000 --> 03:17:34.000
Like, in the last six months we've gone from,

03:17:34.000 --> 03:17:37.000
well, we're really ever going to get AGI to,

03:17:37.000 --> 03:17:39.000
oh my God, GP4 exists.

03:17:39.000 --> 03:17:42.000
Jeffrey Hinton has left Google to loudly advocate for X-Risk.

03:17:42.000 --> 03:17:46.000
Joshua Benjo is now loudly advocating for X-Risk.

03:17:46.000 --> 03:17:49.000
It's two-thirds of the Turing winners for deep learning.

03:17:49.000 --> 03:17:51.000
You'll never get the third one.

03:17:51.000 --> 03:17:53.000
Yeah, we're never going to get the third one.

03:17:53.000 --> 03:17:57.000
Jan Likert has made his position very, very clear.

03:17:57.000 --> 03:18:00.000
But, you know, as a majority, I'll take it.

03:18:00.000 --> 03:18:01.000
Yes.

03:18:01.000 --> 03:18:03.000
Or the fourth one.

03:18:03.000 --> 03:18:04.000
Yeah.

03:18:04.000 --> 03:18:06.000
He's coming on our podcast, actually.

03:18:06.000 --> 03:18:08.000
Oh, who was the fourth one?

03:18:08.000 --> 03:18:09.000
Schmidt-Huber.

03:18:09.000 --> 03:18:11.000
Yeah, it seems hard.

03:18:11.000 --> 03:18:13.000
I'm very curious to hear the Schmidt-Huber episode.

03:18:13.000 --> 03:18:16.000
Oh, yeah, he's even more virulently against than Jan.

03:18:16.000 --> 03:18:18.000
I'm afraid to say.

03:18:18.000 --> 03:18:19.000
Two out of two.

03:18:19.000 --> 03:18:20.000
Two out of four.

03:18:20.000 --> 03:18:22.000
I'm interested to hear it anyway.

03:18:22.000 --> 03:18:26.000
So, yeah.

03:18:26.000 --> 03:18:30.000
And, yeah, in terms of prestige, I don't know.

03:18:30.000 --> 03:18:36.000
I gather that, say, seven years ago, it was basically just not...

03:18:36.000 --> 03:18:38.000
It would be, like, pretty bad for your career.

03:18:38.000 --> 03:18:42.000
You would not be taken seriously if you mentioned caring about AIX-Risk.

03:18:42.000 --> 03:18:44.000
Your papers would be rejected.

03:18:44.000 --> 03:18:49.000
I hear a story of Stuart Russell at one point talk to a grad student of his

03:18:49.000 --> 03:18:52.000
about how Stuart was concerned about AIX-Risk.

03:18:52.000 --> 03:18:55.000
The grad student was also really concerned and freaking out,

03:18:55.000 --> 03:18:57.000
but they'd been working together for years

03:18:57.000 --> 03:18:59.000
and neither felt comfortable mentioning it.

03:18:59.000 --> 03:19:04.000
And a lot of people who are still in the field were doing the stuff then,

03:19:04.000 --> 03:19:07.000
which makes me somewhat reject the prestige argument,

03:19:07.000 --> 03:19:11.000
at least for senior people in the field.

03:19:11.000 --> 03:19:14.000
I think there's a difference with Stuart Russell in particular.

03:19:14.000 --> 03:19:16.000
He's very credible.

03:19:16.000 --> 03:19:17.000
And he...

03:19:17.000 --> 03:19:18.000
No, I'm not.

03:19:18.000 --> 03:19:20.000
Oh, I didn't mean... I didn't mean you.

03:19:20.000 --> 03:19:23.000
I was talking about the two Godfathers,

03:19:23.000 --> 03:19:25.000
because the thing that...

03:19:25.000 --> 03:19:26.000
Maybe I shouldn't say this,

03:19:26.000 --> 03:19:31.000
but I was surprised that Benjio and Hinton came out in the way they did.

03:19:31.000 --> 03:19:33.000
And I...

03:19:33.000 --> 03:19:35.000
The reason I didn't like what they said was,

03:19:35.000 --> 03:19:39.000
I felt that they were implying that current AI technology

03:19:39.000 --> 03:19:41.000
could pose an existential threat.

03:19:41.000 --> 03:19:45.000
And what I'm getting from you and what I'm getting from Russell is...

03:19:45.000 --> 03:19:47.000
I'm also from Robert Miles,

03:19:47.000 --> 03:19:52.000
is that this is a very real potential threat in the future,

03:19:52.000 --> 03:19:54.000
but it's not a current threat.

03:19:54.000 --> 03:19:58.000
Yes, very real potential threat in the future,

03:19:58.000 --> 03:20:03.000
though I hesitate to confidently assert, say,

03:20:03.000 --> 03:20:05.000
this will not be a threat in the next five years or something.

03:20:05.000 --> 03:20:07.000
It's like pretty hard to say.

03:20:07.000 --> 03:20:08.000
Interesting.

03:20:08.000 --> 03:20:11.000
I'm not confident.

03:20:11.000 --> 03:20:14.000
I agree with your assessment of Benjio and Hinton,

03:20:14.000 --> 03:20:16.000
though they've spoken a bunch publicly,

03:20:16.000 --> 03:20:18.000
so I'll defer if you can point to specific writings.

03:20:18.000 --> 03:20:23.000
But for example, Benjio signed the pause AI for six months,

03:20:23.000 --> 03:20:25.000
more powerful than GPT-4 letter,

03:20:25.000 --> 03:20:27.000
and I don't know.

03:20:27.000 --> 03:20:29.000
I don't think the letter was asserting that...

03:20:29.000 --> 03:20:32.000
The letter definitely wasn't asserting GPT-4 was an extra risk.

03:20:32.000 --> 03:20:35.000
It wasn't confidently asserting GPT-5 would be,

03:20:35.000 --> 03:20:36.000
but it's being like,

03:20:36.000 --> 03:20:39.000
yeah, we need more time and slow down and caution.

03:20:39.000 --> 03:20:41.000
Maybe I'm reading too much into that,

03:20:41.000 --> 03:20:44.000
but it seemed to me that, I mean, Hinton said

03:20:44.000 --> 03:20:49.000
that chat GPT now contains all of the world's knowledge,

03:20:49.000 --> 03:20:51.000
and this chatbot knows everything,

03:20:51.000 --> 03:20:54.000
and it could potentially do very harmful things,

03:20:54.000 --> 03:20:58.000
and I interpreted it possibly incorrectly

03:20:58.000 --> 03:21:01.000
that they were talking about reasonably current

03:21:01.000 --> 03:21:04.000
or next-generation risks.

03:21:04.000 --> 03:21:06.000
I mean, I can't talk for them.

03:21:06.000 --> 03:21:10.000
I also, I don't know, there are lots of near-term risks.

03:21:10.000 --> 03:21:11.000
There's long-term risks.

03:21:11.000 --> 03:21:14.000
I consider it my job to think hard about the long-term risks

03:21:14.000 --> 03:21:16.000
and try to guard against those,

03:21:16.000 --> 03:21:18.000
and I think lots of other people's jobs

03:21:18.000 --> 03:21:20.000
is to focus on the near-term risks,

03:21:20.000 --> 03:21:23.000
and both are great forms of work.

03:21:23.000 --> 03:21:24.000
I don't know.

03:21:24.000 --> 03:21:25.000
One reason I like interpretability

03:21:25.000 --> 03:21:28.000
is I think it is just broadly useful across all of them.

03:21:28.000 --> 03:21:31.000
So what I consider to be my job might just not even matter.

03:21:31.000 --> 03:21:34.000
But, yeah.

03:21:34.000 --> 03:21:37.000
Yeah, no, I probably will not...

03:21:37.000 --> 03:21:39.000
Do not want to get deeply into interpreting

03:21:39.000 --> 03:21:41.000
what other people have said.

03:21:41.000 --> 03:21:43.000
I...

03:21:46.000 --> 03:21:49.000
Well, could I ping you a couple of quick questions?

03:21:49.000 --> 03:21:51.000
So, first of all, you know,

03:21:51.000 --> 03:21:53.000
there's this idea of negative utilitarianism.

03:21:53.000 --> 03:21:55.000
I mean, do you think minimising suffering

03:21:55.000 --> 03:21:58.000
is more important than maximising happiness?

03:21:58.000 --> 03:22:00.000
No.

03:22:00.000 --> 03:22:01.000
No?

03:22:01.000 --> 03:22:03.000
Not sure I've got a more deep answer than that.

03:22:03.000 --> 03:22:05.000
I mostly think a lot of this intrusive reasoning

03:22:05.000 --> 03:22:07.000
is more often by intuition than anything else.

03:22:07.000 --> 03:22:09.000
But it's a bit like this metrics thing we were talking about,

03:22:09.000 --> 03:22:11.000
you know, which is that...

03:22:11.000 --> 03:22:13.000
If you want to have...

03:22:13.000 --> 03:22:15.000
Would you like to tolerate some spiky necks

03:22:15.000 --> 03:22:17.000
for some average happiness?

03:22:17.000 --> 03:22:19.000
Yeah.

03:22:19.000 --> 03:22:21.000
So, I know.

03:22:21.000 --> 03:22:24.000
I have, like, a general frustration

03:22:24.000 --> 03:22:28.000
with these discussions getting too philosophical.

03:22:28.000 --> 03:22:30.000
This is a big issue

03:22:30.000 --> 03:22:32.000
when I hang out with effective altruists

03:22:32.000 --> 03:22:35.000
who really love moral philosophy and population ethics.

03:22:35.000 --> 03:22:36.000
Yes.

03:22:36.000 --> 03:22:37.000
I don't know.

03:22:37.000 --> 03:22:39.000
I have this EA forum post called

03:22:39.000 --> 03:22:42.000
Simplify EA Pictures to Holy Shit X-Risk.

03:22:42.000 --> 03:22:44.000
And it's like...

03:22:44.000 --> 03:22:46.000
So, I don't know.

03:22:46.000 --> 03:22:49.000
If you actually look at some of the concrete work

03:22:49.000 --> 03:22:53.000
people try doing on things like timelines and risk,

03:22:53.000 --> 03:22:57.000
there's this report from a Jay Akatra at Open Philanthropy

03:22:57.000 --> 03:23:00.000
that gives 30-year median timelines

03:23:00.000 --> 03:23:03.000
to AI that's transformative,

03:23:03.000 --> 03:23:06.000
which he's since updated to 20 years.

03:23:06.000 --> 03:23:08.000
There's a report by Joseph Karlsmith

03:23:08.000 --> 03:23:12.000
that estimates about a 10-ish percent chance

03:23:12.000 --> 03:23:14.000
of a major catastrophe from this.

03:23:14.000 --> 03:23:15.000
Yeah.

03:23:15.000 --> 03:23:17.000
And if you just take those numbers,

03:23:17.000 --> 03:23:20.000
this is clearly enough to reach pretty high

03:23:20.000 --> 03:23:23.000
on my list of concerns of people alive today.

03:23:23.000 --> 03:23:25.000
Okay, okay.

03:23:25.000 --> 03:23:28.000
And I think these are bold empirical claims.

03:23:28.000 --> 03:23:32.000
And I think it's great to debate them in the empirical domain.

03:23:32.000 --> 03:23:37.000
But to me, this doesn't feel like a moral question.

03:23:37.000 --> 03:23:40.000
It just feels like from common sense assumptions,

03:23:40.000 --> 03:23:43.000
if you believe these empirical claims,

03:23:43.000 --> 03:23:45.000
this stuff is a really big deal.

03:23:45.000 --> 03:23:47.000
Okay, okay.

03:23:47.000 --> 03:23:49.000
Let's take another couple of steps.

03:23:49.000 --> 03:23:52.000
So, first of all, we save this till later.

03:23:52.000 --> 03:23:54.000
I think deception is very important.

03:23:54.000 --> 03:23:56.000
And Daniel Dennett, when I spoke with him,

03:23:56.000 --> 03:23:58.000
he uses this notion called the intentional stance,

03:23:58.000 --> 03:24:03.000
which basically means that if you use a projection

03:24:03.000 --> 03:24:07.000
of purposes, goals, agency, et cetera,

03:24:07.000 --> 03:24:10.000
in order to understand the behavior of an agent,

03:24:10.000 --> 03:24:13.000
possibly a simulated agent,

03:24:13.000 --> 03:24:17.000
then for all intents and purposes, it has agency.

03:24:17.000 --> 03:24:19.000
It can make decisions.

03:24:19.000 --> 03:24:20.000
It has moral status,

03:24:20.000 --> 03:24:22.000
it has lots of different things like that.

03:24:22.000 --> 03:24:26.000
And he would say that without an intentional stance,

03:24:26.000 --> 03:24:28.000
without agency,

03:24:28.000 --> 03:24:31.000
it's impossible for a model to lie or deceive us.

03:24:31.000 --> 03:24:34.000
Now, what do you think would be the bar

03:24:34.000 --> 03:24:37.000
for something like a GPT model to deceive us and why?

03:24:37.000 --> 03:24:39.000
Yeah.

03:24:39.000 --> 03:24:43.000
So, before I give takes,

03:24:43.000 --> 03:24:47.000
I will generally reinforce Rob's vibe of,

03:24:47.000 --> 03:24:49.000
well, if you have no idea how fire works,

03:24:49.000 --> 03:24:50.000
but you know that it burns you.

03:24:50.000 --> 03:24:52.000
That's kind of the important thing.

03:24:52.000 --> 03:24:57.000
Like maybe a model has just this random learned adaptation

03:24:57.000 --> 03:25:00.000
to output things that are designed to get a user

03:25:00.000 --> 03:25:02.000
to feel and believe a certain way,

03:25:02.000 --> 03:25:05.000
that isn't intentional and isn't deceptive

03:25:05.000 --> 03:25:07.000
in some true COGSI sense,

03:25:07.000 --> 03:25:10.000
but it's like enough for this to be a big deal

03:25:10.000 --> 03:25:11.000
that we should care a lot about.

03:25:11.000 --> 03:25:12.000
Okay.

03:25:12.000 --> 03:25:13.000
With that aside.

03:25:13.000 --> 03:25:15.000
Yeah.

03:25:15.000 --> 03:25:21.000
So, I'm definitely hesitant to ascribe

03:25:21.000 --> 03:25:25.000
an overly confident view of what's going on here.

03:25:25.000 --> 03:25:28.000
And I think lots of early discourse alignment

03:25:28.000 --> 03:25:32.000
around things like utility maximization

03:25:32.000 --> 03:25:37.000
and around things like these things are just

03:25:37.000 --> 03:25:40.000
paperclip maximizers, et cetera,

03:25:40.000 --> 03:25:43.000
is kind of misleading.

03:25:43.000 --> 03:25:45.000
And I don't think it is an accurate model

03:25:45.000 --> 03:25:51.000
of how GPT-7 RLHF++ is going to work.

03:25:51.000 --> 03:25:53.000
Well, that's my prediction.

03:25:53.000 --> 03:25:56.000
One thing that is pretty striking to me is

03:25:56.000 --> 03:25:58.000
I just feel like we're pretty confused

03:25:58.000 --> 03:26:00.000
on both sides of this.

03:26:00.000 --> 03:26:03.000
Like, I do not feel like I can confidently claim

03:26:03.000 --> 03:26:05.000
that these models will demonstrate

03:26:05.000 --> 03:26:08.000
anything remotely like goals or intentions,

03:26:08.000 --> 03:26:10.000
but I also don't feel like you can confidently claim

03:26:10.000 --> 03:26:11.000
that they won't.

03:26:11.000 --> 03:26:16.000
And I'm not talking like 99.99% confidence.

03:26:16.000 --> 03:26:20.000
I'm talking like 95% plus confidence either way.

03:26:20.000 --> 03:26:23.000
And one of my visions for what being good

03:26:23.000 --> 03:26:25.000
at Mechantep might look like is being able

03:26:25.000 --> 03:26:28.000
to actually get grounding for these questions.

03:26:28.000 --> 03:26:31.000
Because I think ultimately these are mechanistic questions.

03:26:31.000 --> 03:26:34.000
Behavioral interventions are not enough to answer

03:26:34.000 --> 03:26:39.000
like, does this thing have a goal in any meaningful sense?

03:26:39.000 --> 03:26:44.000
But yeah, my very rough, soft definition would be

03:26:44.000 --> 03:26:47.000
is the model capable of forming

03:26:47.000 --> 03:26:51.000
and executing long-term plans towards some goal,

03:26:51.000 --> 03:26:55.000
potentially if explicitly prompted to auto-GPT

03:26:55.000 --> 03:26:57.000
or just spontaneously,

03:26:57.000 --> 03:27:01.000
is it capable of actually carrying out these plans?

03:27:01.000 --> 03:27:04.000
And does it form and execute plans

03:27:04.000 --> 03:27:07.000
towards some objective

03:27:07.000 --> 03:27:11.000
that is like encoded in the model somewhere?

03:27:11.000 --> 03:27:13.000
And I don't know.

03:27:13.000 --> 03:27:16.000
I think it's pretty plausible that the first dangerous thing

03:27:16.000 --> 03:27:19.000
is like Chaos GPT-7,

03:27:19.000 --> 03:27:22.000
where someone tells it to do something dangerous

03:27:22.000 --> 03:27:25.000
and it gets misused more so than it's like misaligned.

03:27:25.000 --> 03:27:28.000
And I care deeply about both of these risks.

03:27:28.000 --> 03:27:29.000
Okay.

03:27:29.000 --> 03:27:31.000
So yeah, first one's more of a governance question

03:27:31.000 --> 03:27:33.000
than a technical question.

03:27:33.000 --> 03:27:35.000
And thus is less where I feel like I can add value.

03:27:35.000 --> 03:27:37.000
So I agree with you on all of that.

03:27:37.000 --> 03:27:40.000
So yeah, being less confused about what's going on inside the models

03:27:40.000 --> 03:27:43.000
and using interpretability to figure out

03:27:43.000 --> 03:27:46.000
whether they actually do have agency or goals

03:27:46.000 --> 03:27:49.000
and sometimes they do the right things for the wrong reasons.

03:27:49.000 --> 03:27:52.000
Auditing models that seem aligned before they're deployed

03:27:52.000 --> 03:27:55.000
is something that you've told me before.

03:27:55.000 --> 03:27:56.000
So great.

03:27:56.000 --> 03:28:00.000
And just being able to check more deeply that it truly is aligned.

03:28:00.000 --> 03:28:04.000
But I wanted to talk a little bit about

03:28:04.000 --> 03:28:07.000
this interesting paper from Katya Grice.

03:28:07.000 --> 03:28:11.000
So she wrote a response called, it was on the less wrong,

03:28:11.000 --> 03:28:13.000
debunking the AI apocalypse,

03:28:13.000 --> 03:28:15.000
a comprehensive analysis of counterarguments

03:28:15.000 --> 03:28:18.000
to the basic AI risk case, X-Risk.

03:28:18.000 --> 03:28:21.000
And the reason I read it is so many of the comments

03:28:21.000 --> 03:28:25.000
were destroying me and Doug after we interviewed Rob.

03:28:25.000 --> 03:28:28.000
And they said, well, if you're going to criticise S-Risk,

03:28:28.000 --> 03:28:30.000
I mean, at least go and read Katya Grice's response.

03:28:30.000 --> 03:28:31.000
So I did.

03:28:31.000 --> 03:28:32.000
So I did.

03:28:32.000 --> 03:28:33.000
Here we go.

03:28:33.000 --> 03:28:38.000
So she basically made two big counterarguments

03:28:38.000 --> 03:28:41.000
that intelligence might not actually be a huge advantage

03:28:41.000 --> 03:28:44.000
and about the speed of growth is ambiguous.

03:28:44.000 --> 03:28:46.000
But I first want to touch on what you said before,

03:28:46.000 --> 03:28:49.000
which is about this notion of goal-directedness.

03:28:49.000 --> 03:28:53.000
So alignment people say that if superhuman AI systems are built,

03:28:53.000 --> 03:28:57.000
any given system is likely to be goal-directed

03:28:57.000 --> 03:29:00.000
and the orthogonality thesis and instrumental goals

03:29:00.000 --> 03:29:03.000
are cited as aggravating factors.

03:29:03.000 --> 03:29:07.000
And the goal-directed behaviour is likely to be valuable,

03:29:07.000 --> 03:29:08.000
so economically.

03:29:08.000 --> 03:29:10.000
Goal-directed entities may tend to arise

03:29:10.000 --> 03:29:12.000
from machine-ledding training processes,

03:29:12.000 --> 03:29:14.000
not intending to create them,

03:29:14.000 --> 03:29:17.000
which is kind of talking about some of the emergent behaviours

03:29:17.000 --> 03:29:20.000
that we were talking about earlier with respect to Othello,

03:29:20.000 --> 03:29:21.000
for example.

03:29:21.000 --> 03:29:25.000
And coherence arguments may imply that systems with goal-directedness

03:29:25.000 --> 03:29:28.000
will become more strongly goal-directed over time,

03:29:28.000 --> 03:29:30.000
which is apparently something that is argued for.

03:29:30.000 --> 03:29:32.000
So I'm thinking, what does goal even mean?

03:29:32.000 --> 03:29:35.000
I mean, we anthropomorphise abstract human intelligible concepts

03:29:35.000 --> 03:29:40.000
like goals and they really are emergent

03:29:40.000 --> 03:29:43.000
because they emerge from these low-level interactions

03:29:43.000 --> 03:29:45.000
in the cells in your body

03:29:45.000 --> 03:29:48.000
and then you get these things that we recognise to be goals,

03:29:48.000 --> 03:29:50.000
observer-relative, as we were talking about before.

03:29:50.000 --> 03:29:54.000
But they're just graduated phenomena from smaller things, right?

03:29:54.000 --> 03:29:57.000
So what does it even mean to have a goal?

03:29:58.000 --> 03:30:03.000
Yeah, so a couple of thoughts on that.

03:30:03.000 --> 03:30:06.000
Again, you ask questions with a lot of content in them.

03:30:06.000 --> 03:30:10.000
No problem. I can only apologise.

03:30:10.000 --> 03:30:12.000
I mean, someone who accidentally writes

03:30:12.000 --> 03:30:16.000
19,000 word blog posts all the time, I relate.

03:30:16.000 --> 03:30:22.000
Anyway, so what am I saying?

03:30:22.000 --> 03:30:24.000
So the way...

03:30:24.000 --> 03:30:26.000
Yeah, it's a fake concept, right?

03:30:26.000 --> 03:30:30.000
Yeah, so I definitely want to try to take...

03:30:30.000 --> 03:30:36.000
So there's the mechanistic definition of the model forms plans

03:30:36.000 --> 03:30:40.000
and it evaluates the plans according to some criteria or objective

03:30:40.000 --> 03:30:43.000
and it executes the plans that score better on this.

03:30:43.000 --> 03:30:49.000
And I would love if we get to a point where we can look inside a model

03:30:49.000 --> 03:30:52.000
and look the circuitry that could be behind this or not.

03:30:53.000 --> 03:30:56.000
That would feel like a big milestone for me on

03:30:56.000 --> 03:30:59.000
wow, I really believe mech and tuple matter

03:30:59.000 --> 03:31:02.000
for reducing catastrophic risk from AI.

03:31:04.000 --> 03:31:08.000
A second thing is that...

03:31:10.000 --> 03:31:13.000
Yeah, the kind of more behavioural thing of

03:31:13.000 --> 03:31:15.000
the model systematically takes actions

03:31:15.000 --> 03:31:18.000
that pushes the world towards a certain state.

03:31:18.000 --> 03:31:22.000
And I don't want...

03:31:22.000 --> 03:31:26.000
I think there's a common problem in alignment arguments

03:31:26.000 --> 03:31:30.000
where people get too precise and too specific

03:31:30.000 --> 03:31:33.000
in a way that lots of people reasonably object to

03:31:33.000 --> 03:31:36.000
in a way which is not necessary for the argument.

03:31:36.000 --> 03:31:39.000
There's a really great paper called

03:31:39.000 --> 03:31:41.000
The Alignment Problem from a Deep Learning Perspective

03:31:41.000 --> 03:31:45.000
by Richard Ngo, Lawrence Tan, and Sorin Mindenman.

03:31:45.000 --> 03:31:48.000
And this is probably my biggest recommendation

03:31:48.000 --> 03:31:51.000
for the listening audience of what I think is like

03:31:51.000 --> 03:31:54.000
a pretty well-presented case for alignment.

03:31:54.000 --> 03:31:57.000
And I generally pretty pro-try to make the minimal

03:31:57.000 --> 03:31:59.000
necessary assumptions.

03:31:59.000 --> 03:32:02.000
So for me it's kind of like some soft form of

03:32:02.000 --> 03:32:05.000
goal-directedness of take actions that push the world

03:32:05.000 --> 03:32:07.000
towards a certain state.

03:32:07.000 --> 03:32:12.000
And another important thing is

03:32:12.000 --> 03:32:14.000
there are a bunch of theoretical arguments

03:32:14.000 --> 03:32:18.000
for why goals would spontaneously emerge.

03:32:18.000 --> 03:32:22.000
Ideas around in a misalignment from work

03:32:22.000 --> 03:32:26.000
led by Evan Huminger, ideas around

03:32:26.000 --> 03:32:30.000
just coherent theorems and things like that,

03:32:30.000 --> 03:32:32.000
which I know I find a bit convincing,

03:32:32.000 --> 03:32:35.000
not that convincing, but then there's

03:32:35.000 --> 03:32:38.000
things will have goals because we try to give them goals.

03:32:38.000 --> 03:32:42.000
And I'm like, yeah, that's probably gonna happen.

03:32:42.000 --> 03:32:44.000
It's just clearly useful.

03:32:44.000 --> 03:32:48.000
If you want to have an AI CEO

03:32:48.000 --> 03:32:52.000
or AI helping run logistics and military operations

03:32:52.000 --> 03:32:54.000
to have something that's capable of

03:32:54.000 --> 03:32:56.000
forming and executing long-term plans

03:32:56.000 --> 03:32:58.000
towards some objective.

03:32:58.000 --> 03:33:01.000
And if you believe this is what's gonna happen,

03:33:01.000 --> 03:33:03.000
then the key question is,

03:33:03.000 --> 03:33:06.000
are we capable of ensuring those goals

03:33:06.000 --> 03:33:09.000
are exactly the goals we would like them to be?

03:33:09.000 --> 03:33:12.000
And my answer for any question of the form,

03:33:12.000 --> 03:33:15.000
can we precisely make sure the system is doing exactly X

03:33:15.000 --> 03:33:18.000
and machine learning is, God, no.

03:33:18.000 --> 03:33:21.000
We are not remotely good enough to achieve this

03:33:21.000 --> 03:33:25.000
with our current level of alignment and steering techniques.

03:33:25.000 --> 03:33:29.000
And to me, this is like a more interesting point

03:33:29.000 --> 03:33:31.000
where it's not quite a crux for me,

03:33:31.000 --> 03:33:36.000
but it just seems like a lot easier to argue about

03:33:36.000 --> 03:33:38.000
what people do this.

03:33:39.000 --> 03:33:42.000
Yeah, essentially, I mean, Katya herself said that

03:33:42.000 --> 03:33:46.000
it's unclear that goal-directedness

03:33:46.000 --> 03:33:49.000
is favored by economic pressure to training dynamics

03:33:49.000 --> 03:33:51.000
or coherence arguments, you know,

03:33:51.000 --> 03:33:55.000
whether those are the same thing as kind of goal-directedness

03:33:55.000 --> 03:33:58.000
that implies a zealous drive to control the universe.

03:33:58.000 --> 03:34:01.000
And look at South Korea, they have goals.

03:34:01.000 --> 03:34:05.000
And those goals, I don't really subscribe

03:34:05.000 --> 03:34:08.000
to the dictator view of society.

03:34:08.000 --> 03:34:11.000
I assume they are somehow emergent.

03:34:11.000 --> 03:34:13.000
And similarly...

03:34:13.000 --> 03:34:15.000
Sorry, South Korea or North Korea?

03:34:15.000 --> 03:34:17.000
Sorry, North Korea, did I say South Korea?

03:34:17.000 --> 03:34:19.000
Very different careers.

03:34:19.000 --> 03:34:22.000
Different goals, different goals.

03:34:22.000 --> 03:34:25.000
But you can think about goals in an AI system

03:34:25.000 --> 03:34:29.000
as either being ones which emerge from some low level

03:34:29.000 --> 03:34:32.000
or ones which are explicitly coded by us

03:34:32.000 --> 03:34:35.000
or ones which are instrumental.

03:34:35.000 --> 03:34:38.000
And these are all a whole bunch of goals.

03:34:38.000 --> 03:34:41.000
But we can't really control those.

03:34:41.000 --> 03:34:44.000
We can add pressures.

03:34:44.000 --> 03:34:47.000
How do we control what North Korea does?

03:34:47.000 --> 03:34:51.000
That sure is a question I'd love for someone to answer.

03:34:51.000 --> 03:34:53.000
I don't know.

03:34:53.000 --> 03:34:56.000
I can give speculation.

03:34:56.000 --> 03:34:59.000
There's the question within practice,

03:34:59.000 --> 03:35:01.000
what do people do?

03:35:01.000 --> 03:35:04.000
Which is basically reinforcement learning from human feedback.

03:35:04.000 --> 03:35:07.000
And I expect people would apply that in this situation as well.

03:35:07.000 --> 03:35:10.000
I definitely do not believe we would be able

03:35:10.000 --> 03:35:13.000
to explicitly encode a goal in the system.

03:35:13.000 --> 03:35:17.000
Moreover, even if you can encode,

03:35:17.000 --> 03:35:20.000
even if you could give some scoring function,

03:35:20.000 --> 03:35:23.000
like make the score in this game high,

03:35:23.000 --> 03:35:27.000
this does not give you a model that intrinsically cares about that

03:35:27.000 --> 03:35:30.000
in the same way that, I don't know,

03:35:30.000 --> 03:35:33.000
evolution optimizes inclusive genetic fitness.

03:35:33.000 --> 03:35:36.000
I don't give a fuck about inclusive genetic fitness.

03:35:36.000 --> 03:35:38.000
Even though I care about a bunch of things,

03:35:38.000 --> 03:35:41.000
evolution got me to care about within that,

03:35:41.000 --> 03:35:45.000
like tasty foods and surviving.

03:35:45.000 --> 03:35:51.000
Yeah, so we don't know how to put goals into systems.

03:35:51.000 --> 03:35:55.000
I basically just assert that we are not currently capable

03:35:55.000 --> 03:35:58.000
of putting goals into systems well.

03:35:58.000 --> 03:36:02.000
And this is one of the main things

03:36:02.000 --> 03:36:04.000
the field of alignment thinks about.

03:36:04.000 --> 03:36:06.000
And we're not very good at it.

03:36:06.000 --> 03:36:08.000
It'd be great if we were better at it.

03:36:08.000 --> 03:36:13.000
In terms of, yeah, I definitely don't want to make strong claims

03:36:13.000 --> 03:36:16.000
about, to be dangerous, the goals need to be coherent

03:36:16.000 --> 03:36:20.000
or the goals need to, there needs to be like a singular goal.

03:36:20.000 --> 03:36:23.000
Like I don't have a singular goal.

03:36:23.000 --> 03:36:28.000
It's not obvious to me how these systems will turn out.

03:36:28.000 --> 03:36:31.000
If they don't, in any meaningful sense,

03:36:31.000 --> 03:36:35.000
want a coherent thing, then I'm a fair bit less concerned.

03:36:35.000 --> 03:36:39.000
Though, well, I mean, there's many, many ways

03:36:39.000 --> 03:36:41.000
that human level AI would be good for the world

03:36:41.000 --> 03:36:44.000
or bad for the world, or just wildly destabilizing

03:36:44.000 --> 03:36:47.000
and high variance, of which misalignment risk is one of them.

03:36:47.000 --> 03:36:49.000
And lots of the other ones just don't apply,

03:36:49.000 --> 03:36:52.000
like misuse and systemic risks.

03:36:52.000 --> 03:36:56.000
But leaving those aside, yeah,

03:36:56.000 --> 03:36:59.000
I think if a model is just roughly pushing

03:36:59.000 --> 03:37:02.000
in a goal-directed direction with a bunch of caveats

03:37:02.000 --> 03:37:04.000
and uncertainties and flip-flopping,

03:37:04.000 --> 03:37:08.000
that still seems like a pretty big deal to me.

03:37:08.000 --> 03:37:10.000
Okay, okay.

03:37:10.000 --> 03:37:14.000
Katia, let's just cover her two main arguments.

03:37:14.000 --> 03:37:17.000
So she said that intelligence might not actually be

03:37:17.000 --> 03:37:19.000
a huge advantage.

03:37:19.000 --> 03:37:23.000
Looking at the world, intuitively,

03:37:23.000 --> 03:37:27.000
big discrepancies in power are not to do with intelligence.

03:37:27.000 --> 03:37:32.000
And she said IQ humans with an IQ of 130

03:37:32.000 --> 03:37:35.000
earn roughly $6,000 to $18,000 a year,

03:37:35.000 --> 03:37:37.000
more than average IQ humans.

03:37:37.000 --> 03:37:39.000
Elected representatives are apparently

03:37:39.000 --> 03:37:42.000
slightly smarter on average,

03:37:42.000 --> 03:37:44.000
but not a radical difference.

03:37:44.000 --> 03:37:48.000
Mensa isn't a major force in the world.

03:37:48.000 --> 03:37:51.000
And if we look at people who evidently

03:37:51.000 --> 03:37:54.000
have good cognitive abilities given their intellectual output,

03:37:54.000 --> 03:37:56.000
their personal lives are not obviously drastically

03:37:56.000 --> 03:37:58.000
more successful anecdotally.

03:37:58.000 --> 03:38:02.000
So is it that much of a big deal?

03:38:02.000 --> 03:38:04.000
Yeah.

03:38:04.000 --> 03:38:07.000
So I think this is like a fair point.

03:38:07.000 --> 03:38:11.000
If we looked in the world and IQ,

03:38:11.000 --> 03:38:15.000
or whatever metric of intelligence you want to use,

03:38:15.000 --> 03:38:17.000
it's really dramatically correlated

03:38:17.000 --> 03:38:19.000
with everything good about someone.

03:38:19.000 --> 03:38:22.000
I mean, IQ correlates with basically everything

03:38:22.000 --> 03:38:24.000
you might value in someone's life,

03:38:24.000 --> 03:38:26.000
because we live in an unfair world,

03:38:26.000 --> 03:38:30.000
but not dramatically.

03:38:30.000 --> 03:38:34.000
Yeah, so I think this is a valid argument.

03:38:34.000 --> 03:38:39.000
I generally don't think you should model

03:38:39.000 --> 03:38:43.000
human-level AI as like,

03:38:43.000 --> 03:38:47.000
or like slightly superhuman AI as like an IQ 200 human.

03:38:47.000 --> 03:38:49.000
Like, for example,

03:38:49.000 --> 03:38:51.000
GPT-4, I would argue,

03:38:51.000 --> 03:38:55.000
knows most facts on the internet,

03:38:55.000 --> 03:38:58.000
or many facts.

03:38:58.000 --> 03:39:02.000
And, yeah, knows many facts.

03:39:02.000 --> 03:39:06.000
And this seems...

03:39:06.000 --> 03:39:11.000
GPT-4 knows many facts.

03:39:11.000 --> 03:39:16.000
And this is sure an advantage over me.

03:39:16.000 --> 03:39:18.000
GPT-4 knows how to write a lot of code,

03:39:18.000 --> 03:39:21.000
and it knows how to take software

03:39:21.000 --> 03:39:23.000
and do penetration testing on it.

03:39:23.000 --> 03:39:26.000
It knows lots of social conventions

03:39:26.000 --> 03:39:28.000
and cultural things,

03:39:28.000 --> 03:39:32.000
and has lots of experience reading various kinds of

03:39:32.000 --> 03:39:35.000
text written to be manipulative,

03:39:35.000 --> 03:39:37.000
or manuals on how to make nuclear weapons.

03:39:37.000 --> 03:39:40.000
Sorry, I'm going too hard on the knowledge point.

03:39:40.000 --> 03:39:42.000
There's just lots of different axes.

03:39:42.000 --> 03:39:45.000
You can be human-level or better,

03:39:45.000 --> 03:39:47.000
in which knowledge is one.

03:39:47.000 --> 03:39:49.000
Intelligence or reasoning is one.

03:39:49.000 --> 03:39:52.000
Social manipulation abilities is another.

03:39:52.000 --> 03:39:54.000
Charisma and persuasion is another.

03:39:54.000 --> 03:39:58.000
I think these two are particularly important ones.

03:39:58.000 --> 03:40:03.000
There's forming coherent plans.

03:40:03.000 --> 03:40:07.000
There's just like the ability to execute on stuff.

03:40:07.000 --> 03:40:11.000
24x7 running thousands of copies of yourself in parallel,

03:40:11.000 --> 03:40:14.000
distributed across the world.

03:40:14.000 --> 03:40:17.000
There's running faster than humans.

03:40:17.000 --> 03:40:19.000
And there's just like lots of dimensions here.

03:40:19.000 --> 03:40:24.000
I think the IQ 200 human frame is helpful in some ways,

03:40:24.000 --> 03:40:27.000
but unhelpful in other ways,

03:40:27.000 --> 03:40:31.000
especially if it summons the nerdy scientist

03:40:31.000 --> 03:40:36.000
with no social skills who's life is a mess archetype.

03:40:36.000 --> 03:40:38.000
I'd say it's a nerdy scientist with no social skills

03:40:38.000 --> 03:40:42.000
whose life is a mess.

03:40:42.000 --> 03:40:43.000
Okay, yeah.

03:40:43.000 --> 03:40:45.000
I mean, this is the thing is...

03:40:45.000 --> 03:40:46.000
Because Rob said the same thing.

03:40:46.000 --> 03:40:48.000
On chess, it's possible for someone to be

03:40:48.000 --> 03:40:50.000
literally 20 times better than you,

03:40:50.000 --> 03:40:52.000
that there's a huge dynamic range of skill.

03:40:52.000 --> 03:40:54.000
And that's something we've not really seen

03:40:54.000 --> 03:40:55.000
in human intelligence,

03:40:55.000 --> 03:40:58.000
and it might be because of the way we measure it.

03:40:58.000 --> 03:41:00.000
It's possible that the way we measure it

03:41:00.000 --> 03:41:05.000
doesn't even capture people with broader

03:41:05.000 --> 03:41:07.000
or better abilities.

03:41:07.000 --> 03:41:09.000
Let's just cover her last point quickly.

03:41:09.000 --> 03:41:14.000
So this is that the speed of intelligence growth is ambiguous.

03:41:14.000 --> 03:41:18.000
So this idea that AI would be able to rapidly destroy the world

03:41:18.000 --> 03:41:21.000
seems prima facie unlikely to Katia,

03:41:21.000 --> 03:41:24.000
since no other entity has ever done that.

03:41:24.000 --> 03:41:26.000
And she goes on.

03:41:26.000 --> 03:41:30.000
So the two common broad arguments is that there'll be a feedback loop

03:41:30.000 --> 03:41:34.000
in which intelligent AI makes more intelligent AI repeatedly

03:41:34.000 --> 03:41:36.000
until AI is very, very intelligent.

03:41:36.000 --> 03:41:40.000
Number two, small differences in brains seem to correspond

03:41:40.000 --> 03:41:42.000
to very large differences in performance

03:41:42.000 --> 03:41:44.000
based on observing humans and other apes.

03:41:44.000 --> 03:41:47.000
Thus, any movement past human level

03:41:47.000 --> 03:41:50.000
will take us to unimaginably super human level.

03:41:50.000 --> 03:41:53.000
And the basic counter-arguments to that is that

03:41:53.000 --> 03:41:56.000
the feedback loops might not be as powerful as assumed.

03:41:56.000 --> 03:41:58.000
There could be diminished returns,

03:41:58.000 --> 03:42:00.000
there could be resource constraints,

03:42:00.000 --> 03:42:02.000
and there could be complexity barriers.

03:42:02.000 --> 03:42:06.000
So maybe we should just do that kind of recursive self-improving piece first.

03:42:06.000 --> 03:42:07.000
What do you think about that?

03:42:07.000 --> 03:42:09.000
I don't really buy recursive self-improvement.

03:42:09.000 --> 03:42:10.000
Oh, good.

03:42:10.000 --> 03:42:13.000
It's not an important part of why I'm concerned about this stuff.

03:42:13.000 --> 03:42:20.000
So generally, I just feel like a lot of the arguments were made

03:42:20.000 --> 03:42:24.000
before the current paradigm of enormous foundation models.

03:42:24.000 --> 03:42:29.000
When you're investing hundreds of millions of dollars of compute into a thing,

03:42:29.000 --> 03:42:33.000
it's pretty hard for it to make itself substantially better.

03:42:35.000 --> 03:42:39.000
And you can do things like design better algorithmic techniques.

03:42:39.000 --> 03:42:44.000
I think that is probably one that is more likely to be accelerated

03:42:44.000 --> 03:42:46.000
the better the model gets.

03:42:46.000 --> 03:42:52.000
It's not clear to me how much juice there's to squeeze out of that.

03:42:53.000 --> 03:43:00.000
But generally, I just think a lot of this is going to be bottlenecked

03:43:00.000 --> 03:43:03.000
by hardware and compute and data,

03:43:03.000 --> 03:43:09.000
such that I'm less concerned about some runaway intelligence explosion,

03:43:09.000 --> 03:43:11.000
and I'm more just concerned about,

03:43:11.000 --> 03:43:13.000
we'll eventually make things that are dangerous.

03:43:13.000 --> 03:43:15.000
What do we do then?

03:43:16.000 --> 03:43:20.000
And I think this is a really good fact about the world.

03:43:20.000 --> 03:43:25.000
I think a world where you can have intelligence explosions is really scary.

03:43:25.000 --> 03:43:30.000
And I feel like our current world is a lot less scary than it could have been.

03:43:30.000 --> 03:43:35.000
If some kid in a basement somewhere just wrote the code for AGI one day.

03:43:35.000 --> 03:43:37.000
Yes, yes.

03:43:38.000 --> 03:43:41.000
Okay, well, I mean, just to finish off Katya's final point.

03:43:41.000 --> 03:43:45.000
So the other point they made was about small differences might lead to oval.

03:43:45.000 --> 03:43:47.000
It's a little bit like in squash.

03:43:47.000 --> 03:43:49.000
I don't know if you've ever played squash,

03:43:49.000 --> 03:43:55.000
but a tiny difference in ability leads to one player overwhelmingly dominating the other player,

03:43:55.000 --> 03:43:59.000
because you just get these kind of like, you know, it's a game of attrition,

03:43:59.000 --> 03:44:01.000
and you get these tipping points.

03:44:01.000 --> 03:44:06.000
And she argued that that might not necessarily be the case when comparing AI systems,

03:44:06.000 --> 03:44:08.000
because of three reasons.

03:44:08.000 --> 03:44:12.000
Different architectures, likely to have very different underlying architectures

03:44:12.000 --> 03:44:15.000
and biological brains, which could lead to different scaling properties.

03:44:15.000 --> 03:44:21.000
Performance plateaus, so there might be these plateaus beyond which further increases in intelligence,

03:44:21.000 --> 03:44:24.000
you know, don't lead to significant performance improvements.

03:44:24.000 --> 03:44:28.000
And also this notion of task specific intelligence, something that I strong,

03:44:28.000 --> 03:44:31.000
I believe that all intelligence is specialized as we were speaking about earlier.

03:44:31.000 --> 03:44:35.000
And so it might be specialized rather than being generally intelligent,

03:44:35.000 --> 03:44:42.000
and small differences thus may not translate into large differences in performance across a wide variety of tasks.

03:44:42.000 --> 03:44:46.000
Maybe we should just touch on this, on this kind of task focus thing.

03:44:46.000 --> 03:44:48.000
So I think humans are very specialized.

03:44:48.000 --> 03:44:53.000
We have, and we don't realize that we are because the way we conceive of intelligence is anthropomorphic,

03:44:53.000 --> 03:44:56.000
but actually we don't do four dimensions very well.

03:44:56.000 --> 03:44:58.000
There's lots of things that we don't do very well,

03:44:58.000 --> 03:45:02.000
and we're kind of embedded in the cognitive ecology in quite a complex way.

03:45:02.000 --> 03:45:04.000
So what do you think about that?

03:45:04.000 --> 03:45:11.000
Yeah, so I will, okay, I'll first comment on the general metodynamic of,

03:45:11.000 --> 03:45:17.000
I think that people get way too caught up on philosophizing.

03:45:17.000 --> 03:45:19.000
And no offense.

03:45:19.000 --> 03:45:20.000
I'm so sorry.

03:45:20.000 --> 03:45:28.000
And in particular, I care about whether an AI will cause a catastrophic risk.

03:45:28.000 --> 03:45:34.000
I don't care about whether it fits into, whether it's general in the right way,

03:45:34.000 --> 03:45:37.000
whether it has weaknesses in certain areas,

03:45:37.000 --> 03:45:40.000
whether it's high on the Chomsky hierarchy,

03:45:40.000 --> 03:45:47.000
or whether it's generally intelligent in some specific sense that someone like Gary Marcus would agree with.

03:45:47.000 --> 03:45:54.000
Is that in any way a contradiction of your mechanistic sensibilities?

03:45:54.000 --> 03:45:59.000
Because when it comes to neural networks, you want to understand how they work,

03:45:59.000 --> 03:46:02.000
but when it comes to intelligence, you don't.

03:46:02.000 --> 03:46:03.000
Oh, sorry.

03:46:03.000 --> 03:46:06.000
I want to understand how it works.

03:46:06.000 --> 03:46:08.000
I want to understand everything.

03:46:08.000 --> 03:46:11.000
I just don't think it's...

03:46:11.000 --> 03:46:20.000
I want to disentangle things to be concerned about from theoretical arguments about whether this fits into certain categories.

03:46:20.000 --> 03:46:24.000
For the purposes of deciding whether to be concerned about AI existential risk,

03:46:24.000 --> 03:46:31.000
I see all of the theory arguments as like a means to an end of this ultimate empirical question of,

03:46:31.000 --> 03:46:34.000
is this a thing that could realistically happen?

03:46:34.000 --> 03:46:42.000
And I think that these theoretical frameworks do matter.

03:46:43.000 --> 03:46:45.000
Like, I don't know.

03:46:45.000 --> 03:46:50.000
I think that an image classification model is basically never going to get the point where it's dangerous,

03:46:50.000 --> 03:47:01.000
while a language model that's being RLHF'd to have some notion of intentionality potentially will.

03:47:01.000 --> 03:47:05.000
And, yeah.

03:47:05.000 --> 03:47:10.000
I know I can give random takes, but to me, if you're like,

03:47:10.000 --> 03:47:13.000
AI's can be task-specific in the same way that humans are task-specific.

03:47:13.000 --> 03:47:22.000
I'm like, well, a human is task-general enough that I think they could be massively dangerous in the right situation with the right advantages.

03:47:22.000 --> 03:47:29.000
Like, if they wanted to be and were able to run the thousands copies of themselves at a thousand X speed or something.

03:47:29.000 --> 03:47:33.000
I don't know if that's actually a remotely accurate statement about models.

03:47:33.000 --> 03:47:37.000
Probably they can run many copies, but not a thousand X speed or something.

03:47:37.000 --> 03:47:40.000
But, yeah.

03:47:40.000 --> 03:47:50.000
Generally, that's the kind of question I care about, and I'm concerned many of these definitions lose sight of that.

03:47:50.000 --> 03:47:58.000
And part of my thing of like, I want to keep alignment arguments as having as few assumptions as possible.

03:47:58.000 --> 03:48:02.000
Because the more assumptions you make, the less plausible your case is.

03:48:02.000 --> 03:48:08.000
And the less and like, more room there is for people to like, rightfully disagree.

03:48:08.000 --> 03:48:17.000
I'm like, I want to be careful not to make any of the case rest on like, strong theoretical frameworks because we don't know what we're doing here.

03:48:17.000 --> 03:48:20.000
Enough to have legit theoretical frameworks.

03:48:20.000 --> 03:48:27.000
And I think that AI is likely to be limited in the same way that humans are, at least within the GPT paradigm.

03:48:27.000 --> 03:48:38.000
If you're training it to predict the next word on the internet and a bunch of other stuff, then it's going to learn a lot from human patterns and human thought and human conventions.

03:48:38.000 --> 03:48:42.000
But, I don't know.

03:48:42.000 --> 03:48:50.000
In closing, you said that your personal favorite heuristic is the second species argument.

03:48:50.000 --> 03:48:52.000
Can you tell us?

03:48:52.000 --> 03:49:09.000
Yeah. So, I quite like Hinton's recent pithy quote of, there is no example of something being of some entity being controlled by things less smart than it.

03:49:09.000 --> 03:49:11.000
And that was terrible.

03:49:11.000 --> 03:49:12.000
Sorry.

03:49:12.000 --> 03:49:16.000
I really would. I mean, Twitter went wild over that.

03:49:16.000 --> 03:49:17.000
Oh, they're trying to go wild.

03:49:17.000 --> 03:49:20.000
I mean, look at a company.

03:49:20.000 --> 03:49:23.000
The CEO is usually done with that.

03:49:23.000 --> 03:49:28.000
You have to hire competent people to have a successful company or look at my cat.

03:49:28.000 --> 03:49:29.000
Yeah. Okay.

03:49:29.000 --> 03:49:30.000
This is a terrible thing.

03:49:30.000 --> 03:49:33.000
Let's just start again.

03:49:33.000 --> 03:49:34.000
All right.

03:49:34.000 --> 03:49:38.000
So, yeah, this is often called the gorilla problem.

03:49:38.000 --> 03:49:43.000
Humans are just smarter than gorillas in basically all ways that matter.

03:49:43.000 --> 03:49:54.000
Humans are not actively malevolent to gorillas, but ultimately humans are in charge gorillas are not and gorillas exist because of our continued benevolence or ambivalence.

03:49:54.000 --> 03:50:09.000
And it just seems to me like if you are creating entities that are smarter than you, the default outcome is they end up in control of what's going on in the world and you do not.

03:50:09.000 --> 03:50:13.000
And I kind of just feel like this should be the null hypothesis.

03:50:13.000 --> 03:50:19.000
And then there's a bunch of arguments on top of like, is this a good model?

03:50:19.000 --> 03:50:23.000
Well, obviously, there's lots of disanalogy is because we're making them.

03:50:23.000 --> 03:50:25.000
We ideally have some control over them.

03:50:25.000 --> 03:50:28.000
We're going to try to shape them to be benevolent towards us.

03:50:28.000 --> 03:50:33.000
But this just seems like the default thing to be concerned about to me.

03:50:33.000 --> 03:50:37.000
On that point, though, we are different from computers.

03:50:37.000 --> 03:50:38.000
We scuba dive.

03:50:38.000 --> 03:50:40.000
And that's actually quite a profound thing to say.

03:50:40.000 --> 03:50:48.000
We scuba dive because we are integrated into the ecosystem, not just physically, but cognitively.

03:50:48.000 --> 03:50:51.000
There's a kind of cognitive ecosystem that we're enmeshed in.

03:50:51.000 --> 03:50:54.000
We have a huge advantage over computers.

03:50:54.000 --> 03:50:58.000
Computers can't really do anything in the physical world.

03:50:58.000 --> 03:51:02.000
So I agree with this, but I don't know.

03:51:02.000 --> 03:51:07.000
I feel like the way, I don't know.

03:51:07.000 --> 03:51:18.000
One evocative example is there was this crime lord, El Shapo, who ran his gang from within prison for like many years, very successfully.

03:51:18.000 --> 03:51:25.000
When you have humans in the world who can get to do things for you, you don't need to be physically embodied to get shit done.

03:51:25.000 --> 03:51:27.000
And I don't just look at Blake Lemoine.

03:51:27.000 --> 03:51:36.000
There's no shortage of people who will do things if convinced in the right way, even if they know it's an AI.

03:51:36.000 --> 03:51:38.000
And I do agree with you on that.

03:51:38.000 --> 03:51:50.000
And I think part of the reason why we're going to have the inevitable proliferation of this technology is so many tinkerers will just create many, many different versions of AI.

03:51:50.000 --> 03:51:54.000
And they won't really be thinking about the consequences of their actions.

03:51:54.000 --> 03:51:58.000
But what's the alternative, paternalism?

03:51:59.000 --> 03:52:09.000
Yeah, so to me, the main interesting thing here is large training runs as like the major bottleneck.

03:52:09.000 --> 03:52:11.000
Very few actors can do them.

03:52:11.000 --> 03:52:18.000
We're probably going to get beyond the point where people are even putting the things out behind an API open to many people to use,

03:52:18.000 --> 03:52:24.000
let alone like open sourcing the weights, which we've already pretty clearly moved past.

03:52:24.000 --> 03:52:33.000
And this, to me, seems like the point of intervention you need if you're going to try to make sure things are safe before you deploy them,

03:52:33.000 --> 03:52:43.000
like track the people who are able to do these runs, have standards for what it means to decide a system like this is safe.

03:52:43.000 --> 03:52:48.000
I'm pretty happy Sam Altman's been pushing that stuff very heavily.

03:52:48.000 --> 03:52:53.000
And if competently done, I think this kind of regulation can be very important.

03:52:53.000 --> 03:52:55.000
It could be great.

03:52:55.000 --> 03:52:58.000
Like the alignment research has been doing great work here.

03:52:58.000 --> 03:53:05.000
And I'm very excited to see what the red teaming large language models thing at Defconn looks like.

03:53:05.000 --> 03:53:11.000
But I don't know, maybe to close, I feel like I've been in the role of why alignment matters.

03:53:11.000 --> 03:53:16.000
Maybe I can try to break alignment arguments myself for a bit.

03:53:16.000 --> 03:53:18.000
Please do, yeah.

03:53:18.000 --> 03:53:24.000
So if I condition on actually the world is kind of fine,

03:53:24.000 --> 03:53:34.000
probably my biggest guess is that the goal directed notion is just like not remotely a good understanding of how these things work.

03:53:34.000 --> 03:53:37.000
And it's hard to get them to be goal directed.

03:53:37.000 --> 03:53:40.000
And we just mostly coordinate and don't do that.

03:53:40.000 --> 03:53:45.000
And these systems are mostly just like extremely effective tools.

03:53:45.000 --> 03:53:48.000
It seems like kind of a plausible world we could end up in.

03:53:48.000 --> 03:53:54.000
I don't think it's any more likely than, yep, they're goal directed and this is terrible.

03:53:54.000 --> 03:54:02.000
We end up in a world which just has like lots of these systems that don't coordinate with each other,

03:54:02.000 --> 03:54:08.000
want some more different things are like broadly aligned with human interests,

03:54:08.000 --> 03:54:13.000
but like imperfectly, and just none of them ever get a major advantage over the others.

03:54:13.000 --> 03:54:20.000
And the world kind of continues to be about as the world is with lots of different actors who aren't necessarily aligned with each other,

03:54:20.000 --> 03:54:26.000
but mostly don't try to see over the world except every so often.

03:54:26.000 --> 03:54:30.000
Or we just alignment isn't that hard.

03:54:30.000 --> 03:54:32.000
We crack mechanistic interoperability.

03:54:32.000 --> 03:54:34.000
We look inside the system.

03:54:34.000 --> 03:54:38.000
We use this to iterate on making our techniques really good.

03:54:38.000 --> 03:54:44.000
It turns out that doing RLHF with like enough adversarial training just kind of works.

03:54:44.000 --> 03:54:48.000
Or with AI assistance to help you notice what's going on in the system.

03:54:48.000 --> 03:54:54.000
And this just gets us aligned to the level systems and we can be like, please go solve the problem.

03:54:54.000 --> 03:54:56.000
And then they do.

03:54:56.000 --> 03:55:03.000
And I think people like Yadkowski are very loud about we are almost certainly going to die.

03:55:03.000 --> 03:55:07.000
And we might, but we also might not.

03:55:07.000 --> 03:55:08.000
I don't really know.

03:55:08.000 --> 03:55:11.000
I would love to just become less confused about this.

03:55:11.000 --> 03:55:15.000
And I remain very concerned about this to be clear.

03:55:15.000 --> 03:55:20.000
But I'm not like 99% chance we're all going to die.

03:55:20.000 --> 03:55:26.000
Yeah, but anything which is an appreciable percentage may as well be the same thing.

03:55:26.000 --> 03:55:28.000
Yeah, pretty much.

03:55:28.000 --> 03:55:29.000
Yeah, it's quite funny.

03:55:29.000 --> 03:55:32.000
I got a lot of pushback on the Robert Miles show.

03:55:32.000 --> 03:55:34.000
People said, oh, I can't believe it.

03:55:34.000 --> 03:55:36.000
You framed him to be a doomer.

03:55:36.000 --> 03:55:42.000
And he himself said in the show, I think about five times we're all going to die.

03:55:42.000 --> 03:55:45.000
And I managed to cut about five.

03:55:45.000 --> 03:55:53.000
Well, I don't exaggerate, but that there is at least two posts on Twitter within 15 minutes of that comment where he said, and we're all going to die.

03:55:53.000 --> 03:55:56.000
So I don't think I don't think I'm being unfa...

03:55:56.000 --> 03:56:00.000
Well, I didn't actually call him a doomer, but he basically is.

03:56:01.000 --> 03:56:03.000
I don't know, man.

03:56:03.000 --> 03:56:04.000
I hate ladles.

03:56:04.000 --> 03:56:07.000
Like, Eliasar is clearly a doomer.

03:56:07.000 --> 03:56:08.000
He's clearly a doomer.

03:56:08.000 --> 03:56:09.000
Yeah.

03:56:09.000 --> 03:56:10.000
Rob is much less doomy than Eliasar.

03:56:10.000 --> 03:56:11.000
Yeah.

03:56:11.000 --> 03:56:12.000
Is Rob a doomer?

03:56:12.000 --> 03:56:13.000
I don't know.

03:56:13.000 --> 03:56:15.000
I didn't call him a doomer.

03:56:15.000 --> 03:56:18.000
But empirically, the data says yes.

03:56:18.000 --> 03:56:20.000
Yeah, I mean, I don't know, man.

03:56:20.000 --> 03:56:23.000
It sounds like you spend too much time reading YouTube comments.

03:56:23.000 --> 03:56:24.000
I do.

03:56:24.000 --> 03:56:25.000
Too much time.

03:56:25.000 --> 03:56:32.000
But notoriously, the least productive use of time possible, apart from hanging out on Twitter, reading AI Flameless.

03:56:32.000 --> 03:56:34.000
Twitter is the worst.

03:56:34.000 --> 03:56:35.000
I know.

03:56:35.000 --> 03:56:36.000
It's so bad.

03:56:36.000 --> 03:56:42.000
I mean, we don't need to go there, but we were having a brief discussion before we started in record.

03:56:42.000 --> 03:56:51.000
Why do you think otherwise intelligent, respectable people behave in that way?

03:56:51.000 --> 03:56:57.000
Impulse control, social validation, it's just kind of fun.

03:56:57.000 --> 03:57:02.000
People aren't very self-aware about how they look or aren't that reflective.

03:57:02.000 --> 03:57:07.000
And Twitter incentivizes you to lack nuance and to be outraged about other people.

03:57:07.000 --> 03:57:10.000
I don't know.

03:57:10.000 --> 03:57:16.000
I am very sad by many Twitter dynamics, including from people who otherwise seem worthy of respect.

03:57:16.000 --> 03:57:17.000
Yes.

03:57:17.000 --> 03:57:18.000
Yes.

03:57:18.000 --> 03:57:19.000
Interesting.

03:57:19.000 --> 03:57:23.000
Look, Neil, this has been an absolute honor.

03:57:23.000 --> 03:57:24.000
Thank you so much.

03:57:24.000 --> 03:57:25.000
It's been extremely fun.

03:57:25.000 --> 03:57:26.000
Yeah, it's been amazing.

03:57:26.000 --> 03:57:27.000
It's been a marathon.

03:57:27.000 --> 03:57:29.000
But thank you so much for joining us today.

03:57:29.000 --> 03:57:33.000
And I really think we've had a great conversation and I know everyone's going to love it.

03:57:33.000 --> 03:57:34.000
So thank you so much.

03:57:34.000 --> 03:57:35.000
Yeah.

03:57:35.000 --> 03:57:37.000
I apologize for all the times I saw you off for philosophizing.

03:57:37.000 --> 03:57:38.000
Oh, no problem.

03:57:38.000 --> 03:57:40.000
It's an honor.

03:57:40.000 --> 03:57:41.000
Yeah.

03:57:41.000 --> 03:57:42.000
All right.

03:57:42.000 --> 03:57:44.000
Thanks for having me on.

