Coming up later in Machine Learning Street Talk,
Professor Jan Le Koon, the godfather of deep learning.
There's been a lot of people who have been sort of saying
there's a limitation to deep learning, let's say,
or machine learning more generally.
Because it's obvious that those things basically
do curve fitting, and that only works for interpolation
and not for extrapolation.
And that kind of dismissal always sounded wrong to me.
Would this qualitative difference be in the form of fundamentally
different things from deep learning, things that
are like discrete symbolic reasoning
or things of that type?
And to that, my answer is clearly no.
I do not believe that's the case.
That's a limitation of supervised learning.
It has absolutely nothing to do with deep learning.
Computing on things.
OK, and I put a stop right there.
We also had a fascinating conversation
with Randall Bellisterio from Meta AI Research.
So I think it's two different things
to be able to interpolate or move on your manifold
and being in the interpolation regime from your training set.
So is this similar to interpolation?
Well, I mean, all of machine learning
is similar to interpolation if you want, right?
When you train a linear regression on scalar values,
you're training a model, right?
You're giving a bunch of pairs X and Y.
You're asking what are the best values of A and B for Y equals
AX plus B that minimizes the squared error of the prediction
of a line to all of the points, right?
That's linear regression.
That's interpolation.
All of machine learning is interpolation.
In a high-dimensional space, there
is essentially no such thing as interpolation.
Everything is extrapolation.
So imagine you are in a space of images, right?
So you have a car images 256 by 256.
So it's 200,000 dimensional input space.
Even if you have a million samples,
you're only covering a tiny portion of the dimensions
of that space, right?
Those images are in a tiny sliver of surface
among the space of all possible combinations
of values of pixels.
So when you show the system a new image,
it's very unlikely that this image is a linear combination
of previous images.
What you're doing is extra-appalation, not interpolation.
And in high-dimension, all of machine learning
is extrapolation, which is why it's high.
Now, that was a clip that went out on the Chalet show.
And I think I understand now what Lacune meant.
He's saying that our intuition of interpolation
is only correct in a very low number of dimensions.
It's a mathematical impossibility
to have statistical generalization in high dimensions.
So he's not saying that neural networks are really
extrapolating in the sense of continuing the pattern.
He's saying our definition of extrapolation,
the convex whole membership, is broken.
Machine learning does not and will never
work in high dimensions.
That's why we've invented so many tricks
to reduce the statistical and approximation complexity
of problems, just like we do in computer science
of the computational complexity of algorithms.
Lacune is not saying that deep learning models are clairvoyant.
Jan Lacune thinks that it's specious to say
that neural network models are interpolating
because in high dimensions, everything is extrapolation.
Oh boy.
We started thinking about making this show many months ago
when Randall Belastriro and Jerome Pesente
and Jan Lacune first released their paper,
Learning in High Dimensions Always Amounts to Extrapolation.
And it feels like a lot has happened since then.
I mean, they do say the mark of an educated mind
is being able to change your opinion
in light of new evidence.
Where we are now compared to where we were then,
I mean, let's just say, I feel like I'm standing
on the surface of Pluto.
I was initially quite skeptical of this paper
and I was trying to pick it apart.
And now it feels like we've done 180 degrees,
not only on the way we think about the paper,
but how we think about neural networks in general.
And we really wanna try and impart some of that knowledge
with you today in the introduction,
which might be quite ambitious.
So please bear with us.
But generally speaking, the structure of the show today,
it's gonna be a big show.
It's gonna be a big show.
Use the table of contents, skip around a bit.
But there's an intro, there's a, you know,
where we talk about this blind view
or this boundary view of neural networks,
which I think is very interesting.
There's a conversation with Yann LeCun,
the godfather of deep learning.
And we also speak with Randall Belastriro,
who released this beautiful paper
about thinking about neural networks in this new way.
And we have a debrief as well.
So I've not edited the show yet,
but it's gonna be about three hours.
Anyway, the show must go on.
So as I was saying, particularly speaking with Randall,
was a revelation.
I mean, I originally thought that the authors were saying
that people shouldn't think of neural networks
as interpolators because they thought neural networks
were doing something even more sophisticated.
But what I came to realize, at least in Randall's case,
is he's even more cynical about the behavior
of neural networks than Gary Marcus.
He doesn't think that they're fitting curves at all.
Reading Randall's recent spline theory
of deep learning paper has completely changed the way
that I think about their behavior.
So his view, basically, is that neural networks
recursively chop up the input space
into these little convex cells, or polyhedra,
conceptually similar to decision trees,
but one where the regions in the layer can share information,
which means that different faces of these polyhedra
will correspond to different hyperplanes
set down by different neurons in the neural network,
of course, ones which are topologically addressable
from that location.
So to me, this ends the notion that these neural networks
are learning smooth data manifolds
or performing smooth geometric transformations
in the generative setting, which I used to think of
as being a kind of diffeomorphism.
It also gave me the realization that,
at least in inference, each input prediction
is representable with a single linear affine transformation,
which leads directly to the next realization,
which is that the latent space is not homogenous.
It's potentially a different space
for every single input example.
I just didn't realize that before.
You know, suddenly a lot of the magic
of deep learning has vanished,
and I see them in the same light as things like decision trees
and SVMs in classical machine learning.
I mean, probably because I understood
those things very deeply.
So yeah, neural networks have lost
a little bit of their mystery,
but I think it's a good thing.
Reminds me a little bit of this parable
of the blind men and the elephant,
where, let's say you've got four blind men
around an elephant.
One's got his arm around the trunk.
Oh, it feels like a snake.
One's feeling the tail, or it looks like a rope,
or it feels like a rope.
One's on the side, or it feels like a wall.
And it's very similar situation with neural networks
that, you know, the experience of these blind men,
it's all the truth, but it's not the complete truth.
We're all just trying to understand this thing
from different angles.
Now anyway, the two key challenges in machine learning
are one, the curse of dimensionality.
And there's a corollary of that,
knowing what to ignore in the input space,
because it blows up exponentially with the dimensions.
And two, the extrapolation problem.
What happens when you extrapolate
outside of the training data?
And why does this notion of extrapolation even matter?
Well put simply, it matters because of the implications
it has for generalization in deep learning.
Now, why do deep learning models work at all?
Well, I think there's an incredible amount of engineering
which goes into the state of the art machine learning models,
which makes them work very well on specific tasks.
But it's easy to deceive yourself with neural networks.
Literally, everything about your training process
and predictive architecture could be leaking
the main specific information into your model.
Neural networks are not really blank slate models
like we've been led to believe.
A lot of human crafted domain knowledge
goes into these models.
By the way, this is why Francois Schollet,
I mean, he talks about this notion
of developer aware generalization,
which is being able to generalize to tasks
which the developer of the system
didn't know about at the time.
Now, weights and biases is the developer first
of all, MLops platform build better models faster
with experiment tracking, dataset versioning
and model management.
It provides a platform and a set of tools
which you can use for the entire life cycle
of your machine learning process.
Every team has a reliable system of record
for their engineering system, right?
Whether it's Azure DevOps for software engineering
or Confluence for team wikis.
Weights and biases is your system of record
for machine learning.
Now, for your quick link and to help out the podcast,
head over to 1db.me forward slash MLST.
I'm extremely proud actually that we've been sponsored
by weights and biases.
I've been a huge fan of what they've been doing
since the very beginning.
And also I've been a part of their community forum.
They are the one company who in my opinion
has totally changed the game
around what is the highly nuanced process
and the kind of multidisciplinary team actions
that are required to get machine learning models
safely into production.
I've been a chief data scientist for a major corporation
for the last 18 months or so.
And I've always been a big believer
in introducing engineering rigor
to machine learning and data science.
Engineering fundamentals are so important
when you bring machine learning systems to production
as is helping data scientists to become first class citizens
in the entire life cycle of model development and deployment.
It's almost a shame that MLST has become
quite science orientated
because in my day job, I'm an engineer first.
I would love to make more content about this stuff.
Anyway, weights and biases helps you go faster.
It makes your team more resilient
by increasing your knowledge sharing.
It helps you build models
which not only have better predictive performance
but are more safe and secure.
Weights and biases gives you better management information
allowing your company to make better decisions
and build data products with greater transparency
than ever before.
And critically, because weights and biases
orchestrates the entire machine learning process,
your models are reproducible
and important decisions are immortalized.
This fine level of control lets you set guardrails
where you need to and reduce friction wherever possible.
I've designed many ML DevOps systems over the years,
including when I worked at Microsoft.
The technology has really come of age now
and I think you should be using a platform
like weights and biases
to help you train your machine learning models
and to get them into production.
Reproducibility has always been one of the biggest problems
in machine learning.
And the reason for that is that these systems
are quite brittle, frankly.
Their runtime characteristics depend strongly
on the vagaries of the training regime,
the choice of hyperparameters,
even the hardware that they were trained on.
You know, you always get into this syndrome
where it works on my machine
but I can't put it into production
because I've got no idea
how to recreate this thing somewhere else.
I work in a highly regulated industry
and we often need to wind the clock back
to try and understand how a model was created
which decisions were made
and possibly go back later
to reason about a model's behavior.
This is precisely what weights and biases does.
Now, it's completely free for academics
and it's simple to get started.
You just have to add a few lines of code.
So what are you waiting for?
Remember to click on our special link
because it helps us out.
That's 1db.me forward slash MLST.
We are so grateful for the sponsorship
from weights and biases.
And also feel free to get in touch with us
if you're interested in sponsoring the show.
It'll help us scale up the operation a little bit
because at the moment I'm doing all the editing
and it takes a lot of time.
Anyway, thank you very much.
Randall's other recent work demonstrates
that a large class of neural networks
including CNNs, ResNets, RNNs and beyond,
those that use piecewise linear activation functions
like RELU can be entirely rewritten
as compositions of linear functions
arranged in polyhedra or cells in the input space.
It's a high resolution slice and dice into linear forms
like a fruit ninja set loose on a marching cubes algorithm
such as K-means clustering, matched filter banks
and vector quantitization,
which in plain English means locality sensitive clustering.
It also provides new insight
into the functioning of neural networks themselves.
For example, with this view,
what a neural network is doing
is performing an input sensitive lookup
for a matching filter
and then computing a simple inner product
between that filter and the input signal.
If that doesn't shed light
on Francois Chalet's characterization of neural networks
as locally sensitive hash tables,
then I don't know what will.
Another cool thing to come out of Randall's work
was a geometrically principled way
of devising regularization penalty terms
which can improve neural network performance
by orthogonalizing the placement
of those latent hyperplane boundaries
to increase their representational power.
In short, looking at neural networks
through this piecewise linear kaleidoscope, if you will,
is opening new avenues of technical understanding
and exploration.
Now, we spoke with Yannick about this as well,
and he commented that he had looked into these polyhedra,
which he called relu cells in his own research.
While he agreed that the boundaries between them
are not technically smooth,
there are so many of them,
combinatorially many possible polyhedra, in fact,
since each can be defined by any combination
of topologically addressable hyperblogics
that can be found in a neural network.
So, if you look at some of the topologically addressable
hyperplane boundaries, there are so many,
Yannick thinks that neural networks
can make them effectively smooth by arranging them
so that they change very little from neighbor to neighbor.
This insight came from his work in adversarial examples,
where the name of the game is to perturbed the input
as little as possible to the nearest polyhedron
with a different class.
He, Randall's work has transformed the way I think
about neural networks.
I know what they're doing at a much deeper level now.
Each layer of a neural network
contributes a new set of hyperplanes,
and the relu's act to toggle the hyperplanes
in an input sensitive way.
Every layer, indeed every neuron in the network,
not just the final layer,
participates in placing flat decision boundaries,
defining a honeycomb of affine cells.
Each input signal will fall into one of these cells
and be transformed by the combined affine transformation
of every neuron it activated.
I also used to think of a single unified latent space
at each layer.
However, it seems obvious that any particular input
will toggle in an input specific way
a set of hyperplanes.
By virtue of the relu's, it does or does not activate.
Therefore, different populations of input samples
will reside in different latent spaces at any one layer
defined by the activated set of hyperplanes.
So it seems to me that instead of having
a unified latent space,
we have input specific latent spaces plural at each layer.
There's also the matter of whether classifiers
are even learning the data manifold at all.
I have my doubts.
They may be learning predictably useful aspects
of the manifold.
However, neural network classifiers are optimized
to find class boundaries,
and any structure between boundaries can be ignored.
The idea that learning boundary manifolds
necessarily means learning intrinsic connection manifolds
is where I'm really struggling now.
I can't convince myself that optimizing for separation
will give the same outcome as optimizing for connection.
For example, training the same neural network
in two different ways, one for classification
and once for decoding or generation,
will result in very different network weights
and latent structure.
It's even more difficult to accept now
that I'm thinking of these manifolds as piecewise linear,
where what the neural network has learned
is combinations of separating hyperplanes
that chop the space into single class polyhedra.
After all, there is nothing in the objective function
which cares about the structure within a polyhedra.
As long as the class is correct, we're good.
Nor is there anything necessarily optimizing
for global manifold structure
beyond just efficient and parsimonious use
of the available parameter space
and whatever prior structure was hard-coded
by humans and the network constraints and topology itself.
Randall commented in the interview
that in high-dimensional spaces,
you can easily separate everything
but your generalization performance might be bad.
So you want to trade off separability with dimensionality.
He thinks the merit of deep neural networks
is being able to find a nonlinear transformation
which retains separating hyperplanes
while reducing dimensionality enough
to confer generalization power.
I mean, I couldn't help but notice
that everything in the machine learning world is linear.
All the popular algorithms are linear
and any nonlinearity is a trick.
I mean, we just apply some nonlinear transformation
to the data before running it through our algorithms
just like how it is in support vector machines
or kernel ridge regression in Gaussian processors.
Deep learning models are no different.
We're just placing these relus all over the input space
to slice it and dice it.
Let's think about this.
What is the simplest mathematical model?
Linear, right?
What's the next most simple one?
Piecewise linear.
So machine learning hasn't even evolved yet
to the second order.
Neural networks in their contemporary usage
only include the minimum possible nonlinearity
of piecewise linear.
If they didn't, an entire neural network model
could be described as one monolithic linear function
with a single transformation matrix.
This view, I think, should give you a cleaner
and analytical view of neural networks.
Now, there are some reasons, in my opinion,
for the tendency towards linear.
I mean, computability is one of the reasons,
but almost every part of mathematics favours linearization,
whether it's Newton's method or calculus
or linear algebra solving PDEs,
plenty of other examples.
So it shouldn't come as a huge surprise.
Not only that, the function space of nonlinear functions
is exponentially larger.
And remember, in machine learning,
the big challenge is to reduce the size
of the approximation class.
So what about boundaries?
Now, the purpose of every value cell
is to define a boundary in the ambient space
to chop off what is no longer required.
All you have is a linear separating hyperplane.
And downstream, you work out the distance
from the hyperplanes that you're on the right side of,
you know, the ones that didn't get chopped off.
So the magic of neural networks
is actually learning what to ignore in the ambient space.
Now, I think a problem with my previous intuition
is that, like most people, I imagine to neural network,
latent space is a bit like a UMAP or a Disney projection
plot, and this leads us to misunderstand their behavior.
The latent space that these examples get projected into
is not homogeneous.
Depending on which cell you fell into
in the input space or the ambient space,
a different affine transformation will be applied,
sending you to a different region of the latent space.
So the latent space is kind of stitched together
like bits of a cosmic jigsaw puzzle in the ambient space.
And then when you run UMAP on the latent,
you see all of the clusters, but you'd
be forgiven for thinking that it was performing
some kind of smooth, diffeomorphic transformation
of the entire input space and successive layers
in the neural network, or even learning the topology
of the data manifold.
I think it's much better to think of neural networks
as quantizing the input space, much like a vector search
engine does using locality-sensitive hashing.
Now, imagine a classification problem on the Cartesian plane
where the upper right and lower left quadrants are blue,
and the upper left and lower right quadrants are orange.
Now, as a minimal example, if we trained a single layer
for neuron-relu neural network to fit this,
it will fit four diagonal hyperplanes,
two of which are reflected versions of the same hyperplane.
Now, by the way, you can play with this
on the TensorFlow playground, which we think is probably
one of the best tools for building strong intuition
on how neural networks work.
Now, what it does is it shows you how the ambient space
is being effectively subdivided by all of these
addressable hyperplanes.
But this quadrant example is unintuitive in the tool
because getting your head around how the relus act together
to combine these hyperplanes to carve up the ambient space
isn't immediately obvious.
Now, when you run the tool on a more complex example,
so for example, a spiral manifold,
you'll now see the artifacts of these hyperplanes everywhere.
When these piecewise linear chops are composed together
in the second layer, we get a decision surface
in the ambient space, which can appear smooth given
enough pieces, but it's actually a composition
of piecewise linear functions.
Now, as you can see, it's just chopping up the input space
in every neuron in the first layer,
each one with a different angle and translation.
And when you get to the second layer,
it's chopping up the space in a more sophisticated way
by composing together the chops from the previous layer.
Now, the first thing to realize here
is that for each test input example,
and every example in its vicinity,
its class projection is definable
with a single affine projection.
There's no continuous morphing going on here.
It's more like a jigsaw puzzle being pieced together
to form many parts of a bigger ambient space.
Now, oftentimes we see negative weights,
which actually allows the hyperplanes to combine
in interesting ways to kind of partially cancel
each other out to allow more complex decision boundaries.
Any hyperplane can be reused by multiple other neurons,
whichever neurons it can reach
via some topologically addressable pathway.
Every hyperplane can be the face
of multiple polyhedra in the ambient space.
This is how they share information.
And it should be obvious by now
that there are two to the N addressable polyhedra
in the ambient space where N is the number of neurons.
Now, we also observed that the neural network
is only capable of slicing and dicing the input space
with flat planes.
The only smooth nonlinearities enter the picture here
where we as data scientists linearize the data
by performing some smooth nonlinear transformation
before it even enters the neural network,
much like is the case
with other machine learning algorithms.
Now, even on the spiral data set example,
the quickest way to make the neural network fit the data
is to effectively linearize the data
by applying a nonlinear transformation
before it even gets in.
Now, we also played around
with this kind of contrived circular manifold data set
where there's a ball of data encompassed
by a circle around it.
And it's possible to do a better job on that
with one relu neuron, right?
But with two nonlinear transformations
of the input data set,
rather than fit this huge bunch
of piecewise linear relus on the ambient data.
I mean, it should be obvious, right?
But this idea that you don't need feature engineering
in neural networks, I think is nonsense.
Now, given how obvious it is that neural networks
are just chopping up the input space
with these piecewise linear functions,
it begs the question,
how could they possibly extrapolate in any meaningful way,
right, especially in the general setting?
And by the way, when we say extrapolation,
we're talking about this more general sense
of continuing the pattern,
not the convex whole notion in the given in the paper.
Regarding these piecewise linear functions,
I remember an impactful moment
for more than 20 years ago in grad school,
a visiting professor who had multiple science
and nature publications,
you know, a pinnacle of academic achievement,
was sharing his insights on how we students
might also extract such papers ourselves from nature.
And he was showing figures from his paper
and they all had two things in common.
First, they depicted very simple relationships
that were previously undiscovered, okay?
And second, all the fitted models were piecewise linear.
He even explicitly commented along the lines
that of course there's some underlying
smooth nonlinear relationship,
but absent a solid theoretical model of that,
which is often going to be the case for new discoveries,
you're better off with simple piecewise linear models.
I guess when in doubt,
Occam's razor always makes straight cuts.
And now it's clear to me
that the successful deep networks of today
are following that advice.
If you've ever wondered why piecewise linear activation
functions are dominating the field,
maybe it's because they've abandoned all pretense
at finding smooth nonlinear models
and are keeping it simple by fitting piecewise linear models,
albeit at machine scale with billions of parameters.
Randall's spline work makes that bit of philosophical insight
brutally clear in my opinion.
It conjures a scene of Darth Vader,
that cybernetic warlord towering over
a growing neural network saying,
embrace your hyperplanes.
Now, you know, I've always been skeptical
and often remind us of the limitations
of today's machine learning.
I'd say things like ML isn't magic learning,
but for a moment,
I too allowed myself to become deluded into thinking
that they are creating some kind of nonlinearity
beyond piecewise linearity.
But relu is by far the dominant activation function
because it stops pretending at anything other
than piecewise linear.
Just stick in a flat boundary threshold and the line.
A neuron puts in a hyperplane
and then lets the rest of the network
chop more as needed.
All the success of neural networks
seems explained by piecewise linear functions.
I also find it intriguing that our own brains,
our own wet neural networks,
have somehow gained access
to smooth nonlinear imagination.
Just look at the laws we've defined in physics.
Many exhibit nonlinear but smooth structure.
On the other hand,
neural networks chop up the input space
with flat boundaries and sharp edges.
How can we possibly expect them to learn or discover
the kinds of smooth relationships
that seem fundamental to science and reality?
All feature engineering and representation learning
and machine learning is about finding these
interpolative representations.
Now, I saw a really interesting example of this
when I was reading Francois Chouelet's book recently
and he spoke of pixel grids of watch faces, right?
Now, they're not interpolative in the ambient space.
If you take the average of two of these images,
you'll just get four faded clock hands
on top of each other, right?
Superposed, not very helpful.
But however, if you represent the hands
with Euclidean coordinates,
then the problem becomes more interpolated
but you're still not all the way there.
The average of two points of interest
doesn't fall on this intended circle manifold
that we're interested in.
You know, it would be interpolated
if we use some relevant distance function
which was some geodesic interpolation.
But you know, anyway,
if we encoded the problem using polar coordinates,
then it becomes linearly interpolated, right?
The problem is solved
and you can generalize perfectly to any new clock face.
But I mean, in that case,
you wouldn't even need a neural network
and the kicker is that we as data scientists
would have to figure this out ourselves, right?
The neural network wouldn't be particularly efficient
at doing so, especially if we, the data scientists,
didn't feed in the relevant nonlinear transformation
before it went into the model.
So anyway, if there exists
a nonlinear transformation of this problem,
then, you know, which makes it interpolated,
then we can say in some sense
that it's got a lower intrinsic dimension.
So crucially, deep learning models,
they can be understood as unconstrained surfaces.
They do have some structure
coming from their architectural priors,
but that structure only serves to restrict the search space
to a smaller space of unconstrained surfaces.
It doesn't provide the kind of inductive prior
that would enable stronger generalization.
A model architecture does not contain a model of the domain
except in the extremely restricted sense
of injecting priors such as, I don't know,
translational equivalents in convolutional neural networks.
If you want to generalize in a more systematic fashion,
you either need a model with a strong inductive prior
or a model which doesn't operate
by empirically slicing up the ambient space,
yeah, like a deep learning model does.
A discrete symbolic program
would be an interesting alternative.
I mean, just imagine the program Y equals X squared.
It generalizes to any arbitrary number, right?
Because it's highly structured.
You can fit it with three training points.
And once fit, it'll generalize to anything,
even things that are outside of the training range.
But the kicker, of course,
is that you wouldn't know which curve to use
if you only saw three points in the first place.
Now, let's think of another example.
And imagine a discrete staircase space.
So in this particular example,
an unconstrained curve gives you complete garbage,
but a structured model with the correct priors,
you know, it can still be a curve,
will generalize in the extrapolation regime.
And this prior basically means that you need to encode it
or linearize it
before it even gets into the neural network.
In both cases, you're making the neural network
work extremely well for a specific thing,
but not very well for generalizing between new tasks.
So anyway, when we say
that deep learning models generalize via interpolation,
what we're really saying is that these models
that we're using today,
that they work well with data
that's within the training distribution of the problem,
right, well, a problem that's intrinsically interpolative,
but they won't generalize systematically to anything else.
You won't generalize when looking at problems
that are not interpolative in nature
and problems that are outside the training distribution.
So what do we talk about
this binary notion of extrapolation?
Well, the problem with the binary convex
whole notion of extrapolation is that
we're promoting this idea
that the moment an example falls epsilon outside the hole,
that the prediction qualitatively changes.
Instead, I think it would be more of a distance question,
which is to say, the further away you get
from this convex hole, the greater the uncertainty.
I mean, it's not like you fall off a cliff
when you step outside the convex hole.
Your approximation of the latent manifold
remains valid for a little while,
although it will quickly degrade
as you move further outside,
unless your model has some strong structural prior
that matches the regularity of that latent manifold.
So this brings up an important point, right?
The relevant question is not whether you're inside
or outside the convex hole.
It's how far you are from the nearest training data points.
If you're outside the convex hole,
but you remain close to the training data,
then your model remains an accurate approximation
of the latent manifold.
And even if you're inside the convex hole,
you're in a region that wasn't densely sampled
at training time, right?
Where there's no nearby training points.
Your unconstrained model
may not accurately approximate the latent manifold.
So inside or outside doesn't really tell you very much.
It's all about the proximity.
You're performing local generalization
and the quality of your ability to generalize
to new situations depends entirely
on their proximity to known situations.
However, if you're dealing with training data
that is densely sampled
within a certain region of the problem space,
then the question, can I generalize here
becomes approximately equivalent to,
is this inside the convex hole or outside?
Now, the whole point of feature engineering
is to make data sets interpolative.
Either us data scientists design the features by hand
or neural networks learn them as part of the training process
in big air quotes for the podcast listeners.
So, I think when folks make the argument
that machine learning models generalize
via interpolation or that computer vision data sets,
that they're interpolative,
what they mean is that there's an encoding space
for which the problem becomes interpolative
or there exists a non-Euclidean pairwise distance function
between the instances that makes the problem interpolative,
which is basically the same thing.
So, if it's possible to do this,
then you can say that the problem is intrinsically interpolative
but your current representation of the problem is not.
So the key endeavor in machine learning
is to find better representations,
representations which reveal the interpolative nature
of the problem.
Now, you might just cynically write off this paper
as being a trivial finding, right?
You know, it seems trivial that the pixel space
is not linearly interpolatable.
Something that we've known about for decades.
I mean, that's why computer vision engineers in the 1980s
would write feature detectors
to create an interpolative space
for machine learning algorithms to work on, right?
If you take any two images
and you interpolate between them, what do you get?
You just get a faded copy of both.
Everyone in the field has known about this for decades, right?
That examples are not interpolative
in their original encoding space or their pixel space.
I think the most important thing though
is that this paper also shows
that the models are not interpolative
in their latent space either.
That's a shocker.
Now, what does Randall say about all of this?
Well, Randall Bellisterio thinks that the steelman
for this interpolation argument would be,
well, what if you had very low dimensional,
kind of like approximation of your data
with very few factors of variation,
which could be easily linearized in the latent space,
then it might be interpolative.
Randall thinks that the very concept of interpolation,
I mean, it was defined about 50 years ago
to describe these very small models
with very few factors of variation.
And it's just not relevant today, right?
They showed in their paper
that even in the relatively small latent space
on a popular neural network classifier,
interpolation does not occur.
He thinks that most people think of interpolation
by kind of conceptualizing their data
into a few important latent factors.
You might have dogs, for example,
and they might exhibit a latent color,
and a new color might be an interpolation
between observed colors.
But these guys think that we need to have
an entirely new definition of interpolation.
I think the main purpose behind their paper,
according to them, is to show that even though
the intuition that people have of interpolation
works well in low dimensions,
it falls down completely flat on its face
in higher dimensions.
Actually, the probability of your test data
being in the convex hull of your training data
is near zero past a certain number of dimensions.
So the current definition of interpolation,
which is to say like this notion
of convex hull membership,
it's too rigid to think about how interpolation works
in neural networks.
So Randall thinks that any new definition of extrapolation
should be directly linked to generalization itself.
This is what they're trying to get at.
This is a clip from our show with Professor Max Welling.
The first thing I wanna say,
there is no machine learning without assumptions.
It just basically, you have to interpolate between the dots,
and to interpolate means that you have to make assumptions
on smoothness or something like that.
So the machine learning doesn't exist without assumptions.
I think that's very clear.
But clearly it's a dial, right?
So you can have, on the one end,
you can have problems with a huge amount of data.
It has to be available clearly.
And there you can dial down your inductive biases.
You can basically say that the data do most of the work
in some sense.
Now, I think it's a good thing to build intuition
about machine learning principally
as an interpolation problem, right?
So we're given the training data,
and we need to cleverly interpolate
between the training examples to reason
about the statistical context of the test examples.
This is what machine learning is in a nutshell.
And actually, a researcher friend of mine
from MetaAI Research in Silicon Valley,
a guy called Dr. Thomas Lukes,
he published a paper a couple of years ago
called Interpolation of Sparse High-Dimensional Data.
And he showed that it was indeed possible
to perform competitively with multi-layer perceptrons
for a regression problem using pure play interpolation methods
like Voronoi and Dolornoi triangulation and spline methods.
I mean, think of creating simplexes
of the nearest neighbors around a training data
and then averaging the results together.
This simple method works remarkably well
up to about 30 dimensions.
I mean, obviously eventually it gets deranged
by the curse of dimensionality,
but as we'll discover slightly later,
neural networks might have a slight advantage
over pure play simplex interpolation
just because they work principally
by figuring out boundaries of the input space to exclude.
So, you know, if you're on the zero side of the ReLU, that is.
So that helps a lot in high dimensions
when you have a poor sampling density
in a particular region of the input space.
But anyway, if you do principally think of machine learning
as being an interpolation method,
it raises some interesting ideas.
You know, Thomas said to me that the crux of the problem
is that we're trying to approximate a function
that has spatially unique behavior
in more than about 20 or so dimensions.
Like in that case, it's hopeless.
There's no question about it.
He says that you can increase the data density,
you know, like in 100 dimensions,
but to get a grid with 10 points on a side, you know,
which is to say 10 to the power of 100,
there aren't enough protons in the universe
that could convexly cover it.
So, and Thomas concluded by saying,
what this means is that everything that we do successfully
approximate now with millions or billions
or even trillions of data points,
which accounts for a lot of the successes
in machine learning, these data sets
only have spatially novel behavior in very few dimensions,
right, or a varying gradient in very few dimensions,
16 or fewer.
If we suppose that the true function has a varying gradient
in more than that many dimensions,
there's simply not enough data in the world to approximate it.
The mathematics here is unequivocal.
So, what actually happens when the test examples
outside the convex hull of the training data?
What happens when we're in an extrapolative regime?
Well, it's increasingly impossible in high dimensions
for the training set to be statistically representative
of the test set.
Test set instances will often distinctly differ
from anything that we've seen during training.
In machine learning,
the test set will never fully characterize
the problem that we're interested in.
So, in high dimensional settings,
features are often linearly correlated as well,
which makes it challenging to know what information to use
and what to discard.
This is another fundamental problem of machine learning.
Many machine learning approaches depend on modeling
these local statistical relationships
between the training samples,
but in high dimensions,
the probability of a new test example
being in the convex hull of the training data
goes to zero extremely quickly.
So, making machine learning models work
in an extrapolative regime necessitates the introduction
of inductive biases,
which are a way of adding a kind of enforced smoothness
to the model predictions.
So, if you hit the right bias,
then it can be beneficial.
If you impose the wrong bias,
then it's gonna hurt you.
And this is a well-known trade-off.
So, of course, the whole endeavor of machine learning
is defining the right inductive biases
and leaving whatever you don't know to the data,
and then basically learning to focus your models
on the data that you're actually seeing.
This is how we stop models
from fitting the noise in the data
or going completely haywire
outside the convex hull of the training data.
Extrapolation requires that sensible answers
are given for all of the elements of the input space,
even in regions not seen during training.
And in particular, those within the convex hull
of, let's say, some hypothetical,
infinitely-sampled training data set.
Now, what we do in neural networks
and in many other machine learning algorithms,
for that matter, is we just take a bunch
of parameterized basis functions and we stack them
and we fit them to our training data
until it's well-described.
The most important considerations for interpolation
are the smoothness and robustness of these models.
We want our models to produce gradual changes
between the training points.
Points outside the training set
should be handled with great care,
but what exactly that means depends greatly on the problem.
In order to even know that we're in an extrapolative regime,
the basis functions must be based
near to the training examples.
Typically, neural networks do not behave sensibly
in regions where there's no training examples, right,
because their learned basis functions
have been localized around the training data
in the ambient space.
It's also worth noting that extrapolation
and interpolation are two completely different regimes.
Optimizing for one, typically,
means being worse at the other.
Now Randall said that the boundary view
of neural networks is very clear
in the discriminative setting.
You only need interpolation to try
and understand how neural networks work
in the generative setting.
This is a key point.
In the generative setting, when you interpolate the latent,
we see at first glance what looks
like geometric morphing, right?
Looking at this, you'd be forgiven for thinking
that we're traversing some smooth latent manifold,
but on closer inspection, it's not geometric
or diffeomorphic at all.
It's more of a fuzzy, high-resolution fade-in.
Now, one commonly used visual argument
for the manifold hypothesis
is the MNIST digit interpolation in a generative model.
Our own investigation shows that there's a degree
of cherry picking in some of the visual examples
used to demonstrate this.
It's not actually hard to stumble across cases
where purported manifold interpolation is no better
than ambient-space linear interpolation.
There are many examples where latent-space interpolation
gives superposed and fuzzy intermediate representations
and even crisp images, which are unrecognizable.
The blur and cutting and gluing,
which is apparent on these images,
show that it's definitely not a diffeomorphism
in the ambient space.
And even though the interpolation path
is continuous in the latent space,
it's questionable whether that path is semantically relevant
to the classification task at hand.
I mean, just think of the mean value theorem.
It tells you that if you take the value
of any continuous function at two points, right,
you can also find a point between them
where the function hits the average of those values.
So in other words,
any continuous function produces a manifold,
but that doesn't tell you anything interesting
about the function beyond what we already know, right,
that it's continuous.
Now, I'm not so sure that these manifolds are even smooth.
When we're talking about smoothness,
we're only talking about local smoothness
and inside the polyhedrus convex hole.
It's linear, right?
But there's no smooth surfaces or manifolds
anywhere to be seen.
For any particular node in a neural network,
it's actually a linear function of a subset
of the upstream linear functions, right?
Very much like a decision tree,
but with information sharing.
Now, another often used visual aid
is this idea that a neural network
is effectively uncrumpling and smoothing the ambient space,
much like you would do with a sheet of paper
with successive transformations
in the layers of the neural network.
Keith and I previously agreed with this view of neural networks
as progressively kind of flattening out the paper,
but it now seems to us like they may instead
be progressively inserting planes
aligned to the facets of the paper ball
to chop out locally affine polyhedra or cells
to cover the paper's polyhedra.
So for us, this is an entirely new way
to think about neural networks, okay?
So in a way, this is a new parlor trick, right?
Do you remember that thing, the game of life,
Conway's game of life?
It looks like the shapes are smoothly morphing,
but they're actually toggling pixels with discrete rules.
Gary Marcus talks about the parlor trick of intelligence,
but isn't it ironic that there are more parlor tricks
going on than most people realize?
Namely, that rather than doing smooth geometric morphing
via interpolation, these networks are actually chopping up
and composing linear polyhedra.
Now, if you interpolate between two latent classes,
it might traverse several polyhedra in the intermediate space.
Along the way, it would pick up characteristics
from all of those polyhedra.
Some of the black regions are impossible regions,
so it's not possible to get there from the latent space,
which is very, very interesting.
You only see the illusion of continuous morphing,
where the neighboring cells are very small and very similar.
This is Professor Michael Bronstein.
I like to think of geometric deep learning
as not a single method or architecture, but as a mindset.
It's a way of looking at machine learning problems
from the first principles of symmetry and invariance.
And symmetry is a key idea that underpins our physical world
and the data that is created by physical processes.
And accounting for this structure
allows us to beat the course of dimensionality
in machine learning problems.
So what is the manifold hypothesis?
Well, natural data falls on smooth manifolds.
The manifold hypothesis states that real-world
high-dimensional data lie on low-dimensional manifolds,
embedded in the high-dimensional space.
When people invoke this hypothesis
in a machine learning context,
they're generally suggesting that neural networks
are actually learning this data manifold,
which we now find quite hard to believe, frankly.
At best, we think they're learning
some approximate aspects of it.
So essentially, all machine learning problems
that we need to deal with nowadays
are extremely highly dimensional.
Even basic image problems live in thousands
or even millions of dimensions.
Now, I think most people have this intuition
of convex-hole membership, which is to say, in two dimensions.
As you sample more and more training data,
the convex-hole eventually fills the entire space.
But the kicker is that in higher dimensions,
the space is so vast, this will never happen.
High-dimensional learning is impossible
due to the curse of dimensionality.
It only works if we make some very strong assumptions
about the regularities in the space of functions
that we need to search through.
The classical assumptions that we make
in machine learning are no longer appropriate.
So in general, learning in high dimensions is intractable.
The number of samples grows exponentially
with the number of dimensions.
And the curse of dimensionality refers
to the various phenomena that arise
when analyzing and organizing data
in high-dimensional spaces that do not occur
in low-dimensional settings,
such as the three-dimensional physical reality
of everyday experience.
Now, the common theme of these problems
is that when the dimensionality increases,
the volume of the space increases so fast
that the available data becomes sparse.
And this sparsity is problematic for any method
that requires statistical significance.
In order to obtain a statistically sound
and reliable result, the amount of data needed
to support the result grows exponentially
with the dimensionality.
So the curse of dimensionality in a nutshell
is that the probability of a new data point
being inside the convex hull of your training data
decreases exponentially with the number of dimensions.
As an example, to estimate a standard normal density
in 10 dimensions with a relative mean square error
of 0.1 using an efficient non-parametric technique
would require more than 800,000 samples.
Now, in the last show,
we discussed geometric deep learning in great detail
and in some sense extrapolation
in the curse of dimensionality are analogous.
The reason that these folks wanted
to combat the curse of dimensionality
was they wanted to build models
which could extrapolate outside the training range
using geometrical priors.
I think it's possible to build understanding
of the curse of dimensionality
through visual analogy to familiar lower dimensional shapes
such as circles, squares, balls and cubes.
Imagine perfectly sampling an entire space
with a regular grid.
This would partition space into squares or cubes
or hyper cubes with a sample in the center of each.
Now imagine around each sample,
a disc or ball or hyper ball
representing that point's region of nearness or influence.
And let's ask how much of the total volume
of that point's grid cell is actually near the sample?
The answer is a fraction
which diminishes faster than exponentially
with increasing dimension.
First think of the 2D case,
a disc or two ball with diameter one
inscribed in a square or two cube with sides of length one.
In each corner we of course have a dart shaped chunk
that isn't covered by the disc.
A trick to think about this
which I think will help in higher dimensions
is to imagine scanning a line segment
along one dimension from side to side.
At the edge of the square,
none of the segment is covered by the disc.
At the very center,
the entire segment is covered by the disc.
And then as it scans towards the other edge of the square,
the covered section begins shrinking
and then rapidly falls to zero.
The total coverage then is just the sum over that scan.
In two dimensions,
that is just the area of a disc with diameter one
and is about 79%.
Extending to three dimensions,
imagine a ball in a cube
and we scan a square from one face through the ball
to the opposite face.
As it passes the center,
we have our familiar inscribed disc in a square.
This is already missing the corner darts
and as we scan towards the edge of the cube,
the darts expand and surround the disc
as it shrinks to zero.
So by adding a third dimension,
we've lost even more coverage
and the volume of a diameter one ball is only 52%.
In fact, if we continue on to higher dimension,
the volume of a diameter one hyperball
decays faster than exponential,
factorially fast in fact.
The volume decays so fast
that even if we allowed for sampling
with the best possible densely packed balls,
theoretical work on hypersphere packing
tells us that the volume occupied
by optimally packed hyperballs
would still decay at least exponentially
with increasing dimensions.
This is the curse.
As dimensionality grows,
space expands exponentially,
points grow further apart
and the volume near each point vanishes.
In this episode, our guests argue this curse
dunes traditional concepts of interpolation,
even if we allow for the high dimensional
transformative power of deep neural networks.
Yeah, so the course of dimensionality,
it refers generally to the inability of algorithms
to keep certifying certain performance
as the data becomes more complex
and data becoming more complex here
means that you have more and more dimensions,
more and more pixels.
And so this inability of like scaling,
basically it's like it really says
that if I scale up the input,
my algorithm is gonna have more and more trouble
to keep the pace.
And so this curse can take different flavors, right?
So this curse might have like a statistical reason
in the sense that as I make my input space bigger,
there would be many, many, many much exponentially more
functions, real functions out there
that would explain the training set
that would basically pass through the training points.
And so the more dimensions I add,
the more uncertainty I have about the true function, right?
So I would need more and more training samples
to keep the pace.
This curse can also be from the approximation side, right?
So in the sense that the number of neurons
that I'm considering to approximate my target function,
I might need to keep adding more and more neurons
at the rate that is exponentially in dimension.
And the curse can also be from the computational side, right?
The sense that if I keep adding parameters
and parameters to my training model,
I might have to optimize to solve an optimization problem
that becomes exponentially harder.
And so you can see that you are basically bombarded
by three different, by all angles.
And so an algorithm like here in the context
of statistical learning or learning theory, if you want,
having a kind of a theorem that would say,
yes, I can promise you that you can learn,
you need to actually solve these three problems at once, right?
You need to be able to say that in their condition
that you're studying, you have an algorithm
that it does not suffer from approximation
nor statistical nor computational crisis.
So as you can imagine, it's very hard, right?
Today is a big day here at Machine Learning Street Talk.
We have invited one of the Godfathers
of deep learning on the show,
none other than Professor Yan Le-Kun.
Machine learning royalty, he has met the chiefs AI scientists
and a Turing Award winner,
which I don't have to tell you is a big deal.
Wiredly recognized as the Nobel Prize of Computing.
Yan Le-Kun was born in the suburbs of Gay Pari in the 1960s.
He received his PhD in computer science
from the modern day Sir Vaughan University in 1987,
during which he proposed an early form of back propagation,
which is of course the backbone
for training all neural networks.
Whilst he was at Bell Labs,
he invented convolutional neural networks,
which again are the backbone
of most major deep learning architectures in production today.
He has been at New York University since 2003,
where he is still the professor of computer science
and neural science.
Apart from being perhaps the most known researcher
and main ambassador for deep learning,
he has championed self-supervised learning
and energy-based models.
In 2013, he created the hugely prestigious ICLR conference
with Yoshua Ben-Gio.
A lover of the green screen IC.
Well, normally,
when I use Zoom, I put a substituted background.
Well, Tim could do it for you and post, if you want.
If you want.
Well, it's never a very good cut-out.
I did hack together some Python code to do it,
but there's no substitute for the real thing.
Yeah, I attempted to do this.
I'm running Linux, so I attempted to do this
by making a fake video driver,
but introduces a little bit of delay, so.
Professor Lacune, it's an absolute honour
to have you on the show.
When I first started MLST with Yanakin and Keith and Connor,
we were discussing how long it might take
to finally get the main man himself on the show.
You've been a huge inspiration to all of us here on MLST
and also you've inspired millions around the world
to embark on successful careers in data science
and to dream about what might be possible
with artificial intelligence.
Now, we've read your recent paper,
Learning in High Dimensions Always Amounts to Extrapolation,
which you co-authored with Randall,
Belastriro and Jerome Pesente.
Let's get straight into it.
So, we were wondering,
why did you write this paper, basically?
We were thinking whether interpolation and extrapolation
is a useful dichotomy.
I mean, at the end of the day,
we measure performance of learning methods
with accepted metrics of predicted performance,
such as accuracy.
So, suppose everyone adopts your linear convention,
which is a convex whole membership,
and concludes that high-dimensional learning
is always extrapolation.
How is that useful?
What do we do with this knowledge?
And vice versa, suppose that everyone adopts
a different definition
and concludes that learning is always interpolation.
What difference would that make
in machine learning research and practice?
So, in sum, why is it important to distinguish
whether we're predicting by interpolation or extrapolation?
Okay, very interesting question.
So, first of all,
this may be the first time you have me as an interviewee,
but I've watched a bunch of your videos.
So, you've had me as an audience, at least.
Which I find really interesting, actually.
So, the answer to your question is,
the whole point of the paper is to show
that this notion of interpolation versus extrapolation
is not useful, essentially.
That, you know, there's been a lot of people
who've been sort of saying there's a limitation
to deep learning, let's say,
or machine learning more generally,
because it's obvious that those things
basically do curve fitting,
and that only works for interpolation
and not for extrapolation.
And that kind of dismissal always sounded wrong to me
because in high-dimension,
your geometry in high-dimension is very different
from the intuition that we form
with curve fitting and low-dimension, right?
So, part of the motivation for this paper
is it was to kind of, you know,
perhaps help some people gain some intuition
about what really is taking place
in machine learning and high-dimension.
And also kind of, you know,
dispel the myth, essentially,
that machine learning and deep learning, in particular,
only does interpolation.
Of course, it depends a lot
on your definition of interpolation.
So here we adopted definition,
which is, you know, an obvious and simple generalization
of interpolation in low-dimension,
which is that, you know,
you interpolate when a point is in between the points
you already know.
And the generalization of this in high-dimension
is, you know, you interpolate when a new point
is inside the convex hull of the points
that you already know.
Now, what you have to realize, of course,
is that in high-dimension,
the volume of that space is actually tiny
compared to the overall volume of space
that would be filled, you know, in high-dimension.
And so that's kind of the intuition behind this paper
that, you know, any new point,
regardless of how you sample it to some extent,
any kind of reasonable ways to sample points,
you know, in high-dimension,
new points are always going to be outside the convex hull,
almost always going to be outside the convex hull
of existing points with that definition.
Interesting.
Now, there's a second part to your question.
It's that, you know,
is there a more sensible definition
of interpolation and extrapolation?
And the answer is probably yes, you know,
the paper doesn't address it,
but there are other definitions of hulls,
if you want, they're not necessarily convex hulls,
or they're not necessarily the usual type of convex hull.
So for example, there is a definition of a hull
for a cloud of points
that would be the smallest hyperboloid,
paraboloid, or ellipsoid, I should say, actually,
the smallest ellipsoid that contains all the points, right?
So some points are going to be on the surface
of the ellipsoid, but most of them are going to be inside.
And for this type of,
and now your definition of interpolation is that,
is a new point likely to be inside the ellipsoid
of a previous point or outside?
And the answer to that is probably very different
from the one in the paper,
in the sense that it's very likely
for a lot of natural data,
new points are likely to be inside
the containing ellipsoid.
So it very much depends on what you mean,
but it's just that the notion of interpolation
in high dimensional space or intuition
are kind of biased toward low dimension
and we have to be very careful what we say.
So that was the main thing.
And we have a bunch of tons of mathematicians
that worked on these questions for many years
and there's a whole bunch of theorems about this
that we survey in the paper.
Very interesting.
Well, we'll dig more into that in a second,
but folks like Gary Marcus make the case
that deep learning models don't reason and only interpolate.
Now, I know you disagree vehemently,
but my intuition is that reasoning and extrapolation
are somewhat synonymous.
Is it, by arguing that deep learning models extrapolate,
are you kind of making the argument that they reason as well?
Because that would make a strong case
that deep learning models could scout
artificial general intelligence.
Okay, so no, the short answer is no.
But there are important questions in there.
First of all, what's our definition of reasoning?
What is the process by which we elaborate models
and is there a qualitative difference
between a models that merely performs curve feeding
as we normally know it
and a model that has a,
let's say to adopt a terminology that others have proposed
that models that establish sort of a causal model
of the data you're observing,
which can be the basis for reasoning
and things like that, right?
And the answer to this is probably no.
There is a difference, of course,
but is it an essential qualitative difference?
I'm not entirely sure.
And then there is the argument,
if there is a qualitative difference,
which I'm not sure about,
would this qualitative difference be in the form of
fundamentally different things from deep learning,
things that are like discrete symbolic reasoning
or things of that type?
And to that, my answer is clearly no.
I do not believe that's the case.
So I think reasoning is certainly,
I always list in my talks,
the ability, giving the ability to learning machines
to reason is one of the main challenges
of the next decade or perhaps couple of decades in AI.
So I'm clearly aware of the fact
that they don't do this very well at the moment.
The big question I think is,
how do we get machines to reason
in ways that are compatible with deep learning?
Because most of the criticism
that I've heard from Gary Marcus and several others
towards deep learning is not a criticism
towards deep learning.
It's a criticism towards supervised learning.
And I agree with them.
Supervised learning sucks.
I mean, it's very limited in the sense that
you can train machines to do very specific tasks.
And because they're trying to do very specific tasks,
they're going to use all the biases that are in the data
to do that task.
And if you try to get outside of that task,
they're not gonna perform very well.
That's the limitation of supervised learning.
It has absolutely nothing to do with deep learning.
So regardless of which learning techniques you're gonna use,
you're gonna have that problem.
It's a problem with supervised learning.
So I take exception with the confusion
between deep learning and supervised learning.
Now, of course today, most of supervised learning
is deep learning,
but it's the limitation of supervised learning.
And as you probably know,
I've been a very strong advocate of self-supervised learning,
sort of moving away from task-specific supervised learning
towards more kind of generic learning
followed by specialization,
using supervised or reinforcement learning.
And in that, I've kind of followed the path of Jeff Hinton,
who's been basically advocating for this for 40 years now.
And for me, it's less time.
I disagreed with him originally and changed my mind
about 20 years ago.
So that's the story, really.
It's awesome that you have the ability to change your mind
and admit that this is a good thing.
We run into so many people that are just,
they don't wanna change their mind no matter what.
It's seen as a bad thing, which it isn't at all.
No, I mean, I think the essence of being a scientist
is to be able to change your mind in the face of evidence.
You cannot be a scientist if you have preconceived ideas.
On the other hand, I've also been known
to hold very tight to ideas that I thought were true
in the face of considerable differing opinion
from my dear colleague.
So it also helps to have deeply held convictions sometimes.
So to this notion of interpolation,
I think we've also, and you mentioned this, right?
It depends on your definition.
And we've also had a little bit of the feeling
that people might be talking past one another
when they criticize, can it interpolate?
Can't it interpolate?
And we've come up with this example
of I give a classifier a dog.
And that dog is like the most doggy dog I've ever seen.
Like it's like such a dog.
It's more dog than any dog I had in the training dataset.
So clearly that dog is like outside of the training distribution,
outside of the convex hull in any space,
like be that the original space, be that the latent space.
It's like, all that matters is that it's at the correct side
of the classifying hyperplane.
So that would not be contained on sort of in the convex hull.
It would not be contained in the smallest ellipsoid
and so on.
So do you think when people talk about interpolation,
they might be talking about something maybe different?
Because you can also make the example
if I take the convex hull of all the training data points
and I take a point in the middle of it,
it's the neural network going to be pretty bad at it, right?
That's going to be like a messy blur of pixels
and it's not going to be very, very good at it.
So could you maybe make the strongest argument you could
for people saying neural networks just interpolate,
but what notion of interpolation would you substitute?
Would it be like, oh, they just nearest neighbor classify
or if you had to give the best shot at sort of doing the,
oh, neural networks just do something, what would it be?
I wouldn't give any answer of this type
because the answer would very heavily depend on the architecture.
So for example, if most of the layers in a neural net,
I mean, you can use weighted sums and sigmoids
or weighted sums and values, right?
And that effectively performs a classification separating
the input into kind of half, two halves with the hyperplanes.
But you could also use a Euclidean distance unit.
So Euclidean distance unit computes the distance
of the input vector to a weight vector,
which is not a weight vector anymore,
and then passes it through some sort of decreasing function
like an exponential.
And what that gives you is a Gaussian bump in input space
where the activity will be high
if the input is close to the vector and low if it's not.
When you have an attention layer,
where you have a whole bunch of those kind of vector comparison
where the vectors are normalized and then you do a softmax,
you're basically doing sort of a multinomial version of this.
And this is using transformers, right?
And in all kinds of architectures these days.
So those things that compare vectors with each other
and only react to the two vectors nearby
do a sort of glorified nearest neighbor
with some interpolation.
By the way, most kernel-based methods do this, right?
Kernel methods basically are one layer of such PRS comparisons.
I mean comparisons of the input with the training samples.
And then you pass that through some function,
some response function.
So Gaussian SVMs, for example, are the perfect example of this.
And then you take those cores and you compute a linear combination.
That's glorified interpolating nearest neighbor.
To some extent, transformers do this as well.
Transformers are the basic module of a transformer.
It's basically an associative memory that compares the incoming vector
to a bunch of keys and then gives you an answer
that is some linear combination of values, right?
So that is a good idea.
It learns fast.
There used to be a whole series of models that were now forgotten,
that are now forgotten in the 90s called RBF networks.
So an RBF network was basically a two-layer neural net.
Well, the first layer was very much like an SVM.
This was before SVMs that, again,
had riddle-based functions, responses, right?
Comparing an input to vectors and passing it to an exponential
or something like this.
And then you would initialize the first layer.
You could train it with backprop,
but it would get stuck in local minima.
So that wasn't a good idea.
You had to initialize the first layer with something like k-means
or mixer or Gaussian or something like that.
And then you could either just train the second layer
or fine-tune the entire thing with backprop.
And that worked pretty well.
It was actually a fairly fast learner.
They were faster than the neural nets.
So those things, for those things,
the answer to your question is of one type.
They're basically doing interpolation with kernels.
And it's very much like a smooth version of nearest neighbors.
But then for classical neural nets,
where you have either a hyperboleic tantrum,
nonlinearity, or a value, or something of that type,
something with a kink in it, or two kinks,
the answer is different.
There it's a whole cone of response
that will produce a positive response versus not
if you take a combination of units, right?
So does it make sense to talk about interpolation
in that kind of geometry?
I'm not sure.
I think people should just not use
the word interpolation for that situation.
But the fact that kind of things in a neural net,
the response of a neural net actually
are kind of half-spaced, extend beyond the training points,
perhaps has something to do with the fact
that they do extrapolate in certain ways
that may or may not be relevant for the problem,
but they do extrapolate.
Well, so we've taught some about the interpolation
versus extrapolation versus what was in the paper,
because the paper is somewhat a rigid definition.
It's like there's this convex hull,
and if you're inside, it's interpolation.
If you're outside, it's extrapolation.
And you talked about maybe we could use a ball and ellipsoid
instead, but there's kind of a key thing there,
which is that it's going across all dimensions.
So you're inside the convex hull.
It's a necessary condition that on every single dimension,
your sample data point falls within the range
of the training data.
We could kind of go the opposite extreme
and say that you're interpolating if any dimension falls
within the training domain, or rather,
you're extrapolating only if every single dimension falls
outside the training range.
And both of those are kind of these exponential extremes.
And it would seem like the truth is maybe somewhere
in between, like there's a subset of dimensions
that might be salient for any particular data point.
So why are we kind of using this very exponential extreme
definition?
It was still exponential.
Even if you, you know, you can try
to divide the set of dimensions into the ones that are useful
and the ones that are just going to use parameters that
are useless, but that are not relevant for the task at hand.
But first of all, that task is very difficult.
I mean, basically, the entire machine learning problem
is exactly that, trying to figure out
what information in the input is relevant for the task
and what part should basically be considered
as noise or useless parameters.
So solving that problem is solving the machine learning
problem, first of all.
Second of all, what we show in the paper is that regardless,
so the experiment that Randall did, as a matter of fact,
is train a ResNet 18 or 50 or whatever on a ResNet
or CIFAR or MNIST.
And then take the output representation
and see whether this sort of exponential growth of number
of data points to stay within the interpolation regime
still exists.
And it still exists.
It's just that the dimension now, instead of being
the input dimension of the data, which may be very large,
is the dimension of the embedding.
But as soon as the dimension of the embedding
is larger than 20 or so, the number of training samples
you would need to stay within the interpolation regime
is already going to be very large.
2 to the 20 is a large number.
So there is another experiment that
shows that if your entire data set is
contained within a linear subspace,
so the ambient space may be of dimension 100,
but the entire data set is within a linear subspace
of dimension 4, then what matters
is the dimension 4, not the dimension 100.
So automatically, that convex hull process, what
matters to it is not the dimension of the input space,
it's the dimension of the linear subspace that
contains all the points.
Right, but I think what I'm saying though
is that you can invert the definition so that whether or not
you're extrapolating just becomes 1 minus,
whether or not you're interpolating, or vice versa.
And so you can wind up with an equally extreme definition
that concludes everything is interpolation.
So I'm just wondering, it seems like there should be a balance.
And this kind of gets to what you said earlier.
Is there a more useful definition of interpolation
versus extrapolation that could have some utility
for machine learning?
I mean, it's not clear you would get much out of it.
I give you a potential candidate,
which is whether the points are contained in a ellipsoid that
contains all the points, you could make it a sphere.
A sphere would be, of course, bigger volume
because the data doesn't necessarily
have the same radius in all dimensions.
But I think the result would be fairly similar for both,
for both definitions that in that case,
things would be mostly interpolation.
But that would be kind of a weird definition
of interpolation in the sense that it
would rely on a false intuition about high dimensional
geometry.
OK, calling this interpolation basically
would mean that you have a completely wrong idea about how
things behave in high dimension, right,
about geometry in high dimension.
I'm trying to get some intuition about this
because we spoke to Professor Bronstein and his friends,
including Joanne Bruner, and they're
talking about geometric learning and their approach
to defeating the curse of dimensionality
is finding geometric priors to reduce the hypothesis space,
which is quite interesting.
A lot of this is about our intuitions of interpolation.
Because if I take an autoencoder and I train it on MNIST
and I start interpolating between the train
or the test examples, it is learning this continuous
geometric morphing.
And that is what people's intuition is about interpolation.
I know you would say that's extrapolation, right?
Yeah, no, I think it would be a type of interpolation,
but it would be some sort of geodesic interpolation,
interpolation on a manifold, right?
I mean, so certainly if you have some idea
about the structure of the data manifold,
you can interpolate within that manifold
without making big mistakes.
But then you're back to the original problem
of machine learning.
What is that manifold?
How do you learn that manifold?
That's one of the essential problems of machine learning.
And learning the structure of a data manifold
is a much more complex problem than learning a task
of classifying objects on that manifold.
For example, classifying points on that manifold.
So there is this old adage by which
I used to be a big fan of, which is why I was
disagreeing with Jeff Hinton about the usefulness
of unsupervised learning.
Because if you have a task at hand for which you have data
that you can use to train a supervised system,
why would you go to the trouble of pre-training a system
in unsupervised mode knowing that the unsupervised learning
problem is considerably more complicated both from every
aspect you can think of, certainly
from the theoretical point of view.
Vladimir Vapnik actually has kind of a similar opinion.
One of the few things that he and I agree on,
or agreed on, at least, which is why would you
want to solve a more complex problem than you have to?
But of course, that forgets the fact
that you don't want to solve a single problem.
You want to take advantage of multiple problems
and whatever data you have available at your disposal
to prepare for learning a task.
And we have access to considerably more unlabeled data
than we have access to labeled data.
And therefore, why not use unlabeled data in large quantity
to pre-train very large neural nets
so that we can function them for the tasks
that we are interested in?
So that's the whole idea of self-supervised learning.
And of course, we all know that that kind of strategy has
been unbelievably successful in natural language
processing with denoising autoencoder or master
autoencoder or bird-style training of transformers
followed by a supervised phase.
That success has not yet translated
in the domain of vision, although I'm sort of predicting
that it will happen very soon.
But I mean, there's a lot of interesting avenues there
and recent progress.
And then there is the cake analogy, right?
The fact that any sample in sort of a self-supervised context
give you way more information than a supervised sample,
the label from supervised learning,
a fortiori reinforcement for the context of reinforcement.
Absolutely.
We interviewed Ishan.
So we've done a show on self-supervised learning.
We're huge fans of self-supervised learning.
And I know your vision is to get to these latent predictive
models to solve that problem.
That's something else I changed my mind on the last two years.
This may be outside the scope of this particular interview.
But yeah, there's basically two major architectures, right,
for self-supervised learning, particularly
in the context of vision.
And the main characteristic of both of them
is the fact that it can handle multimodal prediction.
So if you have a system, let's say you
want to do video prediction or something like that.
So you have a piece of a video clip.
You want to predict the next video clip.
Or you just want a machine that tells you
whether a proposed video clip continuation clip is
a good continuation of the previous one.
You don't want it to predict.
You just want it to tell you whether it's a good one.
So you have two architectures.
The first one is a latent variable predictive architecture
that predicts the next video clip.
And of course, you have to parameterize the prediction
with a latent variable, because there are multiple predictions
that are plausible.
So I used to be a big fan of that.
And about two years ago, I changed my mind.
Maybe a year and a half.
The other approach is something I played with in the early 90s,
came up with some of the early models for this.
And it's called a joint embedding architecture,
where you have the first and the second video clip both going
through an neural net.
And then what you're doing is you're
training the system so that the representation of the second
video clip is easily predictable from the representation
of the first one.
So there you're not predicting pixels.
You're predicting representations.
But you still have a system that can tell you
here is a video clip and the second one
tell me if they are compatible, if one
is a good continuation of the other.
The main reason why I stayed away from those architectures,
because I knew that you had to use, in the past,
you had to use contrastive learning.
You had to have pairs of things that are compatible,
as well as pairs of things that are incompatible.
And in high dimension, there's just too many ways
two things can be incompatible.
And so that was going to do to failure.
And I played with this back in the suit
to be called Siamese networks at a paper in 1992, 1993,
on doing signature verification using those techniques.
Jeff Hinton had a slightly different method
based on maximizing mutual information with his former
student, Sue Becker.
But then in the last two years, we've
had methods that are non-contrastive
that allows us to train those joint embedding
architectures.
So I've become a big fan of them now.
And it's because of algorithms like BYUL,
from our friends at DeepBind, like Barlow Twins, whose idea came
from Stephane Denis, who's doing a postdoc with me at FAIR.
He's now a professor at the University of Alto in Finland.
And then more recently, Vic Craig,
which is a kind of improvement, if you want,
on Barlow Twins, that is also based
on the same idea that Jeff Hinton and Sue Becker
had of maximizing a measure of mutual information between the
outputs to networks, but in a slightly different way.
So I'm really excited about those things.
And again, I change my mind all the time,
whenever a good idea seems to be overcome by a better one.
So in this whole space of maybe interpolation, extrapolation,
but also data manifolds, and so on,
what's your view on things like data augmentation,
training and simulation, and domain adaptation, and so on?
Because it could be argued that these things increase
the convex hull of the training data.
They sort of make the distribution broader.
Or is that just also out of question?
Not much.
I think data augmentation does not
increase the volume of the data point cloud,
if you want, very much.
Because those augmentations are generally fairly local,
in dimensions that are already explored.
So it may increase it a little bit,
but not significantly, I think.
Obviously, data augmentation is very useful.
I mean, we've used this for decades,
so it's not a new phenomenon either.
There's a lot of ideas along those lines, right?
So it's the idea where you give two examples,
and you have two examples in your training set,
and you actually do an interpolation in input space
to generate a fake example that's in between the two,
and you try to produce the intermediate target
between the two original examples.
Mix up.
Mix up.
Mix up.
Yeah.
There is that.
There is distillation.
There is various techniques like this
are basically implicitly kind of ways
to fill in the space between samples, right,
with other kind of fake samples, or virtual samples,
if you want.
There was a paper by Patrick Simard and me,
and John Denker many years ago, when we were all at Bell Labs,
on something called tangent prop.
So the idea of tangent prop was kind of somewhat similar.
The idea was you take a training sample,
and you're going to be able to distort that training
sample in several ways.
You could generate points by data augmentation.
But the other thing you can do is just figure out
the plane in which those augmentations live, OK?
And that plane is going to be a tangent plane
to the data manifold.
What you want is your, for a given class, for example, right?
So what you want is your input output function
that the neural net learns to be invariant
to those little distortions.
And you can do this by just augmenting the data,
or you can do this by explicitly having a regularization
term that says the overall derivative of the function
in the direction of the spending vectors of that plane
should be 0, or should be whatever it is that you want it
to be, but 0 is a good target for this.
So that's called tangent prop.
And you can think of it as a regularizer that says,
I don't just want my input output function
to have this value at this point.
I also want its derivative to be 0
when I change the input in those directions.
That's another indirect way of doing data augmentation
without doing data augmentation, essentially.
And I think there's a lot of those ideas
that are very useful, certainly.
So I think there seems to be a spectrum coming back
to this interpolation, extrapolation.
You said yourself you don't believe neural networks
can do something like discrete, abstractive reasoning,
and so on.
No, no, no, I didn't say that.
Or I said, we need to do work for them to be able to do that.
But I have no doubt that eventually they will.
I mean, there's a lot of work in this area already.
And I'll give you some more specific examples if you want.
I mean, that was actually kind of the nature of my question.
Where do you think there is this spectrum of what
people think neural networks are doing now
or will be able to do later?
Where do you think the actual biggest disagreement
between the community right now lies?
And what can we do to, what experiments, what evidence
can we gather to solve these disagreements?
Right.
So I think we shouldn't talk too much about neural networks
because people have a relatively narrow picture of what
a neural network is, right?
It's a bunch of layers of neurons
that perform wetted sums and pass a result
through a nonlinear function.
And there's a fixed number of layers.
Maybe they can be recurrent, and you produce an output.
That's a very restricted view of what deep learning systems
can do.
So let me take an example of what reasoning might mean.
Reasoning might be seen at least one particular type of reasoning
might be seen as a minimization of an energy function,
not with respect to parameters in a neural net,
but with respect to latent variables.
And a lot of systems that are in use today do that.
So a lot of speech recognition systems, for example,
have a decoder on the output, which essentially given
a list of scores for what a particular segment of speech
could be in terms of what sound it could be,
or what syllable, or whatever, the decoder basically
figures out what is the best interpretation of that sequence
of sound that makes it a legal sentence in the dictionary.
Techniques like this have been used for 25 years now
in the context of speech recognition and handwriting
recognition, even before neural nets were used for those things.
So back in the old days of hidden Markov models.
And those techniques have been really developed.
Now, if you think about what those techniques do,
first of all, all the operations that are done in those systems
are differentiable.
You can backpropagate gradient through an operation
that will figure out the shortest path in a graph
using dynamic programming.
The first paper on this was in 1991
by my friend Leon Boutou and Xavier de Leoncourt.
This is not recent stuff that we're trying to do speech recognition.
You can backpropagate gradient through a lot of different modules.
What those things do is that they infer a latent variable
by basically doing energy minimization.
Finding the shortest path in a graph
is a form of energy minimization.
And so you can have, in a deep learning system,
you can have a module that has a latent variable.
And what this module will do is figure out
a value of the latent variable that minimizes some energy
function.
It could be a prediction error.
It could be something else.
It could be regularizers in it, et cetera.
But it basically performs a minimization.
And that type of operation basically
can be used to, like, you can formulate
most types of reasoning in that form.
Now, it's not necessarily in a continuous differentiable space,
but almost all reasoning, even in classical AI,
can be formulated in terms of optimizing some sort of function.
Like, you won't do a SAT problem, right?
If you have a collection of Boolean formula,
you want to find a combination of variables
that satisfy this formula.
It's an optimization problem.
It's combinatorial optimization,
but it's an optimization problem.
You want to do planning.
So planning is the best example.
So you can do a planning where the thing you're controlling
as discrete actions and discrete states,
and you have to use dynamic programming.
You can back propagate gradient through a system that
does that.
But more classical planning is in continuous space.
So planning for, like, planning the trajectory of an arm
for a robot, planning the trajectory of a rocket,
what you have is a differentiable model,
dynamical model of what is the state of the system
you're trying to control at time t plus 1
as a function of the state at time t
and as a function of the action you take.
And perhaps as a function of some random variable,
you can't measure from the environment
to something like that to make it non-deterministic.
Now, you can enroll that model.
So start with time equals 0, and then make a hypothesis
about a sequence of action, and then
apply your predictive model of the next state of the system.
And then at the end, you can measure some cost function.
Is my rocket docking with a space station or it's far away?
And how much fuel have I consumed?
Or whatever, right?
Or have I reached the goal of the arm,
avoiding all the obstacles, right?
So what you can do now is an inference process
which consists in figuring out what is the sequence of action
I should take to minimize this cost function according
to my dynamical model.
You can do this by gradient descent.
And basically, that's what the Kitty-Bison algorithm,
in optimal control, that goes back to 1962.
And it consists in basically doing backprop through time.
It's as simple as that, right?
So you do backprop through time.
So in effect, optimal control theorist invented backprop
back in the early 60s.
But nobody realized you could use this for machine
learning until the V-80s, essentially.
Or the 70s.
Just to jump in there, because there's a little caveat there,
which is that you can do that at backprop
if you have a fixed number of steps.
Like what backprop can't handle is the case
where I want some expandable number of steps.
We have no algorithms that can currently optimize.
For example, neural networks with a variable number of layers.
Like we just can't train those, right?
No, it's not true.
It's not true.
That's what recurrent nets are.
You can have a varying number of iterations in your recurrent
net.
And what I'm describing here is an unfolded recurrent net.
It's the same model that you apply every time step, right?
So it's very much like a recurrent net.
And you can very well have an unknown number of steps.
You don't know a priori how long it's
going to take for your rocket to get to the space station, right?
So you may have kind of a large potential number of steps.
And you can have a cost function which will count,
like how much time it's going to take to get there.
And this will be part of the optimization, for example.
And this is classic optimal control.
I'm not telling you anything that I came up with.
This is from at least the 60s and 70s.
No, but the backprop, that variable number of layers
has to be done in kind of a classic iterative,
try out the variable number of layers
and see what the backprop comes up with.
Or you start with a very large number of layers
and then let backprop try and find.
But still, what I'm saying is there is fundamentally
two different kinds of computation that are at play here.
One is like the finite fixed thing,
and then you do some differentiable optimization on it.
Another kind of computation is this discrete symbolic
that has like an expandable memory
and an unbounded amount of time
to sit there kind of computing on things.
Okay, I put a stop right there.
What I'm describing has nothing to do
with symbolic, discrete, or anything.
I understand that, I understand that.
But you seem to be equating a variable number
with discrete symbolic, this is two different things.
Okay, let me pose it in this form,
which is that I can very easily write down
a symbolic program that can output the arbitrary digit of pi,
like the nth digit of pi.
Nobody can train a neural network that can do that.
So what's the difference between these two types
of computation and where in the future might we go
with neural networks or by augmenting neural networks,
whether it's differentiable Turing machines or whatever,
or neural Turing machines,
to try and bridge that gap and capability?
Okay, before we bridge that gap,
the algorithm to compute the digit of pi,
there's only a tiny number of humans,
only in the last few centuries that I figured this one out.
I'm interested in how is it that a cat can plan
to jump on the table and not fall or even open a door
or do things like that, right?
Once we figure this one out,
maybe we can think about kind of more complex stuff,
like designing algorithms that involve
complex mathematical concepts.
But I think we're way, we're not there, right?
We're faced with much more fundamental issues
of how do we learn predictive models to the world
by observing it and things of that type,
which are much more basic that most animals can do
and digit of pi, it's some kind of a line.
So, because you're talking about model predictive control,
is it possible that there are different shapes of problem?
So, for example, some problems are interpolative
and are solvable using differentiable models.
Do you think there exists problems
that are quite discreet in nature
and a different type of approach would be required?
Of course, yeah.
I mean, there is certainly a lot of situations
where the mapping from action to result
is very kind of discontinuous, if you want, right?
This is qualitatively different.
And so there are many situations where
or it's somewhat continuous
and situations where you change your action a little bit
and it results in a completely different outcome.
And so the big question, I think,
is how you handle that kind of uncertainty in the search.
And you can think of sort of two extremes.
So at one extreme in the continuous case
is the case where you're planning the trajectory
of a rocket or that's pretty continuous
and differentiable and everything you want.
And you don't even need to learn the model.
You basically write it down, right?
But there are situations there where you're flying a drone
or something where you might need to learn the model
because there's so many kind of nonlinear effects
that it's probably better to learn it
and people are working on this.
Same for like working robots and stuff like that.
Then there are things that are a little intermediate
where there are sort of hard constraints on what you can do.
So you want to grab an object with a robot arm,
but there is obstacles in between
and you don't want to bump into them and things like this.
So people tend to put like penalty functions
to make this sort of more continuous,
but there's sort of qualitative difference
between using your left arm or your right arm, for example,
or going, scratching your left ear with your left hand
or with your right ear going to the back of your head, right?
Those are qualitatively different solutions.
And then all the way to the other side,
there is intrinsically discrete problems
with that may be fully observable
with some uncertainty like chess and go-playing, okay?
And those we can handle to some extent
because the number of actions is finite.
It goes exponentially, but it's finite.
And so using kind of ways to direct the search,
despite the fact that the search space is exponential,
using neural nets as basically evaluation functions
to direct the search in the right way
and doing multi-color tree search and blah, blah, blah,
will work, okay?
With sufficiently many trials,
in the completely deterministic,
fully observable, differentiable case,
that's classical model predictive control, that's fine.
Then there is some stuff that we really don't know how to do
and it's twofold.
One is the model is not given to us
by equations derived from first principles, right?
So the stuff we're trying to do is in the real world
and it's got complicated dynamics
that we can't just model from first principle.
So we have to learn the model.
That's the first issue.
Second issue, the model lives in the world
and the world is not completely predictable.
It may be deterministic,
but you don't have full observation.
So you cannot predict exactly what's gonna happen
in the world because the world is being the world
or as a consequence of your actions.
So how do you deal with the certainty?
And for that, you need predictive models
that can represent uncertainty.
And we are back to the issue I was telling you about earlier.
Do you need latent variable models
or joint embedding architectures, right?
That's, and I've changed my mind about this as I told you.
So then there is the third obstacle,
which is, is the problem we're trying to solve
of the so continuous differentiable nature
or of the kind of completely discreet,
you know, qualitatively discreet
depending on what action you take nature.
And I think most problems are some combination of the two.
So, you know, you're trying to build a box
at a wood or something like that, right?
You can make the box bigger or smaller.
You know, you can hit the nail in this way or that way.
And that may be sort of continuous and differentiable.
But then there is, you know,
you put glue or screws or nails,
do you, or use kind of more classical carpentry or whatever.
And those are kind of discreet choices.
What type of wood are you using, you know, things like that.
So I think the, you know,
human mind is able to deal with all of those situations,
have, you know, know to use differentiable continuous stuff
when they have to and use the sort of discreet exploration
when we have to as well.
But we have to realize that humans are really, really bad
at the discreet exploration stuff.
We totally suck at it.
If we didn't, then we would be better than computers
at playing chess and go, but we're not.
We're actually really, really bad.
So...
That's fascinating.
So you seem to be saying in a way that you are a fan of hybrid models.
No.
And something, well, something like AlphaGo, for example.
I mean, that's basically, there's an interpolative space
which, you know, guides a discreet search.
Let's say it like that.
Do you think that's, is that a good thing?
Or do you think that's...
So you like that kind of model?
Okay, there is something very interesting about, you know,
in the context of reinforcement running about this,
which is actual critic models, right?
And you could think of all of the stuff that actually works
in reinforcement running.
I mean, they don't work in the real world, right?
But they work in games and stuff and simulated environment.
They very often use actual critic type architectures.
And the idea of a critic, you know,
goes back to early papers by Saturn and Bartow.
And the basic idea of a critic is to have a differentiable
approximation of your value function, right?
So you train a small neural net essentially to compute,
to estimate, to predict the value function from your state.
And the reason you need this is now you can propagate
gradient through it, right?
So that's kind of the first step into sort of making
the world differentiable.
You're just making the value function differentiable.
Now, inside of the world, there's two parts in my opinion.
And this is the source of the idea behind model-based reinforcement
running is that you have the world, right?
The world is going from state to state because it wants to
or because you're taking an action.
And then there is a value function or, you know, a cost function.
I prefer to talk in terms of cost function that takes the state
of the world and gives you kind of estimate of that cost, right?
Or gives you the cost.
Now, the world itself and the value function that takes the state
of the world and gives you pain or pleasure, right, is unknown.
But what you can do is build a differentiable model of that, right?
So a differentiable model of that is what NPC is all about.
You build a model of the world that predicts the new state
of the world as a function of the previous state.
It doesn't have to be a complete state.
It has to be a state that's complete enough to contain the relevant information
about the world, you know, relative to your task, right?
And then you have a differentiable function that you learn
that learns to predict the reward or the cost from your estimate
of the state of the world.
So what you have now is, you know, a neural net inside your agent
basically can simulate the world and simulate the cost
that is going to result from the state of the world in a differentiable way.
So now you can use gradient descent or gradient based methods for two things.
One for inferring a sequence of action that will minimize a particular cost.
Okay, there's no learning there.
Two, to learn a policy that will learn to produce the right action
even the state without having to do model predictive control,
without having to do this inference by energy minimization, if you want.
And that, in my opinion, explains the process that we observe in humans
by which when you learn a new task, you go from the, you know,
Daniel Kahneman system two to Daniel Kahneman system one, right?
So, you know, you learn to drive, you're learning to drive,
you're using, of course, your entire model of the world that you've learned
in the last 18 years, if you are 18, to predict that, you know,
when you turn the wheel of the car to the right, the car will go to the right
and if there's a cliff next to you, the car is going to fall off the cliff
and, you know, you're going to die, right?
You don't have to try this to know that this is going to happen.
You can rely on your internal model to, you know, avoid yourself a lot of pain, right?
And so, but you pay attention to the situation.
You pay a lot of attention to the situation.
You're completely deliberate about it.
You imagine all kinds of scenarios and you drive slowly
so you leave yourself enough time to actually do this kind of reasoning.
And then after maybe 20, 50 hours of practice,
it becomes subconscious and automatic, you know?
That's even true for chess players.
So, that's an interesting thing about chess, right?
I played once. I'm a terrible chess player, by the way.
And I played once a simultaneous game against a grandmaster.
So, he was, you know, he was playing against like 50 other people.
And so, I had plenty of time to think about my move, right?
Because he had to kind of play with the 49 other players before getting to me.
And so, I wait for him to come and make one move.
And then, you know, in one second, or first of all, he does, like, you know,
play something stupid, which I did.
And he moves within one second.
He doesn't have to think about it, right?
It's completely subconscious to him.
You know, it's just pattern recognition, right?
He's got this, you know, covenants predicting the next move,
you know, completely instinctively.
He doesn't have to think because I'm not, you know,
I'm not good enough for him to really kind of cause his system to kick in.
And of course, you know, he beat me in 10, you know, in 10 plays, right?
I mean, as I told you, I'm terrible.
You know, I learned to drive a long time ago.
But as it turned out, very recently, I went to drive a sports car on a raceway.
And, you know, again, the first few times, you were very deliberate about it, you know?
You explain a few things, and then you basically have to integrate all of that by yourself.
Over the course of a day, you get better and better, like much better,
just by, you know, basically compiling what at first is deliberate
into something that becomes automatic and subconscious.
Indeed. And also, abstractly reusing knowledge that you've gleaned elsewhere
and applying it in a new situation.
But anyway, Professor Yanlacun, thank you so much for joining us today.
Thank you. It's been an absolute honor.
Well, it's been a pleasure.
You guys are doing great work.
So, you know, keep going.
Thank you for being a fan, too.
Thank you so much for watching some episodes.
Thank you so much.
Wonderful.
Rando, introduce yourself.
Okay, so I joined FAIR, or should I say Meta AI Research, I guess.
Last June, for a postdoc position, I was doing my PhD at Rice University
under the supervision of Professor Richard Baronyuk.
And during my PhD, I focused on basically trying to understand deep networks
from a geometrical point of view through spline theory.
And now I'm trying to, let's say, expand my horizons
and do more diversified research topics with Yanlacun.
And for this paper, basically, the main goal was to try to understand
through a lot of empirical experiments, what do we understand by interpolation?
Does it occur in practice?
And does it make sense to use interpolation as we know it,
as, let's say, a measure of generalization performance for current networks?
And the main point is really to say that the current definition of interpolation,
which uses this convex hull, might be too rigid to really provide any meaningful intuition.
And so we either need to adapt this definition or just entirely think about this in a different angle.
But yeah, the current definition is not good enough for the current data regimes that we are in right now.
I don't know if it's precise enough.
Perfect.
No, it's wonderful.
Cool.
Sorry, I need to get into the mood again.
Yeah.
Hi, Randall.
It's really cool to have you here.
We've enjoyed reading the paper.
It's quite a short and concise paper, I have to say.
And the experiments are quite, I find them to be really on point,
especially where you look at the latent space experiments.
Because a lot of people would say, of course we're not interpolating in data space.
We're interpolating in the latent space.
Yet, even in the latent space, you know, and we've talked a little bit about the notion of interpolation and extrapolation.
Is it fair to say that the paper is just sort of a negative argument?
Is it fair to say that the main point of the paper is arguing,
look, interpolation is the wrong concept you're looking for when you criticize these models?
Yes.
I don't think it's negative per se.
It's more, let's say, a call to change the definition for the current usage of machine learning models.
Because now we're not in low dimension regimes anymore.
And so using those concepts that have been defined 50 years ago when we were looking at univariate models,
does not really make sense.
So I think the intuition behind what we try to mean by interpolation is right.
We should not change that.
We should change the mathematical definition of it.
And yeah, like you say, people could argue, yes, but you have interpolation in the latent space and so on.
But we showed that even in a classifier setting, it does not happen.
And you can also show that in a generative network setting, it will not happen again or because of the dimensionality.
So it's really about going to the high dimensional setting.
Then things start to break and we have to adapt to that basically.
And we've already asked Jan this question.
But if you had to give your best shot at making the argument that these people want to make when they say,
oh, it's just interpolating.
If you had to give your best shot at making that argument successfully, you know, what would you change?
How would you change the notion of interpolation or what argument would you make for those people?
So I think what most people try to say is they try first to conceptualize the data that they have.
So for example, you have an apple, right?
You have different colors.
And if you think of this color as being your latent space, then you can say, okay, between green and red, you have a new color.
But it's in between the two.
You are in an interpolation regime, right?
So all of this point of view is from a generative perspective.
And this is because you only think of a few factors of variations like this.
And then you think, okay, everything should be in interpolation regime.
But even with that definition, if you start having a lot of factors of variation, then the course of dimensionality will kick again.
And you will never be in interpolation regime, even in a generative setting.
So for them, the best argument would be if I don't consider the real data set, but very low dimensional approximation, very rough, which can be explained only with a very few factors of variation.
And I can somehow linearize those factors in my latent space.
Then I will have more chance at being in an interpolation regime.
So we'll have to have a sort of lossy compression of your data, if you will.
And then you can try to reach there with more chances.
So I think you just still manned the argument for interpolation.
So I think that's precisely what folks do argue is happening in a deep neural network.
Do you not believe that?
Well, as soon as you have high dimensionality settings, then it does not happen almost surely.
And I mean, you could argue, so for example, let's say you take a gun or any big generative network, right?
In the latent space, you have hundreds of dimensions and you sample those guys from a Gaussian distribution.
So even if you were saying, OK, your training set is sample from a generative model and your test set is sample from the same generative model.
And in that latent space, you have interpolation.
Well, it's wrong from the beginning because in that latent space alone, you will never have a Gaussian sample that lies in an interpolation regime as soon as the dimensionality is greater than, let's say, 15 or 20.
So it does not happen because of the dimensionality, unless you have very degenerate things.
Of course, if your generative network just speeds out a constant, then in pixel space, you will have interpolation.
But this is degenerate by definition.
I'd like to pick up on that, if you don't mind.
Yeah, so what I want to pick up on is, again, all of this hinges on definition one, which is in the paper, which is membership within this convex hull.
And there's a sense in which that's an extremely rigid definition of interpolation, right?
And I think I heard you earlier say that what we need to do is redefine what we mean by interpolation in higher dimension because the intuition of interpolation is still correct, but we need to redefine what we mean by interpolation.
So if you took a shot at redefining interpolation, how would you define it? What do you think is a better definition of interpolation?
So I think it's very task dependent. So let's say you want to look at a task which is pure object classification.
In that case, I think going back to what we were saying earlier on first trying to compress some of your data so that you can explain it with as little factors of variation as possible.
And then you can use the current definition just on a compression of your data.
Then it could make sense because you don't really mind about the finicky details of your objects if you just want to differentiate between different classes of objects.
But in another setting where you might be trying, I don't know, to denoise samples or things like that, then you might want to have a very more specific definition based on what you try to denoise in your data and so on.
So I don't think there will be a general definition that works across the board. It will be really dependent on the type of task you are looking down downstream. It's really the key.
Let me throw something out there because I've been thinking about this and I tried to ask Professor Lacune about this, but I didn't communicate it clearly, which is that for something to be in the convex hull, so in dimensions,
for something to be within a convex hull, it's a necessary condition that on every single dimension, the sample data point lies within the range that was sampled.
So that's a necessary but not sufficient condition because the convex hull is actually even smaller than that space.
But it's necessary that it be within that axis-aligned bounding box. It has to be inside there on every dimension.
And so what if I were to invert this and say that, well, instead I'm going to define extrapolation as on every single dimension, it has to be outside the sample range.
So in other words, the point has to be outside the axis-aligned bounding box. That would actually come to the complete inverse conclusion because then exponentially so all machine learning would be interpolation
because it's very, very unlikely that you're outside every single dimension.
So I'm thinking somewhere in between there, there's almost like a fractional concept of interpolation versus extrapolation where we can say it's almost like on average, how many of the dimensions do you hit inside the sample point?
So maybe I'm inside 20% of the dimensions on average. Could that be like a route to any type of improved definition?
So that's actually a very good point. So basically what you are saying is instead looking at, let's say, the closest, enclosing hypercube.
And then you say, okay, if you're in that hypercube or not, you're in interpolation or extrapolation regime.
And that's actually something we're looking at right now, which is not the smallest hypercube but the smallest ellipsoid that encloses your data.
And it's somewhere in between the convex hull and the hypercube that you mentioned.
So for sure, there is some ways in between those two extremes where you will have a meaningful interpolation and extrapolation regime that does not collapse all one way or the other way.
So this is for sure one interesting route. And this could potentially be, let's say, task agnostic.
But then again, would it be precise enough to re-quantify generation performances per se? I don't know yet.
But yeah, that's something we are looking into right now.
And the reason that we kind of came to that thought process is because there is an intuition that machine learning is kind of very good at taking all these dimensions that we sample and saying, you know, it's really only a subset of those dimensions that matter, at least in some transform way.
Like for example, suppose I was doing, suppose I only had 16 dimensions of ambient space and everybody agreed, yeah, given the amount of data we have, we are interpolating, like almost all the time we're in this rigid definition of a convex hull.
And then somebody comes along and says, oh, by the way, we upgraded all our sensors now and we added 240 dimensions.
And if all those dimensions happen to not be useful for the problem that we're trying to solve or classification problem or whatever, it would be weird to say now we're all of a sudden extrapolating.
Because if the neural network is doing what it should be doing, it will ignore all those irrelevant definitions and continue just calculating exactly what it was calculating before.
Yes, that's a very good point.
But I think for this to actually work, you will need to assume that you have enough actual samples so that when you train your network understands that basically those dimensions are pure noise.
It does not need it to solve the task at hand and it disregards it.
So you have some all regularization terms that kicks in and it does not try to use them to minimize the training loss.
But I think in practice, that's not really what we see.
If you take like image task classifications, even especially now with self-supervised learning methods, we see that actually most of the information that we could think is irrelevant for the task is actually kept because it can of course help in reducing the training loss.
So I think for what you said to work, you really need to be sure that either you have the perfect regularizer,
or you have the perfect model and regime of training samples and so on.
But in a general setting, I think it will be tricky to claim it like this.
That's a very interesting point.
So you're saying when you add these extra dimensions, even to learn to ignore them, you have to have lots of data.
Yes, yes, or regularization or some mechanisms that exactly.
Yeah, very interesting.
That's one thing.
That's one reason why, let's say, add up parameterization by end is sometimes useful.
Because if you know which things are useless for your task, you can by end remove them from the data that you feed to your deep net or whatever model.
And then doing this will improve generalization.
So that's why in some cases it's still useful to do those preprocessing steps.
Right.
Could you give me a bit of intuition here?
So we've been listening to quite a few folks that make the argument about the interpolative nature of deep learning.
And you could argue that any kind of processing and machine learning, not just feature transformations,
but even things like regularization and domain randomization,
that there are all ways of making the problem more interpolative and it would be appropriate for an interpolated problem.
So I'm thinking to myself, I mean, in this example, you take pictures of clock faces,
and it's not interpolative in the ambient space, you perform some kind of feature transformation.
There exists some nonlinear transformation that transforms it into an interpolative space.
That's what these people say.
But I'm wondering, you know, I'm interested in the curse of dimensionality.
You know, why does deep learning work at all?
And I've spoken to folks that talk about creating various priors to combat the curse of dimensionality.
But why do you think that deep learning works at all in these high dimensional spaces?
Well, so first, I might say something a bit speculative or not agreed upon by everyone,
but I don't think you can...
We love it. We love it. Please do.
Exactly.
I don't think you can state so generally that deep learning works right.
You need the right architecture.
You need the right loss function.
You need everything right for it to work.
That's why it works now because we spend, I don't know,
much amount of money and manpower to get to where we are now.
But if you just take a plain MLP, you apply it on ImageNet,
I don't think you will say it's working right.
So I think right now it's working.
I mean, what we have is working because we basically, by end,
did all the cross-validation such that the network that we are using is regularized enough
to only learn the necessary or meaningful information,
or at least as best as we can to provide good generalization performances.
So, yeah, it's a bit too general of a statement to say that deep learning works out of the box for everything.
In fact, many cases...
I love that point.
Yes, yes.
Many, if you go into not just image classification, but let's say audio classification
and some maybe trickier data set where you don't have a lot of samples,
then everything starts to break quite rapidly, right?
You have always people trying to go into the medical field as well,
but since it's very hard to generalize between patients,
between recording devices and so on, things get very, very messy
and everything is ad hoc and optimization, basically.
I don't think we're at this point yet where you can say,
that's it, deep learning works out.
Yeah, can I jump in real quickly here, Tim?
Because what I want to say is, brilliant point.
I mean, and when you think about it, let's take Professor LeCun,
you know, one of his great invention, right, the CNN, Convolutional Neural Network.
That was discovered by a human being, like a machine,
and no neural network learned to do convolution.
LeCun taught machines how to do convolution.
And so we oftentimes discount this and say,
look how great machine learning works, we discount all the human engineering
that has gone into actually making machine learning work
through specific architectures.
Yes, exactly, exactly.
Basically, we are guiding where to go,
and then of course, once we guide enough, the machine knows what to do.
I mean, this is not a reduction of what we can do with deep learning, right?
But he's just saying that a generalized statement
out of the box, deep learning works even in a new task that we never tried before
is an overstatement.
But I still want to linger on this point of why you said,
or here's just a random projection of the data,
and then here's a ResNet projection, here's a trained ResNet projection.
What I took from that is irrespective, it's all in an extrapolative regime.
And there was a guy who posted an article
who is disparagingly saying that neural networks are just interpolators,
analogous to a Taylor series approximation
within a small part of the domain of a function,
and it goes haywire outside of the domain.
And there are a lot of people that would make that case,
that neural networks cannot extrapolate outside of the training range,
but they do seem to work remarkably well,
given the curse of dimensionality.
So why is that exactly?
So let's say for simplification that you have a binary classification problem.
So once you go into your Latin space,
or if you just do linear classification,
if you stay in your ambient space,
basically you say you have class 0 or 1
based on which side of an hyperplane you lie in.
Now, if you are on the good side,
you can go to the infinity, right?
You can go to extrapolation regime,
as far as you want from the training set,
as long as you are in the correct side of that hyperplane,
you will have good generalization performance.
So it's not exactly correlated.
The only thing you need,
some O is to have the orientation of the axis
where you will extrapolate
to be somewhat aligned in the orthogonal space of that hyperplane.
Once you have that, then you will generalize,
even though you can be in extrapolation regime
in ambient or feature space.
So that's one thing that is maybe proper to linear classification.
If you were doing maybe some other type of classifier on top of it,
it might change.
But given the current settings,
I don't think people should expect bad performance
because you are not in an interpolation regime.
This is quite of a shortcut,
maybe that it guided by our intuition, right?
We think it's much easier to classify something
if it's in an interpolation regime.
But if you just look at the plane in our classifier,
that's not the case.
Well, so just to steal man a bit
what say others on the interpolation side,
again, that's only if you're buying into a very rigid definition
of interpolation, which the way the paper defines it
is in a linear way.
I mean, it's defining interpolation by this linear convex hull.
And of course, neural networks, for example,
are highly nonlinear systems.
And what you just said, sort of which side of a separating hyperplane you're on,
that itself is a nonlinear function.
And so I think the point you made early on was very good,
which is we have this intuition of what interpolation is.
We haven't yet got a good definition for interpolation
in multi or high-dimensionality nonlinear cases.
But obviously the linear definition doesn't really apply.
Yeah, exactly.
It's really because, I mean, it's linear,
but again, you can consider your data as being nonlinearly transformed.
The problem is still that if you have high-dimensionality,
then to be within a convex hull becomes exponentially hard
using the convex hull of the training side.
If you define something else, like you said before the hypercube,
it could be the opposite.
It's always in interpolation regime.
And the key is basically to find a meaningful set
such that you don't go all the way in one direction or the other.
And because I think the main point also is that this should give intuition
into the generalization performance of a model, right?
Because if you can detect a sample is in an extrapolation regime,
but your model still performs very good on it.
I don't know if it has really a lot of practical use.
So what will be good as well is to have the basically
generalization performance of a model correlate with your definition of extrapolation.
And that's really what we are trying to get.
And that's why I was saying that probably you might have a task-dependent definition,
because that might be where you get the best precise definition to reach that.
I was wondering to what extent, if any of this invalidates the so-called manifold hypothesis.
So I think when most people speak of interpolation and deep learning,
their intuition is something along the lines of,
well, the model learns some manifold in the latent space,
and interpolation means I'm just kind of like traversing the geodesic on that manifold.
And when you visualize the results on an autoencoder or something,
you can see this kind of continuous geometric morphing
of, let's say, one image into another image.
And that manifold, I think you said in your paper that the actual data manifold,
it's not possible to approximate that well, but it's doing something interesting.
And a prediction on that manifold in the intermediate space,
it's not massively deranged, is it?
It's still doing something very useful, statistically.
Yes, yes, yes, totally.
So I think it does not at all contradict it.
And in fact, think of a very, very simple example where you say your data
is a linear manifold of dimension 100, let's say,
which is a relatively meaningful number of dimensions even for images.
But now suppose it's completely linear.
Well, even in that case, it will be hard to be in interpolation regime
just because you have 100 dimensions
and picking a new sample that lies in the convex of your training set is exponentially higher.
So you can have a manifold for your data.
It can even be linear.
So the simplest manifold you wish for,
the only problem is the dimensionality of that manifold.
Now that being said, again, it does not mean that you cannot do
basically moving on the geodesic of your manifold
or you cannot do some sort of interpolation
because that's what happens in most of the current state-of-the-art generative networks, right?
It's just that you don't need to be in interpolation regime
to have a correct generation process, basically.
And that's, again, one thing that is very important.
And you mentioned, you have those examples.
That's true that the interpolation is extremely nice,
but again, it does not say that the points you start with
and the point you end up with are in interpolation regime of your training set.
So I think it's two different things to be able to interpolate or move on your manifold
and being in the interpolation regime from your training set.
So what's intriguing me about, and I want to clarify something earlier,
which is that though a hyperplane is a linear construct,
the function that says you're on one side or the other of a hyperplane is a nonlinear function.
And in that case, it's a digital function.
You're either on this side or that side, it's zero or one.
What's kind of interesting is you gave that as an explanation for why machine learning works at all,
which is that for data sets that we care about, or problem spaces that we care about,
it seems to often be the case that we can do enough sequences of linear and nonlinear transformations
to arrange the data such that it falls on one side or another of a separating plane.
In a sense, it seems like real-world data is often separable if you do enough transformations on it.
And since that separability is a very digital concept, is there anything interesting in that?
Any intuition?
Yes, yes. So for this, you still have to be careful, right?
Because if you go in high-dimensional spaces, you can basically separate everything very easily.
But your generalization performance might be bad.
So basically, what you want is really to have, let's say, a good ratio of how much you expand dimension,
how much you nonlinearly transform your data, to then reach a good generalization performance.
Because otherwise, you can just fall back into a kernel regime, let's say,
and you expand so much your space that sure you can separate everything on your training set,
but then the generalization is going to be very poor.
And that's one strength of deep networks, right, is that you have so much of those nonlinear transformations
that you can somehow not expand the dimension of your space too much or even contract it,
and still have a separating hyperplane in the end where generalization is much higher than in other class of models.
So I think the key is really to have those meaningful nonlinear transformations
such that you don't have to increase your space too much.
You just shape it around, if you will.
And what you said earlier is exactly true.
If you think of which side you are in, basically, you are binarizing your data, right?
And if you have good classification, it means all the classes are assigned to the same labels, which is 1 or 0.
And if you think of it still in interpolation regime, then suddenly you are in interpolation regime, right?
You are a 1. The new sample is a 1 after binarization, and you become interpolation regime, and you have a good performance.
But this comes after this compression step, if you will, or discretization step.
And that might be another direction to explore as well.
If you start to quantize things or to compress things, as we were saying a bit at the beginning,
then you can reach the interpolation much more easily as well.
I have a couple of questions.
So again, in Table 1, I mean, your paper is making the argument that everything is extrapolation,
given this convex notion in high-dimensional space.
But if we zoom in a little bit, though, so you are using a pre-trained ResNet classifier, pre-trained on ImageNet,
and how do all of these things change the structure of the latent space in a meaningful way?
And the embedding space is also highly distributed.
And we were wondering, can you give us some intuition here?
So is the information likely to be quite evenly distributed over the latent,
or do you think it's actually quite bunched up and sparsely encoded in few of the features?
So I think this will depend on which training setting you use, right?
So for example, if you start using dropout and things like that,
you will try to have a more evenly distribution of your information to have a more stable loss when you drop those units.
So I don't think you have a general answer for that.
It will depend on the type of regularization you have or training is done, etc.
But you have to keep in mind that what you try to do with gradient descent is just minimize your loss, right?
But then with cross-ontropolis, let's say, your gradient starts to vanish as you become really good
and you stop learning where you are, basically.
So given that, depending on your initialization, you will still try to make the best of what you get.
And even if it means learning redundant dimensions, if this can reduce your loss further at a more rapid rate,
that's what you will do.
So if you don't impose any regularization or anything, there is no clear reason to assume that everything is well organized and so on.
And that's what we see even in generative networks.
You have to start putting so much regularization to try to have disentanglement and to try to make sense of those latent spaces.
Because otherwise, you just try to learn what minimizes your loss with the most short-term view of your loss landscape.
So basically, that could be built if you have some specific regularizer.
But otherwise, it will not occur naturally.
Of course, and again, if you wanted to only retain the minimal information to solve the task at hand,
then you will see much more interpolation regime in that latent space.
If you think of MNIST, for example, you will disregard the translation of your digits, the rotation of the digits, all those things.
All those things will be disregarded when you reach the latent space.
And then you will basically be in interpolation regime most of the time.
But since you keep as much information as possible to try to minimize your loss as best as you can, then you basically occupy as much as you can.
Unless, again, you have some degeneracies because of the whole architecture tricks.
For example, if you have a bottleneck layer, you will limit the dimensionality of the manifold you span in the latent space.
So you can have all those parameters that can play a big role.
So it will be in the general setting. I don't think you could assume anything.
And I think there's also a relationship, too, between the dimensionality of the latent space and, let's say, some intrinsic dimensionality of the problem.
So if the intrinsic dimensionality of the problem only takes five dimensions to solve, and yet I give a latent space of 256 dimensions,
I think what I hear you saying is that, of course, gradient descent is going to make some use of those other 251 dimensions,
but they're going to have maybe a very minuscule or diminishing effect on the latent space.
Whereas on the other hand, if I then took that same network and increased the complexity of the problem,
we could end up with, for example, it's sparsifying for any particular class.
So if we're doing some multi-class problem, we may find that it sort of arranges these seven dimensions to solve the dog versus hot dog problem,
and these 12 dimensions to dissolve the car versus motorcycle problem.
It might be forced to make more compact use of that latent information space per class. Is that fair?
Yes, so that's a very good point.
So first of the first things that you said, one thing to keep in mind, so let's say your data is even linear manifold of dimension one,
and then you go through a deep net, and then somehow it's already linearly separable.
Then you only need to learn the identity mapping with your deep net to solve the task.
But if you start from a random initialization, it's extremely hard to do that.
That's why people came up with ResNet and all those things.
So already from this point of view, it means that almost surely your linear one-dimensional manifold will become highly non-linear,
still one-dimensional, but very highly non-linear in the latent space of your classifier.
And as we showed in the figure one of the paper, even if the intrinsic dimension is one,
if you are highly non-linear in your space, then you will basically never be in interpolation regime.
So you have those sort of artifacts that come just from the fact that learning simple mapping,
let's say, with a non-linear deep network is not always simple.
And those artifacts will be introduced right away because it's so hard in your parameter space to reach that point where you have identity mapping.
So this is another effect that kicks in and that can somehow remove those assumptions that even if your data is in a low-dimensional regime
and almost linear, it will be preserved in the output space of your network.
So this is something really important to keep in mind as well.
So, Randall, I also noticed in about 2018, you were the first author on some really interesting work and it was called a spline theory of deep learning.
And then I think the next year it got into Neuribs.
So I'm just reading a bit from the abstract.
So you said you built a rigorous bridge between deep networks and approximation theory via spline functions and operators.
And you actually think that this can explain a lot about deep learning in the sense of then being a composition of these max-safine spline operators.
Has any of this work informed your view of deep learning now?
Yes, a lot actually.
So first of all, I want to be precise that of course a lot of people knew about the fact that if you use, for example,
volume activations, absolute value and so on, or max-pulling, then the whole network is continuous piecewise linear mapping.
So what we did mostly is to make this a bit more rigorous and try to understand what the partition of the input space looks like,
what the perigen mapping looks like, and how can we use that to gain some more intuitions into what's happening.
And the nice thing with this is that if you think about it, there is nothing simpler than piecewise linear mappings, right?
Basically, it says that if you are in a region of your space, a region from the partition of your mapping, then the input-output mapping will stay linear.
And this allows to do a lot of analysis, for example, to adversarial perturbation or all these type of things.
And it can also open the door to other lines of research.
For example, one thing we did, I think it was NeurIPS last year or two years ago, was to use that to derive the exact expectation maximization algorithm for the generative networks,
because now you have a cleaner, let's say, or simpler analytical form of your network.
And this really opens to me the door to derive some more interesting theoretical results.
It does not really help intuitively, because I think everyone had this intuition from before already, but it's mostly a mathematical tool that allows to derive with small is some interesting results.
And I think one important thing as well is that you have really this dichotomy right between the, let's say, old school signal processing, template matching type of academia researchers,
and the new school with deep learning, everything has to be trained and so on.
And what is interesting in this paper is that we somehow bridge the two.
We say that basically a deep net is a very smart way to build an adaptive spline, which will learn automatically its partition of the input space and the perigen affine mapping, such that it works in high dimension.
And this was not known before by anyone in the spline theory.
So I think to speak about adaptive spline in high dimension, no one has no idea what to do except dimension two or three, maybe, because a lot of PDE work uses, but outside of dimension three, no one thinks about splines.
So I think there's a very strong result is to bridge those two different fields.
That's fascinating. I think one of the issues, because I'm speaking to Juan Bruno about this, and he was saying there was a big tradition in harmonic analysis of trying to reason about the behavior of these models.
And we did have the universal function approximation theorem, which in a sense is talking about stacking basis functions, you know, to approximate an arbitrary function.
But you say in your abstract here that the spline partition of the input signal opens up in a new, you know, opens up a new geometric avenue to study how deep neural networks organize signals in a hierarchical fashion.
I don't think people really have much of an intuition on how these models behave and how to reason about them.
Yes. Yeah, that's a very good point.
So one thing that we did, for example, is to study all the partition of the mapping evolves as you go through the layers.
And what we show is that at each layer, you keep subdividing your current partition.
And this is very interesting because once you know that if you look at a binary classification, for example, you get that the decision boundary is basically linear in each region of the partition.
And so what this tells you is that as you add layer, what you have to do is to refine the regions that still contain more than one class within them.
So this kind really brings insights and maybe opens the door to building new learning techniques to how many layers you should stack, which regions should they subdivide and so on.
And this is really akin to decision trees and how they build their partition as well.
So there is a lot of work that could be done also to maybe bridge the two or use one to understand the other.
And it's really geometrical because it's in the whole field of computational geometry and so on because we have those hyperplane arrangements, those half-spaces, intersection of them, hyperplane desalation.
It's also a known system.
So it's really geometrical and it can have a lot of interesting insights to understand what's happening.
It also provides nice visualization tools.
It's really fascinating.
Would you be interested in coming back on the show for a future episode just dedicated to this topic?
That'd be amazing.
Randall also actually released a paper called Neural Decision Trees.
And in the abstract he said they propose a synergistic melting of neural networks and decision trees.
So this is something that you've been thinking about actually from many angles.
Yes, yes, yes.
And that's very, very synergic to think about one from the point of view of the other as well, right?
Because if you think of a decision tree, the real limitation of it is that all you subdivide one region by adding a node does not really tell you how to subdivide another region in another part of the space.
You don't have this, let's say, communication or friendly help between the region subdivision.
But in a deep net, what you do actually is that if you know how to subdivide one region, then it will automatically enforce or you subdivide nearby regions.
And through this, suddenly you have a mechanism that appears which is that you don't need samples in each region to know to subdivide them.
You just need samples in some of the regions and then it will guide you on how to subdivide regions of the space without samples.
And that's something that is extremely strong when you go to high dimensional settings because you cannot have samples in all parts of the space by definition.
So I think this is extremely nice to have both point of view because then you can try to use the strengths of one to maybe improve the other.
So that's really interesting to me.
I want to ask you this question that we ask a lot of our guests because it's just at least something that's kind of profound interest to me is that there is this apparent dichotomy between continuous and discreet.
It's like the human brain is at the most lowest possible level and analog kind of continuous system and yet it evolved all these sort of discreet almost computations on top of it like pulses that either fire or don't fire like that type of thing.
And in the in the learning world or let's say really in the computation world we have the fact that you know I can write a very short piece of symbolic code discreet code that that can go and calculate the nth digit of Pi or something like that.
But it's almost impossible to train or it is currently impossible to train any neural network of fixed depth to do the same thing.
And so we have this this weird you know different regimes we have the discreet kind of logic reasoning type world and then we have the continuous differentiable type of world.
Do you view those as I mean are they fundamentally different regimes and we're always going to have hybrid systems that kind of combine both types of reasoning or is it possible to just say project that discreet computation fully into an analog type space if you just have enough you know parameters or something.
Yeah I think so I think it really depends on the resources that you have right because to me at least it seems that the hybrid system might be the most efficient where you can easily let's say cluster different settings into groups and then and for this you can just have a discreet settings and then within each group.
Discreetization is not good enough anymore and you need to go into the continuous regime.
So I think it will depend on the application you have at hand or or efficient you want to be doing it either energy wise or anything else.
So I think it's not not clear if one should dominate those are necessarily like for example in the if you go back to the spline setting of a deep net you have discreet setting which is basically your partition which which which region you are in and then within that region.
You have a linear transformation of the mapping which is basically continuous and both interact if you adapt one the other one change and so on.
And I think having this type of hybrid systems and where somehow learning through the continuous part adapts the discreet part is what is extremely powerful.
And that's I think one extremely beautiful property of current networks is that they do automatically this adaptive training of their discreet path through training of their continuous parameters.
And that's why they are so efficient.
If you think of pure approximation theory and you have an adaptive spline in one day that's why you have the best convergence rate basically.
So I think you really need both systems to interact.
If they don't interact then I think it's really easy to become suboptimal and interesting.
Because we put a similar question to Lacune and he was kind of saying that in an ideal world we would have a discreet system as well.
Humans are really bad at playing chess because we don't have that discreet system built in.
But the problem I think people like Lacune have with these discrete systems is typically they're symbolic and they're statically coded.
You could start talking about getting into a discreet program search and you could even guide that program search based on some deep learning model.
But I don't think to Keith's point I don't think it's really possible to do that well inside the continuous domain because if the problem even was learnable with stochastic gradient descent.
The representation would be glitchy. It just wouldn't work.
I think it depends a lot too on what are you trying to achieve with a model you build.
If you just try to be as close as possible to let's say what the human brain is doing then you might impose yourself to have some restriction on do you want it to be discreet or not.
Or if you just want to have a model that you can deploy on a task and it can solve the task as best as you want.
So I think depending on what is your goal and what are you trying to imitate with the model would change or you answer that as well.
But what if the goal was task acquisition efficiency?
So it's like I don't know what the task is yet.
Yeah I think that's like again to me a hybrid system where you have interaction between both parts intuitively would be the most efficient.
But yeah it might not be true for all settings.
When I was looking at figure three both Tim and I were interested in the fact that if you look at the MNIST data which to a human being is kind of a simpler data set.
One, two, three we know how to do that type of thing.
That as you increase dimensionality it much more rapidly becomes extrapolation versus image net which seems to kind of more slowly transition from interpolation to image extrapolation.
And what I'm wondering is the intuition I got from that and I wonder if this is completely wrong or it's correct is that for machine learning there's a sense in which MNIST is actually a harder problem.
Because it has to look at kind of global relationships like it has to try and say well there's a circle over here that's kind of oriented with respect to a line that's kind of further away.
And so it's harder for it to do that whereas we know that with like image net very frequently ML sort of devolves to looking at these micro texture like you know well everything that has this shade of yellow is a school bus you know type of thing.
Is that an intuition one can take from that plot or what does it mean that MNIST decreases so much more quickly.
Yes, so that's actually some things that we tried to clarify with a figure that comes after this one.
Basically the thing that you really need to be careful about and keep in mind is that if you look at for example a 16 by 16 patch for MNIST.
You have basically maybe most of the information you need to solve the task and you have a lot of texture about your DG etc.
If you look at image net and you take a 16 by 16 patch you have basically no information about the class it's extremely small patch it's almost constant across the spatial dimensions right.
It's basically a very small percentage of your full image.
So that's why MNIST goes much more quickly to extrapolation regime for a fixed dimensionality in pixel space.
Because the information you have in that amount of dimension is greater than for the image net case.
And this is only because it's already much more done sample right.
If we were looking at MNIST but with a 224 by 224 spatial dimension and we look at a fixed dimensionality then you will not have this difference anymore and it might even reverse.
So the really important thing to keep in mind is that even though it's the same dimension for both it does not represent the same proportion of image that is present within that patch.
And that's why you have those differences in that's mostly why you have those differences in those curves.
It's funny because we had the opposite intuition and so in the following figure 4 you're showing then on MNIST that more of the variance is explained with fewer of the principal components on MNIST.
And as you say that that does just because it's those pixels on MNIST are more salient for the problem.
I mean just so I understand because we were debating this a little bit.
Could you just give us your articulation of figure 4?
Yes sure. So basically what we are looking for the increasing dimensionality for the three data set is we pick a number of dimension in the spatial space.
So we do this by extracting a patch.
And then what we do is that we extract of course the same patch for all the samples.
And then we are looking at the proportion of the test set patches that are in interpolation regime and we report this.
Now for the PCA plot what we do is basically we look at once you extract this patch how many principal components you need to explain to perfectly reconstruct those patches.
Or you could say to explain the variance in those patches.
And this gives direct relationship because it shows how concentrated you are on lower dimensional manifolds.
A fine one of course that learns through the principal components.
And it means that if you can encode much more information with much less principal components then you lie on a lower dimensional affine manifold.
And this coupled with figure 1 shows that basically it's much easier to be in interpolation regime.
So the whole point of using this PCA plot was to show how good low dimensional affine manifold represents the current extracted patches.
And then to use this as a way to justify the extrapolation regime curve that we see.
So because again in the PCA regime it's linear manifold.
So if only two components for example perfectly describe all your patches then you will need very few training samples to be in interpolation regime.
Okay that makes sense.
And then the staircase effect on the smooth subsample row is a function of the size of the Gaussian filter used to smooth it?
Yeah so the staircase occurs basically whenever the number of dimensions increase and we get a new bigger patch to get it.
Because there is different ways to get it right.
One would be to always extract the center patch and remove some dimensions if you don't have exact number of dimensions that you can represent with a square patch.
Another thing you could do is to first smooth subsample and then remove the dimensions.
So there is different variants on how to extract those patches.
We try to show two different ones to show that the results do not really depend on how you do this process.
But yeah you will have some little different artifacts like this.
It does not change the overall trend but yeah it can change the small trends when you change from one dimension to another.
Well let me ask a question here about figure four.
Again about the intuition that we had on leaving aside interpolation and extrapolation for the moment.
It seems that MNIST for a given amount of variance explained and for a given dimensionality MNIST requires more principal components.
For a given amount of dimension yeah.
If we fix the dimension and we fix the percent variance explained MNIST requires more principal components than ImageNet.
That seems to me to tell me that MNIST is a more difficult problem.
Is that not true?
I think it's a bit so for a specific number of dimensions you could argue that.
But the thing you have to recall is that because MNIST images are much smaller in spatial dimensions.
If you have a 16 by 16 patch you have basically the whole MNIST dataset let's say.
And so just having a few principal components is not enough to really reconstruct the whole MNIST dataset.
Now on ImageNet a 16 by 16 patch is almost a constant texture right.
You have a few different colors but you don't have a lot of variation.
It's basically through the spatial dimensions it's basically constant.
And what this means is that with only three or four principal components basically one for each color channel.
You can perfectly reconstruct all those 16 by 16 patches.
So to really get to the conclusions you are saying what you will have to do first is to either done sample ImageNet to be 28 by 28
or up sample MNIST to be 224 by 224.
And if you do that then basically I think you will have the roughly same evolution of the interpolation regime.
Because the 16 by 16 patch on this extra up sample MNIST image will be either completely black or completely white.
And in this case you will still need a few principal components.
So this is also something very interesting right because it shows that maybe for the task at hand
you might not need to have such a high resolution image you might done sample.
But because when you done sample you basically keep those in crucial information
you don't necessarily go faster to interpolation regime.
So this is another point.
It's really tricky you should think of it as how much of the image do I uncut given that number of dimensions.
And then given that this plot might be easier to understand.
16 by 16 by each.
Yeah exactly so 16 by 16 on MNIST is maybe around 60-70% of the image.
While 16 by 16 on ImageNet is maybe around maybe 5% of the image.
And that's why you have those different regimes that appear.
Incredible.
Randall thank you so much for joining us this evening.
Sure sure thanks.
So we just spoke with Randall what was your take on that Keith?
Absolutely brilliant it was a true pleasure to speak with him.
For me it cleared up a lot of thoughts and issues I was having with this paper.
So for example right up front what he says is my purpose behind this paper is to show that even though the intuition that people have of interpolation
like the intuition that we have of interpolation is good.
The mathematical definition that we have of interpolation is not useful in high dimension.
What I thought was interesting too is when we asked about well what about the manifold concept.
You know why isn't that sort of the definition of interpolation.
And he brought up a really strong argument there as he said well let's just take the simple case of suppose the problem you're trying to solve
just is linear you know like it just everything is a linear data set linear problem we're trying to solve like even in that case
this linear case in high enough dimension interpolation doesn't work.
And there your manifold is just literally a convex hull you know and so sure you can have kind of a nonlinear transformation
and a nonlinear shape and whatever you're still hit by this curse of dimensionality.
And he you know he brought up the point that like you know of course if your problem compresses down enough to where only a small number of
of transformed dimensions right latent space dimensions matter and everything then you can be said that you're you're interpolating
you know because we're not really hit by that curse of dimensionality because we stripped away all the dimensionality down to these dimensions
we've gotten lucky our problem space or data samples etc allowed us to do that okay but that's not going to be all problems
like some problems may just intrinsically have high dimensionality of interpolation that's not useful.
So we need to do something better we need to we need to come up with a definition of interpolation that maintains the intuitive notion
that we have of interpolation but that continues to work in high dimension.
And you know and he made a very you know very interesting kind of end goal here which is like if we can get a definition of interpolation
that ends the approaching this concept of generalization you know that's that's what we're trying to achieve really.
And what do you think his take on generalization is then.
Well as he made it you made a comment that you know on the one hand you could just ridiculously specify the space and make it
trivially separable but then you lose generalization.
Well yeah and I think I mean because because we did ask or you asked you know put the question what if what you're what if the problem
you're trying to solve is the ability to solve novel problems like you know what happens in of course that was in the context of our
discussion about hybrid systems where you're combining the continuous with with discreet and they're able to ping pong and kind of modify each other.
And I think what we got from him there and consistently really throughout throughout our discussion.
And what's interesting is this this totally aligns actually with Francois Chalet as well which is that this all these questions like a lot of these
very difficult questions that we're asking are problem specific and there is no.
We don't currently have some one size fits all set of concepts that fits well for it for every every problem space you know it's very task specific
very problem specific you know data specific.
So I think that's an area where we got dive deeper with him but I didn't hear anything definitive you know today.
It's a wrap we just interviewed the Godfather of deep learning.
How's that possible.
I think we can just quit now we might as well just shut the channel down.
Yeah.
I mean obviously after we've published it.
That's the singularity.
So what's your take.
I would have I would I think I think this but we also interrupted him a little bit.
Maybe the question was I would have loved for him to be more a bit unlike you know where's the actual disagreement.
Right.
Because a lot of times you know when we when we put put you know questions to him like OK other people say this he was like oh yeah I agree right.
And this seems it seems to be a general sentiment that's also when we talk to other people about you know perceived disagreements they're always very
you know being being being good being academics being also friends probably with a lot of people you always like yeah you know they have they have good point you know we essentially agree on all the
on all the things right but then I sort of want to know where actually do people disagree right if it's if it's you know in and that that that is a little bit I mean I obviously have a feeling but it's still a little bit elusive and
and if people disagree on the actual technical nature or more on the philosophical end of what do what does something mean.
I definitely think there is from hearing now and I think from his answer to that question.
I could hear a little bit in that he seems to be more optimistic on what these learning systems can do.
Then maybe other people are right because some people seem to have really kind of a hard stop on like this will never be possible with like a learning system.
Whereas it seemed it seemed it seemed that he had sort of a more optimistic outlook and said of course they can't do it now right.
However you know we work on it we modify them you know we're we're figuring out how do we need to build these systems such that you know a learning system can conceivably.
You know do many of the things that people would call a kind of reasoning and I think that's why he went into you know let me give you an example of reasoning to sort of show look here you know.
Here is here is an example of a reasoning that neural networks already do and that means that something like reasoning in general isn't too far away.
And that's ultimately where the disagreement might be with other people.
I mean I think that's that was my take on his answer to our first question like the you know why'd you write this paper.
Is I think and I may go back and watch this but I think essentially what he was saying is you know sometimes people.
They have some definition in their mind whatever it is of interpolation and extrapolation.
And they come up with kind of an argument to say see the this this machine learning is doing interpolation and in a binary sense it can't do extrapolation.
And so therefore there's this entire class of things you know that it's not capable of doing and I think that's what his objection was which is look like this is not a useful distinction to say between.
You know it's not a useful distinction to draw between quote unquote interpolation and extrapolation like here's an example of a definition right that's a relatively standard definition of interpolation and if we apply this interpolation to.
Theoretically what machine learning is doing and empirically to what machine learning is doing it's always extrapolating.
So I think that's what he's objecting to is just like look don't come to some strong conclusion about what.
Machine learning neural networks I'm not sure what the right way to phrase things is now what it's capable of doing like we just need to do more work to expand its capabilities.
Folks like Gary Marcus making this case of we need discrete models because they can abstract broadly they give a couple of examples you know binary encoding for numbers.
And there's one example with can you have a model that reverses the bits or I think there was another example in his algebraic mind which was about can you.
Generalize from the even numbers to the odd numbers and you can't do that and the reason you can't do it is the new examples are completely outside of the training space of the input data.
And probably that problem is not interpolative or it's not differentiable.
I mean what lead sub is the same thing he says in language processing in particular can like language understanding in order to have broad generalization you need to have abstract rules.
So for example being able to generalize from Mary loves John to Mary loves Jane they make the argument that with a statistical approach that just wouldn't be possible.
Well I think so I think and this is just my opinion but I think his answer to my digits a pie question kind of shed some light on what his position is on this which is that it's too early to theoretically and scientifically for us to make that that determination
because he said look okay people only been able to calculate digits of pie you know like within the last hundred years or whatever I'm not talking like we're not there yet I'm not talking about that class of problems I'm talking about these.
Try numbers as well.
Right so I'm talking about these things like a cat jumping up here and so some of these examples that Gary Marcus may bring up may fall into that even though they seem simple okay they seem deceptively simple.
They may still fall in the bucket of these kind of much more towards the discrete spectrum of capabilities which we can't currently do with with our methods and machine learning but we're going to continue and continue to get closer to that that into the spectrum.
And so it's and so it's premature to say that like machine learning and again whatever you can see that to be neural networks whatever will never be able to get there.
Like he and he and Jan is optimistic that we'll be able to get there I'm not as optimistic I think there is a very there's just a qualitative difference between you know structural topological discrete reasoning and continuous differentiable reasoning and I don't know how we're ever going to get that gap bridge to brought up some some methods to think about but.
Right but the key question is whether it's possible at all.
And there was an article by a guy called Andrew Yee I think and it really annoyed you and the Coon and he said look people are still making the argument that neural networks interpolate and in that article.
He was basically saying well neural networks are a little bit like a Taylor series approximation you take a function and you just kind of approximate it you know inside a certain range and then it goes haywire outside of the range.
And Francoise Relay came on the podcast and he said look you know it's a little bit like in Fourier analysis when you try and you kind of like.
You take these little signs and cosines and you fit it to a discrete function and it's glitchy right and actually if you look at harmonic analysis this is what.
As you and Bruno said on the show on the on the geometric show he said that what a neural network even the universal function approximation theorem it's all about stacking these basis functions together right to kind of approximate some target function well if that's all neural
networks do how could they possibly generalize.
Look I don't want to use the word like generalized I don't know that what I would say is this which is that.
No matter how discreet in appearance the human brain is you know all the signals that the neurons generate receive whatever you know at the end of the day.
Are continuous functions I mean they're you know a charge that has a continuous function.
Now somehow or another like the brain in its structure does take that continuous analog.
I should have said analog you know probably there were but it takes that analog computational environment and produces digital reasoning so I don't think it's beyond that like I just don't see how anyone could reach the conclusion that it will be impossible like to do.
Discreet reasoning with with neural networks I think it's just a question of like for me I think is more of a question is that the efficient way to do it like you know if you fast forward 2000 years from now when people have figured out all these problems and we have like you know
just walking around killing us or working for us whatever the thing may be you know are they going to be using a neural network for everything or is it going to be a neural network with you know some classic digital compute components with some other stuff like a hybrid kind of structural system that does things I think is more what I'm asking.
Like Lacan hint and they of course that that's what I was going to say they have like nature on their side in that in no matter how much we cut open the human brain we don't find like a discrete computer in there like of course the individual neural spikes are discrete like there's
sort of charge or no charge but then there is like continuous release of neurotransmitter so I agree like the brain is like a very continuous distributed machine and there is no no there's no discrete thing in there there's no part of the brain where it's like look one one brain brain
he said that and there is a level at which that's true which is there's a spike train and you know we can kind of recognize a spike because it's a maximum in this analog dimension but my point is that still an underlying analog dimension and so I don't see why in principle you couldn't build a neural network that you know has like these kind of continuous
values but still ends up with something that that's that synthesizes a a discrete you know decision sure but wouldn't it be glitchy and it still wouldn't extrapolate.
I have no idea like what you know I just don't know I think I think this kind of point is we're too early on to reach these conclusions that it cannot be done.
I do sort of feel that it's just not the efficient way to do it that you know we're going to end up with a case where we're going to have systems that have analog continuous chunks that are neural networks or whatever and we're also going to overlay that with
with digital computation that's implemented by the typical kind of digital computer that's going to be these hybrid systems working together just like you brought up with hey look you know alpha alpha zero alpha go all those things or it can be viewed in the same kind of hybrid way right.
But that's exactly I mean you said this is what you said and also what when he when he said like so first when he said you know we're far away from that.
With the digits of pie and so on but also when he said you know humans are actually pretty bad at chess or at discreet exploration in general and that is.
That's how humans do it right humans build discreet reasoning.
On top of this sort of neural continuous function and it's actually really hard like to do discreet reasoning in your head is you can do it slowly and you have to do it deliberately with attention.
If you do like multiplication of five digit number in your head right this is not a this is not a oh my god feeling.
This is like you like you sit there and you have to like keep all the stuff.
It's not a hash table look up like the multiplying numbers of the thing and we can like we can build children's toys like that for like 20 cents that multiply 10 digit numbers easily right so I like yeah I think this is.
Yeah I think this is we teach children point that way is we teach them hash table look up like memorize you know yeah all the numbers from one to a hundred multiply or something and then we teach them an algorithm.
You know we teach them this exact thing and we teach them hash table look up and and this is built on so if you and I think that if you build if you build this discreet reasoning you can probably build it on top of these models right but yes is it the most efficient way to do it.
Maybe maybe not right but then again if you do it.
If you manage to do it train it you seem at least from at least from from the perspective of humans you seem to have something insanely powerful right because you know if you just have a discrete algorithm you have the algorithm but if you have a.
Discrete if you've actually managed to train a discrete algorithm on top of this continuous function it will be able to sort of learn in the same continuous way that you know a neural network learns but you know it will be sort of be able to self modify its discreet algorithm and that.
Yeah I think you know I agree I'd also be in the maybe longer term future positive that might be possible.
I think that's because I said to him and do you think alpha goes a good thing do you think is a good thing to have a kind of.
Discreet search which is guided by a neural network and he didn't like it and I assume he didn't like it for the reason you just said you and it which is that the discrete part of it is hard coded and it's been told to do a specific task it's it's not learning.
I think look who would rather have something which is entirely differentiable right so because.
Look who wants learning end to end in a in a in a neural network right yeah and and also it's almost like he was saying before well.
Humans can't do discrete reasoning so it would actually be a more sophisticated form of intelligence if we had discrete reasoning but do we necessarily even want that in our models or is it just the fact that he wants it in the models but he wants it to be differentiable and learnable as part of the main model.
I think so because that's where he got with the you know the critic actor critic type models I think if people nowadays talk about differentiable they always.
Talk about sort of they always talk about like single shot feet forward like you know I input something into the machine and outcomes of prediction whereas he also makes a lot of arguments for you know this.
Energy minimization which which means that essentially at test time at inference time I do a minimization algorithm and you know when he when he talked about reasoning he was mentioning that.
As an example as essentially saying look I have my trained neural network and now at inference time I actually still perform an algorithm on top of that right to minimize some energy function given my trained model of the world let's say so.
I think you know we might be able to make that more learned by also learning the algorithm that we do at inference time but I don't necessarily I don't necessarily think that he's talking about you know we need.
Like we need to end to end learn like a machine where you simply input something and then out pops through forward prop out pops and all of these things are just reasoning by the back door.
It's even with a critic as I understand that that's just the way of hacking that the value the advantage function right you know with some.
I'm not an expert in reinforcement learning but I think that's what it is the same thing with the energy minimization.
Yeah I kind of agree with you there.
Yeah but the energy minimization has this particular thing of this idea which I think today is underexplored of to actually do something at inference time.
Like to not just forward prop and that I feel it's I feel it's under underexplored nowadays.
I agree with you there too but I think what kind of sometimes I feel like this is feature creep in a sense it's like you know we have neural networks and we know what deep learning is and some people now want that to be redefined it's just a general purpose research paradigm that includes all possible things that we can do with.
You know machines or chemicals it's like what use is that like what use is it defining you know all types of computation is as differentiable computation like we lose some ability to talk about these and sort of the same way that was kind of like what I was saying you know nobody I mean look.
Allowing for a variable number of layers in a neural network that's discrete computation right that's not differentiable training nobody knows how to train in like an arbitrarily unbounded number of layers in some differentiable way right.
But there's the neural ODE stuff.
Well all those types of things run into big training problems like things become really hard to train when you have these types of yeah of essentially discrete you know transitions right or combinatorial kinds of kinds of transitions.
And this is classic stuff I mean this is really classic stuff it's like even if you try to do mixed integer optimization right so you have a problem that has some combination of integer variables discrete variables and some combination of continuous variables.
There's all kinds of hacks to try to do that in differentiable ways.
They don't in general arrive at the optimal discrete solution like you do some continuous stuff and then at the end you kind of discretize it in one way or another and you wind up with a solution that's kind of approximately correct.
But you're not guaranteed that you're going to find the one that's actually correct if you discreetly combinatorially examine that that space right.
Can we finish by talking about a couple of things so first of all there's table one and the structure of the latent table one is the biggest.
The biggest argument against the most prevalent argument against this paper sorry for the triple negative but you know when when you know when the paper starts out by saying you know we build a convex hull of the training data.
If your point is not in the convex hull you know not interpolating and then people go wait wait wait wait wait but you're talking about the input space of data you're talking about the pixel space and you know for sure we're not talking about the pixel space.
Because you know the neural network is you know the data manifold and who but we're talking about you know if you go to the last layer to the latent space before the classification it's just a linear class.
You know there in that space we're talking about like interpolation right there is where the neural networks interpolate in this experiment clearly shows like.
Okay might be a rigid definition of interpolation but it clearly shows like no even in that space on a trained Resnet there is no like no interpolation going on as soon as you pick like 30 dimensions or more.
It's all outside of the convex hull of training right but but this this this gets to the punchline right because Keith was going to ask a really cool question which is like well imagine it was a very small latent and it was interpolating.
And then what if you up projected it like to one or two four dimensions and that you know with with that suddenly now be extrapolating right so that we almost need to have an information theoretical.
Theoretic way of looking at this but anyway but my intuition is that deep learning models encode the most high frequency information into the latent space right and you know this information would be encoded in a minimally distributed way.
To denoise the predictive task which is to say right you know there's a few dimensions of the latent.
You know which should be encoding the actual things that that you trained it on so my intuition is actually most of the dimensions of the latent are kind of just encoding low frequency information so you can discard them right and I know you said the other day well
Well maybe the whole point of neural networks is there a distributed representation so maybe they are distributed overall of the layers overall of the dimensions in the latent.
I mean I would I would actually argue not that neural networks are putting the data in a minimally distributed way but in like a maximally like why I think just just my intuition is that a neural if if I were a neural network and I had all of these dimensions at my disposal.
I would encode lots of information redundantly if it like if it were too much space.
I would like encode the same information pretty much redundantly in many of the dimensions that I could noisy right so you're taking a soft max and if you're noisily aggregating overall of those features that you don't want to do that.
Yeah but still if I have backprop right and the backprop path goes through each of the dimensions so for each of the dimensions I'm asking myself how can I change this one to make my prediction even better right.
It doesn't matter if over there one of the dimensions is already doing the classification for me right the backprop the nature of backprop means that you know I'm going to every single dimension independently and deciding how can I change this one.
To make it even better so that's why I think lots of information if the latent is too big lots of information will be encoded right there is if the latent is too big and so this is where all the pruning literature comes into which is that.
The majority of the time people are running neural networks and situations where the latent space is too big.
Like they're just you know we just flat out have far too many you know far too large of a latent space and so even though it may be encoded you know densely there like it may be putting all kind of little bits of extra information it's probably only adding little plus or minus you know 0.1% type things to the accuracy.
Whereas and so since it is only adding these little kind of very tiny values there right.
The only meaningful way to talk about interpolation or extrapolation because you've only got little bits of stuff you're using on that entire dimension that you've added there is really more just are you interpolating on the most salient dimensions.
You know which is again back to my question about why are we you know why are we concerned with whether or not every single dimension falls within within the sample range.
Yeah exactly but there's a few things that I mean it's really good that you bring up sparsity I think that's fantastic that you brought that in because as you say most of that information is redundant.
But then what Yannick was saying was interesting does distributing all that information rather than my intuition is it increases noise but actually I think Yannick saying it increases precision.
But in a very tiny tiny like it's a tiny tiny contribution so let's suppose the latent space is not too large right like if the latent space is not too large.
In other words it's let's say it's just barely big enough to classify your images and let's suppose you're doing multi class so we've got 10 10 10 classes you know and we just barely got enough latent space for it.
My intuition would tell me that if those classes are somewhat different from one another like it's not we're classifying brown dogs from white dogs from like every other you know simple kind of dog but they're different from each other and it's not easy to determine.
You know which is which that what would and just just to guess is it probably what would happen is you would find that you know these five bits are kind of the ones that tell you whether something is a dog and these seven bits tell you if it's a duck and these four bits tell you if it's a gun and these five
bits tell you if it's an image of the sky like I would guess that it would end up encoding it that way since they don't have a lot in common you can't really you can't really use entanglement like too much except for a few dimensions there might be some overlaps but.
Yeah I think I think what what people what people mean a little bit is when they when they start going down the interpolation road is that you know we've played with GPT three as well right and then you you do something with it you enter something and in some places you're like.
I see how you did that right you like you just you just took that that newspaper article and you just kind of replace some stuff in it right like you you sort of where whereas if you were to to talk to like a human you you sort of.
Like that stuff wouldn't happen as much even and then of course there's there's the argument I think people that say well it's just interpolating or it might also be you know it's just sort of repeating the stuff in the training data.
I think what they what I'd like to see is more like the pattern that these models extract aren't sufficiently high level for for them right and then I think the entire discussion is can we get to.
Arbitrary high abstract levels of pattern recognition with such models if we engineer then train them in the correct way or is there is there I guess some fundamental limitation to that and yes as we said that the answer the answer might be might be quite far off as as for the.
Number of you know latent dimensions and so on I I mean I agree with with Keith I think.
Having a big latent space and also having big weights and so on is might be more of a of a necessity for training.
Then it really is for for encoding like it appears to help if we have lots of weights to train such that sort of we get to combinatorically try out combinations of weights only few of which might ultimately end up being important right.
That's kind of a lottery hypothesis sort of way of going down so yeah I agree you know most of most of the information ultimately might be.
Only contributing a little bit right but my my my intuition would meet be that you know this is kind of because it's kind of redundant information right because it's like you know I'm encoding this over here I'm also encoding it in like a tiny little different way over here.
Or or some add on or some uncertainty or some one training samples a bit different so I'm going to put that right here.
Yeah that's fascinating what you said about Frank whose work that I never really heard you articulate it like that but it is actually kind of like a search problem rather than a learning problem what you're doing is you're giving it all of the possible you know you give a random initialization on a densely connected network and you're saying you know just go and find the ones that work rather than create it from first principle.
Yeah that's why initializations are so important right is is because you sort of you sort of try like your initialization essentially is is your your your buffet for SGD to like choose a good one from to then go ahead and refine.
But how much refining is it is it more is it more finding things that already work versus refining.
Yeah it's it's it's well the same but what it is not is sort of learning from scratch that that's what like people like we cannot you cannot initialize neural network from zeros and then have it have it learn well at least not today maybe that's going to come in in some form but initialization is actually pretty
important pretty crazy and that's crazy and that's yeah that's like one hint that you know we're we're not essentially we're never essentially training from scratch where we're we're sort of training we're sort of giving the choice of many combinations of good weights or of semi good weights and it has to pick sort of the good ones to continue exactly it's already pre trained you just don't know which.
Yeah I mean that the argument that a little bit of the argument against that is in in sort of the evolutionary approach where you say you know you can make the argument you know humans have sort of develop these abilities to reason to recognize super high level patterns while only having a continuous brain.
But then the other side of this is yeah but it's not like a single human that has achieved that right it's not like one single learning system that has achieved that but it's actually like this evolutionary system which is in essence a massively distributed combinatorical.
Trial and error search right and that is that is not a learning system so to say as we imagine it today it at the end you end up with a learning result but.
The evolutionary algorithm is way different than we imagine learning it's it's not even all it's not even all humans that have learned it it's it's all you know individuals of all tens of millions of species that have ever lived on earth that have that have learned.
Which is pretty much like like the lottery ticket hypothesis let's just let's just randomly train crap tons of you know weights and then slowly prune them and see what happens.
Right brilliant well gentlemen it's been an absolute pleasure I guess I guess we'll make this a Christmas special I mean it is pretty special.
Oh yeah let's be honest so yeah this is gonna warrant a hat we're gonna sign off.
Nice meeting you.
Indeed but thanks for bearing with us folks we have had a couple of months off I've had a bit of a break and you know the guys have had a bit of a break so yeah we're back.
Good to see you all again.
Peace out.
