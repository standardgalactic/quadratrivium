start	end	text
0	9720	Let's have Melanie Mitchell to give our final opening statement, six minutes on the clock,
9720	10720	Melanie.
10720	13860	Yeah, so this is my opportunity to say I love Melanie.
13860	19400	She is amazing, and she's coming back on MLST in about two weeks.
19400	20400	Oh, amazing.
20400	21400	Yeah, that's good.
21400	23840	Yeah, she gives a good representation of herself in here, I think.
23840	24840	Melanie is amazing.
25520	30720	Fears about machines unleashing human extinction have deep roots in our collective psyche.
30720	34480	These fears are as old as the invention of machines themselves.
34480	40360	But tonight, we're debating whether these fears belong in the realm of science fiction
40360	48680	and philosophical speculation, or whether AI is an actual real life existential threat.
48680	55480	I'm going to argue that AI does not pose such a threat in any reasonably near future.
55480	61320	Large language models have sparked heated debate on whether AI's exhibit genuine understanding
61320	63960	of language and the world.
63960	69420	With capabilities rivaling humans across diverse benchmarks, some hail language models as
69420	72320	harbingers of real intelligence.
72320	78400	But skeptics argue that their mastery is skin deep, lacking true comprehension.
78400	83560	So how can we assess these claims and gain insight into the nature of what it means to
83560	84560	understand?
84560	91000	Now, on the show today, we have Professor Melanie Mitchell, a leading thinker on AI and intelligence,
91000	95440	and one of the researchers in the community I personally most align with and look up to
95440	97240	the most.
97240	103520	Melanie's distinguished career crosses computer science, complex systems, and cognitive science,
103520	108620	and she wrote the influential books Artificial Intelligence, A Guide for Thinking Humans,
108620	111680	and also Complexity, A Guided Tour.
111680	117320	Now central to Melanie's perspective is the idea that human understanding relies on flexible
117320	121560	mental models grounded in sensory experience.
121560	128640	Now she wrote that understanding language requires having the concepts that language describes.
128640	134040	Large language models are trained purely on statistical relationships between words.
134040	138600	Their knowledge is not grounded in a causal model of reality.
138600	144320	Now Melanie is the Davis Professor of Complexity at the Santa Fe Institute, and her major work
144320	150600	has been in the areas of analogical reasoning, complex systems, genetic algorithms, and cellular
150600	152100	automata.
152100	155480	She's achieved legendary status in the field of AI.
155480	161320	She received her PhD in 1990 from the University of Michigan under Douglas Hofstadter, the
161320	164240	famous author of Godel Escherbach.
164240	169600	Melanie argues that we must rethink how AI systems are evaluated.
169600	174480	Typical benchmarks summarize aggregate performance and, you know, these obscure failure modes
174480	177680	and mask the underlying mechanisms.
177680	183600	We need rigorous granular testing focused keenly on abstract generalization.
183600	187760	Sort of like a sorcerer's apprentice gone nuclear.
187760	192120	For example, Yoshua Benjiro wrote about this thought experiment.
192120	197360	We might ask an AI to fix climate change and to solve the problem it could design a virus
197360	206360	that decimates the human population, presto, humans dead, no more carbon emissions.
206360	214280	This is an example of what's called the fallacy of dumb superintelligence.
214280	220400	That is, it's a fallacy to think that a machine could be, quote, smarter than humans in all
220400	227320	respects, unquote, and still lack any common sense understanding of humans, such as understanding
227320	232840	why we made the request to fix climate change and the fact that we prefer not to be wiped
232840	235040	out.
235040	242200	This is all about having insight into one's goals and the likely effect of one's actions.
242200	247680	We would never give unchecked autonomy and resources to an AI that lacked these basic
247680	249480	aspects of intelligence.
249480	252320	It just does not make sense.
252320	253320	The third scenario.
253320	254760	Yeah, that is absolutely.
254760	258640	She made that point so much more eloquently than I've tried to make it in the past.
258640	262160	Yeah, even earlier in this conversation, I was trying to get that across, but that's
262160	263160	exactly it.
263160	264160	It's this dumb superintelligence.
265160	266160	Yeah, exactly.
266160	271880	Anyway, folks, I hope you enjoy the show, and now I bring you Professor Melanie Mitchell.
271880	277920	Sounds like almost like there's a very quiet supercomputer running behind the screen.
277920	278920	It's my brain.
278920	279920	Yeah.
279920	281920	I think that's what this is.
281920	286320	You know, we can robustly adapt much more so than GPT-4.
286320	288320	You and I have the same chair.
288320	290320	We have the same chair, I think.
290320	291320	Oh, yeah.
291320	292320	I can't see your chair.
292320	293320	Yeah.
293960	294960	Me too.
294960	295960	They're home in Miller.
295960	298600	Yeah, I think they're all the same chair.
298600	299600	Yeah.
299600	300600	Yeah.
300600	301600	Excellent chair.
301600	302600	Yep, chair buddies.
302600	303600	Yeah.
303600	310840	I felt it would be hundreds of years before anything even remotely like a human mind would
310840	317000	be asymptotically approaching the level of the human mind, but from beneath.
317000	325680	I never imagined that computers would rival or let alone surpass human intelligence, but
325680	329320	it seemed to me like it was a goal that was so far away.
329320	335040	I wasn't worried about it, but when certain systems started appearing and then this started
335040	341400	happening at an accelerating pace, it felt as if not only are my belief systems collapsing,
341400	350560	but it feels as if the entire human race is going to be eclipsed and left in the dust.
350560	355160	Douglas Hofstadter, he came out as a doomer.
355160	359320	Well, I don't know if he came out exactly.
359320	362040	He's been a doomer for quite a while.
362040	363520	Oh, go on.
363520	365320	I wasn't aware of that.
365320	370000	Well, I don't, you know, doomer is, you know, there's different kinds of doomers.
370760	376500	In my AI book, the first chapter, the prologue is called Terrified, and it's all about how
376500	384720	Doug is very terrified about AI and the possible things that are going to come.
384720	394600	That was based on a talk he gave in 2013, and earlier than that, he was extremely worried
394600	401800	about the singularity, the idea of the singularity from Kurzweil, and wrote quite a bit about
401800	402800	that.
402800	409040	So, I feel like that it's not that new, but maybe this is sort of because there's so much
409040	414960	talk about AI, doom, and so on, that this is kind of, people are kind of paying attention
414960	415960	now.
415960	419480	Yeah, I don't know whether I misunderstood something because I read out, you had this
419480	425040	beautiful piece about the Googleplex in Chopin, and he was terrified that cognition might
425040	430560	be disappointingly simple to mechanize, and, you know, surely we couldn't replicate the
430560	436120	infinite nuance of the mental state that went into writing that beautiful music.
436120	440040	But so maybe he was worried about it, but he didn't think it was possible in principle
440040	441040	or something.
441040	446960	Well, no, he was quite worried about that it was going to happen sooner than he thought,
446960	452400	and that, you know, his quote that it's AI is going to leave us in the dust.
452400	454640	So that's kind of his flavor of doomer.
454640	463720	I'm not sure he has the same, like, existential worry about things as, like, Stuart Russell
463720	464720	or somebody.
464720	465720	Okay.
465720	472760	So he's not so worried about them necessarily churning us into, you know, fertilizer or
472760	476920	raw materials or something, but just that it's not so specific, I think.
477880	482680	But yeah, yeah, I talk to him about it all the time, and he wavers.
484000	488480	Oh, interesting, because I've heard you define yourself as a centrist on other podcasts,
488480	495320	because I'm sure the doomers would lump you in with Cholet and Lacune, maybe, and some
495320	501000	of the critics, but you do think that these models are intelligent, right?
501960	503560	I do think that they're intelligent.
503640	509400	Well, you know, intelligence is an ill-defined notion.
510040	510520	Oh, yeah.
510520	516040	It's multidimensional, and, you know, I don't know if we can say yes or no about something
516040	521680	being intelligent rather than, you know, intelligent in certain ways or to certain degrees.
522640	523000	Yeah.
523000	525280	Well, we've got so much to get into.
525280	532440	I mean, I think slowly we'll talk about arc and your concept arc work, but I kind of agree
532440	536600	with you that, and actually you had that paper out about the four fallacies, and you spoke
536600	543400	about this fallacy of pure intelligence, and I kind of agree that the gnarly reality is far
543400	544360	more complex than that.
544360	549800	There was a really interesting paper that you linked on, no, it was an article by Dileep George,
549800	553960	and he said that a university professor has a much better understanding of a vector,
554520	560280	because it's just grounded in so many real-world situations and contexts and so on,
560280	564680	and an undergraduate or, indeed, a language model would have a very ungrounded, very kind
564680	571800	of low-resolution idea of what this concept is, and it kind of leans away from this puritanical,
571800	577240	ungrounded, abstract form of intelligence to something which is really very complex and intermingled.
577960	578360	Yeah.
578360	579560	I mean, I agree with that.
581080	585160	Well, except that there's another aspect to that, too, which you write about, which is,
585960	589720	I agree that that happens, but what the human mind also seems to do is,
590360	596920	as the thing becomes more grounded in more cases, then we develop yet another concept
596920	603480	that kind of describes the similar aspects that we see throughout all those different
603480	604280	concepts, right?
604280	608920	So we're kind of this iterative loop where we're always finding more and more context,
608920	614520	and then we're also finding newer and newer concepts that span those increasing contexts.
614520	615320	Is that fair?
615320	616280	Yeah, sure, yeah.
617000	621080	I mean, that kind of goes along with the whole sort of
623160	631000	metaphor theory of cognition, of Lake Offit at all, and that we're sort of building on these
631000	634440	physical metaphors, and we can build up many, many layers of abstraction.
636760	638280	So, yeah, we can talk about that.
639800	640920	We're not recording yet, right?
641560	642280	Oh, no, we are.
642280	642520	We are.
642520	643240	This is all recording.
643240	643800	Oh, we are?
644040	645800	I hope you're okay with what you said so far.
645800	651960	But yeah, so there's the Lake Off building on the body of symbols as pointers.
651960	655400	And by the way, that Dileep George article was really fascinating because it was saying
655400	657560	that language is a conditioning force.
657560	662360	So actually, we all have these high-resolution world simulators built into us, and we kind of
663000	667960	condition how that operates and generate counterfactuals through language,
667960	669240	which I thought was quite interesting.
670120	670440	Yeah.
671400	671880	Yeah.
671880	675000	But, Tim, why don't you frame up the debate?
675000	676440	Because we found a beautiful paragraph.
677080	677480	We did.
677480	677720	We did.
677720	679640	We found an amazing bit.
679640	680440	Yeah.
680440	684200	But just to close the loop on what I was saying, we were discussing an activism last night.
684200	687560	I'm not sure if you're familiar with some of these externalized forms of cognition,
687560	690200	and we were talking about the concept of a goal.
690760	695480	And agents, of course, they just have these high-resolution belief trajectories of,
696040	697880	you know, I can do all of these different actions.
697880	699320	And that's not really a goal.
699320	704520	You know, a goal is this very abstract thing which emerges at the system level,
704520	707880	and no individual agents in the system have a concept of a goal.
707880	711000	And it might be similar with some of these concepts that we're talking about,
711000	714600	which is, to what extent do they exist, and to what extent are they just
714600	718680	something intelligible that we can point to, but they don't really meaningfully exist
718680	720200	in the system at a high-resolution.
721480	726200	Are you talking about an AI or in people, a little piece here?
726200	726760	All of the above.
727720	730840	I mean, I think goal is a wonderful example, because we think of it.
730840	733560	I mean, it's even one of Spelke's core priors.
733560	735400	It seems like something so primitive.
736520	740440	But I don't think they really do exist in us.
740440	745560	I mean, we're interesting because we have this reflexive conception of a goal,
745560	748120	but does a mouse have a goal?
750520	756680	Right. I mean, goal is another one of those words that, you know, we use in a very fluid
756680	761160	way. So we talk about, for instance, a reinforcement learning agent having a goal
761880	768120	that we've given to it, right? Or it might have a goal to kind of maximize its information gain
768120	776440	or something. But is that the same thing as a human having a goal that it's like,
776440	784760	you know, to graduate from college or to, you know, make something of your life for
784760	788840	all of these things? It's a very different sense of goal.
788840	797960	And so I would say, yes, a mouse has goals, but those goals are different in degree and in kind
797960	803240	of qualitatively than many of the things we call goals in humans and in machines.
803240	809720	So I think goal is one of those sort of anthropomorphizing words that we need to
809800	816600	be careful about when we equate goals in these different systems as being the same thing.
817160	821480	And I actually, you know, had a discussion with, I think it was with Stuart Russell
822760	829960	about the notion of goal. And his view, and I think this is a view of many other people
830840	837880	in AI, is that large language models actually have goals, complex goals,
838440	849000	that they, that emerge out of this, you know, their loss function of predicting the next token,
850120	856440	because the only way to successfully predict the next token in human language is to develop
856440	862120	human-like goals. I find that dubious, but it's an interesting perspective.
862360	868760	Yeah, I'm amenable to it, because there's always this dichotomy, as you say, of there's the objective,
868760	872840	there's perplexity, and there's these emergent goals, and there's even this simulator's theory
872840	878040	of large language models, which is that they're a superposition of agents. And it's quite situated
878040	883240	as well, because goals kind of materialize depending on your perspective. So if you use a
883240	887320	language model in a certain way from a certain perspective, it might appear that there is some
887320	892200	kind of goal there, but of course, it's just an aspect onto something which is very complex.
892760	896760	But I think we should frame up this beautiful piece, actually, from your
897400	901560	Modes of Understanding paper from much this year. I always call it the Modes of Understanding paper.
901560	906040	It was actually titled The Debate over Understanding in AI's Large Language Models.
906040	911480	And you said, towards the end, that the key question of the debate about understanding in
911560	918040	large language models is, one, is talking of understanding in such systems simply a category
918040	922920	error, which is mistaking associations between language tokens for associations between
923560	930200	tokens and physical, social, and mental experience? In short, is it the case that these models are
930200	936760	not and will never be the kind of things that can understand, or conversely, to do these systems or
936760	941720	their near term successes? Actually, even in the absence of physical experience, create something
941720	946520	like the rich concept based mental models that are central to human understanding. And if so,
946520	951800	does scaling these models create even better concepts? Or three, if these systems do not
951800	957320	create such concepts, can their unimaginably large systems of statistical correlations,
957320	962760	produce abilities that are fundamentally equivalent to human understanding, or indeed that enable
962760	968120	new forms of higher order logic that humans are incapable of accessing? And at this point,
968120	975560	will it still make sense to call such correlations spurious and the resulting solutions shortcuts?
975560	981400	And would it make sense to see these systems' behaviors not as competence without comprehension,
981400	986280	but as a new, non-human form of understanding? And you said that these questions are no longer
986280	991720	in the realm of abstract philosophical discussions, but they touch on very real concerns about the
991720	997960	capabilities and robustness and safety and ethics of AI systems. So let's use that as a leader.
997960	1001480	What do you think, Melanie? It's beautiful. That was a beautiful paragraph, by the way.
1001480	1005720	Yeah, it's so good. Wow. This exactly crystallizes the discussion.
1008280	1015400	Yeah, I think that it's something that we in AI are all grappling with now. And I think it's
1015400	1022920	something that the history of AI has forced us to grapple with mental terms like understand,
1023720	1033080	or consciousness, and even intelligence. Because we keep saying, oh, well, understanding, if you
1033080	1043080	can do X, then that means that you're actually understanding. You can't do language translation
1043080	1050280	without understanding. You can't do speech to text without understanding. You can't generate
1050280	1059720	articulate natural language without understanding. And I think this is, in many cases, we then step
1059720	1064360	back and say, wait, that's not what we meant by understanding. It turns out you can do all these
1064360	1069080	things without understanding. So we're sort of saying, well, we didn't really know what we meant
1069080	1076600	by the term understanding, I think. And often, some people criticize that as moving the goalposts.
1078200	1080520	You're moving the goalposts. The so-called AI effect, right?
1080520	1082200	Right. It's the AI effect.
1082200	1091800	But I think of it more as AI is forcing people to really refine their notions
1092680	1097160	of that that have been quite fuzzy about what these terms actually mean.
1098200	1105320	And there was a fantastic talk by Dave Chalmers, the philosopher, who I think you've probably had
1105320	1113640	on this show, where he talks about conceptual engineering, which is something that philosophers
1113640	1119240	do where they take a term like understanding and they refine it. And he said, okay, well, we have
1120200	1128760	p-understanding, which is like personal, phenomenological. And then we have c-understanding
1128760	1134520	and e-understanding and x-understanding and all these different letters that meant to say that
1134520	1141480	this term is not a unified thing that we can apply to a system. We have to really specify what we
1141480	1148040	mean exactly. Well, one way I've come to think about it, and it's largely from reading your work
1148040	1152760	and your assessments about it, is that for the first time, we're actually being forced to do
1153720	1160920	the science of machine cognition, right? Because for too long, it's either just not been sophisticated
1160920	1166840	enough. Why bother? Like it's obviously not doing any cognition. And as you point out, it's now
1166840	1172520	actually having real world impacts. And so we actually have to start doing the science, right?
1172520	1177640	We have to say, okay, does this thing have cognition? Here's a hypothesis. Let's do some
1177640	1182360	test. Okay, it failed. What was the failure mode? Why did it fail? Let's understand that more. How
1182360	1187880	can we engineer it not to fail? It's like we can no longer ignore adversarial examples,
1187880	1191800	shortcut learning, et cetera. We have to finally grapple with it, it seems to me.
1192680	1197080	Yeah, I think that's exactly right. And what's interesting is we, computer scientists, were
1197080	1205480	never trained in experimental methods. We never learned about like controls and confounding things.
1205480	1218840	It's a great point. And so now people are doing, applying human tests of understanding or intelligence
1218840	1226520	or reasoning, what have you, to machines without having the right experimental methods to say whether
1226520	1233480	or not what they're testing is actually valid. So there's a cognitive scientist named Michael
1233480	1238760	Frank at Stanford, who's been writing a lot of stuff about experimental method and how do you
1238760	1248120	apply it to AI and why you need sort of expertise in this area to really make sense of these systems.
1248120	1254120	And I'm totally on board with that. Yeah, we'll talk about your piece with Tannenbaum later,
1254120	1259240	but as you say, a lot of AI folks don't really think about experiment design. But actually,
1259240	1263480	even with Chalet's ARC challenge, maybe we should talk about that. So he invented this
1263480	1268920	measure of intelligence, which unfortunately was not computable, but it was mathematically
1268920	1274760	beautiful. Basically saying that, and he's a huge Spelke fan, I kind of put Chalet very,
1274760	1280680	very close to you actually in AI researcher space. And his measure of intelligence basically says,
1281320	1286360	I give you priors, I give you experience, you give me a skill program, it extrapolates into these
1286360	1291160	different spaces and experience space. And the kind of the conversion ratio between those
1291160	1297000	priors and experience and the space that I get is intelligence. And that's very interesting.
1297000	1301880	And then he produced this corpus, this ARC challenge. And it's a bit like an IQ test. It's
1301880	1309000	this kind of 2D gridded colored cells. And you have a couple of examples, and you have to do
1309000	1313880	another one or two examples. And it was very diverse because it was testing what he called
1313880	1319400	developer aware generalization. And there were a couple of issues with that. So you wrote this
1319400	1325320	beautiful concept ARC paper, and maybe you can introduce that. But one of the things you pointed
1325320	1331880	out, which I felt was quite interesting is that even if people succeeded on Francois's challenge,
1331880	1336760	it wouldn't necessarily be what we would call intelligence, because it's not necessarily
1336760	1342120	demonstrating systematic generalization beyond those one or two examples in his test set.
1342600	1350920	So our motivation was twofold. So first of all, I love the ARC challenge. It's beautiful.
1351880	1363320	It's super elegant. And I'm very sympathetic with Francois' definition of intelligence,
1363320	1369080	although I think there's probably, again, intelligence is very multi-dimensional. But
1369160	1375400	this is one aspect of it for sure. And his problems are great because they
1376200	1382600	just give a few examples, and people are pretty good at abstracting a rule or a concept from
1382600	1391000	just a few examples. And they don't use language, so they don't get into the whole prior knowledge
1391000	1401720	of language and a lot of things that you don't want to confound these tests. But one of the
1401720	1406600	problems with ARC is that many of the problems are quite hard. They're quite hard for people.
1407320	1415720	And they're so hard that they don't really differentiate between different programs that
1415720	1422120	are attempting to solve this challenge. So there was a Kaggle competition with the ARC challenge,
1422120	1430360	and there were two, the two best programs got about, they each got about 20% accuracy on the
1430360	1440360	hidden test set. So it didn't really distinguish them at all. And the other problem was that,
1441320	1446840	as you mentioned, the test wasn't very systematic, meaning that let's say there's a
1447640	1453880	problem in ARC that deals with the concept of inside, something being inside something else.
1453880	1459800	And let's say that something, a program gets that one right. Does that mean that it understands
1459800	1466520	the concept of inside in a general way? Well, we don't know because the test doesn't test that
1466520	1474680	systematically. And that was actually intentional from Sholey, because he didn't want any way to,
1475400	1482920	for programs to be able to reverse engineer the generation process of these problems.
1482920	1488600	So if you say, oh, well, I'm going to deal with these 10 concepts, then somebody presumably
1488600	1496440	could reverse engineer those, the problems and not be general. But for us, we wanted to say, well,
1496440	1502200	how would you just systematically test a program for understanding of a concept of a very basic,
1502200	1508440	spatial or semantic concept? And so what we did was we took the ARC domain and we created
1509560	1517320	about almost 500 new problems that were systematically grouped into concept groups.
1517320	1524440	So like inside of, that was one of the groups. And so we looked at, we created several problems
1524440	1531560	that were variations on that concept. And there were variations that ranged in like abstraction,
1531560	1541080	degree of abstraction, and sort of complexity of the problem. And the hypothesis was that if a
1541720	1549880	human or a program could successfully solve the problems in a given concept group, they really
1549880	1557080	do have a good sort of grasp of that concept. So this was the genesis of concept ARC.
1558200	1563160	You know, it's fascinating because so you're, you're attempting again to build the science of
1563960	1568600	machine cognitive science, essentially. And hey, it has to be systematized, we need to have these
1568600	1574280	concept categories, we need to be able to generate examples of progressive complexity and, you know,
1574280	1579080	layer of abstraction, everything. And then yet you mentioned Chalet intentionally didn't
1579080	1582840	systematize it to avoid reverse engineering. And that's kind of a fascinating
1583480	1589480	point because reverse engineering can even happen, you know, just by way of selection bias. So I mean,
1589480	1593960	researchers are out there, they're fooling around with different neural network structures, maybe
1593960	1599160	I'll add like a you here or some horseshoe over there. And lo and behold, suddenly, it works
1599160	1604440	really well on the concept of inside out. And I'm going to claim this is machine learning,
1604440	1609320	even though it was actually human engineering that sort of put that structure into the network.
1609320	1614440	So in the long term, you know, how do we, how do we balance that? Or how do we avoid it? Or how do
1614440	1620520	we test for machine induced, you know, prior knowledge versus actual machine learning?
1621480	1630440	Yeah, no, I understand it's a hard problem. And I think, you know, the goal with this concept arc
1631720	1637560	benchmark wasn't to sort of supplant arc in any way, it was really meant to be complementary.
1638200	1647480	And it was meant to be kind of a stepping stone to the much larger and more difficult arc set.
1648040	1652760	Because I think, you know, even if I tell you all of these problems have to do with
1652760	1661240	the concept of inside versus outside, you would still have to have a good grasp of those concepts
1661240	1668600	in order to solve these problems. And I'm not sure that you could sort of engineer something
1669160	1677320	that would solve those cons problems of that concept in general, without having a more,
1677320	1681480	you know, really a general understanding in some sense of those that concept.
1682120	1690520	But to Keith's your point, I think having a static benchmark is a problem, sort of putting out a
1690520	1697640	benchmark that everybody can kind of try and optimize their program to solve. We've seen
1697640	1707800	that over and over again. That ends up being sort of a way that people end up reverse engineering
1708680	1716120	to a particular task rather than to a more general set of conceptual understanding. So
1716120	1722360	I do think that we have to keep changing our benchmarks. We can't just say, okay, here's image
1722360	1729160	net, go, you know, beat on that for the next 20 years until you've solved it.
1729720	1732760	That's not going to yield general intelligence.
1733880	1738520	Yeah, I think one of the issues we're talking about in general is extrapolation. So, you know,
1738520	1744920	Sholey used extrapolation to talk about skill programs and being able to do things beyond
1744920	1751000	your priors and experience. But with benchmarks, it's about human extrapolation. So I think part
1751080	1755560	of the problem with the risk debate, by the way, why everyone's so suddenly worried about risk is
1755560	1761560	because of this benchmark problem. And that's because we see that humans who can do A can do
1761560	1767800	B. And now we see machines that can do A. And we have all of these built-in assumptions in
1767800	1772360	benchmarks. And we don't really realize that we're talking about machines now. We're not talking
1772360	1778440	about computers anymore. And I think it's causing a real problem. I don't want to be hyperbolic here,
1778440	1784280	but it feels like there's this massive delusion taking over the entire machine learning community.
1784280	1789560	And we're seriously talking about AI risk. And I think it all comes down to these
1789560	1797160	benchmarks fundamentally. Yeah, I do think all of our benchmarks have, as you say,
1797160	1805320	have this problem of that they have assumptions built in that if a human could do this, that
1805320	1809720	then the machine must, if the machine does it, it has the same kind of
1812040	1817880	generalization capacity as a human who could solve that problem. This goes back all the way to say
1819640	1827480	chess as a benchmark. So people used to think that if, because if a human can play chess at a
1827480	1834280	grandmaster level, that means they must be super intelligent in other ways, that if a machine
1834280	1840920	could play chess at that level, it would also be super intelligent like a human. Herbert Simon
1840920	1848120	even said that explicitly. But then we saw that chess actually could be conquered by very
1848120	1857800	unintelligent brute force search that didn't generalize in any way. So I think this is an issue
1857800	1865480	today with large language models. They can do things like pass the bar exam and pass other
1866120	1874360	standardized human tests of skill or intelligence. But what does that mean? It doesn't necessarily
1874360	1878120	mean the same thing for a machine as it does for a human for many different reasons.
1879320	1883800	Yeah, I guess it's a similar thing with the McCorduck effect that we have relative pointers
1883800	1889240	to what we think of as being intelligence. We just point to something and then when that thing
1890120	1897480	becomes easy, then we need to kind of move the pointer. Yeah, I think it also feeds into, as
1897480	1903560	Tim was saying, I think it heightens the fear of existential risk because of this, this concept
1903560	1910120	that we have of intelligence always wins, which even among humans is, is a flawed concept, right?
1910120	1914360	I mean, you know, many nerds who grew up through elementary school can tell you like
1914360	1920280	intelligence doesn't always win, right? Like sometimes it's numbers or brute force or
1920280	1924520	whatever else kind of kind of wins. And they assume like, well, if we were to have this
1924520	1930600	purified intelligence that was super intelligence, it would be as if a human brain were super
1930600	1935880	intelligent and they'd be able to do everything a human being could do and hurt other people and
1935880	1941640	conquer the world and fight wars. And that again, is this anthropomorphic projection, right?
1942440	1950280	Yeah, I mean, right. So, and it's this notion that intelligence is this thing that you can just
1950280	1957000	have more and more of. Forever. Forever. Or so far that it's just beyond any, you know,
1957000	1962040	it's almost magical, right? And it's capable. Right. And it's not, you know, a different
1962040	1969240	view of intelligence is that it's a collection of adaptations to specific problems for a particular
1969240	1980200	kind of organism in an environment. And it's not the sort of an open-ended, pure domain
1980200	1986920	independent thing. So, I think this is why, you know, you see a lot of discussion of super
1987000	1996680	intelligence, AGI, you know, AI risk in among computer scientists, but you don't see a lot of it
1997240	2004440	discussed among like psychologists or animal intelligence people or other cognitive scientists.
2005400	2008120	Because that's not the way that they understand intelligence.
2009240	2013160	I would love to explore more about that because, I mean, only yesterday when we were talking about
2013240	2019560	an activism, we're also talking about Gibson's ecological psychology. And even Elizabeth Spelke,
2019560	2024520	I mean, this kind of cognitive psychology view is very related to nativism. It's this idea that we
2024520	2030200	have these fundamental cognitive primitives and intelligence in some sense is just traversing
2030200	2036120	or recomposing this library of cognitive modules that we have. And those modules are very physically
2036120	2041560	situated, you know, they tell you something about the environment that you're in. Which means that
2041640	2047000	intelligence is just very gnarly and it's very kind of coupled to the environment we're in. It
2047000	2051560	can't really be magically abstracted in a computer with infinite scale.
2052760	2058360	Yeah, I think that's right. That's, you know, people have different views about the nativism,
2059880	2066040	empiricism, debate. And there's whole different schools and cognitive science about like how
2066760	2072840	how much is learned, how much is evolutionarily built in and all of that. But I think most people
2073400	2083800	in the field would agree with what you said that intelligence is very gnarly. It is situated, it
2083800	2094280	is specific to particular domains of concern to a particular organism, and that it's not easily
2094280	2097960	abstractable. You know, that back in the early days of AI we had
2100520	2106360	Newell and Simon, two of the pioneers of AI who had this thing called the physical symbol system
2106360	2113800	hypothesis, which was that basically you could sift off intelligence from any material substrate
2113800	2118840	like the brain and put it in some other material substrate like a computer. They were thinking
2118840	2127160	about symbols, but nowadays people have the same kind of view with neural nets or
2128600	2134920	transformers or whatever, that you can take human intelligence that's very situated and
2135800	2141720	tied to the environment and sort of sift off the pure part and leave all of that bodily stuff
2142360	2146520	and you can get something like superintelligence. And I don't think most people in cognitive
2146520	2152040	science would agree with that. Well, on the other hand though, I think, and I'd be curious to get
2152040	2157480	your take on this, is one direction that that comes from is for those of us, and I include
2157480	2163480	myself in this camp tentatively, that at the end of the day what the brain does is some form of
2163480	2168120	computation. You know, like absence, the proof that there's such a thing as hypercomputation,
2168120	2174040	like our brain, all of its calculations could be embodied in a large enough
2174600	2179000	you know, Turing machine and a large enough computer of some kind. And therefore, everything
2179000	2187160	that we do, including our intelligent activities, could be coded somehow or another into a Turing
2187160	2191800	equivalent system. And for the record, I don't believe neural networks are. I've said this like
2191800	2195960	multiple times, at least in their current manifestations, they're not, they're just a
2195960	2200120	feed forward, you know, thing at the end of the day. But if you actually had a computer, you could
2200120	2207160	have human symbolic intelligence encoded. Like, where do you stand on that, on that debate, if you
2207160	2216200	will? Yeah, I have nothing against the idea that the brain does computations. I think that's,
2217080	2225480	that's, you know, one possible way to look at it. And that those kinds of computations could be
2225480	2230600	implemented in another kind of computer. But the brain is a very special kind of
2230600	2235480	sort of biological computer that's been evolved to do specific things. And one of the main things
2235480	2241160	the brain has been evolved to do is control the body, and in particular kinds of environments.
2241800	2248680	And so I think the brain is doing computations, but it's doing very, very highly evolved, very
2248680	2259480	domain specific computations that perhaps don't necessarily make sense without having a body.
2261640	2272600	Now, that's debatable. But it does seem like a lot of the way that we reason is by reference to our own
2272760	2277400	sort of episodic experience in the world.
2279160	2284760	Or at least to the capabilities that have been built into us, you know, like visual, using our
2284760	2291240	visual cortex to imagine cubes and steers and whatever else we need to solve a physics problem
2291240	2297480	or a geometric problem. Sure, sure. Yeah, so I'm fine with saying the brain is a computer of a certain
2297480	2308840	kind, but that's not to say that it's going to be, you can just kind of lift off the computations
2311080	2317640	and then put them in a different substrate and kind of get everything that's human like,
2317640	2322520	because I'm not sure that those computations are going to make sense in the absence of the rest
2323080	2329720	of the organism. Yeah, there was something that always confused me about the autopoietic
2329720	2336280	inactivists, because of course they as they issue representationalism and information
2336280	2341640	processing, but they also issue computationalism in general. And as Keith was just saying, I don't
2341640	2346280	even if cognition is externalized, I don't see any reason why in principle, you couldn't just
2346280	2351720	compute the entire system and and recreate the computation. I just wanted to close the loop on
2351720	2358600	the ARC challenge stuff though. So you said that the winning solutions to Francois' challenge on
2358600	2363320	Kaggle, they were quite simplistic in a way. They were like a genetic search over lots of
2363320	2368600	primitive kind of functions. And even the winner said that they didn't feel it was a satisfying
2368600	2373480	solution, which was interesting. And then you tried it on GPT4. And I think you said you got
2373480	2379960	around 30%. There's now a deep mind paper out very recently, which just basically turned it all into
2380040	2385720	a character set with a random mapping, put it into GPT4, I think got nearly 60%. Even
2385720	2391560	even somewhat invariant to the translation between the character set mapping. Some folks on our
2391560	2395800	Discord forum tried to reproduce it and couldn't. That's the problem with GPT4. You can never
2395800	2402600	reproduce anything. But I was just wondering, would you consider that to be an elegant solution?
2402600	2406120	It's not really much better than searching over a DSL, is it?
2406840	2409240	By that, you mean giving it to GPT4?
2410280	2415400	Well, I mean, it's quite an interesting thing, isn't it? If there's the McCorduck effect,
2415400	2421320	and even before you get to a solution, what would a good AI solution look like? I mean,
2421320	2425800	what would someone have to create for you to say, oh, that's a really cool AI solution?
2425880	2436360	Well, if you had a program that really could solve these tasks in a general way,
2436360	2443000	that would, however it worked, it would be a good AI solution. I don't necessarily think we have to
2443000	2449880	have something like the way people do it. Well, let me see if I can guess, though,
2449880	2455480	maybe an extension to what you said. It's in line with your argument that the benchmarks
2455480	2460520	have to evolve. Because I think that these benchmarks really is just first pass or low
2460520	2465320	pass filters. It's like they weed out the junk. It's like, well, if you can't pass the art challenge,
2465320	2469240	I'm not going to bother with you. If you pass the art challenge, now we have to look further,
2469240	2475000	right? Which is like, okay, so it's been able to generalize along these 19 concepts that we've
2475000	2482520	defined in concept art with little pixel grids. What about if we give it full frame pictures
2482520	2488280	or video or something? Is it able to generalize there? No, okay, it failed. Why did it fail?
2488280	2492360	Well, now we need to do some more engineering. It's going to be this kind of never ending sort
2492360	2498280	of iterative process. So I would say if something passes arc or concept arc, then it's worthy of
2498280	2506280	further study. Sure. Yeah, I agree. I mean, one question is that arcs are very idealized kind
2506360	2513880	of micro world type domain. So does it capture what's interesting about the real world
2514600	2521000	in terms of abstraction? To some extent, yes, probably, and to some extent, probably no.
2521000	2528680	So you're right. Solving arc doesn't mean we're at AGI, if you want to talk about that.
2528680	2535240	It's like in chess, what you brought up earlier. If you took whatever the current best,
2535240	2539960	let's say LC zero or something like that, and it's been trained on standard chess,
2539960	2545400	and then you have a go play chess 960, formerly called Fisher random, where you just random,
2545400	2550440	it's going to suck like humans are going to destroy it, right? Because humans have learned
2550440	2557160	a more generalized and by the way, that also destroys human beings who rely on memory and
2557160	2562120	just sort of like the memorized positions that haven't learned, let's say the skill
2562120	2566440	of playing chess, right? And so this is the type of thing that's going to happen, right?
2566440	2570920	It's like you say, when you take this intelligence and try to apply it to a different context,
2571560	2575320	that's when the rubber meets the road as to whether or not you really learned
2575320	2580520	the concepts, right? Yeah, no, definitely. I agree. And I don't think like our concept arc
2580520	2587080	wasn't meant to be like a test of AGI in any sense. It was meant to be kind of a stepping stone to
2587080	2594600	getting to abilities for abstraction. And clearly, if some program was able to solve all of the
2594600	2601080	problems in that domain, and we'd have to then test further, we'd have to have it be able to
2601080	2606760	extrapolate to a new kind of domain that tested the same kinds of concepts. So you're right,
2606760	2612600	there's no end in some sense. But at some point, I guess, and I don't know when that point is,
2612600	2617320	we have to say, well, this thing seems to be understanding this concept.
2619960	2623720	That's the wonderful continuum, though, because you said earlier, there's something deeply
2623720	2630040	unsatisfying about chess brute forcing everything. And when we apply Francois' measure of intelligence,
2630040	2636040	we don't think of that as intelligent because it's just brute force experience. And then we
2636040	2640040	find something which is a little bit more efficient. So it's something which appears to work. But
2640760	2645400	now, another interesting thing is when you talk about concepts, you had this beautiful article
2645400	2650920	out earlier that she had talking about, on top of, she's on top of the world. And what would
2650920	2658200	Dali draw? It would draw a globe with someone dancing on top of it, or I'm on the TV. What
2658200	2663560	does that mean? It should mean that I'm actually being rendered on the TV. Now, it's kind of like
2663560	2668840	what we were saying with goals, isn't it? Because this skill program, someone just goes on Kaggle
2668840	2674200	and they gives you this program and it seems to work. But it's horribly complicated. And how do
2674200	2679880	you know that the internal representations are in any way related to these abstractions? And do
2679880	2685400	you think that the abstractions as well are somehow universal in the same way Spelki would say that
2685400	2692920	the cognitive priors are? Yeah, I think it's something we can't say. And we don't know with
2693000	2699720	humans. And we don't know with machines, because both of these are very complex systems that are
2699720	2708120	hard to kind of pull apart. What are the internal representations? So in most cases, we have to
2708120	2717880	rely on behavior, which is very noisy. It can be misleading. And it turns out that humans
2718680	2728120	often are not, if you give them a problem, like a reasoning problem, in a familiar domain,
2728840	2734040	they're much better at doing that problem as doing the exact same reasoning kind of abstract
2734040	2741720	reasoning task in an unfamiliar domain. And I think that's something that people have shown
2741720	2750600	is also true of large language models, because they've learned from human language and have
2750600	2755960	incorporated sort of the statistics of some of the statistics of human experience that they're
2755960	2761400	much better on familiar domains than on non-familiar domains. But the one thing that humans can do
2761400	2767480	is often they can kind of transcend that and learn how to reason much more abstractly,
2767800	2774760	which I don't know if we will get to that point with language models yet. So there's a wonderful
2774760	2784760	paper that just came out from a group at MIT and some other places called, I can't remember
2784760	2791720	what it was called, it was something like reasoning versus reciting. And what they do is
2792280	2799480	they talk about this notion of a counterfactual task, which is if you can do one task, like
2799480	2804840	addition and base 10, and you really understand that notion of addition, you should be able to do
2804840	2811160	addition and base eight. And so, but you haven't had as much experience as like for a language model,
2811160	2819000	it's not almost all of the training data has to do with base 10. So, but can, so they tested,
2819000	2821960	they did a whole bunch of these so-called counterfactual tasks
2823240	2829080	and showed that GPT-4 is really good at the original task, but not so good at the counterfactual task.
2830280	2834920	So it's not, in some sense, it is relying on sort of patterns in its training data rather than
2836200	2838360	genuine abstraction.
2839880	2841560	It's a stochastic parrot, right?
2842120	2845640	Well, you know, it could be argued that humans do that a lot too.
2846600	2851320	I don't know if you called a stochastic parrot, but it's more like a pattern matcher.
2852520	2859560	And it's not, it's not reasoning about the things in the sense that we think of reasoning,
2859560	2866200	you know, as sort of domain independent ability. It's very domain dependent.
2869160	2872280	Yeah, so the difference is that I guess the difference I would say is that humans,
2872520	2883080	it can kind of overcome that domain dependency in some cases and actually get to the true
2883080	2886680	abstraction, but I don't know that language models can.
2888360	2893880	Yeah, I mean, there's a couple of things here. So first of all, these language models fail at
2893880	2901080	things which four-year-old children can do. And they can pass the bar exam, but as you've said
2901080	2904600	previously, you wouldn't want one of these things to actually go and practice more.
2904600	2911560	My God, could you imagine the thought? And there was this Sparks of AGI paper where they gave this,
2911560	2915160	I mean, maybe you could recite this better than me, but there was the thing about the
2915160	2920440	book Nine Eggs, a laptop, a bottle, and a nail. Can you balance it in a stable manner?
2920440	2925960	And this comes back to the experiment design because, my God, in any other discipline of science,
2925960	2930760	they would just tear this apart. They would say, well, that's not very robust. I mean,
2930760	2938040	you came up with an example with a pudding, a marshmallow, a toothpick. How would it balance it?
2940040	2943800	Yeah, did it not balance the full glass of water on top of the marshmallow?
2944840	2949960	Well, it stuck the toothpick into the marshmallow and then that's not exactly what we had in mind.
2950520	2954440	No, and in fact, the Sparks of AGI paper, they explicitly said,
2955400	2959320	we're doing anthropology, not cognitive science.
2960600	2964520	Well, that's not the way it was interpreted. Unfortunately, there are YouTube channels
2964520	2971160	now dedicated to educating people on AI and they're taking this as gospel. I mean, what's going on?
2971880	2980120	I think there's just not as much of a focus on sort of scientific method in this field as there
2980120	2989960	should be. And I think in science, if you're looking at a phenomenon and you're trying to
2989960	2997240	replicate it, if it only replicates half the time, that's not a replication. That's not a
2997240	3003720	robust replication. Whereas for language models, people are saying, well, if it can do this task
3003720	3011720	once in one particular circumstances, then it probably has this more general capability.
3012280	3018440	So if it can do this stacking problem once, then wow, it has physical common sense.
3020760	3028520	And people with my marshmallow example, people, of course, jumped on it and said, wait,
3028520	3033400	if you prompt it in a certain way and you do all this prompt engineering,
3033400	3039000	human engineering, it does it right. And then like, well, that's not the point.
3039720	3044760	The point is not any particular example. The point is figuring out how to test things
3044760	3051400	so that you actually have some kind of robust ability for replicating a capability,
3052680	3057080	which we haven't seen with experiments on language models very much. I mean, people
3057080	3062840	are starting to do this. People are starting to do this kind of more scientifically grounded,
3062840	3070360	experimental method on language models, but it's still not very, there's not very much of it.
3071240	3076200	So you might appreciate a phrase I recently coined because it covers this leakage too,
3076200	3082120	of like sort of leakage of human knowledge, which is if you can't find the priors, look in the mirror.
3083080	3087320	It's like, we have to learn how to do experimental science and computer science,
3087320	3092280	and you've got to guard against this type of leak at Drillian, human engineering,
3092280	3097960	and over-involved and whatever. And this is why I really want to collaborate with people in
3097960	3104680	developmental psychology, with people in animal cognition who face this kind of issue all the
3104680	3115800	time. And one example was, I got from a developmental psychologist was that sometimes
3115800	3125480	like a three-year-old can tell you something like four plus three is seven, but if you say,
3125480	3130040	if you give them a bunch of marbles and say, pick out four of them, they can't do it.
3130360	3137480	So there you say, okay, that this kid doesn't understand the concept of four,
3137480	3143640	they're kind of just reciting something that they've heard. And this is the kind of experiments
3143640	3150040	that people in developmental psychology do all the time to really tease out what the system,
3150040	3156680	what babies and children know and what they can do. And it's not an easy thing to do in
3157400	3163800	this kind of experiment. The problem with that is it's extremely complex and requires so much
3163800	3168600	domain knowledge. So it takes a very long time, because I think there was another article that
3168600	3174440	spoke about how we study rats. And those folks in different disciplines, they're really,
3174440	3180120	really good experimental design, and they have experts who kind of create very, very clear
3180120	3186040	criteria for measuring this behavior. And with AI, everything's going up on archive,
3186040	3189960	and everything's going a million miles an hour. And by the time you actually design
3189960	3193720	a systematic rigorous study for the first thing, there's already another paper coming out,
3193720	3197160	which is claiming to do it differently. So we just can't keep up. It's just,
3197160	3202440	it's an absolute nightmare. Absolutely. Yeah. Agreed.
3204280	3207880	I want to just, so I'll quickly touch on one more thing. And I know Keith wants to go into
3207880	3212520	complexity. But yeah, so the information leakage is a problem. The brittleness is a problem. I do
3212520	3219080	think of these GPT models a bit like a database. And so anything that requires physical grounding,
3219080	3222680	of course, doesn't work very well. Some things work surprisingly well, like, you know,
3222680	3226760	programming, because programming is mostly in the internet, it still has all sorts of
3226760	3233320	failure modes, and it's not very reliable, but it's surprisingly reliable. But you put a paper
3233320	3237160	out with Tanenbaum and a whole bunch of other people. And you actually said, well, if you want
3237160	3242280	some policy advice, if you really want to think about how we can improve the situation, you said,
3242360	3248840	aggregating benchmarks and also giving instance level failure modes can actually help us understand
3248840	3255080	why things went wrong or, you know, why things gave us the right answer for the wrong reasons.
3255720	3260440	And there were all sorts of limiting factors, you said. You know, we have this kind of
3260440	3265880	censorship by concision. You're only allowed to have seven pages in your conference workshop paper,
3265880	3269640	and there's no policy about this. So can you give us a heads up on that?
3270280	3276600	Yeah, I mean, you know, traditionally in machine learning, people use accuracy and similar kinds
3276600	3284520	of aggregate measures to report their results. And, you know, if someone tells you that the accuracy
3284520	3293000	was, you know, 78%, what does that tell you exactly? I think, you know, this gets back to the idea of
3293000	3297640	scientific method. You know, in science, the most interesting things are the failures.
3298600	3301800	And those are the things you really have to focus on. It's like, why did it fail?
3302440	3308680	And that's what we need to know to understand machine learning systems. So the most simple
3309640	3312840	kind of reporting would be just to report for every instance in your
3313720	3320520	benchmark, your data set. How did the system do? What was its answer? And that's not, you know,
3321320	3330360	it doesn't seem like a very big ask, but it would be very useful. And we now have in conferences,
3331080	3336440	you're allowed to have some kind of supplementary material online. So you could have this available.
3337000	3345320	And we did this for our concept arc paper. We showed for every instance, like what humans did,
3345320	3352120	what machines did, we tried to analyze the errors of the system. And I think this these kinds of
3352120	3357320	reporting will be will give us a lot more insight into what these systems are doing and what their
3357320	3365480	like real capabilities are. Yeah. And it's, and back to the difficulty that Tim mentioned earlier,
3366120	3370840	totally agree. And this is work that has to be done. Like if, if we are going to build a science
3370840	3377000	of machine cognition, you know, this work has to be done. Yeah, I think. And I just want to shout
3377000	3382920	out to Ryan Bernal, who spearheaded that paper, because he really is the one pushing for all
3382920	3389880	this. And I think it's fantastic. So just in the last few minutes, you know, since we have you,
3390680	3396840	complexity and complexity theory is a topic I really love. I'm not an expert in it at all,
3396840	3401480	but I like to think about I like to explore it. I'm just curious, you know, from your perspective,
3401480	3406520	um, what are some of the most interesting things happening right now in complexity theory? And
3406520	3411560	if I wanted to go learn a bit more and check out just some cool, you know, latest stuff, what should
3411560	3418040	what should we go look at? So I think there's, you know, there's a lot of interesting stuff going on,
3418040	3423800	obviously, and complex systems is a huge umbrella for a lot of research. But
3424760	3431480	if you're interested in the one big topic that people look at is called scaling. And it's the
3431480	3438120	question of like, what happens to a system as it gets bigger in some sense? So this started out
3438120	3447880	with some work on the sort of energy use of systems like animals as they as their maths increases.
3448680	3455320	And people discovered some really interesting scaling laws that were very non-intuitive and
3455320	3462840	they were able to explain these laws using ideas like fractal fractals and the fractal structure
3462840	3469640	of complex systems. But now, so this is all on like biological metabolism and things like that.
3470360	3477240	But now a lot of people are extending that scaling work to cities. So asking what happens
3477880	3484360	to cities when they increase in size, either in area or in population size. And
3485400	3489640	there's all kinds of phenomena that you can see, like what's the rate of innovation
3490440	3497880	measured by something like patents? And what's the rate of sort of energy usage by a city?
3499880	3506280	And what's how do these things change? Even like the happiness of the people,
3507240	3511880	you know, are people in New York happier than people in Santa Fe, which is a much smaller city?
3514520	3522040	These things scale in really interesting ways. And it's opening up a lot of new ideas about how
3522760	3530440	social systems work. And how... Is it a similar thing that you can't trust the benchmarks? Because
3530440	3535400	how happy people are, might you look at the rate of antidepressant usage or something?
3535400	3539880	Yeah. So you do have all these... Right. I don't know if that's exactly what they use, but
3541160	3545160	you do have to look at ways to measure these things, which can be questioned.
3546840	3552520	But there are a lot of really... And I think this whole science, the science of cities, is
3553400	3559240	it's very preliminary. And there's a lot of ideas about how to measure these things, how to
3559480	3566360	develop sort of analytical descriptions or laws that govern certain properties and how to
3566360	3572760	interpret them. But there's just a lot of really interesting work in this. And it turns out that
3572760	3580520	now that everybody has a cell phone, you can really do a lot of tracking. A lot of these quantities
3580520	3588280	can be tracked by people's sort of their movement, their interaction with other people, and all these
3588840	3595880	things that you can measure using cell phones. So that's very cool.
3597160	3603000	That is... Yeah. Thank you. That sounds actually fascinating. And one reason why for me particularly
3603000	3610120	is... Are you familiar with Asimov's Foundation series? Yeah. So you know, psycho history in
3610120	3616760	there was the science... And it was almost like a thermodynamics of human behavior that was only
3616840	3621560	applicable at kind of planet scale and beyond. So it's like these scaling laws. So this is maybe
3621560	3626200	one step towards... Very similar, psycho history. Yeah. One step towards psycho history of Asimov's
3626200	3633000	kind. Exactly. Yeah. Cool. And in closing, does that give you intuition on the scaling of intelligence?
3634760	3645160	Well... That's a great question. And I think, you know, one question you can ask is like,
3645160	3649480	there's individual intelligence and then there's collective intelligence.
3650680	3656120	And how much of the intelligence that we have individually is actually grounded in a more
3656120	3663080	collective intelligence? You know, there's many things that I don't know, like I don't understand
3663080	3669960	quantum mechanics or something, but I know somebody who does. And therefore, I feel like it's understood.
3670360	3677480	And a lot of our intelligence, I think, is sort of more social than we think.
3679240	3685880	Oh, absolutely. And folks should definitely read Melanie's book. So your complexity book,
3685880	3690040	we actually covered that quite a lot on our show on Emergence. It's absolutely wonderful. And of
3690040	3696680	course, your book on AI is probably the best book on AI I've ever read. It's up there with
3696680	3701560	Christopher Sommerfield's book. But anyway, Melanie, honestly, you are my hero. Thank you so
3701560	3705960	much for coming on MLS2. I really appreciate it. Thanks so much for having me. I really enjoyed
3705960	3707640	it. It's great talking to you.
