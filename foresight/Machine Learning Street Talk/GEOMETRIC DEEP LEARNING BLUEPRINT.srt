1
00:00:00,000 --> 00:00:04,480
So, you're talking about the symmetries in data.

2
00:00:04,480 --> 00:00:09,680
Could these analogies also be represented using the kind of symmetries that you're talking about?

3
00:00:09,680 --> 00:00:11,680
Good question. Let me think of it a bit.

4
00:00:14,160 --> 00:00:18,720
Modern machine learning operates with large, high-quality data sets,

5
00:00:18,720 --> 00:00:21,840
which together with appropriate computational resources,

6
00:00:21,840 --> 00:00:28,560
motivates the design of rich function spaces with the capacity to interpolate over the data points.

7
00:00:28,560 --> 00:00:33,840
Now, this mindset plays well with neural networks since even the simplest choices of inductive

8
00:00:33,840 --> 00:00:42,720
prior yield a dense class of functions. Now, symmetry, as wide or narrow as you may define its

9
00:00:42,720 --> 00:00:50,800
meaning, is one idea by which man through the ages has tried to comprehend and create order,

10
00:00:51,440 --> 00:00:58,480
beauty, and perfection. And that was a quote from Hermann Weil, a German mathematician

11
00:00:58,480 --> 00:01:05,200
who was born in the 19th century. Now, since the early days, researchers have adapted neural

12
00:01:05,200 --> 00:01:10,720
networks to exploit the low-dimensional geometry arising from physical measurements,

13
00:01:10,720 --> 00:01:18,320
for example, grids in images, sequences in time series, or position and momentum in molecules

14
00:01:18,400 --> 00:01:22,400
and their associated symmetries, such as translation or rotation.

15
00:01:23,280 --> 00:01:29,200
Now, folks, this is an epic special edition of MLST. We've been working on this since May of this

16
00:01:29,200 --> 00:01:33,680
year, so please use the table of contents on YouTube if you want to skip around. The show is

17
00:01:33,680 --> 00:01:39,360
about three and a half hours long. The second half of the show, roughly speaking, is a traditional

18
00:01:39,360 --> 00:01:44,320
style MLST episode, but the beginning part is a bit of an experiment for us, maybe a bit of a

19
00:01:44,320 --> 00:01:49,840
departure. We want to make some Netflix-style content, and we've even been filming on location

20
00:01:49,840 --> 00:01:54,320
with our guests, so I hope you enjoy the show and let us know what you think in the YouTube comments.

21
00:01:55,120 --> 00:02:00,320
Many people intuit that there are some deep theoretical links between some of the recent

22
00:02:00,320 --> 00:02:05,920
deep learning model architectures, particularly the ones on sets, actually. And this may be why

23
00:02:05,920 --> 00:02:11,520
so many popular architectures keep getting reinvented. Now, the other day, Fabian Fuchs from

24
00:02:11,600 --> 00:02:16,320
Oxford University released a really cool blog post about deep learning on sets,

25
00:02:16,320 --> 00:02:21,680
elucidating a math-heavy paper that he co-authored with Edward Wagstaff et al.

26
00:02:22,320 --> 00:02:28,640
Now, he wanted to understand why so many neural network architectures for sets resemble either

27
00:02:28,640 --> 00:02:35,920
deep sets or self-attention, because sets come in any ordering. There are many opportunities to

28
00:02:35,920 --> 00:02:42,160
design inductive priors to capture the symmetries. So, raises the question,

29
00:02:42,800 --> 00:02:48,160
how do we design deep learning algorithms that are invariant to semantically-equivalent

30
00:02:48,160 --> 00:02:54,960
transformations while maintaining maximum expressivity? Now, Fabian pointed out that

31
00:02:54,960 --> 00:03:01,200
the so-called Genosi-pooling framework gives a satisfying explanation. Genosi-pooling is when

32
00:03:01,200 --> 00:03:06,640
you generate all of the k-tuples of a set, an average over your target function, on those

33
00:03:06,640 --> 00:03:12,640
permutations. It gives you a computationally tractable way of achieving permutation invariance.

34
00:03:13,200 --> 00:03:19,600
So, rather than computing n factorial combinations of the examples, you compute n factorial divided

35
00:03:19,600 --> 00:03:26,640
by n minus k factorial, which for small k is very tractable. Now, clearly, setting k to n

36
00:03:26,640 --> 00:03:32,560
gives you the most expressive yet the most expensive model, but that would be cool. It would

37
00:03:32,560 --> 00:03:37,840
model the high-order interactions between the examples, but it turns out that deep sets are

38
00:03:37,840 --> 00:03:44,320
this configuration with k equals 1, and self-attention is this configuration with k equals 2.

39
00:03:45,280 --> 00:03:51,440
Now, Fabian also spoke about approximate permutation invariance, which is when you set k to n,

40
00:03:52,080 --> 00:03:56,000
the number of examples, but you sample the permutations. It turns out you don't have to

41
00:03:56,000 --> 00:04:00,000
sample very many of them to get good results. But anyway, if you want to check out that in a

42
00:04:00,000 --> 00:04:04,640
little bit more detail, go and check out Fabian's blog. I've put a link in the video description.

43
00:04:05,280 --> 00:04:10,800
High-dimensional learning is impossible due to the curse of dimensionality. It only works if we

44
00:04:10,800 --> 00:04:15,600
make some very strong assumptions about the regularities of the space of functions that we

45
00:04:15,600 --> 00:04:20,640
need to search through. Now, the classical assumptions that we make in machine learning

46
00:04:20,720 --> 00:04:26,400
are no longer relevant. Now, in general, learning in high dimensions is intractable.

47
00:04:26,960 --> 00:04:31,920
The number of samples grows exponentially with the number of dimensions. The universal function

48
00:04:31,920 --> 00:04:38,240
approximation theorem popularized in the 1990s states that for the class of shallow neural

49
00:04:38,240 --> 00:04:44,320
network functions, you can approximate any continuous function to arbitrary precision by

50
00:04:44,320 --> 00:04:49,280
just stacking the neurons, assuming that you had enough of them. So it's a bit like kind of

51
00:04:49,280 --> 00:04:55,680
sparse coding, if you like. The curse of dimensionality refers to the various phenomena that arise

52
00:04:55,680 --> 00:05:00,880
when analyzing and organizing data in high-dimensional spaces that do not occur

53
00:05:00,880 --> 00:05:06,640
in low-dimensional settings, such as the three-dimensional physical space of everyday experience.

54
00:05:07,280 --> 00:05:12,080
Now, the common theme of these problems is that when the dimensionality increases,

55
00:05:12,080 --> 00:05:18,240
the volume of the space increases so fast that the available data effectively becomes sparse.

56
00:05:18,320 --> 00:05:23,440
This sparsity is problematic for any method that requires statistical significance. Now,

57
00:05:23,440 --> 00:05:28,720
in order to obtain a statistically sound and reliable result, the amount of data needed

58
00:05:28,720 --> 00:05:34,960
to support the result often grows exponentially with the dimensionality. Most of the information

59
00:05:34,960 --> 00:05:41,840
in data has regularities. Now, what this means in plain English is, just like on a kaleidoscope,

60
00:05:41,840 --> 00:05:46,960
most of the information which has been generated by the physical world is actually redundant,

61
00:05:46,960 --> 00:05:53,680
just many repeated semantically equivalent replicas of the same thing. The world is full

62
00:05:53,680 --> 00:06:00,400
of simulacrums. Machine learning algorithms need to encode the appropriate notion of regularity

63
00:06:00,400 --> 00:06:05,840
to cut down the search space of possible functions. You might have heard this idea referred to

64
00:06:05,840 --> 00:06:12,000
as an inductive bias. Now, machine learning is about trading off these three sources of error,

65
00:06:12,000 --> 00:06:16,640
statistical error from approximating the expectations on a finite sample,

66
00:06:16,640 --> 00:06:22,560
and this grows as you increase your hypothesis space. Approximation error, which is how good

67
00:06:22,560 --> 00:06:28,000
is your model in that hypothesis space? If your function space is too small, then the one that

68
00:06:28,000 --> 00:06:33,920
you find will incur a lot of approximation error. And finally, optimization error, which is the

69
00:06:33,920 --> 00:06:39,520
ability to find a global optimum. Now, even if we make strong assumptions about our hypothesis

70
00:06:39,520 --> 00:06:44,160
space, you know, we should say that it should be lip sheets or in plain English, it should be

71
00:06:44,160 --> 00:06:49,840
locally smooth. It's still way too large. We want to have a way to search through the space to get

72
00:06:49,840 --> 00:06:56,080
anywhere. So the statistical error is cursed by the dimensionality. If we make the hypothesis or the

73
00:06:56,080 --> 00:07:01,920
function space really small, then the search space is smaller, but the approximation error is cursed

74
00:07:01,920 --> 00:07:08,400
by dimensionality. So we need to define better function spaces to search through. But how?

75
00:07:09,360 --> 00:07:13,200
We need to move towards a new class of function spaces, which is to say,

76
00:07:13,840 --> 00:07:20,000
geometrically inspired function spaces. Let's exploit the underlying low dimensional structure

77
00:07:20,000 --> 00:07:24,880
of the high dimensional input space. The geometric domain can give us entirely new

78
00:07:24,880 --> 00:07:30,960
notions of regularity, which we can exploit. Now using geometrical priors, which is to say,

79
00:07:30,960 --> 00:07:36,480
only allowing equivariant functions or ones which respect a particular geometrical principle,

80
00:07:36,480 --> 00:07:40,000
this will reduce the space of possible functions that we search through,

81
00:07:40,000 --> 00:07:45,280
which means less risk of statistical error and less risk of overfitting. We should be able to

82
00:07:45,280 --> 00:07:50,320
do this without increasing approximation error, because we should know for sure that the true

83
00:07:50,320 --> 00:07:56,080
function has a certain geometrical property, which will bias into the model. So introducing the

84
00:07:56,080 --> 00:08:02,880
geometrical deep learning proto book. So recently, Professor Michael Bronstein, Professor Joanne Bruner,

85
00:08:02,960 --> 00:08:10,400
Dr. Tako Kohen and Dr. Peta Velichkovich released an epic proto book called Geometric Deep Learning,

86
00:08:11,200 --> 00:08:18,720
Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical

87
00:08:18,720 --> 00:08:24,160
theory and machine learning and geometry and group theory to deep learning, which is fascinating.

88
00:08:24,800 --> 00:08:30,400
Now the proto book is beautifully written, right? It's presented so well, it even has some helpful

89
00:08:30,400 --> 00:08:35,520
margin notes. And honestly, I could read sections of it out loud on MLST with scant need to change a

90
00:08:35,520 --> 00:08:39,200
single word. It's that well written. I mean, it is, there's a lot of maths in there as well,

91
00:08:39,200 --> 00:08:44,720
let's be honest. I can't dodge that. But I often try to impress upon people that if your writing

92
00:08:44,720 --> 00:08:48,560
sounds weird when you say it out loud, then you're probably writing it the wrong way. But these guys

93
00:08:48,560 --> 00:08:54,720
have written it really well. Now they've essentially created an abstraction or a blueprint, as they call

94
00:08:54,800 --> 00:08:58,960
it, which prototypically describes all of the deep learning architectures,

95
00:08:58,960 --> 00:09:04,560
the geometrical priors that they have described so far. They don't prescribe a specific architecture,

96
00:09:04,560 --> 00:09:10,000
but rather a series of necessary conditions. The book provides a mathematical framework to study

97
00:09:10,000 --> 00:09:16,320
this field. And it's essentially a mindset on, you know, how to build new architectures. It gives

98
00:09:16,320 --> 00:09:23,360
constructive, you know, procedures to incorporate prior physical knowledge into neural architectures.

99
00:09:23,360 --> 00:09:29,120
And it provides a principled way to build future architectures, which have not yet been invented.

100
00:09:29,120 --> 00:09:33,520
The researchers have also recently released a series of 12 brilliant lectures

101
00:09:33,520 --> 00:09:36,640
on all of the material in the book. And I've linked these in the video description.

102
00:09:37,360 --> 00:09:42,240
Now, what are the core domains of geometric deep learning? So in geometric deep learning,

103
00:09:42,240 --> 00:09:48,720
the data lives on a domain. This domain is a set. It might have additional structure,

104
00:09:48,720 --> 00:09:53,200
like a neighborhood in a graph, or it might have a metric such as, you know, what's the

105
00:09:53,200 --> 00:09:58,640
distance between two points in the set. But most of the time, the data isn't the domain itself.

106
00:09:58,640 --> 00:10:04,800
It's a representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries.

107
00:10:04,800 --> 00:10:09,920
Symmetries are really important to understand this framework. So a symmetry of an object

108
00:10:09,920 --> 00:10:15,600
is simply a transformation of that object, which leaves it unchanged. Now, there are

109
00:10:15,600 --> 00:10:19,760
many different types of symmetries in deep learning. I mean, for example, there are symmetries

110
00:10:19,760 --> 00:10:24,400
of the weights. If you take two neurons in a neural network, and you swap them,

111
00:10:24,400 --> 00:10:29,120
the neural network is still graph isomorphic. There are symmetries of the label function,

112
00:10:29,120 --> 00:10:33,840
which means that an image is still a dog, even if you apply a rotation transformation to it.

113
00:10:34,640 --> 00:10:39,600
Actually, if we knew all of the symmetries of a certain class, we would only need one labeled

114
00:10:39,600 --> 00:10:43,760
example, right, because we would recognize any other examples that you give it as kind of

115
00:10:43,760 --> 00:10:48,320
semantically equivalent transformations. But we can't do that, right, because the learning problem

116
00:10:48,320 --> 00:10:53,520
is difficult, which means we don't actually know all of the symmetries in advance. Now,

117
00:10:53,520 --> 00:10:58,480
in the context of geometric deep learning, we talk about symmetries of the core structured

118
00:10:58,480 --> 00:11:04,960
geometric domains that we're interested in. So grids or graphs, for example, a symmetry is any

119
00:11:04,960 --> 00:11:10,240
transformation which preserves the structure of the geometric domain that the signal lives on.

120
00:11:10,240 --> 00:11:16,560
So for example, permutations of a set preserves the set membership or Euclidean transformations

121
00:11:16,560 --> 00:11:22,800
like rotations or reflections preserve distances and angles. There are a few rules to remember,

122
00:11:22,800 --> 00:11:29,200
because the way we deal with this paradigm is we talk about how composable those symmetries are.

123
00:11:29,200 --> 00:11:34,160
The identity transformation is always a symmetry. Composing a symmetry transformation is always

124
00:11:34,160 --> 00:11:39,200
a symmetry. The inverse of a symmetry is always a symmetry. We can formulate this with this

125
00:11:39,200 --> 00:11:44,160
mathematically abstract notion of a group. Group theory in mathematics is fascinating,

126
00:11:44,160 --> 00:11:50,160
because it concerns only with how elements compose with each other, not what they actually are.

127
00:11:50,160 --> 00:11:53,920
So different kinds of objects may have the same symmetry group. For example,

128
00:11:53,920 --> 00:12:01,120
the group of rotational and reflection symmetries of a triangle is the same as the group of permutations

129
00:12:01,120 --> 00:12:07,200
of sequences of three elements. So let's talk about the blueprint itself. The blueprint has

130
00:12:07,280 --> 00:12:16,000
three core principles. Symmetry, scale separation and geometric stability. In machine learning,

131
00:12:16,000 --> 00:12:21,680
multi-scale representations and local invariance are the fundamental mathematical principles

132
00:12:21,680 --> 00:12:26,480
underpinning the efficiency of convolutional neural networks and graph neural networks.

133
00:12:26,480 --> 00:12:31,680
They are typically implemented in the form of local pooling in some sense. Now these principles

134
00:12:31,680 --> 00:12:36,320
give us a very general blueprint of geometric deep learning that can be recognized in the

135
00:12:36,320 --> 00:12:42,960
majority of popular deep neural network architectures. A typical design consists of a sequence of

136
00:12:42,960 --> 00:12:49,440
locally-equivariant layers. I mean, think of the convolution layers in a CNN, then a pooling or

137
00:12:49,440 --> 00:12:55,200
a coarsening layer. So you recognize those in CNNs as well. And finally, followed by a globally

138
00:12:55,200 --> 00:12:59,280
invariant pooling layer. So that might be your classification head. Now these building blocks

139
00:12:59,280 --> 00:13:04,880
provide a rich approximation space, which have prescribed invariance and stability properties

140
00:13:04,880 --> 00:13:09,920
by combining them together into a scheme that these researchers refer to as the geometric

141
00:13:09,920 --> 00:13:16,800
deep learning blueprint. Now the researchers also introduced the concept of geometric stability,

142
00:13:16,800 --> 00:13:22,560
which extends the notion of group invariance and equivalence to approximate symmetry or

143
00:13:22,560 --> 00:13:28,240
transformations around the group. They quantify this in some sense by looking at a metric space

144
00:13:28,240 --> 00:13:33,200
between the transformations themselves. This is Professor Michael Bronstein.

145
00:13:33,840 --> 00:13:39,600
The problem is that traditional machine learning techniques work well with images or audio,

146
00:13:40,160 --> 00:13:45,600
but they are not designed to deal with network structure data. In order to address this challenge,

147
00:13:46,160 --> 00:13:53,360
we've developed a new framework that we call geometric deep learning. It allowed us to learn

148
00:13:53,360 --> 00:14:00,640
the network effects of clinically approved drugs and to predict anti-cancer drug-like

149
00:14:00,640 --> 00:14:08,240
properties of other molecules. For example, molecules contained in food. Neural networks

150
00:14:08,240 --> 00:14:13,520
have exploded, leading to several success stories in industrial applications. And I think it's

151
00:14:13,520 --> 00:14:18,640
quite indicative that last year two major biological journals featured geometric deep learning papers on

152
00:14:18,640 --> 00:14:23,680
their cover, which means that it has already become mainstream and possibly will lead to new

153
00:14:23,680 --> 00:14:29,200
exciting results in fundamental sciences. The book I hold is called The Role to Reality. It's

154
00:14:29,200 --> 00:14:36,320
written by a British mathematician and recent Nobel laureate, Roger Penrose, a professor at Oxford,

155
00:14:36,880 --> 00:14:44,480
and it's really probably one of the most complete attempts to write and describe modern physics

156
00:14:44,480 --> 00:14:50,960
and its mathematical underpinning. And you can see it's very heavy. But if I were to compress the

157
00:14:50,960 --> 00:14:56,960
thousand-plus pages of this book into just a single concept, I can capture it in one word.

158
00:14:56,960 --> 00:15:03,440
And this is symmetry. And symmetry is really fundamental concept and fundamental idea that

159
00:15:03,440 --> 00:15:12,000
underpins all modern physics as we know it. So, for example, the standard model of particle physics

160
00:15:12,000 --> 00:15:17,920
can entirely be derived from the considerations of symmetry. And that's the kind of idea that

161
00:15:17,920 --> 00:15:25,360
we try to use in deep learning to derive and create new neural network architectures entirely from

162
00:15:25,360 --> 00:15:31,040
fundamental concepts and fundamental principles of symmetry. In the past decade, deep learning has

163
00:15:31,040 --> 00:15:35,440
brought a revolution in data science and made possible many tasks previously thought to be

164
00:15:35,440 --> 00:15:41,200
unreached. On the other hand, we now have a zoo of different neural network architectures for

165
00:15:41,200 --> 00:15:46,720
different types of data, but few unifying principles. The authors also point out that

166
00:15:46,720 --> 00:15:53,200
different geometric deep learning methods differ in their choice of domain or symmetry group or the

167
00:15:53,200 --> 00:15:58,400
implementation specific details of those building blocks that we spoke about. But many of the deep

168
00:15:58,400 --> 00:16:04,720
learning architectures currently in use fall into this scheme and can thus be derived from common

169
00:16:04,720 --> 00:16:09,680
geometrical principles. As a consequence, it is difficult to understand the relations between

170
00:16:09,680 --> 00:16:15,200
different methods, which inevitably leads to the reinvention and rebranding of the same concepts.

171
00:16:15,200 --> 00:16:20,320
So, we need some form of geometric unification in the spirit of the Erlangen program that I call

172
00:16:20,320 --> 00:16:25,520
geometric deep learning. It serves two purposes. First, to provide a common mathematical framework

173
00:16:25,520 --> 00:16:30,640
to derive the most successful neural network architectures. And second, to give a constructive

174
00:16:30,640 --> 00:16:35,520
procedure to build future architectures in a principled way. This is a very general design that

175
00:16:35,520 --> 00:16:39,760
can be applied to different types of geometric structures such as grids, homogeneous spaces

176
00:16:39,760 --> 00:16:44,960
with global transformation groups, crafts and manifolds where we have global isometry invariants

177
00:16:44,960 --> 00:16:51,120
as well as local gauge symmetries. We call these the 5G of geometric deep learning. The

178
00:16:51,120 --> 00:16:55,680
implementation of these principles leads to some of the most popular architectures that exist today

179
00:16:55,680 --> 00:17:00,880
in deep learning, such as convolutional networks emerging from translational symmetry, craft neural

180
00:17:00,880 --> 00:17:06,240
networks, deep sets and transformers implementing permutation invariants and intrinsic mesh CNNs

181
00:17:06,240 --> 00:17:11,760
using computer graphics and vision that can be derived from gauge symmetries. People are quite

182
00:17:11,760 --> 00:17:17,440
cynical about the interpolative nature of deep learning and I think that finding this structure,

183
00:17:17,440 --> 00:17:24,160
this deeper structure could allow us to extrapolate in a way which is significantly better than we

184
00:17:24,160 --> 00:17:30,640
can now. And I asked whether he thought deep learning could get us all the way to artificial

185
00:17:30,640 --> 00:17:35,760
general intelligence? It's a hard question because it has several terms that are not well defined.

186
00:17:35,760 --> 00:17:40,240
What do you define by intelligence? So we don't understand what is human intelligence. Everybody

187
00:17:40,240 --> 00:17:45,680
probably gives a different meaning to this term. So it's hard for me to even to define and quantify

188
00:17:46,320 --> 00:17:51,440
artificial intelligence. I don't think that we necessarily need to emulate human intelligence

189
00:17:51,440 --> 00:17:57,440
and, as you mentioned, in the past we thought of artificial intelligence as being able to solve

190
00:17:57,440 --> 00:18:01,680
certain tasks and it's a kind of a moving target. We thought of, I don't know, playing intelligent

191
00:18:01,680 --> 00:18:06,880
games or perception of the visual world like computer vision or understanding and translating

192
00:18:06,880 --> 00:18:12,480
language or even creativity and today we have machine learning systems that are able to address

193
00:18:12,480 --> 00:18:17,840
at least to some extent all of these tasks, sometimes even better than humans and are we there yet

194
00:18:18,800 --> 00:18:23,680
artificial intelligence? I don't think so. And probably artificial intelligence will look

195
00:18:23,680 --> 00:18:27,440
differently from human intelligence. It doesn't need to look like human intelligence. It's of

196
00:18:27,440 --> 00:18:33,200
course an interesting scientific question whether we can reproduce a human in silico, but for solving

197
00:18:33,200 --> 00:18:38,720
practical problems that will make this technology useful for the humanity, for the humankind,

198
00:18:39,680 --> 00:18:44,640
we probably need something different. It will certainly involve a certain level of abstraction

199
00:18:44,640 --> 00:18:48,640
that we currently don't have. It will probably require methods that we currently don't have,

200
00:18:49,600 --> 00:18:55,840
but it doesn't necessarily need to look like a recreation of a human. This is Dr. Petar Velichkovich.

201
00:18:56,560 --> 00:19:01,040
By now you'll have probably seen or heard something about our recently released proto book on

202
00:19:01,040 --> 00:19:06,560
geometric deep learning on grids, graphs, groups, geodesics, and gauges, or as we like to call it

203
00:19:06,560 --> 00:19:11,440
the 5Gs of geometric deep learning, which I've co-authored alongside Michael Bronstein,

204
00:19:11,440 --> 00:19:16,320
John Brunner, and Taco Cohen. And you might be wondering what all the fuss is about, because

205
00:19:16,320 --> 00:19:21,680
there's already a lot of really high quality synthesis textbooks on the field of deep learning

206
00:19:21,680 --> 00:19:25,760
in general, and also on some sub areas of geometric deep learning, such as graph neural

207
00:19:25,760 --> 00:19:30,880
networks, where Will Hamilton recently released a super high quality textbook on that area.

208
00:19:30,880 --> 00:19:38,320
This is Dr. Taco Cohen. So what we've been trying to do in our book project is to show

209
00:19:38,320 --> 00:19:44,960
that this geometric deep learning mindset is not just useful when tackling a new problem,

210
00:19:44,960 --> 00:19:50,720
but actually allows you to derive from first principles of symmetry and skill separation,

211
00:19:50,720 --> 00:19:56,080
many of the architectures and architectural primitives like convolution, attention,

212
00:19:56,080 --> 00:20:01,680
graph convolution, and so forth, that have become popular over the last few years.

213
00:20:02,160 --> 00:20:07,440
Even in cases where these considerations of active variance and skill separation

214
00:20:07,440 --> 00:20:11,600
were not felt at center when the methods were first discovered.

215
00:20:12,720 --> 00:20:19,200
Now we think that this is useful for a number of reasons. First of all, it might help to avoid

216
00:20:20,080 --> 00:20:25,600
reinventing the same ideas over and over. And this can easily happen when the number of papers

217
00:20:25,600 --> 00:20:32,880
that come out every day is far larger than what any one person can possibly read,

218
00:20:33,760 --> 00:20:40,400
and when different sub fields use different language to describe their ideas. Furthermore,

219
00:20:40,400 --> 00:20:46,800
it might help to clarify when a particular method is useful. A geometric deep learning method is

220
00:20:46,800 --> 00:20:52,560
useful when the problem domain has the particular symmetries that are built into the architecture.

221
00:20:53,360 --> 00:21:01,280
And finally, we hope that by making explicit the commonalities between seemingly different

222
00:21:01,280 --> 00:21:07,760
methods, it will become easier for new cars to learn geometric deep learning. Ideally,

223
00:21:07,760 --> 00:21:13,840
one would not have to go through the large number of architectures that have been designed,

224
00:21:13,840 --> 00:21:17,840
but just learn the general ideas of groups,

225
00:21:17,920 --> 00:21:23,760
equivariance, group representations and feature spaces and so on, and then see,

226
00:21:23,760 --> 00:21:28,800
for the particular instances, you're interested in how that fits into the general pattern.

227
00:21:29,520 --> 00:21:35,120
To really illustrate why do we think that such a synthesis is important and relevant for

228
00:21:35,120 --> 00:21:40,720
deep learning research going forward, we have to go way back, way back in the time of Euclid,

229
00:21:40,720 --> 00:21:48,000
around 300 years BC. And as you might know, Euclid is the founding father of Euclidean geometry,

230
00:21:48,000 --> 00:21:54,080
which for many, many years was the only way to do geometry. It relied on a certain set of postulates

231
00:21:54,080 --> 00:22:00,160
that Euclid had that governed all the laws of the geometry that he proposed. All of this started

232
00:22:00,160 --> 00:22:05,840
to drastically change around the 1800s when several mathematicians, in an effort to prove

233
00:22:05,840 --> 00:22:11,760
that Euclid's geometry is the geometry to be following, ended up assuming that one of the

234
00:22:11,760 --> 00:22:16,640
postulates is false and failing to drive a contradiction. They actually ended up deriving

235
00:22:16,640 --> 00:22:22,080
a completely new set of self-consistent geometries, all with their own set of laws and rules,

236
00:22:22,080 --> 00:22:28,960
and also quite differing terminologies. Among some of these popular variants are the hyperbolic

237
00:22:28,960 --> 00:22:35,840
geometry of Lobachevsky and Bolyai, and the elliptic geometries of Riemann. And for a very long time,

238
00:22:35,840 --> 00:22:42,000
because all of these geometries had completely different sets of rules and they were all self-consistent,

239
00:22:42,000 --> 00:22:46,880
people were generally wondering what is the one true geometry that we should be studying.

240
00:22:46,880 --> 00:22:54,160
A solution to this problem came several decades later through the work of a young German mathematician

241
00:22:54,160 --> 00:23:00,000
by the name of Felix Klein, who had just been appointed for a professorship position at the

242
00:23:00,000 --> 00:23:05,760
small Bavarian University of Erlangen, the so-called Friedrich Alexander University in Erlangen,

243
00:23:05,760 --> 00:23:12,400
Nuremberg. While he was at this post, he had proposed a direction that would eventually enable

244
00:23:12,400 --> 00:23:17,840
us to unify all of the geometries that were in existence at the time through the lens of

245
00:23:17,840 --> 00:23:24,240
invariances and symmetry using the language of group theory. And his work is now eponymously

246
00:23:24,240 --> 00:23:30,880
known as the Erlangen program. And there is no way to overstate how much of an important effect

247
00:23:30,880 --> 00:23:36,160
the Erlangen program had on mathematics and beyond. Because of the fact that it provided

248
00:23:36,160 --> 00:23:41,840
a unifying lens of studying geometry, suddenly people didn't need to hunt for the one true

249
00:23:41,840 --> 00:23:46,720
geometry they had a blueprint they could use to drive whatever geometry was necessary for the

250
00:23:46,720 --> 00:23:51,680
problem they were solving. And besides just mathematics, it had amazing spillover effects

251
00:23:51,680 --> 00:23:57,280
to other very important fields of science. For example, in physics, the Erlangen program spilled

252
00:23:57,280 --> 00:24:01,920
over through the work of Emy Nerther, demonstrated that all of the conservation laws in physics,

253
00:24:01,920 --> 00:24:06,320
which previously had to be validated through extensive experimental evaluation, could be

254
00:24:06,320 --> 00:24:11,680
completely derivable through the principles of symmetry. And needless to say, this is a very

255
00:24:11,760 --> 00:24:17,440
fundamental and game changing result in physics, which also allowed us to classify some elementary

256
00:24:17,440 --> 00:24:21,920
particles in what is now known as the standard model. Thinking back towards theoretical computer

257
00:24:21,920 --> 00:24:28,400
science, the Erlangen program also had a spillover effect into category theory, which is one of the

258
00:24:28,400 --> 00:24:32,960
most abstractified areas of theoretical computer science with a lot of potential for unifying

259
00:24:32,960 --> 00:24:37,760
various directions in mathematics. And actually in the words of the founders of category theory,

260
00:24:37,760 --> 00:24:43,040
the whole field of category theory can be seen as an extension of Felix Klein's Erlangen program.

261
00:24:43,680 --> 00:24:51,520
So the Erlangen program demonstrated how it's possible to take a small set of guiding principles

262
00:24:51,520 --> 00:24:57,280
of invariance and symmetry and use it to unify something as broad as geometry. I like to think

263
00:24:57,280 --> 00:25:03,600
of geometric deep learning as not a single method or architecture, but as a mindset. It's a way of

264
00:25:03,600 --> 00:25:08,640
looking at machine learning problems from the first principles of symmetry and invariance.

265
00:25:09,440 --> 00:25:15,520
And symmetry is a key idea that underpins our physical world and the data that is created by

266
00:25:15,520 --> 00:25:21,840
physical processes. And accounting for this structure allows us to beat the curse of dimensionality in

267
00:25:21,840 --> 00:25:27,680
machine learning problems. It is really a very powerful principle and very generic blueprint.

268
00:25:28,240 --> 00:25:32,880
And we find its instances in some of today's most popular deep learning architectures,

269
00:25:32,880 --> 00:25:38,400
whether it's convolutional neural networks, graph neural networks, transformers, LSTMs and many

270
00:25:38,400 --> 00:25:44,320
more. And it is also a way to design new machine learning architectures that are yet to be invented,

271
00:25:44,320 --> 00:25:51,280
maybe in the future, not based on back propagation and incorporate inductive bias in a principled way.

272
00:25:51,280 --> 00:25:56,000
Being a professor and a teacher, I would also like to emphasize the pedagogical dimension of

273
00:25:56,000 --> 00:26:02,160
this geometric unification. What I often see in deep learning when deep learning is taught is

274
00:26:02,960 --> 00:26:09,120
it appears as a bunch of hacks with weaker or no justification. And I think it is best illustrated

275
00:26:09,120 --> 00:26:14,720
with how, for example, the concept of convolution is explained. It is often given as a formula just

276
00:26:14,720 --> 00:26:20,880
out of the blue, maybe with a bit of hand waving. But what we try to show is that you can derive

277
00:26:20,880 --> 00:26:26,080
convolution from first principles, in this particular case, of translational symmetry.

278
00:26:26,640 --> 00:26:31,120
And I think the difference in this approach is best captured by what Elvetsos once said

279
00:26:31,120 --> 00:26:35,680
that the knowledge of principles easily compensates the lack of knowledge effects.

280
00:26:37,040 --> 00:26:44,080
Professor Bronstein has been a professor at Imperial College in London for the last three years

281
00:26:44,080 --> 00:26:51,440
and received his PhD with distinction from Technion, the Israeli Institute of Technology,

282
00:26:51,440 --> 00:26:59,040
in 2007. He's held visiting academic positions at MIT, Harvard and Stanford,

283
00:26:59,040 --> 00:27:07,680
and his work has been cited over 21,000 times. His main expertise is in theoretical and computational

284
00:27:07,680 --> 00:27:13,360
geometric methods for machine learning and data science, and his research encompasses a broad

285
00:27:13,360 --> 00:27:20,640
spectrum of applications ranging from computer vision and pattern recognition to geometry processing,

286
00:27:20,640 --> 00:27:27,840
computer graphics and biomedicine. Professor Bronstein coined and popularized the term

287
00:27:27,840 --> 00:27:36,080
geometric deep learning. His startup company, Fabula AI, which was acquired by Twitter in 2019,

288
00:27:36,080 --> 00:27:42,560
was one of the first applications of graph ML to the problem of misinformation detection. I think

289
00:27:42,560 --> 00:27:48,960
it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert

290
00:27:48,960 --> 00:27:54,960
in graph representation learning research. We are really probably some of the nicest locations

291
00:27:54,960 --> 00:28:01,600
in London, which is Kensington, if you're familiar, so we have the the Natural History Museum, the Science

292
00:28:01,600 --> 00:28:09,760
Museum and the Victoria and Albert Museum and Imperial College is right here, so I think it's

293
00:28:09,760 --> 00:28:15,200
as central as you can get. And if you walk all the way there, then you have Hyde Park,

294
00:28:16,160 --> 00:28:20,800
which is probably one of the nicest parks in London. I'm a professor in the department of

295
00:28:20,800 --> 00:28:25,200
computing at Imperial College London and head of graph learning research at Twitter.

296
00:28:26,000 --> 00:28:31,120
I work on geometric deep learning in particular on graph neural networks and their applications from

297
00:28:31,120 --> 00:28:37,520
computer vision and graphics to computational biology and drug design. Dr Petar Velichkovich

298
00:28:37,520 --> 00:28:44,800
is a senior research scientist at DeepMind in London and he obtained his PhD from Trinity College

299
00:28:44,880 --> 00:28:51,040
in Cambridge. His research has been focused on geometric deep learning and in particular

300
00:28:51,040 --> 00:28:56,960
devising neural network architectures for graph representation learning and its applications

301
00:28:56,960 --> 00:29:02,720
in algorithmic reasoning and computational biology. Petar's work has been published in the

302
00:29:02,720 --> 00:29:08,640
leading machine learning venues. Petar was the first author of graph attention networks,

303
00:29:08,640 --> 00:29:15,040
a popular convolutional layer for graphs and deep graph infomax, a scalable unsupervised

304
00:29:15,040 --> 00:29:20,640
learning pipeline for graphs. Hi everyone, my name is Petar Velichkovich and I'm a senior

305
00:29:20,640 --> 00:29:25,840
research scientist at DeepMind and previously I have done my PhD in computer science at the

306
00:29:25,840 --> 00:29:30,960
University of Cambridge where I'm actually still based and today we're actually here in Cambridge

307
00:29:30,960 --> 00:29:37,040
filming these shots and it is my great pleasure to be talking to you today about our work on

308
00:29:37,040 --> 00:29:43,920
geometric deep learning and related topics. I first got into computer science through competitive

309
00:29:43,920 --> 00:29:48,400
programming contests and classical algorithms the likes of which you might find in a traditional

310
00:29:48,400 --> 00:29:55,040
theoretical computer science textbook and this was primarily influenced by the way schooling

311
00:29:55,040 --> 00:30:00,560
worked for gifted students back in my hometown of Belgrade in Serbia where students were generally

312
00:30:00,560 --> 00:30:05,200
encouraged to take part in these theoretical contests and try to write programs that are just

313
00:30:05,200 --> 00:30:11,600
going to finish as fast as possible or work as efficiently as possible over a certain set of

314
00:30:11,600 --> 00:30:17,200
carefully contrived problems. All of this changed when I actually started my computer science degree

315
00:30:17,200 --> 00:30:22,800
here at Cambridge where I was suddenly exposed to a much wider wealth of computer science topics

316
00:30:22,800 --> 00:30:27,200
than just theoretical computer science and algorithms and for a brief moment my interests

317
00:30:27,200 --> 00:30:33,440
drifted elsewhere. Everything started to come back together when I started my final year project

318
00:30:33,440 --> 00:30:39,760
with Professor Pietro Leo at Cambridge and I had heard that bioinformatics is a topic that's

319
00:30:39,760 --> 00:30:44,320
brimming with classical algorithms and competitive programming algorithms specifically so I thought

320
00:30:44,320 --> 00:30:50,000
a project in this area would be a great way to bring these two closer. Unfortunately that was not

321
00:30:50,000 --> 00:30:57,280
to be as my mentor very quickly drew me into machine learning and that kind of spiraled out into my

322
00:30:57,360 --> 00:31:03,760
PhD topics where I was for a brief moment focused on computational biology topics before

323
00:31:03,760 --> 00:31:09,040
eventually drifting to graph representation learning and eventually geometric deep learning.

324
00:31:09,040 --> 00:31:15,120
My journey into geometric deep learning started actually through investigating

325
00:31:15,840 --> 00:31:21,120
graph representation learning which I think for a very long time these two areas have been seen

326
00:31:21,120 --> 00:31:25,600
as almost synonymous with one another because almost everything you come up with in the area

327
00:31:25,600 --> 00:31:30,640
of geometric deep learning can be if you squint hard enough seen as a special case of graph

328
00:31:30,640 --> 00:31:36,880
representation learning. What originally brought me into this was an internship at Montreal's

329
00:31:36,880 --> 00:31:43,360
Artificial Intelligence Institute Miele where I worked alongside Joshua Bengio and Adriana Romero

330
00:31:43,360 --> 00:31:50,000
on initially methodologies for processing data that lives on meshes of the human brain.

331
00:31:50,640 --> 00:31:55,120
We found out that the existing proposals for processing data over such a mesh

332
00:31:55,120 --> 00:31:59,440
both in graph neural networks and otherwise were not the most adequate for the kind of data

333
00:31:59,440 --> 00:32:04,320
processing that we needed to do and we needed something that would be aligned more with image

334
00:32:04,320 --> 00:32:10,080
convolutions in spirit in a way that allows us to give different influences to different

335
00:32:10,080 --> 00:32:16,080
neighbors in the mesh and this led us to propose graph attention networks which was a paper that

336
00:32:16,080 --> 00:32:21,680
we published at Eichler 2018. It was actually my first top tier conference publication and

337
00:32:22,400 --> 00:32:27,600
what I'm probably most well known for nowadays. The field of graph representation learning has

338
00:32:27,600 --> 00:32:32,720
then spiraled completely out of control in terms of the quantity of papers being proposed.

339
00:32:33,600 --> 00:32:38,640
Only one year after the graph attention network paper came out I was reviewing for

340
00:32:38,640 --> 00:32:43,520
some of the conferences in the area and I found on my reviewing stack four or five papers that

341
00:32:43,520 --> 00:32:47,680
were extending graph attention nets in one way or another so the field certainly has become a lot

342
00:32:47,680 --> 00:32:53,760
more vibrant because of a nice barrier of entry which is not too high. Recently Pettai has been

343
00:32:53,760 --> 00:32:59,920
doing some really interesting research in algorithmic reasoning. Part of the skill of a software

344
00:32:59,920 --> 00:33:06,640
engineer lies in choosing which algorithm to use only rarely will an entirely novel algorithm be

345
00:33:06,640 --> 00:33:13,280
warranted. The key guarantee which traditional symbolic algorithms give us is generalization

346
00:33:13,280 --> 00:33:19,120
to new situations. Traditional algorithms and the predictions given by deep learning models

347
00:33:19,120 --> 00:33:24,800
have very different properties. The former provides strong guarantees but are inflexible

348
00:33:24,800 --> 00:33:30,640
to the problem being tackled while the latter provide few guarantees but can adapt to a wide

349
00:33:30,640 --> 00:33:36,800
range of problems. Now Pettai in his paper proposed a neural architecture which can take in natural

350
00:33:36,800 --> 00:33:43,760
inputs but output a graph of abstract outputs as well as natural outputs. Pettai believes that

351
00:33:43,760 --> 00:33:49,200
neural algorithmic reasoning will allow us to apply classical algorithms on inputs that they were

352
00:33:49,200 --> 00:33:56,880
never originally designed for. I am studying algorithmic reasoning which is a novel area of

353
00:33:56,880 --> 00:34:02,960
representation learning that seeks to find neural networks that are as good as possible at

354
00:34:02,960 --> 00:34:07,520
imitating the computations of exactly the kind of classical algorithms that initially brought me

355
00:34:07,520 --> 00:34:13,760
to computer science. It turns out that this area is remarkably rich and could have remarkably big

356
00:34:13,760 --> 00:34:19,760
implications for machine learning in general because it could bring the best of the algorithmic

357
00:34:19,760 --> 00:34:25,600
domain into the domain of neural networks and if you look at the pros and cons of the two you'll

358
00:34:25,600 --> 00:34:29,840
find that they're very complementary. So the fusion of the two can really bring the kinds of

359
00:34:29,840 --> 00:34:36,640
benefits we haven't seen before. So I am very pleased to say that I'm among those researchers

360
00:34:36,640 --> 00:34:42,640
that is extremely proud and happy of what I do because it brings together some of my earliest

361
00:34:42,640 --> 00:34:47,360
passions in computer science with the latest trends in machine learning and especially

362
00:34:47,360 --> 00:34:53,440
geometric deep learning which we recently released a proto book about with Joan, Michael and Taco.

363
00:34:53,440 --> 00:34:58,800
We spoke to Christian Saagedi and he's doing some interesting work with algorithmic reasoning

364
00:34:58,800 --> 00:35:04,880
creating abstract syntax trees to represent mathematical theorems for example and then

365
00:35:04,880 --> 00:35:09,920
he believes that in that representation space he projects it all into a Euclidean space that

366
00:35:09,920 --> 00:35:15,440
there's some interesting interpolative points in that space but again surely there must be some

367
00:35:15,440 --> 00:35:20,320
deeper structure which analogizes mathematics which would allow us to extrapolate and discover

368
00:35:20,320 --> 00:35:25,280
new interesting mathematics. It just feels that what we're missing is the right kind of structure.

369
00:35:25,280 --> 00:35:30,320
I think for mathematics it's relatively easy to formalize it because well we can write logic

370
00:35:30,320 --> 00:35:36,160
rules and basically we can build mathematics axiomatically from very basic principles. These

371
00:35:36,160 --> 00:35:41,440
methods are already being used for computer proof of certain theorems. I think it's not

372
00:35:41,440 --> 00:35:47,040
well regarded by the purists in pure mathematics but probably they will need to accept it and well

373
00:35:47,040 --> 00:35:52,000
you know maybe there will be fields medal that will be given for a proof that is done by a computer

374
00:35:52,000 --> 00:35:58,160
I think even recently there are some important breakthrough results proofs that were given by a

375
00:35:58,160 --> 00:36:05,440
computer so it is probably just the beginning of a new way of doing science essentially even

376
00:36:05,440 --> 00:36:10,960
as pure science as creative science as mathematics which was considered really the hallmark of human

377
00:36:10,960 --> 00:36:16,080
intelligence it can be maybe if not replaced assisted by by artificial intelligence.

378
00:36:17,040 --> 00:36:22,960
Petr invokes Daniel Kahneman's system one and system two. He thinks that we need something

379
00:36:22,960 --> 00:36:29,920
like system two to achieve the kind of reasoning and generalization which currently eludes us

380
00:36:30,560 --> 00:36:35,920
in deep learning models. What I'm holding in my hands right now is the international bestseller

381
00:36:35,920 --> 00:36:42,960
on thinking fast and slow from the famous Nobel Prize winner Daniel Kahneman. This book can be seen

382
00:36:42,960 --> 00:36:48,720
as one of the main theses behind my ongoing work in algorithmic reasoning and what it stands for

383
00:36:48,720 --> 00:36:54,320
because it argues that fundamentally we as humans employ two different systems that operate at

384
00:36:54,320 --> 00:37:00,400
different rates system one which primarily deals with perceptive tasks and system two which deals

385
00:37:00,400 --> 00:37:07,040
with longer range reasoning tasks and it is my belief that currently where our research in neural

386
00:37:07,040 --> 00:37:13,280
networks has taken us is to get really really good at automating away system one so being able to

387
00:37:13,280 --> 00:37:20,240
perform perceptive tasks from large quantities of observed data probably in a not too dissimilar

388
00:37:20,240 --> 00:37:26,400
manner from the way humans do it. What I feel is really missing from these architectures nowadays

389
00:37:26,400 --> 00:37:31,600
is the system two aspect being able to actually take these percepts that we've acquired from the

390
00:37:31,600 --> 00:37:37,760
environment and actually properly do rigid reasoning over them in a manner that will stay

391
00:37:37,760 --> 00:37:43,200
consistent even if we drastically change the number of objects slightly perturb the laws of physics

392
00:37:43,200 --> 00:37:48,640
or something like that. In my opinion algorithmic reasoning the art of capturing these kinds of

393
00:37:48,640 --> 00:37:53,520
reasoning computations inside a neural network that was trained specifically for that purpose

394
00:37:53,520 --> 00:37:57,840
and then slotting that neural network into a different architecture that works with raw

395
00:37:57,840 --> 00:38:03,360
percepts is one potentially very promising blueprint that will take the space of classical

396
00:38:03,360 --> 00:38:08,640
algorithms that we have been building in this system two space and carry them over into raw

397
00:38:08,640 --> 00:38:13,600
perceptive inputs which these algorithms were very rarely designed to work over. This is Dr.

398
00:38:13,600 --> 00:38:20,960
Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher at Qualco AI Research and I work on

399
00:38:20,960 --> 00:38:26,960
geometric deep learning, equivariate networks and more recently also on causal inference and causal

400
00:38:26,960 --> 00:38:33,920
representation learning. Now I've been interested for a number of years already since about 2013

401
00:38:33,920 --> 00:38:39,280
in the application of ideas around symmetry, invariance, equivariance and the underlying

402
00:38:39,280 --> 00:38:45,440
mathematics of group theory and group representation theory to machine learning and deep learning

403
00:38:45,440 --> 00:38:50,720
specifically. And so it's been quite exciting to see over the last few years really the blossoming

404
00:38:50,720 --> 00:38:57,120
of this field that we now call geometric deep learning. Many new methods such as various kinds

405
00:38:57,120 --> 00:39:03,520
of equivariate convolutions for different spaces, different groups of symmetries, different geometric

406
00:39:03,520 --> 00:39:09,520
feature types, equivariate transformers and attention mechanisms, point cloud networks,

407
00:39:09,520 --> 00:39:15,120
graph neural networks and so forth. And along with these new methods also a large number of

408
00:39:15,680 --> 00:39:21,840
applications that have been tackled. Anything from medical imaging to the analysis of global

409
00:39:21,840 --> 00:39:29,440
weather and climate data to the analysis of DNA sequences and proteins and other kinds of molecules.

410
00:39:29,440 --> 00:39:34,560
So if you apply this mindset, the first question you ask when faced with such a new problem is

411
00:39:34,560 --> 00:39:41,280
what are the symmetries? What are the transformations that I can apply to my data that may change the

412
00:39:41,280 --> 00:39:47,600
numerical representation of my data as stored in my computer, but that nevertheless don't change

413
00:39:47,600 --> 00:39:53,760
the underlying objects we're interested in. Whether that's reordering the nodes of a graph,

414
00:39:54,320 --> 00:40:02,320
rotating a molecule in 3D or many other kinds of symmetries. Once you know the group of symmetries,

415
00:40:02,320 --> 00:40:09,280
you can then develop a neural network that's equivariate symmetries. And hopefully is a

416
00:40:09,360 --> 00:40:16,160
universal approximator among equivariate functions. What we found, what many others have found time

417
00:40:16,160 --> 00:40:22,240
and again, is that if you build this symmetry prior to your network, if you make your network

418
00:40:22,240 --> 00:40:27,760
equivariate, it is bound to be much more data efficient and to generalize much better.

419
00:40:28,560 --> 00:40:38,400
Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this book

420
00:40:38,400 --> 00:40:44,320
is that really a lot of different architectures, they can be understood in one as essentially a

421
00:40:44,320 --> 00:40:50,400
special cases of one generic structure that we call the geometric deep learning blueprint.

422
00:40:51,120 --> 00:40:58,960
So to explain a little bit about what this is all about, the blueprint refers to first of all

423
00:40:58,960 --> 00:41:06,320
feature spaces. So I'll explain how we model those. And then it refers to maps between feature

424
00:41:06,320 --> 00:41:12,480
spaces or layers of the network. And they also have to somehow respect the structure of the

425
00:41:12,480 --> 00:41:18,720
feature spaces. So in all cases, whether it's, you know, graph neural net, a network for processing

426
00:41:18,720 --> 00:41:26,080
images on the plane, or a network for processing signals on a sphere like global weather data,

427
00:41:26,880 --> 00:41:33,120
we're dealing with data that lives on a domain. So the domain in the examples I just gave would

428
00:41:33,120 --> 00:41:39,680
be the set of nodes of the graph, or perhaps also the set of edges, the points on the plane,

429
00:41:39,680 --> 00:41:45,360
or the points on the sphere. And of course, you can think of many other examples as well.

430
00:41:45,360 --> 00:41:51,520
This is what we call the domain. We write it as omega. It's typically, it's a set, first of all,

431
00:41:51,520 --> 00:41:56,640
and it may have some additional structure. So in the case of the sphere, it has some interesting

432
00:41:56,640 --> 00:42:02,560
topology. And typically, we also want to think about the geometry, want to think about distances

433
00:42:02,640 --> 00:42:09,200
and angles. So it's a set with some kind of structure. And in addition, it has some symmetries,

434
00:42:09,200 --> 00:42:15,120
meaning there's some transformations we can do to this set, that will preserve the structure that

435
00:42:15,120 --> 00:42:21,200
we think is important. So if we think distances are important on, let's say the sphere, when the

436
00:42:21,200 --> 00:42:26,240
kinds of symmetries we end up with our rotations, three dimensional rotations of the sphere,

437
00:42:26,320 --> 00:42:34,160
perhaps also reflections. In the case of a graph, the symmetries would be permutations

438
00:42:34,160 --> 00:42:42,800
of the nodes and also corresponding permutation of the edges. So you just change the order of

439
00:42:42,800 --> 00:42:48,320
the nodes. If node one and two were connected, you apply permutation and move those to node three

440
00:42:48,320 --> 00:42:53,520
and four, then now three and four have to be connected. So that's a symmetry of our space

441
00:42:53,520 --> 00:43:00,720
overgon. Now the data, this is a very important point, the data are typically not points in this

442
00:43:00,720 --> 00:43:06,400
space. So when we're classifying images, well, our space is the plane, but the data are not

443
00:43:06,400 --> 00:43:11,200
points in the plane, they're not the two dimensional vectors, the data is really a signal

444
00:43:12,160 --> 00:43:17,280
on the plane, a two dimensional image, which so you can think of that as a function from for each

445
00:43:17,280 --> 00:43:25,600
point in the plane, we have a pixel value. So in the more general cases, it might be

446
00:43:25,600 --> 00:43:31,520
something called a field. So you might say have wind direction on on earth, that's a

447
00:43:32,160 --> 00:43:37,600
vector field on the sphere. Now the symmetries that we talked about, they act on this space,

448
00:43:38,240 --> 00:43:43,760
you could show how they automatically also act on the space of signals. So those are the key

449
00:43:43,760 --> 00:43:50,480
ingredients to define what a feature space is, you have your your space, omega, so set of nodes,

450
00:43:50,480 --> 00:43:55,760
sphere, whatever, then you have a group of symmetries of that space. And then you have

451
00:43:55,760 --> 00:44:01,840
the space of signals or feature maps on this space, and you have the group acting on your

452
00:44:01,840 --> 00:44:09,040
signal. So you can rotate a vector field, or you can shift a planar image or etc. So those are the

453
00:44:09,040 --> 00:44:16,160
key ingredients to define a feature space. And then we can talk about layers. And so the layers or

454
00:44:16,160 --> 00:44:24,080
maps of the network, they have to respect this structure. So if we have a signal on on the sphere,

455
00:44:24,880 --> 00:44:31,280
and we believe that rotating it doesn't change it in any essential way, or we permute the nodes in

456
00:44:31,280 --> 00:44:37,280
the graph, but it's still the same graph, then we want the layers of the network to respect that.

457
00:44:37,280 --> 00:44:43,440
And to respect the structure means to be actually variant to the symmetries. So if we apply our

458
00:44:43,440 --> 00:44:51,440
transformation to our signal, and then we apply our network layer, it should be the same as applying

459
00:44:51,440 --> 00:44:56,640
the network layer to the original input, and then applying a transformation in the output space.

460
00:44:57,440 --> 00:45:01,520
Now how the transformation acts in the output space could be different from the input space. So for

461
00:45:01,520 --> 00:45:10,160
example, you could have a vector field as input, and a scalar field as output, they transform

462
00:45:10,160 --> 00:45:13,760
differently. So that's why for each layer and network, you're going to have a different feature

463
00:45:13,760 --> 00:45:20,160
space with a different action of the group. But it's the same group acting in each feature space,

464
00:45:20,160 --> 00:45:25,440
the maps, they should be equivariant. And these maps, they include both the linear maps, which are

465
00:45:25,440 --> 00:45:32,880
typically the learnable layers, and the non linearities. Now for linear maps, you can study all

466
00:45:32,880 --> 00:45:37,760
sorts of interesting questions, you can ask what is the most general kind of equivariant linear

467
00:45:37,760 --> 00:45:47,040
map. And it turns out that for a large class of group actions, or linear group actions, the most

468
00:45:47,040 --> 00:45:52,400
general kind of equivariant linear maps are generalized forms of convolutions. So that's

469
00:45:52,400 --> 00:46:00,720
really an explanation for why convolutions are so ubiquitous. The final ingredient that I think

470
00:46:00,720 --> 00:46:06,720
is key to the success of many architectures is some kind of pooling or cautioning operation.

471
00:46:07,600 --> 00:46:12,800
So the structures we've talked about so far are, you know, this global space and the symmetries on

472
00:46:12,800 --> 00:46:20,560
it, etc. But typically, there's also a notion of distance or locality in our space. And if we

473
00:46:21,200 --> 00:46:27,680
just enforce that our layers have to respect the symmetries, well that would force us to use a

474
00:46:27,680 --> 00:46:35,200
convolution in many cases, and I just mentioned, but not forces to use local filters. And we all

475
00:46:35,200 --> 00:46:40,000
know that using sort of global filters is not going to be very effective use of parameters.

476
00:46:41,200 --> 00:46:49,760
So locality is another key thing, locality in the filters. And also localities exploited via

477
00:46:49,760 --> 00:46:57,200
some kind of pooling or caution operation. Now, going forward to say the year 2020, deep learning

478
00:46:57,200 --> 00:47:01,920
is all the craze right now. And so many different deep learning architectures are being proposed,

479
00:47:01,920 --> 00:47:07,040
convolutional neural networks, graph neural networks, LSTMs and so on. When these architectures are

480
00:47:07,040 --> 00:47:11,360
being proposed, different terminologies are used because people tend to come from different areas

481
00:47:11,360 --> 00:47:16,800
when they're proposing them. And also, they are usually followed by kinds of bombastic statements

482
00:47:16,800 --> 00:47:20,560
such as everything can be seen as a special case of a convolutional neural network,

483
00:47:20,560 --> 00:47:25,600
transformers use self-attention and attention is all you need. Graph neural networks work on graphs

484
00:47:25,600 --> 00:47:31,520
and everything can be represented as a graph. And LSTMs are turing complete, so why would you ever

485
00:47:31,520 --> 00:47:38,800
need anything else? So I hope that this illustrates how the field of deep learning in the year of 2020

486
00:47:38,800 --> 00:47:44,640
is really not all that different from the state of geometry in the 1800s. And if history teaches

487
00:47:44,640 --> 00:47:49,200
anything about how we can unify these fields together, now is the right time for us to look

488
00:47:49,200 --> 00:47:54,080
back, study the geometric principles underlying the architectures that we use. And as a result,

489
00:47:54,080 --> 00:47:59,120
we might just derive a blueprint that will allow us to reason about all the architectures we have

490
00:47:59,120 --> 00:48:04,480
in the past, but also any architectures that we might come up with in the future. And in my opinion,

491
00:48:04,480 --> 00:48:11,360
that is the key selling point of our recently released proto book. And I hope that it is helpful

492
00:48:11,360 --> 00:48:16,000
in guiding deep learning research going forward. I should highlight that I was also extremely,

493
00:48:17,120 --> 00:48:23,600
extremely honored to deliver the first version of the talk presenting our proto book at

494
00:48:23,600 --> 00:48:30,080
Frederic Alexander University of Erlang in exactly the same place where the Erlang program was

495
00:48:30,080 --> 00:48:36,000
originally brought up, albeit because of the existing COVID restrictions, I had to do so

496
00:48:36,000 --> 00:48:40,960
in a virtual manner. So I think a lot of people understand convolutions in the way of a

497
00:48:40,960 --> 00:48:46,160
traditional plain us, CNN, and the mathematical notion of a convolution, which is very closely

498
00:48:46,160 --> 00:48:51,440
related to, you know, a Fourier transform, for example. But graph convolutional networks,

499
00:48:51,440 --> 00:48:57,120
they seem to abstract the notion of a convolution into some kind of concept of the neighborhood

500
00:48:57,120 --> 00:49:03,760
and local connectivity. And some of your work as well with, you know, equivariate convolutional

501
00:49:03,760 --> 00:49:07,120
neural networks, I think do the same thing. So when we talk about convolution, are we talking

502
00:49:07,120 --> 00:49:12,240
about a very abstract notion of it? Yeah, that's a great question. I think that

503
00:49:12,960 --> 00:49:20,640
there are so many different ways to get at convolution. You can think of it in sort of

504
00:49:20,640 --> 00:49:25,520
you've trying to think of it in terms of sliding a filter over some space, right? So you put it

505
00:49:25,520 --> 00:49:32,240
in a canonical position, and then you slide it around. That is an idea that you can generalize

506
00:49:32,240 --> 00:49:38,880
to not just sliding over a plane, but say applying some transformation from a group

507
00:49:38,880 --> 00:49:43,520
to your filter. So let's say you're up, you have a filter on the sphere, then you can,

508
00:49:44,000 --> 00:49:48,720
you have, you know, the sphere has a symmetry group, mainly three dimensional rotations,

509
00:49:48,720 --> 00:49:53,360
group SO3, you can apply an element of SO3, a three dimensional rotation to your filter,

510
00:49:53,360 --> 00:49:59,200
even sort of slide it over, over the sphere. That leads to something called group convolution.

511
00:50:00,000 --> 00:50:04,800
So that's one way to think about it. There is indeed, as you mentioned, the spectral

512
00:50:04,800 --> 00:50:10,400
wave looking at it. So you could think there's the famous Fourier convolution theorem,

513
00:50:11,360 --> 00:50:19,600
which says that in the spectrum, convolution is just a point wise product. So one way to

514
00:50:19,600 --> 00:50:24,720
implement a convolution would be to take a Fourier transform of your signal, your feature map,

515
00:50:24,720 --> 00:50:30,320
and a Fourier transform of your filter, multiply them point wise, and then inverse Fourier

516
00:50:30,320 --> 00:50:37,280
transform to get the result. And this perspective also generalizes. So it generalizes to graphs,

517
00:50:37,840 --> 00:50:45,200
where the Fourier transform, or something analogous to it, can be obtained via graph

518
00:50:45,200 --> 00:50:52,800
laplacians, as well as some other ideas. That is actually historically how some of the first

519
00:50:52,800 --> 00:51:00,400
graph neural nets were implemented and motivated. There's also a spectral theory for group

520
00:51:00,400 --> 00:51:09,680
convolutions. So indeed, the Fourier transform can be generalized, or the standard Fourier

521
00:51:09,680 --> 00:51:15,600
transform that we all know, is actually the Fourier transform for the plane, the plane being

522
00:51:15,600 --> 00:51:22,000
a particular group, the translation group in two dimensions. So there is a whole, a very beautiful

523
00:51:22,000 --> 00:51:30,880
theory of, let's say, Fourier generalized Fourier transforms for groups, where now the spectrum

524
00:51:32,000 --> 00:51:41,680
is indexed not by the just by the integers, as it is the case for the for the for the line or the

525
00:51:41,680 --> 00:51:47,040
plane, but by something called irreducible representations. In the case of the plane,

526
00:51:47,040 --> 00:51:54,320
those are indeed indexed by integers. And the the spectrum is not just scalar valued,

527
00:51:54,320 --> 00:51:59,360
or complex scalar valued, but it can be matrix valued. If you're interested in this sort of

528
00:51:59,360 --> 00:52:04,000
stuff, you can, you want sort of a very high level description, you can check out our paper on

529
00:52:04,000 --> 00:52:09,680
spherical CNNs, where we actually implement convolution on a sphere, using this kind of

530
00:52:09,680 --> 00:52:15,520
generalized Fourier transform. So that's the Fourier perspective on convolutions.

531
00:52:16,320 --> 00:52:22,880
And there's a final perspective, which I think is quite intuitive, which is that

532
00:52:23,760 --> 00:52:32,320
the convolution is the most general kind of equivariant linear map between certain group

533
00:52:32,320 --> 00:52:39,840
between certain linear group actions. So specifically, these group actions are the

534
00:52:39,840 --> 00:52:46,640
way that groups tend to act on the space of signals on your space. So you might have space

535
00:52:46,640 --> 00:52:52,800
of scalar signals on the sphere. And your feature map, you might want to have another scalar signal

536
00:52:52,800 --> 00:52:57,680
in the sphere or a vector field on the sphere or something. Then you can ask what is the most

537
00:52:57,680 --> 00:53:03,040
general kind of equivariant linear map. And the answer is, it's a convolution. And that is also

538
00:53:03,040 --> 00:53:09,040
true in it in a very general setting. So that that to me is the most intuitive way of understanding

539
00:53:09,120 --> 00:53:15,200
convolutions as the most general kind of maps that are linear maps that are equivariate to

540
00:53:15,200 --> 00:53:20,320
certain kinds of group actions. Professor Bruner, welcome to MLST. It's an absolutely

541
00:53:20,320 --> 00:53:27,120
honor to have you on. Introduce yourself. Yeah. Hi, Tim. I'm Joanne Bruner. I'm a associate professor

542
00:53:27,120 --> 00:53:32,720
at the Quran Institute and Center for Data Science at New York University. And I'm very happy to be

543
00:53:32,720 --> 00:53:40,320
here chatting with you at MLST. Joanne, you just released this geometric deep learning proto book.

544
00:53:40,320 --> 00:53:45,200
You know, what does it mean to you? What was your kind of intellectual journey that led to this?

545
00:53:46,400 --> 00:53:54,080
Yeah. So this journey, in fact, started many years ago. So I would say even like during my postdoc,

546
00:53:54,080 --> 00:54:00,480
I was a postdoc. I was doing my postdoc here at NYU with Yanle Kun. And that was the time

547
00:54:01,040 --> 00:54:09,600
2013, where, you know, confnets were already showing a big promise in image tasks. And

548
00:54:10,160 --> 00:54:16,160
discussing with Yan, the questions are, okay, how about domains that are not like grades, right? So,

549
00:54:16,160 --> 00:54:21,920
and that was the beginning of a journey that also included my former collaborator, Arthur Slump,

550
00:54:21,920 --> 00:54:27,440
who was like another researcher in the lab that had a similar background as me, like coming from

551
00:54:27,440 --> 00:54:32,800
applied math, but looking into into into more and more deep learning. So I would say that the

552
00:54:32,800 --> 00:54:39,040
genesis was our first attempt at extending the success of convolutional networks to these

553
00:54:39,040 --> 00:54:45,200
regular domains. And we published the paper at iClear 2014. And after that, I think things started

554
00:54:45,200 --> 00:54:51,040
like quite naturally because other researchers that had, I would say, came from the same background,

555
00:54:51,040 --> 00:54:56,560
like, you know, maybe from geometric background, also started to realize that there was something

556
00:54:56,560 --> 00:55:00,160
there, maybe bigger than these particular papers, in particular, Michael Brownstone,

557
00:55:01,440 --> 00:55:06,000
it's kind of reached out to us just afterwards saying, oh, and we are also looking at similar

558
00:55:06,000 --> 00:55:11,200
ideas. I think we should just team together and start to think about this more globally. And so,

559
00:55:12,240 --> 00:55:18,800
you know, things that developed from there. And we wrote this journal paper, like a review paper

560
00:55:18,800 --> 00:55:25,920
in 2017, with Arthur, Yan, Michael, the airbender guys, who is another very well known figure in

561
00:55:26,000 --> 00:55:31,280
this area. And so from there, I think that, yeah, things like started to slowly take off. We had

562
00:55:31,280 --> 00:55:37,600
tutorials and new ribs that we had a very successful workshop at IPAM. After this, I think that the

563
00:55:38,560 --> 00:55:42,640
thing were clear that, okay, maybe at some point in the future, we should try to stamp all these

564
00:55:42,640 --> 00:55:48,480
things into a book that tries to reflect something a bit more, let's say, mature and, you know, what

565
00:55:48,480 --> 00:55:54,960
is our, if we wanted to have like some kind of legacy for future generations on how to implement

566
00:55:54,960 --> 00:55:59,760
and communicate these methods, how would that be? So I guess that was the genesis of the book. And so

567
00:56:01,520 --> 00:56:06,960
very soon we said with Michael that we also would like to have some, you know, fresh minds and fresh

568
00:56:06,960 --> 00:56:13,120
energy on board. So naturally, the names of Taco and better came very naturally to us as people

569
00:56:13,120 --> 00:56:18,800
who had been doing excellent work in the domain that would very nicely complement our skills.

570
00:56:18,800 --> 00:56:25,360
So the team was created. And that's the project. And so, yeah, I mean, I think it's been very

571
00:56:25,360 --> 00:56:31,920
interesting so far. Of course, I guess that, as you know, this is just an ongoing project, right?

572
00:56:31,920 --> 00:56:38,080
So it's still not finalized. But hopefully we're getting interest from the community. And this

573
00:56:38,080 --> 00:56:43,040
gives us some kind of, I would say positive vibes to finish it on time. As you know, writing books

574
00:56:43,040 --> 00:56:48,800
is always like this never ending process. So I think that, yeah, that's, it's been an

575
00:56:48,800 --> 00:56:54,800
interesting endeavor so far, for sure. I asked Professor Bromstein what his most passionately

576
00:56:54,800 --> 00:56:59,840
held belief is about machine learning. I think machine learning is such a field where

577
00:56:59,840 --> 00:57:05,600
holding strong beliefs is often counterproductive. It happened to me multiple times that something

578
00:57:05,600 --> 00:57:12,560
that I thought or said was very quickly overturned. And what I mean is that milestones or progress

579
00:57:13,040 --> 00:57:18,720
was achieved much faster than I could even imagine in a wild dream. So making predictions about

580
00:57:18,720 --> 00:57:24,640
machine learning is, to some extent, an ungrateful job. I do, however, believe that in order to make

581
00:57:24,640 --> 00:57:29,920
progress to the next level and make machine learning achieve its potential to become the

582
00:57:29,920 --> 00:57:35,600
transformative technology we trust and use ubiquitously, it must be built on solid mathematical

583
00:57:35,600 --> 00:57:41,200
foundations. And I also think that machine learning will drive future scientific breakthroughs.

584
00:57:41,200 --> 00:57:46,800
And probably a good litmus test would be a Nobel Prize awarded for a discovery made by

585
00:57:46,800 --> 00:57:51,440
or with the help of an ML system. It might already happen in the next decade.

586
00:57:52,080 --> 00:57:58,400
I wanted to go into a few questions actually about the book. So first of all, Joanne, in your

587
00:57:58,400 --> 00:58:04,800
2017 paper, The Mathematics of Deep Learning, you cited the universal function approximation theorem,

588
00:58:04,800 --> 00:58:09,360
which is to say the ability of a shallow neural network to approximate arbitrary functions.

589
00:58:09,360 --> 00:58:14,080
But the performance of wide and shallow neural networks can be significantly beaten by deep

590
00:58:14,080 --> 00:58:18,800
networks. And you said that one of the possible explanations is that deep architectures are able

591
00:58:18,800 --> 00:58:24,560
to better capture invariant properties of the data compared to their shallow counterparts.

592
00:58:24,560 --> 00:58:28,960
Could you just briefly introduce the universal function approximation theorem? And do you think

593
00:58:28,960 --> 00:58:35,520
it's still relevant for today's neural networks? Yeah, I mean, that's a very deep and an important

594
00:58:35,520 --> 00:58:41,520
question. Yeah, so universal approximation theorem, it refers to this very general principle that

595
00:58:41,520 --> 00:58:47,200
once you define parametric class, let's say you're you are on a learn functions using neural

596
00:58:47,200 --> 00:58:52,800
nets, it just describes your ability that as you put more and more parameters into your class,

597
00:58:52,800 --> 00:58:58,240
you're going to able to to approximate essentially anything that data nature throws at you. And so

598
00:58:58,240 --> 00:59:06,160
this might seem like a very powerful property. But in fact, in fact, it's it's something that

599
00:59:06,880 --> 00:59:11,840
you have probably already encountered many times during your undergrad. I mean, if you have any,

600
00:59:11,840 --> 00:59:17,120
let's say, background in, you know, single processing electrical engineering, there's many

601
00:59:17,120 --> 00:59:22,240
ways in which students have learned how to represent data, like signals, for example,

602
00:59:22,240 --> 00:59:27,120
using Fourier transform. So Fourier transforms are an instance of a class that has universal

603
00:59:27,200 --> 00:59:32,320
approximation. So in that sense, it's a it's a I think, going back to the second part of your

604
00:59:32,320 --> 00:59:38,400
question, how relevant it is to in the context of neural networks, and how far does this thing

605
00:59:38,400 --> 00:59:44,560
pushes towards understanding why deep learning works. So I would say there's a there's two sides

606
00:59:44,560 --> 00:59:52,480
of the answer. On one hand, I think that universal approximation is is a tool that when you combine

607
00:59:52,480 --> 01:00:00,160
it with other elements, it becomes something that provides good guiding principles. For instance,

608
01:00:00,160 --> 01:00:04,880
universal approximation of a generic function, we know that yeah, we can as I said, we can obtain

609
01:00:04,880 --> 01:00:10,080
it with, you know, very, very naive architectures, for example, just a shallow neural network without

610
01:00:10,080 --> 01:00:15,920
any kind of physical structure, special structure already has this property. Does it actually help

611
01:00:16,000 --> 01:00:22,160
us to to learn very efficiently? No, right, and I'm going to go at this afterwards. But when you

612
01:00:22,160 --> 01:00:28,400
combine it, for example, with, you know, let's say that now your data lives on a graph, or your data,

613
01:00:28,400 --> 01:00:32,720
I don't know, has a certain like a come from a physical lab that has certain properties,

614
01:00:32,720 --> 01:00:37,200
let's say that it's rotational invariant. Like the first thing that that the designer,

615
01:00:37,200 --> 01:00:41,120
like a domain scientist would like to know, if you come there and you design your neural

616
01:00:41,120 --> 01:00:46,960
net, look, I have a neural net that takes your data and has very good performance. The first thing

617
01:00:46,960 --> 01:00:52,080
that he will ask is, okay, how general is your architecture, right? Can it explain anything

618
01:00:52,080 --> 01:00:57,200
that they could throw at you? It seems like it's a, in that sense, I would, I would present it more

619
01:00:57,840 --> 01:01:03,040
as a sufficient condition, like a check mark that your, your, you know, your architecture needs to

620
01:01:03,040 --> 01:01:07,360
fall, right? If you make more and more parameters, can you express more and more elements functions

621
01:01:07,360 --> 01:01:14,320
from your class? But then, as I said, is it far from being sufficient, right? It's like, it's,

622
01:01:14,320 --> 01:01:18,400
sorry, it's a, it's a necessary condition, but it's far from being sufficient in the sense that

623
01:01:20,000 --> 01:01:27,040
universal approximation has a flavor is a result that does not quantify how many parameters

624
01:01:27,040 --> 01:01:31,600
do I need, right? Like, you know, if I want to approximate function, let's say I want to classify

625
01:01:31,600 --> 01:01:36,320
between different dog breeds, it doesn't tell me this theorem doesn't tell me how many parameters,

626
01:01:36,320 --> 01:01:40,080
how many neurons do I need for that, right? It's, it's a statement that in that sense,

627
01:01:41,120 --> 01:01:44,480
it lets, it leaves you a little bit with your, like, say, like, you know, like,

628
01:01:45,120 --> 01:01:49,040
like the, it's a bittersweet result, right? It doesn't, it doesn't really tell you anything

629
01:01:49,040 --> 01:01:55,600
actual. So that's why, and that's why we enter these, these other questions that is actually

630
01:01:55,600 --> 01:02:02,080
much deeper. And to some extent, still reading them pretty much open, that is, how do you actually

631
01:02:02,080 --> 01:02:06,400
go from this, this statement to something that is quantitative, right? Something that tells you,

632
01:02:06,400 --> 01:02:12,000
okay, you know, you need that many layers, you need that many parameters. And so this is where

633
01:02:12,000 --> 01:02:18,320
the role of depth in neural networks is, you know, becomes essentially the key open question.

634
01:02:18,880 --> 01:02:25,040
And, and yeah, so in this, in this quote that you, that you brought from this paper,

635
01:02:25,680 --> 01:02:31,680
that kind of reflected our understanding at the time of maybe the true power of universal

636
01:02:31,680 --> 01:02:37,680
approximation is, you know, when, as you combine it with these other prior, that is the asymmetries

637
01:02:37,680 --> 01:02:42,400
of the data, like the invariances. So I don't want to represent arbitrary functions, I only want to

638
01:02:42,400 --> 01:02:52,640
represent functions that are invariant to certain transformations of the input. So in fact, our,

639
01:02:52,640 --> 01:02:59,280
at least my particular view of the problem, analysis of the problem has somehow evolved in the last

640
01:02:59,360 --> 01:03:03,760
years, right? Of course, through research that I've done together with my collaborators.

641
01:03:03,760 --> 01:03:11,120
And now, and this is actually the way we present it in the book, we, we, we kind of identify two

642
01:03:11,120 --> 01:03:17,120
different flavors, two different sources of prior information that one needs to bake into the problem,

643
01:03:17,120 --> 01:03:22,320
right? To, to really go beyond this like basic approximation result of neural nets. The first

644
01:03:22,320 --> 01:03:28,640
one you need is invariance, right, is a disability that you need to, like the fact that you actually

645
01:03:28,640 --> 01:03:34,320
put symmetries into the architecture is certainly going to have a benefit in terms of sample

646
01:03:34,320 --> 01:03:38,320
complexity, right? They, I mean, you are going to learn more efficiently if your model is aware

647
01:03:38,320 --> 01:03:45,440
of the symmetries of the world. But in fact, this, this prior, in fact, we know now that it's not

648
01:03:45,440 --> 01:03:51,280
sufficient, right? If you only like agnosticially build your learning system, just with these

649
01:03:51,280 --> 01:03:56,000
symmetries in mind, indeed, you are going to become more efficient that a system that is

650
01:03:56,000 --> 01:04:01,760
completely agnostic to symmetries. But it might not be, you might not be able to formally establish

651
01:04:01,760 --> 01:04:07,440
what we call like a, like a learning guarantee that has good sample complexity. And I guess that I

652
01:04:07,440 --> 01:04:12,480
don't want to go too much into the jargon and the details of what this means. But the idea is that if

653
01:04:12,480 --> 01:04:17,760
I want to, you know, learn this function with certain precision, how many examples, how many

654
01:04:17,760 --> 01:04:22,240
training examples do I need to kind of give you a certificate for authentication, like a guarantee

655
01:04:22,240 --> 01:04:27,520
that I'm going to be able to do that. So with symmetries alone, it's not something that we

656
01:04:27,520 --> 01:04:32,240
know how to do. In fact, we are, we would believe we have strong beliefs that it's not possible,

657
01:04:32,240 --> 01:04:39,280
right? There's examples out there that I could, I could, you know, construct a function that has

658
01:04:39,280 --> 01:04:45,600
the right symmetries, has the right priors, if you want, but still needs a lot, a lot, a lot of

659
01:04:45,600 --> 01:04:51,120
examples to build to learn. So what we need is to add something else into the mix. And this is this

660
01:04:51,120 --> 01:04:56,400
something that we call in the, in the book, this scale separation. And, and, and if you want, I can

661
01:04:56,400 --> 01:05:03,360
try to very briefly give you an intuitive idea of what this means. So if you think about the problem

662
01:05:03,360 --> 01:05:08,560
of classifying an image, like a dog or the cat. So what is given to you is like a big

663
01:05:08,560 --> 01:05:12,720
branch of pixels, right? Every pixel has a color value. So somehow you need to figure out

664
01:05:13,680 --> 01:05:17,920
the thing that you're looking for is lying in some kind of like, it's really through the

665
01:05:17,920 --> 01:05:21,440
interactions between pixels that you get the answer, right? And the question is,

666
01:05:22,480 --> 01:05:26,960
of course, if I have a thousand pixels, how many possible interactions do I have between

667
01:05:26,960 --> 01:05:31,360
thousand elements? So this is where this exponential or the curse of dimensionality appears, right?

668
01:05:31,360 --> 01:05:36,320
I need to, a priori, I should be looking at all possible families of interactions between

669
01:05:36,320 --> 01:05:40,800
pixels. And this is where maybe where my signal would be lived. Of course, if I need to look for

670
01:05:40,800 --> 01:05:47,440
all of these things, it's impossible, right? There's just too many things. If I tell you that the,

671
01:05:47,440 --> 01:05:51,680
you know, these interactions are such that there's this translation symmetry. Well, you might not

672
01:05:51,680 --> 01:05:57,920
be, you might not be needing to look at all of them. But in fact, you don't need, you do not

673
01:05:57,920 --> 01:06:03,840
throw enough. So what is really something that is powerful is that I tell you that maybe the

674
01:06:03,840 --> 01:06:09,200
interactions that matter the most are those between a pixel and its neighbors, right? And if you

675
01:06:09,200 --> 01:06:14,400
understand very well, if you base your initial learning steps into understanding well, which

676
01:06:14,400 --> 01:06:20,560
local interaction matters, maybe you can use them to bootstrap the interactions that go to look at

677
01:06:20,560 --> 01:06:25,440
this neighborhood to slightly bigger neighborhoods, right? And so this idea that you can break a very

678
01:06:25,440 --> 01:06:31,920
complicated problem into a families of sub problems that lives in different scales. This is at the,

679
01:06:31,920 --> 01:06:37,040
I would say, at the intuitive level, something print like at the core of the essence of why

680
01:06:37,040 --> 01:06:44,640
these architectures are so efficient. This idea, as you might imagine, is not new. It's not specific

681
01:06:44,640 --> 01:06:49,600
to deep learning. The idea that you can take a complicated system of interacting particles

682
01:06:50,160 --> 01:06:55,520
and break it into different scales. This is at the, at the basis of essentially all of physics

683
01:06:55,520 --> 01:07:00,160
and chemistry, right? There's many, many, you know, like when people study, even like biology life,

684
01:07:00,160 --> 01:07:06,000
right? You have, you have experts that are very experts at the molecular level. Then you have

685
01:07:06,000 --> 01:07:09,600
experts that, you know, might understand like, you know, doctors that understand things at the

686
01:07:09,600 --> 01:07:13,680
level of okay functions. And then there's maybe experts at the level of the society, right? But

687
01:07:13,680 --> 01:07:19,280
this, you know, it's pretty natural to break the very complicated thing into different scales.

688
01:07:19,280 --> 01:07:23,840
And so deep neural networks somehow are able to do that. We don't have the full mathematical

689
01:07:23,840 --> 01:07:29,440
picture, right? Or for example, why this scale separation is strictly necessary. What we know

690
01:07:29,440 --> 01:07:34,960
from empirical evidence, like that is now I would say indisputable, is that this is an efficient way

691
01:07:34,960 --> 01:07:39,360
to do that, right? Because when I was, when I was reading the prototype book, I noticed that

692
01:07:39,360 --> 01:07:44,080
there was a separation between the symmetries and the scale separation. Could you explain in

693
01:07:44,080 --> 01:07:49,360
simple terms, why is the scale separation not just a symmetry as well? Because it seemed a little bit,

694
01:07:49,360 --> 01:07:54,720
I don't want to say kluge, but it seemed like you had this scale matter and you dealt with it

695
01:07:54,720 --> 01:08:00,880
separately. Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate

696
01:08:00,880 --> 01:08:07,360
these two would be if you think about like an algorithmic instantiation, if you want to have

697
01:08:07,360 --> 01:08:12,960
a network that would just break the problem into different scales, it would be like a neural network

698
01:08:12,960 --> 01:08:18,160
that would operate at different patches. And for every patch of the image, I could be learning

699
01:08:18,160 --> 01:08:24,080
independent set of parameters, right? So that would be a model that is only told that it should be

700
01:08:24,080 --> 01:08:28,160
breaking the problem into different regions. But it's not necessarily told that, you know,

701
01:08:28,160 --> 01:08:31,120
there's a weight sharing, right? There's some kind of like parameter sharing

702
01:08:31,120 --> 01:08:36,720
that somehow is, you are able that, you know, in a sense is helping you to learn with fewer

703
01:08:36,720 --> 01:08:42,480
number of parameters. So somehow these two, these two conditions are slightly complementary.

704
01:08:43,280 --> 01:08:48,400
We, as I said, there's still like a lot of mathematical puzzles as to how these things

705
01:08:48,400 --> 01:08:57,200
interact optimally. And I think that the, the one of the reasons why we chose to explain the story

706
01:08:57,200 --> 01:09:03,280
into two different, in these two different priors is that they all survive this quest for

707
01:09:03,280 --> 01:09:09,280
generality in the sense that these two principles are, again, something that you can think about

708
01:09:09,280 --> 01:09:18,400
for grids, for groups, for graphs, you can also see these principles appearing completely everywhere

709
01:09:18,400 --> 01:09:22,320
as you study physical systems, right? Like the scale and the symmetry

710
01:09:22,320 --> 01:09:29,040
is really at the core of, of many physical theories. And I would say that there's also

711
01:09:31,680 --> 01:09:39,520
at the more maybe technical level, these two priors somehow have been instrumental to

712
01:09:40,560 --> 01:09:45,520
organize, like to basically to have a kind of a recipe to build architectures, right? So,

713
01:09:45,520 --> 01:09:51,600
so maybe now we don't even think about it, right? But when you have a new problem, a new domain,

714
01:09:51,600 --> 01:09:57,120
and you need to build an efficient neural network, we automatically have this idea that, okay,

715
01:09:57,120 --> 01:10:01,760
we are going to start learning by composition, right? So we are going to extract information

716
01:10:01,760 --> 01:10:06,640
one layer at a time. That's the first appearance of scale. And we know that the way we need to

717
01:10:06,640 --> 01:10:11,520
organize these layers, right? How do you parameterize a layer that takes some input features and

718
01:10:11,520 --> 01:10:17,520
produces maybe better features? This idea that we do that by understanding this kind of

719
01:10:17,520 --> 01:10:23,520
equivarian structure, right? We have this notion of filters, right? In convolutional networks,

720
01:10:23,520 --> 01:10:28,160
we have this, as I said, we organize everything in terms of filters. When we talk about message

721
01:10:28,720 --> 01:10:33,680
graph neural networks, we have this kind of like diffusion filters, right? And so there's this

722
01:10:33,680 --> 01:10:37,840
object that we extract from the domain that is helping us, that giving us something very

723
01:10:37,840 --> 01:10:42,880
constructive, very, you know, very relevant, like very practical. And so this is really

724
01:10:42,960 --> 01:10:48,640
the underlying group of transformations that is acting on our domain. And so I would say that,

725
01:10:48,640 --> 01:10:54,720
you know, from a practitioner's perspective, these two principles, right, that I'm going to

726
01:10:54,720 --> 01:11:01,440
learn by composing some fundamental layers that I repeat all the time. And the way this layer

727
01:11:01,440 --> 01:11:06,960
is organized, is structured through this group transformation, this has been, I would say,

728
01:11:07,440 --> 01:11:13,360
like a trademark of, you know, the success of neural networks. Of course, as also we mentioned

729
01:11:13,360 --> 01:11:22,240
in the book, these are, I would say, proto, like meta, you know, meta parameterization, right,

730
01:11:22,240 --> 01:11:27,520
in the sense that there's, as you know, many, many, many variants that people have come up with.

731
01:11:27,520 --> 01:11:32,960
Many, many, let's say, yeah, like modifications on the basic architecture that have really

732
01:11:32,960 --> 01:11:38,560
make dramatic changes in performance, right. So there's, of course, as I say, like the devil

733
01:11:38,560 --> 01:11:44,480
sometimes is in the details, right. And so as we are writing the book, we are realizing exactly,

734
01:11:44,480 --> 01:11:49,120
you know, how some of the changes in the architectures are actually fit into this,

735
01:11:49,120 --> 01:11:51,760
what we call this blueprint, right, this symmetrically learning blueprint.

736
01:11:52,640 --> 01:11:57,200
Fascinating. Okay, so I wanted to come back to what you were saying a few minutes ago about the

737
01:11:58,080 --> 01:12:03,520
the sample efficiency of these models. Now, with graph neural networks, for example,

738
01:12:03,520 --> 01:12:08,880
there are factorially many permutations of adjacency matrices for a given graph. And

739
01:12:09,760 --> 01:12:14,800
I want to talk about a sorting algorithm, right. So François Chollet came on the podcast and he said

740
01:12:14,800 --> 01:12:19,520
that in order to learn a sorting algorithm that generalizes, you would need to learn point by

741
01:12:19,520 --> 01:12:26,240
point, you would need to see factorially many examples of permutations of numbers.

742
01:12:27,040 --> 01:12:32,320
Do you think that we could train a neural network to learn a sorting? I guess what I'm

743
01:12:32,320 --> 01:12:36,720
asking in a roundabout way is, do you think there's a kind of geometry to computation itself?

744
01:12:38,480 --> 01:12:44,560
Good. That's a very good question. And in fact, we have some, some recent work with some collaborators

745
01:12:44,560 --> 01:12:50,400
in my group, where we kind of take up this question from and we try to formalize it mathematically,

746
01:12:50,480 --> 01:12:56,880
and we give answers to this question. And so many of these like a computational tasks that

747
01:12:56,880 --> 01:13:03,280
you were mentioning, for example, sorting or, you know, like algorithmic tasks, they are,

748
01:13:03,920 --> 01:13:08,800
if one wants to put them into some mathematical context, the first thing that comes to mind is

749
01:13:08,800 --> 01:13:13,440
these are functions that already enjoy some symmetries. For example, like a sorting algorithm,

750
01:13:13,440 --> 01:13:18,400
right, is invariant to permutations, right. So the function that you are trying to learn

751
01:13:18,400 --> 01:13:24,480
is an arbitrary function that has this symmetric class. And so as such, as I was saying in the

752
01:13:24,480 --> 01:13:32,160
beginning, you can try to address this question saying, okay, now you give me an arbitrary,

753
01:13:32,160 --> 01:13:38,400
so the question would be relative to an arbitrary learning learner that is agnostic to symmetry,

754
01:13:38,400 --> 01:13:44,320
how much does a symmetric learner gain, right? So you can basically try to understand, quantify

755
01:13:44,320 --> 01:13:49,680
the gains of sample complexity of learning without symmetry versus learning with symmetry.

756
01:13:50,400 --> 01:13:57,840
And so the punchline of this work, the recent work that we completed, is that one can actually

757
01:13:57,840 --> 01:14:03,680
quantify the sample complexity gains, and these are of the order of the size of the group. And so

758
01:14:04,320 --> 01:14:09,280
here in the case of like permutations, what it means is that if a learner is aware of this

759
01:14:09,280 --> 01:14:16,000
symmetry, like one training example of the symmetric learner is rosary equivalent to

760
01:14:16,640 --> 01:14:21,360
n factorial samples of the agnostic learner, right, which is something that you would expect,

761
01:14:21,360 --> 01:14:27,840
like if you think in terms of data mutation, right, like if I tell you in advance that your

762
01:14:27,840 --> 01:14:32,720
function is symmetric, is invariant to permutations, it's, you know, like a brute force approach would

763
01:14:32,720 --> 01:14:36,640
say, okay, you give me an instance training an input, right, and instead of giving you this input,

764
01:14:36,640 --> 01:14:41,840
I'm just going to, you know, like permute, like have any possible permutation of the input, and

765
01:14:41,840 --> 01:14:46,400
you already know the output, right? So you can as well feed it to the learner. So this is like

766
01:14:46,400 --> 01:14:50,880
horrific at this addition turns out to be mathematically correct, precise, at least, you

767
01:14:50,880 --> 01:14:57,520
know, under some conditions, right? But the, I guess the whole point is that these gains might

768
01:14:57,520 --> 01:15:03,280
look amazing, like might look like a, you know, like a big boost in sample complexity. As I said

769
01:15:03,280 --> 01:15:09,120
before, there's a grain of salt here is that the, in these conditions, in general, general

770
01:15:09,120 --> 01:15:14,320
conditions, you are already fighting an essential and impossible problem in the sense that the rate,

771
01:15:14,320 --> 01:15:20,080
like the sample complexity is dominated by a rate of basically the rate in which you learn

772
01:15:20,080 --> 01:15:25,280
is what we call course by dimension. And what it means is that if I want to, you know, I have a

773
01:15:25,280 --> 01:15:30,560
certain performance generalization error, I'm going to say that now I want to ask you the question,

774
01:15:30,560 --> 01:15:36,160
if I want to divide this generalization error by two, how many more samples do I need to give you?

775
01:15:36,160 --> 01:15:42,080
Right? Like what? So if I want to double, you know, like double, like reduce the error by half,

776
01:15:43,120 --> 01:15:47,760
by how much do I need to give you more samples? So this dependency, in fact, is exponential in

777
01:15:47,760 --> 01:15:53,280
dimension, right? So basically the, the, the sample complexity gains by invariance, they are

778
01:15:53,280 --> 01:15:57,840
exponential in dimension, but they are fighting an impossible problem that is already caused by

779
01:15:57,840 --> 01:16:05,120
dimension. So what it means is that at the end of the day, this is what, you know, what was in the,

780
01:16:05,120 --> 01:16:11,280
in the, like in the heart of what I was saying before, is that invariance alone might not be

781
01:16:11,280 --> 01:16:15,920
efficient, might not be sufficient, right? Because you are, okay, you are taking a very hard problem,

782
01:16:15,920 --> 01:16:20,560
you are removing an exponential factor, but you still have many, you know, you have still

783
01:16:20,560 --> 01:16:26,320
have something in the exponent that is exponential, right? So, so, so what it means is that that,

784
01:16:26,320 --> 01:16:33,280
I mean, that's what really underpins why we think in these terms of combining symmetry prior

785
01:16:33,280 --> 01:16:38,720
with the scale separation prior. But certainly the algorithmic tasks are very interesting playground

786
01:16:38,720 --> 01:16:45,440
because I think that for the case of sorting, I mean, as you know, scale separation is also an

787
01:16:45,440 --> 01:16:49,920
issue. It's also a very important thing, right? I mean, it, this is what basically is at the heart

788
01:16:49,920 --> 01:16:54,480
of the dynamic programming approaches, right? Like these efficient algorithms that are not only

789
01:16:54,480 --> 01:16:58,640
officially statistically, but also officially computation, right? This idea that you can

790
01:16:58,640 --> 01:17:05,920
divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual

791
01:17:05,920 --> 01:17:10,960
physical prior, right? Of a scale and invariance. On the course of dimensionality, there's this

792
01:17:10,960 --> 01:17:16,400
issue where you have a data point and you want to surround it by other data points in two dimensions

793
01:17:16,400 --> 01:17:22,480
to create a convex hole. And as you increase the number of dimensions, the number of data points

794
01:17:22,480 --> 01:17:29,120
you need to create this covering to create a kind of interpolative space increases exponentially.

795
01:17:29,120 --> 01:17:33,840
And when you get past a certain number of dimensions, let's say 16 or not, not very many,

796
01:17:33,840 --> 01:17:39,200
you would need essentially more data points than there are atoms in the universe. So this leads

797
01:17:39,200 --> 01:17:44,720
to a very interesting realization. I think some people refer to it as the manifold hypothesis,

798
01:17:44,720 --> 01:17:50,880
which is that most natural data is only really spatially novel on very few dimensions. And a

799
01:17:50,880 --> 01:17:57,360
lot of data falls on very smooth low dimensional manifolds. But what are the implications of this?

800
01:17:58,400 --> 01:18:03,120
Essentially, all machine learning problems that we need to deal nowadays are extremely high

801
01:18:03,120 --> 01:18:08,560
dimensional. So even if we take very modestly sized images, they live in thousands or even in

802
01:18:08,560 --> 01:18:14,080
millions of dimensions. And if you think of machine learning or at least the simplest setting of

803
01:18:14,080 --> 01:18:19,680
machine learning as a kind of glorified function interpolation, the standard approach is to function

804
01:18:19,680 --> 01:18:26,400
interpolation as just use the data points to predict the values of your function will simply

805
01:18:26,400 --> 01:18:30,880
not work because of the phenomenon of cursive, the recursive dimensionality that increasing

806
01:18:30,880 --> 01:18:36,160
the number of dimensions, the number of such points blows up exponentially. So what you really

807
01:18:36,160 --> 01:18:42,800
need to take into account and probably this is really what makes machine learning work in practice

808
01:18:42,800 --> 01:18:46,880
is the assumption that there is some intrinsic structure to the data and it can be captured

809
01:18:46,880 --> 01:18:51,440
in different ways. So it's either the manifold assumption where you can assume that the data,

810
01:18:51,440 --> 01:18:56,000
even though it lives in a very high dimensional space intrinsically, it is low dimensional.

811
01:18:56,000 --> 01:19:01,520
This can be captured also in the form of symmetry. For example, image is not just a high dimensional

812
01:19:01,520 --> 01:19:06,080
vector. It has underlying grid structure and grid structure has symmetry. This is what captured

813
01:19:06,080 --> 01:19:10,800
in convolutional networks in the form of shared weights that translates into the convolution

814
01:19:10,800 --> 01:19:16,000
operation. I think the symmetries are part of the magic here because it's not just the interesting

815
01:19:16,000 --> 01:19:22,160
observation that natural data is only spatially novel in so few dimensions. There's something

816
01:19:22,160 --> 01:19:27,520
magic about symmetries. And when we spoke to François Chalet recently, he invoked the kaleidoscope

817
01:19:27,520 --> 01:19:34,880
effect, which is this notion that almost all information in reality is a copy of some other

818
01:19:34,880 --> 01:19:40,320
information. Probably here it will be a little bit stretching, but I would say that because

819
01:19:40,320 --> 01:19:45,840
data comes from nature, from physical processes that produce it, physics and nature itself

820
01:19:45,840 --> 01:19:52,560
is in a sense low dimensional. So it's application of simple rules at multiple scales. You can create

821
01:19:52,560 --> 01:19:57,760
very complex systems with very simple rules. And this is probably how our data that we are mostly

822
01:19:57,760 --> 01:20:02,880
interested in in machine learning is structured. So you have this manifestation of symmetry and

823
01:20:02,880 --> 01:20:07,200
self-similarity through different scales, the principles of symmetry and certain

824
01:20:07,280 --> 01:20:12,320
environs or equivalents and of scale separation, where you can separate your problem to multiple

825
01:20:12,320 --> 01:20:17,520
scales and deal with it at different levels of resolution. And this is captured, for example,

826
01:20:17,520 --> 01:20:22,480
in pooling operations in convolutional neural networks and in other deep learning architectures.

827
01:20:23,040 --> 01:20:28,800
This is what makes deep learning systems work. Fascinating. It's so good that you raised the

828
01:20:28,800 --> 01:20:32,400
curse of dimensionality because I was going to ask you about that. Could you explain in really

829
01:20:32,400 --> 01:20:38,640
simple terms, so not invoking lipships, I can't even say it properly now, but not invoking a

830
01:20:38,640 --> 01:20:46,160
mathematical jargon. And why exactly in your articulation does geometric deep learning

831
01:20:46,800 --> 01:20:49,440
reduce the impact of the curse of dimensionality?

832
01:20:50,640 --> 01:21:02,000
Yeah. So the curse of dimensionality, it refers generally to the inability of algorithms to

833
01:21:02,000 --> 01:21:07,040
keep certifying certain performance as the data becomes more complex. And data becoming more

834
01:21:07,040 --> 01:21:13,200
complex here means that you have more and more dimensions, more and more pixels. And so this

835
01:21:13,200 --> 01:21:21,840
inability of scaling, basically it really says that if I scale up the input, my algorithm is

836
01:21:21,840 --> 01:21:27,600
going to have more and more trouble to keep the base. And so this curse can take different

837
01:21:27,600 --> 01:21:36,160
flavors. So this curse might have a statistical reason in the sense that as I make my input space

838
01:21:36,160 --> 01:21:43,440
bigger, there would be many, many, many much exponentially more functions, real functions

839
01:21:43,440 --> 01:21:47,760
out there that would explain the training set that would basically pass through the training points.

840
01:21:48,800 --> 01:21:54,720
And so the more dimensions I add, the more uncertainty I have about the true function.

841
01:21:54,800 --> 01:22:00,080
So I would need more and more training samples to keep the base. This curse can also be from the

842
01:22:00,080 --> 01:22:05,440
approximation side. So in the sense that the number of neurons that I'm considering to approximate

843
01:22:05,440 --> 01:22:10,800
my target function, I need to keep adding more and more neurons at the rate that is exponential

844
01:22:10,800 --> 01:22:17,600
in dimension. And the curse can also be from the computational side. The sense that if I keep

845
01:22:17,600 --> 01:22:23,920
adding parameters and parameters to my training model, I might have to optimize to solve an

846
01:22:23,920 --> 01:22:29,600
optimization problem that becomes exponentially harder. And so you can see that you are basically

847
01:22:29,600 --> 01:22:38,000
bombarded by all angles. And so an algorithm like here in the context of statistical learning or

848
01:22:38,000 --> 01:22:43,760
learning theory, if you want, having a kind of a theorem that would say, yes, I can promise you

849
01:22:43,760 --> 01:22:48,240
that you can learn, you need to actually solve these three problems at once. You need to be able

850
01:22:48,240 --> 01:22:53,200
to say that in the conditions that you're studying, you have an algorithm that it does not suffer from

851
01:22:53,200 --> 01:22:59,120
approximation nor statistical nor computational curses. So as you can imagine, it's very hard

852
01:22:59,120 --> 01:23:04,160
because you need to master many things at the same time. So why do we think that geometric

853
01:23:04,160 --> 01:23:11,120
deep learning is at least an important piece to overcome this curse? As I said before, so

854
01:23:11,120 --> 01:23:16,560
geometric deep learning is really a device to put more structure into the target function.

855
01:23:16,560 --> 01:23:22,880
So basically to make the learning problem easier because we are promising the learner

856
01:23:22,880 --> 01:23:27,680
more properties of a target function. We are basically making the hypothesis class if you want

857
01:23:27,680 --> 01:23:36,240
smaller. That said, as I said, there's still some path to go. We're describing just a bunch of

858
01:23:36,240 --> 01:23:41,680
principles that make these hypothesis spaces smaller and more adapted to the real world.

859
01:23:42,320 --> 01:23:47,360
But one thing that we are still lacking, for example, is the guarantee in terms of optimization.

860
01:23:47,600 --> 01:23:52,480
I mean, I described that the depth of these architectures is somehow something that is

861
01:23:52,480 --> 01:23:56,640
akin associated with the scale, the fact that you need to understand things at different scales.

862
01:23:57,280 --> 01:24:03,600
So as you know, from the optimization side, there's still some open questions and open mysteries

863
01:24:03,600 --> 01:24:08,480
as to why the gradient descent, for example, is able to find good solutions. So these are

864
01:24:08,480 --> 01:24:14,560
things that we believe that these architectures can be optimized efficiently just because we have

865
01:24:14,560 --> 01:24:19,040
these experimental evidence that is piling up. But we are, for example, we are still lacking

866
01:24:19,760 --> 01:24:26,160
theoretical guarantees. For the approximation, it's a bit of the same story. So we understand

867
01:24:26,160 --> 01:24:31,600
very well approximation properties of shallow networks, starting from universal approximation,

868
01:24:31,600 --> 01:24:37,120
but of course, many, many recent interesting work. But we are also still lagging a little bit behind

869
01:24:37,760 --> 01:24:43,680
in approximation properties for deeper networks. So as you see, it's like you can see from this

870
01:24:43,760 --> 01:24:50,240
discussion that, yes, we have some good reasons to believe that these are fundamental principles

871
01:24:50,240 --> 01:24:57,440
of learning. But there's also a bunch of mathematical questions that are still open. And this is also

872
01:24:57,440 --> 01:25:01,760
one of the things that I like about writing a book on this topic, because it's a very life domain.

873
01:25:03,760 --> 01:25:09,520
As you can see, the field is still evolving. And I think it's a good time to... Yeah, it's a good

874
01:25:09,520 --> 01:25:14,320
time. I mean, researchers out there are listening to us. It's a good time to think and to work and

875
01:25:14,320 --> 01:25:20,480
to join this program. Amazing. Will we ever understand the approximation properties of deep

876
01:25:20,480 --> 01:25:26,640
networks? Because with the shallow function approximation algorithm, you can almost think

877
01:25:26,640 --> 01:25:31,360
of a neural network as being kind of like sparse coding. And the more neurons you have, you're

878
01:25:31,360 --> 01:25:36,800
just kind of discreetly fitting this arbitrary function. But you don't have that visual intuition

879
01:25:36,800 --> 01:25:43,840
quite so much with the deep networks. Yeah, I mean, it's an important... And it's a deep question.

880
01:25:43,840 --> 01:25:51,680
So indeed, shallow neural networks are really, really correspond to this idea that you learn

881
01:25:51,680 --> 01:25:57,920
a function by stacking a linear combination of basis elements. And this is really at the roots

882
01:25:57,920 --> 01:26:02,400
of essentially all of harmonic analysis or functionalized. I typically think about the

883
01:26:02,400 --> 01:26:06,480
basis and you ask questions about the linear approximation or the approximation, etc.

884
01:26:07,200 --> 01:26:11,120
Deep neural networks, they introduce a fundamentally different way to approximate

885
01:26:11,120 --> 01:26:17,920
functions that is by composing, by composition. And so you're right. Our knowledge about this

886
01:26:17,920 --> 01:26:24,720
question right now is mostly concentrated in what we call separation results, like a depth

887
01:26:24,720 --> 01:26:30,960
separation, which consists in trying to find construct mathematical examples of functions

888
01:26:30,960 --> 01:26:35,760
that cannot be approximated with shallow neural networks with certain number of neurons.

889
01:26:35,760 --> 01:26:40,240
But indeed, they can be much better approximated with deep neural networks. So this is really

890
01:26:40,240 --> 01:26:46,080
understanding which kinds of functions benefit fundamentally from composition rather than from

891
01:26:46,080 --> 01:26:54,320
addition. And so there's a certain mathematical vision and mathematical intuition that is

892
01:26:54,320 --> 01:27:00,240
building up. But of course, it's still very far from explaining the true power of depth. And

893
01:27:00,240 --> 01:27:07,760
just to give you like a final pointer here, there's a very related question that replaces

894
01:27:07,760 --> 01:27:12,080
neural networks as we understand them with what we call Boolean functions, right? Like these are

895
01:27:12,080 --> 01:27:18,160
just circuits, arithmetic circuits that take as input some bit string and they can manipulate the

896
01:27:18,160 --> 01:27:25,840
bits by, you know, or operations and operations and they can keep adding gates. And so this question

897
01:27:25,840 --> 01:27:30,480
about what is the ability of a certain circuit architecture to approximate certain Boolean

898
01:27:30,480 --> 01:27:37,120
functions is actually a notoriously hard and basically has been studied in the theoretical

899
01:27:37,120 --> 01:27:42,240
computer science community since the 50s and the 60s, right? And there's actually very, very,

900
01:27:42,240 --> 01:27:47,920
very deep results and very challenging actually open questions concerning these things. So this

901
01:27:47,920 --> 01:27:54,080
is really, we are really touching here questions that are pretty serious at like the deep mathematical

902
01:27:54,080 --> 01:28:00,000
and theoretical level. And so, yes, you should not expect that in the year in the next year or two,

903
01:28:00,000 --> 01:28:05,120
we have a complete understanding of, you know, approximation powers of any architecture with

904
01:28:05,120 --> 01:28:14,080
any depth. But I think you should expect that the theory like this will continue to try to catch up

905
01:28:14,080 --> 01:28:22,480
with the experiments. And so we are, I think we are hoping to get like a more precise mathematical

906
01:28:22,480 --> 01:28:30,560
understanding of the role of depth. And as I said before, there's one thing that is fascinating

907
01:28:30,560 --> 01:28:36,800
about this domain that is maybe very unique to deep learning is this very strong interaction

908
01:28:36,800 --> 01:28:43,120
between optimization, statistics and approximation, right? Maybe it turns out that, you know, the

909
01:28:43,120 --> 01:28:48,160
huge depth that we have in these residual neural networks might not be necessary from the

910
01:28:48,160 --> 01:28:53,840
approximation side, but in fact, it's so useful for the optimization that overall is a big winner,

911
01:28:53,840 --> 01:29:00,000
right? So there's always these like twists about this question that are fundamentally mixing these

912
01:29:00,000 --> 01:29:05,520
three sources of error. I'm sorry for asking you this question, but can neural networks extrapolate

913
01:29:05,520 --> 01:29:16,240
outside of the training data? That's a good question. I would say that the answer, I guess,

914
01:29:16,320 --> 01:29:23,360
depends on your specifications, right? So I guess that the conservative answer

915
01:29:24,240 --> 01:29:30,480
of a statistical learning person would be no, because we don't have good theorems right now

916
01:29:30,480 --> 01:29:38,240
that tell us that this is the case. There's very like, you know, like a strong effort both from

917
01:29:38,240 --> 01:29:43,440
the practical and the theoretical community to really understand this question, like by trying

918
01:29:43,440 --> 01:29:48,560
to formalize it a bit more. I mean, what do we mean by distribution shift? You know, what kind of

919
01:29:48,560 --> 01:29:53,360
training procedures you can come up with that would give you precisely this kind of robustness?

920
01:29:54,160 --> 01:29:59,760
There's of course, what we call these biases, right? I mean, I can always take it like a

921
01:29:59,760 --> 01:30:04,320
training distribution, I make a choice of a certain architecture, I'm going to learn a function,

922
01:30:04,320 --> 01:30:08,640
and obviously, there's some directions if you want in the space of distributions,

923
01:30:08,640 --> 01:30:12,320
for which my hypothesis will turn out to be have good generalization, there might be

924
01:30:12,320 --> 01:30:16,560
other direction in which the contrary is true. So there's actually very nice work

925
01:30:17,520 --> 01:30:22,960
from Stephanie Gejalka's group at MIT, where they, for example, they studied this question

926
01:30:22,960 --> 01:30:28,320
in the context of value networks, also including graphs, where they, for example, they discover

927
01:30:28,320 --> 01:30:34,000
or they identify this strong preference for value networks to generalize along linear directions,

928
01:30:34,000 --> 01:30:39,360
right? In the sense that if I just decide to now, you know, shift my data in linear directions,

929
01:30:39,360 --> 01:30:44,080
then my function has no trouble generalizing. Maybe there's other directions, right, in which

930
01:30:44,080 --> 01:30:48,800
this thing is actually catastrophic. So I think that, yeah, the question, I think it's very important

931
01:30:49,360 --> 01:30:56,640
from, let's say, it's very important from a kind of a practitioner perspective, right? That's typically,

932
01:30:56,640 --> 01:31:01,760
that's clearly something that a user would like to know. But I think that from a more mathematical

933
01:31:01,760 --> 01:31:05,760
or theoretical level, I think we are still at the stage of trying to formalize like, okay,

934
01:31:05,760 --> 01:31:11,360
what do we exactly mean by extrapolation? And what are the kind of the conditions for

935
01:31:11,360 --> 01:31:16,080
which architecture can do it? And I think that, yeah, this is a, yeah, it's an important question.

936
01:31:16,080 --> 01:31:20,320
But yeah, I think we are still pretty far from having a full answer.

937
01:31:21,360 --> 01:31:25,440
Amazing. And final question, what areas of mathematics do people need to study

938
01:31:25,440 --> 01:31:33,520
before reading the proto book? Good question. So I think that our objective and our really

939
01:31:33,520 --> 01:31:39,360
like the idea is really to have something that is quite self-contained in the sense that we are

940
01:31:39,360 --> 01:31:45,200
going to provide appendices that expand on the areas that is maybe they're not typically

941
01:31:46,000 --> 01:31:50,640
kind of the bread and butter of machine learning people. For example, we are going to have an

942
01:31:50,640 --> 01:31:57,840
appendix on group theory, differential geometry, harmonic analysis. So these are areas that I

943
01:31:57,840 --> 01:32:03,200
think are going to be there for you to delve into, to get like the most of the paper,

944
01:32:03,200 --> 01:32:08,400
the most of the book. But I think that other than that, any basic, you know, any basic

945
01:32:08,400 --> 01:32:14,000
knowledge of linear algebra, statistics and analysis will do. So like, if you have taken

946
01:32:14,000 --> 01:32:19,680
a graduate level class in machine learning, you should be ready to go. Rather than just being

947
01:32:19,680 --> 01:32:26,080
applied in a lot of really important branches of research problems, people outside of pure

948
01:32:26,080 --> 01:32:32,160
research have recognized that a lot of the data that comes to us from nature is most naturally

949
01:32:32,160 --> 01:32:36,640
represented in a graph-structured form. Very rarely will nature give us something

950
01:32:36,640 --> 01:32:42,080
that can be representable as an image or a sequence. That's super rare. So very often,

951
01:32:42,080 --> 01:32:47,200
the structure is more irregular, more graph-like. And therefore, graph neural networks have already

952
01:32:47,200 --> 01:32:53,360
seen a lot of applications in domains where the data is supernaturally represented as a graph.

953
01:32:53,360 --> 01:32:57,920
In the domain of computational chemistry, where you can represent molecules as graphs of atoms

954
01:32:57,920 --> 01:33:03,040
and bonds between them, the graph neural networks have already proven impactful in detecting

955
01:33:04,160 --> 01:33:08,560
novel potent antibiotics that previously were completely overlooked because of their unusual

956
01:33:08,560 --> 01:33:14,320
structure. In the area of chip design, graph neural networks are powering systems that are

957
01:33:14,320 --> 01:33:20,480
developing the latest generation of Google's machine learning chips, the TPU. Furthermore,

958
01:33:20,480 --> 01:33:27,520
graph-structured data is super ubiquitous in social networks and the kinds of networks maintained by

959
01:33:28,480 --> 01:33:34,960
many big industry players. And accordingly, graph neural networks are already used to serve various

960
01:33:34,960 --> 01:33:41,360
kinds of content in production to billions of users on a daily basis. In fact, the recommendation

961
01:33:41,360 --> 01:33:47,600
system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation

962
01:33:47,600 --> 01:33:53,920
system for Uber Eats, all of them are powered using a graph neural network that helps serve the most

963
01:33:53,920 --> 01:34:00,000
relevant content to users on a daily basis. And on a slightly personal note, graph neural networks

964
01:34:00,000 --> 01:34:06,720
have also been used to significantly improve travel time predictions in Google Maps, which

965
01:34:06,720 --> 01:34:12,240
is used also by billions of people every day. So whenever you type a query, how do I get from

966
01:34:12,240 --> 01:34:18,160
point A to point B in the most efficient way? The travel time prediction that you get is powered

967
01:34:18,160 --> 01:34:22,800
by a graph neural network that we have developed that defined in collaboration with the Google Maps

968
01:34:22,800 --> 01:34:29,200
team. And this is of high importance not only to users that use the app on a daily basis to find

969
01:34:29,200 --> 01:34:34,640
the most efficient way to travel. It's also used by the various companies that leverage the Maps

970
01:34:34,640 --> 01:34:41,360
API so they can tell their customers what's the time it will take for a certain vehicle to arrive

971
01:34:41,360 --> 01:34:46,240
to them. So companies such as food delivery companies and ride-sharing companies have also

972
01:34:46,320 --> 01:34:52,880
extensively profited from this system, which in cities such as Sydney has reduced the relative

973
01:34:52,880 --> 01:34:57,920
amount of negative user outcomes in terms of badly predicted travel times by over 40%,

974
01:34:58,800 --> 01:35:03,920
making it one way in which graph representation learning techniques that I have co-developed

975
01:35:03,920 --> 01:35:11,520
are actively impacting billions of people on a daily basis. When I was an undergraduate student,

976
01:35:11,520 --> 01:35:16,800
I was interested in image processing and was excited about variational methods.

977
01:35:17,920 --> 01:35:23,120
I think it's a very elegant idea that you can define a functional that serves as a model for

978
01:35:23,120 --> 01:35:28,960
your ideal image and then use the optimality conditions to derive a differential equation that

979
01:35:28,960 --> 01:35:34,160
flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel,

980
01:35:34,720 --> 01:35:38,960
where you could think of an image as a manifold or a high-dimensional surface

981
01:35:38,960 --> 01:35:45,200
and use an energy that originated in string theory and particle physics to derive a non-euclidean

982
01:35:45,200 --> 01:35:51,120
diffusion PDE called Beltrami Flow that acts as a non-linear image filter. And this is what

983
01:35:51,120 --> 01:35:56,880
made me fall in love with differential geometry and I did a PhD with Ron on this topic. And I think

984
01:35:56,880 --> 01:36:02,080
these were really beautiful and deep ideas that unfortunately now are almost forgotten in the

985
01:36:02,080 --> 01:36:06,960
era of deep learning. And it's a pity that the machine learning research community has such a

986
01:36:06,960 --> 01:36:13,200
short memory because many modern concepts have really ancient roots. Ironically, we've recently

987
01:36:13,200 --> 01:36:18,880
used non-euclidean diffusion equations as a way to reinterpret graph neural networks as neural

988
01:36:18,880 --> 01:36:24,880
PDEs. And I think it really helps sometimes to have a longer time window. Now, equivariated

989
01:36:24,880 --> 01:36:31,440
networks tend to generalize much better and require much less data if the data indeed has

990
01:36:31,440 --> 01:36:37,200
the symmetry that you assumed in your model. But people often ask, you know, why do we even

991
01:36:37,200 --> 01:36:42,320
care about data efficiency when we can just collect more data? We live in the era of big data, right?

992
01:36:43,200 --> 01:36:47,680
And I think the answer why you might still be interested in data efficiency. First of all,

993
01:36:47,680 --> 01:36:53,920
there are applications like say medical imaging, where acquiring labeled data simply is very cost.

994
01:36:53,920 --> 01:36:58,640
You have to get patients, you're dealing with privacy restrictions, you're dealing with

995
01:36:58,640 --> 01:37:03,600
costly, highly trained doctors who have to annotate the data, come together in a committee to

996
01:37:03,600 --> 01:37:09,520
decide on questionable cases and so forth. So this is very expensive. And if you can improve the

997
01:37:09,520 --> 01:37:15,440
data efficiency by a factor of two or 10, or whatever it may be, you might just take a problem

998
01:37:15,440 --> 01:37:20,720
that was in the realm of economically infeasible and take it into the realm of the economically

999
01:37:20,720 --> 01:37:27,520
feasible, which is a very useful thing. There are other cases like graph neural nets, where the

1000
01:37:27,600 --> 01:37:33,360
group of symmetries is so large, in this case, n factorial number of permutations,

1001
01:37:33,360 --> 01:37:40,000
that no amount of data or data augmentation in practice is going to allow you to learn the

1002
01:37:40,000 --> 01:37:45,440
symmetry or to learn the invariance or equivariate in your NAPO. So indeed, you see that in this

1003
01:37:45,440 --> 01:37:50,880
space of graph neural nets, everybody uses equivariate permutation, equivariate network

1004
01:37:50,880 --> 01:37:56,160
architectures. And then finally, you can think about the grand problems of AGI, artificial

1005
01:37:56,160 --> 01:38:03,280
general intelligence and so forth. And here I think that we will most certainly need large data sets,

1006
01:38:05,040 --> 01:38:10,800
large networks, a lot of compute power, and so forth. The current architectures we have clearly

1007
01:38:10,800 --> 01:38:18,400
are performing far fewer computations than the human brain. So there's a ways to go there.

1008
01:38:18,400 --> 01:38:25,520
But I also think that one essential characteristic of intelligence is the ability to learn quickly

1009
01:38:25,600 --> 01:38:32,640
in new situations, situations that are not similar to the ones you've seen in your training data.

1010
01:38:33,520 --> 01:38:38,880
And so data efficiency to me is an essential characteristic of intelligence. It's almost

1011
01:38:38,880 --> 01:38:44,880
like an action that you want your methods to be data efficient. And so I think one of the

1012
01:38:44,880 --> 01:38:51,840
big challenges for the field right now is to try to think of very generic priors, priors that

1013
01:38:51,840 --> 01:38:58,800
apply in a wide range of situations. And to give you, even though they're generic and abstract,

1014
01:38:58,800 --> 01:39:04,080
they give you a lot of bang for the buck in terms of improved generalization and data efficiency.

1015
01:39:04,080 --> 01:39:10,080
I think that the beauty of science and research is in connecting the dots. And I find it fascinating

1016
01:39:10,080 --> 01:39:14,480
that, for example, graph neural networks are connected to the work of Weissfeller and Lehmann

1017
01:39:14,480 --> 01:39:20,160
from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry.

1018
01:39:20,960 --> 01:39:26,160
And chemistry was also the field that drove the research into modern formulation of

1019
01:39:26,160 --> 01:39:31,760
diffusion equations that were adopted in image processing community in the 90s and came back

1020
01:39:31,760 --> 01:39:37,680
recently as a way to reinterpret graph neural networks. And I think such connections give really

1021
01:39:37,680 --> 01:39:43,440
a new and deep perspective. And probably the deeper you dive, the broader they become. But

1022
01:39:43,440 --> 01:39:49,360
it's really an ever-ending story. Today is an incredibly special episode and we're filming

1023
01:39:49,360 --> 01:39:55,440
at 9 o'clock in the morning. It's really rare for us to film this early when I'm still caffeinated.

1024
01:39:55,440 --> 01:39:59,840
Many of our guests are over in the States. It's an absolute honor to have you both on MLST.

1025
01:39:59,840 --> 01:40:04,240
And Professor Bronstein, could you start by briefly telling us how the young mathematician

1026
01:40:04,240 --> 01:40:08,320
Enne Offer used symmetries to discover the conservation laws in physics?

1027
01:40:09,040 --> 01:40:13,280
Maybe I should take a step back and describe the situation that happened

1028
01:40:14,000 --> 01:40:20,480
in the field of geometry towards the end of the 19th century. And it was an incredibly fruitful

1029
01:40:21,200 --> 01:40:28,960
period of time for mathematicians working in this field with the discovery and development of

1030
01:40:28,960 --> 01:40:35,600
different kinds of geometries. So a young mathematician based in Germany called Felix Klein

1031
01:40:36,320 --> 01:40:42,960
proposed this quite remarkable and groundbreaking idea that you can define geometry by studying

1032
01:40:42,960 --> 01:40:47,840
the groups of symmetries, basically the kinds of transformations to which you can subject

1033
01:40:47,840 --> 01:40:54,960
geometric forms and seeing how different properties are preserved or not under these

1034
01:40:54,960 --> 01:41:00,080
transformations. So these ideas appear to be very powerful. And what Amy Neuter showed in her

1035
01:41:00,080 --> 01:41:05,120
work, and she actually worked in the same institution where Klein ended up in Göttingen in

1036
01:41:05,120 --> 01:41:10,640
Germany. And she showed that you can take a physical system that is described as

1037
01:41:11,600 --> 01:41:16,320
functional as a variational system and associate different conservation laws

1038
01:41:16,320 --> 01:41:23,360
with different symmetries of this system. And it was a pretty remarkable result because

1039
01:41:23,360 --> 01:41:29,040
before that, conservation laws were purely empirical. You would make an experiment many times

1040
01:41:29,040 --> 01:41:34,720
and measure, for example, the energy before or after some physical process or chemical reaction,

1041
01:41:34,720 --> 01:41:39,040
and you would come to the conclusion that the energy is preserved. So this is how, for example,

1042
01:41:39,040 --> 01:41:44,720
I think Lavoisier has discovered the conservation of energy. So it was probably for the first time

1043
01:41:45,280 --> 01:41:50,800
that you could derive these laws from first principles. So you would need to assume in case

1044
01:41:50,800 --> 01:41:56,160
of conservation of energy, the symmetry of time. We decided to take a tour into the world of

1045
01:41:56,160 --> 01:42:01,920
algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks

1046
01:42:01,920 --> 01:42:06,720
to find neural networks that are good at imitating the classical algorithms that initially brought

1047
01:42:06,720 --> 01:42:11,760
you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning,

1048
01:42:11,760 --> 01:42:16,400
and it's often claimed that neural networks are turing complete. And, you know, we're told that we

1049
01:42:16,400 --> 01:42:21,120
can think of training neural networks as being a kind of program search. But you argued in your

1050
01:42:21,120 --> 01:42:26,160
paper that algorithms possess fundamentally different qualities to deep learning methods.

1051
01:42:26,160 --> 01:42:30,080
Francois Chollet actually often points out that deep learning algorithms would struggle

1052
01:42:30,080 --> 01:42:35,680
to represent a sorting algorithm without learning point by point, you know, which is to say without

1053
01:42:35,680 --> 01:42:40,000
any generalization power whatsoever. But you seem to be making the argument that the interpolative

1054
01:42:40,000 --> 01:42:44,880
function space of neural networks can model algorithms more closely to real world problems,

1055
01:42:44,880 --> 01:42:49,280
potentially finding more efficient and pragmatic solutions than those classically proposed by

1056
01:42:49,280 --> 01:42:56,080
computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept

1057
01:42:56,080 --> 01:43:02,720
of classical algorithms, as opposed to deep neural networks, at least the way we're currently

1058
01:43:02,720 --> 01:43:08,560
applying them makes all these points about turing completeness a little bit moot, because there's

1059
01:43:08,560 --> 01:43:14,320
quite a few proofs out there saying that you can use neural networks or more recently graph neural

1060
01:43:14,320 --> 01:43:19,840
networks in particular to simulate a particular algorithm perfectly. But all of these proofs are

1061
01:43:19,840 --> 01:43:25,120
sort of a best case scenario. They're basically saying, I can set the weights of my neural network

1062
01:43:25,120 --> 01:43:30,400
to these particular values, and voila, I am imitating the algorithm perfectly, right? So all

1063
01:43:30,400 --> 01:43:34,960
these best case scenarios are wonderful. But in practice, we don't use this kind of best case

1064
01:43:34,960 --> 01:43:40,160
optimization, we use stochastic gradient descent. And we're stuck with whatever stochastic gradient

1065
01:43:40,160 --> 01:43:45,520
descent gives us. So in practice, the fact that a neural network is capable for a particular setting

1066
01:43:45,520 --> 01:43:50,560
of weights to do something doesn't mean that it will actually do that when trained from data.

1067
01:43:51,360 --> 01:43:57,200
So essentially, this is the kind of the big divide that separates deep learning from traditional

1068
01:43:57,200 --> 01:44:01,760
algorithms. And it has a number of other issues as well, not just the fact that we cannot find the

1069
01:44:01,760 --> 01:44:08,000
best case solution. Also, the fact that we are working in this high dimensional space, which is

1070
01:44:08,000 --> 01:44:13,840
not necessarily easily interpretable or composable, because you have no easy way of saying, for example,

1071
01:44:13,840 --> 01:44:18,160
in theoretical computer science, if you want to compose two algorithms, you're working with them

1072
01:44:18,160 --> 01:44:22,640
in a very abstract space, which means that, you know, you can easily reason about stitching the

1073
01:44:22,640 --> 01:44:28,720
output of one to the input of another. Whereas you cannot make that easy of a claim about latent

1074
01:44:28,720 --> 01:44:33,440
spaces of two neural networks, right? So all these kinds of properties, interpretability,

1075
01:44:33,440 --> 01:44:39,360
compositionality, and obviously also out of distribution generalization are plagued not

1076
01:44:39,360 --> 01:44:44,640
by the fact that neural networks don't have the capacity to do this. But the routines we use to

1077
01:44:44,640 --> 01:44:50,960
optimize them are not good enough to to across that divide. So in neural algorithmic reasoning,

1078
01:44:50,960 --> 01:44:55,600
all that we're really trying to do is to bring these two sides closer together by making changes

1079
01:44:55,600 --> 01:44:59,680
either in the structure of the neural network or the training regime of the neural network or the

1080
01:44:59,680 --> 01:45:05,920
kinds of data that we'll let the neural network see so that hopefully it's going to generalize better

1081
01:45:05,920 --> 01:45:11,520
and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems

1082
01:45:11,520 --> 01:45:16,320
that we might see in a computer science textbook. And lastly, I think I'd just like to address the

1083
01:45:16,320 --> 01:45:25,280
point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit

1084
01:45:25,280 --> 01:45:29,760
that in Europe's data set track. I think it should be public even now on GitHub, because that's the

1085
01:45:29,760 --> 01:45:34,720
requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to

1086
01:45:34,720 --> 01:45:42,240
force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution,

1087
01:45:42,240 --> 01:45:47,760
these graph neural networks are capable of imitating the steps of, say, insertion sort. So

1088
01:45:47,760 --> 01:45:51,760
I will say not all is lost if you're very careful about how you tune them. But obviously,

1089
01:45:51,760 --> 01:45:56,640
there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk

1090
01:45:56,640 --> 01:46:01,600
a little bit about how even though we cannot perfectly mimic algorithms, we can still use

1091
01:46:01,600 --> 01:46:07,840
this concept of algorithmic execution today now to help expand the space of applicability of algorithm.

1092
01:46:08,640 --> 01:46:13,200
Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people

1093
01:46:13,200 --> 01:46:18,640
point out as being the limitations of deep learning, right? Sholay spoke about this, but

1094
01:46:18,640 --> 01:46:23,280
I don't think that geometric deep learning would help a neural network learner sorting function,

1095
01:46:23,280 --> 01:46:29,520
because discrete problems in general don't seem amenable to vector spaces, either because the

1096
01:46:29,520 --> 01:46:34,320
representation would be glitchy, or the problem is not interpolative in nature or not learnable

1097
01:46:34,320 --> 01:46:40,240
with stochastic gradient descent. So it would be fascinating if we could overcome these problems

1098
01:46:40,240 --> 01:46:44,400
using continuous neural networks as an algorithmic substrate. Do you think we could?

1099
01:46:46,960 --> 01:46:53,360
I think that it is possible, but it will require us potentially to broaden our lens on what we

1100
01:46:53,360 --> 01:46:57,440
mean by geometric deep learning. And this is something we're already very actively thinking

1101
01:46:57,440 --> 01:47:01,840
about. I think one of our co-authors, Taco Co, and actually thought much more deeply about this

1102
01:47:02,560 --> 01:47:08,400
in recent times. But basically, the idea is we looked at geometric deep learning from a group

1103
01:47:08,400 --> 01:47:13,760
symmetry point of view, which is a very nice way to describe spatial regularities and spatial

1104
01:47:13,760 --> 01:47:19,600
symmetries. But it's not necessarily the best way to talk about, say, invariance of generic

1105
01:47:19,600 --> 01:47:24,480
computation, which you would find in algorithms, right? It's like, I have input that satisfies

1106
01:47:24,480 --> 01:47:28,720
certain preconditions. I want to say something about once I push it through this function,

1107
01:47:28,720 --> 01:47:33,760
it should satisfy certain post conditions. This is not the kind of thing we can very easily express

1108
01:47:33,760 --> 01:47:38,800
using the language of group theory. However, it is something that perhaps we could express more

1109
01:47:38,800 --> 01:47:43,600
nicely using the language of category theory, which is an area of math that I still don't know

1110
01:47:43,600 --> 01:47:48,320
enough about. I'm currently actively learning it. But basically, in the language of category

1111
01:47:48,320 --> 01:47:54,160
theory, groups are super simple categories that have just one node, right? You can do a lot more

1112
01:47:54,160 --> 01:48:00,320
complicated things if you use this more broader abstract language. And, you know, you talk about

1113
01:48:00,320 --> 01:48:04,960
basically anything of interest there in terms of these commutative diagrams. And Taco actually

1114
01:48:04,960 --> 01:48:09,520
recently had a really interesting paper called natural graph networks, where they basically

1115
01:48:09,520 --> 01:48:14,880
generalize the notion of permutation, equivariance that you might find in graph nets to this more

1116
01:48:14,880 --> 01:48:20,400
general concept of natural transformations. So now suddenly, you don't have to have a network that

1117
01:48:20,400 --> 01:48:25,360
does exactly the same transformation in every single part of the graph. What you actually need

1118
01:48:25,360 --> 01:48:30,800
is something a bit more fine grained. You just need for all like locally isomorphic parts of the

1119
01:48:30,800 --> 01:48:35,360
graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think

1120
01:48:35,360 --> 01:48:40,320
that this kind of language, like moving a bit away from the group formalism would allow us to

1121
01:48:40,320 --> 01:48:45,440
talk about say algorithmic invariance and things like this. I don't yet have any theory to properly

1122
01:48:45,440 --> 01:48:50,640
prove this, but it's something that I'm actively working on. And I guess I would say, you know,

1123
01:48:50,640 --> 01:48:54,720
the only question is, would you still call this geometric deep learning? And in my opinion,

1124
01:48:54,720 --> 01:48:59,680
the very creators of category theory have said that category theory is a direct extension of

1125
01:48:59,680 --> 01:49:04,640
Felix Klein's Erlangin program. So since the founders of the field have already made this

1126
01:49:04,640 --> 01:49:10,000
connection, I would expect that, you know, it would be pretty applicable under a geometric lens.

1127
01:49:10,960 --> 01:49:17,680
So it seems to come back to it seems to come back from both of your sides to essentially graphs

1128
01:49:17,680 --> 01:49:26,320
and working on on sort of graphs to capture on the one side, the sort of symmetries that are

1129
01:49:26,320 --> 01:49:31,920
that you either assume in the problem or that you know, or that you want to impose on the other

1130
01:49:31,920 --> 01:49:38,160
side on the other side. Now, these computations can may be well represented in in graphs.

1131
01:49:39,120 --> 01:49:45,440
What's what's so special about graphs in your estimations? Is there something

1132
01:49:46,320 --> 01:49:52,080
fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I

1133
01:49:52,080 --> 01:49:56,880
asked him the same question, what's so special about graphs and his argument was essentially,

1134
01:49:56,880 --> 01:50:00,800
well, it's not about graphs, it's simply a good representation of the problem

1135
01:50:01,440 --> 01:50:07,120
that we can make efficient operations on. Do you have a different opinion on that? Is a graph

1136
01:50:07,120 --> 01:50:13,680
something fundamental that we should look more at than, for example, a tensor?

1137
01:50:14,480 --> 01:50:19,920
Graphs are abstract models for systems of relations or interactions. I should maybe specify

1138
01:50:20,480 --> 01:50:27,520
pairwise relations and interactions. And it happens that a lot of physical or biological

1139
01:50:28,480 --> 01:50:33,840
even social systems can be described at least at some level of abstraction as a graph. So that's

1140
01:50:33,840 --> 01:50:40,880
why it is a so popular modeling tool in many fields. You can also obtain other structures such as

1141
01:50:40,880 --> 01:50:46,960
grids as particular cases. I wouldn't call it fundamental, but it is a very convenient and

1142
01:50:46,960 --> 01:50:53,600
very common, I would say ubiquitous model. Now, what I personally find disturbing and we can talk

1143
01:50:53,600 --> 01:50:59,840
about it in more detail later on is that if you look at the different geometric structures for

1144
01:50:59,840 --> 01:51:05,840
Gamble that we consider in the book, whether it's Euclidean spaces or many phones, they all have

1145
01:51:05,840 --> 01:51:10,560
the discrete counterparts. So you have a plane and you can discretize it as a grid. You have a

1146
01:51:10,560 --> 01:51:16,960
manifold, you can discretize it as a mesh. A graph is inherently discrete. And this is something that

1147
01:51:16,960 --> 01:51:22,560
I find disturbing. There is, in fact, an entire field that is called network geometry that tries to

1148
01:51:22,560 --> 01:51:27,520
look at graphs as continuous objects. So for example, certain types of graph that look like

1149
01:51:27,520 --> 01:51:33,360
social networks, what is called scale free graphs, can be represented as nearest-neighbor graphs in

1150
01:51:33,360 --> 01:51:39,760
some a little bit exotic space with hyperbolic geometry. So if we take this analogy, I think

1151
01:51:39,760 --> 01:51:45,760
it is very powerful because now you can consider graphs as a discretization of something continuous

1152
01:51:45,760 --> 01:51:53,600
and then think for example of graph neural networks as certain types of diffusion processes

1153
01:51:53,680 --> 01:52:01,120
that are just discretized in a certain way. And by making potentially possible different

1154
01:52:01,120 --> 01:52:06,880
discretizations, you will get maybe better performing architectures. One of the core

1155
01:52:06,880 --> 01:52:12,240
dichotomies we talk about on Street Talk is the apparent dichotomy between discrete and continuous.

1156
01:52:12,240 --> 01:52:17,280
And as Yannick was saying, there are folks out there who want our knowledge substrate to be

1157
01:52:17,280 --> 01:52:22,400
discrete but still distributed, record them sub-symbolic folks. And this network geometry

1158
01:52:22,400 --> 01:52:26,080
is fascinating as well because you're saying in some sense you can think of there being some

1159
01:52:26,080 --> 01:52:32,160
unknown continuous geometry. So you're saying there is no dichotomy? This is probably a little

1160
01:52:32,160 --> 01:52:37,920
bit of a wishful thinking as it happens with every model. So I would probably phrase it carefully

1161
01:52:37,920 --> 01:52:45,120
for some kinds of graphs, you can make this continuous model for others, maybe not. Fascinating.

1162
01:52:45,120 --> 01:52:51,200
Well, on to the subject of vector spaces versus discrete, you know, geometric deep learning is

1163
01:52:51,200 --> 01:52:57,920
all about making any domain amenable to vector spaces, right? And indeed artificial neural networks.

1164
01:52:57,920 --> 01:53:02,240
But could these geometric principles be applied to another form of machine learning, let's say

1165
01:53:02,240 --> 01:53:07,600
discrete program synthesis? Certainly a very important question, Tim. And yeah, thanks for

1166
01:53:07,600 --> 01:53:14,240
asking that. I think that there are many ways in which geometric deep learning is already

1167
01:53:14,240 --> 01:53:18,880
at least implicitly powering discrete approaches such as program synthesis because

1168
01:53:19,680 --> 01:53:27,360
there is a pretty big movement on these so-called dual approaches where you stick a geometric

1169
01:53:27,360 --> 01:53:33,360
deep learning architecture within a discrete tool that searches for the best solution. So,

1170
01:53:33,360 --> 01:53:38,320
for example, in combinatorial optimization, a very popular approach recently for a

1171
01:53:38,320 --> 01:53:44,880
neural-resolving mixed integer programs is to like have your typical off-the-shelf

1172
01:53:45,520 --> 01:53:50,800
mip solver that selects variables to optimize one at a time. And, you know, with these kinds of

1173
01:53:50,800 --> 01:53:55,760
algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough about

1174
01:53:55,760 --> 01:54:00,720
how, in what order you select these variables, you can actually solve the problem in linear time,

1175
01:54:00,720 --> 01:54:05,280
which is something we would like to strive towards. And the exact way in which we select

1176
01:54:05,280 --> 01:54:09,440
these variables is a bit of a black magic, like humans have come up with a few heuristics, but

1177
01:54:09,520 --> 01:54:14,960
they don't always work. And whenever you have this kind of setting, as long as you're assuming that

1178
01:54:14,960 --> 01:54:20,080
you're naturally occurring data isn't always throwing the worst possible cases or adversarial

1179
01:54:20,080 --> 01:54:26,160
cases at you, you can usually rely on some kind of modeling technique, for example, a neural network

1180
01:54:26,160 --> 01:54:30,880
to figure out which decisions the model should be taking. So, in this case, for example, for

1181
01:54:30,880 --> 01:54:36,080
mip solving, DeepMind recently published a paper on this where you can treat mip problems as

1182
01:54:36,080 --> 01:54:41,040
bipartite graphs, where you have variables on one side and the constraints on the other.

1183
01:54:41,040 --> 01:54:45,600
And you link them together if a variable appears in a constraint. Then they run a graph neural

1184
01:54:45,600 --> 01:54:50,080
network, which, as we just discussed, is one of the flagship models in geometric deep learning,

1185
01:54:50,080 --> 01:54:56,480
over this bipartite graph to decide which variable the model should select next. And you can train

1186
01:54:56,480 --> 01:55:01,120
this either as a separate kind of supervised technique to learn some kind of heuristic,

1187
01:55:01,120 --> 01:55:06,080
or you can learn it as part of a more broader reinforcement learning framework, right, where

1188
01:55:06,080 --> 01:55:11,600
the reward you get is how close you are to the solution or something like this. So this is one

1189
01:55:12,240 --> 01:55:18,080
kind of clear way in which you can kind of have this synergy between geometric deep learning

1190
01:55:18,080 --> 01:55:25,120
architectures and solutions for, for example, program synthesis. But I would just like to

1191
01:55:25,120 --> 01:55:31,440
offer another angle in which you can think of program synthesis as nothing other than just

1192
01:55:31,440 --> 01:55:37,280
one more way to do language modeling, right, because synthesizing a program is not that different to

1193
01:55:37,280 --> 01:55:43,600
synthesizing a sentence, maybe with a more stringent check on syntax and so forth. But, you know,

1194
01:55:43,600 --> 01:55:48,880
any technique that is applied to language modeling could, in principle, be applied for

1195
01:55:48,960 --> 01:55:55,440
program synthesis. And something that we will be discussing, I believe, later during this conversation,

1196
01:55:56,240 --> 01:56:01,120
one of the flagship models of geometric deep learning is indeed the transformer, which

1197
01:56:02,080 --> 01:56:08,000
we show in our book, and elucidate why it can be seen as a very specific case of a graph neural

1198
01:56:08,000 --> 01:56:14,880
network. And that's one of the flagship models of language modeling. So basically, that's also

1199
01:56:14,880 --> 01:56:20,880
one more way to to unify. Like, you know, just because the end output is discrete doesn't mean

1200
01:56:20,880 --> 01:56:26,400
that you cannot reason about it using representations that are internally vector vector based.

1201
01:56:27,280 --> 01:56:33,760
Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information

1202
01:56:33,760 --> 01:56:38,560
into a continuous representation. But the manifold needs to be smooth, it needs to be

1203
01:56:38,560 --> 01:56:44,320
learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete

1204
01:56:44,320 --> 01:56:49,520
program searches. But then you have this exponential blow up. But maybe that search space, because

1205
01:56:49,520 --> 01:56:54,800
it's interpolative could be found using stochastic gradient descent, if you embed the discrete

1206
01:56:54,800 --> 01:56:59,840
information into some kind of vector space. But Professor Bronstein, I wanted to throw it back

1207
01:56:59,840 --> 01:57:04,320
over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because

1208
01:57:04,320 --> 01:57:10,080
everything we're doing here is embedding discrete information into these Euclidean vector spaces.

1209
01:57:10,160 --> 01:57:15,440
Why are we doing that? There are multiple reasons why vector spaces are so popular in

1210
01:57:15,440 --> 01:57:21,360
representation learning. Vectors are probably the most convenient representation for both humans

1211
01:57:21,360 --> 01:57:26,880
and computers. We can do for a number of operations with them, like addition or subtraction. We can

1212
01:57:26,880 --> 01:57:32,000
represent them as arrays in the memory of the computer. They are also continuous objects,

1213
01:57:32,000 --> 01:57:37,920
so it is very easy to use continuous optimization techniques in the vector spaces. It is difficult

1214
01:57:37,920 --> 01:57:42,480
for Gamble to optimize a graph because it is discrete and requires combinatorial techniques.

1215
01:57:42,480 --> 01:57:47,600
But in a vector representation, I just have a bunch of points that I can continuously move

1216
01:57:47,600 --> 01:57:53,840
in a kind of dimensional space using standard gradient based techniques. Perhaps a more nuanced

1217
01:57:53,840 --> 01:57:59,200
question is what kind of structures can be represented in a vector space? And a typical

1218
01:57:59,200 --> 01:58:05,120
structure is some notion of similarity or distance. We want that the vector representations preserve

1219
01:58:05,120 --> 01:58:10,080
the distances between, let's say, original data points. And here we usually assume that

1220
01:58:10,080 --> 01:58:15,280
the vector space is equipped with the standard Euclidean metric or norm, and we have a problem

1221
01:58:15,280 --> 01:58:21,360
from the domain of metric geometry of representing one metric space in another. And unfortunately,

1222
01:58:21,360 --> 01:58:26,400
the general answer here is negative. You cannot exactly embed an arbitrary metric in Euclidean

1223
01:58:26,400 --> 01:58:32,000
space, but there are, of course, some results such as bogains theorem that, for Gamble, provides

1224
01:58:32,800 --> 01:58:38,240
bounds on the metric distortion in such cases. And in graph learning spaces with

1225
01:58:38,240 --> 01:58:43,280
other more exotic geometries such as hyperbolic spaces, you have recently become popular with,

1226
01:58:43,280 --> 01:58:47,600
for example, papers of Ben Chamberlain, my colleague from Twitter, or Max Nicol from Facebook.

1227
01:58:48,240 --> 01:58:52,880
And you can see that in certain types of graphs, the number of neighbors

1228
01:58:52,880 --> 01:58:58,240
grows exponentially with the radios. If you look, for example, at the number of friends of friends

1229
01:58:58,640 --> 01:59:03,760
and so on in a social network, where we have this small world phenomenon, you can see that

1230
01:59:03,760 --> 01:59:10,240
it becomes exponentially large with the growth of the radios. And now when you try to embed

1231
01:59:10,240 --> 01:59:14,480
this graph in Euclidean space, it will become very crowded because in the Euclidean space,

1232
01:59:14,480 --> 01:59:19,520
the volume of the metric ball grows polynomially with the radios. Think of the two-dimensional

1233
01:59:19,520 --> 01:59:26,480
case that we all know from school, the area of a circle is pi radius squared, right? The volume

1234
01:59:26,480 --> 01:59:31,680
of a ball is exponential with a dimension. So we inevitably need to increase the dimension

1235
01:59:31,680 --> 01:59:37,840
of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very

1236
01:59:37,840 --> 01:59:43,280
different because the volume grows exponentially with the radios. So it is way more convenient

1237
01:59:44,480 --> 01:59:48,480
to use these spaces for graph embeddings. And in fact, recent papers show that

1238
01:59:49,280 --> 01:59:53,200
to achieve the same error in embedding of a graph in the hyperbolic space with, let's say,

1239
01:59:53,200 --> 01:59:57,760
10 dimensions, you would require something like a 100-dimensional Euclidean space.

1240
01:59:58,720 --> 02:00:02,720
Of course, I should say that metrics are just one example of a structure. So the general answer

1241
02:00:02,720 --> 02:00:07,600
to the question whether a vector space is a good model for representing data is, as usual,

1242
02:00:07,600 --> 02:00:14,640
it depends. You mentioned language, Peter, and maybe to both of you, do you think there is a

1243
02:00:16,160 --> 02:00:23,040
geometry to language itself? I mean, obviously, we know about embedding spaces and close things

1244
02:00:23,840 --> 02:00:30,160
somehow share meaning and so on. Do you think it goes beyond that? Because, like,

1245
02:00:30,160 --> 02:00:36,480
do you think there is an inherent geometry to language itself and sort of the meaning of what

1246
02:00:36,480 --> 02:00:43,280
we want to transmit and how that relates to each other? What you probably mentioned is the

1247
02:00:43,280 --> 02:00:49,840
famous series of papers from Facebook where unsupervised language translation can be done

1248
02:00:49,840 --> 02:00:55,600
by a geometric alignment of the latent spaces. In my opinion, it's not something that describes

1249
02:00:55,600 --> 02:01:02,160
geometry of the language. It probably describes in a geometric way some semantics of the world.

1250
02:01:02,160 --> 02:01:06,080
And even though we have, linguistically speaking, very different languages like,

1251
02:01:06,080 --> 02:01:11,840
let's say, English and Chinese, yet they describe the same reality. They describe the same world

1252
02:01:11,840 --> 02:01:17,680
where humans act. So it is probably reasonable to assume that the concepts that they describe

1253
02:01:17,760 --> 02:01:21,600
are similar. And also, while there are some theories and linguistics about

1254
02:01:21,600 --> 02:01:28,320
certain universal structures in languages that are shared, even though the specifics are different,

1255
02:01:28,880 --> 02:01:33,600
I think it's interesting to look maybe at non-human communications. I wouldn't probably

1256
02:01:33,600 --> 02:01:39,200
use the term language because it's a little bit loaded and probably some purists will be shocked

1257
02:01:39,200 --> 02:01:43,760
by me saying that, for example, whales have a language, but we are studying the communication

1258
02:01:43,760 --> 02:01:48,960
of slow whales. So this is a big international collaboration called Project. And I don't think

1259
02:01:48,960 --> 02:01:55,600
that you can really model the concepts that whales need to describe and to deal with

1260
02:01:56,560 --> 02:02:01,520
in the same way as we humans do. So maybe a silly example, we can say in human languages,

1261
02:02:01,520 --> 02:02:07,120
and probably it applies to every language, we can express a concept that something got wet.

1262
02:02:07,120 --> 02:02:12,080
I don't think that a whale would even understand what it means by being wet because

1263
02:02:12,720 --> 02:02:17,600
the whale always lives in water. I would add to that maybe a slightly different view of geometry,

1264
02:02:17,600 --> 02:02:22,640
but it's all about the question of how far are you willing to go and still call it geometry.

1265
02:02:22,640 --> 02:02:27,760
Based on our proto book, at least, I tend to think of graph structures also as a form of

1266
02:02:27,760 --> 02:02:34,000
geometry, even though it's a bit more abstract. And within language, people might not always agree

1267
02:02:34,000 --> 02:02:39,440
what this structure is like. But I think we can be fairly certain that there are explicit links

1268
02:02:39,440 --> 02:02:45,120
between individual words as and when you use them in different forms, syntax trees, or just one

1269
02:02:45,120 --> 02:02:51,200
word precedes another and so on and so forth. And while we may not be necessarily able to easily

1270
02:02:51,200 --> 02:02:59,600
say what is the geometric significance of one word, what we can look at is what is the local

1271
02:02:59,600 --> 02:03:05,440
geometry of the words that you tend to use around it. And I mean, this kind of principle has been

1272
02:03:05,440 --> 02:03:18,480
used all over the place. That has then been extended to graph structured observations,

1273
02:03:18,480 --> 02:03:22,560
generally with models like deep walk and note to back basically the same idea,

1274
02:03:22,560 --> 02:03:28,000
treat a nodes representation as everything that's around it. Basically, the reason why I think that

1275
02:03:28,000 --> 02:03:32,720
analyzing this local topology of how words are used with each other is very powerful.

1276
02:03:33,680 --> 02:03:39,280
I've reinforced that recently, precisely because of the fact I've been delving into category theory,

1277
02:03:39,280 --> 02:03:43,600
because in category theory, your nodes are basically atoms, you're not allowed to look

1278
02:03:43,600 --> 02:03:48,800
inside them, you assume they're this undivisible unit of information. And everything you can

1279
02:03:48,800 --> 02:03:55,200
conclude about the atoms comes from the arrows between them. So using this very simple concept

1280
02:03:55,200 --> 02:04:01,120
with a few additional constraints like compositionality, you can, for example, tell me what are all the

1281
02:04:01,120 --> 02:04:06,320
elements of a set, even though you've abstracted that sets to a single point, just by analyzing

1282
02:04:06,320 --> 02:04:11,280
the arrows between all sets, you can tell me what are all the elements inside a set. So thinking

1283
02:04:11,280 --> 02:04:17,040
about this, I do believe that it is possible to reason about geometric, you know, word to

1284
02:04:17,040 --> 02:04:21,680
vex, for example, does this with the assumption that the structure of the

1285
02:04:21,680 --> 02:04:26,720
are we approaching this at the right level, though, because people have said for quite a

1286
02:04:26,720 --> 02:04:31,760
long time that there's a difference between syntax and semantics. And you could look at the

1287
02:04:31,760 --> 02:04:36,880
geometrical structure of spoken language. Or, for example, you could look at the topology of

1288
02:04:36,880 --> 02:04:41,920
the connections in your brain, the topology of, you know, reference frames in your brain is how

1289
02:04:41,920 --> 02:04:48,400
you actually have learned concepts. Would looking at the topology of spoken language tell you enough

1290
02:04:48,400 --> 02:04:57,600
about abstract categories? That's a good question. I think that if that kind of information is

1291
02:04:57,600 --> 02:05:02,880
necessary, like if the atoms by themselves won't tell you everything. One thing that we actually

1292
02:05:02,880 --> 02:05:07,280
very commonly do in graph representation learning is assume this sort of hierarchical approach,

1293
02:05:07,280 --> 02:05:12,800
where you have like the ground level with your actual individual notes, and then you come up

1294
02:05:12,800 --> 02:05:17,040
with some kind of additional hierarchy that tells you either something about intermediate

1295
02:05:17,040 --> 02:05:22,880
topologies in a graph, or intermediate structures that you care about in this graph, or any abstract

1296
02:05:22,880 --> 02:05:27,040
concepts you might have extracted. And then there's additional links being drawn between these to

1297
02:05:27,040 --> 02:05:33,600
kind of reinforce the knowledge that the graph net can capture. So I think if you have knowledge of

1298
02:05:33,600 --> 02:05:38,080
some abstract concepts that are relevant for your particular task, you can attach them as

1299
02:05:38,080 --> 02:05:43,040
additional pieces of information to this topology. Of course, the more exciting part is could we

1300
02:05:43,120 --> 02:05:47,840
maybe discover them automatically? But that is something that I don't think is potentially in

1301
02:05:47,840 --> 02:05:54,320
scope for this question. When human interpreters need to translate from one language to another,

1302
02:05:54,320 --> 02:05:59,440
they often need to deal with different structures. I think Turkish is actually an extreme example

1303
02:05:59,440 --> 02:06:04,640
where the order of words is completely reversed. It implies that you need probably to hold well

1304
02:06:04,640 --> 02:06:11,040
in computer science terms some kind of a buffer in your brain before you can make the translation

1305
02:06:11,040 --> 02:06:16,720
to another language. So it definitely imposes certain biological network structure in the brain.

1306
02:06:16,720 --> 02:06:23,760
Another interesting observation that I read somewhere about the way that people remember

1307
02:06:23,760 --> 02:06:30,400
certain facts when they speak a certain language. So the particular example that was given is a

1308
02:06:30,400 --> 02:06:38,400
person can remember a perpetrator of a crime and then gives testimony in court. And the reason

1309
02:06:38,400 --> 02:06:44,720
is that in some languages, it is more common to use impersonal pronouns and the personal phrases.

1310
02:06:44,720 --> 02:06:50,000
So you can say, for example, the object was broken. And in some languages, you would say that

1311
02:06:50,000 --> 02:06:56,640
somebody broke the object. So it appeared that languages were of these more impersonal constructions.

1312
02:06:56,640 --> 02:07:02,400
People speaking these languages, I have hard time to remember the perpetrator. So the language

1313
02:07:02,480 --> 02:07:08,720
probably imposes a lot about the way that we perceive world, but it is probably not studied

1314
02:07:08,720 --> 02:07:14,400
sufficiently. But there may be some fuzzy graph isomorphisms, though, between the languages.

1315
02:07:15,280 --> 02:07:18,800
I think there's something really magic about graphs. I think that's what we get into, because

1316
02:07:18,800 --> 02:07:22,400
your lecture series inspired me, actually, Professor Bronstein, where you were talking

1317
02:07:22,400 --> 02:07:26,800
about all the different applications of graphs. But something that a lot of our guests talk about

1318
02:07:26,800 --> 02:07:31,360
are knowledge graphs. Expert systems and the knowledge acquisition bottleneck were the

1319
02:07:31,360 --> 02:07:37,440
cause of the abject failure of good old fashioned AI or some symbolic AI systems in the 1980s. And

1320
02:07:37,440 --> 02:07:41,760
many hybrid or neuro symbolic folks today are still arguing that we need to have a discrete

1321
02:07:41,760 --> 02:07:48,000
knowledge graph, either human designed or learned or evolved or emerged or some combination of those

1322
02:07:48,560 --> 02:07:52,560
things I just said, depending on who you talk to. Now, critically, many go fi people think that

1323
02:07:52,560 --> 02:07:57,760
most knowledge we have is acquired through reasoning, not learning, right, which is really,

1324
02:07:57,760 --> 02:08:02,880
really interesting. So by reasoning, I mean extrapolating new knowledge from existing knowledge.

1325
02:08:03,520 --> 02:08:07,600
It feels like graph neural networks could at least be part of the solution here. And in your

1326
02:08:07,600 --> 02:08:12,480
lecture series, you mentioned the work by Kramner, which was explainable GNN, where they use some

1327
02:08:12,480 --> 02:08:17,920
kind of symbolic regression to get a symbolic model from a graph neural network. So do you think

1328
02:08:17,920 --> 02:08:23,120
there's some really cool work we can do here? There is a little bit of divide in graph learning

1329
02:08:23,120 --> 02:08:28,160
literature. So people working on graph neural networks, and working on knowledge graphs, even

1330
02:08:28,160 --> 02:08:33,280
though, at least in principle, the methods are similar. For example, you typically do some form

1331
02:08:33,280 --> 02:08:38,240
of embedding of the nodes of the graph. Somehow these are distinct communities, probably,

1332
02:08:38,240 --> 02:08:43,280
historically, they evolved in different fields. Yeah, so the paper of Krammer, this is really

1333
02:08:43,280 --> 02:08:49,280
interesting because they use graphs to model physical systems, for example, and body problem

1334
02:08:49,280 --> 02:08:54,240
when they have particles that interact. You can describe these interactions as a graph,

1335
02:08:54,800 --> 02:09:00,400
and you can use standard generic message passing functions to model the interactions.

1336
02:09:00,400 --> 02:09:06,320
Now, the step forward that they do is they replace these generic message passing functions

1337
02:09:06,320 --> 02:09:12,960
with symbolic equations. And not only that this allows to generalize better, but you also have

1338
02:09:12,960 --> 02:09:19,360
an interpretable system, you can recover from your data the laws of motion, right? And if you

1339
02:09:19,360 --> 02:09:24,080
think of how much time it took, historically, to people like Johannes Kepler, for example,

1340
02:09:24,080 --> 02:09:31,280
he spent his entire life on analyzing astronomical observations to derive a law that now bears his

1341
02:09:31,280 --> 02:09:37,200
name, that describes the elliptic orbits of planets. Nowadays, with these methods, you can

1342
02:09:37,200 --> 02:09:43,440
probably do it in a matter of seconds or maybe minutes. I think the point that particularly

1343
02:09:43,440 --> 02:09:48,800
caught my attention in what you asked, Tim, was this interplay between graphs and reasoning

1344
02:09:48,800 --> 02:09:55,520
and extrapolation and how that supports knowledge. Now, when it comes to how critical is this going

1345
02:09:55,520 --> 02:10:02,560
to be, it depends on the environment in which you put your agent. Like, is it a closed environment,

1346
02:10:02,560 --> 02:10:07,760
or is it an open ended environment where new information and new knowledge can come in

1347
02:10:07,760 --> 02:10:14,000
in principle at any time? This basically do want to build a neural scientist, or do you just want

1348
02:10:14,000 --> 02:10:18,560
to build a neural exploiter that takes all the information available right now and then draws

1349
02:10:18,560 --> 02:10:25,280
conclusions based on that. So if the system is closed worlds, you'll probably be able to get

1350
02:10:25,280 --> 02:10:31,040
away without very explicit reasoning, especially if you have tons of data, because we've seen time

1351
02:10:31,040 --> 02:10:36,240
and time again that large scale models can kind of pick up on these regularities if they've seen

1352
02:10:36,240 --> 02:10:45,200
it often enough. But if I give you a solution that involves stacking, for example, n objects,

1353
02:10:45,200 --> 02:10:50,800
and now I ask you to do the same kind of reasoning with two times n objects, the way in which we

1354
02:10:50,800 --> 02:10:56,080
optimize neural networks at least today is typically going to completely fall on its back

1355
02:10:56,080 --> 02:11:02,480
when you do something like this. So if you truly want to take whatever regularities you have come

1356
02:11:02,480 --> 02:11:10,320
across in the world of the training data and hope to at least reasonably gracefully apply them to

1357
02:11:10,320 --> 02:11:19,840
new kinds of rules that come in the future, then you probably want your model to extrapolate to a

1358
02:11:19,840 --> 02:11:26,640
certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a

1359
02:11:26,640 --> 02:11:34,080
very natural area to study under this lens, because they trivially extrapolate, you write an algorithm

1360
02:11:34,080 --> 02:11:40,640
that does a particular thing on a set of n nodes, you can be you can usually mathematically prove

1361
02:11:40,640 --> 02:11:45,520
that it's going to do the same thing equally properly, maybe a bit more slowly, if you give it

1362
02:11:45,520 --> 02:11:50,000
two times n nodes, right? This kind of guarantee typically doesn't come that easily with neural

1363
02:11:50,000 --> 02:11:54,960
networks. And we found that you have to very carefully massage the way you train them, the

1364
02:11:54,960 --> 02:11:59,920
kinds of data you feed to them, the kinds of inductive biases you feed into them, in order to

1365
02:11:59,920 --> 02:12:04,880
get them to do something like this. So if extrapolation is something you truly need, and you know,

1366
02:12:04,880 --> 02:12:09,040
I think for artificial general intelligence, we're going to want to have at least some degree of

1367
02:12:09,040 --> 02:12:14,320
extrapolation as new information will become available to our neural scientists, just as

1368
02:12:14,320 --> 02:12:21,360
you follow the era of time. Basically, for doing something like this, graph neural networks have

1369
02:12:21,360 --> 02:12:26,160
arisen as a very attractive primitive, because there's been a few really exciting theoretical

1370
02:12:26,160 --> 02:12:32,080
results coming out in recent years, saying that the operations of a graph neural network align

1371
02:12:32,080 --> 02:12:37,680
really, really well with dynamic programming algorithms. And dynamic programming is a very

1372
02:12:37,680 --> 02:12:43,360
standard computational primitive, using which you can express most polynomial time heuristics.

1373
02:12:43,360 --> 02:12:48,880
So essentially, that's a really good, you know, that's a really good piece of mind result. The

1374
02:12:48,880 --> 02:12:53,360
unfortunate side of it is that it's a best case result, right? So you can set the weights of

1375
02:12:53,360 --> 02:12:57,120
a neural network of a graph neural network to mimic a dynamic programming algorithm,

1376
02:12:57,120 --> 02:13:02,320
more efficiently or with smaller sample complexity. But, you know, there's still a big

1377
02:13:02,320 --> 02:13:06,720
problem of how do I learn it in a way that it still works when I double the size of my input.

1378
02:13:06,720 --> 02:13:12,080
And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make

1379
02:13:12,080 --> 02:13:17,120
that happen. It's not easy. If you throw the vanilla graph neural network and just input

1380
02:13:17,120 --> 02:13:21,360
output pairs of an algorithm, it will learn to fit them in distribution the moment you give,

1381
02:13:21,360 --> 02:13:25,280
like, ask it to sort and array that's twice as big, it's going to completely collapse. So

1382
02:13:25,840 --> 02:13:31,200
this is the number one thing that the neurosymbolic people say. They say neural networks, they don't

1383
02:13:31,200 --> 02:13:38,320
extrapolate. They only interpolate, you know, it just, it's a continuous geometric model,

1384
02:13:38,320 --> 02:13:44,240
learns point by point, transforms the data onto some continuous, smooth, learnable manifold,

1385
02:13:44,240 --> 02:13:48,080
you interpolate between the data points, you want to have a smooth, you want to have a dense

1386
02:13:48,080 --> 02:13:52,640
sampling of your data. But you're talking about dynamic programming problems, these are discrete

1387
02:13:52,640 --> 02:13:58,400
problems that the structure is discontinuous. But how could you possibly learn that within your

1388
02:13:58,400 --> 02:14:06,800
network? Well, the dynamic programming algorithm could be, could have a discontinuous component

1389
02:14:07,680 --> 02:14:12,800
for example, if you're searching for shortest paths at some point, you will take an argmax over

1390
02:14:12,800 --> 02:14:16,880
all of your neighbor's computed distances and use that to decide what the path is.

1391
02:14:17,520 --> 02:14:23,120
But before you come to the argmax part, there is usually some fairly smooth function being

1392
02:14:23,120 --> 02:14:28,640
computed actually. So in the case of shortest path computations, you know, Bellman Ford or

1393
02:14:28,640 --> 02:14:33,760
something like this, you say something very simple, like, I have a value d of s in every

1394
02:14:33,760 --> 02:14:38,400
single one of my nodes, which is initially infinity everywhere and zero in the source

1395
02:14:38,400 --> 02:14:45,760
vertex. And then at every point, I say, the distance of my particular node is the minimum

1396
02:14:45,760 --> 02:14:52,160
of all the distances of my neighbors plus the edge weight, right. And this kind of function is

1397
02:14:52,960 --> 02:14:58,640
generally more graceful than than taking an argmax. And you can also think of, for example,

1398
02:14:58,640 --> 02:15:02,560
if you have to compute expected values or something like this, using dynamic programming,

1399
02:15:02,560 --> 02:15:07,200
that's also one example where actually summing is what you need to do across all of your neighbors

1400
02:15:07,200 --> 02:15:13,440
or something like this. So yeah, it is true that like, across individual steps, you may be doing

1401
02:15:13,440 --> 02:15:20,640
like discrete optimization steps. But usually, it's propelled by some kind of continuous

1402
02:15:20,640 --> 02:15:24,880
computation under the hood. So that's the part that the graph neural network actually simulates.

1403
02:15:24,880 --> 02:15:28,480
And then the part which does the argmax would be some kind of classifier that you stitch on

1404
02:15:28,480 --> 02:15:34,080
top of that. So in principle, it's not, yeah, it's not too challenging to massage it into a

1405
02:15:34,080 --> 02:15:39,760
neural network framework. So one of the, I think one of you mentioned this before, brought up

1406
02:15:39,760 --> 02:15:46,320
transformers. And, you know, in recent years, we've had, I think about 10 different papers

1407
02:15:46,320 --> 02:15:52,960
saying transformers are something there is transformers are RNNs, transformers are Hopfield

1408
02:15:52,960 --> 02:15:59,840
networks. And also transformers are graph neural networks or compute some kind of

1409
02:15:59,840 --> 02:16:05,440
graph neural networks. Can you maybe speak a bit to that? Are transformers specifically

1410
02:16:05,440 --> 02:16:12,000
graph neural networks? Or are they just so general that you can also formulate a graph problem in

1411
02:16:12,000 --> 02:16:20,080
terms of a transformer? Okay, that's a very good question. I would start off by saying,

1412
02:16:20,960 --> 02:16:25,280
like, I don't want to start this discussion just by saying, yes, transformers are graph

1413
02:16:25,280 --> 02:16:29,520
neural networks. This is why end of story, because I feel like, you know, that doesn't

1414
02:16:29,520 --> 02:16:34,000
touch upon the whole picture. So let's let's look at this from a natural language processing

1415
02:16:34,000 --> 02:16:39,760
angle, which is how most people have come to know about transformers. So imagine that you have a task

1416
02:16:39,760 --> 02:16:46,640
which is specified on a sentence. And you want to exploit the fact that words in the sentence

1417
02:16:46,640 --> 02:16:50,880
interact, right? It's not just a bag of words. There is there's some interesting structure

1418
02:16:50,880 --> 02:16:56,240
inside this bunch of words that you might want to exploit. When we were using recurrent neural

1419
02:16:56,240 --> 02:17:00,480
networks, we assume that the structure between the words was a line graph. So basically,

1420
02:17:01,200 --> 02:17:06,960
every word is preceding another word and so on and so forth. And you kind of just linearly

1421
02:17:06,960 --> 02:17:14,480
process them with a model like LSTM or something. But, you know, basically line graphs, as we know,

1422
02:17:14,480 --> 02:17:19,920
are not the way language is actually structured. There can be super long range interactions

1423
02:17:19,920 --> 02:17:25,120
inside language. So subjects and objects in the same sentence could appear miles away from each

1424
02:17:25,120 --> 02:17:30,080
other. So using the line graph is not the most optimal way of getting that information in the

1425
02:17:30,080 --> 02:17:35,520
fastest possible in the fastest possible way. So, okay, there's clearly some kind of non trivial

1426
02:17:35,520 --> 02:17:41,280
graph structure. What is it? Well, it turns out that people cannot really agree what this optimal

1427
02:17:41,280 --> 02:17:46,480
graph structure is, and it may well be task dependent, actually. So just consider syntax

1428
02:17:46,480 --> 02:17:52,000
trees, for example, like there's not always a unique way of decomposing a sentence into a syntax

1429
02:17:52,000 --> 02:17:56,480
tree. And the exact kind of tree you might wish to use to represent a sentence may be different

1430
02:17:56,480 --> 02:18:01,600
depending on what is the actual thing that you're solving. So, okay, we have a situation where we

1431
02:18:01,600 --> 02:18:06,080
know that there's some connectivity between the words, but we don't know what that connectivity is.

1432
02:18:06,080 --> 02:18:10,240
So in graph representation learning, what we typically do when we don't know the graph,

1433
02:18:10,240 --> 02:18:14,400
as long as the number of objects is not huge, is to assume a complete graph and let the graph

1434
02:18:14,400 --> 02:18:20,320
neural network figure out by itself what the important connections are. And if I now stitch

1435
02:18:20,320 --> 02:18:25,840
an attentional message passing mechanism onto this graph neural network, I have effectively

1436
02:18:25,840 --> 02:18:34,400
rederived the transformer model equation without ever like using this specific transformer lingo.

1437
02:18:34,400 --> 02:18:40,000
So from this kind of angle, the fact that it's a model that operates over a complete graph

1438
02:18:40,560 --> 02:18:46,400
individual words, in a way that you know, once you've put all the embeddings to them is permutation

1439
02:18:46,400 --> 02:18:52,080
equivalent, this describes the central equations of self attention that the transformer uses.

1440
02:18:52,080 --> 02:18:57,760
The part which I think causes a bit of a divide here is the fact that transformers like the model

1441
02:18:57,760 --> 02:19:02,640
that was originally presented are not just the equations of a transformer, they're also the

1442
02:19:02,640 --> 02:19:07,200
positional embeddings of a transformer. And that's the part that sort of gives it a bit more of a

1443
02:19:07,280 --> 02:19:14,800
central structure. Well, actually, if you look at these sine and cosine waves that get attached to

1444
02:19:14,800 --> 02:19:20,240
the individual words in an input to a transformer, you will see that you can you can actually derive

1445
02:19:20,240 --> 02:19:25,200
a pretty good connection between them and the discrete Fourier transform, which actually

1446
02:19:25,200 --> 02:19:31,920
turned out to be the eigenvectors of a graph Laplacian for a line graph. So essentially these

1447
02:19:31,920 --> 02:19:37,040
positional embeddings are hinting to the model that you are the decent that these words in a

1448
02:19:37,040 --> 02:19:41,600
sentence are arranged in a particular way, and you can use that information. But because it's

1449
02:19:41,600 --> 02:19:46,720
fed in as features, the model doesn't have to use any of that information, like sometimes bag of words

1450
02:19:46,720 --> 02:19:52,240
is the right thing to do, for example, right. So essentially, the transformer has a bit of a light

1451
02:19:52,240 --> 02:19:58,640
hint that there's a central structure in there in the form of a line graph. But, you know, the

1452
02:19:58,720 --> 02:20:03,520
model itself is a permutation equivalent model over a complete graph. And from our lens of the

1453
02:20:03,520 --> 02:20:09,120
geometric deep learning, it is effectively a special case of an attentional GNN. Now, I think

1454
02:20:09,120 --> 02:20:13,920
this positional embedding aspect is a super important one. And it could hint to how we might

1455
02:20:13,920 --> 02:20:19,040
extend these transformers from sentences to more general structures. And I think, Michael, you

1456
02:20:19,040 --> 02:20:24,000
might have a lot more thoughts on that than I do. So maybe you can say a bit about that.

1457
02:20:24,640 --> 02:20:28,800
Yeah, so positional encoding has been done for graphs as well. As Petter mentioned,

1458
02:20:28,800 --> 02:20:35,440
in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates

1459
02:20:35,440 --> 02:20:40,000
that are used in transformers using the eigenvectors of the Laplacian. There are other

1460
02:20:40,000 --> 02:20:46,000
techniques you can actually show that you can make it a message passing type neural network

1461
02:20:46,000 --> 02:20:51,520
strictly more powerful than traditional message passing. The equivalent vise for 11 graphics

1462
02:20:51,520 --> 02:20:58,960
or morphism test by using a special kind of structure where positional encoding, for example,

1463
02:20:58,960 --> 02:21:05,280
if you can count substructures of the graph, such as cycles or rings and so on. And this way,

1464
02:21:05,280 --> 02:21:11,040
you have a message passing algorithm that is specialized to the particular position in the

1465
02:21:11,040 --> 02:21:16,640
graph and can, for example, detect structures that the traditional message passing cannot detect.

1466
02:21:17,200 --> 02:21:22,880
So it is at least not less powerful than the vise for 11 algorithm. And we can actually show

1467
02:21:22,880 --> 02:21:28,560
examples on which vise for 11 algorithm or traditional message passing fails, whereas

1468
02:21:28,560 --> 02:21:34,960
this kind of approach succeeds. So the thing I've always wondered about transformers networks

1469
02:21:35,520 --> 02:21:39,440
are the position tokens. I really don't like them and I want them to go away

1470
02:21:39,440 --> 02:21:44,720
because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher

1471
02:21:44,800 --> 02:21:51,680
order structure in the language. And it kind of felt like we were using the position tokens to

1472
02:21:51,680 --> 02:21:55,840
cheat a little bit. So what I'm trying to get across here is that I think the position of a

1473
02:21:55,840 --> 02:22:01,520
token in an utterance should be invariant. I mean, clearly, in different languages,

1474
02:22:01,520 --> 02:22:06,960
the tokens are in different places. In Turkish, the order is completely reversed. And I would

1475
02:22:06,960 --> 02:22:13,120
like to think that our internal language representation ignores the transmission

1476
02:22:13,120 --> 02:22:18,480
arrangement given the particular language and the constraint that we only communicate sequential

1477
02:22:18,480 --> 02:22:24,080
streams of words. However, I do appreciate what Michael is saying above that the position

1478
02:22:24,080 --> 02:22:30,240
encodings can actually encode more powerful structures like cycles and rings. The key question

1479
02:22:30,240 --> 02:22:36,480
is, do we actually need to have these structures in natural language? I don't agree that you want

1480
02:22:36,480 --> 02:22:41,760
to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you

1481
02:22:41,760 --> 02:22:47,840
consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the

1482
02:22:47,840 --> 02:22:52,720
really key characteristics of graphs and sets more generally that you don't have the ordering

1483
02:22:52,720 --> 02:22:58,720
of the nodes. The situations and the problems where transformers are applied, you actually do have

1484
02:22:58,720 --> 02:23:05,680
an order. But you use the graph as Petra described to model different long distance relations

1485
02:23:05,760 --> 02:23:12,400
between different tokens or words in a sentence. So you want to incorporate this prior knowledge

1486
02:23:12,400 --> 02:23:19,120
that these nodes are not in arbitrary order, that they have some sentence order. And this

1487
02:23:19,120 --> 02:23:23,680
principle applied more generally, you can use positional encoding to tell message passing

1488
02:23:23,680 --> 02:23:29,040
not to apply exactly the same function everywhere on the graph, but to make it

1489
02:23:29,040 --> 02:23:33,840
specialized for different portions of the graphs or at least make it a possibility. And then

1490
02:23:34,560 --> 02:23:40,320
the training will decide whether to use this information or not or in which way.

1491
02:23:41,520 --> 02:23:45,360
It's strange because I don't know whether the order is just a function of the communication

1492
02:23:45,360 --> 02:23:51,360
medium. So we transmit the tokens in a sequence. And could we then represent them in our brains

1493
02:23:51,360 --> 02:23:54,560
in a completely different domain where the sequence is no longer relevant? Well,

1494
02:23:54,560 --> 02:23:58,160
actually a lot of neuroscientists think that our brain is a prediction machine and

1495
02:23:58,160 --> 02:24:02,400
it's a sequence prediction machine. So the sequence is kind of fundamentally important.

1496
02:24:03,360 --> 02:24:07,920
Yeah, it's also a function of the specific language that you use. And as we discussed

1497
02:24:07,920 --> 02:24:13,680
before, there are languages which convey the same meaning with a difference in structure.

1498
02:24:13,680 --> 02:24:19,440
I want to get a little bit into what you said about essentially what we're doing with these

1499
02:24:19,440 --> 02:24:24,720
positional encodings is we hint. We hint to the model that there is something here,

1500
02:24:24,720 --> 02:24:30,480
which is a big break from sort of the old approach, let's say, of an LSTM to say,

1501
02:24:30,560 --> 02:24:38,480
this is the structure. So with the world of geometric deep learning, I often have the feeling

1502
02:24:38,480 --> 02:24:43,600
people talk about, they talk about symmetries and we need to exploit these symmetries that are

1503
02:24:43,600 --> 02:24:50,400
present in the world. And there's almost to me two different groups of these symmetries. So one

1504
02:24:50,400 --> 02:24:56,320
group is maybe you would call them like exact symmetries or something like this. When I think

1505
02:24:56,320 --> 02:25:02,640
about Alpha fold, and I think about like a protein, it doesn't, I don't care which side is up, right?

1506
02:25:02,640 --> 02:25:07,840
Like the protein is the same, the same protein, and there's no reason to prefer any direction

1507
02:25:07,840 --> 02:25:14,000
over any other direction. However, if I think of like, because people have made this argument

1508
02:25:14,000 --> 02:25:18,720
for CNNs, for example, they say, well, a CNN is a good architecture because it's translation

1509
02:25:18,720 --> 02:25:24,640
invariant, right? And essentially, if we want to do object recognition or image classification,

1510
02:25:24,640 --> 02:25:30,560
translation invariance is like a given, but but it's not, right? It's not a given the pictures

1511
02:25:30,560 --> 02:25:36,320
that we feed to these algorithms, most often the object is in the middle, most often, you know,

1512
02:25:36,320 --> 02:25:42,560
it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera like this,

1513
02:25:42,560 --> 02:25:50,960
but I don't, right? So it, it seems to be, it seems to be in many cases better to not

1514
02:25:50,960 --> 02:25:57,760
put the symmetries in there until we're like, really, really, really, really sure that these are

1515
02:25:57,760 --> 02:26:05,440
actual symmetries, because with more data, it seems the model that does not have the prior inside

1516
02:26:06,000 --> 02:26:12,960
becomes better than the model that does have the prior inside, if that prior doesn't exactly match

1517
02:26:12,960 --> 02:26:19,280
the world is, do you think that's a fair characterization of, for example, why transformers

1518
02:26:19,280 --> 02:26:24,960
with large data all of a sudden beat classic CNN models or come close to them?

1519
02:26:25,840 --> 02:26:31,760
I think it's, it's always this question of the trade off between how much your, your model and

1520
02:26:31,760 --> 02:26:39,120
how much you learn and I remember when I was a student, there was this maxim that that machine

1521
02:26:39,120 --> 02:26:44,960
learning is always the second best solution. And maybe nowadays with deep learning showing

1522
02:26:44,960 --> 02:26:50,160
some remarkable set of successes, I'm probably less confident in this statement, but it's probably

1523
02:26:50,160 --> 02:26:55,280
still quite true that the more you know about your problem, the better chances that machine

1524
02:26:55,280 --> 02:27:01,280
learning will work for it. To me, it makes sense to model as much as possible and learn what is

1525
02:27:01,280 --> 02:27:07,520
current or impossible to model. And in practice, of course, there is a spectrum of possibilities

1526
02:27:07,520 --> 02:27:13,760
of how much of these prior assumptions are hardwired into the architecture. And usually,

1527
02:27:13,760 --> 02:27:18,400
it's a trade off between the, for example, computational complexity availability of the

1528
02:27:18,400 --> 02:27:24,080
data also hardware friendliness. And if you think of what happened in computer vision,

1529
02:27:24,080 --> 02:27:28,560
it's probably a good illustration that that convolutional networks have translational

1530
02:27:28,560 --> 02:27:33,280
environments, for example, as you mentioned, but in many problems, you might benefit from

1531
02:27:33,280 --> 02:27:37,920
other symmetries such as rotations, again, depends on the application, but imagine that you want to

1532
02:27:38,000 --> 02:27:43,760
recognize, I don't know, traffic signs, when you can also tilt your car. And you may ask why

1533
02:27:43,760 --> 02:27:50,160
in these applications as well CNNs are still so popular. And probably one of the reasons is that

1534
02:27:50,160 --> 02:27:55,840
they met very well to the single instruction multiple data type of hardware architectures

1535
02:27:55,840 --> 02:28:02,720
that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry

1536
02:28:02,720 --> 02:28:07,440
with data augmentation, and more complex architectures and larger training sets. And

1537
02:28:07,440 --> 02:28:13,760
this is exactly what happened a decade ago, this convergence of three trends, the availability

1538
02:28:13,760 --> 02:28:19,120
of compute power, right, the GPUs, algorithms that map well to these computational architectures,

1539
02:28:19,120 --> 02:28:24,800
and these happen to be convolutional networks, and also very large data sets that you can train

1540
02:28:24,800 --> 02:28:32,960
these architectures on, such as ImageNet. So many of the choices that become popular in the

1541
02:28:32,960 --> 02:28:39,440
literature are maybe not necessarily theoretically the best ones. So I think in hardware design,

1542
02:28:39,440 --> 02:28:44,080
there is this phenomenon that is called the hardware lottery, when it's not necessarily the

1543
02:28:44,080 --> 02:28:50,240
best algorithm and the best hardware that solve the problem, it's just some lucky coincidence,

1544
02:28:50,240 --> 02:28:53,440
and they are happy marriage that makes them successful.

1545
02:28:55,120 --> 02:29:00,400
We keep raising this point about how transformers can be seen as special cases of attentional

1546
02:29:00,400 --> 02:29:07,360
GNNs, but the status quo is that people will use transformers for very many tasks nowadays,

1547
02:29:07,360 --> 02:29:12,640
and there may well be a good argument for considering them in this completely separate

1548
02:29:12,640 --> 02:29:18,080
light, and one possible explanation or justification for this is the hardware lottery, because,

1549
02:29:18,720 --> 02:29:23,440
yes, sure, transformers perform permutation equivalent operations over a complete graph,

1550
02:29:23,440 --> 02:29:29,520
but they do so in a way that is very, very highly amenable to the kind of matrix multiplication

1551
02:29:29,600 --> 02:29:36,240
routines that we can support very efficiently on GPUs nowadays. So basically, they can be seen as

1552
02:29:36,240 --> 02:29:41,680
the graph neural network that has won the current hardware lottery, even in cases where maybe it

1553
02:29:41,680 --> 02:29:46,640
will make more sense to consider a more constrained graph, especially if you have low data environments

1554
02:29:46,640 --> 02:29:53,920
or something like this, the potential overheads of running a full graph neural network solution

1555
02:29:53,920 --> 02:30:00,000
with message passing, which, given its extremely sparse nature, doesn't align that well with GPUs

1556
02:30:00,000 --> 02:30:05,600
and TPUs nowadays. Sometimes just using a complete graph neural network is the more economical option

1557
02:30:05,600 --> 02:30:09,840
when you take all factors into account. And that's, and also the fact that they use an attention

1558
02:30:09,840 --> 02:30:16,480
mechanism, which is kind of a middle ground between a simple diffusion process on a graph,

1559
02:30:16,480 --> 02:30:20,960
which we just kind of average things together based on the topology, and the full on message

1560
02:30:20,960 --> 02:30:25,920
passing where you actually compute a full on vector message to be sent across the edges.

1561
02:30:25,920 --> 02:30:31,520
Like it strikes a nice balance of scalability and still being able to represent a lot of functions

1562
02:30:31,520 --> 02:30:36,640
of interest, especially when your inputs are just word tokens, right? So like, you know, in a way,

1563
02:30:36,640 --> 02:30:42,000
it's a GNN that strikes a very nice sweet spot. And that's probably the reason why it's become so

1564
02:30:43,680 --> 02:30:48,880
popular in current times. Now, of course, there is a chance that hardware, and there's actually a

1565
02:30:48,880 --> 02:30:53,600
pretty high probability that hardware will catch up to the trends in graph representation learning,

1566
02:30:53,600 --> 02:30:59,680
and we will start to see a bit more graph oriented hardware. But at least for the time being, yeah,

1567
02:30:59,680 --> 02:31:03,760
there's a bit of a combination of what's theoretically making the most sense for your problem,

1568
02:31:04,320 --> 02:31:09,760
and what the hardware that you have right now will support the most easily. Yeah.

1569
02:31:10,320 --> 02:31:14,880
There is a British startup, I think they have reached recently a unicorn status

1570
02:31:14,880 --> 02:31:19,120
called Graphcore. And you can already hear from the name of the company,

1571
02:31:19,120 --> 02:31:24,080
that there is a graph inside, they try to develop hardware that goes beyond the traditional

1572
02:31:24,080 --> 02:31:28,640
paradigm. Yeah, I'm really interested in the hardware lottery. We had Sarah Hooker on that massive

1573
02:31:28,640 --> 02:31:35,040
shout out to Sarah. And Janik made a video on the hardware lottery paper as well. I mean,

1574
02:31:35,040 --> 02:31:39,920
I'd push back a little bit. I think there's also a bit of an optimization and an algorithmic lottery

1575
02:31:39,920 --> 02:31:44,160
going on. I think there's something very, very interesting about stochastic gradient descent

1576
02:31:44,240 --> 02:31:48,320
and the kind of data that we're working with. But this actually gets to my next question,

1577
02:31:48,320 --> 02:31:53,360
which is about why exactly geometry is a good prior and how principled it is. So

1578
02:31:53,360 --> 02:31:58,560
it seems like these geometric priors are principled. And they have utility because they are

1579
02:31:58,560 --> 02:32:03,600
low level primitives, right? They're ubiquitous and natural data. But why exactly is it a

1580
02:32:03,600 --> 02:32:07,920
principled approach to start with things we know, which is to say geometric primitives,

1581
02:32:07,920 --> 02:32:12,240
and to work upwards from there? You know, what would it look like if we went top down instead?

1582
02:32:12,240 --> 02:32:16,800
And what makes a good prior? I mean, one way to think about it is the actual function space

1583
02:32:17,440 --> 02:32:21,840
that you're searching through, you know, this hypothesis space, it's not just about being able

1584
02:32:21,840 --> 02:32:26,720
to find the function easily in that space, or the simplicity of the function you find.

1585
02:32:26,720 --> 02:32:31,600
Chalet would say it's the information conversion ratio of that function. So, you know, can you

1586
02:32:31,600 --> 02:32:36,640
use this function that you found to convert a very small piece of information and experience space

1587
02:32:36,640 --> 02:32:42,160
into new knowledge or a new ability? But how do you find these functions?

1588
02:32:43,440 --> 02:32:50,480
One of the points that we try to make in the book is the separation between the domain and

1589
02:32:50,480 --> 02:32:55,920
the group that you assume on the domain, the symmetry group. So you might have the same domain,

1590
02:32:55,920 --> 02:33:02,320
like two dimensional grid or two dimensional plane. And for example, the translation group

1591
02:33:02,320 --> 02:33:08,160
or the group of rotations and translations or the group of rigid motions that also include

1592
02:33:08,720 --> 02:33:17,840
reflections. So these are completely separate notions. And which one to choose depends on the

1593
02:33:17,840 --> 02:33:23,360
problem. The choice of the domain really comes from the structure of your data. So if your data

1594
02:33:23,360 --> 02:33:28,960
comes as an image, then of course, you use a grid to represent it as the choice of the domain.

1595
02:33:28,960 --> 02:33:34,800
Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying

1596
02:33:34,800 --> 02:33:40,560
to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road,

1597
02:33:41,120 --> 02:33:46,720
usually the signs will have certain orientation. It's very unlikely that you will see it upside

1598
02:33:46,720 --> 02:33:53,440
down. So really the only invariance or the only kind of symmetry you have is translation. So CNNs

1599
02:33:53,440 --> 02:33:59,040
in this case would work perfectly well. So for example, we have histopathological samples. So

1600
02:33:59,040 --> 02:34:04,320
you have a slice of tissue that you need to put under the microscope. So you can naturally flip

1601
02:34:04,320 --> 02:34:09,600
the glass. You don't know how it is oriented. So reflections are also initial transformation.

1602
02:34:09,600 --> 02:34:14,800
So in the traffic signs, of course, this is not physical unless you see your sign in the back

1603
02:34:14,800 --> 02:34:21,760
mirror. But in this histopathology example, it is an initial transformation. So the choice of the

1604
02:34:21,760 --> 02:34:27,920
symmetry group and what makes a good geometric prior is really dictated by the specific problem.

1605
02:34:28,480 --> 02:34:36,160
It's often very hard, though, to actually choose because we often don't really know, right, coming

1606
02:34:36,160 --> 02:34:43,200
back to what I said before, if we actually hit the group correctly. And in fact, we've sort of seen

1607
02:34:44,160 --> 02:34:49,840
the more successful approach. And this might be hardware specific, but it seems the more

1608
02:34:49,840 --> 02:34:57,200
successful approach is often to actually make data augmentations with respect to what we assume

1609
02:34:57,200 --> 02:35:04,400
are symmetries. So to know, I think of all the color distortions that we do to images, we rotate

1610
02:35:04,400 --> 02:35:10,560
them a bit, we rescale them and so on, it will be definitely possible to build architectures that

1611
02:35:10,560 --> 02:35:17,600
are just invariant to those things. However, it seems to be a more successful approach in practice

1612
02:35:17,600 --> 02:35:26,400
to put this all into the data augmentations. What's your take on that? How do we choose between

1613
02:35:26,400 --> 02:35:32,400
putting prior knowledge into augmentations versus putting prior knowledge into the architecture?

1614
02:35:33,520 --> 02:35:38,800
It's not a binary choice. It's not either your model or your augment with data.

1615
02:35:39,760 --> 02:35:44,160
One of the key principles that we also emphasize in the book is that, of course,

1616
02:35:44,160 --> 02:35:50,560
this perfect invariance or equivariance is a wishful thinking. In many cases, you want to get

1617
02:35:50,560 --> 02:35:56,800
the property that we call geometric stability. You have some transformation that is approximately

1618
02:35:56,800 --> 02:36:01,440
a group. Or imagine that you have a video where two objects are moving, let's say one car moves

1619
02:36:01,440 --> 02:36:06,400
left and another car moves right. So there is no global translation that describes the relation

1620
02:36:06,400 --> 02:36:11,040
between the two frames in this video. The geometric stability principle tells you that

1621
02:36:12,000 --> 02:36:18,480
if you are close enough to an element of the group, if you can describe these transformations

1622
02:36:18,480 --> 02:36:22,560
as an approximate translation, then you will be approximately invariant or approximately

1623
02:36:22,560 --> 02:36:29,280
equivalent. This is actually what happens in CNN. This was shown by Joan. They use this

1624
02:36:29,280 --> 02:36:34,480
motivation to explain why convolutional neural networks are so powerful. Roughly speaking,

1625
02:36:34,480 --> 02:36:39,520
if I don't have a translation, but for example, if I have an MNIS digit and you have different

1626
02:36:39,760 --> 02:36:45,600
styles of the digits, you can think of them as warping of some canonical digits. So in this case,

1627
02:36:45,600 --> 02:36:50,080
even though it's not described as a translation and the neural network will not be invariant or

1628
02:36:50,080 --> 02:36:54,800
equivariant to this kind of transformation, it will be stable under these transformations.

1629
02:36:54,800 --> 02:37:00,160
And that's why data augmentation works in some sense that you're extending your

1630
02:37:00,160 --> 02:37:03,440
invariance or equivariance class to approximate invariance and equivariance.

1631
02:37:04,240 --> 02:37:08,400
Taco also had a few interesting things to say about data augmentations versus

1632
02:37:09,440 --> 02:37:15,760
building the inductive priors into the model. This is Taco. Is your preference towards...

1633
02:37:16,880 --> 02:37:22,320
I assume it is towards creating inductive priors in the architecture around geometry

1634
02:37:22,320 --> 02:37:29,840
instead of data augmentation? Oh, that's a good question. I think it depends on the

1635
02:37:30,800 --> 02:37:37,200
on the problem. Like in some cases, you don't have a choice. So graphs are a great example.

1636
02:37:37,200 --> 02:37:42,320
The group of permutations is n factorial elements. If n is large, you have 1000 nodes,

1637
02:37:42,320 --> 02:37:48,160
you're never ever going to be able to exhaustively sample that group. And so it's better to just

1638
02:37:48,160 --> 02:37:55,040
build it in. And that's also why no graph neural net doesn't respect the symmetry. Nobody's

1639
02:37:55,040 --> 02:38:01,520
suggesting you should do that by data augmentation. In some other cases, it is

1640
02:38:04,080 --> 02:38:10,480
somewhat possible to sample a reasonably dense grid of transformations in your group.

1641
02:38:12,400 --> 02:38:21,440
And indeed, augmentation is turning out to be very important in unsupervised learning and

1642
02:38:21,440 --> 02:38:29,200
self-supervised learning techniques. So I am actually... I look very positively towards that.

1643
02:38:29,200 --> 02:38:35,680
I don't think it's wrong to put in this knowledge using data augmentation. But in some cases,

1644
02:38:35,680 --> 02:38:42,880
like let's say when you're on classifying medical data like cells in a dish, a histopathology image

1645
02:38:42,880 --> 02:38:48,240
or something, you just know for sure there's translation and rotation symmetries. The cells

1646
02:38:48,240 --> 02:38:53,760
don't have a natural orientation. And in those cases, I do think it makes sense to build it

1647
02:38:53,760 --> 02:39:01,600
into the architecture for the simple reason that if you build it into the architecture,

1648
02:39:01,600 --> 02:39:06,640
you're guaranteed that the network will be equivariant, not just at the training data,

1649
02:39:07,600 --> 02:39:13,760
but also at the test data. So you never have that the network would make the correct classification

1650
02:39:13,760 --> 02:39:18,640
for your test data when you have it in one orientation, but when you rotate it, it suddenly

1651
02:39:18,640 --> 02:39:26,640
does something different, which can happen if even when you present your training images in

1652
02:39:26,640 --> 02:39:33,680
all possible orientations. So I think for that reason, equivariance does tend to work better

1653
02:39:33,680 --> 02:39:39,280
in those cases where there's an exact symmetry in the data. We have actually demonstrated that

1654
02:39:39,280 --> 02:39:46,400
empirically, where for example, in a medical imaging problem of detecting lung nodules in

1655
02:39:46,400 --> 02:39:55,920
three dimensional CT scans, we started off with a convolutional network with a data augmentation

1656
02:39:55,920 --> 02:40:01,920
pipeline, which completely tuned what state-of-the-art method at the time. And we simply replaced

1657
02:40:01,920 --> 02:40:06,640
all the convolutions by group convolutions that respect to rotational symmetries as well as

1658
02:40:06,640 --> 02:40:13,760
the translations. And we get very significant improvement in performance. So that goes to show

1659
02:40:13,760 --> 02:40:20,320
that in practice, often data augmentation can't get you all the way. So for the cases where there's

1660
02:40:20,320 --> 02:40:27,680
an exact symmetry, or where the group of symmetries is very large, I think building it into the network

1661
02:40:27,680 --> 02:40:33,680
is the way to go for the foreseeable future. But there are many cases where augmentation is also

1662
02:40:34,640 --> 02:40:36,480
well, where augmentation is the way to go.

1663
02:40:37,440 --> 02:40:43,200
Fascinating. I'm really interested in this notion that could these symmetries actually be

1664
02:40:43,200 --> 02:40:47,680
harmful? I know Joanne, for example, spoke about the three sources of error in machine learning

1665
02:40:47,680 --> 02:40:52,880
models. And one of them is the approximation error. And normally when we get signals, they come to us

1666
02:40:52,880 --> 02:40:58,080
in a contrived form, don't know, they might be projected onto a planar manifold. And

1667
02:40:58,080 --> 02:41:06,240
you know, the sky is always up, for example. Does it really help us having these geometrics

1668
02:41:06,240 --> 02:41:12,960
symmetries as primitives in the model? Yeah, that's a good question. The simple answer is,

1669
02:41:12,960 --> 02:41:20,720
if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data,

1670
02:41:20,960 --> 02:41:27,920
building in a covariance is going to be harmful. And it's better to learn the true structure of

1671
02:41:27,920 --> 02:41:32,320
the data, this approximate symmetry, which you should be able to pick up from the data alone.

1672
02:41:34,000 --> 02:41:39,600
So that's one thing that still means in a low data regime, it can be very useful,

1673
02:41:39,600 --> 02:41:46,000
even when the symmetry is approximate. But I would also say that sometimes,

1674
02:41:46,560 --> 02:41:52,480
in machine learning, we have a tendency to put too much faith into the

1675
02:41:53,440 --> 02:41:59,920
evaluation metrics and data sets. So we say, we want to solve computer vision. And what we mean

1676
02:41:59,920 --> 02:42:05,760
is we want to get a high score on ImageNet. And certainly it's true in ImageNet, the images tend

1677
02:42:05,760 --> 02:42:12,000
to appear in upright position, and they are photographed by humans. So the key objects are

1678
02:42:12,320 --> 02:42:16,720
in the center most of the time, etc. So these are biases that you could exploit,

1679
02:42:17,280 --> 02:42:28,000
and you might stop yourself from exploiting them if you build in the symmetry. But that's only a

1680
02:42:28,000 --> 02:42:35,520
problem if you put on your blinders and you say ImageNet accuracy is the only thing that counts.

1681
02:42:36,160 --> 02:42:41,040
You might very well think, if you want to build a very general vision engine,

1682
02:42:41,040 --> 02:42:46,720
it is useful that it still works if suddenly the robot falls over and has to look at the

1683
02:42:46,720 --> 02:42:53,120
world upside down. So the symmetry can still be there in principle, even if it's not there in

1684
02:42:53,120 --> 02:43:00,800
practice in your data set. And then there's maybe a robustness versus computational efficiency

1685
02:43:00,800 --> 02:43:06,080
tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still

1686
02:43:06,080 --> 02:43:11,920
want it to work. So you want that rotation equivariance. But then again, if we don't have

1687
02:43:11,920 --> 02:43:18,560
to process the images upside down, in the 99% of cases where the images are upright,

1688
02:43:19,360 --> 02:43:24,800
we gain some computational efficiency. So there's a tradeoff there. And yeah, I don't

1689
02:43:24,800 --> 02:43:29,920
think there's a right or wrong answer. It's something you have to look at on a case by case

1690
02:43:29,920 --> 02:43:35,120
basis. When I put this to Professor Bronstein as well, he also said that you folks were looking at

1691
02:43:35,120 --> 02:43:38,560
trying to remember how he described it. I think he said there was a kind of representational

1692
02:43:38,560 --> 02:43:43,840
stability which allowed for approximate symmetries. So it's not necessarily that you're going for

1693
02:43:43,840 --> 02:43:49,360
these precise symmetries, you're actually looking for a little sort of margin of robustness going

1694
02:43:49,360 --> 02:43:52,960
to moving into approximate symmetries that you might not have explicitly captured.

1695
02:43:53,280 --> 02:44:00,960
I agree. I think that's also a very important philosophy and approach. To take the group,

1696
02:44:00,960 --> 02:44:07,040
think of it as somehow embedded in a larger group, like most of the geometrical symmetries

1697
02:44:07,040 --> 02:44:14,560
that we think about are somehow a subgroup of diffeomorphisms. And then if you say,

1698
02:44:14,560 --> 02:44:19,520
I don't want invariance or equivariance, but some kind of stability or smoothness to

1699
02:44:20,480 --> 02:44:28,880
elements in the group plus small diffeomorphisms, for instance, you might get some of the

1700
02:44:28,880 --> 02:44:36,240
generalization benefit without unduly limiting the capacity of your model.

1701
02:44:37,120 --> 02:44:44,160
Is there a hope though that we can get this approximate? Because I see your point, I think,

1702
02:44:44,240 --> 02:44:51,040
is that, or one of the points is, I think, is that if we program like a symmetry into the

1703
02:44:51,040 --> 02:44:55,600
architecture, it will be rather fixed, right? We make an architecture translation invariant,

1704
02:44:55,600 --> 02:45:01,760
it's going to be fully translation invariant. Are there good ways to bring approximate

1705
02:45:01,760 --> 02:45:06,240
invariances into the space of the architectures that we work with?

1706
02:45:06,240 --> 02:45:12,720
Yeah, I mean, I have a very quick answer, maybe not too satisfying, but one very simple one,

1707
02:45:12,720 --> 02:45:17,520
if you think of neural network blocks as like components that implement different symmetries,

1708
02:45:17,520 --> 02:45:21,200
and then you think of like a calculus of these blocks as you know, building your deep learning

1709
02:45:21,200 --> 02:45:28,400
architecture. One very simple representational tool that we can use to allow the model to use

1710
02:45:28,400 --> 02:45:34,720
the symmetry, but also not use the symmetry is the skip connection. So essentially, you could have

1711
02:45:34,720 --> 02:45:39,520
a model that processes, for example, your graph data using a particular connectivity structure

1712
02:45:39,520 --> 02:45:44,160
that you want to be invariant to. And you can also use say a transformer that processes things

1713
02:45:44,160 --> 02:45:49,280
in a completely permutation invariant way over the complete graph. And you can just shortcut the

1714
02:45:49,280 --> 02:45:54,560
results of one model over the other model, if you want to give also the model the choice to ignore

1715
02:45:54,560 --> 02:46:01,360
the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one

1716
02:46:01,360 --> 02:46:05,760
simple way in which we could do something like this. And in some of our more recent algorithmic

1717
02:46:05,760 --> 02:46:10,880
reasoning blueprint papers, we do exactly this, because one very important thing that we're trying

1718
02:46:10,880 --> 02:46:17,280
to solve is apply classical algorithms to problems that would need them. But the data is super rich,

1719
02:46:17,280 --> 02:46:22,000
and it's really hard to, you know, massage it into the abstractified form that the algorithm needs.

1720
02:46:22,000 --> 02:46:26,800
For example, you want to find shortest paths in a road network, a real world road network,

1721
02:46:26,800 --> 02:46:31,760
you cannot just take all the complexity of changing weather conditions, changing diffusion

1722
02:46:31,760 --> 02:46:36,480
patterns on the on the roads and the roadblocks and all these kinds of things and turn that into

1723
02:46:36,480 --> 02:46:41,040
this abstractified graph with exactly one scalar per each edge. So you can apply dykstra or something

1724
02:46:41,040 --> 02:46:46,160
like this, like, it's just not feasible without losing a ton of information. So what we're doing

1725
02:46:46,160 --> 02:46:51,280
here is we make this high dimensional neural network component that simulates the effects of

1726
02:46:51,280 --> 02:46:57,520
dykstra. But we're also mindful of the fact that to compute say the expected travel time,

1727
02:46:57,520 --> 02:47:02,160
there's more factors at play than just the output of a shortest path algorithm, right?

1728
02:47:02,160 --> 02:47:07,520
There could well also be some flow related elements, maybe just some elements related to the

1729
02:47:07,520 --> 02:47:13,760
current time of day, human psychology, whatnot, right? So we start off by assuming the algorithm

1730
02:47:13,760 --> 02:47:21,040
does not give the complete picture in this high dimensional noisy world. So we always, as default,

1731
02:47:21,040 --> 02:47:25,840
as part of our architecture, incorporate a skip connection from just, you know, a raw neural

1732
02:47:25,840 --> 02:47:31,280
network encoder over the algorithm. So in case there's any model free information that you want

1733
02:47:31,280 --> 02:47:36,240
to extract without looking at what the algorithm tells you, you can do that. So maybe I don't

1734
02:47:36,240 --> 02:47:40,880
know, Yannick, if that answers your question about approximate symmetries, but that's, that's the

1735
02:47:40,880 --> 02:47:45,440
kind of divide by God's when I heard the question. I mean, that's a very, that's a very practical

1736
02:47:46,000 --> 02:47:51,920
answer for sure that that, you know, you can actually get out there. It even opens the possibility

1737
02:47:51,920 --> 02:47:59,760
to having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your

1738
02:47:59,760 --> 02:48:07,120
symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe

1739
02:48:07,120 --> 02:48:11,440
another thing we've talked about, you know, there are symmetries, you want to incorporate them into

1740
02:48:11,440 --> 02:48:17,360
your problem and so on. And we've also talked about the symbolic regression beforehand to maybe

1741
02:48:18,320 --> 02:48:24,880
parse out symmetries of the underlying problem. What are the current best approaches if we don't

1742
02:48:24,880 --> 02:48:30,960
know the symmetries? So we have a bunch of data, we suspect there must be some kind of

1743
02:48:30,960 --> 02:48:37,360
symmetries at play because they're usually are in the world, right? And they, if we knew them,

1744
02:48:37,360 --> 02:48:43,440
we could describe our problems in very compact forms and solve them very efficiently, but we don't

1745
02:48:43,440 --> 02:48:50,320
often know. So what are the current state of the art? When I don't know the symmetries, how do I

1746
02:48:50,320 --> 02:48:55,920
discover what group structure is at play in a particular problem? I don't think that there is

1747
02:48:55,920 --> 02:49:01,840
a single approach that solves this problem in a satisfactory manner. And one of the reasons why

1748
02:49:01,840 --> 02:49:06,880
because the problem is ambiguous. So maybe an example, think of objects mostly translate

1749
02:49:06,880 --> 02:49:12,480
horizontally, but you also have a little bit of vertical translation. What is the right

1750
02:49:12,480 --> 02:49:16,240
symmetry structure to model? Is it a one dimensional translation group or a two dimensional

1751
02:49:16,240 --> 02:49:23,520
translation group? Do we want to absorb the vertical, the slight vertical motions as the noise

1752
02:49:23,520 --> 02:49:28,960
and deal with it as a data augmentation, or you want to describe it in the structure of the group

1753
02:49:28,960 --> 02:49:34,960
that you discover? So there is no single answer. So you cannot say that one is correct and another

1754
02:49:34,960 --> 02:49:40,240
one is wrong. Yeah, I think this was kind of where I was going with the question of how

1755
02:49:40,240 --> 02:49:46,080
principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that

1756
02:49:46,080 --> 02:49:52,320
geometries are hierarchical. You were saying that, for example, the projective geometry is kind of

1757
02:49:52,320 --> 02:49:57,360
subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large

1758
02:49:57,360 --> 02:50:03,440
data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers,

1759
02:50:03,440 --> 02:50:09,760
coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the

1760
02:50:09,760 --> 02:50:14,480
simple rule which produced this pattern. Now what kind of regularities would you look for in the

1761
02:50:14,480 --> 02:50:19,120
model that you built? I mean, it seems obvious that there would be an expanding scale symmetry,

1762
02:50:19,120 --> 02:50:24,320
which might resemble the original rule. But it feels like there'd be plenty of other emergent,

1763
02:50:24,320 --> 02:50:28,640
abstract symmetries which are not obviously related to the simple rule which produced the pattern.

1764
02:50:28,640 --> 02:50:33,920
I mean, Janik was just saying, when you look at computer vision, you see a kind of regularity

1765
02:50:33,920 --> 02:50:38,960
or invariance to color shifts, for example. So our fractals are good analogy for physical

1766
02:50:38,960 --> 02:50:43,680
reality. And should we be looking for the low-level primitive regularities which I think you're

1767
02:50:43,680 --> 02:50:48,080
advocating for? Or should we be looking at more abstract emergent symmetries which appear?

1768
02:50:49,040 --> 02:50:56,480
In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claimed really

1769
02:50:56,480 --> 02:51:02,240
unbelievable compression ratios for natural images. And the way it worked was to try to

1770
02:51:02,240 --> 02:51:07,360
reassemble the image from parts of itself. And possibly, of course, you can take parts and

1771
02:51:07,360 --> 02:51:11,760
subject them to some geometric transformation. So roughly, if you have a page of pixels,

1772
02:51:12,400 --> 02:51:17,760
you can approximate it as another page taken from somewhere else in the image that you translate,

1773
02:51:18,720 --> 02:51:24,160
rotate, and scale. And then the image was represented as an operator that makes such a

1774
02:51:24,160 --> 02:51:29,280
decomposition. And this operator was constructed in a special way to be

1775
02:51:29,280 --> 02:51:35,200
contractive. And then they used the Banach-Fix point theorem that you can apply this operator

1776
02:51:35,200 --> 02:51:40,160
to any image. So you can start with the noise for example, completely random image. And you have

1777
02:51:40,160 --> 02:51:46,960
the target image emerge after a few iterations. So that this iterative scheme will converge to the

1778
02:51:46,960 --> 02:51:52,560
fixed point of the operator, which is the image itself. And it was actually used in the industry,

1779
02:51:52,560 --> 02:51:59,440
well, Microsoft and CARTA encyclopedia. I don't know how many viewers are old enough to remember it.

1780
02:52:00,480 --> 02:52:04,960
But the main issue was the difficulty to build such operations. The compression

1781
02:52:04,960 --> 02:52:11,360
was very asymmetric. It was very easy to decode. You just take any image and apply this operator

1782
02:52:11,360 --> 02:52:16,960
multiple times. But it was really very hard to encode. And in fact, some of these constructions

1783
02:52:16,960 --> 02:52:23,120
that showed remarkable compression ratios were constructed semi by hand. I should say that in

1784
02:52:23,120 --> 02:52:27,200
more recent times in computer vision, for example, the group of Michali Rani from the Weizmann

1785
02:52:27,200 --> 02:52:33,120
Institute in Israel used similar ideas for super resolution and image denoising where you can

1786
02:52:33,120 --> 02:52:40,240
build the clean or higher resolution image from bits and pieces of the image itself. So it's a

1787
02:52:40,240 --> 02:52:48,080
single image denoising or super resolution. But what you do is you try to use similarities across

1788
02:52:48,080 --> 02:52:54,400
different positions and scales. That's absolutely fascinating. I mean, I spend a lot of time thinking

1789
02:52:54,400 --> 02:52:59,200
about this because my intuition is that deep learning works quite well because of the strict

1790
02:52:59,200 --> 02:53:04,160
structural limitations of the data which is produced by our physical world, right? And

1791
02:53:04,160 --> 02:53:10,240
would you say that physical reality is highly dimensional or not? If it's highly dimensional,

1792
02:53:10,240 --> 02:53:14,720
is it because it emerged from a simple set of rules or relations like we were just talking

1793
02:53:14,720 --> 02:53:19,840
about? Because I think what you're arguing for is that it could be collapsible in some sense.

1794
02:53:20,800 --> 02:53:26,400
Probably the term dimension is a bit frivolously used here. But I would say that it's

1795
02:53:27,360 --> 02:53:32,400
probably fair to say that at some scale, many physical systems can be described with a small

1796
02:53:32,400 --> 02:53:38,800
number of degrees of freedom, parametres that capture the system. And as we are talking,

1797
02:53:38,800 --> 02:53:43,760
I'm sitting in a room, I'm surrounded by probably a quadrillion of gas molecules in the air that

1798
02:53:43,760 --> 02:53:47,680
fly through the room and collide with each other and the walls of the room. So at the

1799
02:53:48,800 --> 02:53:55,200
microscopic level, the dimension is very high. So it's absolutely intractable if I were to model

1800
02:53:55,200 --> 02:54:00,960
each molecule and how it collides, I will have a huge number of degrees of freedom. And yet if we

1801
02:54:00,960 --> 02:54:06,800
zoom out, we can model the system statistically, and that's exactly the main idea of thermodynamics

1802
02:54:06,800 --> 02:54:14,000
and statistical mechanics. And this macroscopic system is surprisingly simple. It can be described

1803
02:54:14,000 --> 02:54:19,920
by just a few parameters such as temperature. And the example of fractals that you brought up before

1804
02:54:20,800 --> 02:54:25,920
essentially show that you can create very complex patterns with very simple rules that

1805
02:54:25,920 --> 02:54:32,640
apply locally in a repeated way. This might be a question for you, Peter. The geometric blueprint

1806
02:54:32,640 --> 02:54:38,240
works brilliantly in the ideal world where we can compute all of the possible group actions. But

1807
02:54:38,880 --> 02:54:43,520
graph neural networks, for example, you know, the permutation group is factorial in size,

1808
02:54:43,520 --> 02:54:48,800
which means we need to rely on heuristics like graph convolutions. So how much better would graph

1809
02:54:48,880 --> 02:54:53,600
neural networks be if we could compute all of the permutations? I mean, are you happy with these

1810
02:54:53,600 --> 02:55:01,360
heuristics in general? So that is a very good question. And yes, so let's just start from stating

1811
02:55:01,360 --> 02:55:07,760
the obvious. If you want to explicitly express every possible operation that properly commutes

1812
02:55:07,760 --> 02:55:13,840
with the graph structure, and in that sense is a graph convolution, you would not be able to

1813
02:55:13,840 --> 02:55:18,880
represent that properly as a neural network operation because you have to store in principle

1814
02:55:18,880 --> 02:55:24,320
a vector for every single element of the permutation group. So unless your graph is super

1815
02:55:24,320 --> 02:55:32,080
tiny, that is just not going to work. So on one hand, this is a potentially annoying result.

1816
02:55:32,080 --> 02:55:39,360
On another hand, it is also exciting because we know that even though we ended up like doing most

1817
02:55:39,360 --> 02:55:44,400
of our graph neural network research in this very restricted regime of I'm going to define a

1818
02:55:44,400 --> 02:55:49,920
permutation invariant function over my immediate neighbors, and that will as a result translate

1819
02:55:49,920 --> 02:55:55,040
into a permutation equivalent function over the whole graph. Even though most of our research has

1820
02:55:55,040 --> 02:56:01,440
happened in that particular area, we know from this result that there actually exists a huge wealth

1821
02:56:01,440 --> 02:56:09,200
of very interesting architectures beyond that. And I think one of the potentially like earliest

1822
02:56:09,200 --> 02:56:13,840
examples that have demonstrated that there exists this wealth of space is actually one of

1823
02:56:14,720 --> 02:56:21,280
Jean Bruno's earlier papers on the graph Fourier transform that, you know, analyzing from a pure

1824
02:56:21,280 --> 02:56:26,880
signal processing angle, they have shown that you can represent basically every proper graph

1825
02:56:26,880 --> 02:56:34,160
convolution as just, you know, parameterizing its eigenvalues with respect to the eigenvectors of

1826
02:56:34,160 --> 02:56:40,560
the graph Laplacian. So, but the big issue that kind of limits us from going further in this

1827
02:56:40,560 --> 02:56:46,000
direction right now is the issue that Michael highlighted of geometric stability. So basically,

1828
02:56:46,000 --> 02:56:51,280
a lot of these additional graph neural networks that do something more interesting than one

1829
02:56:51,280 --> 02:56:58,320
hop spatial message passing pay the price in being very geometrically unstable. So the graph Fourier

1830
02:56:58,320 --> 02:57:05,600
transform in its most generic form will have every single node in the graph be updated based

1831
02:57:05,600 --> 02:57:11,200
on whatever is located in any other node in the graph, very conditional on the graph topology. So

1832
02:57:11,200 --> 02:57:16,800
if you imagine any kind of approximate symmetry, any kind of perturbation either in the node features

1833
02:57:16,800 --> 02:57:22,240
or the edge structure of the graph, this, you basically don't have any protection against

1834
02:57:22,240 --> 02:57:26,880
that like that error is going to immediately propagate everywhere. And as a result, you'll

1835
02:57:26,880 --> 02:57:32,160
end up with a layer that theoretically works really well. But in practice is very unstable to

1836
02:57:32,160 --> 02:57:37,280
these kinds of numerical or inaccuracy issues. One thing that's also very important to note is that

1837
02:57:37,280 --> 02:57:41,680
often in graph neural networks, we have this subtle assumption of we have the graph and we're

1838
02:57:41,680 --> 02:57:45,440
using this graph that's given to us. But who guarantees that the graph that's given to you is

1839
02:57:45,440 --> 02:57:51,600
the correct one actually very often, we estimate these graphs based on very, very weird heuristics

1840
02:57:51,600 --> 02:57:57,600
ourselves. So basically, all of these kinds of perfection assumptions are what might limit the

1841
02:57:57,600 --> 02:58:02,320
applicability of these kinds of layers. But that being said, I find it comforting that these layers

1842
02:58:02,320 --> 02:58:07,360
exist, which means that there are meaningful ways to push our research forward to potentially

1843
02:58:07,360 --> 02:58:13,680
discover new, you know, wonderful basins of geometric stability inside these different,

1844
02:58:13,680 --> 02:58:18,160
you know, layers that may not just do one hop message passing. So that's my take on this,

1845
02:58:18,160 --> 02:58:22,400
like it gives me, it gives me faith that there's more interesting things to be discovered.

1846
02:58:22,960 --> 02:58:27,760
That being said, it is pretty tricky to find stable layers in that vast landscape.

1847
02:58:29,040 --> 02:58:32,720
Michael, do you have some thoughts on this as well? I know you've worked quite a bit on these

1848
02:58:32,720 --> 02:58:40,400
geometric stability aspects. I just wanted to add one thought about it that essentially,

1849
02:58:40,400 --> 02:58:46,240
locality is a feature, not a bug in many situations. And in convolutional neural networks,

1850
02:58:46,240 --> 02:58:51,600
actually, if you look again, historically, the early architectures like AlexNet, they started

1851
02:58:51,600 --> 02:58:55,680
with very large filters and the few layers or relatively few layers, I think something like

1852
02:58:55,680 --> 02:59:01,680
five or six. And nowadays, what you see is very small filters and hundreds of layers.

1853
02:59:01,680 --> 02:59:06,400
One of the reasons why you can do it is because of compositionality properties. So you can,

1854
02:59:06,400 --> 02:59:12,960
you can create complex features from, from simple primitives. So in some other cases,

1855
02:59:12,960 --> 02:59:19,920
like, like manifolds, there are deeper geometric considerations why you must be local, so related

1856
02:59:19,920 --> 02:59:26,320
to what is called the injectivity radius of the manifold. On graphs, well, maybe we like a little

1857
02:59:26,320 --> 02:59:32,880
bit the theoretical necessity to be local, besides, of course, the computational complexity. But

1858
02:59:33,520 --> 02:59:38,960
in many cases, it is actually a good property because many problems do not really depend on

1859
02:59:38,960 --> 02:59:44,800
distant interactions. So if you think of social networks, probably most of the information comes

1860
02:59:44,800 --> 02:59:50,880
from your immediate neighbors. Is there some sort of, let's assume I, you know, I have a graph,

1861
02:59:51,680 --> 03:00:00,640
and I have my, my symmetries, my groups that I suspect there are in the problem, or I want to

1862
03:00:00,640 --> 03:00:07,680
be invariant to, is there like, can you give us a bit of a practical blueprint of how would I build

1863
03:00:07,760 --> 03:00:15,440
a network that, you know, takes this as an input and applies this? How would you go about this,

1864
03:00:15,440 --> 03:00:20,560
you know, what would be the building blocks that you choose, the orders and so on? Is there

1865
03:00:20,560 --> 03:00:25,760
overarching principles in, in how to do? I don't think that there is really a general

1866
03:00:25,760 --> 03:00:31,360
recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic

1867
03:00:31,360 --> 03:00:35,920
structure that they have in graph is a privatization invariance. This has to do with the

1868
03:00:35,920 --> 03:00:40,240
structure of the graph itself. It says nothing about the structure of the features.

1869
03:00:40,240 --> 03:00:45,760
You might also have some secondary symmetry structure in the feature space. In case of molecules,

1870
03:00:45,760 --> 03:00:50,800
for example, you might have a combination of features. Some of them are geometric. So it's

1871
03:00:50,800 --> 03:00:55,920
actually not an abstract topological graph. It's a geometric graph. A molecule is a graph that

1872
03:00:55,920 --> 03:01:01,200
lives in three dimensional space. And so the features are the positional coordinates of the

1873
03:01:01,200 --> 03:01:05,760
nodes, as well as some chemical properties such as atomic numbers. Now, when you deal with the

1874
03:01:05,760 --> 03:01:10,160
molecule, you usually don't care about how it is positioned in space. It wants to be

1875
03:01:10,160 --> 03:01:16,160
equivariant to rigid transformations. And therefore you need to treat accordingly the

1876
03:01:16,160 --> 03:01:20,480
geometric coordinates of the nodes of this graph. And this is actually what has been successfully

1877
03:01:20,480 --> 03:01:26,000
done. So when you do, for example, virtual drag screening, architectures that do message passing,

1878
03:01:26,000 --> 03:01:31,040
but in a way that is equivariant to these rigid transformations actually are more successful

1879
03:01:31,040 --> 03:01:36,320
than generic graph neural networks. Also, this principle was exploited in the recent version

1880
03:01:36,320 --> 03:01:42,720
of AlphaFold, where I think they call it point invariant attention, which is essentially a form

1881
03:01:42,720 --> 03:01:50,240
of a latent graph neural network or a transformer architecture with equivariant message passing.

1882
03:01:50,240 --> 03:01:55,200
Yeah, I'd just like to add one more point to this conversation, which is maybe a bit more

1883
03:01:55,200 --> 03:02:02,320
philosophical, but it relates to this aspect of building the overarching symmetry discovering

1884
03:02:02,960 --> 03:02:08,720
procedures, which I think would be a really fantastic thing to have in general. And I hope that

1885
03:02:09,440 --> 03:02:16,400
some component of a true AGI is going to be figuring out, making sense of the data you're

1886
03:02:16,400 --> 03:02:22,880
receiving and figuring out what's the right symmetry to bake into it. I don't necessarily

1887
03:02:22,880 --> 03:02:29,920
have a good opinion on what this model might look like. But what I do say is just looking at the

1888
03:02:29,920 --> 03:02:36,080
immediate utility of the geometric deep learning blueprint, we are like, I think very strictly

1889
03:02:36,080 --> 03:02:42,080
saying that we don't want to use this blueprint to propose, you know, the one true architecture.

1890
03:02:43,520 --> 03:02:49,120
Rather, we make the argument that different problems require different specifications,

1891
03:02:49,120 --> 03:02:54,560
and we provide a common language that will allow, say, someone who works on primarily grid data to

1892
03:02:54,560 --> 03:02:59,040
speak with someone who works on manifold data without necessarily thinking that, you know,

1893
03:02:59,920 --> 03:03:04,480
you know, somebody might say, commonets are the ultimate architecture. Someone else might say

1894
03:03:04,480 --> 03:03:08,880
GNNs are the ultimate architecture. And in some ways, they could both be right and they could

1895
03:03:08,880 --> 03:03:14,960
both be wrong. But this blueprint kind of just provides a clear delimiting aspect to these

1896
03:03:14,960 --> 03:03:20,880
things, just like in the 1800s, you had all these different types of geometries that basically lived

1897
03:03:20,880 --> 03:03:27,520
on completely different kinds of geometric objects, right? So hyperbolic, elliptic, and so on and so

1898
03:03:27,520 --> 03:03:33,760
forth. And what Klein-Zerlangen program allowed us to do was, among other things, reason about

1899
03:03:33,760 --> 03:03:38,640
all of these geometries using the same language of group invariance and symmetries, right? But in

1900
03:03:38,640 --> 03:03:44,560
principle, the specifics of whether you want to use a hyperbolic geometry or whether you want to use

1901
03:03:44,560 --> 03:03:49,840
an elliptic geometry, partly rests on your assumption what the main do you actually live in,

1902
03:03:49,840 --> 03:03:56,160
right? When you're doing these computations. So I think just generally speaking, I think that

1903
03:03:56,160 --> 03:04:01,280
having a divide is a potentially useful thing, as long as you have a language that you can use

1904
03:04:01,280 --> 03:04:08,240
to index into this divide, if that makes sense. It does make sense. But I have a feeling that some

1905
03:04:08,240 --> 03:04:14,000
people could benefit from geometric deep learning in their runaways. I mean, I don't want you guys

1906
03:04:14,000 --> 03:04:18,800
to motivate geometric deep learning in general, because I think, you know, a lot of deep learning

1907
03:04:18,800 --> 03:04:23,120
with a structured prior is already geometric deep learning is as you folks demonstrated in

1908
03:04:23,120 --> 03:04:27,520
your blueprint, you know, like RNNs and CNNs, for example. So, you know, like it or not, we're already

1909
03:04:27,520 --> 03:04:32,160
all using geometric deep learning. But some of the esoteric flavors of geometric deep learning,

1910
03:04:32,160 --> 03:04:36,720
particularly on irregular meshes, they seem a little bit out there, don't they? I mean, it's

1911
03:04:36,720 --> 03:04:40,640
possible that many people could benefit from this, but they just don't know about it yet. I was

1912
03:04:40,640 --> 03:04:45,760
thinking that, for example, if I had a LiDAR scanner on my phone, and the result is a point cloud,

1913
03:04:45,760 --> 03:04:51,360
which is not particularly useful. But I would presumably transform it into a mesh, which would

1914
03:04:51,360 --> 03:04:55,520
be more useful. But is it possible that loads of data scientists out there are sitting on data sets

1915
03:04:55,520 --> 03:04:59,920
that they could be thinking about geometrically, but they're not? Many folks are exotic. It's

1916
03:04:59,920 --> 03:05:04,560
probably in the eyes of the beholder. And well, in machine learning, probably they are, to some

1917
03:05:04,560 --> 03:05:11,120
extent, exotic. But joking apart, many folks are a convenient model for all sorts of data.

1918
03:05:11,680 --> 03:05:16,400
And the data might be a high dimensional, but still have a low intrinsic dimensionality or

1919
03:05:16,400 --> 03:05:21,440
can be explained by a small number of parameters or degrees of freedom. And this is really the premise

1920
03:05:21,440 --> 03:05:27,360
of nonlinear dimensionality reduction. And for example, the reasons why data visualization

1921
03:05:27,360 --> 03:05:33,040
techniques such as TSE and E at all work. And maybe the key question, as you're asking is,

1922
03:05:33,120 --> 03:05:38,320
how much of the manifold structure of the continuous manifold you actually leverage? And

1923
03:05:38,320 --> 03:05:43,200
in the TSE example, the only structure that you really use is local distances.

1924
03:05:44,160 --> 03:05:50,000
So if you think of a point cloud, of course, you can deal with it as a set. But if you assume

1925
03:05:50,000 --> 03:05:54,560
that it comes from sampling of some continuous surface, you can probably say more. And this is

1926
03:05:54,560 --> 03:06:00,480
forgivable what we tried to do in some of our works on geometric deep learning in applications

1927
03:06:00,480 --> 03:06:06,880
in computer vision and graphics. And measures are one way of thinking of them is as graphs and

1928
03:06:06,880 --> 03:06:11,520
steroids, where we have additional structure to leverage. So it's not only nodes and edges,

1929
03:06:11,520 --> 03:06:16,160
but also faces. And in fact, measures are what is called simplicial complexes.

1930
03:06:16,800 --> 03:06:22,480
As to the practical usefulness, computer vision and graphics are obviously the two fields where

1931
03:06:23,040 --> 03:06:27,760
geometric deep learning on measures is important. And just to give a recent example

1932
03:06:27,760 --> 03:06:34,560
of a commercial success. There was a British startup called the AI. It was founded by

1933
03:06:34,560 --> 03:06:39,360
my colleague and friend, Yasunos Kokinos. I was also one of the investors. And we had a

1934
03:06:39,360 --> 03:06:44,720
collaboration on three different reconstruction using geometric neural networks. And the company

1935
03:06:44,720 --> 03:06:50,960
was acquired last year by SNAP and these technologies already now part of SNAP products. So you see it

1936
03:06:51,040 --> 03:06:58,400
in the form of some 3D avatars or virtual and augmented reality applications that SNAP is

1937
03:06:58,400 --> 03:07:03,760
developed. Professor Bronstein, I saw that you were doing some really cool stuff with the higher

1938
03:07:03,760 --> 03:07:09,200
order simplicial coverings in graphs. And actually, I was going to call out your recent work on

1939
03:07:09,200 --> 03:07:15,120
diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to

1940
03:07:15,120 --> 03:07:20,000
actually enable a little of this analysis. But there was a question from my good friend,

1941
03:07:20,080 --> 03:07:25,680
Zach Jost, who's one of our staff members here. And he says, what do you think is the most important

1942
03:07:25,680 --> 03:07:29,920
problem to solve with message passing graph neural networks? And what's the most promising path

1943
03:07:29,920 --> 03:07:35,120
forward? Probably we first need to agree about terminology. And to me, message passing and

1944
03:07:35,120 --> 03:07:42,560
here I agree with Petra is just a very generic mechanism for propagating information on the

1945
03:07:42,560 --> 03:07:47,760
graph. Now, traditional message passing that is used in graph neural networks uses the input

1946
03:07:47,760 --> 03:07:52,800
graph for this propagation. And we know, right, as we discussed that it is equivalent to device

1947
03:07:52,800 --> 03:07:59,200
for a lemon graph isomorphism test that has limitations in the kinds of structures it can

1948
03:07:59,200 --> 03:08:05,120
detect. Now, there exists topological constructions that go beyond graphs, such as simplicial and

1949
03:08:05,120 --> 03:08:10,720
cell complexes that you mentioned. And what we did in our recent works is developing a message

1950
03:08:10,720 --> 03:08:16,320
passing mechanism that is able to work on such structures. And of course, you may ask whether

1951
03:08:16,320 --> 03:08:22,720
we do encounter such structures in real life. So first of all, we do the measures that I mentioned

1952
03:08:22,720 --> 03:08:28,320
before are in fact, simplicial complexes. But secondly, what we show in the paper is that we

1953
03:08:28,320 --> 03:08:32,960
can take a traditional graph and lift it into a cell or simplicial complex. And probably a good

1954
03:08:32,960 --> 03:08:38,640
example here is from the domain of computational chemistry, the graph neural network that you

1955
03:08:38,640 --> 03:08:43,040
apply to a molecular graph would consider a molecule just as a collection of nodes and edges,

1956
03:08:43,040 --> 03:08:48,080
atoms, and chemical bonds between them. But this is not how chemists think of molecules.

1957
03:08:48,080 --> 03:08:53,760
They think of them as structures such as, for example, aromatic rings. And with our approach,

1958
03:08:53,760 --> 03:08:59,440
we can regard the rings as cells. So we have a special new object, and we can do a different

1959
03:08:59,440 --> 03:09:03,920
form of message passing on them. And we can also show from the theoretical perspective that

1960
03:09:04,640 --> 03:09:08,640
this kind of message passing is strictly more powerful than the vice for a lemon algorithm.

1961
03:09:09,280 --> 03:09:16,320
Do you see entirely new problems opening up that we wouldn't even have, let's say,

1962
03:09:16,320 --> 03:09:22,960
we wouldn't even have dared to touch before, you know, in, let's say, we simply have our

1963
03:09:22,960 --> 03:09:27,920
classic neural networks or whatnot, or even our classic graph message passing algorithms.

1964
03:09:27,920 --> 03:09:35,120
Do you see new problems that are now in reach that previously with none of these methods were

1965
03:09:35,120 --> 03:09:41,680
really, let's say, better than random guessing? It's a very interesting question that I think

1966
03:09:41,680 --> 03:09:47,040
I'll answer from two angles, because you could, like, there could be like some

1967
03:09:47,040 --> 03:09:52,560
longstanding problem that you knew about and wouldn't dare to attack. And now maybe you feel

1968
03:09:52,560 --> 03:09:57,120
a bit more confident to attack it. There's also the aspect of uncovering a problem,

1969
03:09:57,120 --> 03:10:01,680
because when you start thinking about things in this particular way, you might realize,

1970
03:10:01,680 --> 03:10:05,840
hang on, to make this work, I made some assumptions. And those assumptions actually

1971
03:10:05,840 --> 03:10:10,160
don't really hold at all in principle. So how do I make things, you know, a little bit better?

1972
03:10:10,160 --> 03:10:16,960
So I'll try to give an example for both of those. So in terms of a problem that previously,

1973
03:10:16,960 --> 03:10:23,040
I don't think was very easy to attack. And now we might have some tools that could help us attack

1974
03:10:23,040 --> 03:10:30,080
it better. I have a longstanding interest in reinforcement learning. Actually, when I

1975
03:10:30,160 --> 03:10:36,000
started my PhD, I spent six months attacking a reinforcement learning problem with one super

1976
03:10:36,000 --> 03:10:42,320
tiny GPU. And that was, at that time, a massive time sink. Actually, DeepMind ended up scooping

1977
03:10:42,320 --> 03:10:47,920
my work sometime after that. And I quickly moved to things that were more, you know, doable with

1978
03:10:47,920 --> 03:10:52,800
the kind of hardware that I had at the time. But, you know, I always had a big interest in this area.

1979
03:10:52,800 --> 03:10:58,320
And after joining DeepMind, I started to contribute to these kinds of directions more and more.

1980
03:10:58,960 --> 03:11:07,760
And I think that basically, there are a lot of problems in reinforcement learning

1981
03:11:08,560 --> 03:11:15,920
concerning data efficiency. So when you have to learn how to meaningfully act and do stuff,

1982
03:11:15,920 --> 03:11:21,200
which is actually a fairly like low dimensional signal compared to the potential richness of

1983
03:11:21,200 --> 03:11:25,760
the trajectories that you have to go through before you get that useful signal, like long

1984
03:11:25,760 --> 03:11:31,760
term credit assignment, all these kinds of problems, I feel like we can start to get more

1985
03:11:31,760 --> 03:11:37,520
data efficient reinforcement learning architectures by leveraging geometric concepts and also

1986
03:11:37,520 --> 03:11:44,720
algorithmic concepts. So to give you one example of this, we have some months ago put out a paper

1987
03:11:44,720 --> 03:11:50,880
on the archive called the executed latent value iteration network or x selvin, where we have

1988
03:11:51,200 --> 03:11:58,640
captured the essence of an algorithm in RL, which perfectly solves the RL problem. So the value

1989
03:11:58,640 --> 03:12:03,040
iteration algorithm, assuming you give it a Markov decision process will give you the perfect

1990
03:12:03,040 --> 03:12:07,600
policy for that Markov decision process. So it's a super attractive algorithm to think about when

1991
03:12:07,600 --> 03:12:13,360
you do RL, big caveat, right? You need to know all the dynamics about your environment, and you need

1992
03:12:13,360 --> 03:12:18,320
to know all the reward models of the environment before you can apply the algorithm. So this obviously

1993
03:12:18,400 --> 03:12:23,760
limits its use in the more generic deep reinforcement learning setting. But now with the knowledge of

1994
03:12:24,320 --> 03:12:29,120
the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning

1995
03:12:29,120 --> 03:12:33,360
blueprint, we actually taught the graph neural network, which aligns super well with value

1996
03:12:33,360 --> 03:12:39,200
iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way

1997
03:12:39,840 --> 03:12:45,280
on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation,

1998
03:12:45,360 --> 03:12:49,920
and then we stitched it into a planning algorithm in a deep reinforcement learning setting.

1999
03:12:49,920 --> 03:12:54,880
And just by like training this pipeline end to end with a model free loss, we were able to

2000
03:12:55,520 --> 03:13:01,600
get interesting returns in Atari games much sooner than some of the competing approaches. So

2001
03:13:01,600 --> 03:13:06,640
it's a very small step. It still requires, you know, 100,000 200,000 iterations of

2002
03:13:07,360 --> 03:13:11,440
playing before you get meaning, some meaningful behaviors start to come out. But

2003
03:13:12,080 --> 03:13:17,120
it's a sign that we might be able to move the needle a bit backwards and not require, you know,

2004
03:13:17,120 --> 03:13:22,160
billions and billions of transitions before we start to see meaningful behavior emerge. And I

2005
03:13:22,160 --> 03:13:26,160
think that's very important because in most real world applications of reinforcement learning,

2006
03:13:26,160 --> 03:13:31,040
you don't have a budget for billions of interactions before you have to already learn a

2007
03:13:31,040 --> 03:13:35,440
meaningful policy. So that's one side, I think. And just generally in reinforcement learning,

2008
03:13:35,440 --> 03:13:40,560
graphs appear left, right and center, not just in the algorithms, but also in the structure of the

2009
03:13:40,560 --> 03:13:44,720
environment and these kinds of things. So I think that's one area where geometric deep learning

2010
03:13:44,720 --> 03:13:49,680
could really help us, you know, get better behaviors faster, not necessarily solve it

2011
03:13:49,680 --> 03:13:54,560
better than the standard deep RL, but, you know, get the better behaviors and fewer interactions.

2012
03:13:54,560 --> 03:14:00,240
And as for one problem that we have uncovered through this kind of observational lens,

2013
03:14:01,280 --> 03:14:05,600
you know, as I said, often in graph representation learning, we assume innocently that the graph

2014
03:14:05,600 --> 03:14:13,280
is given to us, whereas very often this is not the case. So this divide has brought about this new

2015
03:14:13,280 --> 03:14:18,480
emerging area of latent graph learning or latent graph inference, where the objective is to learn

2016
03:14:18,480 --> 03:14:24,800
the graph simultaneously with using it for your underlying decision problem. And this is a big

2017
03:14:24,800 --> 03:14:28,800
issue for neural network optimization, because you're fundamentally making a discrete decision in

2018
03:14:28,800 --> 03:14:33,840
there. And if the number of nodes is huge, you cannot afford to start with the n squared approach

2019
03:14:33,920 --> 03:14:39,280
and then gradually refine it. So currently, the state of the art in many regards of what we have

2020
03:14:39,280 --> 03:14:44,720
here is to do a K nearest neighbor graph in the feature space, and just hope that that gets us

2021
03:14:44,720 --> 03:14:50,000
most of the way there. And usually this works quite well for getting, you know, interesting answers,

2022
03:14:50,000 --> 03:14:54,400
because, you know, if you have a decent ish enough KNN graph, you will cover everything that you need

2023
03:14:54,400 --> 03:15:00,320
reasonably quickly. But, you know, then there that raises the issue of what if the graph itself is a

2024
03:15:00,320 --> 03:15:04,640
meaningful output of your problem, what if you're a causality researcher that wants to figure out

2025
03:15:04,640 --> 03:15:09,040
how different, you know, parts of information interact to them, they probably wouldn't be

2026
03:15:09,040 --> 03:15:13,920
satisfied with a K nearest neighbor graph as an output of the system. So yeah, I feel like there's

2027
03:15:13,920 --> 03:15:19,920
a lot of work to be done to actually scalably and usefully do something like this. And I don't

2028
03:15:19,920 --> 03:15:24,240
have a better answer than what I just said is the state of the art. So a potential open problem

2029
03:15:24,240 --> 03:15:28,160
for everybody in the audience today. Absolutely. And you touch on some really interesting things

2030
03:15:28,160 --> 03:15:33,120
that I think causality is a huge area that we could be looking at graphs on. And also we had

2031
03:15:33,120 --> 03:15:38,480
Dr. Tom Zahavi from DeepMind, one of your colleagues, and he said that, you know, he looks at

2032
03:15:38,480 --> 03:15:43,280
meta learning and also diversity preservation in in agent based learning. But he thinks that

2033
03:15:43,280 --> 03:15:47,440
reinforcement learning is just about to have its image net moment where we can discover

2034
03:15:47,440 --> 03:15:51,760
a lot of the structure in these problems, which is fascinating. I would like to bring up

2035
03:15:52,800 --> 03:15:57,680
one application where maybe quantitative improvement that is afforded by a genetic

2036
03:15:57,680 --> 03:16:02,480
deep learning can lead to a qualitative breakthrough. And this is a problem of

2037
03:16:03,120 --> 03:16:08,560
structural biology. Alpha fold is one such example for correctly geometrically modeling

2038
03:16:09,200 --> 03:16:14,480
the problem you get a breakthrough in the performance. So it's indeed an image net

2039
03:16:14,480 --> 03:16:19,840
moment that happened in this field. And now once you have sufficiently accurate

2040
03:16:20,720 --> 03:16:25,200
prediction of 3D structure of proteins, it suddenly enables a lot of interesting applications

2041
03:16:25,200 --> 03:16:30,080
for example, in the field of drug design. So potentially entire pharmaceutical

2042
03:16:30,080 --> 03:16:36,080
pipelines we invented with the use of this technology. And the impact can be extraordinary.

2043
03:16:36,800 --> 03:16:40,720
It's so true. I mean, Professor Bronstein, when I was watching your lecture series,

2044
03:16:40,720 --> 03:16:44,800
it blew me away when you were talking about all of the applications. I think Yannick said a minute

2045
03:16:44,800 --> 03:16:50,960
ago that it's almost as if some of these applications are just so ambitious that the thought

2046
03:16:50,960 --> 03:16:54,560
wouldn't even have crossed our mind that we might be able to do it before, like for example,

2047
03:16:54,560 --> 03:17:00,320
being able to predict facial geometry from a DNA sequence. So we might be able to look at an old

2048
03:17:00,320 --> 03:17:05,120
DNA sequence and actually see what that person looked like. That would have been unimaginable

2049
03:17:05,120 --> 03:17:10,480
just a few years ago. So that's incredible. I'm really interested in your definition

2050
03:17:10,480 --> 03:17:14,000
of intelligence, right? And whether you think neural networks could ever be intelligent.

2051
03:17:14,960 --> 03:17:19,440
Douglas Hofstadter, for example, he wrote the famous book Godel Escher Bach. It was a

2052
03:17:19,440 --> 03:17:25,360
Pulitzer Prize winning book in the 1970s, but he made the argument that analogy is the core of

2053
03:17:25,360 --> 03:17:30,880
cognition. He said that analogies are a bit like the interstate freeway of cognition.

2054
03:17:30,880 --> 03:17:35,840
They're not little modules on the side or something like that. And I think that in a way,

2055
03:17:35,840 --> 03:17:42,080
analogies are also symmetries, right? So when I say that someone is firewalling a person,

2056
03:17:42,720 --> 03:17:47,200
it means that they don't want to talk with that person. It's a symmetry between the abstract

2057
03:17:47,280 --> 03:17:52,560
representation of a real network firewall and an abstract social category.

2058
03:17:52,560 --> 03:17:58,640
So does this require a different neural network architecture or could geometric deep learning

2059
03:17:58,640 --> 03:18:02,800
already deliver the goods, right? It's almost as if it's just a representation problem.

2060
03:18:02,800 --> 03:18:09,120
I think it's a very important question, one which I cannot claim to have the right answer to. And

2061
03:18:09,120 --> 03:18:15,200
my definition is I guess a little bit skewed by the specific research that I do and the

2062
03:18:15,200 --> 03:18:21,200
engineering approaches that I do. But I think in large, I agree with the idea of analogy making,

2063
03:18:21,200 --> 03:18:26,800
and maybe I would take it a step further, right? Where you have a particular set of

2064
03:18:26,800 --> 03:18:32,560
knowledge and conclusions that you've derived so far, a set of primitives that you can use once

2065
03:18:32,560 --> 03:18:37,280
your information comes in to figure out how to recompose them and either discover new analogies

2066
03:18:37,280 --> 03:18:41,920
or just discover new conclusions that you can use in the next step of reasoning.

2067
03:18:42,560 --> 03:18:50,320
And it just feels really amenable to a kind of synergy of, as Daniel Kahneman puts it,

2068
03:18:50,320 --> 03:18:55,680
System 1 and System 2, right? You have the perceptive component that feeds in the raw

2069
03:18:55,680 --> 03:19:01,040
information that you receive as your input data, transforms it into some kind of abstract

2070
03:19:01,040 --> 03:19:07,280
conceptual information. And then in the System 2 land, you have access to this kind of reasoning

2071
03:19:07,280 --> 03:19:13,280
procedures that are able to take all of these concepts and derive new ones from hopefully a

2072
03:19:13,280 --> 03:19:20,320
nice and not very high dimensional set of roles. And this is why I believe that if we, that in

2073
03:19:20,320 --> 03:19:26,240
terms of like moving towards the, an architecture that supports something like this, I think we

2074
03:19:26,240 --> 03:19:30,000
have a lot of the building blocks in place with geometric deep learning, especially if we're

2075
03:19:30,000 --> 03:19:34,080
willing to, as I said, kind of broaden the definition of geometric deep learning to also

2076
03:19:34,080 --> 03:19:39,200
include category theory concepts because that might allow us to reconcile algorithmic computation

2077
03:19:39,200 --> 03:19:44,800
as well into the blueprint. So the idea is, you know, you have this, I mean, there's no need to

2078
03:19:44,800 --> 03:19:49,760
talk at length about all these great perceptive architectures. So I think we're already at a

2079
03:19:49,760 --> 03:19:55,840
point of, if we show our AGI lots and lots of data, it's going to be able to pick up on a lot of

2080
03:19:55,840 --> 03:19:59,280
the interesting things just by observing, like, you know, self-supervised learning,

2081
03:19:59,280 --> 03:20:03,680
unsupervised learning is already showing a lot of promise there. But then the question is,

2082
03:20:03,680 --> 03:20:08,400
where I think we still have quite a bit of work to do is once we have these concepts,

2083
03:20:08,400 --> 03:20:13,040
let's even assume that they're perfect. What do we do with them? How do we meaningfully use them?

2084
03:20:13,040 --> 03:20:17,760
And I think the reason why I believe there's a lot of work to be done there is because one of the

2085
03:20:18,320 --> 03:20:23,440
very key things that I do on a day to day basis is teach graph neural networks to imitate algorithms

2086
03:20:23,440 --> 03:20:28,320
from perfect data. So I give them exactly the abstract input that the algorithm would expect.

2087
03:20:28,320 --> 03:20:34,800
And I ask them, hey, simulate this algorithm for me, please. And it turns out that that is super,

2088
03:20:34,800 --> 03:20:39,280
super hard. Well, it's super easy to do it in distribution, but you're not algorithmic if you

2089
03:20:39,280 --> 03:20:44,400
don't extrapolate. And that's, I think, one of the big challenges that we need to work towards

2090
03:20:44,400 --> 03:20:50,160
addressing. Will geometric deep learning be enough to encompass the ultimate solution? I have a

2091
03:20:50,160 --> 03:20:55,920
feeling that it will. But, you know, I don't necessarily just based on the empirical evidence

2092
03:20:55,920 --> 03:21:02,080
we've been seeing in the recent papers that we've put out. But yeah, I don't I don't have a very

2093
03:21:02,080 --> 03:21:08,960
strong theoretical reason why I think it's going to be enough. Yeah, I'm fascinated by this notion

2094
03:21:08,960 --> 03:21:15,120
that intelligence isn't mysterious as we might think it is. It's a it's a receding horizon. And

2095
03:21:15,120 --> 03:21:22,480
it might in the end be disappointingly simple to mechanize. Actually, if you take the term literally,

2096
03:21:22,480 --> 03:21:28,160
intelligence come from the Latin word that means to understand. And I think what is meant by

2097
03:21:28,160 --> 03:21:32,160
understanding is really a very vague question. And probably if you ask different people, they

2098
03:21:32,160 --> 03:21:38,160
will give you different definitions. I will define it as the faculty to abstract information.

2099
03:21:38,800 --> 03:21:43,280
And in particular, information that is obtained in one context, the ability to use it in other

2100
03:21:43,280 --> 03:21:49,120
contexts, this is what we usually call learning. The way that this information is abstracted and

2101
03:21:49,120 --> 03:21:54,000
represented might be very different in a biological neural network in our brain versus an

2102
03:21:54,000 --> 03:22:01,520
artificial intelligence system. So if you hear some people saying that that the brain probably

2103
03:22:01,520 --> 03:22:08,640
doesn't really do geometric computations, my answer to that would be that we don't necessarily need

2104
03:22:08,640 --> 03:22:15,760
to imitate exactly the way that the brain works. We just need probably to try to achieve this high

2105
03:22:15,760 --> 03:22:21,600
level mechanism that is able to abstract information and applied as knowledge to different

2106
03:22:21,600 --> 03:22:28,000
problems. The definitions of artificial intelligence that are being used, like the famous Turing test,

2107
03:22:28,000 --> 03:22:34,560
I find is very disturbing that it's very anthropocentric. And it is actually probably very

2108
03:22:34,560 --> 03:22:42,240
characteristic of the human species more broadly. And this way, by judging what is intelligent,

2109
03:22:42,640 --> 03:22:47,760
what is not, we might potentially rule out other intelligent species because they are very different

2110
03:22:47,760 --> 03:22:53,040
from us. I may be obsessed with sperm whales because I'm working on studying their communication.

2111
03:22:53,760 --> 03:22:57,360
They are definitely intelligent creatures, but would they pass the Turing test?

2112
03:22:57,920 --> 03:23:04,480
Probably not. It's like subjecting a cat to a swimming test or a fish to climbing on a tree.

2113
03:23:05,360 --> 03:23:12,080
So I would just like to add to Michael's great answer, one quote that I think is very popular

2114
03:23:12,080 --> 03:23:17,360
and applies really well in this setting. The question of whether computers can think is about

2115
03:23:17,360 --> 03:23:23,360
as relevant as whether submarines can swim. When you built submarines, you weren't necessarily

2116
03:23:23,360 --> 03:23:27,680
trying to copy fish. You were solving a problem that was fundamentally slightly different.

2117
03:23:28,320 --> 03:23:30,960
So could be relevant in this case as well.

2118
03:23:31,760 --> 03:23:35,360
We spend quite a lot of time on this podcast talking about whether we should have an

2119
03:23:35,360 --> 03:23:40,320
anthropocentric conception of intelligence. A corporation is intelligent.

2120
03:23:42,080 --> 03:23:45,840
I'm starting to come around to the view of embodiment and thinking that there is something

2121
03:23:45,840 --> 03:23:50,800
very human like about our particular flavour of intelligence. But maybe there is a kind of pure

2122
03:23:50,800 --> 03:23:56,320
intelligence as well. And this was the end of my conversation with Tako Kohen. One of the really

2123
03:23:56,400 --> 03:24:03,840
interesting things is you're getting on to some of the work that you've done are being able to

2124
03:24:03,840 --> 03:24:10,800
think of group convolutions on homogeneous objects like spheres, for example, but also you moved on

2125
03:24:10,800 --> 03:24:20,080
to irregular objects like any mesh and you looked into things like fibre bundles and local convolutions.

2126
03:24:20,080 --> 03:24:24,080
Because these are objects, I think you said a homogeneous object is where you can't

2127
03:24:24,080 --> 03:24:28,240
perform some transformation to get from one place of the object to the other part of the object.

2128
03:24:28,240 --> 03:24:30,880
So what work did you do there on those irregular objects?

2129
03:24:32,000 --> 03:24:36,880
Yeah, that's a great question. So when we think of convolution, we're sort of

2130
03:24:36,880 --> 03:24:43,200
putting together a whole bunch of things, namely this idea of locality. So typically our filters

2131
03:24:43,200 --> 03:24:50,000
are local, but that's a choice ultimately. Convolution doesn't have to use a local filter,

2132
03:24:50,080 --> 03:24:56,240
though in practice we know that works very well. And there's the idea of weight sharing between

2133
03:24:56,240 --> 03:25:00,960
different positions and potentially also between different orientations of the filter.

2134
03:25:01,680 --> 03:25:08,800
And as I mentioned before, this weight sharing really comes from the symmetry.

2135
03:25:09,920 --> 03:25:15,680
So the fact that you use the same filter at each position in your image when you're doing

2136
03:25:15,680 --> 03:25:22,400
a two-dimensional convolution, that's because you want to respect the translation symmetry acting

2137
03:25:22,400 --> 03:25:30,560
on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry.

2138
03:25:31,200 --> 03:25:37,840
If you think of a mesh representing a human figure, or let's say it's some complicated protein

2139
03:25:37,840 --> 03:25:45,200
structure, there may not be a global symmetry. Or sometimes in the case of say a molecule might

2140
03:25:45,200 --> 03:25:51,840
have some six-fold rotational symmetry, this symmetry may not be transitive as it's called,

2141
03:25:51,840 --> 03:25:57,840
meaning you cannot take any two points on your manifold and map one to the other using a symmetry.

2142
03:25:57,840 --> 03:26:02,640
In the case of a sphere, you can do that. Any two points in the sphere are related by rotation.

2143
03:26:02,640 --> 03:26:11,200
So we say a sphere is a homogeneous space, but these let's say this protein shape is not homogeneous,

2144
03:26:11,200 --> 03:26:18,080
even if it has some kind of symmetry. So in that case, if you try to motivate the weight sharing

2145
03:26:18,080 --> 03:26:22,240
via symmetry, you try to take your filter, put it in one position, move it around to a different

2146
03:26:22,240 --> 03:26:26,160
position using a symmetry, you're not going to be able to hit all positions. So you're not going to

2147
03:26:26,160 --> 03:26:34,720
get weight sharing globally. And that just is what it is. If you say I have a signal on this manifold

2148
03:26:35,680 --> 03:26:42,240
and I want to respect the symmetry, well, if there are no global symmetries, there's nothing to

2149
03:26:42,240 --> 03:26:46,320
respect. So you get no code strain. So you can just use arbitrary linear map.

2150
03:26:48,800 --> 03:26:54,560
Now, it turns out there are certain other kinds of symmetries called gauge symmetries that you

2151
03:26:54,560 --> 03:27:04,080
might still want to respect. And in practice, what respecting gauge symmetry will do is we'll

2152
03:27:04,080 --> 03:27:10,880
put some constraints on the filter at a particular position. So for example, that might have to be

2153
03:27:10,880 --> 03:27:16,560
a rotationally equivariant filter, but it doesn't tie the weights of filters at different positions.

2154
03:27:18,160 --> 03:27:25,440
So if you want that as well, then you can maybe motivate it via some kind of notion of local

2155
03:27:25,440 --> 03:27:30,400
symmetry. I have something on a local symmetry group point to motivate that in my thesis.

2156
03:27:31,360 --> 03:27:44,240
But there isn't a very principled way to motivate weight sharing on general manifolds

2157
03:27:44,240 --> 03:27:49,200
between different locations. Yeah, I'm just trying to get my head around this. So you're saying,

2158
03:27:49,200 --> 03:27:55,520
because the whole point that we're trying to achieve here is to have a parameter-efficient

2159
03:27:55,520 --> 03:28:00,080
neural network that uses local connectivity and weight sharing, as we do with, let's say,

2160
03:28:00,080 --> 03:28:04,000
plain RCNN, whereas when you have an irregular object, it's very, very difficult to do that.

2161
03:28:04,000 --> 03:28:07,760
So you're saying, in some restricted domain, you can do it. Let's say if you have a,

2162
03:28:08,640 --> 03:28:13,600
let's say, rotation equivariance, but you can't do the other forms of weight sharing.

2163
03:28:14,160 --> 03:28:17,920
I'm just trying to get my head around this, because with a graph convolution on your network,

2164
03:28:17,920 --> 03:28:24,080
for example, it seems like you can abstract the local neighborhood. This node is connected to

2165
03:28:24,080 --> 03:28:29,040
these other nodes. And potentially, that could translate to a different part of the irregular

2166
03:28:29,120 --> 03:28:36,400
mesh. So why can't you do it more than you suggested? I think if you want to be precise,

2167
03:28:36,400 --> 03:28:42,720
you just have to say, what are the symmetries that we're talking about here? And in a graph,

2168
03:28:42,720 --> 03:28:49,600
the most obvious one is the global permutation symmetry. So you can reorder the nodes of a graph

2169
03:28:50,240 --> 03:28:56,000
and really any graph neural net, whether they're coming from an equivariance perspective or not,

2170
03:28:56,000 --> 03:29:01,600
all graph neural nets in existence that have been proposed, they respect this permutation

2171
03:29:01,600 --> 03:29:08,640
symmetry. And typically, this happens through, let's say, in the most simple kind of graph

2172
03:29:08,640 --> 03:29:14,160
convolutional nets, like the ones by Kip van Belling, for example, there's a sum operation,

2173
03:29:14,160 --> 03:29:20,720
some messages from your neighbors. And some operation does depend on the border of the summands.

2174
03:29:20,720 --> 03:29:24,400
So it doesn't depend on the border of the neighbors. And that's why the whole thing becomes

2175
03:29:24,400 --> 03:29:31,040
every variant. So that's a global symmetry that all graph networks respect.

2176
03:29:31,040 --> 03:29:37,920
On that, though, could you not create a local, let's say if there was a local graph isomorphism,

2177
03:29:37,920 --> 03:29:43,520
and so I have an irregular object, but it has a local isomorphism, could I not use something like

2178
03:29:43,520 --> 03:29:46,960
a GCN, a local version of it to capture that isomorphism?

2179
03:29:47,680 --> 03:29:56,320
Yeah. So actually, this was something we proposed to do in our paper, Natural Graph Networks.

2180
03:29:56,320 --> 03:30:03,040
So this paper really has two aspects to it. One is the naturality as a generalization of

2181
03:30:03,040 --> 03:30:10,240
equivariance, I can talk about that. But another key point was that we can not just develop a

2182
03:30:10,240 --> 03:30:14,880
global natural graph network, as we call it, but also a local one. And what the local one will do

2183
03:30:15,520 --> 03:30:24,800
is it will look at certain local motifs. So maybe if you're analyzing molecular graphs,

2184
03:30:24,800 --> 03:30:31,440
one motif that you often find is, you know, aromatic ring or something, some ring with,

2185
03:30:31,440 --> 03:30:38,960
let's say, six corners, various other kinds of little small graph motifs.

2186
03:30:39,680 --> 03:30:47,120
And these motifs might appear multiple times in the same molecule or across different molecules.

2187
03:30:47,120 --> 03:30:52,080
And so what this method is doing is it's essentially finding those using some kind

2188
03:30:52,080 --> 03:31:00,560
of graph isomorph, local graph isomorphism, and then making sure that whenever we encounter

2189
03:31:00,560 --> 03:31:08,560
this particular motif, we process it in the same way, i.e. using the same weights. And if the

2190
03:31:08,560 --> 03:31:14,400
local motif has some kind of symmetry, like this aromatic ring, you can rotate it six times,

2191
03:31:14,400 --> 03:31:19,440
and it gets back to the origin or you can flip it over. So that's the symmetry of this graph

2192
03:31:19,440 --> 03:31:25,200
structure or an automorphism of this graph. And then the weights will also be constrained by this

2193
03:31:25,200 --> 03:31:31,840
automorphism group, this group of symmetries of the local motif. And various other authors also

2194
03:31:31,920 --> 03:31:38,640
have, I think, even Michael Bronstein and students have developed methods based on

2195
03:31:38,640 --> 03:31:44,800
similar ideas. Awesome. Taiko, it's been such an honor having you on the show. And actually,

2196
03:31:44,800 --> 03:31:48,960
you're coming back on the show in a few weeks' time, so we don't want to spoil the surprise.

2197
03:31:48,960 --> 03:31:54,080
But looking on this proto book that you've written with the other folks, what's the main

2198
03:31:54,080 --> 03:31:56,800
thing that sticks out to you as being the coolest thing in the book?

2199
03:31:57,680 --> 03:32:09,760
I think there's any one particular thing. What excites me is to put some order to the chaos

2200
03:32:10,480 --> 03:32:17,920
of the zoo of architectures and to see, actually, that there is something that they all have in

2201
03:32:17,920 --> 03:32:25,680
common. And I really think this can help new people who are new to the field to learn more

2202
03:32:25,680 --> 03:32:33,680
quickly, to get an overview of all the things that are out there. And I also think that this is

2203
03:32:33,680 --> 03:32:43,440
the start of at least one way in which we can take the black box of deep learning, which often is

2204
03:32:43,440 --> 03:32:49,280
viewed as completely inscrutable and actually start to open it and start to understand how the

2205
03:32:49,280 --> 03:32:57,120
pieces connect, which can then perhaps inform future developments that are guided by both

2206
03:32:57,120 --> 03:33:03,920
empirical results and an understanding of what's going on. Amazing. Thanks so much, Taiko.

2207
03:33:04,960 --> 03:33:06,480
Thanks for having me. It's been a pleasure.

2208
03:33:07,120 --> 03:33:09,840
Joan, thank you so much for joining us. This has been amazing.

2209
03:33:10,400 --> 03:33:15,040
Okay, no, thank you so much, Tim. It was very fun and best of luck. And I think I'm

2210
03:33:15,040 --> 03:33:20,560
let's maybe get in touch. Thank you very much. It's very nice to be talking to you today about

2211
03:33:20,560 --> 03:33:22,480
these completely random topics. Yeah.

