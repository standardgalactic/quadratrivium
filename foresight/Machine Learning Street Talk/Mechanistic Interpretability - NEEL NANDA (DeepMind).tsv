start	end	text
0	10000	Now PA says question for Neil, how does he see interpretability playing a role in AI security, not alignment, for example,
10000	18000	crafting more exotic jail breaks, and he says to tell you to blink twice if you can't answer due to an NDA?
19000	30000	Yes, sorry, jokes aside, what was the question?
30000	43000	So, there was this beautiful meme where you draw ChachiPT as a Shogoth, an eldritch monstrosity from Lovecraftian horror fiction,
43000	48000	with a smiley face on top, because language models are bizarre and confusing things.
48000	53000	That are just, I don't know, they're kind of a compressed version of the entire internet.
53000	57000	That will do bizarre things in bizarre situations.
57000	62000	But then OpenAI tried really hard to get it to be nice and gentle and a harmless assistant,
62000	70000	and look so normal and reasonable and safe, which is the smiley face mask on top of the underlying monstrosity.
70000	77000	But unfortunately, the smiley face mask means people don't realise how weird language models are.
77000	83000	Have you ever stopped to think how strange it is that we're all alive right now?
83000	88000	Out of all the possible times in history, you were born into this generation.
88000	95000	You have the incredible fortune and responsibility of being on the Earth today.
95000	102000	Let's not waste this opportunity. You can use this time to do something meaningful that'll make the world a better place.
102000	113000	But the problem seems so huge. Global pandemics, climate change, the risk of nuclear Armageddon, the threat of AI existential risk.
113000	118000	How can one person have an impact on issues this enormous?
118000	122000	The world is really, really complicated.
122000	130000	Like, if you want to understand a question like, how big a deal is AJAX risk, or should I work on it?
130000	137000	Just like one sub-question I care about is AI timelines. How long until we get human-level AI?
137000	145000	Now, I recently discovered 80,000 hours. They're a non-profit, effective altruism-aligned organisation.
145000	153000	And what they do is they use evidence and analysis to determine how people can have the biggest impact with their careers.
153000	158000	If you want to solve humanity's biggest problems, you have to start at the very core.
158000	166000	We need to focus on safeguarding humanity's entire future, because if civilization just came to an abrupt end,
166000	175000	whether through climate change or nuclear Armageddon, or even AI existential risk, then all progress would just end.
175000	181000	Future generations wouldn't have a chance of building a better world or reaching their full potential.
181000	191000	And the good news is that 80,000 hours have identified a couple of concrete steps so that folks like you can use your careers to combat existential risk,
191000	197000	ensuring that humanity's light continues to shine for generations to come.
197000	201000	Learn more by visiting their website on 80,000hours.org,
201000	208000	grab their free career guide, start planning a career with true purpose.
208000	213000	Because you only have 80,000 hours, so make them count.
213000	218000	There's no catch, there's no secret monetization or anything like that.
218000	222000	These folks have an incredible podcast, they have lots of materials that you can download,
222000	227000	basically to help you have a huge impact with your life and your career,
227000	234000	especially if you're someone who really, really thinks about humanity and our plight in the long-term future.
234000	237000	This is really something you should be looking at.
237000	238000	That is a question.
238000	240000	Right, Nick, Eat The Path.
241000	250000	He says, in broad question, do you see mech interp as chiefly theoretical or an empirical science, and will this change over time?
250000	252000	Yeah.
252000	260000	I see this as very much an empirical science, with some theories sprinkled in, but you need to be incredibly careful.
260000	266000	So fundamentally, I want to understand a model, and I want to understand how the model works.
266000	271000	And a sad fact about models, is models are really fucking cursed.
271000	281000	And just work in weird ways that aren't quite how you expect, which represent concepts a bit differently from how I expect them to.
281000	285000	And just do all kinds of weird stuff I wouldn't have expected until I went and poked around inside of them.
285000	292000	And I think that if you're trying to reverse engineer a network, and you don't have the capacity to be surprised by what you find,
292000	296000	you are not doing real mechanistic interpretability.
296000	305000	It's so easy to trick yourself and to go in with some bold hypothesis of this is what the network should have, and you probe for it, and it looks like it supports that,
305000	307000	but you take further and you are wrong.
307000	311000	And yeah, I think there is room for theory.
311000	318000	I think in particular, we just don't have the right conceptual frameworks to reason about how to understand a model.
318000	322000	And we'll get into fundamental questions like superposition later on.
322000	330000	But yeah, I think that theory needs to come second to empiricism.
330000	335000	If your theoretical model says x and the real model says y, your theory was wrong.
335000	338000	Which is the story of all of machine learning.
338000	342000	So Goji Tech, she says question for Neil.
342000	347000	Does he think a foundational understanding of deep learning models is possible?
347000	352000	And does that extend to prediction using a mathematical theory?
352000	355000	Possible is such a strong word.
355000	361000	Like, if we produce a super intelligent AI, will it be capable of doing this?
361000	363000	Probably.
363000	371000	In terms of foundational understanding, I think there are deep underlying principles of models.
371000	376000	I believe there are scientific explanations for lots of the weird phenomena we see,
376000	382000	like scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.
382000	389000	I'm hesitant to say there's some like strong things here or some strong guarantees.
389000	393000	Like, I don't know, models are weird.
393000	397000	Sometimes if you change the random seed, they will just not learn.
397000	404000	I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning,
404000	410000	because the moment you start trying to impose axioms and assumptions onto things
410000	415000	and they do not perfectly track the underlying reality, your theories break.
415000	419000	But I'm very hesitant to say anything's impossible.
419000	423000	And I think there's far, far more to learn than we have, looks like.
423000	428000	Now, finally, Jumbo Tron, Ian, he says, oh, heck yeah!
428000	430000	I'm glad to see that you brought this guy on.
430000	433000	I've been interested in his work ever since you shared his blog.
433000	438000	Now, the question off the top of Ian's head is, how does your theory, Neil,
438000	441000	of chasing phase changes to create grokking,
441000	445000	have any crossover or links with power law scaling techniques,
445000	451000	like in the scaling laws paper, beyond the scaling laws, beating power law scaling via data pruning?
451000	457000	Yeah, that is...
457000	460000	So we're going to get into this much more later in the podcast.
460000	468000	At a very high level, I would say that grokking is in many ways kind of an illusion, as we'll get to later.
468000	474000	And one notable thing about it is grokking is an overlap between a phase transition,
474000	478000	where the model goes from cannot generalize to can generalize fairly suddenly,
478000	484000	and the phenomena where it's faster to memorize than to generalize.
484000	491000	And these two things on top of each other give you this sudden memorization and failure to generalize,
491000	494000	followed by a sudden convergence later on.
494000	498000	But the interesting thing here is the phase transition.
498000	502000	That's a much more robust result, while grokking is, if you screw around with high parameters enough,
502000	506000	you get it to grok, but it's very delicate and a little bit of an illusion.
506000	511000	And this is a great paper from Eric Michaud in Max Tecmox lab,
511000	518000	showing that, well, providing a conceptual argument and some limited empirical evidence
518000	522000	for the hypothesis that the reason we get these smooth scaling laws
522000	530000	is that models are full of lots of phase transitions, plausibly when they learn individual circuits,
530000	534000	though the paper does not explicitly show this,
534000	540000	and that the smooth scaling laws happen because there are just many, many phase transitions,
540000	545000	and if they follow a certain distribution, you get beautiful smooth powers.
546000	553000	And to me, this kind of thing is the main interesting link
553000	558000	between broader macroscopic phenomena and these tiny things,
558000	563000	though I also think grokking is kind of overhyped and people significantly overestimate
563000	567000	the degree to which it has deep insights for us about how networks work.
567000	573000	And we do think it's a really cute thing that gave me a really fun interactability project.
573000	576000	And we learned a bit about scientific deep learning,
576000	580000	but people often just assume it's like a really deep fact about models.
580000	587000	By the way, there was something I didn't say in the woods, which is that Neil has an amazing YouTube channel.
587000	592000	I've been glued to it all week, actually. Some of them are admittedly quite technical,
592000	595000	but even if you're not interested in mechanistic interpretability,
595000	600000	Neil has an extremely soothing voice, second only to Sam Harris,
600000	602000	and I would recommend listening to him when you go to sleep,
602000	609000	because as Neil's dulcet tones will melt the stress away quicker than a nun's first curry.
611000	617000	Anyway, with that said, we started to talk about what is mechanistic interpretability.
617000	621000	And first of all, I wanted to call out your ridiculously detailed
621000	625000	and exquisite mechanistic interpretability explainer.
625000	627000	Maybe you could just tell us about that quickly.
627000	633000	Yes, so I wanted to try to write a glossary.
633000	638000	There's some basic common terms in mechantup. It's like an appendix to a blog post.
638000	641000	There were a lot of terms in mechantup.
641000	644000	There were a lot of terms in mechantup.
644000	649000	And I like writing and I write about it privately, so I got kind of carried away.
649000	654000	And there's about 33,000 words, a massive, massive exposition.
654000	658000	But importantly, it is designed to be easily searchable.
658000	663000	And mechantup is full of jargon and I'm sure I'll forget to explain everything that I'm saying.
663000	667000	So I'd highly recommend just having it open in a tab as you listen to this.
667000	670000	And if you get lost, just look up terms in there.
670000	674000	And yes, it's both definitions, but it's also long tangents,
674000	679000	giving intuitions and context and related work and common misunderstandings.
679000	682000	It was very fun to write.
682000	687000	So I think first of all, we should introduce this idea of circuits and features
687000	693000	and also this idea of whether interpretation is even possible at all.
693000	697000	Why do you have the intuition that it is possible?
697000	703000	Yeah, so a couple of different takes here.
703000	706000	So the key, yeah.
706000	712000	So fundamentally, neural networks are not incentivized to produce legible interpretable things.
712000	714000	They are a mound of linear algebra.
714000	723000	There's this popular stochastic parrots of you that they are literally a mass of statistical correlations meshed together with no underlying structure.
723000	730000	The reason I think there's any hope whatsoever on a theoretical basis is that ultimately,
730000	737000	they are made of linear algebra and they are being trained to perform some tasks.
737000	743000	And my intuition is that for many tasks, the way to perform well on them is to learn some actual algorithms
743000	750000	and like actual structured processes that maybe from a certain perspective you could consider reasoning.
750000	756000	And models have lots of constraints like they need to fit it into these matrices.
756000	763000	They need to represent things using the attention mechanism and jellies and a transformer.
763000	770000	And there's all kind of properties of the structure that constrain the algorithms and processes that can be expressed.
770000	775000	And these give us all kinds of hooks we can use to get in and understand what's going on.
775000	777000	So that's the theoretical argument.
777000	780000	All theoretical arguments are bullshit unless you have empirics behind it.
780000	788000	And we're going to talk a bunch throughout this podcast about the different bit of different preliminary results we have
788000	792000	that make me feel like there's something here that can be understood.
792000	798000	What I find particularly inspiring is this work it did reverse-end during modular addition, which I think we'll get to shortly.
798000	803000	But I also want to emphasize that I rather see mech and turp as a bet.
803000	811000	There's this stronger hypothesis that if we knew what we were doing, we'd be able to take GPT-7 and fully understand it
811000	815000	and decompile it to an enormous Python code file.
815000	821000	And there's the weaker view that it is a mess and there's lots of illegible things,
821000	827000	but we can find lots of structure and we can find structure for the important part to make a much of progress.
827000	833000	And then there's the, yeah, we've cherry picked like 10 things and the 11th is just going to completely fail
833000	836000	and the field is going to get doomed and run out of steam in like a year.
836000	838000	And I don't really know.
838000	839000	I'm a scientist.
839000	840000	I want to figure out.
840000	846000	I think it is worthy and dignified to make this bet.
846000	852000	But I would be lying if I said I am 100% confident mech and turp will work.
852000	854000	Models are fundamentally understandable.
854000	855000	We will succeed.
855000	856000	Let's go try.
856000	861000	Well, on that note, how does it mean we interviewed Christoph Molnar,
861000	865000	who's one of the main classical interpretability guys.
865000	871000	And I think everyone agrees in principle that you can't just look at the inputs and the outputs like a behaviorist.
871000	874000	We need to understand why these models do what they do,
874000	877000	because sometimes they do the right things for the wrong reasons.
877000	884000	So maybe first of all, without going too deep, I mean, could you just briefly contrast with, you know, classical interpretability?
884000	885000	Yeah.
885000	888000	So there's a couple of...
888000	889000	Okay.
889000	894000	So first off, I think it's very easy to get into kind of nonsense gatekeeping,
894000	900000	because there's both of the cultural mech and turp community centered around Chris Ola,
900000	903000	not that much in academia, though some in academia.
903000	907000	And there's the academic fields of mechanistic interpretability.
907000	908000	Right?
908000	909000	So there's lots of people doing work.
909000	911000	I would consider mechanistic interpretability,
911000	914000	to engage much with the community or don't know who exists.
914000	916000	For example, a friend of mine is Atticus Geiger,
916000	920000	who's doing some great work at Stanford on cause-labs tractions.
920000	924000	I believe discovered about a month ago that the mech and turp community actually existed.
924000	926000	And I don't know.
926000	929000	I don't like gatekeeping.
929000	932000	And there's lots of work that's kind of relevant,
932000	937000	but maybe not quite mech and turp under a strict definition, blah, blah, blah.
937000	939000	With those, with that hedging out of the way.
939000	941000	A couple of key principles.
941000	947000	The first is inputs and outputs are not sufficient.
947000	954000	And I think even within interpretability, this is not a like uncontroversial claim.
954000	957000	There's all kinds of things that are saliency maps,
957000	959000	attributing things to different bits of the inputs.
959000	964000	There are things that the form train an extra head to output an explanation
964000	968000	or just ask the model to output an explanation of why it does what it does.
968000	973000	And I think that if we want something that can actually work for human level systems,
973000	977000	or even this frontier system you have today, this is just not good enough.
977000	983000	Particularly a vocative example to me is in the GPT-4 system card,
983000	989000	the Alignment Research Center, an organization they were getting to help audit and red team GPT-4,
989000	994000	had it try to help a task rabbit worker fill out a capture for it.
995000	998000	The task rabbit worker was like, why do you need this?
998000	1000000	Are you a robot or something?
1000000	1004000	GPT-4 on an eternal scratch pad wrote out,
1004000	1007000	I must not reveal that I am a robot.
1007000	1010000	It then said, oh no, I bought a visual impairment.
1010000	1012000	And the task worker did the capture.
1012000	1017000	I'm like, this isn't some deep sophisticated intentional deception,
1017000	1023000	but it's very much like, well, I don't trust the inputs and outputs of these models.
1023000	1027000	Another really cute example is this paper from Wiles Turpin that just came out
1027000	1030000	about limitations of chain of thought.
1030000	1033000	So chain of thought, you ask the model to explain why it does something,
1033000	1038000	they were giving it multiple choice questions and asking it to explain its answer and then give the answer.
1038000	1043000	And they did five shot-ish, like here's five examples, answer this question,
1043000	1045000	and then it modeled as well.
1045000	1051000	And then they give it something where all of the answers in the prompts are A.
1051000	1054000	Correctly A, they just set it up so the answer is A.
1054000	1058000	The model decides that it should output A,
1058000	1065000	but the model comes up with a false chain of thought reasoning
1065000	1069000	that gets it to the point where it says A is the right answer.
1069000	1076000	And I don't know, some people are trying to use chain of thought as an interpretability method.
1076000	1080000	And I think we need to move beyond this and engage with the internal mechanisms.
1080000	1085000	So that's point one. Point two is ambition.
1085000	1089000	I believe that ambitious interpretability is possible,
1089000	1097000	or at least that if it's not possible, that striving for it will get us to interesting places.
1097000	1102000	These models have legible algorithms, I want to try to reverse engineer them.
1102000	1109000	A third difference is engaging with the actual mechanisms and computation and algorithms learned.
1109000	1115000	There's also work on things like analyzing features of a model, probing individual neurons.
1115000	1118000	And I think this is very relevant to mech and tub,
1118000	1123000	but I want to make sure we aren't just looking at what's inside the model,
1123000	1128000	but also trying to understand how it computes features from earlier features,
1128000	1132000	what applying causal interventions to understand the actual mechanisms,
1132000	1136000	making sure we're not just doing correlational things like probing.
1136000	1145000	And fourth is maybe a more meta principle of favoring depth over breadth.
1145000	1150000	A kind of key underlying belief of a lot of my philosophy of interpretability
1150000	1155000	is that it is so, so easy to trick yourself.
1155000	1159000	There's all kinds of papers about the interpretability illusion,
1159000	1163000	impossibility theorems for feature attribution methods,
1163000	1170000	various many ways that attempts to do interpretability have led to people confusing themselves,
1170000	1172000	or coming to erroneous conclusions.
1172000	1177000	I think that if, but I also think that I want to be in a world
1177000	1181000	where we can actually have scalable, ambitious approaches to interpretability
1181000	1183000	that actually work for frontier systems.
1183000	1185000	But I feel like we don't know what we're doing.
1185000	1190000	And so my vision of mech and tub is start small.
1190000	1194000	Some of the things where we can really rigorously understand what's going on
1194000	1201000	slowly build our way up and like build a foundation of the field of interpretability
1201000	1205000	where we genuinely understand rigorously what is going on
1205000	1208000	and use this foundation to be more ambitious,
1208000	1215000	to try to build real principle techniques to be willing to relax the rigor to be able to go further
1215000	1217000	and see how far we can get.
1217000	1222000	And people and this means I'm happy with things like let's analyze an individual model
1222000	1228000	and understand a small family of features in a lot of detail rather than lots of stuff kind of jankly.
1228000	1233000	There's a lot of stuff in summary having an ambitious vision,
1233000	1236000	not just looking at inputs and outputs,
1236000	1241000	actually trying to engage with internal mechanisms and favoring depth over breadth.
1241000	1244000	But I want to avoid gatekeeping as I said.
1244000	1250000	What would interpretability look like in a world full of GPT-4 models and beyond?
1250000	1257000	I mean, presumably you actually think that they're competent enough to deceive us and manipulate the inputs.
1257000	1261000	I definitely want to clarify that when I say deception or manipulation here,
1261000	1269000	I'm not making the strong claim that it's intentionally realized this for instrumental reasons as part of an overall goal.
1270000	1274000	I'm very happy with there was a prompt saying to deceive someone
1274000	1280000	or it learned that in this context people often output things that are intended to convince someone
1280000	1286000	and it just kind of does this as like a learned pattern of execution.
1286000	1294000	But yeah, my vision of what interpretability would look like is we take some big foundation model
1294000	1301000	like the GPT-4 base model or the fine-tuned GPT-4 that's being used as a base for everything else.
1301000	1306000	We make as much progress as we can understanding the internal circuitry,
1306000	1312000	both taking important parts of it and like important questions about it,
1312000	1316000	e.g. how does it model people it's interacting with?
1316000	1321000	Does it have any notion that it is a machine learning system and like what would this even mean?
1321000	1325000	And being willing to do pretty labor-intensive things on that,
1325000	1330000	having a family of motifs and understood circuits we can automatically look for
1330000	1337000	and very automated tools to make a lot of the labor-intensive stuff as efficient as possible.
1337000	1342000	Things like OpenAI's recent paper using GPT-4 to analyze GPT-2 neurons
1342000	1345000	for like a very key proof of concept here.
1345000	1350000	There needs a lot of work before it can actually be applied rigorously and at scale.
1350000	1357000	And yeah, taking this one big model, trying to understand it as much as we can,
1357000	1364000	one family of techniques we're going to get to is kind of causal abstractions and causal interventions,
1364000	1370000	which are very well suited to taking a model on a certain input or a certain family of inputs
1370000	1372000	and understanding why it does what it does there.
1372000	1378000	There's a much more narrow and that's more tractable question than like, what is GPT-4?
1379000	1384000	And yeah, doing something like if there's a high-profile failure, being able to debug it
1384000	1387000	and really understand the internal circuitry behind that.
1387000	1392000	Or yeah, I don't know, I have a bunch of other random thoughts.
1392000	1397000	One reason I'm emphasizing the focus on the big base model is I think a common critique
1397000	1400000	is this stuff doesn't generalize between models or it's really labor-intensive.
1400000	1406000	But we live in a world where there is just like one big foundation model used in a ton of different use cases.
1406000	1411000	Probably the circuitry doesn't change that much when you give it a prompt or you fine-tune it a bit.
1411000	1417000	And I think getting a deep understanding of a single model is kind of plausibly possible.
1417000	1421000	But do you think it doesn't change that much?
1421000	1424000	So no one's really checked.
1424000	1427000	This is just true of so many things in interpretability.
1427000	1433000	It's like, well, you know, my intuition is that when you fine-tune a model,
1433000	1438000	most of what is going on is that you're rearranging the internal circuitry.
1438000	1442000	Say you fine-tune a Wikipedia, you up-weight the factual recall circuitry,
1442000	1445000	you flesh it out a bit, you down-weight other stuff.
1445000	1448000	And I think this can explain a lot of improved performance.
1448000	1451000	But then if you fine-tune for much longer, you're basically just training the model
1451000	1458000	and it will start to learn more circuitry, more features, more algorithms, more knowledge of the world.
1458000	1462000	And yeah, but no one's really checked.
1462000	1468000	And definitely the longer you fine-tune it and the more you're using weird techniques
1468000	1475000	like reinforcement learning from human feedback, the less I'm confident in this claim.
1475000	1479000	And yeah, if we discovered that every time you fine-tune a model,
1479000	1482000	it will wildly change all of the internal circuitry,
1482000	1485000	maybe like somewhat more pessimistic about Mechantup,
1485000	1491000	unless we can get very good at the automated parts, which we might be able to get good at.
1491000	1495000	I very much think of the field as we're trying to do this hard, ambitious thing.
1495000	1500000	We're making a lot of progress, but I really wish we're making way more progress, way faster.
1500000	1503000	And you, viewer, could help.
1503000	1509000	But I don't know where the difficulty bar is for being useful
1509000	1514000	or the difficulty bar is for being like incredibly, ambitiously useful.
1514000	1519000	And it's plausible already at the point where Mechantup can do real useful things no one else can
1519000	1521000	or no other techniques can.
1521000	1522000	It's plausible.
1522000	1524000	It will take like five years to get to that point.
1524000	1526000	I don't really know.
1526000	1530000	So I wanted to talk about this concept of needs and scruffies.
1530000	1535000	So there have been two divisions in AI research going all the way back to the very, very beginning.
1535000	1542000	And you've said that sometimes understanding specific circuits can teach us universal things about models
1542000	1545000	which bear unimportant questions.
1545000	1549000	So this reminds me of this dichotomy between the needs and the scruffies.
1549000	1555000	Now you seem like a need to me, a need to someone who is quite puritanical
1555000	1557000	and also it's related to universalism.
1557000	1564000	So this idea that there are simple underlying principles that explain an awful lot of things
1564000	1572000	rather than wanting to accept the gnarly kind of reality that everything's so bloody complicated.
1572000	1574000	Where do you fall on that?
1574000	1579000	So I definitely would not.
1579000	1580000	Okay.
1580000	1583000	So there's two separate things here.
1583000	1585000	There's like, what's my aesthetic?
1585000	1587000	Well, I want things to be neat.
1587000	1588000	I want them to be beautiful.
1588000	1589000	I want them to be mathematical.
1589000	1590000	I want them to be elegant.
1590000	1591000	Yes.
1591000	1593000	And then there's what do I do in practice?
1593000	1595000	And what do I believe is true about networks?
1595000	1602000	Well, I think there is a lot more structure than most than many people think.
1602000	1608000	But I also do not think they are just some beautiful purely algorithmic thing
1608000	1611000	that we could uncover if we just knew the right tools.
1611000	1613000	And like maybe they are.
1613000	1615000	We'd fucking great if they were.
1615000	1621000	But I expect they're messy and cursed, but with some deep structure and patterns
1621000	1626000	and how much traction we can get on the weird scruffiness is like somewhat unclear to me.
1626000	1628000	I think we can make a lot more progress than we have.
1628000	1631000	But we might eventually hit a wall.
1631000	1634000	You were saying something quite interesting when we drove over, which is,
1634000	1639000	I mean, my friend Waleed Sabah, he's a linguist and he is a Platonist.
1639000	1645000	He thinks that there are these universal cognitive priors and there's a hierarchy of them.
1645000	1648000	And the complexity collapses.
1648000	1653000	And he thinks that language models have somehow acquired these cognitive priors.
1653000	1657000	And if we did some kind of symbolic decomposition, you know,
1657000	1660000	we would all just kind of like pack itself into this beautiful hierarchy.
1660000	1663000	And you were saying that there are Gabor filters and they're all these different circuits
1663000	1670000	and they have motifs, they have categories, they have flavors for want of a better word.
1670000	1674000	Are you optimistic that something like this could happen?
1674000	1676000	Yeah.
1676000	1682000	So, hmm.
1682000	1693000	So, one interesting point here is often interoperability is fairly different
1693000	1696000	for different modalities and different architectures.
1696000	1701000	A lot of the early work was done on convolutional networks and image classifiers.
1701000	1705000	The field very much nowadays focuses on transformer language models.
1705000	1712000	And I think there's lots of structure to how transformers implement algorithms.
1712000	1717000	Transformers cannot be recursive, but they're incredibly paralyzed.
1717000	1723000	Transformers have this mechanism of attention that tells them how to move information between positions.
1723000	1729000	And there's lots of algorithms and circuitry that can be expressed like this
1729000	1732000	and lots of stuff that's really weird to express.
1732000	1738000	And I think that this constrains them in a way that creates lots of interesting structure
1738000	1742000	that can be understood and patterns that can be understood.
1742000	1746000	Is this inherently true of intelligence? Who knows?
1746000	1753000	But a lot of my optimism for structures within networks is more like that.
1753000	1758000	But I try to think about structure more from a biologist's perspective
1758000	1762000	than a mathematician's or like a philosopher's perspective.
1762000	1766000	Though, I am a pure mathematician and I know nothing about biology.
1766000	1771000	So, if anyone's listening to this, no stuff about biology and thinks I'm talking bullshit, please email.
1771000	1780000	So, if you look at evolutionary biology, model organisms have all of this common shared structure.
1780000	1785000	Like, most things have bones, we have cell nuclei.
1785000	1793000	And hands of mammals tend to be surprisingly similar, but kind of weird and changed in various ways.
1793000	1798000	And I don't know.
1798000	1801000	I don't think these are like hard rules.
1801000	1804000	Most of them have weird exceptions.
1804000	1808000	And obviously a lot of this is due to the shared evolutionary history
1808000	1813000	and is not just inherent to the substrate of you have proteins.
1813000	1817000	Though, in fact, you often train these models on similar data in similar ways
1817000	1822000	and they have the same architecture that constrains them to different kinds of algorithms.
1822000	1828000	It makes me optimistic there's a biologist's level of a structure.
1828000	1831000	Now, you said something interesting which is that transformers can't be used in a recursive way.
1831000	1835000	Now, we'll just touch this very quickly because we've spoken about this a million times on different episodes.
1835000	1842000	But, you know, there's the Chomsky hierarchy and he had this notion of a recursively enumerable language.
1842000	1847000	And these different models, computational models in the Chomsky hierarchy,
1847000	1852000	it's not only about being able to produce a language which exists in a certain set.
1852000	1857000	It's also the ability to recognize that the language belongs in a certain set.
1857000	1864000	And transformers are quite low down on that hierarchy because they're called recurrently not recursively.
1864000	1869000	But I just wondered if you had any just, you know, prima facie if you had any views on that.
1870000	1875000	Yeah, so I'm not a linguist. I'm not particularly familiar with the Chomsky hierarchy.
1875000	1880000	I do think it's surprising how well transformers work.
1880000	1887000	And I have a general skepticism of any theoretical hierarchy, like, I don't know.
1887000	1895000	If you think there's some beautiful structure of algorithms and stuff that's low down,
1896000	1903000	and then GPT-4 happens, I think a framework's wrong rather than transformers are wrong.
1903000	1910000	Just massive stack of matrices plus a massive pile of data gives shockingly effective systems.
1910000	1914000	Theoretical frameworks just often break when they make contact with reality.
1914000	1919000	Well, that's certainly true. I mean, there's a famous expression that all grammars leak.
1919000	1924000	I had rather, I don't know, I guess a similar conclusion to you, which is that if anything,
1924000	1928000	it teaches us how sclerotic and predictable language is,
1928000	1935000	and we don't actually need to have access to this infinite space or even exponentially large space.
1935000	1939000	Most language use and most phenomena that we need, perhaps for intelligence,
1939000	1945000	is surprisingly small and current models can work just well.
1945000	1948000	Why don't we move on to your grokking work?
1948000	1957000	So grokking is this sudden generalization that happens much later in training after...
1957000	1959000	If I can add a brief clarification.
1959000	1960000	Oh, yes, of course.
1960000	1964000	So people often call grokking sudden generalization.
1964000	1966000	My apologies. Go on.
1966000	1970000	Sudden generalization is a much more common phenomena than grokking.
1970000	1974000	It can just generally look like things like, I don't know, the model is trying to learn a task,
1974000	1976000	it's kind of bad at it, and then it suddenly gets good at it.
1976000	1978000	I prefer to call this a phase transition.
1978000	1985000	Grokking is the specific thing where the model initially memorizes and does not generalize,
1985000	1991000	and then there's a sudden phase transition in the test loss, the generalization ability,
1991000	1997000	which creates a convergence after an initial divergence between train and test.
1997000	2002000	And this is like a much, much more specific phenomena than sudden generalization.
2002000	2009000	Okay, so you've spoken about three distinct phases of training underlying grokking.
2009000	2012000	So why don't we go through them one by one?
2012000	2018000	Yeah, so the context of this project, this was a paper called Progress Measures for Grokking
2018000	2025000	by a Mechanistic Interpretability that I recently presented on at iClear.
2026000	2031000	So we were studying a one-layer transformer, we trained to do modular addition,
2031000	2033000	and it grokked modular addition.
2033000	2039000	And the first thing we did was reverse engineer the algorithm behind how the model worked,
2039000	2043000	which we may get into in a bit more detail, but at a very high level.
2043000	2048000	Modular addition is equivalent to composing rotations around the unit circle.
2048000	2052000	Composition adds the angle, circle gives you modularity.
2052000	2057000	You can represent this by trig functions and do composition with trig identities
2057000	2063000	and element-wise multiplication, and we reverse engineered exactly how the model did this.
2063000	2069000	And then this mechanistic understanding was really important for understanding what was up with grokking,
2069000	2076000	because the weird thing behind grokking is that it's not that the model memorizes,
2076000	2078000	or that the model eventually generalizes.
2078000	2084000	The surprising thing is that it first memorizes and then changes its mind and generalizes later.
2084000	2091000	And generalization and memorization are two very different algorithms that both do very well on the training data,
2091000	2095000	and only by understanding the mechanism will be able to disentangle them.
2095000	2100000	And this meant we could look during training how much of the model's performance came from memorization
2100000	2102000	and how much came from generalization.
2102000	2104000	And we found these three distinct faces.
2105000	2107000	There was memorization.
2107000	2112000	The first very short phase, it gets phenomenally good train loss.
2112000	2118000	It got to about 3e-7, which is an absolutely insane log loss.
2118000	2127000	And much, much worse than random on test because memorization is very far from uniform and generalizes extremely badly.
2127000	2132000	And then there was this long-seeming plateau.
2133000	2141000	We call the space circuit formation because it turns out that rather than just continue to memorize for a while
2141000	2147000	and doing a random walk through model space until it eventually gets lucky,
2147000	2153000	the model is systematically transitioning from memorization to generalization.
2153000	2160000	And you can see that its train performance gets worse and worse when you only let it memorize.
2161000	2164000	And then, so why is test loss still bad?
2164000	2168000	Test loss is bad because memorization generalizes terribly.
2168000	2174000	And when the model is like, I don't know, two-thirds memorizing, one-third generalizing, it still does terribly.
2174000	2180000	And it's only when the model gets so good at the trigger-based generalizing algorithm
2180000	2185000	that it no longer needs the memorization parameters and cleans them up that we see grocking.
2185000	2189000	And this happens fairly suddenly.
2189000	2197000	But the, if you, we have this metric called restricted loss where we explicitly clean up the memorization for the model
2197000	2199000	and look at how well it generalizes.
2199000	2203000	And we see that restricted loss drops noticeably before test loss drops,
2203000	2208000	showing that the drop is driven by cleaning up the noise.
2208000	2212000	And this is striking because A.
2212000	2216000	I had no idea it was even possible for a model to transition between two good solutions,
2216000	2219000	maintaining equivalent performance throughout.
2219000	2225000	B. There was this real mystery of deep learning that many people tried to answer,
2225000	2229000	and mechanistic understanding was genuinely useful for answering it.
2229000	2233000	And grocking was an illusion.
2233000	2235000	It was not sudden generalization.
2235000	2238000	It was gradual generalization followed by sudden cleanup.
2238000	2245000	And test loss and test accuracy were just too course-metric to tell the difference.
2245000	2252000	But we were able to design these hidden progress measures using our mechanistic understanding that made everything clear.
2252000	2258000	And we also just have all kinds of pretty animations of qualitatively watching the circuits develop over training,
2258000	2260000	and it's very pretty.
2260000	2261000	So a few things.
2261000	2264000	I mean, first of all, just going back to first principles.
2264000	2268000	The biggest problem in machine learning is this concept called overfitting.
2268000	2272000	And we trained the model on a training set.
2272000	2276000	And there's this horrible phenomenon called the shortcut rule,
2276000	2280000	which is that the model will take the path of least resistance.
2280000	2284000	And when you're training it, it only really knows about the training set.
2284000	2290000	And of course, we can test it on a different set afterwards, which we've held out.
2290000	2294000	And just because of the way that we've structured the model,
2294000	2299000	it may, by hook or by crook, generalize to the test set.
2299000	2302000	But the interesting thing is that generalization isn't a binary.
2302000	2304000	There's a whole spectrum of generalization.
2304000	2307000	So it starts with the training set, and then we have the test set,
2307000	2310000	and then the ideal is out-of-demand generalization.
2310000	2312000	But I would go a step further.
2312000	2315000	There's also algorithmic generalization,
2315000	2318000	which is this notion that as I understand it,
2318000	2323000	neural networks, if you model the function y equals x squared,
2323000	2330000	it will only ever be able to learn the values of that function inside the training support.
2330000	2335000	So presumably you're talking about the ideal form of generalization
2335000	2339000	being not as good as algorithmic generalization,
2339000	2341000	or do you think it could go all the way?
2341000	2348000	So I think one thing which is very important to track
2348000	2355000	is what the domain you're talking about is of which it's even possible to generalize.
2355000	2361000	So I generally think about models that have discrete inputs rather than continuous inputs,
2361000	2366000	because basically no neural network is going to be capable of dealing
2366000	2370000	with like unbounded range continuous inputs.
2371000	2377000	In modular addition, there were just two one-hot encoded inputs between 0 and 113,
2377000	2379000	which is the modular I used.
2379000	2381000	Yeah, the model has a fixed modular.
2381000	2383000	It's not doing modular addition in general.
2383000	2388000	And there's just like 12,000 inputs, and it learns to do all of them.
2388000	2392000	And in, I don't know, behaviorally,
2392000	2395000	you can't even tell the difference between the model memorizes everything
2395000	2398000	and the model learns some true algorithm.
2398000	2401000	Though, with the more cognitivist mechanistic approach,
2401000	2404000	I can just look at it and say, yep, that's an algorithm.
2404000	2406000	It's great. Not a stochastic parrot.
2406000	2408000	Conclusively disprove that hypothesis.
2410000	2412000	And yeah.
2415000	2419000	I think that for the language models, it's more interesting,
2419000	2425000	because I know GbD2, it's got 1,000 tokens, 50,000 vocab.
2425000	2429000	It's like 50,000 to the power of 1,000 possible inputs.
2430000	2436000	And there's a surprising amount of interesting algorithmic generalization.
2437000	2440000	We're going to talk later about induction hits,
2440000	2445000	which is the circuit language models learn to detect and continue repeated text.
2445000	2448000	Like if given the word Neil, you want to know what comes next.
2448000	2452000	Unfortunately, Nanda is not that homelist yet.
2453000	2457000	But if Neil Nanda has come up like five times before in the text,
2457000	2459000	Nanda's pretty likely to come next.
2459000	2467000	And this transfers to, if you get the model, just random tokens,
2467000	2472000	with some repetition, the model can predict the repeated random tokens,
2472000	2475000	because the induction heads are just a real algorithm.
2475000	2481000	And the space of possible repeated random tokens is like enormous.
2481000	2485000	It's like, in some sense, much larger than the space of possible language.
2485000	2488000	And is this algorithmic generalization?
2488000	2490000	I don't really know. It depends on your perspective.
2491000	2496000	Let's bring in this paper by Bilal Tughtay.
2496000	2500000	So it was called a toy model of universality,
2500000	2503000	reverse engineering how neural networks learn group operations,
2503000	2505000	and you supervised that paper.
2505000	2510000	And he was asking the question of whether neural networks learn universal solutions,
2510000	2512000	or these idiosyncratic ones.
2512000	2514000	And he said he found inherent randomness,
2514000	2517000	but models could consistently learn group composition
2517000	2520000	via an interpretable representation theory.
2520000	2523000	So can you give us a quick tour de force of that work?
2523000	2524000	Yeah.
2524000	2527000	Maybe I should detour back to my grokking work
2527000	2529000	and just explain the algorithm we found there
2529000	2531000	and how we know it's the real algorithm.
2531000	2532000	Yeah, sure.
2532000	2533000	This is a good foundation for this paper.
2533000	2534000	Sure, sure.
2534000	2539000	Yeah, so we found this thing we call the Fourier multiplication algorithm.
2539000	2544000	The very high level it composes rotations.
2544000	2548000	You can actually look at how the different bits of the model implement the algorithm,
2548000	2550000	and often just read this off.
2550000	2553000	So the embeddings are just a lookup table
2553000	2558000	mapping the one-hot encoded inputs to these trig terms.
2558000	2560000	Sines and cosines are different frequencies.
2560000	2563000	You can just read this off the embedding weights.
2563000	2567000	Note, people often think that learning sine and cosine is hard.
2567000	2572000	It's actually very easy because you only need it on 113 different data points,
2572000	2574000	such as a lookup table.
2574000	2580000	The model then uses the attention and MLPs to do this composition,
2580000	2584000	to do the multiplication with trig identities
2584000	2590000	to get the, like, composed rotation, the A plus B terms.
2590000	2596000	And here we can just read off the neurons that they have learned these terms
2596000	2598000	and that they were not there beforehand.
2598000	2603000	The model is using its non-linearities in interesting ways to do this.
2603000	2610000	It's also incredibly cursed because ReLUs are not designed to multiply two different inputs,
2610000	2615000	but it turns out they can if you have enough of them and it's sufficiently cursed.
2615000	2619000	And yeah, we can just read this off the neurons.
2619000	2625000	Also, if you just put anything inside the model, it's beautiful and it's so periodic, and I love it.
2626000	2628000	Could I touch on that though?
2628000	2634000	Because you said you don't need to know the sine function because you can just memorise it within an interval.
2634000	2638000	Is that, I don't know, how does that break down?
2638000	2646000	Because it's discretising it and it's kind of assuming that it has the same behaviour in different intervals.
2646000	2657000	So, I think a key thing here is that you are solving modular addition on discrete one-hot encoded inputs
2657000	2660000	rather than for arbitrary continuous inputs.
2660000	2663000	Arbitrary continuous inputs is way harder.
2663000	2668000	And so you, it's not even on an interval, it's just learning snapshot.
2668000	2672000	It's just learning, like, single points on the sine and cosine curves.
2672000	2681000	And, I don't know, there's this family of maths about studying periodic functions with different kinds of Fourier transforms,
2681000	2692000	and this is all discussing discrete Fourier transforms, which are just a reasonable way of looking at periodic sequences of length n.
2692000	2696000	And that's how I recommend thinking about this one.
2696000	2703000	It's kind of, like, just quite different from a model that's trying to learn the true sine and cosine function.
2703000	2713000	And, yeah, the model then needs to convert the composed rotation back to the actual answer,
2713000	2717000	which is an even more galaxy-brained operation that you can read off from the wits.
2717000	2722000	So, you've got terms of the form cos A plus B.
2722000	2726000	The model has some weights mapping to each output C.
2726000	2735000	And it uses further trig identities to get terms of the form cos A plus B minus C times some frequency.
2735000	2738000	And where A and B are the two inputs, C is the output.
2738000	2746000	And you then use the softmax as an argmax to, like, extract the C that maximizes this.
2746000	2751000	And because cos is maximized at zero, this is maximized at C equals A plus B.
2751000	2755000	And if you choose the frequency right, this gets you mod N.
2755000	2759000	And you can just read this off the model weights. It's great.
2759000	2763000	And then, finally, you can verify you've understood it correctly,
2763000	2769000	because if you ablate everything that our algorithm says should not matter, performance improves.
2769000	2774000	While if you ablate any of the bits our algorithm says should matter, performance tanks.
2774000	2777000	Okay. Could you give me some intuition, though?
2777000	2782000	So, we start off in the memorization phase, because I guess you can think of a neural network
2782000	2786000	as doing many different things in a very complicated way,
2786000	2790000	and there's some kind of change in the balance during training.
2790000	2797000	So, it does the easy thing first, and then it gradually learns how to generalize.
2797000	2802000	And in this particular case, how does that thing, because we're using stochastic gradient descent,
2802000	2806000	so we're moving all of these weights around, and the inductive prior is also very important
2806000	2809000	and we'll come to that, I think, after we've spoken about Bill House paper.
2809000	2814000	But how does that happen gradually in really simple terms?
2814000	2820000	Hmm. Is the question kind of, it ends up at this discrete algorithm,
2820000	2823000	but it does survive continuous steps. How does that work?
2823000	2827000	Well, I think the thing that surprised a lot of people about grokking is this,
2827000	2831000	I mean, grokking, the clue's in the name. So, it's gone from memorization,
2831000	2834000	and then we're using stochastic gradient descent,
2834000	2838000	and you would think that it's gotten stuck in some kind of local minima.
2838000	2842000	And you're training, and you're training, and you're training, and then there's a spark.
2842000	2849000	Something happens, and then you get these new modes, kind of like emerging in the network.
2849000	2852000	I'm not sure if emerging is the right term.
2852000	2857000	And it happens gradually, and it happens after a long time.
2858000	2863000	Yeah. So, there's a couple of things here that's pretty easy to misunderstand.
2863000	2867000	The first is that... Hmm.
2867000	2871000	The first is that I think it's pretty hard for a model to ever get stuck,
2871000	2877000	because, I know, this model had about 200,000 parameters, model ones have billions.
2877000	2881000	It's just moving in a very high dimensional space,
2881000	2885000	and you can get stuck on 150,000 dimensions.
2885000	2887000	But you've got 50,000 to play with.
2887000	2894000	And especially for a fairly over-parameterized model like this one,
2894000	2899000	for a fairly simple task, there's just so much room to move around.
2899000	2904000	Another common misunderstanding of grokking is people say,
2904000	2908000	it's memorized, it's got zero loss, so why does it need to learn?
2908000	2910000	Two misunderstandings here.
2910000	2915000	First, zero loss is impossible unless you have bullshit floating point errors,
2915000	2920000	because it's like the average correct log prop.
2920000	2923000	Log of anything can never get to...
2923000	2928000	The log will never quite get to zero because of just how softmax works.
2928000	2932000	And you need to have an infinite logic for that to happen.
2932000	2936000	The one cute thing in an appendix to our paper is that
2937000	2941000	cannot represent log probes less than 1.19e-7,
2941000	2946000	which leads to bizarre loss spikes sometimes, unless you use float64.
2946000	2950000	Anyway, the second is regularization.
2950000	2954000	If you don't have any kind of regularization, the model will just continue to memorize.
2954000	2958000	We use weight decay, dropout also works,
2958000	2964000	and so the model, the kind of core tension behind grokking
2964000	2970000	is there's some feature of the lost landscape that makes it easier to get to memorization.
2970000	2977000	You can memorize faster, while generalization is somehow hard to get to and much more gradual.
2977000	2982000	So the model memorizes first, but it ultimately prefers to generalize,
2982000	2985000	but it's only a mild preference.
2985000	2989000	And the reason for this is we cherry pick the amount of data where it's a mild preference,
2989000	2994000	because there's too little, it will just always memorize if there's too much, it will immediately generalize,
2994000	2997000	because grokking is a little bit cheating.
2997000	3001000	And yeah, you then use this,
3001000	3006000	and because the model's initially memorized, but it wants to generalize,
3006000	3013000	it can follow, it memorizes until the desire to memorize more balances with the desire to have smaller weights,
3013000	3018000	but both of these reinforce the drive to generalize,
3018000	3022000	because that makes both of them happier.
3022000	3028000	And so the model very slowly interpolates, very, very slightly improving test loss,
3028000	3032000	very slowly improving train loss, until it eventually gets there,
3032000	3036000	and has this acceleration at the end, this phase transition,
3036000	3042000	and cleanup, which leads to the seemingly sudden grokking behavior.
3042000	3046000	Okay, and when you were talking about the, it wants the weights to be smaller,
3046000	3050000	so that's weight decay, and it's like an inductive bias,
3050000	3054000	essentially, to tell the model to reduce its complexity,
3054000	3057000	which is a pressure to generalize.
3057000	3061000	But if it wasn't for that, then that wouldn't happen.
3061000	3066000	So in the experiments I ran, if you don't have weight decay,
3066000	3070000	it will just keep memorizing infinitely far,
3070000	3073000	because when you get perfect accuracy, if you double all your logits,
3073000	3076000	you just get more confident in the right answer,
3076000	3078000	and so it just keeps scaling up.
3078000	3081000	I was using full batch training because it's such a tiny problem,
3081000	3084000	this made things smoother and easier.
3084000	3087000	I've heard some attic data that sometimes you can get it to work
3087000	3091000	if you just have mini batch stochastic gradient descent,
3091000	3094000	but I haven't looked into that particularly hard.
3094000	3095000	Interesting.
3095000	3099000	There are some hypotheses that stochasticity acts as an implicit regularizer
3099000	3101000	because it adds noise.
3101000	3102000	I don't really know.
3102000	3104000	So let's go back to Bilal's paper then.
3104000	3107000	So this paper, a toy model of universality,
3107000	3110000	reverse engineering, how neural networks learn group operations.
3110000	3112000	Can you give us an elevated pitch?
3112000	3119000	Yeah, so an observation that actually first discovered at a party
3119000	3122000	in the Bay Area from a guy called Sam Box,
3122000	3125000	is that the modular addition algorithm we found
3125000	3128000	is actually a representation theory algorithm.
3128000	3134000	So group representations are collections of symmetries
3134000	3137000	of some geometric objects that correspond to the group.
3137000	3140000	Modular addition is the cyclic group,
3140000	3148000	and rotations of the regular n-gon are the representations
3148000	3149000	of the cyclic group,
3149000	3152000	and this corresponds to the rotation by the unit circle
3152000	3154000	that can pose that we found.
3154000	3157000	But it turns out you can just make this work for arbitrary groups,
3158000	3161000	you replace the two rotations with just two representations,
3161000	3164000	you compose them, and the model,
3164000	3167000	and it turns out the cos a plus b minus c thing
3167000	3170000	is this math jargon called the character.
3170000	3172000	You don't mean to unsat any of that,
3172000	3175000	but it's very cute if, like me, you have a pure math degree.
3175000	3178000	And for example,
3178000	3182000	if you have the group of permutations of five elements,
3182000	3186000	the 120 different ways to rearrange five objects,
3187000	3189000	one example of representations of this
3189000	3195000	are rotations and reflections of the four-dimensional tetrahedra,
3195000	3199000	and if you train a one-hidden layer MLP to grok this
3199000	3200000	and look inside,
3200000	3203000	you're going to see these rotations that it's learned.
3203000	3205000	It's gorgeous.
3205000	3207000	And so the first half of that paper
3207000	3210000	was just showing that the algorithm worked,
3210000	3213000	showing that this was actually learned in practice.
3214000	3218000	Then the more interesting bit was this focus on universality.
3218000	3221000	So universality is this hypothesis,
3221000	3227000	that models have some intrinsic solutions to a problem
3227000	3229000	that many different models will converge on,
3229000	3232000	at least given similar data and similar architectures,
3232000	3235000	e.g. in image models,
3235000	3238000	models will learn specific neurons that detect curves,
3238000	3240000	and different models and different datasets
3240000	3242000	seem to learn this similar thing.
3243000	3247000	And here this was interesting
3247000	3252000	because groups have a finite set of irreducible representations,
3252000	3254000	maths theorem.
3254000	3255000	You can enumerate these.
3255000	3257000	There are that many of them.
3257000	3260000	And for groups that are not modular addition,
3260000	3263000	these are qualitatively different,
3263000	3266000	like some of them act on a four-dimensional object,
3266000	3267000	like the tetrahedron,
3267000	3270000	some of them act on like 5D or 60 objects.
3270000	3272000	Naively, some of them are simpler than others,
3272000	3274000	but they're definitely different.
3274000	3277000	So what we did is we asked ourselves the question,
3277000	3279000	which one does the model learn?
3279000	3282000	And we found that even if you just vary the random seed,
3282000	3285000	the model will randomly choose a subset of themes
3285000	3286000	each time to learn.
3286000	3288000	And there's some structure,
3288000	3293000	like it tends to learn some of them more often than others.
3293000	3297000	A little bit maps to our insurance of notion of simplicity,
3297000	3298000	but not that much.
3298000	3300000	One of the updates I made in the paper
3300000	3302000	is that simplicity is a really cursed concept,
3302000	3304000	I don't understand very well,
3304000	3305000	where I don't know.
3305000	3307000	If you have rotations of a four-dimensional object,
3307000	3308000	that seems simpler,
3308000	3311000	but maybe the 60 object takes more dimensions
3311000	3313000	but has better loss per unit weight norm,
3313000	3316000	which is simpler, I don't know.
3316000	3318000	But yeah, anyway,
3318000	3320000	we found that each run of the model learns
3320000	3322000	some combination of these circuits
3322000	3324000	for the different representations.
3324000	3326000	It's like normally more than one,
3326000	3328000	the exact number varies,
3328000	3331000	and which ones it learns is seemingly random each time,
3331000	3336000	which suggests that all toy models lie to you, obviously.
3336000	3339000	But if we're trying to reason about real networks,
3339000	3343000	looking at this work might suggest the explanation,
3343000	3347000	the hypothesis that if there are multiple ways
3347000	3348000	to implement a circuit,
3348000	3350000	which in practice they normally are,
3350000	3354000	models may learn different ones of them,
3354000	3357000	kind of for fairly random reasons,
3357000	3359000	and the fully understanding one model
3359000	3362000	will not perfectly transfer to another model.
3362000	3365000	And I think there's loads of really interesting
3365000	3366000	open questions here.
3366000	3369000	Like, I don't know,
3369000	3371000	people have done various work understanding
3371000	3374000	different kinds of specific circuits and models,
3374000	3376000	like the interoperability in the wild paper
3376000	3377000	we'll get to later.
3377000	3380000	What does this look like in other models?
3380000	3383000	Often there's multiple ways to implement a circuit.
3383000	3385000	Can you disentangle the two,
3385000	3387000	do all models learn both,
3387000	3390000	or do some models learn one, some learn the other?
3390000	3392000	I don't really know.
3392000	3393000	So a couple of questions.
3393000	3395000	I mean, first of all, this is leading towards this idea
3395000	3397000	that we were speaking about before,
3397000	3401000	which is that even if different networks,
3401000	3403000	slightly different problems or variations
3403000	3404000	on the same problem,
3404000	3406000	it could learn these algorithmic primitives.
3406000	3409000	Now, the first observation here is that
3410000	3413000	the inductive biases of the network
3413000	3416000	differ massively, right?
3416000	3419000	So to what extent do the inductive biases
3419000	3422000	affect these primitives which are learned?
3422000	3424000	Oh, so much.
3424000	3425000	They do.
3425000	3426000	So...
3426000	3428000	Well, can I frame the question a little bit?
3428000	3430000	Because this reminds me a lot of
3430000	3432000	the geometric deep learning blueprint
3432000	3436000	from Petir and Michael Bronstein and all those guys.
3436000	3438000	And they were coming at this
3438000	3440000	from exactly the same direction as you,
3440000	3443000	that they said there's a representation of a domain,
3443000	3445000	which is basically a symmetry group,
3445000	3448000	and you can do all of these different transformations,
3448000	3451000	and as long as they fall in different positions
3451000	3452000	in the underlying domain,
3452000	3455000	so they respect the structure, then it works.
3455000	3460000	But all of those symmetries are effectively coded
3460000	3462000	into the inductive prior.
3462000	3464000	So, for example, if a CNN
3464000	3467000	works on this gridded 2D manifold
3467000	3471000	and it explicitly models
3471000	3473000	translational equivalents
3473000	3476000	and local connectivity and weight sharing and so on.
3476000	3478000	So I guess what I'm saying is,
3478000	3481000	you're talking about this four-dimensional tetrahedra,
3481000	3486000	and that isn't explicitly modeled in an MLP.
3486000	3487000	Not so.
3487000	3490000	So how are you even recognizing
3490000	3492000	that it's learning those symmetries?
3492000	3493000	How are you even probing it?
3493000	3495000	Maybe we should start with that.
3495000	3499000	So I guess thing one, models are just smarter than you, man.
3499000	3502000	Models can do a lot of weird stuff.
3502000	3505000	I feel like the story of deep learning
3505000	3508000	is people initially thought they needed to spoonfeed
3508000	3513000	these models the right inductive biases over the data.
3513000	3515000	And we've gradually realized,
3515000	3517000	oh, wait, no, no, this is fine.
3517000	3518000	The models can figure it out.
3518000	3520000	For example, early on,
3520000	3523000	image models were convolutional networks.
3523000	3526000	You tell it the key information is nearby,
3526000	3528000	and if you translate the image, it doesn't matter.
3528000	3531000	And now everyone uses transformers, including for images,
3531000	3534000	and transformers replace the convolutional mechanism
3534000	3536000	with attention, where you're now saying,
3536000	3539000	okay, one-sixth of your parameters
3539000	3541000	are dedicated to figuring out
3541000	3544000	where to move information between positions.
3544000	3546000	Sometimes it'll be a convolution,
3546000	3548000	and sometimes models do learn convolution,
3548000	3549000	but often it won't be.
3549000	3551000	And we want you,
3551000	3554000	and you can now spend the parameters to figure this out.
3554000	3559000	And I'm not very familiar with the geometric deep learning literature,
3559000	3561000	but I generally am just kind of like,
3561000	3562000	models can figure it out.
3562000	3565000	The way we figured out that this was what's going on
3565000	3570000	is kind of analogous to what we did in the modular audition case,
3570000	3572000	where we just look at the embedding matrix
3572000	3575000	and just read off the learned sine and cosine terms.
3575000	3578000	Here we said, okay,
3578000	3584000	the rotations of the 4D tetrahedron are these like 4x4 matrices.
3584000	3586000	You can flatten this to a 16-dimensional vector.
3586000	3589000	Let's probe for that linearly.
3589000	3591000	And this kind of works,
3591000	3594000	and you can probe for the different representations
3594000	3597000	and basically see what's going on.
3597000	3601000	Okay, I think that the thrust of the geometric deep learning stuff,
3601000	3604000	or any inductive prior, comes back to the bias-variance trade-off.
3604000	3606000	And the cursive dimensionality.
3606000	3609000	So no one's saying, of course, an MLP,
3609000	3613000	if you look at the function space that it can approximate,
3613000	3616000	it's exponentially larger than that of a CNN.
3616000	3620000	So it was always about sample efficiency.
3620000	3623000	So, yeah, an MLP can learn anything,
3623000	3626000	but we would never be able to train it for most problems.
3626000	3628000	Mm-hmm.
3628000	3630000	Yeah.
3630000	3635000	So, I guess I maybe want to avoid going too deeply into this
3635000	3638000	because I think the modular addition problem
3638000	3642000	and the group problem is just a very weird problem.
3642000	3646000	There's an algorithm that it's fairly natural for a model to learn
3646000	3652000	with literally a single nonlinear step of the matrix multiply.
3652000	3655000	One very cute result from the last paper
3655000	3658000	is that the model can implement two 4x4 matrix multipliers
3658000	3663000	with a single ReLU layer, which is very cute.
3663000	3667000	But, yeah, there's a fairly natural algorithm to implement.
3667000	3669000	That's a certain...
3669000	3673000	Yeah, another useful intuition is that the more data you have,
3673000	3675000	the more complex memorization gets,
3675000	3679000	while generalization is exactly as complex at each point.
3679000	3682000	And, yeah.
3682000	3684000	So there's always going to be a crossover point
3684000	3687000	where we have enough data where it is simpler to learn the circuit
3687000	3690000	that generalizes.
3690000	3692000	And I don't know.
3692000	3696000	I'm hesitant to draw too much from toy models about the real problem.
3696000	3700000	I guess one, two final points I'd want to just leave on this section.
3700000	3702000	The first is I just want to re-emphasize.
3702000	3705000	I did not do the toy model of the universality paper.
3705000	3708000	I was supervising a mentee, Bella Chucktie, who did it.
3708000	3710000	He did a fantastic job.
3710000	3712000	So thanks, Bella, for your listening.
3712000	3716000	Secondly, for the module addition case,
3716000	3719000	I had no idea this outcome was going to be there when I went in.
3719000	3723000	I just poked around, noticed the weird periodicity,
3723000	3726000	realized it was using, I should apply Fourier transforms,
3726000	3729000	and then the whole problem kind of fell together.
3729000	3733000	And to me, the real takeaway of this paper is like,
3733000	3735000	I don't give a fuck about GROCK.
3735000	3740000	It is genuinely possible to understand what is going on in a model.
3740000	3743000	You don't need to know what's going on in advance to discover this.
3743000	3749000	And there is beautiful, non-trivial structure that can be understood.
3749000	3752000	And who knows if this will happen in, like, actual full models.
3752000	3756000	But to me, this is much more compelling than if we had nothing at all.
3756000	3757000	Beautiful.
3757000	3758000	Okay.
3758000	3759000	And just before we move off the section,
3759000	3763000	Biloud had a beautiful Twitter thread, actually.
3763000	3766000	And he was talking about the potential for what he called
3766000	3770000	a periodic table of universal circuits.
3770000	3773000	And I actually think that's a really cool idea.
3773000	3775000	So that would be amazing if that would work out.
3775000	3777000	But he also brought up the lottery ticket hypothesis,
3777000	3780000	and I've interviewed Jonathan Frankel.
3780000	3784000	And the idea there is that some of this information
3784000	3787000	might actually be encoded and understandable at initialization
3787000	3789000	before you even start training.
3789000	3794000	And apparently, you folks have found weak evidence
3794000	3797000	for this in at least one group.
3797000	3798000	Ah.
3798000	3799000	All right.
3799000	3802000	So a couple of things there.
3802000	3805000	So this idea of a periodic table of circuits,
3805000	3809000	I believe, is originated in this post called Circuits Zoom In
3809000	3812000	from Chris Ola.
3812000	3813000	We probably cannot claim credit.
3813000	3814000	Good job, Chris.
3814000	3817000	It's a beautifully evocative term.
3817000	3819000	Yeah, the story of basically everything in Mac and Terp
3819000	3821000	is, yeah, there was this Chris Ola paper from, like,
3821000	3824000	two years ago that has it somewhere inside.
3824000	3827000	Anthropic recently put out this beautiful blog post
3827000	3830000	called Interpretability Dreams about their vision
3830000	3833000	for the field of mechanistic interpretability
3833000	3835000	and the kind of subtext.
3835000	3838000	So they kept just quoting bits of old papers being like,
3838000	3840000	so we already said this, but let's now, like,
3840000	3843000	summarize it better and be clear about how this
3843000	3845000	sits into our overall picture.
3845000	3849000	Anyway, so, yeah, the idea of the periodic table is
3849000	3853000	maybe there is just some finite list of ways a thing
3853000	3856000	can be implemented naturally in a massive stack of matrices
3856000	3860000	that we can enumerate by studying one or maybe several
3860000	3864000	networks, understand them, and then compile all of this
3864000	3867000	into something beautiful.
3867000	3869000	And...
3869000	3873000	which is kind of what we found in the representations case.
3873000	3876000	Though here it was nice because there were
3876000	3880000	genuinely a finite set that we could have fully enumerate.
3880000	3884000	Regarding the lottery ticket stuff,
3884000	3888000	I think this was a random observation I had on the
3888000	3892000	Modular Edition case, partially inspired by a result
3892000	3896000	from Eric Michaud at MIT, who was involved in some
3896000	3899000	other papers on Grocking.
3899000	3903000	And so what we found is that at the end of training,
3903000	3907000	there are these directions in the weights that represent
3907000	3910000	like the sine and cos terms of frequency,
3910000	3913000	14 pi over 113.
3913000	3915000	And...
3915000	3917000	if you look at the embedding at the start and project one
3917000	3921000	to these directions, it's like surprisingly circular.
3921000	3925000	It's like the model has extracted those directions.
3925000	3930000	And my wildly unsubstantiated hypothesis for why models
3930000	3934000	just learn these algorithms and circuits at all
3934000	3937000	is that there are some directions that if you
3937000	3941000	deleted everything else would like form this beautiful circuit.
3941000	3945000	This is kind of a trivial statement about linear algebra for the most part.
3945000	3948000	And this underlying hidden circuit, each bit reinforces
3948000	3951000	each other systematically because they're useful.
3951000	3954000	Well, everything else is kind of noise, so it gets kind of gradually
3954000	3956000	decayed.
3956000	3960000	Over time, this will give you the circuit in a way that looks
3960000	3962000	surprising and emergent.
3962000	3966000	And this also can partially explain why phase transitions
3966000	3968000	happen.
3968000	3970000	There was a really good post from Adam German and Bach Schleggeres
3970000	3976000	called on S-shaped curves, which argue that
3976000	3980000	if you've got something that's like the composition of multiple different
3980000	3983000	weight matrices, let's just say two of them,
3983000	3986000	the gradient on the first is proportional to how good the
3986000	3988000	second is and vice versa.
3988000	3991000	So at the start, they both grow very slowly, but then they'll
3991000	3995000	reinforce each other and eventually cascade as they're optimizing
3995000	3998000	on the problem in a way that looks kind of sudden and S-shaped.
3998000	4001000	And so my understanding is the original lottery ticket
4001000	4004000	hypothesis is kind of discreet.
4004000	4007000	It's looking on the neuron level and it's learning masks over
4007000	4009000	weights and over neurons.
4009000	4013000	And I'm kind of discussing and in some sense much more trivial
4013000	4017000	version, where I'm not assuming there's some canonical basis
4017000	4019000	of neurons.
4019000	4022000	I'm saying, well, there's some directions in space that matter
4022000	4025000	and if you delete all other directions, everything kind of
4025000	4029000	works, which I think is a much more trivial statement,
4029000	4032000	though the space of possible neurons is enormous.
4032000	4034000	Though I don't know.
4034000	4036000	One thing you want to be pretty careful of when discussing this
4036000	4041000	the mask you learn is the computation since, and no,
4041000	4044000	there's probably quite a lot of algorithms can be cleverly
4044000	4048000	expressed with a mask over a Gaussian normal matrix.
4048000	4051000	But I don't know.
4051000	4055000	Part two, how do machine learning models represent their
4055000	4056000	thoughts?
4056000	4059000	Now we're taught in machine learning 101 that neural networks
4059000	4064000	represent hypotheses which live on a geometric domain and
4064000	4067000	the objective priors learn to generalize symmetries which
4067000	4069000	exist on the underlying geometric domain.
4069000	4073000	And you're talking about them representing a space of
4073000	4075000	algorithms, which we're going to explore.
4075000	4078000	Now, one thing that I wanted to touch on is that they learn
4078000	4081000	the mapping to extensional attributes, not intentional
4081000	4084000	attributes, intentions, but with an S.
4084000	4087000	And we'll come back to what I mean by that in a second.
4087000	4090000	But I think it's quite popular for people to think of neural
4090000	4093000	networks principally as a kind of hash table.
4094000	4096000	Or locality sensitive hash table.
4096000	4099000	And the generalization part comes from the representation
4099000	4104000	mapping function, which is on this embedded Hilbert space,
4104000	4108000	which is the vector space of the attributes, which then
4108000	4111000	resolves a pointer to a static location on the underlying
4111000	4112000	geometric domain.
4112000	4115000	Now this can mimic an algorithm, especially when the
4115000	4119000	inductive prior itself is increasingly algorithmic like
4119000	4122000	a graph neural network, for example, which behaves in a
4122000	4126000	very similar way to a prototypical dynamic programming
4126000	4127000	algorithm.
4127000	4129000	There's some great work actually on algorithmic reasoning
4129000	4132000	by Petr Felichkovich, one of your colleagues now at
4132000	4133000	DeepMind.
4133000	4138000	But he showed in his algorithmic reasoning work that
4138000	4142000	transformers can't perform certain graph algorithms.
4142000	4145000	I think he gave Dykstra as an example and he said it's
4145000	4148000	because there's this aggregation function in a
4148000	4151000	transformer, which isn't in a GNN.
4151000	4154000	So I just wondered if you could kind of like compare and
4154000	4159000	contrast whether or not neural networks are performing
4159000	4162000	algorithmic generalization and the differences between
4162000	4165000	let's say GNNs and transformers.
4165000	4170000	Yeah, so I'm not very familiar with GNNs, so I'll probably
4170000	4173000	avoid commenting on GNNs versus transformers, so a fear of
4173000	4175000	embarrassing myself.
4175000	4179000	In terms of the underlying thing.
4179000	4183000	So I definitely think we have some pretty clear evidence at
4183000	4187000	this point that models are doing some genuine algorithms.
4187000	4190000	I don't know if I think my modular addition thing is a
4190000	4192000	pretty clear proof of concept of this.
4192000	4198000	Yeah, so one thing worth stressing is that I generally
4198000	4201000	think of models as having linear representations, more
4201000	4204000	than geometric representations.
4204000	4209000	So I think of an input to a model as having many
4209000	4213000	different possible features where features are kind of a
4213000	4219000	property of the input in an intentional sense, but which is
4219000	4222000	kind of a fuzzy and garbage definition.
4222000	4225000	So I prefer the existential definition of like an example
4225000	4229000	of a feature is like this bit of an image contains a curve
4229000	4233000	or this bit of an image corresponds to a car window or
4233000	4237000	this is the final token in Eiffel Tower or this corresponds
4237000	4241000	to a list variable and Python with at least four elements
4241000	4244000	and all kinds of stuff like that.
4244000	4250000	And well, I know this this scene is shaded blue because
4250000	4253000	someone put the wrong filter on the camera.
4253000	4258000	And yeah, I generally think of models as representing
4258000	4261000	features as linear directions in space.
4261000	4266000	And each input is a linear combination of these directions.
4266000	4271000	And this is kind of the classic words to vet framing, like
4271000	4277000	the king minus man equals queen minus woman thing where you
4277000	4279000	can kind of think of this as there being a gender direction
4279000	4281000	and there being a royalty direction.
4281000	4285000	And these are like the right units of analysis rather than
4285000	4289000	king, queen, man, women being the right units of analysis.
4289000	4293000	But where each of these is made up out of these underlying
4293000	4295000	linear representations.
4295000	4300000	And this is a fairly different perspective to the geometric
4300000	4302000	where are things in a manifold?
4302000	4304000	How close are they together in Euclidean space?
4304000	4309000	Because that's all kind of a global statement about how close
4309000	4314000	two things are where you're comparing all possible features
4314000	4319000	while I don't know the Eiffel Tower and the Colosseum are
4319000	4322000	close together in some conceptual space because they're both
4322000	4325000	European landmarks, but they're also very different because
4325000	4328000	France and Italy are fairly different countries in some
4328000	4329000	sense.
4329000	4333000	And maybe they're different on a bunch of other features or
4333000	4334000	one of them is two words.
4334000	4337000	The other is one word, which really matters in some ways.
4337000	4343000	And Euclidean distance and geometry is it's a gloomy
4343000	4345000	it's a global summary statistic.
4345000	4348000	And all summary statistics light you.
4348000	4350000	There's another motto of mine.
4350000	4354000	But in particular, global ones I'm very skeptical of.
4354000	4361000	And yeah, in general, this how what is the structure of a model
4361000	4362000	representations?
4362000	4364000	I think it's like a really important question.
4364000	4369000	And in particular, models are such high dimensional objects
4369000	4374000	that you really want to be careful to distinguish between the
4374000	4381000	two separate things of sorry models are such high dimensional
4381000	4385000	objects that it's basically impossible to understand GBD
4385000	4387000	three is a 200 billion dimensional vector.
4387000	4392000	You need to be breaking it down into units of analysis that
4392000	4396000	can vary independently and independently meaningful.
4396000	4401000	And the linear representation hypothesis is like a pretty
4401000	4405000	load bearing part of how I think about this stuff because it is
4405000	4411000	so because it allows you to break things down.
4411000	4414000	And it seems to be a true fact about how models do things.
4414000	4417000	Though again, we don't have that much data because we never
4417000	4418000	have enough data.
4418000	4419000	It's really sad.
4420000	4425000	And yeah.
4425000	4427000	Well, that's contrast a little bit.
4427000	4431000	So this linear representation hypothesis, this idea that the
4431000	4435000	models break down inputs into many independently varying features
4435000	4439000	and store them as directions in space, much like word to veck.
4439000	4444000	And the the go fi people, I mean, like photo and pollution,
4444000	4448000	they they they brought out this famous critique of connectionism
4448000	4449000	in 1988.
4449000	4452000	And their main argument was systematicity.
4452000	4455000	And they were talking about intention versus extension.
4455000	4458000	And it might just be worth defining what I mean by that.
4458000	4463000	So if I said the teacher of Socrates was Plato, the extension
4463000	4464000	is Plato.
4464000	4467000	The intention is everything.
4467000	4468000	It's the teacher.
4468000	4469000	It's Socrates.
4469000	4472000	You know, if I said four plus five equals nine, nine is the
4472000	4475000	extension for and plus and five is the intention.
4475000	4477000	So they were saying something very simple.
4477000	4480000	They said in a neural network, the intentional attributes get
4480000	4481000	discarded.
4481000	4484000	And that's why the network don't support what they call
4484000	4485000	compositionality.
4485000	4489000	Now, compositionality is actually quite an abstract term
4489000	4493000	because using vector algebra in these analogical reasoning tasks
4493000	4494000	that you were just talking about.
4494000	4495000	So king and queen and so on.
4495000	4497000	That's a form of compositionality.
4497000	4501000	But they would say it's a poor cousin of compositionality because
4501000	4507000	it's only using, you know, the representation is in a vector
4507000	4508000	space.
4508000	4511000	And in a vector space, you only have very basic primitive
4511000	4512000	transformations.
4512000	4515000	So you wouldn't be able to, I mean, for example, when you're
4515000	4518000	talking about Paris earlier, you wouldn't do the kind of
4518000	4520000	analogical reasoning they were talking about being able to
4520000	4523000	downstream, say, were they in Paris?
4523000	4525000	Is Paris in Europe?
4525000	4529000	Of course, it does happen in this linear representation theory.
4529000	4533000	But it happens in a very different way.
4533000	4534000	Hmm.
4534000	4539000	So I guess I'm not sure I fully followed that.
4539000	4543000	I mean, this might be a cheap gotcha, but a fact about
4543000	4548000	transformers is there's, they have this central object called
4548000	4553000	the residual stream, which I know in standard framing to be
4553000	4556000	thought of as the thing that lives in the skip connections.
4556000	4560000	But not even as like the key thing about a transform where
4560000	4563000	each layer reads its input from the residual stream and adds
4563000	4565000	its output back to the residual stream.
4565000	4568000	And the residual stream is kind of this shared bandwidth and
4568000	4569000	memory.
4569000	4573000	And this means that nothing's ever thrown away unless the
4573000	4577000	model explicitly is trying to do that, or is just applying
4577000	4579000	some gradual decay over time.
4579000	4582000	So, you know, if you've got an MLP layer that's saying I've got
4582000	4585000	four, I've got plus, I've got five, and I want to compute nine,
4585000	4586000	and plus is still there.
4586000	4589000	I don't know if this actually engage with your points and
4589000	4593000	like, I don't know if this matters, but it's true.
4593000	4596000	Yeah, what you're saying is true, but I think the point is
4596000	4601000	that those primitives are not actually representable in a
4601000	4602000	neural network.
4602000	4605000	So you're saying with this residual stream, all of the
4605000	4608000	extensions that came previously also get passed up.
4608000	4612000	So in a later layer, you can refer to an extension.
4612000	4615000	So the, basically the answer of a computation that happened
4615000	4619000	upstream, but what you can't refer to are the intentional
4619000	4622000	attributes of that computation upstream.
4622000	4624000	Why not?
4624000	4626000	Like four is an input.
4626000	4628000	So you can refer to four, because you could think of
4628000	4631000	reading the input as a computation.
4631000	4633000	Plus is another thing you read.
4633000	4635000	Five is another thing you read.
4635000	4638000	Like what is a thing that is not an output of a computation
4638000	4640000	within this framework?
4640000	4643000	I might have to get back to you on that.
4643000	4647000	Where's Keith Duggar when you need him?
4647000	4652000	What would be a good example of that?
4652000	4657000	I mean, I guess it's about symbol manipulation as well.
4657000	4661000	So these things could actually be symbolic operations which
4661000	4664000	can be composed and reused later.
4664000	4667000	And you would appreciate that a neural network is only ever
4667000	4669000	passing values.
4669000	4673000	So for example, if it did something which you could represent
4673000	4676000	with a symbolic operation, if you wanted to use that again,
4676000	4681000	I mean in an MLP, the reason why we use a CNN is because we
4681000	4684000	want to represent the same thing in different places.
4684000	4686000	And an MLP would have to learn it.
4686000	4688000	It doesn't support translational equivalence.
4688000	4691000	So it would have to learn the same thing a million times.
4691000	4694000	And it's the same thing with this symbolic compositional
4694000	4698000	generalization that if it actually had this symbolic
4698000	4702000	representation which it used once, it could use it everywhere.
4702000	4705000	But now it has to relearn it everywhere.
4705000	4707000	Right.
4707000	4713000	Like you could, if the model wants to know that Paris
4713000	4717000	is the capital of France, it can spend some parameters on that.
4717000	4719000	And for every other capital it needs to separately spend
4719000	4723000	parameters and it can't just have a general map country to
4723000	4725000	have a capital operation.
4725000	4727000	Yeah, that's exactly right.
4727000	4729000	Let's use a simple example.
4729000	4734000	So we use an MLP image classifier and I put a tennis ball in
4734000	4737000	and it's in the bottom left of the visual field.
4737000	4742000	And then I put it in the top right and nothing it's learned
4742000	4745000	from the bottom left will be used.
4745000	4749000	So it just feels like we're wasting the representational
4749000	4751000	capacity just doing the same thing again and again.
4751000	4756000	And in a transformer, the only reason it does have that
4756000	4760000	recognition, that that's a equivalence in respect of the
4760000	4763000	position of a pattern is because of the transformer
4763000	4765000	inductive prior, presumably.
4765000	4767000	Yes.
4767000	4771000	So it uses the same parameters at each position in the input
4771000	4772000	sequence.
4772000	4775000	It should be able to do bottom left and top right properly,
4775000	4779000	though it does not necessarily have things like rotation built
4779000	4780000	in.
4780000	4782000	I don't know.
4782000	4785000	I feel like machine learning is full of these people who have
4785000	4787000	all kinds of theoretical arguments.
4787000	4789000	And then they're like, this should be efficient.
4789000	4790000	This should not work.
4790000	4793000	And then GPT-4 lobs at them.
4793000	4795000	And I don't know.
4795000	4800000	No theory is interesting and isolation unless it models
4800000	4802000	reality well.
4802000	4804000	And I don't know.
4804000	4806000	I haven't really engaged with this theory in the same way I
4806000	4809000	haven't engaged with most deep learning theory, because it
4809000	4812000	just doesn't seem to meet my bar of does this make real
4812000	4814000	predictions about models?
4814000	4817000	The maximal update parameterization paper from Greg
4817000	4820000	Yang was actually a recent contradiction to this.
4820000	4821000	Right.
4821000	4824000	Of really interesting theory that makes real predictions
4824000	4827000	about models that bear out and get you zero shot hyper parameter
4827000	4828000	to transfer.
4828000	4831000	But like most things just don't do that.
4831000	4833000	Very interesting.
4833000	4834000	Okay.
4834000	4835000	Okay.
4835000	4838000	Well, I think now is a beautiful opportunity to move over to
4838000	4839000	Othello.
4839000	4842000	Now, there was a recent paper called do large language models
4842000	4845000	learn world models, or are they just surface statistics by
4845000	4846000	Kenneth Lee?
4846000	4849000	And he said that the recent increase in model and data size
4849000	4852000	has brought about qualitatively new behaviors such as writing
4852000	4855000	code or solving logic puzzles.
4855000	4858000	Now, he asked the question, yeah, how do these models achieve
4858000	4859000	this kind of performance?
4859000	4861000	Do they merely memorize training data?
4861000	4864000	Well, are they picking up the rules of English grammar and
4864000	4866000	grammar and the syntax of the sea language?
4866000	4869000	For example, are they building something akin to an internal
4869000	4873000	world model, an understandable model of the process producing
4873000	4874000	the sequences?
4874000	4877000	And he said that some researchers argue that this is fundamentally
4877000	4881000	impossible for models trained with guess the next word to learn
4881000	4885000	the language, meanings of language, and their performances is
4885000	4888000	merely surface statistics, you know, which is to say a long list
4888000	4892000	of correlations that do not reflect a causal model of the
4892000	4894000	process generating the sequence.
4894000	4898000	Now, you said, Neil, that a major source of excitement about
4898000	4901000	the original Othello paper was that it showed that predicting
4901000	4905000	the next word spontaneously learned the underlying structure
4905000	4906000	generating its data.
4906000	4909000	And you said that the obvious inference is that a large
4909000	4912000	language model trained to predict the next token may
4912000	4914000	spontaneously model the world.
4914000	4915000	What do you think?
4915000	4917000	Uh, yes.
4917000	4921000	So I should clarify that paragraph was me modeling why other
4921000	4923000	people are excited about paper.
4923000	4924000	Okay.
4924000	4926000	But whatever, I can roll with this question.
4926000	4930000	So and maybe bring in your less wrong piece as well.
4930000	4931000	Yeah.
4931000	4932000	Yes.
4932000	4935000	So the, yeah, I thought careless paper was super interesting.
4935000	4940000	The exact setup was they train.
4940000	4944000	So Othello is this chess and go like board game.
4944000	4949000	They took a data set of random legal moves in Othello.
4949000	4954000	They trained a model to predict the next move given a bunch
4954000	4956000	of these transcripts.
4956000	4959000	And then they probed the model and found that it had learned a
4959000	4962000	model of the board state, despite only ever being told to
4962000	4964000	predict the next move.
4964000	4968000	And so the way I would define world model is that there's
4968000	4973000	some latent variables that generate the training data.
4973000	4978000	Um, in this case, what the state of the board is, um, these
4978000	4982000	change over time, like over the sequence, but at least for a
4982000	4987000	transform, which has a sequence and the model kind of has an
4987000	4989000	internal representation of this at each point.
4989000	4991000	And they showed that you can probe for this.
4991000	4996000	And they showed that you can causally intervene on this and the
4996000	4999000	model will make legal moves in the new board, even if the
4999000	5001000	board status impossible to reach.
5001000	5003000	Point of order.
5003000	5005000	Can you explain what you mean by probe?
5005000	5007000	Yes.
5007000	5011000	So probing is this like old family of interoperability
5011000	5012000	techniques.
5012000	5017000	The idea is you think a model has represented something like you
5017000	5022000	give it a picture and you tell it as a class by the image and
5022000	5025000	you want to see if it's figured out that the picture is of a
5025000	5027000	red thing versus a blue thing, even though this isn't an
5027000	5029000	explicit part of the output.
5029000	5035000	You take some neuron or layer or just any internal vector of
5035000	5041000	the model and you train some classifier to map that to like
5041000	5045000	red or blue and you do something like a logistic regression
5045000	5048000	to see if you can extract whether it's red or blue from them.
5048000	5052000	And, uh, there's also interesting enough about probing, but
5052000	5054000	I should probably finish explaining the Othello paper first
5054000	5056000	before I get into that tangent.
5056000	5060000	So, yeah, the like reason people are really excited about this
5060000	5064000	paper was recently an oral eichle and generally got a lot of
5064000	5068000	hype was that it was just you train something predict the next
5068000	5072000	token and it forms this rich emergent model of the world.
5072000	5075000	And forming a model of the world is actually incredibly
5075000	5076000	expensive.
5076000	5081000	They like each cell of the 64 cell Othello board has three
5081000	5085000	possible states, three to the 64, it's quite a lot of information
5085000	5088000	to represent, but the model did it.
5088000	5091000	And lots of people were like, oh, clearly language models have
5091000	5093000	walls.
5093000	5099000	My personal interpretation of all this is that language models
5099000	5100000	predict the next token.
5100000	5104000	They learn effective algorithms for doing this within the
5104000	5108000	constraints of what is natural to represent within transformer
5108000	5109000	layers.
5109000	5115000	And what this means is that if predicting the next token is
5115000	5119000	made easier by having a model of the world of like, I don't
5119000	5123000	know who the speaker is, this is a thing that will happen.
5123000	5127000	And in some work led by Wes Gurney that we're going to talk
5127000	5130000	about later, we found neurons that detected things like this
5130000	5133000	Texas in French, this Texas Python code.
5133000	5137000	And in some sense, this is like a particularly trivial
5137000	5138000	model.
5138000	5145000	And so, yeah, that's an interesting thing.
5145000	5148000	In my opinion, it was kind of a priori obvious that language
5148000	5153000	models would learn this if they could and needed to, and it was
5153000	5155000	more efficient.
5155000	5158000	And at the point forward, though, learning that something is
5158000	5161000	French seems categorically different.
5161000	5166000	Because when I read Kenneth's original piece, he showed what
5166000	5170000	looked like a topological representation of the world.
5170000	5174000	So how different state spaces were related to each other in a
5174000	5177000	kind of network structure.
5177000	5180000	Hmm.
5180000	5183000	So.
5183000	5187000	I wonder if you can remember how we produced that diagram.
5187000	5189000	Yeah, so I'm starting to remember the details.
5189000	5191000	I think it was something of the form.
5191000	5194000	Look at how different cells are represented in the model and look
5194000	5197000	at how close together the representations of different
5197000	5201000	cells are, and the model has kind of got internal representations
5201000	5203000	that are close together.
5203000	5206000	I don't think this is fundamentally different from the
5206000	5208000	king, queen, man, women thing.
5208000	5210000	It's just that it's like learn some structural representations
5210000	5212000	that's obviously kind of reasonable.
5212000	5213000	Yeah.
5213000	5217000	I wouldn't read too much into that.
5217000	5220000	Models learn structural representations, I think, as old
5220000	5223000	news at this point.
5223000	5227000	But maybe another interesting angle is that one of the reasons
5227000	5233000	why people like Gary Marcus, they say GPT is parasitic on the
5233000	5234000	data.
5234000	5239000	They say because they are empirical models, most of the meaning,
5239000	5243000	most of the information is not in the data if we have to reason
5243000	5245000	over explicit world models.
5245000	5248000	So he thinks the reason a GPS is so good is because we've
5248000	5251000	imputed this abstract world model.
5251000	5255000	And similarly, when we play chess, we have an abstract world
5255000	5256000	model.
5256000	5259000	And he would argue that the information about that abstract
5259000	5261000	world model doesn't exist in any data.
5261000	5264000	So how do you go from the data to the model?
5264000	5267000	And the Othello games seem to show that you could go from the
5267000	5269000	data to the model.
5269000	5270000	Yeah.
5270000	5273000	I think that viewpoint is just obviously wrong.
5273000	5278000	Like, you're trying to do a data prediction problem.
5278000	5283000	A valid solution to that is to model the underlying world and
5283000	5285000	use this to predict what comes next.
5285000	5287000	There's clearly enough information in an information
5287000	5289000	theoretic sense to do this.
5289000	5294000	And the question is, is a model capable of doing that or not?
5294000	5296000	And I don't know.
5296000	5299000	I'm just like, you can't write poetry with statistical
5299000	5300000	correlations.
5300000	5302000	You need to be learning something.
5302000	5304000	Maybe that's not a good example.
5304000	5306000	I don't believe you can write like...
5306000	5308000	Yes, you can.
5308000	5314000	I don't believe you can produce, like, good answers to, like,
5314000	5317000	difficult code forces problems.
5317000	5319000	It's like, do good software engineering.
5319000	5324000	There's purely a bundle of statistical correlations.
5324000	5326000	Maybe I have too much respect for software engineers.
5326000	5328000	I don't know.
5328000	5330000	So where does it come from then?
5330000	5334000	That flash of inspiration or that higher level...
5334000	5337000	I guess the first question is, is there a jump?
5337000	5340000	Is it actually grounded in the data it's trained on?
5340000	5344000	Or is there some high-level reasoning?
5344000	5347000	Where does that materialize from?
5347000	5351000	So the way I think about it, there is just a space of
5351000	5354000	possible algorithms that can be implemented in a
5354000	5356000	lot of ways.
5356000	5362000	And some of these look like a world model.
5362000	5365000	And some of these look like a bunch of statistical correlations.
5365000	5368000	And models are trading off lots of different resources.
5368000	5370000	Like, how many dimensions do this consume?
5370000	5372000	How much weight norm?
5372000	5374000	How many parameters?
5374000	5378000	How hard is this to get to and how weird and intricate?
5378000	5383000	And models will choose the thing that gets the best loss
5383000	5386000	that is most efficient on these dimensions,
5386000	5389000	assuming they can reach it within the lost landscape.
5389000	5392000	Well, I use choose in a very anthropomorphic sense.
5392000	5396000	Like, Adam chooses good solutions.
5396000	5400000	And I don't know, if you have a sufficiently hard task
5400000	5403000	and forming a world model is like the right solution to it,
5403000	5405000	models can do it.
5405000	5408000	And I think people try to put all of these fancy philosophizing
5408000	5411000	on it in a way that I just think is false.
5412000	5417000	And I think the Othello paper is like a really beautiful,
5417000	5420000	elegant setup that proves this.
5420000	5422000	All right, can I move on to the plot twist?
5422000	5424000	Does it prove it though?
5424000	5428000	It's a very small contrived.
5428000	5434000	It's a big jump to assume that that works on a large language model.
5434000	5437000	So this is kind of the argument I'm making.
5437000	5440000	I think there's the empirical question of do language models
5440000	5445000	do this and the theoretical question of could they do this?
5445000	5448000	And I'm saying I think the theoretical question is nonsense.
5448000	5451000	And I think the Othello paper very conclusively proves
5451000	5454000	the theoretical question is nonsense.
5454000	5458000	They're just like, yeah, when given a bunch of data,
5458000	5462000	you can infer the underlying world model behind it in theory.
5462000	5464000	I would pitch back on that a tiny bit
5464000	5467000	because it's very similar to AlphaGo,
5467000	5471000	which proved that in a closed game,
5471000	5475000	which is systematic and representable,
5475000	5478000	with a finite, obviously exponentially large,
5478000	5480000	but a finite number of board states,
5480000	5484000	you can build an agent which performs really, really well.
5484000	5487000	That seems to me completely different to something like language
5487000	5489000	or acting in the real world that might not be systematic
5489000	5491000	in the same way.
5491000	5493000	We can debate whether or not it's,
5493000	5496000	I think it's an infinite number of possible trajectories,
5496000	5499000	just like language, an infinite number of possible sentences.
5499000	5501000	Man, there's 50,000 to the power of a thousand
5501000	5503000	possible input sequences.
5503000	5506000	Sure is a finite number.
5506000	5508000	You mean in Othello or?
5508000	5509000	No, in GPT2.
5509000	5512000	In GPT2, okay.
5512000	5516000	Bounded context length, bounded vocab size, generally.
5516000	5518000	Bastard.
5518000	5521000	You're not going to write more than one quintillion characters
5521000	5523000	probably.
5523000	5526000	Yeah.
5526000	5531000	Well, I guess it is still a big jump, though, isn't it?
5531000	5537000	From, yes, empirically, it shows that in Othello, it works.
5537000	5541000	Maybe we could debate whether or not it does or not,
5541000	5543000	because there's always this question coming back to what we were saying before,
5543000	5547000	whether it's learning something which is universal
5547000	5549000	or something which is still brittle.
5549000	5554000	So the way that we've evaluated it might lead us to conclude that it's universal,
5554000	5557000	whereas, actually, it's brittle in ways that we don't understand.
5557000	5559000	So that's a very real possibility.
5559000	5560000	Yeah.
5560000	5563000	I mean, everything's brittle in ways you don't understand.
5563000	5567000	It's pretty rare that a model will do everything perfectly
5567000	5570000	in a way that there are no adversarial examples.
5570000	5572000	And this is one of the more interesting things that's come out
5572000	5574000	of the adversarial examples literature to me.
5574000	5578000	It's just like, oh, wow, there's so much stuff here.
5578000	5581000	There's such a high-dimensional input space.
5581000	5584000	There's all kinds of weird things the model wasn't prepared for.
5584000	5586000	And I don't know.
5586000	5591000	My interpretation of the Othello thing is the strong theoretical arguments are wrong.
5591000	5596000	I separately believe that there are world models
5596000	5599000	that could be implemented in a language model's ways.
5599000	5602000	But I also disagree with the strong inference of the paper
5602000	5606000	that this does happen in language models, or that we conclude it does,
5606000	5609000	because world models are often really expensive.
5609000	5612000	Like, in the Othello model, it's consuming 128 dimensions
5612000	5616000	of its 512-dimensional residual stream for this world model.
5616000	5620000	And the problem is set up so that the world model is insanely useful
5620000	5624000	because weather removers' legal is purely determined by the board state,
5624000	5626000	so it's worth the model's while to do this.
5626000	5628000	But this is rarely the case in language.
5628000	5632000	For example, there was all this buzz about Bing chat playing chess
5632000	5634000	and making legal-ish moves.
5634000	5636000	And I don't know, man.
5636000	5638000	If you want to model a chessboard,
5638000	5642000	you just look at the last piece that moves into a cell.
5642000	5644000	That's the piece in that cell.
5644000	5646000	You don't need an explicit representation.
5646000	5648000	You can just use attention heads to do it.
5648000	5650000	And there's all kinds of weird hacks,
5650000	5652000	and, like, models will generally use the best hack.
5652000	5655000	But probably it is worth the model's while
5655000	5658000	to have some kind of an internal representation.
5658000	5662000	Like, I'd bet that if you took a powerful code-playing model
5662000	5666000	and probed it to understand the state of the key variables,
5666000	5668000	it would probably have some representation.
5668000	5674000	But it's moving on to the work I did building on the Othello paper.
5674000	5678000	So one of the things that was really striking to me about the Othello work
5678000	5682000	is, simultaneously, its results were strong enough
5682000	5686000	that something here was clearly real.
5686000	5690000	But they also used techniques that felt more powerful than were needed.
5690000	5694000	Like, rather, they found that linear probes did not work.
5694000	5698000	There weren't just directions in space corresponding to board states,
5698000	5702000	but the non-linear probes, one hidden layer MLPs, did.
5702000	5706000	And the key thing to be careful of when probing
5706000	5708000	is, is your probe doing the computation,
5708000	5712000	or does the model genuinely have this represented?
5712000	5716000	And even with linear probes, this can be misleading.
5716000	5719000	Like, if you're looking at how a model represents coloured shapes,
5719000	5722000	and you find a red triangle direction,
5722000	5724000	it could be that there's a red, green, or blue direction,
5724000	5727000	and a triangle, square, or shape direction,
5727000	5729000	and you're taking the red plus triangle,
5729000	5732000	or it could be the case that each of the nine shapes
5732000	5734000	have their own direction, you found the red triangle one.
5734000	5737000	But non-linear probing is particularly sketchy.
5737000	5743000	Like, in the extreme case, if you train GPD3 on the inputs to something,
5743000	5746000	GPD3 can do a lot of stuff.
5746000	5749000	If you train GPD3 on the activation side network,
5749000	5752000	it can probably recover arbitrary functions of the input,
5752000	5755000	assuming the information of the input hasn't been lost,
5755000	5759000	which it shouldn't have, because there's a residual stream.
5759000	5764000	And what I said is not quite true, but not important.
5764000	5770000	And so I was, and their intervention technique
5770000	5774000	was both got, like, various impressive results,
5774000	5777000	but also involved doing a bunch of complex gradient-scent
5777000	5780000	against their probe, and this all just seemed more powerful
5780000	5782000	than was necessary.
5782000	5784000	And so I did the...
5784000	5786000	I challenged myself to do a weekend hackathon
5786000	5789000	trying to figure out what was going on,
5789000	5792000	and I poked around at some internal circuitry
5792000	5795000	and tried to answer some very narrow questions about the model,
5795000	5798000	and I found this one neuron that seemed to be looking
5798000	5801000	for, like, three cell diagonal lines,
5801000	5804000	where one was blank,
5804000	5807000	the other was white, the next was black.
5807000	5811000	But then sometimes it activated on blank, black, white.
5811000	5814000	And it turned out that the general pattern
5814000	5818000	was that it was blank, current players...
5818000	5822000	sorry, blank opponents color and current players color.
5822000	5825000	And this is a useful motif in Othello,
5825000	5827000	because it makes them move legal.
5827000	5830000	And when I saw this, I made the bold hypothesis
5830000	5833000	maybe the model actually represents things
5833000	5836000	in terms of whether a cell has the current player's color
5836000	5838000	or the current opponent's color,
5838000	5840000	which in hindsight is a lot more natural,
5840000	5843000	because the model plays both black and white,
5843000	5846000	and it's kind of symmetric from the perspective of the current player.
5846000	5848000	And I trained a linear probe on this,
5848000	5851000	and it just worked fabulously,
5851000	5853000	and got near-perfect accuracy.
5853000	5856000	And I tried linear representations on it,
5856000	5859000	and I tried linear interventions, and it just worked.
5859000	5862000	And I even feel really excited about this project
5862000	5864000	for a bunch of reasons.
5864000	5866000	First, while I did it on the weekend,
5866000	5868000	I'm still very proud of this.
5868000	5873000	Secondly, I think that it has vindicated
5873000	5876000	some of my general suspicion of non-linear probing.
5876000	5878000	Like, if you really understand a thing,
5878000	5881000	you should be able to get a linear probe to work.
5881000	5884000	And kind of more deeply, as we discussed,
5884000	5888000	there's this words-to-vex style linear representation
5888000	5892000	of a hypothesis about models that features corresponded directions.
5892000	5895000	The Athero work seemed like pretty strong evidence against.
5895000	5899000	They had chordal interventions showing that the board state was there,
5899000	5903000	but actually non-linear probes did not work.
5903000	5906000	It seemed like they found some non-linear representation.
5906000	5910000	And my and Chris Ola's hypothesis seeing this
5910000	5914000	was that there was a linear representation hiding beneath.
5914000	5917000	Martin Boddenberg, one of the authors of the paper,
5917000	5920000	had the hypothesis that it was, like,
5920000	5922000	an actual non-linear representation,
5922000	5924000	and this was evidence against the hypothesis.
5924000	5927000	And this kind of formed natural experiment
5927000	5929000	where the hypothesis could have been falsified,
5929000	5934000	but my work showed there was a real non-linear representation,
5934000	5937000	and thus that it had predictive power.
5937000	5939000	And so many of our frameworks for mech and turp
5939000	5942000	are just these loose things based on a bunch of data,
5942000	5946000	but not fully rigorous or fully conclusively shown.
5946000	5948000	And so natural experiments like this
5948000	5951000	feel like some of the best data we have.
5951000	5953000	On this linear representation, though,
5953000	5955000	I don't know if you've heard of the spline theory
5955000	5958000	of neural networks by Randall Ballastriero.
5958000	5961000	And without going into too much detail,
5961000	5964000	it's quite a discrete view of MLPs in particular
5964000	5968000	that the relus essentially get activated
5968000	5972000	in an input-sensitive way to carve out these polyhedra
5972000	5976000	in the ambient space, and essentially an input
5976000	5979000	will be mapped into one of these cells in the ambient space,
5979000	5982000	and then there's a kind of discreteness to it,
5982000	5985000	because if you just perturb the input
5985000	5988000	and you move outside of one of these polyhedra,
5988000	5991000	then the model will, if it's a classifier,
5991000	5993000	classify something different.
5993000	5995000	But I guess I want to understand,
5995000	5998000	with this representation theory, if features are directions,
5998000	6001000	does that imply there's a kind of continuity
6002000	6005000	because the network will learn to spread out
6005000	6007000	those representations in the best possible way,
6007000	6010000	but it won't necessarily be a way which is semantically useful,
6010000	6012000	like in Word2Vec,
6012000	6014000	stop and go are very close to each other,
6014000	6016000	and they shouldn't be.
6016000	6019000	And at what point does stop become go?
6019000	6023000	So do you see there being boundaries in these directions?
6024000	6028000	So I think this is, again, my point that I think
6028000	6030000	of linear representations as being
6030000	6033000	importantly different from geometric representations,
6033000	6036000	like stop should be close to go,
6036000	6038000	because in many contexts,
6038000	6044000	they are like a kind of changing of state term,
6044000	6046000	and it's used in similar contexts
6046000	6048000	and has similar grammatical meaning,
6048000	6050000	but then on this single semantic thing,
6050000	6052000	they're quite different.
6052000	6053000	And the natural way to represent this
6053000	6055000	is have them be close together in Euclidean space,
6056000	6059000	but have some crucial like negation dimension
6059000	6061000	where they're different.
6061000	6065000	And the contact and like ultimately neural networks
6065000	6067000	are not geometric objects,
6067000	6069000	they are made of linear algebra.
6069000	6072000	Every neuron's input is just project
6072000	6075000	the residual stream onto some vector,
6075000	6079000	and this involves just selecting some set of directions
6079000	6081000	and taking a linear combination
6081000	6084000	of the feature corresponding to each of those.
6085000	6088000	And this is just the natural way for a model
6088000	6090000	to represent things in my opinion.
6091000	6094000	Okay, well, I think this will in a second lead us
6094000	6096000	on very nicely to superposition,
6096000	6098000	which is that we don't actually think
6098000	6102000	of there being one direction necessarily.
6102000	6104000	Just to close this little piece,
6104000	6106000	now you said in your less wrong article
6106000	6110000	that orthello GPT is likely over parameterized
6110000	6112000	for good performance on this particular task
6112000	6114000	while language models are under parameterized.
6114000	6116000	And of course, we have the ground truth to this task,
6116000	6118000	which makes it very, very easy.
6118000	6119000	So much easier to interpret.
6119000	6122000	100%, but you did conclude saying that
6122000	6124000	this is further evidence that neural networks
6124000	6127000	are genuinely understandable and interpretable,
6127000	6129000	and probing on the face of it seems like
6129000	6131000	a very exciting approach to understand
6131000	6133000	what the models really represent,
6133000	6136000	caveat, mentor, conceptual issues.
6136000	6139000	So let's move on to this superposition,
6140000	6142000	also known as poly semanticity,
6142000	6144000	which is an absolutely beautiful,
6144000	6146000	while you're shaking your head a little bit,
6146000	6148000	maybe you start with that.
6148000	6151000	Yeah, so there's...
6151000	6154000	All right, so what's the narrative here?
6154000	6156000	So fundamentally,
6156000	6160000	we are trying to engage with models
6160000	6162000	as these high dimensional objects
6162000	6165000	in kind of this conceptual way.
6165000	6167000	So we need to be able to decompose them
6167000	6170000	because of the curse of dimensionality.
6170000	6173000	And we think models correspond to features
6173000	6175000	and the features correspond to directions.
6175000	6178000	And the hope in the early field
6178000	6181000	was that features would correspond to neurons.
6181000	6184000	And even if you believe features correspond
6184000	6186000	to orthogonal directions,
6186000	6188000	the same thing they correspond to neurons
6188000	6190000	is like a pretty strong one,
6190000	6193000	because there's no reason to align with the neuron basis.
6193000	6195000	The reason this isn't a crazy belief
6195000	6198000	is that models are incentivized to represent features
6198000	6201000	in ways that can vary independently from each other.
6201000	6206000	And because relus and jelus act element-wise,
6206000	6208000	if there's a feature per neuron,
6208000	6210000	they can vary independently.
6210000	6212000	Well, if there's multiple features in the same neuron,
6212000	6214000	I don't know, if there's a relu,
6214000	6216000	the second feature could change
6216000	6218000	so the relu goes from on to off
6218000	6220000	in a way that changes how the other feature is expressed
6220000	6222000	in the dance room network.
6222000	6224000	And this is like a beautiful theoretical argument.
6224000	6228000	Sadly, it's bullshit because of this phenomena of police mantisity.
6228000	6233000	Police mantisity is a behavioral observation of networks.
6233000	6235000	But when we look at neurons
6235000	6237000	and look at things that activate them,
6237000	6240000	they're often activated by seemingly unrelated things,
6240000	6245000	like the urs in the word strangers
6245000	6247000	and capital letters are proper nouns
6247000	6249000	and news articles about football.
6249000	6252000	It's a particularly fun neural I found one time in a language model.
6252000	6257000	And police mantisity is a purely behavioral thing.
6257000	6259000	We're just saying this neuron activates
6259000	6262000	for a bunch of seemingly unrelated stuff.
6262000	6266000	And it's possible that actually we're missing
6266000	6269000	some galaxy brain abstraction where all of this is related,
6269000	6271000	but my guess is that this is just,
6271000	6274000	the model is not aligning features with neurons.
6274000	6278000	And one explanation of this
6278000	6282000	is you've just got this thing called a distributed representation
6282000	6286000	where a feature is made of the linear combination of different neurons.
6286000	6289000	But it is kind of rotated from the neuron basis.
6289000	6293000	And this argument that neurons can vary independently
6293000	6296000	is a reason to think you wouldn't see this.
6296000	6301000	Where this hypothesis is just that there's still
6301000	6305000	n things when there's n neurons, but they're rotated.
6305000	6307000	But then there's this stronger hypothesis
6307000	6312000	that tries to explain this called the superposition hypothesis.
6312000	6315000	And here the idea is,
6315000	6318000	so if a model wants to be able to recover a feature perfectly,
6318000	6321000	it must be orthogonal from all other features.
6321000	6324000	But if it wants to mostly recover it,
6324000	6327000	it suffices to have almost orthogonal vectors.
6327000	6331000	And you can fit in many, many more almost orthogonal vectors
6331000	6334000	into a space than orthogonal vectors.
6334000	6337000	There's theorem saying that there are exponentially many
6337000	6339000	in the number of dimensions.
6339000	6342000	If you have 100 dimensional vectors,
6342000	6345000	how many orthogonal directions are there?
6345000	6346000	100.
6346000	6347000	100?
6347000	6348000	Yep.
6348000	6355000	This is just the statement that you pick a vector.
6355000	6361000	Sorry, there's 100 vectors that are all orthogonal of each other.
6361000	6363000	Basic proof, you pick a vector,
6363000	6366000	everything's orthogonal to that, that's a 9 to 9 dimensional space.
6366000	6369000	You pick another vector, take everything orthogonal to that,
6369000	6371000	that's a 98 dimensional space,
6371000	6375000	and keep going until you get to nothing.
6375000	6379000	Like if you picture a 2D space, you pick any direction,
6379000	6382000	the only things orthogonal to that are a line,
6382000	6386000	and so there's exactly two orthogonal things you can fit in.
6386000	6389000	And there's like, you can rotate this and you can get
6389000	6392000	many different sets of orthogonal things.
6392000	6395000	Okay, I'm trying to articulate why this doesn't make sense to me.
6395000	6397000	So maybe we should start with the curse of dimensionality,
6397000	6400000	which is that the volume of the space increases exponentially
6400000	6402000	with the number of dimensions.
6402000	6404000	So we'll start with that.
6404000	6407000	And the reason I'm thinking, maybe I'm wrong,
6407000	6410000	but if you've got 100 dimensional vector,
6410000	6414000	every combination of flipping one of the dimensions
6414000	6419000	would produce a vector which is orthogonal to all of the other ones, would it not?
6419000	6420000	No.
6420000	6423000	So let's imagine you've got a vector of all ones.
6423000	6426000	If you pick the first element and you negate it,
6426000	6428000	so it's like minus one, then 99 ones.
6428000	6431000	These are not orthogonal, the dot product is 98.
6431000	6433000	Okay, okay, well that makes sense.
6433000	6437000	So there's a linear number of orthogonal directions,
6437000	6441000	and in which case we actually need to have these
6441000	6443000	approximately orthogonal directions,
6443000	6446000	because that actually does bias an exponential number.
6446000	6449000	Yeah, and so the superpositional hypothesis is that the model represents
6449000	6453000	more features than it has neurons, or that it has dimensions,
6453000	6458000	and it somehow compresses them in as things that are almost orthogonal
6458000	6461000	when it reads them out with a projection to get some interference,
6461000	6465000	but it needs to balance the value of representing more features
6465000	6468000	against the cost of interference.
6468000	6474000	And Anthropic has this fantastic paper called toy models of superposition,
6474000	6478000	which sadly was written off right left, so I can't claim any credit,
6478000	6485000	and they basically build a toy model that exhibits superposition.
6485000	6490000	The exact structure is you have n independent features,
6490000	6494000	each of which is zero most of the time, it's not very prevalent,
6494000	6499000	and there's a linear map from that to a small dimensional space,
6499000	6503000	a linear map back up, and a non-linearity on the output,
6503000	6507000	no non-linearity on the bottleneck in the middle,
6507000	6510000	and you train it to be an autoencoder.
6510000	6514000	Can it recover the features in the input?
6514000	6517000	And because there's many more features than that are in the bottleneck,
6517000	6521000	this tests whether the model can actually do this.
6521000	6524000	And they find that it sometimes does, sometimes doesn't,
6524000	6528000	and then do a lot of really in-depth investigation of how this varies.
6528000	6534000	And yeah, returning to like, is superposition the same thing as police mantisity?
6534000	6538000	I would say no, police mantisity is a behavioral thing.
6538000	6541000	Distributed representations are also a behavioral thing,
6541000	6544000	that it's like not aligned with the basis,
6544000	6549000	and superposition is a mechanistic hypothesis for why both of these will happen,
6549000	6551000	because if you have more features than neurons,
6551000	6555000	obviously you're going to have multiple features per neuron,
6555000	6561000	and probably you're going to have features that are not aligned with neurons.
6561000	6563000	Okay, okay, very interesting.
6563000	6568000	So why do you think that superposition is one of the biggest problems in mech and turp?
6568000	6574000	Yeah, so it's this fundamental thing that we need to be able to decompose a model
6574000	6580000	into individual units, and ideally these would be neurons,
6580000	6584000	but they are not neurons, so we need to figure out what we're doing.
6584000	6591000	And superposition, so in a world where we just have like n meaningful directions,
6591000	6596000	but they weren't aligned with the standard basis, that'd be kind of doable.
6596000	6601000	And indeed models often have like linear bottlenecks,
6601000	6605000	like the residual stream, or the keys, queries, and values of an attention hit,
6605000	6612000	that don't have element-wise linearities, and so have no intrinsically meaningful basis.
6612000	6615000	The jargon here is privileged basis,
6615000	6622000	but superposition means that you can't even say,
6622000	6624000	this feature should be a fucking alter everything else,
6624000	6627000	there's going to be a bunch of interference.
6627000	6633000	There's not even a kind of mathematically,
6633000	6638000	there's not even like a unique set of more than n directions,
6638000	6642000	so describe some kind of vectors in n dimensional space.
6642000	6647000	And I think that understanding how to extract features from superposition,
6647000	6652000	given that superposition seems like a core part of how models do things,
6652000	6657000	though we really do not have as much data here as I would like us to,
6657000	6662000	understanding how to extract the right meaningful units seems really important.
6662000	6666000	Okay, and I think we should clarify the difference between computational
6666000	6668000	and representational superposition.
6668000	6675000	Yeah, so there's kind of, so transformers are interesting,
6675000	6681000	because they often have high dimensional activations
6681000	6685000	that get linearly mapped to low dimensional things.
6685000	6688000	So like in say GPT-2, in say GPT-2 small,
6688000	6692000	the residual stream has 768 dimensions,
6692000	6697000	while each MLP layer has 3000 neurons.
6697000	6701000	And even if we think each neuron just produces a single feature,
6701000	6706000	they need to get compressed down to the 768 dimensional residual stream.
6706000	6711000	And we, or there's like 50,000 input tokens
6711000	6715000	that get compressed to 768 dimensions.
6715000	6720000	And this is called representational superposition.
6720000	6723000	The model is representing, the model's already computed the features,
6723000	6726000	but it's compressing them to some bottleneck space.
6726000	6730000	And this is the main thing studied in the toy models of superposition paper.
6730000	6736000	And what we found, sorry,
6736000	6740000	there's a separate thing of computational superposition,
6740000	6745000	which is when the model is doing, it's computing new features.
6745000	6752000	This needs non-linearities, like attention head softmaxes or MLP jellies.
6752000	6758000	And the non-linearities can compute new features as directions from the old ones,
6758000	6766000	like if this, for example,
6766000	6771000	if the top of an image is a car window and the bottom is a car wheel,
6771000	6774000	then it's a car.
6774000	6779000	Or if the current token is Johnson and the previous token was Boris,
6779000	6781000	this is Boris Johnson.
6781000	6790000	And this is all, how to phrase this?
6790000	6794000	Yeah, this is computational superposition.
6794000	6798000	If the model wants to compute more features than it has neurons.
6798000	6803000	And this is much harder to reason about because linear algebra is nice
6803000	6805000	and fairly well understood.
6805000	6808000	Non-linearities, spoilers in the name, are not linear,
6808000	6810000	and that's way more of a pain.
6810000	6816000	And I think that we generally have a much less good handle on computational superposition,
6816000	6821000	but also that this is way more of where the interestingness lies by my lights.
6821000	6825000	And this is very briefly studied in the toy models of superposition paper,
6825000	6829000	but I would love to see more work looking at this in practice
6829000	6832000	and also looking at this in toy models.
6832000	6837000	So zooming out a tiny bit, there's this paper from Anthropic.
6837000	6841000	And the overall question to me is, does it actually exist?
6841000	6845000	Now, presumably you're satisfied with the evidence that it does exist.
6845000	6850000	And then there's the question of how do neural networks actually do it?
6850000	6853000	And then there's the question of how does the neural network think,
6853000	6855000	anthropomorphic language, I apologize,
6855000	6859000	about the trade-off of more superposition, more features,
6859000	6864000	but more interference versus less interference and more superposition?
6864000	6869000	Yeah, so typing into the final question about interference,
6869000	6877000	a useful conceptual distinction is that there's two different kinds of interference.
6877000	6882000	So if you've got two features that share a dimension or share a neuron.
6882000	6885000	Oh yeah, final note on representational superposition,
6885000	6888000	because I don't think it should even be referred to in terms of neurons,
6888000	6892000	because the individual-based elements don't have intrinsic meaning.
6892000	6896000	Modular weird quirks like Adam.
6896000	6902000	And it annoys me when people refer to the residual stream or key vectors as having neurons.
6902000	6906000	There's no element-wise linearity, it's not privileged.
6906000	6910000	Anyway, yeah, two types of interference.
6910000	6915000	When A and B share a dimension, you can, yeah,
6915000	6919000	let's say this dimension has both dice and poetry.
6919000	6924000	You first off need to tell where if dice is there but poetry is not,
6924000	6928000	you need to tell that dice is there and that poetry is not there.
6928000	6932000	And if both what I call alternating interference,
6932000	6937000	and then there's simultaneous interference where dice and poetry are both there.
6937000	6943000	And you need to tell that both are there, but not that they're both there with like double strength.
6943000	6949000	And as a general rule, models are good at dealing with things of the form.
6949000	6952000	Notice when something is extreme along this dimension,
6952000	6959000	but not notice when it is extreme along a dimension versus when it's not extreme.
6959000	6964000	And alternating interference looks like that.
6964000	6970000	Like if dice is straight up, poetry is at 45 degrees,
6970000	6977000	both have less interference when the other one is active
6977000	6980000	than when the main one is active along their direction.
6980000	6985000	Okay, so you're saying interference from A and not B is far easier than A and B?
6985000	6986000	Yes, exactly.
6986000	6994000	And like a very rough heuristic as models will just not do simultaneous interference,
6994000	6997000	but will do alternating interference.
6997000	7005000	And they observed this in the toy models paper because they varied how often a feature was non-zero.
7005000	7009000	What I think of as the prevalence of the feature that they called it spastic.
7009000	7014000	And what they found is that when the feature was less prevalent,
7014000	7017000	it was much more likely to be in superposition.
7017000	7022000	And the way to think about this is if you have two independent features that both exist with probability P,
7022000	7026000	the rate of simultaneous interference is P squared.
7026000	7028000	The rate of alternating is P.
7028000	7036000	And so and the worth of having the feature is also proportional to P because it occurs P of the time.
7036000	7041000	So the railroad is the less of a big deal simultaneous interferences.
7042000	7045000	And eventually the model uses superposition.
7045000	7051000	There's also there was also an interesting bit looking at correlated features.
7051000	7057000	So correlated features, even if they're not very prevalent, they have pretty high simultaneous interference.
7057000	7062000	And models tend to put correlated features in to be orthogonal,
7062000	7066000	but anti-correlated features, it's very happy for them to share a direction.
7066000	7073000	One way you could think about this is if you've got, say, 25 features about romance novels and 25 features about Python code,
7073000	7081000	you could have 25 directions that each contain a pair of features and then a single disambiguating neuron
7081000	7086000	that is onto Python code off of romance novels that use to disambiguate the two.
7086000	7093000	And yeah, may this be a good time to talk about the finding neurons in a haystack paper?
7093000	7095000	Unless you've got more stuff on this.
7095000	7098000	We'll get to that in just two shakes of a lamb's tail.
7098000	7103000	But just before when I was reading through the paper, I had the mindset of sparsity.
7103000	7107000	And you told me, Tim, don't say sparsity. It's prevalence.
7107000	7109000	It means so many things.
7109000	7111000	It's very overloaded.
7111000	7116000	So, you know, so just quickly touch on the relationship between what is prevalence,
7116000	7119000	the relationship between prevalence and superposition.
7120000	7123000	And just before, well, actually, I've got a couple more questions,
7123000	7131000	but would you also just mind playing devil's advocate and criticising the anthropic paper if you can?
7131000	7132000	Sure.
7132000	7135000	So I should be very clear.
7135000	7138000	This is one of my top three all-time favourite and territoriality papers.
7138000	7140000	It's a fantastic paper.
7140000	7141000	That said.
7141000	7143000	A bad word said about it.
7143000	7144000	Oh, I have so much.
7144000	7146000	I have bad words to say about every paper,
7146000	7150000	especially the ones that I like because I've engaged with them in the most detail.
7150000	7154000	So things which I think were misleading about this paper.
7154000	7160000	The first is I think the representational versus computational superposition distinction is very important.
7160000	7164000	I think computational is a fair bit more interesting.
7164000	7167000	And while I think the authors knew the difference,
7167000	7172000	I think a casual reader often came away not realising the difference,
7172000	7175000	in particular that most of their results were about the residual stream,
7175000	7179000	not about actual neurons and MLP layers.
7179000	7183000	The second is a question of activation range.
7183000	7188000	So they study features that vary uniformly between zero and one.
7188000	7192000	And in practice, I think most features are binary.
7192000	7195000	This is a car wheel or this is not a car wheel.
7195000	7199000	This is Boris Johnson or this is not Boris Johnson.
7199000	7204000	And interference is much worse when they can vary continuously.
7204000	7208000	Because if A and B, if A is up, B is at 45 degrees,
7208000	7217000	you can't distinguish B at strength one from A at strength 0.7-ish.
7217000	7220000	And this is just kind of messy.
7220000	7222000	But the binary is just much easier.
7222000	7227000	And I think this is a source of confusion.
7228000	7235000	I also think the two kinds of interference point was a bit understated.
7235000	7238000	But more broadly, it's just a phenomenal paper.
7238000	7241000	Oh, my other biggest beaver, they just didn't look in real models.
7241000	7243000	And this wasn't the point of the paper.
7243000	7248000	But we're doing so much theory crafting and filming conceptual frameworks,
7248000	7250000	and we haven't really checked very hard
7250000	7254000	whether this is why models actually have police mantisity.
7254000	7259000	Wes Gurney, he's working out of MIT, and you've done a lot of work with him.
7259000	7263000	So you and Wes, but Wes was the first author,
7263000	7266000	wrote a paper called Finding Neurons in a Haystack,
7266000	7268000	case studies with sparse probing,
7268000	7271000	where you empirically studied superposition and language models
7271000	7275000	and actually found that you get lots of superpositions in early layers
7275000	7278000	for features like the security and social security.
7278000	7283000	And fewer in middle layers for complex features like this text
7283000	7285000	is French.
7285000	7289000	And also you can bring in the importance of range activation as well.
7289000	7291000	But can you frame up that paper?
7291000	7292000	Yeah.
7292000	7296000	So first off, this paper was led by Wes Gurney, one of my mentees,
7296000	7297000	did a fantastic job.
7297000	7299000	He deserves like 9% of the credit.
7299000	7300000	Great job, Wes.
7300000	7303000	I believe he listens to this podcast, so hi.
7304000	7310000	And yeah, so the kind of high level pitch behind the paper
7310000	7314000	was what we think superposition is happening.
7314000	7317000	But like, nobody's really checked very hard.
7317000	7319000	And there's like some results in the literature
7319000	7322000	I've since come across in non-transformer models
7322000	7325000	that demonstrate some amount of distributed representations.
7325000	7328000	But what would it look like to check?
7328000	7331000	And would it look like to do this in like a reasonably scalable
7331000	7333000	and quantitative way?
7333000	7337000	And the kind of sparse probing in the title
7337000	7341000	is this technique Wes introduces for,
7341000	7345000	if we think a feature is represented in MLP layer,
7345000	7348000	we can train a linear classifier to extract it,
7348000	7350000	a linear probe from that layer.
7350000	7354000	But if we constrain the probe to use at most K neurons,
7354000	7357000	very K and look at probe performance,
7357000	7359000	this lets us distinguish between features that are represented
7359000	7362000	with like a single neuron and features that are densely spread
7362000	7366000	across all neurons with a lot of methodological neural
7366000	7370000	answers about balanced data sets and avoiding overfitting
7370000	7372000	and fun stuff like that.
7372000	7376000	And most of the interesting bits of the paper,
7376000	7380000	in my opinion, are the various case studies we do where,
7380000	7384000	so probing fundamentally is like a kind of sketchy methodology
7384000	7387000	because probing is correlational.
7387000	7390000	Probing doesn't tell you whether a model uses something
7390000	7392000	and it's so easy to trick yourself about
7392000	7395000	whether you have the right representations.
7395000	7398000	So we use it as a starting point and then dig more deeply
7398000	7400000	into a few more interesting things.
7400000	7405000	One particularly QK study is we looked into factual knowledge neurons,
7405000	7409000	found something that seemed to represent this athlete plays hockey,
7409000	7412000	but then actually turned out to be a Canada neuron,
7412000	7415000	which continues to bring me joy.
7415000	7418000	That activates with things like maple syrup and Canada.
7418000	7422000	Got to love models learning national stereotypes, right?
7422000	7423000	Oh, yes.
7423000	7431000	Anyway, so there were two particularly exciting case studies.
7431000	7436000	The first was looking in early layers at compound word detectors.
7436000	7442000	So if you look at, say, the brain and its visual field,
7442000	7444000	we have all these sensory neurons.
7444000	7447000	We get raw input of light from the environment
7447000	7450000	and it gets converted into stuff our brain can actually manipulate.
7450000	7454000	Image models have Gabor filters that convert the pixels
7454000	7456000	into something a bit more useful.
7456000	7459000	What's the equivalent of language models?
7459000	7463000	And it seems to be these things that we call detokenization neurons
7463000	7469000	and circuitry, where often words are split into multiple tokens
7469000	7474000	or you get compound word phrases like social security
7474000	7479000	or Theresa May or Barack Obama and whatever.
7479000	7484000	And it's often useful for a model to realize
7484000	7487000	this is the second thing in a multi-token phrase,
7487000	7492000	especially if it's like you need both things to know what's going on,
7492000	7494000	like Michael Jordan.
7494000	7496000	Michael has lots of Jordans.
7496000	7499000	It's really important to tell both of them that they're there.
7499000	7504000	And this is a clearly nonlinear thing because it's like a Boolean and.
7504000	7508000	And so we did a lot of probing for different compound words.
7508000	7513000	And we found that they were definitely not represented well by single neurons.
7513000	7517000	We could find some neurons that were okay at detecting them,
7517000	7523000	but there was a lot of interference and a lot of like false positives from other stuff.
7523000	7525000	And when we dug into a bunch of these neurons,
7525000	7528000	we found that they were incredibly polysemantic.
7528000	7532000	They activated for many different compound words.
7532000	7536000	And we showed that it was using superposition
7536000	7541000	by observing that if you took say five social security detecting neurons
7541000	7543000	and add together their activations,
7543000	7547000	they go from okay detectors to a really good detector together.
7547000	7552000	Because even though each is representing like hundreds of compound words,
7552000	7554000	they're representing different compound words,
7554000	7557000	which lets you encode these.
7557000	7563000	And this, what we've shown here is that it's like distributed,
7563000	7567000	that it's a linear combination of neurons.
7567000	7570000	We still haven't shown it perfectly to my dissatisfaction.
7570000	7574000	I think you really need to do things like ablate these linear combinations
7574000	7579000	and see if this systematically damages the model's ability to think about social security, etc.
7579000	7581000	But I'm pretty convinced at this point.
7581000	7586000	And there's like a few properties of compound words
7586000	7590000	that both make it easy to represent in superposition,
7590000	7595000	that make me pretty okay making the jump that there's actual superposition.
7595000	7598000	The first is just that there's tons of compound words.
7598000	7602000	Each one is pretty rare, but each one is like non-trivial or useful.
7602000	7605000	And clearly there are more compound words
7605000	7610000	and there are like thousands of neurons in the MLP layer of this model.
7610000	7613000	The model cares about representing and can represent,
7613000	7615000	that we do not actually check.
7615000	7619000	Because I could not convince Wes to accumulate a list of 2,000 compound words
7619000	7621000	and pray for all of them.
7621000	7624000	But I believe in my heart this is true.
7624000	7626000	Could I have a point of order though?
7626000	7631000	Because I've been reading quite a lot of stuff from linguists like Stephen Piantadosi.
7631000	7636000	A lot of linguists are, some of them hate language models
7636000	7639000	and some of them are well on board with it.
7639000	7642000	Like Raphael Millier for example is a great example.
7642000	7644000	I hate language models too, don't worry.
7644000	7646000	Well, but the question is,
7646000	7648000	because you're talking about compound words and stuff like that
7648000	7652000	and you're still using the language of syntax
7652000	7656000	and these language models, there's this distributional hypothesis.
7656000	7660000	You know the meaning of a word by the company it keeps.
7660000	7663000	But linguists and cognitive scientists kind of ditch that.
7663000	7666000	I don't think they ever believed in the distributional hypothesis.
7666000	7668000	They think about grounding.
7668000	7671000	They think about grounding to things in the world
7671000	7675000	and also inferential references as well
7675000	7679000	which you can think of that as grounding to a model of the mind.
7679000	7681000	And this brings us back to the Othello paper
7681000	7685000	which is that they're not just learning simple kind of compound relationships
7685000	7687000	between the world, between the words.
7687000	7689000	They're learning a world model
7689000	7693000	and they're doing something much more potentially
7693000	7695000	than just predicting the next word.
7695000	7698000	And Piantadosi argued that
7698000	7702000	most of the representational capacity in language models
7702000	7704000	are learning these semantics.
7704000	7707000	They're learning relationships between things in the world model
7707000	7710000	and the particular occurrence of the token.
7710000	7712000	And this superposition idea is very interesting
7712000	7716000	because it actually imbues the representational capacity
7716000	7718000	in a language model to learn those mappings.
7720000	7722000	Okay, so a couple of comments on that.
7722000	7727000	The first is a generally useful way of thinking about models to me.
7727000	7732000	Is as a the early layers devoted to sensory neurons
7732000	7737000	converting the raw input into more useful concepts and representations.
7737000	7742000	The actual processing throughout like all of the middle layers
7742000	7744000	that actually does all the reasoning.
7744000	7746000	And then motor neurons at the end
7746000	7749000	that convert the reasoning to actual output tokens
7749000	7753000	for like the format that the optimizer wants.
7754000	7759000	And it feels like you're mostly talking about the like reasoning internally
7759000	7763000	and the specific case study I'm referring to is on the sensory neurons.
7763000	7766000	Well, like I'm not saying it just detects compound words
7766000	7769000	but obviously that's the first thing it does.
7769000	7771000	I don't know, it's so interesting.
7771000	7774000	I don't mean to push back but in neuroscience
7774000	7777000	the field was held back for decades by this idea
7777000	7780000	of this kind of left to right to processing
7780000	7784000	this hierarchical processing where you have these very, very simple concepts
7784000	7788000	that become increasingly abstract with more processing.
7788000	7790000	And then I think the field has moved away from that.
7790000	7792000	It's far more messy and chaotic than that.
7792000	7795000	Now with a neural network, it actually is hierarchical
7795000	7798000	because the network is basically a DAG.
7798000	7801000	So I suppose it is safe to make this assumption
7801000	7803000	but could I just kind of question you on that?
7803000	7805000	Is it safe to make that assumption?
7805000	7808000	Is there increasing complexity in representation
7808000	7810000	as you go from left to right?
7810000	7812000	Oh, let's see.
7812000	7816000	So yeah, I definitely, yeah.
7816000	7821000	So clarification one, the network has this input sequence
7821000	7823000	which I think was going from left to right
7823000	7826000	and then there's a bunch of layers which I think it was going from like the bottom to the top.
7826000	7827000	Yes.
7827000	7829000	And you're referring to the bottom to top axis, right?
7829000	7833000	Yeah, I'm sorry, I was using an MLP mindset when I asked that question.
7833000	7836000	So as you say, in a transformer it's an autoregressive model
7836000	7841000	and you have stacked attention layers with little MLPs on the end.
7841000	7844000	So I guess the way I was actually meaning the question is,
7844000	7850000	so complexity increases monotonically as you go up the stack of attention layers.
7850000	7852000	Is that a fair assumption?
7852000	7854000	Yep.
7854000	7856000	Again, no one's really shown this properly.
7856000	7859000	But I'm like, surely this is true.
7859000	7863000	And there's been some work doing things like looking at neurons,
7863000	7865000	looking at the text that activates them, looking for patterns
7865000	7868000	and trying to understand what these represent.
7868000	7873000	And it's generally looked like early ones are more about detokenization and syntax.
7873000	7876000	Later ones are doing stuff that's interesting.
7876000	7879000	Final ones are doing this like motor neuron behavior.
7879000	7884000	But I also want to be very clear that networks are cursed.
7884000	7887000	Networks do not fit into nice abstraction.
7887000	7891000	I'm not saying the early layers are literally only doing detokenization.
7891000	7892000	Yeah.
7892000	7895000	But I believe we have shown it's part of what they're doing.
7895000	7898000	And I speculate it is a large part of what they're doing.
7898000	7901000	I'd be very surprised if it's all of what they're doing.
7901000	7905000	Because I heard you on another podcast and you were just talking about the,
7905000	7908000	I mean, I think the curse is the right way to describe it,
7908000	7912000	which is that even when you make modifications,
7912000	7914000	when you manipulate what's happening,
7914000	7917000	the behavior will change in a very reflexive way.
7917000	7920000	So you kind of, you delete one thing and then another neuron
7920000	7923000	will take on the responsibility of the thing you just deleted.
7923000	7927000	And so it's a little bit like manipulating financial markets.
7927000	7932000	You've got almost like this weird collective diffuse intelligence
7932000	7937000	where you make one modification and the whole thing changes in a very complex way.
7937000	7941000	And similarly, I guess that's why I was intuitively questioning the assumption
7941000	7943000	that you have a residual stream.
7943000	7946000	So surely even at the very top of that attention stack,
7946000	7951000	there must be primitive and complex operations going on in some weird mix.
7951000	7954000	Since probably true.
7954000	7961000	Generally, yeah, there's going to be some stuff you can just do with literally the embeddings.
7961000	7966000	Some stuff that you need to wait a bit more before you can do anything useful with.
7966000	7971000	Just like, no, if you got a sentence about Michael Jordan,
7971000	7974000	I don't think you can use Michael Jordan in isolation.
7974000	7977000	So you need to de-tokenize to Michael Jordan.
7977000	7980000	But also, I don't know, if you've got Barack Obama,
7980000	7985000	Obama and Barack both on their own pretty clearly imply it's going to be about Obama.
7985000	7990000	And probably the model can start doing some processing in the early like layer zero.
7990000	7992000	Does it want to?
7992000	7993000	Somewhat unclear.
7993000	7996000	It's going to depend a lot on the model's constraints and other circuitry
7996000	8000000	and how much it's worth spending the premises then versus later.
8000000	8004000	There's also some various things where, I don't know,
8004000	8011000	model memory kind of decays over time because the residual stream's norm gets bigger.
8011000	8015000	So early layer outputs become a smaller fraction of the overall thing.
8015000	8018000	And layer norm sets the norm to be unit.
8018000	8020000	So things kind of decay.
8020000	8024000	And so if you compute a feature in the early in like layer zero,
8024000	8029000	it can be harder to notice by like layer three than if it was computed in layer two.
8029000	8032000	But these are all just kind of like mild nudges.
8032000	8036000	And ultimately neural networks do what neural networks want, man.
8036000	8038000	I know, I know.
8038000	8041000	I just want to close the loop on something I said a little while ago about, you know,
8041000	8046000	potentially large models use most of their representational capacity for, you know,
8046000	8048000	learning these semantic relationships.
8048000	8053000	And empirically, we found that, you know, there's some question recently actually about,
8053000	8056000	do we actually need to have really, really large models?
8056000	8061000	And for pure knowledge representation, the argument seems to be yes,
8061000	8066000	but we can disentangle knowing from reasoning.
8066000	8068000	And there's also this mimicry thing.
8068000	8072000	So it's quite interesting that all of the, you know, like Facebook released their model
8072000	8076000	and very, very quickly people fine-tuned it using the law, you know,
8076000	8079000	the low-rank approximation fine-tuning method.
8079000	8082000	And on all of the benchmarks, the model, I mean even open assistant,
8082000	8085000	there's another great example, Yannick was sitting in your seat just a few weeks ago
8085000	8088000	and we're saying that on many of the benchmarks, the model's working really well,
8088000	8090000	but it's kind of not.
8090000	8094000	It's kind of mimicry, like the big, large models that, you know,
8094000	8098000	Meta and Google and DeepMind and all these people, they spend millions training these models
8098000	8102000	and they have base knowledge about the world,
8102000	8107000	which is not going to be, you know, replicated by fine-tuning, you know,
8107000	8109000	like an open source model anytime soon.
8109000	8111000	The knowledge is based.
8111000	8114000	The knowledge is based.
8114000	8116000	Yes, yes, yes, exactly.
8116000	8120000	Well, okay, so that's very interesting.
8120000	8122000	Let's just quickly talk about the OpenAI microscope,
8122000	8128000	because this is, the OpenAI microscope is this beautiful app that OpenAI released in 2020.
8128000	8132000	And you can go on there and you can click on any of the neurons
8132000	8135000	in popular vision architectures at the time.
8135000	8138000	So, you know, I think most of them are sort of like ImageNet, you know,
8138000	8141000	things like AlexNet and God knows what else.
8141000	8146000	And they solve this optimization problem where they generate an image
8146000	8151000	using stochastic gradient descent that maximally activates a particular neuron,
8151000	8154000	or I think even a layer, using something similar to DeepDream.
8154000	8157000	And you can click on these neurons,
8157000	8162000	and sometimes they are what we will call poly sort of monosemantic,
8162000	8164000	which means it's just Canada.
8164000	8168000	A lot of the time there's a couple of concepts in there that it's weirdly intelligible.
8168000	8172000	You know, you might see, you know, like a playing card or an ace
8172000	8176000	and a couple of like tangentially related concepts.
8176000	8179000	And it always struck me as strange,
8179000	8183000	because I imagine there's a long tail of semantic relationships.
8183000	8187000	And I found it bizarre that there'd only be one or two in this visualization.
8187000	8191000	And I had this intuition that the optimization algorithm is in some sense
8191000	8195000	mode seeking rather than distribution matching,
8195000	8200000	which is to say that it finds the two most or two or three or four
8200000	8204000	most kind of salient semantic mappings,
8204000	8206000	and they dominate what is visualized,
8206000	8211000	and you're almost snipping off the long tail of the other semantic mappings.
8211000	8216000	Yeah, so I think there's two things to disentangle here.
8216000	8222000	The first is what is actually represented by the neuron in terms of ground truth.
8222000	8226000	And the second is what our techniques show us.
8226000	8231000	So the two techniques used in the open-air microscope
8231000	8235000	are looking at the images the most activated neuron,
8235000	8240000	and then this feature visualization technique where they produce a synthetic image
8240000	8242000	that maximally activates it.
8242000	8246000	And to me, this is, these are like,
8246000	8249000	both of these can be misleading,
8249000	8252000	because if the model activates the dice in poetry,
8252000	8256000	but activates dice with strength five and poetry with strength four,
8256000	8259000	then the optimally-bished activated will be dice,
8259000	8263000	and the optimal, the data set examples will also be dice.
8263000	8265000	But really, it'll be about poetry.
8265000	8267000	And you want to get a lot more rigorous.
8267000	8270000	You want to show true monosumanticity.
8270000	8273000	One cute thing is spectrum plots.
8273000	8278000	You take lots of example, data set examples across the full distribution,
8278000	8283000	you have a histogram with the different groups for the different meanings,
8283000	8286000	and then neural activation on the x-axis.
8286000	8290000	We have this really cute plot in Wes' paper called the French Neuron,
8290000	8295000	where all of the French text is on the right,
8295000	8297000	all the non-French text is on the left,
8297000	8300000	and the neuron is just very clearly distinguishing the two
8300000	8304000	in a way that's much more convincing to me than things like Max Act examples.
8305000	8311000	And I actually have a hobby project called Neuroscope at Neuroscopes.io,
8311000	8314000	where you can see the Max activating text examples
8314000	8317000	for every neuron and a bunch of language models,
8317000	8321000	though opening I recently output this paper with one that is better,
8321000	8324000	but only for GP2 XL.
8324000	8327000	Anyway, not that I'm bitter or anything.
8327000	8328000	Not so.
8328000	8335000	And yeah, so yeah, there's the things can lie to and be illusory.
8335000	8340000	There's this interesting paper called the Interruptibility Illusion for But,
8340000	8343000	which investigated this specific phenomena,
8343000	8346000	and in particular that if you take the data set examples
8346000	8350000	over some narrow distribution, like Wikipedia or books,
8350000	8352000	you can get pretty misleading things,
8352000	8355000	though they only looked at residual stream basis elements
8355000	8358000	rather than actual MLP neurons, I believe,
8358000	8361000	which makes it a bit less compelling.
8361000	8362000	Point of order as well.
8362000	8364000	We've been saying residual stream quite a lot,
8364000	8367000	and Microsoft introduced Resnet in 2015,
8367000	8371000	which basically means that between all of the layers,
8371000	8374000	the information is being passed up unadulterated,
8374000	8378000	so the subsequent layer can choose to either essentially shortcut
8378000	8382000	or ignore the previous layer or use some combination,
8382000	8385000	and at the time they kind of said it was about the neural network
8385000	8389000	being able to learn its own capacity in some sense,
8389000	8393000	but could you just give us the way you think about these residual streams?
8393000	8396000	Yeah, so I think the standard view of neural networks,
8396000	8403000	there are just layers, and layer 5's output is layer 6's input, etc.
8403000	8409000	Then people added Resnets, where layer 6's input is layer 5's output,
8409000	8412000	plus layer 5's input with the skip connection,
8412000	8414000	but I think people normally thought of them as like,
8414000	8416000	ah, it's like a cute trick that makes the model better,
8416000	8419000	but doesn't massively change my conceptual picture,
8419000	8424000	and the framing that I believe was introduced in the mathematical framework,
8424000	8428000	this anthropic paper led by Chris Ola, Nelson L. Harsh, and Catherine Olson
8428000	8431000	that I was involved with, is actually,
8431000	8436000	let's call the thing in the skip connection the residual stream
8436000	8438000	and think of it as the central object,
8438000	8443000	and draw our model so the residual stream is this big vertical thing,
8443000	8446000	and each layer is like a small diversion to the side,
8446000	8448000	rather than the other way around,
8448000	8453000	and in practice, most circuits involve things skipping many layers,
8453000	8458000	and each layer is better thought of as like an incremental update,
8458000	8462000	and there's a bunch of earlier transformer interpretability papers
8462000	8465000	that I think miss this conceptual point,
8465000	8468000	like the interpretability delusion for but what I mentioned earlier,
8468000	8475000	and study residual stream basis elements as like layer outputs or something.
8475000	8477000	Yeah, I mean, in a sense, you know,
8477000	8481000	we were talking about being able to reuse things that you've learned before
8481000	8483000	and not having to learn them again,
8483000	8486000	and I guess I think of it as a kind of translational equivalence
8486000	8489000	in the layer regime,
8489000	8493000	which is that you have a computation which is learned early on,
8493000	8496000	and now it can just be composed into subsequent layers.
8496000	8500000	It's like you've got a menu of computational functions
8500000	8504000	that you can call on at any layer.
8504000	8506000	Yeah, pretty much.
8506000	8510000	I think of it as like the shared memory and shared bandwidth of the model.
8510000	8512000	Yeah, almost like a memory bus.
8512000	8517000	Yeah, and sometimes models will dedicate neurons like cleaning up the memory
8517000	8520000	and deleting things that are no longer needed.
8520000	8524000	Yeah, yeah, and is there any interference in that memory bus?
8524000	8526000	So much.
8526000	8528000	This is the thing of superposition, right?
8528000	8529000	Yeah.
8529000	8531000	Like the residual stream is doing everything.
8531000	8535000	Like there's 50,000 input tokens start,
8535000	8539000	and then 4x as many neurons as residual stream dimensions
8539000	8541000	in every MLP layer,
8541000	8543000	and attention heads moving everything around,
8543000	8545000	and it's just a clusterfuck.
8545000	8548000	What if you scale up the bandwidth of the bus?
8549000	8552000	That's basically making the model bigger, right?
8552000	8554000	Which we know makes models better.
8554000	8557000	But I don't know, just thinking out loud,
8557000	8561000	but what if you maintained the original dimensionality of the model
8561000	8564000	but you deliberately upscaled the bus?
8564000	8568000	So like you make the thing inside each layer smaller
8568000	8570000	but make the residual stream bigger?
8570000	8572000	Or just make everything the same as it is,
8572000	8576000	but you just kind of like have a linear transformation on the bus
8576000	8578000	and double the size of the bus.
8578000	8582000	So I don't think that would work
8582000	8584000	without increasing the number of parameters,
8584000	8588000	because like if you...
8588000	8591000	Because like the thing that matters is the smallest bottleneck.
8591000	8595000	The output width of an MLP layer are like 4,000 by 1,000,
8595000	8598000	and in order to make the 1,000 bigger,
8598000	8600000	you need more parameters.
8600000	8604000	And there's like all kinds of studies about the optimal hyperparameters
8604000	8606000	and optimal ratios.
8606000	8610000	My general heuristic is number of parameters are the main thing that matters.
8610000	8611000	I don't know.
8611000	8614000	I don't spend that much time thinking about how to make models better, to be honest.
8614000	8616000	I just want to understand them, goddammit.
8616000	8619000	Yeah, because it's one of those things that it might remove bottlenecks
8619000	8625000	because essentially you're allowing the model to reuse things that it's learned previously.
8625000	8629000	So now every single layer can specialize more than it did before
8629000	8632000	and that might kind of like weirdly remove bottlenecks.
8632000	8634000	Yeah.
8634000	8640000	Yeah, the way I generally think about it is models are ensembles of shallow pods,
8640000	8643000	which is this paper from like five years ago about Resnets.
8643000	8647000	Like, deep-d2-small is 12 layers.
8647000	8652000	Each layer includes an attention block and an attention bit in MLP,
8652000	8658000	but it is not the case that most computation is 24 levels of composition deep.
8658000	8661000	It is the case that most of them involve like, I don't know, four.
8661000	8667000	And they're just intelligently choosing which four and remaking them in interesting ways.
8667000	8673000	And sometimes different things will want to like get to different points
8673000	8678000	and so it's useful to have many layers rather than a few.
8678000	8685000	But also, I don't know, if you halve the residual stream width
8685000	8690000	and give the model 4x as many layers, often performance is like about the same.
8691000	8695000	Or like not that different because the number of parameters is unchanged.
8695000	8700000	And this is just kind of a wild result about models that I think only really makes sense
8700000	8705000	within this framework of it's like an ensemble of shallow pods
8705000	8709000	and it's a trade-off between having more computation and having better memory bandwidth.
8709000	8711000	Yeah, yeah, very interesting.
8711000	8716000	Okay, I mean, just to close, superposition, it might not be a new idea.
8716000	8723000	So Janik did a paper video about this paper called Supermasks in Superposition
8723000	8726000	by Mitchell Wartsman back in 2020.
8726000	8730000	And he was talking about supermass representing sparse subnetworks
8730000	8733000	in respect of catastrophic forgetting and continual learning.
8733000	8736000	But that was slightly different because that was an explicit model
8736000	8741000	to perform masking, create subnetworks and to model, you know,
8741000	8744000	like basically a sparsity aware algorithm.
8744000	8748000	But he was still using a lot of the same language like interference and so on
8748000	8751000	and thinking about superpositions of subnetworks.
8751000	8755000	And I guess the difference is like just as we were talking about with these inductive priors
8755000	8759000	like transformers and CNNs, the models already do this stuff
8759000	8764000	without us having to explicitly code it, which I think is the interesting discovery.
8764000	8769000	Yeah, yeah, one update I've made from Wes's work is that
8769000	8774000	detokenization is probably like a pretty big fraction of what the early layers do.
8774000	8779000	And it's just really easy to represent compound words in superposition
8779000	8783000	because it's a very binary, it's either there or not there.
8783000	8786000	So alternating difference is easy to deal with.
8786000	8791000	They're mutually exclusive, so there's no simultaneous interference.
8791000	8796000	Like you cannot have Boris Johnson and Theresa May co-occur.
8796000	8800000	And there's just like so many of them.
8800000	8805000	One fact about language models that people who haven't played around them may not appreciate
8805000	8808000	is their inputs are these things called tokens.
8808000	8814000	And tokenizers are fucked because they're trained in this bizarre Byzantine way
8814000	8819000	that means that often the rarer words will get broken up into many tokens.
8819000	8820000	Yes.
8820000	8823000	Multi-word phrases are always different tokens.
8823000	8827000	Anything that's weird like a URL gets completely cursed.
8827000	8832000	And models don't want to have this happen.
8832000	8838000	So they devote a bunch of parameters to build a pseudo vocabulary of what's going on.
8838000	8844000	And just returning to your point earlier about, like, is it just these syntax-level things?
8844000	8848000	Is there some actual more semantic stuff going on?
8848000	8853000	We did also have case studies looking at contextual neurons, things like,
8853000	8858000	this code is in Python, this language is in French.
8858000	8862000	And these were seemingly monosemitic.
8862000	8865000	Like it seemed like there were specific neurons here.
8865000	8869000	And we found things like if you ablate the French neuron,
8869000	8873000	loss on French text gets much worse, what other ones are fine.
8873000	8880000	And also some interesting results that the model was, say, using this disambiguate things.
8880000	8887000	Like tokens like D are common in German and also common in Dutch.
8887000	8894000	And the neurons for those languages were being used to disambiguate for that token,
8894000	8897000	whether it was like a German D or a Dutch D,
8897000	8899000	because they've got very different meaning in the two languages.
8899000	8903000	Yeah, I wondered if you'd give me some interest in that, because as you say, in Wes's paper,
8903000	8908000	he did actually find that there are some monosemitic neurons like French, as you just said.
8908000	8913000	And in this case, the model decided that interference, in some sense, wasn't worth the burden.
8913000	8916000	But what does burden mean here?
8916000	8920000	And French is a very vague concept as well.
8920000	8924000	Yes. So, all right, a couple of observations.
8924000	8930000	First is I do not think we have properly shown they are monosemitic neurons.
8930000	8935000	We were looking, these models were trained on the pile, and we were specifically looking at them on Europal,
8935000	8940000	which is like a data set of European Parliament transcripts, which are labeled by language.
8940000	8944000	And we found a neuron that seemed to strongly disambiguate French from non-French.
8944000	8948000	But it was on this domain of parliamentary stuff.
8948000	8952000	And because models really want to avoid simultaneous interference,
8952000	8957000	if they did have superposition, they'd probably want to do it with something that isn't likely to co-occur in this context.
8957000	8962000	I don't know, this is a list variable in Python, which we didn't check very hard for.
8962000	8969000	And in particular, this is messy to check for, because in order to do that, you need to answer these questions like,
8969000	8971000	what is French?
8971000	8976000	Like, there's a bunch of English checks to activate for, but it will activate on words like,
8976000	8979000	Sacre Bleu and Trebilla.
8979000	8986000	And I think I count this as French, but like, I don't have a rigorous definition of French.
8986000	8991000	And I think an open problem I'd love to see someone pursue is just,
8991000	8995000	can you prove one of these neurons is actually a French detecting neuron or not?
8995000	8998000	And what would it even look like to do that?
8998000	9004000	And yeah, regarding interference in the burden, so the way I think about it,
9004000	9009000	if two features are not orthogonal, then,
9009000	9012000	no, sorry, this is more interesting in the case of neurons.
9012000	9016000	If there's multiple things that could all activate a neuron,
9016000	9022000	then it's harder for the downstream bit of the model to know how to use the fact that that neuron activated,
9022000	9027000	because there are multiple things, even if they don't co-occur, because they're mutually exclusive.
9027000	9029000	And this is just a cost.
9029000	9033000	And there's a trade-off between having more features and not having this cost.
9033000	9038000	And features like this is in French are really load-bearing.
9038000	9042000	They're just really important for a lot of circuitry here.
9042000	9048000	And so, theoretically, the model might want to dedicate an entire neuron to this,
9048000	9054000	but if you dedicate an entire neuron, you lose the ability to do as much superposition.
9054000	9058000	My intuition is the number of features that can be represented in superposition
9058000	9063000	is actually grows more than linearly with the number of dimensions.
9063000	9068000	So this might be significantly worse than just having one fewer feature.
9068000	9075000	So we are now in the next chapter of this beautiful podcast, and we're going to talk about transformers.
9075000	9081000	So how exactly do transformers represent algorithms and circuits?
9081000	9085000	And also, you've written this beautiful mathematical framework about transformers,
9085000	9090000	which, of course, is working very closely with Catherine Olsen and Chris Olat.
9090000	9091000	And Nelson Olsh.
9091000	9094000	And Nelson, my apologies.
9094000	9099000	Yeah, so in terms of understanding, yeah.
9099000	9102000	So if you wanted to do a mechanism to interpretability on a model,
9102000	9109000	you need to really deeply understand the structure of the model.
9109000	9112000	What are the layers? What are the parameters? How do they fit together?
9112000	9116000	What are the kinds of things that make sense there?
9116000	9121000	And let's see.
9121000	9130000	So, yeah, there's like a couple of key things I want to emphasize from that paper, though,
9130000	9134000	I don't know, it's also one of my, like, all-time top three interpretability papers.
9134000	9136000	People should just go read it.
9136000	9140000	And after reading it, check out my three-hour video walkthrough about it,
9140000	9144000	which apparently is most useful if you've already read the paper,
9144000	9147000	because it's that deep anyway.
9147000	9149000	Yeah, so a couple of things I want to call out from that,
9149000	9154000	especially for people who are kind of familiar with other network but not transformers.
9154000	9158000	The first, we've already discussed the residual stream as the central object.
9158000	9162000	And the second is how to think about attention,
9162000	9166000	because attention is the main thing which is weird about models.
9166000	9171000	They have these MLP layers, which actually represent, like,
9171000	9175000	two-thirds of the parameters in a transformer, which is often an underrated fact,
9175000	9178000	but attention is the interesting stuff.
9178000	9183000	So, transformers have a separate residual stream for each input token,
9183000	9189000	and this contains, like, all memory the model would store at that position.
9189000	9193000	But MLP layers can only process information in place.
9193000	9196000	You need attention to move things between positions.
9196000	9200000	And classically, people might have used stuff like a 1D convolution.
9200000	9204000	You average over 10 things in a sliding window.
9204000	9211000	This is baking in the inductive bias that nearby information is more likely to be useful.
9211000	9217000	But this is kind of a pretty limited bias to bake in,
9217000	9222000	and the story of deep learning is that over time, people have realized,
9222000	9228000	wait, we should not be trying to force the model to do specific things.
9228000	9232000	We understand, we should not be telling the model how to do its job.
9232000	9236000	If it has enough parameters and is competent enough, it can figure it out on its own.
9236000	9240000	And so the idea here is rather than giving it a convolution,
9240000	9246000	you give it this attention mechanism where each token gets a query saying what it's looking for,
9246000	9251000	each previous token gets a key saying what it has to offer,
9251000	9258000	and the model looks from each destination token to the source tokens earlier on
9258000	9262000	with the keys that are most relevant to the current query.
9262000	9268000	And models, and the way to think about an attention head,
9268000	9272000	so attention layers break up into these distinct bits called heads,
9272000	9277000	which act independently of the others and add to their outputs together,
9277000	9280000	and just directly add to the residual string.
9280000	9284000	This is sometimes phrases concatenate their outputs and then multiply by a map,
9284000	9287000	but this is mathematically equivalent.
9287000	9292000	Each head acts independently and in parallel,
9292000	9298000	and further, you can think of each head as separately breaking down into a
9298000	9303000	which information to move a bit determined by the attention,
9303000	9307000	which are determined by the query and key calculating matrices,
9307000	9311000	and the what information to move once I know where I'm looking,
9311000	9316000	which are determined by the value and output matrices.
9316000	9321000	We often think about these in terms of the QK matrix,
9321000	9326000	WQ times WK transpose, and the OV matrix,
9326000	9332000	WO times WV, because there's no long linearity in between,
9332000	9337000	and these two matrices determine like what the head does.
9337000	9342000	And the reason I say these are kind of independent is that
9342000	9346000	once the model has decided which source tokens to look at,
9346000	9351000	the information that gets output by the head is independent of the destination token,
9351000	9357000	and like the query only matters for choosing where to move information from,
9357000	9361000	and this can result in interesting bugs,
9361000	9366000	like there's this motif of a skip trigram,
9366000	9374000	the model realizes that if the current thing is three and two has appeared in the past,
9374000	9376000	then four is more likely to come next.
9376000	9380000	If the current thing is three and four has appeared in the past,
9380000	9382000	two is more likely to come next.
9382000	9389000	But if you have multiple destination tokens that all want the same source token,
9389000	9394000	for example, the phrase keep in mind can be a skip trigram,
9394000	9396000	really it should be a trigram,
9396000	9401000	but tiny models aren't very good at figuring out what's exactly at the previous position.
9401000	9404000	Keep at bay is another trigram,
9404000	9408000	but in an at, we'll both look at the same keep token,
9408000	9414000	and so they must boost both at and mind for both of them.
9414000	9421000	So we'll also predict keep in bay and keep at mind.
9421000	9429000	And possibly we should move on to induction heads,
9429000	9431000	which are a good illustrative example.
9431000	9433000	Yeah, it's going to come onto that.
9433000	9435000	So on these induction heads,
9435000	9438000	you've said that they seem universal across all models.
9438000	9443000	They underlie more complex behavior, like few-shot learning.
9443000	9445000	They emerge in a phase transition,
9445000	9449000	and they're crucial for this in-context learning.
9449000	9454000	And you said that sometimes specific circuits underlie emergent phenomena,
9454000	9460000	and we may want to predict or understand emergence by studying these circuits.
9460000	9463000	So what do we know so far?
9463000	9465000	A lot of questions in there.
9465000	9467000	All right, taking this in order.
9467000	9470000	So what is an induction head?
9470000	9472000	I've already mentioned this briefly.
9472000	9475000	Text often contains repeated subsequences,
9475000	9480000	like after Tim, Scarf may come next,
9480000	9483000	but if Tim Scarf has appeared like five times,
9483000	9486000	then it's much more likely to come next.
9486000	9490000	In toy two-layer attention-only language models,
9490000	9494000	we found this circuit called an induction head, which does this.
9494000	9498000	It's a real algorithm that works on, say, repeated random tokens.
9498000	9503000	And we have some mechanistic understanding of the basic form of it,
9503000	9508000	where there's two attention heads and two different layers working together.
9508000	9515000	The later one called an induction head looks from Tim to previous occurrences of Scarf.
9515000	9518000	The first one is a previous token head,
9518000	9521000	which on each Scarf looks at what came before,
9521000	9527000	and is like, ah, this is a Scarf token which has Tim before,
9527000	9533000	and then the induction head looks at tokens where the token before them was Tim,
9533000	9537000	or where the token before them was equal to the current token.
9537000	9543000	And when the induction head decided to look at Scarf,
9543000	9547000	which is determined purely by the QK matrix,
9547000	9552000	it then just copies that to the app, which is purely done by the OV matrix.
9552000	9556000	And I think induction heads are a really interesting circuit case study,
9556000	9564000	because induction heads are all of the interesting computation is being done by their attention pattern.
9565000	9568000	Tim's scarf could be anywhere in the previous context,
9568000	9570000	and this algorithm will still work.
9570000	9577000	And this is important, because this is what lets the model do
9577000	9583000	tracking of long-range dependencies in the text, where it looks far back.
9583000	9589000	And you can't bake this in with a simple thing like a convolutional layer.
9590000	9596000	In fact, transformers seem notably better than old architectures like LSTMs and RNNs,
9596000	9601000	in part because they have induction heads that let them track long-range dependencies.
9601000	9604000	And, yeah.
9604000	9609000	And more generally, it often is the case that especially late-layer attention heads,
9609000	9612000	the OV bit is kind of boring, it's just copying,
9612000	9617000	but figure out where to look is where all of the interesting computation lies.
9617000	9620000	So, first of all, just to clarify, because people will know what an attention head is,
9620000	9625000	but an induction head is one of these circuits that you're talking about, just so people understand.
9625000	9632000	And we should get onto this relationship between induction heads and the emergence of in-context learning.
9632000	9640000	And also, you said it's very important that we have this scientific understanding with respect to studying emergence,
9640000	9646000	but rather that than just framing of interpretability kind of makes better models.
9646000	9655000	Yeah. So, maybe I should first explain what emergence is.
9655000	9657000	Let's do that.
9657000	9667000	I'd be really, really interested if you could just give me the simplest possible explanation of what you think emergence is.
9667000	9672000	Sure. Emergence is when things happen suddenly during training,
9672000	9679000	anger from not being there to being there fairly rapidly in a non-convex way, rather than gradually developing.
9679000	9681000	Is this interesting you said that?
9681000	9687000	Because I think of emergence as a surprising change in macroscopic phenomena,
9687000	9694000	and it's an observer-relative term, which means it's always from the perspective of another scale.
9695000	9704000	So, just a transient change in perplexity or capability or something in my mind wouldn't entail emergence.
9704000	9708000	Like, it would need to be some qualitative, meaningful thing,
9708000	9712000	rather than just, oh, the loss curve got notably better in this bit.
9712000	9723000	I think so. It's definitely related to some notion of surprise, which is inherently relative.
9723000	9725000	Yeah. Let's not get hung up on that.
9725000	9729000	So, okay. Let's say it's a transient change in something.
9729000	9731000	Mm-hmm. Yeah.
9731000	9735000	You know, when you call it transient, it's like an unexpected sudden change.
9735000	9740000	Though unexpected has so much semantic meaning on it that I don't want to use.
9740000	9743000	But, yeah, this is an infinite rabbit hole.
9743000	9748000	Yes. But I think this scale thing is relevant as well.
9748000	9752000	We are programming neural networks at the microscopic scale,
9752000	9757000	and there's some macroscopic change in capability, so it's some...
9757000	9759000	Yes. Yeah.
9759000	9761000	Yeah. And there's, like, lots of different dimensions.
9761000	9765000	You can have emergence on. You can have it as you train a model on more data.
9765000	9768000	You can have it as you make the models bigger.
9768000	9771000	And these are both interestingly different kinds.
9771000	9776000	One of the more famous examples is Chain of Thought and Few Shot Prompting,
9776000	9779000	where DP3 is pretty good at this.
9779000	9782000	Earlier models were not good at this. This is kind of surprising.
9782000	9787000	Chain of Thought is particularly striking because people just noticed
9787000	9791000	a while after DP3 was public that if you tell it to think step by step, it becomes much better.
9791000	9797000	There's this recent innovation of Tree of Thought that I'm not particularly familiar with,
9797000	9803000	but I understand as kind of like applying Monte Carlo Tree Search on top of Chain of Thought.
9803000	9805000	Yes. Yes.
9805000	9809000	Where you're like, well, there's many ways we can branch at each point.
9809000	9813000	Let's use Tree Search algorithms to find the ultimate way of doing this.
9813000	9817000	Yeah. But with, let's say, Scratchpad and Chain of Thought,
9817000	9820000	I don't necessarily see that as an emergent...
9820000	9825000	Well, maybe there's an emergent reasoning capability that comes into play
9825000	9828000	when you have a certain threshold size model,
9828000	9836000	but I think of it more as kind of having an intermediate augmented memory in the context.
9836000	9842000	So you're kind of filling in a gap in cognition by saying you're allowed to...
9842000	9847000	It's not just remembering things, it's also reflecting on things that didn't work.
9847000	9854000	Yes. So, yeah, clarifying, when I say emergent, when I say Chain of Thought is an emergent property,
9854000	9860000	I mean, the capacity to productively do Chain of Thought is the emergent thing
9860000	9864000	and telling the model to things step by step is a user-driven thing.
9864000	9867000	But, I don't know, I kind of...
9867000	9874000	Just as a point of order, though, was it just that it was discovered after GPT-3
9874000	9877000	or would it work on GPT-2?
9878000	9884000	I would have guessed it doesn't work very well on GPT-2, but I've not checked.
9884000	9889000	I'd be pretty interested... I'm sure someone has looked into this, I haven't looked very hard.
9889000	9893000	I guess, like, so a lot of my motivation for this work comes from...
9893000	9898000	I care a lot about AIX risk and AI alignment and how to make these systems good for the world.
9898000	9905000	And when I see things like, oh, we realize that you can make GPT-3 much better by asking it to things step by step,
9905000	9908000	I'm like, oh, no.
9908000	9913000	What kinds of things could the systems you make be capable of that we just haven't noticed yet?
9913000	9917000	That's the concern that the genie's already out the bottle.
9917000	9922000	And, I mean, DeepMind just published this Tree of Thought paper.
9922000	9924000	It's a really simple idea.
9924000	9928000	It's basically a star search over trajectories of prompts,
9928000	9933000	and you use the model itself to evaluate the value of a trajectory.
9933000	9936000	And I could have done that.
9936000	9937000	Anyone could.
9937000	9939000	Similar thing with auto-GPT and all this stuff.
9939000	9941000	I'm more skeptical than you are.
9941000	9949000	I think in the case of Tree of Thought, it closes a capability gap in respect of certain tasks which were not working very well
9949000	9952000	because they don't have that kind of system to...
9952000	9954000	Models don't seem to plan ahead very well.
9954000	9959000	But I still think that it's not just going to magically turn into super intelligent.
9959000	9961000	I mean, we can talk about this a little bit later.
9961000	9963000	Yeah, okay.
9963000	9965000	Yeah, so...
9965000	9969000	Yeah, I think this is also pretty relevant to much more near-term risks.
9969000	9973000	I don't know, there's lots of things that a sufficiently capable model could do
9973000	9975000	that might be pretty destabilizing to society,
9975000	9981000	like write actually much better propaganda than human writers can or something.
9981000	9985000	And if Tree of Thought makes it possible to do that
9985000	9988000	in a way that we did not think was possible when GPT-4 was deployed,
9988000	9992000	that's like an interesting thing that I care about noticing.
9992000	9995000	It's not a very good example, but...
9995000	9998000	Yeah, it is.
9998000	10000000	But being able to...
10000000	10005000	I mean, first of all, it's been possible to create misinformation for a long time.
10005000	10010000	This is why I specified be able to do it notably better than humans can.
10010000	10012000	I totally agree.
10012000	10016000	The longer doing it a bit more cheaply and a bit more scale doesn't seem obviously that important.
10016000	10018000	You could argue that, like, I don't know,
10018000	10022000	being a spam bot that feels indistinguishable from a human
10022000	10027000	is like a more novel thing that's actually different.
10027000	10028000	Yeah.
10028000	10031000	But, I don't know, this is like an off-the-cuff example.
10031000	10034000	I don't want to get too deep into this,
10034000	10036000	because it's not a point I care that deeply about.
10036000	10038000	Yeah, I mean, we can come back to it a bit,
10038000	10040000	but I think we are nearly already there.
10040000	10041000	Yeah.
10041000	10043000	You know, this irreversibility thing.
10043000	10046000	We don't know.
10046000	10049000	Computer games are photorealistic.
10049000	10052000	Chatbots are indistinguishable,
10052000	10055000	and AI art is pretty much indistinguishable.
10055000	10057000	And that could work.
10057000	10059000	I mean, I spoke to Daniel Dennett about it last week,
10059000	10064000	and he said he's really worried about the epistemic erosion of our society,
10064000	10067000	more so interestingly than the ontological erosion.
10067000	10071000	And I discovered later that's because he's not a big fan of anything ontological.
10072000	10076000	Yeah, it is potentially a problem,
10076000	10082000	but I guess to me, people might overestimate the scale
10082000	10085000	and magnitude of change of this.
10085000	10089000	I feel that, I know I don't want to echo Sam Altman here,
10089000	10091000	but he said that we are reasonably smart people,
10091000	10097000	and we can adapt and recognize deep fakes and so on.
10097000	10098000	Yeah.
10098000	10102000	These are complicated societal questions.
10102000	10105000	I guess I mostly just have the position of man.
10105000	10108000	It sure is kind of concerning that we have these systems
10108000	10110000	that could potentially pose risks,
10110000	10113000	but you don't know what they do and decide to deploy them,
10113000	10115000	and then we discover things they can do.
10115000	10120000	And I think that the research direction I'm trying to advocate for here
10120000	10124000	is just better learn how to predict this stuff more than anything,
10124000	10127000	which hopefully we can all agree is like an interesting direction.
10127000	10130000	And there's all kind of debates about is emergent phenomena
10130000	10132000	like actually a real thing?
10132000	10135000	Like this recent, is this a mirage paper,
10135000	10138000	which I think was a bit over-claiming,
10138000	10141000	but does make a good point that if you choose your metric
10141000	10145000	to be sufficiently sharp, everything looks dramatic.
10145000	10148000	One thing I've definitely observed is if you have an accuracy graph
10148000	10151000	with a log scale x-axis for grokking,
10151000	10154000	it looks fantastically dramatic.
10154000	10158000	And I was very careful to not do this in my paper
10158000	10161000	because it is cheating.
10161000	10164000	But yeah.
10164000	10170000	So my particular hot take is that I believe emergence is often underlain
10170000	10174000	by the model learning some specific circuit
10174000	10179000	or some small family of circuits in a fairly sudden phase transition
10179000	10183000	that enables this overall emergent thing.
10183000	10186000	And this sequel paper led by Catherine Olson
10186000	10188000	in Contest Learning and Induction Heads
10188000	10191000	is a big motivator of my belief for this.
10191000	10196000	So the idea of the paper is we have this,
10196000	10200000	we found induction heads in these toy-till-artentionally models.
10200000	10203000	We somewhat mechanistically understood them,
10203000	10206000	at least in the simplest case of induction.
10206000	10210000	We use this to come up with more of the behavioral test
10210000	10212000	for whether it's induction heads.
10212000	10214000	You just give them all repeated random tokens
10214000	10216000	and you look at whether it looks induction-y.
10216000	10220000	And we found that these occurred in basically all models we looked at,
10220000	10225000	up to 13B, even though we didn't fully reverse engineer them there.
10225000	10229000	And we then found that this was really deeply linked
10229000	10232000	to the emergence of in-context learning.
10232000	10235000	There's a lot of jargon in there, so let's unpack that.
10235000	10237000	In-context learning, already briefly mentioned,
10237000	10240000	it's like tracking long-range dependencies in text, like,
10240000	10243000	you can use what was on, which was three pages ago,
10243000	10246000	to predict what comes next in the current book,
10246000	10249000	which is a non-trovial thing.
10249000	10252000	It is not obvious to me how I would program a model to do.
10252000	10255000	In-context learning is emergent.
10255000	10259000	If you operationalize it as average loss on the 500th token
10259000	10261000	versus average loss on the 50th token,
10261000	10264000	there's a fairly sudden period in training
10264000	10269000	where it goes from not very good at it to very good at it.
10269000	10271000	Just a tiny point forward of that.
10271000	10273000	One interesting thing about in-context learning
10273000	10276000	is you're learning at inference time, not training time.
10276000	10277000	Yes.
10277000	10280000	But you're not changing anything in the underlying model,
10280000	10283000	which means anything it can do, presumably,
10283000	10289000	must be materializing a competence which was acquired during training.
10289000	10292000	So it's coming back to this periodic table thing, right?
10292000	10294000	You've just learned all these platonic primitives.
10294000	10296000	You do this in-context learning.
10296000	10298000	You say, I want you to do this. Here's an example.
10298000	10303000	And you've got all of these freeze-dried periodic computational circuits,
10303000	10306000	and they spring into life, and they compose together,
10306000	10308000	and they do the thing.
10308000	10309000	Yes.
10309000	10310000	Yes.
10310000	10314000	I think induction heads are, to my eyes,
10314000	10317000	the canonical example of an inference time algorithm
10317000	10320000	stored in the model's weights that get supplied.
10320000	10324000	And I'm sure there's a bunch more that no one has yet found.
10324000	10328000	And, yeah, a lot of my model is that prompt engineering
10328000	10331000	is just telling the model which of its circuits to activate
10331000	10335000	and just engaging with various quirks of training
10335000	10338000	that have made it more or less steerable in different ways.
10338000	10342000	And, yeah, so induction heads also emerge
10342000	10344000	in a fairly sudden phase transition.
10344000	10348000	And we, and exactly at the same time,
10348000	10350000	and we present a bunch more evidence in the paper
10350000	10353000	that there's, like, actually a causal link here.
10353000	10357000	Like, one-layer models have neither the in-context learning
10357000	10359000	or the induction heads phase chain,
10359000	10361000	because they can't do induction heads,
10361000	10363000	because they're only one layer.
10363000	10366000	But if you adapt the architectures,
10366000	10368000	they can form induction heads with only one layer.
10368000	10370000	Now they have both of these phenomena.
10370000	10372000	If you obliterate induction heads,
10372000	10374000	in-context learning gets systematically worse.
10374000	10379000	And a particularly fun qualitative study
10379000	10383000	was looking at soft induction heads,
10383000	10387000	heads that seem to be doing something induction-y
10387000	10389000	in other domains,
10389000	10392000	like a head which attends from the current word in English
10392000	10395000	to the thing after the current word in French.
10395000	10398000	Or, more excitingly, a few-shot learning head
10398000	10403000	on this random synthetic pattern recognition task we made
10403000	10406000	where it attended back to the most relevant examples
10406000	10409000	to the current one.
10409000	10412000	And my interpretation of all this
10412000	10415000	is that there's something fairly fundamental
10415000	10419000	about the induction-y algorithm for in-context learning.
10419000	10422000	So the way I think about it,
10422000	10425000	let's say you've got two...
10425000	10427000	You want to learn some relation.
10427000	10430000	You've got some local context A
10430000	10432000	and some past context B.
10432000	10435000	And if you observe A and you observe B in the past,
10435000	10438000	this gives you some information about what comes next.
10438000	10441000	There's two ways this could work out.
10441000	10443000	It could be symmetric.
10443000	10445000	B helps A and A helps B.
10445000	10447000	Or asymmetric.
10447000	10449000	B helps A, but A does not help B
10449000	10451000	if they're the other way around.
10451000	10454000	Asymmetric might be like knowing the title of a book
10454000	10456000	tells you what comes next,
10456000	10459000	but knowing what's in a random paragraph in the previous bit
10459000	10462000	doesn't tell you the title.
10462000	10465000	While symmetric is like...
10465000	10467000	I know, English sentence helps French sentence,
10467000	10469000	French sentence helps English sentence.
10469000	10474000	And if you have N symmetric relations,
10474000	10478000	like English, French, German, Dutch, Latin, whatever,
10478000	10480000	where each of them helps each other,
10480000	10482000	this is really efficient to represent.
10482000	10484000	Because rather than needing to represent
10484000	10487000	N squared different relations separately,
10487000	10489000	like you would in the asymmetric case,
10489000	10492000	you can just map everything to the same latent space
10492000	10494000	and look for matches.
10494000	10497000	And fundamentally, this is what induction heads are doing.
10497000	10500000	They're mapping current token
10500000	10503000	and previous token of thing in the past
10503000	10505000	to the same latent space and looking for matches.
10505000	10509000	And to me, this is just like a fairly natural primitive
10509000	10511000	of attention.
10511000	10515000	And this is exciting because, A,
10515000	10517000	we found this deep primitive
10517000	10520000	by looking at toy two-layer attentionally models.
10520000	10524000	B, it was important for understanding
10524000	10527000	and ideally for predicting the emergent phenomena
10527000	10529000	of in-context learning.
10529000	10533000	And two takeaways I have from this
10533000	10535000	about work we should be doing.
10535000	10537000	The first is we should be going harder
10537000	10539000	at looking at toy language models.
10539000	10542000	Like open source to scan of 12 of them.
10542000	10544000	And I'd love to see what people can find
10544000	10547000	in one-layer models with MLPs
10547000	10550000	because we really suck at transformer MLP layers.
10550000	10553000	And one layer should just be easier than other ones.
10553000	10556000	And the second thing is
10556000	10559000	I really want a better
10559000	10562000	and more scientific understanding of emergence.
10562000	10564000	Why does that happen?
10564000	10567000	Really understanding particularly notable case studies of it.
10567000	10570000	Testing the hypothesis that it is driven
10570000	10574000	by specific kinds of circuits like induction heads
10574000	10578000	or at least specific families of circuits.
10578000	10580000	Even though, I don't know,
10580000	10582000	you could argue that because we haven't fully reverse engineered
10582000	10584000	the things in the larger models,
10584000	10586000	we really know it's actually an induction head.
10586000	10590000	And yeah.
10590000	10592000	More generally, a lot of my vision
10592000	10594000	for why mechantup matters
10594000	10598000	is this kind of scientific understanding of models.
10598000	10601000	Like I don't care about making models better,
10601000	10603000	but I care about knowing what's going to happen,
10603000	10605000	knowing why stuff happens,
10605000	10607000	achieving real understanding.
10607000	10610000	And getting a scientific understanding
10610000	10612000	of things like emergence
10612000	10614000	seems like one of the things mechantup
10614000	10616000	might be uniquely suited to do,
10616000	10618000	but also no one checked very hard.
10618000	10621000	And you, dear listener, could be the person who checks.
10621000	10624000	So there was a paper by Kevin Wang et al.
10624000	10626000	called Interpretability in the Wild,
10626000	10631000	a circuit for indirect object identification in GPT-2 Small,
10631000	10634000	which found a circuit for indirect object identification.
10634000	10639000	So they discovered backup name and mover heads,
10639000	10641000	which normally don't do much.
10641000	10644000	They take over when the main name mover head are ablated.
10644000	10647000	And they said mechanistic interpretability
10647000	10650000	has a validation set for more scalability techniques.
10650000	10652000	They've understood a clear place
10652000	10655000	that these ablations can be misleading.
10655000	10657000	So...
10657000	10659000	Yeah.
10659000	10661000	So, yeah, bunch one pack in there.
10661000	10665000	So I really like the interpretability in the wild paper.
10665000	10668000	Also, Kevin was only 17 when he wrote it.
10668000	10672000	Like, man, I was doing nothing remotely as interesting
10672000	10674000	when I was in high school.
10674000	10676000	So props to him.
10676000	10679000	But also a sign of how easy it is
10679000	10681000	to pick low hanging fruit
10681000	10684000	and do groundbreaking interpretability work.
10685000	10687000	Such a young field.
10687000	10689000	I know it's so impressive.
10689000	10692000	Yeah, I've just checked his Twitter.
10692000	10694000	Hey, Kevin.
10694000	10697000	And, yeah, so...
10697000	10699000	To me, the underline...
10699000	10701000	Yeah, so I was zooming out a bit.
10701000	10704000	I think there's a family of techniques
10704000	10706000	around causal interventions
10706000	10708000	and their use in mech and top
10708000	10710000	that's useful to understand here.
10710000	10712000	So...
10712000	10716000	The core technique is this idea of activation patching.
10716000	10718000	Where...
10718000	10720000	So let's...
10720000	10722000	So one of the problems with understanding
10722000	10724000	a model's features and circuits
10724000	10728000	is models are full of many, many different circuits.
10728000	10731000	Each circuit does not activate on many inputs.
10731000	10734000	But each circuit will activate...
10734000	10737000	But on each input, many circuits will activate.
10737000	10739000	And in order to do good mech and top work,
10739000	10743000	you need to be incredibly surgical and precise,
10743000	10746000	which means you need to learn how to isolate a specific circuit.
10746000	10750000	And let's consider a statement like...
10754000	10756000	The Eiffel Tower is in Paris
10756000	10758000	versus the Colosseum is in Rome.
10758000	10760000	These are both...
10760000	10762000	There's lots of features happening.
10762000	10764000	There's lots of circuits being activated
10764000	10766000	on the Eiffel Tower is in Paris.
10766000	10768000	This is in English.
10768000	10770000	You're doing factual recall.
10770000	10772000	You are outputting a location.
10772000	10774000	You are outputting a proper noun.
10774000	10776000	This is a European landmark.
10776000	10778000	Et cetera, et cetera.
10778000	10781000	And like, I want to know how the model knows
10781000	10783000	the Eiffel Tower is in Paris.
10783000	10785000	But the Colosseum is in Rome.
10785000	10789000	Controls almost everything apart from the fact.
10789000	10791000	And so...
10791000	10794000	What I can try to do is
10794000	10797000	causally intervene on the Colosseum run.
10797000	10801000	And replace, say, the output of an attention head
10801000	10805000	with its outputs on the Eiffel Tower prompt
10805000	10809000	and see how much this changes the answer from Rome to Paris.
10809000	10813000	And this...
10813000	10819000	Yeah, this patch can let me really isolate
10819000	10823000	how the circuitry for just this specific thing works.
10823000	10827000	And there's all kinds of work around this.
10827000	10830000	Obnoxiously, all of it uses different notation,
10830000	10835000	like resample ablations and causal tracing
10835000	10838000	and causal mediation analysis and entertained interventions.
10838000	10842000	All similar words are basically the same thing.
10842000	10845000	But, yeah.
10845000	10850000	The really key insight here is this kind of surgical intervention.
10850000	10853000	A classic technique in interpretability is ablations,
10853000	10855000	where you just set something to zero.
10855000	10859000	And it's kind of janky because if you break something in the model,
10859000	10862000	which wasn't interestingly used for the task,
10862000	10864000	then everything dies.
10864000	10867000	Or if you break it in interesting ways, everything dies.
10867000	10870000	For example, in GPT2 Small,
10870000	10875000	almost every single task breaks if you delete the 0th MLP layer.
10875000	10878000	Yeah, as far as I can tell,
10878000	10882000	the 0th MLP layer is kind of an extended embedding.
10882000	10886000	GPT2 Small has tied embeddings and unembeddings,
10886000	10888000	so they're transposed of each other,
10888000	10891000	which is wildly unprincipled in my opinion.
10891000	10895000	And the model seems to be both using this for just detokenization
10895000	10898000	and combining nearby things with the first attention layer,
10898000	10903000	the 0th attention layer, and just undoing the tightness.
10903000	10907000	But this means that basically everything is reading from that.
10907000	10910000	And I've seen people do zero ablations and everything
10910000	10912000	and be like, oh, this is an important part of the circuit.
10912000	10915000	Let's get really sidetracked by this.
10915000	10918000	Because the effect size is so big.
10918000	10920000	Man, being a mech interpret research feels my mind
10920000	10922000	with such bizarre trivia like this.
10922000	10924000	It's great.
10924000	10927000	Models, so bizarre.
10927000	10931000	And so, yeah.
10931000	10933000	This calls on intervention.
10933000	10936000	There's kind of two conceptually different kinds of interventions.
10936000	10939000	You can take the Eiffel Tower prompt,
10939000	10942000	patch in something from the Colosseum
10942000	10945000	and see if it breaks the ability to output Paris,
10945000	10949000	to verify which bits kind of are necessary,
10949000	10952000	such that getting rid of them will break something.
10952000	10956000	Or you can patch something from the Paris run into the Colosseum run
10956000	10958000	and see if that makes it output Paris,
10958000	10961000	which is testing for stuff that's sufficient.
10961000	10964000	I call the first one a resample ablation
10964000	10967000	because you're messing up a component by resampling
10967000	10971000	and the second one denoising or causal tracing
10971000	10976000	because you're intervening with a bit of information
10976000	10979000	and seeing if that is sufficient for everything else.
10979000	10981000	Though none of these names are good.
10981000	10983000	I would love someone to come up with better names.
10983000	10987000	And there's all kinds of families of work building on this.
10987000	10992000	I have this post called attribution patching
10992000	10994000	that tries to apply this as an industrial scale
10994000	10997000	by using gradients to approximate it,
10997000	11000000	which is fast enough that you could take GPD3
11000000	11002000	and its four million neurons
11002000	11005000	and do attribution patching on all neurons at once
11005000	11007000	on every position.
11007000	11009000	Great post.
11009000	11014000	Redwood Research has this technique called causal scrubbing,
11014000	11018000	which I view as activation patching
11018000	11021000	gone incredibly hard and rigorous
11021000	11024000	that tries to come up with an automated metric
11024000	11028000	for saying this hypothesis about a model
11028000	11031000	is actually accurate for how it works,
11031000	11033000	where it's kind of complicated.
11033000	11036000	But the core idea is you think of a hypothesis
11036000	11039000	as saying which resample ablations are allowed
11039000	11042000	and you make all of the resample ablations
11042000	11044000	that should be allowed.
11044000	11048000	Like these components of the model shouldn't really matter
11048000	11052000	so we can just patch in stuff from random other inputs.
11052000	11054000	If you've got, say, an induction head,
11054000	11057000	you might think the induction head cares about
11057000	11064000	the current token and the thing before the past token
11064000	11067000	that it's going to inductionally attend to.
11067000	11072000	So let's replace the token that it's going to be attending to
11072000	11075000	with a token from a different input
11075000	11077000	but with the same token before it.
11077000	11079000	My hypothesis about the induction head
11079000	11082000	says this should be allowed, so let's do that.
11082000	11085000	I wouldn't want to introduce a rant
11085000	11088000	but the metric he uses is really important.
11088000	11093000	Yes, this is one of my hobby horses.
11093000	11099000	So some of the original work looking at the patching stuff
11099000	11104000	like David Bow and Kevin Meng's excellent Rome paper
11104000	11109000	uses the probability of Paris as their metric
11109000	11112000	and there are other papers that use things like accuracy
11112000	11116000	as their metric and generally I think of metrics
11116000	11120000	as being on a spectrum from like soft to sharp.
11120000	11127000	So generally I think of models as thinking in log space.
11127000	11130000	They are kind of acting like basions.
11130000	11134000	They are trying to figure out some things in Paris
11134000	11136000	and there will be five separate heads
11136000	11139000	that each contribute one to the correct logit
11139000	11142000	and each of these can be thought of as one bit of information
11142000	11147000	and together they get you the right probability of, say, 0.8.
11147000	11151000	But if you patch in each one in isolation
11151000	11154000	the probability changes negatively
11154000	11158000	because probability is exponential in the logits.
11158000	11160000	So using probability you're like,
11160000	11163000	oh, this head patch doesn't really matter.
11163000	11166000	So in this paper they did this thing of patching in
11166000	11169000	like 10 adjacent layers at once.
11169000	11172000	And to me a really core principle of this kind of causal
11172000	11174000	intervention and mechanistic technique
11174000	11176000	is you want to be as surgical as possible
11176000	11178000	to be as deeply faithful as possible
11178000	11180000	to what the neural model is actually doing.
11180000	11183000	So in this case there was an interaction between them.
11183000	11186000	They were effectively making several interactions
11186000	11188000	at once.
11188000	11193000	Yes, they were replacing 10 adjacent layers
11193000	11195000	and patching things in different layers
11195000	11197000	is always a bit weird.
11197000	11199000	I don't think that part's that objectionable.
11199000	11204000	I mostly just feel like if you choose a metric like log prop
11204000	11209000	it allows you to be much more surgical about how you intervene.
11209000	11214000	It allows you to identify subtle effects of things.
11214000	11217000	Accuracy is even worse because accuracy
11217000	11220000	is basically rounding things to zero or one.
11220000	11222000	So like if the threshold is 2.5
11222000	11225000	any individual patch does nothing.
11225000	11228000	Any re-sample ablation does nothing.
11228000	11231000	But if you patch in like the 10 adjacent layers
11231000	11233000	it will do everything.
11233000	11236000	And this can be kind of misleading.
11236000	11241000	Another one I often see people do is
11242000	11248000	they look at things like the rank of an output.
11248000	11250000	At which point does the model realize Paris
11250000	11252000	is the most likely next token?
11252000	11254000	And this can be super misleading
11254000	11256000	because this will make you think
11256000	11259000	the third head is the only head that matters.
11259000	11261000	When really all five of them matter
11261000	11263000	the order is kind of arbitrary.
11263000	11267000	And yeah, I've seen papers that I think
11267000	11270000	got somewhat misled by using metrics like this.
11271000	11274000	And metrics, they matter so much.
11274000	11276000	It's so easy to trick yourself.
11276000	11278000	My high level pitch is just,
11278000	11281000	mech and tub is great, mech and tub is beautiful.
11281000	11283000	Also the field is incredibly young.
11283000	11285000	There's maybe 30 full time people
11285000	11287000	working on it in the world.
11287000	11289000	There's a ton of low hanging fruits.
11289000	11291000	I've done major research in this field
11291000	11294000	I've been at for like less than two years.
11296000	11299000	I would love people to come and help
11299000	11301000	us solve problems and do research here.
11301000	11304000	And we'll link to my post on getting started
11304000	11308000	and my sequence called 200 Cronkidopin problems
11308000	11311000	in the description to this, hopefully.
11311000	11314000	And yeah, I think there's just,
11314000	11316000	it's not that hard to get started.
11316000	11317000	It's really fun.
11317000	11319000	Hopefully I've nerd sniped you
11319000	11323000	with at least one thing in this podcast.
11323000	11325000	And if you're at least vaguely curious,
11325000	11327000	it's just really easy to open one of the tutorials
11327000	11331000	linked in my posts and just start screwing around.
11331000	11334000	And I'd love to see what you can find.
11334000	11335000	Beautiful.
11335000	11338000	Also the deep mind element team is currently hiring
11338000	11340000	and people should apply,
11340000	11343000	which includes hiring from a mechanistic interoperability team.
11343000	11344000	Amazing.
11344000	11346000	Do they have to do lead code?
11346000	11348000	I have no idea.
11348000	11349000	Can't remember.
11349000	11352000	Yeah, yeah, we did an amazing video
11352000	11355000	with Petr Velichkovich.
11355000	11358000	I gave him one of my lead code challenges
11358000	11360000	and annoyingly he aced it.
11360000	11364000	It's all that deep mind interview practice.
11364000	11368000	Anyway, okay, let's talk about super intelligence.
11368000	11371000	Now, I spoke with our mutual friend,
11371000	11373000	Robert Miles about a month ago.
11373000	11374000	Rob's so great.
11374000	11375000	He's a lovely chap.
11375000	11376000	Spoke all about alignment.
11376000	11379000	And he accused me of over-philosophizing everything
11379000	11381000	because I was talking all about intelligence,
11381000	11383000	one of my favorite topics.
11383000	11387000	And he said, well, what about fire?
11387000	11390000	Fire is something that people didn't understand millennia ago,
11390000	11394000	but they knew that it burnt and they knew that it was bad.
11394000	11397000	And this is like, this is like a fire,
11397000	11398000	which is very interesting.
11398000	11401000	And maybe we can bring in a little bit of effective altruism as well.
11401000	11402000	So, you know, I...
11402000	11403000	If I can just interject.
11403000	11404000	Please do, please.
11404000	11406000	If there is one thing I have learned
11406000	11408000	from the past decade of machine learning programs,
11408000	11412000	is that you do not need to understand a thing in order to make it.
11412000	11416000	And this extends to things that are smarter than us
11416000	11420000	and which are capable of leading to catastrophic risks.
11420000	11422000	Yes, yes.
11422000	11424000	Well, let's...
11424000	11427000	I'll step back a tiny bit and then we'll get there
11427000	11429000	because there's the hypothetical nature,
11429000	11431000	which I guess I have a bit of a problem with.
11431000	11433000	Now, about 10 years ago,
11433000	11436000	I was one of the first supporters of Sam Harris' podcast
11436000	11439000	and he's quite aligned to EA.
11439000	11443000	And he was talking about this very noble idea
11443000	11445000	that everyone matters equally
11445000	11448000	and people on the left should get on board with that intrinsically.
11448000	11451000	And this idea that we should quantitatively analyse
11451000	11453000	the impact of charity work
11453000	11455000	and solve an optimisation problem
11455000	11456000	and earning to give
11456000	11459000	and a lot of the stuff that MacAskill spoke about
11459000	11462000	and also philosophers like Peter Singer.
11462000	11466000	And the focus seemed to be primarily on alleviating poverty,
11467000	11470000	and we don't say the biggest problem,
11470000	11472000	we say a problem.
11472000	11474000	This is another thing our friend Robert Viles said.
11474000	11475000	He said,
11475000	11479000	the problem is when people talk about the problem,
11479000	11482000	there can be more than one problem.
11482000	11486000	But anyway, so it's a big problem.
11486000	11490000	And recently, you and I can agree
11490000	11494000	that EA circles have really laser-focused in
11494000	11498000	on existential risk from AI
11498000	11501000	as opposed to other more plausible ex-risk concerns
11501000	11503000	like pandemics or even nuclear war.
11503000	11506000	I'm not to say that they don't focus on that, but...
11506000	11509000	I am going to push back on other more plausible ex-risk.
11509000	11510000	Go on.
11510000	11511000	Go on.
11511000	11512000	I just wanted to register an objection.
11512000	11513000	Feel free to go.
11513000	11514000	Register an objection.
11515000	11519000	And cynically, from my point of view,
11519000	11523000	I see the influence of Eliezer, Bostrom,
11523000	11528000	Hansen, et cetera, kind of shifting the focus on to ex-risk.
11528000	11533000	And part of the reason for that is also this kind of overly
11533000	11536000	intellectual focus on long-termism.
11536000	11540000	And it's done in a very intellectualized way.
11540000	11545000	So it's based on the utility function now incorporating
11545000	11549000	future simulated humans on different planets,
11549000	11551000	a long time away in the future,
11551000	11554000	and making all of these intellectual jumps.
11554000	11556000	So let's start there.
11556000	11557000	What's your take?
11557000	11558000	All right.
11558000	11560000	So much stuff to respond to in there.
11560000	11561000	Good.
11561000	11563000	So, all right, a couple of things.
11563000	11566000	The first, so, cars on the table.
11566000	11569000	I care a lot about AI existential risk.
11569000	11570000	Yes.
11570000	11572000	The reason I work on mechanistic interpretability
11572000	11576000	is because I think that understanding the mysterious black
11576000	11578000	boxes that are potentially smarter than us
11579000	11582000	and may want things wildly different than what we wanted them
11582000	11586000	to want is just clearly better than not understanding them.
11586000	11587000	Yes.
11587000	11590000	And I think mechanistic interpretability
11590000	11592000	is a promising path here.
11592000	11595000	And I also would consider myself an effective altruist
11595000	11596000	and a rationalist.
11596000	11600000	So cars on the table, there's my biases.
11600000	11605000	So I generally think it's more productive to discuss
11605000	11609000	is AI catastrophic and existential risk a big deal?
11609000	11614000	Then is it the biggest deal or is it worth more resources
11614000	11618000	on the margin than global poverty or climate change
11618000	11619000	or AI ethics?
11619000	11621000	And like, there's just lots of problems.
11621000	11624000	I care way more about convincing people that AI extras
11624000	11628000	could be in your top 10 than it should be in your top one
11628000	11629000	because I feel like for most people,
11629000	11632000	it's not in their top thousand.
11632000	11636000	And there's just so much divisiveness between, say,
11636000	11639000	the AI ethics community and the AI alignment community
11639000	11641000	about whose problem is a bigger deal.
11641000	11643000	And like, both are big problems.
11643000	11645000	Why are we arguing?
11645000	11649000	And part of this is about our moral intuitions.
11649000	11652000	And this is something I spoke a lot with Conor about.
11652000	11656000	He said that in many ways he's got this technical empathy.
11656000	11661000	So sensory empathy is, I really care about my family.
11661000	11664000	I care about these concentric circles of moral status.
11664000	11666000	I really care about my family.
11666000	11669000	And if I try really hard, I can care about people
11669000	11670000	in other countries and so on.
11670000	11672000	And then if I try really, really hard,
11672000	11675000	I can care about future simulated lives on Mars.
11675000	11679000	And Conor said, the idea of this movement is about
11679000	11682000	galaxy-braining yourself into being the most empathetic person
11682000	11684000	imaginable, but it's a kind of empathy
11684000	11687000	that people don't understand.
11688000	11692000	So a separate bit of beef I have is with the entire notion
11692000	11694000	of long-termism.
11694000	11697000	So long-termism is this idea...
11697000	11701000	So long-termism is generally caring about the long-term future.
11701000	11705000	There's the strong form of value in the future
11705000	11708000	basically entirely dominates things today.
11708000	11712000	And weaker forms of just this really, really matters.
11712000	11717000	And a common misconception about AIX risk and AI safety
11717000	11723000	is that you should only work on this if you are a long-termist.
11723000	11727000	That, you know, it's a one in a billion chance of mattering,
11727000	11731000	but there's a quintillion future lives,
11731000	11735000	so this outweighs everyone alive today in moral worth.
11735000	11739000	Or, well, we're only gonna get AGI in like 500 years,
11739000	11742000	but we're gonna work on it now just in case.
11742000	11745000	And like, I think both of these are just nonsense.
11745000	11749000	Like, I guess as a concrete example,
11749000	11753000	Effective Artists have worked on pandemic prevention for many years.
11753000	11758000	And I think it was just clearly the case that pandemics are
11758000	11761000	a major threat to people alive today.
11761000	11764000	And I like to feel that we've been proven right.
11764000	11766000	No one's gonna argue at that point.
11766000	11769000	And, you know, everyone's being like, Effective Artists,
11769000	11771000	why are you working on AI safety?
11771000	11773000	This obviously doesn't matter.
11773000	11776000	You know, I feel like we've got one thing right.
11776000	11779000	Can I be really skeptical, though, for a second?
11779000	11782000	Because, I mean, you're working for DeepMind.
11782000	11786000	There's so much prestige and money attached to AI risk.
11786000	11789000	Elon Musk is talking about it all the time,
11789000	11794000	whereas you could be a scientist working on pandemic responses.
11794000	11796000	And, I mean, let's be honest,
11796000	11799000	it wouldn't be anywhere near the same level of prestige.
11799000	11800000	Yeah.
11800000	11806000	So, couple of takes.
11806000	11811000	It definitely is the case that I,
11811000	11814000	a good chunk of why I personally am working on AI X-Risk
11814000	11819000	rather than say BioX-Risk is that I'm a smart mathematician.
11819000	11820000	I like AI.
11820000	11822000	I like mech and tub.
11822000	11825000	I do not think I would be good at biology in the same way.
11825000	11833000	And I also personally assert that AI X-Risk is more important
11833000	11838000	and, like, more pressing, but, you know, I'm biased.
11838000	11841000	And I think it's fair to flag that bias.
11841000	11844000	In terms of prestige,
11844000	11847000	so I've only really been working on this stuff properly
11847000	11849000	for the past two and a half years,
11849000	11852000	which is, I mean, it's changed dramatically.
11852000	11854000	Like, in the last six months we've gone from,
11854000	11857000	well, we're really ever going to get AGI to,
11857000	11859000	oh my God, GP4 exists.
11859000	11862000	Jeffrey Hinton has left Google to loudly advocate for X-Risk.
11862000	11866000	Joshua Benjo is now loudly advocating for X-Risk.
11866000	11869000	It's two-thirds of the Turing winners for deep learning.
11869000	11871000	You'll never get the third one.
11871000	11873000	Yeah, we're never going to get the third one.
11873000	11877000	Jan Likert has made his position very, very clear.
11877000	11880000	But, you know, as a majority, I'll take it.
11880000	11881000	Yes.
11881000	11883000	Or the fourth one.
11883000	11884000	Yeah.
11884000	11886000	He's coming on our podcast, actually.
11886000	11888000	Oh, who was the fourth one?
11888000	11889000	Schmidt-Huber.
11889000	11891000	Yeah, it seems hard.
11891000	11893000	I'm very curious to hear the Schmidt-Huber episode.
11893000	11896000	Oh, yeah, he's even more virulently against than Jan.
11896000	11898000	I'm afraid to say.
11898000	11899000	Two out of two.
11899000	11900000	Two out of four.
11900000	11902000	I'm interested to hear it anyway.
11902000	11906000	So, yeah.
11906000	11910000	And, yeah, in terms of prestige, I don't know.
11910000	11916000	I gather that, say, seven years ago, it was basically just not...
11916000	11918000	It would be, like, pretty bad for your career.
11918000	11922000	You would not be taken seriously if you mentioned caring about AIX-Risk.
11922000	11924000	Your papers would be rejected.
11924000	11929000	I hear a story of Stuart Russell at one point talk to a grad student of his
11929000	11932000	about how Stuart was concerned about AIX-Risk.
11932000	11935000	The grad student was also really concerned and freaking out,
11935000	11937000	but they'd been working together for years
11937000	11939000	and neither felt comfortable mentioning it.
11939000	11944000	And a lot of people who are still in the field were doing the stuff then,
11944000	11947000	which makes me somewhat reject the prestige argument,
11947000	11951000	at least for senior people in the field.
11951000	11954000	I think there's a difference with Stuart Russell in particular.
11954000	11956000	He's very credible.
11956000	11957000	And he...
11957000	11958000	No, I'm not.
11958000	11960000	Oh, I didn't mean... I didn't mean you.
11960000	11963000	I was talking about the two Godfathers,
11963000	11965000	because the thing that...
11965000	11966000	Maybe I shouldn't say this,
11966000	11971000	but I was surprised that Benjio and Hinton came out in the way they did.
11971000	11973000	And I...
11973000	11975000	The reason I didn't like what they said was,
11975000	11979000	I felt that they were implying that current AI technology
11979000	11981000	could pose an existential threat.
11981000	11985000	And what I'm getting from you and what I'm getting from Russell is...
11985000	11987000	I'm also from Robert Miles,
11987000	11992000	is that this is a very real potential threat in the future,
11992000	11994000	but it's not a current threat.
11994000	11998000	Yes, very real potential threat in the future,
11998000	12003000	though I hesitate to confidently assert, say,
12003000	12005000	this will not be a threat in the next five years or something.
12005000	12007000	It's like pretty hard to say.
12007000	12008000	Interesting.
12008000	12011000	I'm not confident.
12011000	12014000	I agree with your assessment of Benjio and Hinton,
12014000	12016000	though they've spoken a bunch publicly,
12016000	12018000	so I'll defer if you can point to specific writings.
12018000	12023000	But for example, Benjio signed the pause AI for six months,
12023000	12025000	more powerful than GPT-4 letter,
12025000	12027000	and I don't know.
12027000	12029000	I don't think the letter was asserting that...
12029000	12032000	The letter definitely wasn't asserting GPT-4 was an extra risk.
12032000	12035000	It wasn't confidently asserting GPT-5 would be,
12035000	12036000	but it's being like,
12036000	12039000	yeah, we need more time and slow down and caution.
12039000	12041000	Maybe I'm reading too much into that,
12041000	12044000	but it seemed to me that, I mean, Hinton said
12044000	12049000	that chat GPT now contains all of the world's knowledge,
12049000	12051000	and this chatbot knows everything,
12051000	12054000	and it could potentially do very harmful things,
12054000	12058000	and I interpreted it possibly incorrectly
12058000	12061000	that they were talking about reasonably current
12061000	12064000	or next-generation risks.
12064000	12066000	I mean, I can't talk for them.
12066000	12070000	I also, I don't know, there are lots of near-term risks.
12070000	12071000	There's long-term risks.
12071000	12074000	I consider it my job to think hard about the long-term risks
12074000	12076000	and try to guard against those,
12076000	12078000	and I think lots of other people's jobs
12078000	12080000	is to focus on the near-term risks,
12080000	12083000	and both are great forms of work.
12083000	12084000	I don't know.
12084000	12085000	One reason I like interpretability
12085000	12088000	is I think it is just broadly useful across all of them.
12088000	12091000	So what I consider to be my job might just not even matter.
12091000	12094000	But, yeah.
12094000	12097000	Yeah, no, I probably will not...
12097000	12099000	Do not want to get deeply into interpreting
12099000	12101000	what other people have said.
12101000	12103000	I...
12106000	12109000	Well, could I ping you a couple of quick questions?
12109000	12111000	So, first of all, you know,
12111000	12113000	there's this idea of negative utilitarianism.
12113000	12115000	I mean, do you think minimising suffering
12115000	12118000	is more important than maximising happiness?
12118000	12120000	No.
12120000	12121000	No?
12121000	12123000	Not sure I've got a more deep answer than that.
12123000	12125000	I mostly think a lot of this intrusive reasoning
12125000	12127000	is more often by intuition than anything else.
12127000	12129000	But it's a bit like this metrics thing we were talking about,
12129000	12131000	you know, which is that...
12131000	12133000	If you want to have...
12133000	12135000	Would you like to tolerate some spiky necks
12135000	12137000	for some average happiness?
12137000	12139000	Yeah.
12139000	12141000	So, I know.
12141000	12144000	I have, like, a general frustration
12144000	12148000	with these discussions getting too philosophical.
12148000	12150000	This is a big issue
12150000	12152000	when I hang out with effective altruists
12152000	12155000	who really love moral philosophy and population ethics.
12155000	12156000	Yes.
12156000	12157000	I don't know.
12157000	12159000	I have this EA forum post called
12159000	12162000	Simplify EA Pictures to Holy Shit X-Risk.
12162000	12164000	And it's like...
12164000	12166000	So, I don't know.
12166000	12169000	If you actually look at some of the concrete work
12169000	12173000	people try doing on things like timelines and risk,
12173000	12177000	there's this report from a Jay Akatra at Open Philanthropy
12177000	12180000	that gives 30-year median timelines
12180000	12183000	to AI that's transformative,
12183000	12186000	which he's since updated to 20 years.
12186000	12188000	There's a report by Joseph Karlsmith
12188000	12192000	that estimates about a 10-ish percent chance
12192000	12194000	of a major catastrophe from this.
12194000	12195000	Yeah.
12195000	12197000	And if you just take those numbers,
12197000	12200000	this is clearly enough to reach pretty high
12200000	12203000	on my list of concerns of people alive today.
12203000	12205000	Okay, okay.
12205000	12208000	And I think these are bold empirical claims.
12208000	12212000	And I think it's great to debate them in the empirical domain.
12212000	12217000	But to me, this doesn't feel like a moral question.
12217000	12220000	It just feels like from common sense assumptions,
12220000	12223000	if you believe these empirical claims,
12223000	12225000	this stuff is a really big deal.
12225000	12227000	Okay, okay.
12227000	12229000	Let's take another couple of steps.
12229000	12232000	So, first of all, we save this till later.
12232000	12234000	I think deception is very important.
12234000	12236000	And Daniel Dennett, when I spoke with him,
12236000	12238000	he uses this notion called the intentional stance,
12238000	12243000	which basically means that if you use a projection
12243000	12247000	of purposes, goals, agency, et cetera,
12247000	12250000	in order to understand the behavior of an agent,
12250000	12253000	possibly a simulated agent,
12253000	12257000	then for all intents and purposes, it has agency.
12257000	12259000	It can make decisions.
12259000	12260000	It has moral status,
12260000	12262000	it has lots of different things like that.
12262000	12266000	And he would say that without an intentional stance,
12266000	12268000	without agency,
12268000	12271000	it's impossible for a model to lie or deceive us.
12271000	12274000	Now, what do you think would be the bar
12274000	12277000	for something like a GPT model to deceive us and why?
12277000	12279000	Yeah.
12279000	12283000	So, before I give takes,
12283000	12287000	I will generally reinforce Rob's vibe of,
12287000	12289000	well, if you have no idea how fire works,
12289000	12290000	but you know that it burns you.
12290000	12292000	That's kind of the important thing.
12292000	12297000	Like maybe a model has just this random learned adaptation
12297000	12300000	to output things that are designed to get a user
12300000	12302000	to feel and believe a certain way,
12302000	12305000	that isn't intentional and isn't deceptive
12305000	12307000	in some true COGSI sense,
12307000	12310000	but it's like enough for this to be a big deal
12310000	12311000	that we should care a lot about.
12311000	12312000	Okay.
12312000	12313000	With that aside.
12313000	12315000	Yeah.
12315000	12321000	So, I'm definitely hesitant to ascribe
12321000	12325000	an overly confident view of what's going on here.
12325000	12328000	And I think lots of early discourse alignment
12328000	12332000	around things like utility maximization
12332000	12337000	and around things like these things are just
12337000	12340000	paperclip maximizers, et cetera,
12340000	12343000	is kind of misleading.
12343000	12345000	And I don't think it is an accurate model
12345000	12351000	of how GPT-7 RLHF++ is going to work.
12351000	12353000	Well, that's my prediction.
12353000	12356000	One thing that is pretty striking to me is
12356000	12358000	I just feel like we're pretty confused
12358000	12360000	on both sides of this.
12360000	12363000	Like, I do not feel like I can confidently claim
12363000	12365000	that these models will demonstrate
12365000	12368000	anything remotely like goals or intentions,
12368000	12370000	but I also don't feel like you can confidently claim
12370000	12371000	that they won't.
12371000	12376000	And I'm not talking like 99.99% confidence.
12376000	12380000	I'm talking like 95% plus confidence either way.
12380000	12383000	And one of my visions for what being good
12383000	12385000	at Mechantep might look like is being able
12385000	12388000	to actually get grounding for these questions.
12388000	12391000	Because I think ultimately these are mechanistic questions.
12391000	12394000	Behavioral interventions are not enough to answer
12394000	12399000	like, does this thing have a goal in any meaningful sense?
12399000	12404000	But yeah, my very rough, soft definition would be
12404000	12407000	is the model capable of forming
12407000	12411000	and executing long-term plans towards some goal,
12411000	12415000	potentially if explicitly prompted to auto-GPT
12415000	12417000	or just spontaneously,
12417000	12421000	is it capable of actually carrying out these plans?
12421000	12424000	And does it form and execute plans
12424000	12427000	towards some objective
12427000	12431000	that is like encoded in the model somewhere?
12431000	12433000	And I don't know.
12433000	12436000	I think it's pretty plausible that the first dangerous thing
12436000	12439000	is like Chaos GPT-7,
12439000	12442000	where someone tells it to do something dangerous
12442000	12445000	and it gets misused more so than it's like misaligned.
12445000	12448000	And I care deeply about both of these risks.
12448000	12449000	Okay.
12449000	12451000	So yeah, first one's more of a governance question
12451000	12453000	than a technical question.
12453000	12455000	And thus is less where I feel like I can add value.
12455000	12457000	So I agree with you on all of that.
12457000	12460000	So yeah, being less confused about what's going on inside the models
12460000	12463000	and using interpretability to figure out
12463000	12466000	whether they actually do have agency or goals
12466000	12469000	and sometimes they do the right things for the wrong reasons.
12469000	12472000	Auditing models that seem aligned before they're deployed
12472000	12475000	is something that you've told me before.
12475000	12476000	So great.
12476000	12480000	And just being able to check more deeply that it truly is aligned.
12480000	12484000	But I wanted to talk a little bit about
12484000	12487000	this interesting paper from Katya Grice.
12487000	12491000	So she wrote a response called, it was on the less wrong,
12491000	12493000	debunking the AI apocalypse,
12493000	12495000	a comprehensive analysis of counterarguments
12495000	12498000	to the basic AI risk case, X-Risk.
12498000	12501000	And the reason I read it is so many of the comments
12501000	12505000	were destroying me and Doug after we interviewed Rob.
12505000	12508000	And they said, well, if you're going to criticise S-Risk,
12508000	12510000	I mean, at least go and read Katya Grice's response.
12510000	12511000	So I did.
12511000	12512000	So I did.
12512000	12513000	Here we go.
12513000	12518000	So she basically made two big counterarguments
12518000	12521000	that intelligence might not actually be a huge advantage
12521000	12524000	and about the speed of growth is ambiguous.
12524000	12526000	But I first want to touch on what you said before,
12526000	12529000	which is about this notion of goal-directedness.
12529000	12533000	So alignment people say that if superhuman AI systems are built,
12533000	12537000	any given system is likely to be goal-directed
12537000	12540000	and the orthogonality thesis and instrumental goals
12540000	12543000	are cited as aggravating factors.
12543000	12547000	And the goal-directed behaviour is likely to be valuable,
12547000	12548000	so economically.
12548000	12550000	Goal-directed entities may tend to arise
12550000	12552000	from machine-ledding training processes,
12552000	12554000	not intending to create them,
12554000	12557000	which is kind of talking about some of the emergent behaviours
12557000	12560000	that we were talking about earlier with respect to Othello,
12560000	12561000	for example.
12561000	12565000	And coherence arguments may imply that systems with goal-directedness
12565000	12568000	will become more strongly goal-directed over time,
12568000	12570000	which is apparently something that is argued for.
12570000	12572000	So I'm thinking, what does goal even mean?
12572000	12575000	I mean, we anthropomorphise abstract human intelligible concepts
12575000	12580000	like goals and they really are emergent
12580000	12583000	because they emerge from these low-level interactions
12583000	12585000	in the cells in your body
12585000	12588000	and then you get these things that we recognise to be goals,
12588000	12590000	observer-relative, as we were talking about before.
12590000	12594000	But they're just graduated phenomena from smaller things, right?
12594000	12597000	So what does it even mean to have a goal?
12598000	12603000	Yeah, so a couple of thoughts on that.
12603000	12606000	Again, you ask questions with a lot of content in them.
12606000	12610000	No problem. I can only apologise.
12610000	12612000	I mean, someone who accidentally writes
12612000	12616000	19,000 word blog posts all the time, I relate.
12616000	12622000	Anyway, so what am I saying?
12622000	12624000	So the way...
12624000	12626000	Yeah, it's a fake concept, right?
12626000	12630000	Yeah, so I definitely want to try to take...
12630000	12636000	So there's the mechanistic definition of the model forms plans
12636000	12640000	and it evaluates the plans according to some criteria or objective
12640000	12643000	and it executes the plans that score better on this.
12643000	12649000	And I would love if we get to a point where we can look inside a model
12649000	12652000	and look the circuitry that could be behind this or not.
12653000	12656000	That would feel like a big milestone for me on
12656000	12659000	wow, I really believe mech and tuple matter
12659000	12662000	for reducing catastrophic risk from AI.
12664000	12668000	A second thing is that...
12670000	12673000	Yeah, the kind of more behavioural thing of
12673000	12675000	the model systematically takes actions
12675000	12678000	that pushes the world towards a certain state.
12678000	12682000	And I don't want...
12682000	12686000	I think there's a common problem in alignment arguments
12686000	12690000	where people get too precise and too specific
12690000	12693000	in a way that lots of people reasonably object to
12693000	12696000	in a way which is not necessary for the argument.
12696000	12699000	There's a really great paper called
12699000	12701000	The Alignment Problem from a Deep Learning Perspective
12701000	12705000	by Richard Ngo, Lawrence Tan, and Sorin Mindenman.
12705000	12708000	And this is probably my biggest recommendation
12708000	12711000	for the listening audience of what I think is like
12711000	12714000	a pretty well-presented case for alignment.
12714000	12717000	And I generally pretty pro-try to make the minimal
12717000	12719000	necessary assumptions.
12719000	12722000	So for me it's kind of like some soft form of
12722000	12725000	goal-directedness of take actions that push the world
12725000	12727000	towards a certain state.
12727000	12732000	And another important thing is
12732000	12734000	there are a bunch of theoretical arguments
12734000	12738000	for why goals would spontaneously emerge.
12738000	12742000	Ideas around in a misalignment from work
12742000	12746000	led by Evan Huminger, ideas around
12746000	12750000	just coherent theorems and things like that,
12750000	12752000	which I know I find a bit convincing,
12752000	12755000	not that convincing, but then there's
12755000	12758000	things will have goals because we try to give them goals.
12758000	12762000	And I'm like, yeah, that's probably gonna happen.
12762000	12764000	It's just clearly useful.
12764000	12768000	If you want to have an AI CEO
12768000	12772000	or AI helping run logistics and military operations
12772000	12774000	to have something that's capable of
12774000	12776000	forming and executing long-term plans
12776000	12778000	towards some objective.
12778000	12781000	And if you believe this is what's gonna happen,
12781000	12783000	then the key question is,
12783000	12786000	are we capable of ensuring those goals
12786000	12789000	are exactly the goals we would like them to be?
12789000	12792000	And my answer for any question of the form,
12792000	12795000	can we precisely make sure the system is doing exactly X
12795000	12798000	and machine learning is, God, no.
12798000	12801000	We are not remotely good enough to achieve this
12801000	12805000	with our current level of alignment and steering techniques.
12805000	12809000	And to me, this is like a more interesting point
12809000	12811000	where it's not quite a crux for me,
12811000	12816000	but it just seems like a lot easier to argue about
12816000	12818000	what people do this.
12819000	12822000	Yeah, essentially, I mean, Katya herself said that
12822000	12826000	it's unclear that goal-directedness
12826000	12829000	is favored by economic pressure to training dynamics
12829000	12831000	or coherence arguments, you know,
12831000	12835000	whether those are the same thing as kind of goal-directedness
12835000	12838000	that implies a zealous drive to control the universe.
12838000	12841000	And look at South Korea, they have goals.
12841000	12845000	And those goals, I don't really subscribe
12845000	12848000	to the dictator view of society.
12848000	12851000	I assume they are somehow emergent.
12851000	12853000	And similarly...
12853000	12855000	Sorry, South Korea or North Korea?
12855000	12857000	Sorry, North Korea, did I say South Korea?
12857000	12859000	Very different careers.
12859000	12862000	Different goals, different goals.
12862000	12865000	But you can think about goals in an AI system
12865000	12869000	as either being ones which emerge from some low level
12869000	12872000	or ones which are explicitly coded by us
12872000	12875000	or ones which are instrumental.
12875000	12878000	And these are all a whole bunch of goals.
12878000	12881000	But we can't really control those.
12881000	12884000	We can add pressures.
12884000	12887000	How do we control what North Korea does?
12887000	12891000	That sure is a question I'd love for someone to answer.
12891000	12893000	I don't know.
12893000	12896000	I can give speculation.
12896000	12899000	There's the question within practice,
12899000	12901000	what do people do?
12901000	12904000	Which is basically reinforcement learning from human feedback.
12904000	12907000	And I expect people would apply that in this situation as well.
12907000	12910000	I definitely do not believe we would be able
12910000	12913000	to explicitly encode a goal in the system.
12913000	12917000	Moreover, even if you can encode,
12917000	12920000	even if you could give some scoring function,
12920000	12923000	like make the score in this game high,
12923000	12927000	this does not give you a model that intrinsically cares about that
12927000	12930000	in the same way that, I don't know,
12930000	12933000	evolution optimizes inclusive genetic fitness.
12933000	12936000	I don't give a fuck about inclusive genetic fitness.
12936000	12938000	Even though I care about a bunch of things,
12938000	12941000	evolution got me to care about within that,
12941000	12945000	like tasty foods and surviving.
12945000	12951000	Yeah, so we don't know how to put goals into systems.
12951000	12955000	I basically just assert that we are not currently capable
12955000	12958000	of putting goals into systems well.
12958000	12962000	And this is one of the main things
12962000	12964000	the field of alignment thinks about.
12964000	12966000	And we're not very good at it.
12966000	12968000	It'd be great if we were better at it.
12968000	12973000	In terms of, yeah, I definitely don't want to make strong claims
12973000	12976000	about, to be dangerous, the goals need to be coherent
12976000	12980000	or the goals need to, there needs to be like a singular goal.
12980000	12983000	Like I don't have a singular goal.
12983000	12988000	It's not obvious to me how these systems will turn out.
12988000	12991000	If they don't, in any meaningful sense,
12991000	12995000	want a coherent thing, then I'm a fair bit less concerned.
12995000	12999000	Though, well, I mean, there's many, many ways
12999000	13001000	that human level AI would be good for the world
13001000	13004000	or bad for the world, or just wildly destabilizing
13004000	13007000	and high variance, of which misalignment risk is one of them.
13007000	13009000	And lots of the other ones just don't apply,
13009000	13012000	like misuse and systemic risks.
13012000	13016000	But leaving those aside, yeah,
13016000	13019000	I think if a model is just roughly pushing
13019000	13022000	in a goal-directed direction with a bunch of caveats
13022000	13024000	and uncertainties and flip-flopping,
13024000	13028000	that still seems like a pretty big deal to me.
13028000	13030000	Okay, okay.
13030000	13034000	Katia, let's just cover her two main arguments.
13034000	13037000	So she said that intelligence might not actually be
13037000	13039000	a huge advantage.
13039000	13043000	Looking at the world, intuitively,
13043000	13047000	big discrepancies in power are not to do with intelligence.
13047000	13052000	And she said IQ humans with an IQ of 130
13052000	13055000	earn roughly $6,000 to $18,000 a year,
13055000	13057000	more than average IQ humans.
13057000	13059000	Elected representatives are apparently
13059000	13062000	slightly smarter on average,
13062000	13064000	but not a radical difference.
13064000	13068000	Mensa isn't a major force in the world.
13068000	13071000	And if we look at people who evidently
13071000	13074000	have good cognitive abilities given their intellectual output,
13074000	13076000	their personal lives are not obviously drastically
13076000	13078000	more successful anecdotally.
13078000	13082000	So is it that much of a big deal?
13082000	13084000	Yeah.
13084000	13087000	So I think this is like a fair point.
13087000	13091000	If we looked in the world and IQ,
13091000	13095000	or whatever metric of intelligence you want to use,
13095000	13097000	it's really dramatically correlated
13097000	13099000	with everything good about someone.
13099000	13102000	I mean, IQ correlates with basically everything
13102000	13104000	you might value in someone's life,
13104000	13106000	because we live in an unfair world,
13106000	13110000	but not dramatically.
13110000	13114000	Yeah, so I think this is a valid argument.
13114000	13119000	I generally don't think you should model
13119000	13123000	human-level AI as like,
13123000	13127000	or like slightly superhuman AI as like an IQ 200 human.
13127000	13129000	Like, for example,
13129000	13131000	GPT-4, I would argue,
13131000	13135000	knows most facts on the internet,
13135000	13138000	or many facts.
13138000	13142000	And, yeah, knows many facts.
13142000	13146000	And this seems...
13146000	13151000	GPT-4 knows many facts.
13151000	13156000	And this is sure an advantage over me.
13156000	13158000	GPT-4 knows how to write a lot of code,
13158000	13161000	and it knows how to take software
13161000	13163000	and do penetration testing on it.
13163000	13166000	It knows lots of social conventions
13166000	13168000	and cultural things,
13168000	13172000	and has lots of experience reading various kinds of
13172000	13175000	text written to be manipulative,
13175000	13177000	or manuals on how to make nuclear weapons.
13177000	13180000	Sorry, I'm going too hard on the knowledge point.
13180000	13182000	There's just lots of different axes.
13182000	13185000	You can be human-level or better,
13185000	13187000	in which knowledge is one.
13187000	13189000	Intelligence or reasoning is one.
13189000	13192000	Social manipulation abilities is another.
13192000	13194000	Charisma and persuasion is another.
13194000	13198000	I think these two are particularly important ones.
13198000	13203000	There's forming coherent plans.
13203000	13207000	There's just like the ability to execute on stuff.
13207000	13211000	24x7 running thousands of copies of yourself in parallel,
13211000	13214000	distributed across the world.
13214000	13217000	There's running faster than humans.
13217000	13219000	And there's just like lots of dimensions here.
13219000	13224000	I think the IQ 200 human frame is helpful in some ways,
13224000	13227000	but unhelpful in other ways,
13227000	13231000	especially if it summons the nerdy scientist
13231000	13236000	with no social skills who's life is a mess archetype.
13236000	13238000	I'd say it's a nerdy scientist with no social skills
13238000	13242000	whose life is a mess.
13242000	13243000	Okay, yeah.
13243000	13245000	I mean, this is the thing is...
13245000	13246000	Because Rob said the same thing.
13246000	13248000	On chess, it's possible for someone to be
13248000	13250000	literally 20 times better than you,
13250000	13252000	that there's a huge dynamic range of skill.
13252000	13254000	And that's something we've not really seen
13254000	13255000	in human intelligence,
13255000	13258000	and it might be because of the way we measure it.
13258000	13260000	It's possible that the way we measure it
13260000	13265000	doesn't even capture people with broader
13265000	13267000	or better abilities.
13267000	13269000	Let's just cover her last point quickly.
13269000	13274000	So this is that the speed of intelligence growth is ambiguous.
13274000	13278000	So this idea that AI would be able to rapidly destroy the world
13278000	13281000	seems prima facie unlikely to Katia,
13281000	13284000	since no other entity has ever done that.
13284000	13286000	And she goes on.
13286000	13290000	So the two common broad arguments is that there'll be a feedback loop
13290000	13294000	in which intelligent AI makes more intelligent AI repeatedly
13294000	13296000	until AI is very, very intelligent.
13296000	13300000	Number two, small differences in brains seem to correspond
13300000	13302000	to very large differences in performance
13302000	13304000	based on observing humans and other apes.
13304000	13307000	Thus, any movement past human level
13307000	13310000	will take us to unimaginably super human level.
13310000	13313000	And the basic counter-arguments to that is that
13313000	13316000	the feedback loops might not be as powerful as assumed.
13316000	13318000	There could be diminished returns,
13318000	13320000	there could be resource constraints,
13320000	13322000	and there could be complexity barriers.
13322000	13326000	So maybe we should just do that kind of recursive self-improving piece first.
13326000	13327000	What do you think about that?
13327000	13329000	I don't really buy recursive self-improvement.
13329000	13330000	Oh, good.
13330000	13333000	It's not an important part of why I'm concerned about this stuff.
13333000	13340000	So generally, I just feel like a lot of the arguments were made
13340000	13344000	before the current paradigm of enormous foundation models.
13344000	13349000	When you're investing hundreds of millions of dollars of compute into a thing,
13349000	13353000	it's pretty hard for it to make itself substantially better.
13355000	13359000	And you can do things like design better algorithmic techniques.
13359000	13364000	I think that is probably one that is more likely to be accelerated
13364000	13366000	the better the model gets.
13366000	13372000	It's not clear to me how much juice there's to squeeze out of that.
13373000	13380000	But generally, I just think a lot of this is going to be bottlenecked
13380000	13383000	by hardware and compute and data,
13383000	13389000	such that I'm less concerned about some runaway intelligence explosion,
13389000	13391000	and I'm more just concerned about,
13391000	13393000	we'll eventually make things that are dangerous.
13393000	13395000	What do we do then?
13396000	13400000	And I think this is a really good fact about the world.
13400000	13405000	I think a world where you can have intelligence explosions is really scary.
13405000	13410000	And I feel like our current world is a lot less scary than it could have been.
13410000	13415000	If some kid in a basement somewhere just wrote the code for AGI one day.
13415000	13417000	Yes, yes.
13418000	13421000	Okay, well, I mean, just to finish off Katya's final point.
13421000	13425000	So the other point they made was about small differences might lead to oval.
13425000	13427000	It's a little bit like in squash.
13427000	13429000	I don't know if you've ever played squash,
13429000	13435000	but a tiny difference in ability leads to one player overwhelmingly dominating the other player,
13435000	13439000	because you just get these kind of like, you know, it's a game of attrition,
13439000	13441000	and you get these tipping points.
13441000	13446000	And she argued that that might not necessarily be the case when comparing AI systems,
13446000	13448000	because of three reasons.
13448000	13452000	Different architectures, likely to have very different underlying architectures
13452000	13455000	and biological brains, which could lead to different scaling properties.
13455000	13461000	Performance plateaus, so there might be these plateaus beyond which further increases in intelligence,
13461000	13464000	you know, don't lead to significant performance improvements.
13464000	13468000	And also this notion of task specific intelligence, something that I strong,
13468000	13471000	I believe that all intelligence is specialized as we were speaking about earlier.
13471000	13475000	And so it might be specialized rather than being generally intelligent,
13475000	13482000	and small differences thus may not translate into large differences in performance across a wide variety of tasks.
13482000	13486000	Maybe we should just touch on this, on this kind of task focus thing.
13486000	13488000	So I think humans are very specialized.
13488000	13493000	We have, and we don't realize that we are because the way we conceive of intelligence is anthropomorphic,
13493000	13496000	but actually we don't do four dimensions very well.
13496000	13498000	There's lots of things that we don't do very well,
13498000	13502000	and we're kind of embedded in the cognitive ecology in quite a complex way.
13502000	13504000	So what do you think about that?
13504000	13511000	Yeah, so I will, okay, I'll first comment on the general metodynamic of,
13511000	13517000	I think that people get way too caught up on philosophizing.
13517000	13519000	And no offense.
13519000	13520000	I'm so sorry.
13520000	13528000	And in particular, I care about whether an AI will cause a catastrophic risk.
13528000	13534000	I don't care about whether it fits into, whether it's general in the right way,
13534000	13537000	whether it has weaknesses in certain areas,
13537000	13540000	whether it's high on the Chomsky hierarchy,
13540000	13547000	or whether it's generally intelligent in some specific sense that someone like Gary Marcus would agree with.
13547000	13554000	Is that in any way a contradiction of your mechanistic sensibilities?
13554000	13559000	Because when it comes to neural networks, you want to understand how they work,
13559000	13562000	but when it comes to intelligence, you don't.
13562000	13563000	Oh, sorry.
13563000	13566000	I want to understand how it works.
13566000	13568000	I want to understand everything.
13568000	13571000	I just don't think it's...
13571000	13580000	I want to disentangle things to be concerned about from theoretical arguments about whether this fits into certain categories.
13580000	13584000	For the purposes of deciding whether to be concerned about AI existential risk,
13584000	13591000	I see all of the theory arguments as like a means to an end of this ultimate empirical question of,
13591000	13594000	is this a thing that could realistically happen?
13594000	13602000	And I think that these theoretical frameworks do matter.
13603000	13605000	Like, I don't know.
13605000	13610000	I think that an image classification model is basically never going to get the point where it's dangerous,
13610000	13621000	while a language model that's being RLHF'd to have some notion of intentionality potentially will.
13621000	13625000	And, yeah.
13625000	13630000	I know I can give random takes, but to me, if you're like,
13630000	13633000	AI's can be task-specific in the same way that humans are task-specific.
13633000	13642000	I'm like, well, a human is task-general enough that I think they could be massively dangerous in the right situation with the right advantages.
13642000	13649000	Like, if they wanted to be and were able to run the thousands copies of themselves at a thousand X speed or something.
13649000	13653000	I don't know if that's actually a remotely accurate statement about models.
13653000	13657000	Probably they can run many copies, but not a thousand X speed or something.
13657000	13660000	But, yeah.
13660000	13670000	Generally, that's the kind of question I care about, and I'm concerned many of these definitions lose sight of that.
13670000	13678000	And part of my thing of like, I want to keep alignment arguments as having as few assumptions as possible.
13678000	13682000	Because the more assumptions you make, the less plausible your case is.
13682000	13688000	And the less and like, more room there is for people to like, rightfully disagree.
13688000	13697000	I'm like, I want to be careful not to make any of the case rest on like, strong theoretical frameworks because we don't know what we're doing here.
13697000	13700000	Enough to have legit theoretical frameworks.
13700000	13707000	And I think that AI is likely to be limited in the same way that humans are, at least within the GPT paradigm.
13707000	13718000	If you're training it to predict the next word on the internet and a bunch of other stuff, then it's going to learn a lot from human patterns and human thought and human conventions.
13718000	13722000	But, I don't know.
13722000	13730000	In closing, you said that your personal favorite heuristic is the second species argument.
13730000	13732000	Can you tell us?
13732000	13749000	Yeah. So, I quite like Hinton's recent pithy quote of, there is no example of something being of some entity being controlled by things less smart than it.
13749000	13751000	And that was terrible.
13751000	13752000	Sorry.
13752000	13756000	I really would. I mean, Twitter went wild over that.
13756000	13757000	Oh, they're trying to go wild.
13757000	13760000	I mean, look at a company.
13760000	13763000	The CEO is usually done with that.
13763000	13768000	You have to hire competent people to have a successful company or look at my cat.
13768000	13769000	Yeah. Okay.
13769000	13770000	This is a terrible thing.
13770000	13773000	Let's just start again.
13773000	13774000	All right.
13774000	13778000	So, yeah, this is often called the gorilla problem.
13778000	13783000	Humans are just smarter than gorillas in basically all ways that matter.
13783000	13794000	Humans are not actively malevolent to gorillas, but ultimately humans are in charge gorillas are not and gorillas exist because of our continued benevolence or ambivalence.
13794000	13809000	And it just seems to me like if you are creating entities that are smarter than you, the default outcome is they end up in control of what's going on in the world and you do not.
13809000	13813000	And I kind of just feel like this should be the null hypothesis.
13813000	13819000	And then there's a bunch of arguments on top of like, is this a good model?
13819000	13823000	Well, obviously, there's lots of disanalogy is because we're making them.
13823000	13825000	We ideally have some control over them.
13825000	13828000	We're going to try to shape them to be benevolent towards us.
13828000	13833000	But this just seems like the default thing to be concerned about to me.
13833000	13837000	On that point, though, we are different from computers.
13837000	13838000	We scuba dive.
13838000	13840000	And that's actually quite a profound thing to say.
13840000	13848000	We scuba dive because we are integrated into the ecosystem, not just physically, but cognitively.
13848000	13851000	There's a kind of cognitive ecosystem that we're enmeshed in.
13851000	13854000	We have a huge advantage over computers.
13854000	13858000	Computers can't really do anything in the physical world.
13858000	13862000	So I agree with this, but I don't know.
13862000	13867000	I feel like the way, I don't know.
13867000	13878000	One evocative example is there was this crime lord, El Shapo, who ran his gang from within prison for like many years, very successfully.
13878000	13885000	When you have humans in the world who can get to do things for you, you don't need to be physically embodied to get shit done.
13885000	13887000	And I don't just look at Blake Lemoine.
13887000	13896000	There's no shortage of people who will do things if convinced in the right way, even if they know it's an AI.
13896000	13898000	And I do agree with you on that.
13898000	13910000	And I think part of the reason why we're going to have the inevitable proliferation of this technology is so many tinkerers will just create many, many different versions of AI.
13910000	13914000	And they won't really be thinking about the consequences of their actions.
13914000	13918000	But what's the alternative, paternalism?
13919000	13929000	Yeah, so to me, the main interesting thing here is large training runs as like the major bottleneck.
13929000	13931000	Very few actors can do them.
13931000	13938000	We're probably going to get beyond the point where people are even putting the things out behind an API open to many people to use,
13938000	13944000	let alone like open sourcing the weights, which we've already pretty clearly moved past.
13944000	13953000	And this, to me, seems like the point of intervention you need if you're going to try to make sure things are safe before you deploy them,
13953000	13963000	like track the people who are able to do these runs, have standards for what it means to decide a system like this is safe.
13963000	13968000	I'm pretty happy Sam Altman's been pushing that stuff very heavily.
13968000	13973000	And if competently done, I think this kind of regulation can be very important.
13973000	13975000	It could be great.
13975000	13978000	Like the alignment research has been doing great work here.
13978000	13985000	And I'm very excited to see what the red teaming large language models thing at Defconn looks like.
13985000	13991000	But I don't know, maybe to close, I feel like I've been in the role of why alignment matters.
13991000	13996000	Maybe I can try to break alignment arguments myself for a bit.
13996000	13998000	Please do, yeah.
13998000	14004000	So if I condition on actually the world is kind of fine,
14004000	14014000	probably my biggest guess is that the goal directed notion is just like not remotely a good understanding of how these things work.
14014000	14017000	And it's hard to get them to be goal directed.
14017000	14020000	And we just mostly coordinate and don't do that.
14020000	14025000	And these systems are mostly just like extremely effective tools.
14025000	14028000	It seems like kind of a plausible world we could end up in.
14028000	14034000	I don't think it's any more likely than, yep, they're goal directed and this is terrible.
14034000	14042000	We end up in a world which just has like lots of these systems that don't coordinate with each other,
14042000	14048000	want some more different things are like broadly aligned with human interests,
14048000	14053000	but like imperfectly, and just none of them ever get a major advantage over the others.
14053000	14060000	And the world kind of continues to be about as the world is with lots of different actors who aren't necessarily aligned with each other,
14060000	14066000	but mostly don't try to see over the world except every so often.
14066000	14070000	Or we just alignment isn't that hard.
14070000	14072000	We crack mechanistic interoperability.
14072000	14074000	We look inside the system.
14074000	14078000	We use this to iterate on making our techniques really good.
14078000	14084000	It turns out that doing RLHF with like enough adversarial training just kind of works.
14084000	14088000	Or with AI assistance to help you notice what's going on in the system.
14088000	14094000	And this just gets us aligned to the level systems and we can be like, please go solve the problem.
14094000	14096000	And then they do.
14096000	14103000	And I think people like Yadkowski are very loud about we are almost certainly going to die.
14103000	14107000	And we might, but we also might not.
14107000	14108000	I don't really know.
14108000	14111000	I would love to just become less confused about this.
14111000	14115000	And I remain very concerned about this to be clear.
14115000	14120000	But I'm not like 99% chance we're all going to die.
14120000	14126000	Yeah, but anything which is an appreciable percentage may as well be the same thing.
14126000	14128000	Yeah, pretty much.
14128000	14129000	Yeah, it's quite funny.
14129000	14132000	I got a lot of pushback on the Robert Miles show.
14132000	14134000	People said, oh, I can't believe it.
14134000	14136000	You framed him to be a doomer.
14136000	14142000	And he himself said in the show, I think about five times we're all going to die.
14142000	14145000	And I managed to cut about five.
14145000	14153000	Well, I don't exaggerate, but that there is at least two posts on Twitter within 15 minutes of that comment where he said, and we're all going to die.
14153000	14156000	So I don't think I don't think I'm being unfa...
14156000	14160000	Well, I didn't actually call him a doomer, but he basically is.
14161000	14163000	I don't know, man.
14163000	14164000	I hate ladles.
14164000	14167000	Like, Eliasar is clearly a doomer.
14167000	14168000	He's clearly a doomer.
14168000	14169000	Yeah.
14169000	14170000	Rob is much less doomy than Eliasar.
14170000	14171000	Yeah.
14171000	14172000	Is Rob a doomer?
14172000	14173000	I don't know.
14173000	14175000	I didn't call him a doomer.
14175000	14178000	But empirically, the data says yes.
14178000	14180000	Yeah, I mean, I don't know, man.
14180000	14183000	It sounds like you spend too much time reading YouTube comments.
14183000	14184000	I do.
14184000	14185000	Too much time.
14185000	14192000	But notoriously, the least productive use of time possible, apart from hanging out on Twitter, reading AI Flameless.
14192000	14194000	Twitter is the worst.
14194000	14195000	I know.
14195000	14196000	It's so bad.
14196000	14202000	I mean, we don't need to go there, but we were having a brief discussion before we started in record.
14202000	14211000	Why do you think otherwise intelligent, respectable people behave in that way?
14211000	14217000	Impulse control, social validation, it's just kind of fun.
14217000	14222000	People aren't very self-aware about how they look or aren't that reflective.
14222000	14227000	And Twitter incentivizes you to lack nuance and to be outraged about other people.
14227000	14230000	I don't know.
14230000	14236000	I am very sad by many Twitter dynamics, including from people who otherwise seem worthy of respect.
14236000	14237000	Yes.
14237000	14238000	Yes.
14238000	14239000	Interesting.
14239000	14243000	Look, Neil, this has been an absolute honor.
14243000	14244000	Thank you so much.
14244000	14245000	It's been extremely fun.
14245000	14246000	Yeah, it's been amazing.
14246000	14247000	It's been a marathon.
14247000	14249000	But thank you so much for joining us today.
14249000	14253000	And I really think we've had a great conversation and I know everyone's going to love it.
14253000	14254000	So thank you so much.
14254000	14255000	Yeah.
14255000	14257000	I apologize for all the times I saw you off for philosophizing.
14257000	14258000	Oh, no problem.
14258000	14260000	It's an honor.
14260000	14261000	Yeah.
14261000	14262000	All right.
14262000	14264000	Thanks for having me on.
