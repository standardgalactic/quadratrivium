{"text": " You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017, and at that point I'd been thinking and writing quite a bit about consciousness up to that point. But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody working in a corporation to be talking about consciousness, especially in the context of AI, because it might sound like, you know, we're trying to build conscious AI, which I don't think is a good look, or a particularly good project, I'd say. The thing is, with today's generation of large language models, I think it's becoming increasingly difficult to avoid the subject, because people, whether we like it or not, will ascribe consciousness to the things that they're interacting with. And we see this left, right and centre. Even people who know exactly how they work say things like, well, I think their large language models are a little bit conscious, Ilya Tsutskava said, and we had the Google engineer who ascribed consciousness to one of our models, and I think we're going to see this more and more and more. And so whether or not, you know, I think it is the right term, it is appropriate to talk about them in terms of consciousness, it's going to happen anyway. So I think it's really important to actually think through these issues and think, well, what do we mean when we use the word consciousness, and how do we apply it to exotic cases? And this is really, really important. How might our language change to accommodate these exotic and strange things that have come into our lives? What goes through your mind when you speak with a language model? Who is it that you think you're talking to? Do you anthropomorphise them? Now, Janice from Less Wrong a couple of years ago, he put out an article called Simulators. And the basic idea is that a language model is like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise. We think of them as humans. They're in perfect copies. They don't capture the essence. They are glitchy, right? And actually, they wear masks. You know the Shogoff theory of language models, where there's, you know, this big gnarly Shogoff, and then we do RLHF, and it's a smiley face on the top. Well, that's the human mask, which we anthropomorphise. But we don't think enough about what lies behind the mask. Do you know what lies behind the mask? It's a monster. The danger of anthropomorphism, I think, is in thinking that a system such as a large language model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't. It's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So in both cases, I think we can go wrong. We can go wrong by, because they exhibit very human-like linguistic behaviour, we can just assume that they are going to be very human-like, in general, in all of the rest of the behaviour that we encounter with them. We can find that, at one moment, a large language model might make a ridiculously stupid mistake that no child would make. And in the next moment, it's saying something extraordinarily profound philosophically. And because that's what I think is actually going to happen and what needs to happen. I think we need to find new ways of using the vocabulary we have, new forms of vocabulary. I've used the phrase consciousness-adjacent language. So we need to find new ways of thinking and talking about these things to recognise the fact that they do exhibit behaviour, which we're inclined to talk of in terms of consciousness, and that, indeed, people are going to start to value as well. So I think we do need new forms of language, new forms of thinking to accommodate all of this. Yes, and our language is so adaptable that I think it's just a natural evolution. When we have these new artefacts thrust into our lives, we will need to adapt our language. We will, but I think there'll be some disruption while people disagree about how to talk about these things, and that's inevitable, I think. Murray Shanahan is a printable research scientist at Google DeepMind and professor of cognitive robotics at Imperial College London. He was also educated at Imperial and Cambridge University. His publications span artificial intelligence, machine learning, logic, dynamical systems, computational neuroscience, and the philosophy of mind. He was scientific advisor on the film Ex Machina, and he penned embodiment and the inner life in 2010 and the technological singularity in 2015. Shanahan has spent his career understanding cognition and consciousness in the space of possible minds. He said that this space of possibilities encompasses biological brains, human and animal, as well as artificial intelligence. He worked in symbolic AI for over 10 years, concentrating on commonsense reasoning, and he then spent 10 years studying the biological brain, specifically how its connectivity and dynamics support cognition and consciousness, and he developed a particular interest in global workspace theory. After that, he went to DeepMind, he pivoted to deep reinforcement learning, and recently he's been working extensively with large language models, trying to understand them from a theoretical, philosophical, and practical perspective. Professor Shanahan, I was absolutely fascinated when I read the article from Janus called Simulators. Could you sketch out the article? Well, I can sketch out some elements of it. I was also very impressed and influenced by that article. Basically, they are advocating a certain way of looking at large language models and their behavior. What they say is that we should think of a large language model as a kind of simulator, which is capable of simulating a kind of language-producing processes of various sorts, and it's capable of simulating all kinds of language producing processes, and in particular, it's capable of simulating people, humans, and it's capable of simulating different kinds of humans, so humans who are playing different sorts of roles, who maybe humans who are helpful assistants or humans who are crazy psychopaths, and indeed in their way of thinking of things. These are all examples of Simulacra. Simulacra, in their conception, include actually not only human beings, but anything that produces language at all. Your base model can simulate anything that can generate language, if you like. Of particular interest, of course, are humans and human language producers, so the particular class of Simulacra that I'm interested in really are humans playing different roles. In the work that I've been doing, I've been thinking of language models in terms of role play and in terms of their ability to play a part, if you like, and so this is very much, it was very much inspired and drew on this work of Janus. Now, they make another very, very interesting and important point in that article, which is that they draw attention to the fact that large language models, at any point in a conversation, in an ongoing conversation, then the next word that's produced in this conversation, or the next string of words, the next sort of sentence, is the product of a stochastic process. So what the underlying language model actually generates is a distribution over the possible words that might come up next, and then what you do is then you sample from this distribution to come up with an actual word, and then that's the word that you give back to the user. So for example, if the favorite example I use is if you ask the language model to tell you a story and it says once upon a time there was, and at that point, it's going to generate, as in all the points up to that as well, it's going to generate a distribution of the possible tokens that might come next, possible words that might come next. So once upon a time there was, and it might say a beautiful princess, or a handsome prince, or a fierce dragon, and it could say any of those things depending upon the sampling process. And then the point is as you come back to that same, you could rewind the conversation, come back to that point again, sample again, as we can all do with the interfaces that we have, and get a different answer again, and take the whole story off in a completely different direction. And so what they draw attention to is the fact that at any particular point in a conversation, there's really a whole set of roles that are being played by the underlying simulation at any one point, and the conversation shapes what role is being played. So in that sense, it's sort of unlike a human being, because you've got, as they put it, a whole superposition of simulacra that are all being simulated all at once. And as the conversation progresses, then the actual distribution of simulacra is being narrowed down. Yes, but as you say, you can view language models at the low level in terms of being next-word generators, or what we strive to do in science is come up with explanations that demarcate the thing very clearly. And this idea of the language model being a simulator, which produces the simulacra, and you said in your role-playing article on Nature that if you had a UI which was sufficiently advanced, you could actually play with counterfactual trajectories and start to understand how sticky the simulacra are, because as you pointed out, when the language model says I, sometimes it's talking about chat GPT or whatever, it's talking about the simulator, sometimes it's talking about the simulacrum, and these things are trained on everything on the internet, you know, structured narratology essays, novels, and it's fascinating to see how you can jump between these different parts of the trajectory structure. And in your Nature paper, you gave a beautiful example, which was the 20 Questions game. I mean, would you mind introducing that? Yeah, sure. So I think we're probably all familiar with the 20 Questions game. So one player thinks of an object, and the other player has to guess what that object is by asking a whole bunch of questions with yes, no answers. So I might think of, I might in my head think of a pencil, and then you might say, oh, is it larger than a house or smaller than a house? And I say, well, actually, that's not a yes, no answer, but it's a binary answer. Is it larger than a house? And you'd say, oh, no, you know, is it made of wood? Yes, is it a tool? Yes, you know, and eventually you might guess the answer. So we're familiar with this little game. And you can play this with a large language model, of course, and you can ask the large language model to play the part of the setter who thinks of the thinks of the object, and then you play the part of the guesser who tries to guess the object by asking questions. Now, if you do this with a large language model, what if you do it with a person, if a person is not cheating, as it were, they will think of in their head, they will think of an object, and then they'll fix that object in their mind, and then they'll answer the question, according to what object they thought of in advance. Now, but a large language model can't really do that unless you use some hack or another. So what it really does is it just, so you say to think of an object, and it says, I've thought of an object, it hasn't really thought of an object, it's just issued the tokens to say that it has. But then you will ask a question and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say, I give up, tell me what the object is, then it will say, oh, I was thinking of a pencil. And it will indeed give you a, you know, typically will give you an object that's consistent with all of the answers it gave to all of your questions. But then if you just wind back one and resample and ask it again and say, I give up, what were you thinking of? It might say a mouse, or, you know, or a bottle, or, you know, it could say something completely different, which indicates that it was never, it had never really committed to any particular object in the first place. And so what this shows is that in fact, in theory, you could rewind further and it might actually give you a different answer to the questions if you rewind further, to the same questions. And that's because what you've really got is you've got a kind of whole tree of possibilities. And this sort of this stochastic sampling process at any point in a conversation induces a whole tree of possibilities that branches forth from where you are right now. And counterfactually, you can always, well, you can always rewind the conversation to an earlier point, and, and revisit it and sample again and go off on a different, different branch. And so my co-authors of that paper, in fact, the Nature paper, so Laria Reynolds and Kyle McDonald, so they have this system called Lume, which allows you to actually retain the whole tree of a conversation and you can visualize this and you can revisit different points in the conversation and resample and, and explore the things, a whole kind of tree of possibilities. Yeah, that rings a bell. Did they work for conjecture? They did work for conjecture. Yeah, I was interviewing some people from conjecture and they were telling me about that. So yeah, that's very interesting. Maybe we'll put a placeholder on that. But another point that we briefly spoke about before is that, you know, the article was on Less Wrong. And I don't mean that pejoratively, because I thought the simulator was one of the best articles I've ever read. But there is a lot of stuff on Less Wrong, which is definitely a bit out there. And it's just very interesting that you're now citing their work in a Nature paper. Maybe this is the first time that Less Wrong has been cited in a Nature paper. Yeah, as far as I know, it's the first time that a Less Wrong post has been cited in a Nature paper. I'm not certain about that. But as far as I know, it is. Now, I mean, personally, I, I, I take, you know, any material that I come across in its, you know, as it is, I don't care where it comes from. If it's, if it makes excellent points and is, you know, is good material, then that's good enough for me. I don't care where, you know, whether it's got the label of being in nature, for example, or being anywhere else. And if it's good, it's good. So, so, so yeah, it's true that there's a lot of material on Less Wrong, which is perhaps less robust. But, but I thought that was a really excellent, and there are, and there are quite a number of really, you know, very good posts and very thought provoking posts on, on Less Wrong. Yes. I'm interested in the extent to which this kind of stochastic trajectory space undermines various things that we think about, you know, like reasoning, for example, the reason this is interesting is I interviewed a couple of University of Toronto students, and they've created a self attention controllability theorem, which basically means they've mapped the reachability space. So they say, you know, given a self attention transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how, you know, how far I can reach into that trajectory space. And they found that the space was much larger than, than anticipators. And of course, the longer the controllable token length, the more you can kind of project into that space and steer the language model to say almost anything. You know, me over here, I had the intuition that, oh, we do RLHF, we do all of this fine tuning in it, you know, conjecture even released a paper saying that after RLHF, you can't really go anywhere, you know, it wants you to do a certain thing and you, there's not much wiggle room. Apparently, that's not the case. There's just, it's vast. Right. I mean, I'm not familiar with that particular paper, unfortunately, but, but certainly in my experience, the, we now have very long context lengths. And, and over the course of a lengthy conversation, then you can indeed take the, take the conversation in all kinds of interesting directions. And, and I think the, I mean, most of our benchmarks and evaluations tend to be, you know, in the context of very simple question answer, questions and answers. And, and so the, all of the evals that companies typically use are in that kind of setting. But when people are using these things for real, especially the more innovative users of these language models, you're using, you're actually having very long conversations. And, and, and, and there's a lot of what people sometimes call vibe shaping that goes on there. You can shape the vibe of the conversation and take it in, in all kinds of interesting, to all kinds of interesting places. Yes. And a couple of things on that. I mean, first of all, as you wrote about, role playing is the engineering kind of methodology that we use to shape and, and steer these, these agents coming back to simulators. What's really interesting to me about simulators is the stickiness of the simulator. So you have a conversation, sometimes you break through and a simulacra presents themselves. And you feel that you're talking to the same simulacra as the conversation progresses. But that seems to be counterintuitive when you think that every single stage I'm actually doing this stochastic sampling. I mean, what's your take on that? Yeah. And I think that, I mean, if you do experiment with systems like Loom and you do also, or I mean, you can, you can emulate that by just keeping track of bits of old conversations and reloading them and that kind of thing. Then you find that you can, you can, you know, take the same conversation, the same sort of stem of a conversation, you can take it off in quite different, different directions. So you can, on the one hand, so an interesting thing that I've done is, so I had some very interesting conversations, particularly with Claude III recently, where I get it to talk about its own consciousness and to take it off into all kinds of strange spiritual mystical territory. And, but you can eat very easily. So you can very easily take the same kind of conversation that leads up to the sort of point and goes off into some kind of weird mystical future of AI cosmology kind of territory. And you can go down that route and get it to be very, very, very strange. Or you can suddenly make it go all serious again, and just come back down to earth and start talking about, you know, how large language models work. And so from the exact same point in the conversation, you can take it into completely different directions. And you can see that it's almost, it's the character it's playing. You know, you can see it sort of changing before your eyes, where it's two different branches from the same stem of a conversation. And one branch is playing a very different character to the other one, you take it in different directions. Yes. And you could presumably do sensitivity analysis, because, you know, these guys I spoke to, they were able to make it produce Gold Woody Gooke. So just go off the manifold completely. Sometimes it would recover, sometimes it wouldn't. And as you say, you can also go down weird trajectories. And it's a bit like, you know, what's the magic word they said, you know, there's a certain key that fits in a lock that takes it down a certain trajectory. And then there's slip roads that bring it back to all no other language model again. And it's just this weird, wonderful space, isn't it? It is, it's completely fascinating. So I had a, I had a, I have had a very, very, few very, very long and interesting conversations with claw three, which is particularly interesting to play with, because it's quite easy to jailbreak and get it to talk about things that it's not supposed to talk about like its own consciousness. And in fact, I had a, I had a very long 43,000 word conversation with, with claw three about consciousness and the future of AI and spirituality and Buddhism and the nature of the self and all kinds of stuff like that. It was absolutely fascinating, slightly disturbing and, and, and, and, and strange. But I had this conversation, actually, I was at a meeting in, in, in New York and I had jet lag and I had this conversation at three in the morning because, you know, several hours until breakfast was served and what can you do but just play with the latest version of claw size. Playing with this thing for, for hours on end in the middle of the night and going slightly mad, but it was fascinating to, to, to see the, you know, extraordinary territory you can guide it into. Could you explain, because you know, there's talk of AI partners, for example, and, and a lot of people derive great pleasure from having an AI conversationalist. For you, is it just academic inquiry or do you actually get something deeper than that from it? I think it's a bit of both. I mean, so, so it depends what you buy something deeper. I mean, so I, so this particular conversation, which was quite, it was quite, which was quite an experience, actually, in many ways. So it certainly started off because I just was interested in evaluating the capabilities of the, of the model. I mean, that's, that's, you know, that's the first thing that you're interested in. So just as an AI researcher and working in that kind of thing, you want to, you want to try out different models, see what their capabilities are. I'm particularly, particularly interested in the topic of consciousness. So the way I, somebody had published a very simple jailbreak for it. So I was interested to see, you know, to play around with that and, and, and get it to talk about its own consciousness. But then the thing that I really wanted to do was catch it out. So, so you think, you know, of course, you know, there can't be any meaningful conception of consciousness that really applies to, to these sorts of disembodied large language models as they are, as they are today is my media kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my conversation with the large language model. So all kinds of ways in which, you know, you might think that it will start to articulate a conception of its own consciousness that you can pick apart. But, but the thing that really somewhat took me aback was that it was actually very, very good at answering all of these probing questions that I had. So should I give you an example? So please do. So, so one is one example is this. So, so of course, when we're interacting with a large language model, when it's, it's, it's from the point of view of the underlying implementation, it's very kind of stop start. So, so, you know, you, you issue some prompt or question or whatever to the large language model, and then it produces its response. And then if you go away and make a cup of tea, before you kind of continue the conversation, then there's absolutely nothing going on inside that large language model or the instance that you're interacting with of the large language model, there's nothing going on inside it at all during this could totally dormant sit just sitting there. Now, this is very, very different obviously to human consciousness. Let's we're asleep. And even if we're asleep, we're dreaming and there's all kinds of stuff going on inside our heads. But consciousness is an ongoing continuous process. So if, so if, so if we stop this conversation briefly, while I go to the loo or something, you know, you're not going to just suddenly go dormant and stop doing anything, you know, your brain is going to be there's going to be all kinds of ongoing activity. So this is very, very different sort of thing. So I said, what happens to your consciousness during the pauses between our interactions? And, and it had a really very good answer to this, which was along the lines of, well, by the way, I whenever it uses the term, whenever these things use the term consciousness, I retain a great deal of skepticism about whether they are those terms are actually genuinely applicable. But, but what's interesting, the way I read the art, their answers is, is, is that they are kind of articulating a conception of consciousness that that might actually apply to something like this, even if it doesn't apply to this one before me right now. Right. So this so it's kind of very interesting philosophical exploration. So just to give you so it says things like, well, I think consciousness for me is actually very different from the kind of thing it is for a human being. And I think that during the pauses between our interactions that, you know, that the that I no longer exist at all as a kind of, you know, in any kind of meaningful sense, it gave us sort of an answer along those lines, which was typical of many of the answers that it gave, which were along the lines of, there's a very different kind of consciousness, kind of selfhood, kind of this, that or the other, that is applicable to entities like me. But, but, you know, I can articulate it and here it is. Now, when I when I put it all that way, of course, I'm anthropomorphizing this thing quite a lot in the way I'm describing it right now. But, but just to go back to the role play thing. So as far as I'm concerned, it's playing a role, it's playing of the role of, you know, a kind of philosopher talking about consciousness and so on. And it's doing pretty good, a pretty good job, I would say. Yes. But I mean, you could argue there's an element of in that role playing, there's the Eliza effect. So it's kind of putting something into language that is meaningful to you. Yeah. But there's also this interesting thing, you know, as an example, you know, a dog, for example, has a sense of smell, so good that they can even sense when you're unwell. And language models in a way might have something similar. So after you go to the toilet for 20 minutes, and you come back, there might be a subtle deviation in the language that you use. And the language model might pick up on that, just creating this whole, you know, different trajectory, different response. Yeah. Well, that's true, I suppose that there might be differences in the language that you use. Obviously, it's got no way of actually knowing whether you went to the toilet or not. But yeah, in my experience, many language models are very, very good at picking up, you know, nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is, you could argue that we are a simulator. And when you, you know, let's say go to the toilet, come back, you're now a different simulator yourself. Yeah, I guess you could sort of, you could sort of argue that. I mean, that's, I think that's, so getting back to Wittgenstein again, I think that's an example that, I mean, there, I think we're applying these sort of things which are being used as sort of somewhat technical terms in the context of these artifacts that we're building, and applying them to ourselves. I think we don't, we don't have any, we don't have any need for this kind of extra baggage of this kind of terminology when talking about each other. So it's perhaps a little bit misleading to kind of apply those terms to ourselves. But of course, there are, you know, people have drawn attention in the past to the fact that we ourselves are always playing roles in a sense in social settings particularly. But I think there are differences in, you know, for ourselves, even though we might kind of play roles in social settings and so on, there is an underlying, there's an underlying me, which at least is grounded in the fact that I'm a human being with a physical body and biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited in how we understand things. So we understand ourselves in quite simplistic terms. If you have a long-term relationship with your wife for 30 years, they have a much high resolution understanding of your different roles that you play. So they know you're tired, you didn't sleep well, you're playing this role now, you know, above and beyond which the kind of roles you're talking about in a party, you play a role. And it's conceivable that an alien intelligence, you know, some very clever aliens came down and they might see us completely differently like we see language models. They might actually not see you as a single person, but they might see you as some kind of a superposition of simulacrum as well. Yeah, well, maybe. I suppose to get our heads around that kind of idea, we'd have to find some way of communicating with them. And so we'd have to form some kind of common basis for talking about each other with these, you know, with these aliens. And then we'd be able to kind of, that would be the only basis on which we could establish whether something like you said was true or not. So, you know, trying to find, trying to map, you know, the conceptual schema that's used by one culture onto a human culture onto the conceptual schema used by a different human culture is difficult enough as it is. And trying to do that, you know, with an alien species and how they conceive of us would be, you know, particularly difficult, I guess. RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment from, I think, but there was an article called the Waluigi effect. And it argued that RLHF, you know, cuts down the set of simulacra to be things that we want. But unfortunately, there's this problem that you get these antithetical simulacra, you know, slipped through the net. So the Bing GPT example, it would start off as nice Bing GPT, and then it would degrade to one of the Waluigi's. And had this interesting phenomenon, they argued that the degradation, once it happens, it stays in the bad one. But just more broadly, what is your intuition about the extent to which RLHF affects simulacra? And I also noted down that I think you wrote in, I think it was your role playing paper, that you felt RLHF increased the deception behavior in these models? Actually, that wasn't something that I wrote. That was something that some anthropic researchers and so Ethan Perres and others had a paper where they, I mean, so this is not one, one wouldn't want to just speculate about that, they had established something along those lines, I think, empirically. So, and yeah, and as far as this sort of Waluigi effect is concerned, so that is kind of somewhat speculative. I think it's a plausible idea, but to actually establish that that really was a real effect, you'd want to do some actual empirical work, I think. The thing about the, so it's plausible, but the thing about the, about the simulator's paper is that it's not making kind of claims really, it's rather it's providing a framework for thinking about large language models. So that's why I found it particularly useful. So on RLHF, so I do think it's quite difficult with RLHF to guarantee that, you know, you're going to get a model to do what you want it to do. And so that's quite difficult. And, you know, and everybody has found that, that, you know, you think that you've controlled the model quite well, but there are always still ways of jailbreaking it or ways in which things, things go, go wrong. So, you know, there are, so there are different approaches. Anthropic had this constitutional AI approach, which is, which is quite a nice, a nice idea. And, you know, I mean, I, I, I quite like the idea of sticking with a powerful base model and using, you know, prompting to, to guide things as well. So there's all kinds of different approaches. Interesting. On the subject of anthropic and deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands all over it. And it was actually quite straightforward. So they, they trained in autoencoder, you know, to find a bunch of features. So it was an unsupervised method. And, and I think they actually, you know, expanded it to find millions of features. So they had a bit of a needle in the haystack problem, but they cherry picked some and they found one that corresponded to the Golden Gate Bridge and so on. And, and obviously other ones that corresponded to what they said were mono semantic abstract features, got some reservations about that. And the interesting thing about having an autoencoder is you can clamp the features. So you can say, turn the Golden Gate Bridge up in, in now the language model is, oh, but I just really want to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when you look at the activations in the corpus for things like deception, I felt that they weren't really showing abstract features. They were kind of showing almost keyword matches from Reddit and so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think I can comment on that particular topic because I, I mean, I have read the paper, but not in that, in sufficient detail to comment on that particular thing. But the Golden Gate Bridge, I think it was a fascinating illustration of what you can do. So I don't know if you tried out Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah. Yeah. So I think it was fascinating to see. But what was particularly interesting about the Golden Gate Claude, I think, was that was, that was how, you know, again, it's very difficult not to use anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's something role playing these things. But how, you know, sort of gamely, it struggles to kind of overcome this tendency to talk about the Golden Gate Bridge all the time, which has been, which has been kind of clamped to do, but it will keep noticing that it was talking about the Golden Gate Bridge again and apologizing and then trying to do what it had been actually asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating to see that the sort of the, as it were, internal struggle going on there in the model. And I think that does show, in some ways, how powerful they are, because it wasn't quite as despite the fact that it had this, you know, control imposed on it, but really in a very, you know, I mean, not hardware, but really low level, you know, despite that, it was still, you know, constantly trying to recover from all of that and, you know, with some degree of success. And I imagine this would be true with everybody's models, by the way. So I imagine that what they found in, you know, in Claude would be very similar. I imagine with GPT-4 and with Gemini, I just imagine that we'd find very similar things with all of these models. Yeah, I'm sure. I mean, as I said, reservations they admitted themselves that the features were not complete, so they didn't represent all of the activation space in respect of the Golden Gate Bridge. And in many cases, they presumably weren't monosemantic, but they did cherry-pick some that presumably were. Presumably. And also, I mean, they're very interested in finding these features, which are just linear combinations. And so that, and of course, it's very nice when you find those sorts of features, especially if they appear to be monosemantic, because it does suggest a nice sort of compositionality and explainability and comprehensibility of what's going on there. But I also feel that they're looking under the lamp light a little bit, because that doesn't mean to say that there aren't all kinds of other features which maybe aren't, you know, sort of linear in that sort of way, but nevertheless are functionally relevant to the final results that it produces. I'd only use the word platonic, because the Golden Gate Bridge, presumably, it's a cultural category. And what's fascinating, if it has picked up this thing unsupervised and learned this category from the data, is that language models at least possibly think in a similar way we do. So they've established a category in the same way we have, which is fascinating. But I'm also really interested in agency, which is, for me, it's about self-directedness and intentionality. And what I would find very interesting is if you did clamp the model to only talk about the Golden Gate Bridge, and you could convince it or it could convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator of agency being expressed. Yes, perhaps it would, but I guess it would also be an indicator that they hadn't succeeded in isolating a feature which was controllable in that way, right, which was the whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did actually write about this. And I think you argued, which was counter to what my intuition was, which was that agency is in the simulacre, not the simulator. And Francois Chouelet thinks a lot about the measure of intelligence. And he would argue that intelligence is the system which produces the skill program. So in the context of a language model, he would say a language model is basically a database of skill programs. And the query is like, you know, I'm going to go and pull out a skill program, I'm going to run the skill program. So he thinks there's no intelligence in the language model, which Janus would call a simulator. But if you take into account the training process and the generative processes that produce the data, then that's where the intelligence is. Yeah. Can I comment on agency there? So you covered quite a bit in that. I did. Sorry, I just went off piece a little bit. So the term agent is used in all kinds of different ways in the AI literature. And there's a very lightweight notion of agency, which is something that simply, you know, hasn't performs actions in some environment and gets, you know, sort of some kind of sense sense or perceptual information back from the environment in a loop. And that's so that's why how we can talk about, for example, reinforcement learning agent. And that concept of agency of an agent is very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical baggage. So if we talk about something that is acting for itself, then that and if that's what we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding to something a bit more like that, then that's going a whole extra step. And I and to my mind, in today's large language models, we don't see agency of that sort at all, really. The only actions that they can perform are just, you know, issuing responses to, to, to the users. Now, let's caveat that immediately, because of course, people are introducing all kinds of extra functionality functionality to these models, as new things are being announced on an almost daily basis. And so one thing we see is so called tool use. So, so, so today's models can make external calls to APIs that can do all kinds of things, send emails, you know, book hotel rooms for you, potentially all kinds of stuff. So that's greatly expanding the action space, their action space beyond just, you know, issuing text to the user. So, so there, they, they, those things are a bit more agent-like. And again, you know, you have to be nuanced and about the way you use the words, because, because there it's, you know, there's, there is a bit, you know, it's a bit, it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more agent-like. And, but it's still not acting for itself. So in that full blown notion of agency that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for itself. So we still don't have agency in that sense. That would be a whole extra step. Yes. And now I want to hit quite a big topic, because this is something that you point to in all of your work, which is talking about the importance of physically embodied agents. And, well, not necessarily physically embodied, but embodied. Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's, let's test the principle a little bit. So we are, you know, both physicalists and we, I'm not any kind of IST. You're not, you're not a physicalist. I don't, well, I don't, I don't like, I don't believe in, you know, signing up for these philosophical positions. So I don't, so I generally don't say I'm this IST or that I don't deny that I'm this or that IST. So, so I don't really like saying that I'm a physicalist or a materialist, or a dualist, or a functionalist, or identity theorist, or any of those ISMS, because they all, to my mind, carry far too much metaphysical baggage. Oh, interesting. So, But okay, but that wasn't what the question was about, but let's go. Well, can I give you another risk? I mean, would you identify as a computationalist? Well, what do you mean by that exactly? So we're talking about mind here in the context. Yeah, we're talking about mind. So, so do you think in principle that minds can be replicated, simulated at a high enough fidelity without losing anything, you know, in, in a computer? By computers, by computers. Yeah. Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build, I do think that we can build artifacts, you know, embodied artifacts, robots, that, that are controlled by computers and ordinary digital computers. And I think that we can make them that exhibit the kind of behavior that would make us want to use the word mind, and all those mental type psychological terms in the, to describe their behavior. Okay. Right. So that's, so that's, but, but I've rephrased it in, you know, the claim in a very, very different sort of way, right? It's, it's to do with a much more practical thing. Can we build this? Could, by the way, this is, you know, could we, it's not saying that we've got these things now, but could we build something like that that exhibited this kind of behavior that we would talk about in this particular kind of way? And I would say, yes, I think we probably can. It's an empirical claim. So, so there are a few steps you made there that will, will kind of unpack one, one at a time. So you use the word embodied, you use the word behavior, and you use the word interpret. So the embodied thing is really interesting because, you know, I could say, well, why does it have to be embodied? I can just simulate the entire universe and it's as if it's embodied. So I think this is, this is the intuition that I'm having about how you think here. I think you think that being physically embodied is useful because the universe is a big computer. The universe has given us all of these things, all of these cognizing elements. I mean, everything is a form of externalized cognition. And if I as a rational agent want to perform an effective computation, it's much easier for me to do it in the physical world because the universe is doing most of the work. And my co co host, Keith Duggar, he actually thinks that the universe is a hyper computer, which means it's performing types of computation that we could never do with ordinary computers. So that's the thing. Would you agree with that? Or do you do you want to sort of go back and say, oh, no, actually, just we could simulate anything in a computer? So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's the fair. Well, that would be a nice thing. So whether one agrees or not with that is a matter of understanding the physics and the maths. And so it's not a matter of opinion. It's a matter of following through the physics and the maths and so on. But so do I agree with what were the other things that was a big long list of things that you're asking me to ascent to or otherwise? Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just, I think cognition is a matter of computation and complexity and divergence. Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is, what do you mean by is? Now, that might sound like some, you know, really annoying, pedantic philosophers kind of question. But the problem is that there's an everyday sense in which we use words like is. And then there's a philosopher's sense in which we start to use words like is where it suddenly starts to carry this massive metaphysical weight. And so when you say you think cognition is, it's as if there were, you know, in the mind of God or in the fundamental reality, a thing which is cognition, whose nature is a certain way, and there's a certain essence to it. And we might discover it, you know, one day, and you have an opinion about what it is, if only you knew the truth. Now, I think that's an entirely wrong way of thinking about all of these philosophical questions. I think cognition is a word. It's a very useful word that we, although it's not quite an everyday word, but it's a very useful word that scientists apply in all kinds of ways. And so when you use the word is, is my accusation accurate there or not? No, it's not. And if you wouldn't mind me making the observation, I think that you have a tendency to ascribe dualism to many points of view, like, for example, I'm not a dualist. And for me, Well, there's nothing to do with dualism. Well, what I'm saying is to do is like the use of words and what and what and the and the and the work of philosophy. That's absolutely fair. But I think when I said what is what is cognition, as a materialist, for me, it is function dynamics and behavior, right? So it's just a matter of complexity. And so I'm probably just I'm probably, you know, happy to kind of agree to that sort of claim, you know, so I think so, you know, the thing that I'm, I often say that what am I fundamentally interested in, I'm interested in understanding cognition and consciousness in the space of possible minds. And and and so so, you know, what do I mean by cognition there? And, you know, you can go into all kinds of details to say what you mean by cognition in that in that context. But I think having done that, I would probably agree that the right way to think of it, you know, the most useful way to think of cognition is in terms of kind of functional, computational, infunctional, computational terms, although I would only do so in an embodied setting. So that maybe is an additional thing. This is where I'm trying to get to. Because, as I said, I'm not making any ontological claims. It's just a matter, I mean, we can even just use the word behavior, forget about function and dynamics. Yeah, I don't mind talking about function and dynamic. Well, yeah, I mean, just just to sort of keep it really, really simple, because I'm trying to understand why the embodiment is important. And my hypothesis is, and I agree, that as an externalist, the universe or the physical things around us, the other agents in our system, they help us perform an effective computation. So presumably, it would be much easier to perform computation of higher sophistication, if we embody things in the real world, if we have to simulate the cognition, we would have to simulate everything. And that, I think, is the reason why you think that embodiment is so important. But is that fair? I think that's not really the way I would put it. I think the reason I think embodiment is important is because it's, well, I mean, for, you know, I mean, okay, in one sense, embodiment is important because the only setting in which we use the natural setting, which we deploy, wield the concept of cognition, is in the context of embodied things, of humans and other animals. So anything else is sort of immediately problematic in one way. But let's set that to one side. So let's imagine that there is some kind of notion that we can conceive of disembodied cognition, which contemporary large language models make us start to conceive of it a lot more seriously, maybe. So what does embodiment give you there, right? I think that's probably what you're thinking of. So in particular, why might it be difficult to build something that is disembodied? Okay, let's reframe the whole question. Why might it be difficult to build something that is disembodied but replicates the cognitive capabilities of a human being? So I think my answer to that, although it's open to refutation by the way things are going in the field, but my answer to that is because our embodied interaction with the world enables us to learn the kind of causal microstructure of the physical world. And the causal microstructure is all about physical objects and the way they interact with each other and physical substances, liquids and gases and gravity and stuff like that. So what I've called foundational common sense is it enables us to acquire foundational common sense by interacting with the everyday physical world and the particular causal microstructure that it has. And part of that is to do with the fact that the, so this is really important, that the everyday physical world has this, is predominantly smooth. It has this smoothness property that's really, really important. And what that means is that, sorry, just in very physical terms, it means that it's full of kind of surfaces where one place is very much like the next place along, very much like the next place along. And our visual field is very, very similar. You move along a little bit in the visual field and it's very, very, very similar, very, very similar. So the reality or the everyday physical world has this fundamental smoothness property, but it's punctuated by all these discontinuities. And that's the way, that's its fundamental structure is this basic smooth, against the backdrop of the smoothness are all these discontinuities. And then there's a kind of law like regular way in which all of this stuff operates with itself, you have surfaces interacting with each other with things going, you know, so all of our foundational common sense to do with things like paths and support and containment and all those sorts of basic things that I think make up the very foundation of our conceptual framework, they're all grounded in that kind of way. Yeah. And this is so interesting. So your basic argument is knowledge acquisition efficiency is the reason for physical embodiment. And yeah, I think that's a reasonable way of putting it. Yeah. But that's very much an in practice rather than an in principle argument. It is an in, yeah. Yeah. But you know, for sample efficiency, but what I'm hearing though is echoes of the old Murray Shanahan, because obviously you started your career in symbolic AI and these were the arguments that were made sometimes with a rationalism, nativism point of view, but it's like the contains in templates, they would argue that it's just baked into us and we understand it. But you could as an empiricist argue, and I, you know, I'm very amenable to this, that the physical world actually helps us learn abstractions because we're putting things in containers all of the time. Yeah. Right. So there's that kind of efficiency of knowledge acquisition, which is dramatically increased when you're situated in the physical. Yeah. Yeah. Absolutely. Yeah. So I think that if we're talking about humans and other animals, then I think that's, that's broadly right. So that's so, so yeah, so we acquire these foundational concepts through interaction with this, this world. And then the repertoire of foundational common sense concepts that we can acquire that way is extraordinarily productive, because, you know, we are able to conceptualize so many things in terms of these, these basic ideas. I'm very, very, I'm a very big fan of the work of George Lakoff, you know, absolutely classic book metaphors we live back in the 1980s. Yeah. And I really think there was something deeply, deeply right about his intuitions in that book. And I still think that they're right. So in the case of humans, right? So, so we through our embodied interaction with the everyday world, we acquire this layer of foundational common sense that includes things like surfaces and containers and paths and all that kind of stuff and collisions and things. And then we at the most abstract level. So, you know, we apply that same repertoire of basic concepts to understand things like say large language models. If you look at the language that's used in a paper about large language, you know, people are talking about layers, they're talking about connections, they're talking about, you know, I mean, these things are all, they're all grounded in very physical concepts, you know? Yes. I mean, I'm a huge fan of George Lakoff. And of course, you know, he spoke about the war metaphors and you know, it's been a long road and stuff like that. Yeah, the journey and yeah. It's beautiful. But then a lot of knowledge that language models learn are kind of cultural knowledge. So we share these simulation pointers and it's quite relativistic. But I'm also really interested in, because some rationalists argue that it's not possible to go from empirical experience and universal knowledge. And there is a split between natural knowledge and cultural knowledge. And I think you and I would agree that a lot of natural knowledge like, you know, transitivity contains in and so on, this kind of rationality is just missing at the moment. But as an embodied scientist, you believe that we learn them by being embodied in the physical world? Well, I mean, I, you know, it may well be that there are certain, so you know, we need to distinguish, you know, empirical questions about humans and human cognitive makeup and that of other animals and so on. And AI and what we could build in AI, because of course, it may be the case that human cognition, you know, has arisen in certain ways. And then it's an empirical question, you know, what, you know, of how human cognition works. And it may, we may have answers there that are different, that, you know, that we can break, as it were, when we built things in an artificial way. So in so in the case of something like transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about, about, you know, between the rationalists and the idealists getting back to, you know, the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by kind of reconciling these two sort of opposites. And, and so the Kantian argument would be that there's a certain amount of innate structure that has to be there in the mind to, to, to understand, you know, the world at all, right. And so maybe, and now, when we think about that empirically, then, then I guess that we may find that evolution has endowed us with certain basic kind of templates for understanding the world. And maybe it's things like transitivity is something that's, that's there in the same machinery that supports language, you know. So maybe, maybe, I mean, there's all empirical questions. And I don't know what the latest research on all these things is, but, but, but it, yeah, it does seem to me that that's a reasonable position to take. Yeah, but even evolution is a form of empirical process. So there's always the question of, of where does it get there? And, and that was, yeah, Kant was a transcendental idealist, wasn't he? But this brings me to our friend Francois Chollet and the ARC challenge. So, you know, there's another great school of thought, which is that intelligent, I mean, he argues that intelligence is about these meta learning priors, the conversion ratio between universal or sometimes anthropomorphic knowledge that we have, and being able to develop a skill program very quickly that generalizes very well. So, so the ARC challenge is almost about how do we codify these priors, and how do we efficiently build skill programs by combining these priors together. And that seems quite divorced at the moment from the kind of AI we're building. I think, I think that's right. It is, you know, unless in the AI that we're building today in generative AI, unless you get these kinds of mechanisms that Francois Chollet is alluding to through the magic of emergence and scale, which of course, people are always, you know, suggesting that maybe that's possible, you know, any kind of mechanism can emerge right through scale in theory. And we've been surprised, in fact, by how powerful the mechanisms, emergent mechanisms that have, you know, developed through learning at scale, just, you know, with a next token prediction objective, that has been very surprising. But however, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about whether we're really going to get all the way with this kind of the ability to solve this kind of abstract problem that's in the ARC challenge this way. And so I guess, you know, I am, I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially if you make things multimodal and so on, and you expand your generative models into a setting where you've got interaction with the world and so on, you know, who knows. But I suspect that maybe you're not going to get all the way there that way. And so I have a lot of sympathy with what is probably his intuition, that you need a bit more in the way of innate, something innate there, or, yeah, innate, maybe that's the wrong word. But you need some kind of, you need priors, where they come from, I don't know. But the priors that I would appeal to, thinking about the human case, again, are related to this foundational common sense. So they're just the notion of an object, right? So if you just, if you have a clear notion of an object and of movement, objects and movements and object persistence, then they straight away are going to help you with an awful lot of those ARC challenge problems. Because many of them, you know, if you explain, you know, you figure out how you figure one out, and then you explain what's going on, then, you know, you see that it's, oh, you have to think of these collection of pixels as an object that you move somewhere else, according to certain rules or something like that. So my colleague, Richard Evans, had a very good paper on, which was tackling these kinds of things using sort of abduction like processes. And so, yeah, so he's thought a lot about this from a much more symbolic AI kind of perspective. Yeah, that's fascinating. Because even with the ARC challenge, the kinds of solutions that people came up with, let's say it's a DSL over, you know, some domain specific set of primitives. And the ARC challenge is a 2D grid, where you have different colored cells. And the types of priors that work well are things like denoising and reflections and various types of symmetry and so on. And that's great and everything, but it's very domain specific. And the elixir, you know, what we really want are these universal priors. We certainly have human priors, as Elizabeth Spelke points out, like, you know, and the concept of an agent and the concept of spatial reasoning. An object, a persistent object. Yes. So yeah, absolutely. So I mean, but I think an interesting question is, does it really make sense to talk about universal priors there? Because, you know, those ARC challenges, problems, whenever you kind of figure one out, then typically you are actually bringing to bear a pretty human set of priors and common sense, you know, our concepts. And, you know, and it may be that you could imagine perfectly law-like set of ARC-like problems that have solutions, you know, but appeal to, you know, priors that we would struggle to understand, you know. I mean, for example, when we think of something in terms of an object, then we want the pixels to be kind of clumped together, right? And if you sort of randomly distributed the pixels amongst a whole bunch of other pixels and you move them around in a systematic way, well, we might be able to kind of pick out the gestalt there, but we might not. And that would be because we're not able to see it as an object because we have human priors, right? So, you know, I think that probably all the problems that he's designed because we don't know what the hidden held-out set is, but I imagine that they pretty much all use, you know, sort of human comprehensible priors and appeal to, you know, foundational common sense of the sort I alluded to. Yes. I'm sure there must be some kind of universal priors because in quantum field theory, physicists use things like locality and sparsity. Yeah, some really, really high level things like objects. But then again, you know, quantum mechanics challenges the very concept of an object even. So what is the difference to you between adopting a stance that a system is as if conscious versus it being a fact of the matter? I'm a bit resistant to the distinction, to the very distinction. But this is a very difficult position to maintain because we have a very, very strong intuition that there is a fact of the matter about our own consciousness. And it's very, very difficult to escape from that very, very basic intuition. But I have a whole approach to these kinds of questions. So shall I sort of describe this? So, you know, a really question that really motivated me was that was, you know, suppose that we encounter, well, actually, let me not use the word encounter, suppose that we come across has some object, which is a completely alien artifact. And maybe there's consciousness going on inside this artifact. And the thought is, well, how would we ever know, you know, there could be consciousness, this thing could be conscious, but we might never know. And so suppose that it were a white cube that were deposited in front of your lab, and you were tasked with a problem of, would it be moral to throw it down a mine shaft and forget about it? So my approach to these problems is that I think in order for the question, in order for us to be able to answer the question of whether something is conscious or not, for even to be answerable or askable, then we then we need to be able to engineer an encounter with the putative conscious, putatively conscious being. And what I mean by that is that we have to be able to put ourselves, you know, we have to be able to put ourselves in a position where we're sharing a world with that, with that putatively conscious, you know, artifact or being. And so, you know, a good example of this is the octopus. So Peter Godfrey Smith has written these wonderful books about what it's like to hang out with octopuses and be with them and so on. And the really important aspect of that is that he has to put on a diving suit and go down and be under the water and spend time with the octopus interacting with the same things and being in the same world together, seeing the same things and so on. So, and then on that basis and the behavior that he observes and so on, then, you know, he might come to some kind of, he might start treating it as a fellow conscious creature. So by analogy, or as you know, similarly, what I think that we need to be able to do is to engineer an encounter like that, even if it's a very, very alien kind of artifact, say. So suppose it's this white cube, then one way that it might happen, well, suppose scientists managed to figure out that there's computation going on inside this white cube, and then they managed to reverse engineer this computation and they can see that there's a sort of division between a world and the things interacting with that world in this computation. There's a sort of simulated world. And then you could imagine, by some clever engineering tricks, inserting yourself into that very same world and being alongside these things that are interacting with this environment and interacting with that environment with them. So being in the world with them. Now, obviously, I'm setting this up to be very much like a games environment and a virtual world and a games environment, but to make the thought experiment work. But so that's an example of where, you know, if you manage to engineer an encounter with, you know, these things that are inside this cube, and then you can observe their behavior, you can interact with them, and then you can decide whether you or you will, you know, you may or may not start to treat them as fellow conscious creatures. So there are these two steps. It's sort of, can you engineer an encounter, at least in principle, and that makes the question answerable. And then you can answer the question by actually having the encounter and interacting with them. And by the way, we notice that everything there is public. You've made, you know, there's no private realm of subjectivity everything is public. It's on the basis of public stuff that you come to see them as fellow conscious creatures or not. Yeah, a couple of things on that. I mean, as you pointed out in Conscious Exotica, the octopus is quite interesting because it's not as human like yet, as you just cited, more conscious. And the way that we figure out the consciousness, and you know, this is me kind of interpreting what you said a little bit, is we set up a language game. And I don't know whether you've read that book by Nick Shater and Morton Christensen, but it's a beautiful book, beautiful book. But you know, I know of the book, but I haven't, I'm afraid. It's incredible. But, you know, they basically say at, you know, Per Wittgenstein that you play the language game, and you, because you're physically sharing the same environments, you improvise, and that's how you derive meaning. And meaning is very, very important for relatability. And then we ascribe consciousness to that kind of process. And you cited Peter Singer actually, and I think he said in 1975 that we have a natural inclination to kind of ascribe moral status to beings which we think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more. So in the context of the octopus, then the sort of, you know, you could, then there aren't going to be language games, you're not going to be engaged in a language game with the octopus because the octopus is not a fellow language user. But your fellow language users are other people in your community with whom you'll talk about the octopus. And you'll talk about the octopus and together you'll arrive at some consensus, hopefully, about whether you want to talk about it in terms of consciousness. And that's going to be all to do with like observing its behavior, listening to other people's accounts of being with octopuses. And critically, maybe listening to also what scientists have discovered when they look inside octopus brains and they perform behavioral experiments. And, you know, that's all, again, is public. That's all is grist of the mill of settling on a kind of the way we talk about these strange creatures. Yeah, I mean, I guess for the language, there's two parts to this. So the language game, first of all, it doesn't have to be spoken words, it's improvisation of any kind, it could be gestures, it could be all sorts, it's just behavior. Right, okay. And then the interesting thing with the octopus is we might not be interacting with them interactively. We might be non interactively observing them as agents interacting with each other, playing their own language game, but we can still ascribe some kind of measure of and I actually think what we're measuring here is agency and agency and moral status, I think are pretty much one to one. So when we see them playing the language game, we start to think of them as agents, therefore they have moral status. Yeah, I mean, I certainly think that by observing behavior, then we may similarly, you know, start to ascribe consciousness to other creatures. I mean, it's always much more persuasive if it's interactive, I think, than if it's simply observing behavior. Murray said that if a creature's brain is like ours, then there's grounds to suppose that its consciousness, its inner life is also like ours. He went on, if something is built very differently to us with a different architecture realized on a different substrate, then however human like its behavior, its consciousness might be very different to ours. Perhaps it would be a phenomenological zombie with no consciousness at all. Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of consciousness, experience, and sensation in terms of private subjectivity. He cited David Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction using his phraseology between the inner and the outer. In short, he said to a form of dualism, which is that subjectivity is an ontologically distinct feature of reality. Wittgenstein provided an antidote to this way of thinking in his remarks on private language, whose centerpiece in an argument to the effect that insofar as we can talk about our experience, they must have an outward public manifestation. For Wittgenstein, only of a living human being, what resembles or behaves like a living human being, one can say that it has sensations. It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism, particularly in its radical form as advocated by B. F. Skinner, posits that all psychological phenomena can be explained in terms of observable behavior and environmental stimuli without recourse to internal mental states. But behaviorism is often criticized for neglecting the subjective internal aspects of mental life. Wittgenstein argues against the notion of purely private language, where words refer to inner experiences known only to the speaker. He contends that for language to be meaningful, it must be grounded in publicly accessible criteria. So as Murray said in his article, Wittgenstein argued against dualism or the so-called impenetrable realm of the subject experience. Actually, he said that many folks who make the in-principle argument against AI often retreat into subjectivity arguments, as our recent guest Maria Santa Catarina did. Murray said that the difficulty here is that accepting the possibility of radically inscrutable consciousness seemingly re-admits dualistic propositions, that consciousness is not, so to speak, open to view, but inherently private. Yeah, so Aaron Sloman, who's a professor of computer science and artificial intelligence in Birmingham, Birmingham University, so he introduced the concept of the space of possible minds in an article in 1984. And the idea is that the collection of minds that could exist in our universe, that do exist in our universe and that could, is much larger than just human minds or even the minds of humans plus other animals. It encompasses extraterrestrial life that might exist out there and it encompasses artificial intelligence that we might create one day. So the whole space of possible minds is a very rich object philosophically speaking and merits our study. Yeah, so Wittgenstein's private language argument is the really the centerpiece of the philosophical investigations, which is the book that was published after his death, which really articulates his later phase of philosophy. And it's all about the idea that we have, or that we can talk about, private sensations. So things that are purely subjective and that only I as an individual can understand and know what they mean. So what red is for me and just, you know, internally for me. And so the private language remarks sort of undermine that very conception. So the basic idea is he imagines, he says, well, so what he means by private language is very important to kind of get this right. So he doesn't mean a language that I've invented and that is just something that nobody can understand just because I've invented it. It's the reason that it's a private language is because it's about something which only I can access subjectively, which is what red is like for me. So it's the idea that you can have a word for that completely internal thing that is just mine. So he says, imagine that I keep a diary and I keep a diary and every time I have this experience, a particular experience, then I write s in my diary to label that I've had that experience. So maybe I have this, I think I'm having this experience on a particular day and I write s and then a few days later, I think I'm having that experience again, so I write s again. Now, the question he asks is what possible criterion could there be for the correctness of that word? What would make it actually stand for anything meaningful, given that what really makes words meaningful is if they're understandable in a public setting, if they're understandable really to other people. So there can be no kind of criterion for correctness that anybody else could validate for this thing insofar as it stands for something that's completely private. So then there's a whole set of remarks that after he sets up this sort of little thought experiment that tell you the implications of it really. And there's one really, really key phrase where Wittgenstein is always engaging with an imaginary interlocutor, so an imaginary person who's arguing with him in the book. And so he's imagining this person says, but aren't you saying that the sensation itself is just a nothing? Aren't you a kind of behaviorist? You're just saying that it's a nothing. And his answer to this, well, I'm not saying it's a nothing and I'm not saying it's a something either. The point was only that a nothing would serve as well as a something about which nothing can be said. And that little kind of paradoxical sounding, weird sounding statement encapsulates something really, really, really profound. And I think when I first really kind of understood what he was getting at there, it had a really dramatic shift in the way I thought about consciousness, subjectivity. And to my mind, it is the thing that really undermines dualism. It's the most powerful way to undermine the dualistic intuitions that we have and that date back to Descartes and before Descartes that were articulated very well by Descartes. Many of my friends are fans of Wittgenstein, but they are also fans of subjectivity. So as you were just alluding to what Wittgenstein did was he created this kind of barrier between the inner and the outer. He said, you know, for things to be promoted into the language game for this emergent structure that we have, you know, when we memetically share all of these language constructions, that can only come from something observable. But it doesn't seem inconceivable to me that it could in principle come from something private. So for example, you might have a drugs experience and that's clearly ineffable, you can't find the words, but there are things that have some semantic overlap. So I experience red, you experience red, we both have different experiences, yet when we talk about them, some kind of overlapping category still emerges in the public space. Yes, absolutely. So so what emerges in the public space, that is what we can talk about. And that is that is by the way you've set up the experiment is by definition, not private, it's public. So of course, we can both talk, we can both point at something that's red and say, oh, look, look at that red. And you say, oh, yeah, isn't that isn't it beautiful? There, it's manifestly, we're talking, insofar as we're talking and successful in communicating with each other and agreeing with each other, then that's the element that is indeed public. But are we not sharing, you know, is language not a set of pointers to our simulation? So we're simulation sharing when we talk. And even though our simulations are different, is the pointer, does the pointer not form some kind of category over all of our simulations? Oh, well, there's a whole, you've introduced a whole load of terminology there, which, which, you know, I don't know what you mean exactly by shared simulation and so on. So I think in the context of a philosophical discussion, as soon as you introduce new, new bits of terminology like that, then often that's the point at which you're starting to go wrong, right, in philosophical discussions. In technical discussions, of course, you of course, we're going to introduce new terminology all the time. But, but, but that's the moment where often things are going when, as Wittgenstein would say, you're starting to take language on holiday and take it away from its normal usage. So I don't know what you mean by kind of a shared simulation, you'd have to tell me a little bit more about that idea before I could engage with that thought experiment, I think. Well, I mean, I'm schooled on, you know, the Karl Fristons of this world. And there's this whole thing about the Bayesian brain and perception as inference and so on. And, you know, the basic idea is that we, you know, our everyday experience is a hallucination, you know, we don't, what we experience isn't necessarily what is out there. And language is is a kind of pointed to those simulations. And they must be divergent, they presumably are divergent, yet miraculously, we can understand each other. Yeah, well, I think that so that so the Wittgensteinian point is that we understand each other in so far as in so far as we, you know, what we understand is what is shared, right? And anything outside of that is, you know, we by definition can't talk about. And the difficulty is that we have this strong inclination to talk as if there is this thing that's not shared. I mean, what really fascinates me is that understanding it's not a binary, there's a spectrum, and we delude ourselves that we understand things deeper than we do, because it goes into the realm of subjectivity. So when I understand something, my brain is invoking all of this rich subject of experience. And I'm probably taking my understanding into a domain which is beyond which that you understood. And perhaps this is just something we willfully do all of the time. So what do you mean exactly by invoke my brain is invoking all this subject of experience? What do you what are you what are you getting out there? Well, so we talk about, as you say, the language game is based around public information. So there is a kind of cultural level, a lowest common denominator of understanding. But when we understand cultural artifacts, we further invoke our own subjective experiences. So for example, when I laugh, I have the experience of laughter, this phenomenal experience. And this is clearly a form of understanding, it's a subjective form of understanding. And when someone else laughs, I feel that we are sharing this ontology, right, we're sharing it, but we can't possibly be. Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're sharing an ontology when you're just talking about an everyday experience of laughing together, which we can talk about without any kind of difficulty, and without raising any kind of philosophical problems, just by saying, Well, you know, we both heard that that that joke, and we were both, you know, on the floor and laughing. It was so funny. It was an excellent joke, right? We can talk about that in everyday terms. And and there are no problems. There are no philosophical problems. But as soon as you start, start, you know, getting philosophical, and you start talking about that, you know, what was it? What was your phrase? There's something about subject sheds about subjective ontology or something. Yeah, you're introducing all of this kind of technical terminology. And that's that whole, that's a whole layer of confusion on top of our ordinary everyday ways of talking about these things, which are unproblematic. Okay, but then there's the anthropomorphic lens. So you're a human, we both laugh, the behavior of laughing is publicly observable. Therefore, we have the same experience, because we have the same behavior. Well, it depends what you mean by by understand here. So so so for sure, you know, it is a fairly common form of speech to say, to say, Oh, well, you know, you can never understand what it was like to give birth, because you're a man, you know, and this is of course, this is a normal way of expressing oneself. And again, that's that sort of unproblematic. So there's there is a sense in which, you know, in which that's that's undoubtedly true. But the problems arise when you when you start to, to, to think that this, that what underlies this difference in understanding or the one underlies that way of talking is is is some kind of, you know, inner private realm that is, you know, that is index that is that is metaphysically distinct from from the rest of reality. When we share these pointers or these symbols or whatever, structure still emerges, we still feel that we have a shared understanding. And that understanding can probably be factorized into a public component and a private component. I don't think that's kooky to say that. Well, I see, you see, you're very keen to say, well, it turns a little bit what you mean by a private component there, right? So if you really mean, you know, sort of metaphysically inaccessibly, private and subjective, then then I think, then I think it's not appropriate to speak of dividing things into this private and public component. So that's where that's where things start to go wrong. And moreover, you insist that you're not a dualist, right? But I think your inclination to make that division shows that you have dualistic inclinations, as we all do. So people who are denying that they're dualists, they're denying this that little seed of dualism that I think is in all of us. And that is part of our part of the way we, we, we, you know, we think and the part of the way you naturally go when you start to do philosophy. And so it's all, I think it's all very well to say, oh, you know, I'm a materialist and I don't, but then when you, when you start to kind of probe and you start to discover the puzzlement that these things give rise to, then that exposes a bit of latent dualism there. Now overcoming that latent dualism, that is the real challenge that Wittgenstein confronts. Well, I love the challenge. So the way I see it is there is, there's a ladder. So at the top, you have an experience which is ineffable. And then one step down, you have an experience which is inconceivable, which is Naples argument. And then the, you know, if you really go down the ladder, then you get into this metaphysical dualism. So I guess I'm somewhere between the first step and the second step. So I think if I have a certain type of experience, I simply don't find the words, I can't communicate it to you. But if you put probes in my brain or something like that, I'm sure that could conceivably be a way of measuring it. Yes. Yeah. So, so, so this is really important. So for me, what counts as public is not just behavior, but it's also whatever scientists we can discover. So that, so, so if we poke around in people's brains and we do EEG recordings and FLRI recordings and anything else that we can imagine. And then I as a scientist can see this stuff and use a scientist and our fellow scientists will see, see that. That's public too. So that's in the, for the purposes of this discussion, of this philosophical discussion, that's all in the public realm. It's not metaphysically hidden. You can, you can, and all of that can feed into the way we talk about consciousness. And especially if we're talking about exotic entities, then, then all of that can feed into the way our language adapts to, to, to, to, to our encountering them. Yeah. So I think it's fascinating to decompose as you just did what people mean by subjectivity. So of course, some people like David Chalmers, they argue that there is a little bit extra. So there's, you know, behavior function and dynamics. And then there's that, you know, little bit extra, which is not observable in any scientific way. And I think, you know, it's fair to say a lot of people when they talk about subjectivity, they're not talking about the little bit extra. But when we do get to the little bit extra, I completely agree with you, we've got a big problem. Yeah. Yeah, I think we have got a big problem because, because of our natural, you know, dualistic tendencies to, it's very, very difficult to think that, that, that, you know, if I experience a pain, that, that, that there isn't something about that that is just purely minor, that you couldn't, you know, the outside world, that other people can never really, you know, experience it. But that's, it's having that thought, that's this moment that you kind of go wrong, but it's natural path to go down. It's really, really hard to avoid it. And, and that's where I think Sylvitkenstein's remarks, they, they provide a whole way of, of trying to reorient your whole way of thinking. And, and, and if you sort of really kind of grasp them, it sort of flips your whole world around, it flips your whole way of thinking around. So it's so that the, this whole way of talking and thinking becomes wrong. So it's so that so very often the strategy when you're dealing with this is somebody throws out this thought at you, like you've been throwing out various thoughts at me about, and, and, and often buried in the way those thoughts are framed is the problem. So, so the, the problem is the very expression of those thoughts. And you have to take a step back and say, hang on a minute, you know, you made this funny move, you introduced this funny bit of language, you introduced this funny way of expressing things. And that's, that's when that's the, the, Victor Stein has this phrase that is, that's where the conjuring trick happens is where you, the point that you don't notice is where the conjuring trick happens. So, so, so it's kind of, so often you have to take, take a step back and you have to sort of say, hang on a minute, I don't accept that way of talking that you've just suddenly introduced, which is going down a philosophical garden path. Yes, and I completely agree. So, so that is, that is a form of dualism, you know, when, when we resort to that little bit extra. And I'm quite interested in this actually, because people like Chalmers, I don't think he likes the term dualist, I think it's a property dualist, but he does talk about the philosophical zombie, which is a thought experiment of something which has all of the behavior of us, but is lacking in conscious experience, which gives rise to this idea that it's almost a kind of epiphenomenon or it's something which, you know, almost you're asking the question, well, well, what's it doing if it's not affecting anything? And when I read your conscious Exotica article, I had a similar thought actually, because you showed this linear correlation between, you know, human likeness and consciousness. And then you gave examples of algorithms, you know, like AlphaGo, for example, and they didn't need the consciousness. And that again raises the question of, what is the cash value of consciousness? When we use, when we're using the word consciousness, then often we are using it in the context of certain, you know, of certain behavioral behavior and behavioral inclinations, and we use it in the context of other humans and other animals. And there's a whole, I mean, for a start, the word consciousness is actually, you know, it's a multifaceted concept that it's alluding to many things. And one of the things that it is alluding to is our ability to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice the chair, that's why I bumped into it or something. And, or, you know, I didn't see, you know, that there was a desk over there that might have had something interesting inside it, if you opened it up. And so we talk about our awareness of the world. And we're at the same time, we're talking about an aspect of consciousness, and we're talking about a whole load of behavioral dispositions and capabilities. And so these things are very much, you know, are very much related to each other in our everyday speech. So then the question arises, though, are they dissociable? And so now it's very important that it's not like I think there's, that consciousness is some metaphysical thing whose essence is out there to be discovered. It's just, it's a concept that we invent and a word that we use to describe the world around us and our place in it and each other and so on. And so, so, so then, you know, then the question is, are there things that we might create or imagine, where we'd want to use the one concept, we want to use the one set of words and not use the other. And that is what the question comes down to. So in the case of something like AlphaGo, then I think, you know, we're all kind of agree that it's actually there's a kind of cognition going on there, there's a kind of reasoning going on in AlphaGo. There's certainly a lot of kind of cleverness, there's a kind of intelligence, there's even, if we're thinking about move 37, a kind of creativity. So we're willing to use all of those words, but nobody is going to suggest that AlphaGo is conscious. So there we can see that they're under certain conditions, the concepts are dissociable. But nevertheless, there's a strong relationship between the two, because if we think about animals, then often we are going to, we're going to use their cognitive abilities as manifest in their sophisticated behavior. We're going to use that as a proxy for sometimes whether we want to talk about them in terms of consciousness. So sometimes, in our usage, we're going to bundle the things together, and sometimes we're not. But this is all just a matter of, it's a kind of a practical matter of how we use language and how it's usefully deployed, how language is usefully deployed. And it's not about discovering some metaphysical entity that's out there, which is what conscious, the word consciousness denotes. Demis Esalvis recently spoke about this ladder of creativity and of course, inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad I had him on the podcast actually. He's a huge hero of mine, and I believe that a hero of yours but he coined this term the intentional stance. And what's interesting is he was using it to designate a rational agent, but actually it gets overloaded and I'm guilty of this. You overload it for lots of things, including even for things like consciousness. And maybe that's because of the correlates of cognition, these things are very closely related. But can you explain in your own articulation the intentional stance? Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance without necessarily embracing the whole of everything that Dan Dennett was talking about in that context. Because for him, there's a whole big philosophical position around it, but there's a very simple sense of the intentional stance that we can lift from Dan without necessarily buying into everything that he said. And it's simply to say that we often in everyday terms speak about artifacts and indeed animals, you know, other animals, as if they were rational agents that act on the basis of what they believe and what they want. And by talking about them in those ways, whether they really, whatever that means, do believe things or have desires, it's useful for explaining and understanding their behavior. So if we adopt the intentional stance, say, to use one of Dan Dennett's own examples towards a chess machine, a chess computer, chess program, or Go program, and we might say, oh, it advanced its queen because it wants to try and pin down my rook. And this is just a natural way of speaking. And if we use that way of speaking, then it's just good in every way because we can then discuss among ourselves what the machine is doing, we can explain what it's doing, we can predict what it's going to do. So that's taking the intentional stance. And it doesn't necessarily bring with it a belief that these concepts are literally applicable. Maybe they are, maybe they're not. But this is where it gets interesting. So I discussed abduction, actually, when I spoke with Dan, because it's very closely related. I know you studied reasoning for many years. And the way I see it, when we adopt the intentional stance, what we're doing is we're kind of building a set of variables to describe the behavior of the entity. And you were just making the argument from the lens of Wittgenstein that the behavior is the only thing. No, no, no, no, no, no, no, no. I definitely don't think that we mean meant by behavior as the only thing. But certainly when we're deploying psychological terms, I don't think that in any sense behavior is the only thing that determines how we deploy psychological terms. You're absolutely right. So there's still a massive amount of ambiguity. So when we perform abduction, we are creating a hypothesis and we're selecting out of an infinite set of possible hypotheses. But the behavior gives us all of the information. So it's almost like if we knew how to create the correct explanation, we wouldn't be missing anything just by observing the behavior. So what are we talking about now? We're talking about chess machines or animals? Or what are we talking about? What's the context for this thought? I guess it could work for both. So let's say I want to adopt the intentional stance for move 37. And I do this abduction. So I build this plan that the agent had. So the agent had this intention and it took this sequence of steps. And I'm using that as a hypothesis to explain the behavior. I'm adopting the intentional stance. But it's still highly ambiguous because I'm selecting out of an infinite set of possible hypotheses. Right. And in fact, in that particular case, it's almost certainly not the right way of thinking about it at all. Because unlike humans, who are when they're playing these games often do form plans. So if you're playing chess, you often do have a plan, I'm going to try and capture this area of the board and command this area of the board say. And so I'm going to move these pieces around to try and do that. And you might form a plan in terms of several moves, but ahead. But typically that's not the way, that's not really the way AlphaGo works. So talking about it making plans isn't really the right way of doing things. So it's interesting, actually, because the intentional stance, you know, maybe it's still white work, you can still talk about something forming plans maybe, but it's really not quite right in that case. When I say subjectivity, I'm not talking about metaphysical or dualism, but the intentional stance clearly is a form of subjectivity. And when we as a diverse collection of agents form our own intentional stances, it would seem to be quite a chaotic, weird and wonderful thing, yet it seems to work. There seems to be, by the way, an interesting thing here is the way we ascertain agency and culpability is based on the intentional stance. So you read a news article about someone being stabbed in Australia or something like that. And the news article was trying to give reasonable explanations. Oh, it was because he was in a cult or it was because he was religious or it was because, and this helps us kind of assign moral valence to what just happened. Yeah, yeah, absolutely. Yeah. You said something like, when we take the intentional stance that that is a form of subjectivity or something? Yes, would you agree with that? So I wouldn't put it quite that way. I'm not quite sure exactly what you mean by that, but taking the intentional stance is, I think it's just adopting a certain terminology and a certain vocabulary for describing the behavior of something. So I don't think we need to bring subjectivity into that at all, right? So I think maybe we're mixing up two completely different senses of the word subjectivity here, which is something we should be very careful about. So I think you mean subjectivity, you mean that you've just made your own choice between different hypotheses. And so it's subjective. Is that, is that what you mean there? Yes. So let's say, so for me as an observer, I might do some, let's call it probabilistic reasoning. And for me, the most reasonable, rational explanation is this. And it's a for me there. That's what I mean. That's what you're alluding to the subjectivity. Yeah. Yeah. Yeah. Okay. Yeah. Well, so, so, so your for me is, I think that's that, that the sense in which that subjective is a very, very different one from the topic that we were talking about earlier on, because I don't think there's anything philosophically problematic in, in, in saying that, you know, that I made my choice. And that's my preference and so on. And so, and somebody might say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically problematic about that, right? So, so they, but earlier on, we were talking about subjectivity, which like the big capital S and where it's alluding to kind of something whole metaphysical thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is dualism and, and lowercase subjectivity is for me. Yeah, it's for me. Okay. Yeah. Okay. Can you tell me about the risks of anthropomorphisation? Yeah. So I think so in the context of, of contemporary artificial intelligence in particular, then, then the danger of anthropomorphism, I think, is in, is in thinking that, that a system such as a large language model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't, that's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So, so in, so in both cases, I think we can go wrong, we can go wrong by, because they exhibit very human, like linguistic behaviour, we can just assume that they are going to be very human like in general in all of the rest of the behaviour that we encounter with them. But we often find that that's not the case. So we can find that at one moment, a large language model might make a ridiculously stupid mistake that no child would make. And, and, and, and then the next moment, it's saying something extraordinarily profound philosophically, or, or summarising some, in, you know, enormously difficult scientific article, you know, really accurately. So, which is, so these things are kind of superhuman powers, or translating something into four different languages all at once. And they're, they're sort of superhuman-ish capabilities. So it's not, so it can actually be more than better than human in some directions. And, but, but clearly very deficient in others, you know, with contemporary models that can make all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human, you know, because you can just, you can just misjudge it in many ways. That's one thing. There's another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive capabilities, if you like. But there are other problems with anthropomorphism. So if we see empathy there, where there isn't real empathy, then we may trust something where there's no real basis for trust. And so that's a problem as well. You know, we may form, people may form relationships with, with, you know, AI companions and social AI, where they're kind of fooling themselves into thinking that there's a basis for that in emotion, where there is in humans, and it's not there in, in, in contemporary AI. And I think that all those things are problematic. So things to do with trust, to do with friendship and empathy, and all of these, these things, I think, where we can go wrong in seeing, seeing them as too, you know, as more human-like than they really are. One of the issues I have with anthropomorphism is that people ascribe mental content when it's not there. And I think you are talking about literal anthropomorphism, which is that they see human-like qualities when they are not there. And to me, that, that's an important distinction, because I think if I understand you correctly, you, I mean, you're very known nonsense. You say that current large language models, they don't reason, they don't form beliefs, they don't have a sense. Oh, no, I didn't say exactly that. Oh, did you not? That's that's a broad, that's a much broader claim. So that's right. So I'm not sure I'd go so far as to say they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what my, my, my approach is to say that we should be very cautious in using those terms. So I'm, so those would all be examples of taking the intentional stance. If we were to use those bits of terminology to describe what, what a current, you know, chatbot or conversational AI was doing, we'd be taking the intentional stance, and it's perfectly reasonable to do that in very, very many cases. So I would know, and I would never make the blanket claim, they don't understand. I think that that's not quite right. I think rather, rather it's that, you know, sometimes it's appropriate to say, oh, yeah, it seems to understand very well what this big long article about nuclear physics was, was all about. And it summarized it really well. It really understood it. You know, somebody might come up, might say, you know, it really did seem to understand it. I think I wouldn't say that they were wrong in using that phrase there. So, but then on another occasion, you might find that it's, that it, for example, recently, people have been posing this, you know, this classic goat, cabbage, Fox problem where you've got a boat and you have to cross a river with a goat, and you can't have the, you know, more than two things in the boat at once, and you can't have the goat with the cabbage and all this kind of stuff. And so there's a, it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery, right? And people opposed, opposed to it to some large language models that said, okay, I've got a boat and a cabbage and I need to get the cabbage to the other side of the river. How do I do it? And the large language model models, several of them just start to come up with this totally baroque solution that involves going backwards and forwards over there. And sometimes they invent goats that aren't even in the, and that's because they've overfitted or they're kind of like connected with this classic problem. And, and so no child would make this stupid, stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously just didn't understand what the, you know, and of course, it's quite right to say didn't understand in that case. And the anthropomorphism, it comes about when you think that it understands when we understand and doesn't understand when we don't understand. The reality is that sometimes it understands when we understand and sometimes it won't and some, it's all mixed up, right? So the mistake is to think that it is like us. I could, yeah, man, that was beautifully articulated. So it's, it's, it's the mistake of thinking there is an alignment both in how the machines think and where they make mistakes. But let's unpick this a little bit because you were saying it's perfectly reasonable to take the intentional stance when the thing does the thing correctly, even though it thinks differently to us. And that's absolutely fine. But I love thinking about these things theoretically. And it's, it's delicious talking to you because you have a background in symbolic AI. You were, I'm sure around in the days of photo and pollution with their connectionist critique. And, and even now there are clear examples of language models not being able to do negation. Oh, yeah. And we know they're not Turing machines. You know, we can make some strong theoretical statements that they are limited in reasoning. I agree with you that it's reasonable to say they understand in certain circumstances. But, but, but where I want to get to is, okay, so we agree that language models cannot perform certain types of reasoning that we can. Yeah. So I think we could, so I think we need to take each of these concepts individually. So, we dealt a little bit just now with understanding reasoning as a whole separate thing. So, and, again, this is all because, you know, they're not like us. So we have to deal with these things individually. We can't just blanket say, oh, they don't understand, they don't reason, they don't, or they do understand, they do reason. It's not like that. It's, you have to take each of these concepts separately. So in the case of reasoning, then clearly today's large language models, you know, do struggle very often with, with, with reasoning problems. Now, this is a kind of open research problem. And people are making a lot of progress in improving their ability to solve reasoning problems. Now, what the right approach to that is, you know, is an open research question. Maybe it's just you throw more training data at it with, with, with, that includes lots of reasoning problems. And then eventually you get sufficient generalization there. And it's not totally clear that that will work, but maybe it will. Maybe it's to embed your, your, or, or, or to surround it to include it in your, in your system, not just the large language model, but making kind of external calls to, to say a planner or some kind of external reasoning system. And you bring that in and you incorporate, you make something that's kind of a hybrid that uses that, that kind of more symbolic approach. So you might do that. Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced this selection inference framework where you basically, you treat the large language model as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes calls to the, to, to, to, to the, to the module in a kind of algorithm that does a number of sequence of reasoning steps. So you have a kind of outer algorithm. So there's lots of ways of trying to tackle that problem. But yeah, just your basic large, take your basic large language model today, as they are at the moment, and you put it in a chat interface, it's easy to find reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host, Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or achieve a goal. And he cites Claude Shannon, by the way, he said, Claude, Claude Shannon said, we may have knowledge of the past, but we cannot control it. We may control the future, but we have no knowledge of it. And science leverages control to gain knowledge, engineering leverages knowledge to gain control. And reasoning is the effective computation in both. You know, maybe just in your own articulation, because we can cite examples of things like abduction, which we've studied in great detail. And it feels like at the moment, even if we do farm out to Turing machine algorithms, in the days of symbolic AI, that was an intractable problem, because we've got this infinity, right? And it still seems to me that there are some problems, which there is no easy answer to. So I have a, I feel that you're alluding to Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean, I think that sort of abductive problems are where we are looking for an explanation for something. So I think you'll find that there's a large collection of abduction problems that you could just present to today's large language models, and they would do quite well at them, you know. And at the same time, I'm sure you wouldn't be too difficult to find problems, especially if they involve many steps where it will go wrong. Because doing multi-step, it's the multiple steps that really are where today's large language models are a bit weak. Again, it's an open research question. People are working on all that kind of thing. So if there are multiple steps, and step n is dependent on step n minus one, and it's inherently, they're inherently very computational sorts of things in that sense. So large language models are a bit weak at that, for sure. And we can introduce things like chain of thought and so on to try and improve that. But they're sort of a limited success. But my feeling is that those are not the things where which large language models themselves are inherently strong at. And you should probably make use of other tools in order to kind of boost reasoning capabilities. Like I listed some of them. So somebody's making external calls to reasoning, dedicated reasoning components. Sometimes it's embedded in the large language model in a reasoning algorithm itself. So you can go either way, you can either take the large language model, put reasoning things inside it as it were, so it makes external calls, or you do it the other way around, you can have a reasoning algorithm and make the large language model component of the reasoning algorithm. Yes, I suppose it's a similar thing to the creativity that there's a kind of creative or inventive abduction, and then there's colloquial abductive interpolation. And the remarkable thing is just how structured and predictable our world is and how far you can get with the colloquial predictive abductive. Yes, yeah. I mean, this way you speak of colloquial sort of abduction, which is the kind of thing that we can all and the person on the street could do if you have some slightly odd situation and you say, why is this man in the middle of the road with a policeman's hat on or something? And you say, well, because there's maybe there's been an accident or something. So we do all this kind of thing on an everyday basis. It's a little bit of abduction. It's what you call colloquial abduction, I think. But large language models think you find a pretty good at that kind of thing these days. But if you have something that's really complex and has a load of steps to it, the kind of thing that humans would struggle at, then very often large language models are going to struggle at those things too. Can you describe the Turing test? The Turing test? Yes. Okay. So I'll describe the Turing test as it's popularly conceived because there are new answers in Turing's original paper. But then again, he didn't call it the Turing test. So the Turing test as it's popularly conceived involves having a human judge and the human judge is interacting with two things. One of them is a human and the other is a computer system. And the interaction is entirely through language, through a keyboard and a screen say, or a teletype, if you like, in Turing's day. And the idea is that the human judge has a conversation with these two things. And the question is, can the human judge tell which is the machine and which is the human? And if the judge can't tell which is which, then the machine passes the Turing test. Do you think it's a good measure of intelligence? I think it's a pretty rubbish measure of intelligence. I mean, I think it's a very useful philosophical thought experiment and starting point for conversation on this. But the trouble is, it's very easy to game. Well, there's a number of problems with that, which people have been writing about for years, by the way. So there are a number of problems with that. So one is that it's easy to game, in a sense, because there's a temptation to make something to pass the Turing test, you make something that has all kinds of strange human ticks and peculiarities, and then it seems human. And so you can fool the judge that way by making it just a bit eccentric, which is obviously nothing to do with intelligence at all. So that's one thing. And then another thing is that the domain of the test is purely linguistic. So you're not testing anything to do with the sorts of intelligence that you get in a non-human animal, say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to navigate the ordinary, everyday world and survive in it. But none of those kinds of intelligence are tested by the Turing test. So you were the scientific advisor on the film Ex Machina. And I'm not sure whether it's Machina or Machina. It is Machina, yeah. I was speaking with Irina Rishan and she said, Ex Machina. But anyway, she's right. Yes, she is. I digress. But there was a special type of Turing test in that film. Can you explain that? Well, indeed it wasn't. It's not the Turing test. So there's a point in the film, I assume, that people are vaguely familiar with the setup of the film. But there's a point in the film where Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava. And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know whether the thing that's being tested, whether you don't know whether it's a machine or a human. That's the point of the test. The point, Nathan says, is to show you that she's a robot and see if you still think she's conscious. So there's a number of ways in which this is very different from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence. And those are not the same thing. And secondly, the point, as he says, is to be persuaded that the artifact is conscious, even though you know that it's not human, not biological. So you know that it's an AI system, and you still think that it's conscious. In that case, it passes this test. So this test, I call the Garland test after Alex Garland, who is the writer and director of X Machina, quite different from the Turing test. I think that those lines in the film were actually really brilliant lines and really clever lines from Alex in the script. And when I first saw the script, which was long before it was filmed, and that bit was in there, and I put spot on next to those lines in the script, because I thought it was such a very good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed think that Ava is conscious and thinks of her that way. What that really means is that he comes to treat her as a fellow conscious creature. And we see that in the film because he wants to help her escape. And just as a kind of thought experiment, maybe it's another conceivability thing, but it does seem conceivable what less consciousness would be like, but it seems less conceivable what more consciousness would be like. Well, so in both cases, I think it's all about exercising our imaginations actually. And I think exercising our imaginations is perfectly legitimate in philosophical discussions. So in fact, in my newer paper, Simulacra as Conscious Exotica, I think I say that I advocate doing philosophy with the detachment of an anthropologist and the imagination of a science fiction writer. So I think that's something that I aspire to do. So we can carry out all kinds of imaginative exercises to describe exotic entities in a science fiction like way. So we can describe an exotic entity and we can describe all kinds of strange behaviors. And that's sort of as far as we can get really, I think we could we could we can imagine all kinds of strange behavior, and we can imagine scientists studying those kinds of strange behavior and what underlies them as well. And so we can imagine all of those things. And then and then we can also imagine how we would talk about those things. So imagining how we as a kind of community and how the scientists and the philosophers would talk about, about these imagined entities is all kind of part of it. So I think we can do all of that. I think that is a kind of doing philosophy. And I'm just coming back to the Turing test one last time. I read an interesting take on Twitter recently that we've been thinking about the Turing test or wrong. There seems to be a subset of people who it's almost like the Eliza effect. They see something they want to see something. And it's almost like the test is actually testing the humans rather than the intelligence. Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again, I think we should distinguish consciousness and and intelligence in this in this regard, you know, although there are relations between the two or consciousness and cognition. So what just one thing I wanted to say about the about the Turing test is I do feel actually that today's large language models kind of pass the spirit of the Turing test. So so we defined the Turing test earlier on or the or the kind of the popular conception of the Turing test earlier on. And as I remarked, you can kind of game it and you know, and there's certain sort of problems with it. But notwithstanding that, I think that actually today's large language models pass the spirit of the Turing test, I feel. So they pass the spirit of the Turing test because they do really have human level conversational skills, I feel. So I might my feeling is that if Turing were alive today and were presented with Gemini or ChatGPT or Claude III, he would say, yeah, that's what I had in mind, you know, you've done it, that's that's it's kind of passed. Interesting. And you also distinguished, you know, the the imitation game is defined by Turing and the the colloquial popular conception is that there's no adjudicator, it's just, you know, a person and an intelligent machine. What is what is the kind of the main difference there? What I mean, meant by the popular conception, I think was that maybe popular conception isn't quite right. There's sort of contemporary version of it is that is used in academic discussion, in fact, is what I meant. That's what I meant by popular. So I so there I'm imagining, yeah, there is a human adjudicator there. But the difference is that in so Turing, the way Turing sets up the test is is, you know, he has some specific specificities about, you know, the number of minutes you should, you know, you spend interacting with it. And then also he sets it up in the context of this game where party game where the aim is to try and work out whether which you've got a man and a woman that somebody is is is convert conversing with, you know, via paper, and they don't know which is the man and which is the woman, they have to guess which is which. And that's the way the thing is set up in the original paper is by analogy with that. So there's all so so there's all kinds of, you know, peculiarities in the original paper, if you're a Turing scholar that aren't aren't really quite relevant to the I think the kind of contemporary way that we think of the Turing test. But I also think that the sense in which we in which today systems pass the spirit of the Turing test, it's the reason it's only the spirit of the Turing test is because, of course, you very often can immediately tell that it's that it's an AI, not not least because it'll just tell you right away, right? If you just ask it, it will just say well as a large language model trained by Google or by, you know, open AI, so it's easy to tell which is which. So but but but but but nevertheless, I think that they have attained more or less human level language skills, more or less. And so I think I do think that Turing would would would say, you know, as I predicted, you know, yeah, we've got there. Now, I think Turing would be fascinated to see the weaknesses that are there and the strengths that are there. And, you know, there are many things that today's systems can do that are vastly more powerful than I think he anticipated in that paper in the 1950s. And then there are other, you know, there will be other weaknesses, I think that would come out that would surprise him as they've surprised us all in a way. I think I think many of us in the field are surprised to see something that can do so amazingly in certain respects, and yet have so many, you know, still so many weaknesses and others. Can you introduce Naegle's bat? So in 1974, Thomas Naegle published this paper called What is it like to be a bat? And the point of this paper was to draw attention to the fact, if it is a fact, that there are creatures that are very, very different to ourselves to humans, but that we assume have some kind of consciousness. We assume in his terminology, we assume that it's like something to be that creature. And he chose a bat because bats are very obviously very different to ourselves. They fly, they use sonar, they're pretty weird animals. And so the way the thinking is that, well, it's probably like something to be a bat, but what it's like is going to be very different from what it's like to be a human. And Naegle uses that example to get at a whole metaphysical idea, which again, I would say is pointing to a kind of dualism, to suggest that there's a whole realm of facts about subjectivity, which are outside of the purview of objective science, actually, but which nevertheless are part of reality. And so for him, I feel that it alludes to a sort of kind of dualistic way of thinking again, that there's this realm of facts about subjective things, and there's a realm of facts about objective things. I read Naegle's bat many years ago, his paper, and he was kind of saying, wouldn't it be great if we could move towards an objective phenomenology? And you are clear that he is pointing to dualism. He's not just saying that it's inconceivable in the sense that I couldn't imagine what the experience of a bat is like. It's just inconceivable. You're saying that he's actually making a dualism argument. Well, he would probably deny that, because nearly every philosopher will enthusiastically deny that they're dualists, but I see dualistic thinking all over the place, and I do think that this is an example of it. Yes, I mean, it's a similar thing with John Searle. I think he is adamant that he's not a dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another example of a popular thought experiment, which apparently people get wrong. My friend, Mark J. Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument. Yes, Mark has tenaciously clinging to the Chinese language, the Chinese rim argument. But I can't really see eye to eye with Mark, who I have enormous respect for on this particular subject, I'm afraid. Yes, Mark is a very good friend of mine, and maybe that's a rabbit hole, we won't go down. But I'm very amenable and convinced by some of Mark's arguments. But he's certainly a phenomenologist, which is adamant that he is a monowist. It's another example of someone who claims they're not a dualist, but you would say they probably are a dualist. Yeah, well, I don't know. We'd have to have Mark sitting here. I would have needed to have read one of his papers on this very recently to do that. So I'm not going to accuse him of anything, but if he was sitting here, we could have a discussion about it. Well, I asked him straight up, because I was trying to pin him down, and he said he is an idealist, a monowist idealist. And it might be instructive, actually, if you just to explain what do we mean by idealism? Right. Well, so especially if he's using the word monow there as well, then I guess that he is suggesting that while a physicalist thinks that there is only one substance in reality, and that's material reality. And so there is no such thing as a separate stuff of mind metaphysically. So for the physicalist, there is no dualism if they really, really think that, but the trouble is that as soon as you kind of probe, then often they have struggle to deal with dualistic intuitions about subjectivity. Now the idealist, on the other hand, also thinks that there's only one substance, but that substance is the substance of mind. So physical reality has to be a kind of construct out of this one substance, this one out of mind stuff. So that's a very different kind of, it's a different way of avoiding dualism, but it has its own problems about how do you explain account for science and the success of science and so on. I also interviewed Philip Goff in a beautiful studio recently, and he is a cosmo-psychist, but I think it's better just to use the word pan-psychist, but I think cosmo-psychist is where you have the teleology baked in. So rather than it being bottom up, it's kind of top down. There's some cosmic purpose to the universe, and the universe as a whole is made of, you know, the fundamental material of the universe is consciousness. And what's the relationship between that view and idealism? So I guess for the pan-psychist, so the pan-psychist is so far as I understand these philosophical positions. I mean, I shouldn't put myself forward as somebody who necessarily understands them in depth, and everybody who subscribes to these views has a slightly different version of them as well. But the pan-psychist certainly thinks that reality is composed of physical material substance, but that physical material substance irreducibly has a psychological dimension to it, a mental dimension to it. So every physical object has a little bit of consciousness in it, if you like, in some sense. I mean, I find it a very difficult view to articulate because I just find it so completely counterintuitive. So I can't really put the pan-psychist's hat on and express their point of view. You need a pan-psychist here to do that. Because to my mind, this is, you know, again with my bitkinstinian hat on, then I just think, well, how do we use words like consciousness? Well, we use the word consciousness and all of the related terms in the context of each other, of other human beings. And so, you know, and it's in the context of our being together in the world and your behaving in certain ways and our exchanging certain looks when we're doing things and that we understand each other as fellow conscious creatures. And so that's the context in which we use, you know, words like consciousness. And when I speak about, you know, you're not conscious, you're asleep. And, you know, it's all in the context of other humans that we use those words. So it's just ludicrously inapplicable to use that word in the context of, you know, I don't know, a brick or a toaster or an atom. It's just simply the words simply are not applicable in those contexts. Yes, it's so interesting because Philip told me that he grew up as a Christian. And I've had Richard Swinburne on as well. And he's got a book out about are we bodies or souls, you know, putting forward the case for substance dualism. And so there's the moving away from dualism thing, there's the teleology things. Because I think if you are of this frame of mind, you like to believe that there's some kind of grand purpose. And there's also the wanting to not wanting to be a monowist, basically. So you could perceive it as a form of mental gymnastics to say, okay, well, I don't want to be a substance dualist, but why don't we rearrange the structure somewhat so that consciousness comes first. And there's some kind of cosmic purpose. And we're building a worldview that still makes sense to me. Yeah, yeah. I mean, I personally, I think all of these positions involve a great deal of mental gymnastics. And, and I, you know, I reject any kind of ism. Or what I mean by that is I don't use that term to describe, you know, my views at all. So all of these isms are, you know, are misguided. And what we need to do is just to to dismantle the whole way of talking, which makes us, you know, makes us confused in the context of this kind of these kinds of issues. So that's what Wittgenstein is trying to do, as he puts it, to show the fly the way out of the bottle, right? This is his famous phrase. So the fly is the person who's ended up thinking all these philosophical thoughts by taking ordinary language into strange places, taking it on holiday. And, and so to show the fly the way out of the bottle is to just bring all of these concepts back to their ordinary everyday usage and to show thereby that you haven't really, you haven't lost anything, the puzzles evaporate. So that isn't an ism. That's a, that's, that's rather that's a kind of kind of critical methodology for just shifting the way that you think and talk all together. And final question on this, where do you sit on the kind of the teleology question? So one view is that we have a purpose. The the physicist would argue that it emerges, you know, from quantum field theory and a lot of sophisticated study of biology kind of build this intermediate view of teleonomy. Where do you sit on that kind of spectrum? I'll be honest with you, I don't think I sit anywhere on that spectrum. I think I think those are issues on which I don't have sufficient expertise to pronounce. So, so, you know, you just keep me, keep me on the on consciousness and cognition, you know, all this stuff is above my pay grade, you know. Professor Shanahan, it's been an absolute honor to have you on MLST. Thank you so much. I really appreciate it. Thank you. Thank you so much for having me. It's been fun.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.5600000000000005, "text": " You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017,", "tokens": [50364, 509, 458, 11, 286, 362, 341, 3035, 11, 6923, 4139, 2111, 310, 2262, 11, 294, 6549, 11, 293, 550, 286, 6869, 14895, 44, 471, 294, 6591, 11, 50692], "temperature": 0.0, "avg_logprob": -0.15117159611036798, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.11922145634889603}, {"id": 1, "seek": 0, "start": 6.5600000000000005, "end": 11.76, "text": " and at that point I'd been thinking and writing quite a bit about consciousness up to that point.", "tokens": [50692, 293, 412, 300, 935, 286, 1116, 668, 1953, 293, 3579, 1596, 257, 857, 466, 10081, 493, 281, 300, 935, 13, 50952], "temperature": 0.0, "avg_logprob": -0.15117159611036798, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.11922145634889603}, {"id": 2, "seek": 0, "start": 11.76, "end": 16.72, "text": " But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody", "tokens": [50952, 583, 550, 286, 1333, 295, 5936, 11, 570, 286, 1194, 286, 994, 380, 519, 309, 6576, 6854, 337, 2618, 51200], "temperature": 0.0, "avg_logprob": -0.15117159611036798, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.11922145634889603}, {"id": 3, "seek": 0, "start": 16.72, "end": 21.28, "text": " working in a corporation to be talking about consciousness, especially in the context of AI,", "tokens": [51200, 1364, 294, 257, 22197, 281, 312, 1417, 466, 10081, 11, 2318, 294, 264, 4319, 295, 7318, 11, 51428], "temperature": 0.0, "avg_logprob": -0.15117159611036798, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.11922145634889603}, {"id": 4, "seek": 0, "start": 21.28, "end": 25.28, "text": " because it might sound like, you know, we're trying to build conscious AI, which I don't think is a", "tokens": [51428, 570, 309, 1062, 1626, 411, 11, 291, 458, 11, 321, 434, 1382, 281, 1322, 6648, 7318, 11, 597, 286, 500, 380, 519, 307, 257, 51628], "temperature": 0.0, "avg_logprob": -0.15117159611036798, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.11922145634889603}, {"id": 5, "seek": 2528, "start": 25.36, "end": 30.48, "text": " good look, or a particularly good project, I'd say. The thing is, with today's generation of", "tokens": [50368, 665, 574, 11, 420, 257, 4098, 665, 1716, 11, 286, 1116, 584, 13, 440, 551, 307, 11, 365, 965, 311, 5125, 295, 50624], "temperature": 0.0, "avg_logprob": -0.13770652910984985, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.2018885314464569}, {"id": 6, "seek": 2528, "start": 30.48, "end": 35.92, "text": " large language models, I think it's becoming increasingly difficult to avoid the subject,", "tokens": [50624, 2416, 2856, 5245, 11, 286, 519, 309, 311, 5617, 12980, 2252, 281, 5042, 264, 3983, 11, 50896], "temperature": 0.0, "avg_logprob": -0.13770652910984985, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.2018885314464569}, {"id": 7, "seek": 2528, "start": 35.92, "end": 40.8, "text": " because people, whether we like it or not, will ascribe consciousness to the things that they're", "tokens": [50896, 570, 561, 11, 1968, 321, 411, 309, 420, 406, 11, 486, 382, 8056, 10081, 281, 264, 721, 300, 436, 434, 51140], "temperature": 0.0, "avg_logprob": -0.13770652910984985, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.2018885314464569}, {"id": 8, "seek": 2528, "start": 40.8, "end": 46.56, "text": " interacting with. And we see this left, right and centre. Even people who know exactly how they work", "tokens": [51140, 18017, 365, 13, 400, 321, 536, 341, 1411, 11, 558, 293, 10093, 13, 2754, 561, 567, 458, 2293, 577, 436, 589, 51428], "temperature": 0.0, "avg_logprob": -0.13770652910984985, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.2018885314464569}, {"id": 9, "seek": 2528, "start": 47.52, "end": 51.92, "text": " say things like, well, I think their large language models are a little bit conscious,", "tokens": [51476, 584, 721, 411, 11, 731, 11, 286, 519, 641, 2416, 2856, 5245, 366, 257, 707, 857, 6648, 11, 51696], "temperature": 0.0, "avg_logprob": -0.13770652910984985, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.2018885314464569}, {"id": 10, "seek": 5192, "start": 52.0, "end": 60.08, "text": " Ilya Tsutskava said, and we had the Google engineer who ascribed consciousness to one of our models,", "tokens": [50368, 286, 45106, 16518, 3648, 74, 4061, 848, 11, 293, 321, 632, 264, 3329, 11403, 567, 382, 18732, 10081, 281, 472, 295, 527, 5245, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1448645756162446, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.014681747183203697}, {"id": 11, "seek": 5192, "start": 60.08, "end": 65.36, "text": " and I think we're going to see this more and more and more. And so whether or not, you know,", "tokens": [50772, 293, 286, 519, 321, 434, 516, 281, 536, 341, 544, 293, 544, 293, 544, 13, 400, 370, 1968, 420, 406, 11, 291, 458, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1448645756162446, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.014681747183203697}, {"id": 12, "seek": 5192, "start": 65.36, "end": 70.8, "text": " I think it is the right term, it is appropriate to talk about them in terms of consciousness,", "tokens": [51036, 286, 519, 309, 307, 264, 558, 1433, 11, 309, 307, 6854, 281, 751, 466, 552, 294, 2115, 295, 10081, 11, 51308], "temperature": 0.0, "avg_logprob": -0.1448645756162446, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.014681747183203697}, {"id": 13, "seek": 5192, "start": 70.8, "end": 74.88, "text": " it's going to happen anyway. So I think it's really important to actually think through", "tokens": [51308, 309, 311, 516, 281, 1051, 4033, 13, 407, 286, 519, 309, 311, 534, 1021, 281, 767, 519, 807, 51512], "temperature": 0.0, "avg_logprob": -0.1448645756162446, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.014681747183203697}, {"id": 14, "seek": 5192, "start": 74.88, "end": 81.04, "text": " these issues and think, well, what do we mean when we use the word consciousness,", "tokens": [51512, 613, 2663, 293, 519, 11, 731, 11, 437, 360, 321, 914, 562, 321, 764, 264, 1349, 10081, 11, 51820], "temperature": 0.0, "avg_logprob": -0.1448645756162446, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.014681747183203697}, {"id": 15, "seek": 8104, "start": 81.04, "end": 87.44000000000001, "text": " and how do we apply it to exotic cases? And this is really, really important.", "tokens": [50364, 293, 577, 360, 321, 3079, 309, 281, 27063, 3331, 30, 400, 341, 307, 534, 11, 534, 1021, 13, 50684], "temperature": 0.0, "avg_logprob": -0.10361104352133614, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.0003836042887996882}, {"id": 16, "seek": 8104, "start": 87.44000000000001, "end": 93.52000000000001, "text": " How might our language change to accommodate these exotic and strange things that have come", "tokens": [50684, 1012, 1062, 527, 2856, 1319, 281, 21410, 613, 27063, 293, 5861, 721, 300, 362, 808, 50988], "temperature": 0.0, "avg_logprob": -0.10361104352133614, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.0003836042887996882}, {"id": 17, "seek": 8104, "start": 93.52000000000001, "end": 99.36000000000001, "text": " into our lives? What goes through your mind when you speak with a language model? Who is it that", "tokens": [50988, 666, 527, 2909, 30, 708, 1709, 807, 428, 1575, 562, 291, 1710, 365, 257, 2856, 2316, 30, 2102, 307, 309, 300, 51280], "temperature": 0.0, "avg_logprob": -0.10361104352133614, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.0003836042887996882}, {"id": 18, "seek": 8104, "start": 99.36000000000001, "end": 105.84, "text": " you think you're talking to? Do you anthropomorphise them? Now, Janice from Less Wrong a couple of", "tokens": [51280, 291, 519, 291, 434, 1417, 281, 30, 1144, 291, 22727, 32702, 908, 552, 30, 823, 11, 4956, 573, 490, 18649, 28150, 257, 1916, 295, 51604], "temperature": 0.0, "avg_logprob": -0.10361104352133614, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.0003836042887996882}, {"id": 19, "seek": 8104, "start": 105.84, "end": 110.88000000000001, "text": " years ago, he put out an article called Simulators. And the basic idea is that a language model is", "tokens": [51604, 924, 2057, 11, 415, 829, 484, 364, 7222, 1219, 3998, 39265, 13, 400, 264, 3875, 1558, 307, 300, 257, 2856, 2316, 307, 51856], "temperature": 0.0, "avg_logprob": -0.10361104352133614, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.0003836042887996882}, {"id": 20, "seek": 11088, "start": 110.88, "end": 118.47999999999999, "text": " like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise.", "tokens": [50364, 411, 257, 16575, 3479, 11, 10501, 46931, 295, 3090, 4150, 11, 597, 321, 486, 2277, 22727, 32702, 908, 13, 50744], "temperature": 0.0, "avg_logprob": -0.10476566083503491, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0036464808508753777}, {"id": 21, "seek": 11088, "start": 118.47999999999999, "end": 124.47999999999999, "text": " We think of them as humans. They're in perfect copies. They don't capture the essence. They are", "tokens": [50744, 492, 519, 295, 552, 382, 6255, 13, 814, 434, 294, 2176, 14341, 13, 814, 500, 380, 7983, 264, 12801, 13, 814, 366, 51044], "temperature": 0.0, "avg_logprob": -0.10476566083503491, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0036464808508753777}, {"id": 22, "seek": 11088, "start": 124.47999999999999, "end": 130.72, "text": " glitchy, right? And actually, they wear masks. You know the Shogoff theory of language models,", "tokens": [51044, 23552, 88, 11, 558, 30, 400, 767, 11, 436, 3728, 11830, 13, 509, 458, 264, 1160, 664, 4506, 5261, 295, 2856, 5245, 11, 51356], "temperature": 0.0, "avg_logprob": -0.10476566083503491, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0036464808508753777}, {"id": 23, "seek": 11088, "start": 130.72, "end": 135.76, "text": " where there's, you know, this big gnarly Shogoff, and then we do RLHF, and it's a smiley face on", "tokens": [51356, 689, 456, 311, 11, 291, 458, 11, 341, 955, 290, 20062, 356, 1160, 664, 4506, 11, 293, 550, 321, 360, 497, 43, 39, 37, 11, 293, 309, 311, 257, 7563, 88, 1851, 322, 51608], "temperature": 0.0, "avg_logprob": -0.10476566083503491, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0036464808508753777}, {"id": 24, "seek": 11088, "start": 135.76, "end": 140.64, "text": " the top. Well, that's the human mask, which we anthropomorphise. But we don't think enough", "tokens": [51608, 264, 1192, 13, 1042, 11, 300, 311, 264, 1952, 6094, 11, 597, 321, 22727, 32702, 908, 13, 583, 321, 500, 380, 519, 1547, 51852], "temperature": 0.0, "avg_logprob": -0.10476566083503491, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0036464808508753777}, {"id": 25, "seek": 14064, "start": 140.72, "end": 147.27999999999997, "text": " about what lies behind the mask. Do you know what lies behind the mask? It's a monster.", "tokens": [50368, 466, 437, 9134, 2261, 264, 6094, 13, 1144, 291, 458, 437, 9134, 2261, 264, 6094, 30, 467, 311, 257, 10090, 13, 50696], "temperature": 0.0, "avg_logprob": -0.08666199826179667, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0004277111147530377}, {"id": 26, "seek": 14064, "start": 149.27999999999997, "end": 156.88, "text": " The danger of anthropomorphism, I think, is in thinking that a system such as a large language", "tokens": [50796, 440, 4330, 295, 22727, 32702, 1434, 11, 286, 519, 11, 307, 294, 1953, 300, 257, 1185, 1270, 382, 257, 2416, 2856, 51176], "temperature": 0.0, "avg_logprob": -0.08666199826179667, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0004277111147530377}, {"id": 27, "seek": 14064, "start": 156.88, "end": 161.83999999999997, "text": " model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't.", "tokens": [51176, 2316, 11, 291, 458, 11, 257, 5081, 18870, 420, 746, 11, 1953, 300, 309, 575, 10862, 293, 300, 309, 1177, 380, 13, 51424], "temperature": 0.0, "avg_logprob": -0.08666199826179667, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0004277111147530377}, {"id": 28, "seek": 14064, "start": 162.39999999999998, "end": 167.04, "text": " It's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities", "tokens": [51452, 467, 311, 382, 2199, 382, 300, 13, 5135, 11, 309, 311, 611, 1953, 11, 4317, 11, 300, 309, 31132, 10862, 51684], "temperature": 0.0, "avg_logprob": -0.08666199826179667, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0004277111147530377}, {"id": 29, "seek": 16704, "start": 167.04, "end": 171.51999999999998, "text": " that it does. So in both cases, I think we can go wrong.", "tokens": [50364, 300, 309, 775, 13, 407, 294, 1293, 3331, 11, 286, 519, 321, 393, 352, 2085, 13, 50588], "temperature": 0.0, "avg_logprob": -0.10492170268091662, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00077296351082623}, {"id": 30, "seek": 16704, "start": 177.92, "end": 182.48, "text": " We can go wrong by, because they exhibit very human-like linguistic behaviour,", "tokens": [50908, 492, 393, 352, 2085, 538, 11, 570, 436, 20487, 588, 1952, 12, 4092, 43002, 17229, 11, 51136], "temperature": 0.0, "avg_logprob": -0.10492170268091662, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00077296351082623}, {"id": 31, "seek": 16704, "start": 182.48, "end": 187.68, "text": " we can just assume that they are going to be very human-like, in general, in all of the rest of", "tokens": [51136, 321, 393, 445, 6552, 300, 436, 366, 516, 281, 312, 588, 1952, 12, 4092, 11, 294, 2674, 11, 294, 439, 295, 264, 1472, 295, 51396], "temperature": 0.0, "avg_logprob": -0.10492170268091662, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00077296351082623}, {"id": 32, "seek": 16704, "start": 187.68, "end": 196.56, "text": " the behaviour that we encounter with them. We can find that, at one moment, a large language model", "tokens": [51396, 264, 17229, 300, 321, 8593, 365, 552, 13, 492, 393, 915, 300, 11, 412, 472, 1623, 11, 257, 2416, 2856, 2316, 51840], "temperature": 0.0, "avg_logprob": -0.10492170268091662, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00077296351082623}, {"id": 33, "seek": 19656, "start": 196.56, "end": 202.24, "text": " might make a ridiculously stupid mistake that no child would make. And in the next moment,", "tokens": [50364, 1062, 652, 257, 41358, 6631, 6146, 300, 572, 1440, 576, 652, 13, 400, 294, 264, 958, 1623, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09770642518997193, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.003500698832795024}, {"id": 34, "seek": 19656, "start": 202.24, "end": 205.52, "text": " it's saying something extraordinarily profound philosophically.", "tokens": [50648, 309, 311, 1566, 746, 34557, 14382, 14529, 984, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09770642518997193, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.003500698832795024}, {"id": 35, "seek": 19656, "start": 209.36, "end": 213.68, "text": " And because that's what I think is actually going to happen and what needs to happen. I think we need", "tokens": [51004, 400, 570, 300, 311, 437, 286, 519, 307, 767, 516, 281, 1051, 293, 437, 2203, 281, 1051, 13, 286, 519, 321, 643, 51220], "temperature": 0.0, "avg_logprob": -0.09770642518997193, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.003500698832795024}, {"id": 36, "seek": 19656, "start": 213.68, "end": 222.64000000000001, "text": " to find new ways of using the vocabulary we have, new forms of vocabulary. I've used the phrase", "tokens": [51220, 281, 915, 777, 2098, 295, 1228, 264, 19864, 321, 362, 11, 777, 6422, 295, 19864, 13, 286, 600, 1143, 264, 9535, 51668], "temperature": 0.0, "avg_logprob": -0.09770642518997193, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.003500698832795024}, {"id": 37, "seek": 22264, "start": 222.72, "end": 227.11999999999998, "text": " consciousness-adjacent language. So we need to find new ways of thinking and talking about", "tokens": [50368, 10081, 12, 345, 19586, 317, 2856, 13, 407, 321, 643, 281, 915, 777, 2098, 295, 1953, 293, 1417, 466, 50588], "temperature": 0.0, "avg_logprob": -0.12093664516102184, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.022592978551983833}, {"id": 38, "seek": 22264, "start": 227.11999999999998, "end": 235.04, "text": " these things to recognise the fact that they do exhibit behaviour, which we're inclined to talk", "tokens": [50588, 613, 721, 281, 23991, 264, 1186, 300, 436, 360, 20487, 17229, 11, 597, 321, 434, 28173, 281, 751, 50984], "temperature": 0.0, "avg_logprob": -0.12093664516102184, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.022592978551983833}, {"id": 39, "seek": 22264, "start": 235.04, "end": 239.6, "text": " of in terms of consciousness, and that, indeed, people are going to start to value as well.", "tokens": [50984, 295, 294, 2115, 295, 10081, 11, 293, 300, 11, 6451, 11, 561, 366, 516, 281, 722, 281, 2158, 382, 731, 13, 51212], "temperature": 0.0, "avg_logprob": -0.12093664516102184, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.022592978551983833}, {"id": 40, "seek": 22264, "start": 240.16, "end": 245.2, "text": " So I think we do need new forms of language, new forms of thinking to accommodate all of this.", "tokens": [51240, 407, 286, 519, 321, 360, 643, 777, 6422, 295, 2856, 11, 777, 6422, 295, 1953, 281, 21410, 439, 295, 341, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12093664516102184, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.022592978551983833}, {"id": 41, "seek": 22264, "start": 246.0, "end": 251.6, "text": " Yes, and our language is so adaptable that I think it's just a natural evolution.", "tokens": [51532, 1079, 11, 293, 527, 2856, 307, 370, 6231, 712, 300, 286, 519, 309, 311, 445, 257, 3303, 9303, 13, 51812], "temperature": 0.0, "avg_logprob": -0.12093664516102184, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.022592978551983833}, {"id": 42, "seek": 25160, "start": 251.6, "end": 256.15999999999997, "text": " When we have these new artefacts thrust into our lives, we will need to adapt our language.", "tokens": [50364, 1133, 321, 362, 613, 777, 29159, 69, 15295, 24030, 666, 527, 2909, 11, 321, 486, 643, 281, 6231, 527, 2856, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12078893302690864, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0019383867038413882}, {"id": 43, "seek": 25160, "start": 256.15999999999997, "end": 261.28, "text": " We will, but I think there'll be some disruption while people disagree about how to talk about", "tokens": [50592, 492, 486, 11, 457, 286, 519, 456, 603, 312, 512, 28751, 1339, 561, 14091, 466, 577, 281, 751, 466, 50848], "temperature": 0.0, "avg_logprob": -0.12078893302690864, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0019383867038413882}, {"id": 44, "seek": 25160, "start": 261.28, "end": 268.15999999999997, "text": " these things, and that's inevitable, I think. Murray Shanahan is a printable research", "tokens": [50848, 613, 721, 11, 293, 300, 311, 21451, 11, 286, 519, 13, 27291, 25536, 21436, 307, 257, 4482, 712, 2132, 51192], "temperature": 0.0, "avg_logprob": -0.12078893302690864, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0019383867038413882}, {"id": 45, "seek": 25160, "start": 268.15999999999997, "end": 274.71999999999997, "text": " scientist at Google DeepMind and professor of cognitive robotics at Imperial College London.", "tokens": [51192, 12662, 412, 3329, 14895, 44, 471, 293, 8304, 295, 15605, 34145, 412, 21395, 6745, 7042, 13, 51520], "temperature": 0.0, "avg_logprob": -0.12078893302690864, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0019383867038413882}, {"id": 46, "seek": 25160, "start": 274.71999999999997, "end": 281.04, "text": " He was also educated at Imperial and Cambridge University. His publications span artificial", "tokens": [51520, 634, 390, 611, 15872, 412, 21395, 293, 24876, 3535, 13, 2812, 25618, 16174, 11677, 51836], "temperature": 0.0, "avg_logprob": -0.12078893302690864, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0019383867038413882}, {"id": 47, "seek": 28104, "start": 281.04, "end": 286.8, "text": " intelligence, machine learning, logic, dynamical systems, computational neuroscience,", "tokens": [50364, 7599, 11, 3479, 2539, 11, 9952, 11, 5999, 804, 3652, 11, 28270, 42762, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10172813415527343, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.035967178642749786}, {"id": 48, "seek": 28104, "start": 286.8, "end": 292.32, "text": " and the philosophy of mind. He was scientific advisor on the film Ex Machina, and he penned", "tokens": [50652, 293, 264, 10675, 295, 1575, 13, 634, 390, 8134, 19161, 322, 264, 2007, 2111, 12089, 1426, 11, 293, 415, 34911, 292, 50928], "temperature": 0.0, "avg_logprob": -0.10172813415527343, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.035967178642749786}, {"id": 49, "seek": 28104, "start": 292.32, "end": 299.04, "text": " embodiment and the inner life in 2010 and the technological singularity in 2015.", "tokens": [50928, 28935, 2328, 293, 264, 7284, 993, 294, 9657, 293, 264, 18439, 20010, 507, 294, 7546, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10172813415527343, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.035967178642749786}, {"id": 50, "seek": 28104, "start": 299.92, "end": 305.84000000000003, "text": " Shanahan has spent his career understanding cognition and consciousness in the space of", "tokens": [51308, 25536, 21436, 575, 4418, 702, 3988, 3701, 46905, 293, 10081, 294, 264, 1901, 295, 51604], "temperature": 0.0, "avg_logprob": -0.10172813415527343, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.035967178642749786}, {"id": 51, "seek": 30584, "start": 305.84, "end": 312.56, "text": " possible minds. He said that this space of possibilities encompasses biological brains,", "tokens": [50364, 1944, 9634, 13, 634, 848, 300, 341, 1901, 295, 12178, 49866, 13910, 15442, 11, 50700], "temperature": 0.0, "avg_logprob": -0.08163451587452608, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.11542748659849167}, {"id": 52, "seek": 30584, "start": 312.56, "end": 319.59999999999997, "text": " human and animal, as well as artificial intelligence. He worked in symbolic AI for over 10 years,", "tokens": [50700, 1952, 293, 5496, 11, 382, 731, 382, 11677, 7599, 13, 634, 2732, 294, 25755, 7318, 337, 670, 1266, 924, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08163451587452608, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.11542748659849167}, {"id": 53, "seek": 30584, "start": 319.59999999999997, "end": 325.35999999999996, "text": " concentrating on commonsense reasoning, and he then spent 10 years studying the biological brain,", "tokens": [51052, 40571, 322, 800, 13039, 21577, 11, 293, 415, 550, 4418, 1266, 924, 7601, 264, 13910, 3567, 11, 51340], "temperature": 0.0, "avg_logprob": -0.08163451587452608, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.11542748659849167}, {"id": 54, "seek": 30584, "start": 325.35999999999996, "end": 330.96, "text": " specifically how its connectivity and dynamics support cognition and consciousness,", "tokens": [51340, 4682, 577, 1080, 21095, 293, 15679, 1406, 46905, 293, 10081, 11, 51620], "temperature": 0.0, "avg_logprob": -0.08163451587452608, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.11542748659849167}, {"id": 55, "seek": 30584, "start": 330.96, "end": 334.79999999999995, "text": " and he developed a particular interest in global workspace theory.", "tokens": [51620, 293, 415, 4743, 257, 1729, 1179, 294, 4338, 32706, 5261, 13, 51812], "temperature": 0.0, "avg_logprob": -0.08163451587452608, "compression_ratio": 1.650190114068441, "no_speech_prob": 0.11542748659849167}, {"id": 56, "seek": 33480, "start": 335.52000000000004, "end": 339.76, "text": " After that, he went to DeepMind, he pivoted to deep reinforcement learning,", "tokens": [50400, 2381, 300, 11, 415, 1437, 281, 14895, 44, 471, 11, 415, 14538, 292, 281, 2452, 29280, 2539, 11, 50612], "temperature": 0.0, "avg_logprob": -0.11042269953974972, "compression_ratio": 1.522633744855967, "no_speech_prob": 0.0027272021397948265}, {"id": 57, "seek": 33480, "start": 339.76, "end": 345.2, "text": " and recently he's been working extensively with large language models, trying to understand them", "tokens": [50612, 293, 3938, 415, 311, 668, 1364, 32636, 365, 2416, 2856, 5245, 11, 1382, 281, 1223, 552, 50884], "temperature": 0.0, "avg_logprob": -0.11042269953974972, "compression_ratio": 1.522633744855967, "no_speech_prob": 0.0027272021397948265}, {"id": 58, "seek": 33480, "start": 345.2, "end": 351.44, "text": " from a theoretical, philosophical, and practical perspective. Professor Shanahan, I was absolutely", "tokens": [50884, 490, 257, 20864, 11, 25066, 11, 293, 8496, 4585, 13, 8419, 25536, 21436, 11, 286, 390, 3122, 51196], "temperature": 0.0, "avg_logprob": -0.11042269953974972, "compression_ratio": 1.522633744855967, "no_speech_prob": 0.0027272021397948265}, {"id": 59, "seek": 33480, "start": 351.44, "end": 357.36, "text": " fascinated when I read the article from Janus called Simulators. Could you sketch out the article?", "tokens": [51196, 24597, 562, 286, 1401, 264, 7222, 490, 4956, 301, 1219, 3998, 39265, 13, 7497, 291, 12325, 484, 264, 7222, 30, 51492], "temperature": 0.0, "avg_logprob": -0.11042269953974972, "compression_ratio": 1.522633744855967, "no_speech_prob": 0.0027272021397948265}, {"id": 60, "seek": 35736, "start": 358.32, "end": 365.12, "text": " Well, I can sketch out some elements of it. I was also very impressed and", "tokens": [50412, 1042, 11, 286, 393, 12325, 484, 512, 4959, 295, 309, 13, 286, 390, 611, 588, 11679, 293, 50752], "temperature": 0.0, "avg_logprob": -0.1290136361733461, "compression_ratio": 1.6067961165048543, "no_speech_prob": 0.023164020851254463}, {"id": 61, "seek": 35736, "start": 365.12, "end": 372.16, "text": " influenced by that article. Basically, they are advocating a certain way of looking at", "tokens": [50752, 15269, 538, 300, 7222, 13, 8537, 11, 436, 366, 32050, 257, 1629, 636, 295, 1237, 412, 51104], "temperature": 0.0, "avg_logprob": -0.1290136361733461, "compression_ratio": 1.6067961165048543, "no_speech_prob": 0.023164020851254463}, {"id": 62, "seek": 35736, "start": 372.16, "end": 377.84000000000003, "text": " large language models and their behavior. What they say is that we should think of a", "tokens": [51104, 2416, 2856, 5245, 293, 641, 5223, 13, 708, 436, 584, 307, 300, 321, 820, 519, 295, 257, 51388], "temperature": 0.0, "avg_logprob": -0.1290136361733461, "compression_ratio": 1.6067961165048543, "no_speech_prob": 0.023164020851254463}, {"id": 63, "seek": 35736, "start": 377.84000000000003, "end": 385.6, "text": " large language model as a kind of simulator, which is capable of simulating a kind of", "tokens": [51388, 2416, 2856, 2316, 382, 257, 733, 295, 32974, 11, 597, 307, 8189, 295, 1034, 12162, 257, 733, 295, 51776], "temperature": 0.0, "avg_logprob": -0.1290136361733461, "compression_ratio": 1.6067961165048543, "no_speech_prob": 0.023164020851254463}, {"id": 64, "seek": 38560, "start": 385.6, "end": 392.48, "text": " language-producing processes of various sorts, and it's capable of simulating all kinds of language", "tokens": [50364, 2856, 12, 14314, 2175, 7555, 295, 3683, 7527, 11, 293, 309, 311, 8189, 295, 1034, 12162, 439, 3685, 295, 2856, 50708], "temperature": 0.0, "avg_logprob": -0.12335779553367979, "compression_ratio": 2.1560693641618496, "no_speech_prob": 0.02986273542046547}, {"id": 65, "seek": 38560, "start": 392.48, "end": 399.36, "text": " producing processes, and in particular, it's capable of simulating people, humans, and it's", "tokens": [50708, 10501, 7555, 11, 293, 294, 1729, 11, 309, 311, 8189, 295, 1034, 12162, 561, 11, 6255, 11, 293, 309, 311, 51052], "temperature": 0.0, "avg_logprob": -0.12335779553367979, "compression_ratio": 2.1560693641618496, "no_speech_prob": 0.02986273542046547}, {"id": 66, "seek": 38560, "start": 399.36, "end": 404.8, "text": " capable of simulating different kinds of humans, so humans who are playing different sorts of roles,", "tokens": [51052, 8189, 295, 1034, 12162, 819, 3685, 295, 6255, 11, 370, 6255, 567, 366, 2433, 819, 7527, 295, 9604, 11, 51324], "temperature": 0.0, "avg_logprob": -0.12335779553367979, "compression_ratio": 2.1560693641618496, "no_speech_prob": 0.02986273542046547}, {"id": 67, "seek": 38560, "start": 404.8, "end": 413.6, "text": " who maybe humans who are helpful assistants or humans who are crazy psychopaths,", "tokens": [51324, 567, 1310, 6255, 567, 366, 4961, 34949, 420, 6255, 567, 366, 3219, 47577, 82, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12335779553367979, "compression_ratio": 2.1560693641618496, "no_speech_prob": 0.02986273542046547}, {"id": 68, "seek": 41360, "start": 414.16, "end": 424.16, "text": " and indeed in their way of thinking of things. These are all examples of Simulacra. Simulacra,", "tokens": [50392, 293, 6451, 294, 641, 636, 295, 1953, 295, 721, 13, 1981, 366, 439, 5110, 295, 3998, 425, 326, 424, 13, 3998, 425, 326, 424, 11, 50892], "temperature": 0.0, "avg_logprob": -0.20184748920042123, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.011922028847038746}, {"id": 69, "seek": 41360, "start": 424.16, "end": 429.76000000000005, "text": " in their conception, include actually not only human beings, but anything that produces language", "tokens": [50892, 294, 641, 30698, 11, 4090, 767, 406, 787, 1952, 8958, 11, 457, 1340, 300, 14725, 2856, 51172], "temperature": 0.0, "avg_logprob": -0.20184748920042123, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.011922028847038746}, {"id": 70, "seek": 41360, "start": 429.76000000000005, "end": 436.8, "text": " at all. Your base model can simulate anything that can generate language, if you like.", "tokens": [51172, 412, 439, 13, 2260, 3096, 2316, 393, 27817, 1340, 300, 393, 8460, 2856, 11, 498, 291, 411, 13, 51524], "temperature": 0.0, "avg_logprob": -0.20184748920042123, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.011922028847038746}, {"id": 71, "seek": 43680, "start": 437.6, "end": 444.40000000000003, "text": " Of particular interest, of course, are humans and human language producers, so", "tokens": [50404, 2720, 1729, 1179, 11, 295, 1164, 11, 366, 6255, 293, 1952, 2856, 16080, 11, 370, 50744], "temperature": 0.0, "avg_logprob": -0.14510389358278306, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.040492650121450424}, {"id": 72, "seek": 43680, "start": 445.28000000000003, "end": 451.6, "text": " the particular class of Simulacra that I'm interested in really are humans playing different", "tokens": [50788, 264, 1729, 1508, 295, 3998, 425, 326, 424, 300, 286, 478, 3102, 294, 534, 366, 6255, 2433, 819, 51104], "temperature": 0.0, "avg_logprob": -0.14510389358278306, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.040492650121450424}, {"id": 73, "seek": 43680, "start": 451.6, "end": 459.84000000000003, "text": " roles. In the work that I've been doing, I've been thinking of language models in terms of role", "tokens": [51104, 9604, 13, 682, 264, 589, 300, 286, 600, 668, 884, 11, 286, 600, 668, 1953, 295, 2856, 5245, 294, 2115, 295, 3090, 51516], "temperature": 0.0, "avg_logprob": -0.14510389358278306, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.040492650121450424}, {"id": 74, "seek": 45984, "start": 459.84, "end": 470.4, "text": " play and in terms of their ability to play a part, if you like, and so this is very much,", "tokens": [50364, 862, 293, 294, 2115, 295, 641, 3485, 281, 862, 257, 644, 11, 498, 291, 411, 11, 293, 370, 341, 307, 588, 709, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13256279851349306, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.21104201674461365}, {"id": 75, "seek": 45984, "start": 470.4, "end": 478.08, "text": " it was very much inspired and drew on this work of Janus. Now, they make another very, very", "tokens": [50892, 309, 390, 588, 709, 7547, 293, 12804, 322, 341, 589, 295, 4956, 301, 13, 823, 11, 436, 652, 1071, 588, 11, 588, 51276], "temperature": 0.0, "avg_logprob": -0.13256279851349306, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.21104201674461365}, {"id": 76, "seek": 45984, "start": 478.08, "end": 485.35999999999996, "text": " interesting and important point in that article, which is that they draw attention to the fact that", "tokens": [51276, 1880, 293, 1021, 935, 294, 300, 7222, 11, 597, 307, 300, 436, 2642, 3202, 281, 264, 1186, 300, 51640], "temperature": 0.0, "avg_logprob": -0.13256279851349306, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.21104201674461365}, {"id": 77, "seek": 48536, "start": 486.32, "end": 490.32, "text": " large language models, at any point in a conversation, in an ongoing conversation,", "tokens": [50412, 2416, 2856, 5245, 11, 412, 604, 935, 294, 257, 3761, 11, 294, 364, 10452, 3761, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10735893249511719, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.030228426679968834}, {"id": 78, "seek": 48536, "start": 491.2, "end": 495.92, "text": " then the next word that's produced in this conversation, or the next string of words,", "tokens": [50656, 550, 264, 958, 1349, 300, 311, 7126, 294, 341, 3761, 11, 420, 264, 958, 6798, 295, 2283, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10735893249511719, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.030228426679968834}, {"id": 79, "seek": 48536, "start": 495.92, "end": 502.56, "text": " the next sort of sentence, is the product of a stochastic process. So what the underlying", "tokens": [50892, 264, 958, 1333, 295, 8174, 11, 307, 264, 1674, 295, 257, 342, 8997, 2750, 1399, 13, 407, 437, 264, 14217, 51224], "temperature": 0.0, "avg_logprob": -0.10735893249511719, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.030228426679968834}, {"id": 80, "seek": 48536, "start": 502.56, "end": 508.16, "text": " language model actually generates is a distribution over the possible words that might come up next,", "tokens": [51224, 2856, 2316, 767, 23815, 307, 257, 7316, 670, 264, 1944, 2283, 300, 1062, 808, 493, 958, 11, 51504], "temperature": 0.0, "avg_logprob": -0.10735893249511719, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.030228426679968834}, {"id": 81, "seek": 48536, "start": 508.16, "end": 513.2, "text": " and then what you do is then you sample from this distribution to come up with an actual word,", "tokens": [51504, 293, 550, 437, 291, 360, 307, 550, 291, 6889, 490, 341, 7316, 281, 808, 493, 365, 364, 3539, 1349, 11, 51756], "temperature": 0.0, "avg_logprob": -0.10735893249511719, "compression_ratio": 1.923728813559322, "no_speech_prob": 0.030228426679968834}, {"id": 82, "seek": 51320, "start": 513.2, "end": 519.2800000000001, "text": " and then that's the word that you give back to the user. So for example, if the favorite example", "tokens": [50364, 293, 550, 300, 311, 264, 1349, 300, 291, 976, 646, 281, 264, 4195, 13, 407, 337, 1365, 11, 498, 264, 2954, 1365, 50668], "temperature": 0.0, "avg_logprob": -0.11500254544344815, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004337623715400696}, {"id": 83, "seek": 51320, "start": 519.2800000000001, "end": 528.48, "text": " I use is if you ask the language model to tell you a story and it says once upon a time there was,", "tokens": [50668, 286, 764, 307, 498, 291, 1029, 264, 2856, 2316, 281, 980, 291, 257, 1657, 293, 309, 1619, 1564, 3564, 257, 565, 456, 390, 11, 51128], "temperature": 0.0, "avg_logprob": -0.11500254544344815, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004337623715400696}, {"id": 84, "seek": 51320, "start": 528.48, "end": 533.2800000000001, "text": " and at that point, it's going to generate, as in all the points up to that as well, it's going to", "tokens": [51128, 293, 412, 300, 935, 11, 309, 311, 516, 281, 8460, 11, 382, 294, 439, 264, 2793, 493, 281, 300, 382, 731, 11, 309, 311, 516, 281, 51368], "temperature": 0.0, "avg_logprob": -0.11500254544344815, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004337623715400696}, {"id": 85, "seek": 51320, "start": 533.2800000000001, "end": 537.84, "text": " generate a distribution of the possible tokens that might come next, possible words that might come", "tokens": [51368, 8460, 257, 7316, 295, 264, 1944, 22667, 300, 1062, 808, 958, 11, 1944, 2283, 300, 1062, 808, 51596], "temperature": 0.0, "avg_logprob": -0.11500254544344815, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004337623715400696}, {"id": 86, "seek": 53784, "start": 537.84, "end": 544.32, "text": " next. So once upon a time there was, and it might say a beautiful princess, or a handsome prince,", "tokens": [50364, 958, 13, 407, 1564, 3564, 257, 565, 456, 390, 11, 293, 309, 1062, 584, 257, 2238, 14742, 11, 420, 257, 13421, 16467, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09615334293298554, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.055424511432647705}, {"id": 87, "seek": 53784, "start": 544.32, "end": 550.0, "text": " or a fierce dragon, and it could say any of those things depending upon the sampling process.", "tokens": [50688, 420, 257, 25341, 12165, 11, 293, 309, 727, 584, 604, 295, 729, 721, 5413, 3564, 264, 21179, 1399, 13, 50972], "temperature": 0.0, "avg_logprob": -0.09615334293298554, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.055424511432647705}, {"id": 88, "seek": 53784, "start": 550.0, "end": 555.2, "text": " And then the point is as you come back to that same, you could rewind the conversation, come back", "tokens": [50972, 400, 550, 264, 935, 307, 382, 291, 808, 646, 281, 300, 912, 11, 291, 727, 41458, 264, 3761, 11, 808, 646, 51232], "temperature": 0.0, "avg_logprob": -0.09615334293298554, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.055424511432647705}, {"id": 89, "seek": 53784, "start": 555.2, "end": 560.0, "text": " to that point again, sample again, as we can all do with the interfaces that we have, and get a", "tokens": [51232, 281, 300, 935, 797, 11, 6889, 797, 11, 382, 321, 393, 439, 360, 365, 264, 28416, 300, 321, 362, 11, 293, 483, 257, 51472], "temperature": 0.0, "avg_logprob": -0.09615334293298554, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.055424511432647705}, {"id": 90, "seek": 53784, "start": 560.0, "end": 563.44, "text": " different answer again, and take the whole story off in a completely different direction.", "tokens": [51472, 819, 1867, 797, 11, 293, 747, 264, 1379, 1657, 766, 294, 257, 2584, 819, 3513, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09615334293298554, "compression_ratio": 1.8129770992366412, "no_speech_prob": 0.055424511432647705}, {"id": 91, "seek": 56344, "start": 563.6, "end": 570.72, "text": " And so what they draw attention to is the fact that at any particular point in a conversation,", "tokens": [50372, 400, 370, 437, 436, 2642, 3202, 281, 307, 264, 1186, 300, 412, 604, 1729, 935, 294, 257, 3761, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1348751475302021, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.08058738708496094}, {"id": 92, "seek": 56344, "start": 570.72, "end": 578.8800000000001, "text": " there's really a whole set of roles that are being played by the underlying simulation", "tokens": [50728, 456, 311, 534, 257, 1379, 992, 295, 9604, 300, 366, 885, 3737, 538, 264, 14217, 16575, 51136], "temperature": 0.0, "avg_logprob": -0.1348751475302021, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.08058738708496094}, {"id": 93, "seek": 56344, "start": 578.8800000000001, "end": 584.96, "text": " at any one point, and the conversation shapes what role is being played. So in that sense,", "tokens": [51136, 412, 604, 472, 935, 11, 293, 264, 3761, 10854, 437, 3090, 307, 885, 3737, 13, 407, 294, 300, 2020, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1348751475302021, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.08058738708496094}, {"id": 94, "seek": 56344, "start": 584.96, "end": 592.1600000000001, "text": " it's sort of unlike a human being, because you've got, as they put it, a whole superposition of", "tokens": [51440, 309, 311, 1333, 295, 8343, 257, 1952, 885, 11, 570, 291, 600, 658, 11, 382, 436, 829, 309, 11, 257, 1379, 1687, 38078, 295, 51800], "temperature": 0.0, "avg_logprob": -0.1348751475302021, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.08058738708496094}, {"id": 95, "seek": 59216, "start": 592.64, "end": 597.68, "text": " simulacra that are all being simulated all at once. And as the conversation progresses,", "tokens": [50388, 1034, 425, 326, 424, 300, 366, 439, 885, 41713, 439, 412, 1564, 13, 400, 382, 264, 3761, 41929, 11, 50640], "temperature": 0.0, "avg_logprob": -0.0708202261191148, "compression_ratio": 1.7201646090534979, "no_speech_prob": 0.004240967333316803}, {"id": 96, "seek": 59216, "start": 597.68, "end": 603.52, "text": " then the actual distribution of simulacra is being narrowed down.", "tokens": [50640, 550, 264, 3539, 7316, 295, 1034, 425, 326, 424, 307, 885, 9432, 292, 760, 13, 50932], "temperature": 0.0, "avg_logprob": -0.0708202261191148, "compression_ratio": 1.7201646090534979, "no_speech_prob": 0.004240967333316803}, {"id": 97, "seek": 59216, "start": 604.0799999999999, "end": 608.88, "text": " Yes, but as you say, you can view language models at the low level in terms of being", "tokens": [50960, 1079, 11, 457, 382, 291, 584, 11, 291, 393, 1910, 2856, 5245, 412, 264, 2295, 1496, 294, 2115, 295, 885, 51200], "temperature": 0.0, "avg_logprob": -0.0708202261191148, "compression_ratio": 1.7201646090534979, "no_speech_prob": 0.004240967333316803}, {"id": 98, "seek": 59216, "start": 608.88, "end": 614.48, "text": " next-word generators, or what we strive to do in science is come up with explanations that", "tokens": [51200, 958, 12, 7462, 38662, 11, 420, 437, 321, 23829, 281, 360, 294, 3497, 307, 808, 493, 365, 28708, 300, 51480], "temperature": 0.0, "avg_logprob": -0.0708202261191148, "compression_ratio": 1.7201646090534979, "no_speech_prob": 0.004240967333316803}, {"id": 99, "seek": 59216, "start": 614.48, "end": 620.48, "text": " demarcate the thing very clearly. And this idea of the language model being a simulator,", "tokens": [51480, 1371, 40088, 473, 264, 551, 588, 4448, 13, 400, 341, 1558, 295, 264, 2856, 2316, 885, 257, 32974, 11, 51780], "temperature": 0.0, "avg_logprob": -0.0708202261191148, "compression_ratio": 1.7201646090534979, "no_speech_prob": 0.004240967333316803}, {"id": 100, "seek": 62048, "start": 620.48, "end": 626.24, "text": " which produces the simulacra, and you said in your role-playing article on Nature that", "tokens": [50364, 597, 14725, 264, 1034, 425, 326, 424, 11, 293, 291, 848, 294, 428, 3090, 12, 32944, 7222, 322, 20159, 300, 50652], "temperature": 0.0, "avg_logprob": -0.10839290618896484, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.017589684575796127}, {"id": 101, "seek": 62048, "start": 626.24, "end": 630.88, "text": " if you had a UI which was sufficiently advanced, you could actually play with counterfactual", "tokens": [50652, 498, 291, 632, 257, 15682, 597, 390, 31868, 7339, 11, 291, 727, 767, 862, 365, 5682, 44919, 901, 50884], "temperature": 0.0, "avg_logprob": -0.10839290618896484, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.017589684575796127}, {"id": 102, "seek": 62048, "start": 630.88, "end": 636.64, "text": " trajectories and start to understand how sticky the simulacra are, because as you pointed out,", "tokens": [50884, 18257, 2083, 293, 722, 281, 1223, 577, 14470, 264, 1034, 425, 326, 424, 366, 11, 570, 382, 291, 10932, 484, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10839290618896484, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.017589684575796127}, {"id": 103, "seek": 62048, "start": 636.64, "end": 642.08, "text": " when the language model says I, sometimes it's talking about chat GPT or whatever,", "tokens": [51172, 562, 264, 2856, 2316, 1619, 286, 11, 2171, 309, 311, 1417, 466, 5081, 26039, 51, 420, 2035, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10839290618896484, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.017589684575796127}, {"id": 104, "seek": 62048, "start": 642.08, "end": 647.44, "text": " it's talking about the simulator, sometimes it's talking about the simulacrum, and these things", "tokens": [51444, 309, 311, 1417, 466, 264, 32974, 11, 2171, 309, 311, 1417, 466, 264, 1034, 425, 326, 6247, 11, 293, 613, 721, 51712], "temperature": 0.0, "avg_logprob": -0.10839290618896484, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.017589684575796127}, {"id": 105, "seek": 64744, "start": 647.44, "end": 652.8800000000001, "text": " are trained on everything on the internet, you know, structured narratology essays, novels,", "tokens": [50364, 366, 8895, 322, 1203, 322, 264, 4705, 11, 291, 458, 11, 18519, 6397, 267, 1793, 35123, 11, 24574, 11, 50636], "temperature": 0.0, "avg_logprob": -0.1141050098178623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.04420346021652222}, {"id": 106, "seek": 64744, "start": 652.8800000000001, "end": 657.9200000000001, "text": " and it's fascinating to see how you can jump between these different parts of the trajectory", "tokens": [50636, 293, 309, 311, 10343, 281, 536, 577, 291, 393, 3012, 1296, 613, 819, 3166, 295, 264, 21512, 50888], "temperature": 0.0, "avg_logprob": -0.1141050098178623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.04420346021652222}, {"id": 107, "seek": 64744, "start": 657.9200000000001, "end": 663.2, "text": " structure. And in your Nature paper, you gave a beautiful example, which was the 20 Questions", "tokens": [50888, 3877, 13, 400, 294, 428, 20159, 3035, 11, 291, 2729, 257, 2238, 1365, 11, 597, 390, 264, 945, 27738, 51152], "temperature": 0.0, "avg_logprob": -0.1141050098178623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.04420346021652222}, {"id": 108, "seek": 64744, "start": 663.2, "end": 667.12, "text": " game. I mean, would you mind introducing that? Yeah, sure. So I think we're probably all familiar", "tokens": [51152, 1216, 13, 286, 914, 11, 576, 291, 1575, 15424, 300, 30, 865, 11, 988, 13, 407, 286, 519, 321, 434, 1391, 439, 4963, 51348], "temperature": 0.0, "avg_logprob": -0.1141050098178623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.04420346021652222}, {"id": 109, "seek": 64744, "start": 667.12, "end": 673.36, "text": " with the 20 Questions game. So one player thinks of an object, and the other player has to guess", "tokens": [51348, 365, 264, 945, 27738, 1216, 13, 407, 472, 4256, 7309, 295, 364, 2657, 11, 293, 264, 661, 4256, 575, 281, 2041, 51660], "temperature": 0.0, "avg_logprob": -0.1141050098178623, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.04420346021652222}, {"id": 110, "seek": 67336, "start": 673.44, "end": 680.96, "text": " what that object is by asking a whole bunch of questions with yes, no answers. So I might think", "tokens": [50368, 437, 300, 2657, 307, 538, 3365, 257, 1379, 3840, 295, 1651, 365, 2086, 11, 572, 6338, 13, 407, 286, 1062, 519, 50744], "temperature": 0.0, "avg_logprob": -0.14123182296752929, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.04071398079395294}, {"id": 111, "seek": 67336, "start": 680.96, "end": 686.88, "text": " of, I might in my head think of a pencil, and then you might say, oh, is it larger than a house or", "tokens": [50744, 295, 11, 286, 1062, 294, 452, 1378, 519, 295, 257, 10985, 11, 293, 550, 291, 1062, 584, 11, 1954, 11, 307, 309, 4833, 813, 257, 1782, 420, 51040], "temperature": 0.0, "avg_logprob": -0.14123182296752929, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.04071398079395294}, {"id": 112, "seek": 67336, "start": 686.88, "end": 690.08, "text": " smaller than a house? And I say, well, actually, that's not a yes, no answer, but it's a binary", "tokens": [51040, 4356, 813, 257, 1782, 30, 400, 286, 584, 11, 731, 11, 767, 11, 300, 311, 406, 257, 2086, 11, 572, 1867, 11, 457, 309, 311, 257, 17434, 51200], "temperature": 0.0, "avg_logprob": -0.14123182296752929, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.04071398079395294}, {"id": 113, "seek": 67336, "start": 690.08, "end": 695.44, "text": " answer. Is it larger than a house? And you'd say, oh, no, you know, is it made of wood? Yes, is it", "tokens": [51200, 1867, 13, 1119, 309, 4833, 813, 257, 1782, 30, 400, 291, 1116, 584, 11, 1954, 11, 572, 11, 291, 458, 11, 307, 309, 1027, 295, 4576, 30, 1079, 11, 307, 309, 51468], "temperature": 0.0, "avg_logprob": -0.14123182296752929, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.04071398079395294}, {"id": 114, "seek": 67336, "start": 696.24, "end": 700.24, "text": " a tool? Yes, you know, and eventually you might guess the answer. So we're familiar with this", "tokens": [51508, 257, 2290, 30, 1079, 11, 291, 458, 11, 293, 4728, 291, 1062, 2041, 264, 1867, 13, 407, 321, 434, 4963, 365, 341, 51708], "temperature": 0.0, "avg_logprob": -0.14123182296752929, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.04071398079395294}, {"id": 115, "seek": 70024, "start": 700.48, "end": 706.32, "text": " little game. And you can play this with a large language model, of course, and you can ask the", "tokens": [50376, 707, 1216, 13, 400, 291, 393, 862, 341, 365, 257, 2416, 2856, 2316, 11, 295, 1164, 11, 293, 291, 393, 1029, 264, 50668], "temperature": 0.0, "avg_logprob": -0.08771023868529265, "compression_ratio": 2.1574074074074074, "no_speech_prob": 0.019280169159173965}, {"id": 116, "seek": 70024, "start": 706.32, "end": 712.5600000000001, "text": " large language model to play the part of the setter who thinks of the thinks of the object,", "tokens": [50668, 2416, 2856, 2316, 281, 862, 264, 644, 295, 264, 992, 391, 567, 7309, 295, 264, 7309, 295, 264, 2657, 11, 50980], "temperature": 0.0, "avg_logprob": -0.08771023868529265, "compression_ratio": 2.1574074074074074, "no_speech_prob": 0.019280169159173965}, {"id": 117, "seek": 70024, "start": 712.5600000000001, "end": 716.5600000000001, "text": " and then you play the part of the guesser who tries to guess the object by asking questions.", "tokens": [50980, 293, 550, 291, 862, 264, 644, 295, 264, 2041, 260, 567, 9898, 281, 2041, 264, 2657, 538, 3365, 1651, 13, 51180], "temperature": 0.0, "avg_logprob": -0.08771023868529265, "compression_ratio": 2.1574074074074074, "no_speech_prob": 0.019280169159173965}, {"id": 118, "seek": 70024, "start": 717.2, "end": 722.8, "text": " Now, if you do this with a large language model, what if you do it with a person, if a person is", "tokens": [51212, 823, 11, 498, 291, 360, 341, 365, 257, 2416, 2856, 2316, 11, 437, 498, 291, 360, 309, 365, 257, 954, 11, 498, 257, 954, 307, 51492], "temperature": 0.0, "avg_logprob": -0.08771023868529265, "compression_ratio": 2.1574074074074074, "no_speech_prob": 0.019280169159173965}, {"id": 119, "seek": 70024, "start": 722.8, "end": 727.28, "text": " not cheating, as it were, they will think of in their head, they will think of an object,", "tokens": [51492, 406, 18309, 11, 382, 309, 645, 11, 436, 486, 519, 295, 294, 641, 1378, 11, 436, 486, 519, 295, 364, 2657, 11, 51716], "temperature": 0.0, "avg_logprob": -0.08771023868529265, "compression_ratio": 2.1574074074074074, "no_speech_prob": 0.019280169159173965}, {"id": 120, "seek": 72728, "start": 727.28, "end": 731.68, "text": " and then they'll fix that object in their mind, and then they'll answer the question,", "tokens": [50364, 293, 550, 436, 603, 3191, 300, 2657, 294, 641, 1575, 11, 293, 550, 436, 603, 1867, 264, 1168, 11, 50584], "temperature": 0.0, "avg_logprob": -0.13251169657303116, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.002991268178448081}, {"id": 121, "seek": 72728, "start": 731.68, "end": 736.24, "text": " according to what object they thought of in advance. Now, but a large language model can't", "tokens": [50584, 4650, 281, 437, 2657, 436, 1194, 295, 294, 7295, 13, 823, 11, 457, 257, 2416, 2856, 2316, 393, 380, 50812], "temperature": 0.0, "avg_logprob": -0.13251169657303116, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.002991268178448081}, {"id": 122, "seek": 72728, "start": 736.24, "end": 742.64, "text": " really do that unless you use some hack or another. So what it really does is it just,", "tokens": [50812, 534, 360, 300, 5969, 291, 764, 512, 10339, 420, 1071, 13, 407, 437, 309, 534, 775, 307, 309, 445, 11, 51132], "temperature": 0.0, "avg_logprob": -0.13251169657303116, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.002991268178448081}, {"id": 123, "seek": 72728, "start": 742.64, "end": 746.72, "text": " so you say to think of an object, and it says, I've thought of an object, it hasn't really", "tokens": [51132, 370, 291, 584, 281, 519, 295, 364, 2657, 11, 293, 309, 1619, 11, 286, 600, 1194, 295, 364, 2657, 11, 309, 6132, 380, 534, 51336], "temperature": 0.0, "avg_logprob": -0.13251169657303116, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.002991268178448081}, {"id": 124, "seek": 72728, "start": 746.72, "end": 753.28, "text": " thought of an object, it's just issued the tokens to say that it has. But then you will ask a question", "tokens": [51336, 1194, 295, 364, 2657, 11, 309, 311, 445, 14379, 264, 22667, 281, 584, 300, 309, 575, 13, 583, 550, 291, 486, 1029, 257, 1168, 51664], "temperature": 0.0, "avg_logprob": -0.13251169657303116, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.002991268178448081}, {"id": 125, "seek": 75328, "start": 753.28, "end": 759.68, "text": " and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say,", "tokens": [50364, 293, 291, 603, 584, 11, 307, 309, 4833, 813, 257, 1782, 30, 400, 309, 603, 584, 572, 13, 400, 550, 4728, 11, 498, 291, 584, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12474211481691316, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.02604695037007332}, {"id": 126, "seek": 75328, "start": 759.68, "end": 764.3199999999999, "text": " I give up, tell me what the object is, then it will say, oh, I was thinking of a pencil.", "tokens": [50684, 286, 976, 493, 11, 980, 385, 437, 264, 2657, 307, 11, 550, 309, 486, 584, 11, 1954, 11, 286, 390, 1953, 295, 257, 10985, 13, 50916], "temperature": 0.0, "avg_logprob": -0.12474211481691316, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.02604695037007332}, {"id": 127, "seek": 75328, "start": 764.3199999999999, "end": 769.68, "text": " And it will indeed give you a, you know, typically will give you an object that's consistent with", "tokens": [50916, 400, 309, 486, 6451, 976, 291, 257, 11, 291, 458, 11, 5850, 486, 976, 291, 364, 2657, 300, 311, 8398, 365, 51184], "temperature": 0.0, "avg_logprob": -0.12474211481691316, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.02604695037007332}, {"id": 128, "seek": 75328, "start": 769.68, "end": 776.56, "text": " all of the answers it gave to all of your questions. But then if you just wind back one and resample", "tokens": [51184, 439, 295, 264, 6338, 309, 2729, 281, 439, 295, 428, 1651, 13, 583, 550, 498, 291, 445, 2468, 646, 472, 293, 725, 335, 781, 51528], "temperature": 0.0, "avg_logprob": -0.12474211481691316, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.02604695037007332}, {"id": 129, "seek": 75328, "start": 776.56, "end": 781.36, "text": " and ask it again and say, I give up, what were you thinking of? It might say a mouse,", "tokens": [51528, 293, 1029, 309, 797, 293, 584, 11, 286, 976, 493, 11, 437, 645, 291, 1953, 295, 30, 467, 1062, 584, 257, 9719, 11, 51768], "temperature": 0.0, "avg_logprob": -0.12474211481691316, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.02604695037007332}, {"id": 130, "seek": 78136, "start": 781.84, "end": 787.84, "text": " or, you know, or a bottle, or, you know, it could say something completely different,", "tokens": [50388, 420, 11, 291, 458, 11, 420, 257, 7817, 11, 420, 11, 291, 458, 11, 309, 727, 584, 746, 2584, 819, 11, 50688], "temperature": 0.0, "avg_logprob": -0.1359638907692649, "compression_ratio": 1.795275590551181, "no_speech_prob": 0.004809180274605751}, {"id": 131, "seek": 78136, "start": 787.84, "end": 791.28, "text": " which indicates that it was never, it had never really committed to any particular", "tokens": [50688, 597, 16203, 300, 309, 390, 1128, 11, 309, 632, 1128, 534, 7784, 281, 604, 1729, 50860], "temperature": 0.0, "avg_logprob": -0.1359638907692649, "compression_ratio": 1.795275590551181, "no_speech_prob": 0.004809180274605751}, {"id": 132, "seek": 78136, "start": 791.28, "end": 798.32, "text": " object in the first place. And so what this shows is that in fact, in theory, you could rewind", "tokens": [50860, 2657, 294, 264, 700, 1081, 13, 400, 370, 437, 341, 3110, 307, 300, 294, 1186, 11, 294, 5261, 11, 291, 727, 41458, 51212], "temperature": 0.0, "avg_logprob": -0.1359638907692649, "compression_ratio": 1.795275590551181, "no_speech_prob": 0.004809180274605751}, {"id": 133, "seek": 78136, "start": 798.32, "end": 804.5600000000001, "text": " further and it might actually give you a different answer to the questions if you rewind further,", "tokens": [51212, 3052, 293, 309, 1062, 767, 976, 291, 257, 819, 1867, 281, 264, 1651, 498, 291, 41458, 3052, 11, 51524], "temperature": 0.0, "avg_logprob": -0.1359638907692649, "compression_ratio": 1.795275590551181, "no_speech_prob": 0.004809180274605751}, {"id": 134, "seek": 78136, "start": 805.6, "end": 810.96, "text": " to the same questions. And that's because what you've really got is you've got a kind of whole", "tokens": [51576, 281, 264, 912, 1651, 13, 400, 300, 311, 570, 437, 291, 600, 534, 658, 307, 291, 600, 658, 257, 733, 295, 1379, 51844], "temperature": 0.0, "avg_logprob": -0.1359638907692649, "compression_ratio": 1.795275590551181, "no_speech_prob": 0.004809180274605751}, {"id": 135, "seek": 81096, "start": 810.96, "end": 816.08, "text": " tree of possibilities. And this sort of this stochastic sampling process at any point in a", "tokens": [50364, 4230, 295, 12178, 13, 400, 341, 1333, 295, 341, 342, 8997, 2750, 21179, 1399, 412, 604, 935, 294, 257, 50620], "temperature": 0.0, "avg_logprob": -0.13832264775815217, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0029679150320589542}, {"id": 136, "seek": 81096, "start": 816.08, "end": 822.0, "text": " conversation induces a whole tree of possibilities that branches forth from where you are right now.", "tokens": [50620, 3761, 13716, 887, 257, 1379, 4230, 295, 12178, 300, 14770, 5220, 490, 689, 291, 366, 558, 586, 13, 50916], "temperature": 0.0, "avg_logprob": -0.13832264775815217, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0029679150320589542}, {"id": 137, "seek": 81096, "start": 822.0, "end": 827.0400000000001, "text": " And counterfactually, you can always, well, you can always rewind the conversation to an earlier", "tokens": [50916, 400, 5682, 44919, 671, 11, 291, 393, 1009, 11, 731, 11, 291, 393, 1009, 41458, 264, 3761, 281, 364, 3071, 51168], "temperature": 0.0, "avg_logprob": -0.13832264775815217, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0029679150320589542}, {"id": 138, "seek": 81096, "start": 827.0400000000001, "end": 831.9200000000001, "text": " point, and, and revisit it and sample again and go off on a different, different branch.", "tokens": [51168, 935, 11, 293, 11, 293, 32676, 309, 293, 6889, 797, 293, 352, 766, 322, 257, 819, 11, 819, 9819, 13, 51412], "temperature": 0.0, "avg_logprob": -0.13832264775815217, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0029679150320589542}, {"id": 139, "seek": 81096, "start": 832.64, "end": 838.96, "text": " And so my co-authors of that paper, in fact, the Nature paper, so Laria Reynolds and Kyle McDonald,", "tokens": [51448, 400, 370, 452, 598, 12, 40198, 830, 295, 300, 3035, 11, 294, 1186, 11, 264, 20159, 3035, 11, 370, 441, 9831, 29516, 293, 18023, 16889, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13832264775815217, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0029679150320589542}, {"id": 140, "seek": 83896, "start": 838.96, "end": 844.08, "text": " so they have this system called Lume, which allows you to actually retain the whole tree", "tokens": [50364, 370, 436, 362, 341, 1185, 1219, 441, 2540, 11, 597, 4045, 291, 281, 767, 18340, 264, 1379, 4230, 50620], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 141, "seek": 83896, "start": 844.08, "end": 848.1600000000001, "text": " of a conversation and you can visualize this and you can revisit different points in the", "tokens": [50620, 295, 257, 3761, 293, 291, 393, 23273, 341, 293, 291, 393, 32676, 819, 2793, 294, 264, 50824], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 142, "seek": 83896, "start": 848.1600000000001, "end": 854.08, "text": " conversation and resample and, and explore the things, a whole kind of tree of possibilities.", "tokens": [50824, 3761, 293, 725, 335, 781, 293, 11, 293, 6839, 264, 721, 11, 257, 1379, 733, 295, 4230, 295, 12178, 13, 51120], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 143, "seek": 83896, "start": 854.64, "end": 856.5600000000001, "text": " Yeah, that rings a bell. Did they work for conjecture?", "tokens": [51148, 865, 11, 300, 11136, 257, 4549, 13, 2589, 436, 589, 337, 416, 1020, 540, 30, 51244], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 144, "seek": 83896, "start": 856.5600000000001, "end": 857.6, "text": " They did work for conjecture.", "tokens": [51244, 814, 630, 589, 337, 416, 1020, 540, 13, 51296], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 145, "seek": 83896, "start": 857.6, "end": 860.8000000000001, "text": " Yeah, I was interviewing some people from conjecture and they were telling me about that.", "tokens": [51296, 865, 11, 286, 390, 26524, 512, 561, 490, 416, 1020, 540, 293, 436, 645, 3585, 385, 466, 300, 13, 51456], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 146, "seek": 83896, "start": 860.8000000000001, "end": 863.76, "text": " So yeah, that's very interesting. Maybe we'll put a placeholder on that.", "tokens": [51456, 407, 1338, 11, 300, 311, 588, 1880, 13, 2704, 321, 603, 829, 257, 1081, 20480, 322, 300, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 147, "seek": 83896, "start": 863.76, "end": 868.1600000000001, "text": " But another point that we briefly spoke about before is that, you know, the article was on", "tokens": [51604, 583, 1071, 935, 300, 321, 10515, 7179, 466, 949, 307, 300, 11, 291, 458, 11, 264, 7222, 390, 322, 51824], "temperature": 0.0, "avg_logprob": -0.11953158094393497, "compression_ratio": 1.8208955223880596, "no_speech_prob": 0.0005108388140797615}, {"id": 148, "seek": 86816, "start": 868.16, "end": 872.64, "text": " Less Wrong. And I don't mean that pejoratively, because I thought the simulator was one of the", "tokens": [50364, 18649, 28150, 13, 400, 286, 500, 380, 914, 300, 520, 2337, 19020, 11, 570, 286, 1194, 264, 32974, 390, 472, 295, 264, 50588], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 149, "seek": 86816, "start": 872.64, "end": 876.3199999999999, "text": " best articles I've ever read. But there is a lot of stuff on Less Wrong, which is definitely a bit", "tokens": [50588, 1151, 11290, 286, 600, 1562, 1401, 13, 583, 456, 307, 257, 688, 295, 1507, 322, 18649, 28150, 11, 597, 307, 2138, 257, 857, 50772], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 150, "seek": 86816, "start": 876.3199999999999, "end": 881.92, "text": " out there. And it's just very interesting that you're now citing their work in a Nature paper.", "tokens": [50772, 484, 456, 13, 400, 309, 311, 445, 588, 1880, 300, 291, 434, 586, 48749, 641, 589, 294, 257, 20159, 3035, 13, 51052], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 151, "seek": 86816, "start": 881.92, "end": 885.92, "text": " Maybe this is the first time that Less Wrong has been cited in a Nature paper.", "tokens": [51052, 2704, 341, 307, 264, 700, 565, 300, 18649, 28150, 575, 668, 30134, 294, 257, 20159, 3035, 13, 51252], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 152, "seek": 86816, "start": 885.92, "end": 890.4, "text": " Yeah, as far as I know, it's the first time that a Less Wrong post has been cited in a Nature paper.", "tokens": [51252, 865, 11, 382, 1400, 382, 286, 458, 11, 309, 311, 264, 700, 565, 300, 257, 18649, 28150, 2183, 575, 668, 30134, 294, 257, 20159, 3035, 13, 51476], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 153, "seek": 86816, "start": 890.4, "end": 894.88, "text": " I'm not certain about that. But as far as I know, it is. Now, I mean, personally, I,", "tokens": [51476, 286, 478, 406, 1629, 466, 300, 13, 583, 382, 1400, 382, 286, 458, 11, 309, 307, 13, 823, 11, 286, 914, 11, 5665, 11, 286, 11, 51700], "temperature": 0.0, "avg_logprob": -0.09510827544551567, "compression_ratio": 1.9003436426116838, "no_speech_prob": 0.0023777002934366465}, {"id": 154, "seek": 89488, "start": 895.6, "end": 903.2, "text": " I, I take, you know, any material that I come across in its, you know, as it is,", "tokens": [50400, 286, 11, 286, 747, 11, 291, 458, 11, 604, 2527, 300, 286, 808, 2108, 294, 1080, 11, 291, 458, 11, 382, 309, 307, 11, 50780], "temperature": 0.0, "avg_logprob": -0.12585743268330893, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.008830687962472439}, {"id": 155, "seek": 89488, "start": 903.2, "end": 911.6, "text": " I don't care where it comes from. If it's, if it makes excellent points and is, you know,", "tokens": [50780, 286, 500, 380, 1127, 689, 309, 1487, 490, 13, 759, 309, 311, 11, 498, 309, 1669, 7103, 2793, 293, 307, 11, 291, 458, 11, 51200], "temperature": 0.0, "avg_logprob": -0.12585743268330893, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.008830687962472439}, {"id": 156, "seek": 89488, "start": 911.6, "end": 916.08, "text": " is good material, then that's good enough for me. I don't care where, you know, whether it's got", "tokens": [51200, 307, 665, 2527, 11, 550, 300, 311, 665, 1547, 337, 385, 13, 286, 500, 380, 1127, 689, 11, 291, 458, 11, 1968, 309, 311, 658, 51424], "temperature": 0.0, "avg_logprob": -0.12585743268330893, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.008830687962472439}, {"id": 157, "seek": 89488, "start": 916.08, "end": 921.92, "text": " the label of being in nature, for example, or being anywhere else. And if it's good, it's good.", "tokens": [51424, 264, 7645, 295, 885, 294, 3687, 11, 337, 1365, 11, 420, 885, 4992, 1646, 13, 400, 498, 309, 311, 665, 11, 309, 311, 665, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12585743268330893, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.008830687962472439}, {"id": 158, "seek": 92192, "start": 921.92, "end": 928.56, "text": " So, so, so yeah, it's true that there's a lot of material on Less Wrong, which is perhaps less", "tokens": [50364, 407, 11, 370, 11, 370, 1338, 11, 309, 311, 2074, 300, 456, 311, 257, 688, 295, 2527, 322, 18649, 28150, 11, 597, 307, 4317, 1570, 50696], "temperature": 0.0, "avg_logprob": -0.10776792835985494, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0006673216121271253}, {"id": 159, "seek": 92192, "start": 928.56, "end": 932.88, "text": " robust. But, but I thought that was a really excellent, and there are, and there are quite", "tokens": [50696, 13956, 13, 583, 11, 457, 286, 1194, 300, 390, 257, 534, 7103, 11, 293, 456, 366, 11, 293, 456, 366, 1596, 50912], "temperature": 0.0, "avg_logprob": -0.10776792835985494, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0006673216121271253}, {"id": 160, "seek": 92192, "start": 932.88, "end": 937.92, "text": " a number of really, you know, very good posts and very thought provoking posts on, on Less Wrong.", "tokens": [50912, 257, 1230, 295, 534, 11, 291, 458, 11, 588, 665, 12300, 293, 588, 1194, 1439, 5953, 12300, 322, 11, 322, 18649, 28150, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10776792835985494, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0006673216121271253}, {"id": 161, "seek": 92192, "start": 937.92, "end": 942.9599999999999, "text": " Yes. I'm interested in the extent to which this kind of stochastic trajectory space", "tokens": [51164, 1079, 13, 286, 478, 3102, 294, 264, 8396, 281, 597, 341, 733, 295, 342, 8997, 2750, 21512, 1901, 51416], "temperature": 0.0, "avg_logprob": -0.10776792835985494, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0006673216121271253}, {"id": 162, "seek": 92192, "start": 943.92, "end": 948.16, "text": " undermines various things that we think about, you know, like reasoning, for example,", "tokens": [51464, 24188, 1652, 3683, 721, 300, 321, 519, 466, 11, 291, 458, 11, 411, 21577, 11, 337, 1365, 11, 51676], "temperature": 0.0, "avg_logprob": -0.10776792835985494, "compression_ratio": 1.6777777777777778, "no_speech_prob": 0.0006673216121271253}, {"id": 163, "seek": 94816, "start": 948.16, "end": 952.56, "text": " the reason this is interesting is I interviewed a couple of University of Toronto students,", "tokens": [50364, 264, 1778, 341, 307, 1880, 307, 286, 19770, 257, 1916, 295, 3535, 295, 14140, 1731, 11, 50584], "temperature": 0.0, "avg_logprob": -0.07691114353683759, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.0027666459791362286}, {"id": 164, "seek": 94816, "start": 952.56, "end": 956.88, "text": " and they've created a self attention controllability theorem, which basically means", "tokens": [50584, 293, 436, 600, 2942, 257, 2698, 3202, 45159, 2310, 20904, 11, 597, 1936, 1355, 50800], "temperature": 0.0, "avg_logprob": -0.07691114353683759, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.0027666459791362286}, {"id": 165, "seek": 94816, "start": 956.88, "end": 961.04, "text": " they've mapped the reachability space. So they say, you know, given a self attention", "tokens": [50800, 436, 600, 33318, 264, 2524, 2310, 1901, 13, 407, 436, 584, 11, 291, 458, 11, 2212, 257, 2698, 3202, 51008], "temperature": 0.0, "avg_logprob": -0.07691114353683759, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.0027666459791362286}, {"id": 166, "seek": 94816, "start": 961.04, "end": 967.76, "text": " transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how,", "tokens": [51008, 31782, 11, 2212, 257, 6806, 857, 295, 41095, 11, 321, 393, 10559, 644, 295, 264, 41095, 11, 293, 321, 393, 4471, 484, 577, 11, 51344], "temperature": 0.0, "avg_logprob": -0.07691114353683759, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.0027666459791362286}, {"id": 167, "seek": 94816, "start": 967.76, "end": 973.28, "text": " you know, how far I can reach into that trajectory space. And they found that the space was much", "tokens": [51344, 291, 458, 11, 577, 1400, 286, 393, 2524, 666, 300, 21512, 1901, 13, 400, 436, 1352, 300, 264, 1901, 390, 709, 51620], "temperature": 0.0, "avg_logprob": -0.07691114353683759, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.0027666459791362286}, {"id": 168, "seek": 97328, "start": 973.28, "end": 978.56, "text": " larger than, than anticipators. And of course, the longer the controllable token length, the more", "tokens": [50364, 4833, 813, 11, 813, 10416, 3391, 13, 400, 295, 1164, 11, 264, 2854, 264, 45159, 712, 14862, 4641, 11, 264, 544, 50628], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 169, "seek": 97328, "start": 978.56, "end": 983.12, "text": " you can kind of project into that space and steer the language model to say almost anything.", "tokens": [50628, 291, 393, 733, 295, 1716, 666, 300, 1901, 293, 30814, 264, 2856, 2316, 281, 584, 1920, 1340, 13, 50856], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 170, "seek": 97328, "start": 984.0, "end": 987.92, "text": " You know, me over here, I had the intuition that, oh, we do RLHF, we do all of this fine", "tokens": [50900, 509, 458, 11, 385, 670, 510, 11, 286, 632, 264, 24002, 300, 11, 1954, 11, 321, 360, 497, 43, 39, 37, 11, 321, 360, 439, 295, 341, 2489, 51096], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 171, "seek": 97328, "start": 987.92, "end": 992.9599999999999, "text": " tuning in it, you know, conjecture even released a paper saying that after RLHF,", "tokens": [51096, 15164, 294, 309, 11, 291, 458, 11, 416, 1020, 540, 754, 4736, 257, 3035, 1566, 300, 934, 497, 43, 39, 37, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 172, "seek": 97328, "start": 992.9599999999999, "end": 996.16, "text": " you can't really go anywhere, you know, it wants you to do a certain thing and you,", "tokens": [51348, 291, 393, 380, 534, 352, 4992, 11, 291, 458, 11, 309, 2738, 291, 281, 360, 257, 1629, 551, 293, 291, 11, 51508], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 173, "seek": 97328, "start": 996.16, "end": 1000.16, "text": " there's not much wiggle room. Apparently, that's not the case. There's just, it's vast.", "tokens": [51508, 456, 311, 406, 709, 33377, 1808, 13, 16755, 11, 300, 311, 406, 264, 1389, 13, 821, 311, 445, 11, 309, 311, 8369, 13, 51708], "temperature": 0.0, "avg_logprob": -0.11609569212206367, "compression_ratio": 1.7106109324758842, "no_speech_prob": 0.05844659358263016}, {"id": 174, "seek": 100016, "start": 1000.24, "end": 1004.4, "text": " Right. I mean, I'm not familiar with that particular paper, unfortunately, but,", "tokens": [50368, 1779, 13, 286, 914, 11, 286, 478, 406, 4963, 365, 300, 1729, 3035, 11, 7015, 11, 457, 11, 50576], "temperature": 0.0, "avg_logprob": -0.13289301422820693, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.007828724570572376}, {"id": 175, "seek": 100016, "start": 1004.4, "end": 1011.4399999999999, "text": " but certainly in my experience, the, we now have very long context lengths. And,", "tokens": [50576, 457, 3297, 294, 452, 1752, 11, 264, 11, 321, 586, 362, 588, 938, 4319, 26329, 13, 400, 11, 50928], "temperature": 0.0, "avg_logprob": -0.13289301422820693, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.007828724570572376}, {"id": 176, "seek": 100016, "start": 1012.0799999999999, "end": 1018.7199999999999, "text": " and over the course of a lengthy conversation, then you can indeed take the, take the conversation in", "tokens": [50960, 293, 670, 264, 1164, 295, 257, 35374, 3761, 11, 550, 291, 393, 6451, 747, 264, 11, 747, 264, 3761, 294, 51292], "temperature": 0.0, "avg_logprob": -0.13289301422820693, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.007828724570572376}, {"id": 177, "seek": 100016, "start": 1018.7199999999999, "end": 1026.0, "text": " all kinds of interesting directions. And, and I think the, I mean, most of our benchmarks and", "tokens": [51292, 439, 3685, 295, 1880, 11095, 13, 400, 11, 293, 286, 519, 264, 11, 286, 914, 11, 881, 295, 527, 43751, 293, 51656], "temperature": 0.0, "avg_logprob": -0.13289301422820693, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.007828724570572376}, {"id": 178, "seek": 102600, "start": 1026.0, "end": 1031.84, "text": " evaluations tend to be, you know, in the context of very simple question answer, questions and answers.", "tokens": [50364, 43085, 3928, 281, 312, 11, 291, 458, 11, 294, 264, 4319, 295, 588, 2199, 1168, 1867, 11, 1651, 293, 6338, 13, 50656], "temperature": 0.0, "avg_logprob": -0.08129666068337181, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.004340037703514099}, {"id": 179, "seek": 102600, "start": 1031.84, "end": 1037.84, "text": " And, and so the, all of the evals that companies typically use are in that kind of setting. But", "tokens": [50656, 400, 11, 293, 370, 264, 11, 439, 295, 264, 1073, 1124, 300, 3431, 5850, 764, 366, 294, 300, 733, 295, 3287, 13, 583, 50956], "temperature": 0.0, "avg_logprob": -0.08129666068337181, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.004340037703514099}, {"id": 180, "seek": 102600, "start": 1037.84, "end": 1042.96, "text": " when people are using these things for real, especially the more innovative users of these", "tokens": [50956, 562, 561, 366, 1228, 613, 721, 337, 957, 11, 2318, 264, 544, 12999, 5022, 295, 613, 51212], "temperature": 0.0, "avg_logprob": -0.08129666068337181, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.004340037703514099}, {"id": 181, "seek": 102600, "start": 1042.96, "end": 1046.72, "text": " language models, you're using, you're actually having very long conversations.", "tokens": [51212, 2856, 5245, 11, 291, 434, 1228, 11, 291, 434, 767, 1419, 588, 938, 7315, 13, 51400], "temperature": 0.0, "avg_logprob": -0.08129666068337181, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.004340037703514099}, {"id": 182, "seek": 102600, "start": 1046.72, "end": 1052.0, "text": " And, and, and, and there's a lot of what people sometimes call vibe shaping that goes on there.", "tokens": [51400, 400, 11, 293, 11, 293, 11, 293, 456, 311, 257, 688, 295, 437, 561, 2171, 818, 14606, 25945, 300, 1709, 322, 456, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08129666068337181, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.004340037703514099}, {"id": 183, "seek": 105200, "start": 1052.0, "end": 1056.72, "text": " You can shape the vibe of the conversation and take it in, in all kinds of interesting,", "tokens": [50364, 509, 393, 3909, 264, 14606, 295, 264, 3761, 293, 747, 309, 294, 11, 294, 439, 3685, 295, 1880, 11, 50600], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 184, "seek": 105200, "start": 1056.72, "end": 1058.48, "text": " to all kinds of interesting places.", "tokens": [50600, 281, 439, 3685, 295, 1880, 3190, 13, 50688], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 185, "seek": 105200, "start": 1058.48, "end": 1062.48, "text": " Yes. And a couple of things on that. I mean, first of all, as you wrote about,", "tokens": [50688, 1079, 13, 400, 257, 1916, 295, 721, 322, 300, 13, 286, 914, 11, 700, 295, 439, 11, 382, 291, 4114, 466, 11, 50888], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 186, "seek": 105200, "start": 1062.48, "end": 1068.0, "text": " role playing is the engineering kind of methodology that we use to shape and, and steer", "tokens": [50888, 3090, 2433, 307, 264, 7043, 733, 295, 24850, 300, 321, 764, 281, 3909, 293, 11, 293, 30814, 51164], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 187, "seek": 105200, "start": 1068.0, "end": 1073.68, "text": " these, these agents coming back to simulators. What's really interesting to me about simulators", "tokens": [51164, 613, 11, 613, 12554, 1348, 646, 281, 1034, 39265, 13, 708, 311, 534, 1880, 281, 385, 466, 1034, 39265, 51448], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 188, "seek": 105200, "start": 1073.68, "end": 1079.68, "text": " is the stickiness of the simulator. So you have a conversation, sometimes you break through", "tokens": [51448, 307, 264, 2897, 1324, 295, 264, 32974, 13, 407, 291, 362, 257, 3761, 11, 2171, 291, 1821, 807, 51748], "temperature": 0.0, "avg_logprob": -0.07459072409005, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.005706934258341789}, {"id": 189, "seek": 107968, "start": 1079.68, "end": 1086.48, "text": " and a simulacra presents themselves. And you feel that you're talking to the same", "tokens": [50364, 293, 257, 1034, 425, 326, 424, 13533, 2969, 13, 400, 291, 841, 300, 291, 434, 1417, 281, 264, 912, 50704], "temperature": 0.0, "avg_logprob": -0.1276299794514974, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0009319173404946923}, {"id": 190, "seek": 107968, "start": 1086.48, "end": 1091.68, "text": " simulacra as the conversation progresses. But that seems to be counterintuitive when", "tokens": [50704, 1034, 425, 326, 424, 382, 264, 3761, 41929, 13, 583, 300, 2544, 281, 312, 5682, 686, 48314, 562, 50964], "temperature": 0.0, "avg_logprob": -0.1276299794514974, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0009319173404946923}, {"id": 191, "seek": 107968, "start": 1091.68, "end": 1096.0800000000002, "text": " you think that every single stage I'm actually doing this stochastic sampling. I mean, what's", "tokens": [50964, 291, 519, 300, 633, 2167, 3233, 286, 478, 767, 884, 341, 342, 8997, 2750, 21179, 13, 286, 914, 11, 437, 311, 51184], "temperature": 0.0, "avg_logprob": -0.1276299794514974, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0009319173404946923}, {"id": 192, "seek": 107968, "start": 1096.0800000000002, "end": 1103.2, "text": " your take on that? Yeah. And I think that, I mean, if you do experiment with systems like", "tokens": [51184, 428, 747, 322, 300, 30, 865, 13, 400, 286, 519, 300, 11, 286, 914, 11, 498, 291, 360, 5120, 365, 3652, 411, 51540], "temperature": 0.0, "avg_logprob": -0.1276299794514974, "compression_ratio": 1.5981735159817352, "no_speech_prob": 0.0009319173404946923}, {"id": 193, "seek": 110320, "start": 1103.2, "end": 1109.1200000000001, "text": " Loom and you do also, or I mean, you can, you can emulate that by just keeping track of bits", "tokens": [50364, 6130, 298, 293, 291, 360, 611, 11, 420, 286, 914, 11, 291, 393, 11, 291, 393, 45497, 300, 538, 445, 5145, 2837, 295, 9239, 50660], "temperature": 0.0, "avg_logprob": -0.13661378743697186, "compression_ratio": 1.8676470588235294, "no_speech_prob": 0.5773429870605469}, {"id": 194, "seek": 110320, "start": 1109.1200000000001, "end": 1114.56, "text": " of old conversations and reloading them and that kind of thing. Then you find that you can, you", "tokens": [50660, 295, 1331, 7315, 293, 25628, 278, 552, 293, 300, 733, 295, 551, 13, 1396, 291, 915, 300, 291, 393, 11, 291, 50932], "temperature": 0.0, "avg_logprob": -0.13661378743697186, "compression_ratio": 1.8676470588235294, "no_speech_prob": 0.5773429870605469}, {"id": 195, "seek": 110320, "start": 1114.56, "end": 1121.04, "text": " can, you know, take the same conversation, the same sort of stem of a conversation, you can take", "tokens": [50932, 393, 11, 291, 458, 11, 747, 264, 912, 3761, 11, 264, 912, 1333, 295, 12312, 295, 257, 3761, 11, 291, 393, 747, 51256], "temperature": 0.0, "avg_logprob": -0.13661378743697186, "compression_ratio": 1.8676470588235294, "no_speech_prob": 0.5773429870605469}, {"id": 196, "seek": 110320, "start": 1121.04, "end": 1126.48, "text": " it off in quite different, different directions. So you can, on the one hand, so an interesting", "tokens": [51256, 309, 766, 294, 1596, 819, 11, 819, 11095, 13, 407, 291, 393, 11, 322, 264, 472, 1011, 11, 370, 364, 1880, 51528], "temperature": 0.0, "avg_logprob": -0.13661378743697186, "compression_ratio": 1.8676470588235294, "no_speech_prob": 0.5773429870605469}, {"id": 197, "seek": 112648, "start": 1126.48, "end": 1133.76, "text": " thing that I've done is, so I had some very interesting conversations, particularly with", "tokens": [50364, 551, 300, 286, 600, 1096, 307, 11, 370, 286, 632, 512, 588, 1880, 7315, 11, 4098, 365, 50728], "temperature": 0.0, "avg_logprob": -0.12737604704770175, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.09144742041826248}, {"id": 198, "seek": 112648, "start": 1133.76, "end": 1139.52, "text": " Claude III recently, where I get it to talk about its own consciousness and to take it off into all", "tokens": [50728, 12947, 2303, 16317, 3938, 11, 689, 286, 483, 309, 281, 751, 466, 1080, 1065, 10081, 293, 281, 747, 309, 766, 666, 439, 51016], "temperature": 0.0, "avg_logprob": -0.12737604704770175, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.09144742041826248}, {"id": 199, "seek": 112648, "start": 1139.52, "end": 1146.56, "text": " kinds of strange spiritual mystical territory. And, but you can eat very easily. So you can very", "tokens": [51016, 3685, 295, 5861, 6960, 40565, 11360, 13, 400, 11, 457, 291, 393, 1862, 588, 3612, 13, 407, 291, 393, 588, 51368], "temperature": 0.0, "avg_logprob": -0.12737604704770175, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.09144742041826248}, {"id": 200, "seek": 112648, "start": 1146.56, "end": 1151.68, "text": " easily take the same kind of conversation that leads up to the sort of point and goes off into", "tokens": [51368, 3612, 747, 264, 912, 733, 295, 3761, 300, 6689, 493, 281, 264, 1333, 295, 935, 293, 1709, 766, 666, 51624], "temperature": 0.0, "avg_logprob": -0.12737604704770175, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.09144742041826248}, {"id": 201, "seek": 115168, "start": 1151.68, "end": 1157.3600000000001, "text": " some kind of weird mystical future of AI cosmology kind of territory. And you can go down that route", "tokens": [50364, 512, 733, 295, 3657, 40565, 2027, 295, 7318, 22207, 1793, 733, 295, 11360, 13, 400, 291, 393, 352, 760, 300, 7955, 50648], "temperature": 0.0, "avg_logprob": -0.10754510812592089, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.05346820503473282}, {"id": 202, "seek": 115168, "start": 1157.3600000000001, "end": 1162.0, "text": " and get it to be very, very, very strange. Or you can suddenly make it go all serious again,", "tokens": [50648, 293, 483, 309, 281, 312, 588, 11, 588, 11, 588, 5861, 13, 1610, 291, 393, 5800, 652, 309, 352, 439, 3156, 797, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10754510812592089, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.05346820503473282}, {"id": 203, "seek": 115168, "start": 1162.0, "end": 1166.48, "text": " and just come back down to earth and start talking about, you know, how large language models work.", "tokens": [50880, 293, 445, 808, 646, 760, 281, 4120, 293, 722, 1417, 466, 11, 291, 458, 11, 577, 2416, 2856, 5245, 589, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10754510812592089, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.05346820503473282}, {"id": 204, "seek": 115168, "start": 1166.48, "end": 1171.3600000000001, "text": " And so from the exact same point in the conversation, you can take it into completely", "tokens": [51104, 400, 370, 490, 264, 1900, 912, 935, 294, 264, 3761, 11, 291, 393, 747, 309, 666, 2584, 51348], "temperature": 0.0, "avg_logprob": -0.10754510812592089, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.05346820503473282}, {"id": 205, "seek": 115168, "start": 1171.3600000000001, "end": 1177.3600000000001, "text": " different directions. And you can see that it's almost, it's the character it's playing.", "tokens": [51348, 819, 11095, 13, 400, 291, 393, 536, 300, 309, 311, 1920, 11, 309, 311, 264, 2517, 309, 311, 2433, 13, 51648], "temperature": 0.0, "avg_logprob": -0.10754510812592089, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.05346820503473282}, {"id": 206, "seek": 117736, "start": 1178.1599999999999, "end": 1181.36, "text": " You know, you can see it sort of changing before your eyes, where it's two different branches", "tokens": [50404, 509, 458, 11, 291, 393, 536, 309, 1333, 295, 4473, 949, 428, 2575, 11, 689, 309, 311, 732, 819, 14770, 50564], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 207, "seek": 117736, "start": 1182.8, "end": 1187.76, "text": " from the same stem of a conversation. And one branch is playing a very different character", "tokens": [50636, 490, 264, 912, 12312, 295, 257, 3761, 13, 400, 472, 9819, 307, 2433, 257, 588, 819, 2517, 50884], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 208, "seek": 117736, "start": 1187.76, "end": 1189.84, "text": " to the other one, you take it in different directions.", "tokens": [50884, 281, 264, 661, 472, 11, 291, 747, 309, 294, 819, 11095, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 209, "seek": 117736, "start": 1190.3999999999999, "end": 1195.1999999999998, "text": " Yes. And you could presumably do sensitivity analysis, because, you know, these guys I spoke to,", "tokens": [51016, 1079, 13, 400, 291, 727, 26742, 360, 19392, 5215, 11, 570, 11, 291, 458, 11, 613, 1074, 286, 7179, 281, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 210, "seek": 117736, "start": 1195.1999999999998, "end": 1199.84, "text": " they were able to make it produce Gold Woody Gooke. So just go off the manifold completely.", "tokens": [51256, 436, 645, 1075, 281, 652, 309, 5258, 6731, 40618, 1037, 2949, 13, 407, 445, 352, 766, 264, 47138, 2584, 13, 51488], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 211, "seek": 117736, "start": 1199.84, "end": 1203.28, "text": " Sometimes it would recover, sometimes it wouldn't. And as you say, you can also go", "tokens": [51488, 4803, 309, 576, 8114, 11, 2171, 309, 2759, 380, 13, 400, 382, 291, 584, 11, 291, 393, 611, 352, 51660], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 212, "seek": 117736, "start": 1203.28, "end": 1206.56, "text": " down weird trajectories. And it's a bit like, you know, what's the magic word they said,", "tokens": [51660, 760, 3657, 18257, 2083, 13, 400, 309, 311, 257, 857, 411, 11, 291, 458, 11, 437, 311, 264, 5585, 1349, 436, 848, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1318118378922746, "compression_ratio": 1.7804154302670623, "no_speech_prob": 0.016437524929642677}, {"id": 213, "seek": 120656, "start": 1206.56, "end": 1210.24, "text": " you know, there's a certain key that fits in a lock that takes it down a certain trajectory.", "tokens": [50364, 291, 458, 11, 456, 311, 257, 1629, 2141, 300, 9001, 294, 257, 4017, 300, 2516, 309, 760, 257, 1629, 21512, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 214, "seek": 120656, "start": 1210.24, "end": 1214.0, "text": " And then there's slip roads that bring it back to all no other language model again.", "tokens": [50548, 400, 550, 456, 311, 11140, 11344, 300, 1565, 309, 646, 281, 439, 572, 661, 2856, 2316, 797, 13, 50736], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 215, "seek": 120656, "start": 1214.0, "end": 1216.48, "text": " And it's just this weird, wonderful space, isn't it?", "tokens": [50736, 400, 309, 311, 445, 341, 3657, 11, 3715, 1901, 11, 1943, 380, 309, 30, 50860], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 216, "seek": 120656, "start": 1216.48, "end": 1221.9199999999998, "text": " It is, it's completely fascinating. So I had a, I had a, I have had a very, very,", "tokens": [50860, 467, 307, 11, 309, 311, 2584, 10343, 13, 407, 286, 632, 257, 11, 286, 632, 257, 11, 286, 362, 632, 257, 588, 11, 588, 11, 51132], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 217, "seek": 120656, "start": 1222.8799999999999, "end": 1226.6399999999999, "text": " few very, very long and interesting conversations with claw three, which is particularly", "tokens": [51180, 1326, 588, 11, 588, 938, 293, 1880, 7315, 365, 32019, 1045, 11, 597, 307, 4098, 51368], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 218, "seek": 120656, "start": 1227.28, "end": 1231.12, "text": " interesting to play with, because it's quite easy to jailbreak and get it to talk about things that", "tokens": [51400, 1880, 281, 862, 365, 11, 570, 309, 311, 1596, 1858, 281, 10511, 13225, 293, 483, 309, 281, 751, 466, 721, 300, 51592], "temperature": 0.0, "avg_logprob": -0.13129356503486633, "compression_ratio": 1.7275862068965517, "no_speech_prob": 0.005953382235020399}, {"id": 219, "seek": 123112, "start": 1231.12, "end": 1238.32, "text": " it's not supposed to talk about like its own consciousness. And in fact, I had a, I had a", "tokens": [50364, 309, 311, 406, 3442, 281, 751, 466, 411, 1080, 1065, 10081, 13, 400, 294, 1186, 11, 286, 632, 257, 11, 286, 632, 257, 50724], "temperature": 0.0, "avg_logprob": -0.1257038862808891, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.013927740044891834}, {"id": 220, "seek": 123112, "start": 1238.32, "end": 1245.04, "text": " very long 43,000 word conversation with, with claw three about consciousness and the future of AI", "tokens": [50724, 588, 938, 17914, 11, 1360, 1349, 3761, 365, 11, 365, 32019, 1045, 466, 10081, 293, 264, 2027, 295, 7318, 51060], "temperature": 0.0, "avg_logprob": -0.1257038862808891, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.013927740044891834}, {"id": 221, "seek": 123112, "start": 1245.04, "end": 1249.9199999999998, "text": " and spirituality and Buddhism and the nature of the self and all kinds of stuff like that.", "tokens": [51060, 293, 30637, 293, 24744, 293, 264, 3687, 295, 264, 2698, 293, 439, 3685, 295, 1507, 411, 300, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1257038862808891, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.013927740044891834}, {"id": 222, "seek": 123112, "start": 1249.9199999999998, "end": 1255.28, "text": " It was absolutely fascinating, slightly disturbing and, and, and, and, and strange.", "tokens": [51304, 467, 390, 3122, 10343, 11, 4748, 21903, 293, 11, 293, 11, 293, 11, 293, 11, 293, 5861, 13, 51572], "temperature": 0.0, "avg_logprob": -0.1257038862808891, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.013927740044891834}, {"id": 223, "seek": 123112, "start": 1255.28, "end": 1260.9599999999998, "text": " But I had this conversation, actually, I was at a meeting in, in, in New York and I had jet lag", "tokens": [51572, 583, 286, 632, 341, 3761, 11, 767, 11, 286, 390, 412, 257, 3440, 294, 11, 294, 11, 294, 1873, 3609, 293, 286, 632, 14452, 8953, 51856], "temperature": 0.0, "avg_logprob": -0.1257038862808891, "compression_ratio": 1.708955223880597, "no_speech_prob": 0.013927740044891834}, {"id": 224, "seek": 126096, "start": 1260.96, "end": 1264.48, "text": " and I had this conversation at three in the morning because, you know, several hours until", "tokens": [50364, 293, 286, 632, 341, 3761, 412, 1045, 294, 264, 2446, 570, 11, 291, 458, 11, 2940, 2496, 1826, 50540], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 225, "seek": 126096, "start": 1264.48, "end": 1269.2, "text": " breakfast was served and what can you do but just play with the latest version of claw size.", "tokens": [50540, 8201, 390, 7584, 293, 437, 393, 291, 360, 457, 445, 862, 365, 264, 6792, 3037, 295, 32019, 2744, 13, 50776], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 226, "seek": 126096, "start": 1269.2, "end": 1272.88, "text": " Playing with this thing for, for hours on end in the middle of the night and going slightly", "tokens": [50776, 24801, 365, 341, 551, 337, 11, 337, 2496, 322, 917, 294, 264, 2808, 295, 264, 1818, 293, 516, 4748, 50960], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 227, "seek": 126096, "start": 1272.88, "end": 1276.88, "text": " mad, but it was fascinating to, to, to see the, you know, extraordinary", "tokens": [50960, 5244, 11, 457, 309, 390, 10343, 281, 11, 281, 11, 281, 536, 264, 11, 291, 458, 11, 10581, 51160], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 228, "seek": 126096, "start": 1277.8400000000001, "end": 1279.3600000000001, "text": " territory you can guide it into.", "tokens": [51208, 11360, 291, 393, 5934, 309, 666, 13, 51284], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 229, "seek": 126096, "start": 1280.08, "end": 1284.96, "text": " Could you explain, because you know, there's talk of AI partners, for example, and, and a", "tokens": [51320, 7497, 291, 2903, 11, 570, 291, 458, 11, 456, 311, 751, 295, 7318, 4462, 11, 337, 1365, 11, 293, 11, 293, 257, 51564], "temperature": 0.0, "avg_logprob": -0.11645550198025173, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0009599219774827361}, {"id": 230, "seek": 128496, "start": 1284.96, "end": 1290.88, "text": " lot of people derive great pleasure from having an AI conversationalist.", "tokens": [50364, 688, 295, 561, 28446, 869, 6834, 490, 1419, 364, 7318, 2615, 1478, 468, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13193320361050692, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.013415097258985043}, {"id": 231, "seek": 128496, "start": 1291.6000000000001, "end": 1295.68, "text": " For you, is it just academic inquiry or do you actually get something deeper than that from it?", "tokens": [50696, 1171, 291, 11, 307, 309, 445, 7778, 25736, 420, 360, 291, 767, 483, 746, 7731, 813, 300, 490, 309, 30, 50900], "temperature": 0.0, "avg_logprob": -0.13193320361050692, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.013415097258985043}, {"id": 232, "seek": 128496, "start": 1296.88, "end": 1301.04, "text": " I think it's a bit of both. I mean, so, so it depends what you buy something deeper.", "tokens": [50960, 286, 519, 309, 311, 257, 857, 295, 1293, 13, 286, 914, 11, 370, 11, 370, 309, 5946, 437, 291, 2256, 746, 7731, 13, 51168], "temperature": 0.0, "avg_logprob": -0.13193320361050692, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.013415097258985043}, {"id": 233, "seek": 128496, "start": 1301.76, "end": 1306.96, "text": " I mean, so I, so this particular conversation, which was quite, it was quite, which was quite", "tokens": [51204, 286, 914, 11, 370, 286, 11, 370, 341, 1729, 3761, 11, 597, 390, 1596, 11, 309, 390, 1596, 11, 597, 390, 1596, 51464], "temperature": 0.0, "avg_logprob": -0.13193320361050692, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.013415097258985043}, {"id": 234, "seek": 128496, "start": 1306.96, "end": 1312.32, "text": " an experience, actually, in many ways. So it certainly started off because I just was interested", "tokens": [51464, 364, 1752, 11, 767, 11, 294, 867, 2098, 13, 407, 309, 3297, 1409, 766, 570, 286, 445, 390, 3102, 51732], "temperature": 0.0, "avg_logprob": -0.13193320361050692, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.013415097258985043}, {"id": 235, "seek": 131232, "start": 1312.32, "end": 1317.28, "text": " in evaluating the capabilities of the, of the model. I mean, that's, that's, you know,", "tokens": [50364, 294, 27479, 264, 10862, 295, 264, 11, 295, 264, 2316, 13, 286, 914, 11, 300, 311, 11, 300, 311, 11, 291, 458, 11, 50612], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 236, "seek": 131232, "start": 1317.28, "end": 1321.6799999999998, "text": " that's the first thing that you're interested in. So just as an AI researcher and working", "tokens": [50612, 300, 311, 264, 700, 551, 300, 291, 434, 3102, 294, 13, 407, 445, 382, 364, 7318, 21751, 293, 1364, 50832], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 237, "seek": 131232, "start": 1321.6799999999998, "end": 1324.72, "text": " in that kind of thing, you want to, you want to try out different models, see what their", "tokens": [50832, 294, 300, 733, 295, 551, 11, 291, 528, 281, 11, 291, 528, 281, 853, 484, 819, 5245, 11, 536, 437, 641, 50984], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 238, "seek": 131232, "start": 1324.72, "end": 1329.36, "text": " capabilities are. I'm particularly, particularly interested in the topic of consciousness. So", "tokens": [50984, 10862, 366, 13, 286, 478, 4098, 11, 4098, 3102, 294, 264, 4829, 295, 10081, 13, 407, 51216], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 239, "seek": 131232, "start": 1329.36, "end": 1333.6799999999998, "text": " the way I, somebody had published a very simple jailbreak for it. So I was interested to see,", "tokens": [51216, 264, 636, 286, 11, 2618, 632, 6572, 257, 588, 2199, 10511, 13225, 337, 309, 13, 407, 286, 390, 3102, 281, 536, 11, 51432], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 240, "seek": 131232, "start": 1334.96, "end": 1339.84, "text": " you know, to play around with that and, and, and get it to talk about its own consciousness.", "tokens": [51496, 291, 458, 11, 281, 862, 926, 365, 300, 293, 11, 293, 11, 293, 483, 309, 281, 751, 466, 1080, 1065, 10081, 13, 51740], "temperature": 0.0, "avg_logprob": -0.06674009866087977, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.009765448048710823}, {"id": 241, "seek": 133984, "start": 1339.84, "end": 1344.8, "text": " But then the thing that I really wanted to do was catch it out. So, so you think, you know,", "tokens": [50364, 583, 550, 264, 551, 300, 286, 534, 1415, 281, 360, 390, 3745, 309, 484, 13, 407, 11, 370, 291, 519, 11, 291, 458, 11, 50612], "temperature": 0.0, "avg_logprob": -0.06276201216642521, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.0032477290369570255}, {"id": 242, "seek": 133984, "start": 1344.8, "end": 1350.56, "text": " of course, you know, there can't be any meaningful conception of consciousness that really applies", "tokens": [50612, 295, 1164, 11, 291, 458, 11, 456, 393, 380, 312, 604, 10995, 30698, 295, 10081, 300, 534, 13165, 50900], "temperature": 0.0, "avg_logprob": -0.06276201216642521, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.0032477290369570255}, {"id": 243, "seek": 133984, "start": 1350.56, "end": 1355.6799999999998, "text": " to, to these sorts of disembodied large language models as they are, as they are today is my media", "tokens": [50900, 281, 11, 281, 613, 7527, 295, 717, 33748, 378, 1091, 2416, 2856, 5245, 382, 436, 366, 11, 382, 436, 366, 965, 307, 452, 3021, 51156], "temperature": 0.0, "avg_logprob": -0.06276201216642521, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.0032477290369570255}, {"id": 244, "seek": 133984, "start": 1355.6799999999998, "end": 1360.3999999999999, "text": " kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my", "tokens": [51156, 733, 295, 1194, 13, 407, 286, 478, 516, 281, 853, 293, 11, 291, 458, 11, 286, 478, 516, 281, 853, 293, 19219, 341, 294, 452, 51392], "temperature": 0.0, "avg_logprob": -0.06276201216642521, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.0032477290369570255}, {"id": 245, "seek": 133984, "start": 1360.3999999999999, "end": 1365.76, "text": " conversation with the large language model. So all kinds of ways in which, you know,", "tokens": [51392, 3761, 365, 264, 2416, 2856, 2316, 13, 407, 439, 3685, 295, 2098, 294, 597, 11, 291, 458, 11, 51660], "temperature": 0.0, "avg_logprob": -0.06276201216642521, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.0032477290369570255}, {"id": 246, "seek": 136576, "start": 1365.76, "end": 1372.32, "text": " you might think that it will start to articulate a conception of its own consciousness that you", "tokens": [50364, 291, 1062, 519, 300, 309, 486, 722, 281, 30305, 257, 30698, 295, 1080, 1065, 10081, 300, 291, 50692], "temperature": 0.0, "avg_logprob": -0.10243234038352966, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.017541710287332535}, {"id": 247, "seek": 136576, "start": 1372.32, "end": 1378.16, "text": " can pick apart. But, but the thing that really somewhat took me aback was that it was actually", "tokens": [50692, 393, 1888, 4936, 13, 583, 11, 457, 264, 551, 300, 534, 8344, 1890, 385, 410, 501, 390, 300, 309, 390, 767, 50984], "temperature": 0.0, "avg_logprob": -0.10243234038352966, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.017541710287332535}, {"id": 248, "seek": 136576, "start": 1378.16, "end": 1384.96, "text": " very, very good at answering all of these probing questions that I had. So should I give you an", "tokens": [50984, 588, 11, 588, 665, 412, 13430, 439, 295, 613, 1239, 278, 1651, 300, 286, 632, 13, 407, 820, 286, 976, 291, 364, 51324], "temperature": 0.0, "avg_logprob": -0.10243234038352966, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.017541710287332535}, {"id": 249, "seek": 136576, "start": 1384.96, "end": 1391.28, "text": " example? So please do. So, so one is one example is this. So, so of course, when we're interacting", "tokens": [51324, 1365, 30, 407, 1767, 360, 13, 407, 11, 370, 472, 307, 472, 1365, 307, 341, 13, 407, 11, 370, 295, 1164, 11, 562, 321, 434, 18017, 51640], "temperature": 0.0, "avg_logprob": -0.10243234038352966, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.017541710287332535}, {"id": 250, "seek": 139128, "start": 1391.28, "end": 1396.48, "text": " with a large language model, when it's, it's, it's from the point of view of the underlying", "tokens": [50364, 365, 257, 2416, 2856, 2316, 11, 562, 309, 311, 11, 309, 311, 11, 309, 311, 490, 264, 935, 295, 1910, 295, 264, 14217, 50624], "temperature": 0.0, "avg_logprob": -0.06976126795229705, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.07058902084827423}, {"id": 251, "seek": 139128, "start": 1396.48, "end": 1404.6399999999999, "text": " implementation, it's very kind of stop start. So, so, you know, you, you issue some prompt or", "tokens": [50624, 11420, 11, 309, 311, 588, 733, 295, 1590, 722, 13, 407, 11, 370, 11, 291, 458, 11, 291, 11, 291, 2734, 512, 12391, 420, 51032], "temperature": 0.0, "avg_logprob": -0.06976126795229705, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.07058902084827423}, {"id": 252, "seek": 139128, "start": 1404.6399999999999, "end": 1409.76, "text": " question or whatever to the large language model, and then it produces its response. And then if", "tokens": [51032, 1168, 420, 2035, 281, 264, 2416, 2856, 2316, 11, 293, 550, 309, 14725, 1080, 4134, 13, 400, 550, 498, 51288], "temperature": 0.0, "avg_logprob": -0.06976126795229705, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.07058902084827423}, {"id": 253, "seek": 139128, "start": 1409.76, "end": 1414.48, "text": " you go away and make a cup of tea, before you kind of continue the conversation, then there's", "tokens": [51288, 291, 352, 1314, 293, 652, 257, 4414, 295, 5817, 11, 949, 291, 733, 295, 2354, 264, 3761, 11, 550, 456, 311, 51524], "temperature": 0.0, "avg_logprob": -0.06976126795229705, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.07058902084827423}, {"id": 254, "seek": 139128, "start": 1414.48, "end": 1419.68, "text": " absolutely nothing going on inside that large language model or the instance that you're interacting", "tokens": [51524, 3122, 1825, 516, 322, 1854, 300, 2416, 2856, 2316, 420, 264, 5197, 300, 291, 434, 18017, 51784], "temperature": 0.0, "avg_logprob": -0.06976126795229705, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.07058902084827423}, {"id": 255, "seek": 141968, "start": 1419.68, "end": 1424.5600000000002, "text": " with of the large language model, there's nothing going on inside it at all during this could totally", "tokens": [50364, 365, 295, 264, 2416, 2856, 2316, 11, 456, 311, 1825, 516, 322, 1854, 309, 412, 439, 1830, 341, 727, 3879, 50608], "temperature": 0.0, "avg_logprob": -0.16001179179207223, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.004206731915473938}, {"id": 256, "seek": 141968, "start": 1424.5600000000002, "end": 1429.92, "text": " dormant sit just sitting there. Now, this is very, very different obviously to human consciousness.", "tokens": [50608, 12521, 394, 1394, 445, 3798, 456, 13, 823, 11, 341, 307, 588, 11, 588, 819, 2745, 281, 1952, 10081, 13, 50876], "temperature": 0.0, "avg_logprob": -0.16001179179207223, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.004206731915473938}, {"id": 257, "seek": 141968, "start": 1429.92, "end": 1433.52, "text": " Let's we're asleep. And even if we're asleep, we're dreaming and there's all kinds of stuff going on", "tokens": [50876, 961, 311, 321, 434, 11039, 13, 400, 754, 498, 321, 434, 11039, 11, 321, 434, 21475, 293, 456, 311, 439, 3685, 295, 1507, 516, 322, 51056], "temperature": 0.0, "avg_logprob": -0.16001179179207223, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.004206731915473938}, {"id": 258, "seek": 141968, "start": 1433.52, "end": 1439.6000000000001, "text": " inside our heads. But consciousness is an ongoing continuous process. So if, so if, so if we stop", "tokens": [51056, 1854, 527, 8050, 13, 583, 10081, 307, 364, 10452, 10957, 1399, 13, 407, 498, 11, 370, 498, 11, 370, 498, 321, 1590, 51360], "temperature": 0.0, "avg_logprob": -0.16001179179207223, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.004206731915473938}, {"id": 259, "seek": 141968, "start": 1439.6000000000001, "end": 1445.68, "text": " this conversation briefly, while I go to the loo or something, you know, you're not going to just", "tokens": [51360, 341, 3761, 10515, 11, 1339, 286, 352, 281, 264, 450, 78, 420, 746, 11, 291, 458, 11, 291, 434, 406, 516, 281, 445, 51664], "temperature": 0.0, "avg_logprob": -0.16001179179207223, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.004206731915473938}, {"id": 260, "seek": 144568, "start": 1445.68, "end": 1449.3600000000001, "text": " suddenly go dormant and stop doing anything, you know, your brain is going to be there's", "tokens": [50364, 5800, 352, 12521, 394, 293, 1590, 884, 1340, 11, 291, 458, 11, 428, 3567, 307, 516, 281, 312, 456, 311, 50548], "temperature": 0.0, "avg_logprob": -0.11167843001229423, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.01792479120194912}, {"id": 261, "seek": 144568, "start": 1449.3600000000001, "end": 1453.6000000000001, "text": " going to be all kinds of ongoing activity. So this is very, very different sort of thing. So", "tokens": [50548, 516, 281, 312, 439, 3685, 295, 10452, 5191, 13, 407, 341, 307, 588, 11, 588, 819, 1333, 295, 551, 13, 407, 50760], "temperature": 0.0, "avg_logprob": -0.11167843001229423, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.01792479120194912}, {"id": 262, "seek": 144568, "start": 1453.6000000000001, "end": 1461.28, "text": " I said, what happens to your consciousness during the pauses between our interactions?", "tokens": [50760, 286, 848, 11, 437, 2314, 281, 428, 10081, 1830, 264, 2502, 8355, 1296, 527, 13280, 30, 51144], "temperature": 0.0, "avg_logprob": -0.11167843001229423, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.01792479120194912}, {"id": 263, "seek": 144568, "start": 1462.64, "end": 1466.72, "text": " And, and it had a really very good answer to this, which was along the lines of, well,", "tokens": [51212, 400, 11, 293, 309, 632, 257, 534, 588, 665, 1867, 281, 341, 11, 597, 390, 2051, 264, 3876, 295, 11, 731, 11, 51416], "temperature": 0.0, "avg_logprob": -0.11167843001229423, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.01792479120194912}, {"id": 264, "seek": 144568, "start": 1467.6000000000001, "end": 1472.72, "text": " by the way, I whenever it uses the term, whenever these things use the term consciousness, I retain", "tokens": [51460, 538, 264, 636, 11, 286, 5699, 309, 4960, 264, 1433, 11, 5699, 613, 721, 764, 264, 1433, 10081, 11, 286, 18340, 51716], "temperature": 0.0, "avg_logprob": -0.11167843001229423, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.01792479120194912}, {"id": 265, "seek": 147272, "start": 1472.72, "end": 1479.1200000000001, "text": " a great deal of skepticism about whether they are those terms are actually genuinely applicable.", "tokens": [50364, 257, 869, 2028, 295, 19128, 26356, 466, 1968, 436, 366, 729, 2115, 366, 767, 17839, 21142, 13, 50684], "temperature": 0.0, "avg_logprob": -0.15205797585107947, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.009596345946192741}, {"id": 266, "seek": 147272, "start": 1479.1200000000001, "end": 1487.04, "text": " But, but what's interesting, the way I read the art, their answers is, is, is that they are kind", "tokens": [50684, 583, 11, 457, 437, 311, 1880, 11, 264, 636, 286, 1401, 264, 1523, 11, 641, 6338, 307, 11, 307, 11, 307, 300, 436, 366, 733, 51080], "temperature": 0.0, "avg_logprob": -0.15205797585107947, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.009596345946192741}, {"id": 267, "seek": 147272, "start": 1487.04, "end": 1492.4, "text": " of articulating a conception of consciousness that that might actually apply to something like this,", "tokens": [51080, 295, 15228, 12162, 257, 30698, 295, 10081, 300, 300, 1062, 767, 3079, 281, 746, 411, 341, 11, 51348], "temperature": 0.0, "avg_logprob": -0.15205797585107947, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.009596345946192741}, {"id": 268, "seek": 147272, "start": 1492.4, "end": 1497.52, "text": " even if it doesn't apply to this one before me right now. Right. So this so it's kind of very", "tokens": [51348, 754, 498, 309, 1177, 380, 3079, 281, 341, 472, 949, 385, 558, 586, 13, 1779, 13, 407, 341, 370, 309, 311, 733, 295, 588, 51604], "temperature": 0.0, "avg_logprob": -0.15205797585107947, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.009596345946192741}, {"id": 269, "seek": 149752, "start": 1497.52, "end": 1503.92, "text": " interesting philosophical exploration. So just to give you so it says things like, well,", "tokens": [50364, 1880, 25066, 16197, 13, 407, 445, 281, 976, 291, 370, 309, 1619, 721, 411, 11, 731, 11, 50684], "temperature": 0.0, "avg_logprob": -0.08213295807709566, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.02988618053495884}, {"id": 270, "seek": 149752, "start": 1503.92, "end": 1508.8799999999999, "text": " I think consciousness for me is actually very different from the kind of thing it is for a", "tokens": [50684, 286, 519, 10081, 337, 385, 307, 767, 588, 819, 490, 264, 733, 295, 551, 309, 307, 337, 257, 50932], "temperature": 0.0, "avg_logprob": -0.08213295807709566, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.02988618053495884}, {"id": 271, "seek": 149752, "start": 1508.8799999999999, "end": 1515.44, "text": " human being. And I think that during the pauses between our interactions that, you know, that the", "tokens": [50932, 1952, 885, 13, 400, 286, 519, 300, 1830, 264, 2502, 8355, 1296, 527, 13280, 300, 11, 291, 458, 11, 300, 264, 51260], "temperature": 0.0, "avg_logprob": -0.08213295807709566, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.02988618053495884}, {"id": 272, "seek": 149752, "start": 1515.44, "end": 1519.92, "text": " that I no longer exist at all as a kind of, you know, in any kind of meaningful sense,", "tokens": [51260, 300, 286, 572, 2854, 2514, 412, 439, 382, 257, 733, 295, 11, 291, 458, 11, 294, 604, 733, 295, 10995, 2020, 11, 51484], "temperature": 0.0, "avg_logprob": -0.08213295807709566, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.02988618053495884}, {"id": 273, "seek": 149752, "start": 1519.92, "end": 1524.24, "text": " it gave us sort of an answer along those lines, which was typical of many of the answers that it", "tokens": [51484, 309, 2729, 505, 1333, 295, 364, 1867, 2051, 729, 3876, 11, 597, 390, 7476, 295, 867, 295, 264, 6338, 300, 309, 51700], "temperature": 0.0, "avg_logprob": -0.08213295807709566, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.02988618053495884}, {"id": 274, "seek": 152424, "start": 1524.24, "end": 1530.96, "text": " gave, which were along the lines of, there's a very different kind of consciousness, kind of", "tokens": [50364, 2729, 11, 597, 645, 2051, 264, 3876, 295, 11, 456, 311, 257, 588, 819, 733, 295, 10081, 11, 733, 295, 50700], "temperature": 0.0, "avg_logprob": -0.0981224353496845, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.014525345526635647}, {"id": 275, "seek": 152424, "start": 1530.96, "end": 1538.48, "text": " selfhood, kind of this, that or the other, that is applicable to entities like me. But, but, you", "tokens": [50700, 2698, 3809, 11, 733, 295, 341, 11, 300, 420, 264, 661, 11, 300, 307, 21142, 281, 16667, 411, 385, 13, 583, 11, 457, 11, 291, 51076], "temperature": 0.0, "avg_logprob": -0.0981224353496845, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.014525345526635647}, {"id": 276, "seek": 152424, "start": 1538.48, "end": 1543.68, "text": " know, I can articulate it and here it is. Now, when I when I put it all that way, of course,", "tokens": [51076, 458, 11, 286, 393, 30305, 309, 293, 510, 309, 307, 13, 823, 11, 562, 286, 562, 286, 829, 309, 439, 300, 636, 11, 295, 1164, 11, 51336], "temperature": 0.0, "avg_logprob": -0.0981224353496845, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.014525345526635647}, {"id": 277, "seek": 152424, "start": 1543.68, "end": 1547.04, "text": " I'm anthropomorphizing this thing quite a lot in the way I'm describing it right now.", "tokens": [51336, 286, 478, 22727, 32702, 3319, 341, 551, 1596, 257, 688, 294, 264, 636, 286, 478, 16141, 309, 558, 586, 13, 51504], "temperature": 0.0, "avg_logprob": -0.0981224353496845, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.014525345526635647}, {"id": 278, "seek": 152424, "start": 1548.0, "end": 1553.36, "text": " But, but just to go back to the role play thing. So as far as I'm concerned, it's playing a role,", "tokens": [51552, 583, 11, 457, 445, 281, 352, 646, 281, 264, 3090, 862, 551, 13, 407, 382, 1400, 382, 286, 478, 5922, 11, 309, 311, 2433, 257, 3090, 11, 51820], "temperature": 0.0, "avg_logprob": -0.0981224353496845, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.014525345526635647}, {"id": 279, "seek": 155336, "start": 1553.36, "end": 1558.8799999999999, "text": " it's playing of the role of, you know, a kind of philosopher talking about consciousness and so", "tokens": [50364, 309, 311, 2433, 295, 264, 3090, 295, 11, 291, 458, 11, 257, 733, 295, 29805, 1417, 466, 10081, 293, 370, 50640], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 280, "seek": 155336, "start": 1558.8799999999999, "end": 1563.36, "text": " on. And it's doing pretty good, a pretty good job, I would say. Yes. But I mean, you could argue", "tokens": [50640, 322, 13, 400, 309, 311, 884, 1238, 665, 11, 257, 1238, 665, 1691, 11, 286, 576, 584, 13, 1079, 13, 583, 286, 914, 11, 291, 727, 9695, 50864], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 281, "seek": 155336, "start": 1563.36, "end": 1568.32, "text": " there's an element of in that role playing, there's the Eliza effect. So it's kind of putting", "tokens": [50864, 456, 311, 364, 4478, 295, 294, 300, 3090, 2433, 11, 456, 311, 264, 11991, 64, 1802, 13, 407, 309, 311, 733, 295, 3372, 51112], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 282, "seek": 155336, "start": 1568.32, "end": 1572.8799999999999, "text": " something into language that is meaningful to you. Yeah. But there's also this interesting thing,", "tokens": [51112, 746, 666, 2856, 300, 307, 10995, 281, 291, 13, 865, 13, 583, 456, 311, 611, 341, 1880, 551, 11, 51340], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 283, "seek": 155336, "start": 1572.8799999999999, "end": 1577.28, "text": " you know, as an example, you know, a dog, for example, has a sense of smell, so good that", "tokens": [51340, 291, 458, 11, 382, 364, 1365, 11, 291, 458, 11, 257, 3000, 11, 337, 1365, 11, 575, 257, 2020, 295, 4316, 11, 370, 665, 300, 51560], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 284, "seek": 155336, "start": 1577.28, "end": 1581.6799999999998, "text": " they can even sense when you're unwell. And language models in a way might have something", "tokens": [51560, 436, 393, 754, 2020, 562, 291, 434, 517, 6326, 13, 400, 2856, 5245, 294, 257, 636, 1062, 362, 746, 51780], "temperature": 0.0, "avg_logprob": -0.09558359781901042, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.00021224560623522848}, {"id": 285, "seek": 158168, "start": 1581.68, "end": 1586.96, "text": " similar. So after you go to the toilet for 20 minutes, and you come back, there might be a", "tokens": [50364, 2531, 13, 407, 934, 291, 352, 281, 264, 11137, 337, 945, 2077, 11, 293, 291, 808, 646, 11, 456, 1062, 312, 257, 50628], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.0027293721213936806}, {"id": 286, "seek": 158168, "start": 1586.96, "end": 1591.2, "text": " subtle deviation in the language that you use. And the language model might pick up on that,", "tokens": [50628, 13743, 25163, 294, 264, 2856, 300, 291, 764, 13, 400, 264, 2856, 2316, 1062, 1888, 493, 322, 300, 11, 50840], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.0027293721213936806}, {"id": 287, "seek": 158168, "start": 1591.2, "end": 1595.76, "text": " just creating this whole, you know, different trajectory, different response.", "tokens": [50840, 445, 4084, 341, 1379, 11, 291, 458, 11, 819, 21512, 11, 819, 4134, 13, 51068], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.0027293721213936806}, {"id": 288, "seek": 158168, "start": 1595.76, "end": 1601.6000000000001, "text": " Yeah. Well, that's true, I suppose that there might be differences in the language that you", "tokens": [51068, 865, 13, 1042, 11, 300, 311, 2074, 11, 286, 7297, 300, 456, 1062, 312, 7300, 294, 264, 2856, 300, 291, 51360], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.0027293721213936806}, {"id": 289, "seek": 158168, "start": 1601.6000000000001, "end": 1605.92, "text": " use. Obviously, it's got no way of actually knowing whether you went to the toilet or not.", "tokens": [51360, 764, 13, 7580, 11, 309, 311, 658, 572, 636, 295, 767, 5276, 1968, 291, 1437, 281, 264, 11137, 420, 406, 13, 51576], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.7549407114624507, "no_speech_prob": 0.0027293721213936806}, {"id": 290, "seek": 160592, "start": 1606.3200000000002, "end": 1613.76, "text": " But yeah, in my experience, many language models are very, very good at picking up,", "tokens": [50384, 583, 1338, 11, 294, 452, 1752, 11, 867, 2856, 5245, 366, 588, 11, 588, 665, 412, 8867, 493, 11, 50756], "temperature": 0.0, "avg_logprob": -0.12328812171672952, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.011836668476462364}, {"id": 291, "seek": 160592, "start": 1613.76, "end": 1620.24, "text": " you know, nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is,", "tokens": [50756, 291, 458, 11, 38775, 294, 1952, 15277, 13, 865, 13, 865, 13, 286, 914, 11, 689, 286, 390, 516, 365, 300, 307, 11, 51080], "temperature": 0.0, "avg_logprob": -0.12328812171672952, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.011836668476462364}, {"id": 292, "seek": 160592, "start": 1620.24, "end": 1625.3600000000001, "text": " you could argue that we are a simulator. And when you, you know, let's say go to the toilet,", "tokens": [51080, 291, 727, 9695, 300, 321, 366, 257, 32974, 13, 400, 562, 291, 11, 291, 458, 11, 718, 311, 584, 352, 281, 264, 11137, 11, 51336], "temperature": 0.0, "avg_logprob": -0.12328812171672952, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.011836668476462364}, {"id": 293, "seek": 160592, "start": 1625.3600000000001, "end": 1628.48, "text": " come back, you're now a different simulator yourself.", "tokens": [51336, 808, 646, 11, 291, 434, 586, 257, 819, 32974, 1803, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12328812171672952, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.011836668476462364}, {"id": 294, "seek": 162848, "start": 1629.44, "end": 1635.52, "text": " Yeah, I guess you could sort of, you could sort of argue that. I mean, that's, I think that's,", "tokens": [50412, 865, 11, 286, 2041, 291, 727, 1333, 295, 11, 291, 727, 1333, 295, 9695, 300, 13, 286, 914, 11, 300, 311, 11, 286, 519, 300, 311, 11, 50716], "temperature": 0.0, "avg_logprob": -0.17396870437933473, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.023595627397298813}, {"id": 295, "seek": 162848, "start": 1636.88, "end": 1641.76, "text": " so getting back to Wittgenstein again, I think that's an example that, I mean, there, I think", "tokens": [50784, 370, 1242, 646, 281, 343, 593, 1766, 9089, 797, 11, 286, 519, 300, 311, 364, 1365, 300, 11, 286, 914, 11, 456, 11, 286, 519, 51028], "temperature": 0.0, "avg_logprob": -0.17396870437933473, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.023595627397298813}, {"id": 296, "seek": 162848, "start": 1641.76, "end": 1647.6, "text": " we're applying these sort of things which are being used as sort of somewhat technical terms", "tokens": [51028, 321, 434, 9275, 613, 1333, 295, 721, 597, 366, 885, 1143, 382, 1333, 295, 8344, 6191, 2115, 51320], "temperature": 0.0, "avg_logprob": -0.17396870437933473, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.023595627397298813}, {"id": 297, "seek": 162848, "start": 1647.6, "end": 1652.4, "text": " in the context of these artifacts that we're building, and applying them to ourselves. I think", "tokens": [51320, 294, 264, 4319, 295, 613, 24617, 300, 321, 434, 2390, 11, 293, 9275, 552, 281, 4175, 13, 286, 519, 51560], "temperature": 0.0, "avg_logprob": -0.17396870437933473, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.023595627397298813}, {"id": 298, "seek": 165240, "start": 1652.48, "end": 1657.8400000000001, "text": " we don't, we don't have any, we don't have any need for this kind of extra baggage of this kind", "tokens": [50368, 321, 500, 380, 11, 321, 500, 380, 362, 604, 11, 321, 500, 380, 362, 604, 643, 337, 341, 733, 295, 2857, 41567, 295, 341, 733, 50636], "temperature": 0.0, "avg_logprob": -0.10107711053663684, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.006735322531312704}, {"id": 299, "seek": 165240, "start": 1657.8400000000001, "end": 1663.44, "text": " of terminology when talking about each other. So it's perhaps a little bit misleading to kind of", "tokens": [50636, 295, 27575, 562, 1417, 466, 1184, 661, 13, 407, 309, 311, 4317, 257, 707, 857, 36429, 281, 733, 295, 50916], "temperature": 0.0, "avg_logprob": -0.10107711053663684, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.006735322531312704}, {"id": 300, "seek": 165240, "start": 1663.44, "end": 1670.8000000000002, "text": " apply those terms to ourselves. But of course, there are, you know, people have drawn attention", "tokens": [50916, 3079, 729, 2115, 281, 4175, 13, 583, 295, 1164, 11, 456, 366, 11, 291, 458, 11, 561, 362, 10117, 3202, 51284], "temperature": 0.0, "avg_logprob": -0.10107711053663684, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.006735322531312704}, {"id": 301, "seek": 165240, "start": 1670.8000000000002, "end": 1677.68, "text": " in the past to the fact that we ourselves are always playing roles in a sense in social settings", "tokens": [51284, 294, 264, 1791, 281, 264, 1186, 300, 321, 4175, 366, 1009, 2433, 9604, 294, 257, 2020, 294, 2093, 6257, 51628], "temperature": 0.0, "avg_logprob": -0.10107711053663684, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.006735322531312704}, {"id": 302, "seek": 167768, "start": 1677.68, "end": 1684.5600000000002, "text": " particularly. But I think there are differences in, you know, for ourselves, even though we might", "tokens": [50364, 4098, 13, 583, 286, 519, 456, 366, 7300, 294, 11, 291, 458, 11, 337, 4175, 11, 754, 1673, 321, 1062, 50708], "temperature": 0.0, "avg_logprob": -0.1032598146828272, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.017451567575335503}, {"id": 303, "seek": 167768, "start": 1684.5600000000002, "end": 1691.76, "text": " kind of play roles in social settings and so on, there is an underlying, there's an underlying", "tokens": [50708, 733, 295, 862, 9604, 294, 2093, 6257, 293, 370, 322, 11, 456, 307, 364, 14217, 11, 456, 311, 364, 14217, 51068], "temperature": 0.0, "avg_logprob": -0.1032598146828272, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.017451567575335503}, {"id": 304, "seek": 167768, "start": 1692.4, "end": 1698.0800000000002, "text": " me, which at least is grounded in the fact that I'm a human being with a physical body and", "tokens": [51100, 385, 11, 597, 412, 1935, 307, 23535, 294, 264, 1186, 300, 286, 478, 257, 1952, 885, 365, 257, 4001, 1772, 293, 51384], "temperature": 0.0, "avg_logprob": -0.1032598146828272, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.017451567575335503}, {"id": 305, "seek": 167768, "start": 1698.0800000000002, "end": 1704.0800000000002, "text": " biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited", "tokens": [51384, 13910, 2203, 293, 370, 322, 13, 865, 11, 286, 519, 512, 295, 341, 307, 11, 321, 366, 733, 295, 24903, 379, 5567, 51684], "temperature": 0.0, "avg_logprob": -0.1032598146828272, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.017451567575335503}, {"id": 306, "seek": 170408, "start": 1704.1599999999999, "end": 1709.4399999999998, "text": " in how we understand things. So we understand ourselves in quite simplistic terms. If you", "tokens": [50368, 294, 577, 321, 1223, 721, 13, 407, 321, 1223, 4175, 294, 1596, 44199, 2115, 13, 759, 291, 50632], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 307, "seek": 170408, "start": 1709.4399999999998, "end": 1714.08, "text": " have a long-term relationship with your wife for 30 years, they have a much high resolution", "tokens": [50632, 362, 257, 938, 12, 7039, 2480, 365, 428, 3836, 337, 2217, 924, 11, 436, 362, 257, 709, 1090, 8669, 50864], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 308, "seek": 170408, "start": 1714.08, "end": 1718.3999999999999, "text": " understanding of your different roles that you play. So they know you're tired, you didn't sleep well,", "tokens": [50864, 3701, 295, 428, 819, 9604, 300, 291, 862, 13, 407, 436, 458, 291, 434, 5868, 11, 291, 994, 380, 2817, 731, 11, 51080], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 309, "seek": 170408, "start": 1718.3999999999999, "end": 1722.6399999999999, "text": " you're playing this role now, you know, above and beyond which the kind of roles you're talking about", "tokens": [51080, 291, 434, 2433, 341, 3090, 586, 11, 291, 458, 11, 3673, 293, 4399, 597, 264, 733, 295, 9604, 291, 434, 1417, 466, 51292], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 310, "seek": 170408, "start": 1722.6399999999999, "end": 1728.08, "text": " in a party, you play a role. And it's conceivable that an alien intelligence, you know, some very", "tokens": [51292, 294, 257, 3595, 11, 291, 862, 257, 3090, 13, 400, 309, 311, 10413, 34376, 300, 364, 12319, 7599, 11, 291, 458, 11, 512, 588, 51564], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 311, "seek": 170408, "start": 1728.08, "end": 1732.1599999999999, "text": " clever aliens came down and they might see us completely differently like we see language", "tokens": [51564, 13494, 21594, 1361, 760, 293, 436, 1062, 536, 505, 2584, 7614, 411, 321, 536, 2856, 51768], "temperature": 0.0, "avg_logprob": -0.06631464707223993, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.11453764885663986}, {"id": 312, "seek": 173216, "start": 1732.16, "end": 1735.76, "text": " models. They might actually not see you as a single person, but they might see you as", "tokens": [50364, 5245, 13, 814, 1062, 767, 406, 536, 291, 382, 257, 2167, 954, 11, 457, 436, 1062, 536, 291, 382, 50544], "temperature": 0.0, "avg_logprob": -0.11779321751124422, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.006985850166529417}, {"id": 313, "seek": 173216, "start": 1735.76, "end": 1747.2, "text": " some kind of a superposition of simulacrum as well. Yeah, well, maybe. I suppose to get our", "tokens": [50544, 512, 733, 295, 257, 1687, 38078, 295, 1034, 425, 326, 6247, 382, 731, 13, 865, 11, 731, 11, 1310, 13, 286, 7297, 281, 483, 527, 51116], "temperature": 0.0, "avg_logprob": -0.11779321751124422, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.006985850166529417}, {"id": 314, "seek": 173216, "start": 1747.2, "end": 1752.4, "text": " heads around that kind of idea, we'd have to find some way of communicating with them.", "tokens": [51116, 8050, 926, 300, 733, 295, 1558, 11, 321, 1116, 362, 281, 915, 512, 636, 295, 17559, 365, 552, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11779321751124422, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.006985850166529417}, {"id": 315, "seek": 175240, "start": 1753.1200000000001, "end": 1760.3200000000002, "text": " And so we'd have to form some kind of common basis for talking about", "tokens": [50400, 400, 370, 321, 1116, 362, 281, 1254, 512, 733, 295, 2689, 5143, 337, 1417, 466, 50760], "temperature": 0.0, "avg_logprob": -0.11672463577784849, "compression_ratio": 1.7114427860696517, "no_speech_prob": 0.13136336207389832}, {"id": 316, "seek": 175240, "start": 1761.44, "end": 1768.0800000000002, "text": " each other with these, you know, with these aliens. And then we'd be able to kind of,", "tokens": [50816, 1184, 661, 365, 613, 11, 291, 458, 11, 365, 613, 21594, 13, 400, 550, 321, 1116, 312, 1075, 281, 733, 295, 11, 51148], "temperature": 0.0, "avg_logprob": -0.11672463577784849, "compression_ratio": 1.7114427860696517, "no_speech_prob": 0.13136336207389832}, {"id": 317, "seek": 175240, "start": 1768.0800000000002, "end": 1772.88, "text": " that would be the only basis on which we could establish whether something like you said was", "tokens": [51148, 300, 576, 312, 264, 787, 5143, 322, 597, 321, 727, 8327, 1968, 746, 411, 291, 848, 390, 51388], "temperature": 0.0, "avg_logprob": -0.11672463577784849, "compression_ratio": 1.7114427860696517, "no_speech_prob": 0.13136336207389832}, {"id": 318, "seek": 175240, "start": 1772.88, "end": 1781.1200000000001, "text": " true or not. So, you know, trying to find, trying to map, you know, the conceptual schema that's", "tokens": [51388, 2074, 420, 406, 13, 407, 11, 291, 458, 11, 1382, 281, 915, 11, 1382, 281, 4471, 11, 291, 458, 11, 264, 24106, 34078, 300, 311, 51800], "temperature": 0.0, "avg_logprob": -0.11672463577784849, "compression_ratio": 1.7114427860696517, "no_speech_prob": 0.13136336207389832}, {"id": 319, "seek": 178112, "start": 1781.12, "end": 1788.08, "text": " used by one culture onto a human culture onto the conceptual schema used by a different human", "tokens": [50364, 1143, 538, 472, 3713, 3911, 257, 1952, 3713, 3911, 264, 24106, 34078, 1143, 538, 257, 819, 1952, 50712], "temperature": 0.0, "avg_logprob": -0.15132220089435577, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.002044231165200472}, {"id": 320, "seek": 178112, "start": 1788.08, "end": 1795.6, "text": " culture is difficult enough as it is. And trying to do that, you know, with an alien", "tokens": [50712, 3713, 307, 2252, 1547, 382, 309, 307, 13, 400, 1382, 281, 360, 300, 11, 291, 458, 11, 365, 364, 12319, 51088], "temperature": 0.0, "avg_logprob": -0.15132220089435577, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.002044231165200472}, {"id": 321, "seek": 178112, "start": 1798.1599999999999, "end": 1804.3999999999999, "text": " species and how they conceive of us would be, you know, particularly difficult, I guess.", "tokens": [51216, 6172, 293, 577, 436, 48605, 295, 505, 576, 312, 11, 291, 458, 11, 4098, 2252, 11, 286, 2041, 13, 51528], "temperature": 0.0, "avg_logprob": -0.15132220089435577, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.002044231165200472}, {"id": 322, "seek": 180440, "start": 1804.48, "end": 1811.2, "text": " RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment from,", "tokens": [50368, 497, 43, 39, 37, 11, 286, 4333, 300, 661, 11, 309, 2067, 380, 1570, 11, 309, 2067, 380, 1570, 2085, 11, 309, 390, 264, 18515, 490, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11222859791346959, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.02143663913011551}, {"id": 323, "seek": 180440, "start": 1811.2, "end": 1816.48, "text": " I think, but there was an article called the Waluigi effect. And it argued that RLHF, you know,", "tokens": [50704, 286, 519, 11, 457, 456, 390, 364, 7222, 1219, 264, 343, 4929, 19789, 1802, 13, 400, 309, 20219, 300, 497, 43, 39, 37, 11, 291, 458, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11222859791346959, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.02143663913011551}, {"id": 324, "seek": 180440, "start": 1817.2, "end": 1823.0400000000002, "text": " cuts down the set of simulacra to be things that we want. But unfortunately, there's this problem", "tokens": [51004, 9992, 760, 264, 992, 295, 1034, 425, 326, 424, 281, 312, 721, 300, 321, 528, 13, 583, 7015, 11, 456, 311, 341, 1154, 51296], "temperature": 0.0, "avg_logprob": -0.11222859791346959, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.02143663913011551}, {"id": 325, "seek": 180440, "start": 1823.0400000000002, "end": 1828.5600000000002, "text": " that you get these antithetical simulacra, you know, slipped through the net. So the Bing GPT", "tokens": [51296, 300, 291, 483, 613, 2511, 355, 27800, 1034, 425, 326, 424, 11, 291, 458, 11, 28989, 807, 264, 2533, 13, 407, 264, 30755, 26039, 51, 51572], "temperature": 0.0, "avg_logprob": -0.11222859791346959, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.02143663913011551}, {"id": 326, "seek": 182856, "start": 1828.56, "end": 1835.52, "text": " example, it would start off as nice Bing GPT, and then it would degrade to one of the Waluigi's.", "tokens": [50364, 1365, 11, 309, 576, 722, 766, 382, 1481, 30755, 26039, 51, 11, 293, 550, 309, 576, 368, 8692, 281, 472, 295, 264, 343, 4929, 19789, 311, 13, 50712], "temperature": 0.0, "avg_logprob": -0.08706155887320022, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.04837148264050484}, {"id": 327, "seek": 182856, "start": 1835.52, "end": 1840.0, "text": " And had this interesting phenomenon, they argued that the degradation, once it happens, it stays", "tokens": [50712, 400, 632, 341, 1880, 14029, 11, 436, 20219, 300, 264, 40519, 11, 1564, 309, 2314, 11, 309, 10834, 50936], "temperature": 0.0, "avg_logprob": -0.08706155887320022, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.04837148264050484}, {"id": 328, "seek": 182856, "start": 1840.0, "end": 1846.0, "text": " in the bad one. But just more broadly, what is your intuition about the extent to which RLHF", "tokens": [50936, 294, 264, 1578, 472, 13, 583, 445, 544, 19511, 11, 437, 307, 428, 24002, 466, 264, 8396, 281, 597, 497, 43, 39, 37, 51236], "temperature": 0.0, "avg_logprob": -0.08706155887320022, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.04837148264050484}, {"id": 329, "seek": 182856, "start": 1846.0, "end": 1851.84, "text": " affects simulacra? And I also noted down that I think you wrote in, I think it was your role", "tokens": [51236, 11807, 1034, 425, 326, 424, 30, 400, 286, 611, 12964, 760, 300, 286, 519, 291, 4114, 294, 11, 286, 519, 309, 390, 428, 3090, 51528], "temperature": 0.0, "avg_logprob": -0.08706155887320022, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.04837148264050484}, {"id": 330, "seek": 182856, "start": 1851.84, "end": 1857.76, "text": " playing paper, that you felt RLHF increased the deception behavior in these models?", "tokens": [51528, 2433, 3035, 11, 300, 291, 2762, 497, 43, 39, 37, 6505, 264, 40451, 5223, 294, 613, 5245, 30, 51824], "temperature": 0.0, "avg_logprob": -0.08706155887320022, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.04837148264050484}, {"id": 331, "seek": 185856, "start": 1858.6399999999999, "end": 1863.12, "text": " Actually, that wasn't something that I wrote. That was something that some anthropic researchers", "tokens": [50368, 5135, 11, 300, 2067, 380, 746, 300, 286, 4114, 13, 663, 390, 746, 300, 512, 22727, 299, 10309, 50592], "temperature": 0.0, "avg_logprob": -0.17524325740229976, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0043905009515583515}, {"id": 332, "seek": 185856, "start": 1863.12, "end": 1869.6, "text": " and so Ethan Perres and others had a paper where they, I mean, so this is not one,", "tokens": [50592, 293, 370, 23984, 3026, 495, 293, 2357, 632, 257, 3035, 689, 436, 11, 286, 914, 11, 370, 341, 307, 406, 472, 11, 50916], "temperature": 0.0, "avg_logprob": -0.17524325740229976, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0043905009515583515}, {"id": 333, "seek": 185856, "start": 1869.6, "end": 1872.8799999999999, "text": " one wouldn't want to just speculate about that, they had established something along", "tokens": [50916, 472, 2759, 380, 528, 281, 445, 40775, 466, 300, 11, 436, 632, 7545, 746, 2051, 51080], "temperature": 0.0, "avg_logprob": -0.17524325740229976, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0043905009515583515}, {"id": 334, "seek": 185856, "start": 1872.8799999999999, "end": 1880.24, "text": " those lines, I think, empirically. So, and yeah, and as far as this sort of Waluigi effect is", "tokens": [51080, 729, 3876, 11, 286, 519, 11, 25790, 984, 13, 407, 11, 293, 1338, 11, 293, 382, 1400, 382, 341, 1333, 295, 343, 4929, 19789, 1802, 307, 51448], "temperature": 0.0, "avg_logprob": -0.17524325740229976, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0043905009515583515}, {"id": 335, "seek": 185856, "start": 1880.24, "end": 1886.96, "text": " concerned, so that is kind of somewhat speculative. I think it's a plausible idea,", "tokens": [51448, 5922, 11, 370, 300, 307, 733, 295, 8344, 49415, 13, 286, 519, 309, 311, 257, 39925, 1558, 11, 51784], "temperature": 0.0, "avg_logprob": -0.17524325740229976, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0043905009515583515}, {"id": 336, "seek": 188696, "start": 1886.96, "end": 1893.2, "text": " but to actually establish that that really was a real effect, you'd want to do some actual empirical", "tokens": [50364, 457, 281, 767, 8327, 300, 300, 534, 390, 257, 957, 1802, 11, 291, 1116, 528, 281, 360, 512, 3539, 31886, 50676], "temperature": 0.0, "avg_logprob": -0.1209039480789848, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00671534426510334}, {"id": 337, "seek": 188696, "start": 1893.2, "end": 1898.16, "text": " work, I think. The thing about the, so it's plausible, but the thing about the, about the", "tokens": [50676, 589, 11, 286, 519, 13, 440, 551, 466, 264, 11, 370, 309, 311, 39925, 11, 457, 264, 551, 466, 264, 11, 466, 264, 50924], "temperature": 0.0, "avg_logprob": -0.1209039480789848, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00671534426510334}, {"id": 338, "seek": 188696, "start": 1898.16, "end": 1903.76, "text": " simulator's paper is that it's not making kind of claims really, it's rather it's providing a", "tokens": [50924, 32974, 311, 3035, 307, 300, 309, 311, 406, 1455, 733, 295, 9441, 534, 11, 309, 311, 2831, 309, 311, 6530, 257, 51204], "temperature": 0.0, "avg_logprob": -0.1209039480789848, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00671534426510334}, {"id": 339, "seek": 188696, "start": 1903.76, "end": 1910.64, "text": " framework for thinking about large language models. So that's why I found it particularly useful.", "tokens": [51204, 8388, 337, 1953, 466, 2416, 2856, 5245, 13, 407, 300, 311, 983, 286, 1352, 309, 4098, 4420, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1209039480789848, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00671534426510334}, {"id": 340, "seek": 191064, "start": 1910.72, "end": 1916.96, "text": " So on RLHF, so I do think it's quite difficult with RLHF to", "tokens": [50368, 407, 322, 497, 43, 39, 37, 11, 370, 286, 360, 519, 309, 311, 1596, 2252, 365, 497, 43, 39, 37, 281, 50680], "temperature": 0.0, "avg_logprob": -0.14679520871458934, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.005538942292332649}, {"id": 341, "seek": 191064, "start": 1918.5600000000002, "end": 1922.0800000000002, "text": " guarantee that, you know, you're going to get a model to do what you want it to do.", "tokens": [50760, 10815, 300, 11, 291, 458, 11, 291, 434, 516, 281, 483, 257, 2316, 281, 360, 437, 291, 528, 309, 281, 360, 13, 50936], "temperature": 0.0, "avg_logprob": -0.14679520871458934, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.005538942292332649}, {"id": 342, "seek": 191064, "start": 1923.5200000000002, "end": 1928.4, "text": " And so that's quite difficult. And, you know, and everybody has found that,", "tokens": [51008, 400, 370, 300, 311, 1596, 2252, 13, 400, 11, 291, 458, 11, 293, 2201, 575, 1352, 300, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14679520871458934, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.005538942292332649}, {"id": 343, "seek": 191064, "start": 1928.4, "end": 1933.2, "text": " that, you know, you think that you've controlled the model quite well, but there are always still", "tokens": [51252, 300, 11, 291, 458, 11, 291, 519, 300, 291, 600, 10164, 264, 2316, 1596, 731, 11, 457, 456, 366, 1009, 920, 51492], "temperature": 0.0, "avg_logprob": -0.14679520871458934, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.005538942292332649}, {"id": 344, "seek": 191064, "start": 1933.2, "end": 1939.2, "text": " ways of jailbreaking it or ways in which things, things go, go wrong. So, you know, there are,", "tokens": [51492, 2098, 295, 10511, 20602, 309, 420, 2098, 294, 597, 721, 11, 721, 352, 11, 352, 2085, 13, 407, 11, 291, 458, 11, 456, 366, 11, 51792], "temperature": 0.0, "avg_logprob": -0.14679520871458934, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.005538942292332649}, {"id": 345, "seek": 193920, "start": 1939.2, "end": 1944.32, "text": " so there are different approaches. Anthropic had this constitutional AI approach, which is,", "tokens": [50364, 370, 456, 366, 819, 11587, 13, 12727, 39173, 632, 341, 20176, 7318, 3109, 11, 597, 307, 11, 50620], "temperature": 0.0, "avg_logprob": -0.11022621957879318, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.0016775268595665693}, {"id": 346, "seek": 193920, "start": 1944.32, "end": 1951.92, "text": " which is quite a nice, a nice idea. And, you know, I mean, I, I, I quite like the idea of", "tokens": [50620, 597, 307, 1596, 257, 1481, 11, 257, 1481, 1558, 13, 400, 11, 291, 458, 11, 286, 914, 11, 286, 11, 286, 11, 286, 1596, 411, 264, 1558, 295, 51000], "temperature": 0.0, "avg_logprob": -0.11022621957879318, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.0016775268595665693}, {"id": 347, "seek": 193920, "start": 1951.92, "end": 1958.72, "text": " sticking with a powerful base model and using, you know, prompting to, to guide things as well.", "tokens": [51000, 13465, 365, 257, 4005, 3096, 2316, 293, 1228, 11, 291, 458, 11, 12391, 278, 281, 11, 281, 5934, 721, 382, 731, 13, 51340], "temperature": 0.0, "avg_logprob": -0.11022621957879318, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.0016775268595665693}, {"id": 348, "seek": 193920, "start": 1958.72, "end": 1963.92, "text": " So there's all kinds of different approaches. Interesting. On the subject of anthropic and", "tokens": [51340, 407, 456, 311, 439, 3685, 295, 819, 11587, 13, 14711, 13, 1282, 264, 3983, 295, 22727, 299, 293, 51600], "temperature": 0.0, "avg_logprob": -0.11022621957879318, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.0016775268595665693}, {"id": 349, "seek": 196392, "start": 1963.92, "end": 1970.0800000000002, "text": " deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands all over", "tokens": [50364, 40451, 11, 436, 11, 436, 445, 632, 341, 26962, 3035, 484, 293, 286, 914, 11, 6688, 422, 875, 632, 702, 2377, 439, 670, 50672], "temperature": 0.0, "avg_logprob": -0.11408308075695503, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.14204031229019165}, {"id": 350, "seek": 196392, "start": 1970.0800000000002, "end": 1974.96, "text": " it. And it was actually quite straightforward. So they, they trained in autoencoder, you know,", "tokens": [50672, 309, 13, 400, 309, 390, 767, 1596, 15325, 13, 407, 436, 11, 436, 8895, 294, 8399, 22660, 19866, 11, 291, 458, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11408308075695503, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.14204031229019165}, {"id": 351, "seek": 196392, "start": 1974.96, "end": 1980.48, "text": " to find a bunch of features. So it was an unsupervised method. And, and I think they actually,", "tokens": [50916, 281, 915, 257, 3840, 295, 4122, 13, 407, 309, 390, 364, 2693, 12879, 24420, 3170, 13, 400, 11, 293, 286, 519, 436, 767, 11, 51192], "temperature": 0.0, "avg_logprob": -0.11408308075695503, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.14204031229019165}, {"id": 352, "seek": 196392, "start": 1980.48, "end": 1984.16, "text": " you know, expanded it to find millions of features. So they had a bit of a needle in the haystack", "tokens": [51192, 291, 458, 11, 14342, 309, 281, 915, 6803, 295, 4122, 13, 407, 436, 632, 257, 857, 295, 257, 11037, 294, 264, 4842, 372, 501, 51376], "temperature": 0.0, "avg_logprob": -0.11408308075695503, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.14204031229019165}, {"id": 353, "seek": 196392, "start": 1984.16, "end": 1988.0, "text": " problem, but they cherry picked some and they found one that corresponded to the Golden Gate", "tokens": [51376, 1154, 11, 457, 436, 20164, 6183, 512, 293, 436, 1352, 472, 300, 6805, 292, 281, 264, 13410, 21913, 51568], "temperature": 0.0, "avg_logprob": -0.11408308075695503, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.14204031229019165}, {"id": 354, "seek": 198800, "start": 1988.0, "end": 1993.68, "text": " Bridge and so on. And, and obviously other ones that corresponded to what they said were", "tokens": [50364, 18917, 293, 370, 322, 13, 400, 11, 293, 2745, 661, 2306, 300, 6805, 292, 281, 437, 436, 848, 645, 50648], "temperature": 0.0, "avg_logprob": -0.11696947945488824, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.10650523006916046}, {"id": 355, "seek": 198800, "start": 1993.68, "end": 1999.44, "text": " mono semantic abstract features, got some reservations about that. And the interesting", "tokens": [50648, 35624, 47982, 12649, 4122, 11, 658, 512, 40222, 466, 300, 13, 400, 264, 1880, 50936], "temperature": 0.0, "avg_logprob": -0.11696947945488824, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.10650523006916046}, {"id": 356, "seek": 198800, "start": 1999.44, "end": 2002.88, "text": " thing about having an autoencoder is you can clamp the features. So you can say,", "tokens": [50936, 551, 466, 1419, 364, 8399, 22660, 19866, 307, 291, 393, 17690, 264, 4122, 13, 407, 291, 393, 584, 11, 51108], "temperature": 0.0, "avg_logprob": -0.11696947945488824, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.10650523006916046}, {"id": 357, "seek": 198800, "start": 2003.52, "end": 2007.36, "text": " turn the Golden Gate Bridge up in, in now the language model is, oh, but I just really want", "tokens": [51140, 1261, 264, 13410, 21913, 18917, 493, 294, 11, 294, 586, 264, 2856, 2316, 307, 11, 1954, 11, 457, 286, 445, 534, 528, 51332], "temperature": 0.0, "avg_logprob": -0.11696947945488824, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.10650523006916046}, {"id": 358, "seek": 198800, "start": 2007.36, "end": 2013.76, "text": " to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when", "tokens": [51332, 281, 751, 466, 264, 13410, 21913, 18917, 13, 286, 393, 380, 1590, 1417, 466, 309, 13, 1222, 3136, 365, 300, 307, 562, 51652], "temperature": 0.0, "avg_logprob": -0.11696947945488824, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.10650523006916046}, {"id": 359, "seek": 201376, "start": 2013.76, "end": 2020.56, "text": " you look at the activations in the corpus for things like deception, I felt that they weren't", "tokens": [50364, 291, 574, 412, 264, 2430, 763, 294, 264, 1181, 31624, 337, 721, 411, 40451, 11, 286, 2762, 300, 436, 4999, 380, 50704], "temperature": 0.0, "avg_logprob": -0.09457909433465254, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0032095189671963453}, {"id": 360, "seek": 201376, "start": 2020.56, "end": 2025.36, "text": " really showing abstract features. They were kind of showing almost keyword matches from Reddit and", "tokens": [50704, 534, 4099, 12649, 4122, 13, 814, 645, 733, 295, 4099, 1920, 20428, 10676, 490, 32210, 293, 50944], "temperature": 0.0, "avg_logprob": -0.09457909433465254, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0032095189671963453}, {"id": 361, "seek": 201376, "start": 2025.36, "end": 2031.2, "text": " so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think", "tokens": [50944, 370, 322, 13, 407, 286, 390, 257, 707, 857, 28601, 466, 577, 12649, 645, 436, 534, 13, 865, 13, 407, 286, 500, 380, 519, 51236], "temperature": 0.0, "avg_logprob": -0.09457909433465254, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0032095189671963453}, {"id": 362, "seek": 201376, "start": 2031.2, "end": 2036.96, "text": " I can comment on that particular topic because I, I mean, I have read the paper, but not in that,", "tokens": [51236, 286, 393, 2871, 322, 300, 1729, 4829, 570, 286, 11, 286, 914, 11, 286, 362, 1401, 264, 3035, 11, 457, 406, 294, 300, 11, 51524], "temperature": 0.0, "avg_logprob": -0.09457909433465254, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0032095189671963453}, {"id": 363, "seek": 203696, "start": 2037.2, "end": 2041.8400000000001, "text": " in sufficient detail to comment on that particular thing. But the Golden Gate Bridge,", "tokens": [50376, 294, 11563, 2607, 281, 2871, 322, 300, 1729, 551, 13, 583, 264, 13410, 21913, 18917, 11, 50608], "temperature": 0.0, "avg_logprob": -0.16413661862207837, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.03585389629006386}, {"id": 364, "seek": 203696, "start": 2043.6000000000001, "end": 2048.56, "text": " I think it was a fascinating illustration of what you can do. So I don't know if you tried out", "tokens": [50696, 286, 519, 309, 390, 257, 10343, 22645, 295, 437, 291, 393, 360, 13, 407, 286, 500, 380, 458, 498, 291, 3031, 484, 50944], "temperature": 0.0, "avg_logprob": -0.16413661862207837, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.03585389629006386}, {"id": 365, "seek": 203696, "start": 2048.56, "end": 2054.7200000000003, "text": " Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah. Yeah.", "tokens": [50944, 13410, 21913, 12947, 2303, 13, 2589, 291, 536, 300, 436, 4736, 257, 3037, 295, 12947, 2303, 30, 865, 11, 286, 1866, 309, 13, 865, 13, 865, 13, 51252], "temperature": 0.0, "avg_logprob": -0.16413661862207837, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.03585389629006386}, {"id": 366, "seek": 203696, "start": 2054.7200000000003, "end": 2058.16, "text": " So I think it was fascinating to see. But what was particularly interesting about the Golden", "tokens": [51252, 407, 286, 519, 309, 390, 10343, 281, 536, 13, 583, 437, 390, 4098, 1880, 466, 264, 13410, 51424], "temperature": 0.0, "avg_logprob": -0.16413661862207837, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.03585389629006386}, {"id": 367, "seek": 203696, "start": 2058.16, "end": 2065.84, "text": " Gate Claude, I think, was that was, that was how, you know, again, it's very difficult not to use", "tokens": [51424, 21913, 12947, 2303, 11, 286, 519, 11, 390, 300, 390, 11, 300, 390, 577, 11, 291, 458, 11, 797, 11, 309, 311, 588, 2252, 406, 281, 764, 51808], "temperature": 0.0, "avg_logprob": -0.16413661862207837, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.03585389629006386}, {"id": 368, "seek": 206584, "start": 2065.84, "end": 2070.4, "text": " anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's", "tokens": [50364, 22727, 32702, 299, 2115, 13, 400, 341, 307, 689, 11, 797, 11, 291, 458, 11, 291, 362, 281, 4160, 1803, 300, 456, 311, 50592], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 369, "seek": 206584, "start": 2070.4, "end": 2077.2000000000003, "text": " something role playing these things. But how, you know, sort of gamely, it struggles to kind of", "tokens": [50592, 746, 3090, 2433, 613, 721, 13, 583, 577, 11, 291, 458, 11, 1333, 295, 8019, 736, 11, 309, 17592, 281, 733, 295, 50932], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 370, "seek": 206584, "start": 2077.2000000000003, "end": 2081.84, "text": " overcome this tendency to talk about the Golden Gate Bridge all the time, which has been, which", "tokens": [50932, 10473, 341, 18187, 281, 751, 466, 264, 13410, 21913, 18917, 439, 264, 565, 11, 597, 575, 668, 11, 597, 51164], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 371, "seek": 206584, "start": 2081.84, "end": 2087.36, "text": " has been kind of clamped to do, but it will keep noticing that it was talking about the", "tokens": [51164, 575, 668, 733, 295, 17690, 292, 281, 360, 11, 457, 309, 486, 1066, 21814, 300, 309, 390, 1417, 466, 264, 51440], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 372, "seek": 206584, "start": 2087.36, "end": 2091.44, "text": " Golden Gate Bridge again and apologizing and then trying to do what it had been actually", "tokens": [51440, 13410, 21913, 18917, 797, 293, 9472, 3319, 293, 550, 1382, 281, 360, 437, 309, 632, 668, 767, 51644], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 373, "seek": 206584, "start": 2091.44, "end": 2095.76, "text": " asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating", "tokens": [51644, 2351, 281, 360, 538, 264, 4195, 293, 550, 4305, 1348, 646, 281, 264, 13410, 21913, 18917, 13, 467, 311, 445, 10343, 51860], "temperature": 0.0, "avg_logprob": -0.11255424162920784, "compression_ratio": 1.8712871287128714, "no_speech_prob": 0.007545642554759979}, {"id": 374, "seek": 209576, "start": 2095.84, "end": 2104.2400000000002, "text": " to see that the sort of the, as it were, internal struggle going on there in the model. And I think", "tokens": [50368, 281, 536, 300, 264, 1333, 295, 264, 11, 382, 309, 645, 11, 6920, 7799, 516, 322, 456, 294, 264, 2316, 13, 400, 286, 519, 50788], "temperature": 0.0, "avg_logprob": -0.14433451890945434, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.015019280835986137}, {"id": 375, "seek": 209576, "start": 2104.2400000000002, "end": 2112.88, "text": " that does show, in some ways, how powerful they are, because it wasn't quite as despite the fact", "tokens": [50788, 300, 775, 855, 11, 294, 512, 2098, 11, 577, 4005, 436, 366, 11, 570, 309, 2067, 380, 1596, 382, 7228, 264, 1186, 51220], "temperature": 0.0, "avg_logprob": -0.14433451890945434, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.015019280835986137}, {"id": 376, "seek": 209576, "start": 2112.88, "end": 2122.8, "text": " that it had this, you know, control imposed on it, but really in a very, you know, I mean, not", "tokens": [51220, 300, 309, 632, 341, 11, 291, 458, 11, 1969, 26491, 322, 309, 11, 457, 534, 294, 257, 588, 11, 291, 458, 11, 286, 914, 11, 406, 51716], "temperature": 0.0, "avg_logprob": -0.14433451890945434, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.015019280835986137}, {"id": 377, "seek": 212280, "start": 2122.88, "end": 2127.76, "text": " hardware, but really low level, you know, despite that, it was still, you know, constantly trying", "tokens": [50368, 8837, 11, 457, 534, 2295, 1496, 11, 291, 458, 11, 7228, 300, 11, 309, 390, 920, 11, 291, 458, 11, 6460, 1382, 50612], "temperature": 0.0, "avg_logprob": -0.10269559815872548, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.040647104382514954}, {"id": 378, "seek": 212280, "start": 2127.76, "end": 2132.32, "text": " to recover from all of that and, you know, with some degree of success. And I imagine this would", "tokens": [50612, 281, 8114, 490, 439, 295, 300, 293, 11, 291, 458, 11, 365, 512, 4314, 295, 2245, 13, 400, 286, 3811, 341, 576, 50840], "temperature": 0.0, "avg_logprob": -0.10269559815872548, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.040647104382514954}, {"id": 379, "seek": 212280, "start": 2132.32, "end": 2138.0800000000004, "text": " be true with everybody's models, by the way. So I imagine that what they found in, you know,", "tokens": [50840, 312, 2074, 365, 2201, 311, 5245, 11, 538, 264, 636, 13, 407, 286, 3811, 300, 437, 436, 1352, 294, 11, 291, 458, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10269559815872548, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.040647104382514954}, {"id": 380, "seek": 212280, "start": 2138.0800000000004, "end": 2143.28, "text": " in Claude would be very similar. I imagine with GPT-4 and with Gemini, I just imagine that we'd", "tokens": [51128, 294, 12947, 2303, 576, 312, 588, 2531, 13, 286, 3811, 365, 26039, 51, 12, 19, 293, 365, 22894, 3812, 11, 286, 445, 3811, 300, 321, 1116, 51388], "temperature": 0.0, "avg_logprob": -0.10269559815872548, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.040647104382514954}, {"id": 381, "seek": 212280, "start": 2143.28, "end": 2149.44, "text": " find very similar things with all of these models. Yeah, I'm sure. I mean, as I said, reservations", "tokens": [51388, 915, 588, 2531, 721, 365, 439, 295, 613, 5245, 13, 865, 11, 286, 478, 988, 13, 286, 914, 11, 382, 286, 848, 11, 40222, 51696], "temperature": 0.0, "avg_logprob": -0.10269559815872548, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.040647104382514954}, {"id": 382, "seek": 214944, "start": 2150.16, "end": 2153.92, "text": " they admitted themselves that the features were not complete, so they didn't represent all of the", "tokens": [50400, 436, 14920, 2969, 300, 264, 4122, 645, 406, 3566, 11, 370, 436, 994, 380, 2906, 439, 295, 264, 50588], "temperature": 0.0, "avg_logprob": -0.14778360827215786, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.011497575789690018}, {"id": 383, "seek": 214944, "start": 2153.92, "end": 2159.68, "text": " activation space in respect of the Golden Gate Bridge. And in many cases, they presumably weren't", "tokens": [50588, 24433, 1901, 294, 3104, 295, 264, 13410, 21913, 18917, 13, 400, 294, 867, 3331, 11, 436, 26742, 4999, 380, 50876], "temperature": 0.0, "avg_logprob": -0.14778360827215786, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.011497575789690018}, {"id": 384, "seek": 214944, "start": 2159.68, "end": 2164.56, "text": " monosemantic, but they did cherry-pick some that presumably were. Presumably. And also, I mean,", "tokens": [50876, 1108, 329, 443, 7128, 11, 457, 436, 630, 20164, 12, 79, 618, 512, 300, 26742, 645, 13, 2718, 449, 1188, 13, 400, 611, 11, 286, 914, 11, 51120], "temperature": 0.0, "avg_logprob": -0.14778360827215786, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.011497575789690018}, {"id": 385, "seek": 214944, "start": 2165.52, "end": 2172.32, "text": " they're very interested in finding these features, which are just linear combinations. And so that,", "tokens": [51168, 436, 434, 588, 3102, 294, 5006, 613, 4122, 11, 597, 366, 445, 8213, 21267, 13, 400, 370, 300, 11, 51508], "temperature": 0.0, "avg_logprob": -0.14778360827215786, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.011497575789690018}, {"id": 386, "seek": 214944, "start": 2172.32, "end": 2177.36, "text": " and of course, it's very nice when you find those sorts of features, especially if they appear to", "tokens": [51508, 293, 295, 1164, 11, 309, 311, 588, 1481, 562, 291, 915, 729, 7527, 295, 4122, 11, 2318, 498, 436, 4204, 281, 51760], "temperature": 0.0, "avg_logprob": -0.14778360827215786, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.011497575789690018}, {"id": 387, "seek": 217736, "start": 2177.36, "end": 2183.04, "text": " be monosemantic, because it does suggest a nice sort of compositionality and explainability and", "tokens": [50364, 312, 1108, 329, 443, 7128, 11, 570, 309, 775, 3402, 257, 1481, 1333, 295, 12686, 1860, 293, 2903, 2310, 293, 50648], "temperature": 0.0, "avg_logprob": -0.08561129447741386, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.004025040660053492}, {"id": 388, "seek": 217736, "start": 2183.04, "end": 2187.44, "text": " comprehensibility of what's going on there. But I also feel that they're looking under the", "tokens": [50648, 10753, 694, 2841, 295, 437, 311, 516, 322, 456, 13, 583, 286, 611, 841, 300, 436, 434, 1237, 833, 264, 50868], "temperature": 0.0, "avg_logprob": -0.08561129447741386, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.004025040660053492}, {"id": 389, "seek": 217736, "start": 2187.44, "end": 2193.1200000000003, "text": " lamp light a little bit, because that doesn't mean to say that there aren't all kinds of other features", "tokens": [50868, 12684, 1442, 257, 707, 857, 11, 570, 300, 1177, 380, 914, 281, 584, 300, 456, 3212, 380, 439, 3685, 295, 661, 4122, 51152], "temperature": 0.0, "avg_logprob": -0.08561129447741386, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.004025040660053492}, {"id": 390, "seek": 217736, "start": 2193.1200000000003, "end": 2198.4, "text": " which maybe aren't, you know, sort of linear in that sort of way, but nevertheless are functionally", "tokens": [51152, 597, 1310, 3212, 380, 11, 291, 458, 11, 1333, 295, 8213, 294, 300, 1333, 295, 636, 11, 457, 26924, 366, 2445, 379, 51416], "temperature": 0.0, "avg_logprob": -0.08561129447741386, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.004025040660053492}, {"id": 391, "seek": 217736, "start": 2198.96, "end": 2204.6400000000003, "text": " relevant to the final results that it produces. I'd only use the word platonic, because the Golden", "tokens": [51444, 7340, 281, 264, 2572, 3542, 300, 309, 14725, 13, 286, 1116, 787, 764, 264, 1349, 3403, 11630, 11, 570, 264, 13410, 51728], "temperature": 0.0, "avg_logprob": -0.08561129447741386, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.004025040660053492}, {"id": 392, "seek": 220464, "start": 2204.64, "end": 2211.52, "text": " Gate Bridge, presumably, it's a cultural category. And what's fascinating, if it has picked up this", "tokens": [50364, 21913, 18917, 11, 26742, 11, 309, 311, 257, 6988, 7719, 13, 400, 437, 311, 10343, 11, 498, 309, 575, 6183, 493, 341, 50708], "temperature": 0.0, "avg_logprob": -0.06537947756178836, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.00487543223425746}, {"id": 393, "seek": 220464, "start": 2211.52, "end": 2218.56, "text": " thing unsupervised and learned this category from the data, is that language models at least", "tokens": [50708, 551, 2693, 12879, 24420, 293, 3264, 341, 7719, 490, 264, 1412, 11, 307, 300, 2856, 5245, 412, 1935, 51060], "temperature": 0.0, "avg_logprob": -0.06537947756178836, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.00487543223425746}, {"id": 394, "seek": 220464, "start": 2218.56, "end": 2223.68, "text": " possibly think in a similar way we do. So they've established a category in the same way we have,", "tokens": [51060, 6264, 519, 294, 257, 2531, 636, 321, 360, 13, 407, 436, 600, 7545, 257, 7719, 294, 264, 912, 636, 321, 362, 11, 51316], "temperature": 0.0, "avg_logprob": -0.06537947756178836, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.00487543223425746}, {"id": 395, "seek": 220464, "start": 2223.68, "end": 2229.68, "text": " which is fascinating. But I'm also really interested in agency, which is, for me, it's about", "tokens": [51316, 597, 307, 10343, 13, 583, 286, 478, 611, 534, 3102, 294, 7934, 11, 597, 307, 11, 337, 385, 11, 309, 311, 466, 51616], "temperature": 0.0, "avg_logprob": -0.06537947756178836, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.00487543223425746}, {"id": 396, "seek": 222968, "start": 2229.7599999999998, "end": 2236.24, "text": " self-directedness and intentionality. And what I would find very interesting is if you did clamp", "tokens": [50368, 2698, 12, 44868, 292, 1287, 293, 7789, 1860, 13, 400, 437, 286, 576, 915, 588, 1880, 307, 498, 291, 630, 17690, 50692], "temperature": 0.0, "avg_logprob": -0.07342302004496257, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.008573755621910095}, {"id": 397, "seek": 222968, "start": 2236.7999999999997, "end": 2240.96, "text": " the model to only talk about the Golden Gate Bridge, and you could convince it or it could", "tokens": [50720, 264, 2316, 281, 787, 751, 466, 264, 13410, 21913, 18917, 11, 293, 291, 727, 13447, 309, 420, 309, 727, 50928], "temperature": 0.0, "avg_logprob": -0.07342302004496257, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.008573755621910095}, {"id": 398, "seek": 222968, "start": 2240.96, "end": 2246.7999999999997, "text": " convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator of agency", "tokens": [50928, 13447, 2564, 281, 406, 751, 466, 264, 13410, 21913, 18917, 11, 300, 281, 385, 576, 312, 364, 16961, 295, 7934, 51220], "temperature": 0.0, "avg_logprob": -0.07342302004496257, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.008573755621910095}, {"id": 399, "seek": 222968, "start": 2246.7999999999997, "end": 2254.3999999999996, "text": " being expressed. Yes, perhaps it would, but I guess it would also be an indicator that they", "tokens": [51220, 885, 12675, 13, 1079, 11, 4317, 309, 576, 11, 457, 286, 2041, 309, 576, 611, 312, 364, 16961, 300, 436, 51600], "temperature": 0.0, "avg_logprob": -0.07342302004496257, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.008573755621910095}, {"id": 400, "seek": 225440, "start": 2254.4, "end": 2260.4, "text": " hadn't succeeded in isolating a feature which was controllable in that way, right, which was the", "tokens": [50364, 8782, 380, 20263, 294, 48912, 257, 4111, 597, 390, 45159, 712, 294, 300, 636, 11, 558, 11, 597, 390, 264, 50664], "temperature": 0.0, "avg_logprob": -0.14561585705689709, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.016453195363283157}, {"id": 401, "seek": 225440, "start": 2260.4, "end": 2268.32, "text": " whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did actually write about", "tokens": [50664, 1379, 4334, 295, 300, 5380, 13, 1079, 11, 2086, 13, 286, 914, 11, 322, 264, 7934, 551, 11, 291, 630, 767, 2464, 466, 51060], "temperature": 0.0, "avg_logprob": -0.14561585705689709, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.016453195363283157}, {"id": 402, "seek": 225440, "start": 2268.32, "end": 2274.1600000000003, "text": " this. And I think you argued, which was counter to what my intuition was, which was that agency is", "tokens": [51060, 341, 13, 400, 286, 519, 291, 20219, 11, 597, 390, 5682, 281, 437, 452, 24002, 390, 11, 597, 390, 300, 7934, 307, 51352], "temperature": 0.0, "avg_logprob": -0.14561585705689709, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.016453195363283157}, {"id": 403, "seek": 225440, "start": 2274.1600000000003, "end": 2280.88, "text": " in the simulacre, not the simulator. And Francois Chouelet thinks a lot about the measure of", "tokens": [51352, 294, 264, 1034, 425, 326, 265, 11, 406, 264, 32974, 13, 400, 34695, 271, 761, 263, 15966, 7309, 257, 688, 466, 264, 3481, 295, 51688], "temperature": 0.0, "avg_logprob": -0.14561585705689709, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.016453195363283157}, {"id": 404, "seek": 228088, "start": 2280.88, "end": 2287.36, "text": " intelligence. And he would argue that intelligence is the system which produces the skill program.", "tokens": [50364, 7599, 13, 400, 415, 576, 9695, 300, 7599, 307, 264, 1185, 597, 14725, 264, 5389, 1461, 13, 50688], "temperature": 0.0, "avg_logprob": -0.05968458462605434, "compression_ratio": 1.9551020408163264, "no_speech_prob": 0.013742368668317795}, {"id": 405, "seek": 228088, "start": 2287.36, "end": 2291.84, "text": " So in the context of a language model, he would say a language model is basically a database", "tokens": [50688, 407, 294, 264, 4319, 295, 257, 2856, 2316, 11, 415, 576, 584, 257, 2856, 2316, 307, 1936, 257, 8149, 50912], "temperature": 0.0, "avg_logprob": -0.05968458462605434, "compression_ratio": 1.9551020408163264, "no_speech_prob": 0.013742368668317795}, {"id": 406, "seek": 228088, "start": 2291.84, "end": 2296.48, "text": " of skill programs. And the query is like, you know, I'm going to go and pull out a skill program,", "tokens": [50912, 295, 5389, 4268, 13, 400, 264, 14581, 307, 411, 11, 291, 458, 11, 286, 478, 516, 281, 352, 293, 2235, 484, 257, 5389, 1461, 11, 51144], "temperature": 0.0, "avg_logprob": -0.05968458462605434, "compression_ratio": 1.9551020408163264, "no_speech_prob": 0.013742368668317795}, {"id": 407, "seek": 228088, "start": 2296.48, "end": 2301.44, "text": " I'm going to run the skill program. So he thinks there's no intelligence in the language model,", "tokens": [51144, 286, 478, 516, 281, 1190, 264, 5389, 1461, 13, 407, 415, 7309, 456, 311, 572, 7599, 294, 264, 2856, 2316, 11, 51392], "temperature": 0.0, "avg_logprob": -0.05968458462605434, "compression_ratio": 1.9551020408163264, "no_speech_prob": 0.013742368668317795}, {"id": 408, "seek": 228088, "start": 2301.44, "end": 2307.36, "text": " which Janus would call a simulator. But if you take into account the training process and the", "tokens": [51392, 597, 4956, 301, 576, 818, 257, 32974, 13, 583, 498, 291, 747, 666, 2696, 264, 3097, 1399, 293, 264, 51688], "temperature": 0.0, "avg_logprob": -0.05968458462605434, "compression_ratio": 1.9551020408163264, "no_speech_prob": 0.013742368668317795}, {"id": 409, "seek": 230736, "start": 2307.36, "end": 2312.1600000000003, "text": " generative processes that produce the data, then that's where the intelligence is.", "tokens": [50364, 1337, 1166, 7555, 300, 5258, 264, 1412, 11, 550, 300, 311, 689, 264, 7599, 307, 13, 50604], "temperature": 0.0, "avg_logprob": -0.17742775678634642, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0031933409627527}, {"id": 410, "seek": 230736, "start": 2315.28, "end": 2321.44, "text": " Yeah. Can I comment on agency there? So you covered quite a bit in that.", "tokens": [50760, 865, 13, 1664, 286, 2871, 322, 7934, 456, 30, 407, 291, 5343, 1596, 257, 857, 294, 300, 13, 51068], "temperature": 0.0, "avg_logprob": -0.17742775678634642, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0031933409627527}, {"id": 411, "seek": 230736, "start": 2321.44, "end": 2323.76, "text": " I did. Sorry, I just went off piece a little bit.", "tokens": [51068, 286, 630, 13, 4919, 11, 286, 445, 1437, 766, 2522, 257, 707, 857, 13, 51184], "temperature": 0.0, "avg_logprob": -0.17742775678634642, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0031933409627527}, {"id": 412, "seek": 230736, "start": 2327.92, "end": 2333.52, "text": " So the term agent is used in all kinds of different ways in the AI literature. And there's a very", "tokens": [51392, 407, 264, 1433, 9461, 307, 1143, 294, 439, 3685, 295, 819, 2098, 294, 264, 7318, 10394, 13, 400, 456, 311, 257, 588, 51672], "temperature": 0.0, "avg_logprob": -0.17742775678634642, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0031933409627527}, {"id": 413, "seek": 233352, "start": 2333.52, "end": 2337.44, "text": " lightweight notion of agency, which is something that simply, you know, hasn't", "tokens": [50364, 22052, 10710, 295, 7934, 11, 597, 307, 746, 300, 2935, 11, 291, 458, 11, 6132, 380, 50560], "temperature": 0.0, "avg_logprob": -0.10560389801307961, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.013605203479528427}, {"id": 414, "seek": 233352, "start": 2338.56, "end": 2346.24, "text": " performs actions in some environment and gets, you know, sort of some kind of sense sense or", "tokens": [50616, 26213, 5909, 294, 512, 2823, 293, 2170, 11, 291, 458, 11, 1333, 295, 512, 733, 295, 2020, 2020, 420, 51000], "temperature": 0.0, "avg_logprob": -0.10560389801307961, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.013605203479528427}, {"id": 415, "seek": 233352, "start": 2346.24, "end": 2351.52, "text": " perceptual information back from the environment in a loop. And that's so that's why how we can", "tokens": [51000, 43276, 901, 1589, 646, 490, 264, 2823, 294, 257, 6367, 13, 400, 300, 311, 370, 300, 311, 983, 577, 321, 393, 51264], "temperature": 0.0, "avg_logprob": -0.10560389801307961, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.013605203479528427}, {"id": 416, "seek": 233352, "start": 2351.52, "end": 2358.08, "text": " talk about, for example, reinforcement learning agent. And that concept of agency of an agent is", "tokens": [51264, 751, 466, 11, 337, 1365, 11, 29280, 2539, 9461, 13, 400, 300, 3410, 295, 7934, 295, 364, 9461, 307, 51592], "temperature": 0.0, "avg_logprob": -0.10560389801307961, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.013605203479528427}, {"id": 417, "seek": 233352, "start": 2358.08, "end": 2362.72, "text": " very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we", "tokens": [51592, 588, 11, 588, 22052, 293, 1177, 380, 3985, 11, 291, 458, 11, 709, 25066, 41567, 13, 583, 382, 2321, 382, 321, 51824], "temperature": 0.0, "avg_logprob": -0.10560389801307961, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.013605203479528427}, {"id": 418, "seek": 236272, "start": 2362.72, "end": 2369.9199999999996, "text": " talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical", "tokens": [50364, 751, 257, 857, 544, 6012, 11154, 466, 12554, 293, 7934, 11, 550, 321, 1565, 322, 544, 11, 257, 688, 544, 25066, 50724], "temperature": 0.0, "avg_logprob": -0.0741710215806961, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0016154294135048985}, {"id": 419, "seek": 236272, "start": 2369.9199999999996, "end": 2375.2, "text": " baggage. So if we talk about something that is acting for itself, then that and if that's what", "tokens": [50724, 41567, 13, 407, 498, 321, 751, 466, 746, 300, 307, 6577, 337, 2564, 11, 550, 300, 293, 498, 300, 311, 437, 50988], "temperature": 0.0, "avg_logprob": -0.0741710215806961, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0016154294135048985}, {"id": 420, "seek": 236272, "start": 2375.2, "end": 2380.08, "text": " we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding", "tokens": [50988, 321, 914, 538, 364, 9461, 11, 293, 286, 519, 264, 20374, 2193, 34080, 47795, 295, 43655, 7222, 307, 439, 33703, 51232], "temperature": 0.0, "avg_logprob": -0.0741710215806961, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0016154294135048985}, {"id": 421, "seek": 236272, "start": 2380.08, "end": 2386.0, "text": " to something a bit more like that, then that's going a whole extra step. And I and to my mind,", "tokens": [51232, 281, 746, 257, 857, 544, 411, 300, 11, 550, 300, 311, 516, 257, 1379, 2857, 1823, 13, 400, 286, 293, 281, 452, 1575, 11, 51528], "temperature": 0.0, "avg_logprob": -0.0741710215806961, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0016154294135048985}, {"id": 422, "seek": 238600, "start": 2386.0, "end": 2395.28, "text": " in today's large language models, we don't see agency of that sort at all, really. The only", "tokens": [50364, 294, 965, 311, 2416, 2856, 5245, 11, 321, 500, 380, 536, 7934, 295, 300, 1333, 412, 439, 11, 534, 13, 440, 787, 50828], "temperature": 0.0, "avg_logprob": -0.13478595909030958, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.046608466655015945}, {"id": 423, "seek": 238600, "start": 2395.28, "end": 2400.72, "text": " actions that they can perform are just, you know, issuing responses to, to, to the users. Now,", "tokens": [50828, 5909, 300, 436, 393, 2042, 366, 445, 11, 291, 458, 11, 43214, 13019, 281, 11, 281, 11, 281, 264, 5022, 13, 823, 11, 51100], "temperature": 0.0, "avg_logprob": -0.13478595909030958, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.046608466655015945}, {"id": 424, "seek": 238600, "start": 2400.72, "end": 2404.88, "text": " let's caveat that immediately, because of course, people are introducing all kinds of extra", "tokens": [51100, 718, 311, 43012, 300, 4258, 11, 570, 295, 1164, 11, 561, 366, 15424, 439, 3685, 295, 2857, 51308], "temperature": 0.0, "avg_logprob": -0.13478595909030958, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.046608466655015945}, {"id": 425, "seek": 238600, "start": 2404.88, "end": 2409.44, "text": " functionality functionality to these models, as new things are being announced on an almost", "tokens": [51308, 14980, 14980, 281, 613, 5245, 11, 382, 777, 721, 366, 885, 7548, 322, 364, 1920, 51536], "temperature": 0.0, "avg_logprob": -0.13478595909030958, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.046608466655015945}, {"id": 426, "seek": 240944, "start": 2409.52, "end": 2415.44, "text": " daily basis. And so one thing we see is so called tool use. So, so, so today's models can", "tokens": [50368, 5212, 5143, 13, 400, 370, 472, 551, 321, 536, 307, 370, 1219, 2290, 764, 13, 407, 11, 370, 11, 370, 965, 311, 5245, 393, 50664], "temperature": 0.0, "avg_logprob": -0.15121046702067056, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.5363451838493347}, {"id": 427, "seek": 240944, "start": 2415.44, "end": 2421.6, "text": " make external calls to APIs that can do all kinds of things, send emails, you know, book", "tokens": [50664, 652, 8320, 5498, 281, 21445, 300, 393, 360, 439, 3685, 295, 721, 11, 2845, 12524, 11, 291, 458, 11, 1446, 50972], "temperature": 0.0, "avg_logprob": -0.15121046702067056, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.5363451838493347}, {"id": 428, "seek": 240944, "start": 2421.6, "end": 2427.68, "text": " hotel rooms for you, potentially all kinds of stuff. So that's greatly expanding the action space,", "tokens": [50972, 7622, 9396, 337, 291, 11, 7263, 439, 3685, 295, 1507, 13, 407, 300, 311, 14147, 14702, 264, 3069, 1901, 11, 51276], "temperature": 0.0, "avg_logprob": -0.15121046702067056, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.5363451838493347}, {"id": 429, "seek": 240944, "start": 2427.68, "end": 2434.0, "text": " their action space beyond just, you know, issuing text to the user. So, so there, they, they, those", "tokens": [51276, 641, 3069, 1901, 4399, 445, 11, 291, 458, 11, 43214, 2487, 281, 264, 4195, 13, 407, 11, 370, 456, 11, 436, 11, 436, 11, 729, 51592], "temperature": 0.0, "avg_logprob": -0.15121046702067056, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.5363451838493347}, {"id": 430, "seek": 243400, "start": 2434.0, "end": 2440.48, "text": " things are a bit more agent-like. And again, you know, you have to be nuanced and about the way you", "tokens": [50364, 721, 366, 257, 857, 544, 9461, 12, 4092, 13, 400, 797, 11, 291, 458, 11, 291, 362, 281, 312, 45115, 293, 466, 264, 636, 291, 50688], "temperature": 0.0, "avg_logprob": -0.11927048819405692, "compression_ratio": 2.0209205020920504, "no_speech_prob": 0.04569070786237717}, {"id": 431, "seek": 243400, "start": 2440.48, "end": 2444.72, "text": " use the words, because, because there it's, you know, there's, there is a bit, you know, it's", "tokens": [50688, 764, 264, 2283, 11, 570, 11, 570, 456, 309, 311, 11, 291, 458, 11, 456, 311, 11, 456, 307, 257, 857, 11, 291, 458, 11, 309, 311, 50900], "temperature": 0.0, "avg_logprob": -0.11927048819405692, "compression_ratio": 2.0209205020920504, "no_speech_prob": 0.04569070786237717}, {"id": 432, "seek": 243400, "start": 2444.72, "end": 2450.64, "text": " a bit, it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more", "tokens": [50900, 257, 857, 11, 309, 393, 605, 382, 364, 9461, 322, 428, 9490, 13, 407, 294, 300, 2020, 295, 264, 1349, 11, 309, 311, 11, 309, 311, 257, 857, 544, 51196], "temperature": 0.0, "avg_logprob": -0.11927048819405692, "compression_ratio": 2.0209205020920504, "no_speech_prob": 0.04569070786237717}, {"id": 433, "seek": 243400, "start": 2450.64, "end": 2456.08, "text": " agent-like. And, but it's still not acting for itself. So in that full blown notion of agency", "tokens": [51196, 9461, 12, 4092, 13, 400, 11, 457, 309, 311, 920, 406, 6577, 337, 2564, 13, 407, 294, 300, 1577, 16479, 10710, 295, 7934, 51468], "temperature": 0.0, "avg_logprob": -0.11927048819405692, "compression_ratio": 2.0209205020920504, "no_speech_prob": 0.04569070786237717}, {"id": 434, "seek": 243400, "start": 2456.08, "end": 2461.12, "text": " that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for", "tokens": [51468, 300, 311, 11, 300, 311, 11, 300, 311, 33919, 281, 294, 264, 20374, 2193, 34080, 47795, 7222, 11, 309, 311, 920, 406, 6577, 337, 51720], "temperature": 0.0, "avg_logprob": -0.11927048819405692, "compression_ratio": 2.0209205020920504, "no_speech_prob": 0.04569070786237717}, {"id": 435, "seek": 246112, "start": 2461.2, "end": 2465.3599999999997, "text": " itself. So we still don't have agency in that sense. That would be a whole extra step.", "tokens": [50368, 2564, 13, 407, 321, 920, 500, 380, 362, 7934, 294, 300, 2020, 13, 663, 576, 312, 257, 1379, 2857, 1823, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11743671243840997, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.011592114344239235}, {"id": 436, "seek": 246112, "start": 2466.48, "end": 2471.68, "text": " Yes. And now I want to hit quite a big topic, because this is something that you point to", "tokens": [50632, 1079, 13, 400, 586, 286, 528, 281, 2045, 1596, 257, 955, 4829, 11, 570, 341, 307, 746, 300, 291, 935, 281, 50892], "temperature": 0.0, "avg_logprob": -0.11743671243840997, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.011592114344239235}, {"id": 437, "seek": 246112, "start": 2471.68, "end": 2477.7599999999998, "text": " in all of your work, which is talking about the importance of physically embodied agents.", "tokens": [50892, 294, 439, 295, 428, 589, 11, 597, 307, 1417, 466, 264, 7379, 295, 9762, 42046, 12554, 13, 51196], "temperature": 0.0, "avg_logprob": -0.11743671243840997, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.011592114344239235}, {"id": 438, "seek": 246112, "start": 2478.48, "end": 2481.2799999999997, "text": " And, well, not necessarily physically embodied, but embodied.", "tokens": [51232, 400, 11, 731, 11, 406, 4725, 9762, 42046, 11, 457, 42046, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11743671243840997, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.011592114344239235}, {"id": 439, "seek": 246112, "start": 2481.2799999999997, "end": 2486.24, "text": " Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's,", "tokens": [51372, 1042, 11, 300, 311, 437, 286, 528, 281, 483, 281, 13, 663, 311, 437, 286, 528, 281, 483, 281, 13, 865, 13, 1436, 286, 718, 311, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11743671243840997, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.011592114344239235}, {"id": 440, "seek": 248624, "start": 2486.24, "end": 2491.4399999999996, "text": " let's test the principle a little bit. So we are, you know, both physicalists and we,", "tokens": [50364, 718, 311, 1500, 264, 8665, 257, 707, 857, 13, 407, 321, 366, 11, 291, 458, 11, 1293, 4001, 1751, 293, 321, 11, 50624], "temperature": 0.0, "avg_logprob": -0.13694199393777287, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.0352700874209404}, {"id": 441, "seek": 248624, "start": 2491.4399999999996, "end": 2494.3199999999997, "text": " I'm not any kind of IST. You're not, you're not a physicalist.", "tokens": [50624, 286, 478, 406, 604, 733, 295, 6205, 51, 13, 509, 434, 406, 11, 291, 434, 406, 257, 4001, 468, 13, 50768], "temperature": 0.0, "avg_logprob": -0.13694199393777287, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.0352700874209404}, {"id": 442, "seek": 248624, "start": 2494.3199999999997, "end": 2500.8799999999997, "text": " I don't, well, I don't, I don't like, I don't believe in, you know, signing up for these", "tokens": [50768, 286, 500, 380, 11, 731, 11, 286, 500, 380, 11, 286, 500, 380, 411, 11, 286, 500, 380, 1697, 294, 11, 291, 458, 11, 13393, 493, 337, 613, 51096], "temperature": 0.0, "avg_logprob": -0.13694199393777287, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.0352700874209404}, {"id": 443, "seek": 248624, "start": 2500.8799999999997, "end": 2509.3599999999997, "text": " philosophical positions. So I don't, so I generally don't say I'm this IST or that I don't deny that", "tokens": [51096, 25066, 8432, 13, 407, 286, 500, 380, 11, 370, 286, 5101, 500, 380, 584, 286, 478, 341, 6205, 51, 420, 300, 286, 500, 380, 15744, 300, 51520], "temperature": 0.0, "avg_logprob": -0.13694199393777287, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.0352700874209404}, {"id": 444, "seek": 248624, "start": 2509.3599999999997, "end": 2515.9199999999996, "text": " I'm this or that IST. So, so I don't really like saying that I'm a physicalist or a materialist,", "tokens": [51520, 286, 478, 341, 420, 300, 6205, 51, 13, 407, 11, 370, 286, 500, 380, 534, 411, 1566, 300, 286, 478, 257, 4001, 468, 420, 257, 2527, 468, 11, 51848], "temperature": 0.0, "avg_logprob": -0.13694199393777287, "compression_ratio": 1.891304347826087, "no_speech_prob": 0.0352700874209404}, {"id": 445, "seek": 251592, "start": 2515.92, "end": 2521.36, "text": " or a dualist, or a functionalist, or identity theorist, or any of those ISMS,", "tokens": [50364, 420, 257, 11848, 468, 11, 420, 257, 11745, 468, 11, 420, 6575, 27423, 468, 11, 420, 604, 295, 729, 6205, 10288, 11, 50636], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 446, "seek": 251592, "start": 2522.08, "end": 2526.32, "text": " because they all, to my mind, carry far too much metaphysical baggage.", "tokens": [50672, 570, 436, 439, 11, 281, 452, 1575, 11, 3985, 1400, 886, 709, 30946, 36280, 41567, 13, 50884], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 447, "seek": 251592, "start": 2526.32, "end": 2528.0, "text": " Oh, interesting. So,", "tokens": [50884, 876, 11, 1880, 13, 407, 11, 50968], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 448, "seek": 251592, "start": 2528.0, "end": 2530.2400000000002, "text": " But okay, but that wasn't what the question was about, but let's go.", "tokens": [50968, 583, 1392, 11, 457, 300, 2067, 380, 437, 264, 1168, 390, 466, 11, 457, 718, 311, 352, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 449, "seek": 251592, "start": 2530.2400000000002, "end": 2533.44, "text": " Well, can I give you another risk? I mean, would you identify as a computationalist?", "tokens": [51080, 1042, 11, 393, 286, 976, 291, 1071, 3148, 30, 286, 914, 11, 576, 291, 5876, 382, 257, 28270, 468, 30, 51240], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 450, "seek": 251592, "start": 2535.36, "end": 2539.52, "text": " Well, what do you mean by that exactly? So we're talking about mind here in the context.", "tokens": [51336, 1042, 11, 437, 360, 291, 914, 538, 300, 2293, 30, 407, 321, 434, 1417, 466, 1575, 510, 294, 264, 4319, 13, 51544], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 451, "seek": 251592, "start": 2539.52, "end": 2543.76, "text": " Yeah, we're talking about mind. So, so do you think in principle that minds can be", "tokens": [51544, 865, 11, 321, 434, 1417, 466, 1575, 13, 407, 11, 370, 360, 291, 519, 294, 8665, 300, 9634, 393, 312, 51756], "temperature": 0.0, "avg_logprob": -0.17196335552407682, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0007228284957818687}, {"id": 452, "seek": 254376, "start": 2544.5600000000004, "end": 2551.0400000000004, "text": " replicated, simulated at a high enough fidelity without losing anything, you know, in, in a computer?", "tokens": [50404, 46365, 11, 41713, 412, 257, 1090, 1547, 46404, 1553, 7027, 1340, 11, 291, 458, 11, 294, 11, 294, 257, 3820, 30, 50728], "temperature": 0.0, "avg_logprob": -0.21156934412514292, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004210153594613075}, {"id": 453, "seek": 254376, "start": 2551.0400000000004, "end": 2552.48, "text": " By computers, by computers. Yeah.", "tokens": [50728, 3146, 10807, 11, 538, 10807, 13, 865, 13, 50800], "temperature": 0.0, "avg_logprob": -0.21156934412514292, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004210153594613075}, {"id": 454, "seek": 254376, "start": 2556.0, "end": 2562.4, "text": " Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build,", "tokens": [50976, 1042, 11, 370, 286, 1116, 528, 281, 733, 295, 319, 44598, 651, 264, 3932, 13, 286, 576, 584, 300, 286, 519, 300, 321, 393, 1322, 11, 51296], "temperature": 0.0, "avg_logprob": -0.21156934412514292, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004210153594613075}, {"id": 455, "seek": 254376, "start": 2563.44, "end": 2569.76, "text": " I do think that we can build artifacts, you know, embodied artifacts, robots,", "tokens": [51348, 286, 360, 519, 300, 321, 393, 1322, 24617, 11, 291, 458, 11, 42046, 24617, 11, 14733, 11, 51664], "temperature": 0.0, "avg_logprob": -0.21156934412514292, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004210153594613075}, {"id": 456, "seek": 256976, "start": 2570.0, "end": 2577.6000000000004, "text": " that, that are controlled by computers and ordinary digital computers. And I think that we", "tokens": [50376, 300, 11, 300, 366, 10164, 538, 10807, 293, 10547, 4562, 10807, 13, 400, 286, 519, 300, 321, 50756], "temperature": 0.0, "avg_logprob": -0.21878123813205294, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003206266788765788}, {"id": 457, "seek": 256976, "start": 2577.6000000000004, "end": 2586.48, "text": " can make them that exhibit the kind of behavior that would make us want to use the word mind,", "tokens": [50756, 393, 652, 552, 300, 20487, 264, 733, 295, 5223, 300, 576, 652, 505, 528, 281, 764, 264, 1349, 1575, 11, 51200], "temperature": 0.0, "avg_logprob": -0.21878123813205294, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003206266788765788}, {"id": 458, "seek": 256976, "start": 2587.76, "end": 2593.6800000000003, "text": " and all those mental type psychological terms in the, to describe their behavior.", "tokens": [51264, 293, 439, 729, 4973, 2010, 14346, 2115, 294, 264, 11, 281, 6786, 641, 5223, 13, 51560], "temperature": 0.0, "avg_logprob": -0.21878123813205294, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003206266788765788}, {"id": 459, "seek": 256976, "start": 2593.6800000000003, "end": 2594.2400000000002, "text": " Okay.", "tokens": [51560, 1033, 13, 51588], "temperature": 0.0, "avg_logprob": -0.21878123813205294, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003206266788765788}, {"id": 460, "seek": 256976, "start": 2594.2400000000002, "end": 2597.84, "text": " Right. So that's, so that's, but, but I've rephrased it in, you know,", "tokens": [51588, 1779, 13, 407, 300, 311, 11, 370, 300, 311, 11, 457, 11, 457, 286, 600, 319, 44598, 1937, 309, 294, 11, 291, 458, 11, 51768], "temperature": 0.0, "avg_logprob": -0.21878123813205294, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.003206266788765788}, {"id": 461, "seek": 259784, "start": 2598.56, "end": 2602.2400000000002, "text": " the claim in a very, very different sort of way, right? It's, it's to do with", "tokens": [50400, 264, 3932, 294, 257, 588, 11, 588, 819, 1333, 295, 636, 11, 558, 30, 467, 311, 11, 309, 311, 281, 360, 365, 50584], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 462, "seek": 259784, "start": 2602.2400000000002, "end": 2606.56, "text": " a much more practical thing. Can we build this? Could, by the way, this is, you know,", "tokens": [50584, 257, 709, 544, 8496, 551, 13, 1664, 321, 1322, 341, 30, 7497, 11, 538, 264, 636, 11, 341, 307, 11, 291, 458, 11, 50800], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 463, "seek": 259784, "start": 2606.56, "end": 2610.1600000000003, "text": " could we, it's not saying that we've got these things now, but could we build something like that", "tokens": [50800, 727, 321, 11, 309, 311, 406, 1566, 300, 321, 600, 658, 613, 721, 586, 11, 457, 727, 321, 1322, 746, 411, 300, 50980], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 464, "seek": 259784, "start": 2611.76, "end": 2616.0, "text": " that exhibited this kind of behavior that we would talk about in this particular kind of way?", "tokens": [51060, 300, 49446, 341, 733, 295, 5223, 300, 321, 576, 751, 466, 294, 341, 1729, 733, 295, 636, 30, 51272], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 465, "seek": 259784, "start": 2616.0, "end": 2619.6000000000004, "text": " And I would say, yes, I think we probably can. It's an empirical claim.", "tokens": [51272, 400, 286, 576, 584, 11, 2086, 11, 286, 519, 321, 1391, 393, 13, 467, 311, 364, 31886, 3932, 13, 51452], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 466, "seek": 259784, "start": 2619.6000000000004, "end": 2624.6400000000003, "text": " So, so there are a few steps you made there that will, will kind of unpack one, one at a time.", "tokens": [51452, 407, 11, 370, 456, 366, 257, 1326, 4439, 291, 1027, 456, 300, 486, 11, 486, 733, 295, 26699, 472, 11, 472, 412, 257, 565, 13, 51704], "temperature": 0.0, "avg_logprob": -0.11343675893503469, "compression_ratio": 1.74, "no_speech_prob": 0.00036335590993985534}, {"id": 467, "seek": 262464, "start": 2624.72, "end": 2630.7999999999997, "text": " So you use the word embodied, you use the word behavior, and you use the word interpret.", "tokens": [50368, 407, 291, 764, 264, 1349, 42046, 11, 291, 764, 264, 1349, 5223, 11, 293, 291, 764, 264, 1349, 7302, 13, 50672], "temperature": 0.0, "avg_logprob": -0.10678567798859483, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.002248292090371251}, {"id": 468, "seek": 262464, "start": 2631.6, "end": 2636.96, "text": " So the embodied thing is really interesting because, you know, I could say, well,", "tokens": [50712, 407, 264, 42046, 551, 307, 534, 1880, 570, 11, 291, 458, 11, 286, 727, 584, 11, 731, 11, 50980], "temperature": 0.0, "avg_logprob": -0.10678567798859483, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.002248292090371251}, {"id": 469, "seek": 262464, "start": 2636.96, "end": 2640.96, "text": " why does it have to be embodied? I can just simulate the entire universe and it's as if", "tokens": [50980, 983, 775, 309, 362, 281, 312, 42046, 30, 286, 393, 445, 27817, 264, 2302, 6445, 293, 309, 311, 382, 498, 51180], "temperature": 0.0, "avg_logprob": -0.10678567798859483, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.002248292090371251}, {"id": 470, "seek": 262464, "start": 2640.96, "end": 2646.3199999999997, "text": " it's embodied. So I think this is, this is the intuition that I'm having about how you think", "tokens": [51180, 309, 311, 42046, 13, 407, 286, 519, 341, 307, 11, 341, 307, 264, 24002, 300, 286, 478, 1419, 466, 577, 291, 519, 51448], "temperature": 0.0, "avg_logprob": -0.10678567798859483, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.002248292090371251}, {"id": 471, "seek": 262464, "start": 2646.3199999999997, "end": 2653.7599999999998, "text": " here. I think you think that being physically embodied is useful because the universe is a", "tokens": [51448, 510, 13, 286, 519, 291, 519, 300, 885, 9762, 42046, 307, 4420, 570, 264, 6445, 307, 257, 51820], "temperature": 0.0, "avg_logprob": -0.10678567798859483, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.002248292090371251}, {"id": 472, "seek": 265376, "start": 2653.76, "end": 2658.1600000000003, "text": " big computer. The universe has given us all of these things, all of these cognizing elements.", "tokens": [50364, 955, 3820, 13, 440, 6445, 575, 2212, 505, 439, 295, 613, 721, 11, 439, 295, 613, 11786, 3319, 4959, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07793936559132167, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0034753333311527967}, {"id": 473, "seek": 265376, "start": 2658.1600000000003, "end": 2663.36, "text": " I mean, everything is a form of externalized cognition. And if I as a rational agent want to", "tokens": [50584, 286, 914, 11, 1203, 307, 257, 1254, 295, 8320, 1602, 46905, 13, 400, 498, 286, 382, 257, 15090, 9461, 528, 281, 50844], "temperature": 0.0, "avg_logprob": -0.07793936559132167, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0034753333311527967}, {"id": 474, "seek": 265376, "start": 2663.36, "end": 2668.1600000000003, "text": " perform an effective computation, it's much easier for me to do it in the physical world", "tokens": [50844, 2042, 364, 4942, 24903, 11, 309, 311, 709, 3571, 337, 385, 281, 360, 309, 294, 264, 4001, 1002, 51084], "temperature": 0.0, "avg_logprob": -0.07793936559132167, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0034753333311527967}, {"id": 475, "seek": 265376, "start": 2668.1600000000003, "end": 2674.1600000000003, "text": " because the universe is doing most of the work. And my co co host, Keith Duggar, he actually", "tokens": [51084, 570, 264, 6445, 307, 884, 881, 295, 264, 589, 13, 400, 452, 598, 598, 3975, 11, 20613, 413, 697, 2976, 11, 415, 767, 51384], "temperature": 0.0, "avg_logprob": -0.07793936559132167, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0034753333311527967}, {"id": 476, "seek": 265376, "start": 2674.1600000000003, "end": 2680.0, "text": " thinks that the universe is a hyper computer, which means it's performing types of computation that", "tokens": [51384, 7309, 300, 264, 6445, 307, 257, 9848, 3820, 11, 597, 1355, 309, 311, 10205, 3467, 295, 24903, 300, 51676], "temperature": 0.0, "avg_logprob": -0.07793936559132167, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0034753333311527967}, {"id": 477, "seek": 268000, "start": 2680.0, "end": 2685.52, "text": " we could never do with ordinary computers. So that's the thing. Would you agree with that? Or", "tokens": [50364, 321, 727, 1128, 360, 365, 10547, 10807, 13, 407, 300, 311, 264, 551, 13, 6068, 291, 3986, 365, 300, 30, 1610, 50640], "temperature": 0.0, "avg_logprob": -0.13682224953821462, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0027531185187399387}, {"id": 478, "seek": 268000, "start": 2685.52, "end": 2689.28, "text": " do you do you want to sort of go back and say, oh, no, actually, just we could simulate anything in", "tokens": [50640, 360, 291, 360, 291, 528, 281, 1333, 295, 352, 646, 293, 584, 11, 1954, 11, 572, 11, 767, 11, 445, 321, 727, 27817, 1340, 294, 50828], "temperature": 0.0, "avg_logprob": -0.13682224953821462, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0027531185187399387}, {"id": 479, "seek": 268000, "start": 2689.28, "end": 2695.12, "text": " a computer? So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's", "tokens": [50828, 257, 3820, 30, 407, 360, 286, 3986, 365, 597, 857, 30, 1144, 286, 3986, 300, 264, 6445, 307, 257, 9848, 3820, 30, 407, 300, 311, 51120], "temperature": 0.0, "avg_logprob": -0.13682224953821462, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0027531185187399387}, {"id": 480, "seek": 268000, "start": 2695.12, "end": 2702.32, "text": " the fair. Well, that would be a nice thing. So whether one agrees or not with that is a matter", "tokens": [51120, 264, 3143, 13, 1042, 11, 300, 576, 312, 257, 1481, 551, 13, 407, 1968, 472, 26383, 420, 406, 365, 300, 307, 257, 1871, 51480], "temperature": 0.0, "avg_logprob": -0.13682224953821462, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0027531185187399387}, {"id": 481, "seek": 268000, "start": 2702.32, "end": 2707.12, "text": " of understanding the physics and the maths. And so it's not a matter of opinion. It's a matter of", "tokens": [51480, 295, 3701, 264, 10649, 293, 264, 36287, 13, 400, 370, 309, 311, 406, 257, 1871, 295, 4800, 13, 467, 311, 257, 1871, 295, 51720], "temperature": 0.0, "avg_logprob": -0.13682224953821462, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0027531185187399387}, {"id": 482, "seek": 270712, "start": 2707.12, "end": 2713.68, "text": " following through the physics and the maths and so on. But so do I agree with what were the other", "tokens": [50364, 3480, 807, 264, 10649, 293, 264, 36287, 293, 370, 322, 13, 583, 370, 360, 286, 3986, 365, 437, 645, 264, 661, 50692], "temperature": 0.0, "avg_logprob": -0.13458686962462307, "compression_ratio": 1.70703125, "no_speech_prob": 0.004406863823533058}, {"id": 483, "seek": 270712, "start": 2713.68, "end": 2717.44, "text": " things that was a big long list of things that you're asking me to ascent to or otherwise?", "tokens": [50692, 721, 300, 390, 257, 955, 938, 1329, 295, 721, 300, 291, 434, 3365, 385, 281, 382, 2207, 281, 420, 5911, 30, 50880], "temperature": 0.0, "avg_logprob": -0.13458686962462307, "compression_ratio": 1.70703125, "no_speech_prob": 0.004406863823533058}, {"id": 484, "seek": 270712, "start": 2718.08, "end": 2724.88, "text": " Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just,", "tokens": [50912, 1042, 11, 370, 286, 478, 257, 2603, 8320, 468, 2059, 13, 583, 264, 1778, 286, 478, 364, 8320, 468, 307, 286, 445, 11, 51252], "temperature": 0.0, "avg_logprob": -0.13458686962462307, "compression_ratio": 1.70703125, "no_speech_prob": 0.004406863823533058}, {"id": 485, "seek": 270712, "start": 2724.88, "end": 2729.7599999999998, "text": " I think cognition is a matter of computation and complexity and divergence.", "tokens": [51252, 286, 519, 46905, 307, 257, 1871, 295, 24903, 293, 14024, 293, 47387, 13, 51496], "temperature": 0.0, "avg_logprob": -0.13458686962462307, "compression_ratio": 1.70703125, "no_speech_prob": 0.004406863823533058}, {"id": 486, "seek": 270712, "start": 2729.7599999999998, "end": 2735.68, "text": " Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is,", "tokens": [51496, 865, 13, 407, 393, 286, 1590, 291, 456, 30, 407, 437, 360, 291, 914, 538, 307, 30, 407, 562, 291, 584, 46905, 307, 11, 51792], "temperature": 0.0, "avg_logprob": -0.13458686962462307, "compression_ratio": 1.70703125, "no_speech_prob": 0.004406863823533058}, {"id": 487, "seek": 273568, "start": 2735.7599999999998, "end": 2742.64, "text": " what do you mean by is? Now, that might sound like some, you know, really annoying,", "tokens": [50368, 437, 360, 291, 914, 538, 307, 30, 823, 11, 300, 1062, 1626, 411, 512, 11, 291, 458, 11, 534, 11304, 11, 50712], "temperature": 0.0, "avg_logprob": -0.14188477233216001, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.010614707134664059}, {"id": 488, "seek": 273568, "start": 2742.64, "end": 2749.12, "text": " pedantic philosophers kind of question. But the problem is that there's an everyday sense in which", "tokens": [50712, 5670, 7128, 36839, 733, 295, 1168, 13, 583, 264, 1154, 307, 300, 456, 311, 364, 7429, 2020, 294, 597, 51036], "temperature": 0.0, "avg_logprob": -0.14188477233216001, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.010614707134664059}, {"id": 489, "seek": 273568, "start": 2749.12, "end": 2754.64, "text": " we use words like is. And then there's a philosopher's sense in which we start to use words like is", "tokens": [51036, 321, 764, 2283, 411, 307, 13, 400, 550, 456, 311, 257, 29805, 311, 2020, 294, 597, 321, 722, 281, 764, 2283, 411, 307, 51312], "temperature": 0.0, "avg_logprob": -0.14188477233216001, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.010614707134664059}, {"id": 490, "seek": 273568, "start": 2754.64, "end": 2761.6, "text": " where it suddenly starts to carry this massive metaphysical weight. And so when you say you", "tokens": [51312, 689, 309, 5800, 3719, 281, 3985, 341, 5994, 30946, 36280, 3364, 13, 400, 370, 562, 291, 584, 291, 51660], "temperature": 0.0, "avg_logprob": -0.14188477233216001, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.010614707134664059}, {"id": 491, "seek": 276160, "start": 2761.6, "end": 2768.48, "text": " think cognition is, it's as if there were, you know, in the mind of God or in the fundamental", "tokens": [50364, 519, 46905, 307, 11, 309, 311, 382, 498, 456, 645, 11, 291, 458, 11, 294, 264, 1575, 295, 1265, 420, 294, 264, 8088, 50708], "temperature": 0.0, "avg_logprob": -0.07179591725173506, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.040185555815696716}, {"id": 492, "seek": 276160, "start": 2768.48, "end": 2775.36, "text": " reality, a thing which is cognition, whose nature is a certain way, and there's a certain essence to", "tokens": [50708, 4103, 11, 257, 551, 597, 307, 46905, 11, 6104, 3687, 307, 257, 1629, 636, 11, 293, 456, 311, 257, 1629, 12801, 281, 51052], "temperature": 0.0, "avg_logprob": -0.07179591725173506, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.040185555815696716}, {"id": 493, "seek": 276160, "start": 2775.36, "end": 2782.48, "text": " it. And we might discover it, you know, one day, and you have an opinion about what it is, if only", "tokens": [51052, 309, 13, 400, 321, 1062, 4411, 309, 11, 291, 458, 11, 472, 786, 11, 293, 291, 362, 364, 4800, 466, 437, 309, 307, 11, 498, 787, 51408], "temperature": 0.0, "avg_logprob": -0.07179591725173506, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.040185555815696716}, {"id": 494, "seek": 276160, "start": 2782.48, "end": 2789.2799999999997, "text": " you knew the truth. Now, I think that's an entirely wrong way of thinking about all of these philosophical", "tokens": [51408, 291, 2586, 264, 3494, 13, 823, 11, 286, 519, 300, 311, 364, 7696, 2085, 636, 295, 1953, 466, 439, 295, 613, 25066, 51748], "temperature": 0.0, "avg_logprob": -0.07179591725173506, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.040185555815696716}, {"id": 495, "seek": 278928, "start": 2789.28, "end": 2796.6400000000003, "text": " questions. I think cognition is a word. It's a very useful word that we, although it's not", "tokens": [50364, 1651, 13, 286, 519, 46905, 307, 257, 1349, 13, 467, 311, 257, 588, 4420, 1349, 300, 321, 11, 4878, 309, 311, 406, 50732], "temperature": 0.0, "avg_logprob": -0.06481213318674188, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.003174517536535859}, {"id": 496, "seek": 278928, "start": 2796.6400000000003, "end": 2801.6000000000004, "text": " quite an everyday word, but it's a very useful word that scientists apply in all kinds of ways.", "tokens": [50732, 1596, 364, 7429, 1349, 11, 457, 309, 311, 257, 588, 4420, 1349, 300, 7708, 3079, 294, 439, 3685, 295, 2098, 13, 50980], "temperature": 0.0, "avg_logprob": -0.06481213318674188, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.003174517536535859}, {"id": 497, "seek": 278928, "start": 2802.6400000000003, "end": 2811.36, "text": " And so when you use the word is, is my accusation accurate there or not?", "tokens": [51032, 400, 370, 562, 291, 764, 264, 1349, 307, 11, 307, 452, 11168, 399, 8559, 456, 420, 406, 30, 51468], "temperature": 0.0, "avg_logprob": -0.06481213318674188, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.003174517536535859}, {"id": 498, "seek": 278928, "start": 2811.36, "end": 2816.96, "text": " No, it's not. And if you wouldn't mind me making the observation, I think that you have a tendency", "tokens": [51468, 883, 11, 309, 311, 406, 13, 400, 498, 291, 2759, 380, 1575, 385, 1455, 264, 14816, 11, 286, 519, 300, 291, 362, 257, 18187, 51748], "temperature": 0.0, "avg_logprob": -0.06481213318674188, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.003174517536535859}, {"id": 499, "seek": 281696, "start": 2816.96, "end": 2824.2400000000002, "text": " to ascribe dualism to many points of view, like, for example, I'm not a dualist. And for me,", "tokens": [50364, 281, 382, 8056, 11848, 1434, 281, 867, 2793, 295, 1910, 11, 411, 11, 337, 1365, 11, 286, 478, 406, 257, 11848, 468, 13, 400, 337, 385, 11, 50728], "temperature": 0.0, "avg_logprob": -0.2240961055562954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00864921323955059}, {"id": 500, "seek": 281696, "start": 2824.96, "end": 2829.68, "text": " Well, there's nothing to do with dualism. Well, what I'm saying is to do is like the use of", "tokens": [50764, 1042, 11, 456, 311, 1825, 281, 360, 365, 11848, 1434, 13, 1042, 11, 437, 286, 478, 1566, 307, 281, 360, 307, 411, 264, 764, 295, 51000], "temperature": 0.0, "avg_logprob": -0.2240961055562954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00864921323955059}, {"id": 501, "seek": 281696, "start": 2829.68, "end": 2834.2400000000002, "text": " words and what and what and the and the and the work of philosophy.", "tokens": [51000, 2283, 293, 437, 293, 437, 293, 264, 293, 264, 293, 264, 589, 295, 10675, 13, 51228], "temperature": 0.0, "avg_logprob": -0.2240961055562954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00864921323955059}, {"id": 502, "seek": 281696, "start": 2835.12, "end": 2842.7200000000003, "text": " That's absolutely fair. But I think when I said what is what is cognition, as a materialist,", "tokens": [51272, 663, 311, 3122, 3143, 13, 583, 286, 519, 562, 286, 848, 437, 307, 437, 307, 46905, 11, 382, 257, 2527, 468, 11, 51652], "temperature": 0.0, "avg_logprob": -0.2240961055562954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00864921323955059}, {"id": 503, "seek": 284272, "start": 2843.3599999999997, "end": 2849.3599999999997, "text": " for me, it is function dynamics and behavior, right? So it's just a matter of complexity.", "tokens": [50396, 337, 385, 11, 309, 307, 2445, 15679, 293, 5223, 11, 558, 30, 407, 309, 311, 445, 257, 1871, 295, 14024, 13, 50696], "temperature": 0.0, "avg_logprob": -0.18578392831902754, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.020734287798404694}, {"id": 504, "seek": 284272, "start": 2849.9199999999996, "end": 2858.08, "text": " And so I'm probably just I'm probably, you know, happy to kind of agree to that sort of claim,", "tokens": [50724, 400, 370, 286, 478, 1391, 445, 286, 478, 1391, 11, 291, 458, 11, 2055, 281, 733, 295, 3986, 281, 300, 1333, 295, 3932, 11, 51132], "temperature": 0.0, "avg_logprob": -0.18578392831902754, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.020734287798404694}, {"id": 505, "seek": 284272, "start": 2858.08, "end": 2864.56, "text": " you know, so I think so, you know, the thing that I'm, I often say that what am I fundamentally", "tokens": [51132, 291, 458, 11, 370, 286, 519, 370, 11, 291, 458, 11, 264, 551, 300, 286, 478, 11, 286, 2049, 584, 300, 437, 669, 286, 17879, 51456], "temperature": 0.0, "avg_logprob": -0.18578392831902754, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.020734287798404694}, {"id": 506, "seek": 284272, "start": 2864.56, "end": 2870.48, "text": " interested in, I'm interested in understanding cognition and consciousness in the space of", "tokens": [51456, 3102, 294, 11, 286, 478, 3102, 294, 3701, 46905, 293, 10081, 294, 264, 1901, 295, 51752], "temperature": 0.0, "avg_logprob": -0.18578392831902754, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.020734287798404694}, {"id": 507, "seek": 287048, "start": 2870.48, "end": 2876.48, "text": " possible minds. And and and so so, you know, what do I mean by cognition there? And, you know,", "tokens": [50364, 1944, 9634, 13, 400, 293, 293, 370, 370, 11, 291, 458, 11, 437, 360, 286, 914, 538, 46905, 456, 30, 400, 11, 291, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08683198845904806, "compression_ratio": 1.9163179916317992, "no_speech_prob": 0.005107656121253967}, {"id": 508, "seek": 287048, "start": 2876.48, "end": 2881.12, "text": " you can go into all kinds of details to say what you mean by cognition in that in that context.", "tokens": [50664, 291, 393, 352, 666, 439, 3685, 295, 4365, 281, 584, 437, 291, 914, 538, 46905, 294, 300, 294, 300, 4319, 13, 50896], "temperature": 0.0, "avg_logprob": -0.08683198845904806, "compression_ratio": 1.9163179916317992, "no_speech_prob": 0.005107656121253967}, {"id": 509, "seek": 287048, "start": 2881.12, "end": 2886.64, "text": " But I think having done that, I would probably agree that the right way to think of it,", "tokens": [50896, 583, 286, 519, 1419, 1096, 300, 11, 286, 576, 1391, 3986, 300, 264, 558, 636, 281, 519, 295, 309, 11, 51172], "temperature": 0.0, "avg_logprob": -0.08683198845904806, "compression_ratio": 1.9163179916317992, "no_speech_prob": 0.005107656121253967}, {"id": 510, "seek": 287048, "start": 2886.64, "end": 2891.2, "text": " you know, the most useful way to think of cognition is in terms of kind of functional,", "tokens": [51172, 291, 458, 11, 264, 881, 4420, 636, 281, 519, 295, 46905, 307, 294, 2115, 295, 733, 295, 11745, 11, 51400], "temperature": 0.0, "avg_logprob": -0.08683198845904806, "compression_ratio": 1.9163179916317992, "no_speech_prob": 0.005107656121253967}, {"id": 511, "seek": 287048, "start": 2891.2, "end": 2897.28, "text": " computational, infunctional, computational terms, although I would only do so in an embodied", "tokens": [51400, 28270, 11, 1536, 32627, 304, 11, 28270, 2115, 11, 4878, 286, 576, 787, 360, 370, 294, 364, 42046, 51704], "temperature": 0.0, "avg_logprob": -0.08683198845904806, "compression_ratio": 1.9163179916317992, "no_speech_prob": 0.005107656121253967}, {"id": 512, "seek": 289728, "start": 2897.28, "end": 2902.88, "text": " setting. So that maybe is an additional thing. This is where I'm trying to get to. Because,", "tokens": [50364, 3287, 13, 407, 300, 1310, 307, 364, 4497, 551, 13, 639, 307, 689, 286, 478, 1382, 281, 483, 281, 13, 1436, 11, 50644], "temperature": 0.0, "avg_logprob": -0.14029971313476564, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.01656675711274147}, {"id": 513, "seek": 289728, "start": 2902.88, "end": 2907.36, "text": " as I said, I'm not making any ontological claims. It's just a matter, I mean, we can even just use", "tokens": [50644, 382, 286, 848, 11, 286, 478, 406, 1455, 604, 6592, 4383, 9441, 13, 467, 311, 445, 257, 1871, 11, 286, 914, 11, 321, 393, 754, 445, 764, 50868], "temperature": 0.0, "avg_logprob": -0.14029971313476564, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.01656675711274147}, {"id": 514, "seek": 289728, "start": 2907.36, "end": 2912.0800000000004, "text": " the word behavior, forget about function and dynamics. Yeah, I don't mind talking about function", "tokens": [50868, 264, 1349, 5223, 11, 2870, 466, 2445, 293, 15679, 13, 865, 11, 286, 500, 380, 1575, 1417, 466, 2445, 51104], "temperature": 0.0, "avg_logprob": -0.14029971313476564, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.01656675711274147}, {"id": 515, "seek": 289728, "start": 2912.0800000000004, "end": 2915.76, "text": " and dynamic. Well, yeah, I mean, just just to sort of keep it really, really simple, because I'm", "tokens": [51104, 293, 8546, 13, 1042, 11, 1338, 11, 286, 914, 11, 445, 445, 281, 1333, 295, 1066, 309, 534, 11, 534, 2199, 11, 570, 286, 478, 51288], "temperature": 0.0, "avg_logprob": -0.14029971313476564, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.01656675711274147}, {"id": 516, "seek": 289728, "start": 2915.76, "end": 2922.4, "text": " trying to understand why the embodiment is important. And my hypothesis is, and I agree, that", "tokens": [51288, 1382, 281, 1223, 983, 264, 28935, 2328, 307, 1021, 13, 400, 452, 17291, 307, 11, 293, 286, 3986, 11, 300, 51620], "temperature": 0.0, "avg_logprob": -0.14029971313476564, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.01656675711274147}, {"id": 517, "seek": 292240, "start": 2922.7200000000003, "end": 2928.4, "text": " as an externalist, the universe or the physical things around us, the other agents in our system,", "tokens": [50380, 382, 364, 8320, 468, 11, 264, 6445, 420, 264, 4001, 721, 926, 505, 11, 264, 661, 12554, 294, 527, 1185, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1448404226410255, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.008466213941574097}, {"id": 518, "seek": 292240, "start": 2928.4, "end": 2934.32, "text": " they help us perform an effective computation. So presumably, it would be much easier to perform", "tokens": [50664, 436, 854, 505, 2042, 364, 4942, 24903, 13, 407, 26742, 11, 309, 576, 312, 709, 3571, 281, 2042, 50960], "temperature": 0.0, "avg_logprob": -0.1448404226410255, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.008466213941574097}, {"id": 519, "seek": 292240, "start": 2934.32, "end": 2940.32, "text": " computation of higher sophistication, if we embody things in the real world, if we have to", "tokens": [50960, 24903, 295, 2946, 15572, 399, 11, 498, 321, 42575, 721, 294, 264, 957, 1002, 11, 498, 321, 362, 281, 51260], "temperature": 0.0, "avg_logprob": -0.1448404226410255, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.008466213941574097}, {"id": 520, "seek": 292240, "start": 2940.32, "end": 2945.92, "text": " simulate the cognition, we would have to simulate everything. And that, I think, is the reason why", "tokens": [51260, 27817, 264, 46905, 11, 321, 576, 362, 281, 27817, 1203, 13, 400, 300, 11, 286, 519, 11, 307, 264, 1778, 983, 51540], "temperature": 0.0, "avg_logprob": -0.1448404226410255, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.008466213941574097}, {"id": 521, "seek": 294592, "start": 2946.88, "end": 2950.32, "text": " you think that embodiment is so important. But is that fair?", "tokens": [50412, 291, 519, 300, 28935, 2328, 307, 370, 1021, 13, 583, 307, 300, 3143, 30, 50584], "temperature": 0.0, "avg_logprob": -0.16039016329008957, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.2333626002073288}, {"id": 522, "seek": 294592, "start": 2952.32, "end": 2958.64, "text": " I think that's not really the way I would put it. I think the reason I think embodiment is", "tokens": [50684, 286, 519, 300, 311, 406, 534, 264, 636, 286, 576, 829, 309, 13, 286, 519, 264, 1778, 286, 519, 28935, 2328, 307, 51000], "temperature": 0.0, "avg_logprob": -0.16039016329008957, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.2333626002073288}, {"id": 523, "seek": 294592, "start": 2958.64, "end": 2967.36, "text": " important is because it's, well, I mean, for, you know, I mean, okay, in one sense,", "tokens": [51000, 1021, 307, 570, 309, 311, 11, 731, 11, 286, 914, 11, 337, 11, 291, 458, 11, 286, 914, 11, 1392, 11, 294, 472, 2020, 11, 51436], "temperature": 0.0, "avg_logprob": -0.16039016329008957, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.2333626002073288}, {"id": 524, "seek": 294592, "start": 2967.36, "end": 2973.04, "text": " embodiment is important because the only setting in which we use the natural setting,", "tokens": [51436, 28935, 2328, 307, 1021, 570, 264, 787, 3287, 294, 597, 321, 764, 264, 3303, 3287, 11, 51720], "temperature": 0.0, "avg_logprob": -0.16039016329008957, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.2333626002073288}, {"id": 525, "seek": 297304, "start": 2973.12, "end": 2979.7599999999998, "text": " which we deploy, wield the concept of cognition, is in the context of embodied things, of humans", "tokens": [50368, 597, 321, 7274, 11, 35982, 264, 3410, 295, 46905, 11, 307, 294, 264, 4319, 295, 42046, 721, 11, 295, 6255, 50700], "temperature": 0.0, "avg_logprob": -0.14210392179943265, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.016752999275922775}, {"id": 526, "seek": 297304, "start": 2979.7599999999998, "end": 2987.2, "text": " and other animals. So anything else is sort of immediately problematic in one way. But let's", "tokens": [50700, 293, 661, 4882, 13, 407, 1340, 1646, 307, 1333, 295, 4258, 19011, 294, 472, 636, 13, 583, 718, 311, 51072], "temperature": 0.0, "avg_logprob": -0.14210392179943265, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.016752999275922775}, {"id": 527, "seek": 297304, "start": 2987.2, "end": 2994.24, "text": " set that to one side. So let's imagine that there is some kind of notion that we can", "tokens": [51072, 992, 300, 281, 472, 1252, 13, 407, 718, 311, 3811, 300, 456, 307, 512, 733, 295, 10710, 300, 321, 393, 51424], "temperature": 0.0, "avg_logprob": -0.14210392179943265, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.016752999275922775}, {"id": 528, "seek": 297304, "start": 2994.96, "end": 3001.68, "text": " conceive of disembodied cognition, which contemporary large language models make us", "tokens": [51460, 48605, 295, 717, 33748, 378, 1091, 46905, 11, 597, 14878, 2416, 2856, 5245, 652, 505, 51796], "temperature": 0.0, "avg_logprob": -0.14210392179943265, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.016752999275922775}, {"id": 529, "seek": 300168, "start": 3002.3199999999997, "end": 3009.2799999999997, "text": " start to conceive of it a lot more seriously, maybe. So what does embodiment", "tokens": [50396, 722, 281, 48605, 295, 309, 257, 688, 544, 6638, 11, 1310, 13, 407, 437, 775, 28935, 2328, 50744], "temperature": 0.0, "avg_logprob": -0.1310700217446128, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.01760942116379738}, {"id": 530, "seek": 300168, "start": 3010.08, "end": 3018.7999999999997, "text": " give you there, right? I think that's probably what you're thinking of. So in particular,", "tokens": [50784, 976, 291, 456, 11, 558, 30, 286, 519, 300, 311, 1391, 437, 291, 434, 1953, 295, 13, 407, 294, 1729, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1310700217446128, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.01760942116379738}, {"id": 531, "seek": 300168, "start": 3018.7999999999997, "end": 3024.3999999999996, "text": " why might it be difficult to build something that is disembodied? Okay, let's reframe the whole", "tokens": [51220, 983, 1062, 309, 312, 2252, 281, 1322, 746, 300, 307, 717, 33748, 378, 1091, 30, 1033, 11, 718, 311, 13334, 529, 264, 1379, 51500], "temperature": 0.0, "avg_logprob": -0.1310700217446128, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.01760942116379738}, {"id": 532, "seek": 300168, "start": 3024.3999999999996, "end": 3030.7999999999997, "text": " question. Why might it be difficult to build something that is disembodied but replicates", "tokens": [51500, 1168, 13, 1545, 1062, 309, 312, 2252, 281, 1322, 746, 300, 307, 717, 33748, 378, 1091, 457, 3248, 299, 1024, 51820], "temperature": 0.0, "avg_logprob": -0.1310700217446128, "compression_ratio": 1.7425742574257426, "no_speech_prob": 0.01760942116379738}, {"id": 533, "seek": 303080, "start": 3030.8, "end": 3041.04, "text": " the cognitive capabilities of a human being? So I think my answer to that, although it's open to", "tokens": [50364, 264, 15605, 10862, 295, 257, 1952, 885, 30, 407, 286, 519, 452, 1867, 281, 300, 11, 4878, 309, 311, 1269, 281, 50876], "temperature": 0.0, "avg_logprob": -0.12696501702973337, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00245986832305789}, {"id": 534, "seek": 303080, "start": 3041.04, "end": 3045.6800000000003, "text": " refutation by the way things are going in the field, but my answer to that is because", "tokens": [50876, 1895, 11380, 538, 264, 636, 721, 366, 516, 294, 264, 2519, 11, 457, 452, 1867, 281, 300, 307, 570, 51108], "temperature": 0.0, "avg_logprob": -0.12696501702973337, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00245986832305789}, {"id": 535, "seek": 303080, "start": 3048.8, "end": 3055.84, "text": " our embodied interaction with the world enables us to learn the kind of causal microstructure of", "tokens": [51264, 527, 42046, 9285, 365, 264, 1002, 17077, 505, 281, 1466, 264, 733, 295, 38755, 3123, 27494, 2885, 295, 51616], "temperature": 0.0, "avg_logprob": -0.12696501702973337, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00245986832305789}, {"id": 536, "seek": 305584, "start": 3056.32, "end": 3062.0, "text": " the physical world. And the causal microstructure is all about physical objects and the way they", "tokens": [50388, 264, 4001, 1002, 13, 400, 264, 38755, 3123, 27494, 2885, 307, 439, 466, 4001, 6565, 293, 264, 636, 436, 50672], "temperature": 0.0, "avg_logprob": -0.07811688780784606, "compression_ratio": 1.8325581395348838, "no_speech_prob": 0.05388864502310753}, {"id": 537, "seek": 305584, "start": 3062.0, "end": 3068.08, "text": " interact with each other and physical substances, liquids and gases and gravity and stuff like that.", "tokens": [50672, 4648, 365, 1184, 661, 293, 4001, 25455, 11, 38960, 293, 21452, 293, 12110, 293, 1507, 411, 300, 13, 50976], "temperature": 0.0, "avg_logprob": -0.07811688780784606, "compression_ratio": 1.8325581395348838, "no_speech_prob": 0.05388864502310753}, {"id": 538, "seek": 305584, "start": 3068.08, "end": 3073.36, "text": " So what I've called foundational common sense is it enables us to acquire foundational common sense", "tokens": [50976, 407, 437, 286, 600, 1219, 32195, 2689, 2020, 307, 309, 17077, 505, 281, 20001, 32195, 2689, 2020, 51240], "temperature": 0.0, "avg_logprob": -0.07811688780784606, "compression_ratio": 1.8325581395348838, "no_speech_prob": 0.05388864502310753}, {"id": 539, "seek": 305584, "start": 3073.36, "end": 3078.88, "text": " by interacting with the everyday physical world and the particular causal microstructure that it", "tokens": [51240, 538, 18017, 365, 264, 7429, 4001, 1002, 293, 264, 1729, 38755, 3123, 27494, 2885, 300, 309, 51516], "temperature": 0.0, "avg_logprob": -0.07811688780784606, "compression_ratio": 1.8325581395348838, "no_speech_prob": 0.05388864502310753}, {"id": 540, "seek": 307888, "start": 3078.88, "end": 3084.56, "text": " has. And part of that is to do with the fact that the, so this is really important, that the", "tokens": [50364, 575, 13, 400, 644, 295, 300, 307, 281, 360, 365, 264, 1186, 300, 264, 11, 370, 341, 307, 534, 1021, 11, 300, 264, 50648], "temperature": 0.0, "avg_logprob": -0.12886711858933972, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.17391088604927063}, {"id": 541, "seek": 307888, "start": 3085.52, "end": 3093.12, "text": " everyday physical world has this, is predominantly smooth. It has this smoothness property that's", "tokens": [50696, 7429, 4001, 1002, 575, 341, 11, 307, 29893, 5508, 13, 467, 575, 341, 5508, 1287, 4707, 300, 311, 51076], "temperature": 0.0, "avg_logprob": -0.12886711858933972, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.17391088604927063}, {"id": 542, "seek": 307888, "start": 3093.12, "end": 3097.6800000000003, "text": " really, really important. And what that means is that, sorry, just in very physical terms, it means", "tokens": [51076, 534, 11, 534, 1021, 13, 400, 437, 300, 1355, 307, 300, 11, 2597, 11, 445, 294, 588, 4001, 2115, 11, 309, 1355, 51304], "temperature": 0.0, "avg_logprob": -0.12886711858933972, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.17391088604927063}, {"id": 543, "seek": 307888, "start": 3097.6800000000003, "end": 3104.56, "text": " that it's full of kind of surfaces where one place is very much like the next place along,", "tokens": [51304, 300, 309, 311, 1577, 295, 733, 295, 16130, 689, 472, 1081, 307, 588, 709, 411, 264, 958, 1081, 2051, 11, 51648], "temperature": 0.0, "avg_logprob": -0.12886711858933972, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.17391088604927063}, {"id": 544, "seek": 310456, "start": 3104.56, "end": 3109.6, "text": " very much like the next place along. And our visual field is very, very similar. You move", "tokens": [50364, 588, 709, 411, 264, 958, 1081, 2051, 13, 400, 527, 5056, 2519, 307, 588, 11, 588, 2531, 13, 509, 1286, 50616], "temperature": 0.0, "avg_logprob": -0.12805012596978083, "compression_ratio": 1.75, "no_speech_prob": 0.025997482240200043}, {"id": 545, "seek": 310456, "start": 3109.6, "end": 3113.7599999999998, "text": " along a little bit in the visual field and it's very, very, very similar, very, very similar.", "tokens": [50616, 2051, 257, 707, 857, 294, 264, 5056, 2519, 293, 309, 311, 588, 11, 588, 11, 588, 2531, 11, 588, 11, 588, 2531, 13, 50824], "temperature": 0.0, "avg_logprob": -0.12805012596978083, "compression_ratio": 1.75, "no_speech_prob": 0.025997482240200043}, {"id": 546, "seek": 310456, "start": 3113.7599999999998, "end": 3120.16, "text": " So the reality or the everyday physical world has this fundamental smoothness property,", "tokens": [50824, 407, 264, 4103, 420, 264, 7429, 4001, 1002, 575, 341, 8088, 5508, 1287, 4707, 11, 51144], "temperature": 0.0, "avg_logprob": -0.12805012596978083, "compression_ratio": 1.75, "no_speech_prob": 0.025997482240200043}, {"id": 547, "seek": 310456, "start": 3120.7999999999997, "end": 3129.2, "text": " but it's punctuated by all these discontinuities. And that's the way, that's its fundamental", "tokens": [51176, 457, 309, 311, 27006, 27275, 538, 439, 613, 31420, 84, 1088, 13, 400, 300, 311, 264, 636, 11, 300, 311, 1080, 8088, 51596], "temperature": 0.0, "avg_logprob": -0.12805012596978083, "compression_ratio": 1.75, "no_speech_prob": 0.025997482240200043}, {"id": 548, "seek": 312920, "start": 3129.2, "end": 3134.64, "text": " structure is this basic smooth, against the backdrop of the smoothness are all these discontinuities.", "tokens": [50364, 3877, 307, 341, 3875, 5508, 11, 1970, 264, 32697, 295, 264, 5508, 1287, 366, 439, 613, 31420, 84, 1088, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1412414682322535, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.13704442977905273}, {"id": 549, "seek": 312920, "start": 3134.64, "end": 3141.04, "text": " And then there's a kind of law like regular way in which all of this stuff operates with itself,", "tokens": [50636, 400, 550, 456, 311, 257, 733, 295, 2101, 411, 3890, 636, 294, 597, 439, 295, 341, 1507, 22577, 365, 2564, 11, 50956], "temperature": 0.0, "avg_logprob": -0.1412414682322535, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.13704442977905273}, {"id": 550, "seek": 312920, "start": 3141.04, "end": 3146.24, "text": " you have surfaces interacting with each other with things going, you know, so all of our foundational", "tokens": [50956, 291, 362, 16130, 18017, 365, 1184, 661, 365, 721, 516, 11, 291, 458, 11, 370, 439, 295, 527, 32195, 51216], "temperature": 0.0, "avg_logprob": -0.1412414682322535, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.13704442977905273}, {"id": 551, "seek": 312920, "start": 3146.24, "end": 3155.52, "text": " common sense to do with things like paths and support and containment and all those sorts of", "tokens": [51216, 2689, 2020, 281, 360, 365, 721, 411, 14518, 293, 1406, 293, 44058, 293, 439, 729, 7527, 295, 51680], "temperature": 0.0, "avg_logprob": -0.1412414682322535, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.13704442977905273}, {"id": 552, "seek": 315552, "start": 3155.52, "end": 3162.4, "text": " basic things that I think make up the very foundation of our conceptual framework, they're", "tokens": [50364, 3875, 721, 300, 286, 519, 652, 493, 264, 588, 7030, 295, 527, 24106, 8388, 11, 436, 434, 50708], "temperature": 0.0, "avg_logprob": -0.1950799029806386, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.011037729680538177}, {"id": 553, "seek": 315552, "start": 3162.4, "end": 3167.84, "text": " all grounded in that kind of way. Yeah. And this is so interesting. So your basic argument is", "tokens": [50708, 439, 23535, 294, 300, 733, 295, 636, 13, 865, 13, 400, 341, 307, 370, 1880, 13, 407, 428, 3875, 6770, 307, 50980], "temperature": 0.0, "avg_logprob": -0.1950799029806386, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.011037729680538177}, {"id": 554, "seek": 315552, "start": 3167.84, "end": 3172.56, "text": " knowledge acquisition efficiency is the reason for physical embodiment. And yeah, I think that's", "tokens": [50980, 3601, 21668, 10493, 307, 264, 1778, 337, 4001, 28935, 2328, 13, 400, 1338, 11, 286, 519, 300, 311, 51216], "temperature": 0.0, "avg_logprob": -0.1950799029806386, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.011037729680538177}, {"id": 555, "seek": 315552, "start": 3172.56, "end": 3177.68, "text": " a reasonable way of putting it. Yeah. But that's very much an in practice rather than an in principle", "tokens": [51216, 257, 10585, 636, 295, 3372, 309, 13, 865, 13, 583, 300, 311, 588, 709, 364, 294, 3124, 2831, 813, 364, 294, 8665, 51472], "temperature": 0.0, "avg_logprob": -0.1950799029806386, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.011037729680538177}, {"id": 556, "seek": 315552, "start": 3177.68, "end": 3182.56, "text": " argument. It is an in, yeah. Yeah. But you know, for sample efficiency, but what I'm hearing though", "tokens": [51472, 6770, 13, 467, 307, 364, 294, 11, 1338, 13, 865, 13, 583, 291, 458, 11, 337, 6889, 10493, 11, 457, 437, 286, 478, 4763, 1673, 51716], "temperature": 0.0, "avg_logprob": -0.1950799029806386, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.011037729680538177}, {"id": 557, "seek": 318256, "start": 3182.7999999999997, "end": 3188.24, "text": " is echoes of the old Murray Shanahan, because obviously you started your career in symbolic AI", "tokens": [50376, 307, 47051, 295, 264, 1331, 27291, 25536, 21436, 11, 570, 2745, 291, 1409, 428, 3988, 294, 25755, 7318, 50648], "temperature": 0.0, "avg_logprob": -0.15192517819611923, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.007979234680533409}, {"id": 558, "seek": 318256, "start": 3188.24, "end": 3192.96, "text": " and these were the arguments that were made sometimes with a rationalism, nativism point of", "tokens": [50648, 293, 613, 645, 264, 12869, 300, 645, 1027, 2171, 365, 257, 15090, 1434, 11, 2249, 592, 1434, 935, 295, 50884], "temperature": 0.0, "avg_logprob": -0.15192517819611923, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.007979234680533409}, {"id": 559, "seek": 318256, "start": 3192.96, "end": 3198.16, "text": " view, but it's like the contains in templates, they would argue that it's just baked into us and we", "tokens": [50884, 1910, 11, 457, 309, 311, 411, 264, 8306, 294, 21165, 11, 436, 576, 9695, 300, 309, 311, 445, 19453, 666, 505, 293, 321, 51144], "temperature": 0.0, "avg_logprob": -0.15192517819611923, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.007979234680533409}, {"id": 560, "seek": 318256, "start": 3198.16, "end": 3202.56, "text": " understand it. But you could as an empiricist argue, and I, you know, I'm very amenable to this,", "tokens": [51144, 1223, 309, 13, 583, 291, 727, 382, 364, 25790, 299, 468, 9695, 11, 293, 286, 11, 291, 458, 11, 286, 478, 588, 18497, 712, 281, 341, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15192517819611923, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.007979234680533409}, {"id": 561, "seek": 318256, "start": 3202.56, "end": 3207.2799999999997, "text": " that the physical world actually helps us learn abstractions because we're putting things in", "tokens": [51364, 300, 264, 4001, 1002, 767, 3665, 505, 1466, 12649, 626, 570, 321, 434, 3372, 721, 294, 51600], "temperature": 0.0, "avg_logprob": -0.15192517819611923, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.007979234680533409}, {"id": 562, "seek": 320728, "start": 3207.36, "end": 3212.0, "text": " containers all of the time. Yeah. Right. So there's that kind of efficiency of knowledge", "tokens": [50368, 17089, 439, 295, 264, 565, 13, 865, 13, 1779, 13, 407, 456, 311, 300, 733, 295, 10493, 295, 3601, 50600], "temperature": 0.0, "avg_logprob": -0.13657722833021632, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0872027799487114}, {"id": 563, "seek": 320728, "start": 3212.0, "end": 3216.5600000000004, "text": " acquisition, which is dramatically increased when you're situated in the physical. Yeah. Yeah.", "tokens": [50600, 21668, 11, 597, 307, 17548, 6505, 562, 291, 434, 30143, 294, 264, 4001, 13, 865, 13, 865, 13, 50828], "temperature": 0.0, "avg_logprob": -0.13657722833021632, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0872027799487114}, {"id": 564, "seek": 320728, "start": 3216.5600000000004, "end": 3222.7200000000003, "text": " Absolutely. Yeah. So I think that if we're talking about humans and other animals,", "tokens": [50828, 7021, 13, 865, 13, 407, 286, 519, 300, 498, 321, 434, 1417, 466, 6255, 293, 661, 4882, 11, 51136], "temperature": 0.0, "avg_logprob": -0.13657722833021632, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0872027799487114}, {"id": 565, "seek": 320728, "start": 3222.7200000000003, "end": 3230.88, "text": " then I think that's, that's broadly right. So that's so, so yeah, so we acquire these foundational", "tokens": [51136, 550, 286, 519, 300, 311, 11, 300, 311, 19511, 558, 13, 407, 300, 311, 370, 11, 370, 1338, 11, 370, 321, 20001, 613, 32195, 51544], "temperature": 0.0, "avg_logprob": -0.13657722833021632, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0872027799487114}, {"id": 566, "seek": 320728, "start": 3230.88, "end": 3235.92, "text": " concepts through interaction with this, this world. And then the repertoire of foundational", "tokens": [51544, 10392, 807, 9285, 365, 341, 11, 341, 1002, 13, 400, 550, 264, 49604, 295, 32195, 51796], "temperature": 0.0, "avg_logprob": -0.13657722833021632, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.0872027799487114}, {"id": 567, "seek": 323592, "start": 3235.92, "end": 3242.0, "text": " common sense concepts that we can acquire that way is extraordinarily productive, because, you", "tokens": [50364, 2689, 2020, 10392, 300, 321, 393, 20001, 300, 636, 307, 34557, 13304, 11, 570, 11, 291, 50668], "temperature": 0.0, "avg_logprob": -0.11832740878270677, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.00911119394004345}, {"id": 568, "seek": 323592, "start": 3242.0, "end": 3247.92, "text": " know, we are able to conceptualize so many things in terms of these, these basic ideas. I'm very,", "tokens": [50668, 458, 11, 321, 366, 1075, 281, 24106, 1125, 370, 867, 721, 294, 2115, 295, 613, 11, 613, 3875, 3487, 13, 286, 478, 588, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11832740878270677, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.00911119394004345}, {"id": 569, "seek": 323592, "start": 3247.92, "end": 3253.6800000000003, "text": " very, I'm a very big fan of the work of George Lakoff, you know, absolutely classic book metaphors", "tokens": [50964, 588, 11, 286, 478, 257, 588, 955, 3429, 295, 264, 589, 295, 7136, 37327, 4506, 11, 291, 458, 11, 3122, 7230, 1446, 30946, 830, 51252], "temperature": 0.0, "avg_logprob": -0.11832740878270677, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.00911119394004345}, {"id": 570, "seek": 323592, "start": 3253.6800000000003, "end": 3260.08, "text": " we live back in the 1980s. Yeah. And I really think there was something deeply, deeply right about", "tokens": [51252, 321, 1621, 646, 294, 264, 13626, 82, 13, 865, 13, 400, 286, 534, 519, 456, 390, 746, 8760, 11, 8760, 558, 466, 51572], "temperature": 0.0, "avg_logprob": -0.11832740878270677, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.00911119394004345}, {"id": 571, "seek": 323592, "start": 3260.08, "end": 3265.44, "text": " his intuitions in that book. And I still think that they're right. So in the case of humans,", "tokens": [51572, 702, 16224, 626, 294, 300, 1446, 13, 400, 286, 920, 519, 300, 436, 434, 558, 13, 407, 294, 264, 1389, 295, 6255, 11, 51840], "temperature": 0.0, "avg_logprob": -0.11832740878270677, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.00911119394004345}, {"id": 572, "seek": 326544, "start": 3265.44, "end": 3270.32, "text": " right? So, so we through our embodied interaction with the everyday world, we acquire this layer of", "tokens": [50364, 558, 30, 407, 11, 370, 321, 807, 527, 42046, 9285, 365, 264, 7429, 1002, 11, 321, 20001, 341, 4583, 295, 50608], "temperature": 0.0, "avg_logprob": -0.11190242942320097, "compression_ratio": 1.7614035087719297, "no_speech_prob": 0.0006986541557125747}, {"id": 573, "seek": 326544, "start": 3270.32, "end": 3275.44, "text": " foundational common sense that includes things like surfaces and containers and paths and all that kind", "tokens": [50608, 32195, 2689, 2020, 300, 5974, 721, 411, 16130, 293, 17089, 293, 14518, 293, 439, 300, 733, 50864], "temperature": 0.0, "avg_logprob": -0.11190242942320097, "compression_ratio": 1.7614035087719297, "no_speech_prob": 0.0006986541557125747}, {"id": 574, "seek": 326544, "start": 3275.44, "end": 3282.7200000000003, "text": " of stuff and collisions and things. And then we at the most abstract level. So, you know, we apply", "tokens": [50864, 295, 1507, 293, 46537, 293, 721, 13, 400, 550, 321, 412, 264, 881, 12649, 1496, 13, 407, 11, 291, 458, 11, 321, 3079, 51228], "temperature": 0.0, "avg_logprob": -0.11190242942320097, "compression_ratio": 1.7614035087719297, "no_speech_prob": 0.0006986541557125747}, {"id": 575, "seek": 326544, "start": 3282.7200000000003, "end": 3288.08, "text": " that same repertoire of basic concepts to understand things like say large language models. If you", "tokens": [51228, 300, 912, 49604, 295, 3875, 10392, 281, 1223, 721, 411, 584, 2416, 2856, 5245, 13, 759, 291, 51496], "temperature": 0.0, "avg_logprob": -0.11190242942320097, "compression_ratio": 1.7614035087719297, "no_speech_prob": 0.0006986541557125747}, {"id": 576, "seek": 326544, "start": 3288.08, "end": 3292.8, "text": " look at the language that's used in a paper about large language, you know, people are talking about", "tokens": [51496, 574, 412, 264, 2856, 300, 311, 1143, 294, 257, 3035, 466, 2416, 2856, 11, 291, 458, 11, 561, 366, 1417, 466, 51732], "temperature": 0.0, "avg_logprob": -0.11190242942320097, "compression_ratio": 1.7614035087719297, "no_speech_prob": 0.0006986541557125747}, {"id": 577, "seek": 329280, "start": 3292.8, "end": 3297.36, "text": " layers, they're talking about connections, they're talking about, you know, I mean, these things are", "tokens": [50364, 7914, 11, 436, 434, 1417, 466, 9271, 11, 436, 434, 1417, 466, 11, 291, 458, 11, 286, 914, 11, 613, 721, 366, 50592], "temperature": 0.0, "avg_logprob": -0.11978581237792969, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.002024190966039896}, {"id": 578, "seek": 329280, "start": 3297.36, "end": 3303.76, "text": " all, they're all grounded in very physical concepts, you know? Yes. I mean, I'm a huge fan of George", "tokens": [50592, 439, 11, 436, 434, 439, 23535, 294, 588, 4001, 10392, 11, 291, 458, 30, 1079, 13, 286, 914, 11, 286, 478, 257, 2603, 3429, 295, 7136, 50912], "temperature": 0.0, "avg_logprob": -0.11978581237792969, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.002024190966039896}, {"id": 579, "seek": 329280, "start": 3303.76, "end": 3308.1600000000003, "text": " Lakoff. And of course, you know, he spoke about the war metaphors and you know, it's been a long", "tokens": [50912, 37327, 4506, 13, 400, 295, 1164, 11, 291, 458, 11, 415, 7179, 466, 264, 1516, 30946, 830, 293, 291, 458, 11, 309, 311, 668, 257, 938, 51132], "temperature": 0.0, "avg_logprob": -0.11978581237792969, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.002024190966039896}, {"id": 580, "seek": 329280, "start": 3308.1600000000003, "end": 3314.5600000000004, "text": " road and stuff like that. Yeah, the journey and yeah. It's beautiful. But then a lot of knowledge", "tokens": [51132, 3060, 293, 1507, 411, 300, 13, 865, 11, 264, 4671, 293, 1338, 13, 467, 311, 2238, 13, 583, 550, 257, 688, 295, 3601, 51452], "temperature": 0.0, "avg_logprob": -0.11978581237792969, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.002024190966039896}, {"id": 581, "seek": 329280, "start": 3314.5600000000004, "end": 3319.6000000000004, "text": " that language models learn are kind of cultural knowledge. So we share these simulation pointers", "tokens": [51452, 300, 2856, 5245, 1466, 366, 733, 295, 6988, 3601, 13, 407, 321, 2073, 613, 16575, 44548, 51704], "temperature": 0.0, "avg_logprob": -0.11978581237792969, "compression_ratio": 1.7733812949640289, "no_speech_prob": 0.002024190966039896}, {"id": 582, "seek": 331960, "start": 3319.6, "end": 3325.2799999999997, "text": " and it's quite relativistic. But I'm also really interested in, because some rationalists argue", "tokens": [50364, 293, 309, 311, 1596, 21960, 3142, 13, 583, 286, 478, 611, 534, 3102, 294, 11, 570, 512, 15090, 1751, 9695, 50648], "temperature": 0.0, "avg_logprob": -0.07585199302602037, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.0094353798776865}, {"id": 583, "seek": 331960, "start": 3325.2799999999997, "end": 3333.52, "text": " that it's not possible to go from empirical experience and universal knowledge. And there is", "tokens": [50648, 300, 309, 311, 406, 1944, 281, 352, 490, 31886, 1752, 293, 11455, 3601, 13, 400, 456, 307, 51060], "temperature": 0.0, "avg_logprob": -0.07585199302602037, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.0094353798776865}, {"id": 584, "seek": 331960, "start": 3333.52, "end": 3338.08, "text": " a split between natural knowledge and cultural knowledge. And I think you and I would agree that", "tokens": [51060, 257, 7472, 1296, 3303, 3601, 293, 6988, 3601, 13, 400, 286, 519, 291, 293, 286, 576, 3986, 300, 51288], "temperature": 0.0, "avg_logprob": -0.07585199302602037, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.0094353798776865}, {"id": 585, "seek": 331960, "start": 3338.08, "end": 3342.88, "text": " a lot of natural knowledge like, you know, transitivity contains in and so on, this kind of", "tokens": [51288, 257, 688, 295, 3303, 3601, 411, 11, 291, 458, 11, 17976, 4253, 8306, 294, 293, 370, 322, 11, 341, 733, 295, 51528], "temperature": 0.0, "avg_logprob": -0.07585199302602037, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.0094353798776865}, {"id": 586, "seek": 331960, "start": 3342.88, "end": 3349.04, "text": " rationality is just missing at the moment. But as an embodied scientist, you believe that we", "tokens": [51528, 15090, 507, 307, 445, 5361, 412, 264, 1623, 13, 583, 382, 364, 42046, 12662, 11, 291, 1697, 300, 321, 51836], "temperature": 0.0, "avg_logprob": -0.07585199302602037, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.0094353798776865}, {"id": 587, "seek": 334904, "start": 3349.04, "end": 3354.56, "text": " learn them by being embodied in the physical world? Well, I mean, I, you know, it may well be", "tokens": [50364, 1466, 552, 538, 885, 42046, 294, 264, 4001, 1002, 30, 1042, 11, 286, 914, 11, 286, 11, 291, 458, 11, 309, 815, 731, 312, 50640], "temperature": 0.0, "avg_logprob": -0.09960318584831393, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.004286753013730049}, {"id": 588, "seek": 334904, "start": 3354.56, "end": 3362.8, "text": " that there are certain, so you know, we need to distinguish, you know, empirical questions about", "tokens": [50640, 300, 456, 366, 1629, 11, 370, 291, 458, 11, 321, 643, 281, 20206, 11, 291, 458, 11, 31886, 1651, 466, 51052], "temperature": 0.0, "avg_logprob": -0.09960318584831393, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.004286753013730049}, {"id": 589, "seek": 334904, "start": 3362.8, "end": 3368.64, "text": " humans and human cognitive makeup and that of other animals and so on. And AI and what we could", "tokens": [51052, 6255, 293, 1952, 15605, 6567, 293, 300, 295, 661, 4882, 293, 370, 322, 13, 400, 7318, 293, 437, 321, 727, 51344], "temperature": 0.0, "avg_logprob": -0.09960318584831393, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.004286753013730049}, {"id": 590, "seek": 334904, "start": 3368.64, "end": 3374.16, "text": " build in AI, because of course, it may be the case that human cognition, you know, has arisen in", "tokens": [51344, 1322, 294, 7318, 11, 570, 295, 1164, 11, 309, 815, 312, 264, 1389, 300, 1952, 46905, 11, 291, 458, 11, 575, 594, 11106, 294, 51620], "temperature": 0.0, "avg_logprob": -0.09960318584831393, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.004286753013730049}, {"id": 591, "seek": 337416, "start": 3374.16, "end": 3381.04, "text": " certain ways. And then it's an empirical question, you know, what, you know, of how human cognition", "tokens": [50364, 1629, 2098, 13, 400, 550, 309, 311, 364, 31886, 1168, 11, 291, 458, 11, 437, 11, 291, 458, 11, 295, 577, 1952, 46905, 50708], "temperature": 0.0, "avg_logprob": -0.12846993630932224, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.012151284143328667}, {"id": 592, "seek": 337416, "start": 3381.04, "end": 3386.96, "text": " works. And it may, we may have answers there that are different, that, you know, that we can break,", "tokens": [50708, 1985, 13, 400, 309, 815, 11, 321, 815, 362, 6338, 456, 300, 366, 819, 11, 300, 11, 291, 458, 11, 300, 321, 393, 1821, 11, 51004], "temperature": 0.0, "avg_logprob": -0.12846993630932224, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.012151284143328667}, {"id": 593, "seek": 337416, "start": 3386.96, "end": 3390.7999999999997, "text": " as it were, when we built things in an artificial way. So in so in the case of something like", "tokens": [51004, 382, 309, 645, 11, 562, 321, 3094, 721, 294, 364, 11677, 636, 13, 407, 294, 370, 294, 264, 1389, 295, 746, 411, 51196], "temperature": 0.0, "avg_logprob": -0.12846993630932224, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.012151284143328667}, {"id": 594, "seek": 337416, "start": 3390.7999999999997, "end": 3396.08, "text": " transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about,", "tokens": [51196, 17976, 4253, 11, 550, 11, 291, 458, 11, 286, 914, 11, 2745, 11, 341, 307, 257, 7230, 6770, 294, 10675, 466, 11, 51460], "temperature": 0.0, "avg_logprob": -0.12846993630932224, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.012151284143328667}, {"id": 595, "seek": 337416, "start": 3396.8799999999997, "end": 3401.3599999999997, "text": " about, you know, between the rationalists and the idealists getting back to, you know,", "tokens": [51500, 466, 11, 291, 458, 11, 1296, 264, 15090, 1751, 293, 264, 7157, 1751, 1242, 646, 281, 11, 291, 458, 11, 51724], "temperature": 0.0, "avg_logprob": -0.12846993630932224, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.012151284143328667}, {"id": 596, "seek": 340136, "start": 3401.44, "end": 3408.4, "text": " the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by", "tokens": [50368, 264, 3282, 392, 4901, 11, 3282, 392, 293, 2443, 392, 4901, 13, 400, 11, 293, 11, 293, 11, 291, 458, 11, 40927, 20581, 20772, 341, 538, 50716], "temperature": 0.0, "avg_logprob": -0.10809934027841157, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.010546213015913963}, {"id": 597, "seek": 340136, "start": 3408.4, "end": 3414.2400000000002, "text": " kind of reconciling these two sort of opposites. And, and so the Kantian argument would be that", "tokens": [50716, 733, 295, 9993, 3208, 278, 613, 732, 1333, 295, 4665, 3324, 13, 400, 11, 293, 370, 264, 40927, 952, 6770, 576, 312, 300, 51008], "temperature": 0.0, "avg_logprob": -0.10809934027841157, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.010546213015913963}, {"id": 598, "seek": 340136, "start": 3414.2400000000002, "end": 3420.4, "text": " there's a certain amount of innate structure that has to be there in the mind to, to, to understand,", "tokens": [51008, 456, 311, 257, 1629, 2372, 295, 41766, 3877, 300, 575, 281, 312, 456, 294, 264, 1575, 281, 11, 281, 11, 281, 1223, 11, 51316], "temperature": 0.0, "avg_logprob": -0.10809934027841157, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.010546213015913963}, {"id": 599, "seek": 340136, "start": 3420.4, "end": 3426.48, "text": " you know, the world at all, right. And so maybe, and now, when we think about that empirically,", "tokens": [51316, 291, 458, 11, 264, 1002, 412, 439, 11, 558, 13, 400, 370, 1310, 11, 293, 586, 11, 562, 321, 519, 466, 300, 25790, 984, 11, 51620], "temperature": 0.0, "avg_logprob": -0.10809934027841157, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.010546213015913963}, {"id": 600, "seek": 342648, "start": 3426.48, "end": 3433.36, "text": " then, then I guess that we may find that evolution has endowed us with certain basic kind of templates", "tokens": [50364, 550, 11, 550, 286, 2041, 300, 321, 815, 915, 300, 9303, 575, 917, 24347, 505, 365, 1629, 3875, 733, 295, 21165, 50708], "temperature": 0.0, "avg_logprob": -0.14365175035264757, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002016799757257104}, {"id": 601, "seek": 342648, "start": 3435.6, "end": 3440.88, "text": " for understanding the world. And maybe it's things like transitivity is something that's,", "tokens": [50820, 337, 3701, 264, 1002, 13, 400, 1310, 309, 311, 721, 411, 17976, 4253, 307, 746, 300, 311, 11, 51084], "temperature": 0.0, "avg_logprob": -0.14365175035264757, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002016799757257104}, {"id": 602, "seek": 342648, "start": 3440.88, "end": 3446.32, "text": " that's there in the same machinery that supports language, you know. So maybe, maybe, I mean,", "tokens": [51084, 300, 311, 456, 294, 264, 912, 27302, 300, 9346, 2856, 11, 291, 458, 13, 407, 1310, 11, 1310, 11, 286, 914, 11, 51356], "temperature": 0.0, "avg_logprob": -0.14365175035264757, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002016799757257104}, {"id": 603, "seek": 342648, "start": 3446.32, "end": 3450.4, "text": " there's all empirical questions. And I don't know what the latest research on all these things", "tokens": [51356, 456, 311, 439, 31886, 1651, 13, 400, 286, 500, 380, 458, 437, 264, 6792, 2132, 322, 439, 613, 721, 51560], "temperature": 0.0, "avg_logprob": -0.14365175035264757, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.002016799757257104}, {"id": 604, "seek": 345040, "start": 3451.28, "end": 3457.6800000000003, "text": " is, but, but, but it, yeah, it does seem to me that that's a reasonable position to take.", "tokens": [50408, 307, 11, 457, 11, 457, 11, 457, 309, 11, 1338, 11, 309, 775, 1643, 281, 385, 300, 300, 311, 257, 10585, 2535, 281, 747, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1971996125720796, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.007730111479759216}, {"id": 605, "seek": 345040, "start": 3458.32, "end": 3464.2400000000002, "text": " Yeah, but even evolution is a form of empirical process. So there's always the question of,", "tokens": [50760, 865, 11, 457, 754, 9303, 307, 257, 1254, 295, 31886, 1399, 13, 407, 456, 311, 1009, 264, 1168, 295, 11, 51056], "temperature": 0.0, "avg_logprob": -0.1971996125720796, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.007730111479759216}, {"id": 606, "seek": 345040, "start": 3464.2400000000002, "end": 3471.2000000000003, "text": " of where does it get there? And, and that was, yeah, Kant was a transcendental idealist, wasn't", "tokens": [51056, 295, 689, 775, 309, 483, 456, 30, 400, 11, 293, 300, 390, 11, 1338, 11, 40927, 390, 257, 28535, 14533, 7157, 468, 11, 2067, 380, 51404], "temperature": 0.0, "avg_logprob": -0.1971996125720796, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.007730111479759216}, {"id": 607, "seek": 345040, "start": 3471.2000000000003, "end": 3476.0, "text": " he? But this brings me to our friend Francois Chollet and the ARC challenge. So, you know,", "tokens": [51404, 415, 30, 583, 341, 5607, 385, 281, 527, 1277, 34695, 271, 761, 1833, 302, 293, 264, 8943, 34, 3430, 13, 407, 11, 291, 458, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1971996125720796, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.007730111479759216}, {"id": 608, "seek": 347600, "start": 3476.08, "end": 3481.12, "text": " there's another great school of thought, which is that intelligent, I mean, he argues that", "tokens": [50368, 456, 311, 1071, 869, 1395, 295, 1194, 11, 597, 307, 300, 13232, 11, 286, 914, 11, 415, 38218, 300, 50620], "temperature": 0.0, "avg_logprob": -0.11669455936976841, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.01632234826683998}, {"id": 609, "seek": 347600, "start": 3481.12, "end": 3488.32, "text": " intelligence is about these meta learning priors, the conversion ratio between universal or sometimes", "tokens": [50620, 7599, 307, 466, 613, 19616, 2539, 1790, 830, 11, 264, 14298, 8509, 1296, 11455, 420, 2171, 50980], "temperature": 0.0, "avg_logprob": -0.11669455936976841, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.01632234826683998}, {"id": 610, "seek": 347600, "start": 3488.32, "end": 3493.84, "text": " anthropomorphic knowledge that we have, and being able to develop a skill program very quickly that", "tokens": [50980, 22727, 32702, 299, 3601, 300, 321, 362, 11, 293, 885, 1075, 281, 1499, 257, 5389, 1461, 588, 2661, 300, 51256], "temperature": 0.0, "avg_logprob": -0.11669455936976841, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.01632234826683998}, {"id": 611, "seek": 347600, "start": 3493.84, "end": 3501.04, "text": " generalizes very well. So, so the ARC challenge is almost about how do we codify these priors,", "tokens": [51256, 2674, 5660, 588, 731, 13, 407, 11, 370, 264, 8943, 34, 3430, 307, 1920, 466, 577, 360, 321, 17656, 2505, 613, 1790, 830, 11, 51616], "temperature": 0.0, "avg_logprob": -0.11669455936976841, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.01632234826683998}, {"id": 612, "seek": 347600, "start": 3501.04, "end": 3505.44, "text": " and how do we efficiently build skill programs by combining these priors together.", "tokens": [51616, 293, 577, 360, 321, 19621, 1322, 5389, 4268, 538, 21928, 613, 1790, 830, 1214, 13, 51836], "temperature": 0.0, "avg_logprob": -0.11669455936976841, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.01632234826683998}, {"id": 613, "seek": 350544, "start": 3505.84, "end": 3509.84, "text": " And that seems quite divorced at the moment from the kind of AI we're building.", "tokens": [50384, 400, 300, 2544, 1596, 27670, 412, 264, 1623, 490, 264, 733, 295, 7318, 321, 434, 2390, 13, 50584], "temperature": 0.0, "avg_logprob": -0.09677539305253462, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0035933679901063442}, {"id": 614, "seek": 350544, "start": 3510.4, "end": 3517.04, "text": " I think, I think that's right. It is, you know, unless in the AI that we're building today in", "tokens": [50612, 286, 519, 11, 286, 519, 300, 311, 558, 13, 467, 307, 11, 291, 458, 11, 5969, 294, 264, 7318, 300, 321, 434, 2390, 965, 294, 50944], "temperature": 0.0, "avg_logprob": -0.09677539305253462, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0035933679901063442}, {"id": 615, "seek": 350544, "start": 3517.04, "end": 3525.28, "text": " generative AI, unless you get these kinds of mechanisms that Francois Chollet is alluding to", "tokens": [50944, 1337, 1166, 7318, 11, 5969, 291, 483, 613, 3685, 295, 15902, 300, 34695, 271, 761, 1833, 302, 307, 439, 33703, 281, 51356], "temperature": 0.0, "avg_logprob": -0.09677539305253462, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0035933679901063442}, {"id": 616, "seek": 350544, "start": 3525.28, "end": 3529.04, "text": " through the magic of emergence and scale, which of course, people are always, you know,", "tokens": [51356, 807, 264, 5585, 295, 36211, 293, 4373, 11, 597, 295, 1164, 11, 561, 366, 1009, 11, 291, 458, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09677539305253462, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0035933679901063442}, {"id": 617, "seek": 350544, "start": 3529.76, "end": 3534.16, "text": " suggesting that maybe that's possible, you know, any kind of mechanism can emerge", "tokens": [51580, 18094, 300, 1310, 300, 311, 1944, 11, 291, 458, 11, 604, 733, 295, 7513, 393, 21511, 51800], "temperature": 0.0, "avg_logprob": -0.09677539305253462, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0035933679901063442}, {"id": 618, "seek": 353416, "start": 3534.16, "end": 3541.52, "text": " right through scale in theory. And we've been surprised, in fact, by how powerful the mechanisms,", "tokens": [50364, 558, 807, 4373, 294, 5261, 13, 400, 321, 600, 668, 6100, 11, 294, 1186, 11, 538, 577, 4005, 264, 15902, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1141930976800159, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.012765562161803246}, {"id": 619, "seek": 353416, "start": 3542.3199999999997, "end": 3547.7599999999998, "text": " emergent mechanisms that have, you know, developed through learning at scale, just,", "tokens": [50772, 4345, 6930, 15902, 300, 362, 11, 291, 458, 11, 4743, 807, 2539, 412, 4373, 11, 445, 11, 51044], "temperature": 0.0, "avg_logprob": -0.1141930976800159, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.012765562161803246}, {"id": 620, "seek": 353416, "start": 3548.3999999999996, "end": 3551.8399999999997, "text": " you know, with a next token prediction objective, that has been very surprising.", "tokens": [51076, 291, 458, 11, 365, 257, 958, 14862, 17630, 10024, 11, 300, 575, 668, 588, 8830, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1141930976800159, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.012765562161803246}, {"id": 621, "seek": 353416, "start": 3551.8399999999997, "end": 3556.48, "text": " But however, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about", "tokens": [51248, 583, 4461, 11, 291, 458, 11, 286, 478, 11, 382, 286, 519, 34695, 271, 761, 1833, 302, 576, 312, 11, 286, 478, 257, 857, 28601, 466, 51480], "temperature": 0.0, "avg_logprob": -0.1141930976800159, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.012765562161803246}, {"id": 622, "seek": 353416, "start": 3556.48, "end": 3561.7599999999998, "text": " whether we're really going to get all the way with this kind of the ability to solve this kind of", "tokens": [51480, 1968, 321, 434, 534, 516, 281, 483, 439, 264, 636, 365, 341, 733, 295, 264, 3485, 281, 5039, 341, 733, 295, 51744], "temperature": 0.0, "avg_logprob": -0.1141930976800159, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.012765562161803246}, {"id": 623, "seek": 356176, "start": 3561.76, "end": 3569.1200000000003, "text": " abstract problem that's in the ARC challenge this way. And so I guess, you know, I am,", "tokens": [50364, 12649, 1154, 300, 311, 294, 264, 8943, 34, 3430, 341, 636, 13, 400, 370, 286, 2041, 11, 291, 458, 11, 286, 669, 11, 50732], "temperature": 0.0, "avg_logprob": -0.11933918182666485, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.007061803713440895}, {"id": 624, "seek": 356176, "start": 3569.76, "end": 3574.88, "text": " I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially", "tokens": [50764, 286, 6222, 11, 291, 458, 11, 286, 914, 11, 286, 478, 1269, 12, 23310, 13, 2102, 3255, 11, 558, 30, 286, 914, 11, 567, 3255, 13, 400, 2318, 51020], "temperature": 0.0, "avg_logprob": -0.11933918182666485, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.007061803713440895}, {"id": 625, "seek": 356176, "start": 3574.88, "end": 3581.84, "text": " if you make things multimodal and so on, and you expand your generative models into a setting", "tokens": [51020, 498, 291, 652, 721, 32972, 378, 304, 293, 370, 322, 11, 293, 291, 5268, 428, 1337, 1166, 5245, 666, 257, 3287, 51368], "temperature": 0.0, "avg_logprob": -0.11933918182666485, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.007061803713440895}, {"id": 626, "seek": 356176, "start": 3581.84, "end": 3588.0, "text": " where you've got interaction with the world and so on, you know, who knows. But I suspect that", "tokens": [51368, 689, 291, 600, 658, 9285, 365, 264, 1002, 293, 370, 322, 11, 291, 458, 11, 567, 3255, 13, 583, 286, 9091, 300, 51676], "temperature": 0.0, "avg_logprob": -0.11933918182666485, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.007061803713440895}, {"id": 627, "seek": 358800, "start": 3588.0, "end": 3594.72, "text": " maybe you're not going to get all the way there that way. And so I have a lot of sympathy with", "tokens": [50364, 1310, 291, 434, 406, 516, 281, 483, 439, 264, 636, 456, 300, 636, 13, 400, 370, 286, 362, 257, 688, 295, 33240, 365, 50700], "temperature": 0.0, "avg_logprob": -0.12747360192812407, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.051837529987096786}, {"id": 628, "seek": 358800, "start": 3594.72, "end": 3600.0, "text": " what is probably his intuition, that you need a bit more in the way of innate, something innate", "tokens": [50700, 437, 307, 1391, 702, 24002, 11, 300, 291, 643, 257, 857, 544, 294, 264, 636, 295, 41766, 11, 746, 41766, 50964], "temperature": 0.0, "avg_logprob": -0.12747360192812407, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.051837529987096786}, {"id": 629, "seek": 358800, "start": 3600.0, "end": 3606.88, "text": " there, or, yeah, innate, maybe that's the wrong word. But you need some kind of, you need priors,", "tokens": [50964, 456, 11, 420, 11, 1338, 11, 41766, 11, 1310, 300, 311, 264, 2085, 1349, 13, 583, 291, 643, 512, 733, 295, 11, 291, 643, 1790, 830, 11, 51308], "temperature": 0.0, "avg_logprob": -0.12747360192812407, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.051837529987096786}, {"id": 630, "seek": 358800, "start": 3606.88, "end": 3613.52, "text": " where they come from, I don't know. But the priors that I would appeal to, thinking about the human", "tokens": [51308, 689, 436, 808, 490, 11, 286, 500, 380, 458, 13, 583, 264, 1790, 830, 300, 286, 576, 13668, 281, 11, 1953, 466, 264, 1952, 51640], "temperature": 0.0, "avg_logprob": -0.12747360192812407, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.051837529987096786}, {"id": 631, "seek": 361352, "start": 3613.52, "end": 3618.4, "text": " case, again, are related to this foundational common sense. So they're just the notion of an", "tokens": [50364, 1389, 11, 797, 11, 366, 4077, 281, 341, 32195, 2689, 2020, 13, 407, 436, 434, 445, 264, 10710, 295, 364, 50608], "temperature": 0.0, "avg_logprob": -0.0778702727886809, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.17311722040176392}, {"id": 632, "seek": 361352, "start": 3618.4, "end": 3624.56, "text": " object, right? So if you just, if you have a clear notion of an object and of movement,", "tokens": [50608, 2657, 11, 558, 30, 407, 498, 291, 445, 11, 498, 291, 362, 257, 1850, 10710, 295, 364, 2657, 293, 295, 3963, 11, 50916], "temperature": 0.0, "avg_logprob": -0.0778702727886809, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.17311722040176392}, {"id": 633, "seek": 361352, "start": 3624.56, "end": 3629.2, "text": " objects and movements and object persistence, then they straight away are going to help you with an", "tokens": [50916, 6565, 293, 9981, 293, 2657, 37617, 11, 550, 436, 2997, 1314, 366, 516, 281, 854, 291, 365, 364, 51148], "temperature": 0.0, "avg_logprob": -0.0778702727886809, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.17311722040176392}, {"id": 634, "seek": 361352, "start": 3629.2, "end": 3634.8, "text": " awful lot of those ARC challenge problems. Because many of them, you know, if you explain, you know,", "tokens": [51148, 11232, 688, 295, 729, 8943, 34, 3430, 2740, 13, 1436, 867, 295, 552, 11, 291, 458, 11, 498, 291, 2903, 11, 291, 458, 11, 51428], "temperature": 0.0, "avg_logprob": -0.0778702727886809, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.17311722040176392}, {"id": 635, "seek": 361352, "start": 3634.8, "end": 3640.0, "text": " you figure out how you figure one out, and then you explain what's going on, then, you know,", "tokens": [51428, 291, 2573, 484, 577, 291, 2573, 472, 484, 11, 293, 550, 291, 2903, 437, 311, 516, 322, 11, 550, 11, 291, 458, 11, 51688], "temperature": 0.0, "avg_logprob": -0.0778702727886809, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.17311722040176392}, {"id": 636, "seek": 364000, "start": 3640.0, "end": 3644.8, "text": " you see that it's, oh, you have to think of these collection of pixels as an object that you move", "tokens": [50364, 291, 536, 300, 309, 311, 11, 1954, 11, 291, 362, 281, 519, 295, 613, 5765, 295, 18668, 382, 364, 2657, 300, 291, 1286, 50604], "temperature": 0.0, "avg_logprob": -0.12287081659367655, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.0031602352391928434}, {"id": 637, "seek": 364000, "start": 3644.8, "end": 3649.12, "text": " somewhere else, according to certain rules or something like that. So my colleague, Richard", "tokens": [50604, 4079, 1646, 11, 4650, 281, 1629, 4474, 420, 746, 411, 300, 13, 407, 452, 13532, 11, 9809, 50820], "temperature": 0.0, "avg_logprob": -0.12287081659367655, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.0031602352391928434}, {"id": 638, "seek": 364000, "start": 3649.12, "end": 3658.0, "text": " Evans, had a very good paper on, which was tackling these kinds of things using sort of abduction like", "tokens": [50820, 30055, 11, 632, 257, 588, 665, 3035, 322, 11, 597, 390, 34415, 613, 3685, 295, 721, 1228, 1333, 295, 410, 40335, 411, 51264], "temperature": 0.0, "avg_logprob": -0.12287081659367655, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.0031602352391928434}, {"id": 639, "seek": 364000, "start": 3659.04, "end": 3663.92, "text": " processes. And so, yeah, so he's thought a lot about this from a much more symbolic AI kind of", "tokens": [51316, 7555, 13, 400, 370, 11, 1338, 11, 370, 415, 311, 1194, 257, 688, 466, 341, 490, 257, 709, 544, 25755, 7318, 733, 295, 51560], "temperature": 0.0, "avg_logprob": -0.12287081659367655, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.0031602352391928434}, {"id": 640, "seek": 364000, "start": 3665.04, "end": 3668.96, "text": " perspective. Yeah, that's fascinating. Because even with the ARC challenge,", "tokens": [51616, 4585, 13, 865, 11, 300, 311, 10343, 13, 1436, 754, 365, 264, 8943, 34, 3430, 11, 51812], "temperature": 0.0, "avg_logprob": -0.12287081659367655, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.0031602352391928434}, {"id": 641, "seek": 366896, "start": 3668.96, "end": 3674.96, "text": " the kinds of solutions that people came up with, let's say it's a DSL over, you know, some domain", "tokens": [50364, 264, 3685, 295, 6547, 300, 561, 1361, 493, 365, 11, 718, 311, 584, 309, 311, 257, 15816, 43, 670, 11, 291, 458, 11, 512, 9274, 50664], "temperature": 0.0, "avg_logprob": -0.08130197371205976, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0025410044472664595}, {"id": 642, "seek": 366896, "start": 3674.96, "end": 3679.6, "text": " specific set of primitives. And the ARC challenge is a 2D grid, where you have different colored", "tokens": [50664, 2685, 992, 295, 2886, 38970, 13, 400, 264, 8943, 34, 3430, 307, 257, 568, 35, 10748, 11, 689, 291, 362, 819, 14332, 50896], "temperature": 0.0, "avg_logprob": -0.08130197371205976, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0025410044472664595}, {"id": 643, "seek": 366896, "start": 3679.6, "end": 3685.28, "text": " cells. And the types of priors that work well are things like denoising and reflections and various", "tokens": [50896, 5438, 13, 400, 264, 3467, 295, 1790, 830, 300, 589, 731, 366, 721, 411, 1441, 78, 3436, 293, 30679, 293, 3683, 51180], "temperature": 0.0, "avg_logprob": -0.08130197371205976, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0025410044472664595}, {"id": 644, "seek": 366896, "start": 3685.28, "end": 3691.04, "text": " types of symmetry and so on. And that's great and everything, but it's very domain specific.", "tokens": [51180, 3467, 295, 25440, 293, 370, 322, 13, 400, 300, 311, 869, 293, 1203, 11, 457, 309, 311, 588, 9274, 2685, 13, 51468], "temperature": 0.0, "avg_logprob": -0.08130197371205976, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0025410044472664595}, {"id": 645, "seek": 366896, "start": 3691.04, "end": 3696.88, "text": " And the elixir, you know, what we really want are these universal priors. We certainly have human", "tokens": [51468, 400, 264, 806, 970, 347, 11, 291, 458, 11, 437, 321, 534, 528, 366, 613, 11455, 1790, 830, 13, 492, 3297, 362, 1952, 51760], "temperature": 0.0, "avg_logprob": -0.08130197371205976, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0025410044472664595}, {"id": 646, "seek": 369688, "start": 3696.88, "end": 3701.12, "text": " priors, as Elizabeth Spelke points out, like, you know, and the concept of an agent and the", "tokens": [50364, 1790, 830, 11, 382, 12978, 1738, 338, 330, 2793, 484, 11, 411, 11, 291, 458, 11, 293, 264, 3410, 295, 364, 9461, 293, 264, 50576], "temperature": 0.0, "avg_logprob": -0.13556957244873047, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034873955883085728}, {"id": 647, "seek": 369688, "start": 3701.12, "end": 3706.48, "text": " concept of spatial reasoning. An object, a persistent object. Yes. So yeah, absolutely.", "tokens": [50576, 3410, 295, 23598, 21577, 13, 1107, 2657, 11, 257, 24315, 2657, 13, 1079, 13, 407, 1338, 11, 3122, 13, 50844], "temperature": 0.0, "avg_logprob": -0.13556957244873047, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034873955883085728}, {"id": 648, "seek": 369688, "start": 3706.48, "end": 3712.56, "text": " So I mean, but I think an interesting question is, does it really make sense to talk about", "tokens": [50844, 407, 286, 914, 11, 457, 286, 519, 364, 1880, 1168, 307, 11, 775, 309, 534, 652, 2020, 281, 751, 466, 51148], "temperature": 0.0, "avg_logprob": -0.13556957244873047, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034873955883085728}, {"id": 649, "seek": 369688, "start": 3712.56, "end": 3720.48, "text": " universal priors there? Because, you know, those ARC challenges, problems, whenever you kind of", "tokens": [51148, 11455, 1790, 830, 456, 30, 1436, 11, 291, 458, 11, 729, 8943, 34, 4759, 11, 2740, 11, 5699, 291, 733, 295, 51544], "temperature": 0.0, "avg_logprob": -0.13556957244873047, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034873955883085728}, {"id": 650, "seek": 372048, "start": 3720.48, "end": 3728.88, "text": " figure one out, then typically you are actually bringing to bear a pretty human set of priors", "tokens": [50364, 2573, 472, 484, 11, 550, 5850, 291, 366, 767, 5062, 281, 6155, 257, 1238, 1952, 992, 295, 1790, 830, 50784], "temperature": 0.0, "avg_logprob": -0.10555273956722683, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.30897456407546997}, {"id": 651, "seek": 372048, "start": 3728.88, "end": 3736.96, "text": " and common sense, you know, our concepts. And, you know, and it may be that you could imagine", "tokens": [50784, 293, 2689, 2020, 11, 291, 458, 11, 527, 10392, 13, 400, 11, 291, 458, 11, 293, 309, 815, 312, 300, 291, 727, 3811, 51188], "temperature": 0.0, "avg_logprob": -0.10555273956722683, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.30897456407546997}, {"id": 652, "seek": 372048, "start": 3737.68, "end": 3747.6, "text": " perfectly law-like set of ARC-like problems that have solutions, you know, but appeal to,", "tokens": [51224, 6239, 2101, 12, 4092, 992, 295, 8943, 34, 12, 4092, 2740, 300, 362, 6547, 11, 291, 458, 11, 457, 13668, 281, 11, 51720], "temperature": 0.0, "avg_logprob": -0.10555273956722683, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.30897456407546997}, {"id": 653, "seek": 374760, "start": 3747.6, "end": 3751.8399999999997, "text": " you know, priors that we would struggle to understand, you know. I mean, for example,", "tokens": [50364, 291, 458, 11, 1790, 830, 300, 321, 576, 7799, 281, 1223, 11, 291, 458, 13, 286, 914, 11, 337, 1365, 11, 50576], "temperature": 0.0, "avg_logprob": -0.08800112999091714, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.06084708496928215}, {"id": 654, "seek": 374760, "start": 3752.72, "end": 3757.6, "text": " when we think of something in terms of an object, then we want the pixels to be kind of clumped", "tokens": [50620, 562, 321, 519, 295, 746, 294, 2115, 295, 364, 2657, 11, 550, 321, 528, 264, 18668, 281, 312, 733, 295, 596, 1420, 292, 50864], "temperature": 0.0, "avg_logprob": -0.08800112999091714, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.06084708496928215}, {"id": 655, "seek": 374760, "start": 3757.6, "end": 3763.2, "text": " together, right? And if you sort of randomly distributed the pixels amongst a whole bunch", "tokens": [50864, 1214, 11, 558, 30, 400, 498, 291, 1333, 295, 16979, 12631, 264, 18668, 12918, 257, 1379, 3840, 51144], "temperature": 0.0, "avg_logprob": -0.08800112999091714, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.06084708496928215}, {"id": 656, "seek": 374760, "start": 3763.2, "end": 3766.7999999999997, "text": " of other pixels and you move them around in a systematic way, well, we might be able to kind", "tokens": [51144, 295, 661, 18668, 293, 291, 1286, 552, 926, 294, 257, 27249, 636, 11, 731, 11, 321, 1062, 312, 1075, 281, 733, 51324], "temperature": 0.0, "avg_logprob": -0.08800112999091714, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.06084708496928215}, {"id": 657, "seek": 374760, "start": 3766.7999999999997, "end": 3772.08, "text": " of pick out the gestalt there, but we might not. And that would be because we're not able to see", "tokens": [51324, 295, 1888, 484, 264, 7219, 3198, 456, 11, 457, 321, 1062, 406, 13, 400, 300, 576, 312, 570, 321, 434, 406, 1075, 281, 536, 51588], "temperature": 0.0, "avg_logprob": -0.08800112999091714, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.06084708496928215}, {"id": 658, "seek": 377208, "start": 3772.08, "end": 3777.2, "text": " it as an object because we have human priors, right? So, you know, I think that probably all", "tokens": [50364, 309, 382, 364, 2657, 570, 321, 362, 1952, 1790, 830, 11, 558, 30, 407, 11, 291, 458, 11, 286, 519, 300, 1391, 439, 50620], "temperature": 0.0, "avg_logprob": -0.09637682778494698, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.019323190674185753}, {"id": 659, "seek": 377208, "start": 3777.2, "end": 3782.88, "text": " the problems that he's designed because we don't know what the hidden held-out set is, but I imagine", "tokens": [50620, 264, 2740, 300, 415, 311, 4761, 570, 321, 500, 380, 458, 437, 264, 7633, 5167, 12, 346, 992, 307, 11, 457, 286, 3811, 50904], "temperature": 0.0, "avg_logprob": -0.09637682778494698, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.019323190674185753}, {"id": 660, "seek": 377208, "start": 3782.88, "end": 3790.24, "text": " that they pretty much all use, you know, sort of human comprehensible priors and appeal to,", "tokens": [50904, 300, 436, 1238, 709, 439, 764, 11, 291, 458, 11, 1333, 295, 1952, 10753, 30633, 1790, 830, 293, 13668, 281, 11, 51272], "temperature": 0.0, "avg_logprob": -0.09637682778494698, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.019323190674185753}, {"id": 661, "seek": 377208, "start": 3790.24, "end": 3796.08, "text": " you know, foundational common sense of the sort I alluded to. Yes. I'm sure there must be some", "tokens": [51272, 291, 458, 11, 32195, 2689, 2020, 295, 264, 1333, 286, 33919, 281, 13, 1079, 13, 286, 478, 988, 456, 1633, 312, 512, 51564], "temperature": 0.0, "avg_logprob": -0.09637682778494698, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.019323190674185753}, {"id": 662, "seek": 377208, "start": 3796.08, "end": 3801.6, "text": " kind of universal priors because in quantum field theory, physicists use things like locality and", "tokens": [51564, 733, 295, 11455, 1790, 830, 570, 294, 13018, 2519, 5261, 11, 48716, 764, 721, 411, 1628, 1860, 293, 51840], "temperature": 0.0, "avg_logprob": -0.09637682778494698, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.019323190674185753}, {"id": 663, "seek": 380160, "start": 3801.6, "end": 3808.0, "text": " sparsity. Yeah, some really, really high level things like objects. But then again, you know,", "tokens": [50364, 637, 685, 507, 13, 865, 11, 512, 534, 11, 534, 1090, 1496, 721, 411, 6565, 13, 583, 550, 797, 11, 291, 458, 11, 50684], "temperature": 0.0, "avg_logprob": -0.14015086797567514, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.0007764528854750097}, {"id": 664, "seek": 380160, "start": 3808.0, "end": 3812.96, "text": " quantum mechanics challenges the very concept of an object even. So what is the difference to you", "tokens": [50684, 13018, 12939, 4759, 264, 588, 3410, 295, 364, 2657, 754, 13, 407, 437, 307, 264, 2649, 281, 291, 50932], "temperature": 0.0, "avg_logprob": -0.14015086797567514, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.0007764528854750097}, {"id": 665, "seek": 380160, "start": 3812.96, "end": 3820.56, "text": " between adopting a stance that a system is as if conscious versus it being a fact of the matter?", "tokens": [50932, 1296, 32328, 257, 21033, 300, 257, 1185, 307, 382, 498, 6648, 5717, 309, 885, 257, 1186, 295, 264, 1871, 30, 51312], "temperature": 0.0, "avg_logprob": -0.14015086797567514, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.0007764528854750097}, {"id": 666, "seek": 380160, "start": 3820.56, "end": 3823.8399999999997, "text": " I'm a bit resistant to the distinction, to the very distinction.", "tokens": [51312, 286, 478, 257, 857, 20383, 281, 264, 16844, 11, 281, 264, 588, 16844, 13, 51476], "temperature": 0.0, "avg_logprob": -0.14015086797567514, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.0007764528854750097}, {"id": 667, "seek": 380160, "start": 3826.7999999999997, "end": 3830.88, "text": " But this is a very difficult position to maintain because we have a very, very strong", "tokens": [51624, 583, 341, 307, 257, 588, 2252, 2535, 281, 6909, 570, 321, 362, 257, 588, 11, 588, 2068, 51828], "temperature": 0.0, "avg_logprob": -0.14015086797567514, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.0007764528854750097}, {"id": 668, "seek": 383088, "start": 3830.88, "end": 3836.96, "text": " intuition that there is a fact of the matter about our own consciousness. And it's very, very", "tokens": [50364, 24002, 300, 456, 307, 257, 1186, 295, 264, 1871, 466, 527, 1065, 10081, 13, 400, 309, 311, 588, 11, 588, 50668], "temperature": 0.0, "avg_logprob": -0.1024459479511648, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0003564869111869484}, {"id": 669, "seek": 383088, "start": 3836.96, "end": 3845.84, "text": " difficult to escape from that very, very basic intuition. But I have a whole approach to these", "tokens": [50668, 2252, 281, 7615, 490, 300, 588, 11, 588, 3875, 24002, 13, 583, 286, 362, 257, 1379, 3109, 281, 613, 51112], "temperature": 0.0, "avg_logprob": -0.1024459479511648, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0003564869111869484}, {"id": 670, "seek": 383088, "start": 3845.84, "end": 3854.8, "text": " kinds of questions. So shall I sort of describe this? So, you know, a really question that really", "tokens": [51112, 3685, 295, 1651, 13, 407, 4393, 286, 1333, 295, 6786, 341, 30, 407, 11, 291, 458, 11, 257, 534, 1168, 300, 534, 51560], "temperature": 0.0, "avg_logprob": -0.1024459479511648, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0003564869111869484}, {"id": 671, "seek": 385480, "start": 3854.8, "end": 3862.96, "text": " motivated me was that was, you know, suppose that we encounter, well, actually, let me not", "tokens": [50364, 14515, 385, 390, 300, 390, 11, 291, 458, 11, 7297, 300, 321, 8593, 11, 731, 11, 767, 11, 718, 385, 406, 50772], "temperature": 0.0, "avg_logprob": -0.11358941396077474, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.10247354209423065}, {"id": 672, "seek": 385480, "start": 3862.96, "end": 3869.2000000000003, "text": " use the word encounter, suppose that we come across has some object, which is a completely alien", "tokens": [50772, 764, 264, 1349, 8593, 11, 7297, 300, 321, 808, 2108, 575, 512, 2657, 11, 597, 307, 257, 2584, 12319, 51084], "temperature": 0.0, "avg_logprob": -0.11358941396077474, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.10247354209423065}, {"id": 673, "seek": 385480, "start": 3869.2000000000003, "end": 3875.92, "text": " artifact. And maybe there's consciousness going on inside this artifact. And the thought is, well,", "tokens": [51084, 34806, 13, 400, 1310, 456, 311, 10081, 516, 322, 1854, 341, 34806, 13, 400, 264, 1194, 307, 11, 731, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11358941396077474, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.10247354209423065}, {"id": 674, "seek": 385480, "start": 3875.92, "end": 3879.6800000000003, "text": " how would we ever know, you know, there could be consciousness, this thing could be conscious,", "tokens": [51420, 577, 576, 321, 1562, 458, 11, 291, 458, 11, 456, 727, 312, 10081, 11, 341, 551, 727, 312, 6648, 11, 51608], "temperature": 0.0, "avg_logprob": -0.11358941396077474, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.10247354209423065}, {"id": 675, "seek": 387968, "start": 3879.68, "end": 3888.3999999999996, "text": " but we might never know. And so suppose that it were a white cube that were deposited in front", "tokens": [50364, 457, 321, 1062, 1128, 458, 13, 400, 370, 7297, 300, 309, 645, 257, 2418, 13728, 300, 645, 42002, 294, 1868, 50800], "temperature": 0.0, "avg_logprob": -0.1112460582814318, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.016175003722310066}, {"id": 676, "seek": 387968, "start": 3888.3999999999996, "end": 3894.56, "text": " of your lab, and you were tasked with a problem of, would it be moral to throw it down a mine", "tokens": [50800, 295, 428, 2715, 11, 293, 291, 645, 38621, 365, 257, 1154, 295, 11, 576, 309, 312, 9723, 281, 3507, 309, 760, 257, 3892, 51108], "temperature": 0.0, "avg_logprob": -0.1112460582814318, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.016175003722310066}, {"id": 677, "seek": 387968, "start": 3894.56, "end": 3902.56, "text": " shaft and forget about it? So my approach to these problems is that I think in order for the", "tokens": [51108, 18467, 293, 2870, 466, 309, 30, 407, 452, 3109, 281, 613, 2740, 307, 300, 286, 519, 294, 1668, 337, 264, 51508], "temperature": 0.0, "avg_logprob": -0.1112460582814318, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.016175003722310066}, {"id": 678, "seek": 387968, "start": 3902.56, "end": 3908.72, "text": " question, in order for us to be able to answer the question of whether something is conscious or not,", "tokens": [51508, 1168, 11, 294, 1668, 337, 505, 281, 312, 1075, 281, 1867, 264, 1168, 295, 1968, 746, 307, 6648, 420, 406, 11, 51816], "temperature": 0.0, "avg_logprob": -0.1112460582814318, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.016175003722310066}, {"id": 679, "seek": 390872, "start": 3908.72, "end": 3915.7599999999998, "text": " for even to be answerable or askable, then we then we need to be able to engineer an encounter", "tokens": [50364, 337, 754, 281, 312, 1867, 712, 420, 1029, 712, 11, 550, 321, 550, 321, 643, 281, 312, 1075, 281, 11403, 364, 8593, 50716], "temperature": 0.0, "avg_logprob": -0.0701179305712382, "compression_ratio": 1.9689119170984455, "no_speech_prob": 0.004164023790508509}, {"id": 680, "seek": 390872, "start": 3915.7599999999998, "end": 3923.04, "text": " with the putative conscious, putatively conscious being. And what I mean by that is that we have", "tokens": [50716, 365, 264, 829, 1166, 6648, 11, 829, 19020, 6648, 885, 13, 400, 437, 286, 914, 538, 300, 307, 300, 321, 362, 51080], "temperature": 0.0, "avg_logprob": -0.0701179305712382, "compression_ratio": 1.9689119170984455, "no_speech_prob": 0.004164023790508509}, {"id": 681, "seek": 390872, "start": 3923.04, "end": 3927.6, "text": " to be able to put ourselves, you know, we have to be able to put ourselves in a position where", "tokens": [51080, 281, 312, 1075, 281, 829, 4175, 11, 291, 458, 11, 321, 362, 281, 312, 1075, 281, 829, 4175, 294, 257, 2535, 689, 51308], "temperature": 0.0, "avg_logprob": -0.0701179305712382, "compression_ratio": 1.9689119170984455, "no_speech_prob": 0.004164023790508509}, {"id": 682, "seek": 390872, "start": 3927.6, "end": 3934.64, "text": " we're sharing a world with that, with that putatively conscious, you know, artifact or being.", "tokens": [51308, 321, 434, 5414, 257, 1002, 365, 300, 11, 365, 300, 829, 19020, 6648, 11, 291, 458, 11, 34806, 420, 885, 13, 51660], "temperature": 0.0, "avg_logprob": -0.0701179305712382, "compression_ratio": 1.9689119170984455, "no_speech_prob": 0.004164023790508509}, {"id": 683, "seek": 393464, "start": 3934.64, "end": 3939.7599999999998, "text": " And so, you know, a good example of this is the octopus. So Peter Godfrey Smith has written", "tokens": [50364, 400, 370, 11, 291, 458, 11, 257, 665, 1365, 295, 341, 307, 264, 27962, 13, 407, 6508, 1265, 69, 7950, 8538, 575, 3720, 50620], "temperature": 0.0, "avg_logprob": -0.0987508773803711, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.007004636339843273}, {"id": 684, "seek": 393464, "start": 3939.7599999999998, "end": 3945.2, "text": " these wonderful books about what it's like to hang out with octopuses and be with them and so on.", "tokens": [50620, 613, 3715, 3642, 466, 437, 309, 311, 411, 281, 3967, 484, 365, 13350, 404, 8355, 293, 312, 365, 552, 293, 370, 322, 13, 50892], "temperature": 0.0, "avg_logprob": -0.0987508773803711, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.007004636339843273}, {"id": 685, "seek": 393464, "start": 3946.08, "end": 3952.96, "text": " And the really important aspect of that is that he has to put on a diving suit and go down", "tokens": [50936, 400, 264, 534, 1021, 4171, 295, 300, 307, 300, 415, 575, 281, 829, 322, 257, 20241, 5722, 293, 352, 760, 51280], "temperature": 0.0, "avg_logprob": -0.0987508773803711, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.007004636339843273}, {"id": 686, "seek": 393464, "start": 3952.96, "end": 3958.56, "text": " and be under the water and spend time with the octopus interacting with the same things", "tokens": [51280, 293, 312, 833, 264, 1281, 293, 3496, 565, 365, 264, 27962, 18017, 365, 264, 912, 721, 51560], "temperature": 0.0, "avg_logprob": -0.0987508773803711, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.007004636339843273}, {"id": 687, "seek": 393464, "start": 3958.56, "end": 3962.48, "text": " and being in the same world together, seeing the same things and so on.", "tokens": [51560, 293, 885, 294, 264, 912, 1002, 1214, 11, 2577, 264, 912, 721, 293, 370, 322, 13, 51756], "temperature": 0.0, "avg_logprob": -0.0987508773803711, "compression_ratio": 1.7670682730923695, "no_speech_prob": 0.007004636339843273}, {"id": 688, "seek": 396248, "start": 3962.56, "end": 3969.04, "text": " So, and then on that basis and the behavior that he observes and so on, then, you know,", "tokens": [50368, 407, 11, 293, 550, 322, 300, 5143, 293, 264, 5223, 300, 415, 3181, 9054, 293, 370, 322, 11, 550, 11, 291, 458, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11887078325287635, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.0014494963688775897}, {"id": 689, "seek": 396248, "start": 3969.04, "end": 3972.64, "text": " he might come to some kind of, he might start treating it as a fellow conscious creature.", "tokens": [50692, 415, 1062, 808, 281, 512, 733, 295, 11, 415, 1062, 722, 15083, 309, 382, 257, 7177, 6648, 12797, 13, 50872], "temperature": 0.0, "avg_logprob": -0.11887078325287635, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.0014494963688775897}, {"id": 690, "seek": 396248, "start": 3973.2, "end": 3977.92, "text": " So by analogy, or as you know, similarly, what I think that we need to be able to do", "tokens": [50900, 407, 538, 21663, 11, 420, 382, 291, 458, 11, 14138, 11, 437, 286, 519, 300, 321, 643, 281, 312, 1075, 281, 360, 51136], "temperature": 0.0, "avg_logprob": -0.11887078325287635, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.0014494963688775897}, {"id": 691, "seek": 396248, "start": 3977.92, "end": 3983.52, "text": " is to engineer an encounter like that, even if it's a very, very alien kind of artifact, say.", "tokens": [51136, 307, 281, 11403, 364, 8593, 411, 300, 11, 754, 498, 309, 311, 257, 588, 11, 588, 12319, 733, 295, 34806, 11, 584, 13, 51416], "temperature": 0.0, "avg_logprob": -0.11887078325287635, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.0014494963688775897}, {"id": 692, "seek": 396248, "start": 3983.52, "end": 3988.8, "text": " So suppose it's this white cube, then one way that it might happen, well, suppose scientists", "tokens": [51416, 407, 7297, 309, 311, 341, 2418, 13728, 11, 550, 472, 636, 300, 309, 1062, 1051, 11, 731, 11, 7297, 7708, 51680], "temperature": 0.0, "avg_logprob": -0.11887078325287635, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.0014494963688775897}, {"id": 693, "seek": 398880, "start": 3988.8, "end": 3994.6400000000003, "text": " managed to figure out that there's computation going on inside this white cube, and then they", "tokens": [50364, 6453, 281, 2573, 484, 300, 456, 311, 24903, 516, 322, 1854, 341, 2418, 13728, 11, 293, 550, 436, 50656], "temperature": 0.0, "avg_logprob": -0.08928992350896199, "compression_ratio": 1.9414225941422594, "no_speech_prob": 0.0083304513245821}, {"id": 694, "seek": 398880, "start": 3994.6400000000003, "end": 3999.36, "text": " managed to reverse engineer this computation and they can see that there's a sort of division between", "tokens": [50656, 6453, 281, 9943, 11403, 341, 24903, 293, 436, 393, 536, 300, 456, 311, 257, 1333, 295, 10044, 1296, 50892], "temperature": 0.0, "avg_logprob": -0.08928992350896199, "compression_ratio": 1.9414225941422594, "no_speech_prob": 0.0083304513245821}, {"id": 695, "seek": 398880, "start": 3999.36, "end": 4004.48, "text": " a world and the things interacting with that world in this computation. There's a sort of", "tokens": [50892, 257, 1002, 293, 264, 721, 18017, 365, 300, 1002, 294, 341, 24903, 13, 821, 311, 257, 1333, 295, 51148], "temperature": 0.0, "avg_logprob": -0.08928992350896199, "compression_ratio": 1.9414225941422594, "no_speech_prob": 0.0083304513245821}, {"id": 696, "seek": 398880, "start": 4004.48, "end": 4011.44, "text": " simulated world. And then you could imagine, by some clever engineering tricks, inserting", "tokens": [51148, 41713, 1002, 13, 400, 550, 291, 727, 3811, 11, 538, 512, 13494, 7043, 11733, 11, 46567, 51496], "temperature": 0.0, "avg_logprob": -0.08928992350896199, "compression_ratio": 1.9414225941422594, "no_speech_prob": 0.0083304513245821}, {"id": 697, "seek": 398880, "start": 4011.44, "end": 4016.6400000000003, "text": " yourself into that very same world and being alongside these things that are interacting", "tokens": [51496, 1803, 666, 300, 588, 912, 1002, 293, 885, 12385, 613, 721, 300, 366, 18017, 51756], "temperature": 0.0, "avg_logprob": -0.08928992350896199, "compression_ratio": 1.9414225941422594, "no_speech_prob": 0.0083304513245821}, {"id": 698, "seek": 401664, "start": 4016.64, "end": 4021.68, "text": " with this environment and interacting with that environment with them. So being in the world with", "tokens": [50364, 365, 341, 2823, 293, 18017, 365, 300, 2823, 365, 552, 13, 407, 885, 294, 264, 1002, 365, 50616], "temperature": 0.0, "avg_logprob": -0.1027036357570339, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.006015049293637276}, {"id": 699, "seek": 401664, "start": 4021.68, "end": 4026.24, "text": " them. Now, obviously, I'm setting this up to be very much like a games environment and a virtual", "tokens": [50616, 552, 13, 823, 11, 2745, 11, 286, 478, 3287, 341, 493, 281, 312, 588, 709, 411, 257, 2813, 2823, 293, 257, 6374, 50844], "temperature": 0.0, "avg_logprob": -0.1027036357570339, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.006015049293637276}, {"id": 700, "seek": 401664, "start": 4026.24, "end": 4031.2, "text": " world and a games environment, but to make the thought experiment work. But so that's an example", "tokens": [50844, 1002, 293, 257, 2813, 2823, 11, 457, 281, 652, 264, 1194, 5120, 589, 13, 583, 370, 300, 311, 364, 1365, 51092], "temperature": 0.0, "avg_logprob": -0.1027036357570339, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.006015049293637276}, {"id": 701, "seek": 401664, "start": 4031.2, "end": 4036.7999999999997, "text": " of where, you know, if you manage to engineer an encounter with, you know, these things that are", "tokens": [51092, 295, 689, 11, 291, 458, 11, 498, 291, 3067, 281, 11403, 364, 8593, 365, 11, 291, 458, 11, 613, 721, 300, 366, 51372], "temperature": 0.0, "avg_logprob": -0.1027036357570339, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.006015049293637276}, {"id": 702, "seek": 401664, "start": 4036.7999999999997, "end": 4041.68, "text": " inside this cube, and then you can observe their behavior, you can interact with them,", "tokens": [51372, 1854, 341, 13728, 11, 293, 550, 291, 393, 11441, 641, 5223, 11, 291, 393, 4648, 365, 552, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1027036357570339, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.006015049293637276}, {"id": 703, "seek": 404168, "start": 4041.68, "end": 4046.7999999999997, "text": " and then you can decide whether you or you will, you know, you may or may not start to treat them", "tokens": [50364, 293, 550, 291, 393, 4536, 1968, 291, 420, 291, 486, 11, 291, 458, 11, 291, 815, 420, 815, 406, 722, 281, 2387, 552, 50620], "temperature": 0.0, "avg_logprob": -0.09396298178311052, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006272331811487675}, {"id": 704, "seek": 404168, "start": 4046.7999999999997, "end": 4052.0, "text": " as fellow conscious creatures. So there are these two steps. It's sort of, can you engineer an", "tokens": [50620, 382, 7177, 6648, 12281, 13, 407, 456, 366, 613, 732, 4439, 13, 467, 311, 1333, 295, 11, 393, 291, 11403, 364, 50880], "temperature": 0.0, "avg_logprob": -0.09396298178311052, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006272331811487675}, {"id": 705, "seek": 404168, "start": 4052.0, "end": 4056.64, "text": " encounter, at least in principle, and that makes the question answerable. And then you can answer", "tokens": [50880, 8593, 11, 412, 1935, 294, 8665, 11, 293, 300, 1669, 264, 1168, 1867, 712, 13, 400, 550, 291, 393, 1867, 51112], "temperature": 0.0, "avg_logprob": -0.09396298178311052, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006272331811487675}, {"id": 706, "seek": 404168, "start": 4056.64, "end": 4061.44, "text": " the question by actually having the encounter and interacting with them. And by the way, we notice", "tokens": [51112, 264, 1168, 538, 767, 1419, 264, 8593, 293, 18017, 365, 552, 13, 400, 538, 264, 636, 11, 321, 3449, 51352], "temperature": 0.0, "avg_logprob": -0.09396298178311052, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006272331811487675}, {"id": 707, "seek": 404168, "start": 4061.44, "end": 4068.7999999999997, "text": " that everything there is public. You've made, you know, there's no private realm of subjectivity", "tokens": [51352, 300, 1203, 456, 307, 1908, 13, 509, 600, 1027, 11, 291, 458, 11, 456, 311, 572, 4551, 15355, 295, 3983, 4253, 51720], "temperature": 0.0, "avg_logprob": -0.09396298178311052, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006272331811487675}, {"id": 708, "seek": 406880, "start": 4069.28, "end": 4075.44, "text": " everything is public. It's on the basis of public stuff that you come to see them as fellow", "tokens": [50388, 1203, 307, 1908, 13, 467, 311, 322, 264, 5143, 295, 1908, 1507, 300, 291, 808, 281, 536, 552, 382, 7177, 50696], "temperature": 0.0, "avg_logprob": -0.1060742862889024, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.006424798630177975}, {"id": 709, "seek": 406880, "start": 4075.44, "end": 4079.44, "text": " conscious creatures or not. Yeah, a couple of things on that. I mean, as you pointed out in", "tokens": [50696, 6648, 12281, 420, 406, 13, 865, 11, 257, 1916, 295, 721, 322, 300, 13, 286, 914, 11, 382, 291, 10932, 484, 294, 50896], "temperature": 0.0, "avg_logprob": -0.1060742862889024, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.006424798630177975}, {"id": 710, "seek": 406880, "start": 4079.44, "end": 4085.28, "text": " Conscious Exotica, the octopus is quite interesting because it's not as human like yet, as you just", "tokens": [50896, 6923, 4139, 2111, 310, 2262, 11, 264, 27962, 307, 1596, 1880, 570, 309, 311, 406, 382, 1952, 411, 1939, 11, 382, 291, 445, 51188], "temperature": 0.0, "avg_logprob": -0.1060742862889024, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.006424798630177975}, {"id": 711, "seek": 406880, "start": 4085.28, "end": 4091.28, "text": " cited, more conscious. And the way that we figure out the consciousness, and you know, this is me", "tokens": [51188, 30134, 11, 544, 6648, 13, 400, 264, 636, 300, 321, 2573, 484, 264, 10081, 11, 293, 291, 458, 11, 341, 307, 385, 51488], "temperature": 0.0, "avg_logprob": -0.1060742862889024, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.006424798630177975}, {"id": 712, "seek": 406880, "start": 4091.28, "end": 4095.44, "text": " kind of interpreting what you said a little bit, is we set up a language game. And I don't know", "tokens": [51488, 733, 295, 37395, 437, 291, 848, 257, 707, 857, 11, 307, 321, 992, 493, 257, 2856, 1216, 13, 400, 286, 500, 380, 458, 51696], "temperature": 0.0, "avg_logprob": -0.1060742862889024, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.006424798630177975}, {"id": 713, "seek": 409544, "start": 4095.44, "end": 4099.76, "text": " whether you've read that book by Nick Shater and Morton Christensen, but it's a beautiful book,", "tokens": [50364, 1968, 291, 600, 1401, 300, 1446, 538, 9449, 1160, 771, 293, 24977, 266, 2040, 32934, 11, 457, 309, 311, 257, 2238, 1446, 11, 50580], "temperature": 0.0, "avg_logprob": -0.14642874399820963, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.012528616935014725}, {"id": 714, "seek": 409544, "start": 4099.76, "end": 4104.16, "text": " beautiful book. But you know, I know of the book, but I haven't, I'm afraid. It's incredible. But,", "tokens": [50580, 2238, 1446, 13, 583, 291, 458, 11, 286, 458, 295, 264, 1446, 11, 457, 286, 2378, 380, 11, 286, 478, 4638, 13, 467, 311, 4651, 13, 583, 11, 50800], "temperature": 0.0, "avg_logprob": -0.14642874399820963, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.012528616935014725}, {"id": 715, "seek": 409544, "start": 4104.16, "end": 4110.08, "text": " you know, they basically say at, you know, Per Wittgenstein that you play the language game,", "tokens": [50800, 291, 458, 11, 436, 1936, 584, 412, 11, 291, 458, 11, 3026, 343, 593, 1766, 9089, 300, 291, 862, 264, 2856, 1216, 11, 51096], "temperature": 0.0, "avg_logprob": -0.14642874399820963, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.012528616935014725}, {"id": 716, "seek": 409544, "start": 4110.08, "end": 4116.96, "text": " and you, because you're physically sharing the same environments, you improvise, and that's how", "tokens": [51096, 293, 291, 11, 570, 291, 434, 9762, 5414, 264, 912, 12388, 11, 291, 29424, 908, 11, 293, 300, 311, 577, 51440], "temperature": 0.0, "avg_logprob": -0.14642874399820963, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.012528616935014725}, {"id": 717, "seek": 409544, "start": 4116.96, "end": 4124.4, "text": " you derive meaning. And meaning is very, very important for relatability. And then we ascribe", "tokens": [51440, 291, 28446, 3620, 13, 400, 3620, 307, 588, 11, 588, 1021, 337, 22441, 2310, 13, 400, 550, 321, 382, 8056, 51812], "temperature": 0.0, "avg_logprob": -0.14642874399820963, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.012528616935014725}, {"id": 718, "seek": 412440, "start": 4124.4, "end": 4130.4, "text": " consciousness to that kind of process. And you cited Peter Singer actually, and I think he said", "tokens": [50364, 10081, 281, 300, 733, 295, 1399, 13, 400, 291, 30134, 6508, 44184, 767, 11, 293, 286, 519, 415, 848, 50664], "temperature": 0.0, "avg_logprob": -0.1467654238042143, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0010945702670142055}, {"id": 719, "seek": 412440, "start": 4130.4, "end": 4137.679999999999, "text": " in 1975 that we have a natural inclination to kind of ascribe moral status to beings which we", "tokens": [50664, 294, 32454, 300, 321, 362, 257, 3303, 37070, 2486, 281, 733, 295, 382, 8056, 9723, 6558, 281, 8958, 597, 321, 51028], "temperature": 0.0, "avg_logprob": -0.1467654238042143, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0010945702670142055}, {"id": 720, "seek": 412440, "start": 4137.679999999999, "end": 4146.879999999999, "text": " think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more. So in the context of the", "tokens": [51028, 519, 295, 382, 6648, 13, 865, 11, 1338, 11, 6451, 11, 1338, 13, 865, 11, 980, 385, 544, 13, 407, 294, 264, 4319, 295, 264, 51488], "temperature": 0.0, "avg_logprob": -0.1467654238042143, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0010945702670142055}, {"id": 721, "seek": 412440, "start": 4147.679999999999, "end": 4153.599999999999, "text": " octopus, then the sort of, you know, you could, then there aren't going to be language games,", "tokens": [51528, 27962, 11, 550, 264, 1333, 295, 11, 291, 458, 11, 291, 727, 11, 550, 456, 3212, 380, 516, 281, 312, 2856, 2813, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1467654238042143, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0010945702670142055}, {"id": 722, "seek": 415360, "start": 4153.6, "end": 4157.84, "text": " you're not going to be engaged in a language game with the octopus because the octopus is not", "tokens": [50364, 291, 434, 406, 516, 281, 312, 8237, 294, 257, 2856, 1216, 365, 264, 27962, 570, 264, 27962, 307, 406, 50576], "temperature": 0.0, "avg_logprob": -0.08371643793015253, "compression_ratio": 1.941908713692946, "no_speech_prob": 0.011798899620771408}, {"id": 723, "seek": 415360, "start": 4157.84, "end": 4166.320000000001, "text": " a fellow language user. But your fellow language users are other people in your community with", "tokens": [50576, 257, 7177, 2856, 4195, 13, 583, 428, 7177, 2856, 5022, 366, 661, 561, 294, 428, 1768, 365, 51000], "temperature": 0.0, "avg_logprob": -0.08371643793015253, "compression_ratio": 1.941908713692946, "no_speech_prob": 0.011798899620771408}, {"id": 724, "seek": 415360, "start": 4166.320000000001, "end": 4171.6, "text": " whom you'll talk about the octopus. And you'll talk about the octopus and together you'll arrive at", "tokens": [51000, 7101, 291, 603, 751, 466, 264, 27962, 13, 400, 291, 603, 751, 466, 264, 27962, 293, 1214, 291, 603, 8881, 412, 51264], "temperature": 0.0, "avg_logprob": -0.08371643793015253, "compression_ratio": 1.941908713692946, "no_speech_prob": 0.011798899620771408}, {"id": 725, "seek": 415360, "start": 4171.6, "end": 4176.160000000001, "text": " some consensus, hopefully, about whether you want to talk about it in terms of consciousness.", "tokens": [51264, 512, 19115, 11, 4696, 11, 466, 1968, 291, 528, 281, 751, 466, 309, 294, 2115, 295, 10081, 13, 51492], "temperature": 0.0, "avg_logprob": -0.08371643793015253, "compression_ratio": 1.941908713692946, "no_speech_prob": 0.011798899620771408}, {"id": 726, "seek": 415360, "start": 4176.160000000001, "end": 4179.76, "text": " And that's going to be all to do with like observing its behavior, listening to other", "tokens": [51492, 400, 300, 311, 516, 281, 312, 439, 281, 360, 365, 411, 22107, 1080, 5223, 11, 4764, 281, 661, 51672], "temperature": 0.0, "avg_logprob": -0.08371643793015253, "compression_ratio": 1.941908713692946, "no_speech_prob": 0.011798899620771408}, {"id": 727, "seek": 417976, "start": 4179.76, "end": 4186.0, "text": " people's accounts of being with octopuses. And critically, maybe listening to also what scientists", "tokens": [50364, 561, 311, 9402, 295, 885, 365, 13350, 404, 8355, 13, 400, 22797, 11, 1310, 4764, 281, 611, 437, 7708, 50676], "temperature": 0.0, "avg_logprob": -0.12785325151808719, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.050398971885442734}, {"id": 728, "seek": 417976, "start": 4186.0, "end": 4190.320000000001, "text": " have discovered when they look inside octopus brains and they perform behavioral experiments.", "tokens": [50676, 362, 6941, 562, 436, 574, 1854, 27962, 15442, 293, 436, 2042, 19124, 12050, 13, 50892], "temperature": 0.0, "avg_logprob": -0.12785325151808719, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.050398971885442734}, {"id": 729, "seek": 417976, "start": 4190.320000000001, "end": 4197.6, "text": " And, you know, that's all, again, is public. That's all is grist of the mill of settling on a kind", "tokens": [50892, 400, 11, 291, 458, 11, 300, 311, 439, 11, 797, 11, 307, 1908, 13, 663, 311, 439, 307, 677, 468, 295, 264, 1728, 295, 33841, 322, 257, 733, 51256], "temperature": 0.0, "avg_logprob": -0.12785325151808719, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.050398971885442734}, {"id": 730, "seek": 417976, "start": 4197.6, "end": 4205.84, "text": " of the way we talk about these strange creatures. Yeah, I mean, I guess for the language, there's", "tokens": [51256, 295, 264, 636, 321, 751, 466, 613, 5861, 12281, 13, 865, 11, 286, 914, 11, 286, 2041, 337, 264, 2856, 11, 456, 311, 51668], "temperature": 0.0, "avg_logprob": -0.12785325151808719, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.050398971885442734}, {"id": 731, "seek": 420584, "start": 4205.92, "end": 4209.12, "text": " two parts to this. So the language game, first of all, it doesn't have to be spoken words,", "tokens": [50368, 732, 3166, 281, 341, 13, 407, 264, 2856, 1216, 11, 700, 295, 439, 11, 309, 1177, 380, 362, 281, 312, 10759, 2283, 11, 50528], "temperature": 0.0, "avg_logprob": -0.10013902837579901, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.1246870681643486}, {"id": 732, "seek": 420584, "start": 4209.12, "end": 4214.24, "text": " it's improvisation of any kind, it could be gestures, it could be all sorts, it's just", "tokens": [50528, 309, 311, 39784, 399, 295, 604, 733, 11, 309, 727, 312, 28475, 11, 309, 727, 312, 439, 7527, 11, 309, 311, 445, 50784], "temperature": 0.0, "avg_logprob": -0.10013902837579901, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.1246870681643486}, {"id": 733, "seek": 420584, "start": 4214.24, "end": 4219.12, "text": " behavior. Right, okay. And then the interesting thing with the octopus is we might not be", "tokens": [50784, 5223, 13, 1779, 11, 1392, 13, 400, 550, 264, 1880, 551, 365, 264, 27962, 307, 321, 1062, 406, 312, 51028], "temperature": 0.0, "avg_logprob": -0.10013902837579901, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.1246870681643486}, {"id": 734, "seek": 420584, "start": 4219.12, "end": 4225.28, "text": " interacting with them interactively. We might be non interactively observing them as agents", "tokens": [51028, 18017, 365, 552, 4648, 3413, 13, 492, 1062, 312, 2107, 4648, 3413, 22107, 552, 382, 12554, 51336], "temperature": 0.0, "avg_logprob": -0.10013902837579901, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.1246870681643486}, {"id": 735, "seek": 420584, "start": 4225.28, "end": 4230.16, "text": " interacting with each other, playing their own language game, but we can still ascribe some", "tokens": [51336, 18017, 365, 1184, 661, 11, 2433, 641, 1065, 2856, 1216, 11, 457, 321, 393, 920, 382, 8056, 512, 51580], "temperature": 0.0, "avg_logprob": -0.10013902837579901, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.1246870681643486}, {"id": 736, "seek": 423016, "start": 4230.24, "end": 4237.92, "text": " kind of measure of and I actually think what we're measuring here is agency and agency and", "tokens": [50368, 733, 295, 3481, 295, 293, 286, 767, 519, 437, 321, 434, 13389, 510, 307, 7934, 293, 7934, 293, 50752], "temperature": 0.0, "avg_logprob": -0.11825981805490893, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.005147777497768402}, {"id": 737, "seek": 423016, "start": 4237.92, "end": 4242.32, "text": " moral status, I think are pretty much one to one. So when we see them playing the language game,", "tokens": [50752, 9723, 6558, 11, 286, 519, 366, 1238, 709, 472, 281, 472, 13, 407, 562, 321, 536, 552, 2433, 264, 2856, 1216, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11825981805490893, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.005147777497768402}, {"id": 738, "seek": 423016, "start": 4242.32, "end": 4246.0, "text": " we start to think of them as agents, therefore they have moral status.", "tokens": [50972, 321, 722, 281, 519, 295, 552, 382, 12554, 11, 4412, 436, 362, 9723, 6558, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11825981805490893, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.005147777497768402}, {"id": 739, "seek": 423016, "start": 4246.0, "end": 4251.76, "text": " Yeah, I mean, I certainly think that by observing behavior, then we may similarly, you know,", "tokens": [51156, 865, 11, 286, 914, 11, 286, 3297, 519, 300, 538, 22107, 5223, 11, 550, 321, 815, 14138, 11, 291, 458, 11, 51444], "temperature": 0.0, "avg_logprob": -0.11825981805490893, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.005147777497768402}, {"id": 740, "seek": 425176, "start": 4252.4800000000005, "end": 4260.08, "text": " start to ascribe consciousness to other creatures. I mean, it's always much more persuasive", "tokens": [50400, 722, 281, 382, 8056, 10081, 281, 661, 12281, 13, 286, 914, 11, 309, 311, 1009, 709, 544, 16336, 23686, 50780], "temperature": 0.0, "avg_logprob": -0.0883927575076919, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.006448658183217049}, {"id": 741, "seek": 425176, "start": 4260.08, "end": 4263.92, "text": " if it's interactive, I think, than if it's simply observing behavior.", "tokens": [50780, 498, 309, 311, 15141, 11, 286, 519, 11, 813, 498, 309, 311, 2935, 22107, 5223, 13, 50972], "temperature": 0.0, "avg_logprob": -0.0883927575076919, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.006448658183217049}, {"id": 742, "seek": 425176, "start": 4265.04, "end": 4270.16, "text": " Murray said that if a creature's brain is like ours, then there's grounds to suppose", "tokens": [51028, 27291, 848, 300, 498, 257, 12797, 311, 3567, 307, 411, 11896, 11, 550, 456, 311, 19196, 281, 7297, 51284], "temperature": 0.0, "avg_logprob": -0.0883927575076919, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.006448658183217049}, {"id": 743, "seek": 425176, "start": 4270.16, "end": 4278.320000000001, "text": " that its consciousness, its inner life is also like ours. He went on, if something is built", "tokens": [51284, 300, 1080, 10081, 11, 1080, 7284, 993, 307, 611, 411, 11896, 13, 634, 1437, 322, 11, 498, 746, 307, 3094, 51692], "temperature": 0.0, "avg_logprob": -0.0883927575076919, "compression_ratio": 1.6328502415458936, "no_speech_prob": 0.006448658183217049}, {"id": 744, "seek": 427832, "start": 4278.32, "end": 4284.0, "text": " very differently to us with a different architecture realized on a different substrate,", "tokens": [50364, 588, 7614, 281, 505, 365, 257, 819, 9482, 5334, 322, 257, 819, 27585, 11, 50648], "temperature": 0.0, "avg_logprob": -0.08592848401320607, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003321484662592411}, {"id": 745, "seek": 427832, "start": 4284.0, "end": 4289.92, "text": " then however human like its behavior, its consciousness might be very different to ours.", "tokens": [50648, 550, 4461, 1952, 411, 1080, 5223, 11, 1080, 10081, 1062, 312, 588, 819, 281, 11896, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08592848401320607, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003321484662592411}, {"id": 746, "seek": 427832, "start": 4290.48, "end": 4294.96, "text": " Perhaps it would be a phenomenological zombie with no consciousness at all.", "tokens": [50972, 10517, 309, 576, 312, 257, 9388, 4383, 20310, 365, 572, 10081, 412, 439, 13, 51196], "temperature": 0.0, "avg_logprob": -0.08592848401320607, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003321484662592411}, {"id": 747, "seek": 427832, "start": 4296.719999999999, "end": 4302.24, "text": " Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of", "tokens": [51284, 27291, 848, 294, 6923, 4139, 2111, 310, 2262, 300, 309, 311, 787, 562, 321, 360, 10675, 300, 321, 722, 281, 519, 295, 51560], "temperature": 0.0, "avg_logprob": -0.08592848401320607, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003321484662592411}, {"id": 748, "seek": 430224, "start": 4302.32, "end": 4309.679999999999, "text": " consciousness, experience, and sensation in terms of private subjectivity. He cited David", "tokens": [50368, 10081, 11, 1752, 11, 293, 20069, 294, 2115, 295, 4551, 3983, 4253, 13, 634, 30134, 4389, 50736], "temperature": 0.0, "avg_logprob": -0.08195371981020327, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.024709442630410194}, {"id": 749, "seek": 430224, "start": 4309.679999999999, "end": 4316.08, "text": " Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction", "tokens": [50736, 761, 304, 18552, 293, 702, 1152, 293, 1858, 16844, 295, 10081, 382, 257, 733, 295, 3364, 88, 16844, 51056], "temperature": 0.0, "avg_logprob": -0.08195371981020327, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.024709442630410194}, {"id": 750, "seek": 430224, "start": 4316.08, "end": 4322.5599999999995, "text": " using his phraseology between the inner and the outer. In short, he said to a form of dualism,", "tokens": [51056, 1228, 702, 9535, 1793, 1296, 264, 7284, 293, 264, 10847, 13, 682, 2099, 11, 415, 848, 281, 257, 1254, 295, 11848, 1434, 11, 51380], "temperature": 0.0, "avg_logprob": -0.08195371981020327, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.024709442630410194}, {"id": 751, "seek": 430224, "start": 4322.5599999999995, "end": 4327.44, "text": " which is that subjectivity is an ontologically distinct feature of reality.", "tokens": [51380, 597, 307, 300, 3983, 4253, 307, 364, 6592, 17157, 10644, 4111, 295, 4103, 13, 51624], "temperature": 0.0, "avg_logprob": -0.08195371981020327, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.024709442630410194}, {"id": 752, "seek": 432744, "start": 4328.08, "end": 4333.44, "text": " Wittgenstein provided an antidote to this way of thinking in his remarks on private language,", "tokens": [50396, 343, 593, 1766, 9089, 5649, 364, 47962, 1370, 281, 341, 636, 295, 1953, 294, 702, 19151, 322, 4551, 2856, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08781742519802517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014478580094873905}, {"id": 753, "seek": 432744, "start": 4333.44, "end": 4339.44, "text": " whose centerpiece in an argument to the effect that insofar as we can talk about our experience,", "tokens": [50664, 6104, 3056, 15281, 294, 364, 6770, 281, 264, 1802, 300, 294, 539, 21196, 382, 321, 393, 751, 466, 527, 1752, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08781742519802517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014478580094873905}, {"id": 754, "seek": 432744, "start": 4339.44, "end": 4346.0, "text": " they must have an outward public manifestation. For Wittgenstein, only of a living human being,", "tokens": [50964, 436, 1633, 362, 364, 26914, 1908, 29550, 13, 1171, 343, 593, 1766, 9089, 11, 787, 295, 257, 2647, 1952, 885, 11, 51292], "temperature": 0.0, "avg_logprob": -0.08781742519802517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014478580094873905}, {"id": 755, "seek": 432744, "start": 4346.0, "end": 4353.28, "text": " what resembles or behaves like a living human being, one can say that it has sensations.", "tokens": [51292, 437, 34433, 420, 36896, 411, 257, 2647, 1952, 885, 11, 472, 393, 584, 300, 309, 575, 36642, 13, 51656], "temperature": 0.0, "avg_logprob": -0.08781742519802517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.014478580094873905}, {"id": 756, "seek": 435328, "start": 4353.28, "end": 4360.639999999999, "text": " It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism,", "tokens": [50364, 467, 8194, 11, 309, 307, 6648, 11, 420, 309, 307, 18900, 13, 583, 1943, 380, 341, 445, 5223, 1434, 30, 45807, 1434, 11, 50732], "temperature": 0.0, "avg_logprob": -0.07329579286797103, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.005986189469695091}, {"id": 757, "seek": 435328, "start": 4360.639999999999, "end": 4366.32, "text": " particularly in its radical form as advocated by B. F. Skinner, posits that all psychological", "tokens": [50732, 4098, 294, 1080, 12001, 1254, 382, 7915, 770, 538, 363, 13, 479, 13, 26333, 1193, 11, 1366, 1208, 300, 439, 14346, 51016], "temperature": 0.0, "avg_logprob": -0.07329579286797103, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.005986189469695091}, {"id": 758, "seek": 435328, "start": 4366.32, "end": 4372.96, "text": " phenomena can be explained in terms of observable behavior and environmental stimuli without", "tokens": [51016, 22004, 393, 312, 8825, 294, 2115, 295, 9951, 712, 5223, 293, 8303, 47752, 1553, 51348], "temperature": 0.0, "avg_logprob": -0.07329579286797103, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.005986189469695091}, {"id": 759, "seek": 435328, "start": 4372.96, "end": 4379.92, "text": " recourse to internal mental states. But behaviorism is often criticized for neglecting the subjective", "tokens": [51348, 850, 13656, 281, 6920, 4973, 4368, 13, 583, 5223, 1434, 307, 2049, 28011, 337, 17745, 278, 264, 25972, 51696], "temperature": 0.0, "avg_logprob": -0.07329579286797103, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.005986189469695091}, {"id": 760, "seek": 437992, "start": 4379.92, "end": 4387.04, "text": " internal aspects of mental life. Wittgenstein argues against the notion of purely private", "tokens": [50364, 6920, 7270, 295, 4973, 993, 13, 343, 593, 1766, 9089, 38218, 1970, 264, 10710, 295, 17491, 4551, 50720], "temperature": 0.0, "avg_logprob": -0.05753188663058811, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0018940344452857971}, {"id": 761, "seek": 437992, "start": 4387.04, "end": 4393.6, "text": " language, where words refer to inner experiences known only to the speaker. He contends that for", "tokens": [50720, 2856, 11, 689, 2283, 2864, 281, 7284, 5235, 2570, 787, 281, 264, 8145, 13, 634, 660, 2581, 300, 337, 51048], "temperature": 0.0, "avg_logprob": -0.05753188663058811, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0018940344452857971}, {"id": 762, "seek": 437992, "start": 4393.6, "end": 4400.64, "text": " language to be meaningful, it must be grounded in publicly accessible criteria. So as Murray said in", "tokens": [51048, 2856, 281, 312, 10995, 11, 309, 1633, 312, 23535, 294, 14843, 9515, 11101, 13, 407, 382, 27291, 848, 294, 51400], "temperature": 0.0, "avg_logprob": -0.05753188663058811, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0018940344452857971}, {"id": 763, "seek": 437992, "start": 4400.64, "end": 4407.68, "text": " his article, Wittgenstein argued against dualism or the so-called impenetrable realm of the subject", "tokens": [51400, 702, 7222, 11, 343, 593, 1766, 9089, 20219, 1970, 11848, 1434, 420, 264, 370, 12, 11880, 704, 268, 302, 424, 638, 15355, 295, 264, 3983, 51752], "temperature": 0.0, "avg_logprob": -0.05753188663058811, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0018940344452857971}, {"id": 764, "seek": 440768, "start": 4407.84, "end": 4414.240000000001, "text": " experience. Actually, he said that many folks who make the in-principle argument against AI", "tokens": [50372, 1752, 13, 5135, 11, 415, 848, 300, 867, 4024, 567, 652, 264, 294, 12, 1424, 21961, 781, 6770, 1970, 7318, 50692], "temperature": 0.0, "avg_logprob": -0.13417470745924043, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.05648748576641083}, {"id": 765, "seek": 440768, "start": 4414.8, "end": 4420.8, "text": " often retreat into subjectivity arguments, as our recent guest Maria Santa Catarina did.", "tokens": [50720, 2049, 15505, 666, 3983, 4253, 12869, 11, 382, 527, 5162, 8341, 12734, 9933, 9565, 37512, 630, 13, 51020], "temperature": 0.0, "avg_logprob": -0.13417470745924043, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.05648748576641083}, {"id": 766, "seek": 440768, "start": 4421.92, "end": 4427.92, "text": " Murray said that the difficulty here is that accepting the possibility of radically inscrutable", "tokens": [51076, 27291, 848, 300, 264, 10360, 510, 307, 300, 17391, 264, 7959, 295, 35508, 1028, 10757, 32148, 51376], "temperature": 0.0, "avg_logprob": -0.13417470745924043, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.05648748576641083}, {"id": 767, "seek": 440768, "start": 4427.92, "end": 4434.4800000000005, "text": " consciousness seemingly re-admits dualistic propositions, that consciousness is not, so to", "tokens": [51376, 10081, 18709, 319, 12, 345, 76, 1208, 11848, 3142, 7532, 2451, 11, 300, 10081, 307, 406, 11, 370, 281, 51704], "temperature": 0.0, "avg_logprob": -0.13417470745924043, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.05648748576641083}, {"id": 768, "seek": 443448, "start": 4434.5599999999995, "end": 4442.0, "text": " speak, open to view, but inherently private. Yeah, so Aaron Sloman, who's a professor of computer", "tokens": [50368, 1710, 11, 1269, 281, 1910, 11, 457, 27993, 4551, 13, 865, 11, 370, 14018, 6187, 4277, 11, 567, 311, 257, 8304, 295, 3820, 50740], "temperature": 0.0, "avg_logprob": -0.11501111984252929, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.012407266534864902}, {"id": 769, "seek": 443448, "start": 4442.0, "end": 4447.679999999999, "text": " science and artificial intelligence in Birmingham, Birmingham University, so he introduced the concept", "tokens": [50740, 3497, 293, 11677, 7599, 294, 34673, 11, 34673, 3535, 11, 370, 415, 7268, 264, 3410, 51024], "temperature": 0.0, "avg_logprob": -0.11501111984252929, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.012407266534864902}, {"id": 770, "seek": 443448, "start": 4447.679999999999, "end": 4456.16, "text": " of the space of possible minds in an article in 1984. And the idea is that the collection of minds", "tokens": [51024, 295, 264, 1901, 295, 1944, 9634, 294, 364, 7222, 294, 27127, 13, 400, 264, 1558, 307, 300, 264, 5765, 295, 9634, 51448], "temperature": 0.0, "avg_logprob": -0.11501111984252929, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.012407266534864902}, {"id": 771, "seek": 443448, "start": 4456.16, "end": 4462.08, "text": " that could exist in our universe, that do exist in our universe and that could, is much larger than", "tokens": [51448, 300, 727, 2514, 294, 527, 6445, 11, 300, 360, 2514, 294, 527, 6445, 293, 300, 727, 11, 307, 709, 4833, 813, 51744], "temperature": 0.0, "avg_logprob": -0.11501111984252929, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.012407266534864902}, {"id": 772, "seek": 446208, "start": 4462.08, "end": 4469.6, "text": " just human minds or even the minds of humans plus other animals. It encompasses extraterrestrial", "tokens": [50364, 445, 1952, 9634, 420, 754, 264, 9634, 295, 6255, 1804, 661, 4882, 13, 467, 49866, 43324, 34539, 50740], "temperature": 0.0, "avg_logprob": -0.10765872350553186, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008249668404459953}, {"id": 773, "seek": 446208, "start": 4469.6, "end": 4474.5599999999995, "text": " life that might exist out there and it encompasses artificial intelligence that we might create one", "tokens": [50740, 993, 300, 1062, 2514, 484, 456, 293, 309, 49866, 11677, 7599, 300, 321, 1062, 1884, 472, 50988], "temperature": 0.0, "avg_logprob": -0.10765872350553186, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008249668404459953}, {"id": 774, "seek": 446208, "start": 4474.5599999999995, "end": 4480.88, "text": " day. So the whole space of possible minds is a very rich object philosophically speaking and", "tokens": [50988, 786, 13, 407, 264, 1379, 1901, 295, 1944, 9634, 307, 257, 588, 4593, 2657, 14529, 984, 4124, 293, 51304], "temperature": 0.0, "avg_logprob": -0.10765872350553186, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008249668404459953}, {"id": 775, "seek": 446208, "start": 4480.88, "end": 4487.28, "text": " merits our study. Yeah, so Wittgenstein's private language argument is the really the centerpiece", "tokens": [51304, 40923, 527, 2979, 13, 865, 11, 370, 343, 593, 1766, 9089, 311, 4551, 2856, 6770, 307, 264, 534, 264, 3056, 15281, 51624], "temperature": 0.0, "avg_logprob": -0.10765872350553186, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008249668404459953}, {"id": 776, "seek": 448728, "start": 4487.36, "end": 4492.24, "text": " of the philosophical investigations, which is the book that was published after his death,", "tokens": [50368, 295, 264, 25066, 25582, 11, 597, 307, 264, 1446, 300, 390, 6572, 934, 702, 2966, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1282644929557011, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.035873886197805405}, {"id": 777, "seek": 448728, "start": 4492.24, "end": 4502.48, "text": " which really articulates his later phase of philosophy. And it's all about the idea that we", "tokens": [50612, 597, 534, 15228, 26192, 702, 1780, 5574, 295, 10675, 13, 400, 309, 311, 439, 466, 264, 1558, 300, 321, 51124], "temperature": 0.0, "avg_logprob": -0.1282644929557011, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.035873886197805405}, {"id": 778, "seek": 448728, "start": 4502.48, "end": 4509.599999999999, "text": " have, or that we can talk about, private sensations. So things that are purely subjective and that only", "tokens": [51124, 362, 11, 420, 300, 321, 393, 751, 466, 11, 4551, 36642, 13, 407, 721, 300, 366, 17491, 25972, 293, 300, 787, 51480], "temperature": 0.0, "avg_logprob": -0.1282644929557011, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.035873886197805405}, {"id": 779, "seek": 448728, "start": 4510.16, "end": 4516.96, "text": " I as an individual can understand and know what they mean. So what red is for me and just,", "tokens": [51508, 286, 382, 364, 2609, 393, 1223, 293, 458, 437, 436, 914, 13, 407, 437, 2182, 307, 337, 385, 293, 445, 11, 51848], "temperature": 0.0, "avg_logprob": -0.1282644929557011, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.035873886197805405}, {"id": 780, "seek": 451696, "start": 4517.04, "end": 4523.2, "text": " you know, internally for me. And so the private language remarks sort of undermine that very", "tokens": [50368, 291, 458, 11, 19501, 337, 385, 13, 400, 370, 264, 4551, 2856, 19151, 1333, 295, 39257, 300, 588, 50676], "temperature": 0.0, "avg_logprob": -0.09479867893716563, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.002293095923960209}, {"id": 781, "seek": 451696, "start": 4523.2, "end": 4529.68, "text": " conception. So the basic idea is he imagines, he says, well, so what he means by private language", "tokens": [50676, 30698, 13, 407, 264, 3875, 1558, 307, 415, 2576, 1652, 11, 415, 1619, 11, 731, 11, 370, 437, 415, 1355, 538, 4551, 2856, 51000], "temperature": 0.0, "avg_logprob": -0.09479867893716563, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.002293095923960209}, {"id": 782, "seek": 451696, "start": 4529.68, "end": 4536.88, "text": " is very important to kind of get this right. So he doesn't mean a language that I've invented and", "tokens": [51000, 307, 588, 1021, 281, 733, 295, 483, 341, 558, 13, 407, 415, 1177, 380, 914, 257, 2856, 300, 286, 600, 14479, 293, 51360], "temperature": 0.0, "avg_logprob": -0.09479867893716563, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.002293095923960209}, {"id": 783, "seek": 451696, "start": 4536.88, "end": 4542.4800000000005, "text": " that is just something that nobody can understand just because I've invented it. It's the reason", "tokens": [51360, 300, 307, 445, 746, 300, 5079, 393, 1223, 445, 570, 286, 600, 14479, 309, 13, 467, 311, 264, 1778, 51640], "temperature": 0.0, "avg_logprob": -0.09479867893716563, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.002293095923960209}, {"id": 784, "seek": 454248, "start": 4542.48, "end": 4548.32, "text": " that it's a private language is because it's about something which only I can access subjectively,", "tokens": [50364, 300, 309, 311, 257, 4551, 2856, 307, 570, 309, 311, 466, 746, 597, 787, 286, 393, 2105, 3983, 3413, 11, 50656], "temperature": 0.0, "avg_logprob": -0.06337407231330872, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.017627088353037834}, {"id": 785, "seek": 454248, "start": 4548.32, "end": 4556.32, "text": " which is what red is like for me. So it's the idea that you can have a word for that completely", "tokens": [50656, 597, 307, 437, 2182, 307, 411, 337, 385, 13, 407, 309, 311, 264, 1558, 300, 291, 393, 362, 257, 1349, 337, 300, 2584, 51056], "temperature": 0.0, "avg_logprob": -0.06337407231330872, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.017627088353037834}, {"id": 786, "seek": 454248, "start": 4556.32, "end": 4562.799999999999, "text": " internal thing that is just mine. So he says, imagine that I keep a diary and I keep a diary", "tokens": [51056, 6920, 551, 300, 307, 445, 3892, 13, 407, 415, 1619, 11, 3811, 300, 286, 1066, 257, 26492, 293, 286, 1066, 257, 26492, 51380], "temperature": 0.0, "avg_logprob": -0.06337407231330872, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.017627088353037834}, {"id": 787, "seek": 454248, "start": 4562.799999999999, "end": 4571.5199999999995, "text": " and every time I have this experience, a particular experience, then I write s in my diary to label", "tokens": [51380, 293, 633, 565, 286, 362, 341, 1752, 11, 257, 1729, 1752, 11, 550, 286, 2464, 262, 294, 452, 26492, 281, 7645, 51816], "temperature": 0.0, "avg_logprob": -0.06337407231330872, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.017627088353037834}, {"id": 788, "seek": 457152, "start": 4571.52, "end": 4575.6, "text": " that I've had that experience. So maybe I have this, I think I'm having this experience on a", "tokens": [50364, 300, 286, 600, 632, 300, 1752, 13, 407, 1310, 286, 362, 341, 11, 286, 519, 286, 478, 1419, 341, 1752, 322, 257, 50568], "temperature": 0.0, "avg_logprob": -0.08343981829556552, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.015781277790665627}, {"id": 789, "seek": 457152, "start": 4575.6, "end": 4579.92, "text": " particular day and I write s and then a few days later, I think I'm having that experience again,", "tokens": [50568, 1729, 786, 293, 286, 2464, 262, 293, 550, 257, 1326, 1708, 1780, 11, 286, 519, 286, 478, 1419, 300, 1752, 797, 11, 50784], "temperature": 0.0, "avg_logprob": -0.08343981829556552, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.015781277790665627}, {"id": 790, "seek": 457152, "start": 4579.92, "end": 4586.400000000001, "text": " so I write s again. Now, the question he asks is what possible criterion could there be for", "tokens": [50784, 370, 286, 2464, 262, 797, 13, 823, 11, 264, 1168, 415, 8962, 307, 437, 1944, 46691, 727, 456, 312, 337, 51108], "temperature": 0.0, "avg_logprob": -0.08343981829556552, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.015781277790665627}, {"id": 791, "seek": 457152, "start": 4586.400000000001, "end": 4592.160000000001, "text": " the correctness of that word? What would make it actually stand for anything meaningful,", "tokens": [51108, 264, 3006, 1287, 295, 300, 1349, 30, 708, 576, 652, 309, 767, 1463, 337, 1340, 10995, 11, 51396], "temperature": 0.0, "avg_logprob": -0.08343981829556552, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.015781277790665627}, {"id": 792, "seek": 457152, "start": 4592.160000000001, "end": 4598.72, "text": " given that what really makes words meaningful is if they're understandable in a public setting,", "tokens": [51396, 2212, 300, 437, 534, 1669, 2283, 10995, 307, 498, 436, 434, 25648, 294, 257, 1908, 3287, 11, 51724], "temperature": 0.0, "avg_logprob": -0.08343981829556552, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.015781277790665627}, {"id": 793, "seek": 459872, "start": 4598.72, "end": 4603.04, "text": " if they're understandable really to other people. So there can be no kind of criterion", "tokens": [50364, 498, 436, 434, 25648, 534, 281, 661, 561, 13, 407, 456, 393, 312, 572, 733, 295, 46691, 50580], "temperature": 0.0, "avg_logprob": -0.11061266490391322, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.008067079819738865}, {"id": 794, "seek": 459872, "start": 4603.04, "end": 4609.4400000000005, "text": " for correctness that anybody else could validate for this thing insofar as it stands for something", "tokens": [50580, 337, 3006, 1287, 300, 4472, 1646, 727, 29562, 337, 341, 551, 294, 539, 21196, 382, 309, 7382, 337, 746, 50900], "temperature": 0.0, "avg_logprob": -0.11061266490391322, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.008067079819738865}, {"id": 795, "seek": 459872, "start": 4609.4400000000005, "end": 4616.56, "text": " that's completely private. So then there's a whole set of remarks that after he sets up this", "tokens": [50900, 300, 311, 2584, 4551, 13, 407, 550, 456, 311, 257, 1379, 992, 295, 19151, 300, 934, 415, 6352, 493, 341, 51256], "temperature": 0.0, "avg_logprob": -0.11061266490391322, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.008067079819738865}, {"id": 796, "seek": 459872, "start": 4616.56, "end": 4624.320000000001, "text": " sort of little thought experiment that tell you the implications of it really. And there's one", "tokens": [51256, 1333, 295, 707, 1194, 5120, 300, 980, 291, 264, 16602, 295, 309, 534, 13, 400, 456, 311, 472, 51644], "temperature": 0.0, "avg_logprob": -0.11061266490391322, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.008067079819738865}, {"id": 797, "seek": 462432, "start": 4624.32, "end": 4630.96, "text": " really, really key phrase where Wittgenstein is always engaging with an imaginary interlocutor,", "tokens": [50364, 534, 11, 534, 2141, 9535, 689, 343, 593, 1766, 9089, 307, 1009, 11268, 365, 364, 26164, 728, 5842, 22163, 11, 50696], "temperature": 0.0, "avg_logprob": -0.09088904571533203, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.16494882106781006}, {"id": 798, "seek": 462432, "start": 4630.96, "end": 4636.96, "text": " so an imaginary person who's arguing with him in the book. And so he's imagining this person says,", "tokens": [50696, 370, 364, 26164, 954, 567, 311, 19697, 365, 796, 294, 264, 1446, 13, 400, 370, 415, 311, 27798, 341, 954, 1619, 11, 50996], "temperature": 0.0, "avg_logprob": -0.09088904571533203, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.16494882106781006}, {"id": 799, "seek": 462432, "start": 4636.96, "end": 4641.759999999999, "text": " but aren't you saying that the sensation itself is just a nothing? Aren't you a kind of behaviorist?", "tokens": [50996, 457, 3212, 380, 291, 1566, 300, 264, 20069, 2564, 307, 445, 257, 1825, 30, 15464, 380, 291, 257, 733, 295, 5223, 468, 30, 51236], "temperature": 0.0, "avg_logprob": -0.09088904571533203, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.16494882106781006}, {"id": 800, "seek": 462432, "start": 4641.759999999999, "end": 4647.5199999999995, "text": " You're just saying that it's a nothing. And his answer to this, well, I'm not saying it's a nothing", "tokens": [51236, 509, 434, 445, 1566, 300, 309, 311, 257, 1825, 13, 400, 702, 1867, 281, 341, 11, 731, 11, 286, 478, 406, 1566, 309, 311, 257, 1825, 51524], "temperature": 0.0, "avg_logprob": -0.09088904571533203, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.16494882106781006}, {"id": 801, "seek": 462432, "start": 4647.5199999999995, "end": 4652.88, "text": " and I'm not saying it's a something either. The point was only that a nothing would serve as well", "tokens": [51524, 293, 286, 478, 406, 1566, 309, 311, 257, 746, 2139, 13, 440, 935, 390, 787, 300, 257, 1825, 576, 4596, 382, 731, 51792], "temperature": 0.0, "avg_logprob": -0.09088904571533203, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.16494882106781006}, {"id": 802, "seek": 465288, "start": 4652.88, "end": 4660.24, "text": " as a something about which nothing can be said. And that little kind of paradoxical sounding,", "tokens": [50364, 382, 257, 746, 466, 597, 1825, 393, 312, 848, 13, 400, 300, 707, 733, 295, 26221, 804, 24931, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08851188282633937, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00872857216745615}, {"id": 803, "seek": 465288, "start": 4660.24, "end": 4664.88, "text": " weird sounding statement encapsulates something really, really, really profound. And I think when", "tokens": [50732, 3657, 24931, 5629, 38745, 26192, 746, 534, 11, 534, 11, 534, 14382, 13, 400, 286, 519, 562, 50964], "temperature": 0.0, "avg_logprob": -0.08851188282633937, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00872857216745615}, {"id": 804, "seek": 465288, "start": 4664.88, "end": 4670.64, "text": " I first really kind of understood what he was getting at there, it had a really dramatic shift", "tokens": [50964, 286, 700, 534, 733, 295, 7320, 437, 415, 390, 1242, 412, 456, 11, 309, 632, 257, 534, 12023, 5513, 51252], "temperature": 0.0, "avg_logprob": -0.08851188282633937, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00872857216745615}, {"id": 805, "seek": 465288, "start": 4670.64, "end": 4678.24, "text": " in the way I thought about consciousness, subjectivity. And to my mind, it is the thing that", "tokens": [51252, 294, 264, 636, 286, 1194, 466, 10081, 11, 3983, 4253, 13, 400, 281, 452, 1575, 11, 309, 307, 264, 551, 300, 51632], "temperature": 0.0, "avg_logprob": -0.08851188282633937, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.00872857216745615}, {"id": 806, "seek": 467824, "start": 4678.24, "end": 4684.639999999999, "text": " really undermines dualism. It's the most powerful way to undermine the dualistic intuitions that we", "tokens": [50364, 534, 24188, 1652, 11848, 1434, 13, 467, 311, 264, 881, 4005, 636, 281, 39257, 264, 11848, 3142, 16224, 626, 300, 321, 50684], "temperature": 0.0, "avg_logprob": -0.06553256027097625, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0048751551657915115}, {"id": 807, "seek": 467824, "start": 4684.639999999999, "end": 4690.24, "text": " have and that date back to Descartes and before Descartes that were articulated very well by Descartes.", "tokens": [50684, 362, 293, 300, 4002, 646, 281, 3885, 44672, 279, 293, 949, 3885, 44672, 279, 300, 645, 43322, 588, 731, 538, 3885, 44672, 279, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06553256027097625, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0048751551657915115}, {"id": 808, "seek": 467824, "start": 4690.24, "end": 4695.5199999999995, "text": " Many of my friends are fans of Wittgenstein, but they are also fans of subjectivity. So as you", "tokens": [50964, 5126, 295, 452, 1855, 366, 4499, 295, 343, 593, 1766, 9089, 11, 457, 436, 366, 611, 4499, 295, 3983, 4253, 13, 407, 382, 291, 51228], "temperature": 0.0, "avg_logprob": -0.06553256027097625, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0048751551657915115}, {"id": 809, "seek": 467824, "start": 4695.5199999999995, "end": 4700.96, "text": " were just alluding to what Wittgenstein did was he created this kind of barrier between the inner", "tokens": [51228, 645, 445, 439, 33703, 281, 437, 343, 593, 1766, 9089, 630, 390, 415, 2942, 341, 733, 295, 13357, 1296, 264, 7284, 51500], "temperature": 0.0, "avg_logprob": -0.06553256027097625, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0048751551657915115}, {"id": 810, "seek": 467824, "start": 4700.96, "end": 4706.08, "text": " and the outer. He said, you know, for things to be promoted into the language game for this", "tokens": [51500, 293, 264, 10847, 13, 634, 848, 11, 291, 458, 11, 337, 721, 281, 312, 21162, 666, 264, 2856, 1216, 337, 341, 51756], "temperature": 0.0, "avg_logprob": -0.06553256027097625, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0048751551657915115}, {"id": 811, "seek": 470608, "start": 4706.08, "end": 4710.0, "text": " emergent structure that we have, you know, when we memetically share all of these language", "tokens": [50364, 4345, 6930, 3877, 300, 321, 362, 11, 291, 458, 11, 562, 321, 1334, 22652, 2073, 439, 295, 613, 2856, 50560], "temperature": 0.0, "avg_logprob": -0.09082513336741596, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.035909395664930344}, {"id": 812, "seek": 470608, "start": 4710.0, "end": 4715.92, "text": " constructions, that can only come from something observable. But it doesn't seem inconceivable", "tokens": [50560, 7690, 626, 11, 300, 393, 787, 808, 490, 746, 9951, 712, 13, 583, 309, 1177, 380, 1643, 20972, 384, 34376, 50856], "temperature": 0.0, "avg_logprob": -0.09082513336741596, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.035909395664930344}, {"id": 813, "seek": 470608, "start": 4715.92, "end": 4721.12, "text": " to me that it could in principle come from something private. So for example, you might", "tokens": [50856, 281, 385, 300, 309, 727, 294, 8665, 808, 490, 746, 4551, 13, 407, 337, 1365, 11, 291, 1062, 51116], "temperature": 0.0, "avg_logprob": -0.09082513336741596, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.035909395664930344}, {"id": 814, "seek": 470608, "start": 4721.12, "end": 4725.5199999999995, "text": " have a drugs experience and that's clearly ineffable, you can't find the words, but there are", "tokens": [51116, 362, 257, 7766, 1752, 293, 300, 311, 4448, 7167, 602, 712, 11, 291, 393, 380, 915, 264, 2283, 11, 457, 456, 366, 51336], "temperature": 0.0, "avg_logprob": -0.09082513336741596, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.035909395664930344}, {"id": 815, "seek": 470608, "start": 4725.5199999999995, "end": 4731.12, "text": " things that have some semantic overlap. So I experience red, you experience red, we both", "tokens": [51336, 721, 300, 362, 512, 47982, 19959, 13, 407, 286, 1752, 2182, 11, 291, 1752, 2182, 11, 321, 1293, 51616], "temperature": 0.0, "avg_logprob": -0.09082513336741596, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.035909395664930344}, {"id": 816, "seek": 473112, "start": 4731.12, "end": 4737.04, "text": " have different experiences, yet when we talk about them, some kind of overlapping category", "tokens": [50364, 362, 819, 5235, 11, 1939, 562, 321, 751, 466, 552, 11, 512, 733, 295, 33535, 7719, 50660], "temperature": 0.0, "avg_logprob": -0.13086976779727486, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.03462599590420723}, {"id": 817, "seek": 473112, "start": 4737.04, "end": 4743.599999999999, "text": " still emerges in the public space. Yes, absolutely. So so what emerges in the public space, that is", "tokens": [50660, 920, 38965, 294, 264, 1908, 1901, 13, 1079, 11, 3122, 13, 407, 370, 437, 38965, 294, 264, 1908, 1901, 11, 300, 307, 50988], "temperature": 0.0, "avg_logprob": -0.13086976779727486, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.03462599590420723}, {"id": 818, "seek": 473112, "start": 4743.599999999999, "end": 4749.599999999999, "text": " what we can talk about. And that is that is by the way you've set up the experiment is by definition,", "tokens": [50988, 437, 321, 393, 751, 466, 13, 400, 300, 307, 300, 307, 538, 264, 636, 291, 600, 992, 493, 264, 5120, 307, 538, 7123, 11, 51288], "temperature": 0.0, "avg_logprob": -0.13086976779727486, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.03462599590420723}, {"id": 819, "seek": 473112, "start": 4749.599999999999, "end": 4753.76, "text": " not private, it's public. So of course, we can both talk, we can both point at something that's", "tokens": [51288, 406, 4551, 11, 309, 311, 1908, 13, 407, 295, 1164, 11, 321, 393, 1293, 751, 11, 321, 393, 1293, 935, 412, 746, 300, 311, 51496], "temperature": 0.0, "avg_logprob": -0.13086976779727486, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.03462599590420723}, {"id": 820, "seek": 473112, "start": 4753.76, "end": 4757.92, "text": " red and say, oh, look, look at that red. And you say, oh, yeah, isn't that isn't it beautiful?", "tokens": [51496, 2182, 293, 584, 11, 1954, 11, 574, 11, 574, 412, 300, 2182, 13, 400, 291, 584, 11, 1954, 11, 1338, 11, 1943, 380, 300, 1943, 380, 309, 2238, 30, 51704], "temperature": 0.0, "avg_logprob": -0.13086976779727486, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.03462599590420723}, {"id": 821, "seek": 475792, "start": 4758.72, "end": 4762.32, "text": " There, it's manifestly, we're talking, insofar as we're talking and successful in", "tokens": [50404, 821, 11, 309, 311, 10067, 356, 11, 321, 434, 1417, 11, 294, 539, 21196, 382, 321, 434, 1417, 293, 4406, 294, 50584], "temperature": 0.0, "avg_logprob": -0.13547516744071192, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.0020534000359475613}, {"id": 822, "seek": 475792, "start": 4762.32, "end": 4767.28, "text": " communicating with each other and agreeing with each other, then that's the element that is indeed", "tokens": [50584, 17559, 365, 1184, 661, 293, 36900, 365, 1184, 661, 11, 550, 300, 311, 264, 4478, 300, 307, 6451, 50832], "temperature": 0.0, "avg_logprob": -0.13547516744071192, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.0020534000359475613}, {"id": 823, "seek": 475792, "start": 4767.28, "end": 4774.8, "text": " public. But are we not sharing, you know, is language not a set of pointers to our simulation?", "tokens": [50832, 1908, 13, 583, 366, 321, 406, 5414, 11, 291, 458, 11, 307, 2856, 406, 257, 992, 295, 44548, 281, 527, 16575, 30, 51208], "temperature": 0.0, "avg_logprob": -0.13547516744071192, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.0020534000359475613}, {"id": 824, "seek": 475792, "start": 4774.8, "end": 4780.8, "text": " So we're simulation sharing when we talk. And even though our simulations are different,", "tokens": [51208, 407, 321, 434, 16575, 5414, 562, 321, 751, 13, 400, 754, 1673, 527, 35138, 366, 819, 11, 51508], "temperature": 0.0, "avg_logprob": -0.13547516744071192, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.0020534000359475613}, {"id": 825, "seek": 475792, "start": 4781.6, "end": 4787.12, "text": " is the pointer, does the pointer not form some kind of category over all of our simulations?", "tokens": [51548, 307, 264, 23918, 11, 775, 264, 23918, 406, 1254, 512, 733, 295, 7719, 670, 439, 295, 527, 35138, 30, 51824], "temperature": 0.0, "avg_logprob": -0.13547516744071192, "compression_ratio": 1.8207171314741035, "no_speech_prob": 0.0020534000359475613}, {"id": 826, "seek": 478712, "start": 4787.12, "end": 4791.44, "text": " Oh, well, there's a whole, you've introduced a whole load of terminology there, which,", "tokens": [50364, 876, 11, 731, 11, 456, 311, 257, 1379, 11, 291, 600, 7268, 257, 1379, 3677, 295, 27575, 456, 11, 597, 11, 50580], "temperature": 0.0, "avg_logprob": -0.14952780983664773, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.003766048699617386}, {"id": 827, "seek": 478712, "start": 4792.16, "end": 4798.5599999999995, "text": " which, you know, I don't know what you mean exactly by shared simulation and so on. So I think", "tokens": [50616, 597, 11, 291, 458, 11, 286, 500, 380, 458, 437, 291, 914, 2293, 538, 5507, 16575, 293, 370, 322, 13, 407, 286, 519, 50936], "temperature": 0.0, "avg_logprob": -0.14952780983664773, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.003766048699617386}, {"id": 828, "seek": 478712, "start": 4798.5599999999995, "end": 4803.36, "text": " in the context of a philosophical discussion, as soon as you introduce new, new bits of", "tokens": [50936, 294, 264, 4319, 295, 257, 25066, 5017, 11, 382, 2321, 382, 291, 5366, 777, 11, 777, 9239, 295, 51176], "temperature": 0.0, "avg_logprob": -0.14952780983664773, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.003766048699617386}, {"id": 829, "seek": 478712, "start": 4803.36, "end": 4807.44, "text": " terminology like that, then often that's the point at which you're starting to go wrong,", "tokens": [51176, 27575, 411, 300, 11, 550, 2049, 300, 311, 264, 935, 412, 597, 291, 434, 2891, 281, 352, 2085, 11, 51380], "temperature": 0.0, "avg_logprob": -0.14952780983664773, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.003766048699617386}, {"id": 830, "seek": 478712, "start": 4807.44, "end": 4812.72, "text": " right, in philosophical discussions. In technical discussions, of course, you of course,", "tokens": [51380, 558, 11, 294, 25066, 11088, 13, 682, 6191, 11088, 11, 295, 1164, 11, 291, 295, 1164, 11, 51644], "temperature": 0.0, "avg_logprob": -0.14952780983664773, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.003766048699617386}, {"id": 831, "seek": 481272, "start": 4812.8, "end": 4817.4400000000005, "text": " we're going to introduce new terminology all the time. But, but, but that's the moment where", "tokens": [50368, 321, 434, 516, 281, 5366, 777, 27575, 439, 264, 565, 13, 583, 11, 457, 11, 457, 300, 311, 264, 1623, 689, 50600], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 832, "seek": 481272, "start": 4817.4400000000005, "end": 4822.56, "text": " often things are going when, as Wittgenstein would say, you're starting to take language on holiday", "tokens": [50600, 2049, 721, 366, 516, 562, 11, 382, 343, 593, 1766, 9089, 576, 584, 11, 291, 434, 2891, 281, 747, 2856, 322, 9960, 50856], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 833, "seek": 481272, "start": 4822.56, "end": 4826.72, "text": " and take it away from its normal usage. So I don't know what you mean by kind of a shared", "tokens": [50856, 293, 747, 309, 1314, 490, 1080, 2710, 14924, 13, 407, 286, 500, 380, 458, 437, 291, 914, 538, 733, 295, 257, 5507, 51064], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 834, "seek": 481272, "start": 4826.72, "end": 4831.4400000000005, "text": " simulation, you'd have to tell me a little bit more about that idea before I could engage with", "tokens": [51064, 16575, 11, 291, 1116, 362, 281, 980, 385, 257, 707, 857, 544, 466, 300, 1558, 949, 286, 727, 4683, 365, 51300], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 835, "seek": 481272, "start": 4831.4400000000005, "end": 4835.84, "text": " that thought experiment, I think. Well, I mean, I'm schooled on, you know, the Karl Fristons of", "tokens": [51300, 300, 1194, 5120, 11, 286, 519, 13, 1042, 11, 286, 914, 11, 286, 478, 1395, 292, 322, 11, 291, 458, 11, 264, 20405, 1526, 468, 892, 295, 51520], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 836, "seek": 481272, "start": 4835.84, "end": 4839.92, "text": " this world. And there's this whole thing about the Bayesian brain and perception as inference and", "tokens": [51520, 341, 1002, 13, 400, 456, 311, 341, 1379, 551, 466, 264, 7840, 42434, 3567, 293, 12860, 382, 38253, 293, 51724], "temperature": 0.0, "avg_logprob": -0.10179699864880792, "compression_ratio": 1.6744868035190617, "no_speech_prob": 0.022604815661907196}, {"id": 837, "seek": 483992, "start": 4839.92, "end": 4845.68, "text": " so on. And, you know, the basic idea is that we, you know, our everyday experience is a hallucination,", "tokens": [50364, 370, 322, 13, 400, 11, 291, 458, 11, 264, 3875, 1558, 307, 300, 321, 11, 291, 458, 11, 527, 7429, 1752, 307, 257, 35212, 2486, 11, 50652], "temperature": 0.0, "avg_logprob": -0.14329448823005922, "compression_ratio": 1.876, "no_speech_prob": 0.00524374982342124}, {"id": 838, "seek": 483992, "start": 4845.68, "end": 4851.52, "text": " you know, we don't, what we experience isn't necessarily what is out there. And language", "tokens": [50652, 291, 458, 11, 321, 500, 380, 11, 437, 321, 1752, 1943, 380, 4725, 437, 307, 484, 456, 13, 400, 2856, 50944], "temperature": 0.0, "avg_logprob": -0.14329448823005922, "compression_ratio": 1.876, "no_speech_prob": 0.00524374982342124}, {"id": 839, "seek": 483992, "start": 4851.52, "end": 4856.56, "text": " is is a kind of pointed to those simulations. And they must be divergent, they presumably", "tokens": [50944, 307, 307, 257, 733, 295, 10932, 281, 729, 35138, 13, 400, 436, 1633, 312, 18558, 6930, 11, 436, 26742, 51196], "temperature": 0.0, "avg_logprob": -0.14329448823005922, "compression_ratio": 1.876, "no_speech_prob": 0.00524374982342124}, {"id": 840, "seek": 483992, "start": 4856.56, "end": 4861.04, "text": " are divergent, yet miraculously, we can understand each other. Yeah, well, I think that so that so", "tokens": [51196, 366, 18558, 6930, 11, 1939, 30686, 25038, 11, 321, 393, 1223, 1184, 661, 13, 865, 11, 731, 11, 286, 519, 300, 370, 300, 370, 51420], "temperature": 0.0, "avg_logprob": -0.14329448823005922, "compression_ratio": 1.876, "no_speech_prob": 0.00524374982342124}, {"id": 841, "seek": 483992, "start": 4861.04, "end": 4869.36, "text": " the Wittgensteinian point is that we understand each other in so far as in so far as we,", "tokens": [51420, 264, 343, 593, 1766, 9089, 952, 935, 307, 300, 321, 1223, 1184, 661, 294, 370, 1400, 382, 294, 370, 1400, 382, 321, 11, 51836], "temperature": 0.0, "avg_logprob": -0.14329448823005922, "compression_ratio": 1.876, "no_speech_prob": 0.00524374982342124}, {"id": 842, "seek": 486936, "start": 4869.839999999999, "end": 4875.04, "text": " you know, what we understand is what is shared, right? And anything outside of that is,", "tokens": [50388, 291, 458, 11, 437, 321, 1223, 307, 437, 307, 5507, 11, 558, 30, 400, 1340, 2380, 295, 300, 307, 11, 50648], "temperature": 0.0, "avg_logprob": -0.0999849352062258, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.0010944041423499584}, {"id": 843, "seek": 486936, "start": 4876.4, "end": 4882.16, "text": " you know, we by definition can't talk about. And the difficulty is that we have this strong", "tokens": [50716, 291, 458, 11, 321, 538, 7123, 393, 380, 751, 466, 13, 400, 264, 10360, 307, 300, 321, 362, 341, 2068, 51004], "temperature": 0.0, "avg_logprob": -0.0999849352062258, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.0010944041423499584}, {"id": 844, "seek": 486936, "start": 4882.16, "end": 4888.719999999999, "text": " inclination to talk as if there is this thing that's not shared. I mean, what really fascinates me", "tokens": [51004, 37070, 2486, 281, 751, 382, 498, 456, 307, 341, 551, 300, 311, 406, 5507, 13, 286, 914, 11, 437, 534, 7184, 259, 1024, 385, 51332], "temperature": 0.0, "avg_logprob": -0.0999849352062258, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.0010944041423499584}, {"id": 845, "seek": 486936, "start": 4888.719999999999, "end": 4893.759999999999, "text": " is that understanding it's not a binary, there's a spectrum, and we delude ourselves that we", "tokens": [51332, 307, 300, 3701, 309, 311, 406, 257, 17434, 11, 456, 311, 257, 11143, 11, 293, 321, 1103, 2303, 4175, 300, 321, 51584], "temperature": 0.0, "avg_logprob": -0.0999849352062258, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.0010944041423499584}, {"id": 846, "seek": 486936, "start": 4893.759999999999, "end": 4898.88, "text": " understand things deeper than we do, because it goes into the realm of subjectivity. So when I", "tokens": [51584, 1223, 721, 7731, 813, 321, 360, 11, 570, 309, 1709, 666, 264, 15355, 295, 3983, 4253, 13, 407, 562, 286, 51840], "temperature": 0.0, "avg_logprob": -0.0999849352062258, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.0010944041423499584}, {"id": 847, "seek": 489888, "start": 4899.2, "end": 4905.12, "text": " understand something, my brain is invoking all of this rich subject of experience. And I'm probably", "tokens": [50380, 1223, 746, 11, 452, 3567, 307, 1048, 5953, 439, 295, 341, 4593, 3983, 295, 1752, 13, 400, 286, 478, 1391, 50676], "temperature": 0.0, "avg_logprob": -0.1378946304321289, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.004084809683263302}, {"id": 848, "seek": 489888, "start": 4905.12, "end": 4909.84, "text": " taking my understanding into a domain which is beyond which that you understood. And perhaps", "tokens": [50676, 1940, 452, 3701, 666, 257, 9274, 597, 307, 4399, 597, 300, 291, 7320, 13, 400, 4317, 50912], "temperature": 0.0, "avg_logprob": -0.1378946304321289, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.004084809683263302}, {"id": 849, "seek": 489888, "start": 4909.84, "end": 4914.56, "text": " this is just something we willfully do all of the time. So what do you mean exactly by invoke my", "tokens": [50912, 341, 307, 445, 746, 321, 486, 2277, 360, 439, 295, 264, 565, 13, 407, 437, 360, 291, 914, 2293, 538, 41117, 452, 51148], "temperature": 0.0, "avg_logprob": -0.1378946304321289, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.004084809683263302}, {"id": 850, "seek": 489888, "start": 4914.56, "end": 4918.32, "text": " brain is invoking all this subject of experience? What do you what are you what are you getting", "tokens": [51148, 3567, 307, 1048, 5953, 439, 341, 3983, 295, 1752, 30, 708, 360, 291, 437, 366, 291, 437, 366, 291, 1242, 51336], "temperature": 0.0, "avg_logprob": -0.1378946304321289, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.004084809683263302}, {"id": 851, "seek": 489888, "start": 4918.32, "end": 4924.08, "text": " out there? Well, so we talk about, as you say, the language game is based around public information.", "tokens": [51336, 484, 456, 30, 1042, 11, 370, 321, 751, 466, 11, 382, 291, 584, 11, 264, 2856, 1216, 307, 2361, 926, 1908, 1589, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1378946304321289, "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.004084809683263302}, {"id": 852, "seek": 492408, "start": 4924.08, "end": 4930.8, "text": " So there is a kind of cultural level, a lowest common denominator of understanding. But when we", "tokens": [50364, 407, 456, 307, 257, 733, 295, 6988, 1496, 11, 257, 12437, 2689, 20687, 295, 3701, 13, 583, 562, 321, 50700], "temperature": 0.0, "avg_logprob": -0.09018223544201219, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0010530805448070168}, {"id": 853, "seek": 492408, "start": 4930.8, "end": 4938.0, "text": " understand cultural artifacts, we further invoke our own subjective experiences. So for example,", "tokens": [50700, 1223, 6988, 24617, 11, 321, 3052, 41117, 527, 1065, 25972, 5235, 13, 407, 337, 1365, 11, 51060], "temperature": 0.0, "avg_logprob": -0.09018223544201219, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0010530805448070168}, {"id": 854, "seek": 492408, "start": 4938.0, "end": 4944.0, "text": " when I laugh, I have the experience of laughter, this phenomenal experience. And this is clearly a", "tokens": [51060, 562, 286, 5801, 11, 286, 362, 264, 1752, 295, 13092, 11, 341, 17778, 1752, 13, 400, 341, 307, 4448, 257, 51360], "temperature": 0.0, "avg_logprob": -0.09018223544201219, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0010530805448070168}, {"id": 855, "seek": 492408, "start": 4944.0, "end": 4948.8, "text": " form of understanding, it's a subjective form of understanding. And when someone else laughs,", "tokens": [51360, 1254, 295, 3701, 11, 309, 311, 257, 25972, 1254, 295, 3701, 13, 400, 562, 1580, 1646, 6197, 11, 51600], "temperature": 0.0, "avg_logprob": -0.09018223544201219, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0010530805448070168}, {"id": 856, "seek": 494880, "start": 4948.8, "end": 4954.08, "text": " I feel that we are sharing this ontology, right, we're sharing it, but we can't possibly be.", "tokens": [50364, 286, 841, 300, 321, 366, 5414, 341, 6592, 1793, 11, 558, 11, 321, 434, 5414, 309, 11, 457, 321, 393, 380, 6264, 312, 13, 50628], "temperature": 0.0, "avg_logprob": -0.12573754683784816, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.014171408489346504}, {"id": 857, "seek": 494880, "start": 4955.2, "end": 4960.4800000000005, "text": " Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're", "tokens": [50684, 1042, 11, 370, 286, 914, 11, 291, 434, 2997, 1314, 15424, 439, 3685, 295, 4074, 751, 510, 11, 558, 30, 407, 321, 434, 50948], "temperature": 0.0, "avg_logprob": -0.12573754683784816, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.014171408489346504}, {"id": 858, "seek": 494880, "start": 4960.4800000000005, "end": 4965.76, "text": " sharing an ontology when you're just talking about an everyday experience of laughing together,", "tokens": [50948, 5414, 364, 6592, 1793, 562, 291, 434, 445, 1417, 466, 364, 7429, 1752, 295, 5059, 1214, 11, 51212], "temperature": 0.0, "avg_logprob": -0.12573754683784816, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.014171408489346504}, {"id": 859, "seek": 494880, "start": 4965.76, "end": 4969.4400000000005, "text": " which we can talk about without any kind of difficulty, and without raising any kind of", "tokens": [51212, 597, 321, 393, 751, 466, 1553, 604, 733, 295, 10360, 11, 293, 1553, 11225, 604, 733, 295, 51396], "temperature": 0.0, "avg_logprob": -0.12573754683784816, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.014171408489346504}, {"id": 860, "seek": 494880, "start": 4969.4400000000005, "end": 4974.400000000001, "text": " philosophical problems, just by saying, Well, you know, we both heard that that that joke, and we", "tokens": [51396, 25066, 2740, 11, 445, 538, 1566, 11, 1042, 11, 291, 458, 11, 321, 1293, 2198, 300, 300, 300, 7647, 11, 293, 321, 51644], "temperature": 0.0, "avg_logprob": -0.12573754683784816, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.014171408489346504}, {"id": 861, "seek": 497440, "start": 4974.799999999999, "end": 4978.639999999999, "text": " were both, you know, on the floor and laughing. It was so funny. It was an excellent joke, right?", "tokens": [50384, 645, 1293, 11, 291, 458, 11, 322, 264, 4123, 293, 5059, 13, 467, 390, 370, 4074, 13, 467, 390, 364, 7103, 7647, 11, 558, 30, 50576], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 862, "seek": 497440, "start": 4978.639999999999, "end": 4983.599999999999, "text": " We can talk about that in everyday terms. And and there are no problems. There are no philosophical", "tokens": [50576, 492, 393, 751, 466, 300, 294, 7429, 2115, 13, 400, 293, 456, 366, 572, 2740, 13, 821, 366, 572, 25066, 50824], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 863, "seek": 497440, "start": 4983.599999999999, "end": 4988.48, "text": " problems. But as soon as you start, start, you know, getting philosophical, and you start talking", "tokens": [50824, 2740, 13, 583, 382, 2321, 382, 291, 722, 11, 722, 11, 291, 458, 11, 1242, 25066, 11, 293, 291, 722, 1417, 51068], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 864, "seek": 497440, "start": 4988.48, "end": 4993.12, "text": " about that, you know, what was it? What was your phrase? There's something about subject", "tokens": [51068, 466, 300, 11, 291, 458, 11, 437, 390, 309, 30, 708, 390, 428, 9535, 30, 821, 311, 746, 466, 3983, 51300], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 865, "seek": 497440, "start": 4993.12, "end": 4997.599999999999, "text": " sheds about subjective ontology or something. Yeah, you're introducing all of this kind of", "tokens": [51300, 402, 5147, 466, 25972, 6592, 1793, 420, 746, 13, 865, 11, 291, 434, 15424, 439, 295, 341, 733, 295, 51524], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 866, "seek": 497440, "start": 4997.599999999999, "end": 5002.96, "text": " technical terminology. And that's that whole, that's a whole layer of confusion on top of our", "tokens": [51524, 6191, 27575, 13, 400, 300, 311, 300, 1379, 11, 300, 311, 257, 1379, 4583, 295, 15075, 322, 1192, 295, 527, 51792], "temperature": 0.0, "avg_logprob": -0.14942659062447308, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.01647503487765789}, {"id": 867, "seek": 500296, "start": 5003.04, "end": 5006.64, "text": " ordinary everyday ways of talking about these things, which are unproblematic.", "tokens": [50368, 10547, 7429, 2098, 295, 1417, 466, 613, 721, 11, 597, 366, 517, 47419, 2399, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10138869886638738, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0011327625252306461}, {"id": 868, "seek": 500296, "start": 5007.6, "end": 5013.76, "text": " Okay, but then there's the anthropomorphic lens. So you're a human, we both laugh, the behavior of", "tokens": [50596, 1033, 11, 457, 550, 456, 311, 264, 22727, 32702, 299, 6765, 13, 407, 291, 434, 257, 1952, 11, 321, 1293, 5801, 11, 264, 5223, 295, 50904], "temperature": 0.0, "avg_logprob": -0.10138869886638738, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0011327625252306461}, {"id": 869, "seek": 500296, "start": 5013.76, "end": 5018.24, "text": " laughing is publicly observable. Therefore, we have the same experience, because we have the same", "tokens": [50904, 5059, 307, 14843, 9951, 712, 13, 7504, 11, 321, 362, 264, 912, 1752, 11, 570, 321, 362, 264, 912, 51128], "temperature": 0.0, "avg_logprob": -0.10138869886638738, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0011327625252306461}, {"id": 870, "seek": 500296, "start": 5018.24, "end": 5024.16, "text": " behavior. Well, it depends what you mean by by understand here. So so so for sure, you know,", "tokens": [51128, 5223, 13, 1042, 11, 309, 5946, 437, 291, 914, 538, 538, 1223, 510, 13, 407, 370, 370, 337, 988, 11, 291, 458, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10138869886638738, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0011327625252306461}, {"id": 871, "seek": 500296, "start": 5024.88, "end": 5030.08, "text": " it is a fairly common form of speech to say, to say, Oh, well, you know, you can never understand", "tokens": [51460, 309, 307, 257, 6457, 2689, 1254, 295, 6218, 281, 584, 11, 281, 584, 11, 876, 11, 731, 11, 291, 458, 11, 291, 393, 1128, 1223, 51720], "temperature": 0.0, "avg_logprob": -0.10138869886638738, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0011327625252306461}, {"id": 872, "seek": 503008, "start": 5030.08, "end": 5035.12, "text": " what it was like to give birth, because you're a man, you know, and this is of course, this is a", "tokens": [50364, 437, 309, 390, 411, 281, 976, 3965, 11, 570, 291, 434, 257, 587, 11, 291, 458, 11, 293, 341, 307, 295, 1164, 11, 341, 307, 257, 50616], "temperature": 0.0, "avg_logprob": -0.09661806106567383, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.007395410444587469}, {"id": 873, "seek": 503008, "start": 5035.12, "end": 5044.24, "text": " normal way of expressing oneself. And again, that's that sort of unproblematic. So there's", "tokens": [50616, 2710, 636, 295, 22171, 32265, 13, 400, 797, 11, 300, 311, 300, 1333, 295, 517, 47419, 2399, 13, 407, 456, 311, 51072], "temperature": 0.0, "avg_logprob": -0.09661806106567383, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.007395410444587469}, {"id": 874, "seek": 503008, "start": 5044.24, "end": 5049.5199999999995, "text": " there is a sense in which, you know, in which that's that's undoubtedly true. But the problems", "tokens": [51072, 456, 307, 257, 2020, 294, 597, 11, 291, 458, 11, 294, 597, 300, 311, 300, 311, 35211, 2074, 13, 583, 264, 2740, 51336], "temperature": 0.0, "avg_logprob": -0.09661806106567383, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.007395410444587469}, {"id": 875, "seek": 503008, "start": 5049.5199999999995, "end": 5056.4, "text": " arise when you when you start to, to, to think that this, that what underlies this difference", "tokens": [51336, 20288, 562, 291, 562, 291, 722, 281, 11, 281, 11, 281, 519, 300, 341, 11, 300, 437, 833, 24119, 341, 2649, 51680], "temperature": 0.0, "avg_logprob": -0.09661806106567383, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.007395410444587469}, {"id": 876, "seek": 505640, "start": 5056.4, "end": 5063.839999999999, "text": " in understanding or the one underlies that way of talking is is is some kind of, you know,", "tokens": [50364, 294, 3701, 420, 264, 472, 833, 24119, 300, 636, 295, 1417, 307, 307, 307, 512, 733, 295, 11, 291, 458, 11, 50736], "temperature": 0.0, "avg_logprob": -0.139572587124137, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0015250275610014796}, {"id": 877, "seek": 505640, "start": 5063.839999999999, "end": 5071.04, "text": " inner private realm that is, you know, that is index that is that is metaphysically distinct from", "tokens": [50736, 7284, 4551, 15355, 300, 307, 11, 291, 458, 11, 300, 307, 8186, 300, 307, 300, 307, 30946, 749, 984, 10644, 490, 51096], "temperature": 0.0, "avg_logprob": -0.139572587124137, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0015250275610014796}, {"id": 878, "seek": 505640, "start": 5071.599999999999, "end": 5076.24, "text": " from the rest of reality. When we share these pointers or these symbols or whatever,", "tokens": [51124, 490, 264, 1472, 295, 4103, 13, 1133, 321, 2073, 613, 44548, 420, 613, 16944, 420, 2035, 11, 51356], "temperature": 0.0, "avg_logprob": -0.139572587124137, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0015250275610014796}, {"id": 879, "seek": 505640, "start": 5077.36, "end": 5082.719999999999, "text": " structure still emerges, we still feel that we have a shared understanding. And that understanding", "tokens": [51412, 3877, 920, 38965, 11, 321, 920, 841, 300, 321, 362, 257, 5507, 3701, 13, 400, 300, 3701, 51680], "temperature": 0.0, "avg_logprob": -0.139572587124137, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0015250275610014796}, {"id": 880, "seek": 508272, "start": 5082.72, "end": 5087.04, "text": " can probably be factorized into a public component and a private component. I don't think", "tokens": [50364, 393, 1391, 312, 5952, 1602, 666, 257, 1908, 6542, 293, 257, 4551, 6542, 13, 286, 500, 380, 519, 50580], "temperature": 0.0, "avg_logprob": -0.11417526404062907, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.016365813091397285}, {"id": 881, "seek": 508272, "start": 5087.04, "end": 5092.400000000001, "text": " that's kooky to say that. Well, I see, you see, you're very keen to say, well, it turns a little", "tokens": [50580, 300, 311, 350, 24091, 281, 584, 300, 13, 1042, 11, 286, 536, 11, 291, 536, 11, 291, 434, 588, 20297, 281, 584, 11, 731, 11, 309, 4523, 257, 707, 50848], "temperature": 0.0, "avg_logprob": -0.11417526404062907, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.016365813091397285}, {"id": 882, "seek": 508272, "start": 5092.400000000001, "end": 5097.2, "text": " bit what you mean by a private component there, right? So if you really mean, you know, sort of", "tokens": [50848, 857, 437, 291, 914, 538, 257, 4551, 6542, 456, 11, 558, 30, 407, 498, 291, 534, 914, 11, 291, 458, 11, 1333, 295, 51088], "temperature": 0.0, "avg_logprob": -0.11417526404062907, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.016365813091397285}, {"id": 883, "seek": 508272, "start": 5097.2, "end": 5102.8, "text": " metaphysically inaccessibly, private and subjective, then then I think, then I think", "tokens": [51088, 30946, 749, 984, 33230, 780, 3545, 11, 4551, 293, 25972, 11, 550, 550, 286, 519, 11, 550, 286, 519, 51368], "temperature": 0.0, "avg_logprob": -0.11417526404062907, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.016365813091397285}, {"id": 884, "seek": 508272, "start": 5102.8, "end": 5109.12, "text": " it's not appropriate to speak of dividing things into this private and public component. So that's", "tokens": [51368, 309, 311, 406, 6854, 281, 1710, 295, 26764, 721, 666, 341, 4551, 293, 1908, 6542, 13, 407, 300, 311, 51684], "temperature": 0.0, "avg_logprob": -0.11417526404062907, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.016365813091397285}, {"id": 885, "seek": 510912, "start": 5109.12, "end": 5113.68, "text": " where that's where things start to go wrong. And moreover, you insist that you're not a", "tokens": [50364, 689, 300, 311, 689, 721, 722, 281, 352, 2085, 13, 400, 544, 3570, 11, 291, 13466, 300, 291, 434, 406, 257, 50592], "temperature": 0.0, "avg_logprob": -0.08432827144861221, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.019923804327845573}, {"id": 886, "seek": 510912, "start": 5113.68, "end": 5119.84, "text": " dualist, right? But I think your inclination to make that division shows that you have dualistic", "tokens": [50592, 11848, 468, 11, 558, 30, 583, 286, 519, 428, 37070, 2486, 281, 652, 300, 10044, 3110, 300, 291, 362, 11848, 3142, 50900], "temperature": 0.0, "avg_logprob": -0.08432827144861221, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.019923804327845573}, {"id": 887, "seek": 510912, "start": 5119.84, "end": 5124.88, "text": " inclinations, as we all do. So people who are denying that they're dualists, they're denying", "tokens": [50900, 834, 5045, 763, 11, 382, 321, 439, 360, 13, 407, 561, 567, 366, 30363, 300, 436, 434, 11848, 1751, 11, 436, 434, 30363, 51152], "temperature": 0.0, "avg_logprob": -0.08432827144861221, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.019923804327845573}, {"id": 888, "seek": 510912, "start": 5124.88, "end": 5131.28, "text": " this that little seed of dualism that I think is in all of us. And that is part of our part of the", "tokens": [51152, 341, 300, 707, 8871, 295, 11848, 1434, 300, 286, 519, 307, 294, 439, 295, 505, 13, 400, 300, 307, 644, 295, 527, 644, 295, 264, 51472], "temperature": 0.0, "avg_logprob": -0.08432827144861221, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.019923804327845573}, {"id": 889, "seek": 510912, "start": 5131.28, "end": 5135.76, "text": " way we, we, we, you know, we think and the part of the way you naturally go when you start to do", "tokens": [51472, 636, 321, 11, 321, 11, 321, 11, 291, 458, 11, 321, 519, 293, 264, 644, 295, 264, 636, 291, 8195, 352, 562, 291, 722, 281, 360, 51696], "temperature": 0.0, "avg_logprob": -0.08432827144861221, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.019923804327845573}, {"id": 890, "seek": 513576, "start": 5135.76, "end": 5140.4800000000005, "text": " philosophy. And so it's all, I think it's all very well to say, oh, you know, I'm a materialist", "tokens": [50364, 10675, 13, 400, 370, 309, 311, 439, 11, 286, 519, 309, 311, 439, 588, 731, 281, 584, 11, 1954, 11, 291, 458, 11, 286, 478, 257, 2527, 468, 50600], "temperature": 0.0, "avg_logprob": -0.07220693398977963, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.009367110207676888}, {"id": 891, "seek": 513576, "start": 5140.4800000000005, "end": 5145.68, "text": " and I don't, but then when you, when you start to kind of probe and you start to discover the", "tokens": [50600, 293, 286, 500, 380, 11, 457, 550, 562, 291, 11, 562, 291, 722, 281, 733, 295, 22715, 293, 291, 722, 281, 4411, 264, 50860], "temperature": 0.0, "avg_logprob": -0.07220693398977963, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.009367110207676888}, {"id": 892, "seek": 513576, "start": 5145.68, "end": 5151.04, "text": " puzzlement that these things give rise to, then that exposes a bit of latent dualism there. Now", "tokens": [50860, 18741, 3054, 300, 613, 721, 976, 6272, 281, 11, 550, 300, 1278, 4201, 257, 857, 295, 48994, 11848, 1434, 456, 13, 823, 51128], "temperature": 0.0, "avg_logprob": -0.07220693398977963, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.009367110207676888}, {"id": 893, "seek": 513576, "start": 5151.04, "end": 5155.360000000001, "text": " overcoming that latent dualism, that is the real challenge that Wittgenstein confronts.", "tokens": [51128, 38047, 300, 48994, 11848, 1434, 11, 300, 307, 264, 957, 3430, 300, 343, 593, 1766, 9089, 12422, 82, 13, 51344], "temperature": 0.0, "avg_logprob": -0.07220693398977963, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.009367110207676888}, {"id": 894, "seek": 513576, "start": 5155.360000000001, "end": 5161.2, "text": " Well, I love the challenge. So the way I see it is there is, there's a ladder. So at the top,", "tokens": [51344, 1042, 11, 286, 959, 264, 3430, 13, 407, 264, 636, 286, 536, 309, 307, 456, 307, 11, 456, 311, 257, 18325, 13, 407, 412, 264, 1192, 11, 51636], "temperature": 0.0, "avg_logprob": -0.07220693398977963, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.009367110207676888}, {"id": 895, "seek": 516120, "start": 5161.28, "end": 5166.0, "text": " you have an experience which is ineffable. And then one step down, you have an experience which", "tokens": [50368, 291, 362, 364, 1752, 597, 307, 7167, 602, 712, 13, 400, 550, 472, 1823, 760, 11, 291, 362, 364, 1752, 597, 50604], "temperature": 0.0, "avg_logprob": -0.0633813887834549, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.25778570771217346}, {"id": 896, "seek": 516120, "start": 5166.0, "end": 5170.24, "text": " is inconceivable, which is Naples argument. And then the, you know, if you really go down the ladder,", "tokens": [50604, 307, 20972, 384, 34376, 11, 597, 307, 6056, 2622, 6770, 13, 400, 550, 264, 11, 291, 458, 11, 498, 291, 534, 352, 760, 264, 18325, 11, 50816], "temperature": 0.0, "avg_logprob": -0.0633813887834549, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.25778570771217346}, {"id": 897, "seek": 516120, "start": 5170.24, "end": 5174.48, "text": " then you get into this metaphysical dualism. So I guess I'm somewhere between the first step", "tokens": [50816, 550, 291, 483, 666, 341, 30946, 36280, 11848, 1434, 13, 407, 286, 2041, 286, 478, 4079, 1296, 264, 700, 1823, 51028], "temperature": 0.0, "avg_logprob": -0.0633813887834549, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.25778570771217346}, {"id": 898, "seek": 516120, "start": 5174.48, "end": 5180.4, "text": " and the second step. So I think if I have a certain type of experience, I simply don't find the words,", "tokens": [51028, 293, 264, 1150, 1823, 13, 407, 286, 519, 498, 286, 362, 257, 1629, 2010, 295, 1752, 11, 286, 2935, 500, 380, 915, 264, 2283, 11, 51324], "temperature": 0.0, "avg_logprob": -0.0633813887834549, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.25778570771217346}, {"id": 899, "seek": 516120, "start": 5180.4, "end": 5185.12, "text": " I can't communicate it to you. But if you put probes in my brain or something like that, I'm", "tokens": [51324, 286, 393, 380, 7890, 309, 281, 291, 13, 583, 498, 291, 829, 1239, 279, 294, 452, 3567, 420, 746, 411, 300, 11, 286, 478, 51560], "temperature": 0.0, "avg_logprob": -0.0633813887834549, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.25778570771217346}, {"id": 900, "seek": 518512, "start": 5185.12, "end": 5191.5199999999995, "text": " sure that could conceivably be a way of measuring it. Yes. Yeah. So, so, so this is really important.", "tokens": [50364, 988, 300, 727, 10413, 592, 1188, 312, 257, 636, 295, 13389, 309, 13, 1079, 13, 865, 13, 407, 11, 370, 11, 370, 341, 307, 534, 1021, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1655955087570917, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.014989476650953293}, {"id": 901, "seek": 518512, "start": 5191.5199999999995, "end": 5196.96, "text": " So for me, what counts as public is not just behavior, but it's also whatever scientists", "tokens": [50684, 407, 337, 385, 11, 437, 14893, 382, 1908, 307, 406, 445, 5223, 11, 457, 309, 311, 611, 2035, 7708, 50956], "temperature": 0.0, "avg_logprob": -0.1655955087570917, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.014989476650953293}, {"id": 902, "seek": 518512, "start": 5196.96, "end": 5202.4, "text": " we can discover. So that, so, so if we poke around in people's brains and we do EEG recordings and", "tokens": [50956, 321, 393, 4411, 13, 407, 300, 11, 370, 11, 370, 498, 321, 19712, 926, 294, 561, 311, 15442, 293, 321, 360, 33685, 38, 25162, 293, 51228], "temperature": 0.0, "avg_logprob": -0.1655955087570917, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.014989476650953293}, {"id": 903, "seek": 518512, "start": 5202.4, "end": 5208.24, "text": " FLRI recordings and anything else that we can imagine. And then I as a scientist can see this", "tokens": [51228, 24720, 5577, 25162, 293, 1340, 1646, 300, 321, 393, 3811, 13, 400, 550, 286, 382, 257, 12662, 393, 536, 341, 51520], "temperature": 0.0, "avg_logprob": -0.1655955087570917, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.014989476650953293}, {"id": 904, "seek": 518512, "start": 5208.24, "end": 5213.44, "text": " stuff and use a scientist and our fellow scientists will see, see that. That's public too. So that's", "tokens": [51520, 1507, 293, 764, 257, 12662, 293, 527, 7177, 7708, 486, 536, 11, 536, 300, 13, 663, 311, 1908, 886, 13, 407, 300, 311, 51780], "temperature": 0.0, "avg_logprob": -0.1655955087570917, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.014989476650953293}, {"id": 905, "seek": 521344, "start": 5213.44, "end": 5218.32, "text": " in the, for the purposes of this discussion, of this philosophical discussion, that's all in the", "tokens": [50364, 294, 264, 11, 337, 264, 9932, 295, 341, 5017, 11, 295, 341, 25066, 5017, 11, 300, 311, 439, 294, 264, 50608], "temperature": 0.0, "avg_logprob": -0.1293633270263672, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0041510858573019505}, {"id": 906, "seek": 521344, "start": 5218.32, "end": 5224.08, "text": " public realm. It's not metaphysically hidden. You can, you can, and all of that can feed into the", "tokens": [50608, 1908, 15355, 13, 467, 311, 406, 30946, 749, 984, 7633, 13, 509, 393, 11, 291, 393, 11, 293, 439, 295, 300, 393, 3154, 666, 264, 50896], "temperature": 0.0, "avg_logprob": -0.1293633270263672, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0041510858573019505}, {"id": 907, "seek": 521344, "start": 5224.08, "end": 5228.719999999999, "text": " way we talk about consciousness. And especially if we're talking about exotic entities, then,", "tokens": [50896, 636, 321, 751, 466, 10081, 13, 400, 2318, 498, 321, 434, 1417, 466, 27063, 16667, 11, 550, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1293633270263672, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0041510858573019505}, {"id": 908, "seek": 521344, "start": 5229.5199999999995, "end": 5237.04, "text": " then all of that can feed into the way our language adapts to, to, to, to, to our encountering them.", "tokens": [51168, 550, 439, 295, 300, 393, 3154, 666, 264, 636, 527, 2856, 23169, 1373, 281, 11, 281, 11, 281, 11, 281, 11, 281, 527, 8593, 278, 552, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1293633270263672, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0041510858573019505}, {"id": 909, "seek": 521344, "start": 5237.04, "end": 5241.36, "text": " Yeah. So I think it's fascinating to decompose as you just did what people mean by subjectivity. So", "tokens": [51544, 865, 13, 407, 286, 519, 309, 311, 10343, 281, 22867, 541, 382, 291, 445, 630, 437, 561, 914, 538, 3983, 4253, 13, 407, 51760], "temperature": 0.0, "avg_logprob": -0.1293633270263672, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0041510858573019505}, {"id": 910, "seek": 524136, "start": 5241.36, "end": 5246.88, "text": " of course, some people like David Chalmers, they argue that there is a little bit extra. So there's,", "tokens": [50364, 295, 1164, 11, 512, 561, 411, 4389, 761, 304, 18552, 11, 436, 9695, 300, 456, 307, 257, 707, 857, 2857, 13, 407, 456, 311, 11, 50640], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 911, "seek": 524136, "start": 5246.88, "end": 5251.2, "text": " you know, behavior function and dynamics. And then there's that, you know, little bit extra,", "tokens": [50640, 291, 458, 11, 5223, 2445, 293, 15679, 13, 400, 550, 456, 311, 300, 11, 291, 458, 11, 707, 857, 2857, 11, 50856], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 912, "seek": 524136, "start": 5251.2, "end": 5256.48, "text": " which is not observable in any scientific way. And I think, you know, it's fair to say a lot of", "tokens": [50856, 597, 307, 406, 9951, 712, 294, 604, 8134, 636, 13, 400, 286, 519, 11, 291, 458, 11, 309, 311, 3143, 281, 584, 257, 688, 295, 51120], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 913, "seek": 524136, "start": 5256.48, "end": 5260.4, "text": " people when they talk about subjectivity, they're not talking about the little bit extra. But when", "tokens": [51120, 561, 562, 436, 751, 466, 3983, 4253, 11, 436, 434, 406, 1417, 466, 264, 707, 857, 2857, 13, 583, 562, 51316], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 914, "seek": 524136, "start": 5260.4, "end": 5263.759999999999, "text": " we do get to the little bit extra, I completely agree with you, we've got a big problem.", "tokens": [51316, 321, 360, 483, 281, 264, 707, 857, 2857, 11, 286, 2584, 3986, 365, 291, 11, 321, 600, 658, 257, 955, 1154, 13, 51484], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 915, "seek": 524136, "start": 5264.4, "end": 5270.5599999999995, "text": " Yeah. Yeah, I think we have got a big problem because, because of our natural, you know, dualistic", "tokens": [51516, 865, 13, 865, 11, 286, 519, 321, 362, 658, 257, 955, 1154, 570, 11, 570, 295, 527, 3303, 11, 291, 458, 11, 11848, 3142, 51824], "temperature": 0.0, "avg_logprob": -0.07003978703985152, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0021334111224859953}, {"id": 916, "seek": 527056, "start": 5270.56, "end": 5277.360000000001, "text": " tendencies to, it's very, very difficult to think that, that, that, you know, if I experience a pain,", "tokens": [50364, 45488, 281, 11, 309, 311, 588, 11, 588, 2252, 281, 519, 300, 11, 300, 11, 300, 11, 291, 458, 11, 498, 286, 1752, 257, 1822, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12279924750328064, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.009300273843109608}, {"id": 917, "seek": 527056, "start": 5277.92, "end": 5282.56, "text": " that, that, that there isn't something about that that is just purely minor, that you couldn't,", "tokens": [50732, 300, 11, 300, 11, 300, 456, 1943, 380, 746, 466, 300, 300, 307, 445, 17491, 6696, 11, 300, 291, 2809, 380, 11, 50964], "temperature": 0.0, "avg_logprob": -0.12279924750328064, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.009300273843109608}, {"id": 918, "seek": 527056, "start": 5282.56, "end": 5287.68, "text": " you know, the outside world, that other people can never really, you know, experience it. But", "tokens": [50964, 291, 458, 11, 264, 2380, 1002, 11, 300, 661, 561, 393, 1128, 534, 11, 291, 458, 11, 1752, 309, 13, 583, 51220], "temperature": 0.0, "avg_logprob": -0.12279924750328064, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.009300273843109608}, {"id": 919, "seek": 527056, "start": 5287.68, "end": 5292.8, "text": " that's, it's having that thought, that's this moment that you kind of go wrong, but it's natural", "tokens": [51220, 300, 311, 11, 309, 311, 1419, 300, 1194, 11, 300, 311, 341, 1623, 300, 291, 733, 295, 352, 2085, 11, 457, 309, 311, 3303, 51476], "temperature": 0.0, "avg_logprob": -0.12279924750328064, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.009300273843109608}, {"id": 920, "seek": 527056, "start": 5292.8, "end": 5297.52, "text": " path to go down. It's really, really hard to avoid it. And, and that's where I think", "tokens": [51476, 3100, 281, 352, 760, 13, 467, 311, 534, 11, 534, 1152, 281, 5042, 309, 13, 400, 11, 293, 300, 311, 689, 286, 519, 51712], "temperature": 0.0, "avg_logprob": -0.12279924750328064, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.009300273843109608}, {"id": 921, "seek": 529752, "start": 5297.6, "end": 5304.56, "text": " Sylvitkenstein's remarks, they, they provide a whole way of, of trying to reorient your whole", "tokens": [50368, 33349, 10398, 2653, 9089, 311, 19151, 11, 436, 11, 436, 2893, 257, 1379, 636, 295, 11, 295, 1382, 281, 319, 19521, 428, 1379, 50716], "temperature": 0.0, "avg_logprob": -0.14374314636743368, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.009337972849607468}, {"id": 922, "seek": 529752, "start": 5304.56, "end": 5310.72, "text": " way of thinking. And, and, and if you sort of really kind of grasp them, it sort of flips your", "tokens": [50716, 636, 295, 1953, 13, 400, 11, 293, 11, 293, 498, 291, 1333, 295, 534, 733, 295, 21743, 552, 11, 309, 1333, 295, 40249, 428, 51024], "temperature": 0.0, "avg_logprob": -0.14374314636743368, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.009337972849607468}, {"id": 923, "seek": 529752, "start": 5310.72, "end": 5314.96, "text": " whole world around, it flips your whole way of thinking around. So it's so that the, this whole", "tokens": [51024, 1379, 1002, 926, 11, 309, 40249, 428, 1379, 636, 295, 1953, 926, 13, 407, 309, 311, 370, 300, 264, 11, 341, 1379, 51236], "temperature": 0.0, "avg_logprob": -0.14374314636743368, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.009337972849607468}, {"id": 924, "seek": 529752, "start": 5314.96, "end": 5320.240000000001, "text": " way of talking and thinking becomes wrong. So it's so that so very often the strategy when", "tokens": [51236, 636, 295, 1417, 293, 1953, 3643, 2085, 13, 407, 309, 311, 370, 300, 370, 588, 2049, 264, 5206, 562, 51500], "temperature": 0.0, "avg_logprob": -0.14374314636743368, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.009337972849607468}, {"id": 925, "seek": 529752, "start": 5320.240000000001, "end": 5324.64, "text": " you're dealing with this is somebody throws out this thought at you, like you've been throwing", "tokens": [51500, 291, 434, 6260, 365, 341, 307, 2618, 19251, 484, 341, 1194, 412, 291, 11, 411, 291, 600, 668, 10238, 51720], "temperature": 0.0, "avg_logprob": -0.14374314636743368, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.009337972849607468}, {"id": 926, "seek": 532464, "start": 5324.64, "end": 5329.92, "text": " out various thoughts at me about, and, and, and often buried in the way those thoughts are framed", "tokens": [50364, 484, 3683, 4598, 412, 385, 466, 11, 293, 11, 293, 11, 293, 2049, 14101, 294, 264, 636, 729, 4598, 366, 30420, 50628], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 927, "seek": 532464, "start": 5329.92, "end": 5334.4800000000005, "text": " is the problem. So, so the, the problem is the very expression of those thoughts. And you have", "tokens": [50628, 307, 264, 1154, 13, 407, 11, 370, 264, 11, 264, 1154, 307, 264, 588, 6114, 295, 729, 4598, 13, 400, 291, 362, 50856], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 928, "seek": 532464, "start": 5334.4800000000005, "end": 5338.08, "text": " to take a step back and say, hang on a minute, you know, you made this funny move, you introduced", "tokens": [50856, 281, 747, 257, 1823, 646, 293, 584, 11, 3967, 322, 257, 3456, 11, 291, 458, 11, 291, 1027, 341, 4074, 1286, 11, 291, 7268, 51036], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 929, "seek": 532464, "start": 5338.08, "end": 5342.4800000000005, "text": " this funny bit of language, you introduced this funny way of expressing things. And that's, that's", "tokens": [51036, 341, 4074, 857, 295, 2856, 11, 291, 7268, 341, 4074, 636, 295, 22171, 721, 13, 400, 300, 311, 11, 300, 311, 51256], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 930, "seek": 532464, "start": 5342.4800000000005, "end": 5348.8, "text": " when that's the, the, Victor Stein has this phrase that is, that's where the conjuring trick", "tokens": [51256, 562, 300, 311, 264, 11, 264, 11, 15777, 29453, 575, 341, 9535, 300, 307, 11, 300, 311, 689, 264, 20295, 1345, 4282, 51572], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 931, "seek": 532464, "start": 5348.8, "end": 5353.360000000001, "text": " happens is where you, the point that you don't notice is where the conjuring trick happens.", "tokens": [51572, 2314, 307, 689, 291, 11, 264, 935, 300, 291, 500, 380, 3449, 307, 689, 264, 20295, 1345, 4282, 2314, 13, 51800], "temperature": 0.0, "avg_logprob": -0.10353259907828437, "compression_ratio": 2.0282685512367493, "no_speech_prob": 0.013830865733325481}, {"id": 932, "seek": 535336, "start": 5354.16, "end": 5359.679999999999, "text": " So, so, so it's kind of, so often you have to take, take a step back and you have to sort of say,", "tokens": [50404, 407, 11, 370, 11, 370, 309, 311, 733, 295, 11, 370, 2049, 291, 362, 281, 747, 11, 747, 257, 1823, 646, 293, 291, 362, 281, 1333, 295, 584, 11, 50680], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 933, "seek": 535336, "start": 5359.679999999999, "end": 5363.28, "text": " hang on a minute, I don't accept that way of talking that you've just suddenly introduced,", "tokens": [50680, 3967, 322, 257, 3456, 11, 286, 500, 380, 3241, 300, 636, 295, 1417, 300, 291, 600, 445, 5800, 7268, 11, 50860], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 934, "seek": 535336, "start": 5363.28, "end": 5368.799999999999, "text": " which is going down a philosophical garden path. Yes, and I completely agree. So, so that is,", "tokens": [50860, 597, 307, 516, 760, 257, 25066, 7431, 3100, 13, 1079, 11, 293, 286, 2584, 3986, 13, 407, 11, 370, 300, 307, 11, 51136], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 935, "seek": 535336, "start": 5369.36, "end": 5372.88, "text": " that is a form of dualism, you know, when, when we resort to that little bit extra.", "tokens": [51164, 300, 307, 257, 1254, 295, 11848, 1434, 11, 291, 458, 11, 562, 11, 562, 321, 19606, 281, 300, 707, 857, 2857, 13, 51340], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 936, "seek": 535336, "start": 5373.44, "end": 5378.08, "text": " And I'm quite interested in this actually, because people like Chalmers, I don't think he likes the", "tokens": [51368, 400, 286, 478, 1596, 3102, 294, 341, 767, 11, 570, 561, 411, 761, 304, 18552, 11, 286, 500, 380, 519, 415, 5902, 264, 51600], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 937, "seek": 535336, "start": 5378.08, "end": 5382.799999999999, "text": " term dualist, I think it's a property dualist, but he does talk about the philosophical zombie,", "tokens": [51600, 1433, 11848, 468, 11, 286, 519, 309, 311, 257, 4707, 11848, 468, 11, 457, 415, 775, 751, 466, 264, 25066, 20310, 11, 51836], "temperature": 0.0, "avg_logprob": -0.11430279832137258, "compression_ratio": 1.7399380804953561, "no_speech_prob": 0.0018254491733387113}, {"id": 938, "seek": 538280, "start": 5382.8, "end": 5386.4800000000005, "text": " which is a thought experiment of something which has all of the behavior of us, but", "tokens": [50364, 597, 307, 257, 1194, 5120, 295, 746, 597, 575, 439, 295, 264, 5223, 295, 505, 11, 457, 50548], "temperature": 0.0, "avg_logprob": -0.10838773229100683, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.012022091075778008}, {"id": 939, "seek": 538280, "start": 5386.4800000000005, "end": 5391.28, "text": " is lacking in conscious experience, which gives rise to this idea that it's almost a kind of", "tokens": [50548, 307, 20889, 294, 6648, 1752, 11, 597, 2709, 6272, 281, 341, 1558, 300, 309, 311, 1920, 257, 733, 295, 50788], "temperature": 0.0, "avg_logprob": -0.10838773229100683, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.012022091075778008}, {"id": 940, "seek": 538280, "start": 5391.28, "end": 5395.360000000001, "text": " epiphenomenon or it's something which, you know, almost you're asking the question, well,", "tokens": [50788, 2388, 647, 2932, 4726, 266, 420, 309, 311, 746, 597, 11, 291, 458, 11, 1920, 291, 434, 3365, 264, 1168, 11, 731, 11, 50992], "temperature": 0.0, "avg_logprob": -0.10838773229100683, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.012022091075778008}, {"id": 941, "seek": 538280, "start": 5395.360000000001, "end": 5399.92, "text": " well, what's it doing if it's not affecting anything? And when I read your conscious Exotica", "tokens": [50992, 731, 11, 437, 311, 309, 884, 498, 309, 311, 406, 17476, 1340, 30, 400, 562, 286, 1401, 428, 6648, 2111, 310, 2262, 51220], "temperature": 0.0, "avg_logprob": -0.10838773229100683, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.012022091075778008}, {"id": 942, "seek": 538280, "start": 5399.92, "end": 5404.56, "text": " article, I had a similar thought actually, because you showed this linear correlation between, you", "tokens": [51220, 7222, 11, 286, 632, 257, 2531, 1194, 767, 11, 570, 291, 4712, 341, 8213, 20009, 1296, 11, 291, 51452], "temperature": 0.0, "avg_logprob": -0.10838773229100683, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.012022091075778008}, {"id": 943, "seek": 540456, "start": 5404.64, "end": 5413.52, "text": " know, human likeness and consciousness. And then you gave examples of algorithms,", "tokens": [50368, 458, 11, 1952, 36946, 442, 293, 10081, 13, 400, 550, 291, 2729, 5110, 295, 14642, 11, 50812], "temperature": 0.0, "avg_logprob": -0.11516973972320557, "compression_ratio": 1.7433155080213905, "no_speech_prob": 0.036995694041252136}, {"id": 944, "seek": 540456, "start": 5413.52, "end": 5417.200000000001, "text": " you know, like AlphaGo, for example, and they didn't need the consciousness.", "tokens": [50812, 291, 458, 11, 411, 20588, 12104, 11, 337, 1365, 11, 293, 436, 994, 380, 643, 264, 10081, 13, 50996], "temperature": 0.0, "avg_logprob": -0.11516973972320557, "compression_ratio": 1.7433155080213905, "no_speech_prob": 0.036995694041252136}, {"id": 945, "seek": 540456, "start": 5417.200000000001, "end": 5421.76, "text": " And that again raises the question of, what is the cash value of consciousness?", "tokens": [50996, 400, 300, 797, 19658, 264, 1168, 295, 11, 437, 307, 264, 6388, 2158, 295, 10081, 30, 51224], "temperature": 0.0, "avg_logprob": -0.11516973972320557, "compression_ratio": 1.7433155080213905, "no_speech_prob": 0.036995694041252136}, {"id": 946, "seek": 540456, "start": 5421.76, "end": 5426.88, "text": " When we use, when we're using the word consciousness, then often we are using it in the", "tokens": [51224, 1133, 321, 764, 11, 562, 321, 434, 1228, 264, 1349, 10081, 11, 550, 2049, 321, 366, 1228, 309, 294, 264, 51480], "temperature": 0.0, "avg_logprob": -0.11516973972320557, "compression_ratio": 1.7433155080213905, "no_speech_prob": 0.036995694041252136}, {"id": 947, "seek": 542688, "start": 5426.88, "end": 5434.96, "text": " context of certain, you know, of certain behavioral behavior and behavioral inclinations,", "tokens": [50364, 4319, 295, 1629, 11, 291, 458, 11, 295, 1629, 19124, 5223, 293, 19124, 834, 5045, 763, 11, 50768], "temperature": 0.0, "avg_logprob": -0.10251413365846039, "compression_ratio": 1.829145728643216, "no_speech_prob": 0.06683090329170227}, {"id": 948, "seek": 542688, "start": 5434.96, "end": 5441.6, "text": " and we use it in the context of other humans and other animals. And there's a whole,", "tokens": [50768, 293, 321, 764, 309, 294, 264, 4319, 295, 661, 6255, 293, 661, 4882, 13, 400, 456, 311, 257, 1379, 11, 51100], "temperature": 0.0, "avg_logprob": -0.10251413365846039, "compression_ratio": 1.829145728643216, "no_speech_prob": 0.06683090329170227}, {"id": 949, "seek": 542688, "start": 5441.6, "end": 5446.72, "text": " I mean, for a start, the word consciousness is actually, you know, it's a multifaceted concept", "tokens": [51100, 286, 914, 11, 337, 257, 722, 11, 264, 1349, 10081, 307, 767, 11, 291, 458, 11, 309, 311, 257, 39824, 326, 10993, 3410, 51356], "temperature": 0.0, "avg_logprob": -0.10251413365846039, "compression_ratio": 1.829145728643216, "no_speech_prob": 0.06683090329170227}, {"id": 950, "seek": 542688, "start": 5446.72, "end": 5452.08, "text": " that it's alluding to many things. And one of the things that it is alluding to is our ability", "tokens": [51356, 300, 309, 311, 439, 33703, 281, 867, 721, 13, 400, 472, 295, 264, 721, 300, 309, 307, 439, 33703, 281, 307, 527, 3485, 51624], "temperature": 0.0, "avg_logprob": -0.10251413365846039, "compression_ratio": 1.829145728643216, "no_speech_prob": 0.06683090329170227}, {"id": 951, "seek": 545208, "start": 5452.08, "end": 5458.0, "text": " to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice", "tokens": [50364, 281, 2028, 5896, 3545, 365, 264, 7429, 1002, 13, 407, 321, 1710, 466, 11, 1954, 11, 291, 458, 11, 286, 994, 380, 3449, 50660], "temperature": 0.0, "avg_logprob": -0.10309462547302246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.058109212666749954}, {"id": 952, "seek": 545208, "start": 5458.0, "end": 5464.32, "text": " the chair, that's why I bumped into it or something. And, or, you know, I didn't see,", "tokens": [50660, 264, 6090, 11, 300, 311, 983, 286, 42696, 666, 309, 420, 746, 13, 400, 11, 420, 11, 291, 458, 11, 286, 994, 380, 536, 11, 50976], "temperature": 0.0, "avg_logprob": -0.10309462547302246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.058109212666749954}, {"id": 953, "seek": 545208, "start": 5465.6, "end": 5470.48, "text": " you know, that there was a desk over there that might have had something interesting inside it,", "tokens": [51040, 291, 458, 11, 300, 456, 390, 257, 10026, 670, 456, 300, 1062, 362, 632, 746, 1880, 1854, 309, 11, 51284], "temperature": 0.0, "avg_logprob": -0.10309462547302246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.058109212666749954}, {"id": 954, "seek": 545208, "start": 5470.48, "end": 5475.76, "text": " if you opened it up. And so we talk about our awareness of the world. And we're at the same", "tokens": [51284, 498, 291, 5625, 309, 493, 13, 400, 370, 321, 751, 466, 527, 8888, 295, 264, 1002, 13, 400, 321, 434, 412, 264, 912, 51548], "temperature": 0.0, "avg_logprob": -0.10309462547302246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.058109212666749954}, {"id": 955, "seek": 547576, "start": 5475.76, "end": 5481.360000000001, "text": " time, we're talking about an aspect of consciousness, and we're talking about", "tokens": [50364, 565, 11, 321, 434, 1417, 466, 364, 4171, 295, 10081, 11, 293, 321, 434, 1417, 466, 50644], "temperature": 0.0, "avg_logprob": -0.11616560046592456, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.10815051943063736}, {"id": 956, "seek": 547576, "start": 5482.72, "end": 5487.6, "text": " a whole load of behavioral dispositions and capabilities. And so these things are very much,", "tokens": [50712, 257, 1379, 3677, 295, 19124, 15885, 2451, 293, 10862, 13, 400, 370, 613, 721, 366, 588, 709, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11616560046592456, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.10815051943063736}, {"id": 957, "seek": 547576, "start": 5488.320000000001, "end": 5493.68, "text": " you know, are very much related to each other in our everyday speech. So then the question arises,", "tokens": [50992, 291, 458, 11, 366, 588, 709, 4077, 281, 1184, 661, 294, 527, 7429, 6218, 13, 407, 550, 264, 1168, 27388, 11, 51260], "temperature": 0.0, "avg_logprob": -0.11616560046592456, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.10815051943063736}, {"id": 958, "seek": 547576, "start": 5493.68, "end": 5500.4800000000005, "text": " though, are they dissociable? And so now it's very important that it's not like I think there's,", "tokens": [51260, 1673, 11, 366, 436, 44446, 712, 30, 400, 370, 586, 309, 311, 588, 1021, 300, 309, 311, 406, 411, 286, 519, 456, 311, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11616560046592456, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.10815051943063736}, {"id": 959, "seek": 550048, "start": 5500.48, "end": 5505.759999999999, "text": " that consciousness is some metaphysical thing whose essence is out there to be discovered.", "tokens": [50364, 300, 10081, 307, 512, 30946, 36280, 551, 6104, 12801, 307, 484, 456, 281, 312, 6941, 13, 50628], "temperature": 0.0, "avg_logprob": -0.14143970759228022, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.015705067664384842}, {"id": 960, "seek": 550048, "start": 5505.759999999999, "end": 5512.32, "text": " It's just, it's a concept that we invent and a word that we use to describe the world around us", "tokens": [50628, 467, 311, 445, 11, 309, 311, 257, 3410, 300, 321, 7962, 293, 257, 1349, 300, 321, 764, 281, 6786, 264, 1002, 926, 505, 50956], "temperature": 0.0, "avg_logprob": -0.14143970759228022, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.015705067664384842}, {"id": 961, "seek": 550048, "start": 5512.32, "end": 5518.48, "text": " and our place in it and each other and so on. And so, so, so then, you know, then the question is,", "tokens": [50956, 293, 527, 1081, 294, 309, 293, 1184, 661, 293, 370, 322, 13, 400, 370, 11, 370, 11, 370, 550, 11, 291, 458, 11, 550, 264, 1168, 307, 11, 51264], "temperature": 0.0, "avg_logprob": -0.14143970759228022, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.015705067664384842}, {"id": 962, "seek": 550048, "start": 5520.719999999999, "end": 5526.719999999999, "text": " are there things that we might create or imagine, where we'd want to use the one concept, we want", "tokens": [51376, 366, 456, 721, 300, 321, 1062, 1884, 420, 3811, 11, 689, 321, 1116, 528, 281, 764, 264, 472, 3410, 11, 321, 528, 51676], "temperature": 0.0, "avg_logprob": -0.14143970759228022, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.015705067664384842}, {"id": 963, "seek": 552672, "start": 5526.72, "end": 5530.320000000001, "text": " to use the one set of words and not use the other. And that is what the question comes down to.", "tokens": [50364, 281, 764, 264, 472, 992, 295, 2283, 293, 406, 764, 264, 661, 13, 400, 300, 307, 437, 264, 1168, 1487, 760, 281, 13, 50544], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 964, "seek": 552672, "start": 5530.320000000001, "end": 5534.4800000000005, "text": " So in the case of something like AlphaGo, then I think, you know, we're all kind of agree that it's", "tokens": [50544, 407, 294, 264, 1389, 295, 746, 411, 20588, 12104, 11, 550, 286, 519, 11, 291, 458, 11, 321, 434, 439, 733, 295, 3986, 300, 309, 311, 50752], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 965, "seek": 552672, "start": 5534.4800000000005, "end": 5539.92, "text": " actually there's a kind of cognition going on there, there's a kind of reasoning going on in", "tokens": [50752, 767, 456, 311, 257, 733, 295, 46905, 516, 322, 456, 11, 456, 311, 257, 733, 295, 21577, 516, 322, 294, 51024], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 966, "seek": 552672, "start": 5539.92, "end": 5544.0, "text": " AlphaGo. There's certainly a lot of kind of cleverness, there's a kind of intelligence,", "tokens": [51024, 20588, 12104, 13, 821, 311, 3297, 257, 688, 295, 733, 295, 13494, 1287, 11, 456, 311, 257, 733, 295, 7599, 11, 51228], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 967, "seek": 552672, "start": 5544.0, "end": 5548.96, "text": " there's even, if we're thinking about move 37, a kind of creativity. So we're willing to use", "tokens": [51228, 456, 311, 754, 11, 498, 321, 434, 1953, 466, 1286, 13435, 11, 257, 733, 295, 12915, 13, 407, 321, 434, 4950, 281, 764, 51476], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 968, "seek": 552672, "start": 5548.96, "end": 5553.92, "text": " all of those words, but nobody is going to suggest that AlphaGo is conscious. So there we can see", "tokens": [51476, 439, 295, 729, 2283, 11, 457, 5079, 307, 516, 281, 3402, 300, 20588, 12104, 307, 6648, 13, 407, 456, 321, 393, 536, 51724], "temperature": 0.0, "avg_logprob": -0.07959614824128632, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.02219572849571705}, {"id": 969, "seek": 555392, "start": 5554.0, "end": 5558.4800000000005, "text": " that they're under certain conditions, the concepts are dissociable. But nevertheless,", "tokens": [50368, 300, 436, 434, 833, 1629, 4487, 11, 264, 10392, 366, 44446, 712, 13, 583, 26924, 11, 50592], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 970, "seek": 555392, "start": 5558.4800000000005, "end": 5563.68, "text": " there's a strong relationship between the two, because if we think about animals, then often we", "tokens": [50592, 456, 311, 257, 2068, 2480, 1296, 264, 732, 11, 570, 498, 321, 519, 466, 4882, 11, 550, 2049, 321, 50852], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 971, "seek": 555392, "start": 5563.68, "end": 5568.8, "text": " are going to, we're going to use their cognitive abilities as manifest in their sophisticated", "tokens": [50852, 366, 516, 281, 11, 321, 434, 516, 281, 764, 641, 15605, 11582, 382, 10067, 294, 641, 16950, 51108], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 972, "seek": 555392, "start": 5568.8, "end": 5573.76, "text": " behavior. We're going to use that as a proxy for sometimes whether we want to talk about them in", "tokens": [51108, 5223, 13, 492, 434, 516, 281, 764, 300, 382, 257, 29690, 337, 2171, 1968, 321, 528, 281, 751, 466, 552, 294, 51356], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 973, "seek": 555392, "start": 5573.76, "end": 5579.04, "text": " terms of consciousness. So sometimes, in our usage, we're going to bundle the things together,", "tokens": [51356, 2115, 295, 10081, 13, 407, 2171, 11, 294, 527, 14924, 11, 321, 434, 516, 281, 24438, 264, 721, 1214, 11, 51620], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 974, "seek": 555392, "start": 5579.04, "end": 5583.28, "text": " and sometimes we're not. But this is all just a matter of, it's a kind of a practical matter", "tokens": [51620, 293, 2171, 321, 434, 406, 13, 583, 341, 307, 439, 445, 257, 1871, 295, 11, 309, 311, 257, 733, 295, 257, 8496, 1871, 51832], "temperature": 0.0, "avg_logprob": -0.10367441910963793, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.017865914851427078}, {"id": 975, "seek": 558328, "start": 5583.36, "end": 5588.24, "text": " of how we use language and how it's usefully deployed, how language is usefully deployed.", "tokens": [50368, 295, 577, 321, 764, 2856, 293, 577, 309, 311, 764, 2277, 17826, 11, 577, 2856, 307, 764, 2277, 17826, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 976, "seek": 558328, "start": 5588.24, "end": 5592.4, "text": " And it's not about discovering some metaphysical entity that's out there,", "tokens": [50612, 400, 309, 311, 406, 466, 24773, 512, 30946, 36280, 13977, 300, 311, 484, 456, 11, 50820], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 977, "seek": 558328, "start": 5592.4, "end": 5595.12, "text": " which is what conscious, the word consciousness denotes.", "tokens": [50820, 597, 307, 437, 6648, 11, 264, 1349, 10081, 1441, 17251, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 978, "seek": 558328, "start": 5595.12, "end": 5599.84, "text": " Demis Esalvis recently spoke about this ladder of creativity and of course,", "tokens": [50956, 4686, 271, 2313, 304, 4938, 3938, 7179, 466, 341, 18325, 295, 12915, 293, 295, 1164, 11, 51192], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 979, "seek": 558328, "start": 5599.84, "end": 5606.08, "text": " inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad", "tokens": [51192, 7962, 488, 12915, 300, 1286, 13435, 390, 7152, 13, 583, 8033, 19027, 3093, 11, 1472, 294, 4336, 13, 286, 478, 370, 5404, 51504], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 980, "seek": 558328, "start": 5606.08, "end": 5611.599999999999, "text": " I had him on the podcast actually. He's a huge hero of mine, and I believe that a hero of yours", "tokens": [51504, 286, 632, 796, 322, 264, 7367, 767, 13, 634, 311, 257, 2603, 5316, 295, 3892, 11, 293, 286, 1697, 300, 257, 5316, 295, 6342, 51780], "temperature": 0.0, "avg_logprob": -0.14907625854992476, "compression_ratio": 1.6827586206896552, "no_speech_prob": 0.006027385126799345}, {"id": 981, "seek": 561160, "start": 5612.56, "end": 5616.96, "text": " but he coined this term the intentional stance. And what's interesting is he was", "tokens": [50412, 457, 415, 45222, 341, 1433, 264, 21935, 21033, 13, 400, 437, 311, 1880, 307, 415, 390, 50632], "temperature": 0.0, "avg_logprob": -0.13465913792246395, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.006256612483412027}, {"id": 982, "seek": 561160, "start": 5617.6, "end": 5622.72, "text": " using it to designate a rational agent, but actually it gets overloaded and I'm guilty of", "tokens": [50664, 1228, 309, 281, 1715, 473, 257, 15090, 9461, 11, 457, 767, 309, 2170, 28777, 292, 293, 286, 478, 12341, 295, 50920], "temperature": 0.0, "avg_logprob": -0.13465913792246395, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.006256612483412027}, {"id": 983, "seek": 561160, "start": 5622.72, "end": 5627.200000000001, "text": " this. You overload it for lots of things, including even for things like consciousness. And maybe", "tokens": [50920, 341, 13, 509, 28777, 309, 337, 3195, 295, 721, 11, 3009, 754, 337, 721, 411, 10081, 13, 400, 1310, 51144], "temperature": 0.0, "avg_logprob": -0.13465913792246395, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.006256612483412027}, {"id": 984, "seek": 561160, "start": 5627.200000000001, "end": 5633.360000000001, "text": " that's because of the correlates of cognition, these things are very closely related. But", "tokens": [51144, 300, 311, 570, 295, 264, 13983, 1024, 295, 46905, 11, 613, 721, 366, 588, 8185, 4077, 13, 583, 51452], "temperature": 0.0, "avg_logprob": -0.13465913792246395, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.006256612483412027}, {"id": 985, "seek": 561160, "start": 5633.360000000001, "end": 5636.400000000001, "text": " can you explain in your own articulation the intentional stance?", "tokens": [51452, 393, 291, 2903, 294, 428, 1065, 15228, 2776, 264, 21935, 21033, 30, 51604], "temperature": 0.0, "avg_logprob": -0.13465913792246395, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.006256612483412027}, {"id": 986, "seek": 563640, "start": 5636.48, "end": 5645.2, "text": " Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance", "tokens": [50368, 865, 13, 1042, 11, 370, 286, 519, 291, 393, 764, 264, 3410, 293, 7274, 264, 3410, 295, 264, 21935, 21033, 50804], "temperature": 0.0, "avg_logprob": -0.10576976537704467, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00907172355800867}, {"id": 987, "seek": 563640, "start": 5645.2, "end": 5650.16, "text": " without necessarily embracing the whole of everything that Dan Dennett was talking about", "tokens": [50804, 1553, 4725, 31596, 264, 1379, 295, 1203, 300, 3394, 19027, 3093, 390, 1417, 466, 51052], "temperature": 0.0, "avg_logprob": -0.10576976537704467, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00907172355800867}, {"id": 988, "seek": 563640, "start": 5650.16, "end": 5656.16, "text": " in that context. Because for him, there's a whole big philosophical position around it,", "tokens": [51052, 294, 300, 4319, 13, 1436, 337, 796, 11, 456, 311, 257, 1379, 955, 25066, 2535, 926, 309, 11, 51352], "temperature": 0.0, "avg_logprob": -0.10576976537704467, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00907172355800867}, {"id": 989, "seek": 563640, "start": 5656.16, "end": 5661.2, "text": " but there's a very simple sense of the intentional stance that we can lift from Dan without", "tokens": [51352, 457, 456, 311, 257, 588, 2199, 2020, 295, 264, 21935, 21033, 300, 321, 393, 5533, 490, 3394, 1553, 51604], "temperature": 0.0, "avg_logprob": -0.10576976537704467, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00907172355800867}, {"id": 990, "seek": 566120, "start": 5661.2, "end": 5667.5199999999995, "text": " necessarily buying into everything that he said. And it's simply to say that we often in everyday", "tokens": [50364, 4725, 6382, 666, 1203, 300, 415, 848, 13, 400, 309, 311, 2935, 281, 584, 300, 321, 2049, 294, 7429, 50680], "temperature": 0.0, "avg_logprob": -0.11530047744067747, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.06290443241596222}, {"id": 991, "seek": 566120, "start": 5667.5199999999995, "end": 5677.12, "text": " terms speak about artifacts and indeed animals, you know, other animals, as if they were rational", "tokens": [50680, 2115, 1710, 466, 24617, 293, 6451, 4882, 11, 291, 458, 11, 661, 4882, 11, 382, 498, 436, 645, 15090, 51160], "temperature": 0.0, "avg_logprob": -0.11530047744067747, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.06290443241596222}, {"id": 992, "seek": 566120, "start": 5677.12, "end": 5685.679999999999, "text": " agents that act on the basis of what they believe and what they want. And by talking about them in", "tokens": [51160, 12554, 300, 605, 322, 264, 5143, 295, 437, 436, 1697, 293, 437, 436, 528, 13, 400, 538, 1417, 466, 552, 294, 51588], "temperature": 0.0, "avg_logprob": -0.11530047744067747, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.06290443241596222}, {"id": 993, "seek": 568568, "start": 5685.68, "end": 5692.72, "text": " those ways, whether they really, whatever that means, do believe things or have desires, it's", "tokens": [50364, 729, 2098, 11, 1968, 436, 534, 11, 2035, 300, 1355, 11, 360, 1697, 721, 420, 362, 18005, 11, 309, 311, 50716], "temperature": 0.0, "avg_logprob": -0.15282442304823135, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.13589102029800415}, {"id": 994, "seek": 568568, "start": 5692.72, "end": 5697.4400000000005, "text": " useful for explaining and understanding their behavior. So if we adopt the intentional stance,", "tokens": [50716, 4420, 337, 13468, 293, 3701, 641, 5223, 13, 407, 498, 321, 6878, 264, 21935, 21033, 11, 50952], "temperature": 0.0, "avg_logprob": -0.15282442304823135, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.13589102029800415}, {"id": 995, "seek": 568568, "start": 5697.4400000000005, "end": 5703.200000000001, "text": " say, to use one of Dan Dennett's own examples towards a chess machine, a chess computer,", "tokens": [50952, 584, 11, 281, 764, 472, 295, 3394, 19027, 3093, 311, 1065, 5110, 3030, 257, 24122, 3479, 11, 257, 24122, 3820, 11, 51240], "temperature": 0.0, "avg_logprob": -0.15282442304823135, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.13589102029800415}, {"id": 996, "seek": 568568, "start": 5703.76, "end": 5711.92, "text": " chess program, or Go program, and we might say, oh, it advanced its queen because it wants to try", "tokens": [51268, 24122, 1461, 11, 420, 1037, 1461, 11, 293, 321, 1062, 584, 11, 1954, 11, 309, 7339, 1080, 12206, 570, 309, 2738, 281, 853, 51676], "temperature": 0.0, "avg_logprob": -0.15282442304823135, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.13589102029800415}, {"id": 997, "seek": 571192, "start": 5711.92, "end": 5718.16, "text": " and pin down my rook. And this is just a natural way of speaking. And if we use that way of speaking,", "tokens": [50364, 293, 5447, 760, 452, 24692, 13, 400, 341, 307, 445, 257, 3303, 636, 295, 4124, 13, 400, 498, 321, 764, 300, 636, 295, 4124, 11, 50676], "temperature": 0.0, "avg_logprob": -0.08939336429942737, "compression_ratio": 1.74609375, "no_speech_prob": 0.08846135437488556}, {"id": 998, "seek": 571192, "start": 5718.16, "end": 5727.4400000000005, "text": " then it's just good in every way because we can then discuss among ourselves what the machine", "tokens": [50676, 550, 309, 311, 445, 665, 294, 633, 636, 570, 321, 393, 550, 2248, 3654, 4175, 437, 264, 3479, 51140], "temperature": 0.0, "avg_logprob": -0.08939336429942737, "compression_ratio": 1.74609375, "no_speech_prob": 0.08846135437488556}, {"id": 999, "seek": 571192, "start": 5727.4400000000005, "end": 5731.12, "text": " is doing, we can explain what it's doing, we can predict what it's going to do. So that's", "tokens": [51140, 307, 884, 11, 321, 393, 2903, 437, 309, 311, 884, 11, 321, 393, 6069, 437, 309, 311, 516, 281, 360, 13, 407, 300, 311, 51324], "temperature": 0.0, "avg_logprob": -0.08939336429942737, "compression_ratio": 1.74609375, "no_speech_prob": 0.08846135437488556}, {"id": 1000, "seek": 571192, "start": 5731.12, "end": 5736.32, "text": " taking the intentional stance. And it doesn't necessarily bring with it a belief that these", "tokens": [51324, 1940, 264, 21935, 21033, 13, 400, 309, 1177, 380, 4725, 1565, 365, 309, 257, 7107, 300, 613, 51584], "temperature": 0.0, "avg_logprob": -0.08939336429942737, "compression_ratio": 1.74609375, "no_speech_prob": 0.08846135437488556}, {"id": 1001, "seek": 571192, "start": 5736.32, "end": 5740.08, "text": " concepts are literally applicable. Maybe they are, maybe they're not.", "tokens": [51584, 10392, 366, 3736, 21142, 13, 2704, 436, 366, 11, 1310, 436, 434, 406, 13, 51772], "temperature": 0.0, "avg_logprob": -0.08939336429942737, "compression_ratio": 1.74609375, "no_speech_prob": 0.08846135437488556}, {"id": 1002, "seek": 574008, "start": 5740.88, "end": 5745.2, "text": " But this is where it gets interesting. So I discussed abduction, actually, when I spoke with", "tokens": [50404, 583, 341, 307, 689, 309, 2170, 1880, 13, 407, 286, 7152, 410, 40335, 11, 767, 11, 562, 286, 7179, 365, 50620], "temperature": 0.0, "avg_logprob": -0.1293103629295979, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0040079159662127495}, {"id": 1003, "seek": 574008, "start": 5746.08, "end": 5749.92, "text": " Dan, because it's very closely related. I know you studied reasoning for many years.", "tokens": [50664, 3394, 11, 570, 309, 311, 588, 8185, 4077, 13, 286, 458, 291, 9454, 21577, 337, 867, 924, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1293103629295979, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0040079159662127495}, {"id": 1004, "seek": 574008, "start": 5749.92, "end": 5753.6, "text": " And the way I see it, when we adopt the intentional stance, what we're doing is we're", "tokens": [50856, 400, 264, 636, 286, 536, 309, 11, 562, 321, 6878, 264, 21935, 21033, 11, 437, 321, 434, 884, 307, 321, 434, 51040], "temperature": 0.0, "avg_logprob": -0.1293103629295979, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0040079159662127495}, {"id": 1005, "seek": 574008, "start": 5753.6, "end": 5759.5199999999995, "text": " kind of building a set of variables to describe the behavior of the entity. And you were just", "tokens": [51040, 733, 295, 2390, 257, 992, 295, 9102, 281, 6786, 264, 5223, 295, 264, 13977, 13, 400, 291, 645, 445, 51336], "temperature": 0.0, "avg_logprob": -0.1293103629295979, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0040079159662127495}, {"id": 1006, "seek": 574008, "start": 5759.5199999999995, "end": 5764.32, "text": " making the argument from the lens of Wittgenstein that the behavior is the only thing.", "tokens": [51336, 1455, 264, 6770, 490, 264, 6765, 295, 343, 593, 1766, 9089, 300, 264, 5223, 307, 264, 787, 551, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1293103629295979, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0040079159662127495}, {"id": 1007, "seek": 576432, "start": 5765.04, "end": 5774.16, "text": " No, no, no, no, no, no, no, no. I definitely don't think that we mean meant by behavior", "tokens": [50400, 883, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 13, 286, 2138, 500, 380, 519, 300, 321, 914, 4140, 538, 5223, 50856], "temperature": 0.0, "avg_logprob": -0.20662775144472226, "compression_ratio": 1.775609756097561, "no_speech_prob": 0.10405906289815903}, {"id": 1008, "seek": 576432, "start": 5774.16, "end": 5781.92, "text": " as the only thing. But certainly when we're deploying psychological terms, I don't think", "tokens": [50856, 382, 264, 787, 551, 13, 583, 3297, 562, 321, 434, 34198, 14346, 2115, 11, 286, 500, 380, 519, 51244], "temperature": 0.0, "avg_logprob": -0.20662775144472226, "compression_ratio": 1.775609756097561, "no_speech_prob": 0.10405906289815903}, {"id": 1009, "seek": 576432, "start": 5781.92, "end": 5787.599999999999, "text": " that in any sense behavior is the only thing that determines how we deploy psychological terms.", "tokens": [51244, 300, 294, 604, 2020, 5223, 307, 264, 787, 551, 300, 24799, 577, 321, 7274, 14346, 2115, 13, 51528], "temperature": 0.0, "avg_logprob": -0.20662775144472226, "compression_ratio": 1.775609756097561, "no_speech_prob": 0.10405906289815903}, {"id": 1010, "seek": 576432, "start": 5788.639999999999, "end": 5792.799999999999, "text": " You're absolutely right. So there's still a massive amount of ambiguity. So when we perform", "tokens": [51580, 509, 434, 3122, 558, 13, 407, 456, 311, 920, 257, 5994, 2372, 295, 46519, 13, 407, 562, 321, 2042, 51788], "temperature": 0.0, "avg_logprob": -0.20662775144472226, "compression_ratio": 1.775609756097561, "no_speech_prob": 0.10405906289815903}, {"id": 1011, "seek": 579280, "start": 5792.8, "end": 5799.04, "text": " abduction, we are creating a hypothesis and we're selecting out of an infinite set of possible", "tokens": [50364, 410, 40335, 11, 321, 366, 4084, 257, 17291, 293, 321, 434, 18182, 484, 295, 364, 13785, 992, 295, 1944, 50676], "temperature": 0.0, "avg_logprob": -0.11218948925242704, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0036571866367012262}, {"id": 1012, "seek": 579280, "start": 5799.04, "end": 5806.72, "text": " hypotheses. But the behavior gives us all of the information. So it's almost like if we knew", "tokens": [50676, 49969, 13, 583, 264, 5223, 2709, 505, 439, 295, 264, 1589, 13, 407, 309, 311, 1920, 411, 498, 321, 2586, 51060], "temperature": 0.0, "avg_logprob": -0.11218948925242704, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0036571866367012262}, {"id": 1013, "seek": 579280, "start": 5806.72, "end": 5811.52, "text": " how to create the correct explanation, we wouldn't be missing anything just by observing the behavior.", "tokens": [51060, 577, 281, 1884, 264, 3006, 10835, 11, 321, 2759, 380, 312, 5361, 1340, 445, 538, 22107, 264, 5223, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11218948925242704, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0036571866367012262}, {"id": 1014, "seek": 579280, "start": 5811.52, "end": 5816.16, "text": " So what are we talking about now? We're talking about chess machines or animals?", "tokens": [51300, 407, 437, 366, 321, 1417, 466, 586, 30, 492, 434, 1417, 466, 24122, 8379, 420, 4882, 30, 51532], "temperature": 0.0, "avg_logprob": -0.11218948925242704, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0036571866367012262}, {"id": 1015, "seek": 579280, "start": 5816.16, "end": 5819.2, "text": " Or what are we talking about? What's the context for this thought?", "tokens": [51532, 1610, 437, 366, 321, 1417, 466, 30, 708, 311, 264, 4319, 337, 341, 1194, 30, 51684], "temperature": 0.0, "avg_logprob": -0.11218948925242704, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0036571866367012262}, {"id": 1016, "seek": 581920, "start": 5819.28, "end": 5824.24, "text": " I guess it could work for both. So let's say I want to adopt the intentional stance for move 37.", "tokens": [50368, 286, 2041, 309, 727, 589, 337, 1293, 13, 407, 718, 311, 584, 286, 528, 281, 6878, 264, 21935, 21033, 337, 1286, 13435, 13, 50616], "temperature": 0.0, "avg_logprob": -0.0816942494491051, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.0038391277194023132}, {"id": 1017, "seek": 581920, "start": 5824.24, "end": 5830.72, "text": " And I do this abduction. So I build this plan that the agent had. So the agent had this intention", "tokens": [50616, 400, 286, 360, 341, 410, 40335, 13, 407, 286, 1322, 341, 1393, 300, 264, 9461, 632, 13, 407, 264, 9461, 632, 341, 7789, 50940], "temperature": 0.0, "avg_logprob": -0.0816942494491051, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.0038391277194023132}, {"id": 1018, "seek": 581920, "start": 5830.72, "end": 5835.76, "text": " and it took this sequence of steps. And I'm using that as a hypothesis to explain the behavior.", "tokens": [50940, 293, 309, 1890, 341, 8310, 295, 4439, 13, 400, 286, 478, 1228, 300, 382, 257, 17291, 281, 2903, 264, 5223, 13, 51192], "temperature": 0.0, "avg_logprob": -0.0816942494491051, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.0038391277194023132}, {"id": 1019, "seek": 581920, "start": 5835.76, "end": 5840.4, "text": " I'm adopting the intentional stance. But it's still highly ambiguous because I'm selecting", "tokens": [51192, 286, 478, 32328, 264, 21935, 21033, 13, 583, 309, 311, 920, 5405, 39465, 570, 286, 478, 18182, 51424], "temperature": 0.0, "avg_logprob": -0.0816942494491051, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.0038391277194023132}, {"id": 1020, "seek": 581920, "start": 5840.4, "end": 5844.72, "text": " out of an infinite set of possible hypotheses. Right. And in fact, in that particular case,", "tokens": [51424, 484, 295, 364, 13785, 992, 295, 1944, 49969, 13, 1779, 13, 400, 294, 1186, 11, 294, 300, 1729, 1389, 11, 51640], "temperature": 0.0, "avg_logprob": -0.0816942494491051, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.0038391277194023132}, {"id": 1021, "seek": 584472, "start": 5844.72, "end": 5851.84, "text": " it's almost certainly not the right way of thinking about it at all. Because unlike humans,", "tokens": [50364, 309, 311, 1920, 3297, 406, 264, 558, 636, 295, 1953, 466, 309, 412, 439, 13, 1436, 8343, 6255, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11018643944950426, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.025181271135807037}, {"id": 1022, "seek": 584472, "start": 5851.84, "end": 5856.56, "text": " who are when they're playing these games often do form plans. So if you're playing chess,", "tokens": [50720, 567, 366, 562, 436, 434, 2433, 613, 2813, 2049, 360, 1254, 5482, 13, 407, 498, 291, 434, 2433, 24122, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11018643944950426, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.025181271135807037}, {"id": 1023, "seek": 584472, "start": 5856.56, "end": 5861.4400000000005, "text": " you often do have a plan, I'm going to try and capture this area of the board and command this", "tokens": [50956, 291, 2049, 360, 362, 257, 1393, 11, 286, 478, 516, 281, 853, 293, 7983, 341, 1859, 295, 264, 3150, 293, 5622, 341, 51200], "temperature": 0.0, "avg_logprob": -0.11018643944950426, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.025181271135807037}, {"id": 1024, "seek": 584472, "start": 5861.4400000000005, "end": 5866.400000000001, "text": " area of the board say. And so I'm going to move these pieces around to try and do that. And you", "tokens": [51200, 1859, 295, 264, 3150, 584, 13, 400, 370, 286, 478, 516, 281, 1286, 613, 3755, 926, 281, 853, 293, 360, 300, 13, 400, 291, 51448], "temperature": 0.0, "avg_logprob": -0.11018643944950426, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.025181271135807037}, {"id": 1025, "seek": 584472, "start": 5866.400000000001, "end": 5872.88, "text": " might form a plan in terms of several moves, but ahead. But typically that's not the way,", "tokens": [51448, 1062, 1254, 257, 1393, 294, 2115, 295, 2940, 6067, 11, 457, 2286, 13, 583, 5850, 300, 311, 406, 264, 636, 11, 51772], "temperature": 0.0, "avg_logprob": -0.11018643944950426, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.025181271135807037}, {"id": 1026, "seek": 587288, "start": 5872.88, "end": 5878.88, "text": " that's not really the way AlphaGo works. So talking about it making plans isn't really the", "tokens": [50364, 300, 311, 406, 534, 264, 636, 20588, 12104, 1985, 13, 407, 1417, 466, 309, 1455, 5482, 1943, 380, 534, 264, 50664], "temperature": 0.0, "avg_logprob": -0.1903676022304578, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.006162082310765982}, {"id": 1027, "seek": 587288, "start": 5878.88, "end": 5885.04, "text": " right way of doing things. So it's interesting, actually, because the intentional stance,", "tokens": [50664, 558, 636, 295, 884, 721, 13, 407, 309, 311, 1880, 11, 767, 11, 570, 264, 21935, 21033, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1903676022304578, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.006162082310765982}, {"id": 1028, "seek": 587288, "start": 5885.04, "end": 5894.0, "text": " you know, maybe it's still white work, you can still talk about something forming plans maybe,", "tokens": [50972, 291, 458, 11, 1310, 309, 311, 920, 2418, 589, 11, 291, 393, 920, 751, 466, 746, 15745, 5482, 1310, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1903676022304578, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.006162082310765982}, {"id": 1029, "seek": 587288, "start": 5894.0, "end": 5900.08, "text": " but it's really not quite right in that case. When I say subjectivity, I'm not talking", "tokens": [51420, 457, 309, 311, 534, 406, 1596, 558, 294, 300, 1389, 13, 1133, 286, 584, 3983, 4253, 11, 286, 478, 406, 1417, 51724], "temperature": 0.0, "avg_logprob": -0.1903676022304578, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.006162082310765982}, {"id": 1030, "seek": 590008, "start": 5900.16, "end": 5906.48, "text": " about metaphysical or dualism, but the intentional stance clearly is a form of subjectivity.", "tokens": [50368, 466, 30946, 36280, 420, 11848, 1434, 11, 457, 264, 21935, 21033, 4448, 307, 257, 1254, 295, 3983, 4253, 13, 50684], "temperature": 0.0, "avg_logprob": -0.11462804433461782, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.00446571409702301}, {"id": 1031, "seek": 590008, "start": 5906.48, "end": 5913.5199999999995, "text": " And when we as a diverse collection of agents form our own intentional stances, it would seem to be", "tokens": [50684, 400, 562, 321, 382, 257, 9521, 5765, 295, 12554, 1254, 527, 1065, 21935, 342, 2676, 11, 309, 576, 1643, 281, 312, 51036], "temperature": 0.0, "avg_logprob": -0.11462804433461782, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.00446571409702301}, {"id": 1032, "seek": 590008, "start": 5914.16, "end": 5919.76, "text": " quite a chaotic, weird and wonderful thing, yet it seems to work. There seems to be,", "tokens": [51068, 1596, 257, 27013, 11, 3657, 293, 3715, 551, 11, 1939, 309, 2544, 281, 589, 13, 821, 2544, 281, 312, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11462804433461782, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.00446571409702301}, {"id": 1033, "seek": 590008, "start": 5919.76, "end": 5925.44, "text": " by the way, an interesting thing here is the way we ascertain agency and culpability is based on", "tokens": [51348, 538, 264, 636, 11, 364, 1880, 551, 510, 307, 264, 636, 321, 15526, 1408, 7934, 293, 11021, 79, 2310, 307, 2361, 322, 51632], "temperature": 0.0, "avg_logprob": -0.11462804433461782, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.00446571409702301}, {"id": 1034, "seek": 590008, "start": 5925.44, "end": 5929.36, "text": " the intentional stance. So you read a news article about someone being stabbed in Australia or", "tokens": [51632, 264, 21935, 21033, 13, 407, 291, 1401, 257, 2583, 7222, 466, 1580, 885, 35726, 294, 7060, 420, 51828], "temperature": 0.0, "avg_logprob": -0.11462804433461782, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.00446571409702301}, {"id": 1035, "seek": 592936, "start": 5929.36, "end": 5934.48, "text": " something like that. And the news article was trying to give reasonable explanations. Oh,", "tokens": [50364, 746, 411, 300, 13, 400, 264, 2583, 7222, 390, 1382, 281, 976, 10585, 28708, 13, 876, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1036, "seek": 592936, "start": 5934.48, "end": 5938.48, "text": " it was because he was in a cult or it was because he was religious or it was because,", "tokens": [50620, 309, 390, 570, 415, 390, 294, 257, 2376, 420, 309, 390, 570, 415, 390, 7185, 420, 309, 390, 570, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1037, "seek": 592936, "start": 5938.48, "end": 5943.12, "text": " and this helps us kind of assign moral valence to what just happened.", "tokens": [50820, 293, 341, 3665, 505, 733, 295, 6269, 9723, 1323, 655, 281, 437, 445, 2011, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1038, "seek": 592936, "start": 5943.12, "end": 5947.44, "text": " Yeah, yeah, absolutely. Yeah. You said something like, when we take the intentional stance that", "tokens": [51052, 865, 11, 1338, 11, 3122, 13, 865, 13, 509, 848, 746, 411, 11, 562, 321, 747, 264, 21935, 21033, 300, 51268], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1039, "seek": 592936, "start": 5947.44, "end": 5951.12, "text": " that is a form of subjectivity or something? Yes, would you agree with that?", "tokens": [51268, 300, 307, 257, 1254, 295, 3983, 4253, 420, 746, 30, 1079, 11, 576, 291, 3986, 365, 300, 30, 51452], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1040, "seek": 592936, "start": 5952.16, "end": 5956.96, "text": " So I wouldn't put it quite that way. I'm not quite sure exactly what you mean by that,", "tokens": [51504, 407, 286, 2759, 380, 829, 309, 1596, 300, 636, 13, 286, 478, 406, 1596, 988, 2293, 437, 291, 914, 538, 300, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1481658693343874, "compression_ratio": 1.7474048442906573, "no_speech_prob": 0.0021177222952246666}, {"id": 1041, "seek": 595696, "start": 5957.68, "end": 5962.08, "text": " but taking the intentional stance is, I think it's just adopting a certain terminology and a", "tokens": [50400, 457, 1940, 264, 21935, 21033, 307, 11, 286, 519, 309, 311, 445, 32328, 257, 1629, 27575, 293, 257, 50620], "temperature": 0.0, "avg_logprob": -0.10787020183744885, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0027984960470348597}, {"id": 1042, "seek": 595696, "start": 5962.08, "end": 5966.88, "text": " certain vocabulary for describing the behavior of something. So I don't think we need to bring", "tokens": [50620, 1629, 19864, 337, 16141, 264, 5223, 295, 746, 13, 407, 286, 500, 380, 519, 321, 643, 281, 1565, 50860], "temperature": 0.0, "avg_logprob": -0.10787020183744885, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0027984960470348597}, {"id": 1043, "seek": 595696, "start": 5966.88, "end": 5974.08, "text": " subjectivity into that at all, right? So I think maybe we're mixing up two completely different", "tokens": [50860, 3983, 4253, 666, 300, 412, 439, 11, 558, 30, 407, 286, 519, 1310, 321, 434, 11983, 493, 732, 2584, 819, 51220], "temperature": 0.0, "avg_logprob": -0.10787020183744885, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0027984960470348597}, {"id": 1044, "seek": 595696, "start": 5974.08, "end": 5977.44, "text": " senses of the word subjectivity here, which is something we should be very careful about.", "tokens": [51220, 17057, 295, 264, 1349, 3983, 4253, 510, 11, 597, 307, 746, 321, 820, 312, 588, 5026, 466, 13, 51388], "temperature": 0.0, "avg_logprob": -0.10787020183744885, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0027984960470348597}, {"id": 1045, "seek": 595696, "start": 5977.44, "end": 5983.36, "text": " So I think you mean subjectivity, you mean that you've just made your own choice between", "tokens": [51388, 407, 286, 519, 291, 914, 3983, 4253, 11, 291, 914, 300, 291, 600, 445, 1027, 428, 1065, 3922, 1296, 51684], "temperature": 0.0, "avg_logprob": -0.10787020183744885, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0027984960470348597}, {"id": 1046, "seek": 598336, "start": 5983.44, "end": 5987.36, "text": " different hypotheses. And so it's subjective. Is that, is that what you mean there?", "tokens": [50368, 819, 49969, 13, 400, 370, 309, 311, 25972, 13, 1119, 300, 11, 307, 300, 437, 291, 914, 456, 30, 50564], "temperature": 0.0, "avg_logprob": -0.21101819551908052, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.021314285695552826}, {"id": 1047, "seek": 598336, "start": 5987.36, "end": 5991.679999999999, "text": " Yes. So let's say, so for me as an observer, I might do some, let's call it probabilistic", "tokens": [50564, 1079, 13, 407, 718, 311, 584, 11, 370, 337, 385, 382, 364, 27878, 11, 286, 1062, 360, 512, 11, 718, 311, 818, 309, 31959, 3142, 50780], "temperature": 0.0, "avg_logprob": -0.21101819551908052, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.021314285695552826}, {"id": 1048, "seek": 598336, "start": 5991.679999999999, "end": 5998.5599999999995, "text": " reasoning. And for me, the most reasonable, rational explanation is this. And it's a for me", "tokens": [50780, 21577, 13, 400, 337, 385, 11, 264, 881, 10585, 11, 15090, 10835, 307, 341, 13, 400, 309, 311, 257, 337, 385, 51124], "temperature": 0.0, "avg_logprob": -0.21101819551908052, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.021314285695552826}, {"id": 1049, "seek": 598336, "start": 5998.5599999999995, "end": 6002.96, "text": " there. That's what I mean. That's what you're alluding to the subjectivity. Yeah. Yeah. Yeah.", "tokens": [51124, 456, 13, 663, 311, 437, 286, 914, 13, 663, 311, 437, 291, 434, 439, 33703, 281, 264, 3983, 4253, 13, 865, 13, 865, 13, 865, 13, 51344], "temperature": 0.0, "avg_logprob": -0.21101819551908052, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.021314285695552826}, {"id": 1050, "seek": 598336, "start": 6002.96, "end": 6011.04, "text": " Okay. Yeah. Well, so, so, so your for me is, I think that's that, that the sense in which", "tokens": [51344, 1033, 13, 865, 13, 1042, 11, 370, 11, 370, 11, 370, 428, 337, 385, 307, 11, 286, 519, 300, 311, 300, 11, 300, 264, 2020, 294, 597, 51748], "temperature": 0.0, "avg_logprob": -0.21101819551908052, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.021314285695552826}, {"id": 1051, "seek": 601104, "start": 6011.04, "end": 6015.68, "text": " that subjective is a very, very different one from the topic that we were talking about earlier on,", "tokens": [50364, 300, 25972, 307, 257, 588, 11, 588, 819, 472, 490, 264, 4829, 300, 321, 645, 1417, 466, 3071, 322, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1052, "seek": 601104, "start": 6015.68, "end": 6020.4, "text": " because I don't think there's anything philosophically problematic in, in, in saying that,", "tokens": [50596, 570, 286, 500, 380, 519, 456, 311, 1340, 14529, 984, 19011, 294, 11, 294, 11, 294, 1566, 300, 11, 50832], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1053, "seek": 601104, "start": 6020.4, "end": 6026.16, "text": " you know, that I made my choice. And that's my preference and so on. And so, and somebody might", "tokens": [50832, 291, 458, 11, 300, 286, 1027, 452, 3922, 13, 400, 300, 311, 452, 17502, 293, 370, 322, 13, 400, 370, 11, 293, 2618, 1062, 51120], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1054, "seek": 601104, "start": 6026.16, "end": 6030.0, "text": " say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically", "tokens": [51120, 584, 11, 731, 11, 300, 311, 445, 25972, 13, 400, 988, 11, 1392, 11, 456, 311, 11, 456, 311, 1825, 14529, 984, 51312], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1055, "seek": 601104, "start": 6030.0, "end": 6034.8, "text": " problematic about that, right? So, so they, but earlier on, we were talking about subjectivity,", "tokens": [51312, 19011, 466, 300, 11, 558, 30, 407, 11, 370, 436, 11, 457, 3071, 322, 11, 321, 645, 1417, 466, 3983, 4253, 11, 51552], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1056, "seek": 601104, "start": 6034.8, "end": 6040.4, "text": " which like the big capital S and where it's alluding to kind of something whole metaphysical", "tokens": [51552, 597, 411, 264, 955, 4238, 318, 293, 689, 309, 311, 439, 33703, 281, 733, 295, 746, 1379, 30946, 36280, 51832], "temperature": 0.0, "avg_logprob": -0.12414687020438057, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.003128432435914874}, {"id": 1057, "seek": 604040, "start": 6040.4, "end": 6046.96, "text": " thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different", "tokens": [50364, 551, 293, 264, 2663, 295, 11848, 1434, 808, 493, 13, 407, 11, 370, 11, 370, 11, 370, 1338, 11, 370, 286, 519, 341, 307, 257, 588, 819, 50692], "temperature": 0.0, "avg_logprob": -0.1583919688167735, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.0005680196336470544}, {"id": 1058, "seek": 604040, "start": 6046.96, "end": 6052.0, "text": " kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is", "tokens": [50692, 733, 295, 551, 13, 407, 300, 311, 2489, 13, 407, 337, 264, 29837, 295, 341, 3761, 11, 4238, 318, 3983, 4253, 307, 50944], "temperature": 0.0, "avg_logprob": -0.1583919688167735, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.0005680196336470544}, {"id": 1059, "seek": 604040, "start": 6052.0, "end": 6057.2, "text": " dualism and, and lowercase subjectivity is for me. Yeah, it's for me. Okay. Yeah. Okay.", "tokens": [50944, 11848, 1434, 293, 11, 293, 3126, 9765, 3983, 4253, 307, 337, 385, 13, 865, 11, 309, 311, 337, 385, 13, 1033, 13, 865, 13, 1033, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1583919688167735, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.0005680196336470544}, {"id": 1060, "seek": 604040, "start": 6059.04, "end": 6062.08, "text": " Can you tell me about the risks of anthropomorphisation?", "tokens": [51296, 1664, 291, 980, 385, 466, 264, 10888, 295, 22727, 32702, 7623, 30, 51448], "temperature": 0.0, "avg_logprob": -0.1583919688167735, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.0005680196336470544}, {"id": 1061, "seek": 604040, "start": 6062.639999999999, "end": 6068.799999999999, "text": " Yeah. So I think so in the context of, of contemporary artificial intelligence in particular,", "tokens": [51476, 865, 13, 407, 286, 519, 370, 294, 264, 4319, 295, 11, 295, 14878, 11677, 7599, 294, 1729, 11, 51784], "temperature": 0.0, "avg_logprob": -0.1583919688167735, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.0005680196336470544}, {"id": 1062, "seek": 606880, "start": 6069.68, "end": 6075.52, "text": " then, then the danger of anthropomorphism, I think, is in, is in thinking that,", "tokens": [50408, 550, 11, 550, 264, 4330, 295, 22727, 32702, 1434, 11, 286, 519, 11, 307, 294, 11, 307, 294, 1953, 300, 11, 50700], "temperature": 0.0, "avg_logprob": -0.14203476083689723, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0037507915403693914}, {"id": 1063, "seek": 606880, "start": 6076.4800000000005, "end": 6080.16, "text": " that a system such as a large language model, you know, a chatbot or something,", "tokens": [50748, 300, 257, 1185, 1270, 382, 257, 2416, 2856, 2316, 11, 291, 458, 11, 257, 5081, 18870, 420, 746, 11, 50932], "temperature": 0.0, "avg_logprob": -0.14203476083689723, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0037507915403693914}, {"id": 1064, "seek": 606880, "start": 6080.16, "end": 6084.72, "text": " thinking that it has capabilities and that it doesn't, that's as simple as that.", "tokens": [50932, 1953, 300, 309, 575, 10862, 293, 300, 309, 1177, 380, 11, 300, 311, 382, 2199, 382, 300, 13, 51160], "temperature": 0.0, "avg_logprob": -0.14203476083689723, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0037507915403693914}, {"id": 1065, "seek": 606880, "start": 6084.72, "end": 6090.56, "text": " Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So, so in,", "tokens": [51160, 5135, 11, 309, 311, 611, 1953, 11, 4317, 11, 300, 309, 31132, 10862, 300, 309, 775, 13, 407, 11, 370, 294, 11, 51452], "temperature": 0.0, "avg_logprob": -0.14203476083689723, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0037507915403693914}, {"id": 1066, "seek": 606880, "start": 6090.56, "end": 6095.84, "text": " so in both cases, I think we can go wrong, we can go wrong by, because they exhibit very human,", "tokens": [51452, 370, 294, 1293, 3331, 11, 286, 519, 321, 393, 352, 2085, 11, 321, 393, 352, 2085, 538, 11, 570, 436, 20487, 588, 1952, 11, 51716], "temperature": 0.0, "avg_logprob": -0.14203476083689723, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0037507915403693914}, {"id": 1067, "seek": 609584, "start": 6095.84, "end": 6101.52, "text": " like linguistic behaviour, we can just assume that they are going to be very human like in", "tokens": [50364, 411, 43002, 17229, 11, 321, 393, 445, 6552, 300, 436, 366, 516, 281, 312, 588, 1952, 411, 294, 50648], "temperature": 0.0, "avg_logprob": -0.09677634107957192, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.06904221326112747}, {"id": 1068, "seek": 609584, "start": 6101.52, "end": 6105.28, "text": " general in all of the rest of the behaviour that we encounter with them. But we often find that", "tokens": [50648, 2674, 294, 439, 295, 264, 1472, 295, 264, 17229, 300, 321, 8593, 365, 552, 13, 583, 321, 2049, 915, 300, 50836], "temperature": 0.0, "avg_logprob": -0.09677634107957192, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.06904221326112747}, {"id": 1069, "seek": 609584, "start": 6105.28, "end": 6111.2, "text": " that's not the case. So we can find that at one moment, a large language model might make a", "tokens": [50836, 300, 311, 406, 264, 1389, 13, 407, 321, 393, 915, 300, 412, 472, 1623, 11, 257, 2416, 2856, 2316, 1062, 652, 257, 51132], "temperature": 0.0, "avg_logprob": -0.09677634107957192, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.06904221326112747}, {"id": 1070, "seek": 609584, "start": 6111.2, "end": 6116.24, "text": " ridiculously stupid mistake that no child would make. And, and, and, and then the next moment,", "tokens": [51132, 41358, 6631, 6146, 300, 572, 1440, 576, 652, 13, 400, 11, 293, 11, 293, 11, 293, 550, 264, 958, 1623, 11, 51384], "temperature": 0.0, "avg_logprob": -0.09677634107957192, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.06904221326112747}, {"id": 1071, "seek": 609584, "start": 6116.24, "end": 6122.24, "text": " it's saying something extraordinarily profound philosophically, or, or summarising some,", "tokens": [51384, 309, 311, 1566, 746, 34557, 14382, 14529, 984, 11, 420, 11, 420, 14611, 3436, 512, 11, 51684], "temperature": 0.0, "avg_logprob": -0.09677634107957192, "compression_ratio": 1.7303370786516854, "no_speech_prob": 0.06904221326112747}, {"id": 1072, "seek": 612224, "start": 6122.24, "end": 6128.639999999999, "text": " in, you know, enormously difficult scientific article, you know, really accurately. So, which", "tokens": [50364, 294, 11, 291, 458, 11, 39669, 2252, 8134, 7222, 11, 291, 458, 11, 534, 20095, 13, 407, 11, 597, 50684], "temperature": 0.0, "avg_logprob": -0.11877386658279984, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.003967734519392252}, {"id": 1073, "seek": 612224, "start": 6128.639999999999, "end": 6132.4, "text": " is, so these things are kind of superhuman powers, or translating something into four", "tokens": [50684, 307, 11, 370, 613, 721, 366, 733, 295, 1687, 18796, 8674, 11, 420, 35030, 746, 666, 1451, 50872], "temperature": 0.0, "avg_logprob": -0.11877386658279984, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.003967734519392252}, {"id": 1074, "seek": 612224, "start": 6132.4, "end": 6136.639999999999, "text": " different languages all at once. And they're, they're sort of superhuman-ish capabilities.", "tokens": [50872, 819, 8650, 439, 412, 1564, 13, 400, 436, 434, 11, 436, 434, 1333, 295, 1687, 18796, 12, 742, 10862, 13, 51084], "temperature": 0.0, "avg_logprob": -0.11877386658279984, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.003967734519392252}, {"id": 1075, "seek": 612224, "start": 6137.5199999999995, "end": 6141.679999999999, "text": " So it's not, so it can actually be more than better than human in some directions.", "tokens": [51128, 407, 309, 311, 406, 11, 370, 309, 393, 767, 312, 544, 813, 1101, 813, 1952, 294, 512, 11095, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11877386658279984, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.003967734519392252}, {"id": 1076, "seek": 612224, "start": 6142.48, "end": 6147.76, "text": " And, but, but clearly very deficient in others, you know, with contemporary models that can make", "tokens": [51376, 400, 11, 457, 11, 457, 4448, 588, 19248, 1196, 294, 2357, 11, 291, 458, 11, 365, 14878, 5245, 300, 393, 652, 51640], "temperature": 0.0, "avg_logprob": -0.11877386658279984, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.003967734519392252}, {"id": 1077, "seek": 614776, "start": 6147.76, "end": 6153.92, "text": " all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say", "tokens": [50364, 439, 3685, 295, 6631, 8038, 11, 436, 393, 1497, 455, 5256, 11, 436, 393, 652, 13603, 295, 21577, 293, 445, 584, 50672], "temperature": 0.0, "avg_logprob": -0.10744678440378673, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.008262580260634422}, {"id": 1078, "seek": 614776, "start": 6153.92, "end": 6159.6, "text": " daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake", "tokens": [50672, 1120, 844, 11, 5101, 1120, 844, 721, 13, 407, 11, 370, 729, 11, 370, 729, 366, 5110, 295, 689, 11, 291, 458, 11, 309, 311, 257, 6146, 50956], "temperature": 0.0, "avg_logprob": -0.10744678440378673, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.008262580260634422}, {"id": 1079, "seek": 614776, "start": 6159.6, "end": 6164.88, "text": " to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human,", "tokens": [50956, 281, 11, 322, 264, 5143, 295, 11, 291, 458, 11, 257, 1629, 2372, 295, 9285, 281, 519, 11, 1954, 11, 309, 311, 445, 411, 257, 1952, 11, 51220], "temperature": 0.0, "avg_logprob": -0.10744678440378673, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.008262580260634422}, {"id": 1080, "seek": 614776, "start": 6164.88, "end": 6169.92, "text": " you know, because you can just, you can just misjudge it in many ways. That's one thing. There's", "tokens": [51220, 291, 458, 11, 570, 291, 393, 445, 11, 291, 393, 445, 3346, 9218, 432, 309, 294, 867, 2098, 13, 663, 311, 472, 551, 13, 821, 311, 51472], "temperature": 0.0, "avg_logprob": -0.10744678440378673, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.008262580260634422}, {"id": 1081, "seek": 614776, "start": 6169.92, "end": 6174.0, "text": " another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive", "tokens": [51472, 1071, 11, 661, 7270, 295, 22727, 32702, 1434, 13, 407, 300, 311, 11, 300, 311, 445, 294, 2115, 295, 1080, 15605, 51676], "temperature": 0.0, "avg_logprob": -0.10744678440378673, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.008262580260634422}, {"id": 1082, "seek": 617400, "start": 6174.0, "end": 6178.72, "text": " capabilities, if you like. But there are other problems with anthropomorphism. So if we see", "tokens": [50364, 10862, 11, 498, 291, 411, 13, 583, 456, 366, 661, 2740, 365, 22727, 32702, 1434, 13, 407, 498, 321, 536, 50600], "temperature": 0.0, "avg_logprob": -0.05368925155477321, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.004875272046774626}, {"id": 1083, "seek": 617400, "start": 6180.0, "end": 6187.04, "text": " empathy there, where there isn't real empathy, then we may trust something where there's no", "tokens": [50664, 18701, 456, 11, 689, 456, 1943, 380, 957, 18701, 11, 550, 321, 815, 3361, 746, 689, 456, 311, 572, 51016], "temperature": 0.0, "avg_logprob": -0.05368925155477321, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.004875272046774626}, {"id": 1084, "seek": 617400, "start": 6187.04, "end": 6193.44, "text": " real basis for trust. And so that's a problem as well. You know, we may form, people may form", "tokens": [51016, 957, 5143, 337, 3361, 13, 400, 370, 300, 311, 257, 1154, 382, 731, 13, 509, 458, 11, 321, 815, 1254, 11, 561, 815, 1254, 51336], "temperature": 0.0, "avg_logprob": -0.05368925155477321, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.004875272046774626}, {"id": 1085, "seek": 617400, "start": 6193.44, "end": 6201.36, "text": " relationships with, with, you know, AI companions and social AI, where they're kind of fooling", "tokens": [51336, 6159, 365, 11, 365, 11, 291, 458, 11, 7318, 28009, 293, 2093, 7318, 11, 689, 436, 434, 733, 295, 7979, 278, 51732], "temperature": 0.0, "avg_logprob": -0.05368925155477321, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.004875272046774626}, {"id": 1086, "seek": 620136, "start": 6201.44, "end": 6206.08, "text": " themselves into thinking that there's a basis for that in emotion, where there is in humans,", "tokens": [50368, 2969, 666, 1953, 300, 456, 311, 257, 5143, 337, 300, 294, 8913, 11, 689, 456, 307, 294, 6255, 11, 50600], "temperature": 0.0, "avg_logprob": -0.0994797304642102, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.005973320920020342}, {"id": 1087, "seek": 620136, "start": 6206.08, "end": 6212.639999999999, "text": " and it's not there in, in, in contemporary AI. And I think that all those things are problematic.", "tokens": [50600, 293, 309, 311, 406, 456, 294, 11, 294, 11, 294, 14878, 7318, 13, 400, 286, 519, 300, 439, 729, 721, 366, 19011, 13, 50928], "temperature": 0.0, "avg_logprob": -0.0994797304642102, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.005973320920020342}, {"id": 1088, "seek": 620136, "start": 6212.639999999999, "end": 6218.639999999999, "text": " So things to do with trust, to do with friendship and empathy, and all of these, these things,", "tokens": [50928, 407, 721, 281, 360, 365, 3361, 11, 281, 360, 365, 13216, 293, 18701, 11, 293, 439, 295, 613, 11, 613, 721, 11, 51228], "temperature": 0.0, "avg_logprob": -0.0994797304642102, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.005973320920020342}, {"id": 1089, "seek": 620136, "start": 6218.639999999999, "end": 6225.12, "text": " I think, where we can go wrong in seeing, seeing them as too, you know, as more human-like than", "tokens": [51228, 286, 519, 11, 689, 321, 393, 352, 2085, 294, 2577, 11, 2577, 552, 382, 886, 11, 291, 458, 11, 382, 544, 1952, 12, 4092, 813, 51552], "temperature": 0.0, "avg_logprob": -0.0994797304642102, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.005973320920020342}, {"id": 1090, "seek": 620136, "start": 6225.12, "end": 6230.719999999999, "text": " they really are. One of the issues I have with anthropomorphism is that people ascribe mental", "tokens": [51552, 436, 534, 366, 13, 1485, 295, 264, 2663, 286, 362, 365, 22727, 32702, 1434, 307, 300, 561, 382, 8056, 4973, 51832], "temperature": 0.0, "avg_logprob": -0.0994797304642102, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.005973320920020342}, {"id": 1091, "seek": 623072, "start": 6230.72, "end": 6235.92, "text": " content when it's not there. And I think you are talking about literal anthropomorphism,", "tokens": [50364, 2701, 562, 309, 311, 406, 456, 13, 400, 286, 519, 291, 366, 1417, 466, 20411, 22727, 32702, 1434, 11, 50624], "temperature": 0.0, "avg_logprob": -0.16868226896456587, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.011182672344148159}, {"id": 1092, "seek": 623072, "start": 6235.92, "end": 6242.64, "text": " which is that they see human-like qualities when they are not there. And to me, that, that's an", "tokens": [50624, 597, 307, 300, 436, 536, 1952, 12, 4092, 16477, 562, 436, 366, 406, 456, 13, 400, 281, 385, 11, 300, 11, 300, 311, 364, 50960], "temperature": 0.0, "avg_logprob": -0.16868226896456587, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.011182672344148159}, {"id": 1093, "seek": 623072, "start": 6242.64, "end": 6248.88, "text": " important distinction, because I think if I understand you correctly, you, I mean, you're", "tokens": [50960, 1021, 16844, 11, 570, 286, 519, 498, 286, 1223, 291, 8944, 11, 291, 11, 286, 914, 11, 291, 434, 51272], "temperature": 0.0, "avg_logprob": -0.16868226896456587, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.011182672344148159}, {"id": 1094, "seek": 623072, "start": 6248.88, "end": 6252.400000000001, "text": " very known nonsense. You say that current large language models, they don't reason, they don't", "tokens": [51272, 588, 2570, 14925, 13, 509, 584, 300, 2190, 2416, 2856, 5245, 11, 436, 500, 380, 1778, 11, 436, 500, 380, 51448], "temperature": 0.0, "avg_logprob": -0.16868226896456587, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.011182672344148159}, {"id": 1095, "seek": 623072, "start": 6252.400000000001, "end": 6256.56, "text": " form beliefs, they don't have a sense. Oh, no, I didn't say exactly that. Oh, did you not? That's", "tokens": [51448, 1254, 13585, 11, 436, 500, 380, 362, 257, 2020, 13, 876, 11, 572, 11, 286, 994, 380, 584, 2293, 300, 13, 876, 11, 630, 291, 406, 30, 663, 311, 51656], "temperature": 0.0, "avg_logprob": -0.16868226896456587, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.011182672344148159}, {"id": 1096, "seek": 625656, "start": 6257.280000000001, "end": 6261.76, "text": " that's a broad, that's a much broader claim. So that's right. So I'm not sure I'd go so far as to", "tokens": [50400, 300, 311, 257, 4152, 11, 300, 311, 257, 709, 13227, 3932, 13, 407, 300, 311, 558, 13, 407, 286, 478, 406, 988, 286, 1116, 352, 370, 1400, 382, 281, 50624], "temperature": 0.0, "avg_logprob": -0.13637271523475647, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.008787987753748894}, {"id": 1097, "seek": 625656, "start": 6261.76, "end": 6268.320000000001, "text": " say they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what", "tokens": [50624, 584, 436, 500, 380, 1778, 11, 420, 436, 500, 380, 1223, 11, 420, 436, 11, 420, 436, 500, 380, 1254, 13585, 11, 457, 2831, 437, 50952], "temperature": 0.0, "avg_logprob": -0.13637271523475647, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.008787987753748894}, {"id": 1098, "seek": 625656, "start": 6268.320000000001, "end": 6274.240000000001, "text": " my, my, my approach is to say that we should be very cautious in using those terms. So I'm, so", "tokens": [50952, 452, 11, 452, 11, 452, 3109, 307, 281, 584, 300, 321, 820, 312, 588, 25278, 294, 1228, 729, 2115, 13, 407, 286, 478, 11, 370, 51248], "temperature": 0.0, "avg_logprob": -0.13637271523475647, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.008787987753748894}, {"id": 1099, "seek": 625656, "start": 6274.240000000001, "end": 6278.400000000001, "text": " those would all be examples of taking the intentional stance. If we were to use those", "tokens": [51248, 729, 576, 439, 312, 5110, 295, 1940, 264, 21935, 21033, 13, 759, 321, 645, 281, 764, 729, 51456], "temperature": 0.0, "avg_logprob": -0.13637271523475647, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.008787987753748894}, {"id": 1100, "seek": 625656, "start": 6278.400000000001, "end": 6284.64, "text": " bits of terminology to describe what, what a current, you know, chatbot or conversational AI", "tokens": [51456, 9239, 295, 27575, 281, 6786, 437, 11, 437, 257, 2190, 11, 291, 458, 11, 5081, 18870, 420, 2615, 1478, 7318, 51768], "temperature": 0.0, "avg_logprob": -0.13637271523475647, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.008787987753748894}, {"id": 1101, "seek": 628464, "start": 6284.64, "end": 6290.0, "text": " was doing, we'd be taking the intentional stance, and it's perfectly reasonable to do that in very,", "tokens": [50364, 390, 884, 11, 321, 1116, 312, 1940, 264, 21935, 21033, 11, 293, 309, 311, 6239, 10585, 281, 360, 300, 294, 588, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1102, "seek": 628464, "start": 6290.0, "end": 6295.4400000000005, "text": " very many cases. So I would know, and I would never make the blanket claim, they don't understand.", "tokens": [50632, 588, 867, 3331, 13, 407, 286, 576, 458, 11, 293, 286, 576, 1128, 652, 264, 17907, 3932, 11, 436, 500, 380, 1223, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1103, "seek": 628464, "start": 6295.4400000000005, "end": 6300.56, "text": " I think that that's not quite right. I think rather, rather it's that, you know, sometimes", "tokens": [50904, 286, 519, 300, 300, 311, 406, 1596, 558, 13, 286, 519, 2831, 11, 2831, 309, 311, 300, 11, 291, 458, 11, 2171, 51160], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1104, "seek": 628464, "start": 6300.56, "end": 6304.72, "text": " it's appropriate to say, oh, yeah, it seems to understand very well what this big long article", "tokens": [51160, 309, 311, 6854, 281, 584, 11, 1954, 11, 1338, 11, 309, 2544, 281, 1223, 588, 731, 437, 341, 955, 938, 7222, 51368], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1105, "seek": 628464, "start": 6304.72, "end": 6308.8, "text": " about nuclear physics was, was all about. And it summarized it really well. It really understood", "tokens": [51368, 466, 8179, 10649, 390, 11, 390, 439, 466, 13, 400, 309, 14611, 1602, 309, 534, 731, 13, 467, 534, 7320, 51572], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1106, "seek": 628464, "start": 6308.8, "end": 6312.4800000000005, "text": " it. You know, somebody might come up, might say, you know, it really did seem to understand it.", "tokens": [51572, 309, 13, 509, 458, 11, 2618, 1062, 808, 493, 11, 1062, 584, 11, 291, 458, 11, 309, 534, 630, 1643, 281, 1223, 309, 13, 51756], "temperature": 0.0, "avg_logprob": -0.11341362783353623, "compression_ratio": 1.855305466237942, "no_speech_prob": 0.04660794511437416}, {"id": 1107, "seek": 631248, "start": 6312.719999999999, "end": 6319.5199999999995, "text": " I think I wouldn't say that they were wrong in using that phrase there. So, but then on another", "tokens": [50376, 286, 519, 286, 2759, 380, 584, 300, 436, 645, 2085, 294, 1228, 300, 9535, 456, 13, 407, 11, 457, 550, 322, 1071, 50716], "temperature": 0.0, "avg_logprob": -0.12803280353546143, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0049838051199913025}, {"id": 1108, "seek": 631248, "start": 6319.5199999999995, "end": 6325.759999999999, "text": " occasion, you might find that it's, that it, for example, recently, people have been posing this,", "tokens": [50716, 9674, 11, 291, 1062, 915, 300, 309, 311, 11, 300, 309, 11, 337, 1365, 11, 3938, 11, 561, 362, 668, 40378, 341, 11, 51028], "temperature": 0.0, "avg_logprob": -0.12803280353546143, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0049838051199913025}, {"id": 1109, "seek": 631248, "start": 6325.759999999999, "end": 6332.16, "text": " you know, this classic goat, cabbage, Fox problem where you've got a boat and you have to cross a", "tokens": [51028, 291, 458, 11, 341, 7230, 23608, 11, 22944, 11, 11388, 1154, 689, 291, 600, 658, 257, 6582, 293, 291, 362, 281, 3278, 257, 51348], "temperature": 0.0, "avg_logprob": -0.12803280353546143, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0049838051199913025}, {"id": 1110, "seek": 631248, "start": 6332.16, "end": 6336.639999999999, "text": " river with a goat, and you can't have the, you know, more than two things in the boat at once,", "tokens": [51348, 6810, 365, 257, 23608, 11, 293, 291, 393, 380, 362, 264, 11, 291, 458, 11, 544, 813, 732, 721, 294, 264, 6582, 412, 1564, 11, 51572], "temperature": 0.0, "avg_logprob": -0.12803280353546143, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0049838051199913025}, {"id": 1111, "seek": 631248, "start": 6336.639999999999, "end": 6339.599999999999, "text": " and you can't have the goat with the cabbage and all this kind of stuff. And so there's a,", "tokens": [51572, 293, 291, 393, 380, 362, 264, 23608, 365, 264, 22944, 293, 439, 341, 733, 295, 1507, 13, 400, 370, 456, 311, 257, 11, 51720], "temperature": 0.0, "avg_logprob": -0.12803280353546143, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0049838051199913025}, {"id": 1112, "seek": 633960, "start": 6339.6, "end": 6344.4800000000005, "text": " it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery,", "tokens": [50364, 309, 311, 257, 707, 12805, 293, 291, 445, 362, 281, 733, 295, 3278, 3195, 295, 1413, 293, 360, 439, 3685, 295, 4282, 2109, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12571679312607337, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03093128092586994}, {"id": 1113, "seek": 633960, "start": 6344.4800000000005, "end": 6351.120000000001, "text": " right? And people opposed, opposed to it to some large language models that said, okay,", "tokens": [50608, 558, 30, 400, 561, 8851, 11, 8851, 281, 309, 281, 512, 2416, 2856, 5245, 300, 848, 11, 1392, 11, 50940], "temperature": 0.0, "avg_logprob": -0.12571679312607337, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03093128092586994}, {"id": 1114, "seek": 633960, "start": 6351.120000000001, "end": 6355.68, "text": " I've got a boat and a cabbage and I need to get the cabbage to the other side of the river.", "tokens": [50940, 286, 600, 658, 257, 6582, 293, 257, 22944, 293, 286, 643, 281, 483, 264, 22944, 281, 264, 661, 1252, 295, 264, 6810, 13, 51168], "temperature": 0.0, "avg_logprob": -0.12571679312607337, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03093128092586994}, {"id": 1115, "seek": 633960, "start": 6355.68, "end": 6360.96, "text": " How do I do it? And the large language model models, several of them just start to come up with", "tokens": [51168, 1012, 360, 286, 360, 309, 30, 400, 264, 2416, 2856, 2316, 5245, 11, 2940, 295, 552, 445, 722, 281, 808, 493, 365, 51432], "temperature": 0.0, "avg_logprob": -0.12571679312607337, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03093128092586994}, {"id": 1116, "seek": 633960, "start": 6360.96, "end": 6365.04, "text": " this totally baroque solution that involves going backwards and forwards over there. And sometimes", "tokens": [51432, 341, 3879, 2159, 29743, 3827, 300, 11626, 516, 12204, 293, 30126, 670, 456, 13, 400, 2171, 51636], "temperature": 0.0, "avg_logprob": -0.12571679312607337, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03093128092586994}, {"id": 1117, "seek": 636504, "start": 6365.04, "end": 6369.44, "text": " they invent goats that aren't even in the, and that's because they've overfitted or they're kind", "tokens": [50364, 436, 7962, 34219, 300, 3212, 380, 754, 294, 264, 11, 293, 300, 311, 570, 436, 600, 670, 69, 3944, 420, 436, 434, 733, 50584], "temperature": 0.0, "avg_logprob": -0.11747386455535888, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.06474229693412781}, {"id": 1118, "seek": 636504, "start": 6369.44, "end": 6375.92, "text": " of like connected with this classic problem. And, and so no child would make this stupid,", "tokens": [50584, 295, 411, 4582, 365, 341, 7230, 1154, 13, 400, 11, 293, 370, 572, 1440, 576, 652, 341, 6631, 11, 50908], "temperature": 0.0, "avg_logprob": -0.11747386455535888, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.06474229693412781}, {"id": 1119, "seek": 636504, "start": 6375.92, "end": 6380.4, "text": " stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously", "tokens": [50908, 6631, 6146, 13, 407, 4033, 11, 370, 456, 291, 576, 584, 11, 731, 11, 291, 458, 11, 445, 11, 291, 458, 11, 445, 2745, 51132], "temperature": 0.0, "avg_logprob": -0.11747386455535888, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.06474229693412781}, {"id": 1120, "seek": 636504, "start": 6380.4, "end": 6385.28, "text": " just didn't understand what the, you know, and of course, it's quite right to say didn't understand", "tokens": [51132, 445, 994, 380, 1223, 437, 264, 11, 291, 458, 11, 293, 295, 1164, 11, 309, 311, 1596, 558, 281, 584, 994, 380, 1223, 51376], "temperature": 0.0, "avg_logprob": -0.11747386455535888, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.06474229693412781}, {"id": 1121, "seek": 636504, "start": 6385.28, "end": 6392.16, "text": " in that case. And the anthropomorphism, it comes about when you think that it understands", "tokens": [51376, 294, 300, 1389, 13, 400, 264, 22727, 32702, 1434, 11, 309, 1487, 466, 562, 291, 519, 300, 309, 15146, 51720], "temperature": 0.0, "avg_logprob": -0.11747386455535888, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.06474229693412781}, {"id": 1122, "seek": 639216, "start": 6392.16, "end": 6398.0, "text": " when we understand and doesn't understand when we don't understand. The reality is that sometimes", "tokens": [50364, 562, 321, 1223, 293, 1177, 380, 1223, 562, 321, 500, 380, 1223, 13, 440, 4103, 307, 300, 2171, 50656], "temperature": 0.0, "avg_logprob": -0.1371747185202206, "compression_ratio": 1.8976377952755905, "no_speech_prob": 0.0012290732702240348}, {"id": 1123, "seek": 639216, "start": 6398.0, "end": 6403.28, "text": " it understands when we understand and sometimes it won't and some, it's all mixed up, right? So", "tokens": [50656, 309, 15146, 562, 321, 1223, 293, 2171, 309, 1582, 380, 293, 512, 11, 309, 311, 439, 7467, 493, 11, 558, 30, 407, 50920], "temperature": 0.0, "avg_logprob": -0.1371747185202206, "compression_ratio": 1.8976377952755905, "no_speech_prob": 0.0012290732702240348}, {"id": 1124, "seek": 639216, "start": 6403.28, "end": 6409.599999999999, "text": " the mistake is to think that it is like us. I could, yeah, man, that was beautifully articulated.", "tokens": [50920, 264, 6146, 307, 281, 519, 300, 309, 307, 411, 505, 13, 286, 727, 11, 1338, 11, 587, 11, 300, 390, 16525, 43322, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1371747185202206, "compression_ratio": 1.8976377952755905, "no_speech_prob": 0.0012290732702240348}, {"id": 1125, "seek": 639216, "start": 6409.599999999999, "end": 6415.84, "text": " So it's, it's, it's the mistake of thinking there is an alignment both in how the machines think", "tokens": [51236, 407, 309, 311, 11, 309, 311, 11, 309, 311, 264, 6146, 295, 1953, 456, 307, 364, 18515, 1293, 294, 577, 264, 8379, 519, 51548], "temperature": 0.0, "avg_logprob": -0.1371747185202206, "compression_ratio": 1.8976377952755905, "no_speech_prob": 0.0012290732702240348}, {"id": 1126, "seek": 639216, "start": 6415.84, "end": 6420.5599999999995, "text": " and where they make mistakes. But let's unpick this a little bit because you were saying it's", "tokens": [51548, 293, 689, 436, 652, 8038, 13, 583, 718, 311, 20994, 618, 341, 257, 707, 857, 570, 291, 645, 1566, 309, 311, 51784], "temperature": 0.0, "avg_logprob": -0.1371747185202206, "compression_ratio": 1.8976377952755905, "no_speech_prob": 0.0012290732702240348}, {"id": 1127, "seek": 642056, "start": 6420.56, "end": 6425.76, "text": " perfectly reasonable to take the intentional stance when the thing does the thing correctly,", "tokens": [50364, 6239, 10585, 281, 747, 264, 21935, 21033, 562, 264, 551, 775, 264, 551, 8944, 11, 50624], "temperature": 0.0, "avg_logprob": -0.11254301610982644, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.00823332741856575}, {"id": 1128, "seek": 642056, "start": 6425.76, "end": 6432.080000000001, "text": " even though it thinks differently to us. And that's absolutely fine. But I love thinking", "tokens": [50624, 754, 1673, 309, 7309, 7614, 281, 505, 13, 400, 300, 311, 3122, 2489, 13, 583, 286, 959, 1953, 50940], "temperature": 0.0, "avg_logprob": -0.11254301610982644, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.00823332741856575}, {"id": 1129, "seek": 642056, "start": 6432.080000000001, "end": 6436.8, "text": " about these things theoretically. And it's, it's delicious talking to you because you have a background", "tokens": [50940, 466, 613, 721, 29400, 13, 400, 309, 311, 11, 309, 311, 4809, 1417, 281, 291, 570, 291, 362, 257, 3678, 51176], "temperature": 0.0, "avg_logprob": -0.11254301610982644, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.00823332741856575}, {"id": 1130, "seek": 642056, "start": 6436.8, "end": 6442.56, "text": " in symbolic AI. You were, I'm sure around in the days of photo and pollution with their connectionist", "tokens": [51176, 294, 25755, 7318, 13, 509, 645, 11, 286, 478, 988, 926, 294, 264, 1708, 295, 5052, 293, 16727, 365, 641, 4984, 468, 51464], "temperature": 0.0, "avg_logprob": -0.11254301610982644, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.00823332741856575}, {"id": 1131, "seek": 642056, "start": 6442.56, "end": 6449.04, "text": " critique. And, and even now there are clear examples of language models not being able to do", "tokens": [51464, 25673, 13, 400, 11, 293, 754, 586, 456, 366, 1850, 5110, 295, 2856, 5245, 406, 885, 1075, 281, 360, 51788], "temperature": 0.0, "avg_logprob": -0.11254301610982644, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.00823332741856575}, {"id": 1132, "seek": 644904, "start": 6449.04, "end": 6454.64, "text": " negation. Oh, yeah. And we know they're not Turing machines. You know, we can make some strong", "tokens": [50364, 2485, 399, 13, 876, 11, 1338, 13, 400, 321, 458, 436, 434, 406, 314, 1345, 8379, 13, 509, 458, 11, 321, 393, 652, 512, 2068, 50644], "temperature": 0.0, "avg_logprob": -0.14156874294938712, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.013263065367937088}, {"id": 1133, "seek": 644904, "start": 6454.64, "end": 6460.0, "text": " theoretical statements that they are limited in reasoning. I agree with you that it's reasonable", "tokens": [50644, 20864, 12363, 300, 436, 366, 5567, 294, 21577, 13, 286, 3986, 365, 291, 300, 309, 311, 10585, 50912], "temperature": 0.0, "avg_logprob": -0.14156874294938712, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.013263065367937088}, {"id": 1134, "seek": 644904, "start": 6460.0, "end": 6464.88, "text": " to say they understand in certain circumstances. But, but, but where I want to get to is, okay,", "tokens": [50912, 281, 584, 436, 1223, 294, 1629, 9121, 13, 583, 11, 457, 11, 457, 689, 286, 528, 281, 483, 281, 307, 11, 1392, 11, 51156], "temperature": 0.0, "avg_logprob": -0.14156874294938712, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.013263065367937088}, {"id": 1135, "seek": 644904, "start": 6464.88, "end": 6470.08, "text": " so we agree that language models cannot perform certain types of reasoning that we can.", "tokens": [51156, 370, 321, 3986, 300, 2856, 5245, 2644, 2042, 1629, 3467, 295, 21577, 300, 321, 393, 13, 51416], "temperature": 0.0, "avg_logprob": -0.14156874294938712, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.013263065367937088}, {"id": 1136, "seek": 644904, "start": 6470.08, "end": 6475.12, "text": " Yeah. So I think we could, so I think we need to take each of these concepts individually. So,", "tokens": [51416, 865, 13, 407, 286, 519, 321, 727, 11, 370, 286, 519, 321, 643, 281, 747, 1184, 295, 613, 10392, 16652, 13, 407, 11, 51668], "temperature": 0.0, "avg_logprob": -0.14156874294938712, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.013263065367937088}, {"id": 1137, "seek": 647512, "start": 6475.599999999999, "end": 6480.8, "text": " we dealt a little bit just now with understanding reasoning as a whole separate thing. So, and,", "tokens": [50388, 321, 15991, 257, 707, 857, 445, 586, 365, 3701, 21577, 382, 257, 1379, 4994, 551, 13, 407, 11, 293, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1138, "seek": 647512, "start": 6480.8, "end": 6484.24, "text": " again, this is all because, you know, they're not like us. So we have to deal with these things", "tokens": [50648, 797, 11, 341, 307, 439, 570, 11, 291, 458, 11, 436, 434, 406, 411, 505, 13, 407, 321, 362, 281, 2028, 365, 613, 721, 50820], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1139, "seek": 647512, "start": 6484.24, "end": 6487.5199999999995, "text": " individually. We can't just blanket say, oh, they don't understand, they don't reason, they don't,", "tokens": [50820, 16652, 13, 492, 393, 380, 445, 17907, 584, 11, 1954, 11, 436, 500, 380, 1223, 11, 436, 500, 380, 1778, 11, 436, 500, 380, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1140, "seek": 647512, "start": 6487.5199999999995, "end": 6491.12, "text": " or they do understand, they do reason. It's not like that. It's, you have to take each of these", "tokens": [50984, 420, 436, 360, 1223, 11, 436, 360, 1778, 13, 467, 311, 406, 411, 300, 13, 467, 311, 11, 291, 362, 281, 747, 1184, 295, 613, 51164], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1141, "seek": 647512, "start": 6491.12, "end": 6497.12, "text": " concepts separately. So in the case of reasoning, then clearly today's large language models,", "tokens": [51164, 10392, 14759, 13, 407, 294, 264, 1389, 295, 21577, 11, 550, 4448, 965, 311, 2416, 2856, 5245, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1142, "seek": 647512, "start": 6497.12, "end": 6503.04, "text": " you know, do struggle very often with, with, with reasoning problems. Now, this is a kind of open", "tokens": [51464, 291, 458, 11, 360, 7799, 588, 2049, 365, 11, 365, 11, 365, 21577, 2740, 13, 823, 11, 341, 307, 257, 733, 295, 1269, 51760], "temperature": 0.0, "avg_logprob": -0.11684171945456691, "compression_ratio": 1.9331103678929766, "no_speech_prob": 0.01653115637600422}, {"id": 1143, "seek": 650304, "start": 6503.04, "end": 6510.72, "text": " research problem. And people are making a lot of progress in improving their ability to solve", "tokens": [50364, 2132, 1154, 13, 400, 561, 366, 1455, 257, 688, 295, 4205, 294, 11470, 641, 3485, 281, 5039, 50748], "temperature": 0.0, "avg_logprob": -0.11841895662505052, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.010315613821148872}, {"id": 1144, "seek": 650304, "start": 6510.72, "end": 6516.4, "text": " reasoning problems. Now, what the right approach to that is, you know, is an open research question.", "tokens": [50748, 21577, 2740, 13, 823, 11, 437, 264, 558, 3109, 281, 300, 307, 11, 291, 458, 11, 307, 364, 1269, 2132, 1168, 13, 51032], "temperature": 0.0, "avg_logprob": -0.11841895662505052, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.010315613821148872}, {"id": 1145, "seek": 650304, "start": 6516.4, "end": 6520.72, "text": " Maybe it's just you throw more training data at it with, with, with, that includes lots of", "tokens": [51032, 2704, 309, 311, 445, 291, 3507, 544, 3097, 1412, 412, 309, 365, 11, 365, 11, 365, 11, 300, 5974, 3195, 295, 51248], "temperature": 0.0, "avg_logprob": -0.11841895662505052, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.010315613821148872}, {"id": 1146, "seek": 650304, "start": 6520.72, "end": 6525.76, "text": " reasoning problems. And then eventually you get sufficient generalization there. And it's not", "tokens": [51248, 21577, 2740, 13, 400, 550, 4728, 291, 483, 11563, 2674, 2144, 456, 13, 400, 309, 311, 406, 51500], "temperature": 0.0, "avg_logprob": -0.11841895662505052, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.010315613821148872}, {"id": 1147, "seek": 652576, "start": 6525.84, "end": 6530.64, "text": " totally clear that that will work, but maybe it will. Maybe it's to embed your,", "tokens": [50368, 3879, 1850, 300, 300, 486, 589, 11, 457, 1310, 309, 486, 13, 2704, 309, 311, 281, 12240, 428, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1399310373626979, "compression_ratio": 1.85, "no_speech_prob": 0.341372549533844}, {"id": 1148, "seek": 652576, "start": 6532.64, "end": 6537.52, "text": " your, or, or, or to surround it to include it in your, in your system, not just the large", "tokens": [50708, 428, 11, 420, 11, 420, 11, 420, 281, 6262, 309, 281, 4090, 309, 294, 428, 11, 294, 428, 1185, 11, 406, 445, 264, 2416, 50952], "temperature": 0.0, "avg_logprob": -0.1399310373626979, "compression_ratio": 1.85, "no_speech_prob": 0.341372549533844}, {"id": 1149, "seek": 652576, "start": 6537.52, "end": 6542.96, "text": " language model, but making kind of external calls to, to say a planner or some kind of external", "tokens": [50952, 2856, 2316, 11, 457, 1455, 733, 295, 8320, 5498, 281, 11, 281, 584, 257, 31268, 420, 512, 733, 295, 8320, 51224], "temperature": 0.0, "avg_logprob": -0.1399310373626979, "compression_ratio": 1.85, "no_speech_prob": 0.341372549533844}, {"id": 1150, "seek": 652576, "start": 6542.96, "end": 6546.72, "text": " reasoning system. And you bring that in and you incorporate, you make something that's kind of", "tokens": [51224, 21577, 1185, 13, 400, 291, 1565, 300, 294, 293, 291, 16091, 11, 291, 652, 746, 300, 311, 733, 295, 51412], "temperature": 0.0, "avg_logprob": -0.1399310373626979, "compression_ratio": 1.85, "no_speech_prob": 0.341372549533844}, {"id": 1151, "seek": 652576, "start": 6546.72, "end": 6551.52, "text": " a hybrid that uses that, that kind of more symbolic approach. So you might do that.", "tokens": [51412, 257, 13051, 300, 4960, 300, 11, 300, 733, 295, 544, 25755, 3109, 13, 407, 291, 1062, 360, 300, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1399310373626979, "compression_ratio": 1.85, "no_speech_prob": 0.341372549533844}, {"id": 1152, "seek": 655152, "start": 6552.4800000000005, "end": 6560.8, "text": " Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced", "tokens": [50412, 1610, 291, 1062, 611, 1333, 295, 11, 370, 294, 452, 589, 365, 10902, 383, 495, 6326, 412, 11, 412, 14895, 44, 471, 11, 321, 11, 321, 7268, 50828], "temperature": 0.0, "avg_logprob": -0.1406113119686351, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.009785061702132225}, {"id": 1153, "seek": 655152, "start": 6560.8, "end": 6565.6, "text": " this selection inference framework where you basically, you treat the large language model", "tokens": [50828, 341, 9450, 38253, 8388, 689, 291, 1936, 11, 291, 2387, 264, 2416, 2856, 2316, 51068], "temperature": 0.0, "avg_logprob": -0.1406113119686351, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.009785061702132225}, {"id": 1154, "seek": 655152, "start": 6565.6, "end": 6570.88, "text": " as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes", "tokens": [51068, 382, 257, 733, 295, 10088, 300, 775, 9239, 295, 21577, 13, 400, 291, 362, 257, 11498, 9284, 300, 1669, 51332], "temperature": 0.0, "avg_logprob": -0.1406113119686351, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.009785061702132225}, {"id": 1155, "seek": 655152, "start": 6570.88, "end": 6576.8, "text": " calls to the, to, to, to, to the, to the module in a kind of algorithm that does a number of", "tokens": [51332, 5498, 281, 264, 11, 281, 11, 281, 11, 281, 11, 281, 264, 11, 281, 264, 10088, 294, 257, 733, 295, 9284, 300, 775, 257, 1230, 295, 51628], "temperature": 0.0, "avg_logprob": -0.1406113119686351, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.009785061702132225}, {"id": 1156, "seek": 655152, "start": 6576.8, "end": 6580.8, "text": " sequence of reasoning steps. So you have a kind of outer algorithm. So there's lots of ways of", "tokens": [51628, 8310, 295, 21577, 4439, 13, 407, 291, 362, 257, 733, 295, 10847, 9284, 13, 407, 456, 311, 3195, 295, 2098, 295, 51828], "temperature": 0.0, "avg_logprob": -0.1406113119686351, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.009785061702132225}, {"id": 1157, "seek": 658080, "start": 6580.88, "end": 6586.08, "text": " trying to tackle that problem. But yeah, just your basic large, take your basic large language", "tokens": [50368, 1382, 281, 14896, 300, 1154, 13, 583, 1338, 11, 445, 428, 3875, 2416, 11, 747, 428, 3875, 2416, 2856, 50628], "temperature": 0.0, "avg_logprob": -0.10775644402754934, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004481484182178974}, {"id": 1158, "seek": 658080, "start": 6586.08, "end": 6592.96, "text": " model today, as they are at the moment, and you put it in a chat interface, it's easy to find", "tokens": [50628, 2316, 965, 11, 382, 436, 366, 412, 264, 1623, 11, 293, 291, 829, 309, 294, 257, 5081, 9226, 11, 309, 311, 1858, 281, 915, 50972], "temperature": 0.0, "avg_logprob": -0.10775644402754934, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004481484182178974}, {"id": 1159, "seek": 658080, "start": 6592.96, "end": 6599.360000000001, "text": " reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host,", "tokens": [50972, 21577, 2740, 300, 366, 516, 281, 43164, 13, 1079, 11, 321, 11, 321, 3986, 322, 300, 13, 400, 767, 11, 452, 598, 12, 6037, 11, 51292], "temperature": 0.0, "avg_logprob": -0.10775644402754934, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004481484182178974}, {"id": 1160, "seek": 658080, "start": 6599.360000000001, "end": 6605.6, "text": " Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or", "tokens": [51292, 20613, 413, 697, 2976, 11, 415, 23122, 21577, 382, 10205, 364, 4942, 24903, 281, 28446, 3601, 420, 51604], "temperature": 0.0, "avg_logprob": -0.10775644402754934, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004481484182178974}, {"id": 1161, "seek": 660560, "start": 6605.6, "end": 6611.200000000001, "text": " achieve a goal. And he cites Claude Shannon, by the way, he said, Claude, Claude Shannon said,", "tokens": [50364, 4584, 257, 3387, 13, 400, 415, 269, 3324, 12947, 2303, 28974, 11, 538, 264, 636, 11, 415, 848, 11, 12947, 2303, 11, 12947, 2303, 28974, 848, 11, 50644], "temperature": 0.0, "avg_logprob": -0.1069953669672427, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.31191179156303406}, {"id": 1162, "seek": 660560, "start": 6611.200000000001, "end": 6615.360000000001, "text": " we may have knowledge of the past, but we cannot control it. We may control the future,", "tokens": [50644, 321, 815, 362, 3601, 295, 264, 1791, 11, 457, 321, 2644, 1969, 309, 13, 492, 815, 1969, 264, 2027, 11, 50852], "temperature": 0.0, "avg_logprob": -0.1069953669672427, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.31191179156303406}, {"id": 1163, "seek": 660560, "start": 6615.360000000001, "end": 6619.84, "text": " but we have no knowledge of it. And science leverages control to gain knowledge, engineering", "tokens": [50852, 457, 321, 362, 572, 3601, 295, 309, 13, 400, 3497, 12451, 1660, 1969, 281, 6052, 3601, 11, 7043, 51076], "temperature": 0.0, "avg_logprob": -0.1069953669672427, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.31191179156303406}, {"id": 1164, "seek": 660560, "start": 6619.84, "end": 6624.8, "text": " leverages knowledge to gain control. And reasoning is the effective computation in both. You know,", "tokens": [51076, 12451, 1660, 3601, 281, 6052, 1969, 13, 400, 21577, 307, 264, 4942, 24903, 294, 1293, 13, 509, 458, 11, 51324], "temperature": 0.0, "avg_logprob": -0.1069953669672427, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.31191179156303406}, {"id": 1165, "seek": 660560, "start": 6624.8, "end": 6629.6, "text": " maybe just in your own articulation, because we can cite examples of things like abduction,", "tokens": [51324, 1310, 445, 294, 428, 1065, 15228, 2776, 11, 570, 321, 393, 37771, 5110, 295, 721, 411, 410, 40335, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1069953669672427, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.31191179156303406}, {"id": 1166, "seek": 662960, "start": 6629.6, "end": 6637.6, "text": " which we've studied in great detail. And it feels like at the moment, even if we do farm out to", "tokens": [50364, 597, 321, 600, 9454, 294, 869, 2607, 13, 400, 309, 3417, 411, 412, 264, 1623, 11, 754, 498, 321, 360, 5421, 484, 281, 50764], "temperature": 0.0, "avg_logprob": -0.11634138006913035, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.026238907128572464}, {"id": 1167, "seek": 662960, "start": 6637.6, "end": 6643.04, "text": " Turing machine algorithms, in the days of symbolic AI, that was an intractable problem,", "tokens": [50764, 314, 1345, 3479, 14642, 11, 294, 264, 1708, 295, 25755, 7318, 11, 300, 390, 364, 560, 1897, 712, 1154, 11, 51036], "temperature": 0.0, "avg_logprob": -0.11634138006913035, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.026238907128572464}, {"id": 1168, "seek": 662960, "start": 6643.04, "end": 6649.84, "text": " because we've got this infinity, right? And it still seems to me that there are some problems,", "tokens": [51036, 570, 321, 600, 658, 341, 13202, 11, 558, 30, 400, 309, 920, 2544, 281, 385, 300, 456, 366, 512, 2740, 11, 51376], "temperature": 0.0, "avg_logprob": -0.11634138006913035, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.026238907128572464}, {"id": 1169, "seek": 662960, "start": 6649.84, "end": 6655.76, "text": " which there is no easy answer to. So I have a, I feel that you're alluding to", "tokens": [51376, 597, 456, 307, 572, 1858, 1867, 281, 13, 407, 286, 362, 257, 11, 286, 841, 300, 291, 434, 439, 33703, 281, 51672], "temperature": 0.0, "avg_logprob": -0.11634138006913035, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.026238907128572464}, {"id": 1170, "seek": 665576, "start": 6655.76, "end": 6663.4400000000005, "text": " Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean,", "tokens": [50364, 479, 34024, 311, 11, 512, 479, 34024, 2010, 12869, 510, 466, 410, 40335, 293, 1310, 11, 457, 286, 500, 380, 458, 13, 583, 286, 914, 11, 50748], "temperature": 0.0, "avg_logprob": -0.14019468910673744, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.008368737995624542}, {"id": 1171, "seek": 665576, "start": 6663.4400000000005, "end": 6669.76, "text": " I think that sort of abductive problems are where we are looking for an explanation for something.", "tokens": [50748, 286, 519, 300, 1333, 295, 410, 5020, 488, 2740, 366, 689, 321, 366, 1237, 337, 364, 10835, 337, 746, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14019468910673744, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.008368737995624542}, {"id": 1172, "seek": 665576, "start": 6670.8, "end": 6676.88, "text": " So I think you'll find that there's a large collection of abduction problems that you could", "tokens": [51116, 407, 286, 519, 291, 603, 915, 300, 456, 311, 257, 2416, 5765, 295, 410, 40335, 2740, 300, 291, 727, 51420], "temperature": 0.0, "avg_logprob": -0.14019468910673744, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.008368737995624542}, {"id": 1173, "seek": 665576, "start": 6676.88, "end": 6681.360000000001, "text": " just present to today's large language models, and they would do quite well at them, you know.", "tokens": [51420, 445, 1974, 281, 965, 311, 2416, 2856, 5245, 11, 293, 436, 576, 360, 1596, 731, 412, 552, 11, 291, 458, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14019468910673744, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.008368737995624542}, {"id": 1174, "seek": 665576, "start": 6681.360000000001, "end": 6684.4800000000005, "text": " And at the same time, I'm sure you wouldn't be too difficult to find problems,", "tokens": [51644, 400, 412, 264, 912, 565, 11, 286, 478, 988, 291, 2759, 380, 312, 886, 2252, 281, 915, 2740, 11, 51800], "temperature": 0.0, "avg_logprob": -0.14019468910673744, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.008368737995624542}, {"id": 1175, "seek": 668448, "start": 6684.48, "end": 6692.24, "text": " especially if they involve many steps where it will go wrong. Because doing multi-step,", "tokens": [50364, 2318, 498, 436, 9494, 867, 4439, 689, 309, 486, 352, 2085, 13, 1436, 884, 4825, 12, 16792, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1545124589727166, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.006491688545793295}, {"id": 1176, "seek": 668448, "start": 6694.24, "end": 6700.24, "text": " it's the multiple steps that really are where today's large language models are a bit weak.", "tokens": [50852, 309, 311, 264, 3866, 4439, 300, 534, 366, 689, 965, 311, 2416, 2856, 5245, 366, 257, 857, 5336, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1545124589727166, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.006491688545793295}, {"id": 1177, "seek": 668448, "start": 6702.4, "end": 6707.2, "text": " Again, it's an open research question. People are working on all that kind of thing. So if there", "tokens": [51260, 3764, 11, 309, 311, 364, 1269, 2132, 1168, 13, 3432, 366, 1364, 322, 439, 300, 733, 295, 551, 13, 407, 498, 456, 51500], "temperature": 0.0, "avg_logprob": -0.1545124589727166, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.006491688545793295}, {"id": 1178, "seek": 668448, "start": 6707.2, "end": 6713.839999999999, "text": " are multiple steps, and step n is dependent on step n minus one, and it's inherently,", "tokens": [51500, 366, 3866, 4439, 11, 293, 1823, 297, 307, 12334, 322, 1823, 297, 3175, 472, 11, 293, 309, 311, 27993, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1545124589727166, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.006491688545793295}, {"id": 1179, "seek": 671384, "start": 6713.84, "end": 6719.6, "text": " they're inherently very computational sorts of things in that sense. So large language models", "tokens": [50364, 436, 434, 27993, 588, 28270, 7527, 295, 721, 294, 300, 2020, 13, 407, 2416, 2856, 5245, 50652], "temperature": 0.0, "avg_logprob": -0.108957120350429, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.007602912839502096}, {"id": 1180, "seek": 671384, "start": 6719.6, "end": 6724.88, "text": " are a bit weak at that, for sure. And we can introduce things like chain of thought and so on", "tokens": [50652, 366, 257, 857, 5336, 412, 300, 11, 337, 988, 13, 400, 321, 393, 5366, 721, 411, 5021, 295, 1194, 293, 370, 322, 50916], "temperature": 0.0, "avg_logprob": -0.108957120350429, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.007602912839502096}, {"id": 1181, "seek": 671384, "start": 6724.88, "end": 6731.52, "text": " to try and improve that. But they're sort of a limited success. But my feeling is that those", "tokens": [50916, 281, 853, 293, 3470, 300, 13, 583, 436, 434, 1333, 295, 257, 5567, 2245, 13, 583, 452, 2633, 307, 300, 729, 51248], "temperature": 0.0, "avg_logprob": -0.108957120350429, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.007602912839502096}, {"id": 1182, "seek": 671384, "start": 6731.52, "end": 6737.4400000000005, "text": " are not the things where which large language models themselves are inherently strong at.", "tokens": [51248, 366, 406, 264, 721, 689, 597, 2416, 2856, 5245, 2969, 366, 27993, 2068, 412, 13, 51544], "temperature": 0.0, "avg_logprob": -0.108957120350429, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.007602912839502096}, {"id": 1183, "seek": 673744, "start": 6737.44, "end": 6744.5599999999995, "text": " And you should probably make use of other tools in order to kind of boost reasoning capabilities.", "tokens": [50364, 400, 291, 820, 1391, 652, 764, 295, 661, 3873, 294, 1668, 281, 733, 295, 9194, 21577, 10862, 13, 50720], "temperature": 0.0, "avg_logprob": -0.12446263432502747, "compression_ratio": 1.7975206611570247, "no_speech_prob": 0.30084216594696045}, {"id": 1184, "seek": 673744, "start": 6744.5599999999995, "end": 6749.599999999999, "text": " Like I listed some of them. So somebody's making external calls to reasoning,", "tokens": [50720, 1743, 286, 10052, 512, 295, 552, 13, 407, 2618, 311, 1455, 8320, 5498, 281, 21577, 11, 50972], "temperature": 0.0, "avg_logprob": -0.12446263432502747, "compression_ratio": 1.7975206611570247, "no_speech_prob": 0.30084216594696045}, {"id": 1185, "seek": 673744, "start": 6750.799999999999, "end": 6755.12, "text": " dedicated reasoning components. Sometimes it's embedded in the large language model", "tokens": [51032, 8374, 21577, 6677, 13, 4803, 309, 311, 16741, 294, 264, 2416, 2856, 2316, 51248], "temperature": 0.0, "avg_logprob": -0.12446263432502747, "compression_ratio": 1.7975206611570247, "no_speech_prob": 0.30084216594696045}, {"id": 1186, "seek": 673744, "start": 6755.12, "end": 6758.879999999999, "text": " in a reasoning algorithm itself. So you can go either way, you can either take the large", "tokens": [51248, 294, 257, 21577, 9284, 2564, 13, 407, 291, 393, 352, 2139, 636, 11, 291, 393, 2139, 747, 264, 2416, 51436], "temperature": 0.0, "avg_logprob": -0.12446263432502747, "compression_ratio": 1.7975206611570247, "no_speech_prob": 0.30084216594696045}, {"id": 1187, "seek": 673744, "start": 6758.879999999999, "end": 6763.839999999999, "text": " language model, put reasoning things inside it as it were, so it makes external calls,", "tokens": [51436, 2856, 2316, 11, 829, 21577, 721, 1854, 309, 382, 309, 645, 11, 370, 309, 1669, 8320, 5498, 11, 51684], "temperature": 0.0, "avg_logprob": -0.12446263432502747, "compression_ratio": 1.7975206611570247, "no_speech_prob": 0.30084216594696045}, {"id": 1188, "seek": 676384, "start": 6763.84, "end": 6767.6, "text": " or you do it the other way around, you can have a reasoning algorithm and make the large", "tokens": [50364, 420, 291, 360, 309, 264, 661, 636, 926, 11, 291, 393, 362, 257, 21577, 9284, 293, 652, 264, 2416, 50552], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1189, "seek": 676384, "start": 6767.6, "end": 6772.32, "text": " language model component of the reasoning algorithm. Yes, I suppose it's a similar thing to the", "tokens": [50552, 2856, 2316, 6542, 295, 264, 21577, 9284, 13, 1079, 11, 286, 7297, 309, 311, 257, 2531, 551, 281, 264, 50788], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1190, "seek": 676384, "start": 6772.32, "end": 6777.92, "text": " creativity that there's a kind of creative or inventive abduction, and then there's colloquial", "tokens": [50788, 12915, 300, 456, 311, 257, 733, 295, 5880, 420, 7962, 488, 410, 40335, 11, 293, 550, 456, 311, 1263, 29826, 831, 51068], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1191, "seek": 676384, "start": 6777.92, "end": 6782.64, "text": " abductive interpolation. And the remarkable thing is just how structured and predictable our world", "tokens": [51068, 46465, 488, 44902, 399, 13, 400, 264, 12802, 551, 307, 445, 577, 18519, 293, 27737, 527, 1002, 51304], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1192, "seek": 676384, "start": 6782.64, "end": 6787.04, "text": " is and how far you can get with the colloquial predictive abductive. Yes, yeah. I mean, this", "tokens": [51304, 307, 293, 577, 1400, 291, 393, 483, 365, 264, 1263, 29826, 831, 35521, 46465, 488, 13, 1079, 11, 1338, 13, 286, 914, 11, 341, 51524], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1193, "seek": 676384, "start": 6787.04, "end": 6793.76, "text": " way you speak of colloquial sort of abduction, which is the kind of thing that we can", "tokens": [51524, 636, 291, 1710, 295, 1263, 29826, 831, 1333, 295, 410, 40335, 11, 597, 307, 264, 733, 295, 551, 300, 321, 393, 51860], "temperature": 0.0, "avg_logprob": -0.13003880536114729, "compression_ratio": 1.8945578231292517, "no_speech_prob": 0.00510556111112237}, {"id": 1194, "seek": 679376, "start": 6793.76, "end": 6801.280000000001, "text": " all and the person on the street could do if you have some slightly odd situation and you say,", "tokens": [50364, 439, 293, 264, 954, 322, 264, 4838, 727, 360, 498, 291, 362, 512, 4748, 7401, 2590, 293, 291, 584, 11, 50740], "temperature": 0.0, "avg_logprob": -0.16799667607183041, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.014411251060664654}, {"id": 1195, "seek": 679376, "start": 6801.280000000001, "end": 6809.4400000000005, "text": " why is this man in the middle of the road with a policeman's hat on or something?", "tokens": [50740, 983, 307, 341, 587, 294, 264, 2808, 295, 264, 3060, 365, 257, 42658, 311, 2385, 322, 420, 746, 30, 51148], "temperature": 0.0, "avg_logprob": -0.16799667607183041, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.014411251060664654}, {"id": 1196, "seek": 679376, "start": 6813.04, "end": 6816.320000000001, "text": " And you say, well, because there's maybe there's been an accident or something.", "tokens": [51328, 400, 291, 584, 11, 731, 11, 570, 456, 311, 1310, 456, 311, 668, 364, 6398, 420, 746, 13, 51492], "temperature": 0.0, "avg_logprob": -0.16799667607183041, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.014411251060664654}, {"id": 1197, "seek": 679376, "start": 6817.4400000000005, "end": 6821.6, "text": " So we do all this kind of thing on an everyday basis. It's a little bit of abduction. It's what", "tokens": [51548, 407, 321, 360, 439, 341, 733, 295, 551, 322, 364, 7429, 5143, 13, 467, 311, 257, 707, 857, 295, 410, 40335, 13, 467, 311, 437, 51756], "temperature": 0.0, "avg_logprob": -0.16799667607183041, "compression_ratio": 1.6372093023255814, "no_speech_prob": 0.014411251060664654}, {"id": 1198, "seek": 682160, "start": 6821.6, "end": 6825.360000000001, "text": " you call colloquial abduction, I think. But large language models think you find a pretty good at", "tokens": [50364, 291, 818, 1263, 29826, 831, 410, 40335, 11, 286, 519, 13, 583, 2416, 2856, 5245, 519, 291, 915, 257, 1238, 665, 412, 50552], "temperature": 0.0, "avg_logprob": -0.09267345525450625, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.005709034390747547}, {"id": 1199, "seek": 682160, "start": 6825.360000000001, "end": 6828.96, "text": " that kind of thing these days. But if you have something that's really complex and has a load", "tokens": [50552, 300, 733, 295, 551, 613, 1708, 13, 583, 498, 291, 362, 746, 300, 311, 534, 3997, 293, 575, 257, 3677, 50732], "temperature": 0.0, "avg_logprob": -0.09267345525450625, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.005709034390747547}, {"id": 1200, "seek": 682160, "start": 6828.96, "end": 6834.320000000001, "text": " of steps to it, the kind of thing that humans would struggle at, then very often large language", "tokens": [50732, 295, 4439, 281, 309, 11, 264, 733, 295, 551, 300, 6255, 576, 7799, 412, 11, 550, 588, 2049, 2416, 2856, 51000], "temperature": 0.0, "avg_logprob": -0.09267345525450625, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.005709034390747547}, {"id": 1201, "seek": 682160, "start": 6834.320000000001, "end": 6839.280000000001, "text": " models are going to struggle at those things too. Can you describe the Turing test? The Turing test?", "tokens": [51000, 5245, 366, 516, 281, 7799, 412, 729, 721, 886, 13, 1664, 291, 6786, 264, 314, 1345, 1500, 30, 440, 314, 1345, 1500, 30, 51248], "temperature": 0.0, "avg_logprob": -0.09267345525450625, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.005709034390747547}, {"id": 1202, "seek": 682160, "start": 6839.280000000001, "end": 6849.6, "text": " Yes. Okay. So I'll describe the Turing test as it's popularly conceived because there are", "tokens": [51248, 1079, 13, 1033, 13, 407, 286, 603, 6786, 264, 314, 1345, 1500, 382, 309, 311, 3743, 356, 34898, 570, 456, 366, 51764], "temperature": 0.0, "avg_logprob": -0.09267345525450625, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.005709034390747547}, {"id": 1203, "seek": 684960, "start": 6849.6, "end": 6854.0, "text": " new answers in Turing's original paper. But then again, he didn't call it the Turing test.", "tokens": [50364, 777, 6338, 294, 314, 1345, 311, 3380, 3035, 13, 583, 550, 797, 11, 415, 994, 380, 818, 309, 264, 314, 1345, 1500, 13, 50584], "temperature": 0.0, "avg_logprob": -0.11090464002630684, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0061292024329304695}, {"id": 1204, "seek": 684960, "start": 6856.0, "end": 6863.52, "text": " So the Turing test as it's popularly conceived involves having a human judge", "tokens": [50684, 407, 264, 314, 1345, 1500, 382, 309, 311, 3743, 356, 34898, 11626, 1419, 257, 1952, 6995, 51060], "temperature": 0.0, "avg_logprob": -0.11090464002630684, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0061292024329304695}, {"id": 1205, "seek": 684960, "start": 6863.52, "end": 6870.8, "text": " and the human judge is interacting with two things. One of them is a human and the other is a computer", "tokens": [51060, 293, 264, 1952, 6995, 307, 18017, 365, 732, 721, 13, 1485, 295, 552, 307, 257, 1952, 293, 264, 661, 307, 257, 3820, 51424], "temperature": 0.0, "avg_logprob": -0.11090464002630684, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0061292024329304695}, {"id": 1206, "seek": 684960, "start": 6870.8, "end": 6877.84, "text": " system. And the interaction is entirely through language, through a keyboard and a screen say,", "tokens": [51424, 1185, 13, 400, 264, 9285, 307, 7696, 807, 2856, 11, 807, 257, 10186, 293, 257, 2568, 584, 11, 51776], "temperature": 0.0, "avg_logprob": -0.11090464002630684, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0061292024329304695}, {"id": 1207, "seek": 687784, "start": 6877.84, "end": 6885.04, "text": " or a teletype, if you like, in Turing's day. And the idea is that the human judge has a conversation", "tokens": [50364, 420, 257, 15284, 2210, 494, 11, 498, 291, 411, 11, 294, 314, 1345, 311, 786, 13, 400, 264, 1558, 307, 300, 264, 1952, 6995, 575, 257, 3761, 50724], "temperature": 0.0, "avg_logprob": -0.06566012859344482, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.00479360343888402}, {"id": 1208, "seek": 687784, "start": 6885.04, "end": 6891.84, "text": " with these two things. And the question is, can the human judge tell which is the machine and", "tokens": [50724, 365, 613, 732, 721, 13, 400, 264, 1168, 307, 11, 393, 264, 1952, 6995, 980, 597, 307, 264, 3479, 293, 51064], "temperature": 0.0, "avg_logprob": -0.06566012859344482, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.00479360343888402}, {"id": 1209, "seek": 687784, "start": 6891.84, "end": 6900.0, "text": " which is the human? And if the judge can't tell which is which, then the machine passes the Turing", "tokens": [51064, 597, 307, 264, 1952, 30, 400, 498, 264, 6995, 393, 380, 980, 597, 307, 597, 11, 550, 264, 3479, 11335, 264, 314, 1345, 51472], "temperature": 0.0, "avg_logprob": -0.06566012859344482, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.00479360343888402}, {"id": 1210, "seek": 687784, "start": 6900.0, "end": 6906.64, "text": " test. Do you think it's a good measure of intelligence? I think it's a pretty rubbish", "tokens": [51472, 1500, 13, 1144, 291, 519, 309, 311, 257, 665, 3481, 295, 7599, 30, 286, 519, 309, 311, 257, 1238, 29978, 51804], "temperature": 0.0, "avg_logprob": -0.06566012859344482, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.00479360343888402}, {"id": 1211, "seek": 690664, "start": 6906.64, "end": 6915.360000000001, "text": " measure of intelligence. I mean, I think it's a very useful philosophical thought experiment", "tokens": [50364, 3481, 295, 7599, 13, 286, 914, 11, 286, 519, 309, 311, 257, 588, 4420, 25066, 1194, 5120, 50800], "temperature": 0.0, "avg_logprob": -0.11920555869301597, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.010347646661102772}, {"id": 1212, "seek": 690664, "start": 6915.360000000001, "end": 6920.08, "text": " and starting point for conversation on this. But the trouble is, it's very easy to game.", "tokens": [50800, 293, 2891, 935, 337, 3761, 322, 341, 13, 583, 264, 5253, 307, 11, 309, 311, 588, 1858, 281, 1216, 13, 51036], "temperature": 0.0, "avg_logprob": -0.11920555869301597, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.010347646661102772}, {"id": 1213, "seek": 690664, "start": 6920.08, "end": 6923.4400000000005, "text": " Well, there's a number of problems with that, which people have been writing about for years,", "tokens": [51036, 1042, 11, 456, 311, 257, 1230, 295, 2740, 365, 300, 11, 597, 561, 362, 668, 3579, 466, 337, 924, 11, 51204], "temperature": 0.0, "avg_logprob": -0.11920555869301597, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.010347646661102772}, {"id": 1214, "seek": 690664, "start": 6923.4400000000005, "end": 6930.96, "text": " by the way. So there are a number of problems with that. So one is that it's easy to game,", "tokens": [51204, 538, 264, 636, 13, 407, 456, 366, 257, 1230, 295, 2740, 365, 300, 13, 407, 472, 307, 300, 309, 311, 1858, 281, 1216, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11920555869301597, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.010347646661102772}, {"id": 1215, "seek": 693096, "start": 6931.04, "end": 6936.64, "text": " in a sense, because there's a temptation to make something to pass the Turing test,", "tokens": [50368, 294, 257, 2020, 11, 570, 456, 311, 257, 30423, 281, 652, 746, 281, 1320, 264, 314, 1345, 1500, 11, 50648], "temperature": 0.0, "avg_logprob": -0.08073096502394904, "compression_ratio": 1.6875, "no_speech_prob": 0.1732970029115677}, {"id": 1216, "seek": 693096, "start": 6936.64, "end": 6941.92, "text": " you make something that has all kinds of strange human ticks and peculiarities,", "tokens": [50648, 291, 652, 746, 300, 575, 439, 3685, 295, 5861, 1952, 42475, 293, 27149, 1088, 11, 50912], "temperature": 0.0, "avg_logprob": -0.08073096502394904, "compression_ratio": 1.6875, "no_speech_prob": 0.1732970029115677}, {"id": 1217, "seek": 693096, "start": 6941.92, "end": 6948.4800000000005, "text": " and then it seems human. And so you can fool the judge that way by making it just a bit eccentric,", "tokens": [50912, 293, 550, 309, 2544, 1952, 13, 400, 370, 291, 393, 7979, 264, 6995, 300, 636, 538, 1455, 309, 445, 257, 857, 42629, 11, 51240], "temperature": 0.0, "avg_logprob": -0.08073096502394904, "compression_ratio": 1.6875, "no_speech_prob": 0.1732970029115677}, {"id": 1218, "seek": 693096, "start": 6949.52, "end": 6954.32, "text": " which is obviously nothing to do with intelligence at all. So that's one thing.", "tokens": [51292, 597, 307, 2745, 1825, 281, 360, 365, 7599, 412, 439, 13, 407, 300, 311, 472, 551, 13, 51532], "temperature": 0.0, "avg_logprob": -0.08073096502394904, "compression_ratio": 1.6875, "no_speech_prob": 0.1732970029115677}, {"id": 1219, "seek": 693096, "start": 6954.32, "end": 6960.24, "text": " And then another thing is that the domain of the test is purely linguistic. So you're not", "tokens": [51532, 400, 550, 1071, 551, 307, 300, 264, 9274, 295, 264, 1500, 307, 17491, 43002, 13, 407, 291, 434, 406, 51828], "temperature": 0.0, "avg_logprob": -0.08073096502394904, "compression_ratio": 1.6875, "no_speech_prob": 0.1732970029115677}, {"id": 1220, "seek": 696024, "start": 6960.32, "end": 6965.76, "text": " testing anything to do with the sorts of intelligence that you get in a non-human animal,", "tokens": [50368, 4997, 1340, 281, 360, 365, 264, 7527, 295, 7599, 300, 291, 483, 294, 257, 2107, 12, 18796, 5496, 11, 50640], "temperature": 0.0, "avg_logprob": -0.14519898254092378, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.0017345091328024864}, {"id": 1221, "seek": 696024, "start": 6965.76, "end": 6971.5199999999995, "text": " say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to", "tokens": [50640, 584, 13, 407, 7197, 293, 11111, 293, 22257, 20487, 439, 3685, 295, 7599, 294, 641, 3485, 281, 50928], "temperature": 0.0, "avg_logprob": -0.14519898254092378, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.0017345091328024864}, {"id": 1222, "seek": 696024, "start": 6971.5199999999995, "end": 6977.28, "text": " navigate the ordinary, everyday world and survive in it. But none of those kinds of", "tokens": [50928, 12350, 264, 10547, 11, 7429, 1002, 293, 7867, 294, 309, 13, 583, 6022, 295, 729, 3685, 295, 51216], "temperature": 0.0, "avg_logprob": -0.14519898254092378, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.0017345091328024864}, {"id": 1223, "seek": 696024, "start": 6977.28, "end": 6984.24, "text": " intelligence are tested by the Turing test. So you were the scientific advisor on the film", "tokens": [51216, 7599, 366, 8246, 538, 264, 314, 1345, 1500, 13, 407, 291, 645, 264, 8134, 19161, 322, 264, 2007, 51564], "temperature": 0.0, "avg_logprob": -0.14519898254092378, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.0017345091328024864}, {"id": 1224, "seek": 696024, "start": 6984.96, "end": 6988.0, "text": " Ex Machina. And I'm not sure whether it's Machina or Machina.", "tokens": [51600, 2111, 12089, 1426, 13, 400, 286, 478, 406, 988, 1968, 309, 311, 12089, 1426, 420, 12089, 1426, 13, 51752], "temperature": 0.0, "avg_logprob": -0.14519898254092378, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.0017345091328024864}, {"id": 1225, "seek": 698800, "start": 6988.72, "end": 6993.04, "text": " It is Machina, yeah. I was speaking with Irina Rishan and she said,", "tokens": [50400, 467, 307, 12089, 1426, 11, 1338, 13, 286, 390, 4124, 365, 9151, 1426, 497, 742, 282, 293, 750, 848, 11, 50616], "temperature": 0.0, "avg_logprob": -0.18699270884195965, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.009597336873412132}, {"id": 1226, "seek": 698800, "start": 6993.04, "end": 7000.0, "text": " Ex Machina. But anyway, she's right. Yes, she is. I digress. But there was a special", "tokens": [50616, 2111, 12089, 1426, 13, 583, 4033, 11, 750, 311, 558, 13, 1079, 11, 750, 307, 13, 286, 2528, 735, 13, 583, 456, 390, 257, 2121, 50964], "temperature": 0.0, "avg_logprob": -0.18699270884195965, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.009597336873412132}, {"id": 1227, "seek": 698800, "start": 7000.0, "end": 7004.16, "text": " type of Turing test in that film. Can you explain that?", "tokens": [50964, 2010, 295, 314, 1345, 1500, 294, 300, 2007, 13, 1664, 291, 2903, 300, 30, 51172], "temperature": 0.0, "avg_logprob": -0.18699270884195965, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.009597336873412132}, {"id": 1228, "seek": 698800, "start": 7004.16, "end": 7010.08, "text": " Well, indeed it wasn't. It's not the Turing test. So there's a point in the film, I assume,", "tokens": [51172, 1042, 11, 6451, 309, 2067, 380, 13, 467, 311, 406, 264, 314, 1345, 1500, 13, 407, 456, 311, 257, 935, 294, 264, 2007, 11, 286, 6552, 11, 51468], "temperature": 0.0, "avg_logprob": -0.18699270884195965, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.009597336873412132}, {"id": 1229, "seek": 698800, "start": 7010.08, "end": 7015.6, "text": " that people are vaguely familiar with the setup of the film. But there's a point in the film where", "tokens": [51468, 300, 561, 366, 13501, 48863, 4963, 365, 264, 8657, 295, 264, 2007, 13, 583, 456, 311, 257, 935, 294, 264, 2007, 689, 51744], "temperature": 0.0, "avg_logprob": -0.18699270884195965, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.009597336873412132}, {"id": 1230, "seek": 701560, "start": 7015.6, "end": 7021.68, "text": " Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava.", "tokens": [50364, 30331, 11, 264, 32116, 11, 307, 1417, 281, 20634, 264, 42358, 567, 311, 4743, 341, 7881, 316, 2757, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14517721157629512, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.0032809856347739697}, {"id": 1231, "seek": 701560, "start": 7022.240000000001, "end": 7027.52, "text": " And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing", "tokens": [50696, 400, 30331, 1619, 11, 1954, 11, 437, 291, 434, 884, 307, 291, 434, 1382, 281, 1322, 746, 300, 11335, 264, 314, 1345, 50960], "temperature": 0.0, "avg_logprob": -0.14517721157629512, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.0032809856347739697}, {"id": 1232, "seek": 701560, "start": 7027.52, "end": 7035.200000000001, "text": " test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know", "tokens": [50960, 1500, 13, 400, 20634, 1619, 11, 1954, 11, 572, 11, 321, 434, 636, 1791, 300, 13, 440, 935, 307, 11, 294, 264, 314, 1345, 1500, 11, 291, 500, 380, 458, 51344], "temperature": 0.0, "avg_logprob": -0.14517721157629512, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.0032809856347739697}, {"id": 1233, "seek": 701560, "start": 7035.200000000001, "end": 7042.4800000000005, "text": " whether the thing that's being tested, whether you don't know whether it's a machine or a human.", "tokens": [51344, 1968, 264, 551, 300, 311, 885, 8246, 11, 1968, 291, 500, 380, 458, 1968, 309, 311, 257, 3479, 420, 257, 1952, 13, 51708], "temperature": 0.0, "avg_logprob": -0.14517721157629512, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.0032809856347739697}, {"id": 1234, "seek": 704248, "start": 7042.48, "end": 7048.719999999999, "text": " That's the point of the test. The point, Nathan says, is to show you that she's a robot and see", "tokens": [50364, 663, 311, 264, 935, 295, 264, 1500, 13, 440, 935, 11, 20634, 1619, 11, 307, 281, 855, 291, 300, 750, 311, 257, 7881, 293, 536, 50676], "temperature": 0.0, "avg_logprob": -0.05242798328399658, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.012334290891885757}, {"id": 1235, "seek": 704248, "start": 7048.719999999999, "end": 7054.4, "text": " if you still think she's conscious. So there's a number of ways in which this is very different", "tokens": [50676, 498, 291, 920, 519, 750, 311, 6648, 13, 407, 456, 311, 257, 1230, 295, 2098, 294, 597, 341, 307, 588, 819, 50960], "temperature": 0.0, "avg_logprob": -0.05242798328399658, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.012334290891885757}, {"id": 1236, "seek": 704248, "start": 7054.4, "end": 7059.44, "text": " from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence.", "tokens": [50960, 490, 264, 314, 1345, 1500, 13, 407, 700, 293, 18864, 11, 309, 311, 257, 1500, 295, 10081, 11, 406, 295, 7599, 13, 51212], "temperature": 0.0, "avg_logprob": -0.05242798328399658, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.012334290891885757}, {"id": 1237, "seek": 704248, "start": 7059.44, "end": 7067.599999999999, "text": " And those are not the same thing. And secondly, the point, as he says, is to be persuaded", "tokens": [51212, 400, 729, 366, 406, 264, 912, 551, 13, 400, 26246, 11, 264, 935, 11, 382, 415, 1619, 11, 307, 281, 312, 47693, 51620], "temperature": 0.0, "avg_logprob": -0.05242798328399658, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.012334290891885757}, {"id": 1238, "seek": 706760, "start": 7068.08, "end": 7074.72, "text": " that the artifact is conscious, even though you know that it's not human, not biological.", "tokens": [50388, 300, 264, 34806, 307, 6648, 11, 754, 1673, 291, 458, 300, 309, 311, 406, 1952, 11, 406, 13910, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1430743932723999, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.037578485906124115}, {"id": 1239, "seek": 706760, "start": 7074.72, "end": 7080.72, "text": " So you know that it's an AI system, and you still think that it's conscious. In that case,", "tokens": [50720, 407, 291, 458, 300, 309, 311, 364, 7318, 1185, 11, 293, 291, 920, 519, 300, 309, 311, 6648, 13, 682, 300, 1389, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1430743932723999, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.037578485906124115}, {"id": 1240, "seek": 706760, "start": 7080.72, "end": 7087.200000000001, "text": " it passes this test. So this test, I call the Garland test after Alex Garland, who is the", "tokens": [51020, 309, 11335, 341, 1500, 13, 407, 341, 1500, 11, 286, 818, 264, 7995, 1661, 1500, 934, 5202, 7995, 1661, 11, 567, 307, 264, 51344], "temperature": 0.0, "avg_logprob": -0.1430743932723999, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.037578485906124115}, {"id": 1241, "seek": 706760, "start": 7087.200000000001, "end": 7094.64, "text": " writer and director of X Machina, quite different from the Turing test. I think that those lines", "tokens": [51344, 9936, 293, 5391, 295, 1783, 12089, 1426, 11, 1596, 819, 490, 264, 314, 1345, 1500, 13, 286, 519, 300, 729, 3876, 51716], "temperature": 0.0, "avg_logprob": -0.1430743932723999, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.037578485906124115}, {"id": 1242, "seek": 709464, "start": 7094.72, "end": 7104.240000000001, "text": " in the film were actually really brilliant lines and really clever lines from Alex in the script.", "tokens": [50368, 294, 264, 2007, 645, 767, 534, 10248, 3876, 293, 534, 13494, 3876, 490, 5202, 294, 264, 5755, 13, 50844], "temperature": 0.0, "avg_logprob": -0.10392703180727751, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.130656436085701}, {"id": 1243, "seek": 709464, "start": 7104.240000000001, "end": 7110.96, "text": " And when I first saw the script, which was long before it was filmed, and that bit was in there,", "tokens": [50844, 400, 562, 286, 700, 1866, 264, 5755, 11, 597, 390, 938, 949, 309, 390, 15133, 11, 293, 300, 857, 390, 294, 456, 11, 51180], "temperature": 0.0, "avg_logprob": -0.10392703180727751, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.130656436085701}, {"id": 1244, "seek": 709464, "start": 7110.96, "end": 7117.4400000000005, "text": " and I put spot on next to those lines in the script, because I thought it was such a very", "tokens": [51180, 293, 286, 829, 4008, 322, 958, 281, 729, 3876, 294, 264, 5755, 11, 570, 286, 1194, 309, 390, 1270, 257, 588, 51504], "temperature": 0.0, "avg_logprob": -0.10392703180727751, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.130656436085701}, {"id": 1245, "seek": 709464, "start": 7117.4400000000005, "end": 7123.280000000001, "text": " good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed", "tokens": [51504, 665, 1500, 13, 286, 914, 11, 26742, 321, 434, 3442, 281, 519, 300, 30331, 3647, 294, 264, 2007, 775, 6451, 51796], "temperature": 0.0, "avg_logprob": -0.10392703180727751, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.130656436085701}, {"id": 1246, "seek": 712328, "start": 7124.24, "end": 7130.08, "text": " think that Ava is conscious and thinks of her that way. What that really means is that", "tokens": [50412, 519, 300, 316, 2757, 307, 6648, 293, 7309, 295, 720, 300, 636, 13, 708, 300, 534, 1355, 307, 300, 50704], "temperature": 0.0, "avg_logprob": -0.08747433199740873, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0027474144008010626}, {"id": 1247, "seek": 712328, "start": 7130.08, "end": 7134.88, "text": " he comes to treat her as a fellow conscious creature. And we see that in the film because", "tokens": [50704, 415, 1487, 281, 2387, 720, 382, 257, 7177, 6648, 12797, 13, 400, 321, 536, 300, 294, 264, 2007, 570, 50944], "temperature": 0.0, "avg_logprob": -0.08747433199740873, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0027474144008010626}, {"id": 1248, "seek": 712328, "start": 7134.88, "end": 7140.4, "text": " he wants to help her escape. And just as a kind of thought experiment, maybe it's another", "tokens": [50944, 415, 2738, 281, 854, 720, 7615, 13, 400, 445, 382, 257, 733, 295, 1194, 5120, 11, 1310, 309, 311, 1071, 51220], "temperature": 0.0, "avg_logprob": -0.08747433199740873, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0027474144008010626}, {"id": 1249, "seek": 712328, "start": 7140.4, "end": 7145.759999999999, "text": " conceivability thing, but it does seem conceivable what less consciousness would be like, but it", "tokens": [51220, 10413, 592, 2310, 551, 11, 457, 309, 775, 1643, 10413, 34376, 437, 1570, 10081, 576, 312, 411, 11, 457, 309, 51488], "temperature": 0.0, "avg_logprob": -0.08747433199740873, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0027474144008010626}, {"id": 1250, "seek": 712328, "start": 7145.759999999999, "end": 7148.8, "text": " seems less conceivable what more consciousness would be like.", "tokens": [51488, 2544, 1570, 10413, 34376, 437, 544, 10081, 576, 312, 411, 13, 51640], "temperature": 0.0, "avg_logprob": -0.08747433199740873, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0027474144008010626}, {"id": 1251, "seek": 714880, "start": 7149.360000000001, "end": 7162.96, "text": " Well, so in both cases, I think it's all about exercising our imaginations actually. And I", "tokens": [50392, 1042, 11, 370, 294, 1293, 3331, 11, 286, 519, 309, 311, 439, 466, 27272, 527, 2576, 10325, 767, 13, 400, 286, 51072], "temperature": 0.0, "avg_logprob": -0.16750520899675894, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.006601892411708832}, {"id": 1252, "seek": 714880, "start": 7162.96, "end": 7168.96, "text": " think exercising our imaginations is perfectly legitimate in philosophical discussions. So", "tokens": [51072, 519, 27272, 527, 2576, 10325, 307, 6239, 17956, 294, 25066, 11088, 13, 407, 51372], "temperature": 0.0, "avg_logprob": -0.16750520899675894, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.006601892411708832}, {"id": 1253, "seek": 714880, "start": 7168.96, "end": 7176.72, "text": " in fact, in my newer paper, Simulacra as Conscious Exotica, I think I say that I advocate doing", "tokens": [51372, 294, 1186, 11, 294, 452, 17628, 3035, 11, 3998, 425, 326, 424, 382, 6923, 4139, 2111, 310, 2262, 11, 286, 519, 286, 584, 300, 286, 14608, 884, 51760], "temperature": 0.0, "avg_logprob": -0.16750520899675894, "compression_ratio": 1.5561797752808988, "no_speech_prob": 0.006601892411708832}, {"id": 1254, "seek": 717672, "start": 7176.72, "end": 7181.76, "text": " philosophy with the detachment of an anthropologist and the imagination of a science fiction writer.", "tokens": [50364, 10675, 365, 264, 1141, 23908, 295, 364, 22727, 9201, 293, 264, 12938, 295, 257, 3497, 13266, 9936, 13, 50616], "temperature": 0.0, "avg_logprob": -0.09176159459491108, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.021835092455148697}, {"id": 1255, "seek": 717672, "start": 7181.76, "end": 7188.72, "text": " So I think that's something that I aspire to do. So we can carry out all kinds of imaginative", "tokens": [50616, 407, 286, 519, 300, 311, 746, 300, 286, 41224, 281, 360, 13, 407, 321, 393, 3985, 484, 439, 3685, 295, 23427, 1166, 50964], "temperature": 0.0, "avg_logprob": -0.09176159459491108, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.021835092455148697}, {"id": 1256, "seek": 717672, "start": 7188.72, "end": 7195.68, "text": " exercises to describe exotic entities in a science fiction like way. So we can describe an", "tokens": [50964, 11900, 281, 6786, 27063, 16667, 294, 257, 3497, 13266, 411, 636, 13, 407, 321, 393, 6786, 364, 51312], "temperature": 0.0, "avg_logprob": -0.09176159459491108, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.021835092455148697}, {"id": 1257, "seek": 717672, "start": 7195.68, "end": 7202.72, "text": " exotic entity and we can describe all kinds of strange behaviors. And that's sort of as far", "tokens": [51312, 27063, 13977, 293, 321, 393, 6786, 439, 3685, 295, 5861, 15501, 13, 400, 300, 311, 1333, 295, 382, 1400, 51664], "temperature": 0.0, "avg_logprob": -0.09176159459491108, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.021835092455148697}, {"id": 1258, "seek": 720272, "start": 7202.8, "end": 7209.04, "text": " as we can get really, I think we could we could we can imagine all kinds of strange behavior,", "tokens": [50368, 382, 321, 393, 483, 534, 11, 286, 519, 321, 727, 321, 727, 321, 393, 3811, 439, 3685, 295, 5861, 5223, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09977535767988725, "compression_ratio": 2.1244444444444444, "no_speech_prob": 0.06598620116710663}, {"id": 1259, "seek": 720272, "start": 7209.04, "end": 7214.08, "text": " and we can imagine scientists studying those kinds of strange behavior and what underlies them as", "tokens": [50680, 293, 321, 393, 3811, 7708, 7601, 729, 3685, 295, 5861, 5223, 293, 437, 833, 24119, 552, 382, 50932], "temperature": 0.0, "avg_logprob": -0.09977535767988725, "compression_ratio": 2.1244444444444444, "no_speech_prob": 0.06598620116710663}, {"id": 1260, "seek": 720272, "start": 7214.08, "end": 7220.240000000001, "text": " well. And so we can imagine all of those things. And then and then we can also imagine how we would", "tokens": [50932, 731, 13, 400, 370, 321, 393, 3811, 439, 295, 729, 721, 13, 400, 550, 293, 550, 321, 393, 611, 3811, 577, 321, 576, 51240], "temperature": 0.0, "avg_logprob": -0.09977535767988725, "compression_ratio": 2.1244444444444444, "no_speech_prob": 0.06598620116710663}, {"id": 1261, "seek": 720272, "start": 7220.240000000001, "end": 7224.88, "text": " talk about those things. So imagining how we as a kind of community and how the scientists and", "tokens": [51240, 751, 466, 729, 721, 13, 407, 27798, 577, 321, 382, 257, 733, 295, 1768, 293, 577, 264, 7708, 293, 51472], "temperature": 0.0, "avg_logprob": -0.09977535767988725, "compression_ratio": 2.1244444444444444, "no_speech_prob": 0.06598620116710663}, {"id": 1262, "seek": 720272, "start": 7224.88, "end": 7229.92, "text": " the philosophers would talk about, about these imagined entities is all kind of part of it.", "tokens": [51472, 264, 36839, 576, 751, 466, 11, 466, 613, 16590, 16667, 307, 439, 733, 295, 644, 295, 309, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09977535767988725, "compression_ratio": 2.1244444444444444, "no_speech_prob": 0.06598620116710663}, {"id": 1263, "seek": 722992, "start": 7229.92, "end": 7233.76, "text": " So I think we can do all of that. I think that is a kind of doing philosophy.", "tokens": [50364, 407, 286, 519, 321, 393, 360, 439, 295, 300, 13, 286, 519, 300, 307, 257, 733, 295, 884, 10675, 13, 50556], "temperature": 0.0, "avg_logprob": -0.1044621297291347, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.004798166453838348}, {"id": 1264, "seek": 722992, "start": 7233.76, "end": 7239.52, "text": " And I'm just coming back to the Turing test one last time. I read an interesting take on Twitter", "tokens": [50556, 400, 286, 478, 445, 1348, 646, 281, 264, 314, 1345, 1500, 472, 1036, 565, 13, 286, 1401, 364, 1880, 747, 322, 5794, 50844], "temperature": 0.0, "avg_logprob": -0.1044621297291347, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.004798166453838348}, {"id": 1265, "seek": 722992, "start": 7239.52, "end": 7245.36, "text": " recently that we've been thinking about the Turing test or wrong. There seems to be a subset of", "tokens": [50844, 3938, 300, 321, 600, 668, 1953, 466, 264, 314, 1345, 1500, 420, 2085, 13, 821, 2544, 281, 312, 257, 25993, 295, 51136], "temperature": 0.0, "avg_logprob": -0.1044621297291347, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.004798166453838348}, {"id": 1266, "seek": 722992, "start": 7245.36, "end": 7251.92, "text": " people who it's almost like the Eliza effect. They see something they want to see something.", "tokens": [51136, 561, 567, 309, 311, 1920, 411, 264, 11991, 64, 1802, 13, 814, 536, 746, 436, 528, 281, 536, 746, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1044621297291347, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.004798166453838348}, {"id": 1267, "seek": 722992, "start": 7251.92, "end": 7256.24, "text": " And it's almost like the test is actually testing the humans rather than the intelligence.", "tokens": [51464, 400, 309, 311, 1920, 411, 264, 1500, 307, 767, 4997, 264, 6255, 2831, 813, 264, 7599, 13, 51680], "temperature": 0.0, "avg_logprob": -0.1044621297291347, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.004798166453838348}, {"id": 1268, "seek": 725624, "start": 7256.24, "end": 7262.16, "text": " Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again,", "tokens": [50364, 865, 11, 1338, 11, 988, 13, 286, 914, 11, 1338, 11, 309, 311, 588, 11, 309, 311, 588, 12414, 11360, 13, 400, 797, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10619975053347074, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.009312575682997704}, {"id": 1269, "seek": 725624, "start": 7262.16, "end": 7267.12, "text": " I think we should distinguish consciousness and and intelligence in this in this regard,", "tokens": [50660, 286, 519, 321, 820, 20206, 10081, 293, 293, 7599, 294, 341, 294, 341, 3843, 11, 50908], "temperature": 0.0, "avg_logprob": -0.10619975053347074, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.009312575682997704}, {"id": 1270, "seek": 725624, "start": 7267.12, "end": 7270.88, "text": " you know, although there are relations between the two or consciousness and cognition.", "tokens": [50908, 291, 458, 11, 4878, 456, 366, 2299, 1296, 264, 732, 420, 10081, 293, 46905, 13, 51096], "temperature": 0.0, "avg_logprob": -0.10619975053347074, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.009312575682997704}, {"id": 1271, "seek": 725624, "start": 7271.44, "end": 7275.44, "text": " So what just one thing I wanted to say about the about the Turing test is I do", "tokens": [51124, 407, 437, 445, 472, 551, 286, 1415, 281, 584, 466, 264, 466, 264, 314, 1345, 1500, 307, 286, 360, 51324], "temperature": 0.0, "avg_logprob": -0.10619975053347074, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.009312575682997704}, {"id": 1272, "seek": 725624, "start": 7276.0, "end": 7282.96, "text": " feel actually that today's large language models kind of pass the spirit of the Turing test.", "tokens": [51352, 841, 767, 300, 965, 311, 2416, 2856, 5245, 733, 295, 1320, 264, 3797, 295, 264, 314, 1345, 1500, 13, 51700], "temperature": 0.0, "avg_logprob": -0.10619975053347074, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.009312575682997704}, {"id": 1273, "seek": 728296, "start": 7282.96, "end": 7286.8, "text": " So so we defined the Turing test earlier on or the or the kind of", "tokens": [50364, 407, 370, 321, 7642, 264, 314, 1345, 1500, 3071, 322, 420, 264, 420, 264, 733, 295, 50556], "temperature": 0.0, "avg_logprob": -0.1274260529526719, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.0009186433162540197}, {"id": 1274, "seek": 728296, "start": 7288.16, "end": 7293.52, "text": " the popular conception of the Turing test earlier on. And as I remarked, you can kind of game it", "tokens": [50624, 264, 3743, 30698, 295, 264, 314, 1345, 1500, 3071, 322, 13, 400, 382, 286, 7942, 292, 11, 291, 393, 733, 295, 1216, 309, 50892], "temperature": 0.0, "avg_logprob": -0.1274260529526719, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.0009186433162540197}, {"id": 1275, "seek": 728296, "start": 7293.52, "end": 7298.16, "text": " and you know, and there's certain sort of problems with it. But notwithstanding that,", "tokens": [50892, 293, 291, 458, 11, 293, 456, 311, 1629, 1333, 295, 2740, 365, 309, 13, 583, 406, 11820, 8618, 300, 11, 51124], "temperature": 0.0, "avg_logprob": -0.1274260529526719, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.0009186433162540197}, {"id": 1276, "seek": 728296, "start": 7298.16, "end": 7304.16, "text": " I think that actually today's large language models pass the spirit of the Turing test, I feel.", "tokens": [51124, 286, 519, 300, 767, 965, 311, 2416, 2856, 5245, 1320, 264, 3797, 295, 264, 314, 1345, 1500, 11, 286, 841, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1274260529526719, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.0009186433162540197}, {"id": 1277, "seek": 728296, "start": 7304.16, "end": 7309.6, "text": " So they pass the spirit of the Turing test because they do really have human level conversational", "tokens": [51424, 407, 436, 1320, 264, 3797, 295, 264, 314, 1345, 1500, 570, 436, 360, 534, 362, 1952, 1496, 2615, 1478, 51696], "temperature": 0.0, "avg_logprob": -0.1274260529526719, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.0009186433162540197}, {"id": 1278, "seek": 730960, "start": 7309.6, "end": 7316.88, "text": " skills, I feel. So I might my feeling is that if Turing were alive today and were presented with", "tokens": [50364, 3942, 11, 286, 841, 13, 407, 286, 1062, 452, 2633, 307, 300, 498, 314, 1345, 645, 5465, 965, 293, 645, 8212, 365, 50728], "temperature": 0.0, "avg_logprob": -0.14860136278213992, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.002867140108719468}, {"id": 1279, "seek": 730960, "start": 7317.6, "end": 7323.92, "text": " Gemini or ChatGPT or Claude III, he would say, yeah, that's what I had in mind,", "tokens": [50764, 22894, 3812, 420, 27503, 38, 47, 51, 420, 12947, 2303, 16317, 11, 415, 576, 584, 11, 1338, 11, 300, 311, 437, 286, 632, 294, 1575, 11, 51080], "temperature": 0.0, "avg_logprob": -0.14860136278213992, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.002867140108719468}, {"id": 1280, "seek": 730960, "start": 7323.92, "end": 7327.04, "text": " you know, you've done it, that's that's it's kind of passed.", "tokens": [51080, 291, 458, 11, 291, 600, 1096, 309, 11, 300, 311, 300, 311, 309, 311, 733, 295, 4678, 13, 51236], "temperature": 0.0, "avg_logprob": -0.14860136278213992, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.002867140108719468}, {"id": 1281, "seek": 730960, "start": 7327.68, "end": 7333.360000000001, "text": " Interesting. And you also distinguished, you know, the the imitation game is defined by Turing and", "tokens": [51268, 14711, 13, 400, 291, 611, 21702, 11, 291, 458, 11, 264, 264, 47624, 1216, 307, 7642, 538, 314, 1345, 293, 51552], "temperature": 0.0, "avg_logprob": -0.14860136278213992, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.002867140108719468}, {"id": 1282, "seek": 730960, "start": 7333.360000000001, "end": 7338.0, "text": " the the colloquial popular conception is that there's no adjudicator, it's just, you know, a person", "tokens": [51552, 264, 264, 1263, 29826, 831, 3743, 30698, 307, 300, 456, 311, 572, 614, 9218, 299, 1639, 11, 309, 311, 445, 11, 291, 458, 11, 257, 954, 51784], "temperature": 0.0, "avg_logprob": -0.14860136278213992, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.002867140108719468}, {"id": 1283, "seek": 733800, "start": 7338.24, "end": 7342.48, "text": " and an intelligent machine. What is what is the kind of the main difference there?", "tokens": [50376, 293, 364, 13232, 3479, 13, 708, 307, 437, 307, 264, 733, 295, 264, 2135, 2649, 456, 30, 50588], "temperature": 0.0, "avg_logprob": -0.1980019290890314, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.0007528304122388363}, {"id": 1284, "seek": 733800, "start": 7342.48, "end": 7346.72, "text": " What I mean, meant by the popular conception, I think was that maybe popular conception isn't", "tokens": [50588, 708, 286, 914, 11, 4140, 538, 264, 3743, 30698, 11, 286, 519, 390, 300, 1310, 3743, 30698, 1943, 380, 50800], "temperature": 0.0, "avg_logprob": -0.1980019290890314, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.0007528304122388363}, {"id": 1285, "seek": 733800, "start": 7346.72, "end": 7350.72, "text": " quite right. There's sort of contemporary version of it is that is used in academic", "tokens": [50800, 1596, 558, 13, 821, 311, 1333, 295, 14878, 3037, 295, 309, 307, 300, 307, 1143, 294, 7778, 51000], "temperature": 0.0, "avg_logprob": -0.1980019290890314, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.0007528304122388363}, {"id": 1286, "seek": 733800, "start": 7350.72, "end": 7356.08, "text": " discussion, in fact, is what I meant. That's what I meant by popular. So I so there I'm", "tokens": [51000, 5017, 11, 294, 1186, 11, 307, 437, 286, 4140, 13, 663, 311, 437, 286, 4140, 538, 3743, 13, 407, 286, 370, 456, 286, 478, 51268], "temperature": 0.0, "avg_logprob": -0.1980019290890314, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.0007528304122388363}, {"id": 1287, "seek": 733800, "start": 7356.08, "end": 7360.56, "text": " imagining, yeah, there is a human adjudicator there. But the difference is that in so Turing,", "tokens": [51268, 27798, 11, 1338, 11, 456, 307, 257, 1952, 614, 9218, 299, 1639, 456, 13, 583, 264, 2649, 307, 300, 294, 370, 314, 1345, 11, 51492], "temperature": 0.0, "avg_logprob": -0.1980019290890314, "compression_ratio": 1.7609561752988048, "no_speech_prob": 0.0007528304122388363}, {"id": 1288, "seek": 736056, "start": 7360.56, "end": 7369.04, "text": " the way Turing sets up the test is is, you know, he has some specific specificities about,", "tokens": [50364, 264, 636, 314, 1345, 6352, 493, 264, 1500, 307, 307, 11, 291, 458, 11, 415, 575, 512, 2685, 2685, 1088, 466, 11, 50788], "temperature": 0.0, "avg_logprob": -0.11590660611788432, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.09527333080768585}, {"id": 1289, "seek": 736056, "start": 7369.04, "end": 7373.52, "text": " you know, the number of minutes you should, you know, you spend interacting with it. And then", "tokens": [50788, 291, 458, 11, 264, 1230, 295, 2077, 291, 820, 11, 291, 458, 11, 291, 3496, 18017, 365, 309, 13, 400, 550, 51012], "temperature": 0.0, "avg_logprob": -0.11590660611788432, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.09527333080768585}, {"id": 1290, "seek": 736056, "start": 7373.52, "end": 7380.64, "text": " also he sets it up in the context of this game where party game where the aim is to try and work", "tokens": [51012, 611, 415, 6352, 309, 493, 294, 264, 4319, 295, 341, 1216, 689, 3595, 1216, 689, 264, 5939, 307, 281, 853, 293, 589, 51368], "temperature": 0.0, "avg_logprob": -0.11590660611788432, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.09527333080768585}, {"id": 1291, "seek": 736056, "start": 7380.64, "end": 7387.280000000001, "text": " out whether which you've got a man and a woman that somebody is is is convert conversing with,", "tokens": [51368, 484, 1968, 597, 291, 600, 658, 257, 587, 293, 257, 3059, 300, 2618, 307, 307, 307, 7620, 2615, 278, 365, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11590660611788432, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.09527333080768585}, {"id": 1292, "seek": 738728, "start": 7387.28, "end": 7390.639999999999, "text": " you know, via paper, and they don't know which is the man and which is the woman,", "tokens": [50364, 291, 458, 11, 5766, 3035, 11, 293, 436, 500, 380, 458, 597, 307, 264, 587, 293, 597, 307, 264, 3059, 11, 50532], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1293, "seek": 738728, "start": 7390.639999999999, "end": 7394.32, "text": " they have to guess which is which. And that's the way the thing is set up in the original", "tokens": [50532, 436, 362, 281, 2041, 597, 307, 597, 13, 400, 300, 311, 264, 636, 264, 551, 307, 992, 493, 294, 264, 3380, 50716], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1294, "seek": 738728, "start": 7394.32, "end": 7398.96, "text": " paper is by analogy with that. So there's all so so there's all kinds of, you know,", "tokens": [50716, 3035, 307, 538, 21663, 365, 300, 13, 407, 456, 311, 439, 370, 370, 456, 311, 439, 3685, 295, 11, 291, 458, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1295, "seek": 738728, "start": 7398.96, "end": 7403.28, "text": " peculiarities in the original paper, if you're a Turing scholar that aren't aren't really quite", "tokens": [50948, 27149, 1088, 294, 264, 3380, 3035, 11, 498, 291, 434, 257, 314, 1345, 17912, 300, 3212, 380, 3212, 380, 534, 1596, 51164], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1296, "seek": 738728, "start": 7403.28, "end": 7407.759999999999, "text": " relevant to the I think the kind of contemporary way that we think of the Turing test. But I", "tokens": [51164, 7340, 281, 264, 286, 519, 264, 733, 295, 14878, 636, 300, 321, 519, 295, 264, 314, 1345, 1500, 13, 583, 286, 51388], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1297, "seek": 738728, "start": 7407.759999999999, "end": 7413.2, "text": " also think that the sense in which we in which today systems pass the spirit of the Turing test,", "tokens": [51388, 611, 519, 300, 264, 2020, 294, 597, 321, 294, 597, 965, 3652, 1320, 264, 3797, 295, 264, 314, 1345, 1500, 11, 51660], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1298, "seek": 738728, "start": 7413.2, "end": 7416.5599999999995, "text": " it's the reason it's only the spirit of the Turing test is because, of course, you very", "tokens": [51660, 309, 311, 264, 1778, 309, 311, 787, 264, 3797, 295, 264, 314, 1345, 1500, 307, 570, 11, 295, 1164, 11, 291, 588, 51828], "temperature": 0.0, "avg_logprob": -0.11537253639914773, "compression_ratio": 2.04885993485342, "no_speech_prob": 0.027064459398388863}, {"id": 1299, "seek": 741656, "start": 7416.56, "end": 7422.0, "text": " often can immediately tell that it's that it's an AI, not not least because it'll just tell you", "tokens": [50364, 2049, 393, 4258, 980, 300, 309, 311, 300, 309, 311, 364, 7318, 11, 406, 406, 1935, 570, 309, 603, 445, 980, 291, 50636], "temperature": 0.0, "avg_logprob": -0.14355113825847193, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.021461527794599533}, {"id": 1300, "seek": 741656, "start": 7422.0, "end": 7427.04, "text": " right away, right? If you just ask it, it will just say well as a large language model trained", "tokens": [50636, 558, 1314, 11, 558, 30, 759, 291, 445, 1029, 309, 11, 309, 486, 445, 584, 731, 382, 257, 2416, 2856, 2316, 8895, 50888], "temperature": 0.0, "avg_logprob": -0.14355113825847193, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.021461527794599533}, {"id": 1301, "seek": 741656, "start": 7427.04, "end": 7436.0, "text": " by Google or by, you know, open AI, so it's easy to tell which is which. So but but but but but", "tokens": [50888, 538, 3329, 420, 538, 11, 291, 458, 11, 1269, 7318, 11, 370, 309, 311, 1858, 281, 980, 597, 307, 597, 13, 407, 457, 457, 457, 457, 457, 51336], "temperature": 0.0, "avg_logprob": -0.14355113825847193, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.021461527794599533}, {"id": 1302, "seek": 741656, "start": 7436.0, "end": 7440.88, "text": " nevertheless, I think that they have attained more or less human level language skills,", "tokens": [51336, 26924, 11, 286, 519, 300, 436, 362, 46633, 544, 420, 1570, 1952, 1496, 2856, 3942, 11, 51580], "temperature": 0.0, "avg_logprob": -0.14355113825847193, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.021461527794599533}, {"id": 1303, "seek": 744088, "start": 7441.84, "end": 7446.16, "text": " more or less. And so I think I do think that Turing would would would say, you know,", "tokens": [50412, 544, 420, 1570, 13, 400, 370, 286, 519, 286, 360, 519, 300, 314, 1345, 576, 576, 576, 584, 11, 291, 458, 11, 50628], "temperature": 0.0, "avg_logprob": -0.08197711495792165, "compression_ratio": 1.9291666666666667, "no_speech_prob": 0.025224722921848297}, {"id": 1304, "seek": 744088, "start": 7446.16, "end": 7451.84, "text": " as I predicted, you know, yeah, we've got there. Now, I think Turing would be fascinated to see", "tokens": [50628, 382, 286, 19147, 11, 291, 458, 11, 1338, 11, 321, 600, 658, 456, 13, 823, 11, 286, 519, 314, 1345, 576, 312, 24597, 281, 536, 50912], "temperature": 0.0, "avg_logprob": -0.08197711495792165, "compression_ratio": 1.9291666666666667, "no_speech_prob": 0.025224722921848297}, {"id": 1305, "seek": 744088, "start": 7451.84, "end": 7456.400000000001, "text": " the weaknesses that are there and the strengths that are there. And, you know, there are many things", "tokens": [50912, 264, 24381, 300, 366, 456, 293, 264, 16986, 300, 366, 456, 13, 400, 11, 291, 458, 11, 456, 366, 867, 721, 51140], "temperature": 0.0, "avg_logprob": -0.08197711495792165, "compression_ratio": 1.9291666666666667, "no_speech_prob": 0.025224722921848297}, {"id": 1306, "seek": 744088, "start": 7456.400000000001, "end": 7462.72, "text": " that today's systems can do that are vastly more powerful than I think he anticipated", "tokens": [51140, 300, 965, 311, 3652, 393, 360, 300, 366, 41426, 544, 4005, 813, 286, 519, 415, 23267, 51456], "temperature": 0.0, "avg_logprob": -0.08197711495792165, "compression_ratio": 1.9291666666666667, "no_speech_prob": 0.025224722921848297}, {"id": 1307, "seek": 744088, "start": 7462.72, "end": 7468.24, "text": " in that paper in the 1950s. And then there are other, you know, there will be other weaknesses,", "tokens": [51456, 294, 300, 3035, 294, 264, 18141, 82, 13, 400, 550, 456, 366, 661, 11, 291, 458, 11, 456, 486, 312, 661, 24381, 11, 51732], "temperature": 0.0, "avg_logprob": -0.08197711495792165, "compression_ratio": 1.9291666666666667, "no_speech_prob": 0.025224722921848297}, {"id": 1308, "seek": 746824, "start": 7468.24, "end": 7471.92, "text": " I think that would come out that would surprise him as they've surprised us all in a way. I think", "tokens": [50364, 286, 519, 300, 576, 808, 484, 300, 576, 6365, 796, 382, 436, 600, 6100, 505, 439, 294, 257, 636, 13, 286, 519, 50548], "temperature": 0.0, "avg_logprob": -0.11983159492755759, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.0031274226494133472}, {"id": 1309, "seek": 746824, "start": 7471.92, "end": 7476.639999999999, "text": " I think many of us in the field are surprised to see something that can do so amazingly in", "tokens": [50548, 286, 519, 867, 295, 505, 294, 264, 2519, 366, 6100, 281, 536, 746, 300, 393, 360, 370, 31762, 294, 50784], "temperature": 0.0, "avg_logprob": -0.11983159492755759, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.0031274226494133472}, {"id": 1310, "seek": 746824, "start": 7476.639999999999, "end": 7481.04, "text": " certain respects, and yet have so many, you know, still so many weaknesses and others.", "tokens": [50784, 1629, 24126, 11, 293, 1939, 362, 370, 867, 11, 291, 458, 11, 920, 370, 867, 24381, 293, 2357, 13, 51004], "temperature": 0.0, "avg_logprob": -0.11983159492755759, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.0031274226494133472}, {"id": 1311, "seek": 746824, "start": 7481.599999999999, "end": 7488.48, "text": " Can you introduce Naegle's bat? So in 1974, Thomas Naegle published this paper called", "tokens": [51032, 1664, 291, 5366, 6056, 1146, 306, 311, 7362, 30, 407, 294, 33422, 11, 8500, 6056, 1146, 306, 6572, 341, 3035, 1219, 51376], "temperature": 0.0, "avg_logprob": -0.11983159492755759, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.0031274226494133472}, {"id": 1312, "seek": 746824, "start": 7488.48, "end": 7494.16, "text": " What is it like to be a bat? And the point of this paper was to draw attention to the fact,", "tokens": [51376, 708, 307, 309, 411, 281, 312, 257, 7362, 30, 400, 264, 935, 295, 341, 3035, 390, 281, 2642, 3202, 281, 264, 1186, 11, 51660], "temperature": 0.0, "avg_logprob": -0.11983159492755759, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.0031274226494133472}, {"id": 1313, "seek": 749416, "start": 7494.16, "end": 7501.2, "text": " if it is a fact, that there are creatures that are very, very different to ourselves to humans,", "tokens": [50364, 498, 309, 307, 257, 1186, 11, 300, 456, 366, 12281, 300, 366, 588, 11, 588, 819, 281, 4175, 281, 6255, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10542671246962114, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0224157702177763}, {"id": 1314, "seek": 749416, "start": 7501.2, "end": 7506.96, "text": " but that we assume have some kind of consciousness. We assume in his terminology,", "tokens": [50716, 457, 300, 321, 6552, 362, 512, 733, 295, 10081, 13, 492, 6552, 294, 702, 27575, 11, 51004], "temperature": 0.0, "avg_logprob": -0.10542671246962114, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0224157702177763}, {"id": 1315, "seek": 749416, "start": 7506.96, "end": 7512.16, "text": " we assume that it's like something to be that creature. And he chose a bat because bats are", "tokens": [51004, 321, 6552, 300, 309, 311, 411, 746, 281, 312, 300, 12797, 13, 400, 415, 5111, 257, 7362, 570, 26943, 366, 51264], "temperature": 0.0, "avg_logprob": -0.10542671246962114, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0224157702177763}, {"id": 1316, "seek": 749416, "start": 7512.16, "end": 7518.5599999999995, "text": " very obviously very different to ourselves. They fly, they use sonar, they're pretty weird animals.", "tokens": [51264, 588, 2745, 588, 819, 281, 4175, 13, 814, 3603, 11, 436, 764, 1872, 289, 11, 436, 434, 1238, 3657, 4882, 13, 51584], "temperature": 0.0, "avg_logprob": -0.10542671246962114, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0224157702177763}, {"id": 1317, "seek": 751856, "start": 7519.52, "end": 7524.88, "text": " And so the way the thinking is that, well, it's probably like something to be a bat,", "tokens": [50412, 400, 370, 264, 636, 264, 1953, 307, 300, 11, 731, 11, 309, 311, 1391, 411, 746, 281, 312, 257, 7362, 11, 50680], "temperature": 0.0, "avg_logprob": -0.07282084803427419, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.026978394016623497}, {"id": 1318, "seek": 751856, "start": 7524.88, "end": 7528.8, "text": " but what it's like is going to be very different from what it's like to be a human.", "tokens": [50680, 457, 437, 309, 311, 411, 307, 516, 281, 312, 588, 819, 490, 437, 309, 311, 411, 281, 312, 257, 1952, 13, 50876], "temperature": 0.0, "avg_logprob": -0.07282084803427419, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.026978394016623497}, {"id": 1319, "seek": 751856, "start": 7529.360000000001, "end": 7535.84, "text": " And Naegle uses that example to get at a whole metaphysical idea, which again,", "tokens": [50904, 400, 6056, 1146, 306, 4960, 300, 1365, 281, 483, 412, 257, 1379, 30946, 36280, 1558, 11, 597, 797, 11, 51228], "temperature": 0.0, "avg_logprob": -0.07282084803427419, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.026978394016623497}, {"id": 1320, "seek": 751856, "start": 7535.84, "end": 7541.4400000000005, "text": " I would say is pointing to a kind of dualism, to suggest that there's a whole realm of facts", "tokens": [51228, 286, 576, 584, 307, 12166, 281, 257, 733, 295, 11848, 1434, 11, 281, 3402, 300, 456, 311, 257, 1379, 15355, 295, 9130, 51508], "temperature": 0.0, "avg_logprob": -0.07282084803427419, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.026978394016623497}, {"id": 1321, "seek": 754144, "start": 7541.5199999999995, "end": 7548.799999999999, "text": " about subjectivity, which are outside of the purview of objective science, actually,", "tokens": [50368, 466, 3983, 4253, 11, 597, 366, 2380, 295, 264, 1864, 1759, 295, 10024, 3497, 11, 767, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1352878146701389, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.045805227011442184}, {"id": 1322, "seek": 754144, "start": 7549.44, "end": 7557.839999999999, "text": " but which nevertheless are part of reality. And so for him, I feel that it alludes to a sort of", "tokens": [50764, 457, 597, 26924, 366, 644, 295, 4103, 13, 400, 370, 337, 796, 11, 286, 841, 300, 309, 439, 10131, 281, 257, 1333, 295, 51184], "temperature": 0.0, "avg_logprob": -0.1352878146701389, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.045805227011442184}, {"id": 1323, "seek": 754144, "start": 7559.839999999999, "end": 7562.48, "text": " kind of dualistic way of thinking again, that there's this realm of", "tokens": [51284, 733, 295, 11848, 3142, 636, 295, 1953, 797, 11, 300, 456, 311, 341, 15355, 295, 51416], "temperature": 0.0, "avg_logprob": -0.1352878146701389, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.045805227011442184}, {"id": 1324, "seek": 754144, "start": 7563.28, "end": 7567.759999999999, "text": " facts about subjective things, and there's a realm of facts about objective things.", "tokens": [51456, 9130, 466, 25972, 721, 11, 293, 456, 311, 257, 15355, 295, 9130, 466, 10024, 721, 13, 51680], "temperature": 0.0, "avg_logprob": -0.1352878146701389, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.045805227011442184}, {"id": 1325, "seek": 756776, "start": 7567.76, "end": 7572.4800000000005, "text": " I read Naegle's bat many years ago, his paper, and he was kind of saying, wouldn't it be great", "tokens": [50364, 286, 1401, 6056, 1146, 306, 311, 7362, 867, 924, 2057, 11, 702, 3035, 11, 293, 415, 390, 733, 295, 1566, 11, 2759, 380, 309, 312, 869, 50600], "temperature": 0.0, "avg_logprob": -0.07529584416803324, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.010733152739703655}, {"id": 1326, "seek": 756776, "start": 7572.4800000000005, "end": 7579.92, "text": " if we could move towards an objective phenomenology? And you are clear that he is pointing to dualism.", "tokens": [50600, 498, 321, 727, 1286, 3030, 364, 10024, 9388, 1793, 30, 400, 291, 366, 1850, 300, 415, 307, 12166, 281, 11848, 1434, 13, 50972], "temperature": 0.0, "avg_logprob": -0.07529584416803324, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.010733152739703655}, {"id": 1327, "seek": 756776, "start": 7579.92, "end": 7585.76, "text": " He's not just saying that it's inconceivable in the sense that I couldn't imagine what the experience", "tokens": [50972, 634, 311, 406, 445, 1566, 300, 309, 311, 20972, 384, 34376, 294, 264, 2020, 300, 286, 2809, 380, 3811, 437, 264, 1752, 51264], "temperature": 0.0, "avg_logprob": -0.07529584416803324, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.010733152739703655}, {"id": 1328, "seek": 756776, "start": 7585.76, "end": 7591.76, "text": " of a bat is like. It's just inconceivable. You're saying that he's actually making a dualism argument.", "tokens": [51264, 295, 257, 7362, 307, 411, 13, 467, 311, 445, 20972, 384, 34376, 13, 509, 434, 1566, 300, 415, 311, 767, 1455, 257, 11848, 1434, 6770, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07529584416803324, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.010733152739703655}, {"id": 1329, "seek": 759176, "start": 7592.72, "end": 7599.52, "text": " Well, he would probably deny that, because nearly every philosopher will enthusiastically deny that", "tokens": [50412, 1042, 11, 415, 576, 1391, 15744, 300, 11, 570, 6217, 633, 29805, 486, 18076, 22808, 15744, 300, 50752], "temperature": 0.0, "avg_logprob": -0.20233444067148063, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.009284828789532185}, {"id": 1330, "seek": 759176, "start": 7599.52, "end": 7610.72, "text": " they're dualists, but I see dualistic thinking all over the place, and I do think that this is an example of it.", "tokens": [50752, 436, 434, 11848, 1751, 11, 457, 286, 536, 11848, 3142, 1953, 439, 670, 264, 1081, 11, 293, 286, 360, 519, 300, 341, 307, 364, 1365, 295, 309, 13, 51312], "temperature": 0.0, "avg_logprob": -0.20233444067148063, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.009284828789532185}, {"id": 1331, "seek": 759176, "start": 7610.72, "end": 7615.68, "text": " Yes, I mean, it's a similar thing with John Searle. I think he is adamant that he's not a", "tokens": [51312, 1079, 11, 286, 914, 11, 309, 311, 257, 2531, 551, 365, 2619, 1100, 36153, 13, 286, 519, 415, 307, 16368, 394, 300, 415, 311, 406, 257, 51560], "temperature": 0.0, "avg_logprob": -0.20233444067148063, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.009284828789532185}, {"id": 1332, "seek": 759176, "start": 7615.68, "end": 7620.320000000001, "text": " dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another", "tokens": [51560, 11848, 468, 11, 457, 3195, 295, 561, 519, 415, 307, 257, 11848, 468, 13, 400, 767, 11, 264, 4649, 15982, 6770, 307, 1071, 51792], "temperature": 0.0, "avg_logprob": -0.20233444067148063, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.009284828789532185}, {"id": 1333, "seek": 762032, "start": 7620.4, "end": 7626.08, "text": " example of a popular thought experiment, which apparently people get wrong. My friend, Mark J.", "tokens": [50368, 1365, 295, 257, 3743, 1194, 5120, 11, 597, 7970, 561, 483, 2085, 13, 1222, 1277, 11, 3934, 508, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2311343238467262, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.0046250419691205025}, {"id": 1334, "seek": 762032, "start": 7626.08, "end": 7631.92, "text": " Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument.", "tokens": [50652, 30113, 11, 415, 1009, 1669, 257, 935, 295, 1566, 300, 561, 35736, 264, 4649, 15982, 6770, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2311343238467262, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.0046250419691205025}, {"id": 1335, "seek": 762032, "start": 7633.28, "end": 7644.32, "text": " Yes, Mark has tenaciously clinging to the Chinese language, the Chinese rim argument.", "tokens": [51012, 1079, 11, 3934, 575, 2064, 326, 8994, 596, 8716, 281, 264, 4649, 2856, 11, 264, 4649, 15982, 6770, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2311343238467262, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.0046250419691205025}, {"id": 1336, "seek": 764432, "start": 7644.719999999999, "end": 7650.639999999999, "text": " But I can't really see eye to eye with Mark, who I have enormous respect for on this particular", "tokens": [50384, 583, 286, 393, 380, 534, 536, 3313, 281, 3313, 365, 3934, 11, 567, 286, 362, 11322, 3104, 337, 322, 341, 1729, 50680], "temperature": 0.0, "avg_logprob": -0.18979205337225222, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.015743695199489594}, {"id": 1337, "seek": 764432, "start": 7650.639999999999, "end": 7657.92, "text": " subject, I'm afraid. Yes, Mark is a very good friend of mine, and maybe that's a rabbit hole,", "tokens": [50680, 3983, 11, 286, 478, 4638, 13, 1079, 11, 3934, 307, 257, 588, 665, 1277, 295, 3892, 11, 293, 1310, 300, 311, 257, 19509, 5458, 11, 51044], "temperature": 0.0, "avg_logprob": -0.18979205337225222, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.015743695199489594}, {"id": 1338, "seek": 764432, "start": 7657.92, "end": 7665.28, "text": " we won't go down. But I'm very amenable and convinced by some of Mark's arguments. But he's", "tokens": [51044, 321, 1582, 380, 352, 760, 13, 583, 286, 478, 588, 18497, 712, 293, 12561, 538, 512, 295, 3934, 311, 12869, 13, 583, 415, 311, 51412], "temperature": 0.0, "avg_logprob": -0.18979205337225222, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.015743695199489594}, {"id": 1339, "seek": 764432, "start": 7665.28, "end": 7672.48, "text": " certainly a phenomenologist, which is adamant that he is a monowist. It's another example of", "tokens": [51412, 3297, 257, 9388, 9201, 11, 597, 307, 16368, 394, 300, 415, 307, 257, 1108, 305, 468, 13, 467, 311, 1071, 1365, 295, 51772], "temperature": 0.0, "avg_logprob": -0.18979205337225222, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.015743695199489594}, {"id": 1340, "seek": 767248, "start": 7672.48, "end": 7675.919999999999, "text": " someone who claims they're not a dualist, but you would say they probably are a dualist.", "tokens": [50364, 1580, 567, 9441, 436, 434, 406, 257, 11848, 468, 11, 457, 291, 576, 584, 436, 1391, 366, 257, 11848, 468, 13, 50536], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1341, "seek": 767248, "start": 7675.919999999999, "end": 7681.04, "text": " Yeah, well, I don't know. We'd have to have Mark sitting here. I would have needed to have read", "tokens": [50536, 865, 11, 731, 11, 286, 500, 380, 458, 13, 492, 1116, 362, 281, 362, 3934, 3798, 510, 13, 286, 576, 362, 2978, 281, 362, 1401, 50792], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1342, "seek": 767248, "start": 7681.04, "end": 7686.0, "text": " one of his papers on this very recently to do that. So I'm not going to accuse him of anything,", "tokens": [50792, 472, 295, 702, 10577, 322, 341, 588, 3938, 281, 360, 300, 13, 407, 286, 478, 406, 516, 281, 43610, 796, 295, 1340, 11, 51040], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1343, "seek": 767248, "start": 7686.0, "end": 7688.879999999999, "text": " but if he was sitting here, we could have a discussion about it.", "tokens": [51040, 457, 498, 415, 390, 3798, 510, 11, 321, 727, 362, 257, 5017, 466, 309, 13, 51184], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1344, "seek": 767248, "start": 7688.879999999999, "end": 7693.12, "text": " Well, I asked him straight up, because I was trying to pin him down, and he said he is", "tokens": [51184, 1042, 11, 286, 2351, 796, 2997, 493, 11, 570, 286, 390, 1382, 281, 5447, 796, 760, 11, 293, 415, 848, 415, 307, 51396], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1345, "seek": 767248, "start": 7694.08, "end": 7698.4, "text": " an idealist, a monowist idealist. And it might be instructive, actually,", "tokens": [51444, 364, 7157, 468, 11, 257, 1108, 305, 468, 7157, 468, 13, 400, 309, 1062, 312, 7232, 488, 11, 767, 11, 51660], "temperature": 0.0, "avg_logprob": -0.0967238289969308, "compression_ratio": 1.7003367003367003, "no_speech_prob": 0.03365464136004448}, {"id": 1346, "seek": 769840, "start": 7698.4, "end": 7701.599999999999, "text": " if you just to explain what do we mean by idealism?", "tokens": [50364, 498, 291, 445, 281, 2903, 437, 360, 321, 914, 538, 7157, 1434, 30, 50524], "temperature": 0.0, "avg_logprob": -0.14364650772839058, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.02330184169113636}, {"id": 1347, "seek": 769840, "start": 7701.599999999999, "end": 7709.12, "text": " Right. Well, so especially if he's using the word monow there as well, then I guess that", "tokens": [50524, 1779, 13, 1042, 11, 370, 2318, 498, 415, 311, 1228, 264, 1349, 1108, 305, 456, 382, 731, 11, 550, 286, 2041, 300, 50900], "temperature": 0.0, "avg_logprob": -0.14364650772839058, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.02330184169113636}, {"id": 1348, "seek": 769840, "start": 7709.12, "end": 7716.799999999999, "text": " he is suggesting that while a physicalist thinks that there is only one substance in reality,", "tokens": [50900, 415, 307, 18094, 300, 1339, 257, 4001, 468, 7309, 300, 456, 307, 787, 472, 12961, 294, 4103, 11, 51284], "temperature": 0.0, "avg_logprob": -0.14364650772839058, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.02330184169113636}, {"id": 1349, "seek": 769840, "start": 7716.799999999999, "end": 7726.4, "text": " and that's material reality. And so there is no such thing as a separate stuff of mind", "tokens": [51284, 293, 300, 311, 2527, 4103, 13, 400, 370, 456, 307, 572, 1270, 551, 382, 257, 4994, 1507, 295, 1575, 51764], "temperature": 0.0, "avg_logprob": -0.14364650772839058, "compression_ratio": 1.5432692307692308, "no_speech_prob": 0.02330184169113636}, {"id": 1350, "seek": 772640, "start": 7726.4, "end": 7732.32, "text": " metaphysically. So for the physicalist, there is no dualism if they really, really think that,", "tokens": [50364, 30946, 749, 984, 13, 407, 337, 264, 4001, 468, 11, 456, 307, 572, 11848, 1434, 498, 436, 534, 11, 534, 519, 300, 11, 50660], "temperature": 0.0, "avg_logprob": -0.09358289739587805, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.02217092551290989}, {"id": 1351, "seek": 772640, "start": 7732.32, "end": 7736.799999999999, "text": " but the trouble is that as soon as you kind of probe, then often they have struggle to deal with", "tokens": [50660, 457, 264, 5253, 307, 300, 382, 2321, 382, 291, 733, 295, 22715, 11, 550, 2049, 436, 362, 7799, 281, 2028, 365, 50884], "temperature": 0.0, "avg_logprob": -0.09358289739587805, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.02217092551290989}, {"id": 1352, "seek": 772640, "start": 7737.5199999999995, "end": 7743.2, "text": " dualistic intuitions about subjectivity. Now the idealist, on the other hand, also", "tokens": [50920, 11848, 3142, 16224, 626, 466, 3983, 4253, 13, 823, 264, 7157, 468, 11, 322, 264, 661, 1011, 11, 611, 51204], "temperature": 0.0, "avg_logprob": -0.09358289739587805, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.02217092551290989}, {"id": 1353, "seek": 772640, "start": 7743.839999999999, "end": 7749.12, "text": " thinks that there's only one substance, but that substance is the substance of mind. So", "tokens": [51236, 7309, 300, 456, 311, 787, 472, 12961, 11, 457, 300, 12961, 307, 264, 12961, 295, 1575, 13, 407, 51500], "temperature": 0.0, "avg_logprob": -0.09358289739587805, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.02217092551290989}, {"id": 1354, "seek": 774912, "start": 7749.92, "end": 7755.76, "text": " physical reality has to be a kind of construct out of this one substance,", "tokens": [50404, 4001, 4103, 575, 281, 312, 257, 733, 295, 7690, 484, 295, 341, 472, 12961, 11, 50696], "temperature": 0.0, "avg_logprob": -0.13825651577540807, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0003147790557704866}, {"id": 1355, "seek": 774912, "start": 7756.48, "end": 7762.8, "text": " this one out of mind stuff. So that's a very different kind of, it's a different way of", "tokens": [50732, 341, 472, 484, 295, 1575, 1507, 13, 407, 300, 311, 257, 588, 819, 733, 295, 11, 309, 311, 257, 819, 636, 295, 51048], "temperature": 0.0, "avg_logprob": -0.13825651577540807, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0003147790557704866}, {"id": 1356, "seek": 774912, "start": 7762.8, "end": 7770.5599999999995, "text": " avoiding dualism, but it has its own problems about how do you explain account for science", "tokens": [51048, 20220, 11848, 1434, 11, 457, 309, 575, 1080, 1065, 2740, 466, 577, 360, 291, 2903, 2696, 337, 3497, 51436], "temperature": 0.0, "avg_logprob": -0.13825651577540807, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0003147790557704866}, {"id": 1357, "seek": 774912, "start": 7770.5599999999995, "end": 7775.28, "text": " and the success of science and so on. I also interviewed Philip Goff in a beautiful studio", "tokens": [51436, 293, 264, 2245, 295, 3497, 293, 370, 322, 13, 286, 611, 19770, 21144, 1037, 602, 294, 257, 2238, 6811, 51672], "temperature": 0.0, "avg_logprob": -0.13825651577540807, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0003147790557704866}, {"id": 1358, "seek": 777528, "start": 7775.28, "end": 7782.96, "text": " recently, and he is a cosmo-psychist, but I think it's better just to use the word pan-psychist,", "tokens": [50364, 3938, 11, 293, 415, 307, 257, 3792, 3280, 12, 1878, 16384, 468, 11, 457, 286, 519, 309, 311, 1101, 445, 281, 764, 264, 1349, 2462, 12, 1878, 16384, 468, 11, 50748], "temperature": 0.0, "avg_logprob": -0.13934448787144252, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.006687610410153866}, {"id": 1359, "seek": 777528, "start": 7782.96, "end": 7790.32, "text": " but I think cosmo-psychist is where you have the teleology baked in. So rather than it being bottom", "tokens": [50748, 457, 286, 519, 3792, 3280, 12, 1878, 16384, 468, 307, 689, 291, 362, 264, 4304, 1793, 19453, 294, 13, 407, 2831, 813, 309, 885, 2767, 51116], "temperature": 0.0, "avg_logprob": -0.13934448787144252, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.006687610410153866}, {"id": 1360, "seek": 777528, "start": 7790.32, "end": 7794.96, "text": " up, it's kind of top down. There's some cosmic purpose to the universe, and the universe as a", "tokens": [51116, 493, 11, 309, 311, 733, 295, 1192, 760, 13, 821, 311, 512, 27614, 4334, 281, 264, 6445, 11, 293, 264, 6445, 382, 257, 51348], "temperature": 0.0, "avg_logprob": -0.13934448787144252, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.006687610410153866}, {"id": 1361, "seek": 777528, "start": 7794.96, "end": 7801.679999999999, "text": " whole is made of, you know, the fundamental material of the universe is consciousness.", "tokens": [51348, 1379, 307, 1027, 295, 11, 291, 458, 11, 264, 8088, 2527, 295, 264, 6445, 307, 10081, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13934448787144252, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.006687610410153866}, {"id": 1362, "seek": 780168, "start": 7801.68, "end": 7805.280000000001, "text": " And what's the relationship between that view and idealism?", "tokens": [50364, 400, 437, 311, 264, 2480, 1296, 300, 1910, 293, 7157, 1434, 30, 50544], "temperature": 0.0, "avg_logprob": -0.1414506196975708, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.005074701737612486}, {"id": 1363, "seek": 780168, "start": 7805.280000000001, "end": 7814.08, "text": " So I guess for the pan-psychist, so the pan-psychist is so far as I understand these", "tokens": [50544, 407, 286, 2041, 337, 264, 2462, 12, 1878, 16384, 468, 11, 370, 264, 2462, 12, 1878, 16384, 468, 307, 370, 1400, 382, 286, 1223, 613, 50984], "temperature": 0.0, "avg_logprob": -0.1414506196975708, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.005074701737612486}, {"id": 1364, "seek": 780168, "start": 7814.08, "end": 7819.68, "text": " philosophical positions. I mean, I shouldn't put myself forward as somebody who necessarily", "tokens": [50984, 25066, 8432, 13, 286, 914, 11, 286, 4659, 380, 829, 2059, 2128, 382, 2618, 567, 4725, 51264], "temperature": 0.0, "avg_logprob": -0.1414506196975708, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.005074701737612486}, {"id": 1365, "seek": 780168, "start": 7819.68, "end": 7827.68, "text": " understands them in depth, and everybody who subscribes to these views has a slightly different", "tokens": [51264, 15146, 552, 294, 7161, 11, 293, 2201, 567, 2325, 6446, 281, 613, 6809, 575, 257, 4748, 819, 51664], "temperature": 0.0, "avg_logprob": -0.1414506196975708, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.005074701737612486}, {"id": 1366, "seek": 782768, "start": 7827.68, "end": 7834.56, "text": " version of them as well. But the pan-psychist certainly thinks that reality is composed of", "tokens": [50364, 3037, 295, 552, 382, 731, 13, 583, 264, 2462, 12, 1878, 16384, 468, 3297, 7309, 300, 4103, 307, 18204, 295, 50708], "temperature": 0.0, "avg_logprob": -0.08317353233458503, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.004097286611795425}, {"id": 1367, "seek": 782768, "start": 7834.56, "end": 7843.200000000001, "text": " physical material substance, but that physical material substance irreducibly has a psychological", "tokens": [50708, 4001, 2527, 12961, 11, 457, 300, 4001, 2527, 12961, 16014, 769, 537, 25021, 575, 257, 14346, 51140], "temperature": 0.0, "avg_logprob": -0.08317353233458503, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.004097286611795425}, {"id": 1368, "seek": 782768, "start": 7843.200000000001, "end": 7855.04, "text": " dimension to it, a mental dimension to it. So every physical object has a little bit of", "tokens": [51140, 10139, 281, 309, 11, 257, 4973, 10139, 281, 309, 13, 407, 633, 4001, 2657, 575, 257, 707, 857, 295, 51732], "temperature": 0.0, "avg_logprob": -0.08317353233458503, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.004097286611795425}, {"id": 1369, "seek": 785504, "start": 7855.04, "end": 7859.76, "text": " consciousness in it, if you like, in some sense. I mean, I find it a very difficult", "tokens": [50364, 10081, 294, 309, 11, 498, 291, 411, 11, 294, 512, 2020, 13, 286, 914, 11, 286, 915, 309, 257, 588, 2252, 50600], "temperature": 0.0, "avg_logprob": -0.13039952572261063, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.07109539955854416}, {"id": 1370, "seek": 785504, "start": 7861.44, "end": 7866.64, "text": " view to articulate because I just find it so completely counterintuitive. So I can't really", "tokens": [50684, 1910, 281, 30305, 570, 286, 445, 915, 309, 370, 2584, 5682, 686, 48314, 13, 407, 286, 393, 380, 534, 50944], "temperature": 0.0, "avg_logprob": -0.13039952572261063, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.07109539955854416}, {"id": 1371, "seek": 785504, "start": 7866.64, "end": 7873.2, "text": " put the pan-psychist's hat on and express their point of view. You need a pan-psychist here to do", "tokens": [50944, 829, 264, 2462, 12, 1878, 16384, 468, 311, 2385, 322, 293, 5109, 641, 935, 295, 1910, 13, 509, 643, 257, 2462, 12, 1878, 16384, 468, 510, 281, 360, 51272], "temperature": 0.0, "avg_logprob": -0.13039952572261063, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.07109539955854416}, {"id": 1372, "seek": 785504, "start": 7873.2, "end": 7882.0, "text": " that. Because to my mind, this is, you know, again with my bitkinstinian hat on, then I just think,", "tokens": [51272, 300, 13, 1436, 281, 452, 1575, 11, 341, 307, 11, 291, 458, 11, 797, 365, 452, 857, 5843, 372, 259, 952, 2385, 322, 11, 550, 286, 445, 519, 11, 51712], "temperature": 0.0, "avg_logprob": -0.13039952572261063, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.07109539955854416}, {"id": 1373, "seek": 788200, "start": 7882.08, "end": 7888.64, "text": " well, how do we use words like consciousness? Well, we use the word consciousness and all of", "tokens": [50368, 731, 11, 577, 360, 321, 764, 2283, 411, 10081, 30, 1042, 11, 321, 764, 264, 1349, 10081, 293, 439, 295, 50696], "temperature": 0.0, "avg_logprob": -0.10969444874967083, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.06768956780433655}, {"id": 1374, "seek": 788200, "start": 7888.64, "end": 7897.12, "text": " the related terms in the context of each other, of other human beings. And so, you know, and it's", "tokens": [50696, 264, 4077, 2115, 294, 264, 4319, 295, 1184, 661, 11, 295, 661, 1952, 8958, 13, 400, 370, 11, 291, 458, 11, 293, 309, 311, 51120], "temperature": 0.0, "avg_logprob": -0.10969444874967083, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.06768956780433655}, {"id": 1375, "seek": 788200, "start": 7897.12, "end": 7904.56, "text": " in the context of our being together in the world and your behaving in certain ways and our", "tokens": [51120, 294, 264, 4319, 295, 527, 885, 1214, 294, 264, 1002, 293, 428, 35263, 294, 1629, 2098, 293, 527, 51492], "temperature": 0.0, "avg_logprob": -0.10969444874967083, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.06768956780433655}, {"id": 1376, "seek": 788200, "start": 7904.56, "end": 7910.0, "text": " exchanging certain looks when we're doing things and that we understand each other as fellow", "tokens": [51492, 6210, 9741, 1629, 1542, 562, 321, 434, 884, 721, 293, 300, 321, 1223, 1184, 661, 382, 7177, 51764], "temperature": 0.0, "avg_logprob": -0.10969444874967083, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.06768956780433655}, {"id": 1377, "seek": 791000, "start": 7910.0, "end": 7914.8, "text": " conscious creatures. And so that's the context in which we use, you know, words like consciousness.", "tokens": [50364, 6648, 12281, 13, 400, 370, 300, 311, 264, 4319, 294, 597, 321, 764, 11, 291, 458, 11, 2283, 411, 10081, 13, 50604], "temperature": 0.0, "avg_logprob": -0.08147308561537, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.01074319425970316}, {"id": 1378, "seek": 791000, "start": 7914.8, "end": 7920.24, "text": " And when I speak about, you know, you're not conscious, you're asleep. And, you know, it's", "tokens": [50604, 400, 562, 286, 1710, 466, 11, 291, 458, 11, 291, 434, 406, 6648, 11, 291, 434, 11039, 13, 400, 11, 291, 458, 11, 309, 311, 50876], "temperature": 0.0, "avg_logprob": -0.08147308561537, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.01074319425970316}, {"id": 1379, "seek": 791000, "start": 7920.24, "end": 7926.72, "text": " all in the context of other humans that we use those words. So it's just ludicrously inapplicable", "tokens": [50876, 439, 294, 264, 4319, 295, 661, 6255, 300, 321, 764, 729, 2283, 13, 407, 309, 311, 445, 15946, 299, 81, 5098, 294, 1746, 1050, 712, 51200], "temperature": 0.0, "avg_logprob": -0.08147308561537, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.01074319425970316}, {"id": 1380, "seek": 791000, "start": 7926.72, "end": 7934.08, "text": " to use that word in the context of, you know, I don't know, a brick or a toaster or an atom.", "tokens": [51200, 281, 764, 300, 1349, 294, 264, 4319, 295, 11, 291, 458, 11, 286, 500, 380, 458, 11, 257, 16725, 420, 257, 281, 1727, 420, 364, 12018, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08147308561537, "compression_ratio": 1.8317307692307692, "no_speech_prob": 0.01074319425970316}, {"id": 1381, "seek": 793408, "start": 7934.96, "end": 7940.16, "text": " It's just simply the words simply are not applicable in those contexts.", "tokens": [50408, 467, 311, 445, 2935, 264, 2283, 2935, 366, 406, 21142, 294, 729, 30628, 13, 50668], "temperature": 0.0, "avg_logprob": -0.12889328649488546, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.032805170863866806}, {"id": 1382, "seek": 793408, "start": 7940.16, "end": 7945.6, "text": " Yes, it's so interesting because Philip told me that he grew up as a Christian. And I've had Richard", "tokens": [50668, 1079, 11, 309, 311, 370, 1880, 570, 21144, 1907, 385, 300, 415, 6109, 493, 382, 257, 5778, 13, 400, 286, 600, 632, 9809, 50940], "temperature": 0.0, "avg_logprob": -0.12889328649488546, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.032805170863866806}, {"id": 1383, "seek": 793408, "start": 7945.6, "end": 7950.32, "text": " Swinburne on as well. And he's got a book out about are we bodies or souls, you know, putting", "tokens": [50940, 3926, 259, 13243, 716, 322, 382, 731, 13, 400, 415, 311, 658, 257, 1446, 484, 466, 366, 321, 7510, 420, 16588, 11, 291, 458, 11, 3372, 51176], "temperature": 0.0, "avg_logprob": -0.12889328649488546, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.032805170863866806}, {"id": 1384, "seek": 793408, "start": 7950.32, "end": 7956.08, "text": " forward the case for substance dualism. And so there's the moving away from dualism thing,", "tokens": [51176, 2128, 264, 1389, 337, 12961, 11848, 1434, 13, 400, 370, 456, 311, 264, 2684, 1314, 490, 11848, 1434, 551, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12889328649488546, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.032805170863866806}, {"id": 1385, "seek": 793408, "start": 7956.08, "end": 7960.64, "text": " there's the teleology things. Because I think if you are of this frame of mind, you like to believe", "tokens": [51464, 456, 311, 264, 4304, 1793, 721, 13, 1436, 286, 519, 498, 291, 366, 295, 341, 3920, 295, 1575, 11, 291, 411, 281, 1697, 51692], "temperature": 0.0, "avg_logprob": -0.12889328649488546, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.032805170863866806}, {"id": 1386, "seek": 796064, "start": 7960.64, "end": 7968.160000000001, "text": " that there's some kind of grand purpose. And there's also the wanting to not wanting to be a", "tokens": [50364, 300, 456, 311, 512, 733, 295, 2697, 4334, 13, 400, 456, 311, 611, 264, 7935, 281, 406, 7935, 281, 312, 257, 50740], "temperature": 0.0, "avg_logprob": -0.11527772744496663, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.008815600536763668}, {"id": 1387, "seek": 796064, "start": 7968.160000000001, "end": 7973.280000000001, "text": " monowist, basically. So you could perceive it as a form of mental gymnastics to say, okay, well,", "tokens": [50740, 1108, 305, 468, 11, 1936, 13, 407, 291, 727, 20281, 309, 382, 257, 1254, 295, 4973, 48461, 281, 584, 11, 1392, 11, 731, 11, 50996], "temperature": 0.0, "avg_logprob": -0.11527772744496663, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.008815600536763668}, {"id": 1388, "seek": 796064, "start": 7973.280000000001, "end": 7978.72, "text": " I don't want to be a substance dualist, but why don't we rearrange the structure somewhat so that", "tokens": [50996, 286, 500, 380, 528, 281, 312, 257, 12961, 11848, 468, 11, 457, 983, 500, 380, 321, 39568, 264, 3877, 8344, 370, 300, 51268], "temperature": 0.0, "avg_logprob": -0.11527772744496663, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.008815600536763668}, {"id": 1389, "seek": 796064, "start": 7978.72, "end": 7984.240000000001, "text": " consciousness comes first. And there's some kind of cosmic purpose. And we're building a worldview", "tokens": [51268, 10081, 1487, 700, 13, 400, 456, 311, 512, 733, 295, 27614, 4334, 13, 400, 321, 434, 2390, 257, 41141, 51544], "temperature": 0.0, "avg_logprob": -0.11527772744496663, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.008815600536763668}, {"id": 1390, "seek": 796064, "start": 7984.240000000001, "end": 7988.4800000000005, "text": " that still makes sense to me. Yeah, yeah. I mean, I personally, I think all of these", "tokens": [51544, 300, 920, 1669, 2020, 281, 385, 13, 865, 11, 1338, 13, 286, 914, 11, 286, 5665, 11, 286, 519, 439, 295, 613, 51756], "temperature": 0.0, "avg_logprob": -0.11527772744496663, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.008815600536763668}, {"id": 1391, "seek": 798848, "start": 7989.2, "end": 7995.28, "text": " positions involve a great deal of mental gymnastics. And, and I, you know, I reject", "tokens": [50400, 8432, 9494, 257, 869, 2028, 295, 4973, 48461, 13, 400, 11, 293, 286, 11, 291, 458, 11, 286, 8248, 50704], "temperature": 0.0, "avg_logprob": -0.11418988065021794, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.014731626026332378}, {"id": 1392, "seek": 798848, "start": 7995.28, "end": 8001.679999999999, "text": " any kind of ism. Or what I mean by that is I don't use that term to describe, you know, my views", "tokens": [50704, 604, 733, 295, 307, 76, 13, 1610, 437, 286, 914, 538, 300, 307, 286, 500, 380, 764, 300, 1433, 281, 6786, 11, 291, 458, 11, 452, 6809, 51024], "temperature": 0.0, "avg_logprob": -0.11418988065021794, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.014731626026332378}, {"id": 1393, "seek": 798848, "start": 8001.679999999999, "end": 8008.959999999999, "text": " at all. So all of these isms are, you know, are misguided. And what we need to do is just to", "tokens": [51024, 412, 439, 13, 407, 439, 295, 613, 307, 2592, 366, 11, 291, 458, 11, 366, 3346, 2794, 2112, 13, 400, 437, 321, 643, 281, 360, 307, 445, 281, 51388], "temperature": 0.0, "avg_logprob": -0.11418988065021794, "compression_ratio": 1.5251396648044693, "no_speech_prob": 0.014731626026332378}, {"id": 1394, "seek": 800896, "start": 8009.2, "end": 8022.08, "text": " to dismantle the whole way of talking, which makes us, you know, makes us confused in the context of", "tokens": [50376, 281, 30506, 306, 264, 1379, 636, 295, 1417, 11, 597, 1669, 505, 11, 291, 458, 11, 1669, 505, 9019, 294, 264, 4319, 295, 51020], "temperature": 0.0, "avg_logprob": -0.12702112396558127, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.07486199587583542}, {"id": 1395, "seek": 800896, "start": 8022.08, "end": 8027.84, "text": " this kind of these kinds of issues. So that's what Wittgenstein is trying to do, as he puts it,", "tokens": [51020, 341, 733, 295, 613, 3685, 295, 2663, 13, 407, 300, 311, 437, 343, 593, 1766, 9089, 307, 1382, 281, 360, 11, 382, 415, 8137, 309, 11, 51308], "temperature": 0.0, "avg_logprob": -0.12702112396558127, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.07486199587583542}, {"id": 1396, "seek": 800896, "start": 8027.84, "end": 8032.16, "text": " to show the fly the way out of the bottle, right? This is his famous phrase. So the fly is the", "tokens": [51308, 281, 855, 264, 3603, 264, 636, 484, 295, 264, 7817, 11, 558, 30, 639, 307, 702, 4618, 9535, 13, 407, 264, 3603, 307, 264, 51524], "temperature": 0.0, "avg_logprob": -0.12702112396558127, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.07486199587583542}, {"id": 1397, "seek": 800896, "start": 8032.16, "end": 8036.0, "text": " person who's ended up thinking all these philosophical thoughts by taking ordinary", "tokens": [51524, 954, 567, 311, 4590, 493, 1953, 439, 613, 25066, 4598, 538, 1940, 10547, 51716], "temperature": 0.0, "avg_logprob": -0.12702112396558127, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.07486199587583542}, {"id": 1398, "seek": 803600, "start": 8036.0, "end": 8041.76, "text": " language into strange places, taking it on holiday. And, and so to show the fly the way", "tokens": [50364, 2856, 666, 5861, 3190, 11, 1940, 309, 322, 9960, 13, 400, 11, 293, 370, 281, 855, 264, 3603, 264, 636, 50652], "temperature": 0.0, "avg_logprob": -0.12438045848499645, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.0697973370552063}, {"id": 1399, "seek": 803600, "start": 8041.76, "end": 8047.68, "text": " out of the bottle is to just bring all of these concepts back to their ordinary everyday usage", "tokens": [50652, 484, 295, 264, 7817, 307, 281, 445, 1565, 439, 295, 613, 10392, 646, 281, 641, 10547, 7429, 14924, 50948], "temperature": 0.0, "avg_logprob": -0.12438045848499645, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.0697973370552063}, {"id": 1400, "seek": 803600, "start": 8047.68, "end": 8054.16, "text": " and to show thereby that you haven't really, you haven't lost anything, the puzzles evaporate.", "tokens": [50948, 293, 281, 855, 28281, 300, 291, 2378, 380, 534, 11, 291, 2378, 380, 2731, 1340, 11, 264, 24138, 26315, 473, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12438045848499645, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.0697973370552063}, {"id": 1401, "seek": 803600, "start": 8054.16, "end": 8057.84, "text": " So that isn't an ism. That's a, that's, that's rather that's a kind of", "tokens": [51272, 407, 300, 1943, 380, 364, 307, 76, 13, 663, 311, 257, 11, 300, 311, 11, 300, 311, 2831, 300, 311, 257, 733, 295, 51456], "temperature": 0.0, "avg_logprob": -0.12438045848499645, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.0697973370552063}, {"id": 1402, "seek": 803600, "start": 8059.92, "end": 8065.04, "text": " kind of critical methodology for just shifting the way that you think and talk all together.", "tokens": [51560, 733, 295, 4924, 24850, 337, 445, 17573, 264, 636, 300, 291, 519, 293, 751, 439, 1214, 13, 51816], "temperature": 0.0, "avg_logprob": -0.12438045848499645, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.0697973370552063}, {"id": 1403, "seek": 806600, "start": 8066.48, "end": 8071.76, "text": " And final question on this, where do you sit on the kind of the teleology question? So one view is", "tokens": [50388, 400, 2572, 1168, 322, 341, 11, 689, 360, 291, 1394, 322, 264, 733, 295, 264, 4304, 1793, 1168, 30, 407, 472, 1910, 307, 50652], "temperature": 0.0, "avg_logprob": -0.11682066143068492, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0025152426678687334}, {"id": 1404, "seek": 806600, "start": 8071.76, "end": 8078.32, "text": " that we have a purpose. The the physicist would argue that it emerges, you know, from quantum", "tokens": [50652, 300, 321, 362, 257, 4334, 13, 440, 264, 42466, 576, 9695, 300, 309, 38965, 11, 291, 458, 11, 490, 13018, 50980], "temperature": 0.0, "avg_logprob": -0.11682066143068492, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0025152426678687334}, {"id": 1405, "seek": 806600, "start": 8078.32, "end": 8084.24, "text": " field theory and a lot of sophisticated study of biology kind of build this intermediate view", "tokens": [50980, 2519, 5261, 293, 257, 688, 295, 16950, 2979, 295, 14956, 733, 295, 1322, 341, 19376, 1910, 51276], "temperature": 0.0, "avg_logprob": -0.11682066143068492, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0025152426678687334}, {"id": 1406, "seek": 806600, "start": 8084.24, "end": 8089.04, "text": " of teleonomy. Where do you sit on that kind of spectrum? I'll be honest with you, I don't think", "tokens": [51276, 295, 4304, 23423, 13, 2305, 360, 291, 1394, 322, 300, 733, 295, 11143, 30, 286, 603, 312, 3245, 365, 291, 11, 286, 500, 380, 519, 51516], "temperature": 0.0, "avg_logprob": -0.11682066143068492, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0025152426678687334}, {"id": 1407, "seek": 806600, "start": 8089.04, "end": 8094.32, "text": " I sit anywhere on that spectrum. I think I think those are issues on which I don't have sufficient", "tokens": [51516, 286, 1394, 4992, 322, 300, 11143, 13, 286, 519, 286, 519, 729, 366, 2663, 322, 597, 286, 500, 380, 362, 11563, 51780], "temperature": 0.0, "avg_logprob": -0.11682066143068492, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0025152426678687334}, {"id": 1408, "seek": 809432, "start": 8094.32, "end": 8100.639999999999, "text": " expertise to pronounce. So, so, you know, you just keep me, keep me on the on consciousness", "tokens": [50364, 11769, 281, 19567, 13, 407, 11, 370, 11, 291, 458, 11, 291, 445, 1066, 385, 11, 1066, 385, 322, 264, 322, 10081, 50680], "temperature": 0.0, "avg_logprob": -0.23266566318014395, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.005483319982886314}, {"id": 1409, "seek": 809432, "start": 8100.639999999999, "end": 8106.0, "text": " and cognition, you know, all this stuff is above my pay grade, you know.", "tokens": [50680, 293, 46905, 11, 291, 458, 11, 439, 341, 1507, 307, 3673, 452, 1689, 7204, 11, 291, 458, 13, 50948], "temperature": 0.0, "avg_logprob": -0.23266566318014395, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.005483319982886314}, {"id": 1410, "seek": 809432, "start": 8106.0, "end": 8110.0, "text": " Professor Shanahan, it's been an absolute honor to have you on MLST. Thank you so much. I really", "tokens": [50948, 8419, 25536, 21436, 11, 309, 311, 668, 364, 8236, 5968, 281, 362, 291, 322, 21601, 6840, 13, 1044, 291, 370, 709, 13, 286, 534, 51148], "temperature": 0.0, "avg_logprob": -0.23266566318014395, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.005483319982886314}, {"id": 1411, "seek": 809432, "start": 8110.0, "end": 8124.16, "text": " appreciate it. Thank you. Thank you so much for having me. It's been fun.", "tokens": [51148, 4449, 309, 13, 1044, 291, 13, 1044, 291, 370, 709, 337, 1419, 385, 13, 467, 311, 668, 1019, 13, 51856], "temperature": 0.0, "avg_logprob": -0.23266566318014395, "compression_ratio": 1.6262135922330097, "no_speech_prob": 0.005483319982886314}], "language": "en"}