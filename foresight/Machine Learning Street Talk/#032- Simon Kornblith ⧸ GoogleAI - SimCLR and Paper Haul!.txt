Welcome back. Today we're talking with Dr Simon Cornblith, a research scientist in the
Google Brain team. Simon is most famous for being one of the authors on Simclear, the computer
vision paper that used self-supervised learning and the contrastive loss with loads of cool
image augmentations. Simon also used to be a neuroscientist.
When I was pretty young, I was interested in consciousness and how we create this kind
of impression of the external world inside our heads. And so I guess it's pretty obvious
how that translates into an interest in brains and how the brain works.
Turns out the neuroscience is really difficult. Progress is really slow and tedious. Simon's
goal is to understand the inner workings of neural networks, both in meat space and
in silicon. He initially thought that the artificial variety might be easier to understand.
He was in for a rude awakening.
So in a neural network, we can record all the neurons, which is extremely challenging
in a biological organism. And we can also manipulate the system in any kind of way that
we can imagine. But it still seems really hard to understand neural networks. I think there
are a lot of ideas from machine learning that will ultimately help us understand brains.
Maybe we could make some headway that might eventually translate back to brains. And so
that's how I ended up in machine learning.
People often try and anthropomorphise neural networks.
People try to relate whatever neural network they've built back to a brain and they say
that it works like the brain, but it doesn't work like the brain.
So Simon was involved in this paper, do wide and deep networks learn the same things, uncovering
how neural network representations vary with width and depth.
Simon pioneered this really fascinating way of comparing representations by comparing
features. And what this essentially amounts to is we need to have a similarity function
so that we can compare the representations in layers to themselves in different parts
of the network or indeed to other networks.
And so for this similarity measure to work well, the first thing Simon did was take two
architecturally identical networks, A and B, trained from different random initialisations
and just ensure that the third convolution layer is more self-similar to its counterpart
than any of the other layers.
If that works, then you're onto something. Turns out that's not super simple to do, but
Simon came up with this concept called the centred kernel alignment, which we'll talk
about on the call.
But this is actually super fascinating. We're talking about this idea here of using self-similarity
to reason about the evolution of representations throughout successive layers in the neural
network. And what Simon found is that you get this kind of characteristic blockiness.
So when you see these large blocks, what it means is that the representations are no longer
evolving in respect of time.
So it's showing here the representational similarity of all of the layers against themselves
and against all of the other layers.
So clearly there's this characteristic diagonal down the matrix, as you would see with any
self-similarity matrix.
And because this blockiness appears, it means that nothing is happening.
And what Simon realised is you can actually delete these layers from the neural network
and it wouldn't make any difference because it hasn't learned anything new.
But it's also a really interesting way of reasoning about a kind of pathology, a weird
thing that happens when you saturate a neural network.
So he said that this presence of this block structure is an indicator of the halting of
evolution and a strong indicator of over-parameterisation.
And he actually shows that this blockiness appears on deeper networks and wider networks.
But this concept of self-similarity analysis is not new to me.
On my PhD, I was fascinated in segmenting DJ-mixed music shows and I actually used the same techniques
for learning regimes in financial datasets later on.
This is an example of a DJ-mix which I segmented.
I came up with a dynamic programming algorithm which would essentially sum up all of the
tiles along this diagonal and compute the lowest costs contiguous segmentation.
And it's super interesting.
So here are two music tracks and you can see that they are more self-similar to each other
than they are any of the other tracks just because of the tone of the colour here.
And if you zoom into a track, you can even see that there are symmetries.
This part of the track here is a repetition from this part of the track here.
And you can tell that from this kind of symmetry pattern on the diagonal.
And you can see that there's a little bit in the track in the middle here which is not
similar to any other part of the track.
You see some really interesting stuff here and essentially I'm a huge fan of anyone using
self-similarity matrices for reasoning about the evolution of representations.
I think it's a fascinating idea.
So how did Simon come up with this measure of similarity?
The centred kernel alignment.
Jeff Hinton had another idea and I tried the idea that it worked but then we wondered is
there a simpler thing that worked.
And that's how we ended up with centred kernel alignment.
The blockiness in these matrices is absolutely fascinating but how much can we read into
it?
It's not clear what we should really expect in terms of how a neural network representation
evolves through the layers.
I think there's kind of some theory on what we should expect if all the layers are linear.
But like obviously the neural networks that we train are nonlinear and it's really important
to have a nonlinearity in between the layers.
If we see that nothing is changing from one layer to the next that's a really bad sign.
If the neural network representation isn't changing then obviously nothing's happening.
We couldn't have predicted this ahead of time based on what we know about neural network
theory and we couldn't have predicted it ahead of time based on the accuracy of the network.
Does this apply to ResNets though?
I thought that they could learn their own capacity.
You can either look at networks without residual connections where you do actually find that
at some depth the accuracy will start going down and in networks without residual connections
we find that the depth where accuracy starts to go down is like around the same depth where
you begin seeing this kind of block structure where many successive layers have similar
representations and it looks like the representation is no longer getting refined through the network.
Once you start getting these blocks making the network deeper, making the network wider
no longer really gives you any improvement in accuracy.
So it seems like this is basically telling you that the network has fit the data as much
as it can and there's no real advantage to using something bigger.
Next we move on to Simon's paper about using different loss functions on image classifiers
and he made some really interesting findings actually so the loss functions only really
seem to affect the penultimate layers in the neural network.
This also gives us some pretty useful insight into transfer learning.
The last third of the network is setting up the penultimate layer representation in a
way that is good for your loss function but the first two thirds of the network are somehow
just learning general features.
I think this also corresponds with the success of transfer learning where we can take features
that we've learned on one task and transfer them to some other task.
What's the implication though?
It seems, is the implication that the loss function is not having any impact on the representations
early on in the network?
That seems like quite a big implication.
Ultimately we're asking the network to do the same thing just in a slightly different
way.
Like some inverse correlation between the gains you get from a loss function and how
good it is for transfer learning.
If you use loss functions that give you higher accuracy on ImageNet, you tend to learn representations
that transfer substantially worse in that setting.
The loss functions that perform better lead classes to become more separated in the penultimate
layers.
To standard softmax loss, actually the classes are not that separated from each other in
the penultimate layer representation.
Right now on whatever TensorFlow Hub or Hugging Face repositories and so on, we have these
pre-trend models and the pre-trend models, they're like full stack models and people
usually take some sort of last or next to last hidden layer but maybe we should much
more focus on actually providing like half of a network to share, like determining which
are actually the best, good or general representations from a data set and so on.
It's a really interesting question.
If we just want to turn an image into a vector that we could then train a linear classifier
on top of, what is the best way of doing that?
Self-supervised pre-training, just like word vectors, gives us a really great starting
point to vectorize an image into a semantically relevant geometric space.
It's been a real game changer in the computer vision world since about 2018.
We want that neural network to learn a representation such that when we then just train a linear
classifier on top of that representation to classify image net, it's going to do well.
But we want to learn the initial representation without using any kind of labels.
So what is self-supervised pre-training for vision?
People came up with these kinds of tasks that you could try to train a neural network to
do so that it would learn some kind of good representation.
You're trying to learn some kind of representation space where you've got different patches from
an image or different augmentations from an image, just different representations of the
same image and you want to learn a representation space where these representations of the same
image are all close together in that representation space and they're far apart from the representations
of other images.
This surprisingly seems to lead to very good representations.
I was very fascinated by all of these different tricks that you apparently have to get and
so big kudos to figuring all of this out for the rest of us.
Data augmentation is absolutely key to making this work.
The important part of the recipe is data augmentation.
There are really only two super important data augmentations that we need.
So we have to take two different crops from the same image and then we have to do some
kind of color distortion.
Turns out though the architecture isn't that important.
You don't have to worry about architecture, engineering specifically or contrastive learning.
What was new in the CIM CLR paper?
We introduced the idea of this projection head in CIM Clear and we also spend a lot
of time studying the augmentation.
And what about the bring your own latent paper?
I don't really have any insight into how either BYOL or the more recent papers actually
are learning a representation that doesn't end up collapsing.
Why it doesn't happen relates to some mysteries about neural network training dynamics that
we still don't entirely understand.
We dive deep into data augmentation in general.
The data augmentation that you need for contrastive learning is different from the data augmentation
that you need for supervised learning because the task is different.
When you have contrastive learning, you have this problem that if there's just one feature
in your data that can be used to do the contrastive task to get images of the same example or
views of the same example close together and far apart from views of all the other examples.
If you could do that with one feature, that would be the only feature the network would
ever learn or it might be the only feature the network would ever learn.
And so with the augmentation, you're making the task harder so that the network actually
has to learn many different kinds of features.
We find that this color distortion actually is very important for self-supervised learning,
for contrastive learning, or as it doesn't really matter for supervised learning.
There seems to be this fascinating universality of representations, especially in vision.
I'm not trying to be flippant when I say this because practitioners have used ImageNet
on a variety of downstream tasks.
For example, they might use it for classifying circuit boards or something.
And the miraculous thing is it just seems to work quite well.
So do you think in your opinion that there is some kind of universality?
I'm very skeptical about universality of ImageNet for different tasks.
Even though there are lots of cars in ImageNet, if you pre-train on ImageNet and you fine
tune on that data set, you will learn to classify it faster and fewer steps than if you had
trained from scratch on the Stanford cars data set.
But you won't actually perform any better at the end.
Representations of images are not that universal.
And at least what works for natural images like those in ImageNet may not work on other
data sets.
Taking a bit from the universality of representations to the universality of augmentations, since
this is such a crucial part, do you think that there is a systematic way how we can
discover augmentations?
Right now, it seems to be kind of a whack-a-mole, right?
It's okay.
We just feed images and say, no, that's too easy.
We crop them.
Oh, no, it's the color histogram.
So we like whack on the color and then it works better.
Maybe someone finds out, oh, there is still this easy feature that the network, every
noun, then pays attention to, so we design a method to whack on that a bit.
Do you think there is a systematic way or will this kind of philosophically always rely
on us humans having a higher level inside of what we want to do with the data set?
So what comes next after data augmentation?
So would the next step be some simulation?
Do you know what I mean?
Where we impute physics and we impute some world knowledge and then, I don't know, whether
we train a machine learning model from that?
Yeah, I think there are definitely shortcomings in our current machine learning models, understandings
of the world.
There are probably things that we can't just solve by throwing more static images at them.
I think maybe the next step, rather than trying to immediately situate the machine
learning model in a simulated world, we could just think about video.
I think probably representation learning from video is going to be a big thing next year
or the year after, something sometime in the near future.
Finally, we talk about Simon's paper.
Big self-supervised models are strong semi-supervised learners.
What is a practical problem is the situation where you have a lot of unlabeled data and
then a very small amount of labeled data.
What I find fascinating is how many ideas come together in this paper.
You probably didn't sit down after a SimClear one and be like, all right, what do we do
for SimClear two?
Okay, let's do this.
So it tells me there was this process.
Could you, if you can, maybe elaborate a bit on how did you going to build up the system
towards the final output?
We also tried the approach of first fine-tuning the big network and then distilling it.
It turned out that worked a lot better.
What we found was this approach of pre-training, then fine-tuning, then distilling works a
lot better than pre-training, then distilling, then fine-tuning.
We probably shouldn't expect distillation of the kind that we do in SimClear v2 to work
substantially better than supervised distillation, which has been around for quite a while now.
I think what's impressive is that in the self-supervised case, in the contrastive case, distillation
basically allows you to recover the same accuracy that you would get from training supervised
from scratch, whereas without it, the accuracy is a lot worse.
So it seems like it maybe matters more in this contrastive case.
But I think generally when you do distillation in the supervised case, you can get maybe
a percentage point gain, maybe a couple of percentage points.
And I think that's probably about the limit in terms of the improvement that you could
get from any kind of distillation-based approach over supervised training from scratch.
Can you use GANs, Generative Adversarial Neural Networks, to do data augmentation?
Or is that just a myth?
Simon certainly seems to think so.
Using a GAN to do data augmentation, you have this problem that you still don't actually
have more data.
You have a GAN that's trained on the same data.
And so it might help you because your way of encoding inductive bias into the GAN is
different from your way of encoding inductive bias into the neural network.
And maybe by having more inductive bias, you can learn a better function.
You still don't have more data, and it seems like without having more data, there's no
reason to expect a priority that you will be able to learn a better function.
Ironically, when you do the simple data augmentation, you do have more data because you put all
the knowledge in there as a human of what makes two images dissimilar visually, but
still equivalent semantically, which, again, is exactly the opposite.
It gives you images that are visually similar, but it has no intuition of what the semantic
similarity is.
We round off the show by talking about Simon's love of the Julia language.
Julia is a much better programming language than Python in many ways.
Julia is designed for these situations where maybe beyond just matrices, you have these
funny types of structured matrices, you have sparse matrices, and you can define special
methods for the product of a sparse matrix in a vector, or all sorts of things where
you might want different methods depending on the types.
I really hope you've enjoyed the show today.
We've had so much fun making it.
Remember to like, comment, and subscribe.
We love reading your comments, every single one of them, and we'll see you back next
week.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my
two compadre, Syac, the neural network pruner, Paul, and Yannick, the Lightspeed protein
folder, Kiltcher.
Today we have an incredibly special guest, Simon Cornblith, and Simon got his PhD in
brain and cognitive sciences from MIT.
His undergrad was from Caltech, and he's a research scientist at Google Brain.
He's been there since about 2017.
He's been cited nearly 2,000 times, which for someone quite early in career is seriously
impressive.
He's got a keen interest in the digital humanities, in philosophy, computer science, machine learning,
computer vision, and neuroscience.
He used to be a neuroscientist before we started doing machine learning, and he tells us that
he's got some very strong opinions about neuroscience and machine learning, which we certainly will
be getting on to later.
He's a huge lover of the Julia language, so if you Google Simon's name, you'll see him
talking at about a million Julia conferences, so definitely check that out as well.
Simon pioneered the use of centered kernel alignment as a way of analyzing the evolution
of representations in layers, in network, and between networks of different architectures.
Now Simon, like me, is a lover of similarity matrices, and what can be gleaned from them?
On my own PhD, I worked with them a lot for music segmentation, and also for detecting
regimes in financial data sets.
When a block in a ResNet is no longer self-similar to previous layers early on, you might intuit
that it's moving into a new representational regime, or maybe it's just started hallucinating.
All of this stuff was covered in his paper, Do Wide and Deep Neural Networks Learn the
Same Things, and I find it fascinating that representation of self-similarity can reveal
network pathology.
Now in his paper, What's in a Loss Function for Image Classification, he noted that different
losses and regularizers have similar accuracies on several data sets, but using the same representational
evolution analysis, Simon gleaned that these losses and regularizers only affected the
penultimate layers in the neural network, revealing inherent limitations in what can
be achieved in manipulating the loss on a network.
Now next in the session today, we're going to talk about the Simclear paper, and this
was an incredibly exciting paper for unsupervised contrastive image learning with augmentations.
It introduced a learnable nonlinear transformation between the representations and the contrastive
loss, which massively improved the representations.
The composition of augmentations is super important, and whenever anyone asks me about
what are the different data augmentations in computer vision, I always point them to
the SimCLR paper because it's got this wonderful matrix, and in that matrix it was shown that
the crop and the color I think were the most effective augmentations, but Simon also noted
that the batch sizes were super important, and the paper improved over the state of the
art on the ImageNet top one, and actually matched unsupervised methods for the first
time, albeit with many more parameters.
But the final paper we're going to talk about today is Big Self-Supervised Models, a strong
semi-supervised learners, and this is where you can learn from fewer labeled examples while
making use of a large amount of unlabeled data, and with unsupervised pre-training
on SimCLR v2, supervised fine-tuning on a few labeled examples, and then distillation
with unlabeled examples, this approach improved the label efficiency over previous state-of-the-art
methods.
I remember Yannick Lightspeed Kilcher made a video on this one, which I watched a few
months ago, so Yannick will have all of that completely fresh in his mind.
Anyway, Simon, it's an absolute pleasure to welcome you to the show.
Thank you so much for coming.
It's great to be here.
Amazing.
How did you get into machine learning?
So I guess first I got into neuroscience, and then I got disillusioned with neuroscience.
When I was pretty young, I was interested in consciousness and how we create this kind
of impression of the external world inside our heads.
And so I guess it's pretty obvious how that translates into an interest in brains and
how the brain works.
So I spent both four years as an undergraduate doing neuroscience research, and then seven
years working with monkeys at MIT trying to figure out how monkey brains work.
And then after that, I felt like we weren't getting very far by trying to record from
neurons in monkeys' brains and figure out how those neurons work.
So I thought about what other ways are there approaching this problem?
How could we think about how to understand how the brain is doing tasks?
And it seemed like maybe by building systems that can do those tasks well that are not
biological, we could learn more.
So that's how I got into machine learning.
I joined the Google AI residency program, which is like this great program that Google
has to take people who have extensive background in some field that is not machine learning
and train them to do machine learning.
And I ended up at Google, and initially I thought I'm going to spend a year here learning
about machine learning related stuff, and then maybe I'll go back to neuroscience and
I'll decide the tools for machine learning could be applied back to brains, and maybe
we can learn more about brains by applying the tools of machine learning there.
But ultimately I decided I was more interested in just looking at how the neural networks
work and also in the engineering challenges of building better neural networks, which
I actually think are fun.
One of the thoughts that came to my mind is it's fascinating looking at the kind of introspective
analysis that you've been conducting with neural networks, but could you contrast that
with neuroscience?
Because as I understand, you have MRI scans and you have different ways of trying to visualize
and reason about the behavior of a brain, but you can't really tweak the architecture
and tweak all of the knobs and the levers in quite the same way you do in machine learning.
Yeah, so like in neuroscience people also use this analysis across different individuals
or different organisms or whatever.
It is a tool that people use in neuroscience as well, but I guess they're limited in the
ways in which they could manipulate the systems that are providing these representations.
So in neuroscience, you're always constrained by data, so you can compare representations
of images across individuals by doing MRI scans.
But first of all, you might not get a very good idea of how the brain is representing
those images because there's a lot of noise in the MRI scan and there's a limit to how
long you can scan each person, whereas I guess in a neural network, noise is not a problem.
The entire system's deterministic, we just pass in the image and we get the representation
vector.
And you also have these kinds of limits of, like, we can't see what happens if people
have bigger brains, like we can't manipulate the architecture in those kinds of ways.
So even though we can look at how intact brains are working, we can't see how representations
change when we manipulate them all that easily.
And I guess, again, in machine learning, like we can do all of those things.
We can look at what happens when we change the loss function.
We can look at what happens when we make the network deeper or wider.
So I think there are, like, some really cool ways that even the same techniques can be
applied in machine learning that they couldn't be applied in neuroscience.
I felt like we weren't getting very far by trying to record from neurons in monkey's
brains and figure out how those neurons work.
Like it didn't really seem like a very effective way of figuring out how the brain constructs
this kind of internal representation of the world.
So from there, I thought about what could we actually do to understand this?
And it seemed like the most promising thing to do was to look at what happens in simpler
systems that we can construct ourselves and where we can analyze the behavior of everything
inside the system.
So in a neural network, we can record all the neurons, which is extremely challenging
in a biological organism.
And we can also manipulate the system in any kind of way that we can imagine.
But it still seems like really hard to understand neural networks.
So it seemed like maybe this was a more tractable challenge and a challenge where maybe we could
make some headway that might eventually translate back to brains.
And so that's how I ended up in machine learning.
I guess there are, like, other great things about machine learning.
I guess the pay is much better than in, like, academic neuroscience.
But really, I think, like, it's a logical progression based on the ideas that I was
interested in.
And I am still interested in the same sorts of ideas.
Do you still think now that you're in machine learning and have made some progress here that
there is a good chance that we're going to map our knowledge that we gain back to the brain?
Or do you think there is a bit of a disconnect?
I think that's a really good question.
I think there is definitely some knowledge that we're going to get from machine learning
that will map back to the brain.
I think, like, in terms of general principles and ways of looking at how, like, information
processing systems work, I think there are a lot of ideas from machine learning that
will ultimately help us understand brains.
I'm a little less sure whether we're going to build, like, a machine learning system
that is a brain.
I think there's a disconnect between the way that the systems that we build work and
the way that biology works.
And I think that's insurmountable just because there's differences between what you can
build efficiently with cells and what you can build efficiently in silicon.
But in terms of approaches to understanding, in terms of building tools to understand things,
the tools that we build in machine learning, I think will eventually be useful in neuroscience.
So people make a lot of analogies and they make a lot of claims about neuroscience in
connections with neural networks.
Is there a statement or a bunch of statements that you hear over and over again where you
just cringe because they're so wrong?
Is that something that happens to you?
I can imagine it would.
Yeah.
Yeah.
So I think there's this, like, kind of basic fact that neural networks are inspired by
brains, which is true.
Then there's all this other stuff where people try to relate whatever neural network they've
built back to a brain and they say that it works like the brain, but it doesn't work
like the brain.
There's still this huge kind of disconnect in how the system is actually operating.
The brain is not literally doing back prop.
It might be doing something that's like back prop.
We still don't really know, but it's not literally computing gradients by automatic differentiation.
And I'm fascinated to talk about this line of reasoning that you have because you're
clearly the kind of guy that you want to reason about the behavior of models and in particular
the evolution of representations.
And I watched one of your presentations on YouTube where you were talking about how you
can compare the representations by comparing features.
And of course, the naive way of doing is the dot product or some variations of that.
Turns out that doesn't work very well.
And you came up with this wonderful metric called the centered kernel alignment.
So how did that all come about?
The way we came up with that idea was that Jeff Hinton had another idea and I tried the
idea and it worked, but then we wondered, is there a simpler thing that worked?
And that's how we ended up with centered kernel alignment.
I guess the problem that we had in trying to come up with a way of comparing similarity
of neural network representations is that it's really hard to know what is a good way.
Like it's not something where you can really develop a good benchmark.
So like in the paper, we came up with this simple sanity check where the idea is basically
we've got two architecturally identical neural networks and we just train them from different
random initializations.
And so we want it to be the case that if you measure like the similarity between a layer
from network A and all the layers from network B, that the most similar layer in network
B is going to be the architecturally corresponding layer.
So if we have layer two from network A, it should be more similar to layer two from network
B than layer three or layer four.
And so like basically we found that what people had been doing before didn't always pass that
sanity check.
And we basically tried to come up with the simplest way of building a similarity index
that did actually pass that sanity check.
And that's how we ended up with centered kernel alignment.
Yeah, because I think you showed that the canonical correlation analysis only worked
about, I think at an accuracy of about 1.4%.
So it's complete apples and oranges.
But this absolutely fascinates me though, because when you plot this thing in this kind
of self similarity matrix, you can glean so much about the evolution as a function of
time.
And because you talk about this in one of your other papers as well, that there's this
characteristic blockiness.
And when you see blockiness, that successive layers are similar to versions of themselves
in the past.
And that kind of means that they're not evolving anymore.
And you then made the intuition in your paper that, well, essentially it's redundant information.
If it's not learning anything new, I can just delete that block.
I can just delete those layers from the neural network and it won't make any difference.
And indeed it didn't.
Yeah.
Could you, for people listening, explain the similarity measure you came up with in principle,
just so we can imagine something, how that should even work?
Yeah.
So I guess the idea is you've got a neural network and you feed some set of examples,
like multiple examples through the neural network.
And now you've got some matrix where the rows of the matrix are different examples and the
columns are different neurons.
So yeah, you can imagine this as if you have vectors of activations for each example, you've
stacked them real wise.
So now what do we do with that to compare two neural networks trained from different
random initializations?
The problem is if we were to just take the square difference between those matrices,
we have this problem that the neurons between these two different networks aren't necessarily
aligned in any way if they're trained from different random initializations.
Even if we had exactly the same neurons, we shouldn't expect that neuron one would be
the same, representing the same thing in both networks.
So we need some way to get around that problem.
One way around this problem is instead of comparing these original matrices, we're going
to make matrices that measure the similarity of each example to each other example for
one particular network.
So if we've got example A and example B, we can measure their similarity very simply
just by taking the dot product between those two vectors.
And now because we're measuring similarity from the same network, we don't have to worry
about this alignment problem.
And we get some idea of how similar different examples are to each other according to the
representation in network A. So if we do that for all the examples, we get some examples
by examples matrix.
And then we can do that both for our first network and for our second network.
So after we've done that, we've got these two examples by examples matrices.
And then the easy way to compare those matrices is we just reshape them to vectors and we
take the dot product again between those vectors.
So now we've measured the similarities between the similarities of the examples.
And this doesn't have this problem of aligning the neurons because instead of measuring similarities
of neurons, we're measuring similarities of examples and then we're comparing those
similarities.
So ultimately, we do that, we take that dot product and then we normalize it in a way
that makes it invariant to scaling.
So if you just took the dot product, you'd have this problem that scaling all of the
features by some number, if you scale everything by a factor of two, the dot product will go
up by a factor of two.
And so we just apply some normalization so that kind of scaling will not affect the similarity
index and we get centered kernel alignment, which gives us a similarity score between
zero and one.
The fascinating thing is that you can replace that dot product with a kernel because it's
a gram matrix.
Yeah.
So did you find that it made a difference if you use, let's say, the RBF kernel?
Yeah.
Yeah.
So, yeah, basically when we're measuring the similarities between examples, we can just
instead of taking the dot product between the representations of the different examples,
we can take the kernel between one example and another example because the kernel is
also a way of measuring similarity.
And so we tried that.
It turns out that like for CNNs, it didn't really make a difference.
Like the RBF kernel worked, but sort of just taking a regular dot product.
But we did find in the appendix of that paper that if you instead use an RBF kernel with
a transformer, it actually does work better than taking a normal dot product.
And I think like part of what's going on is that sometimes you want it to be the case
that when you're measuring similarity, you care more about the distances between the
examples that you're close to than the distances to the examples that you're far away from.
Like once you're really far away from something, maybe it doesn't matter so much if you're
10 times as far away because like you're already so far, you're already not going to...
You don't really care how far away something is once you're far enough.
And the RBF kernel takes that into account in a way that a linear dot product wouldn't.
The linear dot product is like very sensitive to the global distances in the space.
What I find fascinating is that you can glean so much from the blockiness, right?
So you are saying that as it becomes blockier, it might be an indication that it's become
saturated in some sense.
And I'm also interested in a way, we already know that the representations in neural networks
are increasingly abstract, so they don't necessarily bear any resemblance to the beginning.
So when we're looking at the cell similarity matrix, we don't necessarily want the representations
on the final penultimate layers to be similar to the ones at the beginning.
We want there to be a continuous evolution.
We don't want to have a stalled evolution because that would correspond to this blockiness.
But is it when you've stalled for a long time, is that when it becomes pathological?
Because we want it to evolve in stops and starts, don't we?
Yeah.
I think it's not clear what we should really expect in terms of how a neural network representation
evolves through the layers.
I think there's kind of some theory on what we should expect if all the layers are linear.
But obviously, the neural networks that we train are nonlinear, and it's really important
to have a nonlinearity in between the layers.
And so at that point, it's really hard to reason about what the optimal thing for a
neural network to do actually is.
I think it's something that we can really only study empirically.
On the other hand, I do think if we see that nothing is changing from one layer to the
next, that's a really bad sign.
If the neural network representation isn't changing, then obviously nothing's happening.
But I guess it's unclear whether we should expect abrupt shifts or we want things to
happen slowly between the layers.
I'm not sure whether we really have the theoretical knowledge to say what is best.
Yeah.
I'd love to see this as a kind of tool in our toolbox that we could use on different network
architectures.
But you said that the other learn features are shared across different initializations
and architectures, particularly across the depths of the network.
So it almost seems as if this blockiness is separate to your work in wide and deep neural
networks because you showed that the width and the depth have got different effects on
network predictions at the example level or at the class level.
But the blockiness almost seems to be an orthogonal thing.
That's just when you have this kind of saturation of the network, you see the blockiness.
Yeah.
Yeah.
So initially we had hoped that we could look at other similarities between wide networks
and deep networks in their representations.
But like when we did those experiments, we actually just found that if you make the network
really wide, you get this kind of blockiness in the representations and those blocks are
like dissimilar across different initializations.
And then the same thing happens if you make the network really deep.
We see these like big blocks in the representations.
So that made it hard to study these very wide and very deep networks from the representational
similarity perspective.
But at the same time, I think it's like a really interesting observation.
Like it's something where we couldn't have predicted this ahead of time based on what
we know about neural network theory and we couldn't have predicted it ahead of time based
on the accuracy of the network.
It's something where we really needed like these techniques for looking at the internal
representations of neural networks to see what was happening inside of them.
There's this whole literature that takes a look at a network's expressibility with
regards to its depth and width.
So could you just explain it to us whether or not we should be able to meaningfully quantify
or formulate the expressibility of a neural network with regards to your analysis made
on that?
Yeah.
So there's this work that looks at like kind of the functions that can be expressed by
wide networks and the functions that can be expressed by deep networks.
And I guess like the neural networks seem to become exponentially more expressive as
you make them deeper.
So it seems like in that sense depth is more important than width.
But on the other hand, like the neural networks that we actually train in this paper, like
both the wide networks and the deep networks are big enough that they can overfit the entire
training set.
So in this case, like the expressibility of the network is not really important.
Important is the function that the network actually ends up learning.
So I guess even though networks could express more functions when they're deep, what we're
really studying is the function that you actually get when you train the neural network by gradient
descent on some data, what the optimization process actually finds.
One thing as well in that paper, you talked about the network pathology, right?
You said that two times depth accuracy 95%, four times 93.2, eight times 91.9.
So because this is this runs counter to what a lot of us would intuit.
We think that you can have as much depth as you want.
And architectures like ResNet in some sense, they learn their own capacity.
There is a pathology there happening clearly.
And how would you determine that from this visualization?
Yeah, so I guess there are two kind of results.
So like you can either look at networks without residual connections, where you do actually
find that at some depth, the accuracy will start going down.
And in networks without residual connections, we find that like that the depth where accuracy
starts to go down is like around the same depth where you begin seeing this kind of block
structure where many successive layers have similar representations.
And it looks like the representation is no longer getting refined through the network.
Yeah, I mean, with ResNets, you can make them much deeper.
And it seems like it doesn't hurt accuracy as much even once you start getting these blocks.
But it also seems once you start getting these blocks, making the network deeper, making
that work wider, no longer really gives you any improvement in accuracy.
So it seems like this is basically telling you that the network has fit the data as much as it can.
And and there's no real advantage to using something bigger.
Fascinating. Let's move on to another paper that you've done, which is quite related in
terms of you've used the same analysis to reason about it.
But you had a paper called What's in a Loss Function for Image Classification.
And you looked at a whole bunch of different label smoothing and regularizers, which are
things that you do on the end of the network.
And you identified differences in accuracy and calibration and out of domain distribution.
And you made some really interesting observations.
So by the way, we're talking about things like do we use the softmax or the squared
area or dropout or label smoothing or logic penalty.
But you noticed using the same analysis technique that only affected the
representations on the penultimate layers of the neural network.
What's going on there?
Yeah, so it's not just the penultimate layer.
It's like the the last maybe third of the network is affected by the loss function.
But then the first two thirds of the network, it seems like you learn the same representation
no matter what loss function you use.
So it doesn't change if you use label smoothing.
It doesn't even change if you use mean squared error instead of using softmax cross entropy.
You still basically learn the same representation for the first two thirds of the network.
And I think it's still it's a bit of a puzzle to us why this happens.
Clearly, it matters that you're training the network with the loss function.
There's those layers in the first two thirds of the network do change from the initialization.
But I guess it seems that the last third of the network is setting up the penultimate
layer representation in a way that is good for your loss function.
But the first two thirds of the network are somehow just learning general features.
I think this also like corresponds with the success of transfer learning,
where we can take features that we've learned on one task and transfer them to some other task.
What's the implication that it seems is the implication that the loss function
is not having any impact on the representations early on in the network?
That seems like quite a big implication.
Yeah, I think the loss function must have some impact because if you don't train
the network, if you don't have any loss function at all, then the representation
in that first two thirds of the network is actually quite different.
I think what's really happening is there are these differences among the loss functions,
which don't really matter except later in the network.
Although they will give you a slight change in accuracy and slight changes in robustness,
they don't matter for this general feature learning process.
I guess maybe it's what we should expect because ultimately we're asking the network
to do the same thing just in a slightly different way.
We're still asking the network to classify images.
We're just asking it to provide slightly different outputs to produce a slightly
different representation at the penultimate layer.
Maybe we should expect that those earlier features that are just trying to
represent general things about images, those will be the same no matter what loss function we pick.
In your experiments, did you find whether or not model capacity has anything to do with it?
Yeah, so we didn't really investigate different model capacities in that paper.
I would expect that the same thing holds for a wide range of model capacities.
There's no indication from the experiments that if you use a bigger network or a slightly
smaller network that things would change all that much.
Yeah, I think it's still an open question how model capacity changes things.
I guess in the Sinclair paper, we found that model capacity can matter quite a bit.
Yeah, so the general hypothesis there should also hold,
even though your model is bigger or smaller, no matter how big or smaller your network is,
the general feature learning regime or paradigm should still hold no matter what
loss function you would end up using.
Yeah, that would be my guess. I think if you're in a regime where it's really hard for you to fit
the training data, if you have a very small network, it might be the case that you see more
differences in the earlier layers because it might be that the loss function really affects
what features are best there in a way that it wouldn't if the network is a bit bigger and if
it's more capable of fitting your training data. But I don't really know. I think this
is something that is probably worth looking at in some follow-up work.
And you found also there's implications for transfer learning with respect to the loss
function. There seems to be some inverse correlation between the gains you get from a loss
function and how good it is for transfer learning. Or is there a connection between
loss functions and regularizers and all of that?
Yeah, we look at just linear transfer in the paper. So if we take the features from the
penultimate layer and we try to use them directly for some other task, how good are those features
going to be? And what we found was that if you use loss functions that give you higher accuracy
on ImageNet, you tend to learn representations that transfer substantially worse in that setting.
And our intuition is you could learn many different kinds of representations in that
penultimate layer and still do a reasonable job of classifying ImageNet. But what seems to happen
is that the loss functions that perform better lead classes to become more separated in the
penultimate layer. So like class A and class B will be farther apart relative to the variability
within the class. And when you have a situation like that, you have this penultimate layer
representation that's specialized for the classes in ImageNet. Like you've got a thousand clusters
corresponding to the thousand classes in ImageNet. And so then if you want to use like those kinds
of representations for some other task, it will only really work well if you have exactly the
same classes that are in ImageNet because they're already organized by the ImageNet classes.
On the other hand, what we found is if you just use standard softmax loss,
actually the classes are not that separated from each other in the penultimate layer
representation. And because they're not that separated, there are these features that you
could use to classify things that are not ImageNet that still convey some kind of
useful information about the images that are not just their ImageNet class labels.
It hints at a bit of a future where, you know, like right now on whatever TensorFlow Hub or
HuggingFace repositories and so on, we have these pre-trend models. And the pre-trend models,
they're like full stack models and people usually take some sort of last or next to last hidden layer.
But maybe we should much more focus on actually providing like half of a network to share. Like
determining which are actually the best good or general representations from a data set and so on.
Do you have any of this in mind when you do work like this?
Yeah, at Google, what is generally best for us to do is just to fine-tune the whole network.
And if you fine-tune the whole network, it eliminates some of these issues with
the actual form of the representation in the penultimate layer. Because even if you have this
kind of highly specialized penultimate layer, when if you're allowed to change all the other
weights in the network, you can fix that and you can specialize the rest of the network
for some other task. But yeah, I think like it's a really interesting question. If we just want
to turn an image into a vector that we could then train a linear classifier on top of,
what is the best way of doing that? How should we approach that problem? And how should we approach
that problem if we want this very like general universal vector representation of an image that
would work well for a lot of different tasks? And I think we don't really have good ways of
doing that because basically this is all empirical, right? Like, we don't know what makes a good
universal representation of an image. We've just got to try a bunch of things and figure out what
works best. And I guess, yeah, the insight from this paper is like actually the loss function
that you use to train the network can make a huge difference there. Fascinating. I guess without
any further ado, we should move on to SIMCLEAR, a simple framework for contrastive learning of
visual representations. We've been absolutely fascinated by this concept of unsupervised
contrastive image representation learning algorithms. We've seen such a huge kind of step
forward, haven't we, over the last couple of years in this area? Yeah, it's pretty amazing to me.
Could you just go back to real basics? Imagine that people out there have been living in a
cave. They don't know what contrastive learning is. They don't know about image augmentation.
How would you frame the whole thing up? The self-supervised learning setup is we've got a
bunch of images and at least the initial like historical self-supervised learning setup is
we've got a bunch of images. We want to train some kind of neural network on it. And we want
that neural network to learn a representation such that when we then just train a linear
classifier on top of that representation to classify ImageNet, it's going to do well. But
we want to learn the initial representation without using any kind of labels. And yeah,
I guess there are a lot of different approaches that people tried for this problem. Like people
tried things like let's train a neural network so that we can cut up the image into just a grid
and shuffle the grid. And then the neural network has to figure out how to assemble these puzzle
pieces back into the original image. And maybe that'll give us a good representation. Or let's try
just rotating the images so we can have images that are rotated 90, 180, 270 degrees. And then
we'll have the neural network try to classify what rotation we fed into it. And so people came up
with these kinds of tasks that you could try to train a neural network to do so that it would learn
some kind of good representation. They were defined in this ad hoc way. Let's come up with some kind
of funny thing where you don't need a label. You can have the neural network trained to do
this kind of thing. And maybe it'll learn something about images. Starting in around 2018, there are
a few papers that basically suggested this alternative approach where you're trying to learn
some kind of representation space where you've got different patches from an image or different
augmentations from an image, just different representations of the same image. And you want
to learn a representation space where these representations of the same image are all close
together in that representation space. And they're far apart from the representations of other images.
This surprisingly seems to lead to very good representations. But it turns out there are a
lot of very important details to get this to work well. So it's really like a situation where the
basic idea is very simple. Let's create multiple views of an image and try to get them close to
each other and far away from everything else. But things like augmentation and things like the
exact way we set up the network end up being very important to learning a good representation with
this kind of technique. How does the negative sampling work? People have done this in different
ways. So in Simclear, our way of doing negative sampling is very simple. So basically,
we are attracting two views of the same image. And then we have a mini batch that has 4,096
images in it and two augmentations of each image. And so we are repelling using a softmax from all
of the other 8,190 views in that mini batch. Basically, we want our two augmentations of
the same image close and we want them to be far from the other 8,190 images.
Yeah, it's a bit of a throwback to work to VEC. I think it's pretty cool how these ideas just come
up through the eras and through the different models and so on. And there is seemingly always
another layer on top of these ideas. Pretty cool. Yeah. So if you only consider, you know,
two views that are coming out of the same image as the positive pair, so to speak, and all the other
views are coming out of the different images located in the same mini batch, wouldn't this hurt
the representation space to some extent? Let's say you have multiple images of dogs in a mini batch
of 4,096 samples. We would essentially want the representations of different dogs to map together
as closer as possible in the representation space while representations of cats from dogs would
get further away. Wouldn't we expect this? But how does Simclear ensure this rigorously? I guess
it's because of the larger batch sizes you use, but I still wanted to know from you.
Yeah, one thing is, even if we've got other kinds of images that we want to be close together in the
mini batch, even if we've got like a dog image and then another dog image and ultimately we want to
learn a representation space where maybe they're not so far apart, like on average, most of the
images in the mini batch are things that we want to be really far apart from. So maybe it doesn't
hurt that much if we're repelling from everything as opposed to just repelling from images that are
part of other classes. I think this actually is something that hurts current self-supervised
learning techniques and hurts contrastive techniques because we also know when you do the
contrastive loss, if you don't contrast against examples that are very close to you, that actually
improves things a little bit. So if you don't contrast against the very hard negatives,
we've found that gives you slightly higher accuracy when you do this linear evaluation.
That kind of suggests that this really is a problem with these techniques that maybe sometimes you
don't want to be as far apart from other images as the losses encouraging you to be. Now there's
one other aspect which is that in Simclear, we don't actually use the representation that's
feeding into the loss function. Like we have this projection head, an MLP on top of the network,
and instead of using that representation at the end of the network, we use a representation
that's two layers back. And so by using a representation that's two layers back, even if
in the final layer we're pushing things apart, we kind of figure that this earlier representation
might not have pushed apart the things that really are semantically similar. And indeed,
we find that using this earlier representation in the network leads to higher linear evaluation
accuracy. So it works better. I was very fascinated by all of these different tricks that you apparently
have to get. And so big kudos to figuring all of this out for the rest of us. There has been a
lot of follow-up work on this idea. A lot of modifications. There is this bootstrap your own
latent where they completely leave out the negative sampling. Then I think just like one or two weeks
ago, there was a paper saying if you build in a stop gradient into the contrastive loss,
you also apparently don't need a negative and so on. Do you have maybe from your own work or
from work of others, do you have any sort of current? If I were to build a self-supervised
contrastive representation learner today, what is the top things I should do? What is my recipe?
How do I go about it? The most important part of the recipe is data augmentation. So we're
going to use two views from the same image and it's very important how those two views are
constructed. But they're really only two super important data augmentations that we need. So
we have to take two different crops from the same image and then we have to do some kind of color
distortion. So in SimClear we use very aggressive color distortion. So that is probably the most
important part of the recipe. Then I guess we feed that representation into a neural network
and fortunately we found that you can just use a regular ResNet 50 for this part. You don't have
to worry about architecture, engineering, specifically for contrastive learning. Then
I think all of the work since SimClear also uses this idea of putting an MLP on top of the end of
the network and then using that to get whatever representation goes into the loss function,
but then discarding part of the MLP when we later just want the representation for a downstream
task. All of those pieces are pieces that are shared by all of these modern self-supervised
learning techniques. So like we introduced the idea of this projection head in SimClear and we
also spend a lot of time studying the augmentation although we were not the first people to
come up with the idea that the augmentation was important. Yeah, in terms of what the loss function
is, I guess it's surprising that there are so many things that work that we use this contrastive
loss in SimClear because it was what previous work had done and it's like intuitive that you might
want to learn a space where you're explicitly pushing away representations of other examples,
but I guess like in BYOL they aren't explicitly contrasting against representations of other
examples. So instead they have a network where they're taking a moving average of
the weights that they've been learning and they try to match the representation that's coming
out of the network that they're training to this representation of this moving average network
and somehow magically that works and I guess it doesn't even have to be a moving average. I think
you were referring to earlier like you can just match the representation of one network to
stop gradient of the same network as long as you're matching the representation in an earlier
layer and I think like it's still like mysterious why that should work. I don't really have any
insight into how either BYOL or the more recent papers actually are learning a representation
that doesn't end up collapsing. The problem is if you're trying to match some earlier representation
you could just collapse to the point where all of your representations are the same and then
like you would trivially be matching the earlier representation, but this doesn't happen and I
think why it doesn't happen relates to some mysteries about neural network training dynamics
that we still don't entirely understand. I'm absolutely fascinated by this concept of data
augmentation. Early on in my neural network career I just imagined it as being a way of
increasing the size of your training set, but in a sense you're not really adding new information.
You are creating semantically equivalent noise perturbations or examples similar to how BERT
works the NLP model it's like a denoising autoencoder and you're creating noise diversions of the
same thing and pushing the examples off the manifold. So there seems to be a dichotomy between
on the one hand augmenting your data and it's almost like you're stopping the neural network
from overfitting on things like the color or some specific feature you don't want to. You want to
have a bit of generalization, but at the same time you are saying those things over there it's
definitely nothing like that. The data augmentation that you need for contrastive learning is
different from the data augmentation that you need for supervised learning because the task is
different. When you have contrastive learning you have this problem that if there's just one
feature in your data that can be used to do the contrastive task to get images of the same example
or views of the same example close together and far apart from views of all the other examples.
If you could do that with one feature that would be the only feature the network would
ever learn or it might be the only feature the network would ever learn. And so with the augmentation
you're making the task harder so that the network actually has to learn
many different kinds of features. So I guess we find that this color distortion
actually is very important for self-supervised learning, for contrastive learning,
whereas it doesn't really matter for supervised learning. And what we think is going on is that
if you have two crops from the same image, generally their color histograms are surprisingly
similar. If you just plot out the intensity histogram of the image you can see that the
crops came from the same image. And that's a trick that the network is very good at doing
because I guess if you have ReLU activations they're very good at computing histograms.
And so by doing the color distortion we basically we don't let the network just learn
the color histograms in order to do the contrastive task. We force the network
to actually use other sorts of information and that ends up being like critical to the
performance of these contrastive methods. Like it basically doesn't work unless you do
that kind of aggressive color distortion. Because that seems to be the key thing then. So you're
not telling it to learn things, you're telling it not to learn things. We're telling it to learn
one thing. We're telling it to learn figure out which views came from the same image. But then,
yeah, we have to make sure that it learns to do that with a diverse set of features instead
of just doing it in one way. Because I guess it's like a task that's actually pretty easy to do if
you don't have this kind of aggressive augmentation. Yeah, I think in a way it helps the network to
also differentiate what actually what is the thing that differentiates two images. I think
it helps the network to learn, you know, pick up on that signal. To that end, I also wanted to ask
for a custom dataset, if I wanted to, you know, apply a sim clear, what pointers should I take
into consideration while designing my augmentation policy? I'm sure you have been asked about this
question quite a few times. But yeah, I think it's a good question. Like I think like we actually
still don't really know how generalizable these contrastive learning techniques are beyond image
net. Like we know they work super well on image net. But like image net is like a boring dataset to
apply contrastive learning to because we actually already have all the labels and we could just be
doing supervised learning. But I think starting with the crop and color augmentation is definitely a
good idea, at least like for datasets that have color, I guess if you don't have color, then maybe
think about distorting intensities instead of colors. But beyond that, I think it depends on the
specific task and what you really want the neural network to pick up out of the dataset.
I feel like there are probably some sorts of data where I wouldn't really expect
contrastive learning to work well. So for example, like if you try to do contrastive learning on a
dataset of medical images where you've just got healthy patients, and then you want to translate
that to like some sort of dataset of people with some kind of pathology, you might never pick up the
features that are important for detecting the pathology. But yeah, I think this question of how
do you design the augmentation, what augmentation works well for datasets that maybe aren't natural
images like these kinds of medical images or maybe like satellite images. That's an important
question that we haven't addressed yet. There seems to be this fascinating universality of
representations, especially in vision. This is exactly the kind of thing you can test with your
wonderful similarity matrix idea. I'm not trying to be flippant when I say this because
practitioners have used ImageNet on a variety of downstream tasks. For example, they might use it for
classifying circuit boards or something. And the miraculous thing is it just seems to work quite
well. So do you think in your opinion that there is some kind of universality? I'm very skeptical
about like universality of ImageNet for different tasks. Like in the past, we did some work where
we looked at how well ImageNet networks transfer to other tasks. And it seems like
there are actually some tasks which are just like datasets of natural images where pre-training an
ImageNet doesn't really help at all. And those datasets just seem to be too different from ImageNet.
They're things like this Stanford cars dataset where you have to like classify different cars
according to their make, model, and year. It turns out even though there are lots of cars in ImageNet,
if you pre-train on ImageNet and you fine-tune on that dataset, you will learn to classify it
faster in fewer steps than if you had trained from scratch on the Stanford cars dataset.
But you won't actually perform any better at the end. And that's true even though the
Stanford cars dataset has 10,000 images. So it's tiny compared to ImageNet. So I think like actually
representations of images are not that universal. And at least what works for natural images for
images like those in ImageNet may not work on other datasets. I think there's also like limited
evidence for transfer from ImageNet to medical datasets. It seems if you don't like work really
hard at tuning hyperparameters or if you don't train for long enough, you will get better accuracy
by starting with an ImageNet pre-trained network. But if you do very thorough experiments and you
train for long enough, you try different learning rates and weight decay parameters, like actually
it seems like training from scratch on most medical datasets will give you the same accuracy as if
you started from a network that's pre-trained on some other giant dataset. Maybe this makes sense
because if you think about radiologists, like it's not like a radiologist can just like at the
beginning of their education, they can't just look at an MRI or an X-ray image and say this is
where the tumor is. It's something that takes them years of training to learn how to do. And so maybe
it also makes sense that like our neural networks can't just immediately easily without lots of
training pick up on very different image distributions. It does seem to make sense,
going a bit from the universality of representations to the universality of
augmentation, since this is such a crucial part. Do you think that there is a systematic way how
we can discover augmentations? Because it seems right now, it seems to be kind of a whack-a-mole,
right? It's okay, we just feed images and it's no, that's too easy. We crop them. Oh no, it's the
color histogram. So we like whack on the color and then it works better. But maybe someone finds out
oh, there is still this easy feature that the network every now and then pays attention to. So
we design a method to whack on that a bit. Do you think there is a systematic way or will this
kind of philosophically always rely on us humans having a higher level inside of what we want to
do with the dataset? Yeah, so I think actually I'm hopeful that at least for natural images,
just like crops and color distortions are enough, because I guess like what we found is
you combine those two augmentations and if you do that, like that gets you most of the way to
supervised accuracy. So maybe we shouldn't expect huge gains from adding additional augmentations on
top of that, even though there are like in the Sinclair paper, we add like Gaussian blur, which
gives slight gains on top of that. I guess in the BYL paper, they add even more augmentations on top
of that. So you can get small gains, but it seems like the gains are much smaller once you've got
the crops and the color distortions there. In terms of systematic ways of discovering
what set of augmentations we should be using, I guess there's a paper that I saw where
they basically use linear evaluation on the rotation prediction task to see whether the
augmentations are good and they claim that actually works for evaluating the augmentations.
So maybe that's one way, I don't know. There are all sorts of ways of designing augmentations
for supervised learning that could conceivably be applied to the self-supervised learning setting,
to the contrastive setting. There are these meta-learning based approaches for learning
data augmentation. I'm not sure, those techniques tend to be pretty complicated. I'm not sure whether
it's actually easier to deal with those techniques than just like trying a bunch of things, but I
think that maybe it doesn't matter that much. Maybe like just, at least if you're dealing with natural
images, maybe crop and color distortion is enough. I guess if you think about other images,
I don't really have any idea. I guess it depends on what the images look like.
There are lots of things that you could be expressing as images like a spectrogram or
like some kind of chart or whatever, where you could be applying a neural network to it,
but the further you get from natural images, the less clear it is what kind of augmentations
you should be working with. It is a fascinating thought though, this universality of augmentations.
When you said cropping and color, that made me think it seems to be related to the inductive
priors in the CNN architecture that we use and also to things like its regularly sampled, gridded,
discrete image data, because we're speaking with Max Welling the other week and as he's
created lots of other interesting inductive priors for computer vision models. It does set
my mind racing a little bit because presumably there's a continuum. On the one hand, we don't do
any augmentation and we just learn from examples. In the middle, we do the augmentation and then
maybe in the future, because some people have said that computer vision systems don't have seen
understanding. They don't understand physics. The ball might be on the table, but we don't know that
it's not falling and so on. There's a lot of missing information. Would the next step be some
simulation? Do you know what I mean? Where we impute physics and we impute some world knowledge
and then I don't know whether we train a machine learning model from that?
Yeah, I think there are definitely shortcomings in our current machine learning models,
understandings of the world. There are probably things that we can't just solve by throwing more
static images at them. I think maybe the next step, rather than trying to immediately situate
the machine learning model in a simulated world, we could just think about video. I think there's
already a lot of additional information in video that a neural network could use to learn
interesting representations. It seems like if you just see static images, it's hard to learn
how to segment objects. It's hard to learn where the object's boundaries are, but once you have
video, it's like the stuff that's moving together is an object and you can tell that because it's
moving together. I think there's a lot of potential for learning better visual representations
and maybe eventually from these kinds of interactions in simulated environments. I
think ultimately it becomes a computational headache. Even video is a computational headache
because suddenly you've got all of these frames that you have to deal with. You probably want to be
thinking about how representations change over time and video data is just huge. It's especially
huge if you have to process many frames at once on your accelerators. I think that's why this hasn't
taken off yet, but I think probably representation learning from video is going to be a big thing
next year or the year after or sometime in the near future. We would love to talk about your big
self-supervised models, our strong semi-supervised learners. This is super interesting because
you're combining the unsupervised stuff that we've been talking about in Simclear, but now
we're in the semi-supervised domain where the label efficiency becomes super important. What's
the deal there? Yeah. I guess in Simclear we focus on this question of linear evaluation
accuracy. We're just learning a representation without any labels and then training a linear
classifier on top of that representation on the same data, but now with all the labels. It turns
out that's not really a very practical problem if you have all the labels. There's not necessarily any
reason in practice that you would want to first learn this representation and then train the
classifier versus just doing standard supervised end-to-end training. What is a practical problem
is the situation where you have a lot of unlabeled data and then a very small amount of labeled
data. That's the situation that we look at in Simclear v2 in that paper. What we find there is
that you can train this network fully unsupervised without using the labels on all the data and then
you can fine-tune it on just the subset where you've got the labels. If you do that, it's possible to
get very high accuracy, especially if the network is very big. Basically, we find if you have a
really big ResNet, if you have ResNet 152 and then you make the layers three times wider,
when you do that, you can get accuracy when you fine-tune on 10% of the labels that's substantially
better than if you trained ResNet 50 from scratch with all the labels. Once you have that really big
network, it turns out you don't have to put the really big network into production. You can take
the really big network and you can then distill it back into a standard ResNet 50 and you can retain
almost all of the accuracy when you do that. I guess what's important about this distillation
process is we're not just going to distill on the labeled dataset. We're going to also use
the labels that this giant network, which we fine-tuned on a small subset of the data,
we're going to use the labels that it gives on all of our unlabeled data. We're going to use it to
generate labels and then we're just going to use those labels to train a much smaller network. If
we do that, we get accuracy that's similar to or maybe even slightly better than standard supervised
training from scratch. This becomes a highly practically relatable approach toward doing
computer vision related things. We see all the times that folks have a huge corpus of unlabeled
images, but they only have maybe 5% or 10% labeled images. This immediately becomes a practically
applicable recipe for them. I definitely am looking forward to seeing this thing
implemented at scale at different companies. That's there. If I understood it correctly,
just for the viewers, you folks used a variant of distillation here, which is more popularly
referred to as self-training. I don't think there's really a difference between
what we call distillation and what other people call self-training. I guess the idea is basically
we will pass information into the network. We get its output probabilities and then we train
another neural network with those output probabilities as the targets.
What I find fascinating is how many ideas come together in this paper. There's first,
there's this, let's do representation learning and then we have these just small labels. We find
tune and then there's big networks, small networks. Then you label, but you also apply
some noise, if I understand correctly, in the process of transferring. Maybe I'm misremembering
that, but there's a lot of ideas that come together. Yannick, you probably confused it with
noisy student training, where they impose noise during the student training.
Sorry, maybe not, but there is a lot of ideas that come together. Something tells me that
there was a process behind going, you probably didn't sit down after SimClear1 and be like,
all right, what do we do for SimClear2? Okay, let's do this. It tells me there was this process.
If you can maybe elaborate a bit on how did you going to build up the system towards the final
output? Was there dead ends or was it like, let's build up until we can no longer make it better?
How was that? Yeah, I guess there was a bit of a process. After the original SimClear paper,
I guess it's clear from the SimClear paper that when we have this bigger network, we get much
higher linear evaluation accuracy than we do if we just train SimClear with a ResNet50. Then the
question was, is there some way that we can somehow eliminate this dependence on this giant
network? Because the giant network is annoying to work with, it's computationally expensive,
it's big. We first tried what happens if you distill the unsupervised network. We basically
have this task that is set up as a form of cross entropy loss when we're doing the contrast of
learning. You can also think about distilling on that task where you have a probability distribution
that corresponds to the similarity between an image and all the other images. You could use
those kinds of targets to distill. We tried that and that kind of worked. Then we also tried the
approach of first fine-tuning the big network and then distilling it. It turned out that worked
a lot better. I guess we jumped straight to distillation because we knew that we could get
much better results by using a giant network with SimClear. Then once you realize that distillation
is going to be important, the only thing you've got to figure out is what kind of distillation
should you be doing. What we found was this approach of pre-training, then fine-tuning,
then distilling works a lot better than pre-training, then distilling, then fine-tuning.
How far down do you think one can go with this final distillation step? Is this something that is
conceivably going to be available on, let's say, edge devices at some point, like that
our glasses or something run with similar accuracies to these giant networks? Or
is it more a factor of four, a factor of 10 kind of stuff?
I think there are clearly some limits to distillation. I guess we probably shouldn't
expect distillation of the kind that we do in SimClear v2 to work substantially better than
supervised distillation, which has been around for quite a while now. I think what's impressive is
that in the self-supervised case, in the contrastive case, distillation basically allows you to
recover the same accuracy that you would get from training supervised from scratch, whereas without
it, the accuracy is a lot worse. It seems like it maybe matters more in this contrastive case,
but I think generally when you do distillation in the supervised case, you can get maybe a
percentage point gain, maybe a couple of percentage points. I think that's probably about the limit
in terms of the improvement that you could get from any kind of distillation-based approach
over supervised training from scratch. Fascinating. I don't know if you know that we've been playing
with GPT-3 and you said something quite interesting just now. You said that they're deterministic,
but in GPT-3, that's not really the case. If you sample from it deterministically,
it gets stuck in cycles. You have to do some kind of trickery, some kind of random sampling from
this distribution. It might be the case in future computer vision models as well, that we have to
randomly sample from them in some way, because otherwise it would get into some pathological
behavior. Maybe to do that, we need to have some kind of controller on the top. I suppose my question
is in the future, maybe we will be in the stochastic regime. What do you think about that?
With GPT-3, you're trying to generate data. When you generate data, there has to be some
kind of noise that's coming from somewhere. The process of generating data is like turning
noise into data. For image classification, we have the data and we just want to turn it into
a label. Maybe there's this implicit notion of stochasticity in that the network gives some
output distribution. I think we still want everything to be deterministic if it can be
deterministic. We basically want the network to say, this is a dog with probability x.
If there's some way to improve it with stochasticity, I don't know. I guess dropout used to be very
popular, but it seems like it's not so popular anymore and it doesn't really help us very much
with vision models. Also, even dropout is generally only used when we train the neural
networks. On the other hand, the brain is very stochastic. The brain has lots of noise. I guess
that suggests that maybe there's some way to leverage noise to learn better representations
in neural networks as well and we just don't quite know the right way yet.
That's right. Max Welling said to us that he thinks that the future of AI will have a
generative component. He thinks that we have the matrix in our minds. We have these simulations
going on all the time and we're generating new scenarios. It's related to the data
augmentation thing as well. Some people have said to me in the past that using a GAN might be a way
of doing data augmentation. Presumably, that would require some kind of stochastic sampling as well.
I suppose it's just quite interesting to see where these two things might meet in the middle at
some point. Max Yeah. I don't know. I guess like with a GAN, using a GAN to do data augmentation,
you have this problem that you still don't actually have more data. You have a GAN that's
trained on the same data. It might help you because your way of encoding inductive bias into the GAN
is different from your way of encoding inductive bias into the neural network. Maybe by having
more inductive bias, you can learn a better function. You still don't have more data at it.
Without having more data, there's no reason to expect a priority that you will be able to learn
a better function. Paul I'm so glad you said that. It was always my intuition. The amount of people
that have said to me that you should use a GAN for data augmentation. Anyway.
Max What's the first thing you think about if you're like, oh, what could I use a GAN for?
And then you learn about data, and they're like, wait, this is so much more data. Yeah,
but conceptually, yes, you don't have more data. And ironically, when you do the simple
data augmentation, you do have more data because you put all the knowledge in there
as a human of what makes two images dissimilar visually, but still equivalent semantically,
which again, is exactly the opposite. It gives you images that are visually similar,
but it has no intuition of what the semantic similarity is. For my last question, I actually
want to switch topics just a little bit to what Tim said at the beginning, namely your love of Julia.
So I have seen a number of especially data scientists be strong advocates for Julia as a
language and so on. Do you want to give anyone sort of the pitch? Why should we even consider this?
Yeah, so I think Julia is a much better programming language than Python in many ways.
I guess one thing and I guess the thing that first attracted me to Julia is that it's really
fast, like you can write Julia code and with very little work, you will end up running as
fast as equivalent C code. So that's something that you can't get out of standard Python code.
If you're just writing a for loop in Python, it's going to be super slow. But in Julia,
you don't have to worry about all of that. And you don't have to worry about like Cython or
number all of this other stuff that people have hacked on top of Python. Julia is just
designed to be fast and it works. I think there are other advantages to Julia as a language
beyond that. I guess it's built on this idea of generic functions where you have a function that
can take multiple types and you can define the function differently for the different types.
And this is something that we do all the time when we're doing like machine learning, like we
have matrix multiplication, which is a different form of multiplication that takes matrices and
produces something. And I guess like in Python, it's so it used to be that you had to type dot dot
to multiply things, but now it's like they have this add symbol that does matrix multiplication.
But Julia is designed for these situations where maybe beyond just matrices,
you have these funny types of structured matrices, you have sparse matrices, and you can define
special methods for the product of a sparse matrix and a vector or all sorts of things where you
might want different methods depending on the types. And even though it seems like this is
complicated and you might have some trouble picking which version of the function is going to be called
at runtime, because Julia is ultimately compiling everything when you call it. And because it has
this kind of strong type system, it can ultimately pick which method is going to be used and compile
that method call in and you don't have to worry about picking which one. And so it still ends up
being fast. I also think like there's this question of whether like the object oriented Python or
the object oriented paradigm and Python is really like the best paradigm for machine learning.
Because I guess like it's like we have data and then we have functions that operate on the data.
But in the object oriented paradigm, you want the functions that operate on the data to be
attached to the data, which is like a weird way of setting things up. And that Julia is not set up
that way. You have these data structures. And then because you are able to create functions that
specialize on the data structures, you don't have to worry about attaching those functions to the
data structures themselves. Amazing. Dr. Simon Cornblith. Thank you very much for joining us
this evening. It's been an absolute pleasure. Thanks for having me. Thank you. I really hope
you've enjoyed the show today. We've had so much fun making it. Remember to like, comment,
and subscribe. We love reading your comments, every single one of them. And we'll see you back
next week.
