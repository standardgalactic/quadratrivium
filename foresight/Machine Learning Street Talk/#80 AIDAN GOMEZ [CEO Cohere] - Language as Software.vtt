WEBVTT

00:00.000 --> 00:07.440
Aiden Gomez is a computer scientist and widely recognized AI expert who co founded AI company co here before that.

00:07.440 --> 00:14.480
He worked as an intern in the Google brain team in Toronto, alongside Jeffrey Hinton, one of the Godfathers of deep learning.

00:14.480 --> 00:17.880
Actually, the only Godfather who hasn't been on MLST.

00:18.000 --> 00:19.960
Aiden, you need to put a good word in for us.

00:21.360 --> 00:23.360
I will bring it up.

00:24.280 --> 00:28.880
So Gomez was the kind of person Hinton recalled who had so many ideas.

00:29.160 --> 00:33.920
It was difficult to pin him down and to get him to focus on what he was actually supposed to be doing.

00:34.520 --> 00:37.760
Now Gomez was particularly interested in learning to translate languages.

00:38.120 --> 00:50.360
And while he was at it, he casually invented transformers, which, as we all know, have become the de facto basic reference architecture for all neural networks and not only for language tasks.

00:51.120 --> 00:58.200
Now co here is a startup which uses artificial intelligence to help users build the next generation of language based applications.

00:58.640 --> 01:05.480
It's headquartered in Toronto and the company has raised $175 million in funding so far.

01:05.880 --> 01:15.920
And the first round was from index ventures and the round also included Hinton, who we just spoke about, Fei-Fei Li and Peter Abial, who are very, very famous folks in the ML space.

01:16.680 --> 01:28.160
Now co here say that their competitive advantage or at least at least one of their competitive advantages lies in its focus on safety, which is crucial for customers deploying models which could potentially out for something harmful.

01:28.520 --> 01:30.600
And we'll discuss what we mean by that a bit later on.

01:31.240 --> 01:37.080
Now, I can honestly say personally that language models have transformed how I use computers over the last six months or so.

01:37.080 --> 01:53.760
I've been using them on a daily basis in the form of scripts, which helped me use the command line better using an interactive REPL playground for doing all sorts of stuff like composing emails for me, understanding error logs when I'm coding, summarizing information, performing repetitive tasks.

01:53.760 --> 01:54.320
Do you name it?

01:54.600 --> 02:00.640
Actually, I regularly discover new tasks that I can use for language models just through kind of creative exploration.

02:00.640 --> 02:05.440
So I'm a bit of a convert, although caveat M tour is still definitely in effect.

02:05.760 --> 02:09.920
The words of Chomsky, Fodor and Felician are still ringing loudly in my ears.

02:09.920 --> 02:12.040
We just did some content about that on our last show.

02:12.040 --> 02:13.880
But anyway, Aidan, it's an absolute honor.

02:13.880 --> 02:15.120
Welcome to MLST.

02:15.400 --> 02:19.440
And what's it like being the CEO of one of the fastest growing startups?

02:21.760 --> 02:22.920
Thank you so much for having me.

02:23.160 --> 02:25.800
I'm stoked to get to meet you and chat about Coheir.

02:27.200 --> 02:29.520
What's it like being the CEO of Coheir?

02:29.520 --> 02:35.440
It's a privilege and a thrilling ride.

02:35.560 --> 02:39.960
Like you're just hanging on for dear life as things are going and trying to keep up.

02:41.000 --> 02:47.400
But it's honestly just such a privilege to get to work with the people that I work with on the problem that we work on.

02:47.800 --> 02:52.640
I think I'm just so, so lucky and fortunate.

02:53.040 --> 02:58.000
I'm really, really excited to get into the discussion about transformers and attention is all you need.

02:58.000 --> 03:04.960
So I mean, I remember when this came out in about 2017, actually, it changed the landscape of deep learning forever.

03:05.320 --> 03:07.840
It took me a very long time to understand it.

03:07.840 --> 03:12.960
And I probably only did understand it after reading Jay's famous blog post, you know, The Illustrator Transformer.

03:13.200 --> 03:15.600
Jay, of course, is he your engineering director now?

03:16.480 --> 03:17.200
Yeah, yeah.

03:17.200 --> 03:22.680
So Jay is the best communicator in machine learning that I know.

03:22.840 --> 03:23.680
He's incredible.

03:24.960 --> 03:32.120
And I think if you search, I think if you search Transformer NeuralNet on Google, it's not our paper that comes up.

03:32.120 --> 03:33.800
It's Jay's blog.

03:34.240 --> 03:38.760
So it tells you like how much better his communication is than ours.

03:39.640 --> 03:40.640
It was an epic blog.

03:40.640 --> 03:44.480
He's actually done a new blog post on stable diffusion as well.

03:44.880 --> 03:47.760
And folks should also subscribe to his YouTube channel.

03:47.760 --> 03:49.000
He's got an amazing YouTube channel.

03:49.000 --> 03:58.720
But anyway, there was Jay's blog post and Janik's video that made me finally understand it because at the time it was the most exciting kind of technology.

03:59.080 --> 04:06.800
And I remember I was interviewing for the Bing team at the time and I spent a long time studying the paper just so that I could be conversant in it when I had that interview.

04:06.800 --> 04:12.160
But anyway, could you tell us, you know, what was the story behind the paper and also what's special about transformers?

04:12.920 --> 04:31.480
Yeah, so the story behind the paper, I should say that, like, I was the intern on this project and so I was the baby, like, doe-eyed, showed up in Google Brain down in Mountain View and my contribution was really on the software.

04:31.480 --> 04:59.120
So there were existing threads inside of Google Brain, primarily driven by Noam Shazir, Jakob Uskarite, Lukash Ashish, which were pushing along this idea of deploying these autoregressive sequence models for text because they had been really popularized in WaveNet for audio and language generation.

05:01.600 --> 05:12.640
But there was like a big push towards how do we deploy these against text and how do we incorporate attention, which was something that was groundbreaking for RNNs, like the previous generation of model.

05:13.040 --> 05:16.960
They wanted to incorporate this into these new autoregressive sequence models.

05:17.640 --> 05:31.280
And so when I came in, I was working with Lukash focused on, like, the software side of things, scalable training frameworks, supporting training that, you know, could be distributed across not just tens of

05:31.280 --> 05:35.000
machines or hundreds, but thousands of accelerators.

05:37.000 --> 05:43.040
And so we built Tensor2Tensor, which was the framework that was used to develop the transformer.

05:45.240 --> 05:58.040
Shortly after we started putting that together, I was sitting next to Noam Shazir and we heard that he was also kind of thinking along similar threads for these autoregressive models.

05:58.040 --> 06:03.600
And so Lukash and I convinced him to come over to Tensor2Tensor and start doing it on our framework.

06:04.600 --> 06:07.160
And then we heard over in the translate department.

06:07.160 --> 06:09.200
So Brain was one division of Google.

06:09.560 --> 06:10.840
Translate was a separate one.

06:11.440 --> 06:27.680
Over in translate, there was Yakka, Bashish, Nikki, working on a similar project to Noam where they wanted to create a purely attentive model, strip back all the complexity of RNNs, all of these, like, very complex,

06:27.680 --> 06:37.640
gates and states and et cetera, and just rip that all away and just have MLPs and attention layers.

06:40.200 --> 06:41.360
And so we all came together.

06:41.360 --> 06:45.280
We all consolidated on this framework called Tensor2Tensor.

06:45.880 --> 06:57.640
And the next three months, up until the NERP deadline, was just a sprint, like sleeping in the office, just going as fast as you can, running experiments.

06:57.680 --> 07:00.640
And iteration and iteration and finding this bug and that bug.

07:02.840 --> 07:07.960
And so really like a huge piece of the project came together within 12 weeks.

07:10.440 --> 07:14.800
So it was like an extraordinary pace.

07:17.240 --> 07:24.320
And it was, as an intern, this was like my first experience in like proper research.

07:24.800 --> 07:26.080
And I just thought this was normal.

07:26.080 --> 07:28.720
I thought, OK, this is what everyone does.

07:28.720 --> 07:30.880
We all just crank out papers in three months.

07:30.880 --> 07:32.000
We sleep at the office.

07:34.920 --> 07:39.680
And I didn't really have an appreciation for what we had accomplished at the end.

07:39.720 --> 07:44.480
Like, I can't say that I was particularly prescient at the time.

07:44.920 --> 07:49.920
I remember like the night before the NERP deadline, the night before we had to submit.

07:50.760 --> 08:05.840
It was like whatever, 3 a.m. Ashish and I were sitting on a couch in the brain office and he turned to me and he he said, you know, like, Aiden, like, this is going to be huge.

08:06.600 --> 08:11.760
And my reaction was like, we bumped up blue, like the blue score by one point.

08:12.000 --> 08:13.040
I was like, really?

08:13.040 --> 08:13.680
You think so?

08:13.840 --> 08:14.400
Oh, cool.

08:14.400 --> 08:16.600
Like, don't we all like, isn't this what research is?

08:16.600 --> 08:17.640
We just bumped it up a point.

08:17.800 --> 08:18.560
What's the big deal?

08:19.400 --> 08:32.080
But I think what I didn't appreciate was the fact that such a simple method could achieve such insanely high performance.

08:32.960 --> 08:38.560
Like, at that time, we were training on eight accelerators for one of these models and they could be trained in within a day.

08:41.560 --> 08:46.720
But the architecture was so stripped down, it was so refined, it was so easy to scale and to grow.

08:47.120 --> 08:50.080
I just, it was hard to see the future.

08:51.280 --> 08:55.400
And it was hard to see what would happen, which was this massive scaling project.

08:55.840 --> 08:59.520
And I think Ashish saw that, that happening.

09:01.680 --> 09:03.080
So yeah, that was my contribution.

09:03.080 --> 09:06.160
That was the part of the team that I brought.

09:08.040 --> 09:11.320
But yeah, it was a very, very exciting few months.

09:11.320 --> 09:16.920
And then it was an extremely exciting few years afterwards before I left Google and started cohere.

09:17.280 --> 09:18.000
I can imagine.

09:18.040 --> 09:20.840
And as you say, some of the best work happens in sprints.

09:20.840 --> 09:30.920
And even though it only moved the needle a tiny bit on the blur score, something we'll explore later is sometimes it might actually be performing better than the metric might suggest.

09:31.160 --> 09:33.240
And simple models are often better.

09:33.280 --> 09:35.800
But I wanted to ask a bit of a technical question.

09:35.800 --> 09:40.920
So DeepMind recently released a paper called Neural Networks and the Chomsky Hierarchy.

09:41.680 --> 09:46.720
They grouped a bunch of tasks according to the formal language classes in the Chomsky Hierarchy.

09:47.160 --> 09:53.920
And they showed that transformers were only able to represent finite languages, so not not even regular languages, which RNNs could do.

09:54.640 --> 10:00.640
Neural networks could only support the higher modes, like context-free languages, if they were memory augmented with a stack.

10:01.200 --> 10:03.880
And they said that this had implication for scaling laws.

10:04.640 --> 10:10.320
So for example, the transformers architecture, they said, could never perfectly learn tasks which were higher up in the Hierarchy.

10:10.640 --> 10:18.080
So we've done some shows on MLSD where we've spoken about neural networks not being too incomplete because they are finite state automators without the augmented memory.

10:18.080 --> 10:21.360
So, for example, they couldn't approximate pi to the nth digit.

10:21.720 --> 10:26.240
And there's this photo and collision connectionism critique paper, which we spoke about on the last show.

10:26.280 --> 10:33.640
And that basically means that neural networks are not able to perform symbolic compositional reasoning or in plain English, they can't represent infinite objects.

10:34.040 --> 10:43.880
And we spoke to Randall Belastriereo and he had this paper called The Spline Theory of Neural Networks, which showed that a neural network is a linear operation given a single input example.

10:43.880 --> 10:54.840
So given all of this, I personally find the conversation about scaling laws, I'm a bit skeptical about it, although I'm genuinely interested in emergent properties or transients that happen during the scaling process.

10:54.840 --> 11:01.760
But given all of this, I mean, how do you think about the practical limitations of transformers in terms of what algorithms they can learn?

11:03.720 --> 11:08.200
Yeah, I really hope the transformers aren't the last architecture.

11:10.400 --> 11:16.480
I would be extremely disappointed if this is as creative and high performing as we can get.

11:16.840 --> 11:22.480
I think that transformers took off because of their scaling properties and also because of a network effect.

11:23.040 --> 11:31.640
The community consolidated around this one architecture, we started to build all of this infrastructure, specifically for transformers.

11:32.080 --> 11:35.080
There was a network effect and it had very nice scaling properties.

11:35.080 --> 11:42.360
And so the community really came together around this architecture and built up infrastructure to support its adoption.

11:44.080 --> 11:48.080
I think that's what's led to their proliferation or their success.

11:50.720 --> 11:56.440
I hope that it's not the last architecture that would be super, super disappointing and boring.

11:57.160 --> 12:02.160
You point out that they're not Turing complete.

12:02.160 --> 12:10.880
I should clarify that I'm not a linguist, I'm not too familiar with Trump's hierarchies or the implications of the deep mind paper.

12:10.880 --> 12:14.920
But one thing that's interesting about it, when I read it,

12:17.480 --> 12:24.440
they're not speaking in theoretical terms or speaking in empirical terms of what functions are achievable.

12:25.440 --> 12:28.440
From a normal initialization.

12:31.440 --> 12:33.440
I think that's a fascinating lens.

12:33.440 --> 12:40.440
In theory, a transformer is a universal approximator and it might be even Turing complete.

12:40.440 --> 12:46.440
But in practice, if you can't explore all permutations of parameters,

12:46.440 --> 12:52.440
it's very true that it'll find the simplest function that satisfies the task.

12:55.440 --> 13:00.440
I think that's like a good guiding lens when thinking about what architectures come next.

13:00.440 --> 13:02.440
Where do we go from here?

13:02.440 --> 13:12.440
What are the sorts of components we need to add into neural networks to support them in representing these more complex functions?

13:15.440 --> 13:22.440
So I do think that transformers are limited and I really hope they're not our final architecture.

13:22.440 --> 13:30.440
I hope that we come up with something that's significantly better and I see promising efforts along those directions.

13:30.440 --> 13:38.440
I think that retro from deep mind, like augmenting transformers with a searchable memory, I think that's a huge step forward.

13:38.440 --> 13:44.440
The next thing we need to support is the ability for these transformers to keep state over long time horizons,

13:44.440 --> 13:51.440
to be able to write into their own memory in order to make notes about what they've seen in the past.

13:53.440 --> 13:58.440
And so I think there's a lot of work to be done and it's not happening fast enough.

13:58.440 --> 14:05.440
I think more people should pick up these research questions and look for new scalable ways of doing it.

14:05.440 --> 14:13.440
Because to speak to the scaling hypothesis, the real bottleneck for feature adoption,

14:13.440 --> 14:19.440
the real bottleneck for adding in a new component to the transformer architecture,

14:19.440 --> 14:27.440
it's all scale and efficiency. People will just adopt the thing that is simplest, fastest and best performing.

14:27.440 --> 14:35.440
And so we need to do the work in order to make these other components like addressable memory efficient and scalable

14:35.440 --> 14:40.440
in the same way that the core vanilla transformer architecture is today.

14:40.440 --> 14:49.440
Yeah, it's such a paradox, isn't it? Because the deep mind paper was saying that a Turing machine is the most powerful computational model,

14:49.440 --> 14:53.440
but neural networks are trainable and they scale really, really well.

14:53.440 --> 14:58.440
So I'm sure we'll find some innovation that will somehow bridge that gap somehow.

14:58.440 --> 15:00.440
But I wanted to learn a little bit more about Cohere.

15:00.440 --> 15:05.440
So could you tell us about your core product portfolio and what's on your roadmap?

15:05.440 --> 15:11.440
Yeah, for sure. So the mission with Cohere is to really just give technology language.

15:11.440 --> 15:16.440
And the way that we want to do that is put the tech, put these large language models into more hands.

15:16.440 --> 15:23.440
Today, if you're building a product and you want to deploy a language model as part of that,

15:23.440 --> 15:30.440
you've got to learn how to use some framework like PyTorch or Jax or TensorFlow.

15:30.440 --> 15:38.440
You need to learn how to install CUDA kernels on a VM, which is actually a huge, huge task.

15:38.440 --> 15:43.440
And so it's this very painful process, requires a lot of learning.

15:43.440 --> 15:49.440
It's very unnatural and requires a ton of prerequisite knowledge.

15:49.440 --> 15:52.440
So for Cohere, what we want to do is abstract that away.

15:52.440 --> 15:57.440
We want to present an interface which is just intuitive, natural, like Cohere.classify.

15:57.440 --> 16:01.440
You feed in the tweet that you want to classify.

16:01.440 --> 16:05.440
You give some examples of tweets being classified into the categories that you care about,

16:05.440 --> 16:13.440
and then you get a response which says, yeah, that tweet fits into sports or whatever.

16:13.440 --> 16:19.440
So we want to create a portfolio of these endpoints, which just makes this technology more accessible.

16:19.440 --> 16:24.440
And the goal being that it starts to proliferate.

16:24.440 --> 16:29.440
Because I think the transformer was released half a decade ago.

16:29.440 --> 16:39.440
And I saw extraordinary research-level results or contributions from it, right?

16:39.440 --> 16:49.440
Like the ability to write really compelling tags, the ability to few-shot prompt and get pretty good performance on a huge swath of problems.

16:49.440 --> 16:55.440
But it just hasn't been changing the fabric of consumer applications.

16:55.440 --> 16:58.440
And I'm a consumer. You're a consumer. We all use these apps.

16:58.440 --> 17:03.440
And so as a researcher who's seen the potential of the technology, it's super frustrating.

17:03.440 --> 17:12.440
You just wonder, like, what's stopping this from getting out there into apps faster?

17:12.440 --> 17:17.440
And I think two of the reasons are there's this huge compute barrier, right?

17:17.440 --> 17:22.440
When you train these big models, you need a supercomputer and tons of data.

17:22.440 --> 17:27.440
That's very difficult to use and collect.

17:27.440 --> 17:29.440
So the compute is definitely one of the big barriers.

17:29.440 --> 17:35.440
But the second piece is that really it's like the people, right?

17:35.440 --> 17:40.440
At the moment, yeah, we have millions of developers on our planet.

17:40.440 --> 17:46.440
But a tiny, tiny, tiny fraction of them actually know how to do this specific thing, machine learning and training models.

17:46.440 --> 17:52.440
So there's not a lot of people out there actually doing the work to integrate this into every product on Earth.

17:52.440 --> 18:00.440
And so for us, like, what we want to do is just blow that open, put this stuff into the hands of every single developer.

18:00.440 --> 18:05.440
It doesn't matter what your specialty is if you're a database developer, whatever you do, it doesn't matter.

18:05.440 --> 18:10.440
The important thing is that now you can build with large language models because you're given an interface,

18:10.440 --> 18:15.440
which doesn't require, you know, three years of study to get up to speed.

18:15.440 --> 18:18.440
So that's really what we're pushing for.

18:18.440 --> 18:22.440
It's quite ironic actually because I was involved in ML DevOps for many years,

18:22.440 --> 18:27.440
and normally introducing machine learning into software engineering makes it exponentially harder.

18:27.440 --> 18:31.440
But now we seem to have jumped to language models where it's become easier again.

18:31.440 --> 18:38.440
And I want to talk about the language being a new type of interface for software, but we'll save that just for a minute.

18:38.440 --> 18:42.440
I want to talk about the friction using large language models as a startup owner.

18:42.440 --> 18:47.440
So I'm a startup founder myself, and I want to use large language models because I'm very excited about them.

18:47.440 --> 18:53.440
And, you know, I can speak to some of the friction that I've been experiencing looking into this.

18:53.440 --> 19:02.440
So, I mean, if we look at what's happening over OpenAI, for example, the Microsoft Signup form and the content policy over there is quite intimidating.

19:02.440 --> 19:05.440
They've made it quite challenging to use in production.

19:05.440 --> 19:09.440
And actually, it makes me wonder how many people are using it in production.

19:09.440 --> 19:13.440
They say that content creation applications have a higher chance of misuse,

19:13.440 --> 19:18.440
which seems like an oxymoron to me because GPT-3 is literally like a content creator.

19:18.440 --> 19:21.440
They say you're going to expect a call from the Microsoft vetting service.

19:21.440 --> 19:24.440
They can pull the rug from under you at any time for any reason.

19:24.440 --> 19:27.440
They log and record everything under the guise of safety.

19:27.440 --> 19:29.440
They can't be held liable for anything.

19:29.440 --> 19:31.440
As I said, open-ended applications are refused.

19:31.440 --> 19:36.440
And in my opinion, the most exciting applications are the open-ended applications, right?

19:36.440 --> 19:38.440
I love the playful nature of it.

19:38.440 --> 19:44.440
Inside my application, I want to build a community of tinkerers who discover interesting new sub-applications, right?

19:44.440 --> 19:49.440
I want to build a marketplace of prefab prompt structures on top of my platform.

19:49.440 --> 19:54.440
I don't want to specify exactly how my application will be used, right?

19:54.440 --> 19:56.440
You know, I want my application to be fluid.

19:56.440 --> 20:03.440
And large language models make the consumers of an application kind of like programmers of that platform themselves.

20:03.440 --> 20:05.440
I think it's an entirely new paradigm.

20:05.440 --> 20:11.440
So with that in mind and the friction, do you think that would ever be allowed on Cohir?

20:11.440 --> 20:13.440
The startup that you're describing.

20:13.440 --> 20:19.440
Well, this idea that I shouldn't need to say to Cohir exactly how I'm going to be using the platform.

20:19.440 --> 20:21.440
I want it to be very open-ended and playful.

20:21.440 --> 20:27.440
I want my users to create new prompts and share the prompts and use it in interesting ways.

20:27.440 --> 20:30.440
I want it to be as open-ended as possible.

20:30.440 --> 20:40.440
Yeah, I think on Cohir, that would be fine subject to your users complying with some basic ethical principles.

20:40.440 --> 20:49.440
So presumably, I imagine that you don't want your users creating bots which propagate false information or hate speech on Twitter.

20:49.440 --> 20:56.440
You don't want like a billion bot accounts responding to every article.

20:56.440 --> 21:02.440
And so I assume that you're also incentivized to have some degree of terms of use.

21:02.440 --> 21:04.440
Is that right?

21:04.440 --> 21:07.440
Yeah, so I completely agree with your terms of use.

21:07.440 --> 21:09.440
I think it's very good actually.

21:09.440 --> 21:17.440
The one sticking point for me is that in order to have my application vetted, I have to say how it's being used.

21:17.440 --> 21:22.440
And it seems to be slightly away from having just quite an open-ended application.

21:22.440 --> 21:26.440
But I absolutely agree with all of the points in your content policy.

21:26.440 --> 21:27.440
Okay, yeah.

21:27.440 --> 21:30.440
So I think that startup should absolutely exist.

21:30.440 --> 21:31.440
That sounds awesome.

21:31.440 --> 21:36.440
Like a community of people sharing prompts, iterating with each other, figuring out stuff that works, doesn't work.

21:36.440 --> 21:40.440
I think we saw a lot of that with mid-journey and stable diffusion, right?

21:40.440 --> 21:44.440
Like just this collective effort to like, let's figure this thing out.

21:44.440 --> 21:47.440
Let's discover new ways to use it.

21:47.440 --> 21:54.440
That should 100% exist in the world and cohere would 100% support that.

21:54.440 --> 22:06.440
I think at the same time, there are just application domains that we don't want people building, like, you know, the ones that I just described.

22:06.440 --> 22:16.440
So as long as you're okay filtering those out and working with us to make sure that your product doesn't get used in those ways,

22:16.440 --> 22:17.440
we're fully on board.

22:17.440 --> 22:18.440
Like that should exist.

22:18.440 --> 22:20.440
That sounds amazing.

22:20.440 --> 22:28.440
And I think like more broadly, coherence is we really want to see a proliferation of this technology.

22:28.440 --> 22:32.440
We want to see a million new startups born from it.

22:32.440 --> 22:41.440
And so we view our users as like partners in bringing this to fruition and putting this tech in front of more users, more consumers, more businesses.

22:41.440 --> 22:44.440
And so we're very collaborative.

22:44.440 --> 22:46.440
Like we don't want to rug pull anyone.

22:46.440 --> 22:48.440
We don't want any surprises.

22:48.440 --> 22:52.440
So long as like our terms are met, we want to be a partner.

22:52.440 --> 22:54.440
We want to help you build.

22:54.440 --> 22:56.440
We want to like support you in the best way possible.

22:56.440 --> 23:01.440
I think some of this paradox might be from a legal point of view because I agree with you.

23:01.440 --> 23:05.440
You call it the playground and that's a great term for it.

23:05.440 --> 23:10.440
I think that some of the most exciting applications of large language models haven't been discovered yet.

23:10.440 --> 23:16.440
And they'll be discovered when you have a diverse community of people kind of sharing and trying interesting things.

23:16.440 --> 23:18.440
But just from that legal point of view.

23:18.440 --> 23:28.440
So we were a bit worried actually that some of the customers of our application might send us up malicious prompts and have our service terminated.

23:28.440 --> 23:37.440
And the way that we've been thinking about working around it on open AI, it's not possible on Coho, I don't think, is kind of getting the users to sign up for the service directly.

23:37.440 --> 23:44.440
And then just pasting their key inside our application so that they're responsible, they're getting themselves blocked if they do anything bad.

23:44.440 --> 23:47.440
And legally that puts us in a much safer position.

23:47.440 --> 23:49.440
But how do you feel about that?

23:49.440 --> 23:51.440
I mean, that's a great technique.

23:51.440 --> 23:54.440
I think it should be supported on Coho if it's not already.

23:54.440 --> 23:55.440
I think it is.

23:55.440 --> 24:02.440
We have a few people doing stuff like that, like a bring your own key type application.

24:02.440 --> 24:15.440
Alternatively, Coho would want to work with you to help you moderate use to catch bad actors, to catch misuse, out of terms use, etc.

24:15.440 --> 24:17.440
So we'll be very collaborative.

24:17.440 --> 24:18.440
We'll help you do that.

24:18.440 --> 24:19.440
We'll help you look at the data.

24:19.440 --> 24:21.440
We'll help you find users who are misusing it.

24:21.440 --> 24:29.440
We won't just blanket ban you because one of your users is trying to adversarily attack your business.

24:29.440 --> 24:40.440
We're trying to build with you and so I think we'll be quite reasonable assuming that you're reasonable too and that you don't want that type of activity on your product.

24:40.440 --> 24:50.440
So it's about supporting startup founders, helping them build their own tools to catch this sort of stuff and ban those users.

24:50.440 --> 24:58.440
It's like a collaborative building approach as opposed to, yeah, you're just a user, either comply or get banned.

24:59.440 --> 25:06.440
We're much more, I don't know, present, engaged.

25:06.440 --> 25:07.440
That makes sense.

25:07.440 --> 25:16.440
I mean, you can appreciate my fear that in a sense startup founders have lost their autonomy because it costs millions and millions of dollars.

25:16.440 --> 25:25.440
You folks are hiring some of the most talented people in the world to do this stuff and we're building on top of that foundation, which at the moment it seems like a risk.

25:25.440 --> 25:27.440
I appreciate that in spirit.

25:27.440 --> 25:31.440
It shouldn't actually be a risk, but just talking high level.

25:31.440 --> 25:37.440
So how would you distinguish your service from, let's say, GPT-3?

25:37.440 --> 25:47.440
So I think the Open AI team and GPT-3, they did like a fantastic thing by opening that up and giving it to the world.

25:47.440 --> 25:54.440
In terms of distinguishing ourselves from them, I think they've taken a very hands-off approach to this stuff.

25:55.440 --> 26:01.440
And they put out endpoints and it's kind of like a good luck, have fun, go build.

26:01.440 --> 26:10.440
With Cohere, we're trying to be more present and engaged and we're trying to tailor our roadmap towards the needs of users.

26:10.440 --> 26:18.440
So, for instance, we're listening to users and seeing what they're signing up for, what they're asking from us.

26:18.440 --> 26:21.440
One of those things is summarization.

26:21.440 --> 26:30.440
And so now we've spun up an effort to release a summarization endpoint that's generally useful across summarizing chat transcripts,

26:30.440 --> 26:33.440
like long documents, that type of thing.

26:33.440 --> 26:35.440
And so it's a two-way street.

26:35.440 --> 26:41.440
It's not just that our users are the consumers of our path towards whatever we want.

26:41.440 --> 26:45.440
It's like a dialogue and a conversation of what do you need?

26:45.440 --> 26:46.440
What should we build next?

26:46.440 --> 26:48.440
What do you see coming?

26:48.440 --> 26:50.440
And then we go build it.

26:50.440 --> 26:57.440
So it's very much a, it feels more two-way, community-oriented.

26:57.440 --> 27:02.440
We're trying to build the right product for our users and the most useful product possible.

27:02.440 --> 27:10.440
And so the way we do that is just through dialogue and conversation and people asking for the thing that they need, they want.

27:10.440 --> 27:14.440
OpenAI service suddenly kind of got a lot better recently.

27:14.440 --> 27:17.440
And I think they call it DaVinci 2.

27:17.440 --> 27:22.440
It's a bit of a mystery because I reviewed GPT-3 when it first came out a couple of years ago.

27:22.440 --> 27:24.440
And recently it seems much better.

27:24.440 --> 27:33.440
And if I understand correctly, they've done some kind of fine-tuning using reinforcement learning to align it to human preferences that instruct GPT or something like that.

27:33.440 --> 27:35.440
I don't even know if that's the case.

27:35.440 --> 27:41.440
But I just wondered if you could comment on that and do you folks plan to do something similar?

27:41.440 --> 27:44.440
Yeah, so we don't call them instruct models.

27:44.440 --> 27:48.440
We call them command models because of the co and co here.

27:48.440 --> 27:53.440
But we do have something currently in private beta.

27:53.440 --> 27:56.440
Hopefully we'll release it soon.

27:56.440 --> 27:59.440
But yeah, it has a huge impact on model performance.

27:59.440 --> 28:10.440
Like the ability to specify an instruction, specify an intent, describe the type of problem that you're solving completely changes model performance.

28:10.440 --> 28:12.440
And in some ways it's surprising.

28:12.440 --> 28:18.440
In other ways, it's very not surprising in that these models are just trained on web scraped data.

28:18.440 --> 28:21.440
Like there's no reason why you would expect them to behave the way that they do.

28:21.440 --> 28:27.440
We're just kind of lucky that they work as few-shot programs.

28:27.440 --> 28:38.440
And so I think this aligning with humans intent, human commands, human instructions,

28:38.440 --> 28:41.440
it's just a much more natural way to interface with these models.

28:41.440 --> 28:50.440
I think before instruct style models, it was a bit like you would have to discover the language of the model, right?

28:50.440 --> 28:55.440
And it was like this very opaque process of shifting things around, rephrasing things,

28:55.440 --> 29:00.440
trying to figure out what makes sense to this model.

29:00.440 --> 29:04.440
Super brittle, super painful.

29:04.440 --> 29:09.440
This command style model actually pulls that away and it's much more intuitive, it's much more fluid.

29:09.440 --> 29:12.440
It's the way that you would expect to interact with the model.

29:12.440 --> 29:16.440
Yeah, it's really interesting how, you know, when Steve Jobs released the iPad,

29:16.440 --> 29:19.440
he said there was something magic about it, something of magic about that interface.

29:19.440 --> 29:26.440
And similarly with these newer models, it feels like an invisible boundary is being crossed where I trust it.

29:26.440 --> 29:31.440
In a way, it's a trap because I'm anthropomorphizing it more because of exactly what you just said before.

29:31.440 --> 29:35.440
But I wanted to get into some engineering characteristics of code here.

29:35.440 --> 29:37.440
So I'll send you some quick fire questions.

29:37.440 --> 29:40.440
So how many tokens in a context window?

29:40.440 --> 29:45.440
So at the moment, it's 2048 and it should flip to 4096 shortly.

29:45.440 --> 29:50.440
But our goal is for an infinite token width.

29:50.440 --> 29:51.440
Wow.

29:51.440 --> 29:55.440
And so there's a few efforts that we're pursuing to enable that.

29:55.440 --> 29:56.440
Yeah.

29:56.440 --> 30:00.440
How many concurrent requests can your customers have, Perky?

30:00.440 --> 30:04.440
Infinite, as many as you'd like.

30:04.440 --> 30:06.440
Oh, even now?

30:06.440 --> 30:07.440
Yes, yeah.

30:07.440 --> 30:14.440
I think there's a top level bottleneck which we can actually remove for specific customers who need more.

30:14.440 --> 30:21.440
I think it's like 10,000 queries per minute.

30:21.440 --> 30:27.440
But we have folks doing billions and billions of characters a day.

30:27.440 --> 30:34.440
And so, yeah, we're happy to remove that restriction for those applications that need it.

30:34.440 --> 30:42.440
Do you support, let's say, enterprise security scenarios like single sign-on, key rotation, that kind of thing?

30:42.440 --> 30:45.440
So we do SSO.

30:45.440 --> 30:46.440
We don't do key rotation yet.

30:46.440 --> 30:54.440
We're hoping to build that out like with other enterprise requirements like co-location and that sort of thing.

30:54.440 --> 30:59.440
We're also building out the capability to do that on different clouds.

30:59.440 --> 31:04.440
But at present, we don't have that yet, but it's roadmaped.

31:04.440 --> 31:05.440
Okay.

31:05.440 --> 31:12.440
And you were just touching on this before, but what's the kind of largest, most highly-scaled application deployed on Co-Hit?

31:12.440 --> 31:23.440
So, similar to, I think, what everyone's been seeing, the first sort of application that has hit product market fit is copywriting.

31:23.440 --> 31:24.440
Yeah.

31:24.440 --> 31:29.440
There's a lot of companies, Jasper, Copy AI, HyperWrite.

31:29.440 --> 31:39.440
They've really found something that works for the average person, drives tons of value, speeds people, makes them more efficient, makes them more creative.

31:39.440 --> 31:44.440
And so that's where we're seeing volumes just skyrocket.

31:44.440 --> 31:49.440
So I'm really excited about the prospect of stacking calls to these large language models.

31:49.440 --> 31:54.440
So building a large computational graph of recursive calls.

31:54.440 --> 31:57.440
But doing all the round trips at the moment is pretty slow.

31:57.440 --> 32:00.440
It's almost like, I want to create this graph, this computation graph.

32:00.440 --> 32:02.440
I want to ship it over to you.

32:02.440 --> 32:05.440
You do it behind the scenes on your app fabric with parallelism.

32:05.440 --> 32:10.440
You send me the results because right now there's a significant engineering challenge for me to do that.

32:10.440 --> 32:15.440
But I think that the next generation of application platforms will be doing something like that.

32:15.440 --> 32:17.440
There'll be a fabric on top of Co-Hit.

32:17.440 --> 32:23.440
So are you planning anything to kind of make my life easier to do that?

32:23.440 --> 32:25.440
That's so cool.

32:25.440 --> 32:27.440
I hadn't actually thought about that.

32:27.440 --> 32:28.440
Sorry.

32:28.440 --> 32:31.440
I hadn't actually thought about that.

32:31.440 --> 32:39.440
Like basically compiling a graph of chained prompts to Co-Hit and shipping it over.

32:39.440 --> 32:42.440
That's fascinating.

32:42.440 --> 32:49.440
I guess what you gain is the two-way network cost, right?

32:49.440 --> 32:51.440
Like that's what you're saving.

32:51.440 --> 32:55.440
And network costs tend to be quite small in practice.

32:55.440 --> 33:00.440
So depending on how big your prompt is and how many tokens you're generating,

33:00.440 --> 33:06.440
I could see it not giving you a huge amount of lift.

33:06.440 --> 33:14.440
But yeah, I'd love to look at your use case and kind of understand

33:14.440 --> 33:20.440
is the issue actually with the fact that you're having to make multiple network requests in sequence?

33:20.440 --> 33:23.440
Or is it that the underlying model itself is too small?

33:23.440 --> 33:28.440
And what you need is actually just speed-ups to that core model.

33:28.440 --> 33:34.440
Another thing that Co-Hit does is we'll do specific deployments for customers.

33:34.440 --> 33:37.440
So some folks have very low latency requirements.

33:37.440 --> 33:43.440
And we can split up our model across more nodes, more GPUs.

33:43.440 --> 33:47.440
And that makes the latency go way, way down.

33:47.440 --> 33:52.440
So if you were looking for like a 6x speed-up, a 4x speed-up in latency,

33:52.440 --> 33:56.440
that's one easy way to do it.

33:56.440 --> 33:59.440
Yeah, I think we're only just scratching the surface of what's possible here.

33:59.440 --> 34:05.440
So if I did this, what I just spoke about, I would need to create a workflow engine.

34:05.440 --> 34:10.440
I would need to do all sorts of DAG optimization and parallelism.

34:10.440 --> 34:14.440
And I might be able to cache certain steps.

34:14.440 --> 34:18.440
And certain steps I might be able to do some metamachine learning to optimize the performance.

34:18.440 --> 34:22.440
That needs a large one. That needs a small one.

34:22.440 --> 34:26.440
There's a whole kind of universe I think that we can explore here,

34:26.440 --> 34:29.440
if you know, just going one level of abstraction higher.

34:29.440 --> 34:32.440
You know, because the next startup founders will be building platforms

34:32.440 --> 34:39.440
that essentially compose human knowledge on top of these large language models in some kind of a fabric.

34:39.440 --> 34:43.440
So I think that's what excites me at the moment.

34:43.440 --> 34:45.440
That's really cool. That's really cool.

34:45.440 --> 34:51.440
Okay, so you express this graph of like prompts chaining into each other.

34:51.440 --> 34:56.440
You might have loops and conditions and you compile that,

34:56.440 --> 35:00.440
you ship it to the large language model provider, they perform optimizations,

35:00.440 --> 35:03.440
they handle all the parallelism, all the conditions.

35:03.440 --> 35:06.440
That's a really fascinating product idea. I like that.

35:06.440 --> 35:09.440
Yeah, please build it so I don't have to.

35:09.440 --> 35:13.440
Anyway, I wanted to come on to some more LLM discussion.

35:13.440 --> 35:19.440
So our friend, Francois Chouelet, he says that large language models are a bit like databases, right?

35:19.440 --> 35:23.440
But to me, they seem like so much more than that, right?

35:23.440 --> 35:26.440
So I think Francois was saying they are the representation, you know,

35:26.440 --> 35:32.440
equivalent to the B tree structure in a database, but I think they're also the database engine as well.

35:32.440 --> 35:34.440
But it's a new type of database engine.

35:34.440 --> 35:37.440
We don't understand how this database engine works.

35:37.440 --> 35:38.440
It's kind of unintelligible to us.

35:38.440 --> 35:47.440
But do you think that's a good analogy or like how do you mentally kind of think about what's going on in a language model?

35:47.440 --> 35:52.440
Yeah, I like the analogy. I like yours a bit better.

35:52.440 --> 36:05.440
I think my own, I kind of view just a raw large language model trained on the web as the next iteration of a search engine

36:05.440 --> 36:14.440
instead of explicitly retrieving results and references out into the web.

36:14.440 --> 36:17.440
So I like to use the analogy of like search engines.

36:17.440 --> 36:24.440
And instead of like explicit hard references where you make a query, you get back a link out to some site.

36:24.440 --> 36:27.440
A language model, it's kind of more like a dialogue.

36:27.440 --> 36:29.440
You can extract information from it.

36:29.440 --> 36:33.440
It has all of this knowledge that it's seen throughout the web.

36:33.440 --> 36:37.440
It's like a soft version of a search engine.

36:37.440 --> 36:44.440
And the mode of discovery is still language is just a much more natural interface.

36:44.440 --> 36:51.440
It's like a conversation as opposed to what we type into Google, which is kind of its own language, right?

36:51.440 --> 36:53.440
Like we don't query in the same way.

36:53.440 --> 36:58.440
We would ask our teacher, our Calc teacher about a theorem.

36:58.440 --> 37:01.440
We write in a very strange way.

37:01.440 --> 37:09.440
I think language models are a much more natural modality to the corpus of human knowledge.

37:09.440 --> 37:13.440
It's much more intuitive, natural, seamless.

37:13.440 --> 37:16.440
The problem is they hallucinate a lot.

37:16.440 --> 37:18.440
And so they'll fill in gaps.

37:18.440 --> 37:19.440
They're compressors, right?

37:19.440 --> 37:25.440
And so they see the web and they try to compress that down into their parameters and they lose little bits and pieces.

37:25.440 --> 37:33.440
And then when they have to regurgitate it, reconstruct it, they'll just fill in a plausible answer inside those gaps.

37:33.440 --> 37:40.440
And so the other reason I'm really excited about retrieval models is that they ground them more in reality.

37:40.440 --> 37:47.440
You put less burden on the model's parameters to memorize every single fact that's out there.

37:47.440 --> 37:51.440
And you instead just have them do the distillation process.

37:51.440 --> 37:57.440
So the model makes a query out to this knowledge base, this database of true facts.

37:57.440 --> 37:59.440
It pulls back some references.

37:59.440 --> 38:04.440
And then the model is just asked, hey, given these references, answer my question.

38:04.440 --> 38:14.440
And so it can actually make reference to true facts instead of having to hallucinate and imagine its own facts.

38:14.440 --> 38:20.440
So yeah, my kind of like the analogy that I like to build off of is this is the next search engine.

38:20.440 --> 38:22.440
It's the next interface to the internet.

38:22.440 --> 38:26.440
It's the next interface to human knowledge.

38:26.440 --> 38:29.440
It's funny you say that it might actually be the next search engine soon.

38:29.440 --> 38:33.440
I think search engines are about to be revolutionized.

38:33.440 --> 38:37.440
But yeah, I was going to ask you about the next frontier of large language models.

38:37.440 --> 38:39.440
And we've already been speaking about some of it.

38:39.440 --> 38:48.440
We've been talking about the integration of information retrieval, which will ground the responses, you know, based on actual things which exist in your operational systems.

38:48.440 --> 38:58.440
We just spoke before about hierarchical compositions, having this exciting, you know, computational graph, maybe some different architecture types or interaction patterns as well.

38:58.440 --> 39:03.440
I mean, you were just saying that there's this back and forth interaction, which I think is really interesting.

39:03.440 --> 39:11.440
On Google at the moment, you just type in a search and you press search once, you're not iteratively refining reflexively, recursively.

39:11.440 --> 39:15.440
Multimodality was another thing I thought of, you know, maybe having language in and text out.

39:15.440 --> 39:23.440
But I mean, those are a few examples, but does anything come top of mind to you for the next frontier?

39:23.440 --> 39:32.440
Yeah, like in addition to what you just listed, I think the ability to keep state over a long term horizon, like my dream for Coheir

39:32.440 --> 39:36.440
is that it knows its users.

39:36.440 --> 39:40.440
It can see into the past of all the past interactions with each one of these users.

39:40.440 --> 39:42.440
It knows their preferences.

39:42.440 --> 39:47.440
It knows about the user and the way that they like to interact with this model.

39:47.440 --> 39:51.440
And to do that, it can't be transactional.

39:51.440 --> 40:00.440
It can't just be like text in, text out, you know, forget everything else.

40:00.440 --> 40:05.440
In the history, we need to be able to like maintain a state between the model and its user.

40:05.440 --> 40:08.440
We need to be able to keep a record of that.

40:08.440 --> 40:17.440
I was talking about with retro, the ability to add into that corpus, keep notes, kind of like maintain an internal dialogue.

40:17.440 --> 40:29.440
We've seen how useful that is with like the scratch pad techniques that people have been working on, just giving the model space to write out its thoughts before giving an answer.

40:29.440 --> 40:36.440
I think we need the ability to store that forever and to have that referenceable down the line in the future.

40:36.440 --> 40:39.440
So, yeah, I think that's one of the most exciting ones.

40:39.440 --> 40:44.440
And then of course, there's multi-modality, which is blowing up now.

40:44.440 --> 40:47.440
Again, it's like another form of grounding.

40:47.440 --> 40:55.440
It ties the model into the real world, into the physical reality of our world.

40:55.440 --> 40:57.440
I think that's super exciting.

40:57.440 --> 41:03.440
So, yeah, I think the ability to reference external knowledge bases contribute to its own knowledge base.

41:03.440 --> 41:09.440
The ability to understand the world not just through text, but through audio, video, images.

41:09.440 --> 41:16.440
And then lastly, augmenting large language models with the ability to use tools.

41:16.440 --> 41:21.440
Obviously, like, we built this world for ourselves.

41:21.440 --> 41:24.440
We built all these little things to integrate with us.

41:24.440 --> 41:27.440
And one of our primary modalities is language.

41:27.440 --> 41:31.440
And so a lot of our tools are language-driven.

41:31.440 --> 41:37.440
If you think about web browsing, it's mostly you reading text and clicking links and following.

41:37.440 --> 41:39.440
It's very text-driven.

41:39.440 --> 41:48.440
And so something I'm super excited about is, okay, well, we have these language models which have pretty good grasp of language.

41:48.440 --> 41:52.440
They seem to have some modest level of understanding.

41:52.440 --> 41:59.440
Can we actually get them to use the tools that we built for us humans, stuff like web browsers?

41:59.440 --> 42:04.440
And the results we're seeing there are just super exciting, super exciting.

42:04.440 --> 42:05.440
Cool.

42:05.440 --> 42:09.440
I wanted to ask you about different modes of understanding.

42:09.440 --> 42:13.440
I'm actually making a video at the moment on the Chinese rim argument.

42:14.440 --> 42:18.440
They say that shortcut learning is an often discussed characteristic of machine learning.

42:18.440 --> 42:23.440
So when a system achieves human-like performance on a benchmark by the slight of hand,

42:23.440 --> 42:30.440
if you like, of spurious correlations in the data, rather than what we intuit to be human-like comprehension.

42:30.440 --> 42:35.440
Now, large language models have learned this very intricate statistical correlation,

42:35.440 --> 42:39.440
which allows near-perfect performance in lieu of human cognition.

42:39.440 --> 42:46.440
And it's possible that evaluating these algorithms against standards designed to gauge human cognition might be barking up the wrong tree.

42:46.440 --> 42:51.440
Now, human comprehension is difficult to pin down exactly what it means,

42:51.440 --> 42:57.440
but it doesn't seem to be entirely reflected in large language models, not always in how they behave at least.

42:57.440 --> 43:03.440
Now, for humans, it's not always enough to know the statistical features of linguistic symbols.

43:03.440 --> 43:09.440
We also need to grasp the underlying ideas and contexts in which those symbols convey,

43:09.440 --> 43:14.440
while language models can pick up on these tiny statistical patterns that we never could.

43:14.440 --> 43:22.440
Melanie Mitchell said in her most recent paper that recent years in AI have produced machines with new modes of knowledge.

43:22.440 --> 43:26.440
So in the same way that various animals are better suited to different settings,

43:26.440 --> 43:32.440
so too will our intelligent systems be, you know, more adapted to various issues.

43:32.440 --> 43:39.440
She said that large-scale statistical models will continue to be favored for matters requiring these vast amounts of historically encoded knowledge

43:39.440 --> 43:46.440
and where performance is paramount, and human intelligence might be favored for problems where we have very limited knowledge

43:46.440 --> 43:48.440
and we have strong causal mechanisms.

43:48.440 --> 43:53.440
So, you know, my question to you basically is what is the difference between a machine learning algorithm

43:53.440 --> 43:59.440
which has learned these very intricate statistical correlations and a human which has developed a more lifelike comprehension?

43:59.440 --> 44:05.440
I think that the first and shortest answer is probably the objective function, right?

44:05.440 --> 44:11.440
Like the context that we evolved in is very, very different than the context that we're training these models in.

44:11.440 --> 44:15.440
Certainly, like humans have a model of language.

44:15.440 --> 44:19.440
We're doing language modeling for sure, but we have to do a lot on top of that.

44:19.440 --> 44:29.440
And that's just one component that we rely on in order to achieve our objective and go through life and procreate.

44:29.440 --> 44:34.440
I think for these language models, they're given just that one piece of it.

44:34.440 --> 44:39.440
And there's a lot within that one piece, just within language modeling, like you're forced to learn a ton of facts,

44:39.440 --> 44:42.440
you're forced to learn a ton of patterns.

44:43.440 --> 44:53.440
And so, it is quite extraordinary that all of this amazing behavior falls out of just the language modeling problem.

44:53.440 --> 44:55.440
But it's also not, right?

44:55.440 --> 45:01.440
Because in order to accurately model language in a generalizable way, you're forced to pick up stuff like,

45:01.440 --> 45:04.440
how do I translate? How do I classify stuff?

45:04.440 --> 45:07.440
Because that's represented in the data.

45:07.440 --> 45:12.440
But when you're asking questions about how come language models get this thing wrong,

45:12.440 --> 45:17.440
which humans find super intuitive or it's so obvious, so logical,

45:17.440 --> 45:19.440
the contexts are completely different, right?

45:19.440 --> 45:21.440
We didn't evolve to just be a language model.

45:21.440 --> 45:28.440
I think that anyone who's claiming that has a high burden of proof.

45:29.440 --> 45:37.440
And it also points back to what we were saying earlier around, is this the last architecture?

45:37.440 --> 45:40.440
Is it just this plus scale?

45:40.440 --> 45:45.440
There's more that needs to be there and also in the objective function, right?

45:45.440 --> 45:48.440
We can't just do language modeling all the way.

45:48.440 --> 45:51.440
I think language modeling will get us very far.

45:51.440 --> 45:57.440
And I think that we can take language models and augment them with these very useful additional components.

45:57.440 --> 46:01.440
We can give them access to tools.

46:01.440 --> 46:09.440
But fundamentally, if you're looking for something that's human-like and acts and behaves like us,

46:09.440 --> 46:14.440
there's an objective function change that needs to happen.

46:14.440 --> 46:16.440
It can't just be language modeling.

46:16.440 --> 46:18.440
There needs to be something more.

46:18.440 --> 46:22.440
Yeah, we all anthropomorphize AI to different degrees, I think.

46:22.440 --> 46:27.440
I mean, what I took from Melanie is that she thinks that language models are not anthropomorphic,

46:27.440 --> 46:29.440
that it's a completely different mode.

46:29.440 --> 46:35.440
But do you think that humans do think like language models and we have some other apparatus on the top,

46:35.440 --> 46:39.440
or do you kind of agree with Melanie that they're completely different?

46:39.440 --> 46:41.440
I don't think they're completely different.

46:41.440 --> 46:48.440
I would not agree that they're categorically different.

46:48.440 --> 46:53.440
I think they're categorically our brains and modeling and statistical models.

46:53.440 --> 46:59.440
I think that they overlap very, very heavily.

46:59.440 --> 47:03.440
Again, I think the objective function is just completely different.

47:03.440 --> 47:09.440
I'm a big believer that the real value of AI models exists as a kind of entangled form of interactive,

47:09.440 --> 47:13.440
creative pairing between a human brain and a model.

47:13.440 --> 47:17.440
So David Chalmers coined the term extended mind to talk about us and phones.

47:17.440 --> 47:22.440
And I think actually large language models, that's the real extended mind

47:22.440 --> 47:25.440
because I feel this when I play with large language models.

47:25.440 --> 47:31.440
I feel that it's truly intelligent in combination or in tandem with me.

47:31.440 --> 47:36.440
So large language models, I don't have agency or intentionality, but we do,

47:36.440 --> 47:40.440
and we can use them and something very interesting emerges from that.

47:40.440 --> 47:44.440
As an example, I want to build a virtual assistant that comes with me everywhere.

47:44.440 --> 47:49.440
It's part of my everyday experience and I'm just in tandem with a large language model

47:49.440 --> 47:56.440
producing all sorts of creative thoughts that are embodied in the environment I'm in.

47:56.440 --> 48:00.440
That, I think, is an extension of my intelligence.

48:00.440 --> 48:03.440
Yeah, I 100% agree.

48:03.440 --> 48:12.440
I'm thinking about what happened with search and the outsourcing of knowledge.

48:12.440 --> 48:13.440
Not even that long ago.

48:13.440 --> 48:15.440
Like pre-Google, right?

48:15.440 --> 48:17.440
We had to memorize a lot of stuff.

48:17.440 --> 48:19.440
It had to be digested.

48:19.440 --> 48:22.440
Some stuff we could outsource into books and that type of thing

48:22.440 --> 48:29.440
and we would retrieve them very slowly and very painfully as needed.

48:29.440 --> 48:32.440
And then came the search engine and mobile phones

48:32.440 --> 48:35.440
and at any moment, any piece of information that you need,

48:35.440 --> 48:40.440
you have a source to go get it.

48:40.440 --> 48:44.440
And so we took that taxing on our neurons,

48:44.440 --> 48:49.440
taxing on our time, activity of having to pull a book off the shelf

48:49.440 --> 48:52.440
and we turned it into like an instantaneous thing.

48:52.440 --> 49:00.440
And so it freed up our minds, freed up our time to do a lot more high impact activities.

49:00.440 --> 49:05.440
And we've been significantly more productive as a product.

49:05.440 --> 49:12.440
And I think the next step is to continue to outsource things that we don't like doing,

49:12.440 --> 49:16.440
things that are taxing, they're time consuming,

49:16.440 --> 49:22.440
that we put off, that we don't like to hand more of that work over to the language model,

49:22.440 --> 49:27.440
over to these ML systems, AI systems,

49:27.440 --> 49:31.440
and to free ourselves up to just focus on the things that humans care about,

49:32.440 --> 49:35.440
the things that we're best at.

49:35.440 --> 49:38.440
So I think it's another complete transformation.

49:38.440 --> 49:43.440
It just turns out that you can hand way, way more than just the knowledge over to a language model.

49:43.440 --> 49:46.440
You can hand entire activities.

49:46.440 --> 49:49.440
Yeah, that's what I'm excited about.

49:49.440 --> 49:52.440
I like your idea of like a personal assistant coming around with you.

49:52.440 --> 49:54.440
You can give it an instructor,

49:54.440 --> 49:58.440
oh yeah, I need to buy body wash and it shows up at your door tomorrow.

49:58.440 --> 49:59.440
I'm building it.

49:59.440 --> 50:00.440
I'm building it.

50:00.440 --> 50:01.440
You're building it.

50:01.440 --> 50:05.440
I haven't done that topic for another day.

50:05.440 --> 50:07.440
Quickfire question.

50:07.440 --> 50:11.440
We spoke about language being the next interface for application development.

50:11.440 --> 50:14.440
So I'm really excited about this possible future.

50:14.440 --> 50:15.440
I'm already building it.

50:15.440 --> 50:22.440
But there's also a bit of a panacea element to it because the sales pitch is that

50:22.440 --> 50:26.440
even my community, my users could become software developers.

50:26.440 --> 50:28.440
They could actually build my platform.

50:28.440 --> 50:34.440
My platform becomes kind of like amorphous because it's so distributed.

50:34.440 --> 50:39.440
But the worry is that we'll get into this situation where we might feel we need to reinvent code

50:39.440 --> 50:45.440
because the prompts might become so complex that you need the equivalent of lawyers to understand them.

50:45.440 --> 50:48.440
Do you think about that concern?

50:48.440 --> 50:55.440
I have not because Coher's whole point is to abstract that away

50:55.440 --> 50:58.440
and to try and make things as simple to use as possible

50:58.440 --> 51:05.440
and to come up with canonical standards, abstractions even above prompting

51:05.440 --> 51:08.440
that are easier to use so that it pushes out further.

51:08.440 --> 51:14.440
It becomes less of a dark art or becomes less complex.

51:14.440 --> 51:18.440
I think in order for this stuff to proliferate, that's like a necessary condition.

51:18.440 --> 51:19.440
Things have to get easier.

51:19.440 --> 51:28.440
It can't be like you're speaking to an alien or you're having to divine a prompt that happens to work.

51:28.440 --> 51:33.440
So yeah, I think that's where the arrow of progress in this field points

51:33.440 --> 51:37.440
is towards more abstraction, easier to use, more intuitive interfaces.

51:37.440 --> 51:43.440
So I'm not too concerned about that because that's something that's very much top of mind.

51:44.440 --> 51:47.440
Aiden Gomez, it's been an absolute honor. Thank you for joining us.

51:47.440 --> 51:48.440
Likewise.

51:48.440 --> 51:50.440
Likewise. Thank you so much, Tim.

