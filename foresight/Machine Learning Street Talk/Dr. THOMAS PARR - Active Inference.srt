1
00:00:00,000 --> 00:00:01,440
So, welcome back to MLST.

2
00:00:01,440 --> 00:00:07,320
Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo,

3
00:00:07,320 --> 00:00:09,320
and Professor Carl Friston.

4
00:00:09,320 --> 00:00:15,280
Now the book is Active Inference, the free energy principle in mind, brain, and behavior.

5
00:00:15,280 --> 00:00:19,680
So the book, from a pedagogical perspective, it's describing active inference from the

6
00:00:19,680 --> 00:00:22,400
high road and the low road.

7
00:00:22,400 --> 00:00:26,920
And the high road is a little bit kind of helicopter view, so it's saying, OK, we've

8
00:00:26,920 --> 00:00:33,640
got these biological organisms or these living systems, and what do they do in order to be

9
00:00:33,640 --> 00:00:34,640
living systems?

10
00:00:34,640 --> 00:00:40,560
Well, they resist entropic forces acting on them by minimizing their free energy.

11
00:00:40,560 --> 00:00:44,800
So it goes into the how question, but it also goes into the why question from a helicopter

12
00:00:44,800 --> 00:00:45,800
view.

13
00:00:45,800 --> 00:00:51,080
The low road of active inference is far more mechanistic, far more mathematical, and obviously

14
00:00:51,080 --> 00:00:54,640
both all of the roads lead to Rome, if you like.

15
00:00:54,640 --> 00:00:58,200
But the low road is talking about things like Bayesian mechanics, there's a primer

16
00:00:58,200 --> 00:01:02,520
on probability theory, talking about things like variational inference, which is the way

17
00:01:02,520 --> 00:01:09,320
that we solve these intractable optimization problems in active inference, and also talking

18
00:01:09,320 --> 00:01:14,520
about framing active inference as a process theory, which is the latest incarnation of

19
00:01:14,520 --> 00:01:17,440
the description of active inference.

20
00:01:17,440 --> 00:01:20,840
So Professor Carl Friston wrote a preface for the book.

21
00:01:20,840 --> 00:01:25,040
He said, active inference is a way of understanding sentient behavior.

22
00:01:25,040 --> 00:01:31,760
The very fact that you are reading these lines means that you are engaging in active inference,

23
00:01:31,760 --> 00:01:38,520
namely actively sampling the world in a particular way, because you believe you will learn something.

24
00:01:38,520 --> 00:01:40,640
You are palpating.

25
00:01:40,640 --> 00:01:42,040
This is beautiful, by the way.

26
00:01:42,040 --> 00:01:47,280
Friston uses the most beautiful language, it's his signature, if you like, it's his

27
00:01:47,280 --> 00:01:48,520
calling card.

28
00:01:48,520 --> 00:01:53,040
He said, you are palpating this page with your eyes, simply because this is the kind

29
00:01:53,040 --> 00:01:58,480
of action that will resolve uncertainty about what you're going to do next, indeed what

30
00:01:58,480 --> 00:02:00,800
these words convey.

31
00:02:00,800 --> 00:02:07,480
In short, he said, active inference puts action into perception, whereby perception is treated

32
00:02:07,480 --> 00:02:11,120
as perceptual inference or hypothesis testing.

33
00:02:11,120 --> 00:02:16,680
Active inference goes even further and considers planning as inference, that is inferring what

34
00:02:16,680 --> 00:02:21,520
you're going to do next to resolve uncertainty about your lived world.

35
00:02:21,520 --> 00:02:26,440
So I'm about to show you a sneaky clip of Professor Friston that we filmed in January.

36
00:02:26,440 --> 00:02:31,120
I might publish the full show on MLSD in the future, but I just want to take this as an

37
00:02:31,120 --> 00:02:34,080
opportunity to thank you so much for all of our Patreon supporters.

38
00:02:34,080 --> 00:02:39,760
Honestly, it means so much to me because the last few months I've just been, you know,

39
00:02:39,760 --> 00:02:44,880
trying to make this activity of mine, this passion of mine, a full-time job.

40
00:02:44,880 --> 00:02:47,960
And it's not just because you love the show and you want to support me, you get early

41
00:02:47,960 --> 00:02:51,680
access to content, you can join our private Patreon Discord.

42
00:02:51,680 --> 00:02:55,920
We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you

43
00:02:55,920 --> 00:02:58,160
also get early access to lots of our content.

44
00:02:58,160 --> 00:02:59,760
So, you know, please check that out.

45
00:02:59,760 --> 00:03:04,440
But in the meantime, here's a little sneaky clip from Professor Friston.

46
00:03:04,440 --> 00:03:12,960
The neural network is a generative model of the way in which its content work was generated.

47
00:03:12,960 --> 00:03:19,080
And its only job is effectively to learn to be a good model of the content that it has

48
00:03:19,080 --> 00:03:22,120
to assimilate.

49
00:03:22,120 --> 00:03:25,280
If you put agency into the mix, you get to active inference.

50
00:03:25,280 --> 00:03:32,200
And now that we've got a generative model that now has to decide which data to go and

51
00:03:32,200 --> 00:03:33,200
solicit.

52
00:03:33,200 --> 00:03:38,000
And that's actually quite a key move and also quite a thematic move.

53
00:03:38,000 --> 00:03:40,120
So we're moving from perception machines.

54
00:03:40,120 --> 00:03:45,800
We're moving from sort of neural networks in the service of, say, face recognition into

55
00:03:45,800 --> 00:03:55,680
a much more natural science problem of how would you then choose which data in a smart

56
00:03:55,680 --> 00:04:03,040
way you go and solicit in order to build the best models of the causes of the data that

57
00:04:03,040 --> 00:04:05,240
you are in charge of gathering.

58
00:04:05,560 --> 00:04:10,480
Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging

59
00:04:10,480 --> 00:04:14,960
at the Queen Square Institute of Neurology at University College London and a practice

60
00:04:14,960 --> 00:04:15,960
in clinician.

61
00:04:15,960 --> 00:04:19,240
Now, one of the reviews from the book was from Andy Clark.

62
00:04:19,240 --> 00:04:21,080
He said, it should have been impossible.

63
00:04:21,080 --> 00:04:26,200
A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual

64
00:04:26,200 --> 00:04:31,280
landscape from the formal schemas and some of the neurobiology and then garnished with

65
00:04:31,280 --> 00:04:33,920
practical recipes for active model design.

66
00:04:33,920 --> 00:04:38,080
Philosophically astute and scientifically compelling, this book is essential reading

67
00:04:38,080 --> 00:04:41,400
for anyone interested in minds, brains and action.

68
00:04:41,400 --> 00:04:43,960
Well, I mean, thank you very much for having me on.

69
00:04:43,960 --> 00:04:45,920
So I'm Thomas Parr.

70
00:04:45,920 --> 00:04:51,760
I'm both a clinician and a theoretical neuroscientist.

71
00:04:51,760 --> 00:04:57,160
So I've been working in active inference for a number of years now since I did my PhD

72
00:04:57,160 --> 00:05:03,880
back in 2016 with Carl at the theoretical neurobiology group at Queen Square.

73
00:05:03,880 --> 00:05:10,760
And I'm now based at Oxford where I split my time between research and clinical practice.

74
00:05:10,760 --> 00:05:13,880
So tell me about the first time you met Carl.

75
00:05:13,880 --> 00:05:19,720
The first time I met Carl, I was considering, so I was a medical student at the time at

76
00:05:19,720 --> 00:05:23,600
UCL and I was considering doing a PhD.

77
00:05:23,600 --> 00:05:27,320
And I remember arranging to meet with him and obviously being a relatively nerve-wracking

78
00:05:27,320 --> 00:05:33,760
experience meeting one of the most famous neuroscientists in the world.

79
00:05:33,760 --> 00:05:37,360
I remember discussing with him about it and saying, you know, this is what I'm interested

80
00:05:37,360 --> 00:05:38,720
in.

81
00:05:38,720 --> 00:05:43,760
Would you consider supervising my PhD if I were to get the funding for it?

82
00:05:43,760 --> 00:05:48,840
And I remember he said, yes, all right, then, anything else.

83
00:05:48,840 --> 00:05:51,400
And I asked, do you want to see my CV or anything like that?

84
00:05:51,400 --> 00:05:53,880
And he said, no, I'll only forget it.

85
00:05:53,880 --> 00:05:54,880
Yes.

86
00:05:55,560 --> 00:05:59,000
That was my first encounter with Carl.

87
00:05:59,000 --> 00:06:05,040
But since then, he's always been immensely supportive and has been, you know, exactly

88
00:06:05,040 --> 00:06:11,360
the sort of mentor that I think anybody would want to be able to develop a skill set and

89
00:06:11,360 --> 00:06:12,760
sort of proceed in science.

90
00:06:12,760 --> 00:06:15,520
I come from a machine learning background.

91
00:06:15,520 --> 00:06:21,440
And since discovering active inference and Carl's work, it's really broadened my horizons.

92
00:06:21,480 --> 00:06:26,120
And at the moment, there's an obsession with things like chat GPT.

93
00:06:26,120 --> 00:06:32,120
And I just wondered in your own articulation, how would you kind of pose the work that you

94
00:06:32,120 --> 00:06:34,720
do in relation to that kind of technology?

95
00:06:34,720 --> 00:06:36,120
It's a good question.

96
00:06:36,120 --> 00:06:42,120
And I suppose there are many levels at which it could be answered, aren't there?

97
00:06:42,120 --> 00:06:46,560
I guess thinking about something like chat GPT in that style of technology, it's clearly

98
00:06:46,560 --> 00:06:50,440
been very, very effective at what it does.

99
00:06:50,440 --> 00:06:53,560
But it's worth thinking about what is it that it does?

100
00:06:53,560 --> 00:06:57,080
And I think chat GPT is an excellent example because so many people are familiar with it.

101
00:06:57,080 --> 00:07:03,880
It has such impressive results in terms of being able to simulate very effectively what

102
00:07:03,880 --> 00:07:06,960
it's like to have a conversation.

103
00:07:06,960 --> 00:07:11,880
But ultimately, it is like most deep learning architectures.

104
00:07:11,880 --> 00:07:13,960
It's a form of function approximation.

105
00:07:13,960 --> 00:07:22,120
It's a form of being able to capture very well the output that would be expected under

106
00:07:22,120 --> 00:07:26,000
some set of conditions given some input.

107
00:07:26,000 --> 00:07:32,040
So you give it some text and it knows which text to predict.

108
00:07:32,040 --> 00:07:33,040
And it's very good at that.

109
00:07:33,040 --> 00:07:35,360
But in a sense, that's where it stops.

110
00:07:35,360 --> 00:07:38,520
It doesn't necessarily do anything else.

111
00:07:38,520 --> 00:07:44,120
That's very different to what you and I do when we engage with the world around us, when

112
00:07:44,120 --> 00:07:48,040
we want to learn about the world around us, when we want to form our own beliefs about

113
00:07:48,040 --> 00:07:49,120
what's going on.

114
00:07:49,120 --> 00:07:53,040
And those are the things that I think it doesn't have in the same way.

115
00:07:53,040 --> 00:07:59,920
It certainly can't act and go and seek out specific exchanges, specific conversations

116
00:07:59,920 --> 00:08:02,360
that it might want to learn from.

117
00:08:02,360 --> 00:08:05,440
Whereas you or I might do that if we wanted to know about something specifically, we'd

118
00:08:05,440 --> 00:08:08,600
go and look for information about that thing.

119
00:08:08,600 --> 00:08:13,560
And I think that's where active inference and the idea of having a generative world

120
00:08:13,560 --> 00:08:18,560
model and understanding of what's there in your world that you can alter yourself, that

121
00:08:18,560 --> 00:08:24,240
you can change is very different to a lot of more passive artificial intelligence.

122
00:08:24,240 --> 00:08:28,640
Probably the point where things become closer is in fields like robotics, where you have

123
00:08:28,640 --> 00:08:30,080
to account for both of those things.

124
00:08:30,080 --> 00:08:35,840
You have to model a world that has yourself in it, where your actions affect the data

125
00:08:35,840 --> 00:08:36,840
that you get in.

126
00:08:36,840 --> 00:08:40,560
And I think that's probably where more of the convergence is likely to happen.

127
00:08:40,560 --> 00:08:41,560
Yes.

128
00:08:41,560 --> 00:08:48,640
So you're describing the difference, I guess, between an observational system and an interactive

129
00:08:48,640 --> 00:08:49,640
system.

130
00:08:49,640 --> 00:08:56,000
So in an interactive system, an agent can seek information and change or bend the environment

131
00:08:56,000 --> 00:08:57,520
to suit its will.

132
00:08:57,800 --> 00:09:03,040
Just to linger on this for a second, though, there are folks who do argue that neural networks

133
00:09:03,040 --> 00:09:07,560
are more than hash tables, because I think of them the same way you do.

134
00:09:07,560 --> 00:09:10,240
They essentially learn a function.

135
00:09:10,240 --> 00:09:15,320
And if you densely sample it enough, just like a hash table, it can go and retrieve what

136
00:09:15,320 --> 00:09:17,880
that function says given a certain input.

137
00:09:17,880 --> 00:09:23,000
But there are folks who say, no, no, no, these models learn a world model.

138
00:09:23,000 --> 00:09:29,120
So is given as an example, or with SORA, they say it's learned Navier stokes.

139
00:09:29,120 --> 00:09:30,600
It's a really good question.

140
00:09:30,600 --> 00:09:34,240
And I think there are some open questions here, and I wouldn't claim to have all the answers

141
00:09:34,240 --> 00:09:35,960
to this one.

142
00:09:35,960 --> 00:09:42,160
I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly

143
00:09:42,160 --> 00:09:44,880
it has captured something about the statistics of language.

144
00:09:44,880 --> 00:09:49,720
It's uncovered something about the hidden causes.

145
00:09:49,720 --> 00:09:54,240
So you could argue there is potentially an element of world modeling in there that is

146
00:09:54,240 --> 00:09:56,240
left implicit.

147
00:09:56,240 --> 00:10:01,280
I think it would be very difficult to pull that out or to sort of see that with any transparency

148
00:10:01,280 --> 00:10:03,480
with something like chat GPT.

149
00:10:03,480 --> 00:10:08,640
And so if it does have something of that sort, probably it's the methods that neuroscientists

150
00:10:08,640 --> 00:10:11,960
have been using for years to understand the brain that might help to try and pull out

151
00:10:11,960 --> 00:10:16,960
those same things in those sorts of architectures.

152
00:10:16,960 --> 00:10:21,920
Maybe some sorts of deep learning and neural network models are very good at picking up

153
00:10:21,920 --> 00:10:27,840
regularities in terms of dynamics as well and being able to predict trajectories.

154
00:10:27,840 --> 00:10:35,840
And I think it's important to say that describing something as a function approximator is not

155
00:10:35,840 --> 00:10:37,360
to criticize or belittle it.

156
00:10:37,360 --> 00:10:40,480
It's a very important thing to be able to do.

157
00:10:40,480 --> 00:10:44,200
And it may also be very important in certain types of inference.

158
00:10:44,200 --> 00:10:49,280
So for instance, things like variational autoencoders are based upon often deep learning

159
00:10:49,280 --> 00:10:51,600
neural network architectures.

160
00:10:51,600 --> 00:10:55,800
But the function that is learned is the one that maps from the data I've got coming in

161
00:10:55,800 --> 00:10:59,800
to the posterior beliefs or the parameters of the posterior beliefs that I would arrive

162
00:10:59,800 --> 00:11:05,040
at were I to perform inference of the sort we might do in active inference.

163
00:11:05,040 --> 00:11:10,280
So you've written an absolutely beautiful book on active inference.

164
00:11:10,360 --> 00:11:18,800
And active inference, in my view, it's a theory of agency, which is to say it describes what

165
00:11:18,800 --> 00:11:19,800
an agent does.

166
00:11:19,800 --> 00:11:21,120
And I'm fascinated by agency.

167
00:11:21,120 --> 00:11:24,760
But could you just start by, I mean, from your perspective, could you introduce the

168
00:11:24,760 --> 00:11:27,080
book and tell us about your experience writing it?

169
00:11:27,080 --> 00:11:28,080
Of course.

170
00:11:28,080 --> 00:11:33,400
So the active inference book that we've written is a collaboration between myself, Giovanni

171
00:11:33,400 --> 00:11:38,840
Pazzullo is based in Rome and Carl Friston, who has to take credit for development of

172
00:11:38,840 --> 00:11:42,080
active inference in the first place.

173
00:11:42,080 --> 00:11:51,520
And the book sort of rose out of our sense that there wasn't a unified book out there

174
00:11:51,520 --> 00:11:56,200
or a resource out there to help people learn about what is ultimately a very interdisciplinary

175
00:11:56,200 --> 00:11:58,120
field.

176
00:11:58,120 --> 00:12:02,800
And so we've all had experience with students coming to us asking for resources, asking

177
00:12:02,800 --> 00:12:04,200
what they need to read.

178
00:12:04,200 --> 00:12:08,120
And it may be we refer them to a little bit of neuroscience work, a little bit of machine

179
00:12:08,400 --> 00:12:14,440
learning, textbooks or specific pages on variational inference or whatever else, giving

180
00:12:14,440 --> 00:12:19,840
people introductions to or places they can learn about the maths they need to be able

181
00:12:19,840 --> 00:12:20,840
to do it.

182
00:12:20,840 --> 00:12:24,840
But then also the biology, the underlying psychology, the long sort of tradition of

183
00:12:24,840 --> 00:12:29,320
previous scientists who worked in related areas.

184
00:12:29,320 --> 00:12:32,520
And so the book was an attempt to try and provide a place that people could find all

185
00:12:32,520 --> 00:12:36,840
of that, or at least references to all the relevant things they needed for that, to stick

186
00:12:36,920 --> 00:12:42,120
to the same sort of notation, which is one of the things that's often very difficult,

187
00:12:42,120 --> 00:12:47,800
and the same formalisms and try and introduce everything in a very systematic way to people.

188
00:12:47,800 --> 00:12:54,920
So I'm pleased here that you found it useful, and I hope other people will as well.

189
00:12:54,920 --> 00:13:01,880
The experience of writing it, I mean, so that took place over several years, partly because

190
00:13:01,880 --> 00:13:05,240
the pandemic got in the way in the middle.

191
00:13:05,240 --> 00:13:10,600
So Giovanni and I were passing notes between one another over email and weren't able to

192
00:13:10,600 --> 00:13:13,000
sort of meet in person to discuss it during that time period.

193
00:13:15,160 --> 00:13:21,320
But I think we're all quite proud of the result that we've got out of that, and people seem

194
00:13:21,320 --> 00:13:22,920
to have responded quite well to it.

195
00:13:22,920 --> 00:13:28,200
You start off by talking about what you call a high road and a low road to active inference.

196
00:13:28,200 --> 00:13:29,400
Can you sketch that out?

197
00:13:30,280 --> 00:13:36,920
Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it,

198
00:13:36,920 --> 00:13:39,640
because as I say, it's very multidisciplinary.

199
00:13:39,640 --> 00:13:43,640
There are lots of ways into active inference, and one of the things that's most difficult

200
00:13:43,640 --> 00:13:47,320
for people who are getting into the field for the first time is knowing where to start.

201
00:13:47,320 --> 00:13:53,080
Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas

202
00:13:53,080 --> 00:13:59,080
like that, or do they start from a physics-based perspective and start working their way towards

203
00:13:59,800 --> 00:14:01,560
something that looks like sentience?

204
00:14:02,600 --> 00:14:06,120
And there are lots of different, lots of alternative ways people get into it.

205
00:14:06,120 --> 00:14:09,800
The fact that you become interested via machine learning, the fact that other people

206
00:14:09,800 --> 00:14:14,360
have become interested through biology, I developed an interest through neuroscience

207
00:14:15,720 --> 00:14:16,920
while I was at medical school.

208
00:14:19,240 --> 00:14:21,960
And the high road and the low road was a way of just trying to acknowledge

209
00:14:22,760 --> 00:14:28,520
that difference or that difficulty of knowing where to begin, and saying that that's okay,

210
00:14:28,600 --> 00:14:32,120
there are lots of different roads, but they ultimately end up in the same place.

211
00:14:33,080 --> 00:14:37,640
The idea of the low road was to say, well, let's just take observations and psychology

212
00:14:37,640 --> 00:14:40,760
sort of development of a number of ideas that are built up over time

213
00:14:42,440 --> 00:14:47,320
that come to the idea that we're using internal models to explain our world,

214
00:14:47,320 --> 00:14:52,360
that the brain is using something like Bayesian inference, or at least can be described as using

215
00:14:52,360 --> 00:14:59,400
Bayesian inference, and go from there through the advances that lead you to active inference,

216
00:14:59,400 --> 00:15:04,520
the idea that it's not just a passive process that you're also inferring what I'm going to do.

217
00:15:05,640 --> 00:15:10,040
And furthermore, that when we're doing inference, we're changing our beliefs

218
00:15:10,760 --> 00:15:14,840
to reflect what's in the world around us and to explain our sensory data.

219
00:15:15,480 --> 00:15:18,840
But actually, when we're acting in the world, we can also change the world itself

220
00:15:18,920 --> 00:15:25,400
to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the

221
00:15:25,400 --> 00:15:31,080
world to also changing the world to reflect our beliefs. And that fascinating move that

222
00:15:31,080 --> 00:15:35,720
actually both can be seen as optimization of exactly the same objective that they have the

223
00:15:35,720 --> 00:15:41,800
same goal, that in both cases, it's really just improving the fit between us and our world.

224
00:15:43,000 --> 00:15:47,720
So that's the sort of low road perspective. The high road perspective was the idea of saying,

225
00:15:48,360 --> 00:15:52,920
well, let's start from the minimum number of assumptions we can, let's start from first

226
00:15:52,920 --> 00:16:00,360
principles. And that takes a much more physics based approach. It says, if you have a creature

227
00:16:00,360 --> 00:16:06,360
that is interacting with its world, then there are a number of things you've already committed to,

228
00:16:06,360 --> 00:16:11,000
and that includes things like the persistence of that creature over a reasonable length of time,

229
00:16:11,000 --> 00:16:14,920
the maintenance of a boundary between that creature and its world, and that sort of

230
00:16:14,920 --> 00:16:21,880
self world distinction. And once you've committed to those things, you can then start to write down

231
00:16:21,880 --> 00:16:26,520
the constraints that those imply in terms of the physical dynamics of that system.

232
00:16:26,520 --> 00:16:30,760
And you can start to interpret those dynamics in terms of the functions they might be

233
00:16:30,760 --> 00:16:36,920
optimizing, much like, much like if you were to write down the equations that underpin Newtonian

234
00:16:36,920 --> 00:16:41,880
dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's

235
00:16:41,880 --> 00:16:46,600
following the same sort of logic to then get to flows on free energy functions, where free energy

236
00:16:46,600 --> 00:16:52,120
is just a measure of that fit between us and our world. And so both roads ultimately end up leading

237
00:16:52,120 --> 00:17:00,600
to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we

238
00:17:00,600 --> 00:17:07,160
live in, we have to be able to change our beliefs, to reflect what's going on around us and change

239
00:17:07,240 --> 00:17:14,840
the world through our dynamical flows on a free energy functional, to best fit with the sorts

240
00:17:14,840 --> 00:17:19,400
of creatures we are. When we first started looking at the free energy principle, we were talking

241
00:17:19,400 --> 00:17:27,400
about things. It was known as a theory of every thing, every space thing, which is to say, roughly

242
00:17:27,400 --> 00:17:35,160
speaking, if a thing exists, what must the thing do to continue to exist? And just their continued

243
00:17:35,160 --> 00:17:41,640
existence resisting entropic forces is what defines them, which gets us into the second law

244
00:17:41,640 --> 00:17:46,280
of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to

245
00:17:46,280 --> 00:17:51,640
resist entropic forces? And I think there's a development in how a lot of these ideas are

246
00:17:51,640 --> 00:17:57,880
presented over time, which you expect and hope for in science. And I think we've often taken

247
00:17:57,880 --> 00:18:03,800
different perspectives at different points in time as to how we explain these ideas.

248
00:18:04,760 --> 00:18:11,000
And resisting entropic forces is an idea that I think most people find relatively intuitive.

249
00:18:11,640 --> 00:18:19,960
So the idea that the physical systems will tend to increase their entropy over time,

250
00:18:21,640 --> 00:18:28,200
at least close systems, so that over time things will gradually dissipate, things that are highly

251
00:18:28,200 --> 00:18:32,200
structured and highly ordered and can only exist in a very small number of configurations and

252
00:18:32,280 --> 00:18:37,240
more likely to change into something that can exist in many different configurations than they

253
00:18:37,240 --> 00:18:43,640
are to go in the opposite direction. But anything that persists over time and maintains its form

254
00:18:43,640 --> 00:18:49,640
clearly resists that process of decay, at least to some extent, or at least for some period of time.

255
00:18:51,320 --> 00:18:56,440
However, the opposite is also true. We're also not creatures that tend towards a zero entropy

256
00:18:56,440 --> 00:19:02,840
state. We don't end up in a single configuration. We have to be flexible. We have to change in

257
00:19:02,840 --> 00:19:11,320
various ways throughout our lifetime or even throughout our daily routine. So it's not quite

258
00:19:11,320 --> 00:19:17,960
as simple as just saying you have to resist entropic change. It's more to say that entropic

259
00:19:17,960 --> 00:19:22,760
change or the amount of entropy that you expect to develop has to be bounded both from above

260
00:19:22,760 --> 00:19:28,600
and below, that there is a sort of optimum level to be at. And that optimum probably varies from

261
00:19:28,600 --> 00:19:33,560
different, well, from person to person, from creature to creature, from thing to thing.

262
00:19:34,120 --> 00:19:39,240
You could imagine a rock that doesn't need to do much. Its interface with the environment is

263
00:19:39,240 --> 00:19:47,160
quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist,

264
00:19:47,160 --> 00:19:51,080
we have many more ways of interfacing with the environment and we need to plan

265
00:19:51,080 --> 00:19:56,520
many more steps ahead. So is that just a pure continuum between rocks and people?

266
00:19:59,160 --> 00:20:07,880
I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy

267
00:20:07,880 --> 00:20:11,720
will depend very much on what you are. And as you say, you could imagine a whole scale of

268
00:20:11,720 --> 00:20:17,960
things in between. I mean, in a way that as you've highlighted with the rock,

269
00:20:17,960 --> 00:20:21,560
some of the most boring things are the things with the, sorry, I shouldn't say that,

270
00:20:21,560 --> 00:20:28,600
poor geologists who might find rocks very interesting. And I'm sure are very complex, but

271
00:20:30,760 --> 00:20:35,720
from a sort of behavioral perspective, clearly things like us are much more interesting to study

272
00:20:35,720 --> 00:20:40,600
than things like a rock. And part of that is that we actually have a higher degree of entropy in

273
00:20:40,600 --> 00:20:47,640
how we live our daily lives compared to things like, I almost said organisms like rocks,

274
00:20:47,640 --> 00:20:53,080
but things like rocks that are not behaving. The reason I was thinking about this is the

275
00:20:53,080 --> 00:20:58,520
second law of thermodynamics was conceived, I don't know, 150 years ago or something like that.

276
00:20:58,520 --> 00:21:03,720
And many people at the time thought that it was an affront on free will. I think the religious

277
00:21:03,720 --> 00:21:09,560
people at the time were aghast at the idea that things were mapped out in this way.

278
00:21:09,560 --> 00:21:15,560
It's always worth saying in this discussion that obviously the tendency for entropy to increase

279
00:21:15,560 --> 00:21:22,520
from a physical perspective generally relates to closed systems of which we are not. And as soon

280
00:21:22,520 --> 00:21:27,640
as you start talking about different compartments and interactions between them, you also introduce

281
00:21:27,640 --> 00:21:33,560
the idea of several coupled systems. And so you can start to ask questions about the overall entropy

282
00:21:33,560 --> 00:21:41,720
or the entropy of specific parts of that system. And agents and worlds are two compartments and

283
00:21:41,720 --> 00:21:47,160
systems that exchange things with one another. And so are not closed systems almost by definition

284
00:21:47,160 --> 00:21:53,000
that a closed system, again, from a sort of neuroscience standpoint is not necessarily

285
00:21:53,000 --> 00:22:00,840
a very interesting system. So probably that deals with a large part of that. The question of free

286
00:22:00,840 --> 00:22:05,720
will is always an interesting one and always a thorny one that I'm not going to claim to have any

287
00:22:05,800 --> 00:22:12,920
expertise on or be able to answer. But I think it probably tackles a slightly

288
00:22:12,920 --> 00:22:19,400
different thing from a cognitive science perspective, which is whether or not we believe

289
00:22:19,400 --> 00:22:24,920
that the actions we're taking are actions that we've chosen. And that probably comes back into

290
00:22:24,920 --> 00:22:30,440
another aspect of active inference, which is that idea that the way we're regulating our

291
00:22:30,440 --> 00:22:37,160
worlds, the way we're perhaps changing the entropy of our environment depends upon our own

292
00:22:38,680 --> 00:22:45,160
choices about it, our inferences about which one we're going to do next. And that feeds into things

293
00:22:45,160 --> 00:22:52,600
like we've spoken about free energy, that that quantity that we use to both choose our actions,

294
00:22:53,480 --> 00:22:58,040
an act in the world around us while also drawing inferences. But we can also talk about things

295
00:22:58,040 --> 00:23:03,640
like expected free energy, which is a way of evaluating our future state and what would be a

296
00:23:03,640 --> 00:23:09,400
good trajectory or a good way for the world to play out. And their entropy has a completely

297
00:23:09,400 --> 00:23:15,560
different meaning and there are different sorts of entropy. So for instance, if I were choosing

298
00:23:15,560 --> 00:23:19,240
between several different eye movements I could make while looking around this room,

299
00:23:21,320 --> 00:23:26,840
the best eye movements I might choose are those for which I'm least certain about what I would see.

300
00:23:26,840 --> 00:23:32,280
In other words, the highest entropy distribution. So once you start planning in the future and once

301
00:23:32,280 --> 00:23:36,920
you start selecting things to resolve your uncertainty and to be more confident about the

302
00:23:36,920 --> 00:23:42,680
world around you, you actually end up seeking out entropy, which it seems to then very much

303
00:23:42,680 --> 00:23:46,200
contradict some of the other ideas that we were talking about, the idea that we're constantly

304
00:23:46,200 --> 00:23:52,200
resisting it. But actually it's by seeking out the things that we're least certain about that we

305
00:23:52,200 --> 00:23:57,800
can start to resolve that uncertainty and start to become more confident and more certain about the

306
00:23:57,800 --> 00:24:02,920
world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what

307
00:24:02,920 --> 00:24:09,560
you were saying just a second ago about this description of how agents operate, it's very

308
00:24:09,560 --> 00:24:15,320
principled. We were talking about this balancing epistemic foraging versus sticking with what you

309
00:24:15,320 --> 00:24:23,080
know. And more broadly speaking, thinking of agency as this sophisticated cognition of

310
00:24:23,080 --> 00:24:27,400
having preferences and bending the environment and so on. And I guess where I was going before

311
00:24:27,400 --> 00:24:35,160
is it's tempting to think that this erodes free will. And I think of them quite adjacently in

312
00:24:35,160 --> 00:24:41,480
my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't

313
00:24:41,560 --> 00:24:49,800
matter that it's predetermined. For me, free will, I'll try not to use the word free will, but

314
00:24:49,800 --> 00:24:54,360
thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant.

315
00:24:54,360 --> 00:25:02,360
It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency

316
00:25:02,360 --> 00:25:07,960
is better to think of than free will, if that makes sense. Yeah. And I think that's probably right.

317
00:25:08,200 --> 00:25:12,600
And the experience of and the inference of agency as well, I think is part of that.

318
00:25:13,800 --> 00:25:17,800
There's a potential link that you can draw here also to the idea of chaotic,

319
00:25:17,800 --> 00:25:27,000
dynamical systems of which we essentially are examples. And the idea of chaos in that setting

320
00:25:27,000 --> 00:25:33,960
is that if you start from two ever so slightly different initial conditions, your path and your

321
00:25:33,960 --> 00:25:39,000
future may unfold in a completely different way. And I think that fits very nicely with what you're

322
00:25:39,000 --> 00:25:45,000
saying about distinguishing my agency from somebody else's because you don't see it as if I were,

323
00:25:45,800 --> 00:25:51,800
you know, the time going to behave in exactly the same way somebody else's. And part of the

324
00:25:51,800 --> 00:25:56,680
reason for that is that you end up starting from a slightly different perspective to where they are,

325
00:25:56,680 --> 00:25:59,880
and that might lead to wildly different futures for both of you.

326
00:26:00,440 --> 00:26:09,480
So something I think about a lot is whether agents are ontologically real or whether they

327
00:26:09,480 --> 00:26:14,760
are an instrumental fiction. And I think part of the complexity, especially with active inference

328
00:26:14,760 --> 00:26:20,920
and the free energy principle is this hierarchical nesting. So we can think of agents inside agents

329
00:26:20,920 --> 00:26:27,800
inside agents. And I guess the first question is, are they real and does it matter?

330
00:26:28,600 --> 00:26:30,840
Define real for me.

331
00:26:36,520 --> 00:26:46,600
Well, one argument would be that they are epiphenomenal, that they themselves don't affect

332
00:26:46,600 --> 00:26:50,520
the system that they are. Is this a good way to think about it?

333
00:26:53,960 --> 00:26:57,480
It is a very difficult question to try and contend with, isn't it? Because I think there

334
00:26:57,480 --> 00:27:02,680
are so many words that come up here that are kind of laden with different semantics or different

335
00:27:02,680 --> 00:27:09,960
meanings depending upon who you speak to and which camp they come from in the sort of philosophical

336
00:27:09,960 --> 00:27:18,360
world. And that's why I sort of asked you to define real. And it's really difficult to define

337
00:27:18,360 --> 00:27:24,840
what real means in that setting, isn't it? And I guess coming back to your original question there,

338
00:27:24,840 --> 00:27:29,960
for me, does it matter if they're a sort of real thing or not? Probably not. It matters

339
00:27:29,960 --> 00:27:35,960
whether it's useful. And I guess that sort of brings me to a point about one of the things I

340
00:27:35,960 --> 00:27:41,400
find quite appealing about active inference as a way of doing science. And I think,

341
00:27:43,720 --> 00:27:47,960
you know, having had an interest in things like neuroscience and psychology for some time,

342
00:27:47,960 --> 00:27:51,960
I often found it quite frustrating to understand what people meant and the different language

343
00:27:51,960 --> 00:27:55,800
they used in psychology to understand different aspects of cognitive function.

344
00:27:57,320 --> 00:28:01,400
And I think, you know, it's worth acknowledging that actually lots of people mean completely

345
00:28:01,400 --> 00:28:08,520
different things when they say attention. And some people say attention to mean the sort of

346
00:28:08,520 --> 00:28:13,640
overt process of looking at something and paying attention to it. Other people use it to talk

347
00:28:13,640 --> 00:28:19,720
about that the differences in gain in different sensory channels that they're trying to pay

348
00:28:19,720 --> 00:28:24,120
attention to or not, you know, am I paying attention to colors versus something else?

349
00:28:24,120 --> 00:28:27,160
And that's just turning up the volume of different pathways in your brain.

350
00:28:28,360 --> 00:28:32,040
And I'm sure there are a world of other things that people mean by it as well.

351
00:28:32,840 --> 00:28:36,040
But the idea of then trying to commit to a mathematical description of these things

352
00:28:36,600 --> 00:28:41,320
means that a lot of that ambiguity just disappears, that if you put a word to a

353
00:28:41,320 --> 00:28:45,320
particular mathematical quantity, as long as you define what that mathematical quantity is

354
00:28:45,320 --> 00:28:50,600
and how it interacts with other things, then a lot of that ambiguity just isn't there.

355
00:28:50,600 --> 00:28:56,200
And it forces you to commit to your assumptions in a much more specific way.

356
00:28:57,160 --> 00:29:01,960
And so that's why I come back to say, does it necessarily matter if an agent is real or not?

357
00:29:01,960 --> 00:29:07,640
I don't really know what that means. But if an agent is just a description of something that is

358
00:29:07,640 --> 00:29:12,280
separated from its environment that persists for a certain length of time, that has a dynamical

359
00:29:12,360 --> 00:29:16,600
structure that can be written down and a set of variables that can be partitioned off from another

360
00:29:16,600 --> 00:29:23,240
bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one.

361
00:29:23,880 --> 00:29:31,800
Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up

362
00:29:31,800 --> 00:29:35,720
in an intelligent way that explains what things do and what they don't do.

363
00:29:36,360 --> 00:29:42,120
And I guess the ontological statement, maybe we can park that to one side,

364
00:29:42,760 --> 00:29:47,320
because as you say, from a semantics point of view, people have very relativistic

365
00:29:47,320 --> 00:29:53,080
understandings of things. And there's always the philosophical turtles all the way down. Well,

366
00:29:53,080 --> 00:29:57,640
is it really real? Is it really real? But one thing that is interesting, though,

367
00:29:57,640 --> 00:30:03,560
about active inference is that it's quite mathematically abstract. So when we were

368
00:30:03,560 --> 00:30:09,560
saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just

369
00:30:09,560 --> 00:30:16,520
talking about the statistical independence between states. And those don't necessarily

370
00:30:16,520 --> 00:30:21,960
correspond to physical things. So I guess it could be applied to almost anything. It could be applied

371
00:30:21,960 --> 00:30:27,640
to culture or memes or language or something like that. And it has been. Yes, indeed.

372
00:30:28,520 --> 00:30:33,640
Yeah, it's a good point. And then you end up sort of dragged into the questions of what is

373
00:30:33,640 --> 00:30:39,000
physical. What does that mean? Is physical just an expression of dynamics that evolve in time?

374
00:30:39,000 --> 00:30:42,680
Because I mean, even committing to a temporal dimension tells you something about the world

375
00:30:42,680 --> 00:30:48,760
you're living in. Are the boundaries that we're talking about, are the partitions,

376
00:30:48,760 --> 00:30:55,960
are they spatial in nature or not? And, you know, I remember there was an article a little

377
00:30:55,960 --> 00:31:02,040
while back that sort of made a lot of argument about this as to whether the partitions that

378
00:31:02,040 --> 00:31:07,160
divide creatures from their environments are equivalent to statements of conditional independence

379
00:31:07,160 --> 00:31:12,200
of the sort that are seen in machine learning or various other things. And arguing that there's

380
00:31:12,200 --> 00:31:19,720
something inherently different about a physical boundary. For me, I was never completely convinced

381
00:31:19,720 --> 00:31:23,800
by that, but partly because you have to then define what you mean by a physical boundary.

382
00:31:23,800 --> 00:31:27,960
And I suspect it's the same sort of boundary, it's the same sort of conditional

383
00:31:27,960 --> 00:31:34,680
dependencies and independences. But where those have specific semantics, whether those be temporal,

384
00:31:34,680 --> 00:31:40,840
whether they be something where, you know, you can actually define a proper spatial metric

385
00:31:40,840 --> 00:31:46,840
underneath the things that you're separating out. And clearly, that sort of boundary is very

386
00:31:46,840 --> 00:31:51,640
important. But for me, that is just another form of the same sort of boundary. And as you say,

387
00:31:51,640 --> 00:31:57,160
you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical,

388
00:31:57,160 --> 00:32:07,240
whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing

389
00:32:07,240 --> 00:32:14,280
him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose,

390
00:32:14,280 --> 00:32:19,160
even though mathematically, you could apply it first to other geometries, that would be

391
00:32:19,240 --> 00:32:23,160
quite easy, because they have certain mathematical properties in terms of like, you know,

392
00:32:23,160 --> 00:32:27,400
being locally connected and measure spaces and all that kind of stuff. But if you did say,

393
00:32:27,400 --> 00:32:35,800
okay, I want to have an agent that represents a meme. How would that act? I don't know,

394
00:32:35,800 --> 00:32:46,040
you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge

395
00:32:46,040 --> 00:32:51,080
is defining the boundary. I think the boundary is a very difficult thing to define sometimes

396
00:32:51,080 --> 00:32:55,720
when you're dealing with something non spatial. That boundary, though, might be reflected in

397
00:32:55,720 --> 00:33:04,440
the interactions between a meme and a community that that engage with it. It might be to do with

398
00:33:04,440 --> 00:33:09,560
the expression of a meme in different parts of, I don't know, a network of some sort or social

399
00:33:09,560 --> 00:33:15,480
network. I don't know how easy it would be. I've not tried to do it in that context. And I think

400
00:33:15,480 --> 00:33:19,240
with many of these things, you never really know until you've had to go at doing it. But

401
00:33:21,400 --> 00:33:26,280
I suppose the key things I would be thinking about are, is there a clean way of defining

402
00:33:26,280 --> 00:33:31,240
a boundary for a meme? Is there something that the meme is doing to the outside world?

403
00:33:31,960 --> 00:33:34,440
Is there something that the outside world is doing to the meme?

404
00:33:36,680 --> 00:33:41,800
And I think if you're able to define those things convincingly, then perhaps there is a form of

405
00:33:41,880 --> 00:33:49,960
agent that may be non physical, if that's how you choose to define it. But then I'm not sure what

406
00:33:49,960 --> 00:33:53,560
physical means in this setting. Is there also an account of saying, well, actually,

407
00:33:53,560 --> 00:33:56,600
if you can write down the dynamics of how a meme propagates through a network,

408
00:33:57,320 --> 00:34:00,680
is that any different writing down the dynamics of another sort of physical system?

409
00:34:01,800 --> 00:34:09,080
Yes, possibly not. But it is really interesting to me that something like language could be seen

410
00:34:09,080 --> 00:34:14,440
as a life as a super organism, or even something like religion. And it seems to tick all of the

411
00:34:14,440 --> 00:34:21,640
boxes that we talk about with a gentleness in physical agents, which is to say, let's say

412
00:34:21,640 --> 00:34:27,560
a religion or even nationalism, you could say that the state of the Netherlands has certain

413
00:34:27,560 --> 00:34:35,880
objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,

414
00:34:36,360 --> 00:34:43,000
our collective behavior influences the state. But this then, I think the reason why people don't

415
00:34:43,000 --> 00:34:49,000
like to think in this way is we have psychological priors. So we are biased towards seeing a

416
00:34:49,000 --> 00:34:55,960
gentleness in individual humans, but we tend not to think of non physical or diffuse things as being

417
00:34:55,960 --> 00:35:01,480
agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole

418
00:35:01,480 --> 00:35:07,480
issue about the language that we use, that it comes laden with lots of prior beliefs about what it

419
00:35:07,480 --> 00:35:16,360
means, which may vary from person to person. And there comes a point where you say, how important

420
00:35:16,360 --> 00:35:20,920
is it that I commit to using this particular word to mean this particular thing in this setting?

421
00:35:21,720 --> 00:35:29,880
But again, in your example of taking a nation or nation state as being a form of organism at a

422
00:35:30,120 --> 00:35:37,400
higher level or form of agent, if you can show that there is a way of summarizing the dynamics of

423
00:35:37,400 --> 00:35:43,960
that system, maybe some high order summary of the behavior of people in that system, voting

424
00:35:43,960 --> 00:35:50,360
intentions, I don't know, you might then be able to show that it behaves in exactly the same way

425
00:35:50,360 --> 00:35:57,480
mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've

426
00:35:57,480 --> 00:36:02,040
been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.

427
00:36:02,040 --> 00:36:07,720
And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey

428
00:36:07,720 --> 00:36:15,080
Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have

429
00:36:15,080 --> 00:36:23,240
built these abstract models to reason about how we think. So we do planning, and we do reasoning,

430
00:36:23,240 --> 00:36:28,440
and we have perception, and we do this, and we do that. And also, we try and generate explanations

431
00:36:28,440 --> 00:36:34,760
for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly

432
00:36:34,760 --> 00:36:41,960
incoherent and inconsistent. And he was talking all about how the brain is actually a kind of

433
00:36:41,960 --> 00:36:49,640
predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see

434
00:36:49,640 --> 00:36:54,840
things that aren't there. And I think you speak about this in your book that there was a really

435
00:36:54,840 --> 00:37:00,760
big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of

436
00:37:00,760 --> 00:37:06,440
prediction machine, rather than our brain just kind of like building a simulacrum of the world

437
00:37:06,440 --> 00:37:13,000
around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction

438
00:37:13,000 --> 00:37:19,480
has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling

439
00:37:19,480 --> 00:37:25,640
us to our world that without prediction, you know, if you're purely simulating what might be going on

440
00:37:25,640 --> 00:37:29,960
without actually then correcting your simulation based upon what's actually going on or the input

441
00:37:29,960 --> 00:37:35,240
you're getting from the world, then you're not going to get very far. So prediction is just an

442
00:37:35,240 --> 00:37:40,520
efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want

443
00:37:40,520 --> 00:37:44,520
to call it a simulation? My simulation, my internal simulation of what's going on outside.

444
00:37:45,240 --> 00:37:51,400
And once you cease to have that constraint, once the world ceases to constrain the simulation,

445
00:37:51,400 --> 00:37:55,480
that's the point at which you start, as you say, hallucinating, seeing things that aren't there

446
00:37:56,360 --> 00:38:00,920
and developing beliefs that just bear no relationship to or little relationship to reality.

447
00:38:02,200 --> 00:38:09,160
Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a

448
00:38:09,160 --> 00:38:15,640
complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have

449
00:38:16,360 --> 00:38:23,080
a self model, I have a model of your mind, and I observe behavior and I kind of impute

450
00:38:23,960 --> 00:38:30,200
onto you a model and I can generate explanations. So as I say, Thomas did that because he must

451
00:38:30,200 --> 00:38:36,280
have wanted to do this. And I guess you could argue that all of this is just a confabulation.

452
00:38:36,280 --> 00:38:44,040
It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.

453
00:38:44,040 --> 00:38:48,920
But then there's the question of, well, it's not that it doesn't exist. It's just that your mind

454
00:38:49,480 --> 00:38:55,960
is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as

455
00:38:55,960 --> 00:39:01,480
the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an

456
00:39:01,480 --> 00:39:08,120
interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and

457
00:39:08,120 --> 00:39:14,280
the idea of deep learning neural networks with multiple layers. And I think you're right that

458
00:39:14,280 --> 00:39:19,880
depth is an important part of our generative models as well, of our brains models of the world.

459
00:39:20,840 --> 00:39:27,720
And part of that comes from the fact that the world actually does separate out into a whole

460
00:39:27,720 --> 00:39:32,760
different series of temporal scales of things that happen slowly, that contextualize things that

461
00:39:32,760 --> 00:39:37,960
happen more quickly, that contextualize things that are even faster than that. And so one good

462
00:39:37,960 --> 00:39:42,760
example of depth might be that if you're reading a book, then you have to bear in mind which page

463
00:39:42,760 --> 00:39:47,160
you're on within that page, which sentence or which paragraph you want, within paragraph,

464
00:39:47,160 --> 00:39:52,840
which sentence, within the sentence, which word, within the word, which letter. And by combining

465
00:39:52,840 --> 00:39:57,720
your predictions sort of both down the system that way, but then updating your predictions

466
00:39:57,720 --> 00:40:02,680
all the way back up again, you start to be able to make inferences about the overall narrative

467
00:40:02,680 --> 00:40:11,400
that you're reading. The other thing you mentioned that I thought was interesting was the idea of

468
00:40:11,400 --> 00:40:17,400
confabulation and of how we come to beliefs about other people's behavior. And I think the same

469
00:40:17,400 --> 00:40:21,800
thing is also true about our own behavior and sort of making an inference about what we've done.

470
00:40:22,440 --> 00:40:27,160
And this comes all the way back to the sense of agency again, doesn't it? It comes back to the

471
00:40:27,160 --> 00:40:31,960
idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,

472
00:40:31,960 --> 00:40:39,400
because I had this goal in mind. And to come back to the other question, is that real? Or is it

473
00:40:39,400 --> 00:40:45,560
simply an inference about what I've done? I would suggest that it's certainly an inference about

474
00:40:45,560 --> 00:40:53,080
what I've done, whether or not it's real. Giovanni and I put together some simulations

475
00:40:53,080 --> 00:40:58,680
and some theoretical work a couple of years ago after a discussion at a conference about or a

476
00:40:58,680 --> 00:41:04,200
workshop about machine understanding, suggesting that machine intelligence is one thing, but

477
00:41:04,200 --> 00:41:09,640
actually understanding why you've come to a particular conclusion. ChatGPT being able to

478
00:41:09,640 --> 00:41:15,800
explain to you why it came up with a specific sequence of words or why a convolutional neural

479
00:41:15,800 --> 00:41:22,280
network classified an image in a particular way is one of the big issues really, and there are

480
00:41:22,280 --> 00:41:26,840
solutions coming up, but it's one of the big issues in the deep learning community as to how

481
00:41:26,840 --> 00:41:30,600
you have that transparency in terms of what the models are doing and why they're doing it.

482
00:41:33,320 --> 00:41:36,840
Giovanni and I put together some work following that, looking at

483
00:41:37,560 --> 00:41:41,560
understanding of our own actions from an active inference perspective, and there it was very much

484
00:41:41,560 --> 00:41:46,760
framed as I have a series of hypotheses of things I might do, of reasons why I might do that.

485
00:41:47,560 --> 00:41:52,200
And then after observing myself behaving in a particular way, I can then use my own behavior

486
00:41:52,200 --> 00:41:58,200
as data that I then have to come up with an explanation for. And it's very interesting to

487
00:41:58,200 --> 00:42:03,080
see what happens if you start depriving that of aspects of its behavior and to see the confabulations

488
00:42:03,080 --> 00:42:08,600
that result from that. I can't remember where it came from originally, the idea of hallucinations

489
00:42:08,600 --> 00:42:17,960
being a perception generally being effectively a constrained hallucination, where you take your

490
00:42:17,960 --> 00:42:21,800
hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.

491
00:42:22,920 --> 00:42:26,440
But you could argue that actually a lot of our understanding about what we're doing is also

492
00:42:26,440 --> 00:42:29,720
just a constrained confabulation in exactly the same way.

493
00:42:30,680 --> 00:42:36,200
Yes, which is very ironic because people diminish GPT and because they say it's just

494
00:42:36,200 --> 00:42:43,800
confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument

495
00:42:43,800 --> 00:42:49,480
about how the brain works, and even our communication now on conditioning your simulator.

496
00:42:49,480 --> 00:42:53,480
So the semantics are drawn by your own model in simulation of the world,

497
00:42:53,480 --> 00:42:58,040
rather than being the simulacrum of mine. You spoke about machine understanding,

498
00:42:58,040 --> 00:43:04,040
I mean, there's this Chinese Rem argument. And we're in a really interesting time now because

499
00:43:04,680 --> 00:43:11,880
we have artifacts that behave in a way which is isomorphic in many ways.

500
00:43:12,440 --> 00:43:20,680
And it's so tempting to say, well, we're different. And you could make the ontological argument,

501
00:43:20,680 --> 00:43:26,280
but this psychological argument is a big one as well, which is we're different because we have

502
00:43:27,080 --> 00:43:31,400
beliefs, motives, volition, desires, we have all of these things.

503
00:43:31,400 --> 00:43:35,160
But as we were just saying before, this is all post hoc confabulated.

504
00:43:35,160 --> 00:43:39,560
We actually don't have consistent beliefs and desires. It's just a fiction.

505
00:43:41,240 --> 00:43:43,960
Was it a fiction or is it a plausible explanation?

506
00:43:45,480 --> 00:43:50,920
Well, I guess the thing that breaks it for me is the incoherence and inconsistency,

507
00:43:51,000 --> 00:43:58,040
because you would think that we would be fully fledged human agents if we had consistent beliefs

508
00:43:58,040 --> 00:44:02,920
and desires. And it's not to say that we don't because it feels like some of our goals are

509
00:44:05,000 --> 00:44:11,560
they grounded in some way, like we need to eat food. But we think of ourselves as being

510
00:44:11,560 --> 00:44:17,320
unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental

511
00:44:17,720 --> 00:44:22,280
to eating food. And I guess those things in particular might be confabulatory.

512
00:44:23,160 --> 00:44:27,720
Yes. So on the volition thing, that's something that really interests me.

513
00:44:30,440 --> 00:44:37,080
An active inference agent is we draw a boundary around a thing and it can act in the environment

514
00:44:37,080 --> 00:44:42,600
and it has preferences. And essentially, it has a generative model where it can produce these

515
00:44:42,600 --> 00:44:48,360
plans, these policies, if you like, and at the end of every single plan is an end state.

516
00:44:49,480 --> 00:44:56,360
So it's got all of these different goals in mind, if you like. And in the real world,

517
00:44:57,240 --> 00:45:04,280
real in big air quotes, these things emerge. But when we design these agents, we need to

518
00:45:04,280 --> 00:45:11,080
somehow impute the preferences onto them. So it feels like they have less agency if we

519
00:45:11,880 --> 00:45:17,560
impute the preferences. Would you agree with that? Interesting question.

520
00:45:19,800 --> 00:45:29,800
And a very relevant question in the current number of industry related applications of

521
00:45:29,800 --> 00:45:34,360
active inference. I think we were speaking about earlier, there are a number of companies now

522
00:45:34,360 --> 00:45:39,800
that have been set up looking at use of active inference based principles for various problems.

523
00:45:40,680 --> 00:45:45,400
Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.

524
00:45:47,160 --> 00:45:52,920
And the issue there is very much, it's a different kind of issue to the biological

525
00:45:52,920 --> 00:45:58,680
issue of describing how things work. And it's the issue of saying, if I now want to design an

526
00:45:58,680 --> 00:46:03,160
agent to behave in a particular way, as you say, am I taking some agency away from that?

527
00:46:03,160 --> 00:46:09,400
There are a couple of things to think about there. I suppose one is thinking about

528
00:46:10,840 --> 00:46:15,240
do biological agents actually select their own preferences to begin with?

529
00:46:17,080 --> 00:46:21,720
And I think most people would probably say they don't most of the time. There may be certain

530
00:46:21,720 --> 00:46:26,360
circumstances where they do or where a particular preference might be conditionally dependent upon

531
00:46:27,240 --> 00:46:32,280
the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's

532
00:46:32,280 --> 00:46:37,160
not that I'm actually selecting this is what I want to want. There is a famous quote here,

533
00:46:37,160 --> 00:46:40,280
but I can't remember what it is. I don't know whether you do. No.

534
00:46:43,960 --> 00:46:51,000
No, it's escaped me about wanting what you want or wanting what you do or something along those

535
00:46:51,000 --> 00:46:57,160
lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us

536
00:46:57,160 --> 00:47:02,360
effectively through a process of evolution, natural selection, previous experience that has

537
00:47:02,360 --> 00:47:09,000
affected what is a good set of states to occupy. And those will often be a good set of states that

538
00:47:09,000 --> 00:47:18,920
help my survival, that help the persistence of the species that I'm a part of. And arguably,

539
00:47:18,920 --> 00:47:25,240
the same thing is true when you as a designer of a particular algorithm or an agent are giving it

540
00:47:25,320 --> 00:47:30,120
a set of preferences. From its perspective, it's never selected them anyway. And that's the same

541
00:47:30,120 --> 00:47:36,200
as you or I not necessarily having selected our preferences. There's one additional element that

542
00:47:36,200 --> 00:47:43,960
I think is interesting to think about. And one of my colleagues and collaborators,

543
00:47:43,960 --> 00:47:48,600
Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own

544
00:47:48,600 --> 00:47:54,280
preferences, of actually saying, let's create an agent that isn't given preferences to begin with,

545
00:47:54,280 --> 00:48:00,600
but is allowed to learn as it behaves what sort of goal states it ends up in.

546
00:48:01,880 --> 00:48:08,280
And there you get some very interesting results. So she showed that these sorts of agents

547
00:48:09,640 --> 00:48:13,720
may end up doing things that you just don't want them to do, that they end up forming a

548
00:48:13,720 --> 00:48:18,040
particular pattern of being or a particular way of being that you as a designer might never have

549
00:48:18,040 --> 00:48:23,160
envisaged. For example, in an environment with lots of potential holes that it can fall into,

550
00:48:23,160 --> 00:48:28,280
some of these agents just become hole dwellers. They just decide, I found that the first few

551
00:48:28,280 --> 00:48:32,280
times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature

552
00:48:32,280 --> 00:48:38,520
that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe

553
00:48:38,520 --> 00:48:44,520
that agency is the ability to sort of disagree with what you as a designer might expect or want

554
00:48:44,520 --> 00:48:50,120
from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a

555
00:48:50,120 --> 00:48:59,000
discussion about cybernetics and externalism. But so what you're describing there is the reason

556
00:48:59,000 --> 00:49:06,680
why AI systems today are not sophisticated is because they are convergent. And that's usually

557
00:49:06,680 --> 00:49:13,240
because they don't actually have any agency. So one of the hallmarks of the physical real

558
00:49:13,240 --> 00:49:19,400
systems in the real world is that they have these divergent properties. And that's because you have

559
00:49:19,480 --> 00:49:24,600
lots of independent agents following their own directiveness doing epistemic foraging. So

560
00:49:24,600 --> 00:49:28,760
interesting stepping stones get discovered. And sometimes those stepping stones aren't what the

561
00:49:28,760 --> 00:49:34,120
designer of the system would have liked, as you just said. So there's an interesting kind of paradox

562
00:49:34,120 --> 00:49:41,160
there of how much agency do you want to imbue in the agents. But the other paradox is the physical

563
00:49:41,160 --> 00:49:47,480
and social embeddedness. Because as you just said, cynically, we don't have as much agency as we

564
00:49:47,480 --> 00:49:52,760
think we do, because we're embedded in the dynamics around us. And being part of this

565
00:49:52,760 --> 00:49:59,960
overall system means that our agency is defined not just by our boundary, but it's by the history

566
00:49:59,960 --> 00:50:05,160
of the system. It's the history of us sharing information of all of the things around us.

567
00:50:05,160 --> 00:50:10,680
And all of these things inform what we do and what our preferences are. And then you say, well,

568
00:50:10,680 --> 00:50:15,960
we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out

569
00:50:16,040 --> 00:50:21,640
of water. It's not embedded in the ways that things that emerged in that system were in the

570
00:50:21,640 --> 00:50:29,080
first place. But this does get us onto this discussion of externalism. So part of the fiction

571
00:50:29,080 --> 00:50:37,960
of how we think about cognition is that we think of ourselves as islands that don't share information

572
00:50:37,960 --> 00:50:42,760
dynamically with the outside world. And of course, active inference is a way of bridging

573
00:50:42,760 --> 00:50:45,400
these two schools of thought. So can you kind of bring that in?

574
00:50:47,720 --> 00:50:52,040
I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on

575
00:50:52,040 --> 00:51:01,160
that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.

576
00:51:01,160 --> 00:51:08,360
It's the idea that our brains and our internal state evolves in such a way that reflects beliefs

577
00:51:08,360 --> 00:51:13,880
about what's outside. And I think that's one of the key things that you have to have for any sort

578
00:51:13,880 --> 00:51:19,320
of intelligent system. And that doesn't necessarily exist with other approaches that exist in

579
00:51:19,320 --> 00:51:25,960
neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much

580
00:51:25,960 --> 00:51:33,320
being, the aboutness is the key thing that what's happening in my head is a reflection or is a

581
00:51:33,320 --> 00:51:40,280
description in some way is about what's happening outside my head. And maybe that's the link with

582
00:51:40,280 --> 00:51:45,080
this sort of externalism. But it's not just unidirectional either. It's the fact that

583
00:51:46,520 --> 00:51:50,120
I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing

584
00:51:50,120 --> 00:51:55,160
the outside world to change it to fit with the beliefs I have about how it should be.

585
00:51:55,160 --> 00:52:07,640
Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around

586
00:52:07,640 --> 00:52:14,520
us. And we influence the world around us. And that's essentially what active inference is.

587
00:52:14,520 --> 00:52:21,480
I guess it might be useful just to sketch out the cognitive science idea of an activism or

588
00:52:21,480 --> 00:52:27,640
cybernetic. So there were folks who really railed against this idea of representationalism,

589
00:52:27,640 --> 00:52:34,520
which is this idea of model building in principle. And active inference is an integrated approach

590
00:52:34,520 --> 00:52:40,120
where we allow some model building, but we also think of the world itself as being its own best

591
00:52:40,120 --> 00:52:47,400
representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in

592
00:52:47,400 --> 00:52:51,800
the distinction between the sort of inactivists, radical inactivists, the sort of different levels

593
00:52:51,800 --> 00:53:01,400
of stance you can take on this. And I think it comes down to that, that from an active inference

594
00:53:01,400 --> 00:53:07,240
perspective, both your representations, if that's the right word, the beliefs you have about the world,

595
00:53:07,240 --> 00:53:11,640
whether or not that meets the criteria for representation from an inactivist perspective

596
00:53:12,600 --> 00:53:19,080
is very important. But it is only important in terms of how you act. If your beliefs did not

597
00:53:19,080 --> 00:53:23,960
affect how you acted, clearly natural selection would not have selected you to form those beliefs.

598
00:53:23,960 --> 00:53:29,480
I think it's the simple way of putting it. So let's talk about some of the kind of

599
00:53:30,280 --> 00:53:36,680
the mathematical underpinnings here. So I think probably one of the main concepts we

600
00:53:36,680 --> 00:53:42,040
should start on is this idea of surprise. And maybe we can talk about it in general terms,

601
00:53:42,040 --> 00:53:48,680
and then we can move on to Bayesian surprise. So why is surprise so important in the free energy

602
00:53:48,680 --> 00:53:56,760
principle? Well, it's central to it. It is the key thing that matters. And we talk about the free

603
00:53:56,760 --> 00:54:03,880
energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,

604
00:54:03,880 --> 00:54:10,040
what do we mean by surprise? And it's another one of those things like the high road and

605
00:54:10,040 --> 00:54:13,560
the low road that you can approach from several different angles or several different lines of

606
00:54:13,560 --> 00:54:25,000
attack. If you were modeling something, if you were a Bayesian, so if you took a particular

607
00:54:25,000 --> 00:54:29,720
stance on probability theory and wanted to know, given my model, given my hypothesis,

608
00:54:29,800 --> 00:54:34,360
what's the evidence for it? What you would normally do is calculate something known as

609
00:54:34,360 --> 00:54:41,960
a marginal likelihood, which is just a measure of the fit between your model and the data that

610
00:54:41,960 --> 00:54:53,480
you have that you're trying to explain. That fit trades off various different things. So it can

611
00:54:53,480 --> 00:54:58,760
trade off how accurately your model is explaining the data against how far you've had to deviate

612
00:54:58,760 --> 00:55:03,880
from your prior beliefs or from your initial assumptions in order to arrive at that explanation.

613
00:55:06,040 --> 00:55:12,680
So that marginal likelihood, that evidence is effectively just the negative or the inverse

614
00:55:12,680 --> 00:55:19,480
of surprise. So that that's one perspective on it, the better the fit, the simpler and most

615
00:55:19,480 --> 00:55:25,400
accurate my explanation for something, the less surprised I will be by it. Another perspective

616
00:55:25,400 --> 00:55:31,560
on surprise is just this more colloquial sense. It's the idea that, given what I would predict,

617
00:55:31,560 --> 00:55:38,120
how far out of that prediction is it? One could take a more biological perspective on it and say,

618
00:55:38,120 --> 00:55:43,560
imagine we are, well, we are homeostatic systems that have some set points. We want to keep our

619
00:55:43,560 --> 00:55:47,160
temperature within a certain range, our blood pressure within a certain range, our heart rate

620
00:55:47,160 --> 00:55:52,920
within a certain range. If we find ourselves deviating from that, that is effectively a surprise

621
00:55:52,920 --> 00:55:57,640
because we're not where we expect to be. And so we enact various changes to bring

622
00:55:58,840 --> 00:56:04,840
those parameters back in range. So we might, if our blood pressure is too low, we might increase

623
00:56:04,840 --> 00:56:08,520
our heart rate to bring our blood pressure back up to the range we expect it to be in.

624
00:56:09,400 --> 00:56:16,200
And that is, in a sense, what active inference is all about. It's just this idea of keeping things

625
00:56:16,200 --> 00:56:22,040
within that minimally surprising range. But of course, once you put dynamics on it, once you

626
00:56:22,040 --> 00:56:27,880
start unfolding that in time, you end up having to not just deal with how surprising things are now,

627
00:56:27,880 --> 00:56:33,560
but you've got to try and anticipate surprise and behave in such a way that you allostatically

628
00:56:33,560 --> 00:56:39,960
control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,

629
00:56:39,960 --> 00:56:49,160
etc., but also your extraceptive sensations, your vision, your audition, and the like.

630
00:56:52,040 --> 00:56:56,120
And there's almost no end to the perspective you could take on surprise. Another perspective

631
00:56:56,120 --> 00:57:04,280
on it is that it's a reflective of, in a physical system, the improbability of being in a particular

632
00:57:04,280 --> 00:57:11,000
state. From a lot of physics perspectives, improbability is also associated with energy.

633
00:57:11,000 --> 00:57:18,520
It takes energy to bring things into less probable states. And without inputting energy into a system,

634
00:57:18,520 --> 00:57:23,640
it will generally end up in its most probable state in the absence of that.

635
00:57:25,320 --> 00:57:29,320
You think of things like Boltzmann's equation and the relationship there between energy and

636
00:57:29,320 --> 00:57:38,360
probability. And that also has a link then to the idea of either a Hamiltonian or indeed a

637
00:57:38,360 --> 00:57:43,400
steady state distribution, which is just what is the distribution things will end up in if left

638
00:57:43,400 --> 00:57:49,880
to their own devices for a certain amount of time until things have probabilistically converged.

639
00:57:49,880 --> 00:57:55,000
And that means that if I would construct a probability distribution over where things

640
00:57:55,000 --> 00:57:59,160
will be at a long point of time in the future, there will come a point at which that probability

641
00:57:59,160 --> 00:58:05,880
won't change any further. And the tendency of physical systems to go to those more probable

642
00:58:05,880 --> 00:58:14,440
states is exactly the same as the tendency to avoid surprising states. And again, we could

643
00:58:14,440 --> 00:58:18,920
sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully

644
00:58:18,920 --> 00:58:24,440
that sort of explains why it's such an important thing that underpins so much of what we do.

645
00:58:25,240 --> 00:58:29,640
We're either trying to sort of evolve as a physical system towards more probable states.

646
00:58:31,000 --> 00:58:36,600
Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within

647
00:58:36,600 --> 00:58:43,640
the right set points. Or we are more colloquially just trying to avoid things that are different to

648
00:58:43,640 --> 00:58:50,520
what we predict. Or we are statisticians trying to fit our model to the world as best we can.

649
00:58:50,520 --> 00:58:53,160
And all of those things come under the same umbrella of surprise.

650
00:58:54,920 --> 00:58:59,160
Free energy comes in because surprise is not a trivial thing to compute.

651
00:59:01,640 --> 00:59:07,080
Mathematically, it's often either intractable mathematically or computationally. And so it's

652
00:59:07,080 --> 00:59:12,120
just not efficient to be able to calculate. But free energy is a way of then approximating

653
00:59:12,120 --> 00:59:17,240
that surprise. It's a way of coming up with something that is close enough to it. Or

654
00:59:18,040 --> 00:59:22,840
even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,

655
00:59:24,040 --> 00:59:33,160
then that limits how high your surprise can be. The key additional thing in free energy is that

656
00:59:33,160 --> 00:59:38,760
the distance between that bound, your free energy and your surprise depends on how good your beliefs

657
00:59:38,760 --> 00:59:44,120
about the world are. And that's where perception comes in. That by getting the best beliefs you

658
00:59:44,120 --> 00:59:50,040
possibly can, you minimize the distance between your free energy and which is up a bounding of

659
00:59:50,040 --> 00:59:54,920
surprise and the surprise itself. So then any further reduction in free energy, you would expect

660
00:59:54,920 --> 00:59:59,720
to also result in a decrease. Sorry, any further decrease in free energy would also result in a

661
00:59:59,720 --> 01:00:04,680
further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,

662
01:00:04,680 --> 01:00:11,880
what struck me is that we're using the language of things like statistical mechanics and Bayesian

663
01:00:12,280 --> 01:00:19,000
statistics and information theory, things like entropy and so on. And we're interchangeably

664
01:00:20,120 --> 01:00:24,040
kind of speaking about the same thing from the perspective of different disciplines,

665
01:00:24,040 --> 01:00:31,960
which I find very, very interesting. And on the surprise thing, even though in this formalism,

666
01:00:31,960 --> 01:00:38,040
we are minimizing surprise, I think there's an interesting perspective that sometimes surprise

667
01:00:38,040 --> 01:00:44,920
is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when

668
01:00:44,920 --> 01:00:51,000
something surprising happens that the weights get updated because it's information. Or people on

669
01:00:51,000 --> 01:00:57,640
YouTube, my videos are that they get more views when they have a cash value, which means they

670
01:00:57,640 --> 01:01:01,960
have information content, which means that, you know, they're actually surprising your predictive

671
01:01:01,960 --> 01:01:06,360
model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.

672
01:01:06,360 --> 01:01:10,520
You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So

673
01:01:10,520 --> 01:01:15,160
there's this interesting juxtaposition between actually seeking out surprise, even though you

674
01:01:15,160 --> 01:01:20,200
can think of our brains overall as minimizing surprise. And what was the other thing I was

675
01:01:20,200 --> 01:01:24,120
going to say? Yeah, you were just getting onto variational inference, which is really interesting.

676
01:01:24,120 --> 01:01:30,040
So there's a couple of intractable statistical quantities in this mixture that we're talking

677
01:01:30,040 --> 01:01:36,520
about. I think it's the log model evidence and the Bayesian posterior. And we can't represent

678
01:01:36,520 --> 01:01:41,480
those things directly. So we have to put a proxy in there, which kind of captures most of the

679
01:01:41,480 --> 01:01:46,360
information, but it's still possible to deal with it. So how does this variational inference work?

680
01:01:47,320 --> 01:01:52,440
Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian

681
01:01:52,440 --> 01:01:56,920
inference is. I suppose we've been talking about it quite a lot without necessarily defining it.

682
01:01:56,920 --> 01:02:04,280
And many of you listeners, I'm sure, will know already. But the idea is actually relatively

683
01:02:04,280 --> 01:02:08,520
straightforward and well-established and quite widely used. And it's the idea that if I have

684
01:02:08,520 --> 01:02:15,640
some beliefs about things that are in my world that I can't directly observe, I may have a sense

685
01:02:15,640 --> 01:02:21,080
of what's plausible to begin with. And that's what we refer to as a prior probability. I then also

686
01:02:21,080 --> 01:02:28,600
need to have a model that says, given the world is this way, what would I expect to actually observe?

687
01:02:29,160 --> 01:02:36,920
So for instance, given where you are relative to me, I can predict a certain pattern on my retina.

688
01:02:37,560 --> 01:02:41,560
And if you were somewhere else, I would expect a different pattern on my retina. So I might have

689
01:02:41,560 --> 01:02:45,720
a prior range of plausibilities as to where you are relative to me. And then I have a model that

690
01:02:45,720 --> 01:02:50,840
explains how I'm going to generate some data based upon that. And Bayesian inference basically

691
01:02:50,840 --> 01:02:57,720
takes those two things and inverts them using Bayes' theorem and effectively just flips both of them

692
01:02:57,720 --> 01:03:03,880
round. So you now say instead of a distribution of where you are relative to me, I'm now talking

693
01:03:03,880 --> 01:03:09,640
about a distribution of all the possible things that I could see on my retina. And instead of

694
01:03:09,640 --> 01:03:17,000
predicting the distribution on the retina given where you are, I now want to know the distribution

695
01:03:17,000 --> 01:03:23,400
of where you are given what's on my retina. And Bayesian inference, much like active inference,

696
01:03:23,400 --> 01:03:27,640
is full of all these interesting inversions where you sort of flip things round from how

697
01:03:27,640 --> 01:03:34,280
they initially appeared. But the problem is calculating those two things, calculating the

698
01:03:34,280 --> 01:03:39,640
flipped model. So the distribution of all the things on my retina here would now be my model

699
01:03:39,640 --> 01:03:48,360
evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina

700
01:03:49,240 --> 01:03:53,800
is my posterior distribution. But those things are not always straightforward to calculate.

701
01:03:54,360 --> 01:04:00,200
And so variational inference takes that problem and makes it into an optimization problem. It

702
01:04:00,200 --> 01:04:08,120
writes down a function that quantifies how far am I away from my, or what would be the true posterior

703
01:04:08,120 --> 01:04:15,480
if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior

704
01:04:15,480 --> 01:04:20,120
probability. So come up with a function that represents a probability distribution that's

705
01:04:20,120 --> 01:04:24,520
easy to characterize, something like a Gaussian distribution where I know I just need my mean

706
01:04:24,520 --> 01:04:30,920
and my variance. And then just changes that mean and variance until you minimize this function

707
01:04:30,920 --> 01:04:36,360
that represents that discrepancy, minimize this free energy, also sometimes known as an evidence

708
01:04:36,360 --> 01:04:42,120
lower bound, in which case you maximize it. And interestingly, once you've maximized your

709
01:04:42,120 --> 01:04:46,440
evidence lower bound or minimized your free energy, you end up with a situation where

710
01:04:47,720 --> 01:04:54,200
the free energy starts to approximate your log model evidence or your negative log surprise.

711
01:04:55,480 --> 01:05:01,240
And your approximate posterior distribution, your variational distribution starts to look

712
01:05:01,240 --> 01:05:08,760
much more like your exact posterior probability distribution. So it's another one of those

713
01:05:08,760 --> 01:05:13,800
interesting scenarios where doing one thing optimizing one quantity ends up having a dual

714
01:05:13,800 --> 01:05:18,040
purpose. And in active inference, the only additional thing you throw into that is that you

715
01:05:18,040 --> 01:05:25,400
want to then also change your data itself. So you do the third thing you act on the world

716
01:05:25,400 --> 01:05:30,360
to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting

717
01:05:30,360 --> 01:05:35,800
to machine learning again. So in machine learning, we also have these big parameterized models and we

718
01:05:35,800 --> 01:05:41,160
do stochastic gradient descent. And some might think of deep learning, because obviously you

719
01:05:41,160 --> 01:05:45,240
can think of everything as a Bayesian. So you can think of machine learning as being maximum

720
01:05:45,240 --> 01:05:52,920
likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not

721
01:05:52,920 --> 01:05:58,680
something like maximum likelihood estimation? It's an interesting question. And there are a couple

722
01:05:58,680 --> 01:06:04,120
of answers you could give again, some of which are more technical, but some of which are

723
01:06:07,240 --> 01:06:12,440
some of which are slightly more intuitive. And I think one of the more intuitive answers is that

724
01:06:12,440 --> 01:06:18,360
by having an expression of plausibility of things in advance, you just maintain things

725
01:06:18,360 --> 01:06:24,120
within a plausible region. So maximum likelihood for those who are unaware is where you essentially

726
01:06:24,200 --> 01:06:29,560
throw away that prior probability, where you throw away any prior plausibility as to as to

727
01:06:29,560 --> 01:06:36,440
what the state of the world might be. And you just try and find the value that would maximize

728
01:06:36,440 --> 01:06:41,480
your likelihood, which is your prediction of how things would be under some hypothesis or under

729
01:06:41,480 --> 01:06:53,080
some parameter setting. And I think the first thing to say is if you throw away that prior

730
01:06:53,080 --> 01:06:57,960
information, then you end up potentially coming up with quite implausible solutions.

731
01:06:59,000 --> 01:07:03,320
That's particularly relevant if you're dealing with what's known as an inverse problem. So where

732
01:07:03,320 --> 01:07:08,840
there are multiple different things that could have caused the same outcome. An example that's

733
01:07:08,840 --> 01:07:13,720
often given is that for any given shadow, there's almost an infinite number of things, configurations

734
01:07:13,720 --> 01:07:18,040
of the sun and the shape of the thing that's casting the shadow that could lead to exactly the

735
01:07:18,040 --> 01:07:22,520
same shadow. And so maximum likelihood approach just won't be able to tell the difference between

736
01:07:22,520 --> 01:07:27,880
all of those things. However, if you have some prior on top of that, if you have some statement

737
01:07:27,880 --> 01:07:32,760
of the plausible things that might cause it, you can come up with a much better estimate of those

738
01:07:32,760 --> 01:07:43,800
sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood

739
01:07:43,800 --> 01:07:49,160
estimate, you're throwing away all uncertainty about the solution. So you're coming up with a

740
01:07:49,160 --> 01:07:53,320
point estimate and you're saying this is the most likely thing, but you're ignoring all of your

741
01:07:53,320 --> 01:07:59,080
uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to

742
01:07:59,080 --> 01:08:03,880
the problem of overfitting, where you start to become very confident about what you can see from

743
01:08:03,880 --> 01:08:14,920
a relatively small sample of things and you can end up with all of these well-described in the media

744
01:08:14,920 --> 01:08:20,520
scenarios of complete misclassifications based upon that sort of overconfidence just because

745
01:08:20,520 --> 01:08:27,960
all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about

746
01:08:28,680 --> 01:08:36,120
what a free energy is. So free energy is our measure of our marginal likelihood that we're

747
01:08:36,120 --> 01:08:42,760
using when we're doing Bayesian inference. And one way of separating out what a free energy

748
01:08:42,760 --> 01:08:49,720
looks like is to have our complexity, which is effectively how far we needed to deviate from

749
01:08:49,720 --> 01:08:55,000
our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit

750
01:08:55,000 --> 01:09:02,520
our model. Accuracy is common to both maximum likelihood type approaches because we're trying

751
01:09:02,520 --> 01:09:10,120
to find the value that most accurately predicts our data and also to Bayesian approaches.

752
01:09:10,120 --> 01:09:16,200
Both want to do that. But what's thrown away in the maximum likelihood type approach is the

753
01:09:16,200 --> 01:09:23,800
complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,

754
01:09:23,800 --> 01:09:30,200
the idea that the simplest explanation is a priori more likely that you get from a Bayesian

755
01:09:30,200 --> 01:09:34,200
approach that you throw away when you're dealing with maximum likelihood estimation.

756
01:09:35,160 --> 01:09:41,480
I wondered to what extent does the active part play a role here. So even in machine learning,

757
01:09:43,000 --> 01:09:47,800
there's something called active learning, where you dynamically retrain the model,

758
01:09:47,800 --> 01:09:52,280
or there's something called machine teaching, where you dynamically select more salient data

759
01:09:52,280 --> 01:09:57,880
to train the model, and the model gets much better. And in things like Bayesian optimization,

760
01:09:57,880 --> 01:10:03,800
for example, by maintaining this distribution of all of your uncertainty in a principled way,

761
01:10:03,880 --> 01:10:09,720
you can go and seek and find more information to kind of improve your knowledge on subsequent steps.

762
01:10:09,720 --> 01:10:15,240
So I guess it's sort of bringing in this idea of it's not just what happens now,

763
01:10:15,240 --> 01:10:21,000
it's about how can I improve my knowledge of the world over several steps.

764
01:10:21,720 --> 01:10:25,640
Yes, and that reminds me about the point you were making earlier, that sometimes we actually do

765
01:10:25,640 --> 01:10:33,640
things to surprise ourselves, which seems very counter-intuitive in the context of the idea that

766
01:10:33,640 --> 01:10:41,480
we're trying to minimize surprises as our sole objective in life. And sometimes people talk about

767
01:10:41,480 --> 01:10:46,040
this in terms of a dark room problem, the idea that actually if all you want to do is minimize

768
01:10:46,040 --> 01:10:50,040
your surprise, you just go into a room, turn off the lights and stay there because you're not going

769
01:10:50,040 --> 01:11:00,120
to experience anything that's going to surprise you. I mean, the answer to this problem is that

770
01:11:00,120 --> 01:11:08,600
actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the

771
01:11:08,600 --> 01:11:15,320
sort of organism that would be is, again, probably not a very interesting one. And that what we predict,

772
01:11:15,320 --> 01:11:21,080
what we'd be surprised by might be permanently staying in a dark room. But it goes even further

773
01:11:21,080 --> 01:11:27,160
than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable

774
01:11:27,160 --> 01:11:31,800
world where I know what's going to happen next. The best way of doing that is to actually gather

775
01:11:31,800 --> 01:11:36,040
as much information as you can about the world around you. So the first thing you do really is

776
01:11:36,040 --> 01:11:39,640
you turn on the light and see what the room looks like, because that might then predict all the sorts

777
01:11:39,640 --> 01:11:43,720
of things that could fall on you in that room and could potentially cause surprise. And by knowing

778
01:11:43,720 --> 01:11:49,640
about it, you mitigate the surprise that you might get in the future. And as you say, you can only

779
01:11:49,640 --> 01:11:54,120
really do that if you know what you're certain about. And so if you take a maximum likelihood

780
01:11:54,200 --> 01:11:58,520
approach, if you work based on point estimates and you have no measure of your uncertainty,

781
01:11:59,320 --> 01:12:03,560
then there's no way you can possibly know what you're uncertain about to be able to resolve

782
01:12:03,560 --> 01:12:10,600
that uncertainty. So this brings me on to causality. We know that predictive systems,

783
01:12:10,600 --> 01:12:16,120
which are aware of causal relationships, work better. But if we just bring it back to physics

784
01:12:16,120 --> 01:12:21,080
first, I mean, to you, what do you think causality is?

785
01:12:22,200 --> 01:12:27,160
It is a tricky issue as to what causality is. And I think whether it exists or not is really a

786
01:12:27,160 --> 01:12:34,840
matter of how you define it, isn't it? And some would define that purely in terms of conditional

787
01:12:34,840 --> 01:12:41,320
dependencies, that the behavior of one thing is conditionally dependent upon something else,

788
01:12:41,400 --> 01:12:46,040
and therefore you could say that the one thing causes the other. But as we know from Bayes'

789
01:12:46,040 --> 01:12:50,600
theorem, that's not quite good enough, because you can swap any conditional relationship around

790
01:12:52,440 --> 01:12:59,000
through that process of inverting your model. Sometimes that causality is written into the

791
01:12:59,000 --> 01:13:05,480
dynamics of a model. So this would be the approach used in things like dynamic causal

792
01:13:05,480 --> 01:13:10,520
modeling of brain data, where you might say that the current neural activity in one area of the

793
01:13:10,520 --> 01:13:15,160
brain affects maybe the rate of change of neural activity in another part of the brain.

794
01:13:15,800 --> 01:13:19,480
And it's the way in which those dynamics are written in, the fact that it's one affects the

795
01:13:19,480 --> 01:13:27,400
rate of change of the other, that gives it that causal flavor and a very directed perspective on

796
01:13:27,400 --> 01:13:35,480
it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and

797
01:13:35,560 --> 01:13:41,800
a lot of his work on causality. There's a lot of detail about the notion of an intervention.

798
01:13:42,360 --> 01:13:47,720
And I suppose you can think of this in terms of how you might establish causation in a clinical

799
01:13:47,720 --> 01:13:53,000
context. If you were to run a trial to try and establish whether one thing's caused another,

800
01:13:53,000 --> 01:13:57,400
you need to make sure you're not inadvertently capturing a correlation or a conditional dependence

801
01:13:57,400 --> 01:14:03,000
that could go either way, or a common cause of both things that depends upon something else.

802
01:14:03,880 --> 01:14:09,800
And typically the way you do that is you intervene on the system. You randomize at the beginning

803
01:14:09,800 --> 01:14:14,040
to make sure that people are assigned to different treatment groups at random,

804
01:14:14,040 --> 01:14:20,200
so that you break that dependency upon something prior to it. And then anything that happens going

805
01:14:20,200 --> 01:14:27,880
forward is going to depend on the intervention that you're doing. So I think that's probably the

806
01:14:27,960 --> 01:14:33,720
key thing that gives you causality or perhaps defines causality. It's the idea that an intervention

807
01:14:33,720 --> 01:14:38,040
is what will change it. If you intervene in one thing, that should then in a way that doesn't

808
01:14:38,040 --> 01:14:43,240
necessarily match its natural distribution if you hadn't intervened at all, and then see what the

809
01:14:43,240 --> 01:14:50,520
effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study

810
01:14:50,520 --> 01:14:56,520
his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.

811
01:14:56,520 --> 01:15:03,160
But I suppose one way to think about it is if you go back to the core physical, in physics,

812
01:15:03,160 --> 01:15:07,240
there's a whole bunch of equations to describe the world we live in. And those equations don't

813
01:15:07,240 --> 01:15:12,760
have, they don't say anything about causality, and they're even reversible. And then you can think,

814
01:15:12,760 --> 01:15:17,000
okay, well, maybe it's a little bit like the free energy principle. It's a lens,

815
01:15:17,000 --> 01:15:21,560
like really, there's only dynamics. But when you look at these dynamical systems,

816
01:15:21,560 --> 01:15:28,600
then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,

817
01:15:28,600 --> 01:15:34,360
and it's something which is statistically efficacious to build it into our models. But

818
01:15:36,440 --> 01:15:42,440
where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular

819
01:15:42,440 --> 01:15:51,080
pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern

820
01:15:51,080 --> 01:15:55,880
of how one thing reacts to another. So if you imagine you've got the classic physics example,

821
01:15:55,880 --> 01:16:03,160
billiard balls bouncing into one another, how do you know that the collision of one ball with

822
01:16:03,160 --> 01:16:10,280
another is causative of the subsequent motion of the second ball? And you could argue that that's

823
01:16:10,280 --> 01:16:15,160
due to a particular pattern of which variables affect which other variables and the particular

824
01:16:15,160 --> 01:16:20,280
exchange between them. And this comes back quite nicely to things like the physics perspective

825
01:16:20,680 --> 01:16:26,440
on the free energy principle, the idea that actually one could see the location of a particular

826
01:16:26,440 --> 01:16:34,520
ball as being, you know, maybe it's internal state, and then the action that that then causes

827
01:16:34,520 --> 01:16:41,000
is perhaps the, or in fact, you could say that the action is the position of the ball, the force

828
01:16:41,000 --> 01:16:46,840
that results from that action is the sensory state of the next ball, which then changes its

829
01:16:46,840 --> 01:16:53,960
velocity to then change its action relative to something else. You can sort of rearrange those

830
01:16:53,960 --> 01:16:59,800
labels slightly, but there is a directional element to it. And in that sort of pattern of

831
01:16:59,800 --> 01:17:05,240
causation, you really do expect the position of one ball to have an effect on the rate of change,

832
01:17:05,240 --> 01:17:09,720
or in fact, even the rate of rate of change of the second ball, which again, I think brings us

833
01:17:09,720 --> 01:17:15,480
back to those kinds of dynamical descriptions of causality where one thing might affect how

834
01:17:15,480 --> 01:17:21,160
another thing changes. So you almost get it from the dynamics itself. But again, to some extent,

835
01:17:21,160 --> 01:17:25,800
it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose

836
01:17:25,800 --> 01:17:29,960
cause is a hypothesis as to a particular configuration of things. But then you've got to

837
01:17:29,960 --> 01:17:34,840
write down what does that hypothesis mean? What's my model of what a causation involves?

838
01:17:35,560 --> 01:17:41,720
Yes, yes. I mean, we were just talking about, you know, build building these models. And one of

839
01:17:41,720 --> 01:17:48,520
the bright differences from machine learning is that we need to build a generative model by hand.

840
01:17:49,560 --> 01:17:54,520
So we have to define these these variables, and some of them are presumably observed, and some of

841
01:17:54,520 --> 01:18:03,320
them are not observed. They're inferred. And that process seems like you would need to have a lot

842
01:18:03,320 --> 01:18:10,440
of domain expertise. And it seems like something which is at least has a degree of subjectivity.

843
01:18:10,440 --> 01:18:14,760
I mean, we were just talking about causality, for example, there are many ways you could model

844
01:18:15,320 --> 01:18:21,160
the risk of cancer from smoking. It seems like there are many, many different ways of building

845
01:18:21,160 --> 01:18:27,960
those models. So that subjectivity is interesting. I mean, are there principled ways of building

846
01:18:27,960 --> 01:18:33,800
these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to

847
01:18:33,800 --> 01:18:42,280
which model minimizes the surprise the best. And but there are interesting questions amongst that.

848
01:18:42,280 --> 01:18:48,840
So how do you actually choose the space of models that you want to compare? So you're right to say

849
01:18:48,840 --> 01:18:54,520
that that that often there is some specific prior information that's put into models and active

850
01:18:54,520 --> 01:18:59,000
inference. And very often we do end up sort of building models by hand to demonstrate a specific

851
01:18:59,720 --> 01:19:04,760
outcome or a specific cognitive function. But there's no reason why it has to be that way.

852
01:19:04,760 --> 01:19:14,040
You can build models through exposure to data, where where the models are selecting the data to

853
01:19:14,040 --> 01:19:19,240
best build themselves. But the question is how you do that, how you start to add on additional

854
01:19:19,240 --> 01:19:24,200
things, how you start to change the structure of your model. But there's a lot of ongoing research

855
01:19:24,200 --> 01:19:29,000
into that. And I think there are now methods that are coming out that will allow you to allow an

856
01:19:29,000 --> 01:19:34,520
active inference model to build itself. And the way it will do that will be sort of adding on

857
01:19:34,520 --> 01:19:41,240
additional states and potential causes, adjusting beliefs about the mappings and the distributions

858
01:19:41,240 --> 01:19:49,400
and the parameters of given this than that, adding an additional paths that different

859
01:19:50,120 --> 01:19:58,040
or different transitions that systems will pursue. So it's a fascinating area. I think

860
01:19:58,040 --> 01:20:03,880
it's one that's still a growing area. But it's this idea of structure learning of comparing

861
01:20:03,880 --> 01:20:08,840
each alternative model based upon its free energy or model evidence or surprise as a way of

862
01:20:10,520 --> 01:20:13,080
minimizing that by being able to better predict things.

863
01:20:13,960 --> 01:20:22,120
Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,

864
01:20:22,120 --> 01:20:30,760
via abduction, we can select relevant models to explain behavior, you know, what we observe.

865
01:20:30,760 --> 01:20:36,120
But we also have the ability to create models. In fact, I think of intelligence as the ability

866
01:20:36,120 --> 01:20:42,760
to create models. So we experience something. And I now construct a model to explain this

867
01:20:42,760 --> 01:20:51,240
and similar experiences in experience space. But in a machine, it's really difficult. So in

868
01:20:51,240 --> 01:20:56,280
machine learning, there's this bias variance trade off. So we deliberately reduce the size

869
01:20:56,280 --> 01:21:02,200
of the approximation space to make it computationally tractable. And when we're talking about

870
01:21:02,200 --> 01:21:07,720
building these models, just from observational data, it feels like there's an exponential

871
01:21:07,720 --> 01:21:12,440
blow up of possible models. So I can imagine there might be a whole bunch of heuristics around

872
01:21:12,440 --> 01:21:17,560
library learning or having modules. So these modules have worked well over there. So we'll

873
01:21:17,560 --> 01:21:22,520
try composing together known modules rather than starting from scratch every single time. I mean,

874
01:21:22,520 --> 01:21:26,680
what kind of work is being done there? I mean, I think I think you're right about, you know,

875
01:21:26,680 --> 01:21:33,720
it's not going to be worth starting from scratch every time. You can sort of build models by saying,

876
01:21:33,720 --> 01:21:38,840
okay, let's start with something very simple with a sort of known structure. And I think it's

877
01:21:38,840 --> 01:21:44,280
sensible to use some priors in that rather than starting from complete, completely nothing,

878
01:21:44,280 --> 01:21:47,720
because there are some things that we know about in the world. And there's no point hiding that

879
01:21:47,720 --> 01:21:53,640
from the models we're trying to build. And that might be a simple structural thing like things

880
01:21:53,640 --> 01:21:57,480
evolve in time. So one thing is conditioned upon the next is conditioned upon the next.

881
01:21:57,480 --> 01:22:02,120
And things now will influence the data I observe things well in the past might not anymore.

882
01:22:05,720 --> 01:22:10,520
But then then there's the question of, well, how can a model then grow? What are the things that

883
01:22:10,520 --> 01:22:15,800
you can add to it or subtract from it? And subtraction is another key element. Because you

884
01:22:15,800 --> 01:22:19,320
could take this whole problem from the other direction, and you could say, well, let's start

885
01:22:19,320 --> 01:22:23,080
with a model that just has everything in it and take away bits until we've got the model that's

886
01:22:23,080 --> 01:22:26,920
relevant to where we are at the moment. And we know that during development, there's a lot of

887
01:22:26,920 --> 01:22:32,200
synaptic pruning that goes on and removal of synapses that we have when we're much younger

888
01:22:33,080 --> 01:22:39,720
compared to compared to as you get older. So what can you add on? Well, it depends what your model

889
01:22:39,720 --> 01:22:43,400
looks like. So if your model says there's a set of states that can evolve over time, there are

890
01:22:43,400 --> 01:22:46,920
a set of outcomes that are generated, well, we know what the outcomes are, we know what the

891
01:22:46,920 --> 01:22:52,680
data are, because we know what our sensory organs are. So it's the states that are going to change

892
01:22:53,160 --> 01:23:00,280
so do we add in more states? Do we allow them to take more alternative values? Do we allow

893
01:23:00,280 --> 01:23:05,960
their transitions to change in more than one different way? Which ones can I change? Which

894
01:23:05,960 --> 01:23:11,800
ones can I not change? And it's really just asking these questions that helps you to grow your model.

895
01:23:11,800 --> 01:23:16,440
So you say, well, let's try it. If I allow this state to take additional values, if it's not

896
01:23:16,440 --> 01:23:22,520
providing a sufficiently good explanation for how things are at the moment. And if that improves

897
01:23:22,520 --> 01:23:28,040
your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need

898
01:23:28,040 --> 01:23:35,800
to include additional state factors? So you could either say there is one sort of state of the world

899
01:23:35,800 --> 01:23:39,960
that can take multiple different values, or you could actually this is contextualized by something

900
01:23:39,960 --> 01:23:44,440
completely separate. So where am I along an x coordinate? You also need to know where you are

901
01:23:44,440 --> 01:23:51,320
along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in

902
01:23:51,320 --> 01:23:55,560
a model? How do you build a model almost gives you the answers to the ways or the directions in

903
01:23:55,560 --> 01:24:01,400
which you can grow it. The other thing you can then do when you're trying to work out how to grow it

904
01:24:01,400 --> 01:24:06,120
is to say, well, let's treat this as the same sort of problem as exploring my world,

905
01:24:06,120 --> 01:24:10,040
selecting actions that will then give me more information about the world. You could say,

906
01:24:10,040 --> 01:24:14,360
well, actually, now let's treat my exploration of model space as being a similar process of

907
01:24:14,360 --> 01:24:23,320
exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping

908
01:24:23,320 --> 01:24:28,360
between what I'm predicting or what's in my world and what I'm currently predicting?

909
01:24:28,920 --> 01:24:35,240
Yes, it rather brings me back to our comments about the space or the manifold that the models

910
01:24:35,240 --> 01:24:39,960
sit on, whether they would have a kind of contiguity or whether they would have a gradient.

911
01:24:40,360 --> 01:24:45,480
I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether

912
01:24:45,480 --> 01:24:50,600
it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,

913
01:24:51,400 --> 01:24:58,600
we must do this. Of course, there's this idea of nativism. Some psychologists think that we have

914
01:24:58,600 --> 01:25:03,320
these models built in from birth and then the other school of thought is that we're just a

915
01:25:03,320 --> 01:25:09,720
complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical

916
01:25:09,720 --> 01:25:16,280
thing that just builds models on the fly. But perhaps one difference at least between brains

917
01:25:16,280 --> 01:25:24,120
and machines is the multi-modality, which is to say we have so many different senses that

918
01:25:25,160 --> 01:25:35,080
creates a gradient or that makes it tractable. Because when a model from a particular sensation

919
01:25:35,080 --> 01:25:39,000
and starts predicting well, we can rapidly optimise and go in the right direction.

920
01:25:39,080 --> 01:25:43,560
Because the problem seems to be that there are so many directions where we can go in,

921
01:25:45,160 --> 01:25:51,000
doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the

922
01:25:51,000 --> 01:25:58,920
search space, so we've wasted our time. Yeah, I think that's a really good point,

923
01:25:58,920 --> 01:26:07,160
absolutely. As soon as you know how one thing works or how vision works, I suppose vision

924
01:26:07,160 --> 01:26:13,400
and proprioception is a good example, isn't it? If I recognise where my hand is and I can

925
01:26:13,400 --> 01:26:17,880
make a good estimate of that visually, then that helps me tune my joint position sense as to where

926
01:26:17,880 --> 01:26:24,840
my arm might be. And it's always fascinating to see situations where that breaks down, so there

927
01:26:24,840 --> 01:26:29,880
are a number of conditions where if you lose your joint position sense, you're perfectly okay holding

928
01:26:29,880 --> 01:26:34,040
your arm out like that until you close your eyes, at which point you start getting all these interesting

929
01:26:34,040 --> 01:26:39,880
twitches and changes. So yes, the multimodality I think probably is a really key thing that really

930
01:26:39,880 --> 01:26:44,840
does help constrain the other senses because you're just getting more information about each thing.

931
01:26:45,800 --> 01:26:50,440
Maybe we should just talk about chapter 10 in general, because that was kind of like the

932
01:26:50,440 --> 01:26:56,280
homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch

933
01:26:56,280 --> 01:27:03,240
that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together

934
01:27:03,240 --> 01:27:07,320
a lot of the themes that had been discussed earlier on, but to also make the point that,

935
01:27:10,600 --> 01:27:15,880
well, I'll come back to one of the things you said earlier was about how it seems we're talking about

936
01:27:15,880 --> 01:27:21,400
lots of different things from different perspectives, but actually they're really the same thing.

937
01:27:21,400 --> 01:27:29,320
So we talked about how surprise is also a measure of steady state of energies of various sorts of

938
01:27:30,280 --> 01:27:36,120
of statistics and model comparison of homeostatic set points, you know, that all of these things

939
01:27:36,120 --> 01:27:44,600
can be seen through the same lens. But again, taking one of those inversions, you can invert

940
01:27:44,600 --> 01:27:47,960
that lens and say, well, actually, you can start from the same thing and now project back into

941
01:27:47,960 --> 01:27:55,480
all of these different fields. And I think that's a useful thing to do because I think it helps foster

942
01:27:55,560 --> 01:28:00,600
multidisciplinary work, helps to engage people from different fields and areas,

943
01:28:01,560 --> 01:28:08,280
and helps us know what's happening elsewhere so that you're not just duplicating everything that

944
01:28:08,280 --> 01:28:13,640
people have already done. So I think it's really important to have those connections to different

945
01:28:13,640 --> 01:28:19,800
areas. And the chapter 10 from the book was an aim to try and connect to those different areas,

946
01:28:19,800 --> 01:28:23,240
whether it be to things you've spoken about, like cybernetics and inactivism,

947
01:28:23,320 --> 01:28:27,800
and just to try and understand the relationship between each of them.

948
01:28:27,800 --> 01:28:32,520
Well, I mean, quite a lot of people use this as a model of, you know, just things like

949
01:28:32,520 --> 01:28:38,440
sentience and consciousness in general. And I often speak about the strange bedfellows of

950
01:28:38,440 --> 01:28:43,480
the free energy principle. So, you know, there are, you know, autopoietic and activists and

951
01:28:43,480 --> 01:28:47,640
phenomenologists and, you know, people talking about sentience and consciousness, you know,

952
01:28:47,640 --> 01:28:53,480
obviously you're a clinician, you know, you're working in a hospital. So it's just this

953
01:28:53,480 --> 01:28:57,960
incredible conflation of different people together, and they all bring their own lexicon with them.

954
01:28:57,960 --> 01:29:02,360
But maybe we should just get on to this kind of sentience and consciousness thing, because that

955
01:29:02,360 --> 01:29:08,840
seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,

956
01:29:08,840 --> 01:29:16,120
which is that the specific words we use for things in the effect that different people,

957
01:29:16,120 --> 01:29:19,720
that has on different people. So some people, I think, would probably get very angry with the idea

958
01:29:19,720 --> 01:29:29,640
of using sentience to describe some of the sort of simulations and models that we would develop.

959
01:29:31,400 --> 01:29:36,120
But that comes down to what you mean by sentience. And I think one of the key things for sentience

960
01:29:36,120 --> 01:29:42,360
is the aboutness we were talking about before. The idea that our brains or any sentient system

961
01:29:42,360 --> 01:29:48,520
really is trying to try not to anthropomorphise too much, but it's almost impossible to do in

962
01:29:48,520 --> 01:29:57,080
this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system

963
01:29:57,080 --> 01:30:01,800
are reflective of what's going on external to it, and that you can now start to see those dynamics

964
01:30:01,800 --> 01:30:07,000
as being optimization of beliefs. And those beliefs are about what's happening in the outside world

965
01:30:07,080 --> 01:30:12,680
and about how I'm affecting the outside world. And I think that probably gets to the root of

966
01:30:12,680 --> 01:30:18,600
at least a definition of sentience and one that I'd be happy with, which is just the

967
01:30:20,040 --> 01:30:26,120
dynamics of beliefs about what's external to us and how we want to change it.

968
01:30:27,800 --> 01:30:31,880
And there are very few things other than that sort of inferential formalism that give you that.

969
01:30:32,840 --> 01:30:38,200
Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,

970
01:30:38,200 --> 01:30:47,640
so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are

971
01:30:47,640 --> 01:30:53,560
those who believe that these kind of qualities that we're speaking about, certainly with

972
01:30:53,560 --> 01:30:59,800
conscious experience, for example, that it's not reducible to these kind of simple explanations

973
01:30:59,880 --> 01:31:06,120
that we're talking about, that it has a different character. David Chalmers talks about a philosophical

974
01:31:06,120 --> 01:31:14,360
zombie. So for example, you might behave just like a real human being, but you could be divorced of

975
01:31:14,360 --> 01:31:22,120
conscious experience. So he says that you can think of behavior, dynamics, and function,

976
01:31:22,120 --> 01:31:27,320
and conscious experience as something entirely different. But as an observer, you would never

977
01:31:27,320 --> 01:31:34,440
know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of

978
01:31:34,440 --> 01:31:43,400
people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions

979
01:31:43,400 --> 01:31:50,680
like consciousness as well, I mean, I think it does become very, very difficult, because once

980
01:31:50,680 --> 01:31:56,120
you're putting forward or advocating a theoretical framework that seems like it's supposed to have

981
01:31:56,120 --> 01:32:02,200
all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be

982
01:32:02,200 --> 01:32:08,680
able to ask the right questions or to be able to articulate your hypotheses. So if you think that

983
01:32:08,680 --> 01:32:15,720
consciousness is based upon the idea of having some sense of trajectory of temporal extent and

984
01:32:15,720 --> 01:32:21,640
different worlds I can choose between or different futures I can choose between, that might be a key

985
01:32:21,640 --> 01:32:24,920
part of it. But for some people, that's not what they mean by consciousness.

986
01:32:27,560 --> 01:32:36,280
I found in a particular reading books by people like Anil Seth on this sort of topic, I found one

987
01:32:36,280 --> 01:32:43,480
of the interesting comparisons being the questions about consciousness versus questions about life.

988
01:32:43,480 --> 01:32:48,120
And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,

989
01:32:48,920 --> 01:32:54,600
just because we've had so much of an understanding of the processes involved in life, the dynamics of

990
01:32:54,600 --> 01:33:01,160
life and the way biology works, it's still much more to go. But the question of what life is just

991
01:33:01,160 --> 01:33:06,200
doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions

992
01:33:06,200 --> 01:33:10,440
that were being posed. And perhaps we'll see the same thing with questions like consciousness.

993
01:33:11,800 --> 01:33:17,800
Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting

994
01:33:17,800 --> 01:33:24,760
thought experiment just to get someone to explain just an everyday thing, you know, like what happens

995
01:33:24,760 --> 01:33:32,840
when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and

996
01:33:32,840 --> 01:33:37,240
incomplete the explanations are. And it's the same thing with life, it's the same thing of

997
01:33:37,240 --> 01:33:41,960
consciousness, it's the same thing of causality, agency, intelligence, all of these different things.

998
01:33:41,960 --> 01:33:48,200
And I guess most people don't spend time digging into their understandings of these things and

999
01:33:48,200 --> 01:33:54,360
realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,

1000
01:33:54,360 --> 01:34:02,200
because I think one of the achievements of active inference is blurring the definition of or the

1001
01:34:02,200 --> 01:34:10,120
demarcation between things which are and are not alive. For example, the orthopedic anactivists,

1002
01:34:10,120 --> 01:34:16,040
they think of biology as being instrumental. And what the, you know, free energy principle

1003
01:34:16,040 --> 01:34:21,320
does in my opinion, is it removes the need for this, it almost removes the need for biology

1004
01:34:21,320 --> 01:34:27,240
entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that

1005
01:34:27,240 --> 01:34:32,440
point, though, I think many of our ideas about the world are quite incoherent.

1006
01:34:33,880 --> 01:34:38,040
Yeah. And I think it's interesting that, you know, one of the things that you're saying,

1007
01:34:38,040 --> 01:34:42,280
and I would agree with you as one of the big advantages of active inference-based formalisms,

1008
01:34:42,280 --> 01:34:47,480
you'll probably find some people will say, that's a problem with it, that actually there is a clean

1009
01:34:47,480 --> 01:34:53,240
distinction in their mind between these different things. But then I think the challenge is to work

1010
01:34:53,240 --> 01:34:59,720
out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't

1011
01:34:59,720 --> 01:35:07,480
exist in somebody else's mind. And so getting people to try and or trying to support people to

1012
01:35:07,480 --> 01:35:14,280
be able to express that in a very precise mathematical hypothesis, I think is quite a

1013
01:35:14,280 --> 01:35:19,000
useful way of trying to explore those problems. Because clearly, for some people, there is

1014
01:35:19,000 --> 01:35:22,680
something that's getting at it that is not quite explaining. And it's interesting to try

1015
01:35:22,680 --> 01:35:28,280
and explore that and to work out what that thing is. Indeed, indeed. And just final question,

1016
01:35:28,280 --> 01:35:32,520
what was your experience writing a book? And would you recommend it to other people?

1017
01:35:33,000 --> 01:35:46,120
I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of

1018
01:35:46,120 --> 01:35:51,320
the time compared to, you know, I think anyone who's had some experience of writing papers will

1019
01:35:51,320 --> 01:35:55,160
often find that at the point where you're ready to submit it, you're just sick of it and want to see

1020
01:35:55,160 --> 01:36:01,320
the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments

1021
01:36:01,640 --> 01:36:07,880
writing a book, it obviously takes you much longer. So you end up being almost more sick of it at

1022
01:36:07,880 --> 01:36:13,080
various times. But it's quite fun as a collaborative project. It's quite interesting to get other

1023
01:36:13,080 --> 01:36:17,320
people's perspectives on it. And I was lucky to have great collaborators to write it with.

1024
01:36:18,440 --> 01:36:23,000
And I think it really is a good way of organizing your thoughts in a slightly more holistic way

1025
01:36:23,800 --> 01:36:27,640
than you would while focusing on a very specific topic in a research paper.

1026
01:36:28,600 --> 01:36:33,320
And I've also just enjoyed the response I've had from people who've read it,

1027
01:36:34,040 --> 01:36:41,880
some of whom have picked out a number of errors, not many. But generally,

1028
01:36:41,880 --> 01:36:48,680
everybody's been very supportive of that and people seem to have responded well to it,

1029
01:36:48,680 --> 01:36:51,480
which I think is always encouraging. And that's what we hope should happen.

1030
01:36:52,040 --> 01:36:56,360
Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really

1031
01:36:56,360 --> 01:37:08,920
appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.

