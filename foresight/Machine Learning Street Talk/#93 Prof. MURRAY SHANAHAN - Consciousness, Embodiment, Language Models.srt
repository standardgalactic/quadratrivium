1
00:00:00,000 --> 00:00:05,360
Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior

2
00:00:05,360 --> 00:00:11,040
research scientist at DeepMind. He graduated from Imperial College with a first in computer science

3
00:00:11,040 --> 00:00:18,400
in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the

4
00:00:18,400 --> 00:00:24,080
fields of artificial intelligence, robotics and cognitive science. He's published books such as

5
00:00:24,080 --> 00:00:29,440
Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner

6
00:00:29,440 --> 00:00:34,560
Life was a significant influence on the film Ex Machina for which he was a scientific advisor.

7
00:00:35,920 --> 00:00:40,640
Now Professor Shanahan is a renowned researcher on sophisticated cognition

8
00:00:40,640 --> 00:00:45,920
and its implications for artificial intelligence. His work focuses on agents that are coupled to

9
00:00:45,920 --> 00:00:51,600
complex environments through sensory motor loops such as robots and animals. He's also

10
00:00:51,600 --> 00:00:56,240
particularly interested in the relationship between cognition and consciousness and has

11
00:00:56,240 --> 00:01:01,920
developed a strong understanding of the biological brain and cognitive architectures more generally.

12
00:01:01,920 --> 00:01:07,360
In addition Professor Shanahan is interested in the dynamics of the brain including metastability,

13
00:01:07,360 --> 00:01:12,960
dynamical complexity and criticality as well as the application of this understanding to

14
00:01:12,960 --> 00:01:18,240
machine learning. He's also fascinated by the concept of global workspace theory as proposed

15
00:01:18,240 --> 00:01:23,040
by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive

16
00:01:23,040 --> 00:01:28,240
architecture comprising a set of parallel specialist processes and a global workspace.

17
00:01:28,240 --> 00:01:32,640
Professor Shanahan is committed to understanding the long-term implications of artificial

18
00:01:32,640 --> 00:01:38,960
intelligence both its potential and its risks. His research has been published extensively

19
00:01:38,960 --> 00:01:42,800
and he's a member of the External Advisory Board for the Cambridge Centre of the Study of

20
00:01:42,800 --> 00:01:48,560
Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness.

21
00:01:49,280 --> 00:01:55,600
Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica

22
00:01:55,600 --> 00:02:03,040
in 2016 where he invited us to explore the space of possible minds, a concept first proposed by

23
00:02:03,040 --> 00:02:09,760
philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds

24
00:02:09,760 --> 00:02:15,600
which could exist from those of other animals such as chimpanzees to those of life forms that could

25
00:02:15,600 --> 00:02:21,120
have evolved elsewhere in the universe and indeed those of artificial intelligences.

26
00:02:21,120 --> 00:02:27,760
Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity

27
00:02:27,760 --> 00:02:34,480
for consciousness and human likeness of the behavior. According to Shanahan the space of

28
00:02:34,480 --> 00:02:40,080
possible minds must include forms of consciousness that are so alien that we wouldn't even recognize

29
00:02:40,080 --> 00:02:47,120
them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience,

30
00:02:47,120 --> 00:02:53,840
remember we were talking about Nagel's bat on the Charmer's show, insisting instead that

31
00:02:53,840 --> 00:03:00,320
nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that

32
00:03:00,320 --> 00:03:07,120
while no artifacts exist today, which has anything even approaching human-like intelligence,

33
00:03:07,200 --> 00:03:12,160
the potential for variation in artificial intelligences far outstrips the potential

34
00:03:12,160 --> 00:03:18,160
for variation in naturally evolved intelligence. This means that the majority of the space of

35
00:03:18,160 --> 00:03:25,440
possible minds may be occupied by non-natural variants such as the conscious exotica of

36
00:03:25,440 --> 00:03:31,600
which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites

37
00:03:31,600 --> 00:03:37,600
us to consider the possibility for human-like minds but also for those that are radically different

38
00:03:37,600 --> 00:03:44,560
and inscrutable. He concludes that although we may never understand these alien forms of consciousness,

39
00:03:44,560 --> 00:03:48,400
we can still recognize them as part of the same reality as our own.

40
00:03:50,160 --> 00:03:55,760
So Professor Shanahan has just dropped a brand new paper called Talking about large language models

41
00:03:55,760 --> 00:04:00,960
in which he discusses the capabilities and limitations of large language models.

42
00:04:00,960 --> 00:04:05,680
Now in order to properly comprehend the capacities and boundaries of these models,

43
00:04:05,680 --> 00:04:09,360
we must first grasp the relationship between humans and these systems.

44
00:04:09,920 --> 00:04:15,280
Humans have evolved to survive in a common world and have cultivated a mutual understanding

45
00:04:15,280 --> 00:04:21,200
reflected in their ability to converse about convictions and other mental states. Conversely,

46
00:04:21,200 --> 00:04:26,240
AI systems lack this shared comprehension, so attributing beliefs to them should be done

47
00:04:26,320 --> 00:04:31,040
circumspectly. Now prompt engineering is something that we've all become very familiar with,

48
00:04:31,040 --> 00:04:35,520
we've discussed it a lot on this show recently, and it's almost become a fact of the matter when

49
00:04:35,520 --> 00:04:41,600
it comes to these large language models. It involves exploiting prompt prefixes to adjust

50
00:04:41,600 --> 00:04:46,800
the language models to diverse tasks without needing any supplementary training, allowing for

51
00:04:46,800 --> 00:04:53,680
more effective communication between humans and machines. Nevertheless, lacking a more profound

52
00:04:53,760 --> 00:04:57,840
understanding of the system and its relationship to the external world,

53
00:04:57,840 --> 00:05:02,720
it's difficult to be certain whether the arguments produced by a large language model

54
00:05:02,720 --> 00:05:10,080
are genuine reasoning or simply mimicry. Large language models can be integrated into a variety

55
00:05:10,080 --> 00:05:16,240
of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily

56
00:05:16,240 --> 00:05:22,000
mean that these systems possess completely human-like language abilities. Even though the robot in the

57
00:05:22,000 --> 00:05:28,080
SAKAN system is physically embodied and interacts with the real world, its language is still learned

58
00:05:28,080 --> 00:05:33,200
and used in a dramatically different manner than humans. So in summary, although Professor

59
00:05:33,200 --> 00:05:39,280
Shanahan concludes that large language models are formidable and versatile, they're fundamentally

60
00:05:39,280 --> 00:05:44,800
unlike humans and we must be wary of ascribing human-like characteristics to these systems.

61
00:05:44,800 --> 00:05:50,480
We must find a way to communicate the nature of these systems without resorting to simple terms.

62
00:05:50,480 --> 00:05:55,360
This may necessitate an extended period of interaction and experimentation with the technology,

63
00:05:55,360 --> 00:06:01,440
but it's a fundamental step if we are to accurately portray the capabilities and limitations of

64
00:06:01,440 --> 00:06:07,760
large language models. So anyway, without any further delay, I give you Professor Murray Shanahan.

65
00:06:08,960 --> 00:06:14,320
Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your

66
00:06:14,320 --> 00:06:19,680
background. My background? Well, I've been interested in artificial intelligence for as

67
00:06:19,680 --> 00:06:25,120
long as I can remember since I was a child, really, and I was very much drawn to it by

68
00:06:25,120 --> 00:06:32,800
science fiction, by science fiction movies and books. And then I studied computer science

69
00:06:32,800 --> 00:06:38,400
right from when I was a teenager and got very much drawn into programming, was fascinated by

70
00:06:38,400 --> 00:06:46,240
programming. I really did my 10,000 hours of programming experience when I was quite young

71
00:06:46,240 --> 00:06:51,120
and I went on to do computer science at Imperial College London. That was my degree.

72
00:06:51,920 --> 00:06:56,560
And then still fascinated by artificial intelligence, I went on to Cambridge

73
00:06:57,520 --> 00:07:02,880
and did my PhD in AI in Cambridge, very much in the symbolic school then.

74
00:07:04,560 --> 00:07:10,240
And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic

75
00:07:10,240 --> 00:07:16,160
AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of

76
00:07:16,160 --> 00:07:22,960
segued into studying the brain, which was the obvious example of actual general

77
00:07:22,960 --> 00:07:30,960
intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience

78
00:07:30,960 --> 00:07:37,920
and computational neuroscience and that kind of thing. And then deep learning and deep

79
00:07:37,920 --> 00:07:43,280
reinforcement learning happened in the early 2010s and AI started to get interesting again.

80
00:07:43,280 --> 00:07:51,040
And I got very much back into it that way. And I was particularly impressed by DeepMind's

81
00:07:51,040 --> 00:07:56,800
DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic

82
00:07:56,800 --> 00:08:04,000
step forward. And I really kind of went back to my roots and back to AI at that point.

83
00:08:04,000 --> 00:08:08,080
Yeah, and I think we'll talk about DQN when we speak about your article on consciousness.

84
00:08:08,080 --> 00:08:16,560
But so having such a diverse set of experiences in adjacent fields, how have they influenced each

85
00:08:16,560 --> 00:08:22,880
other? Yeah, well, and one thing I didn't mention is that I've also had a long standing

86
00:08:22,880 --> 00:08:29,280
interest in philosophy. And I very often think that what I am is a sort of weird kind of

87
00:08:29,280 --> 00:08:35,440
philosopher, really. And philosophical questions have had a great attraction for me. So I think

88
00:08:36,560 --> 00:08:42,080
there's a sort of three way into relationship between artificial intelligence, neuroscience,

89
00:08:42,080 --> 00:08:47,440
and the other cognitive sciences and philosophy. And I think they all kind of mutually inform

90
00:08:47,440 --> 00:08:53,280
each other, really. Yeah. Fantastic. So you wrote a book called

91
00:08:53,280 --> 00:08:57,040
Embodiment and the Inner Life. What motivated you to write that book?

92
00:08:57,040 --> 00:09:06,720
Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of

93
00:09:06,720 --> 00:09:14,320
long excursion into thinking about consciousness and about brains, which took place after I had

94
00:09:14,320 --> 00:09:19,600
moved away from symbolic AI, really. So I was thinking about the biological brain.

95
00:09:19,680 --> 00:09:23,520
In the back of my mind, I'd always been fascinated by these philosophical questions about

96
00:09:23,520 --> 00:09:30,800
consciousness. And then I went a bit kind of crazy and started thinking about these things

97
00:09:30,800 --> 00:09:36,160
seriously. It became kind of my day job to think about neuroscience, and about consciousness.

98
00:09:36,160 --> 00:09:42,000
And around about that time, the science of consciousness was taking off as a serious

99
00:09:42,000 --> 00:09:46,800
academic discipline with proper experimental paradigms. So that was really fascinating.

100
00:09:47,440 --> 00:09:54,800
And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,

101
00:09:54,800 --> 00:09:58,960
global workspace theory being one of the leading contenders for a scientific

102
00:09:58,960 --> 00:10:05,200
theory of consciousness. And I was very drawn to global workspace theory, and partly because

103
00:10:06,000 --> 00:10:11,520
it was a computational sort of theory. It drew very heavily on computer science

104
00:10:12,480 --> 00:10:17,600
and computer architectures. There was a computer architecture at the center of the theory.

105
00:10:20,320 --> 00:10:25,520
So this kind of collection of interests, along with my philosophical interests, which all came

106
00:10:25,520 --> 00:10:31,520
together, and I wanted to put them all into a book where I expressed my kind of ideas about,

107
00:10:31,520 --> 00:10:36,080
first of all, from the philosophical side, very heavy influence of Wittgenstein about how we

108
00:10:36,080 --> 00:10:41,600
address these problems at all, then lots of global workspace theory and a certain kind of

109
00:10:41,600 --> 00:10:47,280
global workspace architecture, how that might be realized in the brain, drawing also on the

110
00:10:47,280 --> 00:10:53,600
work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea,

111
00:10:54,160 --> 00:10:57,600
and putting all these things together into one big book.

112
00:10:58,240 --> 00:11:02,560
Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper

113
00:11:02,640 --> 00:11:07,520
and your consciousness paper. But two things that did trigger or prick my ears up,

114
00:11:08,160 --> 00:11:13,520
computationalism, which is quite interesting, because some folks in the cognitive science arena,

115
00:11:13,520 --> 00:11:21,440
especially with the fouries, like examples to escape from computationalism. We did a show on

116
00:11:21,440 --> 00:11:27,600
cells, Chinese room argument the other day. He's probably one of the most known people who do

117
00:11:28,400 --> 00:11:31,200
issue computationalism. So what do you think about that?

118
00:11:31,200 --> 00:11:36,080
Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it

119
00:11:38,880 --> 00:11:46,000
comes out of a kind of computational architecture. But in fact, where I took it was very much moving

120
00:11:46,000 --> 00:11:53,200
away from that original presentation, which drew heavily on a kind of quite an old-fashioned

121
00:11:53,200 --> 00:11:57,120
architectural perspective, sort of boxes and how they communicate with each other and so on.

122
00:11:58,000 --> 00:12:01,760
And I was much more interested in taking it in a direction which is very much more

123
00:12:01,760 --> 00:12:09,200
connectionist and drawing much more heavily on the underlying biology and neuroscience,

124
00:12:09,200 --> 00:12:13,600
which in fact is also a direction that Bernie Barnes himself had moved in, because the book

125
00:12:14,640 --> 00:12:21,280
that originally put forward his theory is from 1988. So that was the predominant way of thinking

126
00:12:21,280 --> 00:12:27,840
at the time was this very computational cognitiveist perspective. So by 2010, when my book was

127
00:12:27,840 --> 00:12:34,160
published, I was very much more interested in a kind of more connectionist perspective on things.

128
00:12:35,120 --> 00:12:39,440
So that's the way that it's portrayed in the book, the theory.

129
00:12:40,000 --> 00:12:47,120
Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation,

130
00:12:48,080 --> 00:12:52,880
because people talk about the church-turing hypothesis and this idea that the universe

131
00:12:52,880 --> 00:12:58,080
could be made of information, which is quite interesting. But do you believe that the world

132
00:12:58,080 --> 00:13:02,240
that we live in could be computationally represented and computed?

133
00:13:04,240 --> 00:13:07,600
Well, I'm not sure that I have a belief on that particular one.

134
00:13:09,760 --> 00:13:15,520
So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on

135
00:13:15,520 --> 00:13:22,400
quantum mechanics, and he thinks that quantum effects are important for consciousness. But

136
00:13:22,400 --> 00:13:28,000
I mean, that's very much a minority, a tiny minority view within the people who study

137
00:13:28,000 --> 00:13:34,480
consciousness from a scientific standpoint. And so I don't really subscribe to that

138
00:13:35,040 --> 00:13:39,200
interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke

139
00:13:39,200 --> 00:13:44,080
to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists,

140
00:13:45,040 --> 00:13:51,200
their big argument is about the subject of experience and the limits of our cognitive

141
00:13:51,200 --> 00:13:57,920
horizon and the inability really for us to reduce things into a comprehensible framework of

142
00:13:57,920 --> 00:14:02,800
understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched

143
00:14:02,800 --> 00:14:10,800
right into some really big, difficult topics here, right? So in my book, Embodiment in the

144
00:14:10,960 --> 00:14:15,120
Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of

145
00:14:15,120 --> 00:14:23,040
consciousness. But one of the big sort of outstanding things for me in one of the

146
00:14:23,040 --> 00:14:27,520
outstanding questions that I have not really answered, I felt in that book, is very much

147
00:14:27,520 --> 00:14:35,520
related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the

148
00:14:35,520 --> 00:14:42,320
idea that there's a sort of intuitive idea that maybe there can be very exotic entities,

149
00:14:42,320 --> 00:14:48,560
very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of

150
00:14:48,560 --> 00:14:54,800
consciousness there that we could barely grasp its nature. And this is a sort of natural

151
00:14:54,800 --> 00:14:59,200
intuitive thought. And especially when we look at other animals, like bats, and especially if we

152
00:14:59,200 --> 00:15:05,360
look at an animal that's a bit different from us, then we get hints that there's some

153
00:15:05,760 --> 00:15:11,280
one at home, as it were, and that there's consciousness there. I think we, I'm sure all

154
00:15:11,280 --> 00:15:18,320
of us believe that cats and dogs, and many other animals are conscious and are capable of suffering

155
00:15:18,320 --> 00:15:24,640
and having awareness of the world that's like our awareness and are aware of us and each other.

156
00:15:28,000 --> 00:15:34,080
I mean, I take that as almost axiomatic. That's just the way we treat those creatures.

157
00:15:34,080 --> 00:15:38,640
But then when we think about something like a bat, it's very different from us. So the natural

158
00:15:38,640 --> 00:15:44,320
thing thought is that maybe what it's like is very, very different from what it's like for us,

159
00:15:44,320 --> 00:15:52,800
and it's a natural thought to express. And of course, Nagel takes that thought

160
00:15:54,640 --> 00:16:03,600
to suggest that there are something that is inaccessible to us, which is what is it like to

161
00:16:03,600 --> 00:16:09,280
be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm

162
00:16:09,280 --> 00:16:17,520
very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very

163
00:16:17,520 --> 00:16:22,000
natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says,

164
00:16:22,000 --> 00:16:27,600
for example, you know, nothing is hidden. So he's very, you know, and the whole private language

165
00:16:27,600 --> 00:16:32,480
remarks are all about sort of saying, well, this intuition that we have that there's this

166
00:16:32,560 --> 00:16:39,600
private realm of experience is actually just, it's just a philosophical trick of the mind

167
00:16:39,600 --> 00:16:45,600
to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective

168
00:16:45,600 --> 00:16:53,680
experience in others. And that's his whole thrust of his philosophy or that aspect of it

169
00:16:53,680 --> 00:16:57,200
is to try and undermine that. So these two things are intention, right? So there's this

170
00:16:57,280 --> 00:17:02,960
natural thought that bats, you know, it must be like something to be a bat, but what is it like

171
00:17:02,960 --> 00:17:06,960
and how could we ever know? And then there's the Viconstinian thought, which is actually very

172
00:17:06,960 --> 00:17:11,760
difficult to kind of really embrace. But it's that there's a sense in which nothing is really

173
00:17:11,760 --> 00:17:16,720
metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't

174
00:17:16,720 --> 00:17:21,280
know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined

175
00:17:21,280 --> 00:17:25,680
their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden,

176
00:17:25,680 --> 00:17:29,280
whereas Nagel's point is that there's something that's deeply, profoundly,

177
00:17:29,280 --> 00:17:33,360
philosophically, metaphysically hidden, which is the subjective. Now we can extend that,

178
00:17:33,360 --> 00:17:40,320
shall carry on. So I'm rambling now. So now we can extend that thought about bats,

179
00:17:40,320 --> 00:17:44,160
now, you know, especially from the perspective of the sort of thing that I'm interested in,

180
00:17:44,160 --> 00:17:48,400
to, well, not just bats, but what about the whole space of possible minds to use

181
00:17:48,400 --> 00:17:53,600
Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,

182
00:17:54,480 --> 00:17:59,200
you know, who surely there is extraterrestrial intelligence out there, it's going to be very,

183
00:17:59,200 --> 00:18:04,640
very, very different to us. So, and then what about the things that we build? Maybe we can build

184
00:18:05,520 --> 00:18:11,840
things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build

185
00:18:11,840 --> 00:18:15,440
something that is also conscious, it's the kind of thing that's depicted in science fiction all

186
00:18:15,440 --> 00:18:19,920
the time. In science fiction, it's often depicted as very human-like, but there's no reason why it

187
00:18:19,920 --> 00:18:24,880
should be human-like at all. And so we can imagine these very, very exotic entities, and then the

188
00:18:24,880 --> 00:18:29,200
question is even bigger, you know, there could be something that we, we won't even be able to recognize

189
00:18:29,200 --> 00:18:33,760
that there was even the possibility of consciousness, but maybe it's buried there inside this complex

190
00:18:33,760 --> 00:18:38,720
thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this

191
00:18:38,720 --> 00:18:44,240
paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian

192
00:18:44,240 --> 00:18:50,640
perspective encompass this possibility as well. Yeah, and maybe we should talk about that before

193
00:18:50,640 --> 00:18:56,640
the language paper, just because it's, it's what we're talking about now. But there's a few things

194
00:18:56,640 --> 00:19:01,440
you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the

195
00:19:01,440 --> 00:19:08,320
machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know,

196
00:19:08,320 --> 00:19:14,640
introduced this kind of mind-body dualism, you know, which was kind of a move away from

197
00:19:14,640 --> 00:19:18,400
the previous desire to have a mechanistic understanding of the world that we live in.

198
00:19:18,400 --> 00:19:24,400
Humans want to understand, and actually so many things in the world eludes our understanding.

199
00:19:24,400 --> 00:19:29,520
And then that brings us on to David Chalmers' point that the hard problem of consciousness,

200
00:19:29,520 --> 00:19:35,280
which I suppose is an extension of the mind-body problem. And it's, as you were saying, this

201
00:19:35,280 --> 00:19:39,920
little bit extra, right? So we think about, and I agree with Chalmers that intelligence and

202
00:19:39,920 --> 00:19:44,800
consciousness are likely entangled or would co-occur together. But he always said that there's

203
00:19:44,800 --> 00:19:50,560
function, dynamics, and behavior. And then there's that little subjective thing on the top. And for

204
00:19:50,560 --> 00:19:55,280
Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just

205
00:19:55,280 --> 00:19:59,760
something on top. It's not really requisite for anything else. And I believe it might be requisite

206
00:19:59,760 --> 00:20:04,880
for intentionality and agency as so did. But what's your take?

207
00:20:04,880 --> 00:20:08,480
Well, it's interesting because the whole way that you put that and the whole way that

208
00:20:08,480 --> 00:20:13,680
people often talk about this thing is you speak about consciousness. Like, there's this thing,

209
00:20:13,680 --> 00:20:17,920
which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe

210
00:20:17,920 --> 00:20:25,360
it's this, maybe it's that. But I think that whole way of talking is, which is natural for us

211
00:20:25,360 --> 00:20:29,200
in many everyday situations. But when it comes to this kind of conversation, I think that whole

212
00:20:29,200 --> 00:20:35,440
way of talking is maybe not quite right, because we're thinking of consciousness as this, you know,

213
00:20:35,440 --> 00:20:39,200
we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to

214
00:20:39,200 --> 00:20:44,160
take a step back and we have to say, well, when we talk about, when we use that word,

215
00:20:44,160 --> 00:20:48,720
conscious or consciousness, so we use it in all kinds of different ways in different contexts.

216
00:20:48,720 --> 00:20:55,200
And so when we talk about, you know, we might talk about it in the context of an animal, we might

217
00:20:55,200 --> 00:21:01,520
say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see

218
00:21:02,080 --> 00:21:07,120
the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see

219
00:21:07,120 --> 00:21:11,840
the squirrel, you know, and it can smell more like you'd smell all of these things as well.

220
00:21:12,400 --> 00:21:18,080
So we use consciousness, you know, we talk about consciousness in that sense. And we also talk

221
00:21:18,080 --> 00:21:23,520
about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts

222
00:21:23,520 --> 00:21:30,560
and we talk about our inner life and we use consciousness to encompass that as well.

223
00:21:31,520 --> 00:21:37,920
We often use consciousness in the context scientifically of a distinction between

224
00:21:38,720 --> 00:21:42,640
conscious and unconscious processes. And that's a very interesting distinction because

225
00:21:43,360 --> 00:21:48,640
when we're consciously aware of a stimulus as humans, then a whole lot of things come together.

226
00:21:48,880 --> 00:21:55,040
We're able to kind of like deal with novelty better, we're able to report it, we're able to

227
00:21:55,040 --> 00:22:01,280
remember things better. So whereas when we perhaps are unconsciously or there's a kind

228
00:22:01,280 --> 00:22:05,680
of unconscious processing of the stimulus, then we still can respond to it behaviorally, but

229
00:22:06,560 --> 00:22:10,320
and it can have queuing effects and so on, but it doesn't have all those other things.

230
00:22:10,320 --> 00:22:16,320
So this and that's kind of, there's a kind of integrative function for consciousness there.

231
00:22:16,320 --> 00:22:22,240
And then on top of all of that, there is the capacity for suffering and joy that comes with.

232
00:22:22,240 --> 00:22:29,280
So often there's valence to consciousness, you know, so that's another thing.

233
00:22:29,280 --> 00:22:32,800
So all of these things, they come as a package in humans, but when we speak about

234
00:22:33,360 --> 00:22:41,200
edge cases, then these things come apart and we need to speak about them separately, I think.

235
00:22:41,200 --> 00:22:46,240
Fascinating. I mean, there are two kind of minor digressions there. I mean,

236
00:22:46,240 --> 00:22:50,000
you were talking about these planes of consciousness, which is also very interesting.

237
00:22:50,000 --> 00:22:54,640
And maybe we could get into the integrated information theory or the global workspace

238
00:22:54,640 --> 00:22:57,760
theory just for the audience, just to give them some context.

239
00:22:57,760 --> 00:23:00,560
Yeah, sure. Or do you want me to say a few words about that?

240
00:23:00,560 --> 00:23:01,520
Oh, please, yeah.

241
00:23:01,520 --> 00:23:07,920
Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness.

242
00:23:08,720 --> 00:23:12,160
And you just mentioned two of the leading ones, which are global workspace theory and

243
00:23:12,160 --> 00:23:17,600
integrated information theory. And so global workspace theory. So that's, that's Bernie

244
00:23:17,600 --> 00:23:23,360
Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues.

245
00:23:23,360 --> 00:23:28,240
So the idea there is it's, it does rest on this sort of architectural idea, which is that,

246
00:23:30,160 --> 00:23:35,440
which is that we think of the brain, the biological brain as comprising, you know,

247
00:23:35,440 --> 00:23:39,600
a very large number of parallel processes. This is kind of a natural way to think of the brain,

248
00:23:39,600 --> 00:23:45,120
a large number of parallel processes. And it, and the global workspace theory posits a particular

249
00:23:45,120 --> 00:23:50,480
way in which these, these processes interact and communicate with each other. And that is via

250
00:23:50,480 --> 00:23:56,000
this global workspace. And the idea there is that, is that there are sort of two modes of

251
00:23:56,000 --> 00:24:03,040
processing that go on. So in one mode of processing, the, these parallel processes just do their,

252
00:24:03,040 --> 00:24:09,440
their own thing independently. And in the other mode of processing, they are working via this

253
00:24:09,440 --> 00:24:14,640
global workspace theory. So the idea is that they, you might think of them as, as, you know,

254
00:24:14,640 --> 00:24:19,680
depositing messages, if you like, in this global workspace, which are then broadcast out to all

255
00:24:19,680 --> 00:24:23,440
of the other processes. So, so it's, so there's this kind of, but I think thinking of it in

256
00:24:23,440 --> 00:24:27,600
terms of messages is not quite the right way of thinking of it is better to think in terms of

257
00:24:27,600 --> 00:24:31,840
kind of signaling and information and so on. But that's a natural way to think of it. But

258
00:24:31,840 --> 00:24:39,600
so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence

259
00:24:39,600 --> 00:24:45,200
to all the other processes. And that's the global kind of broadcast aspect of it. And that's when

260
00:24:45,200 --> 00:24:50,640
consciousness, well, that's when information processing is conscious, according to global

261
00:24:50,640 --> 00:24:54,560
workspace theory, as opposed to when it's all just local and the processes are doing their own

262
00:24:54,560 --> 00:25:00,480
thing. That's, that's not that that processing is not conscious. So there's a dist, so it's

263
00:25:00,480 --> 00:25:05,200
about teasing out this distinction between conscious information processing and unconscious

264
00:25:05,200 --> 00:25:11,040
information processing. Now, all of those terms, by the way, are deeply philosophically problematic

265
00:25:11,040 --> 00:25:15,360
and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all

266
00:25:15,360 --> 00:25:20,320
in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about

267
00:25:20,320 --> 00:25:24,800
so the essential idea, though, is to do with broadcast and dissemination of information

268
00:25:24,800 --> 00:25:29,120
throughout the brain and going from like local processes and help them having global influence.

269
00:25:29,200 --> 00:25:32,240
And that's what consciousness is all about according to global workspace theory.

270
00:25:33,200 --> 00:25:37,920
Okay, so integrated information theory. So I think so integrated information theory,

271
00:25:37,920 --> 00:25:48,000
which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible

272
00:25:48,000 --> 00:25:52,640
in some ways with with global workspace theory. But I don't think that's, that's true. I think

273
00:25:52,640 --> 00:25:56,320
I think that there's a lot of synergy between the two theories, in fact.

274
00:25:56,800 --> 00:26:03,920
But but that's because they so they come with the same for integrated information theory

275
00:26:04,640 --> 00:26:11,040
has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down

276
00:26:11,040 --> 00:26:17,120
a property, which is almost like a physical property, which is identical with consciousness.

277
00:26:17,120 --> 00:26:22,800
So you can actually speak about the amount of consciousness in any system that you that you

278
00:26:22,800 --> 00:26:28,080
look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of

279
00:26:28,080 --> 00:26:33,680
how much consciousness is present in the system, like, like part of your brain, your whole brain,

280
00:26:33,680 --> 00:26:39,440
or you as a person, or a flock of bats, or whatever, so you can or toaster, you know,

281
00:26:39,440 --> 00:26:44,720
so you can give a number to how much consciousness there is, there is there according to his theory.

282
00:26:44,720 --> 00:26:51,360
And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's

283
00:26:51,440 --> 00:26:57,120
all about trying to see how much information is processed by the individual parts of the system

284
00:26:57,760 --> 00:27:04,240
versus how much information is processed by all the parts put together. And it's and it's to do

285
00:27:04,240 --> 00:27:10,320
with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how

286
00:27:10,320 --> 00:27:17,120
much consciousness there is there. And, and in a way, it actually has some synergies. If you as

287
00:27:17,120 --> 00:27:22,160
long as you don't think that it's necessarily measuring, you know, this property of the of

288
00:27:22,160 --> 00:27:26,560
the universe, which you can put a number on. But it has some synergies with global workspace theory,

289
00:27:26,560 --> 00:27:34,000
because they're both distinguishing between global holistic things versus local things. And the

290
00:27:34,000 --> 00:27:40,640
and the consciousness is in the kind of global holistic processing versus the local, you know,

291
00:27:40,640 --> 00:27:44,640
local processing in both those theories. So there's a kind of, you know, there's some

292
00:27:44,640 --> 00:27:48,640
intuitions that they have in common, I think. Interesting. And it also reminds me a little

293
00:27:48,640 --> 00:27:55,120
bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain

294
00:27:55,120 --> 00:28:01,520
types of information processing. And the processing must represent causal structures as well. So it

295
00:28:01,520 --> 00:28:07,360
can't it's it's not an appeal to panpsychism per se. And although with with all of the things

296
00:28:07,360 --> 00:28:11,520
that you've just spoken about, what do they work in another universe? I mean, I guess what I'm

297
00:28:11,520 --> 00:28:17,520
saying is, is it just the the physical and the information processing or in a different universe

298
00:28:17,520 --> 00:28:22,000
might it not emerge in the same way? Yeah, which depends what you mean by a different universe,

299
00:28:22,000 --> 00:28:25,280
I guess. What do you mean by a different universe? Well, if the laws of nature were different.

300
00:28:25,840 --> 00:28:28,800
Yeah, okay. So if the laws of physics were different.

301
00:28:30,560 --> 00:28:38,800
Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an

302
00:28:38,800 --> 00:28:47,440
ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess

303
00:28:48,720 --> 00:28:58,160
I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I

304
00:28:58,160 --> 00:29:07,520
subscribe to these to these isms. So what I really mean by that is that is that I imagine that a

305
00:29:07,520 --> 00:29:12,560
system that is organized in a particular way functionally in terms of its information processing.

306
00:29:12,560 --> 00:29:18,720
And if that system is in is embodied in the broadest sense, and, you know, and meets lots of

307
00:29:18,720 --> 00:29:24,560
other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word

308
00:29:24,560 --> 00:29:30,240
conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature.

309
00:29:30,800 --> 00:29:37,200
So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need

310
00:29:37,760 --> 00:29:42,400
to give rise to the behavior you need to talk about thing, the thing in a certain way.

311
00:29:43,520 --> 00:29:47,120
My question today, because I posed this question to Chalmers last week, because he's also a

312
00:29:47,120 --> 00:29:51,600
functionalist. And I agree with the degree of functionalism describing intelligence,

313
00:29:51,600 --> 00:29:55,760
but less so with consciousness, you know, there's not a Turing test for consciousness, for example.

314
00:29:56,240 --> 00:30:01,440
But the thing is with functionalism, we're at risk of doing what you said people do with

315
00:30:01,440 --> 00:30:05,120
large language models, which is anthropomorphizing them, because these functions are intelligible

316
00:30:05,120 --> 00:30:09,360
to us. And then our conception of intelligence becomes somewhat observer relative.

317
00:30:13,760 --> 00:30:21,840
Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious

318
00:30:21,840 --> 00:30:30,240
to you, but not to someone else? Well, so, so, so in all of these cases, I mean,

319
00:30:30,240 --> 00:30:35,920
I think it's about the words that we use in our language to talk about the things. So, so,

320
00:30:35,920 --> 00:30:41,440
so if there's someone else is someone just like us, right, then we have to and if we want to use

321
00:30:41,440 --> 00:30:46,160
the words in different ways. So, so the large language models are a great case in point, right.

322
00:30:46,560 --> 00:30:53,280
So, so suddenly we're arriving at a point where somebody can describe something as conscious.

323
00:30:54,240 --> 00:30:58,480
And others can say that's rubbish, you know, it's not that's not true at all. And so we,

324
00:30:58,480 --> 00:31:03,280
so we've, we've arrived at a point where these philosophically problematic words, which,

325
00:31:04,240 --> 00:31:10,400
which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in

326
00:31:10,400 --> 00:31:14,960
agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so

327
00:31:14,960 --> 00:31:18,960
much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an

328
00:31:18,960 --> 00:31:23,520
anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain.

329
00:31:24,880 --> 00:31:31,760
Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know,

330
00:31:31,760 --> 00:31:36,960
that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then,

331
00:31:37,760 --> 00:31:42,880
and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action.

332
00:31:42,880 --> 00:31:47,360
So there we're using the terms in ways that we all understand. But now we're getting to a point

333
00:31:47,360 --> 00:31:52,640
where suddenly, these words or these concepts are being used, you know, we don't have an way,

334
00:31:52,640 --> 00:31:57,440
we don't have agreement about how to use these words, right? Because it's, there are these exotic

335
00:31:57,440 --> 00:32:02,400
edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there

336
00:32:02,400 --> 00:32:11,840
a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say

337
00:32:11,840 --> 00:32:16,480
is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to,

338
00:32:17,120 --> 00:32:23,360
I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive

339
00:32:23,360 --> 00:32:27,600
at a new consensus about how we use these words. So that might mean that we extend the words,

340
00:32:27,600 --> 00:32:32,960
we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the

341
00:32:32,960 --> 00:32:38,560
world from self awareness, from integration, cognitive integration from the capacity for

342
00:32:38,560 --> 00:32:43,280
suffering, because suddenly we have things that where that they don't all come as a package. And

343
00:32:43,920 --> 00:32:47,600
when we need to kind of be a bit more nuanced in the way that we use these words, we need to use

344
00:32:47,600 --> 00:32:52,400
them in new ways. But then there's a kind of transition period, because we don't, you know,

345
00:32:52,400 --> 00:32:56,640
we're all arguing about how to use these words all of a sudden, because we've got weird edge cases.

346
00:32:56,640 --> 00:33:01,280
So there's going to be a time when it'll take a time for language to settle back down again.

347
00:33:01,280 --> 00:33:07,280
So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you

348
00:33:07,280 --> 00:33:14,800
like, but then, but then there's a kind of consensus needs to emerge, right? But so many

349
00:33:14,800 --> 00:33:21,120
things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible.

350
00:33:21,680 --> 00:33:27,680
And what platonic? Because what we're talking about here is reductionism and the, I mean,

351
00:33:27,680 --> 00:33:32,000
the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,

352
00:33:32,000 --> 00:33:36,400
complex phenomenon beyond our cognitive horizon. And as much as we don't want to,

353
00:33:36,400 --> 00:33:40,720
we use functions derived from behavior to have some common understanding of this thing.

354
00:33:40,720 --> 00:33:44,560
But I wasn't being reductionist, was I? Do you think I was being reductionist?

355
00:33:44,560 --> 00:33:51,120
Well, no. So you said that the language game converges. And in some cases, we will arrive on

356
00:33:51,120 --> 00:33:55,360
a common definition, but like you can bring in Hofstatter as well. Well, not a common definition,

357
00:33:55,360 --> 00:34:01,680
but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement,

358
00:34:01,680 --> 00:34:06,880
right? So that's what I, and the reason why I mean, I would, and the reason I would balk at

359
00:34:06,880 --> 00:34:11,920
using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well

360
00:34:11,920 --> 00:34:18,640
to any kind of ism is because I just think that that may be the way things are organized when

361
00:34:18,640 --> 00:34:24,640
you take them apart. So, you know, brains, right, when you examine them on the inside,

362
00:34:24,640 --> 00:34:30,800
like animal brains, you might look at how an octopus's brain works. And that might inform

363
00:34:30,800 --> 00:34:35,280
whether you think that it suffers, can experience this pain or not. Or we might break apart, you

364
00:34:35,280 --> 00:34:39,920
know, an AI of the system of the future, right? You know, and we might break it apart and we may

365
00:34:39,920 --> 00:34:45,440
look at its functional organization. And that all is just is grist to the mill of how our language

366
00:34:45,440 --> 00:34:51,360
might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this

367
00:34:51,360 --> 00:34:57,600
is that it with some big metaphysical capital letters on the is, right? That's really important.

368
00:34:58,320 --> 00:35:03,680
So the functional organization of these other things, which when we study and look at it,

369
00:35:03,680 --> 00:35:09,120
is all just part becomes part of a conversation that eventually is going to help us to settle on

370
00:35:09,120 --> 00:35:14,560
maybe new ways of talking about these things. I think we agree with each other. I think the

371
00:35:14,560 --> 00:35:18,880
difference is, so with the parable of the blind men and the elephant, all of the men around the

372
00:35:18,880 --> 00:35:24,480
elephant saw something which was part of the truth. And I think that's what we're describing with

373
00:35:24,480 --> 00:35:31,440
the function. So we can all agree on what perception means or what action means. The thing is,

374
00:35:31,440 --> 00:35:35,440
there will be many other functions that will represent a different slice of that cognitive

375
00:35:35,440 --> 00:35:39,520
phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because

376
00:35:39,520 --> 00:35:43,200
there's lots of people coming with kind of like new ideas and new theories. I mean,

377
00:35:44,400 --> 00:35:48,320
Anil Seth, for example, have you had Anil on your on your not yet being right?

378
00:35:49,200 --> 00:35:54,960
So Anil's written this great book called Being You. And Anil brings in a whole kind of new set

379
00:35:54,960 --> 00:36:02,080
of ideas. He's particularly interested in the sort of top down effects on perception, top

380
00:36:02,080 --> 00:36:06,160
down effects on perception. So then he brings in this kind of top down influence and perception

381
00:36:06,160 --> 00:36:11,520
as a big part of things. And then there's Graziano has written this book using this about

382
00:36:11,600 --> 00:36:18,800
his attention schema theory of consciousness. And that's, and, you know, there's a whole set

383
00:36:18,800 --> 00:36:22,640
of interesting ideas there. And I think you're absolutely right. I think there's, I think there's

384
00:36:22,640 --> 00:36:31,120
aspects of all of these things all feed into, you know, all feed into the way, you know,

385
00:36:32,320 --> 00:36:40,080
brains and animals work and all of them feed into the, you know, why they behave the way they do

386
00:36:40,080 --> 00:36:44,560
and why we use those words when we use those words, you know, conscious and aware and so

387
00:36:44,560 --> 00:36:50,560
fascinating. We'll get to your article in a second. But as someone who has such a diverse

388
00:36:50,560 --> 00:36:55,120
and interesting background, who is allowed to ask these philosophical questions? So it reminds me

389
00:36:55,120 --> 00:37:00,240
and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers

390
00:37:00,240 --> 00:37:05,600
about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question.

391
00:37:05,600 --> 00:37:09,360
You know, I mean, so why should I have any right to speak about any of these things at all? Because

392
00:37:09,360 --> 00:37:18,000
I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to,

393
00:37:18,000 --> 00:37:21,600
to, I guess there are two things, right? There's, I guess, I guess there's, there's

394
00:37:21,600 --> 00:37:26,640
in that kind of discussion between the neuroscientists and the, and the philosophers. So there you,

395
00:37:26,640 --> 00:37:30,160
there you're not talking about, you know, the everyday conversation that we're all having as,

396
00:37:30,160 --> 00:37:35,840
as, as humanity or as English speakers, or as Chinese speakers about how we use these, these,

397
00:37:35,920 --> 00:37:40,720
these words. So there it's a much more kind of confined to the, to, to different, two different

398
00:37:40,720 --> 00:37:50,400
schools or disciplines within academia. So there, I mean, I do think that the people who work in AI

399
00:37:50,400 --> 00:37:56,000
and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical

400
00:37:56,000 --> 00:37:59,840
debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with,

401
00:37:59,840 --> 00:38:06,240
with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101,

402
00:38:06,880 --> 00:38:11,440
which people should at least be aware of Descartes arguments and then Chalmers, and the different

403
00:38:11,440 --> 00:38:15,920
kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean,

404
00:38:15,920 --> 00:38:20,160
you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you

405
00:38:20,160 --> 00:38:25,920
hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists

406
00:38:25,920 --> 00:38:32,160
need to, you know, you know, they need to kind of have a, a past to enter the conversation,

407
00:38:32,160 --> 00:38:37,760
which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with

408
00:38:37,760 --> 00:38:42,560
the, with the ethics folks, actually, because, because we have a lot of them fields of expertise,

409
00:38:43,120 --> 00:38:46,720
and engineers should learn more about ethics. Yeah, absolutely. But when they do have an

410
00:38:46,720 --> 00:38:51,280
opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit

411
00:38:51,280 --> 00:38:57,040
naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but

412
00:38:57,040 --> 00:39:01,520
that's an interesting and the difficult area in itself. Because of course, you know, you,

413
00:39:03,120 --> 00:39:08,240
as a scientist, it's important that you take responsibility as a scientist and the, and that

414
00:39:08,240 --> 00:39:12,560
you take, you know, some ethical responsibility. But at the same time, you know, you've only got

415
00:39:12,560 --> 00:39:18,240
so much time to become an expert. So, so it's difficult to at the same time, take ethical

416
00:39:18,320 --> 00:39:24,880
responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read,

417
00:39:24,880 --> 00:39:31,280
you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody

418
00:39:31,280 --> 00:39:37,360
again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses

419
00:39:37,360 --> 00:39:42,400
teach, you know, ethics, these days, many kind of science and computer science. It's part of,

420
00:39:42,400 --> 00:39:46,800
you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting

421
00:39:46,800 --> 00:39:50,960
analogy between the functionalism that we were speaking about in consciousness. I mean, even

422
00:39:50,960 --> 00:39:55,360
in our own research domain, we have the function of ethics, and we have linguists, and we have

423
00:39:55,360 --> 00:39:59,360
all sorts of different people. And that is the blind man and the elephant. And, you know, I

424
00:39:59,360 --> 00:40:05,040
tend to believe that even though the views from these diverse folks are inconsistent, diversity

425
00:40:05,040 --> 00:40:10,080
is very important. Oh, incredibly important. Intellectual diversity is, you know, every

426
00:40:10,080 --> 00:40:14,720
kind of diversity is important. And intellectual diversity is immensely important. And having

427
00:40:14,800 --> 00:40:19,680
these conversations, these interdisciplinary conversations is absolutely, you know, essential.

428
00:40:19,680 --> 00:40:24,800
So at least if people are talking to each other, that's a really, really positive thing, I think.

429
00:40:24,800 --> 00:40:29,840
Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way,

430
00:40:29,840 --> 00:40:35,120
Chalmers took a very functionalist approach to talking about consciousness. But I guess,

431
00:40:35,920 --> 00:40:40,080
after that tweet, everyone in the community started thinking about and talking about

432
00:40:40,080 --> 00:40:43,840
consciousness. So maybe let's just start from that tweet. How did you find it?

433
00:40:43,920 --> 00:40:51,280
Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large

434
00:40:51,280 --> 00:41:00,240
language models are slightly conscious. And then I replied, tweeted back in the same sense that

435
00:41:00,800 --> 00:41:07,200
may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean,

436
00:41:07,200 --> 00:41:11,840
I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent

437
00:41:11,840 --> 00:41:17,920
out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you

438
00:41:17,920 --> 00:41:28,400
know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think

439
00:41:28,400 --> 00:41:36,080
about, about what he said at that point. But then, but then I after tweeting my, my flippant

440
00:41:36,160 --> 00:41:42,880
response, I then I was violating all my own Twitter rules in in just sending back a flippant

441
00:41:42,880 --> 00:41:49,040
response, because I generally don't do that. I would rather kind of, you know, be professional,

442
00:41:49,040 --> 00:41:53,360
engage professionally. And so I thought it's very important to follow that on with a, you know,

443
00:41:53,360 --> 00:41:58,720
with a little explanation of why, you know, why I thought that it wasn't really appropriate to

444
00:41:58,720 --> 00:42:04,160
speak about today's large language models in those terms. Yeah. And for me, the number one thing is

445
00:42:04,160 --> 00:42:12,560
to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us

446
00:42:12,560 --> 00:42:18,400
being able to use that, that word, use words like consciousness and so on, you know, in the way that

447
00:42:18,400 --> 00:42:24,640
we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something

448
00:42:26,160 --> 00:42:31,360
that that inhabits our world. And by inhabits, I don't mean just has a physical, you know,

449
00:42:31,360 --> 00:42:34,960
like a computer is obviously a physical thing in our world, but inhabits our world means that,

450
00:42:35,680 --> 00:42:42,480
you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the

451
00:42:42,480 --> 00:42:50,320
same world as us and interacts with it and, and, and, and you know, and interacts with the objects

452
00:42:50,320 --> 00:42:58,800
in it and with other, with other creatures like ourselves. So, so that to my mind, that is,

453
00:42:58,800 --> 00:43:04,560
that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase

454
00:43:04,560 --> 00:43:09,280
trans channeling Wittgenstein that that only in the context of something that

455
00:43:11,040 --> 00:43:17,440
that exhibits purposeful behavior, do we speak of consciousness. And the way that that's

456
00:43:17,440 --> 00:43:22,960
phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things,

457
00:43:22,960 --> 00:43:27,840
because you notice that he's saying that he's making what he's saying is actually he's talking

458
00:43:27,840 --> 00:43:32,720
about the way we use the word. So he's not making a metaphysical claim. This is essential. He's

459
00:43:32,720 --> 00:43:38,800
saying that this is just this is the circumstances under which we use this word. So we use this word

460
00:43:38,800 --> 00:43:44,720
in the context of fellow creatures, basically. And so, so that's the kind of the starting point.

461
00:43:44,720 --> 00:43:49,040
So a large, and of course, we, of course, we talk to people on the telephone and over the

462
00:43:49,040 --> 00:43:54,400
internet and so on. And we don't, you know, we may not, you know, we can't see them or anything.

463
00:43:54,400 --> 00:44:00,880
So we, but, but, but we still, we know that there is, you know, or we assume, we've always been

464
00:44:00,880 --> 00:44:04,720
able to assume up to this point that there is a fellow creature at the other end. And that's the

465
00:44:04,720 --> 00:44:11,040
kind of grounding for kind of thinking that way and using using that word. Now that is absent

466
00:44:11,040 --> 00:44:15,680
in large language models. So large language models do not inhabit the world that we do.

467
00:44:19,280 --> 00:44:24,720
Now, I mean, we have to caveat that because, of course, it's possible to embed a large language

468
00:44:24,720 --> 00:44:30,800
model in a, you know, in a, we always do embed it in a larger system. It might be very simple

469
00:44:30,800 --> 00:44:35,200
embedding. It might be just a chatbot, or it might be much more complicated, like it might

470
00:44:35,200 --> 00:44:40,560
be be part of a system that enables a robot to kind of move around and interact with the world

471
00:44:40,560 --> 00:44:47,120
and take instructions and so on. So there was a great, some great work from Google with their

472
00:44:47,120 --> 00:44:51,760
Palm Seican robot, for example, where there's this embedded large language model. So, so,

473
00:44:51,760 --> 00:44:57,120
so there you're kind of moving in a, in a direction where maybe where these, where these words, you

474
00:44:57,120 --> 00:45:04,080
know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.

475
00:45:04,080 --> 00:45:08,800
Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,

476
00:45:08,800 --> 00:45:14,560
right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for,

477
00:45:14,560 --> 00:45:19,600
for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,

478
00:45:19,600 --> 00:45:23,360
but at least it would, you'd meet the necessary conditions, right? Yes. But the large language

479
00:45:23,360 --> 00:45:30,240
models by themselves do not meet even, they're not even candidates. Yes, I agree. And we,

480
00:45:30,240 --> 00:45:34,000
there's so many things we can do here, because we can, we can talk about embodiment in general. I

481
00:45:34,000 --> 00:45:39,360
mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a

482
00:45:39,360 --> 00:45:44,160
representationalist view of artificial intelligence or, or rejecting, rejecting a representation.

483
00:45:44,160 --> 00:45:48,080
Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best

484
00:45:48,080 --> 00:45:52,880
representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking

485
00:45:52,880 --> 00:45:57,600
about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity,

486
00:45:57,600 --> 00:46:01,760
and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your

487
00:46:01,760 --> 00:46:06,080
paper, you said, although the language model component of SACAN provides natural language

488
00:46:06,080 --> 00:46:10,480
descriptions of low level actions, it doesn't take into account what the environment actually

489
00:46:10,480 --> 00:46:15,280
affords the robots. And there's this whole affordance thing as well. So, I mean, how do you

490
00:46:15,280 --> 00:46:22,800
think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we

491
00:46:22,800 --> 00:46:30,720
have as of, you know, the end of 2022 of something that we really can describe as, as, as, as

492
00:46:30,720 --> 00:46:37,120
intelligent as generally intelligent is, is the biological brain, biological brains of humans,

493
00:46:37,120 --> 00:46:44,960
but also of other animals. And the biological brain, you know, at its very, it's very kind of

494
00:46:44,960 --> 00:46:52,320
nature is it's there to help a creature to move around in the world, to move, right? It's there

495
00:46:52,320 --> 00:46:59,520
to move, help to guide a creature and help it move in order to help it survive and reproduce.

496
00:46:59,520 --> 00:47:03,920
That's what brains are for. So that's what that from an evolutionary point of view, that's that

497
00:47:03,920 --> 00:47:12,160
they developed in order to help a creature to move. And they are so they and they are, you know,

498
00:47:12,160 --> 00:47:19,360
they're the bit that's comes between the sensory input and the motor output. And as far as you

499
00:47:19,360 --> 00:47:24,640
can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their

500
00:47:24,640 --> 00:47:30,480
purpose is to intervene in the sensory motor loop in a way that benefits the organism. And

501
00:47:30,480 --> 00:47:38,560
everything else is on built on top of that. So, so, so the capacity to recognize objects in our

502
00:47:38,560 --> 00:47:45,200
environments and categorize them and the ability to kind of manipulate objects in the environment,

503
00:47:45,200 --> 00:47:52,400
pick them up and so on. And all of that is there, you know, initially to help the, the, the organism

504
00:47:52,480 --> 00:48:01,840
to survive. And, and, and, you know, and that's what brains brains are there for. And then,

505
00:48:01,840 --> 00:48:09,600
then when it comes to, you know, the ability to work out how the world works and to do things

506
00:48:09,600 --> 00:48:15,040
like figure out how to gain access to some item of food that's difficult to get hold of, then all

507
00:48:15,040 --> 00:48:21,840
kinds of cognitive capabilities might be required to understand how you get inside a, you know,

508
00:48:22,480 --> 00:48:29,280
a shell or something to get the fruit inside it or something like that, complex cognitive

509
00:48:29,280 --> 00:48:33,760
abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of

510
00:48:34,320 --> 00:48:38,640
more and more and more complex cognitive cognition until we get to language and, you know,

511
00:48:38,640 --> 00:48:43,360
we need to interact with each other because that that's all very much a part of it, the social,

512
00:48:43,360 --> 00:48:48,320
social side of it. And then language is part of that. So as I see it, it's all built on top of

513
00:48:49,280 --> 00:48:54,160
this fundamental fact of the embodiment of the animal and the organism. So that's in the

514
00:48:54,160 --> 00:49:01,280
biological case. So that's sort of our starting point. So, and so that seems to me to be the,

515
00:49:01,280 --> 00:49:06,720
the most natural way to, to understand the very nature of intelligence.

516
00:49:07,520 --> 00:49:10,640
Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell

517
00:49:10,640 --> 00:49:14,640
recently had a paper out talking about the Four Misconceptions and one of them, of course,

518
00:49:14,640 --> 00:49:19,440
citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've

519
00:49:19,440 --> 00:49:24,160
basically spoken about. But her fourth one was about embodiment. And she spoke about this in

520
00:49:24,160 --> 00:49:28,160
her book as well. She said that one of the misconceptions of AI is that people have this

521
00:49:28,160 --> 00:49:32,080
notion of a pure intelligence, you know, something which works in isolation from the

522
00:49:32,080 --> 00:49:36,720
environment. And you're talking about social embeddedness and embodiment. And I guess my

523
00:49:36,720 --> 00:49:41,920
point with the complexity argument is I'm saying that the brain itself doesn't actually do everything.

524
00:49:42,000 --> 00:49:47,440
It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So

525
00:49:47,440 --> 00:49:54,400
there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew

526
00:49:54,400 --> 00:50:00,960
Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course,

527
00:50:00,960 --> 00:50:05,280
that's very much part of it is the, is the idea that, you know, there's the extended mind, we use

528
00:50:05,280 --> 00:50:13,600
the environment, you know, as, as, as a kind of memory, for example, we deposit things in the

529
00:50:13,600 --> 00:50:20,000
environment, writing, you know, as an example and so on. And then there's, people talk about

530
00:50:20,000 --> 00:50:24,720
morphological computation, I'm sure you're familiar with that. Well, so that's the idea

531
00:50:24,720 --> 00:50:31,040
that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes,

532
00:50:31,040 --> 00:50:38,560
you know, the aspects of intelligence are actually outshort outsourced into the physical shape of,

533
00:50:38,560 --> 00:50:44,400
of your body. So where you might think about designing a robot, where you, where you put a lot

534
00:50:44,400 --> 00:50:49,040
of work into the control aspect of it, so that it's, so that it can kind of walk in this very

535
00:50:49,040 --> 00:50:53,840
carefully engineered ways that it's always permanently stable, or alternatively, you can

536
00:50:53,840 --> 00:51:00,160
make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you

537
00:51:00,160 --> 00:51:06,080
kind of rely on the combination of the physics of it constantly falling with, with a control system

538
00:51:06,080 --> 00:51:12,000
that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very

539
00:51:12,000 --> 00:51:17,840
much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this

540
00:51:17,840 --> 00:51:21,920
sort of way. So I think that's, I think that's a very natural way of thinking.

541
00:51:22,480 --> 00:51:26,080
But in a way that this gets to the, to the complexity argument, because I guess what I'm

542
00:51:26,080 --> 00:51:32,480
saying is that life is much more brittle than anyone realises. You were just pointed to some

543
00:51:32,480 --> 00:51:37,200
sources of brittleness that most people never would have thought of, which is, which is fascinating.

544
00:51:37,200 --> 00:51:43,280
So I think there's a very important relationship between embodiment and language. And this also

545
00:51:43,280 --> 00:51:51,840
brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied

546
00:51:51,840 --> 00:51:59,360
phenomenon. It's, it's, it's something that is, it's a social practice, something that take that

547
00:51:59,360 --> 00:52:05,520
it's a phenomenon that occurs in the context of other language users who inhabit the same world

548
00:52:05,520 --> 00:52:10,640
as we do. And where we have kind of like joint activities, we're triangulating on the same world,

549
00:52:10,640 --> 00:52:15,520
and we're acting on the same world together. And that's the that's what we're talking about when

550
00:52:15,520 --> 00:52:22,400
we use language. So there's this, so that, that's an inherently convincing view of language. And I

551
00:52:22,400 --> 00:52:28,640
think it's deeply profoundly correct view of language. That's, that's what, that's what language

552
00:52:28,640 --> 00:52:33,680
is there for us is so that we can talk about the same things together so that we can, our collective

553
00:52:33,680 --> 00:52:42,640
activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's,

554
00:52:42,640 --> 00:52:48,000
so I think that's a really important perspective on language is Wittgenstein perspective. And, and

555
00:52:48,000 --> 00:52:52,880
embodiment is at the heart of it, embodiment and inhabiting the same world as our other language

556
00:52:52,880 --> 00:52:58,880
users. And, you know, that's the way we learn language, we learn language by being around

557
00:52:58,880 --> 00:53:06,480
other language users like our parents and carers and, and, and peers. And, and that's again a very

558
00:53:06,480 --> 00:53:13,280
important aspect of the nature of human language. Now large language models, they learn language in

559
00:53:13,280 --> 00:53:18,880
a very different way indeed, they do not inhabit the same world as us, they do not kind of sense

560
00:53:19,440 --> 00:53:24,640
the world in the way that we do, they don't learn language from, from other language users,

561
00:53:24,640 --> 00:53:29,680
from their peers in the way that we do. But rather, you know, will we know how large language

562
00:53:30,800 --> 00:53:35,760
models work, there's trained on a very, very large corpus of textual, of textual data.

563
00:53:35,760 --> 00:53:40,720
So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you

564
00:53:40,720 --> 00:53:47,920
know, by the time they become a proficient language user at a young age. And what they're trained to

565
00:53:47,920 --> 00:53:54,720
do is, is not to kind of engage in activities with other language users, but to predict what the

566
00:53:54,720 --> 00:53:59,520
next, you know, what the next token is going to be, which is a very, very different sort of thing.

567
00:53:59,520 --> 00:54:04,160
So they're very, very different sorts of things. And the, and the role of embodiment is really

568
00:54:04,240 --> 00:54:10,480
really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen,

569
00:54:10,480 --> 00:54:13,840
he's really, really interested in the grounding problems. I mean, would you mind just speaking

570
00:54:13,840 --> 00:54:18,720
about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course,

571
00:54:18,720 --> 00:54:25,760
this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998.

572
00:54:25,760 --> 00:54:31,200
Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called.

573
00:54:31,200 --> 00:54:40,640
And, and, and, you know, he does argue broadly that the symbols in AI systems,

574
00:54:42,160 --> 00:54:45,120
the kinds of AI systems he was thinking about at the time were kind of, you know,

575
00:54:45,120 --> 00:54:49,040
sort of expert systems say or something like that. And the symbols there are not grounded,

576
00:54:49,040 --> 00:54:53,680
they're provided by the human programmers and they're just sort of typed in. Whereas for us,

577
00:54:53,680 --> 00:54:59,520
for us, the words we use, those symbols are grounded in, in our activity in the world. So

578
00:54:59,520 --> 00:55:05,440
that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with

579
00:55:05,440 --> 00:55:09,200
other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And

580
00:55:09,200 --> 00:55:16,400
we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all

581
00:55:16,400 --> 00:55:21,760
kind of contextualize that. But all of that, that that is kind of grounded in the real world in

582
00:55:21,760 --> 00:55:27,360
our and in our perception of the things in question. So that so that's this. So that's what

583
00:55:27,360 --> 00:55:32,480
sort of is meant by grounding or that at least that's the original meaning of the word grounding

584
00:55:32,480 --> 00:55:37,920
from Stephen Harled's paper. And I think that's a really, really important concept because,

585
00:55:38,880 --> 00:55:44,800
because, you know, in an important sense, large language models, the symbols that are used in

586
00:55:44,800 --> 00:55:50,480
large language models are not really grounded in that kind of way. Now this, you know, I should

587
00:55:51,120 --> 00:55:56,640
be absolutely clear that large language models are immensely powerful and immensely useful and,

588
00:55:56,800 --> 00:56:02,240
and so that, you know, so, but it's interesting that to what extent the lack of grounding here

589
00:56:02,960 --> 00:56:09,200
that we have in today's large language models, you know, might affect how good they are. So,

590
00:56:09,760 --> 00:56:16,560
so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating

591
00:56:16,560 --> 00:56:23,040
and, and if you look at multimodal language models that maybe we'll talk about an image that you

592
00:56:23,040 --> 00:56:27,840
present to them, then, you know, they, you can have a very interesting conversation, but sometimes

593
00:56:27,840 --> 00:56:32,640
they'll go off pieced and start talking about things that are not in the image at all and as

594
00:56:32,640 --> 00:56:39,760
if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that

595
00:56:39,760 --> 00:56:45,440
so the words are not kind of grounded in the images in, in quite the way that we would like. So

596
00:56:45,440 --> 00:56:49,360
that's, it's an important topic of research, I think. Yes, indeed. And although some people do

597
00:56:49,360 --> 00:56:53,520
believe there's this magical word called emergence and possibly some emergent symbol

598
00:56:53,520 --> 00:56:57,280
grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce

599
00:56:57,280 --> 00:57:03,280
your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.

600
00:57:03,280 --> 00:57:10,400
And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think

601
00:57:10,400 --> 00:57:16,480
in today's large, large language models. So, so, so, you know, even though they're, they're, so

602
00:57:16,560 --> 00:57:22,000
they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I

603
00:57:22,000 --> 00:57:25,760
should have made this maybe a bit clearer in the paper, but of course, it's not always next word

604
00:57:25,760 --> 00:57:30,160
prediction, because there are different models learned to actually, you know, predict what's

605
00:57:30,160 --> 00:57:35,120
in the middle of a, of a sequence rather than kind of generally, you know, they're interested in,

606
00:57:35,120 --> 00:57:42,080
in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're,

607
00:57:42,080 --> 00:57:46,560
so they're trained to just to do next word prediction. Now, the astonishing thing is,

608
00:57:46,560 --> 00:57:53,920
as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful,

609
00:57:54,560 --> 00:58:01,520
can be used to do all sorts of extraordinary things. Because if you provide, you know,

610
00:58:01,520 --> 00:58:07,280
the prompt that describes, you know, describes some kind of complicated thing, you know,

611
00:58:07,280 --> 00:58:15,680
situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's

612
00:58:15,680 --> 00:58:20,320
20 meters long and three meters. Well, you know, you start to describe this thing, you know, and

613
00:58:20,320 --> 00:58:26,880
each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large

614
00:58:26,880 --> 00:58:32,720
language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right?

615
00:58:32,720 --> 00:58:38,560
Well, this is astonishing, because it was only trained to do next word prediction. And so there's

616
00:58:38,560 --> 00:58:43,680
a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing

617
00:58:43,680 --> 00:58:50,800
next word prediction, because in the vast and immensely complex distribution of tokens in human

618
00:58:51,360 --> 00:58:59,440
text that's out there, then the correct answer is actually the thing that's most likely to come up.

619
00:58:59,520 --> 00:59:04,480
And that's, but it's got to discover a mechanism for producing that, right? And so that is where

620
00:59:04,480 --> 00:59:09,840
the emergence comes in. And I think that, you know, these capacities are astonishing, the fact

621
00:59:09,840 --> 00:59:14,720
that they, that it can discover mechanisms, you know, emergently that will do that sort of thing.

622
00:59:14,720 --> 00:59:18,560
Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan

623
00:59:18,560 --> 00:59:23,520
of emergence. And, and as you say, the decode is trained to predict the next token or the

624
00:59:23,520 --> 00:59:28,640
denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are

625
00:59:28,640 --> 00:59:32,480
different ways of thinking about emergence. So there's weak emergence, which might be thought as

626
00:59:33,360 --> 00:59:38,560
computational irreducibility, or a surprising macroscopic change, or strong emergence when

627
00:59:38,560 --> 00:59:41,520
it's not directly deducible from truths and the lower level to make, you know, lots of things.

628
00:59:41,520 --> 00:59:46,960
Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably,

629
00:59:47,600 --> 00:59:52,320
it's trained on something quite trivial. So all of this is about convention, right? All of this is

630
00:59:52,320 --> 00:59:57,440
about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't

631
00:59:57,440 --> 01:00:02,880
think, you know, I'm not making metaphysical claims about, about, about these things. So it's

632
01:00:02,880 --> 01:00:08,160
all about what, you know, when is it appropriate to use words, to use certain words? And, and

633
01:00:08,160 --> 01:00:12,000
because when, when this becomes problematic is when they're philosophically difficult words,

634
01:00:12,000 --> 01:00:20,000
like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we,

635
01:00:20,000 --> 01:00:25,760
I do think it's not unreasonable to, to, to use that term to describe what some of the,

636
01:00:25,760 --> 01:00:30,640
these models do today. And that's partly because of the content neutrality of,

637
01:00:31,600 --> 01:00:37,360
of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the

638
01:00:37,360 --> 01:00:43,440
paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well,

639
01:00:43,520 --> 01:00:51,840
you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets

640
01:00:51,840 --> 01:00:58,560
complicated because we do use the word believes in this intentional stance way to, to talk about

641
01:00:58,560 --> 01:01:05,280
ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's

642
01:01:05,280 --> 01:01:10,560
British summertime, you know, you know, because we, and then, but then you'd say, then somebody

643
01:01:10,560 --> 01:01:15,280
says to you, what you, what you mean, your, your car clock and think, you say, no, obviously,

644
01:01:15,280 --> 01:01:19,280
I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to

645
01:01:19,280 --> 01:01:24,320
these large language models, we start to use the words like thinks and believes and so on,

646
01:01:24,320 --> 01:01:28,800
because they're so powerful, it starts to get ambiguous and yours, and your, and, you know,

647
01:01:29,520 --> 01:01:33,360
and some people will say, well, actually, I really didn't mean that it can think or that it believes.

648
01:01:33,360 --> 01:01:39,760
So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could,

649
01:01:39,760 --> 01:01:44,240
could you tease apart that work? So you resist anthropomorphic language in terms of belief,

650
01:01:44,240 --> 01:01:49,920
knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition

651
01:01:49,920 --> 01:01:55,440
is that reasoning rather depends on those things that I just said before. Well, I, so I think it

652
01:01:55,440 --> 01:02:02,880
doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because,

653
01:02:02,880 --> 01:02:09,440
because reason, because logic is content neutral. So if I tell you that every, could you just explain

654
01:02:09,520 --> 01:02:14,880
what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense

655
01:02:14,880 --> 01:02:22,720
syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and,

656
01:02:22,720 --> 01:02:26,720
you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds

657
01:02:26,720 --> 01:02:32,000
of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones.

658
01:02:32,000 --> 01:02:40,880
Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung

659
01:02:40,880 --> 01:02:46,480
forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean,

660
01:02:46,480 --> 01:02:52,720
but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have

661
01:02:52,720 --> 01:03:00,000
to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way

662
01:03:00,000 --> 01:03:05,360
that a theorem prover can do logic, then so can a large language model do logic. So in that sense,

663
01:03:05,360 --> 01:03:10,240
I think large, it is reasonable to use the word reasoning in that logical sense in the

664
01:03:10,240 --> 01:03:14,080
context of large language models. I don't think that's a problem. Of course, we may think that

665
01:03:14,080 --> 01:03:18,480
they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the

666
01:03:18,480 --> 01:03:25,280
word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment

667
01:03:25,280 --> 01:03:32,880
is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's,

668
01:03:32,880 --> 01:03:38,480
it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper,

669
01:03:39,200 --> 01:03:46,880
if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not,

670
01:03:46,880 --> 01:03:52,560
it does depend upon facts that are out there in the world. And then to, to really have a belief,

671
01:03:52,640 --> 01:03:59,680
at least you've got to be able to somehow try and kind of justify those facts, or at least, and you

672
01:03:59,680 --> 01:04:05,200
got to be at least built in such a way that you can, you know, interact with the external world

673
01:04:05,200 --> 01:04:10,960
and do that sort of thing, right? And, and verify that something is true or false or do an experiment

674
01:04:10,960 --> 01:04:16,400
or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside

675
01:04:16,400 --> 01:04:21,360
of ourselves and, and in order to establish whether something a belief is true or not. And so,

676
01:04:22,080 --> 01:04:26,880
you've got to at least be capable of doing that. Whereas large language models, the bare bones,

677
01:04:26,880 --> 01:04:32,560
large language model is not capable of doing that at all, right? Now you can embed it in a larger

678
01:04:32,560 --> 01:04:36,080
system. This is a really important distinction that I've tried and make over and again in the

679
01:04:36,080 --> 01:04:41,360
paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever

680
01:04:41,360 --> 01:04:46,000
a large language model is used, it's not the bare bones, large language model, which just does

681
01:04:46,000 --> 01:04:50,560
sequence prediction, but it's embedded in a larger system. When we embed it in a larger system,

682
01:04:50,560 --> 01:04:55,280
well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot

683
01:04:55,280 --> 01:04:59,040
that goes and investigates the world. So that's a whole other thing. But then you have to look at

684
01:04:59,040 --> 01:05:04,800
each case in point and, and, and ask yourself whether it's a, whether, you know, whether we

685
01:05:04,800 --> 01:05:10,560
really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just

686
01:05:10,560 --> 01:05:15,760
in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of,

687
01:05:15,840 --> 01:05:22,880
of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not

688
01:05:22,880 --> 01:05:28,560
using the word in the way that we, in the full blown sense that we use it, where we talk about

689
01:05:28,560 --> 01:05:34,480
each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful

690
01:05:34,480 --> 01:05:39,120
way of thinking about artificial intelligence, allowing us to view computer programs as intelligent

691
01:05:39,120 --> 01:05:42,880
agents, even though they may lack the same kind of understanding as a human. And then you cited

692
01:05:42,960 --> 01:05:48,160
the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of

693
01:05:48,160 --> 01:05:52,560
Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind

694
01:05:52,560 --> 01:05:58,320
of like, it's just distinguishing what it, what it means to know, you know, for humans and, and,

695
01:05:58,320 --> 01:06:03,600
and for machine. So I think it's, it's useful to think about something like Wikipedia. So,

696
01:06:04,720 --> 01:06:11,600
so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?

697
01:06:11,600 --> 01:06:16,960
And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia.

698
01:06:16,960 --> 01:06:22,960
And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we

699
01:06:22,960 --> 01:06:27,280
use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses

700
01:06:27,280 --> 01:06:31,840
that word, hey, you know, I don't think you should use the word knows there. And you know, that would,

701
01:06:31,840 --> 01:06:36,000
you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think,

702
01:06:36,000 --> 01:06:42,400
these kinds of words in this ordinary, every day sense. And we do that all the time. And that's

703
01:06:42,400 --> 01:06:48,640
sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls

704
01:06:48,640 --> 01:06:56,880
the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs,

705
01:06:56,880 --> 01:07:02,080
desires and intentions, because it's a kind of convenient shorthand. And especially if you've

706
01:07:02,080 --> 01:07:05,840
got something that's a bit more complicated, like say your car sat down for something or

707
01:07:05,840 --> 01:07:11,520
you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense

708
01:07:11,520 --> 01:07:16,720
to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them,

709
01:07:16,720 --> 01:07:22,080
right? And without getting overly complicated, without knowing the underlying mechanisms. But

710
01:07:22,080 --> 01:07:25,280
there's an important sense in which we don't mean it literally. So you know, in the case of

711
01:07:25,280 --> 01:07:28,880
Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say,

712
01:07:28,880 --> 01:07:33,200
hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and,

713
01:07:33,200 --> 01:07:40,000
and, and, and, and all, and all the things that, that go with us as humans actually knowing things.

714
01:07:40,000 --> 01:07:47,120
And it's just a turn of phrase. Now, things get sort of interesting with large language models,

715
01:07:47,120 --> 01:07:52,560
and with large language model based systems and the kinds of things that we're starting to see

716
01:07:52,560 --> 01:07:57,280
in the world, because we're starting to get into this kind of blurry territory where it,

717
01:07:57,760 --> 01:08:03,600
we're blurring between the intentional stance and, and, you know, meaning the meaning it literally.

718
01:08:03,600 --> 01:08:07,920
And this is where we need to be really, really kind of careful, I think. So at what point does

719
01:08:07,920 --> 01:08:15,520
do things shade over into where it's legitimate to use that word, you know, literally, in, in the

720
01:08:15,520 --> 01:08:21,680
context of something that we've built, you know, I don't think we're at that point yet. And, and

721
01:08:21,680 --> 01:08:28,320
we need to be very careful about, about using the word as if we were using it literally.

722
01:08:29,920 --> 01:08:34,720
You know, that's the sort of anthropomorphization, because the problem is that we can then

723
01:08:35,600 --> 01:08:43,280
impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there.

724
01:08:44,000 --> 01:08:48,560
Yes. And I suppose we could tease apart knowledge. So it justified true belief from

725
01:08:48,560 --> 01:08:54,160
knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization.

726
01:08:54,160 --> 01:08:56,080
But you had Chomsky, you've had Chomsky on.

727
01:08:56,800 --> 01:09:01,520
I can tell you a story about that. I mean, the recording messed up. So when we were interviewing

728
01:09:01,520 --> 01:09:05,680
him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to

729
01:09:05,680 --> 01:09:10,080
regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated

730
01:09:10,080 --> 01:09:15,120
deep learning and how useless it was. And then we used deep learning to rescue his interview.

731
01:09:15,440 --> 01:09:21,040
And he gave us his permission to publish it. That is wonderful.

732
01:09:21,040 --> 01:09:25,120
So it's quite ironic. But no, he always says it's wonderful for engineering,

733
01:09:25,120 --> 01:09:27,840
but not a contribution to science. Yes, sure. Yeah.

734
01:09:29,600 --> 01:09:32,880
Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a

735
01:09:32,880 --> 01:09:36,560
contribution to science. So who else have you had? I mean, you've had a lot of people on.

736
01:09:36,560 --> 01:09:41,120
I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great.

737
01:09:41,120 --> 01:09:44,960
So he's great. Andrew is somebody I do work with quite closely.

738
01:09:45,840 --> 01:09:50,960
So it was interesting listening to him because Andrew had quite a big influence on this paper,

739
01:09:50,960 --> 01:09:56,640
by the way. Oh, okay. But I think I might have had a bit of influence on him as well,

740
01:09:56,640 --> 01:10:02,640
to listening to him. I think so. Because that interview was just after he'd read,

741
01:10:02,640 --> 01:10:06,640
and he read my paper, gave me lots of comments. And we had a lot of discussion about it.

742
01:10:07,120 --> 01:10:14,480
And that interview, looking at the recording date, was sort of just after this. And it's interesting.

743
01:10:14,560 --> 01:10:18,800
I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting

744
01:10:18,800 --> 01:10:26,240
because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know.

745
01:10:26,240 --> 01:10:31,600
I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that,

746
01:10:31,600 --> 01:10:36,480
actually, just when you were speaking. But it's funny because we've spent a lot of time arguing

747
01:10:36,480 --> 01:10:42,640
with each other about it. And I often feel like we're coming from very different perspectives

748
01:10:43,200 --> 01:10:49,840
on this. But in fact, I think there's convergence, really. What are your areas of disagreement?

749
01:10:53,360 --> 01:11:00,080
Well, you see, I would have thought that Andrew would have been more on the side of,

750
01:11:00,080 --> 01:11:07,280
we can do things without embodiment, and without grounding, or to kind of take grounding in a

751
01:11:08,080 --> 01:11:18,960
more liberal sense. Because some people would talk about grounding, so they say that large

752
01:11:18,960 --> 01:11:26,160
language models, they are grounded. Prompt Engineering is the process of using prompt

753
01:11:26,160 --> 01:11:32,320
prefixes to allow LLMs to understand better. So the context and the purpose of a conversation

754
01:11:32,320 --> 01:11:36,800
in order to generate more appropriate responses. What do you think is going on with Prompt

755
01:11:36,800 --> 01:11:42,560
Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process

756
01:11:42,560 --> 01:11:47,120
of allowing the models to understand better is what you're better. Of course, I don't think

757
01:11:47,120 --> 01:11:52,000
guilty as charged. I don't think I don't think that's the right way of characterizing it at all.

758
01:11:53,280 --> 01:11:59,760
So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's

759
01:11:59,760 --> 01:12:04,560
something that's entered our world as AI researchers, very prominently, just in the

760
01:12:04,560 --> 01:12:10,000
last two years. And it's amazing. Of course, we have Prompt Engineering in the context of

761
01:12:10,000 --> 01:12:15,760
large language models. We also have Prompt Engineering in the context of the generative

762
01:12:17,360 --> 01:12:24,320
image models as well, like Dali and so on. And that's really fascinating as well, how by

763
01:12:25,520 --> 01:12:30,640
engineering the Prompt to be just the right sort of thing, you can coax the model into doing

764
01:12:30,640 --> 01:12:36,400
something which you might not otherwise do. And it's a great example of how alien these things

765
01:12:36,400 --> 01:12:41,040
are. Because if you were giving a human being the same instructions, then you wouldn't necessarily

766
01:12:41,040 --> 01:12:49,040
do quite what you do with either an LLM or an image model in order to get it to do the

767
01:12:49,040 --> 01:12:55,680
thing that you want it to do. You have to kind of get into the zone with these models and figure

768
01:12:55,680 --> 01:13:00,800
out kind of what strange incantations are going to make it do the things that you want it to do.

769
01:13:00,800 --> 01:13:06,320
Now, I think an interesting thing is that we may be looking at a moment, a very short moments in

770
01:13:06,320 --> 01:13:12,880
the history of the field where Prompt Engineering is relevant. Because if language models become

771
01:13:12,880 --> 01:13:18,080
good enough, then we're not going to need to talk to them in this weird way,

772
01:13:18,800 --> 01:13:23,680
engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier.

773
01:13:23,920 --> 01:13:27,440
Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the

774
01:13:27,440 --> 01:13:36,480
case as they get better. But at the moment, you can use a strange incantation like thinking steps

775
01:13:36,480 --> 01:13:41,840
and suddenly the large language model will be much more effective on reasoning problems than it

776
01:13:41,840 --> 01:13:45,920
was if you didn't use the incantation thinking steps. So that's really fascinating. So what's

777
01:13:45,920 --> 01:13:50,960
going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that

778
01:13:50,960 --> 01:13:57,680
what the model is really trained to do is next word prediction. But we have to remember that

779
01:13:57,680 --> 01:14:06,240
it's doing next word prediction in this unimaginably complex distribution. So we have to remember

780
01:14:06,240 --> 01:14:11,760
that it's not just the distribution of what a single human would, the distribution of

781
01:14:11,760 --> 01:14:16,480
the sequence of words that a single human will come out with, but of all the sort of text of

782
01:14:16,800 --> 01:14:22,640
millions of humans on the internet, plus actually a load of other stuff like code and

783
01:14:22,640 --> 01:14:28,160
things which we don't come out with in ordinary everyday language. Well, people do it deep

784
01:14:28,160 --> 01:14:36,160
mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's

785
01:14:36,160 --> 01:14:45,760
happening with prompt engineering is that you're sort of channeling it into some portion of the

786
01:14:45,760 --> 01:14:54,640
distribution. So you're queuing it up with a prompt. And this kind of context is putting it

787
01:14:54,640 --> 01:14:59,600
into some portion of this distribution. And that is what's going to enable it to do something

788
01:14:59,600 --> 01:15:04,560
different than it would have done if you had a different set of words. And that would have

789
01:15:04,560 --> 01:15:10,240
put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably

790
01:15:10,240 --> 01:15:15,040
complex distribution. You're finding the bit of it that you want to then concentrate on.

791
01:15:15,120 --> 01:15:19,280
Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree

792
01:15:19,280 --> 01:15:22,960
with you that they are statistical language models. I'm also a fan of the spline theory of neural

793
01:15:22,960 --> 01:15:28,400
networks, which is this idea that you just kind of tessellate the ambient space into these little

794
01:15:29,040 --> 01:15:34,400
affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's

795
01:15:34,400 --> 01:15:38,800
quite, it's quite a simple way of looking at it, because you were talking about emergence before.

796
01:15:38,800 --> 01:15:43,600
And emergence is all about this paradigmatic surprise, a bit like the mind body dualism,

797
01:15:43,600 --> 01:15:47,200
if you like, there's something that happens up here, which is paradigmatically

798
01:15:47,200 --> 01:15:50,880
completely different to what happens down there. So on the one hand, we're kind of saying, oh,

799
01:15:50,880 --> 01:15:55,680
they're just simple interpolators or statistical models. But on the other hand,

800
01:15:55,680 --> 01:15:59,600
they really are doing something remarkable up here. So, so which is it?

801
01:16:00,480 --> 01:16:07,760
Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand

802
01:16:08,720 --> 01:16:15,040
these models in a in a in a more scientific way, which we surely do, you know, even if we're not,

803
01:16:15,040 --> 01:16:19,280
even if we're not engineering them in an old fashioned sense of engineering them,

804
01:16:19,280 --> 01:16:24,480
but, but rather they kind of, you know, emerge from the, from the learning process,

805
01:16:24,480 --> 01:16:30,000
we still want to reverse engineer them to try and get as great as as as comprehensive

806
01:16:30,800 --> 01:16:35,520
a scientific understanding of these things as possible. So, so we want to understand it all

807
01:16:35,520 --> 01:16:39,360
these levels, right? We, of course, the foundation of that understanding is that we

808
01:16:39,360 --> 01:16:42,720
need to understand the actual mechanisms that we've programmed in there, right? So,

809
01:16:42,720 --> 01:16:46,320
though, you know, so you've that's essential, you want to, you know, if you want to really

810
01:16:46,320 --> 01:16:51,600
understand these things, you've got to understand transformer architectures, the different kinds

811
01:16:51,600 --> 01:16:56,320
of transformer architectures that you've got, the, you know, what happens when you use kind of

812
01:16:56,320 --> 01:17:01,680
different parameter settings, whether it's sparse or dense, whether it's a decoder only

813
01:17:01,760 --> 01:17:06,000
architecture, or how you're doing the tokenization, how you're doing the embedding,

814
01:17:06,000 --> 01:17:09,760
when all of these things are essential to understanding, you know, and that's all at the

815
01:17:09,760 --> 01:17:14,720
absolutely at the engineering level. So we want to understand all of that. But then we can do a

816
01:17:14,720 --> 01:17:18,640
whole load of reverse engineering, you know, at another level, and do the sort of thing that

817
01:17:18,640 --> 01:17:25,200
the people at Anthropic AI have done, for example, with, with these induction heads and, and, and,

818
01:17:25,200 --> 01:17:30,320
and understanding in terms of transformers in terms of residual streams and induction heads,

819
01:17:30,320 --> 01:17:33,920
which I think is fabulous work. So that kind of thing is looking, it's still

820
01:17:34,560 --> 01:17:39,520
quite a low level, but it's kind of the next level up, and explaining a little bit about how these

821
01:17:39,520 --> 01:17:45,360
things work, and work along those lines, I think is like really essential. And then the more complex

822
01:17:45,360 --> 01:17:50,640
these things are, the, you know, the heart, the more we need to kind of ascend these levels of

823
01:17:50,640 --> 01:17:57,120
understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one

824
01:17:57,120 --> 01:18:01,120
that is the right one. It's, you want to understand that things are all levels.

825
01:18:01,120 --> 01:18:04,880
Yeah, different levels of description. You said something before, which really

826
01:18:04,880 --> 01:18:09,760
interested me. You said when the language models get good enough, maybe we won't need the prompts

827
01:18:09,760 --> 01:18:14,640
anymore. And I'd love to explore that duality, because it's a similar duality to how we talk

828
01:18:14,640 --> 01:18:18,720
about embodiment, you know, you can think of the language model being embodied in the prompt in,

829
01:18:18,720 --> 01:18:23,600
in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts,

830
01:18:23,600 --> 01:18:28,960
I think about them as a new type of program interpreter. And there are some remarkable

831
01:18:28,960 --> 01:18:33,920
examples of scratch pad and chain of thought and even algorithmic prompting for getting

832
01:18:33,920 --> 01:18:39,200
insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah.

833
01:18:39,200 --> 01:18:43,360
And, you know, these, these models are not Turing machines, they're finite state

834
01:18:43,360 --> 01:18:47,680
automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt

835
01:18:47,680 --> 01:18:52,160
seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is

836
01:18:52,160 --> 01:18:57,600
going to go away. But I think that the, and who knows, right? But, but I think that prompt

837
01:18:57,600 --> 01:19:06,000
engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people

838
01:19:06,000 --> 01:19:10,320
talk about that as being a kind of a whole new job description as prompt engineer. And so that,

839
01:19:10,320 --> 01:19:14,640
that as a whole new job description, I'm not quite sure how long exactly that will last because,

840
01:19:15,760 --> 01:19:20,400
because prompt prompting may be just, you know, interacting with a thing in a much more natural

841
01:19:21,360 --> 01:19:26,080
language way in the way we would with another person, right? So, you know, I don't, I don't,

842
01:19:26,080 --> 01:19:31,280
when I, when I, I don't have to kind of think of some peculiar incantation in order to,

843
01:19:32,640 --> 01:19:40,080
you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you

844
01:19:40,080 --> 01:19:46,960
know, to cook a meal together with somebody, we just, we just use our natural kind of forms

845
01:19:46,960 --> 01:19:51,520
of communication. Of course, of course, it does involve, you know, discussion and negotiation,

846
01:19:51,520 --> 01:19:55,680
but it's in this, it's just the same as we use with other humans, right? So, so it may be that,

847
01:19:56,240 --> 01:20:01,520
rather than it being a peculiar thing in itself with all these funny phrases that just work

848
01:20:01,520 --> 01:20:07,600
for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan,

849
01:20:08,480 --> 01:20:12,400
thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots

850
01:20:12,400 --> 01:20:14,400
of fun. Absolute honor. Absolute honor. Why?

