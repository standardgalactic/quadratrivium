start	end	text
0	3560	my pleasure. And I must say, I've been really impressed by all your
3560	8360	questions. It showed that you did prepare and read papers and think
8360	12760	about it. And that's very much appreciated. Thank you.
21040	24120	Today is an incredibly special occasion. We have Professor
24120	27960	Yoshua Benjiro on the show. Just honestly, I just can't get over
28080	31920	it. But first of all, a little bit of housekeeping. So we've
31920	35320	just launched a new Discord community. So please jump in
35320	39160	there, say hello, introduce yourself. If you want to be, you
39160	42640	know, part of the moderating community or just help us do
42640	45320	stuff over there, we would love to talk with you. By popular
45320	47480	demand, we've also added a couple of ways in which you can
47480	51200	support us. So we now have a Patreon and a merch store. If
51200	53840	you're interested in supporting some of the episodes of MLST
53840	55440	then get in touch with us because we'd love to have a
55440	59240	conversation with you. We're just doing so much cool stuff
59240	62360	this year. We've already recorded about six episodes that we
62360	65320	haven't released. And we've got some amazing people booked as
65320	68800	well. So yeah, it's going to be incredible. As always, if you
68800	71240	like the content here, please consider hitting the like and
71240	74120	subscribe button and rating our podcast on iTunes because it
74120	76640	really, really helps us out. I called it iTunes. Is it
76640	79760	iTunes? Apple podcasts? I don't know, whatever it's called.
81200	84520	Wait and Biases is the developer first MLS platform. And
84520	87400	we're extremely proud today that they are sponsoring this
87400	90840	episode. Now tracking machine learning experiments is
90840	94840	difficult. Using the winging it methodology can only get you so
94840	98960	far. What we need is a platform where we can compare models and
98960	102160	visualize their performance characteristics against all of
102160	105400	the previous runs and figure out the best hyper parameters to
105400	109240	use. Now most importantly of all, this process needs to be
109320	112920	reproducible. Sounds like a tool order, right? Well, this is
112920	117320	exactly what the Wait and Biases platform does for you. Now you
117320	120680	can even follow metrics from long running experiments in real
120680	124240	time. I think it's really important to lean into the
124240	128600	complex interaction between science and engineering in the
128600	132840	ML DevOps lifecycle. Data scientists need valuable feedback
132920	135320	and they need to communicate why they're running given
135320	138680	experiments and they need to share their notes around the next
138680	142880	steps. Reports keep this work well organized and connected to
142880	146080	the Waits and Biases experiments which were run as opposed to
146080	150080	just sharing random screenshots in Slack. It's so easy to create
150080	152720	a report and share it with your team after you finished with
152720	155560	your experimentation. You could just add notes for yourself as
155560	158640	well to explore later on. You can keep a work log and you can
158640	162280	even share your findings internally or externally. This is
162280	165960	an absolute game changer. I'm a big believer in this kind of
165960	169520	engineering rigor. I'm the CEO of a code review startup called
169520	173600	Merge these days and I love how the pull request process and
173600	177280	tooling immortalizes important collective decisions which were
177280	180520	made during the software development lifecycle. Similarly,
180720	184000	Waits and Biases immortalizes important decisions that were
184000	187880	made during model development, experimentation and
187880	192320	deployment. Remember, check out Waits and Biases today by going
192320	196720	to 1db.com forward slash MLST and if you're interested in
196720	199960	sponsoring future episodes, get in touch with us. Waits and
199960	203240	Biases are currently sponsoring our premiere shows but we have
203240	205520	lots of other content coming and opportunities for
205520	207720	sponsorship so let us know. Cheers.
209480	212240	Professor Yoshua Benjo has just released a bunch of papers
212280	216160	around G-Flow Nets. Now G-Flow Nets exists squarely in the
216160	220800	domain of active learning which is a model that can economically
220800	223920	ask an oracle which is probably the real world for the most
223920	228280	salient training examples to continue learning. The learner
228280	231720	can choose or have an influence on the examples it gets and we
231720	234600	want to learn a function which approximates the oracle
234640	238320	efficiently. How should we pick the queries? How should we take
238320	242400	into account not just the value of the predictor but also how
242400	245280	certain we are about the predictors from the learning
245280	250000	system? Areas of uncertainty or entropy are kind of like
250040	253800	interesting candidates for us to explore further. We need to
253800	258760	be able to imagine or invent queries to give to the oracle.
259320	261800	Now one of the reasons that machine learning models are so
261800	265120	sample and efficient is because of the combinatorial space of
265120	268920	possible input examples. We can't train on everything because
268920	272880	the space is just too large, it's vast. So you might have heard
272880	275640	of a related concept of active learning called machine
275640	278880	teaching which is an interactive version where the human
279160	283400	interactively selects the most salient data to train a machine
283400	287480	learning model maximizing the information gain in respect of
287480	293200	the training samples. Now the reality is the function space
293280	296760	that we're learning here is highly structured. We only really
296760	299760	need to sample training data where most of the rich
299760	302920	information exists in that function space. I mean, if you
302920	305520	think about it, a machine learning model, it's just a joint
305520	309520	probability distribution between signals and labels. And this
309520	316280	distribution has modes or areas of density or information. And
316400	320520	actually most of it is just areas of nothingness, which
320520	325600	require fewer training examples to learn and to represent. Now,
326040	329280	if you spoke to a to a Bayesian person like my friend Conor
329280	333000	Tan at work, you know how to learn this distribution, they
333000	336720	would bring up Markov chain Monte Carlo quicker than a whip it
336720	340920	with a bumful of dynamite. Now Markov chain Monte Carlo is an
340920	344200	increasingly popular sampling method for obtaining asymptotic
344200	347160	information about unnormalised distributions or energy
347160	350320	functions, especially for estimating the posterior
350320	353000	distribution in Bayesian inference, which is where you've
353000	355880	probably heard of it before. Now you can characterize a
355880	358400	distribution without knowing all of the distributions
358400	360600	mathematical properties. So if you don't have an analytical
360600	364080	representation for it, just by randomly sampling values out of
364080	367880	the distribution. Now a particular strength of Markov chain
367880	370640	Monte Carlo is that it can be used to draw samples from
370640	374320	distributions, even when all that is known about the
374320	377680	distribution is how to calculate the density for different
377680	382280	samples. Now the the Markov property of Markov chain Monte
382280	386400	Carlo is this idea that random samples are generated by a
386400	390520	special sequential process. And each random sample is used as a
390520	395680	stepping stone to generate the next random sample. Now this
395680	399200	might sound very complex, but the practical implementation is
399200	402320	pretty simple. Markov chain Monte Carlo just starts with an
402320	406520	initial guess, just one value that might plausibly be drawn
406520	410040	from the distribution. And then we produce a chain of samples
410040	414000	from this initial guess by adding random perturbations in the
414000	418240	neighborhood of that example. And each new proposal drawn from
418240	420960	that random perturbation distribution is either rejected
421040	424360	or accepted. There are different flavors of this, of course, I
424360	427960	mean, in particular, like tweaking, how the random
427960	430920	proposals in the neighborhood are selected or whether the
430920	435200	proposals are selected. The simplest heuristic being whether
435200	439080	it's below the function or not. Now the idea is that Markov
439080	442440	chain Monte Carlo methods, they capture a distribution with
442440	445960	only a relatively small number of random samples. But the
445960	451240	reality is anything but in high dimensions, and where the
451240	454560	distribution has many modes spread far apart, it's actually
454680	459200	exponentially expensive. There's a bunch of human orientated
459200	462240	hacks to try and make this work well in specific cases. But we're
462240	466040	missing a much more general machine learnable solution. This is
466040	468720	the main reason why we haven't seen it used in many machine
468720	471480	learning applications yet. Assuming that the function we want to
471480	475480	learn has underlying structure, then we can escape the
475480	479120	exponential time of Markov chain Monte Carlo with machine
479120	482680	learning. And this is what Benzio calls systematic
482720	487280	generalization, which is to say, how do we generalize far from
487280	491680	the data in a way which is meaningful. Now G flow nets are
491680	494600	an active learning framework, where the name of the game is to
494600	498080	generate salient and diverse training data to augment our
498080	501200	model in the most sample efficient way possible. For G flow
501200	504960	nets to work, we need a reward function and a deterministic
504960	508680	episodic environment. Does that sound familiar? Yes, just like
508680	512520	reinforcement learning. Now a flow network is a directed graph
512720	518080	with sources and sinks and edges carrying some amount of flow
518080	520880	between them, you know, through intermediate nodes. So I think a
520880	524000	good way to think about this is pipes of water. Now for our
524000	528680	purposes, we define a flow network with a single source. So the
528680	531280	root nodes, or you might say the sinks of the network
531320	535000	correspond to the terminal states. Now it's designed to find
535000	539360	the possible trajectories through our system. Okay, and just
539360	541560	think of Alpha zero as being like a good analogy for these
541560	544920	trajectories. Now the training objective is to make them
544960	549120	approximately sample in proportion to the given reward
549120	552680	function. This is in stark contrast to Alpha zero where we
552680	556480	were sampling to maximize the expected reward. So Benzio's big
556480	559360	idea is that we can have an interacting loop between a
559360	562400	generative model and the real world. The real world is
562400	566160	expensive. So why not train an imagination machine in our mind
566280	569560	until we're ready and waiting to produce good questions to the
569560	572920	real world, we could use imagined experiments to train our
572920	578680	generator, then produce queries to the real world. We were
578680	583480	thinking about a way to visualize how G flow nets work when the
583480	588640	idea of a Galton board came to mind. A Galton board also known
588640	593200	as a beam machine is a common prop in statistics courses,
593560	599520	science museums, and fun gadget stores. The board has rows of
599520	605240	interleaved pegs above a bottom row of buckets. Beads are filled
605240	609000	into a funnel at the top of the board and then sprinkled on the
609000	613360	top center peg. The beads bounce either to the left or to the
613360	617520	right as they hit the pegs and eventually collect into buckets
617560	622280	at the bottom. If the pegs are precisely and symmetrically
622280	625640	arranged, the beads will aggregate at the bottom into a
625640	631320	familiar binomial bell curve. Now imagine that the pegs were
631320	635760	instead flow gates with adjustable valves that could
635760	640400	direct the beads more to the left or more to the right to
640400	644680	bias the flow paths. With such a machine, you could adjust the
644680	651120	valves or flow rates to create any distribution. For example, to
651120	654680	create a uniform distribution, we'd open up the gates flowing
654720	659960	away from the center line of the board to drive more bead flow
660080	664080	to the fewer number of paths leading to the edges and the
664080	670000	corners. Or to create a multimodal distribution, we'd arrange
670000	673920	the gates to split the flows into two or more streams that would
673920	679920	then pile up in multiple humps or modes below. There's a lot of
679920	683840	flexibility here. Indeed, given a distribution, there are
683840	688720	generally multiple flow gate solutions to produce it. It'd be
688720	692920	nice, wouldn't it? If we had an intelligent, principled way to
692920	698320	train these gates. Enter G flow nets. G flow nets put a neural
698320	702920	network, a brain behind the flow adjustments. A brain which can
702920	707880	optimize the gates to match any distribution we desire. Here,
707880	711680	we're interested in sampling a reward function in the context
711680	716480	of reinforcement learning. In that context, this is a powerful
716480	721320	simulation and sampling paradigm. You see, once the brain has
721320	725240	tuned the flow weights, such a modified Galton board, or more
725240	730400	generally, a flow network, can sample diverse paths quickly
730520	735200	and efficiently, leading to the reward distribution. It's
735200	738960	important to point out that the path sampling is more diverse
739120	743640	doing it this way. Unlike classic reinforcement learning, a G
743640	747080	flow net doesn't just fixate on a small number of high reward
747080	751600	paths, it happens to find first. Instead, it stochastically
751600	755440	samples a broad spectrum of paths in proportion to their reward.
755880	759440	Sure, high reward paths will be sampled with higher weight. But
759440	763640	the far larger population of low reward paths will get a share
763680	767240	of the sampling as well. Why should we even bother with such
767240	772160	paths? The answer is we need to balance exploitation or high
772160	776160	reward with exploration or learning to better learn the
776160	779760	reward function. This is especially important when dealing
779760	784960	with complex real world scenarios of high uncertainty. For
784960	789200	example, think of molecular drug discovery and design or
789440	794120	navigating jungle terrain. In both those scenarios, we really
794120	799080	know very little about how a particular path may play out. We
799080	803000	might stumble into the next miracle cure or a pitfall of
803000	807520	quicksand. To find the globally best paths, it's important to
807520	812800	keep our options open. Beyond this sampling diversity, G
812800	816400	flow nets also bring the full power of neural networks to
816440	821640	discover latent structure and learn the reward function. This
821680	826160	combined with their diverse sampling also makes G flow nets
826160	830160	more robust when dealing with multimodal distributions, which
830160	834360	are a common trap for greedy algorithms and Markov chain,
834400	838760	Monte Carlo. If there is structure linking the multiple
838760	843920	nodes, G flow nets can learn it and extrapolate to new modes
844000	848840	and once discovered, they will by design drive the sampling to
848840	854240	cover those modes and learn more structure. Overall, G flow
854240	859520	nets seem to offer an intriguing new path pun intended for an
859520	864680	intelligent sampling paradigm. So you might ask how are G
864680	867920	flow nets different from Alpha zero? Well, the policy network
867920	871440	and Alpha zero gives you a set of actions. Given a state, Alpha
871480	875600	zero trains the policy network to maximize reward so that the
875600	879640	trajectories all end up at the highest reward. Now what G
879640	883560	flow nets do is they train so that the actions are distributed
883720	887280	in proportion to the reward. So rather than pruning away all of
887280	890640	the low reward trajectories, it will sample them just less
890640	896160	often. Now there is a manifest difference between G flow nets
896160	898880	in respect of exploration. I mean, you might argue that the
898880	902440	Monte Carlo tree search is still doing wide exploration at the
902440	905920	beginning. But in spite of its rapid convergence and pruning of
905920	909280	load reward trajectories, it's still sampling from the
909280	911400	underlying probability distribution, which has been
911400	914720	scaled with a softmax. So that's actually not that much to
914720	918400	explore in the first place. So in summary, G flow nets are
918400	921600	better than Alpha zero Monte Carlo tree search in some sense,
921600	924760	because they achieve the same goal by offloading the burning
924760	927600	time and the stabilization time of Markov chain Monte Carlo.
927760	930360	Remember this whole thing can be trained offline. And then
930360	933200	when in inference mode, we can do it in a single shot. Whereas
933200	936080	with Monte Carlo tree search, we actually had to do it in
936080	939760	inference mode as well. The other thing is we're kind of
940440	943320	offloading all of the human engineering required to sample
943320	946040	efficiently from Markov chain Monte Carlo. And the other thing
946040	949480	is diversity, baby. I mean, consider the difference between
949480	952200	how G flow nets and Alpha zero sample the reward path
952200	954880	distribution. If you looked at the distributions, you would
954880	959080	see that Alpha zero has a little box around the mode. G flow
959080	962400	nets is the whole distribution. We know very well that
962400	965840	diversity preservation is critical in order to discover
965840	969320	interesting stepping stones and search problems. Now finally,
969320	972200	Benzio has published results showing the G flow nets converge
972200	975640	exponentially faster than Markov chain Monte Carlo and PPL on
975640	979480	some problems and finds more of the modes in the distribution
979480	982480	function faster. Enjoy the show folks.
983320	987920	Professor Yoshio Benzio is recognized worldwide as one of
987920	992480	the leading experts in artificial intelligence. Indeed, a god
992480	995600	father of deep learning. His pioneering work in deep learning
995600	998880	earned him the Turing Award, which is the Nobel Prize of
998880	1002480	computing. He's a full professor at the University of Montreal,
1002680	1006600	and the founder and scientific director of Miele, which is a
1006600	1010800	prestigious community of more than 900 researchers specializing
1010800	1014560	in machine learning and AI. He's one of the most cited
1014560	1018280	computer scientists on the planet. And I can't even begin to
1018280	1021200	articulate how honored we are today to have this conversation.
1021600	1025120	Yoshio has done a lot of work recently on G flow nets, which
1025120	1028040	are an active learning framework in a reinforcement learning
1028040	1032160	configuration, where the name of the game is to request salient
1032200	1036000	and diverse training data from the real world to augment our
1036000	1039960	learned models in the most sample efficient way possible. Now
1039960	1042640	we're trying to minimize the divergence between the path
1042640	1046120	distribution and the reward distribution, and then sample
1046120	1050040	paths according to the reward distribution. This is in stark
1050040	1052800	contrast with traditional reinforcement learning, where we
1052800	1056760	trying to maximize the expected reward. This approach is likely
1056760	1060280	to find diverse strategies instead of being greedy and
1060280	1063720	converging quickly after finding a single one. Anyway,
1063840	1066840	Professor Benzio, this is amazing. Can you tell us about this
1066840	1068840	exciting work in some of its applications?
1069520	1074520	Yeah, I'm, I don't think I've been as excited about a new topic.
1076400	1080800	At least in the last six or seven years as I'm now with G flow
1080800	1086240	nets. And it's actually even much more than what you've been
1086240	1091200	talking about. The way I think about G flow nets is a kind of
1091720	1094120	framework for generic
1095920	1102840	learnable inference for probabilistic machine learning. So
1103280	1108160	one way to think about this is it's a learnable replacement for
1108200	1112560	Monte Carlo Markov chain sampling. But actually, so there's
1112560	1117640	that and I'll explain if you want why this is important and to use
1117640	1121160	machine learning there. But but also, it can be used to
1121160	1124960	estimate probabilities themselves, not just sampling, but
1124960	1125920	also estimate
1127560	1131600	intractable quantities like partition functions and a
1131600	1134960	condition probabilities that would otherwise require summing
1134960	1140120	over an intractable number of terms. So I think of this as
1140120	1143640	the potentially, you know, there's still we're still at the
1143640	1148280	beginnings of this has a Swiss army knife of probabilistic
1148280	1151960	modeling that uses machine learning to be able to do things
1151960	1156120	that look intractable, but do them efficiently thanks to
1156120	1158280	generalization power of large neural nets.
1159520	1162960	We've been trying to think of a way to help our listeners
1163120	1168680	visualize what a what a G flow net does. And I wanted to run by
1168720	1171520	a possibility to you. So I'm not sure if you've heard of
1172000	1176360	Galton boards also called, you know, bean machines. And what
1176360	1179680	they are is this prop that's often used by statistics
1179680	1183080	professors at the start of say, an elementary introductory
1183080	1187080	course to give a visual intuition. And it's a board that has
1187080	1191640	these vertical buckets down at the bottom with interleaved rows
1191640	1197080	of pegs above the buckets, and then beads are filled in into the
1197080	1199800	top of the board, and they bounce either left or right as
1199800	1203200	they hit the pegs. And they eventually collect down at the
1203200	1205960	bottom. Yeah, yeah. Yeah, now now if the peg is a very good
1205960	1208960	analogy, except that it's not a tree, I don't know how these
1208960	1212200	things are, but you know, the ball can come to a place from
1212200	1214800	two different paths or an potentially large number of
1214800	1218800	paths. Right, right. And I think, given given there are some
1218840	1222520	some differences, you know, the idea was that if the pegs or
1222520	1225400	no, it's it's pretty close to exactly what it is. Yeah, and
1225400	1228360	what we were thinking is that if the pegs on the Galton board
1228360	1231720	are precisely and symmetrically arranged, you know, the beads
1231760	1236200	will form a nice binomial curve at the bottom. And it seems
1236200	1239520	like what G flow nets are capable of doing when they
1239520	1244360	optimize the pathways. They're tweaking the pegs a little bit
1244360	1248160	to the left, or a little to the right, to bias the flow of
1248160	1251440	beads one way or the other. And in this way, a G flow net
1251440	1254240	could arrange the pegs so that the beads could form any
1254240	1257640	distribution at the bottom that we want. And for our purposes,
1257640	1261800	that means the distribution that matches the reward function. So
1261800	1265520	is this a good way to think about G flow nets? Yes, it is. Now,
1265560	1267880	it's missing a really important aspect of it, which would be
1267880	1274800	difficult to send visually, but that all of these peg weights,
1275400	1280960	like the polytip that are boggles left or right, are not just
1280960	1283920	like learned independently, like as a tabular machine
1283920	1288480	earning, but that there's like one neural net that knows about
1288520	1294560	the locations in this big board as input and tells, you know, how
1294560	1298080	much relative weight should I, you know, go to go left or right
1298080	1302640	at this position. So the reason this is important is because it
1302640	1307000	allows for generalization. Because this board is huge, it's
1307000	1310120	exponentially large. So there's no way you're going to learn, like
1310120	1314280	a separate parameter for each of these choices. And so you have
1314280	1317960	this neural net or potentially several neural nets, but that
1317960	1322560	share allow you to share statistical strength, as we
1322560	1326800	call it, share information across all the possible positions, so
1326800	1330200	that you can generalize to places paths that it has never
1330200	1334080	seen from a finite number of training trajectories that it
1334080	1336940	sees while it's being trained. And that's crucial. Otherwise, you
1336940	1339980	couldn't scale to large problems, which is really what we
1339980	1340500	want to do.
1341220	1344260	Professor Benjo, we spoke with Professor Carl Friston about his
1344260	1347780	free energy principle, an active inference, which is pretty much
1347780	1350620	a Bayesian flavored version of reinforcement learning. And he
1350620	1353220	said that while we need to maintain entropy and stop
1353220	1356500	models from increasing too much in complexity, we should balance
1356500	1358860	entropy with accuracy in a principled way. And by the way,
1358860	1360420	you can kind of think of them in just the audience think of
1360420	1364500	entropy as keeping your options open. But Friston thinks that
1364500	1366860	the Bellman-esque idea of reinforcement learning, which is
1366860	1370020	to say maximizing expected reward is the objective is
1370020	1372740	misguided. And we should instead perform inference over future
1372740	1376700	paths, balancing expected reward of relative entropy. Is there a
1376700	1379260	connection between these ideas? I mean, it seems like G flow
1379260	1381420	nets are sampling paths proportional to the reward
1381420	1384820	function, that will maintain as much entropy as the reward
1384820	1385620	function itself.
1387580	1391620	Yes, yes, exactly. It's a translation of the reward
1391620	1397140	function into machinery that can sample, you know, the equivalent,
1397140	1401340	the corresponding distribution. So yeah, I completely agree with
1401620	1407660	what Carl was saying here. But as I said, what's interesting is,
1409140	1415060	we can do things with G flow nets. In principle, we've done the
1415060	1417780	math and some small scale experiments that we have now a
1417780	1421060	number of papers, we can do things that go beyond sampling.
1422180	1427660	But for example, estimate entropy itself. So entropy is
1428060	1437220	notoriously difficult to estimate. And I mentioned in my talks on
1437220	1442140	G flow nets that we can use the G flow net machinery to estimate
1442180	1446620	entropy of say, an action distribution or a distribution
1446620	1449980	over Bayesian parameters, for example, which is would be
1449980	1453260	something you'd like to minimize if you're going to take an
1453260	1456900	action in the world. And you have a model of the world that has
1456900	1460780	uncertainty. And that connects with Carl, for instance,
1460780	1464860	interest, you'd like to be able to choose an action that
1464860	1468460	minimizes your uncertainty about how the world works, we know
1468460	1472900	what are the latent things that may have happened. And good, you
1472900	1476540	know, an important part of that is estimating the reward for
1476580	1480660	the these exploratory actions, like, you know, children playing
1480660	1485540	around is how much reduction in entropy of my knowledge of the
1485540	1488060	world, I'm going to get through that action. So you need to be
1488060	1491380	able to compute that reward. That reward word is basically an
1491380	1495860	entropy over something you care about. And it turns out you can
1495860	1497540	also do that with G flow nets.
1497700	1499980	We're actually speaking with Friston again next week, do you
1499980	1502260	have a question that you would like us to put to him?
1503140	1507540	Well, he, you know, he's on the biology side of things much
1507540	1514140	more than I am. And I believe there are amazing scientific
1514140	1522380	opportunities to explore how the kind of machinery that G
1522380	1527260	flow nets offer could be used by brains in order to do some of
1527260	1531580	the things they do. Using your nets to model the probabilistic
1531580	1533860	structure of the world, including uncertainty, which is
1533860	1538420	something he cares about. But but also taking into consideration
1538420	1541780	things like high level cognition, the global workspace theory,
1541780	1545620	which is something I care a lot about, attention, they all kind
1545620	1553780	of fit in the picture of G flow nets. So so I think there's a
1553820	1559020	huge potential of research at the synergy of computational and
1559020	1563460	theoretical neuroscience, and machine learning, probabilistic
1563460	1567940	modeling of the kind that G flow nets propose to come up with a
1568980	1573860	some proposals for explanatory theories about what the brain
1573860	1578300	does, that's probabilistic. And, you know, I think he would be a
1578300	1579900	great person to be part of that.
1580220	1584220	Fascinating. Well, going a little bit further down that line,
1584420	1587340	there are folks in the community who are huge advocates of
1587340	1589220	biologically inspired approaches to machine
1589220	1592540	intelligence. And, you know, one of the key ideas actually is
1592540	1595620	diversity, discovery and preservation, both in how
1595620	1598260	knowledge is acquired and represented. I mean, specifically
1598260	1601300	evolutionary algorithm advocates, they differentiate
1601300	1604060	themselves from gradient based single agent monolithic
1604060	1607140	approaches like reinforcement learning. And they point out that
1607220	1610380	their approaches overcome so called deception and search
1610380	1612540	problems, you know, which is to say they don't get stuck in
1612540	1615700	local minima, your approach seems to be achieving something very
1615700	1618460	similar in the context of a gradient based reinforcement
1618460	1620420	learning package. I mean, I don't see it as being mutually
1620420	1621940	exclusive. But what's your take on this?
1622460	1627260	Yeah, diversity is important when you're exploring and humans,
1627260	1630220	especially young ones are exploration machines, they're
1630220	1632460	trying to understand how the world works and they're acting in
1632460	1636500	the world in order to get that information. Yeah, I agree that
1636940	1643740	that search process needs to have a big bonus on on diversity,
1643740	1648340	like on trying different ways of achieving something good, like
1648860	1653900	better understanding how the world works. So it turns out that
1653900	1659220	in the G flow net framework, you, you have a training objective
1659220	1663420	that yields this kind of diversity and exploration, but is
1663420	1667220	based on training large neural nets end to end. Now, it's a bit
1667220	1672260	different from the usual end to end training, because we don't
1672260	1675340	have an objective that objective we're trying to optimize
1675340	1680660	is not tractable, actually. But we can sample these trajectories,
1680660	1684220	which I think of like sampling thoughts, like our thought
1684220	1687820	process is going through some chain of explanations, not a
1687820	1690620	complete, and it doesn't represent all the explanations,
1690620	1694380	but but what we found with our training objectives for G
1694380	1699260	flow nets is that these sort of random randomized kind of views
1699260	1703100	of the world are sufficient to give a training signal to the
1703100	1705300	neural nets that do the real job.
1706300	1710460	I'm curious. So this trade off between exploration versus
1710500	1714820	exploitation. And this has come up in so many contexts, you know,
1714820	1717580	throughout our show. And one in particular, as we talked to, you
1717580	1722100	know, we've talked to multi arm banded folks, right? And G flow
1722100	1725740	net seemed to capture this balance between exploration
1725740	1729740	and exploitation. But the multi arm banded folks, you know, they
1729740	1733700	dive deep in that research circle into this into this trade off.
1733700	1737940	And I think they have some very principled ways and even very
1737940	1742420	rigorous ways to analyze this fundamental trade off. To what
1742420	1745620	extent do you think that that their research maybe could be
1745620	1750100	applied to future G flow net variations? Like do you think
1750100	1754060	maybe it might open up more options to fine tune the
1754060	1756460	trade off between exploration and exploitation?
1757460	1762300	Yeah, I mean, the banded research is very, very closely
1762300	1767260	related to the G flow net thread. But G flow nets, as we have
1767260	1770860	been using them, for example, for drug discovery, they are
1770860	1775380	banded. It's just that the action space is not, you know, one
1775380	1780540	out of n things. It's, it's combinatorial because you build
1780540	1783860	these pieces. So the action space is not something you can
1784020	1788060	enumerate. So you can't apply the typical banded algorithms, but
1788060	1792300	a lot of the math is totally applicable. And in fact, what we
1792300	1799260	use in the drug discovery setting is UCB upper confidence
1799260	1810820	bound objective to learn a good exploration policy. So that comes
1810860	1816020	out of the banded research. It what it does is it, you know, it
1816020	1821980	combines the risk and reward expected reward, one of these
1821980	1827100	together in a way that in theory guarantees that you will do an
1827100	1831740	efficient exploration and find that where is the, you know,
1832140	1835620	where's the money? Where's the reward, right? All of the
1835620	1837660	possible places where you can get the reward.
1838140	1843660	So in, in, in the G flow net papers, you often describe it as, you
1843660	1848180	know, we want to sample not only the maximum reward path, in
1848180	1851700	order to have more diversity in order to maybe figure out
1851700	1855260	something that we didn't know if we were just to go to the
1855260	1858740	maximum reward. And that speaks a little bit to the, like the
1858740	1863820	things that we know that we don't know, right? We maybe know
1863820	1868180	that, right, this seems like a lower reward trajectory might
1868180	1872220	turn out to be a higher reward trajectory. However, exploration
1872220	1874900	and reinforcement learning is also fundamentally addressing the
1874900	1878860	things about the things that I don't know that I don't know,
1878940	1883100	which is where stuff like random exploration and things like
1883100	1886980	this comes in. Could you maybe comment a little bit on how you
1886980	1891300	see sort of, because it seems to me that if I managed to sample
1891300	1895700	according to what I think is the reward distribution, right, I
1895700	1899220	still have this problem of maybe there is a deceptive rewards
1899220	1902780	there are, you know, I need to take a step back, I may not know
1902780	1906820	some sort of some, some area of the search space. And don't I
1906820	1908980	just run into the same problems again?
1910340	1916700	So, so the important trick here is you need your model of the
1916700	1922300	reward distribution, or the reward function to be one that
1922300	1925700	captures uncertainty, like, maybe in a Bayesian way, or, you
1925700	1929540	know, whichever way, the Bayesian way, by the way, fits well
1929540	1935660	with the G flow net framework, because we can consider the
1935940	1940380	parameters of the reward function as latent variables, like
1940380	1942500	you don't actually know the reward function, you're trying
1942500	1945500	to figure it out from experiments. So the G flow
1945500	1952140	net can sample, and not just like what you should be doing in
1952140	1957540	order to acquire information, but also potential reward function.
1957540	1961740	So, you know, we don't actually have a knowledge of how the, you
1961740	1963220	know, what's going to be the rewards we're going to get in
1963220	1968380	the world. Classical IRL is going, as you said, to the expected
1968380	1972820	value and try to maximize that, whereas the G flow net approach
1972860	1976420	is trying to acquire as much knowledge as possible about the
1976420	1979380	underlying reward function. So you're trying to minimize the
1979380	1984860	uncertainty. So your model with the G flow net is modeling the
1984860	1991460	uncertainty, and then it can use it as a reward for the policy
1991460	1994660	that is going to do action in the real world. So we're talking
1994660	1997460	about different G flow nets. There's a G flow net that models
1997460	2000420	the uncertainty in the reward that you're going to get from the
2000420	2004660	real world. And that's like a Bayesian model. And then you have
2004660	2009300	another G flow net that controls the policy that searches to
2009380	2013140	and its reward is how much uncertainty reduction you're
2013140	2020100	going to get by doing this or that. So, so yeah, you need to
2020100	2025460	have a part of your model that is kind of aware of the fact
2025460	2028060	that there are whole areas in the world that you don't know
2028060	2030580	about or aspects of the world that you don't know about so
2030580	2032780	that you can drive the exploration.
2033580	2036140	I would love to know where some of the magic is coming from.
2036780	2039740	The promise of G flow nets is that we can discover as many
2039740	2043420	modes as possible in the path distribution. Traditionally in
2043540	2046660	Markov chain Monte Carlo, we had to hack priors into the
2046660	2049380	algorithm by hand, you know, to find new modes or areas of
2049380	2051580	information efficiently, especially when they were very
2051580	2055100	far apart or not very sharp. The hypothesis of G flow nets is
2055140	2058380	that the structure of these modes is learnable on many
2058380	2061060	problems, even in high dimensions. It's a little bit like
2061060	2063100	saying we're getting a free lunch. I mean, actually, I think
2063100	2065980	you used that exact phrase to describe what we're doing here.
2066220	2069700	Many research avenues have tried to develop general methods to
2069700	2072780	discover these structures and have failed. How do you think
2072780	2076020	G flow nets will overcome this seemingly intractable curse?
2077020	2080340	There is no guarantee that they will, because if there is no
2080340	2083740	structure in the underlying function you're trying to
2083740	2087020	discover. So let's say the reward function or the energy
2087020	2092100	function that you care about, then having visited some finite
2092100	2098220	number of modes like regions where your reward is high site is
2098220	2100340	not going to tell you anything about what are the other good
2100340	2104580	places, the other modes. So so there's no guarantee that it
2104580	2109120	will work. But if if there is structure, then there is a free
2109120	2112780	lunch. And we know machine learning is good at that. Like
2113340	2117540	the last 10 years of deep learning and its success. What is it
2117540	2120140	telling us? It's telling us that you can generalize right that
2120140	2123540	these nets, I'm not saying they generalize perfectly, but they
2123540	2128900	can generalize. So you can think of it like the machine
2128900	2134260	learning problem is given some examples of good things, like,
2134300	2137260	you know, places where you get reward, you can you generalize
2137260	2141180	to other places. And the supervised learning way of
2141180	2144060	thinking about it is, you know, given a candidate place, tell me
2144060	2147740	how much reward I think I would get. The G for that sampler is
2147740	2150900	learning the inverse function is like to sample, but it's kind of
2150900	2154020	the same thing. It's just going in the other direction. Give me
2154020	2157700	some, you know, sample some some good places that that you know,
2157700	2162520	where the reward is high. So we now have a lot of experience in
2162520	2167780	designing powerful your nets that can be leveraged to
2167780	2172020	generalize in those spaces where we normally use MCMC. And if
2172020	2177500	there is kind of regularities that allow to generalize, then all
2177500	2180300	of that can be, you know, put to use.
2181100	2183820	We mentioned earlier, reinforcement learning often
2183820	2187260	being applied in a context where you have this kind of solid
2187260	2190100	reward function. So let's say games, you know, playing chess.
2190540	2193700	I'm really curious, what would happen hypothetically, if we
2193940	2199540	applied G flow net, you know, to something like chess. So I mean,
2199540	2202180	I think given the fact that reinforcement learning like say
2202180	2205300	alpha zero is trained specifically to choose the best
2205300	2209580	move rather than diverse moves, it seems obvious that maybe if
2209580	2213860	given equal resources to both alpha zero and flow zero, alpha
2213860	2218060	zero would probably beat flow zero. However, I think if flow
2218060	2221620	zero were given more resources, say and trained to the same
2221620	2225940	rating, say the same elo rating as alpha zero, it seems like
2226060	2229740	flow zero, if you would, would play significantly more
2229740	2234460	diverse and interesting games with a wider variety of styles.
2234460	2237060	And I think you could even imagine also that it could be
2237060	2241500	possible, even if given equal resources, but sufficiently
2241500	2245660	high enough resources, that a hypothetical flow zero would
2245660	2249260	consistently reach higher ratings, because it might find, you
2249300	2252340	know, more interesting stepping stones that have the
2252340	2256340	potential to avoid deception, because it can explore seemingly
2256620	2261060	lower reward paths that ultimately develop into higher reward.
2261100	2262820	More curious if you have any thoughts on that?
2263780	2271100	Yeah. It's a good question. I would say where the kind of
2271100	2275020	approach we've been pioneering with G flow nets might be really
2275020	2279860	paying off is if you think about it from the perspective of the
2279860	2283340	learner has a finite computational, you know, amount of
2283340	2287700	resources, because in principle, right, if you had infinite
2287700	2291940	compute, and you know the reward function, like the rules of
2291940	2296620	chess or go, then you can just crank and find, you know, the
2296620	2301780	policy that's best in every possible setting. Now, if you
2301780	2304780	have finite resources, like, you know, you, you have a budget
2304780	2310780	of compute, you'd like to use it efficiently. And so that's
2310780	2313820	where the exploration exploitation trade off becomes
2313820	2324340	important. And if you if you had a, say, a current policy that
2324340	2328900	you're not completely sure is the right one. And, and then
2328900	2334220	you're trying to say, Well, what, how should I play so that I'm
2334220	2337860	going to improve my policy the most as in I'm going to reduce
2338220	2341220	the uncertainty that, you know, it is the right policy, like
2341220	2345620	that it picks the right things. So now we're getting closer to
2345620	2348740	the kind of setting where it makes sense to use G flow nets.
2349420	2354020	And then what I would expect, if we do the engineering work
2354020	2357540	here, but based on the sort of much simpler problems we've
2357540	2362660	looked at, is that it would converge faster. So given, if you
2362660	2368860	look at on the x axis, the number of games you're playing. And
2368860	2372900	on the y axis, how good is your policy measured like on other
2372900	2375980	games. So that's where you would get. In other words, it's the
2375980	2379580	learning curve that you might gain on asymptotically, everything
2379580	2383700	is going to converge the optimal chess player, right? So the
2383700	2386900	the place where it's interesting is to look at the learning
2386980	2390340	curve how fast you learn. And here you want to sort of active
2390340	2393580	learning thinking like, Well, I'm not just trying to win here.
2394220	2398980	I'm trying to gather information so that I'll win more in the
2398980	2402300	future. And it's a different objective. And that's where you
2402300	2405220	need diversity and exploration and like a model of your own
2405220	2408180	uncertainty and an active learning policy.
2409380	2414580	How much do you think this could be part of not maybe only
2414580	2420540	reward maximization things, but information collection, things
2420540	2425940	like, I'm sure you're you're thinking about in, let's say the
2425940	2429380	brain, there is there's sort of maybe a similar process going
2429380	2432860	on and what do I still need to retrieve in order to give certain
2432860	2437780	answers to questions, or maybe in our, let's say, big search
2437780	2442540	engine, let's just name one for naming sake, let's Google, or
2442580	2447820	so would would try to answer your query, not by just searching
2447820	2450700	through their index, but by actively doing this multiple
2450820	2453780	multiple things like, is this enough? Is this enough? Is this
2453780	2458260	enough? Do you see connections to these types of things? Or are
2458260	2461500	they inherently different? Because they might be not learning
2461500	2462260	on the spot?
2462820	2468060	What they're doing on the spot is acquiring information. And you
2468060	2470140	want to do it in an efficient way. And that's where sort of the
2470140	2471740	active learning thinking comes in.
2472700	2477860	And I think it's actually a very big, practical problem in
2477860	2481260	the deployment of like AI dialogue systems that are not
2481260	2486260	chit chat, but they're trying to say help a user achieve, you
2486260	2488900	know, get something get information or something like
2488900	2492460	this. This is this is a huge need for this in, you know, the
2492460	2495980	business world and search engines, and you know, it's much
2495980	2499340	more than search engines. So I don't think we have the
2499340	2503980	algorithms that do that right now. And it's kind of painful. The
2503980	2509420	human has to know, you know, is driving. But if, if we had
2509420	2513980	systems that could explicitly model their own, say,
2513980	2517580	uncertainty about what the user needs or wants, or where to
2517580	2523580	find information. And then, and you need like pretty powerful
2523580	2526300	models of that, like it's not just galaxies, they're simple
2526300	2529620	things. That's where G flow net strengths comes in, you can
2529620	2532660	represent very, very complex distributions over
2532660	2539140	compositional objects. It's not just a few numbers. And then I
2539140	2543420	think you could get to much more efficient human machine
2543420	2550420	interfaces. And the same, I believe the same methodology
2550420	2553420	could be used more generally in scientific discovery. So what
2553460	2555940	is scientific discovery? Like what is it that scientists do?
2556580	2560500	They plan experiments that are going to allow them to reduce
2560500	2563980	the uncertainty on their theories of, you know, some
2563980	2566420	aspect of the world. It's the same problem. Yeah, you have a
2566420	2570340	series of questions you're allowed to ask to nature. And you
2570340	2573340	try to ask as few questions as possible to as quickly as
2573340	2575060	possible, understand what's going on.
2576020	2579220	Is there a connection fundamentally to I'm thinking of
2579220	2582980	causality, which also I've seen a number of papers that you've
2583020	2586420	collaborated on with people who are who are deep into
2586460	2590900	causality research and so on. What do you think there is a
2592740	2598180	a connection there where an agent could learn to uncover if
2598180	2600700	you think about scientific discovery to uncover the
2600700	2604780	fundamental causal structure of the world by asking such
2604780	2607980	questions, like could there be a connection to that branch of
2607980	2612340	research? And could this finally be like the unification of
2612620	2617500	of something machine learning and the the world of causality?
2618740	2621220	Yes, you guys are really asking all the right questions. Thank
2621220	2626540	you so much. In fact, one of my main motivations for the
2626580	2631260	pursuing the the G flow net research program is that I think
2631260	2638100	it's the it's an ideal tool for implementing what I called in
2638100	2643460	my talks, system to inductive biases. So what this means is
2643940	2646580	there are lots of things we know from neuroscience and
2646580	2652220	cognitive science about how we think. And we can bring that
2652260	2658620	into the design of probabilistic machine learning, you know,
2658620	2662940	based on deep learning is the building blocks. And one of the
2662940	2666220	inductive biases, like one of the characteristics of how we
2666220	2669140	think is we think causally, we're constantly asking the why
2669140	2674020	questions we're trying to find explanations and so on. And, and
2674060	2677940	and that connects with classical AI, like the way we think, to
2677940	2682020	some extent, has also inspired classical AI, you know, rules
2682020	2686620	and logic and and reasoning. And we haven't yet found the way
2686620	2690380	to integrate these abilities in deep learning. And of course,
2690380	2694700	lots of people are like, trying to and and that's important.
2695700	2699100	But but I think the reason why G for nets give us an amazing
2699100	2703940	handle on this is because they they're really good at
2703940	2708300	representing distributions and sampling over graphs. And, and
2708300	2712660	like a reasoning or a set of possible reasoning to explain
2712660	2718540	something or to, you know, for planning. The these are graphs.
2719780	2724300	And your thoughts can be seen as graphs, right? So think of like,
2724460	2727140	maybe a simple version of this, think of a parse, like a
2727140	2732500	semantic and syntactic parse of a sentence is a graph. But
2732500	2734260	usually it's, you know, it's more than a tree, there are all
2734260	2737300	sorts of semantic connections, including with knowledge graphs,
2737300	2742900	right, which also graphs. So the ability to implicitly represent
2742940	2746900	those distributions and sample pieces of them as thoughts is, I
2746900	2750740	think, fundamental to how we think. And going back to
2750740	2753020	causality, one of the hard questions that I think G for
2753020	2756940	nets can help us with is causal discovery. So in other words,
2757060	2759420	what is the underlying cause structure of the world, including
2759420	2763980	the uncertainty about it? Given the things we observe, a lot of
2763980	2766540	the research and causality has been okay, we observe these,
2766580	2770900	these random variables, discover, you know, make inferences
2770900	2774140	about, you know, whether what we can say about whether it goes
2774140	2779500	to be and so on. But it's much harder to discover the causal
2779540	2784020	graph that that, you know, in a large set of variables, and
2784020	2786980	it's even harder. And really, nobody's done a real job there.
2787420	2791060	To do this when what the learner sees is not the causal
2791060	2794140	variables, but just like low level pixels. And you also have
2794140	2796260	to figure out what are the causal variables and how they're
2796260	2799220	related causal. And I think G planets can help us do that.
2799940	2803060	This this opens up, this is so many avenues of questions, I
2803060	2806780	think it'll probably almost be a future episode in itself. But
2806780	2812140	let me just ask you about some of the basic ones, which is, as
2812140	2814740	you mentioned, kind of learning the causality causality
2814740	2818580	structure, much more difficult problem. And the first question
2818580	2821900	is just how to represent the causality. And so you, you, you
2821900	2824380	mentioned graphs, you know, graphs is one way. And of course,
2824740	2829260	you can develop, you know, isomorphic ways of representing
2829660	2832740	certain parts of logic as graphs, etc, depending on how, you
2832740	2836500	know, how rich you make the graph structure. But there's
2836500	2838860	also the other issue of, you know, when you're trying to
2838860	2841620	build, and I think it's probably correct to call this a world
2841620	2844220	model, right, like we're trying to build a causal
2844420	2847780	that's the word I use. Okay, great. And I, and so I have one
2847780	2851860	quick question about that, which is, you know, to me, to some
2851860	2854860	people, world model is only the discriminative function. It's
2854860	2857700	just that, you know, probability y given x, to me, it's more
2857700	2861300	general. It's also the structure of x. Is that, is that also
2861300	2864540	your, your view as well? Yes. Yes. Okay. And so in
2864580	2867700	constructing those, those world models, some of the, let's say
2867700	2872700	the pushback on on these type of generative techniques from, from
2872700	2875620	folks that are more skew more towards the discriminative side
2875980	2878860	is, hey, look, fine, you're going to go and try and build this
2878860	2881580	generative model, it's going to be even more complicated than
2881580	2884860	this discriminative model, because it also has to learn, you
2884860	2888020	know, the structure on x. But I think the possible free lunch
2888020	2893140	here, is that you can learn abstract structure on on x. And
2893140	2897100	so if you learn these abstract world models, throwing away all
2897100	2899700	the nitty gritty that doesn't really matter, you can potentially
2899700	2903660	have very powerful, you know, predictive encoding, if you will,
2903700	2905420	like, what's, what's your thoughts on that?
2905900	2910740	Oh, that's what I've been thinking for almost 20 years. And
2910740	2914140	one of the reasons why I've been interested in deep learning as
2914180	2919140	a way to think of discovering abstract representations, you
2919140	2924020	know, from the early days of deep learning, as in like mid like
2924020	2929180	2005 or something. And, and in the paper that Jan McCarr and I
2929180	2933020	wrote about, and also other papers I wrote with some of my
2933020	2936540	colleagues at the University of Montreal on, you know, deep
2936540	2940780	learning around 2010, they are all about that notion that we
2940780	2945860	would like these unsupervised learning procedures to discover
2945860	2950780	these abstract factors, as we call them. But now I think it's
2950780	2954460	not just the factors like the variables, but it's also more
2954460	2958100	importantly, even how they're related to each other, which in
2958100	2964460	the causal language is what we call causal mechanisms. And so
2964500	2967260	here's a fundamental way of thinking about this. If you
2967260	2971020	don't introduce the abstract kind of structure that exists in
2971020	2976900	the world, then representing p of x, the input distribution is
2976900	2980740	very difficult. It's, in other words, you'll need a lot of data
2980740	2984540	to learn it. And it's not going to be generalizing very well. The
2984540	2989260	whole point of abstraction is that it gives you very powerful
2989260	2991620	abilities to generalize to new settings, including out of
2991620	2993860	distribution, which is one of the hardest topics in machine
2993860	2997100	learning right now. How do we extend what we do so that it
2997100	3001300	generalizes well in new settings? And thinking causally
3001300	3005900	about these abstract causal dependencies, as the things that
3005900	3010580	are preserved across changes in distribution, like, if I go to
3010580	3014820	the moon, it's the same laws of physics, but the distribution is
3014820	3018460	very different. How do I generalize, you know, across such
3018460	3023980	changes in distribution? It's because the learner is us, you
3023980	3028060	know, if we, if we were, if we had the right education, has
3028060	3031980	figured out the underlying, at least, you know, enough of the
3031980	3035740	underlying causal mechanisms, that we can be transported in a
3035740	3039940	different world, but where there's the same laws of physics,
3040340	3044300	and we can predict what's going to happen, even though it looks
3044300	3047860	completely different from, you know, our training environment.
3048380	3055820	So the, the idea of extraction is really that if you introduce
3055820	3061500	abstractions, the description length of the data becomes way
3061500	3063780	smaller. And that's why you get generalization.
3065380	3066060	Absolutely.
3066220	3070220	I'm fascinated by these abstract categories. I think it's the
3070220	3073060	most exciting thing in AI. I mean, Douglas Hofstadter spoke
3073060	3076860	about cognitive categories, like the concept of sour grapes,
3076860	3079900	for example, to represent the certain thing. And almost
3079900	3083740	magically, our brain seems to arrange these cognitive
3083740	3085660	categories. And it's not entirely clear to me whether they're
3085660	3089260	an emergent phenomenon, or whether it's some other process.
3089540	3091980	But the modes that you're discovering in G flow nets,
3091980	3095540	they're a kind of category, these cognitive categories that I
3095540	3098180	just spoke about our abstractions, also things like
3098180	3101060	causality and geometric deep learning that they are kinds of
3101060	3103420	categories. But I've always had this intuition that deep
3103420	3107700	learning doesn't learn the categories on its own, it needs
3107700	3112220	humans to kind of put priors into the model, as we do with
3112260	3115100	geometric deep learning. Do you think that that will always be
3115100	3117380	the case? Or can we have that meta level of learning?
3118180	3122820	Yes. What I really want to do is build machines that can
3122820	3126140	discover their own semantic categories, abstract ones that
3126140	3129740	really help them understand the world. And of course, they're
3129740	3133220	going to learn, you know, better and faster if we help them just
3133220	3135700	like, you know, we teach kids, we don't let them discover the
3135700	3141780	world by themselves. But we do have an ability to invent new
3141780	3144020	categories. That's what scientists do all the time,
3144020	3149380	right? Or artists and, you know, writers and philosophers and
3149380	3152700	scholars, and ordinary people who find new solutions to
3152700	3156060	problems, we do that all the time, our brain is a machine
3156100	3159620	discovers new abstractions. Of course, that usually it's just
3159620	3162580	like one little bit on top of all the things we got from our
3162620	3168180	cultural input. But but that's the ability that we don't have
3168180	3173020	right now in machine learning. And that is going to, I think, be
3173020	3176500	a huge advantage. So now we're not in reinforcement learning,
3176500	3179620	we're not in active learning, we're talking about unsupervised
3179620	3184580	learning. So we're talking about how can a machine discover
3184580	3194260	these often discrete concepts that somehow help it understand.
3194260	3197020	So in other words, build a compact understanding of lots of
3197020	3202380	things that generalize across many settings. And yeah, that
3202420	3208900	that's that the path to build that is, is becoming more and more
3209900	3215860	firm in my mind, as I move forward with G flow nets. So as a
3215860	3219420	clue, there was a paper we had recently, I think in Europe's
3219820	3223820	on that's connected to the global workspace theory that says
3223820	3227500	that it's about discrete valued neural communication, I think
3227500	3231860	is a title where the one interesting intuition here is
3231860	3238660	connected to this is if you if you constrain the communication
3238660	3241060	between different modules, say in the brain or in machine
3241060	3244540	learning system, to use as few bits as possible and discrete is
3244540	3248220	the way to get the very few bits. You can get better
3248220	3252180	generalization. And there are good reasons for that that we
3252180	3254980	try to explain in the paper. But but that's, that's it, you
3254980	3259980	know, there's a clue here that discrete concepts emerge as a
3259980	3261460	way to get better generalization.
3262860	3267820	You you mentioned before, and in terms of discreteness, and
3267860	3272140	what you mentioned before with graphs being very fundamental, it
3272140	3275620	connects a little bit back to a paper that you, I think,
3275620	3280860	provocatively titled the consciousness prior, where where
3280860	3284980	you connect sort of the ideas of attention, sparse factor,
3284980	3288660	graphs, language, things being discreet, things being
3288660	3294500	describable by language, right? And, and I find that all to be
3294540	3300100	very interesting. On the topic of consciousness, we would be, it
3300100	3303100	would not be appropriate for us to not put this question to you.
3303100	3307060	So you're not, you're not very active on Twitter, which is
3307060	3310940	probably why you're so productive. But if currently,
3311060	3314580	there is a bit of a of a thing happening on Twitter, namely,
3315020	3321900	Ilya Satskever of Open AI has tweeted out a seemingly innocuous
3322180	3327660	tweet saying, it may be that today's large neural networks are
3327660	3333580	slightly conscious, which has resulted in quite a, let's say,
3333580	3338300	a storm on of people agreeing, disagreeing. Obviously, he's
3338300	3341300	he's talking about maybe, you know, the large language models
3341300	3344540	we have today, which do incorporate a lot of the things
3344540	3347860	you talk about, they do incorporate attention mechanisms,
3347940	3352140	lots of them. Presumably, it's all one needs. They do
3352140	3355260	incorporate language, they do incorporate discrete things with
3355260	3359140	you know, discrete tokens and so on. What do you make of a
3359140	3362300	statement like this? It may be that today's large neural
3362300	3364300	networks are slightly conscious.
3365380	3370060	Well, this one fundamental problem with such statements,
3371500	3375940	which is we don't know what consciousness really is. So I
3375980	3381420	think we have to have a bit of humility here. And I can't say
3381460	3384740	what Ilya is saying is true or not. I think that this is more to
3384740	3389060	consciousness than what we have in these large language models by
3389060	3395540	a big gap. But that being said, and you know, we do need to work
3395540	3399580	with our colleagues in your science and kind of science who
3399580	3402860	are trying to figure out what consciousness is from a scientific
3403300	3407220	perspective and philosophers who are helping also to make sense of
3407220	3414060	that landscape. So we have to be careful with the use of those
3414060	3416660	words. And you know, I was a bit liberal in the title of my
3416780	3421420	paper. And I learned a lot about consciousness since then,
3422260	3425420	learned that there's a lot that we don't understand that at the
3425420	3428500	same time, there are enough bits that we know from from
3428540	3434860	cognitive neuroscience that can serve as inspiration for how we
3434860	3438540	could build machine learning systems that have similar, say
3438540	3441580	conscious processing machinery. Okay, let's not say consciousness
3441580	3443500	but just conscious processing machine because that's less
3443500	3447300	controversial. And by the way, the word consciousness has been
3447300	3452260	taboo with most of science for a long time. And it has become
3452300	3456540	untapped, you know, the tabooed in neuroscience, because we're
3456540	3459060	starting to be able to make measurements of what's going on
3459100	3462820	inside your brain, while you're doing things consciously or not
3462820	3466940	and so on and distinguish the parts that you're consciously
3466940	3469620	aware of and the parts that are there in your brain, but you're
3469620	3472140	not conscious. So we're trying to we're starting to make a lot of
3472140	3475420	progress of what it means to be conscious of something or not.
3476940	3482180	And I, you know, I think this is a very exciting and important
3482220	3488540	scientific question. And I would rather like work on exploring
3488540	3492460	hypotheses and theories to explain our conscious abilities,
3493420	3496580	rather than make bold statements about whether current neural
3496580	3497900	nets are conscious or not.
3498620	3502580	Professor Benjo, we've got some David Chalmers on the show next
3502580	3505580	month. Do you have any questions that you had put to him?
3506420	3516060	I very much like a hypothesis about consciousness that Michael
3516060	3525380	Graziano has put out to help explain the qualia, the subjective
3525380	3531340	experience part that Chalmers wrote might be something science
3531340	3538740	can't really, you know, touch. And so what's, you know, I'd like
3538740	3543060	to hear what he has to say about these kinds of approaches. And
3543220	3550700	one of the basic premise here is is very grounded in things we
3550700	3556460	can do scientifically. It's to say, well, let's not try to
3556460	3559780	figure out what is consciousness or subjective
3559780	3564940	experience more specifically, you know, from a philosopher's
3564940	3570100	chairs. But let's let's consider that as a phenomenon that is
3570100	3572580	happening in the brain. I mean, unless you believe in sort of
3572580	3575300	supernatural things, if it is happening, something is happening
3575300	3578360	in the brain, and we can report about it. And we can, we can
3578360	3581780	like, measure what's going on in various parts of your brain
3581780	3590540	while this is happening. And then, you know, can we then come up
3590540	3595580	with theories that explain why we feel that we have subjective
3595580	3597960	experience? It's not saying whether consciousness exists or
3597960	3600940	not or subjectivity. It's not whether it exists or not in some
3600940	3603540	sort of logical sense. It's whether, you know, what is it
3603540	3606340	that's going down in our brain that gives us that feeling and
3606340	3612540	then make us say, Well, I am, you know, I'm conscious of x, y,
3612540	3617860	or z. So so that's the that's the direction I find interesting
3617860	3621420	because it opens the door for a scientific investigation. And
3621420	3626340	Michael Grosjean has a specific theory about that which I find
3626340	3632380	compelling that is really rooted in the idea that we have a world
3632380	3638180	model. And then we we because we have an attention that focuses
3638180	3643980	only parts of it at a time. And we need to have like a little
3644100	3648820	mini world model that controls that attention. That creates a
3648820	3655860	sort of separation between the the where the real knowledge is
3655860	3660580	and sort of this more abstract control and machinery that could
3660620	3666020	well, give us this illusion of Cartesian dualism, which I think
3666060	3669980	is an illusion, but but you know, must be grounded in some
3671860	3672980	you know, biological
3675740	3679580	reality. And that's I think understanding that is is a very
3679580	3682580	good question to ask. And I'd like to get to know what he
3682580	3684180	thinks about such a research program.
3685300	3686220	Thank you very cool.
3686780	3690780	Yeah, thank you. I do have one kind of nitty gritty question
3690780	3694460	because and partly partly based on some of your recent work on
3694460	3700580	becoming more of a fan of semi supervised learning. And you
3700580	3704180	know, you had a recent paper that was on interpolation
3704180	3708900	consistency training. And what I found interesting about that is
3708900	3712580	that if we consider one of the biggest challenges that we face
3712580	3715580	in machine learning pretty much across the board is an
3715620	3718700	overcoming the various, you know, curses, if you will, the
3718700	3722660	various forms of intractability that we have an empirical
3722660	3726500	learning methods. And in this context of semi supervised
3726500	3731260	learning, that recent paper, it found significant improvements
3731300	3735900	over state of the art by forcing linearity. So in this case, it
3735900	3740660	was by this mix up between the unlabeled samples and their
3740940	3745740	interpolated fake labels. And in the last decade, we've also
3745740	3749740	seen values come to dominance in the field of neural networks,
3749740	3754900	their piecewise linear recent work by Randall Belastriero,
3755220	3757980	developed an interesting frame of reference which cast
3757980	3761860	multi layer perceptrons as a decomposition method, which
3761860	3766700	produces a honeycomb of linear cells in the ambient space and
3766780	3770660	they're activated turned off or on by input examples. So my
3770660	3774260	question is, why is linearity, whether it's piecewise or
3774260	3778460	otherwise, dominating the state of the art in approximation
3778460	3781580	methods, it almost seems to me like we've kind of gone back to
3781580	3784660	the future, if you will, sort of leaving behind attempts at more
3784940	3788740	smooth nonlinear methods and gone back to newer, albeit more
3788740	3792980	complicated forms of linear approximation.
3793860	3798940	Right. I would say something that's roughly linear is
3798940	3802820	simpler. So having a regularizer that says, oh, you want to be
3802820	3806260	roughly linear or locally linear, at least to as much
3806260	3810900	extent as you can is a smoothness prior. So that's going to
3810900	3816300	help generalization. But it could also hurt if that is too
3816300	3820180	strong. And so having these piecewise linear kind of more
3820780	3825980	type of solution is a good compromise. It says as few pieces
3825980	3830260	as possible, and ideally organized in a compositional way. So
3830260	3835660	that it's not just like a relu, it's more like the discrete
3835700	3839340	abstract logic, you know, reasoning, things sitting on
3839340	3844060	top, that's controlling the pieces. But but otherwise fairly
3844060	3848460	simple in each how each of the pieces are, you know, like
3848460	3852420	linear, for example. So one way to look at this is, if you
3852420	3855660	look at classical, the kind of rules that classical AI
3855660	3859660	researchers were using, each rule is fairly simple. It's, you
3859660	3865980	know, like, it's almost linear, or it's very simple logic. But
3865980	3869900	it's the composition of all those rules that gives the power of
3869900	3872620	expression of these systems. Of course, the problem then is that
3872620	3880220	they didn't know how to train them properly. But yeah, I think
3881380	3894340	we, I think we learn to come up with these discrete ways of
3894380	3901020	breaking up things into simpler pieces. And that in fact, I
3901020	3903900	think if you're Bayesian about it, it just comes out naturally.
3904020	3906140	And they're very, very weak assumptions.
3907500	3911940	So in a way, it's it's almost, it is piecewise abstraction. So
3911940	3915740	we're kind of back. Yes, that's what I would lean to, rather
3915740	3920020	than piecewise linear. But linear, of course, is a broad part
3920020	3922580	of, you know, it's an easy way to get simple.
3923700	3926740	Amazing. Professor Benjo, I'm interested in your personal
3926740	3929540	journey. So we've been talking about diverse trajectories. And
3929620	3932180	I wanted to know about your own trajectory of research over the
3932180	3935540	last 10 years. Now, one of my mates, a psychologist and
3935540	3939020	symbolist, Professor Gary Marcus, presumably one of your best
3939020	3942340	friends, by the way, he pointed out in his 2012 New Yorker
3942340	3945660	article that MLPs lacked ways of representing causal
3945660	3948940	relationships such as between diseases and their symptoms. And
3949060	3950900	I think this has been a significant focus of yours in
3950900	3954540	recent years as we've discussed. And he thought at the time that
3954540	3958420	you were a bit too quote system one all the way. And he spoke
3958420	3960780	then about the need for heterogeneous architectures and
3960780	3963700	the acquisition of abstract concepts, compositionality and
3963700	3967060	extrapolation, which I think has also been a huge focus of yours
3967060	3969780	in the last decade or so. We really enjoyed watching your
3969780	3973340	debate with Marcus. And by the way, we would love to host V2 of
3973340	3975100	that debate. So if you're interested, you just let us
3975100	3978020	know we'll do that. But he's often viewed as a heretic. And,
3978140	3980580	you know, just forgetting about symbols versus neural networks
3980580	3983260	for a minute. Am I right in thinking that you've converged in
3983260	3985500	at least some ways in your thinking? And how would you
3985500	3986940	characterize that from your perspective?
3987820	3998900	So, yeah, I used to be in the 90s, a, you know, pure neural net
4001380	4010480	subsymbolic connectionists researcher. And I did my grad
4010480	4015740	studies at a time on neural nets at a time when the dominant way
4015740	4019580	of thinking was these, you know, classical AI rule based system
4019580	4023220	with no learning at all, and was dominant, meaning that the
4023220	4026580	little group like, you know, Jan and Jeff and I and others who
4026580	4033700	were thinking otherwise, had to, you know, defend our views. And
4034740	4040700	and maybe that led to a kind of, you know, us versus them, I
4040700	4048460	think, unhealthy way of thinking. And of course, I matured. And
4050060	4053260	one of the big, so there, I think there are several turning
4053260	4058300	points on that journey. Well, one of them in the in the 2000s
4058300	4061420	was the realization of the importance of abstraction. So
4062500	4064800	and the way to think about this maybe more concretely, because
4064800	4066900	what does it mean to be abstract? Is that I was thinking,
4067220	4071820	well, what would be the right kind of representation we want to
4071820	4074820	have at the top level of our unsupervised deep nets, because
4074820	4076980	we were doing mostly like unsupervised deep nest, like,
4076980	4081480	you know, deep boz machines and stuff in that decade. And I was
4081480	4084940	thinking, well, it would be things like words, right, things
4084940	4088380	like the sort of concepts that we manipulate at the top level,
4088380	4092380	well, it's words or, you know, the equivalent, maybe, with
4092980	4099260	disambiguated. But yeah, we, it didn't seem that we have the
4099260	4103020	right tools for that. And then it remained like an objective. And
4103020	4112160	then in 2014, we discovered the power of attention. And that's
4112180	4115780	closely connected to abstraction, because what it does is
4115780	4119140	it focuses on a few things. And of course, that's our, you
4119140	4123060	know, that's very much a characteristic of how we think a
4123060	4126620	thought has very few elements in it. That means we have selected
4126620	4129300	those elements. And that's where attention comes in. So it's
4129300	4135580	getting closer to this ideal of building machines that think like
4135580	4140060	humans. And then of course, in 2017, I wrote this consciousness
4140060	4142740	prior paper where, you know, I discovered all the work on global
4142740	4146100	workspace theory and, and it, you know, and the momentum is
4146100	4152620	built up. And of course, now, you know, humans think and they
4152620	4157460	use symbols, and they understand the very abstract
4157460	4160420	relationships between them. And we need to build neural nets that
4160420	4166700	can do that. So I guess where I've maybe departed from Gary,
4166700	4171180	but maybe he's moved to is, it's going to be neural nets that
4171180	4173620	do it, right? It's just that we're going to be training them in
4173660	4176420	a special way. And that's what G flow nets really aiming at.
4177660	4180820	So can I just say we, we asked many guests, these, these
4180820	4183860	questions about their, their evolution. And sometimes they,
4184220	4187620	they tend to be spicier than others. But I have to say, from my
4187620	4192060	perspective, your answer was the most informative, the most
4192060	4196860	gracious and the most noble of answers that we've heard so far
4196860	4200780	to similar questions. So kudos to you. That was awesome.
4200860	4201300	Thanks.
4202500	4205700	I just cannot believe it. And we always do a hell of a lot of
4205700	4208540	preparation. But it's gone to the point now where we know that
4208540	4211540	we're not going to get more than about six questions in. So we,
4211780	4214380	you know, we kind of like exponentially, you know, have an
4214380	4216060	exponential prior on our questions.
4216060	4219020	Well, he was awesome, though, with like, you know, we asked him
4219140	4223100	to give relatively sort of three minute answers. And he stuck to
4223100	4226020	that, which was really cool. I mean, that's, that's very
4226020	4229460	helpful to have an interesting dialogue. And I, I can't believe
4229500	4232820	how proud I am, you know, that he's, that he appreciates that we
4232820	4236340	put the prep time into it. And, you know, had had decent
4236340	4239700	questions that were hopefully interesting for him, as well as
4239700	4244460	our, as well as our audience. So Dr. Kilcher, lightspeed
4244460	4246100	Kilcher, what should I take?
4246820	4251700	It's cool is, I mean, his, um, yeah, I think is the thing he
4251700	4254700	mentioned at the end, like his humility, it kind of shines
4254700	4258740	through everything he does. And he answers, he's like, you know,
4258780	4263380	here's the best answer I can give. But, you know, he seems to
4263380	4270740	be very, like, open and not, not, not very, yeah, one notices
4270740	4274660	he's not on Twitter. It's like, it's noticed that was a
4274660	4278460	brilliant question. I think we should post that question on our
4278460	4281860	Twitter. Because, you know, that there's that a bit for people
4281860	4283700	watching this in a year's time, it's probably forgotten about
4283700	4287260	but yeah, that ilia guy from open AI said that the models might
4287260	4291180	be slightly conscious. I was exasperated by that. Because I
4291180	4294500	watched his interview on Lex. And I know by saying bad things
4294500	4296340	about him, he will never come on our podcast, but I don't think
4296340	4299020	he would have done anyway. So it doesn't matter. But yeah, I
4299020	4301700	think that it's pretty bad.
4302380	4307380	What? Why? Yeah, why? It's like, it's like, you don't think
4307380	4312060	it's bad? No, he says, I think, because a lot of the folks at
4312060	4315620	Open AI, they are, you know, like in the rationalist community,
4315820	4320420	and they seriously believe that we're an imminent threat of the AI
4320660	4323940	taking over the world and us being paper clips. And I think
4323940	4327420	it's next, I listened to his interview on Lex, and he sounded
4327420	4330420	like a salesman, talking about Codex and how it was going to
4330420	4333420	revolutionize everything. And I honestly think that there's just
4333420	4337900	such a divergence between what they're saying and reality right
4337900	4338140	now.
4339300	4343740	Well, not to drift too far away from from our guests today.
4344100	4348260	But so I thought, I thought it was just kind of a shower
4348260	4352380	thought, you know, like, you know, the the large neural
4352380	4355500	networks of today might be a little bit conscious, right?
4355500	4359060	And, and, and you just like, yeah, well, yeah, well, shower
4359060	4361460	thought, and it is a shower thought like it needs on
4361460	4365500	Twitter, it's just something you tweet out. And, and it brings up
4365500	4368100	interesting questions, like it brings up interesting questions,
4368100	4371100	like, you know, you're a you're a ball of neurons, like you're
4371140	4373740	just a slap together piece of matter, right? You have
4373740	4378340	consciousness. So clearly, like something about, you know,
4378540	4381900	learning systems combined with data, or maybe not even
4381900	4386100	combined with data gives rise to consciousness. So why can't
4386500	4392540	why can't another, you know, in silico, slap together system
4392540	4397660	of neurons ingested with data be slightly conscious or have
4397700	4401820	like, some properties, like, and that's that's essentially, yeah,
4402620	4407780	Benjo refused to give like a humble, the humble person he is,
4407780	4411820	he refused to give like, you know, the the the strong take on
4411820	4416620	that, but that would have because he might just this is my
4416620	4420700	opinion, not his obviously, but reading the consciousness prior
4420700	4425980	paper, it is not too far off. He formulates consciousness as
4425980	4430140	having these elements of, you know, I have my internal state,
4430140	4434860	which is sort of everything in my brain that I could bring bring
4434860	4438860	up into my forefront, then I get some input from the outside
4438860	4443660	world. And through the input, I then filter, like with an
4443660	4449420	attention mechanism, I do I look what in my mind, could I now
4449420	4454060	bring into focus, right? And that is by use of something like
4454060	4459620	an attention mechanism. And then I take that thing. And I put it
4459620	4466540	into these abstract concepts I use I represent. I represent the
4466540	4470980	concepts in my head as a sparse factor graph. And by focusing on
4470980	4474380	parts of that, I can then make inferences in this sparse
4474380	4478260	factor graph and so on. Now, obviously, something like GPT three
4478260	4482820	doesn't have all of that, at least not explicitly, but some of
4482820	4486780	it is there, right? It's, you know, I have a piece of input, I
4486780	4490660	have giant amount of weights, I use an attention mechanism to
4490660	4492700	sort of see what I can focus on.
4493660	4496140	Yeah, but yeah, but I think that I think that's a very
4496140	4500180	declarative description of consciousness. And at its roots,
4500180	4503660	it's about the phenomenological experience. Right. And I know
4503660	4507420	we discussed computationalism and panpsychism. Let's not go down
4507420	4511660	that rabbit hole. But surely, they don't think that this model can
4511700	4513220	feel well, but so this is
4513220	4517060	consciousness is not about feeling. It's about being being
4517060	4522820	like aware of of like, I don't even know what it is. I'm just
4522820	4527780	saying that it sounded not too far away from what the
4527780	4531700	consciousness prior paper was about. And yes, I realize it's
4531700	4534940	called the consciousness prior and not consciousness. But you
4534940	4535180	know,
4535980	4538300	yeah, I mean, I think he answered it the way a scientist
4538380	4541180	should answer it. And I was really happy with his answer,
4541180	4547500	which is, okay, a consciousness has to be some activity of
4547500	4550300	neurons and firings or whatever in the brain or else we're
4550300	4554300	talking about magic. And that's not in the field of science. And
4554300	4557200	B, you know, whatever that thing is, it's obviously quite
4557200	4560940	nuanced and complicated. And we don't have we don't know yet.
4560980	4565100	So we need to have some humility here, which means we
4565100	4568140	shouldn't be alarmist. So we don't need to be going in, you
4568980	4573020	know, burning books tomorrow because because we created a, you
4573020	4576500	know, GPT, whatever, that anytime its wheel is spinning,
4576500	4578860	and it's actually suffering. You know, if you ask it a
4578860	4581060	question that's too hard, and it's spinning, it's because
4581060	4583660	you're hurting it and it's suffering. And so we need to
4583660	4586340	turn it off like right away. But wait, we can't turn it off
4586340	4589300	because then we'd be like, murdering, you know, a sentient
4589300	4593460	being or something, like we're way, way too, in our infantile
4593500	4597960	understanding of, you know, this type of complex, complex
4597960	4601180	behavior, that's the human mind and consciousness to be at that
4601180	4605660	point. So from my perspective, he answered it completely 100%
4605660	4608860	scientifically. And there's a lot of folks out there who are
4608860	4612140	supposed to be scientists that spend a lot of time with, you
4612140	4617220	know, unscientific, you know, thinking about it. Cool. Let's
4617220	4620420	talk a little bit. What one of the things that I really found
4620420	4623940	interesting about Benjo's ideas, other than the causality
4623940	4627780	stuff and the system to stuff is this notion of diversity.
4628380	4630900	We've had conversations with Kenneth Stanley all about open
4630900	4633580	endedness and diversity preservation. We've also had
4633580	4638340	conversations with Friston about the importance of balancing
4638340	4642060	relative entropy and so on. And we have all of these curses in
4642060	4644700	empirical learning, right? The statistical curses, the
4644700	4645900	approximation curses.
4646740	4650340	Dimensionality, we have to mention dimensionality and
4650340	4652980	even, I mean, you know, we're talking about curses in the
4652980	4655740	Monty, you know, the Markov chain Monte Carlo in the sense of
4655740	4658660	it being a high dimensional space. And we need to assume that
4658660	4662700	there's some structure around where these modes are. So all of
4662700	4665900	these approaches are ways of simultaneously, and, you know,
4665900	4668780	being able to explore but not being cursed. So yeah, what was
4668780	4669300	your take on that?
4670260	4677020	Well, any one of my take was that I like that he's so
4677060	4681180	interested in abstraction, because, you know, to me, that's
4681180	4684140	been not only one, you know, it's not only one of the larger
4684180	4687580	mysteries, at least for me, I mean, I don't know, of the kind of
4687580	4692500	the universe is abstraction, idealism, you know, platonic
4692500	4695100	thinking, whatever. I mean, the whole point is just that he
4695100	4700060	views abstraction as a key to pragmatically useful, you know,
4700060	4704860	pass forward. And it's a hard problem, a really hard problem.
4704940	4709180	And, you know, his focus right now is on kind of graph based
4709620	4712540	structures. And I have to admit, you know, for me to you,
4712540	4715060	they're quite seductive and appealing. I don't know if
4715060	4717980	they're the the right path forward, but it's definitely
4717980	4721740	cool to see a lot of research, looking into graph based, you
4721740	4724940	know, methods, or, you know, hyper graph based methods,
4724940	4728620	whatever they are, they seem to definitely be a promising
4728620	4732180	path forward. And I think we're in for, hopefully, if we can
4732180	4735380	continue to progress at a reasonable rate, you know, some
4735380	4737260	some interesting decades ahead.
4738740	4745340	I mean, I would, I would also postulate that maybe our most of
4745340	4748820	our, let's say benchmarks that we're thinking about today aren't
4748860	4754740	necessarily suited to to because his argument was by creating
4754740	4759700	abstractions, it might actually, you know, help your ability to
4759740	4762740	learn something, right, which is a thing that we all
4762740	4765340	intuitively understand in the world, if I have good
4765340	4768100	abstractions, I can transfer my knowledge from here to here and
4768100	4771020	from here to here. Yeah, in something like image net
4771020	4774300	classification, or whatnot, or most of the benchmarks we have
4774300	4778980	today, the necessity of abstractions is probably not like
4778980	4782420	the data, the hardness of the problem probably doesn't
4782420	4786980	require abstractions to be introduced. And therefore, the
4786980	4790460	limiting factor here might not only be the models themselves,
4790460	4797140	but also, let's say, our ability to even measure the progress
4797140	4800460	one could make with abstractions. And I think that's gonna change
4800460	4803620	maybe in the near future, because people are going into
4803620	4808300	multimodality research and so on. And there, I think the concept
4808300	4812860	of sort of concepts, maybe not abstractions, but at least
4812860	4815980	something like concepts is way more, more important.
4817020	4819540	Yeah, there's, let me just follow real quickly there, Tim,
4819540	4822380	because there's something very interesting there to Yannick,
4822380	4827100	which is the lack of good tools to deal with multimodal, you
4827100	4830260	know, sets of data results. And a lot of times, we're just
4830260	4833820	throwing out kind of valuable, valuable sources of data, just
4833820	4836820	because, you know, we don't have a good tool sets to do with
4836820	4839220	them, like think about the self driving car, like the whole,
4839620	4843140	should it be vision versus LiDAR debate? Why? Why isn't it
4843140	4846820	both? I mean, you know, if you can for $5, you can throw on
4846820	4850620	some cheap, you know, LiDAR sensors or something, maybe not
4850620	4852940	something fancy, but something cheap, why wouldn't we take
4852940	4856860	advantage of that data? And it's, it's really because we don't
4856860	4860700	have good tools to deal with, with multimodal data.
4861340	4864620	We got to a good point in the discussion where we were talking
4864620	4868500	about the nature of finding abstractions. And I wonder where
4868500	4870980	the neural networks can find abstractions. Now, the, the
4871020	4874940	cynical view is that humans kind of create these inductive
4874940	4877300	priors, and they represent the abstraction. So certainly in the
4877300	4879900	case of geometric deep learning, and that's kind of what's
4879900	4883220	happening, we put the, the priors in there to reduce the size of
4883220	4885780	the approximation space. And Keith and I had an interesting
4885780	4888140	idea yesterday that there's a kind of analogy between geometric
4888140	4890620	deep learning and causal representation learning. So I
4890620	4892900	think Keith, you went online and you found a really interesting
4892900	4895820	definition of a causal model, which is that it's kind of
4895820	4900100	immune to, let's say, adversarial examples. So what a model
4900140	4903100	does right now, is it learns a relationship essentially between
4903100	4906220	let's say every single pixel and something happening, right,
4906260	4909060	which is why that model is vulnerable.
4909580	4912620	Yeah, so that was the, and yeah, and I would love to get your
4912620	4916020	comment on that. But that was, you know, this paper, and I
4916020	4918420	could go dig it up, and I can get the reference right now where
4918420	4922260	it said, you know, hey, what is the difference between a causal
4922580	4926460	or prediction from a causal model versus a prediction from a
4926460	4930940	non causal model. And the point was that, well, almost by
4930940	4935740	definition, really, if you have a causal model, then if you
4935740	4938460	perturbed the inputs, the prediction that you get out of
4938460	4942340	it remains a valid, a valid output, because after all, if
4942340	4945060	it's a causal model, and it's reflective of a sort of the
4945060	4948140	causal structure of the world or whatnot, then sure, that's a
4948140	4952060	valid, valid output. Whereas, if it's non causal, it has the
4952060	4955580	potential to learn all these kind of spurious, spurious
4955580	4957580	structures, and therefore, that's why you get the
4957580	4960940	capability of these adversarial examples where you just, you
4960940	4964540	know, put a little rainbow pixel somewhere, and it messes up the
4964540	4967140	class because it had this spurious connection.
4967580	4971020	I mean, in the same vein, you could also, the adversarial
4971020	4974380	examples are there because of inaccuracies, because we don't
4974380	4977740	have the perfect discriminative function, right? I could also
4977740	4981500	say, well, if I just had the correct discriminative functions,
4981500	4984860	it doesn't need to be causal. If I just had like the right
4984860	4989780	partitioning of my input space, then, you know, I'm super not
4989780	4993780	vulnerable to adversarial attacks. I guess the real question
4993780	4999260	would be, would that technically amount to a causal model if I
4999260	5002940	had, you know, the perfect partitioning of the input
5002940	5007140	space into my classes? I don't know, that's like, is there like
5007140	5010340	a mathematical equivalent from that to a causal model? Who
5010340	5010820	knows?
5011180	5015300	Right. Yeah, I think there's probably, it's probably, certainly,
5015740	5018740	if you have the perfect discriminative function, it's
5018740	5021940	probably the discriminative function that you would derive
5021940	5026140	from a causal model. I'm not 100% sure you can go go in the
5026140	5028780	reverse, because I imagine there probably is some some
5028780	5033620	information loss going from, you know, a causal model to, you
5033620	5037780	know, like, for example, I'll give you an example. In the cases
5037820	5040660	of, say, production systems, you know, so, so these little
5041140	5045740	rewriting rules or whatever, the definition there of a causal
5046100	5051820	system is one in which all the potential graphs, all the
5051820	5054780	potential transition graphs that you can get to a particular
5054780	5058980	output are isomorphic. So even though you have you can have
5059340	5062980	like the perfect discriminative kind of function, there may be
5062980	5066140	multiple possible graphs that you could have gotten there, but
5066140	5069460	they're isomorphic. So I'm not quite sure, you know, how that
5069460	5073740	would translate into this, this, this point. But I think you'd
5073740	5077100	be just as good for the purpose of discriminating.
5078740	5081180	I think it's related to the semantics discussion we're
5081180	5085140	having in NLP. So people like Walid Saber say that neural
5085140	5088180	networks don't have semantics. And in the same way, as I was
5088180	5091340	just saying, blue pixels, I mean, in the real world, let's say
5091340	5095820	male testosterone levels is causally linked to incidents of
5095860	5099500	car crashes, which means you can now take the model in in
5099500	5102100	Holland in a different country. And because it's a causal
5102100	5105380	factor, it will extrapolate in the same way. But neural
5105380	5109140	networks models, because what a human does is we would come up
5109140	5112420	with the right representational abstraction, we would build a
5112420	5115660	model, which is very reductionist, a neural network models
5115660	5118980	everything to everything. And the semantics are all one thing.
5119380	5124220	Well, okay, I don't, I'm not, I'm not too, too keen on
5124580	5128740	discussing like semantics and whatnot with with NLP people.
5128900	5138860	But I don't know, you know, like, like, I don't know, it
5138860	5144060	often it often veers away and veers into semantics. It's like
5144060	5148860	it's a bit too, you know, I like what what I think Conor Conor
5148860	5153060	Lay, he said, like, when we talked him along. Oh, he wouldn't
5153300	5156260	like that. I talked him a long time ago. And I happen to agree
5156260	5159220	with him there is that you sort of have to see everything from
5159220	5163700	the perspective of these models. Like if I'm a GPT three, my
5163700	5167580	entire world is text input, right? And people can't somehow
5167580	5172260	judge GPT three by, well, you don't even have whatever a
5172260	5174860	connection to the real world, you don't even know that you don't
5174860	5179340	go see a doctor if your plant is sick, right? Like, how can you
5179340	5181940	not know that? Like, okay, they don't live in the real world,
5181940	5185820	they live in the text world of the internet. And in that world,
5186060	5191140	I'm not sure if there is not a level of abstraction happening
5191260	5199300	in these models. Like, it's, it's, it's, um, yeah, I'm, I don't
5199300	5203860	want to, I don't want to claim that these things do not form
5204140	5208180	abstract things, it might not be the same abstract classes that
5208180	5212780	we form, but they definitely form some level of abstraction.
5213100	5216100	And of course, they can't transfer it because we only give
5216100	5219660	them the one modality, right? But they may be able to transfer
5219660	5225020	it between, you know, different areas of text, which they
5225140	5230500	sometimes do, right? And yeah, so that's, I just wouldn't be so
5230540	5233260	conclusive with respect to these things.
5233300	5237020	That that's true. I think we've gone full circle now. So after
5237060	5241740	speaking with Randall Ballastriro about the splines, that almost
5242180	5245100	results in such a cynical reading of MLPs that they're just
5245100	5247980	hash tables, but we're not using MLPs, we're using
5247980	5251140	transformers and we're using CNNs. And actually, if you think
5251140	5254740	of abstraction, just as being extrapolation, I think they are
5254740	5257980	basically synonymous, it's about being able to extrapolate
5257980	5261820	outside of your training set. Then those inductive priors are
5261820	5266460	indeed producing abstractions. But the problem is humans
5266460	5269780	design those inductive priors. What we want is to learn
5269780	5272020	abstractions. And that's the thing that I don't think is
5272020	5277820	happening. I'm kind of I'm kind of on the same page as Yannick
5277820	5282700	and Connor on the one hand, which is, hey, if an abstraction is
5282700	5288220	just a compression, you know, encoding of the input space, then
5288220	5290500	of course, they're learning abstractions, right? I mean,
5290500	5293740	they are throwing away, you know, information and retaining
5293740	5297220	some, some abstract thing. I think, but I think that just kind
5297220	5303580	of devolves into somewhat like, you know, bastardization, if you
5303580	5307140	will, of what people mean when they say abstractions, because
5307500	5311140	the types of abstractions that traditionally we think about as
5311140	5314740	abstractions are simplifications. You know, they're, they're
5314740	5319780	like, simplifications of more general longer range kind of
5319780	5322980	structures. Whereas we know, and I think we all know this for
5322980	5327740	sure, that a lot of the quote unquote abstractions that did a
5327780	5332340	neural network learns are these kind of like shortcuts, right?
5332340	5336540	They're like these low level borderline spurious kinds of
5336580	5339420	abstractions. And that's why they break so easy. That's why
5339420	5342340	they're so brittle. And I mean, there is this vagueness here,
5342340	5345020	right? Like when is an abstraction, a good abstraction,
5345020	5347660	I don't know. But I think it all kind of in a way misses the
5347660	5352660	point. Like, what we're talking about here is that, and this is a
5352660	5356580	lot of what Benjio said, right, which is that the goal here is
5356580	5361820	to figure out how to get machine learning to learn
5362380	5368460	structures that by virtue of their simplification, their simple
5368460	5373220	abstractions are more generalizable out of distribution.
5373980	5377540	Right, like that's, that's really kind of the goal here. And I
5377540	5381420	mean, so the rest of it is just semantics, pun intended. I mean,
5381420	5381740	the
5384020	5389900	if you look across the world, a lot of, let's say, cultures and
5389900	5395980	humans and so on must have the same abstractions, right? So it
5396100	5400060	must mean a little bit that it's not just something you learn
5400060	5404700	during your lifetime, right? So, right? Oh, absolutely.
5404700	5408180	Not correct. It's it's learned by the it's learned by evolution,
5408220	5410540	by the species, by, by life itself.
5410580	5416140	Exactly, right. But but is like the, the analogy to us building
5416140	5420380	in the correct ones as a shortcut for just evolution doing it
5420380	5425460	using essentially random search, right? That is, it might, right,
5425460	5428980	it's, it's a different, it's a different quality of we want
5429020	5432940	machines to learn something. Because usually we think of when
5432940	5435500	we say we want machines to learn something is we want him to
5435500	5441060	ingest data akin to maybe what a human does during its lifetime.
5441540	5445540	But the when you know, these sort of abstractions and the
5445540	5448780	ability to form abstractions, they seem to be happening on a
5448780	5451020	more fundamental shared level.
5451300	5455140	Yes, you just put the pin in the center of the bullseye. I think
5455140	5458060	that's exactly right. You know, there's a lot to be said for
5458100	5461900	me. It's an epiphenomenon. And a lot of intelligence is
5461900	5466060	embodied. And I agree that there's an awful lot of stuff going
5466060	5470540	on and unbeknown to us with this clearly something that most
5470540	5474420	people don't have a grasp on. Maybe this is why at the
5474420	5477820	population level, maybe this is why I'm frequently miscommunicating
5477820	5481060	with people because I never assumed that learning was about,
5481540	5484300	you know, what a human being learns and a human being's
5484340	5488140	lifetime. Like it's, to me, it's always been the evolution,
5488740	5491740	you know, paradigm, it's like what's encoded in your neurons,
5491740	5495500	what's encoded in your DNA, you know, what was learned by
5495500	5498940	bacteria a long time ago, and how did that translate into what
5498940	5502460	human beings are doing. So I don't know why, like, why so
5502460	5506140	many people are focused on what a human being learns in their
5506140	5509380	lifetime. I mean, it's more, you know, why is that the goal?
5509420	5510100	I'm not sure.
5510300	5513340	I know, but we run the risk of being very reductionist because
5513380	5517060	Connolly, he said that it's an open question where the humans
5517060	5521100	are even intelligent. And if you go down that line, very
5521100	5524060	quickly, you start saying, oh, human beings are just hash
5524060	5527660	tables like GBT three, clearly humans are intelligent in some
5527660	5527940	way.
5528020	5530420	Well, you can just take it as a, you know, matter of
5530420	5533780	definition, but it's not a binary thing. Like again, why are
5533780	5536620	we always into this black and white concept, something is or
5536620	5539980	is not intelligent? Like that's not how I view things. I think
5539980	5544780	there's a spectrum of intelligence from like zero to, I
5544780	5547740	don't know, maybe infinity or something, some really large
5547740	5551540	number beyond what what human beings are. And so it's this
5551540	5555300	continuum. So that's why I like chelets kind of on the measure
5555300	5559420	of intelligence, because even though it doesn't actually give
5559420	5563860	us a, you know, quantitative way yet to measure intelligence, it
5563860	5566580	at least is thinking along the right directions, which is how
5566620	5571180	do you measure intelligence? And how do you define it as a
5571180	5576460	category of activity? And then we can kind of get past this
5576460	5578220	black and white, you know, thinking.
5580540	5581500	Well, gentlemen,
5582060	5585260	always a pleasure. Absolutely. Yeah, absolutely.
