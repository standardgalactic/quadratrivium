start	end	text
0	5720	Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington
5720	11240	and machine learning researcher, probably best known as the author of the master algorithm,
11240	14480	popular science introduction to machine learning.
14480	20680	I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.
20680	26020	The two that I found most interesting and are closest to my own research are Neurosymbolic
26020	28520	AI and symmetry-based learning.
28520	33440	Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.
33440	38840	Pedro is a professor at the University of Washington and give us a quick introduction
38840	44880	to yourself, to your experience here in Europe so far and what's top of mind for you?
44880	48680	I'm a machine learning researcher, I've worked in most of the major areas.
48680	54160	I've also written a popular science book on machine learning called the Master Algorithm.
54160	59200	I'm having a lot of fun here at NeurIPS, listening to various talks like David Chalmers
59200	65320	on Consciousness and Geoff Hinton on Sleep and looking forward to the rest of it.
65320	70480	Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first
70480	76000	thing I wanted to talk about just because it's top of mind is this whole galactica situation.
76000	80360	So first of all, I was speaking with Ian the other day and I think it's a little bit unfair
80360	83720	that Meta really bear the brunt of this.
83720	89440	OpenAI have just released this new chat GPT bot which suffers from similar failure modes
89440	94240	and it just kind of feels that they're not getting anywhere near as much stick as Meta is.
94240	102040	Well I think, I agree with you, I think the brouhaha about galactica is way overblown.
102040	104480	That system is really largely harmless.
104480	108400	It's just another large language model that's designed for actually something that to me
108400	110960	as a scientist is very interesting.
110960	115560	I would love to have a system like that to help me out with certain things and I think
115560	120920	it's a step in the right direction and I think the brouhaha however is an instance of people
120920	126280	jumping the gun on a lot of these AI things in a way that to me is very excessive.
126280	131400	Having said that in a way they set themselves up for it in a way that they need and have,
131400	137560	they kind of over claimed what it did and the problem with these LLMs is that they generate
137560	142560	a lot of stuff that looks good but can be completely wrong and in a way there's no worse
142560	145600	place to do that than in writing scientific articles.
145600	150320	So when they came out with it, they should have been more careful about how they frame
150320	151320	it.
151320	154520	I think they took concerns sort of like this competition and one upping each other on who
154520	159800	comes up with the frilliest demo and that kind of backfired.
159800	165200	So they shouldn't have had to withdraw it.
165200	170840	I think that's all pathetic and hopefully they've learned the lesson that next time
170840	172520	they will do it slightly differently.
172520	173520	Yes.
173520	174520	Okay.
174520	175520	Okay.
175520	179880	Well, so Gary Marcus has been very loud about this on Twitter so he's really pushing
179880	185160	the point about misinformation and the thing is as well I don't want to characterize the
185160	189160	ethics folks as having monolithic views because they don't have monolithic views and I also
189200	192880	think that a lot of the ethics guidelines for large language models are very reasonable.
192880	196840	Like I interviewed the CEO of Coheir the other week, Aidan Gomez.
196840	200800	I went through their terms and conditions and policies all very reasonable.
200800	203360	The only sticking point for me is the misinformation one.
203360	211720	I think the kind of the moral valence of it is in its use and especially with misrepresentation.
211720	215360	I don't like this paternalism telling me what's good for me.
215360	218160	I've just lost out on using a really cool tool basically.
218840	219880	I completely agree with you.
219880	221120	I would take it even further.
221120	226400	I do not want other people deciding for me what is misinformation and what is therefore
226400	229200	allowed to be said because it's misinformation or not.
229200	230200	For a couple of reasons.
230200	234040	One is that these people who claim to be big critics of misinformation, a lot of them
234040	236680	are misinformers themselves.
236680	242120	And the bottom line is that you always have your ideology that informs what you think
242120	244000	is true and false.
244000	250520	And I don't want anybody, every one of us in a democracy should be deciding for themselves
250520	253800	what is true and what is false and what is valid and what isn't.
253800	260320	And I have no fear of attempts to misinform me as long as I have a multiplicity of sources.
260320	266360	The biggest misinformation danger is when you have only one monolithic source of truth,
266360	270760	whatever it is, which is unfortunately what a lot of these anti-misinformation people,
270760	273720	I think consciously or unconsciously want.
273720	276800	Give me 10 things, 9 of which are misinformation.
276800	279960	I can do the job of figuring out which one I think is valid.
279960	283880	Give me only one of those things and chances are 9 in 10 that it is misinformation and
283880	285640	then I have no chance to overcome it.
285640	289880	So this whole attack on things because they're misinformation.
289880	295120	And I mean, I understand the impulse that like, why have all this falsehood flying around?
295120	299080	But the way to overcome that falsehood is not by censoring it.
299080	300880	You should know this, right?
300880	305280	You should be having to refight all of this over again in the context of social media
305280	309200	and large language models and so on.
309200	313000	So you said something really interesting, which is that this notion of a pure truth
313000	319560	or a monolithic truth, and there's this concept of epistemic subjectivity, right?
319560	325240	Or things observe a relative, even complex phenomena like intelligence.
325240	326480	No one understands what it is.
326480	328440	You can't reduce it to one particular thing.
328440	329960	People have different views on it, right?
329960	335440	So this notion that there is a pure monolithic truth of the world, I think is horrifying.
335440	339880	Well, I would put it slightly differently.
339880	344000	So first of all, there's a question, is there one reality or not, right?
344000	347080	Is there truth or is there my truth and your truth, right?
347080	351520	And actually, I understand the impulse to talk about my truth and your truth, but I
351520	354080	think as a...
354080	355200	So what is really true?
355200	356360	We don't know.
356360	357760	But as a...
357760	363880	I think the most useful, including socially useful working hypothesis is that there is
363880	368680	a single reality and the single truth, but it's extremely complex.
368680	371440	So no single one of us can get at it.
371440	375280	So what we need is many different people coming at it from different angles.
375280	378960	But with the premise that we need to try to make these things consistent.
378960	383640	So just saying, oh, we have different truths and there's no reality, that is actually very
383640	388080	counterproductive because it gives everybody a pass to just believe whatever wacky thing
388080	389080	they want.
389080	392840	And then the consequences of that when you have to make the real decisions are very bad.
392840	396640	At the same time, I agree with you.
396640	401080	If I think that I have access to that truth and everybody just needs to, you know, count
401080	402680	out to it, that is very dangerous.
402680	406960	So I think we need to entertain these two ideas that there is a truth, but it's very
406960	409680	complex and no one has a monopoly on it.
409680	414360	And the key is, you know, like objective truth is what different observers can agree on.
414360	416600	And now we can figure out what it is that we agree on.
416600	419280	And that way we make progress in understanding reality.
419280	423800	And we also tend to make more of the right decisions because we're closer to the truth.
423800	424800	Okay.
424800	429600	But do you see it as, I mean, I think the reality thing is interesting, but do you just see
429600	432280	it cynically as gatekeeping?
432280	434520	As in having a clerical class control?
434520	435520	Oh, absolutely.
435520	436520	No, absolutely.
436880	442960	That's precisely the danger that I was referring to is that if you, let me put it this way.
442960	447920	If ever there is, and this is a commonly mooted proposal, right, and not even proposed like,
447920	453280	are we going to have a truth commission of people who decide what is true on whatever,
453280	455160	Twitter or something, right?
455160	460760	That is a really alarming thing because there is no commission that can do that.
460760	464120	What they're going to do is they're going to impose their version of reality on everybody
464120	467640	else, which unfortunately is what a lot of these people want to do.
467640	470600	They convince that they have the truth and they want to impose it on the rest of us.
470600	472040	And that is really alarming.
472040	475200	We know historically what happens when people succeed in doing that.
475200	476200	Yes.
476200	477200	Yes.
477200	480480	But I suppose my point with the gatekeeping is it almost gets you to the actual truth
480480	482120	of the matter is irrelevant.
482120	483760	It's actually about power.
483760	487920	But what's your take on, I don't know whether you think this is putting it too strongly,
487920	493800	but this being a form of industrial kind of gaslighting, kind of, you know, in an Orwellian
493800	499000	sense, trying to shape people's reality through, you know, language, culture and interactions
499000	500520	on the internet.
500520	503600	I think a lot of it is deliberate.
503600	509720	Some of it is, I mean, I'm an optimist about human nature at the end of the day.
509720	512720	Maybe in, you know, maybe with justification, maybe without.
512720	518200	I think so there's this postmodern view that it's all about power and it's certainly partly
518200	519200	about power.
519200	524600	I think a lot of the people doing this, unfortunately, or maybe fortunately, they are, they're not
524600	526880	seeking power for its own sake.
526880	529760	They have a set of beliefs that they think is right.
529760	533120	And then the means, you know, they unjustify the means, right?
533120	534120	That's the problem.
534120	541480	So that gatekeeping, you know, and that gaslighting happen not because, not for their own sake,
541480	542880	they happen for the sake of a cause.
542880	547080	And now there's two problems with this is that these days the causes on behalf of which
547080	550440	this is being done, in my view, are largely wrong, right?
550440	552160	But whether they're right or wrong, right?
552160	555480	The problem is that this is just noxious in its own right.
555480	561840	And also then a lot of sort of like, again, personal desire for power and promotion and
561840	562840	prevailing over others.
562840	565080	Then of course, you know, hitches are right onto this.
565080	566080	Yeah, interesting.
566080	570000	I mean, we'll get into consequentialism in a minute because I think there's quite an
570000	571720	interesting journey we can go there.
571720	575560	But I wanted to cite Francois Chollet, I'm a big fan of him.
575560	579760	He just tweeted saying, I'm not too concerned of whether what I read is right or wrong.
579760	582040	I can figure that part out myself.
582040	586040	I'm interested in things that are useful, thought-provoking, novel.
586040	589200	Sometimes the most creative thinkers have a bias towards wrongness, but they're still
589200	590200	worth reading.
590200	591200	Would you agree with that?
591200	592680	Yes, I largely agree with that.
592680	599080	So as I was saying before, you know, I can tell for myself or I can do that exercise
599080	600840	of figuring out what is right and wrong.
600840	606000	The most important thing that I want is to not miss out on things that I don't want to
606000	607000	miss out on.
607000	611320	Like, you know, the known unknowns and the unknown unknowns, the biggest killer is the
611320	612320	unknown unknowns.
612320	617540	So if anybody trying to learn or understand something, they're for person or organization
617540	619040	or society, right?
619040	623800	If all they do is move the unknown unknowns to known unknowns, they've already gone an
623800	625840	enormous distance, right?
625840	630640	And so I appreciate people who I disagree with, first of all, because that's how you sharpen
630640	631640	ideas.
631640	632640	Yeah.
632640	638280	But also because they may just bring things to my attention that if we were all conforming
638280	642520	to the more majority view, would not come to our attention.
642520	645000	And then those more often than not are the ones that kill you.
645000	646000	Yeah, that's so interesting.
646000	649760	I mean, there's a real analogy here, even this might be tenuous, but between symbolism
649760	654840	and connectionism or, you know, Rich Sutton said we shouldn't be hand-crafting our AI
654840	655840	systems.
655840	657000	We should kind of let them emerge.
657000	660040	And it's a similar thing with our moral framework, but you're kind of saying that it should be
660040	665200	emergent from low level complexity and diversity and interestingness.
665200	668280	And there is another school of thought, which is that we should be top down and we already
668280	669440	have a representation.
669440	673600	I actually think it should be, it needs to be a combination.
673600	675320	We need to have both.
675320	678560	This is one of those debates that in some sense puzzles me because to me the obvious
678560	680880	answer is that we need both.
680880	683160	And then if you read the master argument, this is what I do.
683160	686720	I look at the different paradigms of machine learning and I don't come out in favor of
686720	690480	any of them because I actually think we need ideas from all of them.
690480	692840	And then we need to combine them into something coherent.
692840	696960	And if you look at psychology, like your brain does bottom up and top down processing.
696960	700400	And if it only did one of them, either one, it wouldn't work.
700400	703880	And I think as we try to build a larger intelligence, it's the same thing.
703880	708160	We definitely need the bottom up part and, you know, by volume, the bottom up part is
708160	709160	going to be bigger.
709160	712960	So if you could only have one, that would probably be, you know, the choice.
712960	716200	But the top down part is also very important.
716240	720440	If you go all the way back, the top down part probably started this bottom up and got synthesized
720440	721440	and improved.
721440	722880	But now we need that loop.
722880	724320	The loop is actually very important.
724320	725320	Well, that's interesting.
725320	729600	So in your book, I guess I want to sketch out different types of AI architecture.
729600	732840	So, you know, you get universalists to this kind of deep mind idea that a very simple
732840	735000	underlying algorithm could produce everything.
735000	738000	And then you get, you know, hybrid folks on the other side of the spectrum.
738000	740000	And then there's an integrated approach in the middle.
740000	742680	Like, where would you kind of place yourself on that continuum?
742680	748480	I would place myself very much in the frame of mind, well, let me put it this way.
748480	751320	I don't know, but which is nobody does, right?
751320	754640	If somebody tells you that they know how we're going to get to intelligence, you should be
754640	756200	suspicious right away.
756200	760920	But what do I think is the most promising approach and the one that ideally would be
760920	762600	the best one if we can pull it off?
762600	764520	It's there being a single algorithm.
764520	769240	So at that level, I very much sympathize with what is effectively the deep mind's agenda.
769240	775080	Now, we're at part with a lot of these people is that I don't think the algorithm that we
775080	779240	need is as simple as many of those people think it is.
779240	781760	And I don't think it exists.
781760	785840	It is probably the case that the algorithm that we really need at the end of the day
785840	789040	doesn't even look that much like any of the things that we have now.
789040	792480	So I think hopefully there is such an algorithm, but we're still far from it.
792480	793480	Interesting.
793480	797760	I mean, they would cite the example of evolution as being a very simple underlying algorithm.
797760	802040	Although Ken Stanley would say that people misunderstand evolution.
802040	803720	So I agree with them at that level.
803720	807920	In fact, in the master algorithm, I have a chapter where I go over the objections and
807920	811040	the reasons to believe that there is a master algorithm.
811040	815720	The majority of the people even in the field are skeptical of that notion, even though
815720	818640	I would claim that effectively that's what they're pursuing.
818640	821680	People like Rich Sutton and Jeff Hitton, I asked a bunch of people before I wrote the
821680	825000	book and they do believe in this idea of having a master algorithm.
825000	829920	A lot of people believe that, but intuitively a lot of people believe that no, there is
829920	831560	no such thing.
831560	840920	And I understand that intuition, but I don't think it's a well-founded intuition.
840920	841920	Let me put it that way.
841920	845520	But in a sense, we know there is such a thing, because look at cellular automata, look at
845520	847360	what we've already done with deep learning.
847360	851440	I think the context is, is there such a thing that will produce what we want?
852440	861720	To take your example or DeepMind's example of evolution, in the book I mentioned empirical
861720	865840	evidence that there is a master algorithm and exhibit one is evolution.
865840	868880	If you think of evolution as an algorithm, which by the way is a very old idea, I think
868880	875200	it was George Bull that said, God does not create animals and plants, he creates the
875200	878320	algorithm by which animals and plants come about.
878600	881720	He didn't use the word algorithm, but that's essentially what he said.
881720	883920	This I think is right on.
883920	886520	Another example is your brain.
886520	890480	If the algorithm doesn't have to be something as simple as backprop and you're a materialist
890480	894640	like most of the scientists are, your brain, if the master algorithm is an algorithm that
894640	898200	can learn anything you do, then your brain is that algorithm.
898200	902600	But then there's another one which is even more fundamental, but I think from the point
902600	907160	of view of this debate is very illuminating, which is the laws of physics.
907160	908680	Why stop at evolution?
908680	911480	The laws of physics are the master algorithm.
911480	913080	Evolution is very complicated.
913080	917800	In fact, what I think about evolution in AI currently is that evolution in reality is
917800	921760	much more complex than we give it credit for, which is why a lot of our current generic
921760	923320	algorithms don't work that well.
923320	926560	But the laws of physics at this level are much simpler.
926560	931480	If you think about it, from the laws of physics comes evolution and comes all the intelligence
931480	933480	that we have.
933480	937160	It's very intriguing why that happens and why the laws are such that that happens.
937160	940160	But even just the laws of physics are already a master algorithm.
940160	944680	Now what you could say, and many people immediately say is like, oh, but if you start from there,
944680	946200	you'll never get anywhere, right?
946200	952280	But then you can say evolution is the laws of physics sped up in a certain direction.
952280	955120	And then our reinforcement learning is like evolution.
955120	957920	People have pointed out the same way except it's faster.
957920	961680	And in a way what we're trying to do now in machine learning is the same thing yet again
961720	963920	except only even faster.
963920	965520	But what are the consequences of?
965520	968760	I mean, let's say it is actually a very high resolution algorithm.
968760	973600	So it's something that appears to be completely unintelligible to in respect of the output
973600	974480	phenomena.
974480	976240	Is that is that even a good place to be?
976240	980800	Because, you know, just like with cellular automata, there's no real paradigmatic
980800	983600	relationship between the underlying rules and the emergent phenomena, right?
983600	986400	So is that really even something we want?
986400	987520	No, I think there is.
987520	989600	So we don't know.
989600	993360	But I think people and this is very common among connections is to say this stuff is
993360	996520	also complex that we can't possibly have a handle on it.
996520	998000	We just have to let it happen.
998000	1001840	And I think that is not giving enough credit to our human brains, right?
1001840	1006320	We are incredibly good at making sense of things that in the beginning, I mean, over
1006320	1010280	and over and over in the history of science and technology, you start out with things
1010280	1012600	that you don't understand very well at all.
1012600	1017320	But then over time, we kind of change our representation of the world to make those
1017320	1019520	things actually be intelligible to us.
1019520	1023480	And we should not a priori assume that that's not going to be the case here.
1023480	1028800	So, for example, cellular automata, amazing things come out of whatever the game of life
1028800	1032840	that seemed completely disconnected from those, but they aren't, right?
1032840	1038160	And, you know, there's various depths at which you could go into this.
1038160	1042400	There will probably at the end of the day be some large element of this that we can't
1042400	1046800	figure out very well, but we can figure out enough that we have a handle on it.
1046800	1051400	So, this singularity notion that at some point AI is just completely beyond our understanding.
1051400	1053040	I tend not to buy.
1053040	1055600	I don't think it will be completely beyond our understanding.
1055600	1060320	But it's an analog back to our Twitter discussion, like, because we can only understand it through.
1060320	1063480	It's like having views on a mountain range, you know, the view looks different depending
1063480	1065080	on where you're standing.
1065080	1068760	And it's the same thing with the emergent phenomena in a cellular automata.
1068760	1069760	No, very good.
1069760	1074400	And, you know, the classic example of this is the blind man and the elephant, right?
1074400	1077920	And that's actually the metaphor that is in the book, as I say, you know, the different
1077920	1080640	tribes are like different blind men, right?
1080640	1083480	But precisely so, AI is one of the blind men.
1083480	1087800	I can see part of the elephant, but it'd be who's meant to also talk to you who see another
1087800	1088800	part of the elephant.
1088800	1092600	And then each of us understands a little bit more of the elephant than we would if we were
1092600	1093600	on our own.
1093600	1098800	But most importantly, we collectively, which is what really matters, actually understand
1098800	1103840	maybe not the elephant completely, but much more of the elephant than either any of us
1103960	1105560	would alone, right?
1105560	1108320	And it's certainly a lot better than just giving up and say, like, oh, we're never going to
1108320	1110560	understand this strange thing that's in front of us.
1110560	1111560	Interesting.
1111560	1113080	But that's a great argument to what you were saying before.
1113080	1115880	So it's beyond our cognitive horizon.
1115880	1118680	Therefore we need to have diversity of aspect.
1118680	1122360	There's a, yes, there's a question of whether it's beyond the cognitive ability of a single
1122360	1123360	human.
1123360	1124360	Yeah.
1124360	1126400	And then there's the question of whether it's beyond the cognitive ability of an entire
1126400	1127920	society of humans.
1127920	1131840	And obviously, there'll be things that are beyond the cognitive ability of a single human,
1131840	1133920	but not beyond the cognitive ability of a society.
1133920	1136280	Also these days, we have computers.
1136280	1138680	So our cognitive power is augmented by our machine.
1138680	1143320	So we can understand things or bring things to the point where we understand them to a
1143320	1145640	degree today that we couldn't a hundred years ago.
1145640	1147200	Right now, that is a fascinating point.
1147200	1153080	So it's beyond our cognitive horizon individually, but it might not be beyond the cognitive horizon
1153080	1157080	of loads and loads of humans on the internet, you know, the wisdom of crowds.
1157080	1161160	But we don't, I mean, how do we know that the crowd understands?
1161160	1166320	But we know, well, that's the, in some sense, the beauty of this, right, is that we never,
1166320	1168040	what is the crowd really understanding, right?
1168040	1171920	And again, once the crowd is augmented by machines, like machine learning algorithms,
1171920	1176800	right, we can ask what do we as a society equipped with all of our, you know, large
1176800	1180120	language models and so on and so forth, what do we really understand, right?
1180120	1184480	Now, at some level, you can't answer that question individually because you are just
1184480	1188120	an individual, but right, there's a couple of very important things that we shouldn't
1188120	1189120	forget.
1189240	1194080	You could, one thing you can do and that I do do is say, like, do I now actually even
1194080	1199320	just individually understand things better than I did before when it was just me looking
1199320	1200320	at it?
1200320	1202760	And the answer to that is almost invariably yes, right?
1202760	1205000	So there is a big game to be heard there.
1205000	1211440	And the second one is that you, ultimately, you tell by the consequences, right?
1211440	1214120	And like, for example, take a deep network, right?
1214120	1217920	And you may not know how it works, but if it's doing medical diagnosis, you can tell
1217920	1222520	whether it, you know, gets the diagnosis right more often than it did before or more
1222520	1223520	often than another model.
1223520	1228920	So we as a society may not, you know, we individuals may not very understand very well what we
1228920	1232880	as a society understand, but we can see the consequences and at some level, that's the
1232880	1233880	point.
1233880	1234880	Yeah.
1234880	1237760	So on that, I mean, that sounds like a bit of an appeal to behaviorism and we're going
1237760	1239760	to talk about that in respect of charmers as well.
1239760	1243840	But it also brings us back to, you know, we were talking about empiricism versus rationalism
1243840	1246600	and nativism and all of these topics.
1246600	1251200	Would you place yourself in that camp of being a nativist and a rationalist or completely
1251200	1252200	the other way?
1252200	1253440	No, absolutely not.
1253440	1259040	Again, this is one of the points that I, you know, go back to is there are the empiricists
1259040	1263640	and there are the rationalists and you could see naively machine learning as being the
1263640	1268000	triumph of the empiricists, but it actually is not there are very fundamental reasons
1268000	1269000	why it's not.
1269000	1272720	And I really do think, and this is not just think there's this thing called the no free
1272720	1274560	lunch theorem, right?
1274560	1278840	And if you take those things seriously, the solution has to be a combination of empiricism
1278840	1279840	and rationalism.
1279840	1283840	I don't think either side alone has or even can have the whole answer.
1283840	1286200	So very much we need both of those.
1286200	1291160	And if you're a pure empiricist or a pure rationalist, I'm already suspicious of you.
1291160	1292160	Wonderful.
1292160	1296640	Coming back to what Franz Walsh said in his quote, he said, you know, producing things
1296640	1299360	that are thought provoking novel and all the rest of it.
1299360	1302360	And I was speaking to some alignment folks yesterday and we'll pivot to that in a minute.
1302960	1307160	The big thing for me after doing an episode on sales, the Chinese room is, you know, where
1307160	1311640	does intentionality come from and Chomsky talks about agency, for example, we do things
1311640	1314200	that are appropriate to the situation, but not caused by them.
1314200	1317280	So from my perspective, all these generative models, all these large language models and
1317280	1321120	so on, the creativity, the real spark of genius still comes from us, right?
1321120	1326680	We've just kind of like, you know, the boring bit of actually doing the task is now delegated
1326680	1328240	to the algorithm.
1328240	1329600	I would disagree with that.
1329600	1334360	I think you are, I mean, your position is very reasonable and actually, I would say
1334360	1339880	probably the most common, but I think when you do that, you are giving us too much credit
1339880	1342920	and the large language models too little.
1342920	1347840	We tend to have this notion that creativity is something magical.
1347840	1352240	In fact, I remember for many years, so quick parenthesis, in the previous life I was a
1352240	1353240	musician.
1353240	1357840	So, you know, I, in some sense know about, and a lot of my job was composing songs, right?
1357840	1361880	And I was always, at the same time I was already studying AI and I couldn't help but connect
1361880	1362880	it too, right?
1362880	1368560	And think about like, what would an AI look that was able to compose music, right?
1368560	1373480	And talking to late people who are not musicians, they think that composing songs is some kind
1373480	1378600	of magic thing that comes from, you know, whatever the great beyond, and it's not.
1378600	1382280	It's a very human enterprise and it can very well be automated.
1382280	1386600	It's actually now, you know, people, I used to say to people like, people always say like,
1386600	1390120	well, creativity will be the last thing that we automate because we humans can do it and
1390120	1392920	there's no machine school and be like, no, it's going to be the opposite.
1392920	1396920	You automate creativity long before many other things and we're there now, right?
1396920	1403680	In just the last, so I think when you, let me put it this way, your prompt to the LLM,
1403680	1408200	let's say, is like the grain of sand to the oyster, right?
1408200	1411680	You should not give yourself credit for having made the pearl because it put the grain of
1411680	1412680	sand in there.
1412680	1414960	That's a, that's a brilliant analogy, right?
1414960	1420440	So it is still the LLM, we need to, we can critique how creative it is or not and there's
1420440	1424200	a lot to be said there and a lot of progress to be made, but we need to give it credit
1424200	1425600	for what it does, right?
1425600	1427960	It is well or not so well, right?
1427960	1432680	Maybe it's more of an illusion that we're giving credit for and whatnot, but that text
1432680	1437760	or that image or whatever, they were created by the AI.
1437760	1441440	And in many ways, the thing that was created by the AI is no worse than what would have
1441440	1445240	been created by an artist if I gave them the prompt.
1445240	1446240	No, okay.
1446240	1447240	Well, on that, I agree with you.
1447240	1450560	I mean, Melanie Mitchell had this wonderful anecdote from the Google Plex when she was
1450560	1454080	with Douglas Hofstadter and he was talking at the time about, you know, how he would
1454080	1458920	be devastated if an AI could produce a Chopin piece, you know, which was indistinguishable
1458920	1464520	from one which he actually created and of course, that did happen, but then we get into
1464520	1467360	this discussion of where does it start, right?
1467360	1469040	Where does it start?
1469040	1470920	Computers only do what we tell them to do, right?
1470920	1475160	They've been trained and actually I was speaking to Sep about this the other day that, you
1475160	1478560	know, all of the abstractions, all of the things that the computers and the models do,
1478560	1482680	they are crystallized snapshots of things that humans have previously done and we've
1482680	1484920	written the computer code.
1484920	1486720	So where does the creativity start?
1486720	1493880	Well, but we, by that standard, we humans also only do what we're told to do.
1493880	1495640	We do what we're told to do by our genes.
1495640	1499040	Our genes do what they're told to do by evolution, which does what is told to do by the laws
1499040	1500040	of physics, right?
1500160	1501160	Right.
1501160	1505680	And now, again, this gets back to this notion that there's nothing magical about creativity.
1505680	1510920	Creativity really is, to a large extent, cutting and pasting stuff and satisfying consistency
1510920	1512320	constraints between them.
1512320	1516720	And I'm not just saying this in the abstract, like long before the modern era, there's this
1516720	1523120	guy called David Cope, you know, a composer and professor of music at UC Santa Cruz who
1523120	1528760	created these programs that exactly they would write, they can write, this was pre-machine
1528760	1529760	learning.
1529760	1534520	It was list code that what it did was basically have rules about how music should be.
1534520	1537200	And then it takes snippets and combines them, right?
1537200	1540720	You could say it's just parroting those bits, but the truth is at the end of the day and
1540720	1541720	you can choose.
1541720	1544080	You say, like, give me something in the style of Mozart.
1544080	1550040	And it creates something that looks indistinguishable from what Mozart did, but all it's doing
1550040	1552640	is this kind of recombination of pieces.
1552640	1559440	So we humans, we have too much respect for appreciation of our own intelligence.
1559440	1561000	That's also what we're doing.
1561000	1562760	Yeah, I think I agree with you.
1562760	1566760	I mean, first of all, intelligence is a receding horizon and there's the McCorduck effect.
1566760	1568160	I agree with all of that.
1568160	1574180	But yeah, I think it's a similar thing to how we anthropomorphise large language models
1574180	1577320	and even, you know, it's tempting to say large language models are slightly conscious
1577320	1581760	and we'll talk about that in a minute, but maybe like we also anthropomorphise our own
1581760	1582760	agency, right?
1582760	1586480	We have like a little bubble around ourselves and we kind of delude ourselves that we exist
1586480	1591320	as an individual unit with agency disconnected from the rest of the world.
1591320	1597400	Well, precisely the problem with how we largely take AI today, this has always been the case,
1597400	1602760	by the way, is that we have a new resistible notion to anthropomorphise anything that behaves
1602760	1604200	even remotely like us.
1604200	1608800	We're the only intelligent things that we know, so if something starts behaving intelligently,
1608800	1613240	then we project onto it all of these other human characteristics.
1613240	1615400	Same with consciousness, same with creativity.
1615400	1617600	We don't know anything else that's creative besides us.
1617600	1621840	So once a machine starts behaving creatively, we cannot help but project a lot of things
1621840	1622840	onto it.
1622840	1624840	It's just reasoning by analogy, right?
1624840	1629320	So it's a kind of analogical, so like you're like me in this respect, so you probably are
1629320	1630320	in this other spec.
1630320	1633760	Now, the good news is that we always start out with this kind of very good reasoning
1633760	1637920	by analogy, but after a while, we actually start to build a model of the real thing.
1637920	1643320	So AI for the public at large right now is very new, but gradually we'll come to a point
1643320	1649120	where we zero in on what AI really is rather than just the shallow analogies that we initially
1649120	1650120	used to try to understand.
1650120	1651120	Okay.
1651120	1652560	Well, I'll try it from a slightly different angle.
1652560	1656040	So we were just saying Seoul makes the argument that it's a biological property and that's
1656040	1660480	where intentionality and consciousness comes from and it's a requisite, but we'll leave
1660480	1662160	that for the time being.
1662160	1667720	Let's go the Fodor and the Gary Marcus and the Chomsky group, and they would argue that
1667720	1673400	creativity is basically this notion of, or even analogy making by extension, is this
1673400	1678440	notion of being able to select from a set which has an infinite cardinality.
1678440	1682840	And as you know, neural networks can't represent infinite sets because they're finite state
1682840	1686800	automators, therefore they make the move we need to have this compositionality.
1686800	1687800	What do you say to that?
1687800	1689920	Well, there's a lot to unpack there.
1689920	1693040	I think we definitely need compositionality, right?
1693040	1696840	If somebody asked me, make a list of how there's some things that are actually essential
1696840	1700400	for intelligence, compositionality would be one of them, right?
1700400	1705040	And this, of course, is the thing that people like Chomsky and Gary are not really care
1705040	1706040	about, right?
1706040	1711560	Having said that, I think first of all, there is no such thing as an infinite set, right?
1711560	1720320	Like infinite set is a useful but extremely dangerous and confusing mathematical tool,
1720320	1721320	right?
1721960	1726520	There is no such thing as an infinite anything and there never will be.
1726520	1730880	So I would just raise this at, well, yes, creativity and almost anything we can do in
1730880	1736680	AI is selecting from a very large set, not infinite, but very large, right?
1736680	1740480	And now, but now we don't just select like one full element at a time.
1740480	1744280	We compose it out of pieces and that's actually where the intelligence comes in.
1744280	1745280	Interesting.
1745280	1747760	I don't want to go too far down the digital physics route, but we did just have Yoshua
1747840	1752480	Barkon and I mean, just to reclamify on that, would you place yourself in that camp that
1752480	1755040	the universe is digital and made of information?
1755040	1757120	Valid question.
1757120	1760280	I certainly think the universe is finite.
1760280	1767920	I think, I mean, like Seth Lloyd says, the universe is a computer, right?
1767920	1774440	And I think that is true or false depending on what you take the word computer to mean,
1774440	1775440	right?
1776440	1782920	So if you say that the universe is digital or is a computer as kind of like an analogy
1782920	1786120	that lets us understand it better, I'm all for that.
1786120	1789280	I don't think the universe is little, you know, here's the way to put this.
1789280	1791080	The universe is a computation.
1791080	1795120	Like, I don't know what the computer is or if there is one.
1795120	1800160	Now the universe is digital in the sense that deep down at the most basic level, the universe
1800160	1802120	is made of discrete things.
1802360	1805880	OK, is this like the it from bit, the John Wheeler type hypothesis?
1806960	1808440	Yes.
1808440	1813360	I mean, if you read that paper, it is, I mean, John Wheeler was a brilliant person.
1814280	1819720	Again, very, you know, to get back to François Chalice's tweet, he was very good at coming
1819720	1821760	up with his provocative notions, right?
1821760	1825040	And the it from the thing, of course, is like newly unveiled today.
1825440	1830760	And I do so at that level, I do agree that looking at the universe is being made of
1830760	1832960	information is very useful.
1833040	1838520	And in particular, if you want a grand unified master algorithm, in some sense, the only
1838520	1843120	way that I at least can see of doing that is by seeing everything as information.
1843680	1848240	So I think, and if I do something that I am working on that, looking at everything as
1848240	1851000	information is a very productive thing to do.
1851280	1857040	Yeah, but but my caution is that information is one aspect of everything.
1857680	1860960	So I can give you a theory of everything that's based on information.
1861400	1863560	But it's not truly a theory of everything.
1863560	1865520	It's a theory of one aspect of everything.
1866080	1867520	And I think there's a lot to be done there.
1867520	1871280	But again, we shouldn't forget what we're living out when we focus on that aspect.
1871640	1873920	Yeah, I mean, we've spoken a lot on the show about, you know,
1874480	1878720	Penrose's view and obviously Sal's view that arises from from biology.
1878720	1882680	And I know if Keith was here, he would argue strongly that he believes in in
1882680	1887840	continuum, and therefore we would need, you know, hyper computation to have this
1887840	1891160	universe. That would be an interesting discussion to have, because I really don't
1891160	1895440	see where there is physical or any evidence for continuum of any kind.
1895680	1899880	The evidence is always that continuum are a useful approximation, but always
1899880	1902000	underlying the continuum is a discrete reality.
1902240	1904880	You take a sensor of anything, right?
1904880	1908080	You know, quantum mechanics is like the quintessential example of this, right?
1908520	1912240	What do we measure at the end that it's always discrete events?
1912680	1918960	Right? Like it's the detection of a photon by, you know, by whatever detector, right?
1918960	1921560	Could be a model of Dobson or a CCD or whatever.
1921800	1923640	But it's a it's a it's a change of state.
1923640	1924680	It really is a bit.
1925320	1927080	Oh, interesting. Well, how would you contrast that?
1927080	1930960	You know, Stephen Wolfram has got this idea of of digital physics and, you know,
1931280	1935720	maybe and again, unfortunately, we have to use arguments from behavior, you know,
1935720	1939440	to kind of say, well, we've we've got potentially a graph cellular automata
1939680	1943280	and it creates this beautiful emergent structure, which is very much like the universe.
1944120	1948760	But, you know, Scott Aronson would make the argument that he's discounting quantum mechanics.
1948760	1951240	I mean, what would you say to that?
1951240	1955800	So I think Steve Wolfram's theory is very interesting.
1956280	1959560	And he gets some things right that a lot of other physicists don't,
1960440	1963600	in particular, that the universe at heart is discrete.
1964360	1967320	So I'm very much with him on that aspect of his agenda.
1968320	1971240	And thank God there's someone like him and a number of others,
1971960	1973640	you know, going that route, right?
1973640	1975240	They're the minority in physics.
1975240	1978120	But actually, I think if you look at just what has happened in the last 10 years,
1978320	1980160	things are very much moving in this direction.
1980280	1982400	And I think they're going to move more, right?
1982720	1987680	Now, having said that, his specific theory, I think has a lot of shortcomings.
1988520	1993840	And I don't think it's the ultimate theory or maybe even the best path to a theory,
1994600	1996200	you know, to a discrete theory of the universe.
1996640	2002120	Scott Aaronson's critique in that regard, I think misses the point, right?
2002440	2004840	It's interesting because it's interesting that you should like pair those two
2004840	2008800	because Steve Wolfram, in a way, is a physicist to become a computer scientist.
2008800	2010640	And Scott is the other way around, right?
2010840	2012920	And I think, you know, like I greatly admire both of them
2012920	2014400	and I'm friends with both of them.
2014400	2016400	I've had many discussions with them.
2016400	2020800	I think, you know, just to very, you know, cruelly caricature things in a way,
2020800	2024320	the problem with Steve is that he has bought into the computer science
2024320	2025960	assumptions too much.
2025960	2029400	And the problem with Scott is that he has bought into the quantum physics
2029400	2030760	assumptions too much, right?
2030760	2034200	So if you really think carefully and rigorously about quantum mechanics,
2034480	2039480	all that continuous mathematics is there just to make discrete predictions.
2040280	2041760	So the continuity may be useful.
2041760	2042520	It is useful.
2042520	2045640	I'm not arguing against the use of continuity and infinity in our mathematics.
2045640	2047520	In fact, we'd be nowhere if we didn't have it.
2047760	2050120	We just have to remember that it's an approximation.
2050120	2051880	It's a useful fiction, right?
2051920	2058720	So quantum mechanics in no way invalidates Steve Wolf from theories, right?
2059280	2062000	The problem, however, is that he has not entirely, you know,
2062360	2064320	ever since cellular automata days, right?
2064320	2066800	He was always saying like, oh, you know, the laws of physics will come out
2066800	2068080	of this cellular automata, right?
2068080	2070440	And people had these objections and, and, you know,
2070440	2072560	now he and others have partly answered some of them.
2072560	2075960	But the truth is at the end of the day, he the only way to answer
2075960	2079680	that objection is to say, look, here is how quantum mechanics arises
2079680	2081800	from my discrete model of the world.
2081800	2085040	And I think this will happen, but it hasn't happened yet.
2085240	2087400	Interesting. OK, we'd love to get Steven on the show.
2087400	2089720	Actually, he's got a he's got a new book out now,
2089720	2092200	which is kind of like expanding on his previous work.
2092600	2097160	But OK, I was having a chat with some alignment folks yesterday.
2097200	2101000	And it's something that I'm a bit naive to, but as I said, I've just read a book.
2101360	2104520	I think it's called The Rationalists Guide to the to the Universe.
2104840	2107640	And it kind of talks all about the the early embryonic stages
2107640	2110960	of Robin Hansen and Nick Bostrom and Eliezer Yuckowski
2110960	2113720	and the Les Ron community and, you know, the info hazards.
2113720	2116800	And, you know, I don't know if you've heard of Rocco's Basilicist
2116800	2118960	and all of this line of thought, basically.
2119320	2122080	And yeah, so where to go with this?
2122080	2127720	Now, I kind of put forward that part of my problem with their conception
2127720	2133480	is that it relies on this rational agent making trajectories of optimal decisions.
2133880	2137720	And also, they they tend to be utilitarianism
2137720	2140720	and utilitarianists and consequentialists.
2141120	2144720	And yeah, I just wondered, what's your kind of like high level take on this?
2146160	2148880	Well, there's many aspects to this, right?
2148880	2151760	I think let me put it this way.
2151760	2157160	I. A rational agent, right, is an agent that maximizes
2157880	2159920	maximizes expected utility, right?
2159960	2163360	The definition of rationality is that it's, you know,
2163360	2165680	expected utility maximization, right?
2165920	2168400	And there is a lot of content to this, right?
2168400	2172400	And, you know, people in many fields like economics and and and, you know, AI, right?
2172840	2174400	They make good use of it.
2174400	2178760	It doesn't answer the question of what is the utility that you're maximizing, right?
2178760	2181640	So if you give me utility function, right?
2181640	2184400	Now, if you maximize it, you're rational.
2184840	2186000	You can it can be bound.
2186000	2188640	You can be boundedly rational because you're and indeed this
2188640	2191880	this is the interesting and prevalent cases that you can only maximize it
2191880	2194840	within bounds and you have to make compromises, says this fast and what not.
2194840	2196760	But still, you're rational.
2196760	2199160	If you don't do that, you are irrational, right?
2199480	2203200	So rational is a very, you know, like so many mistakes that we make
2203200	2207680	as a society as individuals would be avoided if only we were rational in that sense.
2208080	2210760	So at that level, I sympathize very much with that view of the world.
2210760	2214680	Having said that, there's a huge gaping hole in the middle of this, which is like,
2214880	2216600	but what is your utility function?
2216640	2220120	Right. And and, you know, one attitude is like, oh, that's for you to decide.
2220160	2222560	You know, you you tell what me or your utility function is.
2222560	2225320	But but then you you you're you're entitled to sell what like.
2225320	2228480	But like my whole problem is that I want to figure out what my
2228480	2229680	utility function should be.
2229680	2233560	And at that point, this whole theory of rationality just doesn't help you at all.
2233960	2236040	The utility function is an input, right?
2236040	2238760	So another question becomes what is your utility function, right?
2238920	2242080	And then there's a very related, but as Hume said, very different question,
2242080	2245160	which is like, what should your utility function be like?
2245200	2248280	Should is a very loaded worth here, right?
2248640	2251480	And then what what usually happens is things like this is that our
2251480	2255880	notions of morality and so on are trying to impose a should on you,
2256200	2261200	a utility function that you should have because it serves the utility of the society.
2261960	2264840	Now, from the point of view, the society, this is good, right?
2265080	2267760	But from the point of view, because the society hopefully will live
2267760	2271680	and prosper if its elements contribute to its utility, not just their own, right?
2271880	2273640	But it still doesn't answer the question.
2273640	2275280	So you can you're entitled to ask.
2275280	2276760	So what does answer the question?
2276760	2282080	Right. And my view on this is that none of these people and these people
2282080	2285400	include Kant and Bentham and, you know, Plato and everybody, right?
2285920	2288120	They you can't do that, right?
2288120	2294200	The the the so to me, the supreme reality of life, Supreme
2294200	2298520	maybe is a bad word, but like the overarching reality is evolution, right?
2299120	2301360	Everything we are is created by evolution.
2301360	2304360	And as somebody famously said, nothing in biology makes sense,
2304360	2307960	except in light of evolution, nothing in morality makes sense,
2307960	2310920	except in light of evolution, not just biological evolution, even though
2310920	2313560	that's part of it, but also social and cultural evolution.
2313560	2316800	So at the end of the day, the question that you need to ask yourself is like,
2316800	2319440	is which utility functions are fitter?
2319760	2321440	And those are the ones that will prevail.
2321440	2322920	So so let's let's go there.
2322920	2323600	That's really interesting.
2323600	2328400	Now, you're known as a skeptic of collectivist thought, right?
2328400	2331040	We know, and there's this interesting dichotomy we were talking about
2331040	2335520	of, you know, monolithic truth, but the utility functions interesting as well.
2335520	2338440	Because in a sense, I mean, I know these folks are consequentialists,
2338440	2341120	but in a sense, that's more leaning towards deontology.
2341840	2345800	I did it again, deontology, you know, which is this idea that
2345800	2349520	we have kind of like a principled approach to to morality.
2349760	2353360	And I'm skeptical as well that it's possible to create such a utility
2353360	2355360	function because it wouldn't really be parsimonious.
2355360	2359440	But how do you wrestle that, that you have a simple utility function,
2359440	2361880	even though you believe in diversity of ideas?
2361880	2364240	Oh, no, I didn't say simple.
2364240	2364560	Go on.
2364560	2367920	Crucial point, the utility function could be extremely complex.
2368880	2370480	And in fact, the utility function.
2370480	2373600	So first of all, there's there's more than one level to this.
2374720	2378560	You to let's say you believe in utilitarianism, right, which I don't,
2378560	2381760	but have, you know, compared to the others, it's probably the least bad.
2381760	2383040	Right. Yeah.
2383040	2387440	Believing you tell if you believe in maximizing utility function
2387440	2391120	that in no way sanctions collectivism.
2392320	2396560	Collectivism is one particular strength that historically came out of that.
2396560	2397280	Right.
2397280	2400640	And, and, and, you know, again, and Bentham is responsible for it.
2400640	2404560	But it's this notion that you should have a utility function
2404560	2406240	in which everybody counts equally.
2407440	2409680	This is now making a choice of utility function,
2409680	2411280	which is different from having one.
2411280	2413360	Okay, but I think you're saying something quite interesting as well,
2413360	2414880	which is that at the moment,
2415600	2417760	the utility is a function of market value,
2417760	2422080	which is very much inspired by Adam Smith's hidden hand of the market.
2422080	2425920	But I think your views against collectivism is very much against
2425920	2427920	this idea of equality of outcome.
2427920	2429200	And that's definitely not what you're saying.
2429200	2431120	No, I mean, that's even going beyond that, right?
2431120	2435600	Equality of outcome is actually irrational, frankly, to we could go into that.
2435600	2437280	But, you know, you mentioned the market, right?
2437280	2439200	And the market is the size utility.
2439200	2442320	Again, that is a one way to decide.
2442320	2446480	I mean, that is also a very critical approximation to what you really want.
2446480	2449920	So actually, all we have with capitalism or carnimism at this point,
2449920	2453360	in terms of utility function, are very imperfect, right?
2454160	2456080	And that's even saying it generously.
2456080	2458640	And really, our job is to try to come up with something better,
2458640	2460800	which I totally think we can, right?
2460800	2463600	And by the way, one very salient question here,
2463600	2466720	which again, for economists, it's very salient,
2466720	2468560	is this question of like,
2468560	2472720	should you have one utility function overarching, controlling everything,
2472720	2474560	even if it's complex, right?
2474560	2476480	Or should you not, right?
2476480	2479680	And that I think is a very interesting question, right?
2479680	2482240	And there are good arguments in both directions, right?
2482240	2484400	So let me just give you one silly example,
2484400	2486640	which then I think also generalizes two other things.
2486640	2489520	Does your brain have a singular utility function?
2489520	2491440	And I think the answer is no.
2491440	2494000	Now, you could say from an evolutionary point of view,
2494000	2496000	the overarching utility is fitness.
2496640	2498800	But then the way that cashes out in your brain
2498800	2503760	is that your genes need to control this adaptive machine, right?
2504400	2509440	In such a way that you give the machine freedom, right?
2509440	2511520	To do things that the genes by themselves couldn't.
2511520	2513280	But at the same time, at the end of the day,
2513280	2517040	that machine has to subserve the propagation of those genes.
2517040	2518480	And the way you do this, right,
2518480	2520000	at least the way evolution seems to have done,
2520000	2521360	and I think it makes a lot of sense,
2521360	2523600	is that you don't just have one utility.
2523600	2526880	You have several ones which correspond to your emotions.
2527840	2529040	And then they fight it out.
2529760	2531280	So I actually think there's this connection
2531280	2533520	between rationality and the emotion that people don't make,
2533520	2537360	which is that your emotions are really your utility functions.
2537360	2541360	You just have different ones that cater to different things, right?
2541360	2543840	You know, fear and anger and so on.
2543840	2545520	And so I think in reality,
2545520	2547600	we actually have multiple utility functions.
2547600	2550160	But because, again, it gets to this problem that
2550160	2551920	what we're trying to do is approximate something
2551920	2554080	that is very complex and difficult to get at,
2554080	2555440	maybe it is just one,
2555440	2557280	but we're better off trying to approximate it
2557280	2558560	with 10 or 20 different things
2558560	2560560	than just trying to nail that one thing.
2561200	2562000	That's really interesting.
2562000	2564880	And is your view then on having this diversity
2564880	2567600	of utility functions analogous to your views
2567600	2568960	on the master algorithm?
2569680	2571680	Huh. It's analogous,
2571680	2573760	but you're actually talking about different dimensions, right?
2573760	2575840	You could make a table where on one side
2575840	2577840	you have all the different utilities,
2577840	2579920	and then on the other side you have the algorithms.
2579920	2582080	And now you can pair off any one of them.
2582080	2584480	I can say, I'm going to pursue this,
2584480	2588400	you know, minimize your fear using symbolism or minimum.
2588400	2589680	So any combination is valid.
2591040	2592160	Really, really interesting.
2592160	2595520	Okay. And then let's get into meritocracy, for example.
2595520	2597520	So at the moment, we do have the market system.
2597520	2599680	And presumably you think that some people
2599680	2601760	do genuinely have more market value than others.
2602560	2604720	For sure. No. And by the way, I think,
2605360	2608320	I'm definitely a big believer in meritocracy.
2608320	2611200	I think. But what does it mean to you?
2611200	2614000	Right. Very good. So let's get that down first, right?
2614000	2618240	Meritocracy, so our goal is to have the society
2618240	2622480	that functions best and provides best for everybody, right?
2623280	2625200	And I mean, we could refine even that,
2625200	2627680	but let's just take that for now as our assumption, right?
2627680	2629200	But then if that is the case,
2629200	2632320	one of our primary goals, maybe even the most important one,
2632320	2636640	is to get everybody to contribute the most they can, right?
2636640	2638880	Meritocracy is often seen as like,
2638880	2640320	I'm going to rank all the people,
2640320	2642400	and at the top is the greatest genius,
2642400	2644320	and at the bottom is the most useless person,
2644320	2646080	and this is wrong, right?
2646080	2649520	Meritocracy is a many-dimension thing, right?
2649520	2652160	The goal of meritocracy is to find for everybody
2652160	2654720	what they're best at doing so that they can do it,
2655280	2658000	maximize everybody's contribution to society, right?
2659120	2660960	And this is a very complicated process.
2660960	2663520	There isn't a single scale of intelligence or anything else.
2664080	2667040	Having said that, it very much is the case
2667040	2670720	that some people are better for some things than others, right?
2670720	2671920	And if you deny that,
2671920	2674320	you are actually in the process of destroying the society
2674320	2675440	and making it dysfunctional.
2675440	2681120	So I find the attacks on meritocracy extremely disturbing, right?
2681120	2682480	And a lot of them are,
2682480	2685040	I've talked with many people who have those beliefs, right?
2685040	2686880	And the number one thing that they say is,
2686880	2688320	it basically boils down to like,
2688320	2691120	oh, meritocracy isn't perfect, so we should junk it.
2691920	2694880	Something not being perfect has never been a reason to junk it.
2694880	2696000	It's a reason to improve it.
2696000	2699200	So there's a lot of room to improve in meritocracy,
2699200	2701680	but if you throw it away, you are destroying society.
2702240	2706640	Well, I mean, you can trace this back to our argument about utility.
2706640	2711840	But the thing is though, if we had a value function
2711840	2714160	which represented actual market contributions
2714160	2716560	or even societal contributions, that would be one thing.
2716560	2720480	But would you agree that we have a lot of game playing at the moment?
2720480	2723360	So utility is based on playing the success game
2723360	2725120	or the dominance game or the virtue game,
2725120	2726560	as Will Stor said in his book.
2726560	2728800	So we've got these kind of emerging games
2728800	2732960	and it's not really representing utility.
2733600	2734560	Well, absolutely.
2734560	2738320	So far we've been talking about utility, right?
2738320	2739760	But what happens in the real world
2739760	2741760	is that there are multiple agents,
2741760	2743360	each with their own different utility.
2744160	2747120	And at this point, what you have is game theory, right?
2747120	2748400	Game theory is just what you have
2748400	2750960	when there's not a single optimization going on,
2750960	2752400	but multiple optimizations
2752400	2755120	which are partly contradictory, maybe partly not.
2755120	2757280	So the best way to understand everything
2757280	2760400	that we've been talking about, including society and evolution
2760400	2763520	and even what happens inside your brain is as a big game.
2764240	2766320	A much bigger and more complex game
2766320	2768880	than game theorists and economists and so on
2768880	2772480	and evolutionary biologists, right, prominently,
2772480	2774960	have been able to handle in the past.
2774960	2778720	But I think they are very much in the right track
2778720	2781040	and we can understand a lot of these phenomena
2781040	2784480	that you're referring to as they are games being played
2784480	2786880	by people that have certain utilities, right?
2787440	2789840	And now you are going to impose your, you know, like,
2789840	2791120	and it's a game, right?
2791120	2792960	I don't, you don't know who's going to win
2792960	2795360	until you actually do the linear program
2795360	2797440	and figure out how this is going to, you know,
2798000	2800720	and of course games are, you know, in reality,
2800720	2803040	you know, most games are not single round games, right?
2803040	2804720	They're continuing games, right?
2804960	2806880	So things get very, very interesting,
2806880	2810080	but this I think is the most productive way
2810080	2811200	to look at all of this.
2811200	2812160	Okay, good, good.
2812160	2815600	But then some might say that this is a platonic way
2815600	2817280	of looking at the world
2817280	2819600	and the world is actually much more complicated than that.
2819600	2821920	And again, we're kind of fooled by randomness
2821920	2823760	because we're anthropomorphizing the world
2823760	2825520	and we're kind of framing it as a game.
2825520	2827600	It might be much more complicated than that.
2827600	2829440	And I've already said this a couple of times,
2829440	2831440	but you know, the concept of power, for example,
2831440	2833200	did when Napoleon said,
2833200	2836000	I want the men to march into this country,
2836000	2838880	is it just a simple kind of chain of command that goes down?
2838880	2839440	No, it's not.
2839440	2840880	It's so much more complicated than that.
2840880	2842800	Well, yes, but that's, actually,
2842800	2843920	I'm not even sure what you mean
2843920	2846480	by when you say it's much more complicated than a game.
2846480	2847840	Again, when I say a game,
2848800	2850720	maybe what comes to your mind is something simple,
2850720	2853920	like in a prisoner's dilemma, two players, two moves.
2853920	2856800	It's a game with, you know, with a vast number of players,
2856800	2858400	each with a vast number of moves.
2859280	2861120	Interesting, but I think this gets to the core
2861120	2862480	of what the rationalists talk about.
2862800	2864000	They have these thought experiments.
2864000	2865600	They talk about prisoners dilemma.
2865600	2867440	They have that, I forget the name of that game
2867440	2868640	where there's the two boxes,
2868640	2871040	and you have to choose the box, I forget that.
2871040	2872640	But I guess what I'm saying is that
2873280	2875280	if you do have this rationalist conception of the world,
2875280	2878160	and think about it in terms of game theory,
2878160	2879920	just like the symbolists do,
2879920	2882480	and the people who handcraft cognitive architectures do,
2882480	2884160	or even with causality, for example,
2884160	2886960	we create these variables, it's all anthropomorphic.
2887600	2892160	Well, I would not, so let me put it this way, right?
2892960	2895120	You can model almost anything,
2896080	2897760	can is an important word here.
2897760	2900400	You can model almost anything in the world,
2900400	2902960	in any domain, from physics to psychology
2902960	2905920	to sociology, name it, as optimizing a function.
2906880	2909520	Whether you should is a debatable question,
2909520	2911360	but you can, right?
2911360	2913360	But now, what really happens is that
2913360	2915040	there are many different optimizations
2915040	2916560	going on at the same time,
2916560	2918560	all the way from maximizing entropy
2918560	2920480	to me deciding what I have for lunch today.
2920960	2924000	And now what you have is all of these interlocking optimizations,
2924000	2926320	and that's what I'm calling game theory, right?
2926320	2928320	One of those optimizations is I'm Napoleon,
2928320	2929600	I want to conquer Russia,
2929600	2932080	you're the Tsar of Russia, you don't want to be conquered, right?
2932080	2933680	And then we play a very complicated game,
2933680	2936160	which includes other agents, like your soldiers,
2936160	2938960	which maybe, you know, I, a French soldier,
2938960	2940160	you know, want to conquer Russia,
2940160	2941600	but I also want to stay alive,
2941600	2943120	whereas an opponent really couldn't care less
2943120	2945200	whether I, in particular, stay alive or not,
2945200	2946720	as long as he conquers Russia in the end.
2946720	2950080	So this very complex game, I think, is what goes on.
2950160	2952640	I don't think framing things in this way
2952640	2954000	is anthropomorphizing them.
2954000	2956080	In fact, I think this is our best hope
2956080	2957760	to not anthropomorphize things,
2957760	2959200	although at the end of the day,
2959200	2961520	I think you can look at almost anything
2961520	2965280	and see a ghost of anthropomorphization there.
2965280	2968000	But if there's a less anthropomorphic way
2968000	2970000	to look at the universe than through this lens,
2970000	2971600	I'd be interested to see what it is.
2971600	2973040	Well, the only reason I'm saying this is,
2973040	2975360	first of all, I want to play devil's advocate a little bit,
2975360	2977840	and we even spoke about the blind men
2977840	2979360	and the elephant a little while ago,
2979360	2982480	and I'm sure folks on the left, as they did,
2982480	2984480	they criticized Ayan Ran, for example,
2984480	2986720	and they said that she had this very transactional way
2986720	2988720	of viewing the world as this kind of
2988720	2991680	Nash equilibrium of self-interested actors.
2991680	2993120	And are we guilty of doing that?
2993120	2995760	Are we kind of cutting off many aspects of the truth
2995760	2997040	by doing this? I guess that's what I'm saying.
2997920	3001600	So we are always cutting off some aspect of the truth
3001600	3003920	when we look at anything in any way,
3003920	3006640	which is not a reason to look at nothing in no way.
3007280	3010880	So I think this is a very productive way to look at things,
3010880	3012080	but not the only one.
3012080	3013760	It doesn't exhaust what there is to be said,
3013760	3015440	but I personally feel like it's the one
3015440	3017600	where the most progress can come from.
3017600	3018640	Interesting.
3018640	3022000	Now, that's sort of like Ayan Randian
3022000	3023280	simplification of the world.
3025200	3029280	Looking at things this way does not imply over-simplifying them.
3029280	3032240	On the contrary, I would actually say it gives us a handle
3032240	3034960	on how to go into the complexity and not get lost
3035440	3037440	and not devolve into like platitudes
3037440	3039120	or over-simplifying ideologies.
3039920	3042320	The fact that there's a mathematical component to this
3042320	3043120	is very important.
3044080	3045600	Mathematics, when you can apply it,
3045600	3047840	gives you a very solid handle on things.
3047840	3050880	We are now at the point where we can handle
3050880	3053840	a lot of things mathematically slash computationally
3053840	3054800	that we couldn't before.
3054800	3057760	So when von Neumann invented game theory,
3057760	3060800	he said, this is the future of the social sciences.
3060800	3062640	And so far it hasn't been,
3062640	3064240	but I think we're actually now at the point
3064800	3066160	partly because we have the data.
3067280	3069680	We actually can now usefully apply this point of view
3069680	3071200	in a way that we couldn't before.
3071200	3073120	How far it takes us, we'll see.
3073840	3075760	It's not the only possible to look at things,
3075760	3078720	but I do think it's probably the most productive at this point.
3078720	3079360	Interesting.
3079360	3080000	Okay.
3080000	3082240	So coming back to this rationalist school of thought,
3082240	3085680	one thing that I'm interested in is morality.
3085680	3087520	But let's go one step at a time.
3087520	3089600	So I think Bostrom came up with this idea
3089600	3091280	of instrumental convergence,
3091280	3093040	which is this notion that in the pursuit
3093040	3094480	of doing a particular task,
3094480	3097760	the intelligence system might actually potentially kill
3097760	3100080	everyone on the planet or do adjacent.
3100080	3101840	And this is where the interesting thing comes from.
3101840	3105200	So one task, but adjacent multitask ability
3105200	3108240	and potential intelligence and so on.
3108240	3110240	So there was an example of a cauldron.
3110240	3113440	So you've got someone filling up a cauldron
3113440	3115440	and in the pursuit of filling up the cauldron
3115440	3116320	to just the right level,
3116320	3119120	they might kill the person who looks after the cauldron room
3119120	3121840	just so that the agent could do it more efficiently.
3122800	3125200	Are you cynical about that or what do you think?
3126000	3127760	No, I'm not cynical about that,
3127760	3129040	but let me put it this way.
3129680	3133920	I don't lose any sleep worrying about the paperclip factory
3133920	3135360	that's going to take over the world.
3136400	3139600	I think you have to take that as a philosopher's thought experiment.
3141120	3143200	The philosopher being Nick in this case.
3144320	3149680	I think there's a real danger that there's putting its finger on,
3150400	3154880	but it's also mistaking reality for something else.
3154880	3156160	So let's look at both parts of that.
3156880	3162000	The real danger is that if you give an AI
3163600	3167520	an oversimplified, a hugely oversimplified objective function,
3167520	3170160	and at the same time a very large amount of power,
3170160	3173040	bad things will happen and we need to worry about that.
3175360	3177040	By the way, this is already a problem today
3177040	3178800	in many maybe more modest ways,
3179520	3181040	but also more relevant, frankly.
3181600	3182640	So what do you do?
3182640	3186480	First of all, the utility function needs to be as rich
3187280	3191680	and as complex and as subtle as the people that it's trying to serve.
3193680	3197120	As long as what you have to take a really world example today,
3197120	3201520	social media, who are all designed to just maximize engagement,
3202880	3206400	you have an enormous amount of AI at the service of maximizing engagement.
3206880	3211760	I understand why companies do it and partly they have the right to.
3211760	3212880	We can get into that.
3212880	3216560	But the point is, it's ignoring too many things.
3216560	3221360	So one line of defense against is that you have to enrich your utility function
3221360	3224640	until it's like a bit, and then this is an open-ended problem.
3225600	3227760	We're never going to have the final utility function.
3227760	3230240	It's something that the AIs have to be continually,
3230240	3232960	you know, AIs, I think Stuart Russell said this and I agree,
3232960	3237440	like they should spend half their time figuring out what the utility function is
3237440	3239120	and then the other half maximizing it.
3239120	3242480	Whereas today it's like I wrote down my utility function in one line
3242480	3245920	and now I spend this enormous amount of power maximizing it.
3245920	3247120	So that's one line.
3247120	3251840	The other line or like one other line is you have to put constraints on the machine.
3251840	3252960	Hard constraints.
3252960	3255840	You can win the pursuit of this utility function.
3255840	3257280	You can think of it as like, you know,
3257280	3259280	terms with infinite weight in the utility function.
3259280	3260480	You can't go outside this.
3260480	3264240	And then the other one is the single biggest reason why I sort of like
3264240	3268000	this paperclip experiment is silly is that, you know,
3268000	3269920	along with that paperclip factoring the world,
3269920	3272480	there are going to be a million other AIs, you know,
3272480	3273920	each of which is doing the same thing.
3273920	3278400	So none of them is ever going to acquire the power to cause that damage
3278400	3281360	unless it's doing something very different from just trying to make paperclips.
3281360	3284960	So at some level that example is extremely unrealistic
3284960	3286720	and leads us down the wrong track.
3286720	3288160	Right, loads of places to go there.
3288160	3292320	But first of all, I think you do believe in AI alignment then
3292320	3294240	because you're saying exactly the same as what they do,
3294240	3296800	which is that we need to have the utility function
3296800	3299920	that represents the richness of the human condition.
3299920	3300880	So that's the first thing.
3300880	3303680	So essentially you're all on board of alignment.
3303680	3307680	Well, I believe in AI alignment in one sense of it.
3308320	3311360	Many different things get go under that umbrella of AI alignment.
3311360	3312080	Right.
3312080	3315200	I think in the near term, thinking of things into,
3315200	3317600	I mean, if I like, let me put it this way.
3317600	3322880	If AI alignment is just trying to have a really accurate utility function,
3322880	3323840	then yes.
3323840	3326000	And then the machines are optimizing that function.
3326000	3326640	Absolutely.
3326640	3327120	Right.
3327120	3327600	Yeah.
3327600	3331520	And in the near term, I think talking about AI alignment is a little,
3331520	3335280	I mean, the problem that I have with the concept of alignment
3335280	3337200	is that goes far beyond that.
3337200	3341840	It tends to see AIs as these independent agents
3342640	3346800	that we have to align their goals to ours.
3346800	3347280	Right.
3347760	3350400	And if that just caches out as like,
3350400	3351840	here's the utility function, that's fine.
3351840	3354320	But the problem is AIs are not independent agents.
3354320	3355600	AIs are our tools.
3356640	3358320	Just to push back on that a little bit,
3358320	3360880	because I always had that conception of these folks.
3360880	3363120	I thought I was arguing against people who believed
3363120	3364960	in a pure monolithic intelligence.
3364960	3367520	And a lot of them are transhumanists actually,
3367520	3371680	and they say that they want to ensure human flourishing
3371680	3374960	through the use of AIs in tandem,
3374960	3378160	almost as a kind of extended mind from David Chalmers.
3378160	3381760	But then I really wanted to get into their fears
3381760	3384400	of recursive self-improving intelligence
3384400	3385360	and superintelligence.
3385360	3389360	Because when you do have this kind of heterogeneous approach
3389360	3391120	to humans and machines,
3391120	3393040	there are going to be bottlenecks everywhere.
3393040	3395920	Now, I like to think of it a bit like the market efficiency
3395920	3398800	hypothesis, which is that you reach an equilibria
3398800	3403120	where the individual actors in the market become more efficient,
3403120	3404640	will become more efficient programmers.
3404640	3406000	Because we're using codecs.
3406000	3407840	But we will reach a limit, surely.
3408720	3413040	Well, to touch on transhumanism for just a second,
3413040	3415760	because I do agree, at least sociologically,
3415760	3417280	a lot of that crowd is the same.
3418720	3419840	Let me put it this way.
3419840	3421920	And I'm sure this is a controversial statement.
3421920	3425600	But maybe in the long run, the AIs should take over the world.
3426480	3429920	Why are we so arrogant that we think whatever the AI is,
3429920	3431440	it should always be there to serve us.
3432240	3433280	We are a step.
3433280	3436480	If you take the long view of this, we're a step in evolution.
3437280	3437920	We're amazing.
3437920	3440560	Maybe I'm a human chauvinist, but I do think we are amazing.
3440560	3441920	But we're not the last word.
3443680	3448160	So the other day, I tweeted something that is maybe provocative,
3448160	3451600	but it's like, I think in Gemswich, which is,
3452240	3456000	I said that the killer app of humans is producing AI.
3457040	3460320	Maybe our role in evolution is that we're going to produce an AI.
3461040	3463920	That is the next level of whatever you like.
3463920	3466880	Consciousness, intelligence, et cetera, et cetera.
3466880	3469520	And so the notion that in the very long term,
3469520	3471760	the AIs should still be there to serve us,
3472320	3474240	by this point of view, is actually silly.
3475280	3479040	Right, but a lot of folks, let's say the ethics folks,
3479040	3480160	would find it horrifying.
3480960	3483040	And I was speaking to Irina actually yesterday,
3483040	3485600	and she said something a little bit tongue in cheek,
3485600	3486400	which is that which is actually...
3486400	3487360	Who, sorry?
3487360	3490240	Irina from Montreal, Mila, and Irina Rich.
3490400	3491440	Oh yeah, I know her, yeah.
3491440	3493360	We were classmates at UC Irvine.
3493360	3495280	Amazing, yeah, I really love her.
3495280	3496960	But no, she was kind of joking
3496960	3500400	that we should almost align human values to the AGI values.
3501200	3502880	Well, that I find alarming.
3503440	3505840	Well, I think she was saying it tongue in cheek.
3505840	3507680	I'm not alarmed by a lot of things, but yeah.
3508240	3512880	But what do you think about this ethical concern
3512880	3515280	that if it is the case that you believe
3515280	3518880	that we're just one rung on the ladder and transhumanism
3518960	3520880	is more AI than it is human?
3521520	3522800	People would find that horrifying.
3523600	3527840	Well, I understand why people would find that horrifying.
3527840	3529600	And I mean, again, we have to distinguish
3529600	3531760	the short from the meaning from the long term.
3531760	3532880	When I say something like this,
3532880	3535200	I'm talking about the very long term, right?
3535200	3538400	Trying to make humans subservient to AI today
3538400	3540240	is a horrifying idea, right?
3540240	3541920	Now, I think the reason a lot of people
3541920	3544560	are horrified with this idea period, right?
3544640	3548240	Is natural, but in my view, naive is just,
3548880	3552320	they are seeing humans as the end goal.
3554000	3555360	If humans are the end goal,
3555360	3557200	then the idea that they should be subservient
3557200	3560720	to developing the next level of AI is horrifying.
3560720	3565600	If you have a moral system where humans are the be all
3565600	3567760	and end all, then all of this is horrifying.
3567760	3570560	But again, if you take the long view of evolution,
3571840	3573600	humans are not the be all and end all.
3574240	3576080	Okay, I mean, eventually this might take us
3576080	3577760	to the effective altruism discussion.
3577760	3579920	But I think, as we were saying,
3579920	3581680	Sam Harris recently had a podcast
3581680	3583120	talking about the FTX disaster.
3583120	3584480	And he was kind of making the argument
3584480	3585840	that we're all consequentialists,
3585840	3587120	even if we don't realize it,
3587120	3589280	but there are different degrees of consequentialism.
3589280	3592560	And I think a lot of the ethics folks at the moment,
3592560	3594480	they really, really don't like what's going on
3594480	3595840	with long termism.
3595840	3598320	And it's because there's this slippery slope
3598320	3600880	of the kind of horizon of consequentialism.
3600880	3602000	So with Nick Bostrom,
3602000	3603600	he came up with this number
3603600	3605920	that there could be simulated humans
3606640	3608480	living on other planets in the future.
3608480	3609440	It's a very big number.
3609440	3612160	I think it's got a lot of zeros on it.
3612160	3615600	And what's to stop us from just making the argument
3615600	3618080	and what's to stop AIs from making the argument
3618080	3620720	that those simulated lives have more value than our lives?
3622160	3623840	Okay, there's a lot to unpack there.
3624720	3626480	So, but let's take this one step at a time.
3627280	3630720	I very much by the idea of effective altruism on principle.
3630720	3633280	I think that is the way to go about a lot of things.
3633280	3634320	I think in some ways,
3635520	3638240	if you are not an effective altruist,
3638240	3641920	maybe unconsciously, you are being irrational
3641920	3643840	or maybe evil, right?
3643840	3645120	If you believe in altruism,
3645120	3647440	I mean, think about both parts of that, right?
3647440	3650800	If altruism is good, then let's say we take that, right?
3650800	3653120	And then why should you be in favor
3653120	3655520	of ineffective altruism, right?
3655520	3656800	If you're an altruist,
3656800	3658320	if you want the good of other people,
3658320	3660960	you should try to do the best you can, right?
3660960	3663600	And so, for example, I very much by the notion that like,
3663600	3666640	you want to make the most money you can,
3666640	3668320	so then you can give away that money
3668320	3670720	as opposed to volunteering at the soup kitchen.
3670720	3672400	Volunteering at the soup kitchen for, say,
3672400	3674160	someone with a PhD in machine learning
3674160	3676480	is an ineffective form of altruism.
3676480	3678240	Now, having said that,
3678240	3682240	I think the focus on the long term has been in many ways,
3682240	3685120	I mean, certainly the long term is important, right?
3685120	3687840	But the problem with the whole effective altruism movement
3687840	3690240	is that it got overly focused on that,
3690240	3691760	and we can talk about why.
3691760	3694400	And then even, and then a further mistake
3694400	3695760	is that it got overly focused
3695760	3697920	on these supposedly existential dangers
3697920	3701120	that are much less of a big deal than people think like AI.
3701120	3704160	So between effective altruism and fixating on AI
3704160	3706720	as an existential danger lies a huge gulf.
3706720	3709200	I'm for effective altruism, I think, you know,
3709200	3713280	the long term, you know, there's ins and outs there, right?
3713280	3715840	And then this focus on like these existential dangers
3716400	3717840	is very problematic.
3717840	3720080	You know, for example, you know, to get back to the,
3720080	3722240	you know, Bostromian notion of like all these minds
3722240	3724080	that matter more than us and whatnot,
3724080	3725600	there is a basic idea, right,
3725600	3727440	that like any economist knows,
3727440	3729600	which is that you have to discount the future.
3729600	3732560	And the question is what your discount rate is, right?
3732560	3734960	And if your discount rate is high, right,
3734960	3737200	those minds matter not at all.
3737200	3739280	And now why do you have that discount rate?
3739280	3740320	The primary reason is that
3740320	3742480	there's uncertainty about the future, right?
3742480	3747200	I have to weigh the certain benefit of helping you today
3747200	3749200	with the increasingly hypothetical benefit
3749200	3750000	of helping your mind.
3750000	3752160	There's less and less likely to exist in the future.
3752160	3753680	So in many of those cases,
3753680	3756240	the present and the short-term do win.
3756240	3758080	Okay, but a couple of things to contrast that.
3758080	3760880	So a lot of effective altruism is this idea
3760880	3762800	that we're born with faulty programming, right?
3762800	3765200	So we have these views, you know,
3765200	3768400	like we have this concept of moral value
3768400	3771200	and it gets discounted in space and time, right?
3771200	3773440	So we need ways of overcoming our programming.
3773440	3776560	But you were saying that we should be thinking about this,
3776560	3778880	but contrast that with your, you know,
3778880	3781760	with your statement about Ayan Rand earlier, right?
3781760	3784480	So Ayan Rand was very, very transactional
3784480	3787600	because I think the folks that criticize this movement
3787600	3790080	are suspicious that we are actually being
3790080	3792240	a bit more like Ayan Rand,
3792240	3794960	but with the guise of altruism.
3794960	3798720	And I think they think of the FTX disaster
3798720	3800640	as being kind of like evidence of that.
3801920	3803840	A lot of different points there.
3803840	3806400	The FTX disaster actually has nothing whatsoever
3806400	3808240	to do with any of this, right?
3808240	3810720	Sam Beckman Fried was one guy or is one guy,
3810720	3812240	funny that I used the past tense.
3812240	3815520	He's one guy who believed in effective altruism,
3815520	3816560	good for him, right?
3816560	3819360	He was, I mean, the whole FTX thing was also obviously,
3819360	3820960	I mean, yeah, we could get into that,
3820960	3823360	but the point is you should not,
3824080	3827280	I understand why people's image of effective altruism
3827280	3829440	would be tainted by what happened with Sam Beckman Fried,
3829440	3830880	but really it shouldn't be, right?
3831360	3834480	An idea is not responsible for the mistakes
3834480	3837280	that its believers make in unrelated domains,
3837280	3840160	point one, point two, transactionalism.
3840160	3842480	There is nothing in what I've said whatsoever
3842480	3845440	that implies transactionalism, in fact, the opposite, right?
3845440	3849040	I think relationalism is actually the key concept.
3849040	3852640	And part of this is that games are not one shot.
3852640	3854400	Your games are played in a repeated way.
3854400	3855680	And famously, for example,
3855680	3857520	if you play things like Prisoners of the Lemon
3857520	3859680	and whatnot repeated, like you're like,
3859760	3861600	cooperate, defect and whatnot,
3861600	3863680	as soon as you start bringing in these other things,
3863680	3865440	like that make things more realistic,
3865440	3866800	you actually start to get behavior
3866800	3869360	that is much more, what's the way to put it,
3869360	3872080	rational in some ways and human and whatnot, right?
3872080	3874400	Another one is that traditional economics,
3874400	3876560	which I think Ayn Rand was influenced by,
3876560	3878640	viewed and still views the world as linear,
3879360	3880640	but the world is non-linear.
3881200	3883680	Once you start seeing the world as non-linear,
3883680	3885600	all of these things really change,
3885600	3887600	the face of them changes, right?
3887680	3889840	So I think we have to look at all these concepts
3889840	3891840	in this view, right?
3891840	3894480	And we want to focus on the long, so...
3896640	3899600	So to get back to your first point, right?
3900160	3901760	We are born with faulty programming.
3902800	3903520	Part of our fa...
3903520	3906240	And that's what if effective alteration
3906240	3907760	is there to overcome, right?
3908320	3909840	Part of our faulty programming
3909840	3912000	is that our discount rate is too high,
3913120	3914640	because we evolved in a world
3914640	3916400	where your time horizon was very short.
3917360	3918640	The fact that it's too high
3918640	3920400	doesn't mean that we should make it zero
3921600	3923040	and care only about the future.
3923600	3924400	But what would...
3924400	3926560	You know, the ethics folks who advocate
3926560	3928160	for gatekeeping and paternalism,
3929120	3930960	couldn't you just say that they're doing the same thing?
3932480	3934160	Well, you should ask them, right?
3934160	3938000	Wouldn't they lead by saying our programming is faulty
3938000	3940080	and therefore, you know, we need to...
3940080	3942800	No, I mean, look, we can...
3942800	3945760	So part one, we can debate whether our programming
3945760	3947520	is faulty or not and why.
3947520	3950560	And so to just start by touching on that,
3950560	3952240	our programming is faulty.
3952240	3954480	So our programming is not faulty
3954480	3956400	in the sense that we evolved
3957040	3959680	for a particular set of conditions, right?
3959680	3961360	And that evolution may not be complete
3961360	3962640	or optimal, et cetera, et cetera.
3962640	3965840	But roughly speaking, we are not faulty in that sense.
3965840	3966400	The reason...
3966400	3968400	Because evolution is doing its job, right?
3968400	3971440	We have all those impulses for a reason, right?
3971440	3976240	Now, the problem is that we, unlike any other species,
3976240	3979120	we actually have actually succeeded in creating a world
3980240	3981440	that is better for us.
3982160	3984320	But at the same time, and this is the problem,
3984320	3986640	we're actually now adapted to a different world
3986640	3987760	from the one that we live in.
3988560	3990480	So the faulty program just comes from the fact
3990480	3992640	that we evolved for one set of conditions.
3992640	3994800	For example, among many other examples
3994800	3996960	where your time horizon was very short,
3996960	3998960	and now we live in a very different world.
3998960	4001360	And so our job as rational people,
4001360	4002960	that's what our rational minds are for,
4002960	4005520	among other things, is to now adapt ourselves
4005520	4007360	to the world that we really are in
4007360	4008880	so that we do things that are rational
4008880	4010000	in the world that we're in, right?
4010000	4013120	So now, the fact that our programming is faulty
4013120	4015840	does not see anything about what are the faults
4015840	4016800	and how you fix them.
4016800	4018240	And what these people have, I think,
4018240	4021200	is first of all, the wrong notion of what our faults are
4021200	4022240	and then on top of that,
4022240	4023760	the wrong notion of how to fix them.
4024800	4028240	Okay, now, I want to get into the utility function again.
4028720	4030880	Again, one of the things that makes me skeptical
4030880	4034320	is this notion of immutability, both of what we're doing
4034320	4036560	and in the case of what we've been speaking about
4036560	4039680	with utilitarianism, what the utility function is.
4039680	4041680	Now, you were kind of hinting to something interesting before,
4041680	4043280	which is that it might be diverse
4043280	4045360	and it might also be self-updating.
4045360	4047840	But I'm constantly asking myself the question,
4047840	4049520	how does that work and who gets to say?
4050560	4053360	Well, so very much, I think it's complex
4053360	4054800	and it should be self-updating, right?
4054800	4056400	We're never going to final itself.
4057360	4060640	If you buy this notion that the ultimate arbiter is evolution,
4061200	4064320	then utility functions are subject to evolution.
4065680	4067360	Right, so you think about it
4067360	4068560	or you can't think about it.
4068560	4070880	It's a wrong world.
4070880	4072080	You can't think about this
4072080	4074320	and it's useful to think about this in the following ways.
4074960	4076240	To a first approximation,
4076240	4079360	the number one entity that's evolving is utility functions.
4079360	4081040	What you have in the world at any point
4081040	4083840	is a population of utility functions, right?
4083920	4086320	And now they combine, they evolve,
4086320	4088080	you have next generation of utility functions.
4088080	4092560	And then there's also how the utility function gets optimized.
4092560	4094880	That is also subject to evolution, right?
4094880	4097440	And now how the utility function is optimized
4097440	4098560	changes a lot faster
4098560	4101200	and this is a lot more complex than the utility function itself,
4101200	4102800	which is the point, right?
4102800	4104480	So at a certain time horizon,
4104480	4106880	it's reasonable to approximate utilities as being fixed.
4106880	4110240	Like for example, the utilities that are encoded in your brain
4110240	4113360	are fixed by your genes, right?
4113360	4115760	So in the context of our present human moment
4115760	4117360	and effective ultramism or not,
4117360	4119760	it makes perfect sense to think of utilities fixed.
4119760	4122240	But it is evolving and not just on
4122240	4125520	eon time scales, but by the generation, right?
4125520	4126960	Things evolve by the generation.
4126960	4128960	Okay, but it's still relatively glacial
4128960	4132000	and I take your point that there's a kind of divergence
4132000	4132960	between the world we live in
4132960	4134480	and the programming that we've got.
4134480	4138240	But then, okay, let's imagine that we create a new population
4138240	4140960	and I guess what I'm saying is that
4141040	4144800	you think that the utility function should emerge and evolve,
4144800	4148560	but I would argue for some kind of morphogenetic engineering
4148560	4151280	where it's a kind of hybrid between something which is emergent
4151280	4153120	but something which we can nudge.
4153120	4155920	Oh, I mean, I'm glad you brought that up.
4156800	4158400	Nudging is a form of emergence.
4159120	4160400	You yourself are emergent
4160400	4162160	and the things that you do are emergent as well.
4162160	4163520	Everything is emergent, right?
4163520	4165040	Utilities are emergent.
4165040	4166800	Maybe the laws of physics aren't emergent.
4166800	4168320	Some people will say even those are, right?
4168320	4170640	Like, you know, we live in a universe with this constant
4170720	4171760	because blah, blah, blah, right?
4171760	4173840	So, but to first approximation,
4173840	4176640	every single thing that we've been talking about is emergent.
4176640	4180400	We make a distinction between emergent and designed
4180400	4182720	because that is anthropomorphic, right?
4182720	4184880	Is this things that we do are not emergent?
4184880	4186480	Actually, no, when you nudge something
4186480	4188160	that is an emergent behavior, right?
4188160	4189920	We are emergent as well, right?
4189920	4191520	So everything that is human, you know,
4191520	4193840	so here's a very good way, I think,
4193840	4194960	to think about a lot of things
4194960	4197360	which I first saw, you know, in Richard Dawkins,
4197360	4199600	which is although he really didn't go into this
4199600	4201360	and I wish he had like this notion
4201360	4203440	of the extended phenotype, right?
4204080	4206960	Technology is our extended phenotype.
4206960	4208800	So all these things that we do, right?
4208800	4209840	All these things that we build,
4209840	4211200	including AS and whatnot,
4211200	4213440	they are extensions of our phenotype.
4213440	4215840	So if you take the long view, all of, you know,
4215840	4219360	technology is the continuation of biology by another means.
4219920	4221120	So when you make this distinction
4221120	4222640	between emergent and not emergent
4222640	4225200	and top down and bottom up, it's all emergent.
4225200	4227600	Interesting. Well, we recently did a show on emergence
4227600	4229840	and it's a topic of interest to me personally
4229840	4232400	and there's weak emergence and strong emergence
4232400	4234240	and there's, you know, like the view of weak emergence,
4234240	4237360	so there's some, you know, surprising macroscopic phenomena,
4237360	4239200	maybe something which transiently emerges
4239200	4241200	and Wolfram would add in the whole, you know,
4241200	4243200	computational irreducibility angle.
4243200	4244560	And then with the strong emergence,
4244560	4245840	Chalmers would say it's something
4245840	4247600	which is paradigmatically surprising.
4247600	4249360	It's something which is not deducible
4249360	4251440	for many fundamental truths in the lower level domain.
4251440	4254720	But I just wondered, like, how do you think about emergence?
4254720	4256880	Well, I think that is a very, the distinction
4256880	4259520	between weak and strong immersion is a very useful one.
4259520	4260320	Right.
4260320	4263840	And I would actually phrase it in slightly different terms,
4263840	4266320	which is starting from physics, right?
4267280	4274000	I think most physicists and scientists believe in weak emergence.
4274000	4276160	Well, could I, could I add that Sabine Hossenfelder
4276160	4278880	had a paper and she frames it with this idea
4278880	4281440	of the resolution of physical theories.
4281440	4283360	So like, like a lower resolution theory
4283360	4285760	as weakly emergent from a high resolution theory.
4285760	4286400	Well, exactly.
4286400	4288000	And, you know, like, I like Sabine,
4288000	4289680	but this is not her idea, right?
4289680	4291760	This far predates all of us here, right?
4291760	4292320	Yeah.
4292320	4294560	And again, it's, it's a very interesting history
4294560	4296000	and a very important concept.
4296000	4300960	Now, so my point was that I think few people have a quarrel
4300960	4302400	with the notion of weak emergence
4302400	4303920	in the sense that, you know,
4303920	4305360	I can give you a theory of everything
4305360	4307040	in the form of whatever string theory
4307040	4308640	let's take a candidate, right?
4308640	4310880	But no string theory claims that that's a theory
4310880	4312800	of everything in the sense that like now,
4312800	4314960	to study biology or psychology or sociology,
4314960	4316080	you should just study string theory.
4316080	4317600	No one believes that, right?
4317600	4319440	There's actually interesting things to be said there,
4319440	4321120	but, but let's not, let's look at,
4321120	4322880	let's not go there for a second, right?
4322880	4326560	There are these levels that emerge weakly
4326560	4329760	in the sense that they are determined by the lower levels.
4330400	4331840	They're just so much more complex
4331840	4334000	that you're better off focusing on the menu.
4334000	4335520	Now there's this other notion which to me
4335520	4337360	is the really interesting one,
4337360	4340480	which is that there is, there are phenomena
4340480	4342000	that are at the higher levels
4342000	4346160	that are just not reducible to the lower levels, right?
4346160	4347840	So the true emergent is in some sense
4347840	4349440	is someone who believes the latter.
4349440	4350640	And now you can ask the question,
4350640	4353040	like do you believe in that or not, right?
4353040	4357360	And I think to give the very short answer first
4357360	4359920	is that ultimately there's probably no way of knowing.
4361120	4364960	But pragmatically, you're actually probably better off
4364960	4367600	treating the world as if it has strong emergence.
4367600	4369280	And now strong emergence is actually
4369280	4370960	a very strong segment to make is to say,
4370960	4373200	and by the way, going down to the lowest levels
4373200	4374560	to make things very clear,
4374560	4376640	you don't need to think about biology or society
4376640	4378480	or consciousness or anything.
4378480	4380720	Condensed metaphysics, right?
4381280	4383360	The particle physicists tend to believe
4383360	4386400	that what they do is what everything reduces to.
4386400	4388800	You talk to the condensed metaphysicists.
4388800	4390240	This was actually an interesting discussion
4390240	4392000	that I had with Scott, you know, Aronson,
4392000	4393520	because like he was very much on the,
4393520	4395040	we're both computer scientists,
4395040	4397120	but he was very much on the side of the particle physicists,
4397120	4400080	I don't know very much on the side of the condensed metaphysicists.
4400080	4402560	What they will tell you over and over again,
4402560	4406080	they see is things that you cannot explain
4406080	4407680	using quantum mechanics.
4407680	4409200	And now people say like,
4409200	4411440	oh, but you can always explain things in quantum mechanics.
4411440	4413120	You just haven't done the calculations.
4413120	4416320	But the point is precisely that you can't do the calculations, right?
4416320	4417840	The calculations are chaotic.
4418640	4421280	I have a theory, I can come up with 500 theories
4421280	4423840	of these phenomena and semiconductors and whatnot.
4423840	4425840	And like, I never actually get to test them
4425840	4428560	because the computations diverge before I get to test them.
4428560	4432640	So for all intents and purposes, it is strong emergence.
4432640	4435520	Whether truly that came from below is unanswerable
4435520	4437280	because you can't compute the predictions.
4437280	4438320	Well, we spoke about that.
4438320	4441120	So I think Keith would call that semi-strong emergence,
4441120	4443360	which is like, you know, whether it's computationally reachable
4443360	4446400	from the lower resolution to the high resolution
4446400	4447760	to the lower resolution.
4447760	4451120	But no, Sabine in her paper, A Case for Strong Emergence,
4451120	4452640	she was talking about singularities
4452640	4456080	as being a really good example of what might be strong emergence.
4456080	4459440	And the philosopher Mark Badau, I think, said that
4459440	4461360	strong emergence is ridiculous.
4461360	4463760	It's basically an affront on physicalism.
4464640	4467600	Well, certainly, you know, strong emergence and physicalism,
4467600	4469440	or let's just call it reductionism, right?
4469440	4470160	Reductionism, yeah.
4470160	4473040	Strong emergence and reductionism are incompatible.
4473040	4473680	Yeah.
4473680	4477520	And we scientists tend to be reductionists, right?
4477520	4481040	Now, at some level, I'm both a reductionist
4481040	4483280	and someone who is willing to believe in strong emergence.
4483280	4485840	Again, I don't believe in strong emergence.
4485840	4489440	I just don't see a way to disprove it, right?
4489440	4492320	And like, you know, if there's an empirical way
4492320	4494800	to distinguish semi-strong from strong emergence,
4494800	4496800	I'd be very interested to know what it is.
4496800	4499040	But now, I think the thing that is very important
4499040	4501440	that a lot of people, including a lot of physicists
4501440	4505760	and scientists don't see is that we have this hypothesis
4505760	4508080	that everything can be reduced to the laws of physics
4508080	4508960	as we know it.
4508960	4512080	We should not forget that it's just a hypothesis.
4512080	4513760	And it's a hypothesis that, again,
4513840	4515600	counter to a lot of people's say,
4515600	4518720	is very, very, very, very far from established.
4518720	4520800	And usually, people say like, oh, but, you know,
4520800	4524000	look at all the successes of the laws of physics and blah, blah.
4524000	4524960	And then I say, like, you know,
4524960	4526560	putting on my machine learning hat,
4526560	4530000	the sample that you've used to validate the laws of physics
4530000	4536160	is extraordinarily biased in the direction of simple systems.
4536160	4539360	OK, so you can't make this claim of if the data was IID,
4539360	4541320	I could say with great confidence,
4541320	4542800	these laws apply universally.
4542800	4543920	But I haven't done it.
4543920	4547280	It's more like I've just landed in a new continent
4547280	4549200	and I've sealed up all the rivers.
4549200	4551760	And I say, I know what this continent looks like.
4551760	4553200	You've never climbed the mountains.
4553200	4554560	You've never gone in the jungle.
4554560	4556640	So like this notion that the laws of physics
4556640	4558640	capture everything about daily life,
4558640	4560320	we just don't know how exactly.
4560320	4562160	Maybe it's true, but it could also
4562160	4563600	equally well be completely false.
4564560	4564880	Brilliant.
4564880	4566880	Well, you gave a bit of a hint to this earlier, actually,
4566880	4569200	because you used the word relationism, right?
4569200	4570240	Which is basically the...
4570240	4571600	Or relationalism.
4571600	4572000	Relationalism.
4572080	4574000	Maybe it should be shortened to relationalism.
4574000	4574800	Relationalism.
4575680	4578720	But yeah, I think Rosen is a great advocate of this,
4578720	4582240	and he has a whole category theory calculus
4582240	4584480	for describing living systems.
4585040	4586880	And also we spoke to Bob Koek,
4586880	4589760	the quantum physics professor from Cambridge,
4589760	4592720	and he was talking about this concept of Cartesian togetherness,
4592720	4594400	which is another category or framework.
4594400	4597280	But I just wondered, does that inform your view?
4598160	4601200	Well, relationalism, at least in one way
4601200	4604240	of defining the term, very much informs my view, right?
4604240	4606560	And one way to come at this is to say,
4607200	4609680	the world is not made of independent entities.
4609680	4611040	Actually, let's just start with machine learning,
4611040	4612800	which is a very concrete way to look at this.
4613360	4615840	A very large part, maybe even the largest part
4615840	4617680	of my research in the last 20 years,
4617680	4621840	has been to do away with the assumption of IID data, right?
4621840	4623840	That the world is made of independent entities,
4623840	4626560	in particular, society is made of independent agents,
4626560	4627840	et cetera, et cetera, right?
4627840	4629600	Now, we make this assumption,
4629600	4632400	both as human beings, to some extent,
4632400	4634400	and certainly very much so in science,
4634400	4635840	because it makes life easier.
4636720	4639680	The math is way, way, way easier when you assume independence.
4639680	4642880	But it's a blatantly false assumption, right?
4642880	4644320	Unfortunately, a lot of, for example,
4644320	4647600	economics prominently has embedded in it this notion
4647600	4649680	that the world is a bunch of independent agents,
4649680	4651520	and it just doesn't work like that.
4651520	4654720	And moreover, it's a distinction that is full of consequences.
4654720	4658400	A society and economy is a network of agents,
4658400	4660960	and almost all the action is in their interactions.
4661680	4663760	Until you really start taking that seriously,
4663760	4665040	you really don't understand the world.
4665040	4667440	Again, I have no quarrel with classic economics
4667440	4668640	as a first approximation.
4668640	4670160	It's exactly what it should be, right?
4670720	4671760	But then, and by the way,
4671760	4673440	you should also not just throw it away and say,
4673440	4675280	like, oh, this is garbage, like some people say.
4675280	4676400	You have to go the next stage.
4676400	4678720	It's actually now we have the mathematical
4678720	4680320	and computational tools to do,
4680320	4683280	and understand it as being a system of interacting agents.
4683840	4685840	And all of the questions that we are talking about,
4685840	4690080	including in evolution, even in physics, right?
4691200	4694720	A piece of condensed matter is a network of interacting,
4694720	4696240	spins, et cetera, et cetera, you name it.
4696240	4698800	So the relations are at the heart of it,
4698800	4700560	and moreover, like as I said,
4700560	4703440	a lot of my work is we now have the representations,
4703440	4705440	the learning inference algorithms to handle things
4705440	4707520	that are big piles of relations,
4707520	4710080	and the whole world is better understood in those terms,
4710080	4712080	and we just need people to catch up with that.
4712800	4714240	You know, once you do that,
4714240	4716080	you get into things that can easily be
4716080	4718000	computational, intractable, and so on and so forth.
4718000	4720160	But there's a lot of things that we can do there
4720160	4721440	and a lot more that we'll do.
4721440	4724160	So at this level, I think relationalism
4724160	4726560	is really should be a cornerstone
4726560	4728160	of our understanding of the world
4728160	4729920	in a way that it hasn't been in the past.
4730480	4733600	Okay, and which existing complexity science brings to mind?
4733600	4737440	But I mean, which existing techniques and areas
4737440	4740560	can folks look into to take that on board?
4740640	4742400	Well, you know, Markov logic,
4742400	4745120	which is what I developed for this purpose essentially,
4745120	4747280	and I do think, you know,
4747280	4748800	this is my talking about my work,
4748800	4750320	so you should naturally be suspicious,
4750320	4752960	but I think it's the best that we have,
4752960	4755120	and I think by a wide measure,
4755120	4757120	compared to anything else that we have so far.
4757120	4758560	Okay, and can you sketch it out?
4758560	4762880	Yeah, so to sketch it out in the simplest terms, right,
4762880	4766240	we want to combine all the traditional goodies
4766240	4768400	that we have from assuming the world is IID
4768400	4770480	with the power to model, you know, relationships,
4770480	4772640	there are themselves potentially very complicated.
4772640	4774720	The way we do the Markov logic is,
4774720	4775840	there's the logical part.
4776640	4779840	We actually do not need to solve a new,
4779840	4781680	the problem of how to represent
4781680	4783120	and do inference with relations.
4783120	4785200	We have first order logic for that.
4785760	4788240	First order logic is the language of relations.
4788240	4790320	That's actually the term that is used, right,
4790320	4792080	and how the relations depend predicates.
4792080	4793280	Sometimes they're called predicates,
4793280	4795440	but let's just call them relations, right?
4795440	4797760	We have a formal language to talk about relations.
4797760	4800080	And by the way, essentially all of computer science
4800080	4800960	can be reduced to that.
4800960	4803120	You give me your favorite, you know, whatever,
4803120	4806400	knowledge representation, data structure, et cetera, et cetera.
4806400	4809680	And I, anybody who knows can immediately say
4809680	4810720	how to do that in logic.
4810720	4811920	So that's one part.
4811920	4814720	The other part is the statistical, you know,
4814720	4817280	machine learning probabilistic aspect of the world, right?
4817280	4820080	And then again, going all the way back to physics, right?
4820800	4822160	All of these things that we deal with
4822160	4824160	are essentially special cases of what are
4824160	4826000	variously called Markov networks,
4826000	4828080	which is where the name Markov comes from.
4828080	4830960	Or graphical models, or log linear models,
4830960	4834160	Gibbs distributions, Boltzmann machines, right?
4834160	4836480	All of these things are essentially the same, right?
4836480	4839440	That whole neck of the woods is captured by Markov networks.
4839440	4840240	Let's call them that.
4841200	4843840	And Markov logic is combining Markov networks
4843840	4845920	with first order logic in a single language,
4845920	4847840	which you can now do everything with.
4847840	4848480	Okay, okay.
4848480	4850960	So just to like push back a tiny bit.
4850960	4853760	So in the past, we've tried to create,
4854720	4856000	let's say things like psych,
4856000	4858720	which is a knowledge representation of the world.
4858720	4861120	Folks like Montague have tried to do semantics
4861120	4863760	using first order logic to set some,
4863760	4865760	you know, varying degrees of success.
4865760	4867120	And then we have the grounding problem
4867120	4869440	we were just talking before about, you know,
4869440	4870480	like even Searle said this,
4870480	4873840	that you have kind of epistemic objectivity
4873840	4876320	and subjectivity and some things are observer relative,
4876320	4878560	like even economics is observer relative.
4878560	4882080	So with this kind of formalism, how would that work?
4882080	4883280	That's so very good.
4883280	4885600	The problem with or the main problem
4885600	4887520	with a lot of these things that you mentioned,
4887520	4889920	like, you know, certain types of semantics and whatnot,
4889920	4892240	that are based essentially on first order logic, right?
4892240	4893360	Is that they're too brittle.
4894000	4897600	In fact, the problem with symbolic AI is that it's too brittle.
4898160	4901040	And this is exactly what Markov logic fixes.
4901040	4903440	It fixes it by making it statistical.
4903440	4905280	When I give you a logical statement now,
4905280	4907440	I'm no longer, for example, simple logical statement,
4908240	4910960	you know, a smoking causes cancer, right?
4911680	4913520	In English, this is a valid statement.
4913520	4914880	Smoking does cause cancer.
4914880	4916640	But actually, once you translate it to logic
4916640	4919280	for every x, smokes of x implies cancer of x,
4919280	4922320	it's false because some smokers don't get cancer, right?
4922320	4924160	What this really was meant to be all along
4924160	4926240	is a statistical statement that says,
4926240	4928240	smokers are more likely to get cancer.
4929120	4931520	So the way we overcome a lot of those problems
4931520	4934560	is precisely that we take all of this logic,
4934560	4935920	which again, the language exists,
4935920	4938480	we don't have to change it, we can, but we don't have to.
4938480	4939920	And we make it statistical.
4940480	4942800	As a result of which, it's no longer brittle.
4942800	4944240	Or at least now it's only as brittle
4944240	4946320	as machine learning and graphical model or not.
4946320	4949440	It's not as brittle as, you know, traditional symbolic AI.
4949440	4951520	Okay. And so we speak into a lot of Go-Fi people
4951520	4953680	and I mean, Wally Subba, for example, he's a rationalist
4953680	4955040	and what's interesting about the rationalist
4955040	4957920	is they hate any form of uncertainty, right?
4957920	4959360	They think in absolute binaries.
4959360	4960400	You either know it or you don't.
4960400	4962160	No, I mean, let me push back on that.
4962160	4964480	There's this, again, you need to distinguish, you know,
4964480	4967760	a general field or idea from its subtypes, right?
4967760	4969680	There is a type of rationalist.
4969760	4973280	That hits uncertainty, big mistake, big, big mistake.
4973280	4975200	There's a type of rationalist that, you know,
4975200	4977200	uncertainty is what they, you know, like,
4978720	4981840	an uncertainty calculus is a type of rationalism.
4981840	4984640	And some of the best, you know, AI, philosophy, etc.
4984640	4985280	is just that.
4985280	4987920	So there is no incompatibility at all
4987920	4989520	between rationalism and uncertainty.
4989520	4993600	In fact, if rationalism, if being rational is maximizing
4993600	4997120	expected utility, notice the expected in there, right?
4997120	5000160	You cannot be rational if you ignore the uncertainty.
5000160	5000800	Interesting.
5000800	5003680	Okay, but then what about the resolution of modeling?
5003680	5005680	I mean, smoking is a really good one.
5005680	5008320	So us humans, we anthropomorphize things.
5008320	5010640	We understand the world in macroscopic terms
5010640	5013360	using macroscopic ideas that we understand.
5013360	5015760	And that kind of leads to a certain type of modeling.
5015760	5017840	And that modeling presumably would be represented
5017840	5020320	at that resolution, you know, using this formalism.
5021360	5022800	Sure. And what's the question?
5022800	5026160	Well, it seemed, again, like I'm intuitively suspicious
5026160	5028480	that we were just saying the world is a complex place.
5028480	5030960	And with a lot of causal modeling, for example,
5030960	5033440	a lot of the art is understanding what is relevant
5033440	5034880	and what is not relevant.
5034880	5038080	What is relevant might just be kind of, you know, relevant to us.
5038640	5041120	No, well, what is relevant is what is relevant
5041120	5042800	relative to your utility function.
5043520	5046480	Okay, again, it gets back to that precisely.
5047680	5049920	The whole problem is that the world is infinitely complex
5049920	5051920	and we have only finite computational resources,
5051920	5054080	whether it's in our brains or our computers or whatever, right?
5054080	5055280	So now what do you do, right?
5055360	5058160	You are forced to oversimplify the world,
5058160	5060800	not just simplify, but oversimplify, right?
5060800	5063200	But now the whole art, that's actually a good word to use,
5063200	5064480	even if it's done with computers,
5064480	5068960	is how do you not only oversimplify as little as you can,
5069520	5075200	but pick out the simplifications that are least harmful to your objective.
5075200	5077520	By the way, the art of the physicist,
5077520	5080960	physicist would tell you, is precisely doing this, right?
5080960	5083840	Physicists are very good at deciding what to simplify.
5083840	5085600	And in fact, almost, I think at some level,
5085600	5087760	almost any good scientist, this is what they do, right?
5087760	5091680	So, and now how do I decide what and how to simplify
5091680	5094640	is by relevance to my utility function, right?
5095280	5097280	I want to ignore parts of the world
5097280	5100800	that do not affect my utility function, number one, right?
5100800	5103040	And for example, the notion of conditional independence,
5103040	5105440	which is the foundation of graphical models,
5105440	5106720	that's what the whole idea is.
5106720	5108640	It's like, once I know these things,
5108640	5110320	I don't have to know about those others.
5110320	5112080	Thank God, right?
5112080	5114080	Okay, but if Ken Stanley was here,
5114080	5116640	he says that the great thing about evolution is it's divergent,
5116640	5119680	it's not convergent, it's discovering new information.
5119680	5121600	And my worry is with a system like this,
5121600	5123920	with any form of anthropomorphic design,
5123920	5125840	would inevitably become convergent.
5125840	5128640	And it might look like, oh, those things over there
5128640	5129840	that we're ignoring don't matter,
5129840	5131600	but actually they might really matter
5131600	5132880	if they got introduced into the utility.
5132880	5138800	Well, I wouldn't say that maximizing expected utility is anthropomorphic, right?
5138800	5140080	In fact, it's one of the least...
5141040	5143680	I think maybe there's some degree of anthropomorphism
5143680	5145120	is almost anything we do,
5145120	5149040	and the progress of science is becoming less and less anthropomorphic,
5149040	5150800	and we should keep pushing on that.
5150800	5153760	But I would say that maximizing expected utility
5153760	5156480	is one of the least anthropomorphic things we can do.
5156480	5158240	Well, this is actually a really interesting point,
5158240	5160720	because one of the key tenets of the rationalist movement
5160720	5162000	and their conception of intelligence,
5162000	5165200	because all of the other definitions of intelligence
5165200	5166880	are anthropomorphic.
5166880	5169600	So, you know, there's based on behavior, capability, AI,
5170320	5175280	principal function is a big one, you know, from Norvig.
5175280	5178160	And this is the principal based AI,
5178160	5181280	which is just making rational moves.
5181280	5186160	So why is there such a push to be as, you know,
5186160	5189840	to be as the least amount anthropomorphic?
5190480	5194000	Oh, the push is not to be, at least in my view,
5194480	5196960	being less anthropomorphic is not a goal.
5197600	5199040	That's not the goal.
5199040	5202720	The goal is to be as accurate and complete as we can
5202720	5204880	in modeling the world, right?
5204880	5207680	We're just trying to understand the world better, right?
5208320	5210400	For whatever purpose, maybe for its own sake,
5210400	5214080	maybe for the purpose of the utility and the evolution and so on, right?
5214080	5214960	But that's the goal.
5214960	5217040	The problem is that,
5217040	5219600	and this has been the problem since they won, right?
5219600	5221200	They won of humanity,
5221200	5223840	is that because we anthropomorphize the world,
5223840	5227360	that gets in the way of understanding how it really works, right?
5227360	5230560	If I say the wind is some God blowing, right?
5230560	5231600	I understand, right?
5231600	5233040	That's all they could think of.
5233040	5235600	But it's a big obstacle to understanding what the wind really is,
5235600	5238560	like there's a pressure difference, et cetera, et cetera, right?
5238560	5241440	And we've done away with a lot of anthropomorphism.
5241440	5243200	By the way, one of the problems that we're always having
5243200	5245760	is that it's always pushing back, right?
5245760	5247120	You know, there's always, you know, again,
5247120	5250560	intuitively we have a very strong tendency to anthropomorphism,
5251040	5253600	as much as science broadly construed as a great victory,
5253600	5255360	it's always in danger from this, right?
5255360	5257200	But even within science,
5257200	5262160	we've gone from doing away with the obvious forms of anthropomorphism
5262160	5263600	and anthropomorphism
5263600	5266400	to having many things that are still there
5266400	5270480	that are less obviously anthropomorphic, but still are, right?
5270480	5273120	But if there's something anthropomorphic that's actually is accurate,
5273120	5274480	then more power to it.
5274480	5275120	Interesting, yeah.
5275120	5277920	And then I guess we have so many cognitive priors, right?
5278160	5280480	In our brains that give us a cone of attention,
5280480	5283280	which is completely anthropocentric.
5283280	5285120	Well, very good.
5285120	5289040	So those priors, and maybe a better term is heuristics, right?
5289040	5293040	Our brains are full of heuristics that evolution put there
5293040	5296240	for a good reason, because those heuristics work, right?
5296240	5297840	But they are heuristics.
5297840	5299760	So they have failure modes, right?
5299760	5304160	And we need to understand what is that those heuristics really are getting at
5304160	5307760	so that we also, so that we use them when they're good.
5307760	5309760	But then when they're not good, we use something else.
5310640	5311920	Brilliant, brilliant.
5311920	5314000	So Pedro, we're here at NeurIPS this week.
5314000	5318080	And could you just like sketch out some of the some of the things you've seen?
5318080	5322640	And I also know that you're a huge fan in that there's a Neurosymbolic algorithm
5322640	5323680	that you want to tell us about.
5323680	5325360	So let's let's hear it.
5325360	5329440	So I indeed, I've been enjoying NeurIPS this week.
5329440	5334640	One of the big things in AI in the last several years has been Neurosymbolic AI,
5335600	5340240	which you probably will not surprise by the fact that I very much believe in.
5340240	5342960	So and I believe this since I was a grad student
5342960	5346640	and the whole idea of Neurosymbolic AI was something that nobody was interested in, right?
5346640	5349360	And now suddenly everybody is, which I think is a good development.
5349360	5353280	And this is the idea that if we really want to solve AI by some definite,
5353280	5358800	if we want to get to like human level intelligence, etc, etc, we need to have both,
5360320	5364160	you know, like, for example, deep learning is not enough, right?
5364160	5367920	There are symbolic reasoning capabilities that we have and that are essential.
5368640	5369840	And we need to get them.
5369840	5373280	And I think, you know, intelligent connection is like, I don't know,
5373280	5376960	Yoshua Ben-Joe, you know, Yanle Kunase, they don't disagree with this.
5377520	5381120	But one way to look at this is say, we're just going to realize those,
5381120	5384880	you know, capabilities using purely connectionist means, right?
5385520	5388160	And what I see happening in that direction,
5388160	5390480	unfortunately, is a lot of reinventing the wheel.
5390480	5395680	So I do think, you know, symbolic AI got wedged for some reasons, including brittleness.
5396800	5399920	And, you know, and we have learned from that at the same time,
5399920	5404320	they did discover and understand a lot of things that are extremely relevant.
5404320	5406880	So it's just not good science to ignore it.
5406880	5412960	So I'm working on an approach to combine, you know, symbolic AI with deep learning.
5412960	5417440	Again, this is a popular exercise, there are many interesting approaches out there.
5417520	5418960	As much as I sympathize with them,
5418960	5420880	I think they're all very far from solving the problem.
5420880	5424080	They are over complicated and not powerful enough.
5424080	5426960	So, you know, I've been working on an approach called TensorFlow logic
5426960	5433200	that I do believe is as simple as it can be and as general as it can or needs to be.
5433760	5438240	And this, you know, it really is a deep unification of the two things
5438240	5442320	in the sense that it's not just that you combine them using, you know,
5442320	5445040	a neural model that causes symbolic one or vice versa,
5445040	5447520	which is a lot of what these things that you have today do.
5447520	5451680	And a lot of the claims that like, oh, this system is neuro symbolic, which it is.
5451680	5455600	It's like, you know, AlphaGo is neuro symbolic because some of what it does is symbolic.
5455600	5460960	But I'm talking about something much deeper, which is once you start doing AI,
5460960	5463440	learning inference representation in TensorFlow logic,
5463440	5468720	there's just no distinction between symbolic and neural at all anymore.
5469840	5470960	Can you explain that?
5470960	5476000	So TensorFlow logic, I'm just inferring from that that the primary representation
5476000	5478640	of substrate is a continuous vector space. Is that right?
5478640	5481200	Are you encoding discrete information into the vector space?
5481200	5483280	So it's a vector space. Yeah.
5483280	5488080	Right. In fact, this was the original term that we had for this was vector space logic.
5488080	5490880	But then we changed it to TensorFlow logic because it's much more appropriate.
5490880	5494960	But it's it's vector space in the abstract algebra sense of vector space,
5494960	5497200	not in the traditional, you know, vectors of numbers.
5497760	5506000	But anyway, so as the name implies, right, TensorFlow logic is a combination or unification
5506000	5510800	of tensor algebra on the one hand and logic programming on the other.
5510800	5515200	So is it similar because Bob Koeck had a similar idea using like tensor outer products?
5516000	5517040	Is it that kind of?
5517040	5519520	It's related, but I think it goes well beyond.
5519520	5520160	Okay.
5520160	5523120	And the basic idea is actually pretty simple.
5523120	5528160	And it's just the following, right, without going into too much, you know, technical detail.
5529200	5532320	All of deep learning can be done using tensor algebra.
5532320	5536320	Yeah, you know, plus univariate nonlinearities.
5536320	5538880	Right. So we've got the tensor algebra to do that.
5538880	5541680	All of symbolic AI can be done using logic programming.
5541680	5543920	And moreover, it has been done using logic program.
5543920	5547120	So if you can unify these two things, this part of the job is done.
5547120	5552720	Right. And as it turns out, you can unify them shockingly easily because a tensor
5553200	5559120	so tensor algebra is operating on tensors, you know, in that logic, so logic programming,
5559120	5563840	and then for learning in that logic program and symbolic AI, they are all operating on relations.
5563840	5564160	Yeah.
5564160	5567680	Right. So what is the relationship between the tensor and the relation?
5567680	5572560	Right. A relation is just this and efficiently represented sparse Boolean tensor.
5573520	5577200	So at this point, we actually know that the foundation of these two things is actually the
5577200	5582480	same. If your tensor is Boolean and is very sparse, now I'm better off representing it with
5582480	5586000	a relation, but at a certain level of abstraction, nothing has changed.
5586000	5591280	Right. So by this prism, you can look at logic programming and logic programming is doing tensor
5591280	5591760	algebra.
5592720	5598000	Okay. Just help me understand this a little bit. So, you know, the main criticism of using a neural
5598000	5604480	network as a combined computational and memory substrate is that it's a finite state automator.
5604480	5609200	So without having the augmented memory like a Turing machine, you can't represent infinite
5609200	5612560	objects. That's the main reason the symbol is, you know, that's the main argument I used.
5612560	5615520	So wouldn't that argument still be leveled against you?
5615520	5620560	Well, no, because I'm glad you brought that up because there is a very common misconception.
5620560	5624800	If you realize that there is no such thing as infinity, right? And in particular, there is no
5624800	5630960	such thing as an infinite memory. That problem doesn't arise. So there's the, so the, unfortunately,
5630960	5636640	a lot of theorists, including computer theorists, they foster this misconception, right?
5636640	5641680	There's the Chomsky hierarchy, right? With finite automata at the bottom and Turing complete,
5641680	5646000	you know, Turing machines bubble at the top, right? If your Turing machine has only a finite
5646000	5651920	tape, it's a finite automata. So everything is just finite automata. Let's get that out of the way,
5651920	5656160	right? A lot of what people do is like completely mistaken because of that. Now,
5656880	5661360	the fact that everything is finite automata does not mean that everything is equally good.
5661360	5667040	Some representations are far more efficient, compact, etc., etc., for certain purposes than
5667040	5671600	others. And the whole game here is that like, I'm not going to solve a finite automata. The question
5671600	5676000	is like, what do I need to do? Not because I need to go to a higher level of Chomsky hierarchy,
5676000	5681840	because in reality, they don't exist. But because, you know, I mean, if you have infinite resources,
5681840	5686000	you could solve a gap with a lookup table. But would you, would you not? I mean, for example,
5686000	5689280	there was this DeepMind paper that mapped architectures to different levels of the
5689280	5695600	Chomsky hierarchy transformers, I think were, you know, FSAs, RNNs actually were one step higher.
5695600	5699200	They could represent regular languages and they got context-free languages. I mean,
5699200	5702640	do you think there's any meaningful distinction between those language levels?
5702640	5707120	As I said, there is a meaningful distinction, but it's not the distinction that people usually make,
5707120	5712160	because once you, I mean, you can debate whether the universe is finite, but certainly computers
5712160	5716000	are finite. So as far as anything that you're going to run on a computer, there truly is no
5716000	5721120	distinction at this theoretical level between a Turing machine and a finite automata. That does,
5721120	5725520	so like, I can reduce and people have, there are papers reducing, you know, any of these things to
5725520	5730320	any of the others, right? It's like it's a fairly trivial exercise. So at that level, those distinctions
5730320	5735040	are completely meaningless. However, they are meaningful in the sense that for many purposes,
5735040	5740320	I am better off having an RNN than having, you know, a transformer. And for many purposes,
5740320	5744960	I'm better off. So like, let's take, you know, propositional logic versus first-order logic,
5744960	5750080	right? If there's no such thing as infinity, first-order logic is reducible to propositional
5750080	5755280	logic. But that does not mean that it's useless because it can represent a lot of things exponentially
5755280	5760160	more compactly than propositional logic. If I want to represent the rules of chess in first-order
5760160	5765040	logic, it's a page, right? If I want to represent them in propositional logic, it's more pages that
5765040	5768480	you can have. Okay, well, I think that that's a very, very good point. But I mean, just, just
5768480	5771760	a devil's advocate from the psychologist, you know, do you remember that, that photo,
5771760	5776640	Felician Connectionism critique paper, arguing productivity and systematicity? Productivity
5776640	5780000	is all about the infinite cardinality of language. I mean, presumably you would agree that language
5780000	5784160	has an infinite cardinality. No, well, again, another instance of the same problem. Productivity
5784160	5789280	is very important. But the point to just be a little precisely for a second is, is to be able to
5789280	5796320	generate a vast number of things beyond the ones that you started with. Vast, not infinite. In fact,
5796320	5803360	mathematically, infinity is not a number. Infinity is just a shorthand for something that is so
5803360	5809360	large that it doesn't matter how large it is. Okay. I mean, at the end of the day, I'm not a
5809360	5813760	mathematician, but surely mathematicians would push back on this because, you know, infinity is,
5813760	5820800	is a quantity in mathematics. No, I mean, again, people in every field, mathematicians, physicists,
5820800	5826800	computer scientists are all are often guilty of they, they, they, they have this notational shorthand
5826800	5832080	or like, you know, this terminological shorthand that serves them well. But then they, and then
5832080	5836720	they use that and then the newer generations come along and the, and the public also, right,
5836720	5841040	they don't even realize that what's being talked about is a little bit different.
5841040	5845920	Infinity is a perfect example. Any serious mathematician will tell you that infinity does
5845920	5850080	not have the properties of a number. So for example, if I multiply infinity by 2, I still
5850080	5856000	get infinity. There's no number that that happens to, right? So infinity is, is not a number, right?
5856000	5860320	When I say infinity is not a number, mathematicians might quibble about the way I'm stating it,
5860320	5865440	but this is a, this is a mathematical truth, right? Infinity truly isn't, I'm being colloquial,
5865440	5868880	of course, when I say that it's a shorthand for something that is so large that it doesn't
5868880	5873680	matter how large it is. When you take limits, you know, in calculus in anything and the limit of
5873680	5878320	this blah, blah, as I go to infinity, this is exactly what I'm doing. I'm going to the point
5878320	5882160	where I'm saying like, at this point, it doesn't matter how large the number is, the result will
5882160	5888800	be the same. And in this way, infinitely is an extraordinarily useful concept. So I'm not here
5888800	5892640	to rail against infinity. I'm just saying like, we really need to understand, I mean, like, let me
5892640	5898960	give you a very banal example, right? From the point of view of, you know, what to have for lunch,
5898960	5905600	right? Because some things cost more than others. Elon Musk is infinitely rich. He does not have
5905600	5910480	infinite money. But it makes no difference whatsoever whether he has whatever 100 billion
5910480	5916080	or 200 billion to what he's going to have for lunch. You know, like a street person who has
5916080	5921360	$5 to them, like their fortune is not infinite, because it very much matters what lunch costs,
5921360	5925680	right? So this is the real sense of infinity, which we can and should use, but we shouldn't
5925680	5931200	confuse it with like, oh, but then your, you know, like your formalism is incomplete because it
5931280	5935760	doesn't encompass infinity. Yeah, it doesn't need to infinity doesn't exist.
5935760	5939440	Okay, okay. Well, let's come at it from the other from the composition, you know,
5939440	5946000	compositionality and systematicity. So that's all about being able to do, you know, like their main
5946000	5951520	argument was when you have a symbolic representation, you can kind of reuse the previous representations
5951520	5957040	downstream composition, compositionally. And when you take a discrete symbolic representation,
5957040	5961520	and you kind of encode it in the envelope of a vector space, you have a real problem doing that
5961520	5966320	because it's now like, it's irreversible that transformation, right? You can't go back to the
5966960	5972560	original variables. Well, it is reversible if you realize that all those real numbers are actually
5972560	5978800	finite. Right? So notice that real number, there's nothing less real than a real number,
5978800	5983280	real numbers are imaginary, right? Real numbers are numbers with infinite precision, which is
5983280	5988080	a monstrosity. And many people have said this, including mathematicians and physicists, right?
5988080	5992720	The notion of an infinite, of a number with an infinite number of digits is just monstrous.
5992720	5998720	And again, in particular on a computer, even if you use, you know, you know, like numbers with
5998720	6004560	unlimited floating point precision, right? It's limited by the size of your memory. So this transfer
6004560	6008240	from which is actually very important that again, that's what tensor logic largely is about,
6008320	6013520	from purely symbolic structures to embeddings in a vector space, right? That vector space
6013520	6017200	is still finite. So there's actually nothing irreversible about what happened there.
6017200	6021920	Interesting. Okay. So how can people, you know, find out more information about this? And can you
6021920	6026160	just sketch out, you know, just, just to bring it home to people where they could actually use it
6026160	6028880	and how it would be, you know, better than what they can currently do?
6028880	6032400	Right. The answer to the first question, unfortunately, is this is not published yet,
6032400	6037920	but hopefully it will be soon. So for the moment, there is no very good place to point people to
6038000	6045840	unfortunately, but that hopefully will be fixed soon. The question of where to apply it is,
6046480	6051680	our goal for this is that this should become the language or hope, I should say our hope,
6051680	6056560	is that this will become the language we're doing just about anything in AI. So for example,
6056560	6061120	if what you want to do is actually nothing symbolic, but you just want to build a convent,
6062000	6067760	you can express a convent incredibly elegantly in tensor logic. Like if you think of, for example,
6067760	6073920	tensor floor or PyTorch versus NumPy, right, they allow that thing to be said much more compactly,
6073920	6078880	compared to tensor logic, they are as bad as NumPy is compared to them. Right. Same thing on the
6078880	6082880	symbolic side. But of course, the real action comes in all the problems where you have both
6082880	6088640	components, the problem with all those problems, which ultimately is every problem in AI, right.
6088640	6092000	You're always like, what happens today that is very frustrating. And that's what we're trying to
6092000	6096480	overcome is like, you start from one of these sides these days, mainly the connectionist one,
6096560	6100640	which you have a good mastery of. And then the other side, for example, the symbolic one,
6100640	6105520	the knowledge representation, the reasoning, the composability, you just hack. Yeah. And your
6105520	6109760	hack solution is terrible. You're like, you're inventing the wheel, you're making it square,
6109760	6114240	you're trying to make it turn, but it's square, right. It's just, you know, it's a disaster.
6114240	6118560	And with tensor logic, you can actually have a very well founded, very well understood
6119200	6123360	basis on either side. So now you don't have to hack either side. Now there's, of course,
6123360	6126560	still things that you're going to have to hack at the end of the day, because at the end of the day,
6126560	6129840	you know, AI is intractable and things are heuristic. But this, you know, is,
6130960	6134320	you know, you know, this notion of a tradeoff that is very important in engineering. Like,
6134320	6139760	people have been exploring different points on this tradeoff curve. The point of tensor logic is
6139760	6147120	that whatever your application is, we're moving you to a better tradeoff curve. It's still a
6147120	6152480	tradeoff curve, but it dominates the old one. For any given X, you have a better Y and vice
6152480	6157360	versa. Okay. And just help me understand, because we'll move over to, you know, the discrete program
6157360	6161600	search and some of Josh Tannenbaum's work in a moment. But there are two schools of thought,
6161600	6165200	right? There's discrete first and there's continuous first, you're on the continuous
6165200	6170160	substrate. But usually the reason for the continuous substrate is stochastic gradient descent,
6170160	6174480	learnability, et cetera, et cetera. And like, help me understand. So are you saying we start
6174480	6178480	with symbolic representation and then we encode it into the envelope? So where does learning come
6178480	6183200	into it? No, very good. So in tensor logic, you can do broadly speaking, two kinds of learning.
6183200	6187760	You can learn the structure of these tensor equations, as we call them, using inductive logic
6187760	6192080	programming techniques. Again, that whole technology is there. And then once you have that, you can
6192080	6197040	learn the numbers by the back prop in particular ways called back propagation through structure,
6197040	6201120	because the structure can vary from example to example, but we know what the type parameters
6201120	6205680	are. So all of the machinery of inductive logic programming and all the machinery of gradient
6205760	6210400	descent and deep learning or not, they're both there available to be used as you traditionally
6210400	6215280	have. Okay, what if I made the argument, though, that it's almost like the inductive logic, you
6215280	6219440	know, like the program search, that's the hard bit. So if you've already got the program, why do I
6219440	6222800	then need to put it into a vector space? No, actually, these are also at the end of the day,
6222800	6226880	in machine learning, we're always trying to learn a program of some kind, right? The question is like,
6226880	6231600	what is the easiest way to do that? And precisely the problem with ILPS with symbolic logic is,
6231600	6235840	that's really a couple of problems. One is that if all that you do, you learn programs that are
6235840	6241440	too brittle, and we don't want them to be brittle, right? And the other one is that each type of
6241440	6248080	search has its limitations. So in particular, in symbolic AI, including ILP, we tend to use a lot
6248080	6253040	of combinatorial optimization types of search, right? What we in AI call search is discrete search.
6253040	6257120	And that is good in some ways, but also very limited in others. The same thing is true of gradient
6257120	6263840	descent, right? And now to go to that for just a second. Gradient descent is not a continuous
6263840	6270400	optimization algorithm. It's not, right? Again, those real numbers are not infinite precision.
6270400	6274960	There's actually nothing continuous going on in the computer. Gradient descent truly literally
6274960	6279360	rigorously mathematically is a discrete optimization algorithm. It takes discrete steps.
6280080	6284960	The assumption that gradient descent depends on, which is that the infinitesimally small updates
6284960	6290960	do not hold. And moreover, in machine learning, as a numerical analysis, we are constantly dealing
6290960	6295440	with this fact that there's a mismatch between our mathematical conceptual model of the space
6295440	6300160	that we're working with as continuous with the reality of the computer that is not continuous.
6300160	6306160	So now this is not, but gradient descent still is a different optimization technique
6306160	6311440	with some very important advantages, in particular the key, right? The power of gradient descent
6311440	6316400	comes from the fact that to move from my current point to a better one, I don't need to try out
6316400	6320480	all the neighboring points because that takes order of the time of the neighboring points.
6320480	6326160	I have a closed form way to compute what is the best one, right? And then I move there.
6326160	6329680	And this is absolutely brilliant, right? Like we don't want to let go of that, right? This is,
6329680	6334960	you know, Newton's enlightenment is bright idea, right? The price of that is that in order to do
6334960	6340880	that you have to make this approximation, which again, calculus is an approximation. It assumes
6340880	6346320	that certain effects are second order and can be ignored. Now, ironically, when you learn a large
6346320	6350800	deep network these days, you're actually in a regime where they cannot be ignored, right? Because
6350800	6356880	these infinitesimal changes are not that infinitesimal because you take a finite step, right? The gradient
6356880	6361200	descent is always taking finite steps, which is why it's a discrete algorithm. And once you take
6361200	6366240	that finite step for any reasonable learning rate, the total effect of the approximations that you've
6366240	6372560	made typically swamps the step that you're taking. So the assumption of calculus that
6372560	6377600	gradient descent is founded on is actually false. Now, in some ways this invalidates a lot of our
6377600	6383280	intuitions. In many ways, and again, this remains to be resolved, a lot of why gradient descent works
6383280	6387600	better than people expected to is in fact that it's doing something else. It's doing stochastic
6387600	6391920	search partly because of the SGD as opposed to being matched partly because of things like this.
6391920	6395520	Okay, well, this is really interesting. A couple of places we can go. But first of all, I remember
6395520	6401280	you did the paper and that introduced elements of NTK theory as well, which might be an argument
6401280	6405760	against the discreteness of the optimization. But also, I wanted to trade off the two types.
6405760	6407440	Well, why is there an argument against the discreteness?
6407440	6411040	Well, isn't there a, with NTK, isn't there like a closed form solution? Doesn't that kind of like
6411040	6416000	erode the discreteness of the optimization? No, I mean, so there's several things here. But like,
6416560	6421280	if you have a closed form solution, absolutely brilliantly go for it, right? There's nothing,
6421920	6426400	having a closed form solution in no implies that it's continuous or discrete or any other thing,
6426400	6432000	right? So, let's say there was a closed form solution and it was like an infinite kernel when
6432000	6436880	it represented some neural network, doesn't that erode the argument? Well, so first, okay, so first
6436880	6440320	of all, in the work that, so the work that I've done that I think you're referring to is like,
6440320	6444960	I have a proof that every model learned by gradient descent is a kernel machine.
6444960	6448640	Yeah, right. And it's something called the path kernel, which is the integral
6448640	6454320	of the neural tangent kernel over the overgraded descent, right? Yeah. And now the neural tangent
6454320	6459920	kernel does not assume that your network is infinite. Most of the theory that people have done
6459920	6465600	with it assumes that the network is infinitely wide, but the definition absolutely does not
6465600	6470320	require that. And none of what I do, and in fact, that's part of why, you know, of its part is that
6470320	6476160	it assumes no infinity of anything. It's for any architecture that you use, and in particular,
6476160	6480960	you know, finite architectures. Okay, interesting. Okay, so hence the discreteness, but can we come
6480960	6485360	back to this contrasting of the discrete program search and the, you know, stochastic gradient
6485360	6491200	descent on a vector space? Now, in the vector space, there are certain characteristics, you know,
6491200	6494880	there are certain symmetries, and even though it's a discrete search through the space, I would argue
6494880	6500160	that it's still continuous in nature, it has certain characteristics. So contrast those two
6500160	6505280	forms of optimization. Precisely so. Exactly. I mean, I think you've put your finger in now.
6505280	6509840	The whole point of these continuous spaces, right, is not that they're continuous, because again,
6509840	6515200	that's, that's a fiction, is that they have a certain locality structure, yeah, that you can
6515200	6521280	exploit to very good effect. And this is exactly what we're going to send us, right? Now, that
6521280	6526880	locality structure doesn't have to be infinitesimal, right? You don't need points to be infinitely close
6526880	6531040	for all this to apply approximately. And again, they never are, and it's always an approximation.
6531040	6537840	Now, the question is, do you want to make these locality assumptions or not, right? Making them
6537840	6542960	buys you certain things, right? But it's also potentially unrealistic in some ways, right? Now,
6542960	6548560	this actually, to take a very concrete instance of this, think of space, right? We model space in
6548560	6554720	science and physics and in anything as a continuous thing, which it is not, right? Which is not to
6554720	6559280	say that, and by the way, physicists are coming to this conclusion, right? These days, the prevailing
6559280	6563520	views is that it's from big thing, is that like, it's, you know, space arises from entanglement,
6563520	6568000	et cetera, et cetera, like space is not the fundamental reality, right? And now, I think
6568000	6572400	that where this is inevitably going one way or another is that we realize that space is discrete,
6572400	6578880	right? But, and this is key, it has certain properties, including symmetries like translations
6578880	6583760	in variance, rotation in variance, et cetera, et cetera, that whole, approximately or exactly,
6583760	6590400	but if those hold a whole bunch of things like that, then you have, you know, your latent variable
6590400	6595600	structure, right, is very well approximated by our notion of continuous space, in which case,
6595600	6600160	it would be foolish to not use it, right? To formulate the laws of physics and to do computer
6600160	6606000	vision and so on and so forth. But at the same time, right, if we believe in it too literally,
6606000	6610560	we walk ourselves into a blind alley. So concretely, look at computer vision, right?
6610560	6614240	People in the universities of computer vision started out trying to do it with differential
6614240	6619280	equations and Fourier analysis and all of that could continue with stuff, right? Because that
6619280	6624880	was the obvious thing to do, right? And it failed. That doesn't work. That's why we need things like
6624880	6629760	deep learning and, you know, Markov random fields that are discrete grids that use, you know,
6629760	6634080	to model the images and whatnot, because you are, along with the approximate continuity,
6634080	6639360	you also often have large discontinuities. And if you can only model the world continuously,
6639920	6643360	you don't know what to do. And the problem precisely is that you have all these phenomena
6643360	6647760	that are like this, including, you know, in vision, but also in, in turbulence and condensed
6647760	6652800	metaphysics and so on, you've got to realize that there are discontinues and not try to shoehorn
6652800	6657360	them into continuity when that's no longer appropriate. Interesting. Okay. Well, can we bring
6657360	6663680	in ILP and can you contrast like the kind of function spaces that are learnable in both methods?
6663680	6669680	Yeah. So ILP, so let me actually preface this with the following. People in every one of these
6669680	6676880	schools of AI tend to have this view that I can represent everything in the world using my approach.
6677680	6682640	So I can like, look, prologue is too incomplete. So why do you need neural networks? But I can also
6682640	6687600	say neural networks are too incomplete. So why do I need prologue? And in fact, kernel machines have
6687600	6691760	a represented theorem that says you can approximate any function, blah, blah, blah, right? So everybody
6691760	6696080	has one of these represent their theorems, right? That says, I can represent anything, right? So in
6696080	6702160	particular, you can do, right? I mean, look, first, our logic was invented by, by Frege, essentially,
6702160	6708080	to, to model the real numbers. So it can almost by definition model real numbers, right? Anything
6708080	6712160	you might want to say about real numbers and, and weight and descent and neural networks. And in
6712160	6718400	fact, people have even done this. So you can say it all in, in logic programming, right? So why not
6718400	6722720	just do that? Well, precisely because certain things are much more easily done in other ways,
6722720	6726720	right? So what you have to ask about anything, but then about, you know, not the logic
6726720	6731280	program in particular, like, what things are well represented in this way, like compactly
6731280	6737440	represented, and then in such a way that learning them and doing inference with them is easy, right?
6737440	6741120	And those things are different for logic programming and for things like deep learning,
6741120	6745680	which is why we need a unification of both. So what is things like logic programming and
6745680	6750480	ILP good for, right? It's precisely, I mean, it's many things, but the key thing is,
6750480	6756560	it's precisely for learning pieces of knowledge that can then be reused and composed in arbitrary
6756560	6764240	ways. This is the huge power symbolic AI that connectionism does not have, right? It's like,
6764240	6769040	I learned the fact here, I learned a rule there. And tomorrow you ask me a question,
6769040	6773040	and I combine that fact, actually, several rules by rule changing, right? There's a whole proof
6773040	6777360	tree of rules that could have come from very different places. And I do a completely novel
6777360	6782240	chain of inference that answers your question. This is spectacular, right? And this is surely
6782240	6787760	court-wide intelligence is all about. And the symbolists know how to do it. The connectionists
6787760	6791040	don't. But if I was a connectionist, I'd be like, you know, I know if it was a good one,
6791040	6795680	and the better ones like Yoshio Benji are doing this, right? It's like, go and try to understand
6795680	6799120	what those people understand so that you can then not combine it with those other ideas.
6799120	6804560	Yes. Yeah. Yeah, I completely agree. So a huge part of intelligence is this symbolic,
6804560	6811600	you know, extrapolation. Yeah. So how do you bring abstraction into this? Because the thing
6811600	6816800	that I always get caught on is that the traditional go fi vision was to, you know, handcraft the
6816800	6821520	knowledge. And actually, what we need is dynamic knowledge acquisition. And we need the ability
6821520	6826160	to create abstractions on the fly rather than just what we do now, which is crystallizing
6826240	6831120	existing human abstraction. How could we do that bit? Well, abstraction traditionally was and
6831120	6837840	still is a central topic in symbolic AI, right? Like be precise. I mean, I think nobody questions
6837840	6842800	that having levels of abstraction, someone is very important. The only question is how. So if you
6842800	6848080	look at classic knowledge representation, planning, et cetera, et cetera, abstraction is all over
6848080	6852640	the place. If you look at things like reinforcement learning, and I mean, even like, you know, the
6852640	6857920	whole idea or hope of a convent is that it captures objects at multiple levels of abstraction,
6857920	6862320	at least to some degree. In reality, it doesn't, right? But that's what people are trying to do
6862320	6867440	and not quite doing, right? Well, good. Let's touch on that then. So I mean, certainly in
6867440	6872560	Jan McCoon's view, I spoke with Jan the other day, he's got this autonomous path, a paper. And,
6873440	6877120	you know, his system is learning abstractions, but they're abstractions which are deducible from
6877120	6882640	base abstraction priors, like objectness and, you know, basic visual priors. And so there's this
6882640	6887280	assumption that everything is deducible from the priors that we put into the model. But I have this
6887280	6893360	kind of intuition that abstraction space is much larger than that. Yeah. I mean, so I would even
6893360	6898960	say that if you arrive at your abstractions solely by deduction, you have a very impoverished notion
6898960	6905120	of abstraction. In fact, most of inductive learning is forming abstractions. And form abstractions at
6905120	6909520	the most basic level is something very trivial. It's like, I have an example described by a thousand
6909520	6915520	attributes. If from that I induce a rule that uses only 10, I've abstracted the way the other 990,
6916240	6921360	right? But if a symbolist was here, they would talk about intention versus extension, and they
6921360	6924640	would say that, you know, you're selecting from this infinite set of possible attributes. You
6924640	6928400	couldn't possibly represent all of the attributes in this. I mean, just to give you a concrete
6928400	6934000	example, you know, you could have a, a, a, a, a, a, a, you know what I mean? You can have like this.
6934000	6939120	Again, I hate to bring up infinity again, because that's always what these folks bring up. But
6939120	6944240	how could you select from a set that large? Well, I don't need to because it is finite.
6944240	6948400	But what I need to do is so, so, but there is actually a good example. And, you know,
6948400	6954080	infinity does not bother us at all, at all there, because what it's like, if my training set,
6954080	6959280	right, is a set of strings, and those strings are a, a, a, a, a, a, a, right? Going up to
6959280	6962720	whatever number you want to pick, like, you know, a million or a quid drill in, you know,
6962720	6968480	or a Google, right? Then R is your learning algorithm able to induce that the, the language
6968480	6972960	that these rules come out of, right? The grammar is, you know, it's a series of A's, right? You
6972960	6977600	and I can do that immediately. You know, most deep networks have no end of trouble doing that,
6977600	6982080	even though it's that basic. So it is a very good example of what symbolic learning and
6982080	6986000	reasoning can do versus connection is you don't need to go anywhere near infinity to actually
6986000	6990240	have that be a very elegant example. Well, let me bring up just one other, we've touched on a
6990240	6994000	lot of great things, right? There's one in this space of things that we've been talking about,
6994000	6998640	there's one that I think is very important, which I believe you're also a fan of. And I very much
6998640	7002400	am. And I think it's going to, you know, maybe you're going back to the question of what I'm
7002400	7007040	interested in that's happening at, at new reps right now or not. So new symbolic AI is definitely a
7007040	7013280	big one. Another big one. And to my mind, maybe these are the two biggest ones are most interesting
7013280	7019040	is, is what I call symmetry based learning. And these days is more popular known by the,
7019040	7022880	by the, by the name of like geometric deep learning and things like that. I tend to view
7022880	7027680	geometric deep learning as a special case of symmetry based learning. But this idea of,
7028720	7034000	I think, let me, you know, to go straight to the punchline, we know that, for example, AI and
7034320	7038960	machine learning in particular, have as foundations, things like, you know, logic,
7038960	7044000	probability optimization. And I think another foundation is symmetry group theory. In fact,
7044000	7048080	I was having, you know, dinner with, with Max Welling just the other day, who, who, of course,
7048080	7052320	have also interviewed and is, you know, like a great, you know, person in this area. And we,
7052320	7057280	you know, I think we have very similar views on this. Well, Pedro, yesterday, and Taka Kohen
7057280	7061760	was sitting where you were sitting. So there you go. Yeah. Again, I remember talking with Taka
7062000	7065520	Kohen, some ICML many years ago, where he published one of the first papers on this.
7065520	7071040	And I was like, and he seemed a little disheartened by the lack of interest that people had. And I
7071040	7075440	said to him, just wait, this is going to be big and we're there now, right? And it's going to be
7075440	7079520	even bigger, I think. But also, I think to become bigger and again, to jump straight to the punch
7079520	7085360	line, most of the work, including me, that people have done to date has been exploiting known
7085360	7089440	symmetries, like, you know, translation invariance is the quintessential example. For example,
7089440	7093840	we have something called deep affine networks that generalize coordinates to, you know,
7093840	7098400	rotation, you know, scaling, et cetera, et cetera. This is all well and good. But I think
7098400	7103440	if this is, and if you look at New York's today, for example, most is in that vein.
7103440	7107120	And there's a lot of good work to be done there. But if that's all we ever do, we will always
7107120	7112000	remain a niche in AI with certain very good applications, like science applications,
7112000	7116240	where we know that certain symmetries hold and whatnot. Max and Taka are doing things like that.
7116240	7119680	But I don't want to just do that. I really, you know, I'm trying to make progress towards
7119680	7124000	human level AI. And I think the key there is to discover symmetries from data.
7124560	7129040	Yeah. And I think most of us agree with this. It's a hard problem, right? But that's what we're
7129040	7134000	here for. We want to discover symmetries from data. And, you know, there's an interesting,
7134000	7137520	you know, discussion of how to do that, you know, I have a number of ideas and a number of people
7137520	7142160	have, then the power of discovering symmetries, right, connecting back to our early conversation
7142160	7147200	is that symmetries can, individual symmetries can be very easy to discover because they're
7147200	7152880	often very simple. But then, right, by the group axioms, axioms, you can compose them arbitrarily.
7152880	7157840	Yeah. Which means I can, for example, by learning 100 different symmetries of a cat
7157840	7162640	from 100 different examples, then I can compose them and correctly recognize as a cat
7163440	7168000	something that is extremely different from any concrete example of a cat that I saw before.
7168960	7172480	Could I push back on a tiny bit? So, I mean, in the geometric deep learning prototype book,
7172480	7177200	I mean, they spoke about, you know, the various symmetries of groups like SO3, you know, preserves
7177200	7182960	translations and angles, you know, like how primitive and how platonic are these symmetries?
7182960	7186640	And aren't they just like obvious in respect of the domain that you're in?
7186640	7191680	No, very good. So this is actually a key question. Symmetry group theory is one of them.
7191680	7196240	It's a central area in mathematics that it's a very highly developed and it's the foundation
7196240	7201440	of modern physics, like the standard model is a bunch of symmetries and so on. But the way,
7201440	7208000	and there is an exhaustive listing of what all the possible symmetry groups are, discrete ones,
7208000	7213760	you know, continuous ones, you know, so-called lead groups, etc., etc. So at that level, this is
7213760	7219600	not naive because people already have a handle on what the space is, right? But crucially for our
7219600	7226480	purpose is for AI, that's not enough because precisely because those, again, the analogy
7226480	7231440	with logic is actually a very good one here. First of all, the logic is to brittle, right?
7231440	7235040	And plain symmetry group theory, the way people have mostly applied so far,
7235040	7240080	is also too brilliant for the same reason. So for example, right? Something like, you know,
7240080	7244080	people almost always immediately come up with, so like, oh, I understand, you know,
7244080	7248560	I like symmetries with the light to recognize, you know, perturbed digits, but a 6 is not a 9.
7249520	7253280	So some, like, if you just take naive symmetry group theory and you say, like, well, arbitrary
7253280	7256880	composability, as I was just talking about, I was like, well, now you've just said that a 6,
7256880	7261680	you've lost the ability to distinguish a 6 from a 9, right? Now, what we need precisely is to
7261680	7265520	combine symmetry group theory with the other things like statistics and optimization and
7265520	7270720	say something like the following. The space of things that you can compose is unlimited. You
7270720	7275600	can have, you know, unlimited compositions, but for example, you pay a cost for composing more
7275600	7280240	symmetries. And now when you find the least cost path, and that's how you're going to match things,
7280240	7285360	or, you know, your digit becomes less and less probable to be in 6, the more you've rotated
7285360	7290000	it, right? So now we know how to do all of that very well. So we know symmetry group theory very
7290000	7294240	well. We know how to do all these probabilistic costs, minimizing blah, blah, blah things,
7294240	7298880	machine learning very well. We just need to combine it to the same way that we have previously
7298880	7302960	combined these things with first order logic. So I'm glad you brought in the cost that that was
7302960	7306880	really, really good. So there were trade offs everywhere. I mean, for example, if you want to
7306880	7312160	make the models more fair and, you know, prioritize the low frequency attributes on the long tail,
7312160	7316080	the headline accuracy goes down. Same thing with robustness. If you robustify a model,
7316080	7319760	the headline accuracy goes down. Same thing with symmetry groups. If you introduce other
7319760	7324480	symmetry groups, you know, that the headline accuracy goes down. So it all comes back to the
7324480	7329440	bias variance trade off at the end of the day. And, you know, where is the limit here? How much
7329440	7335200	can we optimize these models and what does good look like? The bias variance trade off is a very
7335200	7342720	useful tool, right? But it's not the deepest reality, right? The way to think about bias variance is
7342720	7347120	that, again, talking about this notion of a trade off curve, there's a trade off between bias and
7347120	7351200	variance, right, which is in some sense unavoidable, right? In machine learning, if you have finite
7351200	7355600	data, you're trying to learn powerful models, bias variance is a trade off. And it's a very
7355600	7359440	consequential trade off in the sense that, for example, the things that work best with small
7359440	7362960	amounts of data tend not to work best with large amounts of data, right? This is something that we
7362960	7367040	should all, you know, grow up knowing in machine learning. But so many mistakes have been done
7367040	7371200	because of that, because people study things in the easy or historically, that's all they had,
7371200	7374880	right? And then they're very surprised when something that seemed not very good, like, say,
7374880	7378800	deep learning, right, turns out to be better when you have a large amount of data, or they believe
7378800	7383600	in, like, silly things like, you know, Occam's razor version that, you know, accurate, you know,
7383600	7388240	simply is more accurate and whatnot. So a lot of mistakes have been made because of lack of
7388240	7393840	understanding of this. Having said that, what you really want is to move to a better trade off
7393840	7399760	curve between bias and variance, which you can, if you get at what the reality is, right? So the
7399760	7404080	real game in machine, once you're evaluating your learner and figuring out, you're like, how much
7404080	7409040	to prune and whatnot, or how much to regulate bias variance is very important. But before that,
7409040	7413200	the most important question is like, what we're trying to do here is figure out what are the
7413200	7418720	inductive biases? What are the regularities that the world really has, at least approximately,
7418720	7423520	that we build our algorithms on top of that? And then if you give me a better one than I have now,
7423520	7427840	I'll still have a bias variance trade off, but I'll be in a curve where for the same variance,
7427840	7431520	I can have less bias and vice versa. And that's where the real action is.
7431600	7434640	Oh, interesting. Well, I didn't quite understand that because bias and variance,
7434640	7439120	they are mutually exclusive. And I thought at first you were saying, well, if we understand
7439120	7443840	what the biases are better, the prototypical symmetries of the world we live in, then we
7443840	7447280	can have more bias without having an approximation error, basically.
7447280	7451360	The confusion arises because bias is a very unfortunately overloaded term.
7451360	7455520	Right. This is not even getting into the psychological notion of bias like in Danny
7455520	7460240	Kahneman's work, or even the sociological notion of bias like racial biases, gender biases and
7460240	7465600	whatnot. So we need to distinguish. I just used my bad, the word bias into completely
7465600	7470080	different senses, completely but not unrelated. That's the thing. One of them is the statistical
7470080	7475920	notion of bias. There really is a trade off between the two. There's a sum of squares,
7475920	7481600	blah, blah, blah. The machine learning notion of inductive bias, it's the preference that you
7481600	7486400	have for certain models of our others, which is really just another way of saying your priors,
7487360	7493200	whether they are assumptions or knowledge. Maybe actually instead of bias, they're like,
7493200	7496880	what you really want to do is figure out what are the priors? What are the model classes?
7496880	7502080	What are the preferences? The bias is a kind of preference that really line up with the world
7502080	7506400	in reality or the domain and therefore let you move to a better trade off curve
7506400	7513600	among statistical bias and statistical variance. Amazing. Well, Pedro, just tell us a little bit
7513600	7517040	about what have you seen at NeurIPS and how's the week been for you?
7518400	7520880	We've already touched on some of the interesting things that I saw,
7522240	7526560	in particular some of the areas that I'm interested in. The thing about NeurIPS is this,
7526560	7530800	of course, is that it's a vast conference. In the early days, I used to at least go through
7530800	7536240	the proceedings and look at the title and maybe the abstract of every paper. This is now impossible.
7537040	7541200	Now, these days, if all you do is try to walk through the poster sessions,
7541280	7546480	you never get to the end. I haven't been to a single poster session in this NeurIPS
7546480	7550960	where I actually got through all. I like to go through the poster sessions quickly once
7551680	7556080	and then just to see what's there and then go back to the ones that I found really interesting.
7556080	7561920	I haven't actually been able to even finish that walk through because they're so vast. You're also
7561920	7566880	running to people which is part of the point and talk and whatnot, but when there's 500 posters in
7566880	7570640	every session and there's 3,000 papers in the conference, it becomes very hard to find the
7570640	7575120	ones that are most relevant. Of course, an easy thing to do is look at what they, I mean,
7576400	7580160	something about NeurIPS this year that I honestly thought was absolutely terrible,
7580160	7587760	like a really, really terrible idea is that it's a hybrid conference and their idea of
7587760	7594080	a hybrid conference is that there are no talks. The talks are all virtual next week. Nips this
7594080	7600160	year to a first approximation was one big poster session, which I mean, to me, this is just an
7600160	7606080	incredibly bad idea. In that sense, I haven't gotten as much out of Nips by this point of the
7606080	7611840	conference as I would have in most years. There's also looking at the papers that were usually
7611840	7616880	selected as oral, but this time they call them oral equivalent because there are no oral papers,
7616880	7623200	but they still want to have that distinction. The number of those papers these days is 160 or
7623200	7630080	something, which is bigger than Nips and ICML some years ago. Usually from those papers,
7630480	7635680	some of them kind of like jump out at you as being great and very relevant. I've only looked at
7635680	7643440	them briefly, so don't quote me on this, if you will, but none of those have jumped out to me
7645120	7648560	as like, oh, yeah, this sounds like something really brilliant and that I want to dig into,
7648560	7652640	but there probably are many. I just haven't really had a chance to look at them yet.
7652640	7657760	Yeah. I mean, I have a similar reaction. I mean, it feels like we're at the point of saturation
7657760	7662640	and there are loads and loads of microvariations on the same idea. It's completely overwhelming,
7662640	7666800	but what I find is that it's a very social experience. When I walk through the posters,
7666800	7671040	I just immediately become engrossed in conversation and hours go by and I just think, oh my God,
7671040	7675680	what have I just been doing for the last year? That's the real point. The posters are very good.
7677280	7680880	It's like the grain of sand and the oyster. The poster is the grain of sand. The oyster is the
7680880	7685360	conversation that you have with the person at the poster or with other people around there.
7685360	7688160	To touch on another point that you made that I think is actually important.
7689520	7693680	New Europe's and ICML and so on are bigger today than they've ever been. Actually,
7693680	7697360	not strictly true because these recent lips, surprisingly, they tend to have gone down a
7697360	7706720	lot. We can and should ask why, but we need to scale. There are bigger conferences,
7706720	7711280	like the New Science Conference is one conference and it's 35,000 people every year and they make
7711280	7717360	it work. It's good to experiment. I think New Europe's at the scale that it is today can work,
7717360	7722000	but it is not working very well. One of the ways in which it's not working very well is that
7722560	7726240	we need to think a lot more. I don't understand this is working. It's hard and people have day
7726240	7731520	jobs or not, so this is not a criticism in that sense. We need to really work on making it easy
7731520	7736640	for people to find the papers that are relevant to them. Number one, number two, and maybe even
7736640	7741760	more important, there is more machine learning research today than ever, but in some sense the
7741760	7746880	diversity of that research is in some ways lower than ever. Another point that you brought up and
7746880	7751280	I think is very important to do with the scaling of New Europe's and the machine learning communities
7751280	7757680	that we have in just raw numbers, more machine learning and AI research going on today than ever
7757680	7762720	before by an order of magnitude. But in terms of diversity, there's probably less diversity in the
7762720	7768160	research now than there was before, which is a tragedy. I understand why people have kind of
7768160	7772240	like converged to deep learning. I'm a huge fan of deep learning. I was doing it before it was
7772240	7777520	cool as they say and whatnot, but the extent to which 90% of the community, not just in machine
7777520	7783280	learning but AI, is not just pursuing and not even deep learning, but a special type of deep
7783280	7791120	learning, which you might call applications of backprop, is extremely undesirable. We have
7792080	7797120	an infinite number of micro-improvement papers along a particular direction that is almost
7797120	7802000	certainly a local optimum, and we're just digging into that local optimum with ever more papers and
7802000	7808080	never more, you know, minimal publishable units when this large amount of manpower that has come
7808080	7814400	into the field or is moving around, we really need to have a greater diversity of research in
7814400	7821680	machine learning, within deep learning, within AI, and so like we are making very poor use of our
7821680	7826320	research, you know, manpower right now, and we see that very much at NeurIPS today.
7826320	7829200	Yeah, I mean, Sarah Hooker talked about the hardware lottery, you know, being stuck in a
7829200	7833600	basin of attraction determined by hardware, but there's also an idea lottery. It might just be
7833600	7838320	the case that NeurIPS historically has always been very connectionist anyway. I mean, maybe it
7838320	7841520	hasn't, right? That's one of the ironies, but it's something as well. I wasn't aware of that. Okay.
7841520	7847920	Oh, absolutely not. I mean, in fact, the joke is, right, that NeurIPS started in the 80s,
7847920	7852400	it was called Neural Information Processing Systems, and by the 90s, it should have become
7853040	7857600	BIPs for Patient Information Processing Systems, right? There was this study that they did at one
7857600	7862320	point of predictors of acceptance and rejection among words in the title, and the biggest predictor
7862320	7867120	of rejection was the world neural. Really? And this was very famous in the field, because
7867200	7872240	indeed, if you could, you know, 1990 something, you were submitting papers to NIPs with the
7872240	7876800	world neural in the title, you didn't know what you were doing. And then in the 2000s, right,
7876800	7882240	it became BIPs, or should have become BIPs, sorry, KIPs, Kernal Information Processing Systems.
7882240	7887200	And in fact, I remember having lunch with Yoshio Bingo at the ICML in Montreal in 2009,
7887200	7891440	and we were talking about this, right? The fact that every day kid, and, you know,
7891440	7895680	not a new paradigm, but another one of the same paradigm seems to now be on top, right?
7895680	7899520	And, you know, he asked, like, so what is the next decade going to be? And I said,
7899520	7904720	it's going to be DIPs, Deep Information Processing Systems. And then we both laughed,
7904720	7909280	and I could tell that I believe this, but he, Yoshio Bingo, was actually skeptical of this.
7909280	7913920	So, you know, the deep, little did we know, right? If somebody told us that, you know,
7913920	7916960	this is going to be on the page of the, on the front page of the New York Times,
7916960	7921360	in a couple of years would be like, what are you smoking, right? So the way to which this decade
7921360	7926320	has been DIPs is just mind-blowing, but looking forward, right? And to this point of, you know,
7926320	7931280	diversity in research approaches, I think if you extrapolate naively from the past,
7931280	7935600	the next decade will be about something else. And the trillion-dollar question
7935600	7941840	is what, what is that else going to be? Amazing. Okay. You watched Charma's talk, right?
7941840	7946400	Yeah. What's your high-level view? I thought it was a nice talk. I thought it was a very
7946400	7950720	appropriate talk for an opening talk at the conference. Actually, if New Europe's had,
7950720	7956240	like, some conferences, a dinner talk, right? Which is supposed to be interesting, but not as,
7956240	7959840	you know, deep or as technical as other. This would have been the perfect dinner talk for
7959840	7965520	New Europe's, because the topic is very current, right? Our machine's sentient. And, you know,
7965520	7970880	who better to talk about it than Dave Chalmers, right? The world's expert on, on, on, on consciousness,
7970880	7977120	right? And by and large, I thought the talk was excellent. In fact, you know, when journalists
7977120	7981360	ask me questions, you know, consciousness is like one of their top three, right? Along with
7981360	7986080	Terminator and, you know, Unfairness or something like that, right? And I will point them to this
7986080	7992880	talk because it kind of like lays out, you know, the, you know, the ground. And, you know, it's good
7992880	7998880	for people to at least have those things in mind. At the end of the day, so I think, of course,
7998880	8004560	the notion that Lambda was sentient is, you know, ridiculous, as, as most of us do.
8005280	8010480	You could ask a slightly more fine-going question was if, if, if, if, if consciousness is on a
8010480	8015680	continuum, right? Which I think Dave believes in. And if you believe in like this, you know,
8015680	8020000	IT theory and phi and whatnot, you know, like, phi is never zero, right? So there's always some
8020000	8024640	consciousness, right? Pensychism and whatnot. I'm not saying I believe in that. We could,
8024640	8028480	we could go into the, but like, if you believe in that, then you can ask, well, on that scale,
8028480	8034880	you know, where is Lambda? Where are these large language models? And, and, and surely higher than
8034880	8040960	previous AI systems, right? But in my view, still very, very, very far. And I think what you want
8040960	8045840	to keep in mind is that consciousness does not, does not increase continuously. Precisely,
8045840	8051200	there's these transitions where you go, you know, more is different is the, is the famous,
8051200	8055520	you know, phrase about emergence, right? Consciousness is very much an emerging,
8055600	8059840	you know, phenomenon. And I think what happens is that there are points at which your
8059840	8064160	consciousness will leap. Maybe a thermostat does have consciousness, like, you know,
8064960	8069360	or, you know, or purpose or whatever, right? Like, like people in, like people like McCarthy,
8069360	8074480	for example, had had had that as an example. But the amount of consciousness is minuscule.
8074480	8080640	And, and that, and the way I will put that is that these large language models still have not
8080640	8085360	passed that first threshold. Interesting. So, so in a similar way to some of the discussion
8085360	8090160	about large language models, there are kind of scaling breaks in the levels of consciousness.
8090160	8093920	I mean, Chalmers made the comment, though, that rather than it being a pure continuum,
8093920	8098160	he said that a bottle was not conscious, but then there was a kind of. No, yes. So very key
8098160	8104560	point. Scaling is part of it, but not only. It's not just that. So your cortex to first
8104560	8110160	approximation is a monkey brain scaled up, right? There was a module there that evolution
8110160	8114960	discovered, and it really paid to keep making more and more of it. And we can easily speculate why.
8114960	8120320	But the point is, so let me contrast two things, right? Which is true for consciousness, but also
8120320	8125920	for just AI in general. A lot of people are scaling believers and like open AI is the poster child
8125920	8129920	of this in a quite conscious ways. Like, we're just going to scale the heck out of things.
8129920	8133280	And then a lot of people, like, you know, Gary Marcus being a good example, they just
8133280	8137840	completely poo poo that they say, like, oh, no, this is a joke. Right. And I think the truth is that
8138480	8143120	scaling is good, right? Again, you know, part of what we are, our intelligence is scaling.
8143120	8148240	But the question is, what are you scaling? And the things that we're scaling today,
8148240	8153360	it doesn't matter how much we scale them, we never get to human level intelligence or consciousness.
8153360	8158080	So I think we need some fundamentally different algorithms, if you want to think at the level
8158080	8162320	of algorithms, or fundamentally different architect architectures, if you want to think
8162320	8167040	about it in a way, and then scaling those up at some point will give us consciousness.
8167040	8170880	If you live that it's possible for a computer to be conscious, but we're not there yet,
8170880	8175120	either in terms of the scaling, although actually scaling is actually the easier part of this way,
8175120	8179200	we're actually at the point where a computer can have the same amount of computing power that
8179840	8184880	your brain does, which was not the case before. But the bigger deeper problem, and the more
8184880	8189600	fundamental one is like, we need the architecture to scale. Right. And this is where I sympathize,
8189600	8194400	you know, with people like Jeff Hinton, who's just, you know, playing with, you know, ideas
8194400	8199280	using Mathematica and very small examples, which in some ways, sounds very underpowered,
8199280	8204240	but I think it's people like that, they are going to come up with the things that we then scale.
8204240	8208320	As in fact, it was David Roemmerhardt doing that kind of work that invented backprop.
8209200	8213120	Right. If he hadn't invented backprop, this whole industry would not exist. So
8213120	8217920	what I think is that the real backprop, the real master algorithm is not there yet,
8217920	8223600	and we need to discover that first. And then we, and then when we scale that up, which will not
8223600	8228960	be trivial, but will be much easier by comparison, then we'll have, you know, human level, intelligence,
8228960	8234800	consciousness, et cetera. Interesting. Okay. And so Charmes is a structuralist computationalist.
8234800	8242720	So, you know, he thinks information, not biology. And he's also a functionalist, right? So, you
8242720	8248720	know, which is very similar to behavior. And, you know, Hillary Putnam made the move that you can
8248800	8253600	kind of like represent a computation in any open physical system. And he kind of like used that
8253600	8258160	on, you know, if you follow that line of thought, it almost trivializes computationalism because,
8258160	8262800	you know, it leads to panpsychism very, very quickly. So, first of all, I mean, what's your
8262800	8266800	take on this idea that information could give rise to intelligence and consciousness?
8266800	8271440	So I agree, like most scientists, and I think in particular most computer scientists, that
8272160	8276880	to a first approximation, the substrate does not matter. And in particular,
8276880	8281680	you're not going to convince me that something is not conscious just because it's not biological.
8282320	8286320	There is no reason to think that only biological things can have consciousness. Now,
8286320	8292000	the deeper problem, and you know, indeed the hard problem, is that so as Dave Chalmers defined it,
8293040	8296160	so there's a basic fork here, which you've alluded to, which is,
8297280	8303200	if consciousness is subjective experience, then all these questions about consciousness are
8303200	8309600	ultimately unresolvable, because only I have my subjective experience. I know that I'm conscious,
8310160	8315120	no one can persuade me of the contrary. I don't even know if you are conscious, let alone some machine.
8315680	8321120	Right? So if consciousness is an intrinsic property of something that cannot be evaluated
8321120	8325920	from the outside, then we're doomed. We're never going to answer this question. And maybe that is
8325920	8330320	the case. Right? So I'm not saying that's false, and you need to always keep that in mind. But now,
8330400	8335840	if we're going to make any kind of progress, right, we need to look at what are, to generalize a well
8335840	8341120	known term, the external correlates of consciousness. Right? One of those which has been well studied by
8341120	8345760	people like Christoph Koch and so on, and I think that's a very good direction, is the neural
8345760	8350000	correlates of consciousness. Right? What goes on in your brain that correlates with consciousness?
8350000	8354080	And we've made a lot of progress with that. You can also talk about what are sort of like the
8354080	8359760	informational computational correlates of consciousness. Are there computational structures
8359760	8364640	that support consciousness and the ones that don't? I think that is also a useful thing to do.
8364640	8370000	Let's develop. It actually interests this panpsychism because it's not like everything is
8370000	8375200	consciousness just because it can compute. Some computations after this emergence and these,
8375200	8380880	you know, phase transitions may give rise to consciousness. Whereas others, it doesn't matter
8380880	8384880	how much of them you have, they will never be conscious. So I think this is also a very useful
8384880	8391120	way to make progress on this question and one to which AI versus, you know, a neuroscience or
8391120	8396400	psychology is very well suited to. Interesting. So on the functionalism point, and I think
8396400	8402080	Chalmers has been very, very consistent. He uses this kind of calculi to reason about
8402080	8407760	intelligence as well. So a system is intelligent if it can perform reasoning, if it can perform
8407760	8412320	planning, if it has sensing and so on. So we have this collection of functions. And then
8412400	8416480	he's kind of like moved this over to the domain of consciousness. So similarly,
8416480	8423040	if a system performs these functions and is used in a positive and a negative way. So some
8423040	8428320	functions would indicate an absence of consciousness and some functions would, you know, lead to the
8428320	8433760	presence of consciousness. And it's kind of like leading towards a, you know, touring test for
8433760	8438800	consciousness. I mean, do you kind of support that? That's a very interesting question. In fact,
8438800	8443760	you know, I was having dinner with Dave after his talk and I actually brought this up because it
8443760	8450160	wasn't clear from his talk. And I said, look, this is the answer that I usually give to journalists
8450160	8454080	when they ask me, you know, will machines ever be conscious and whatnot? And asked me a few,
8454080	8458800	and asked me if he agreed with it and actually expected him to disagree. But I think again,
8458800	8463520	don't want to put words in his mouth, but that he agreed, right? And the answer is the following,
8463520	8468480	is that human beings, right? As we've discussed, have an amazing tendency to
8468480	8473200	anthropoformize things. It's reasoning by analogy. And what happens, I used to say,
8473200	8477280	this is what's going to happen at this point is this is what is already happening is that
8477280	8484000	as soon as a machine behaves externally, even vaguely like it's consciousness, we immediately
8484000	8489280	start treating it as if it's consciousness. So if you look for 10, 20, 50 years from now,
8489280	8493920	we will just treat AI's as if they're consciousness and people won't even ask that question.
8493920	8498320	They will assume AI's are conscious in the same way that we assume that each other,
8498320	8504160	that we're conscious, right? But then, and so like from that pragmatic external point of view,
8504160	8509520	maybe the question is answered, right? But you could be a philosopher or like sort of like a very,
8509520	8514960	you know, rigorous, you know, technical person and so like, no, no, no, no, I really want to know
8514960	8519760	if things, they may look, you know, conscious from the outside, but are they really, right?
8520320	8524720	But that question, as far as I can tell, unfortunately, at the end of the day is probably
8524720	8529600	unanswerable. Now, there's a middle ground between these two things that maybe is where
8529600	8533920	we'll wind up. And to me, sounds like probably the best thing that we're going to be able to do,
8533920	8538800	which is that like, our understanding of the neural informational, et cetera, correlates
8538800	8544640	of consciousness evolves to a point where we have the feeling that we do understand consciousness.
8544960	8548560	It's not just the late person calls this consciousness even though haha, it's not like
8548560	8551680	lambda is not conscious, you know, poor bozo, et cetera, et cetera. It's like,
8552400	8555840	you know, there are many analogies to that in the history of science. There used to be a lot of
8555840	8561280	things that were like magical, right? And we were like, oh, we're never going to stand like life was
8561280	8565920	magical, right? Life did not obey the laws of physics. It's just something else, right? This
8565920	8570240	sounds laughable right now, but it wasn't laughable at all then, right? And now, it's not like we've
8570240	8574640	understood everything about life very far from it. When you say like, there's DNA and their
8574640	8579120	cells and then this is how it all arises, right? And I think we're at the point in consciousness
8579120	8584320	where it's to like, oh, consciousness is some so beyond us, right? I think we will get, you know,
8584320	8589360	there will be a structure of DNA moment in the history of the study of consciousness.
8589360	8593600	And I think, yeah, I think things like Phi and this, you know, IT3 and whatnot,
8593600	8598400	they're very brave attempts to make progress in this direction. I think, you know, like Julia
8598880	8604400	Tononi in a way is, you know, very deluded in thinking that he has nailed what consciousness
8604400	8610400	is, right? I think, you know, Phi maybe is an upper bound on consciousness, but with steps like this,
8610400	8615040	hopefully at some point, and very much with the help of AI, right? AI is really useful for this,
8615040	8620240	because it's a brain that might be consciousness that we have a lot of control of. And you can do
8620240	8625920	experiments that you can't, you know, with people, right? So I think we will make at least some progress
8625920	8630800	in that direction for sure. Maybe to the point where we feel that, yes, we do understand what
8630800	8635200	consciousness is, we're not asking ourselves that question anymore. And then we can point to things
8635200	8639600	and say, this is consciousness, this is that kind of consciousness, that amount of consciousness,
8639600	8643440	and so on. Yeah, that's really interesting. I agree, we're making a lot of progress in getting
8643440	8648000	a handle on this. And although the biggest game in town is still the computationalism game. And
8648000	8653040	as you say, historically, the only alternative was mysterious. And my friend, Professor Mark
8653120	8657200	Bishop, that he said that that's one of the reasons why he's become interested in the
8657200	8661120	forays in cognitive science, because for the first time, it's given him a kind of robust
8661120	8665520	alternative to computationalism. But just coming back quickly, you know, as Charlie's
8665520	8669920	reference, Thomas Nagel, you know, which is that it is something it is like to be a bat.
8670560	8676000	What do you think about that? So I'm not sure your question is, but let me check.
8676000	8679840	Well, what do you mean? Do you agree that there is something it is like to be a bat?
8679840	8685760	Oh, absolutely. Right. So there is more and more than that, right? There is something that it's
8685760	8692320	like to be a bat. And it's very different from being a human, right? And we grossly underestimate,
8692320	8696960	right? Again, we do this thing that again, it's a heuristic, it works very well as like,
8696960	8701120	we project ourselves into the bat, because what else could we do, right? But then what you see
8701120	8706160	is a bat seen through the mind of a human, right? And in fact, there's this famous, I would say,
8706160	8711760	even more famous, you know, you know, notion from, from Wittgenstein, right? That if the
8711760	8718720	lion could talk, I would not understand anything that the lion was saying. Because his world is
8718720	8724560	so different from mine. Now, I actually think, I think this is a very important position to,
8724560	8728880	as a reference point, right? Certainly a defensible one. And, you know, Wittgenstein was a good
8728880	8734400	defender of it. But I actually think that this is going too far. I think, ultimately, I mean,
8734400	8738960	never be able to completely know what it's like to be a lion. But we can make a lot,
8738960	8742640	don't underestimate us either, right? We can make a lot of inwards into understanding what
8742640	8748320	it's like to be a lion, much more than we understand today. Same thing for a bat. And,
8748320	8753280	you know, you could also ask that for a fruit fly, right? In a way, a fruit fly is more different
8753280	8758000	from us than a lion, but it's easy to understand, right? Because at some level, that thing is so
8758000	8761520	simple that we can understand what's going on with it, because it's not that deep.
8761600	8765680	Yeah, that's a beautiful quote, actually. So, closing this off, do you think that large
8765680	8768160	language models are slightly conscious or will be in the near future?
8768800	8774160	I think language, I think large language models are not slightly conscious by the reasonable,
8774160	8778560	you know, everyday definition of the world slightly, meaning that their consciousness,
8778560	8785760	so I think that either their consciousness is just zero, right? If somebody asked me, like, you know,
8785760	8789520	how much, you know, consciousness does, you know, lambda half, tell me in one word, and the answer
8789520	8795600	would be zero, right? But another answer which is hard to distinguish from the first one is epsilon,
8795600	8801360	right? Maybe it has a very tiny amount of consciousness, but it's so tiny that it doesn't
8801360	8806320	even qualify as slightly. Again, this gets back to what its architecture is. It actually gets
8806320	8811600	too lot of things, but for purposes of this discussion, right, lambda and these large
8811600	8816480	language models are not very different from a big lookup table. Any big lookup table is not
8816480	8820960	conscious. Now, I mean, there are a lot of interesting distinctions that you can make it well.
8820960	8825040	What if what I have is an efficient approximation to a lookup table? Isn't that what your brain is,
8825040	8829520	right? And I would say yes, and then people say, well, but then why is your brain conscious
8829520	8834400	but not the lookup table, right? And precisely the interesting question is that the consciousness
8834400	8840240	comes about from the fact that you have to concentrate all of this information, you know,
8840240	8845360	in real time, into something, you know, very compact and that leads to action continuously,
8845360	8850960	right? So to put this in another way, maybe God is unconscious because he doesn't need to be,
8850960	8855520	right? If you're omnipotent and omniscient, you don't need to be conscious. You are effectively
8855520	8860240	just a lookup table. Exactly. And I loved your response earlier about the grain of sand and
8860240	8863920	the oyster. I thought that was a beautiful way of looking at it. And having recently studied
8863920	8869200	so, I mean, personally, I think it's a lot to do with intentionality and agency, but I remember
8869200	8876160	you responded to that. Just final quick question. What's your definition of intelligence?
8877920	8882560	So let me start with the technical definition, which is unfortunately not widely known enough
8882560	8887920	and not appreciated enough. But I think it's a really important one to have, right? Intelligence
8887920	8894880	is solving NP-complete problems using heuristics. This is the real technical definition of AI,
8894880	8899440	right? And there's a lot packed into that, right? The fact that it's NP-complete problems and the
8899440	8904400	fact that it's using heuristics. If your problem is solvable with a lookup table with polynomial
8904400	8908960	algorithms, you don't need intelligence and there's no intelligence there. It's when you start solving
8908960	8916320	hard problems using heuristics that you're getting into the realm of intelligence. Moreover, NP-complete
8916320	8922160	is not the same as exponential, right? The crucial thing about an NP-complete problem that connects
8922160	8927120	very directly to our entire discussion of utility and whatnot is that the solution is easy to check.
8928000	8933440	This is the key. If you're working on problems whose solution is impossible to check effectively,
8933440	8937920	I can't even tell if you're intelligent or not. The whole thing about intelligence in humans and
8937920	8942960	machines is that how you solve the problem requires a lot of intelligence, a lot of computing power
8942960	8946720	and whatnot, but then I can easily check the solution. Now, hang on a minute, could that
8946720	8951440	say a step away from behavior then if you're saying that, you know, like you have the percepts,
8951440	8954720	the state and the action and you're saying the state is also important?
8954720	8962240	No, so to answer that head on, intelligence is not behavior, right? Intelligence to give a slightly
8962240	8966240	more general definition and then there's several and they all have their merits. Intelligence is the
8966240	8971200	ability to solve hard problems. Then more concretely, it's NP-complete problems and using heuristics,
8971200	8978000	but like, for example, if you create an AI system that cures cancer, it doesn't behave in the sense
8978000	8982720	that a human and a robot behave, but, you know, it's damn intelligence, it's more intelligent
8982720	8988080	than we are, right? It would be childish to deny intelligence to that system, no matter how it solves
8988080	8994720	cancer. If it finds a ridiculously simple way to solve cancer, then it's even more brilliant,
8994720	8999360	right? In fact, the simpler your outcome, the more intelligent you are, right? It takes intelligence
8999360	9004800	to produce something simple. Wow. Concretely, in many circumstances, in particular evolution,
9004800	9009840	right? Intelligence manifests itself as behavior. There's a sequential decision making problem,
9009840	9013840	there's an agent in the world that said a certain stuff, being a stochastic parrot.
9013840	9018640	And I think also from, you know, theoretical reasons, by analyzing what a transformer can
9018640	9023360	represent and how it can learn, my best guess, which could be wrong again, I don't think anybody
9023360	9028880	has the answer to this and it's interesting question is that those transformers, right,
9028880	9033680	not LLM scholars, that means more of like a task rather than the, you know, than the architecture.
9034320	9038000	Transformers have a certain limited ability to do compositionality,
9038560	9044480	very limited to compare to full logic programming, etc., but exponentially better than something like
9044480	9049920	an ordinary multilayer perceptron. And if you just, I mean, even a multilayer perceptron or any
9049920	9055760	learning algorithm is more than a stochastic parrot, because it's general, the whole point
9055840	9060320	of machine learning is to generalize beyond the data. If you generalize correctly beyond
9060320	9064880	the data, you're not just a parrot anymore. And, you know, I think it's not an accident that that
9064880	9069840	term stochastic parrot came from Emily Bender, my linguistics colleague at UW, who does not
9069840	9075360	understand machine learning. She's a classic linguist of the Chomsky and Variety, who does,
9075360	9079440	you know, does not fundamentally understand what I think, you know, she might disagree,
9079440	9083040	what machine learning is all about. And she would probably look at any learning algorithm and say
9083040	9087760	that it's a stochastic parrot, missing the fact that the whole point of machine learning and the
9087760	9092800	thing that we focus on from, you know, beginning to end is generalizing. And as soon as you're
9092800	9097680	generalizing correctly, even if you have no compositionality, you're already doing something
9097680	9102320	that has a little bit of intelligence, and that's beyond what a parrot would do.
9102320	9106480	Yeah, I mean, to be fair, it's not a binary. And at the time, I thought they were stochastic
9106480	9111200	parents as well. I've updated my view. And you were talking as well about creativity. There's
9111200	9115280	a kind of blurred hyperplane of creativity. And we discussed, you know, where that hyperplane
9116000	9120240	sits. But, you know, what's really interested me, I've interviewed quite a few people that are
9120240	9124800	working on working on in context learning in these language models. And it seems like these
9124800	9131120	language models are almost almost like a new type of compiler, you know, you're writing a program
9131120	9137120	inside the language prompt. And they seem to work extremely well outside of the training
9137120	9140000	range if you're doing like basic multiplication tasks.
9140000	9143840	I think it is useful to look at them as a new type of compiler. In fact, I've been saying for a
9143840	9149280	long time that, you know, like, there's this continuum from programming an assembly code to
9149280	9154880	high level languages to doing AI, right? The point of AI is to continue along that path
9154880	9160000	to making the language that computers speak ever closer to ours, so that we can just program them
9160000	9164640	by talking to them or writing things at them, right? Having said that, I think that, you know,
9165600	9171360	what goes on in the innards of a transformer, right, is actually still
9173920	9179920	very primitive, for lack of a better word, right? There's a lot of, so something I tweeted that
9179920	9184400	got a lot of follow up from people like Yan and Gary and who the pro because they were all bringing
9184400	9188720	in their own angles. So this was like, I said, and I think this is an interesting question. It's
9188720	9193520	like the interesting question about transformers is what needs to be added to them to get real
9193520	9198640	intelligence. So we should not deny what they have, like the attention mechanism in particular,
9198640	9203200	right? And the embeddings and the context. So like, there are two very important things in
9203200	9208240	transformers that are beyond what was in neural networks 10 years ago and are key. One of them
9208240	9214720	is attention, right? Attention is a real advance. And the other one is context specific embeddings,
9214720	9218720	right? Each of these ideas is important in its own right and combining them together is very
9218720	9223440	powerful, right? Again, because the context sensitive embeddings get that the similarity
9223440	9227840	part of intelligence, the attention combined with the context sensitivity of the embeddings
9227840	9233440	gets at the compositionality part. So they do have, so there are a couple of steps forward on
9233440	9238400	the road to human level intelligence, but there are many more. And rather than either saying like,
9238400	9242640	oh, they're just parrots, they don't do anything, we're saying like, we've almost solved the eye,
9242640	9247360	what we really should, we should try to understand better, you know, how the, you know,
9247360	9252560	the attention and the context is dependent embeddings work, which we don't. But we also need
9252560	9257040	to focus like, now, what are we still missing? Because we definitely are. And that's really
9257040	9260960	where most of our focus should be. Yeah, I completely agree. And also just in defense
9260960	9265760	of Bender, I mean, I think she's a brilliant linguist. And I personally think having that
9265760	9272960	diversity of views from different people is useful. No, I mean, so I very much think that having a
9272960	9276560	diversity of views is very important. And I think something that I'm always saying to my
9276560	9281200	deep learning friends who can't stand, you know, who hate the guts of Gary Marcus is
9281920	9288560	we really, really need informed critics. Yeah. And very typically, your informed critics are not
9288560	9294240	people in the field. We are experts, but then we also suffer from the distortion of being experts.
9294240	9300080	It's people in adjacent areas. And people like linguists and psychologists are very much those
9300080	9305280	people, they're in adjacent areas, enough to have a good critique of AI. So for example,
9305280	9311200	something that Jan is always throwing at Gary Marcus, that kind of doesn't sit well with me,
9311200	9315040	says like, well, you should try building a real system sometime, and you can criticize this until
9315040	9319200	we do. If we take the attitude that only engineers can criticize engineers, we're doomed.
9320240	9324560	Having said that, there is a very big distinction between the knowledgeable informed critics like
9324560	9329280	Gary Marcus, and the not so knowledgeable, not so well informed ones, which unfortunately,
9329280	9333680	Emily is an example. I mean, she's my colleague at UW. And I've talked with her about some of these
9333680	9338320	things. And her criticism of machine learning, unfortunately, like a lot of people, comes from
9338320	9343840	a place of actually not fundamental understanding it very well. But people do say that Gary isn't
9343840	9348480	an expert in deep learning and that he's, you know, attention seeking. What would you say to that?
9348480	9357360	No, he's not an expert in deep learning. And so like, I agree with some of his criticisms,
9357360	9363040	I disagree with others. Probably on balance, I disagree more with him than I agree. But
9363680	9367840	first of all, there is a value to having critics like that, number one. But then number two,
9368800	9372880	the reason his criticism, I mean, it would be better if he was also an expert in deep learning
9372880	9377440	and made the same criticisms. And then the problem is that often his criticisms are wrong
9377440	9382800	because he has a mental model of deep learning that is already outdated, or is oversimplified,
9382800	9387440	right? But that to some degree is unavoidable. But the thing that makes his criticism valuable
9387440	9392800	is that he's doing it at a level where on a good day, on a bad day, his criticisms miss the mark.
9392800	9397200	But on a good day, which is the ones that matter, his criticism is actually useful because it's
9397200	9401920	at a level where you don't need to understand the details. It's like, you claim to be producing
9401920	9407440	intelligence. I as a psychologist know a lot about intelligence. That's what I study for a living,
9407440	9412800	right? He knows more about aspects of intelligence than I do. Yeah. And from that point of view,
9412800	9417840	what you're doing is lacking. And that I mean, like, he's written the whole books about, you know,
9417840	9422000	again, because this goes back to when he was a PhD student and, you know, and symbolic learning and
9422000	9428480	whatnot, there are very, you know, the deep learning folks have repeatedly underestimated
9428480	9433200	how well he understands some of these problems. Because as a psychologist in particular interested
9433200	9438560	in language learning, he's actually thought very long and hard about them. Oh, I know. So I've
9438560	9443520	read his book and we've had him on the show three times. Which book? The algebraic mind.
9443520	9447280	Yeah. So that's the most relevant one here. Yeah. As a psychologist, you know, he spent a lot of
9447280	9451600	time studying how children learn rules. Right. And he talks very elegantly about a
9451600	9455760	compositionality. And we've spoken about this. It's irrefutable. And I agree with him and we've
9455760	9462400	supported him. I guess some of the things he argues are based on ethics, politics and virtue.
9462400	9468320	And some of the things like compositionality, I think are irrefutable. I mean, I think irrefutable
9468320	9472480	is a very strong word. I wouldn't say that they're irrefutable. I would say that they have,
9473440	9479120	they have very strong backing, which the connectionists have not been able to effectively
9479120	9484880	refute. But some of the criticisms that they have, you know, meaning people like Pinker and Prince
9484880	9490320	and whatnot, famously of connectionists in the 80s, some of them are still valid, which is very
9490320	9495440	salient. But some of them not really. And again, to go back to the daddy of this whole school of
9495440	9500880	thought, who's Chomsky, right? His, you know, he made his name basically panning things like,
9500960	9505920	you know, Markov models of language in Graham models, which he could say large language models
9505920	9511440	are just a very glorified version of, right? But and at the time, you could, that criticism was
9511440	9517120	very apt and, you know, and timely and it was useful, right? But, but, but, and famously, it's
9517120	9520880	like, it's like, you can't learn a context free grammar, but context free grammar is what you
9520880	9526160	do. Well, actually, now we know formally that you can learn a context free grammar. And, and,
9526160	9530080	you know, because you only have to learn it probabilistically, which is what we do. And
9530160	9535920	what our systems do. So his criticism was just, you know, mathematically off the mark. But also,
9535920	9541360	when you look at systems that do speech language, et cetera, et cetera, it is that statistical
9541360	9546320	approach that he made his name panning that has prevailed. And for reasons that we understand
9546320	9551040	very well, and large language models are just the latest greatest expression of that. So at that
9551040	9556880	level, a whole Chomsky and Pinker, Gary Marcus view of things, not only is it not irrefutable,
9556880	9563600	it has been refuted. Okay. Let's just quickly come back to your definition of intelligence. So
9563600	9569520	solving NP hard problems, I assume you would zoom out a little bit and, you know, it's more of a
9569520	9573760	meta learning algorithm. So the ability to sell to sell different problems.
9576160	9583920	Yes. So it's, if very good point, if all you have is the ability to solve one NP complete problem,
9583920	9588880	that does not qualify as general intelligence, right? There's like, these days, this is a common
9588880	9593920	definition to make this different difference between, you know, narrow intelligence and general
9593920	9598720	intelligence and AGI and whatnot, right? And if you only solve one NP complete problem very well,
9598720	9602800	you have narrow intelligence is the way I would put it, but you do not have general intelligence.
9602800	9608320	General intelligence is precisely the ability to solve a limitless variety of problems, all that
9608320	9613600	have this characteristic of they're hard to solve, but the solution is easy to check. Right? I mean,
9613600	9617360	if you have the ability to solve problems, whose solution isn't easy to check, then maybe you're
9617360	9623360	intelligent, but I can't decide whether intelligent or not. Interesting. Okay. And actually, Gary did,
9623360	9627440	he put a paper about 20 years ago talking about how neural networks can't extrapolate. I think it
9627440	9633040	was when he encoded numbers with a binary encoding or whatever. And we've been on a bit of a journey
9633040	9637360	on this. So we had Randall Bellistrier, I've interviewed him yesterday, he's got this paper
9637360	9642640	called the spline theory of neural networks. It basically says that a neural network decomposes
9642720	9646960	an input space into these input activated polyhedra. And when we first read that,
9646960	9652640	we felt that it kind of indicated Francois Chollet's assertion that neural networks are
9652640	9657600	locality sensitive hashing tables, and they only generalize within, you know, these tiny
9657600	9663360	polyhedra. And Randall's now updated this view to say in contrast to decision trees, these
9663360	9668480	hyperplanes, they actually inform a lot of information in the extrapolative regime outside
9668480	9673280	of the training range. So I always thought it was the inductive priors that gave the extrapolative
9673280	9678480	performance on neural networks by photocopying the information everywhere. And so like, you know,
9678480	9683040	this is a great example of where, you know, Gary might update his views because even basic MLPs
9683040	9688720	are far more extrapolative than anyone realized. This is a very interesting question. But the
9688720	9694160	way I would put it is that in that regard, in some sense, both of the sides are right.
9694880	9698400	And the reason they're both right is that we're in very high dimensional spaces.
9698400	9702800	Yeah. And we're in a very high dimensional space. The follow thing can happen, which is,
9702800	9708880	you know, you have a data point, and you generalize to a vast region around that data point. And it's
9708880	9714000	unfair to characterize these things as saying they just interpolate. In some sense, they really do
9714000	9720080	extrapolate. But at the same time, that vast region that they generalize correctly to is an
9720080	9725040	infinitesimal fraction of the much, much vaster reason that they have not generalized to but you
9725040	9731280	and I can. So you got to keep that distinction in mind. And then in particular, right, I like to
9731280	9737760	say that deep learning is nearest neighbor in curved space. And both parts of that are very
9737760	9745520	important, right? So, you know, Jan Lacoon was famous, you know, during the glory days of kernel
9745520	9751040	machines for saying that kernel machines are just glorified template matches. Right. And of
9751040	9754720	course, they didn't earn him any friends, but he was right. They really are just glorified template
9754720	9759440	matches. Kernel machine is really a souped up, more mathematically elegant and blah, blah,
9759440	9763840	blah version of nearest neighbor. Right. And the nearest neighbor is just a template matcher.
9764400	9768400	The beauty in the power of nearest neighbor, though, is that there is a neighborhood within
9768400	9774080	which often it generalizes very well. Right. Now, I think what Jan was missing, and I probably
9774080	9780960	still is, is that coordinates and deep learning, they are still just a glory. They are also glorified
9780960	9786400	nearest neighbor, except more glorified. And the way in which they're more glorified, which is
9786400	9790960	very important is that they are doing nearest neighbor in curved space. They are still just
9790960	9795760	doing, you know, generalization by similarity, which you could argue is all that machine learning
9795760	9800800	does is generalizing by similarity. Another notion of similarity can vary. Right. But the
9800800	9804080	important thing that they've done is that nearest neighbor just uses some distance
9804080	9809120	measured in the original space, whereas the neural networks are warping the space to make
9809120	9814880	the problem easier for the nearest neighbor, you know, essentially dot product based similarity
9816160	9820080	computation that they're actually doing. Oh, sure. But you're very much arguing,
9820080	9823120	this is the way Francois Chouelet puts it, that, you know, you have all of these
9824080	9830080	transformations and you kind of distort the space, you know, to represent the data manifold. And,
9830080	9834720	you know, you want it to, you stop SGD at the right time so that you approximate the data
9834720	9839600	manifold and you can do this kind of latent space, you know, interpolation on the geodesic of that
9839600	9844800	manifold. But, you know, Randall's idea is completely away from that idea of, you know,
9844800	9852160	these models learning this curved space. And so if you do slice the space up with these hyperplanes,
9852160	9856160	rather than it being a locality prior, which is what you're talking about, these hyperplanes give
9856160	9861360	you globally relevant information to things that are, you know, miles away from the training data.
9861360	9869040	Yeah, so, but these two perspectives are more similar than you might think, because I can take
9869680	9875680	a distorted version of space and decompose it into polyhedron, right? And one or the other might
9875680	9879920	approximate what's really going on better. I mean, these neural networks do form curved spaces,
9879920	9883840	except they're in practice, they're not curved because they find it, but ignoring that, right?
9884800	9892080	When, let me put it this way, an eloquent example of this is if you look back at the original space,
9892080	9898000	right? Again, treat this thing as a black box. Where does it generalize to? Does it generalize
9898000	9903280	only to things, neural networks as we have them today? Does it generalize correctly only to things
9903280	9910640	that are locally near the data point, or you can generalize well to things that are far, right?
9910640	9915760	And the thing is that with nearest neighbor, you buy, you know, almost intrinsically, you only
9915760	9921840	generalize period at all to things that are local. The beauty of deep learning and of the
9921840	9926000	space swapping that's going on is, again, going back to this notion of the path kernel is that
9926000	9930160	you're actually doing a nearest neighbor computation, not just in a space that's swapped,
9930160	9934240	but you're doing it in the space of gradients, which actually means that you can generalize
9934240	9939680	correctly to things that are very far from your examples, except they look similar in gradient
9939680	9945600	space. A very simple example of this is a sine wave, right? If I try to learn a sine wave using
9945600	9950880	nearest neighbor, I need an infinite number of examples, right? Because, you know, like what
9950880	9955840	I've learned over here helps me not at all with the next turn of the sine wave, like that continuous
9955840	9960400	extrapolation, right? At some point, there's this disaster where if the last piece of the
9960400	9965040	sine was going up, I just keep going up and getting more and more wrong, right? And in fact,
9965040	9969520	this kind of thing does happen in neural networks, but they also have the part to say like, and this
9969600	9975760	again, this also happens, which is I'm going to transform this space more into a more intelligent
9975760	9982320	one, which is the space of the slopes, right? And now if I've seen one cycle of the sine wave
9983120	9989680	with some density of examples, by similarity in that transformed space, I generalize correctly
9989680	9995760	and trivially to every other turn of the sine wave. So there's a very big fundamental difference
9995840	9999600	between the two. Interesting. And you think with an MLP, it would be possible to have that kind
9999600	10004240	of extrapolative generalization on a sine wave? Well, so people have studied this in multiple
10004240	10011840	ways. And the problem, so the question is, it depends on what are the basis functions that it's
10011840	10017520	using. Yes. So something that we didn't allude to at all in this conversation, but analyze all of
10017520	10021840	this is like, what is your choice of basis functions, right? And the thing is, an MLP with
10021920	10026560	the traditional, say, sigmoid or allude basis functions will not learn this, no matter, for
10026560	10031040	obvious reasons, right? And again, you can represent it, right? The representative theorem is there,
10031040	10035120	like the sine wave is just one sigmoid and then another one, you know, with a minus sign and
10035120	10041120	then another one, but the data doesn't let you learn it. If as a basis function, you have sine
10041120	10044960	waves, which is nothing unimaginable, that's what a Fourier transform is then, then you can learn
10044960	10050640	it so easily, it's not even funny. So it depends dramatically on the basis function. And the
10050640	10055360	question really becomes, what are the basis functions and the architect that let me generalize
10055360	10060240	correctly to a lot of things, including this, such that, for example, and this is a very simple test,
10060240	10066160	is like, I can nail a sine wave with a small number of examples without it being one of my basis
10066160	10070080	functions. Yeah, exactly. And then this all comes back to, you know, we're talking about inductive
10070080	10074160	prize and the bias variance trade off and even symmetries, actually. I mean, the Taco Cohen once
10074160	10079680	said that, you know, if you encode all of the symmetries into the label function, then you would
10079680	10084480	only need one labeled example. So it's always a trade off between how much induction are you doing?
10084480	10089680	Well, interesting, you should say that I understand why he says that and it's, and it's not technically
10089680	10095760	wrong. But I would say that practically what you need is such a set of symmetries per region of
10095760	10102160	the space, per cluster, right? But, you know, in another way, I would actually make an even
10102160	10108080	stronger statement, which again, is very perfectly mathematical, sounds same when you say, an object
10108080	10114240	is just the sum of its symmetries or a function. If you tell me all the symmetries, every last one
10114240	10120320	of an object, you've defined the object. So if I can learn the symmetries at that level, I don't
10120320	10124640	need anything else. Of course, as we already discussed, that's not the whole answer. Likewise,
10124640	10130000	with any function, if you tell me all the properties of the function, there are there, you know,
10130560	10134160	to be more precise, all the symmetries of a function at some point, you've told me the whole
10134160	10138640	function. And vice versa, from the function, I can, you know, I can read out all the symmetries
10138640	10142960	that it has. In principle, doing that in practice can be, you know, a very difficult and subtle
10142960	10147120	thing to do. That's a beautiful thing to say. You give me the symmetries and I'll give you the
10147120	10151440	object. Yeah, exactly. Amazing. Professor Pedro Domingos, thank you so much for joining us today.
10151440	10153840	It's been an honor. Thanks for having me. Amazing.
