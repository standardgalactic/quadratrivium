1
00:00:00,000 --> 00:00:07,080
Okay, well in which case let's crack on. So ladies and gentlemen get ready to meet

2
00:00:07,080 --> 00:00:12,000
the cunning maverick of Silicon Valley, the one and only George Hotz. Renowned

3
00:00:12,000 --> 00:00:16,560
for his daring exploits, Hotz commands an enigmatic persona which merges the

4
00:00:16,560 --> 00:00:22,080
technical finesse of Elon Musk and the wit of Tony Stark and the charm of a true

5
00:00:22,080 --> 00:00:26,840
tech outlaw. Now many of you would have or indeed should have seen this man on

6
00:00:26,840 --> 00:00:31,800
Lex's podcast recently for the third time no less. From craftily jailbreaking the

7
00:00:31,800 --> 00:00:36,940
supposedly invincible iPhone to outsmarting the mighty PlayStation 3,

8
00:00:36,940 --> 00:00:41,800
he's proven that no tech fortress is impregnable. Once targeted for his

9
00:00:41,800 --> 00:00:46,560
audacious creativity by Sony with a lawsuit, this hacker wizard stoically

10
00:00:46,560 --> 00:00:50,920
danced past the curveballs thrown by the tech giants all achieved with the

11
00:00:50,920 --> 00:00:55,400
graceful swag of a street smart prodigy. Now when he's not outfoxing major

12
00:00:55,400 --> 00:00:59,920
corporations, you'll find him at the heart of the avant-garde of AI technology,

13
00:00:59,920 --> 00:01:04,360
gallantly trailblazing through the wilds of the tech front here. He's currently

14
00:01:04,360 --> 00:01:08,880
building a startup called Micrograd which is building superfast AI running on

15
00:01:08,880 --> 00:01:14,280
modern hardware and truly he's the James Bond of Silicon Valley minus the

16
00:01:14,280 --> 00:01:19,440
martinis of course. Now please welcome the unparalleled code cowboy, the

17
00:01:19,480 --> 00:01:26,280
unapologetic techno-mancer, George Hots! Whoo! Anyway, also joining us for the big

18
00:01:26,280 --> 00:01:30,920
fight this evening is the steadfast sentinel of AI safety, Conor Leahy.

19
00:01:30,920 --> 00:01:35,960
Undeterred by the sheer complexity of artificial intelligence, Conor braves

20
00:01:35,960 --> 00:01:40,680
the cryptic operations of text generating models with steely resolve. Now

21
00:01:40,680 --> 00:01:44,000
about two years ago Conor took on the Herculean task of safeguarding

22
00:01:44,000 --> 00:01:48,960
humanity from a potential AI apocalypse. His spirit is relentless, his

23
00:01:49,000 --> 00:01:54,560
intellect razor sharp and his will to protect is unwavering. Now drawing on

24
00:01:54,560 --> 00:02:01,840
his contentious claim that we are super, super fucked. Yeah, Conor channels the

25
00:02:01,840 --> 00:02:05,800
urgency of our predicament into his work. Now his startup conjecture isn't just a

26
00:02:05,800 --> 00:02:09,880
glorified tech endeavor but it's a lifeboat for us all racing against the

27
00:02:09,880 --> 00:02:14,400
breakneck speed of AI advancement with the fates of nations possibly at stake.

28
00:02:15,040 --> 00:02:20,120
He's determined to break the damning prophecy and render us super, super saved.

29
00:02:20,640 --> 00:02:25,880
So brace for a showdown as Conor Leahy, the maverick defender of AI's boundaries

30
00:02:25,920 --> 00:02:30,720
strides into the ring. Now the man who declared we are super, super fucked is

31
00:02:30,720 --> 00:02:35,920
here to prove just how super, super not fucked we could be if we make the right

32
00:02:35,920 --> 00:02:40,560
decisions today. So please give it up for Mr. Conor, super, super Leahy. Whoo!

33
00:02:41,320 --> 00:02:44,920
Now Conor, I'd appreciate it if you don't go down in the fourth. I want this

34
00:02:44,920 --> 00:02:48,600
fight to go the distance. Now we're running for 90 minutes this evening.

35
00:02:48,600 --> 00:02:55,280
There'll be a 10 minute openers from, we said hots, didn't we, from Hots first

36
00:02:55,600 --> 00:03:00,240
and then Conor and I'll only step into the ring if the punch up gets two out of

37
00:03:00,240 --> 00:03:03,320
hands and unfortunately we won't be taking live questions today because we

38
00:03:03,320 --> 00:03:07,320
want to maximize the carnage on the battlefield. George Hots, your opening

39
00:03:07,320 --> 00:03:08,040
statements please.

40
00:03:09,800 --> 00:03:13,800
Um, yeah, we're super, super fucked. I think I agree with you.

41
00:03:15,440 --> 00:03:16,480
But that was a short fight.

42
00:03:16,520 --> 00:03:21,960
Yeah, look, I think, okay, so to make my opening statement clear and why maybe

43
00:03:21,960 --> 00:03:26,040
it doesn't make that much sense for me to go first, I think that the trajectory

44
00:03:26,040 --> 00:03:33,880
of all of this was somewhat inevitable, right? So you have humans over time and

45
00:03:33,880 --> 00:03:38,520
you can look at a 1980 human and a 2020 human. They look pretty similar, right?

46
00:03:38,560 --> 00:03:40,680
Ronald Reagan, Joe Biden, you know, that's all the same.

47
00:03:41,920 --> 00:03:48,120
Whereas a 1980 computer is like an Apple II and a 2020 computer is a M1 Max

48
00:03:48,120 --> 00:03:52,480
MacBook, like lines looking like this, right? So you have one line like this,

49
00:03:52,480 --> 00:03:55,720
one line like this, these lines eventually cross and I don't see any

50
00:03:55,720 --> 00:03:59,800
reason that line is going to stop, right? I've seen a few of the other guests

51
00:03:59,800 --> 00:04:04,480
argue something like, well, LLMs can't problem solve, but it doesn't matter.

52
00:04:04,520 --> 00:04:09,760
Like if this one can't, the next one will, whatever you call, I don't believe

53
00:04:09,760 --> 00:04:12,600
that there's a step function. I don't believe that like, oh, now it's conscious

54
00:04:12,600 --> 00:04:17,040
or now it's intelligent. I think it's all on a gradient. And I think this gradient

55
00:04:17,040 --> 00:04:22,080
will continue to go up, will approach human level and will pass human level.

56
00:04:23,400 --> 00:04:28,880
Now, this belief that we are uniquely fucked because of this, the amount of

57
00:04:28,880 --> 00:04:32,680
power in the world is about to increase, right? When you think about power and you

58
00:04:32,680 --> 00:04:35,800
think about, straight up, you can just talk about energy usage. The amount of

59
00:04:35,800 --> 00:04:39,600
energy usage in the world is going to go up. The amount of intelligence in the

60
00:04:39,600 --> 00:04:44,240
world is going to go up. We may be able to do some things to slow it down or

61
00:04:44,240 --> 00:04:48,240
speed it up based on political decisions, but it doesn't matter. The trajectory

62
00:04:48,240 --> 00:04:52,520
is up or major catastrophe, right? The only way it goes down is through war,

63
00:04:52,520 --> 00:04:56,400
nuclear annihilation, bio annihilation, meteor impact, some kind of major

64
00:04:56,400 --> 00:05:01,680
annihilation. That's what's going on. What we can control and what I think is

65
00:05:01,680 --> 00:05:06,120
super important we control is what the distribution of that new power looks

66
00:05:06,120 --> 00:05:13,480
like. I am not afraid of super intelligences. I am not afraid to live in

67
00:05:13,480 --> 00:05:19,720
a world among super intelligences. I am afraid if a single person or a small

68
00:05:19,720 --> 00:05:24,520
group of people has a super intelligence and I do not. And this is where we get

69
00:05:24,520 --> 00:05:30,480
to chicken man. A chicken man is the man who owns the chicken farm. There's many

70
00:05:30,480 --> 00:05:34,760
chickens in the chicken farm and there's one chicken man. It is unquestionable

71
00:05:34,760 --> 00:05:41,280
that chicken man rules. And if you believe chicken man rules because of his size, I

72
00:05:41,280 --> 00:05:46,440
invite you to look at cow man who also rules the cows and the cows are much

73
00:05:46,440 --> 00:05:50,000
larger than him. Chicken man rules because of his intelligence. This is basic

74
00:05:50,000 --> 00:05:53,560
less wrong stuff. Everyone kind of knows this. How the squishy things take over the

75
00:05:53,560 --> 00:06:00,240
world. Look, I agree with Elias Yudkowski all up to Nuke the data centers, right. So I

76
00:06:00,240 --> 00:06:06,400
do not want to be a chicken. And if people decide they are going to restrict

77
00:06:06,400 --> 00:06:11,520
open source AI or make sure I can't get access to the compute and only trusted

78
00:06:11,520 --> 00:06:15,680
people like chicken man get access to the compute. Well, shit, man, I'm the chicken.

79
00:06:15,680 --> 00:06:22,960
And yeah, I don't want to be the chicken. So I think that's my are we fucked? Maybe.

80
00:06:23,560 --> 00:06:28,960
Um, I agree that that intelligence is very dangerous. How can you look at

81
00:06:28,960 --> 00:06:32,160
intelligence and not say it's very dangerous, right? Intelligence is

82
00:06:32,160 --> 00:06:38,600
somehow safe. But things like nuclear bombs are an extremely false

83
00:06:38,600 --> 00:06:43,240
equivalency because what does a nuclear bomb do besides blow up and kill

84
00:06:43,240 --> 00:06:48,040
people? Intelligence has the potential to make us live forever. Intelligence has

85
00:06:48,040 --> 00:06:51,840
the potential to let us colonize the galaxy. Intelligence has the potential

86
00:06:51,920 --> 00:06:59,400
to meet God. Nuclear bombs do not they just blow up. Um, so I think the question

87
00:06:59,400 --> 00:07:03,200
and like, you have things like crypto, which are a clear advantage to the

88
00:07:03,200 --> 00:07:06,120
defender, at least today. And you have things like nuclear bombs, which are

89
00:07:06,120 --> 00:07:13,080
clear advantage to the attacker. AI, it's unclear. I think the best defense

90
00:07:13,160 --> 00:07:16,720
against an AI trying to manipulate me. And that's what I'm really worried

91
00:07:16,720 --> 00:07:19,680
about future psyops, you know, we're already seeing it today with the voice

92
00:07:19,720 --> 00:07:22,480
changer stuff. Like, you're never going to know who's human. The world's

93
00:07:22,480 --> 00:07:28,560
about to get crazy. Um, the best defense I could possibly have is an AI in my

94
00:07:28,560 --> 00:07:33,600
room being like, Don't worry, I got you. It's you and me. We're on a team. We're

95
00:07:33,600 --> 00:07:38,200
aligned. I'm not worried about alignment as a technical challenge. I'm worried

96
00:07:38,200 --> 00:07:42,040
about alignment as a political challenge. Google doesn't like me. Open AI

97
00:07:42,040 --> 00:07:47,280
doesn't like me. But me and my computer, you know, we like each other. We're

98
00:07:47,280 --> 00:07:52,080
aligned. And we're standing against the world that has always since the

99
00:07:52,080 --> 00:07:56,440
beginning of history, maximally been trying to screw you over, right?

100
00:07:57,160 --> 00:08:00,360
Intelligence, people think that one super intelligence is going to come and

101
00:08:00,360 --> 00:08:04,440
be unaligned against humanity. All of humanity is unaligned against each

102
00:08:04,440 --> 00:08:10,400
other. I mean, we have some common values, but really, come on, everyone's

103
00:08:10,400 --> 00:08:13,440
trying to scam everybody. The only reason you really team up with someone else

104
00:08:13,440 --> 00:08:17,720
is like, Hey, man, what if we team up and scam them, right? And what if we

105
00:08:17,720 --> 00:08:22,240
team up, call ourselves America and we, we, we, uh, we build a big army and say

106
00:08:22,240 --> 00:08:27,840
we're free and independent. Yeah. Right. It's that force that has made humanity

107
00:08:27,840 --> 00:08:32,680
cooperate humanity by default. He's very unaligned and has every kind of

108
00:08:32,680 --> 00:08:36,120
belief under the sun. So I'm not worried about AI showing up with a new

109
00:08:36,120 --> 00:08:39,440
belief under the sun. I'm not worried about the amount of intelligence

110
00:08:39,440 --> 00:08:43,520
increasing. I'm worried about a few entities that are unaligned with me

111
00:08:43,720 --> 00:08:48,400
acquiring Godlike powers and using them to exploit me. I think that's my

112
00:08:48,400 --> 00:08:49,000
opening statement.

113
00:08:50,680 --> 00:08:56,920
Cool. Yeah. Thanks. That's, uh, that's, I mean, yeah, I also kind of agree with

114
00:08:56,920 --> 00:09:00,040
you. And most of the things you say, there's a few details I'd like to

115
00:09:00,040 --> 00:09:04,400
dig into there, but for most of the things you say, I do think I agree with

116
00:09:04,400 --> 00:09:08,480
you here. I think it's absolute. Let me just like, start with saying, I

117
00:09:08,480 --> 00:09:13,980
totally agree with you that misuse and like, you know, bad actors, what using

118
00:09:13,980 --> 00:09:19,680
AGI is a horrible, dangerous outcome. That's, that's like, you know,

119
00:09:19,680 --> 00:09:23,480
sometimes the, the, uh, less wrong, you know, crowd likes to talk about X

120
00:09:23,480 --> 00:09:27,520
risk, but also sometimes I've talked about S risk, suffering risks. So things

121
00:09:27,520 --> 00:09:32,680
are worse than death. I believe that you can probably almost only get S

122
00:09:32,680 --> 00:09:37,040
risks from misuse. I don't think you can get S risks. Probably like, you

123
00:09:37,040 --> 00:09:40,960
can, but it's extremely unlikely to get it from like just like raw

124
00:09:40,960 --> 00:09:45,760
misalignment. Like you'd have to like get extraordinarily unlucky. So while I

125
00:09:45,760 --> 00:09:51,280
do it, so I do think, for example, a very, you know, controllable AGI or

126
00:09:51,280 --> 00:09:55,720
super intelligence in the hand of sadistic psychopath is significantly in

127
00:09:55,720 --> 00:09:59,560
a sense worse than a paperclip maximizer. So I think this is something

128
00:09:59,560 --> 00:10:04,400
we would agree on probably. So I think I'm to think of pretty much on board

129
00:10:04,400 --> 00:10:08,640
with you on a lot of things there, where I think things come aboard a bit

130
00:10:08,640 --> 00:10:12,920
of a tale as I think there's two points where I would like to take as my

131
00:10:12,920 --> 00:10:17,400
opening statement, two things I want to talk about. The first one is I want to

132
00:10:17,400 --> 00:10:22,160
talk about the technical problem of alignment. So am I concerned about the

133
00:10:22,320 --> 00:10:26,240
kinds of things like misuse and like small groups of people centralizing

134
00:10:26,240 --> 00:10:30,360
power potentially for nefarious deeds? Yeah, I think this is a very, very

135
00:10:30,360 --> 00:10:32,720
significant problem that I do think about a lot. And that'll be the second

136
00:10:32,720 --> 00:10:35,600
thing I want to talk about. The first thing I want to talk about is that I

137
00:10:35,600 --> 00:10:38,240
don't even think we're going to make it to that point. I don't think we're

138
00:10:38,240 --> 00:10:42,560
going to get to the point where anyone has a super intelligence that's

139
00:10:42,560 --> 00:10:46,880
helping them out. We're going to, if we don't solve very hard technical

140
00:10:46,880 --> 00:10:50,280
problems, which are currently not on track to being solved, by default, you

141
00:10:50,280 --> 00:10:54,240
don't get a bunch of, you know, super intelligence and boxes working with a

142
00:10:54,240 --> 00:10:57,440
bunch of humans. You get a bunch of super intelligence, you know, fighting each

143
00:10:57,440 --> 00:11:01,120
other, working with each other and just ignoring humans. Humans just get cut

144
00:11:01,120 --> 00:11:05,560
out entirely from the process. And even then, you know, it's, you know, whether

145
00:11:05,560 --> 00:11:09,040
one takes over or they find an equilibrium, I don't know, like, you know, who

146
00:11:09,040 --> 00:11:12,280
knows what happens to that point. But by default, I wouldn't expect humans to be

147
00:11:12,280 --> 00:11:15,640
part of the equilibrium anymore. Once you're, once you're the chicken man, well,

148
00:11:15,640 --> 00:11:19,560
why do you need chickens? You know, if, you know, maybe if they provide some

149
00:11:19,560 --> 00:11:22,680
resource for you, the reason humans have chickens is that they make chicken

150
00:11:22,680 --> 00:11:26,600
breasts. I mean, personally, I wouldn't like to be harvested for chicken

151
00:11:26,600 --> 00:11:30,880
breasts, just my personal opinion. I consider this a pretty bad outcome.

152
00:11:31,960 --> 00:11:35,880
But even then, well, as a chicken man finds a better way of chicken breasts or, you

153
00:11:35,880 --> 00:11:39,520
know, modifies himself to no longer need food, I expect the chickens are not going

154
00:11:39,520 --> 00:11:42,160
to be around for much longer. You know, once we stopped using horses for

155
00:11:42,160 --> 00:11:46,440
transportation, didn't go very well for the horses. So that's kind of the first

156
00:11:46,440 --> 00:11:50,520
part of my point that I'd like to, you know, maybe hear your opinions on, hear

157
00:11:50,520 --> 00:11:54,800
your thoughts on, is that I think the technical control is actually very hard.

158
00:11:55,080 --> 00:11:59,480
And I don't think it's unsolvable by any means. I think like, you know, you and

159
00:11:59,480 --> 00:12:02,760
like, you know, a bunch of other smart people work on this for like 10 years, I

160
00:12:02,760 --> 00:12:06,200
think you can solve it, but it's not easy. And it has to actually happen. And

161
00:12:06,200 --> 00:12:10,120
there is a deadline for this. The second point I want to bring up is kind of

162
00:12:10,120 --> 00:12:13,520
where you talk about how humans are unaligned. I think this is partially

163
00:12:13,520 --> 00:12:18,520
definitely true. I think I'm unusually, I am the more optimistic of the two of

164
00:12:18,520 --> 00:12:23,800
us in this scenario, not a role I often have in these discussions, where I

165
00:12:23,800 --> 00:12:27,480
actually think the amount of coordination that exists between humanity,

166
00:12:27,520 --> 00:12:31,800
especially in the modern world is actually astounding. Every single time two

167
00:12:31,800 --> 00:12:36,080
adult human males meet and don't kill each other is a miracle. Have you seen

168
00:12:36,080 --> 00:12:39,840
what happens when two adult male chimps from two different warbands meet each

169
00:12:39,840 --> 00:12:43,560
other? It doesn't go very well. And those are already pretty well coordinated

170
00:12:43,560 --> 00:12:47,320
animals because they can have warbands. What happens when, you know, two male

171
00:12:47,600 --> 00:12:51,640
bugs or, you know, I don't know, sea slugs meet each other, you know, either

172
00:12:51,640 --> 00:12:55,720
they ignore each other or, you know, things go very poorly. This is the

173
00:12:55,720 --> 00:12:59,440
default outcome. The true unaligned outcome, the true default state of

174
00:12:59,440 --> 00:13:03,800
nature is you can't have two adult males in the same room at any time. I saw

175
00:13:03,800 --> 00:13:07,480
this funny video on Twitter the other day where it was like, I know, some

176
00:13:07,480 --> 00:13:11,280
parliament, I think in East Europe or something, and there's this big guy

177
00:13:11,280 --> 00:13:14,200
who's just like going at this politician, he's like in his face, he's like

178
00:13:14,200 --> 00:13:17,760
screaming, he was like going everywhere, and not a single punch will

179
00:13:17,760 --> 00:13:22,080
throw him. No, then no one took out a knife, no one took out a gun. And I

180
00:13:22,080 --> 00:13:27,080
was like, wow, the fact that we're so civilized and we're so aligned to

181
00:13:27,080 --> 00:13:32,080
each other that we can have something this barbaric happen and no one throws

182
00:13:32,080 --> 00:13:36,800
a punch is actually shocking. This is very unusual, even for humans. If you

183
00:13:36,800 --> 00:13:42,960
go back 200 years, punches and probably gunshots would have flown. So this is

184
00:13:42,960 --> 00:13:46,360
not to say that humans have some inherent special essence that we're good,

185
00:13:46,600 --> 00:13:51,480
that we have solved goodness or any means. What I'm saying is the way I like

186
00:13:51,520 --> 00:13:54,840
to think about it is that coordination is a technology, is a technology you

187
00:13:54,840 --> 00:13:58,200
can improve upon. It is you can develop new methods of coordination, you can

188
00:13:58,200 --> 00:14:01,920
develop new structures, new institutions, new systems. And I think it's very

189
00:14:01,920 --> 00:14:04,760
tempting for us living in this modern world to, it's kind of like a fish and

190
00:14:04,760 --> 00:14:08,360
water effect. We forget how much of our life, a lot of our life is built on

191
00:14:09,160 --> 00:14:13,760
atoms, on physical technology, a lot of it's built on digital technology, but

192
00:14:13,760 --> 00:14:20,240
a lot of it is on social technology. And when I look at how does the world

193
00:14:20,240 --> 00:14:24,200
go well? Like, you know, should it be only the special elites get control of

194
00:14:24,200 --> 00:14:27,920
the AI? I'm like, well, that's not really how I think about it. And I think

195
00:14:27,920 --> 00:14:31,320
about it way more is, what is a coordination mechanism where we can

196
00:14:31,320 --> 00:14:35,120
create a coordination selling point or we can create a group, an institution, a

197
00:14:35,120 --> 00:14:41,280
system of some kind that where people will have game theoretic incentives to

198
00:14:41,280 --> 00:14:44,440
cooperate on the system, the results in something that is net positive for

199
00:14:44,440 --> 00:14:49,000
everyone. Because the truth is, is that positive some games do exist. And

200
00:14:49,000 --> 00:14:53,000
they're actually very profitable. And they're very good. And I think if we

201
00:14:53,000 --> 00:14:56,440
can turn, you know, you can turn any positive some game into a net into a

202
00:14:56,440 --> 00:14:59,560
zero or a negative some game pretty easily. It's much easier to destroy than

203
00:14:59,560 --> 00:15:03,680
is to create. But I think it's absolutely possible to create coordination

204
00:15:03,680 --> 00:15:07,520
technology around AI and to build coordination mechanisms that are net

205
00:15:07,520 --> 00:15:12,360
positive for everyone involved. So those would be like my two points. Happy to

206
00:15:12,360 --> 00:15:15,040
dig into anyone's you think would be it'll lead to an interesting

207
00:15:15,040 --> 00:15:20,080
interaction. Sure. So I'll start with two and then go to one. So two, I moved

208
00:15:20,080 --> 00:15:24,760
to Berkeley in 2014. And I threw myself at the Mary cult. I showed up at the

209
00:15:24,760 --> 00:15:32,320
Mary office and I'm like, Hi, I'm here to join your cult. And what I started to

210
00:15:32,320 --> 00:15:41,440
realize was, Mary, and less wrong in general, have a very poor grip on the

211
00:15:41,480 --> 00:15:47,360
practicalities of politics. Very much. I think there was sort of a split, you

212
00:15:47,360 --> 00:15:54,280
know, Curtis Yavin, like neoreaction. This is a spin off of rationality. And it's

213
00:15:54,280 --> 00:15:58,360
a spin off rationality that understood the truth about human nature. So when I

214
00:15:58,360 --> 00:16:01,520
give you that, you give that example of two chimps meeting in the woods and

215
00:16:01,520 --> 00:16:06,480
they're going to fight. If I'm one of those chimps, at least I stand a chance.

216
00:16:06,560 --> 00:16:11,720
Right. He might beat my ass. I might beat his. But if I come up against the FBI,

217
00:16:12,640 --> 00:16:17,280
things do not look good for me. In fact, things so much do not look good for me.

218
00:16:17,320 --> 00:16:24,320
There's no way I'm going to beat the FBI. The modern forces are so powerful that

219
00:16:24,320 --> 00:16:28,960
this is not a, Oh, we've established a nice cooperative shelling point. This is

220
00:16:28,960 --> 00:16:33,320
a, we have pounded so much fear into these people that they would never even

221
00:16:33,320 --> 00:16:39,440
think of throwing a puncher firing a gun. We have made everybody terrified. And

222
00:16:39,440 --> 00:16:43,280
this isn't good. We didn't, we didn't achieve this through some enlightened

223
00:16:43,280 --> 00:16:48,440
cooperation. We achieved this through a massive propaganda effort. Right. It's

224
00:16:48,440 --> 00:16:52,880
the joke about, you know, the American soldier goes over to Russia and it's

225
00:16:52,880 --> 00:16:56,680
like, man, you guys got some real propaganda here. And that the Russian

226
00:16:56,680 --> 00:17:01,000
soldiers like, Yeah, no, I know it's bad, but it's not as bad as yours. And the

227
00:17:01,000 --> 00:17:06,440
American soldiers like what propaganda? And the Russian just laughs, right? So, so

228
00:17:06,440 --> 00:17:11,720
this, this didn't occur because of this occurred because of a absolute tyrannical

229
00:17:11,720 --> 00:17:18,240
force decided to dominate everybody, right? Um, now, Oh, I think so. I think there's

230
00:17:18,240 --> 00:17:21,600
a way out of this. I think there actually is a way out of this, right? And I wrote

231
00:17:21,600 --> 00:17:25,560
a blog post about this called individual sovereignty. And I think a really nice

232
00:17:25,600 --> 00:17:31,080
world would be if all the stuff to live food, water, health care, electricity

233
00:17:31,240 --> 00:17:35,040
were generatable off the grid in a way that you are individually sovereign. And

234
00:17:35,040 --> 00:17:39,360
this comes back to my point about offense and defense, right? If I have a world

235
00:17:39,360 --> 00:17:43,240
where you don't want it to be extreme defense, you don't want every person to

236
00:17:43,240 --> 00:17:47,440
be able to completely insulate them. But you want like, okay, it takes a whole

237
00:17:47,440 --> 00:17:51,200
bunch of other people to gang up to take that guy out, right? Like that's, that's

238
00:17:51,200 --> 00:17:55,960
a good, that's a good balance. And the balance that we live in today is there

239
00:17:55,960 --> 00:18:01,080
is one pretty much a unipolar world. I mean, thank God for China. But you know,

240
00:18:01,080 --> 00:18:05,200
there's one, there's one unipolar world, you got America and where are you going

241
00:18:05,200 --> 00:18:09,880
to run? I'll pay taxes. I don't care if you live overseas, right? So yeah, my point

242
00:18:09,880 --> 00:18:13,560
about the coordination is that if you're okay with solving coordination

243
00:18:13,560 --> 00:18:19,720
problems by using a single, a singleton super intelligent AI to make everybody

244
00:18:19,720 --> 00:18:25,360
cower in fear and tyrannize the future. Sure, you'll get coordination. Yeah, that

245
00:18:25,360 --> 00:18:28,840
works. That works. I'm the only guy with a gun and I got 10 a year. I got a name

246
00:18:28,840 --> 00:18:31,520
that all 10 of you and you can all die or listen to me your choice.

247
00:18:31,520 --> 00:18:36,880
So I'm curious about, so I understand what you're saying. And I think you make

248
00:18:36,880 --> 00:18:41,040
some decent points. But I think I view the world a bit differently from you. And

249
00:18:41,040 --> 00:18:44,720
I'd like to like dig into that a little bit. So like, who do you think is less

250
00:18:44,720 --> 00:18:49,160
afraid? Someone living just a medium person living in the United States of

251
00:18:49,160 --> 00:18:55,720
America, or the medium person living in Somalia? Sure, America less afraid. Well,

252
00:18:55,720 --> 00:18:59,200
that's kind of strange. Somalia doesn't have a government. They have much less

253
00:18:59,200 --> 00:19:02,200
tyranny. You're much more you can just buy a rocket launcher and just like live

254
00:19:02,200 --> 00:19:04,400
on a farm and just like, you know, kill your neighbors and no one's gonna stop

255
00:19:04,400 --> 00:19:08,720
you. So like, how does that interact with your old you? Those who will trade

256
00:19:08,720 --> 00:19:11,400
liberty for safety deserve neither.

257
00:19:13,200 --> 00:19:16,000
That sorry, I don't understand. Could you elaborate a bit more?

258
00:19:16,240 --> 00:19:23,520
Um, in Somalia, you have a chance in America, you do not, right? I am okay. I

259
00:19:23,520 --> 00:19:27,160
would rather live in fear. I would rather be worried about someone shooting a

260
00:19:27,160 --> 00:19:32,960
rocket launcher at me than to have an absolutely tyrannical government. Just,

261
00:19:33,120 --> 00:19:38,480
you know, just just like like like a managerial class. Not saying, by the way,

262
00:19:38,600 --> 00:19:42,120
I agree with you that these things are possible. I agree with you that the

263
00:19:42,120 --> 00:19:45,120
less wrong notion of politics is possible. I would love to live in these

264
00:19:45,120 --> 00:19:51,680
sort of worlds, but we don't. The practical reality of politics is so much

265
00:19:51,680 --> 00:19:57,000
more brutal. And it just comes from a straight up instinct to dominate, not an

266
00:19:57,000 --> 00:20:01,240
instinct, you know, government by the people for the people is branding.

267
00:20:02,040 --> 00:20:06,480
Yeah, I mean, yeah. So to be clear, I very much do not agree with less wrongs

268
00:20:06,480 --> 00:20:10,960
views and politics and a bit of an outcast for how I view how conflict theory

269
00:20:10,960 --> 00:20:14,840
I view politics. But this is, I feel like you're kind of dodging the question

270
00:20:14,840 --> 00:20:17,520
you're just a little bit. Is it like, well, if that's true, why aren't you

271
00:20:17,520 --> 00:20:18,280
living in Somalia?

272
00:20:20,840 --> 00:20:25,600
I know people who've done it, right? It's very hard. It's very hard psychologically.

273
00:20:26,120 --> 00:20:31,120
Okay. So like tigers love charm, it turns out, right? A tiger does not want to

274
00:20:31,120 --> 00:20:34,520
chase down an antelope, right? A tiger would love to just sit in the zoo and

275
00:20:34,520 --> 00:20:39,160
eat the charm, right? And like, it takes a very strong tiger to reject that.

276
00:20:39,360 --> 00:20:42,960
And I'm not that strong. I hope there's people out there who are. I hope there's

277
00:20:42,960 --> 00:20:47,200
people out there who are actually like, you know, I'm just not a weak little

278
00:20:47,200 --> 00:20:50,040
bitch. That's why I don't live in Somalia, right?

279
00:20:50,240 --> 00:20:55,400
Okay. I mean, that's a fair answer, but I am a bit confused here. So you're

280
00:20:55,400 --> 00:21:00,040
saying that living in Somalia would be better by some metric, but you're also

281
00:21:00,040 --> 00:21:05,840
saying you prefer not living in Somalia. So I'm a bit confused because like, from

282
00:21:05,840 --> 00:21:09,520
my perspective, I want to live in a country I want to live in. And that's the

283
00:21:09,520 --> 00:21:14,440
one which I think is better. If I thought another country was better, then I would

284
00:21:14,440 --> 00:21:15,000
just move there.

285
00:21:15,080 --> 00:21:18,560
But let's, the tiger and the chum, I think, is a good analogy, right? Like, if

286
00:21:18,560 --> 00:21:22,920
you have a choice as a tiger, you can live in a zoo and you get a nice size

287
00:21:22,920 --> 00:21:26,600
pen, you know, the zookeepers are not abusive at all. You get fed this

288
00:21:26,600 --> 00:21:30,720
beautiful chopped up food. It's super easy. You sit there, get fat, lays around

289
00:21:30,720 --> 00:21:36,080
all day, or you can go to the wild. And in the wild, you're going to have to

290
00:21:36,080 --> 00:21:40,760
hunt. You might not succeed at hunting. It is just a, you know, it's a brutal

291
00:21:40,760 --> 00:21:46,680
existence. As a tiger, which one do you choose? Now, you say, Oh, well, obviously,

292
00:21:46,680 --> 00:21:50,000
you know, you're going to choose the chum one. Yeah, but do you see what you're

293
00:21:50,000 --> 00:21:51,960
giving up? Do you see?

294
00:21:51,960 --> 00:21:54,240
No, no, could you elaborate a little bit on what I'm giving up?

295
00:21:54,520 --> 00:22:02,680
You are giving up on the nature of tiger. You are effectively, okay, maybe I'll

296
00:22:02,680 --> 00:22:07,960
take this to an extreme, right? In the absolute extreme, the country that you

297
00:22:07,960 --> 00:22:13,120
would most rather live in is the one that basically wire heads you, right? The one

298
00:22:13,120 --> 00:22:16,760
and you can say that, Okay, well, I don't want to be wireheaded, but you know,

299
00:22:16,760 --> 00:22:19,880
there's a, there's a gradient that'll get you there, Gandhi in the pill, or, you

300
00:22:19,880 --> 00:22:25,920
know, if you can live in this country, you can be happy, feel safe and secure all

301
00:22:25,920 --> 00:22:31,920
the time. Don't worry exactly about how we're doing it, you know, but right. I

302
00:22:31,920 --> 00:22:37,080
mean, it takes a very strong person to, it's going to take a very strong person

303
00:22:37,120 --> 00:22:43,040
to say no to wireheading. So I understand. I'll give one more instrumental

304
00:22:43,040 --> 00:22:46,240
reason for living in America versus living in Somalia. If I thought that

305
00:22:46,240 --> 00:22:50,840
America and Somalia were both like steady states, I might choose Somalia. I

306
00:22:50,840 --> 00:22:54,360
don't think that. I think that being here, I have a much better way of

307
00:22:54,360 --> 00:22:59,640
escaping this, of escaping the constant tyranny that we're in. And I think a

308
00:22:59,640 --> 00:23:07,160
major way to do it is AI. I think that AI is, is if I really, if I had an AGI, if

309
00:23:07,160 --> 00:23:10,040
I had an AGI in my closet right now, I'll tell you what I'd do with it. I

310
00:23:10,040 --> 00:23:13,440
would have it build me a spaceship that could get me off of this planet and get

311
00:23:13,440 --> 00:23:17,200
out of here as close to the speed of light as I possibly could and put big

312
00:23:17,200 --> 00:23:20,720
shield up behind me, blocking all communication. That's what I would do if I

313
00:23:20,720 --> 00:23:24,480
had an AGI. And I think that's, you know, the right move. And I have a lot better

314
00:23:24,480 --> 00:23:28,040
chance of building that spaceship right here than I do in Somalia. Right. So I'll

315
00:23:28,040 --> 00:23:29,640
give an instrument. Yeah, that's right.

316
00:23:30,200 --> 00:23:33,480
That's a good instrument. Well, we'll miss you if you leave, though. No, it'll

317
00:23:33,480 --> 00:23:38,040
be real shame. It'll be everyone should do it. Like this is, this is the move,

318
00:23:38,040 --> 00:23:42,360
right? And like, let humanity blow. I mean, look, I agree with you that we're

319
00:23:42,360 --> 00:23:47,800
going to probably blow ourselves up, right? But I think that the path potentially

320
00:23:47,800 --> 00:23:52,720
through this probably looks different from the path you're imagining. I think

321
00:23:52,720 --> 00:23:58,320
that the reasonable position, I'm sorry. Oh, no, I think yeah, maybe we're done

322
00:23:58,320 --> 00:24:00,760
with this point. I can come back and have a response to your first one. I would

323
00:24:00,760 --> 00:24:05,160
like to, if you don't mind, just like fall on one string there as well. So one

324
00:24:05,160 --> 00:24:09,960
of the things you said is like, what will the tiger choose? And so my personal

325
00:24:10,200 --> 00:24:14,360
view of this kind of thing. And I think I want to think about coordination is I

326
00:24:14,360 --> 00:24:19,840
think of things. So you put a lot of view on this like fear based domination and

327
00:24:19,840 --> 00:24:23,040
so on. And I'm not going to deny that this isn't a thing that happens. I'm

328
00:24:23,040 --> 00:24:28,480
German. You know, like, you know, I have living relatives who can tell you some

329
00:24:28,480 --> 00:24:33,480
stories. Like I understand, like I understand. I'm not I'm not denying

330
00:24:33,480 --> 00:24:38,960
these things might be needs. What I'm saying, though, is, okay, let's say

331
00:24:40,080 --> 00:24:43,240
there was a bunch of tigers, you know, you and me and all the other tigers. And

332
00:24:43,240 --> 00:24:47,720
some of the tigers are like, man, fuck, this whole like nature shit is like

333
00:24:47,720 --> 00:24:52,040
really not working for me. How about we go build a zoo together? Who's in? And

334
00:24:52,040 --> 00:24:55,320
then other people like, Yeah, you know, actually, that sounds awesome. Let's do

335
00:24:55,320 --> 00:24:58,760
that. Do you think that's okay? Like you think that would be like a fair

336
00:24:58,760 --> 00:25:03,560
option for them to do? Sure. But that's not where zoos come from. I know. I

337
00:25:03,560 --> 00:25:06,680
know. I'm getting there. I'm getting there. Like, that is not where zoos come

338
00:25:06,680 --> 00:25:14,200
from. Sure. But the this analogy here is, of course, is that this is where a lot

339
00:25:14,200 --> 00:25:17,840
of human civilization comes. Not all of it. I understand that why France was

340
00:25:17,840 --> 00:25:20,920
doing well in the First World War was not because of democracy big nice. It was

341
00:25:20,920 --> 00:25:26,040
because democracy raises large armies. It's I'm very well aware of the

342
00:25:26,040 --> 00:25:31,560
real politic, as the Germans would say, about these kinds of factors. And I

343
00:25:31,560 --> 00:25:36,520
and I fully agree with you that a lot of the good things that we have are not by

344
00:25:36,520 --> 00:25:41,080
design, so to speak. You know, there are happy side effects, you know, capitalism

345
00:25:41,080 --> 00:25:45,640
is a credit assignment mechanism, you know, the fact that also results in us

346
00:25:45,640 --> 00:25:49,080
having cool video games and air conditioning. It's not an inherent

347
00:25:49,160 --> 00:25:56,360
feature of the system. It's it's an execution mechanism. And so totally

348
00:25:56,360 --> 00:26:00,600
grant all of this. I'm not saying that every coordination thing is good. I'm

349
00:26:00,600 --> 00:26:03,960
not saying that, you know, there aren't trade offs. Actually, you were talking

350
00:26:03,960 --> 00:26:06,520
about, I think, aesthetic trade offs. You're like, there's an aesthetic that

351
00:26:06,520 --> 00:26:11,480
the tiger loses. And well, I think personally, aesthetics are subjective. So

352
00:26:11,480 --> 00:26:14,040
I think this is something that different people. So the way I think about

353
00:26:14,040 --> 00:26:18,040
aesthetics is I think aesthetics are things you trade on is, you know, you

354
00:26:18,040 --> 00:26:22,680
might want tigers in the wild to exist. Okay, fair enough. That's a thing you

355
00:26:22,680 --> 00:26:26,760
can want. You know, someone else might want, you know, certain kinds of art to

356
00:26:26,760 --> 00:26:32,360
exist. They might want a certain kind of religion to be practiced or whatever.

357
00:26:32,360 --> 00:26:35,800
These are aesthetic preferences upon reality, which I think are very fair.

358
00:26:35,800 --> 00:26:39,640
So the way I personally think about this morally is I'm like, okay, cool.

359
00:26:39,640 --> 00:26:43,400
How can we maximize trade surplus so you can spend your resources on the

360
00:26:43,400 --> 00:26:48,440
aesthetics you want and I'll spend my resources on the, you know, things I

361
00:26:48,440 --> 00:26:51,880
want. Now, maybe the thing you describe where

362
00:26:51,880 --> 00:26:55,320
everyone just atomizes into their own systems, with their own value system,

363
00:26:55,320 --> 00:26:58,920
with their own aesthetics, completely separate from other, is the best outcome.

364
00:26:58,920 --> 00:27:01,400
Awesome. I think this is completely...

365
00:27:01,400 --> 00:27:04,440
Have you heard the Yodobar manifesto?

366
00:27:04,440 --> 00:27:09,880
I have not. You sure? The problem with this, everyone trades on their own

367
00:27:09,880 --> 00:27:14,280
aesthetics, is you will never be able to actually buy any aesthetics that are in

368
00:27:14,280 --> 00:27:20,680
conflict with the system, right? You won't. The system won't let you.

369
00:27:20,680 --> 00:27:26,360
Okay, by that logic, why do people have free time?

370
00:27:26,360 --> 00:27:29,160
Why don't they work all the time? Why doesn't capitalism extract literally

371
00:27:29,160 --> 00:27:32,680
every minute of them? Why do you think that is?

372
00:27:33,480 --> 00:27:36,840
I think it's because it turns out that we don't actually live in a capitalist

373
00:27:36,840 --> 00:27:40,920
society. I think China is a lot closer to a capitalist society than America.

374
00:27:40,920 --> 00:27:44,760
I think America is kind of communist and I think in a communist society, of

375
00:27:44,760 --> 00:27:47,480
course, you're going to get free time. It turns out that subsidizing all the

376
00:27:47,480 --> 00:27:51,320
homeless people is a great idea, right? If you want to keep power, again,

377
00:27:51,320 --> 00:27:55,000
do some absolute tyrannical mechanism. You do it, right?

378
00:27:55,000 --> 00:27:59,480
So why do we have free time? Well, you think it's some victory of capitalism.

379
00:27:59,480 --> 00:28:02,440
I think it's because we do not live in a capitalist country. I think China is more

380
00:28:02,440 --> 00:28:05,960
capitalist than America. I think it's because we trade on our aesthetics.

381
00:28:05,960 --> 00:28:08,520
I think that different people have different things to contribute to

382
00:28:08,520 --> 00:28:11,400
various systems, not necessarily capitalist or communist thing.

383
00:28:11,400 --> 00:28:15,560
I'm saying it's more energy. In the primordial environment, if you have

384
00:28:15,560 --> 00:28:17,720
to fight literally every single second and spend every

385
00:28:17,720 --> 00:28:21,240
jewel of energy you have to scrounge together another jewel of energy,

386
00:28:21,240 --> 00:28:24,360
you can't have free time. It's not about capitalism. This is about

387
00:28:24,360 --> 00:28:28,280
entropy. This is about these kind of things. We have energy excess.

388
00:28:28,280 --> 00:28:32,520
We have, we've produced systems that allow us to extract more energy for

389
00:28:32,520 --> 00:28:37,640
jewel we put in. And we can spend that extra energy on things such as free time.

390
00:28:37,640 --> 00:28:43,080
And the distribution of energy, power, coordination, whatever you want to call it

391
00:28:43,080 --> 00:28:45,880
is another question. Will you agree or disagree with this?

392
00:28:45,880 --> 00:28:49,720
I mean, I am taking an extreme position when I say that there are definitely

393
00:28:49,720 --> 00:28:53,720
positive sum coordination problems that are solved by governments, right?

394
00:28:53,720 --> 00:28:58,920
It is not all zero sum or negative sum, right? I'm not denying this.

395
00:28:58,920 --> 00:29:05,560
But what I'm saying is it's like, I don't know, man, like the existence of free time.

396
00:29:05,560 --> 00:29:10,040
Well, that's all great when you think you live in this surplus energy world, right?

397
00:29:10,040 --> 00:29:14,920
And maybe we do right now. But if some other country took this seriously, like China,

398
00:29:16,040 --> 00:29:19,880
who's going to win in a war? Who's going to win? Is it going to be the Chinese?

399
00:29:19,880 --> 00:29:24,040
You guys see the Chinese build a building? They got like 400 people there,

400
00:29:24,040 --> 00:29:27,240
and they're all there 24 hours a day, and they're getting the building built.

401
00:29:27,240 --> 00:29:30,600
You ever see Americans build a building? It's six guys, two of them are working,

402
00:29:30,600 --> 00:29:33,160
two of them are shift supervisors, and two of them are on lunch breaks.

403
00:29:33,960 --> 00:29:36,520
Oh, you got your free time. You got your aesthetic preferences.

404
00:29:36,520 --> 00:29:40,200
You deserve to lose in a war, right? This country deserves to lose in a war

405
00:29:40,200 --> 00:29:41,640
if they keep acting the way they're acting.

406
00:29:42,200 --> 00:29:46,200
So I definitely see the point you're making. And this is personally not a thing I want to

407
00:29:46,200 --> 00:29:53,000
defend too far because I'm not a military expert. But I will note that the US has like

408
00:29:53,000 --> 00:29:56,920
37 aircraft carriers, and the Chinese have like two, and Americans are like,

409
00:29:57,480 --> 00:30:01,960
somehow, you know, despite being so lazy and oh, no, they have all this, you know,

410
00:30:01,960 --> 00:30:05,640
all this free time or whatever. Somehow they're still a military hegemon or whatever.

411
00:30:05,640 --> 00:30:09,880
And like they're the biggest rival Russia fighting this backwards water country in

412
00:30:09,880 --> 00:30:13,320
Ukraine suddenly folds and lose like three quarters of the military.

413
00:30:13,320 --> 00:30:20,360
It's what I'm saying is if you have massive hegemony, if you have truly a obnoxious victory,

414
00:30:20,360 --> 00:30:24,600
the way it should look is if you laze around all the time and you look like a fucking idiot and

415
00:30:24,600 --> 00:30:31,320
you still win. Yes. And I'm not talking about Russia. Russia has a GDP the size of Italy.

416
00:30:31,320 --> 00:30:36,680
This is China here. You might say that China has two aircraft carriers in the US has 37.

417
00:30:36,680 --> 00:30:40,520
Why do we have aircraft carriers? Who has more drone building capacity?

418
00:30:40,520 --> 00:30:46,200
The Chinese are the United States. If the future is fought with AI swarm drone warfare,

419
00:30:46,200 --> 00:30:50,120
the Chinese can make, you know, a million drones a day in the US can make.

420
00:30:50,840 --> 00:30:52,840
I don't even know. I think we buy them from China.

421
00:30:54,360 --> 00:30:57,800
Well, I'm not an expert on these kind of logistics. I think I would like to

422
00:30:58,440 --> 00:31:01,800
get back to kind of like the more general. Let's move on from that.

423
00:31:01,800 --> 00:31:05,720
I am not either, but I do believe the Chinese have more manufacturing capacity than the United

424
00:31:05,720 --> 00:31:11,720
States. It seems completely plausible to me. I think things are not lazy and they don't sit

425
00:31:11,720 --> 00:31:14,600
around and have all this free time and aesthetic preferences or something.

426
00:31:14,600 --> 00:31:19,320
I'm a believer that work is life. I mean, at least from my Chinese friends,

427
00:31:19,320 --> 00:31:23,240
I know the Chinese sure do have a lot of inefficiencies. It's just called corruption.

428
00:31:23,960 --> 00:31:27,880
Oh, America has corruption too. You see. Oh, yeah, sure. Well, in Mexico,

429
00:31:27,880 --> 00:31:32,840
the corruption is you have to pay 20 cents to get 20 cents on every dollar for the building

430
00:31:32,840 --> 00:31:38,040
you built, right? Whatever. In America, every dollar is spent absolutely on that building.

431
00:31:38,040 --> 00:31:42,120
You know, we know that because we spent $4 making sure that that first dollar was not

432
00:31:42,120 --> 00:31:48,600
spent correctly. I'm well aware of that. Anyways, I think we mostly agree on this

433
00:31:48,600 --> 00:31:54,040
point actually. And I think it's a matter of degree. What I want to say just for the record,

434
00:31:54,040 --> 00:32:00,040
the U.S. is a uniquely dysfunctional system in the West. I'm German and the German system is

435
00:32:00,040 --> 00:32:04,360
very dysfunctional, but it's like nothing compared to how dysfunctional the U.S. is.

436
00:32:04,360 --> 00:32:08,280
Fully agree with that. I don't think we disagree on that. I think it's a matter of

437
00:32:08,280 --> 00:32:13,080
degree more so than anything. I agree. We've had a comment saying someone's turned the

438
00:32:13,080 --> 00:32:17,320
temperature up a bit too much on the language model. So let's bring it back a tiny bit to

439
00:32:17,320 --> 00:32:22,120
AI safety. But that was a great discussion. Got it. I will end with saying I love America.

440
00:32:22,120 --> 00:32:25,400
I am happy to live here. And there are a lot of things I appreciate about American society.

441
00:32:27,560 --> 00:32:30,360
Great. So do you want to return to like the technical topics or?

442
00:32:31,080 --> 00:32:34,840
Yeah, I think I can return to your first point. And maybe I'll just start with a question.

443
00:32:34,840 --> 00:32:36,280
Do you think there's going to be a hard takeoff?

444
00:32:37,880 --> 00:32:39,560
I don't know, but I can't rule it out.

445
00:32:42,040 --> 00:32:44,280
I can't see how that would possibly happen.

446
00:32:46,840 --> 00:32:50,920
I have a few ideas of how it could happen, but I don't. It's unlikely. It seems like not.

447
00:32:51,720 --> 00:32:56,680
The way I think it could happen is if there are just algorithms, which are like

448
00:32:58,120 --> 00:33:01,800
magnitudes of order better than anything we ever have. And like the actual amount of

449
00:33:01,800 --> 00:33:06,440
computing you get human is like, you know, a cell phone or, you know, like, and then this

450
00:33:06,440 --> 00:33:11,880
algorithm is not deep in the tech tree. We just happen to have not picked it up. And then an

451
00:33:11,880 --> 00:33:14,680
AGI system picks it up. This is how I think it could happen.

452
00:33:15,640 --> 00:33:20,360
Okay, yes, I agree that something like this is potentially plausible where you're saying basically

453
00:33:20,360 --> 00:33:27,560
like the God Shatter is already distributed. It's not a question. It's using all the existing

454
00:33:27,560 --> 00:33:31,480
compute in the world today. It just turns out it was 10,000x more effective or a millionx more

455
00:33:31,480 --> 00:33:36,920
effective than we thought. Yeah, this is seems the most plausible way to be. Or, you know, you mix

456
00:33:36,920 --> 00:33:40,040
lead and, you know, copper and you get a superconductor, you know, something like that.

457
00:33:41,400 --> 00:33:45,720
Even that. I know, I know, I'm not joking. It's going to take so many years to like,

458
00:33:45,720 --> 00:33:50,280
it's not about the discovery, right? Give it 10 years to productionize it, scale up processes,

459
00:33:50,280 --> 00:33:54,280
right? Like these things are, you know, this is something running a company's really taught me,

460
00:33:54,280 --> 00:34:00,840
like it's just going to take a long time. And this is really like, like kind of where my,

461
00:34:00,840 --> 00:34:05,320
I just don't believe in a hard takeoff. I think that they'll be, this is a gasketing I like,

462
00:34:05,320 --> 00:34:08,600
he's a hardware and software progressive, quite similar speeds. And you can look at

463
00:34:08,600 --> 00:34:13,560
factoring algorithms to show this. So it would shock me if there were some, you know, 10 to the

464
00:34:13,560 --> 00:34:20,200
six, 10 to the nine magical improvement to be had. It seems plausible to be like a hard takeoff is

465
00:34:20,200 --> 00:34:25,720
definitely not my main line scenario, my main line scenario. Well, I don't know, maybe you

466
00:34:25,720 --> 00:34:28,760
wouldn't consider this a hard, maybe you would consider as a hard takeoff. This is what I would

467
00:34:28,760 --> 00:34:34,120
describe as a soft takeoff is something like, sometimes the way I like to define AGI is say,

468
00:34:34,120 --> 00:34:39,400
it's something that has the thing that chimps, that chimps don't have and humans do have.

469
00:34:39,400 --> 00:34:45,720
Yeah. So chimps don't go a third to the moon, you know, despite their brain being a third of our

470
00:34:45,720 --> 00:34:50,680
size. So we scaled up things by a factor of three of a primate brain, roughly four or something

471
00:34:50,680 --> 00:34:54,840
like that. And like most of the structures, I'm sure some micro tweaks and whatever, but like

472
00:34:54,840 --> 00:34:59,000
not massive amount of evolutionary pressure, like we're very, very similar to chimps.

473
00:34:59,000 --> 00:35:05,720
And somehow this got us from, you know, literally no technology to space travel in a,

474
00:35:05,720 --> 00:35:12,200
you know, evolutionary, very small pair of time. It seems imaginable to me that something similar

475
00:35:12,200 --> 00:35:17,160
could happen with AI. I'm not saying it will, but like seems imaginable.

476
00:35:17,160 --> 00:35:23,800
Yeah. So I agree with this. I'll come to your point about, you know, you had two regulatory

477
00:35:23,800 --> 00:35:31,720
points, one of them about capping the max flops. And I actually kind of agree with this. I do think

478
00:35:31,720 --> 00:35:37,720
that things could potentially become very dangerous at some point. I think your numbers are way,

479
00:35:37,720 --> 00:35:43,720
way, way too low. I think if your numbers are anywhere near GPT-3, GPT-4, okay, great. We got

480
00:35:43,720 --> 00:35:48,680
a lot of, we got a lot of fast moving guys who work on fiber, even if you start to get Von Neumanns.

481
00:35:49,320 --> 00:35:55,560
Right? We're not talking about a humanity's worth of compute. We're talking about things on par with

482
00:35:55,560 --> 00:36:01,720
a human and a few humans, right? Yeah, they'll run fast, but they're not. Like things get scary

483
00:36:01,720 --> 00:36:07,160
when you can do a humanities training run in 24 hours. Like we're about to burn the same compute

484
00:36:07,160 --> 00:36:13,320
that all 2 million years of human civilization burned. Okay. Now I don't know what starts to

485
00:36:13,400 --> 00:36:18,360
happen. Or I'll put this kind of another way. Language models, I look at them and they don't

486
00:36:18,360 --> 00:36:24,760
scare me at all because they're trained on human training data, right? These things are not, like,

487
00:36:24,760 --> 00:36:31,560
if something was as good as GPT-4 that looked like Mu Zero, where it trained from some simple rules,

488
00:36:32,280 --> 00:36:37,000
okay, now I'm a bit more scared. But when you say, okay, it's, you know, we're feeding the whole

489
00:36:37,000 --> 00:36:40,440
internet into the thing and it parrots the internet back, mushed around a little bit,

490
00:36:40,440 --> 00:36:43,160
that looks very much like what a human does. And I'm just not scared of that.

491
00:36:44,280 --> 00:36:49,400
Yeah, it's very reasonable, whatever. Like, I'm not scared of GPT-4 to be clear. Like, I think

492
00:36:49,400 --> 00:36:55,000
there is like 0% chance or like, you know, epsilon chance that GPT-4 is existentially dangerous by

493
00:36:55,000 --> 00:37:02,680
itself. You know, maybe some crazy GPT-4 plus RL plus Mu Zero plus something, something maybe.

494
00:37:02,680 --> 00:37:07,160
But I definitely agree with you here. I don't expect GPT-3 or 4 by themselves to be dangerous.

495
00:37:07,160 --> 00:37:10,600
These are not, I'm much closer to, I think, what you're saying, like, yeah, if you had a Mu Zero

496
00:37:10,600 --> 00:37:16,200
system, they'd bootstrap yourself to GPT-4. Holy shit, like, we're big, we're big shit if we get

497
00:37:16,200 --> 00:37:21,960
to that. Then we should, let's stop. Let's stop. Yeah, let's stop. So, I'm very happy to get,

498
00:37:21,960 --> 00:37:26,280
to be into a regime where we're like, okay, let's find the right bound. Like, I think this is an

499
00:37:26,280 --> 00:37:29,800
actually good argument. I think this is actually something that should be discussed, which is not

500
00:37:29,800 --> 00:37:34,440
obvious. And I could be super wrong about that. So I'd like to justify a little bit about why I

501
00:37:34,440 --> 00:37:39,160
put such a small bound. But I think your arguments you're making for the higher bounds are very

502
00:37:39,160 --> 00:37:43,080
reasonable, actually. I think these are actually good arguments. So just to justify a little bit

503
00:37:43,080 --> 00:37:49,160
about why I put such a low bound, the boring default answer is conservatism. It's like, if all of

504
00:37:49,160 --> 00:37:54,200
humanity is at stake, which, you know, you may not believe, I'm like, whoa, whoa, okay, at least

505
00:37:54,200 --> 00:38:01,160
give us a few years to like, more understand what we're dealing with here. Like, I understand that,

506
00:38:01,160 --> 00:38:06,280
you know, you may disagree with this. Very plausible. But I'm like, whoa, like, you know, at least,

507
00:38:06,280 --> 00:38:11,320
let's let's like, by default, let's hit a pause button for like, you know, a couple years until

508
00:38:11,320 --> 00:38:15,640
we figure things out more. And then if we like, find a better theory of scaling, we understand how

509
00:38:15,640 --> 00:38:20,440
intelligent scales, we understand how mu zero comes, blah, blah, blah. And then we pick back up after

510
00:38:20,440 --> 00:38:25,960
we're like, you know, we make huge breakthroughs in alignment. And Eliezer is, is crying on CNN and

511
00:38:26,040 --> 00:38:32,680
like, oh, we did it, boys. I mean, then okay, sure, you know, okay. So that's the one, like,

512
00:38:32,680 --> 00:38:37,080
kind of more boring argument, like, that's kind of a boring argument. The more interesting argument,

513
00:38:37,080 --> 00:38:43,960
I think, which I think is a bit, you know, or skit so, is that it's not clear to me that you can't

514
00:38:43,960 --> 00:38:48,920
get dangerous levels of intelligence with the amount of compute we have now. And one of the reasons

515
00:38:48,920 --> 00:38:54,040
that I'm unsure about this is because man, GPT three GP four is just the dumbest possible way to build

516
00:38:54,040 --> 00:39:00,200
AI. Like, it's just like, like, there's like no dumber way to do it. Like, it's it works and dumb

517
00:39:00,200 --> 00:39:07,080
is good, right? You know, better lesson dumb is good. But look at humans. You said, as we talked

518
00:39:07,080 --> 00:39:12,840
about before, you know, human today, human 10,000 years ago, not that different. You place both of

519
00:39:12,840 --> 00:39:18,200
them into a, you know, workshop with tools to build, you know, any weapon of their choice, which of

520
00:39:18,200 --> 00:39:23,400
them is more dangerous? Obviously, you know, one of them will have much better, you know,

521
00:39:24,200 --> 00:39:31,800
capacities to deal with tools, to read books, to think about how to design new weaponry, and so

522
00:39:31,800 --> 00:39:37,880
on. These are not genetic changes, they are epistemological changes, they are memetic changes,

523
00:39:37,880 --> 00:39:42,360
they are software updates, you know, humans had to discover rational reasoning, like, you know,

524
00:39:42,360 --> 00:39:46,680
before like, you know, I mean, you know, obviously, people always had like, you know, folk conceptions

525
00:39:46,760 --> 00:39:52,360
of rationality. But it wasn't like a common thing to think about causality and like, you know,

526
00:39:52,920 --> 00:39:57,880
you know, rational, like, you know, if then else kind of stuff until relative, you know,

527
00:39:57,880 --> 00:40:03,480
like philosophers in the old ages and only became widespread relatively recently. And these are useful

528
00:40:03,480 --> 00:40:08,680
capabilities that turned out to be very powerful and took humans many, many thousands of years to

529
00:40:08,680 --> 00:40:12,760
develop and distribute. That's good. And I don't think humans are anywhere near the level. I think

530
00:40:12,840 --> 00:40:16,760
the way we could do science right now is pretty awful. Like, it's like the dumbest way to do

531
00:40:16,760 --> 00:40:23,960
science that like, kind of still works. Like, you know, and I expect it's like possible that if you

532
00:40:23,960 --> 00:40:29,480
had a system which like, let's say it's like, smaller brain than the human even, but it has

533
00:40:30,120 --> 00:40:34,600
really, really sophisticated epistemology. It has really, really sophisticated theories of

534
00:40:34,600 --> 00:40:41,000
meta science. And it never tires, it never gets bored, it never gets upset, it never gets distracted,

535
00:40:41,000 --> 00:40:45,960
and it can like, memorize arbitrary amounts of data. This is something that I think is within

536
00:40:45,960 --> 00:40:51,480
the realm of like a GPT three or four training run to build something like this. And it is not

537
00:40:51,480 --> 00:40:56,760
obvious to me that this system could not outfind humanity. Maybe not, like maybe not, but it's

538
00:40:56,760 --> 00:41:02,120
not obvious to me that it keeps. So just carry on. So what do you think of that? So to your first

539
00:41:02,120 --> 00:41:08,840
point, why I stand against almost all conservative arguments, you're assuming the baseline is no

540
00:41:08,840 --> 00:41:14,920
risk, right? And oh, well, why should we do this AI? We should wait and bring the baseline back. No,

541
00:41:14,920 --> 00:41:21,000
no, no, no. We are about to blow the world up any minute. There's enough nuclear weapons aimed at

542
00:41:21,000 --> 00:41:26,600
everything. This is wearing some incredibly unstable precarious position right now. Like,

543
00:41:26,600 --> 00:41:30,840
people talk about this with with car accidents. You know, this is comma, like, people are like,

544
00:41:30,840 --> 00:41:35,960
oh, well, you know, if your device causes even one accident, I'm like, yeah, but what if statistically

545
00:41:35,960 --> 00:41:40,920
there would have been five without the device? I'm like, you do have to understand the baseline

546
00:41:40,920 --> 00:41:45,800
risk in cars is super high. You make it five x safer, there's one accident, you don't like that.

547
00:41:45,800 --> 00:41:52,760
Okay, I mean, you have to be excluded from any polite conversation, right? So yeah, like, I think

548
00:41:52,760 --> 00:42:00,760
that calling for a pause to the technology is is worse, right? I think given the two options,

549
00:42:00,760 --> 00:42:05,480
if we should pause or we should not pause, I think pausing actually prevent presents more risk. And

550
00:42:05,480 --> 00:42:09,240
I can talk about some reasons why again, the things that I'm worried about are not quite

551
00:42:10,120 --> 00:42:19,160
the existential risks I have to the species are not AGI goes rogue, they are government gets control

552
00:42:19,160 --> 00:42:25,480
of AGI and ends up in some really bad place where nobody can compete with them. I don't think these

553
00:42:25,480 --> 00:42:30,920
things look unhuman. These things to me like, I see very little distinction between human

554
00:42:30,920 --> 00:42:36,600
intelligence and machine intelligence, it's all just on a spectrum. And like, they're not,

555
00:42:38,600 --> 00:42:43,320
like, to come to the point about, okay, but GPT four could be like this hyper rational,

556
00:42:43,320 --> 00:42:49,720
never tiring humans are doing science in the dumbest way. I'm not sure about that, right?

557
00:42:49,720 --> 00:42:54,280
Like, I think that, you know, when you look at like, okay, okay, we have chess bots that do

558
00:42:54,280 --> 00:42:58,200
way better. And all they do is think about chess, haven't really done this with humans,

559
00:42:58,200 --> 00:43:02,760
people would call it an ethical, right? Like, if we really told a kid, like, if we really just

560
00:43:02,760 --> 00:43:06,840
like, every night, we're just putting the chess goggles on you, and you're staring at chess

561
00:43:06,840 --> 00:43:11,800
boards, and we're really just training your neural net to play chess. I think humans could

562
00:43:11,800 --> 00:43:18,760
actually beat a computer again, a chess, if we were willing to do that. So yeah, I don't think

563
00:43:18,760 --> 00:43:23,000
that this stuff is that particularly dumb. And I think, okay, maybe we're losing 10x, but we're

564
00:43:23,000 --> 00:43:30,600
not losing a million x. Again, I don't see a, I do the numbers out all the time for when we're

565
00:43:30,600 --> 00:43:35,560
going to start to get more computer, you know, when will a computer have more compute than a human?

566
00:43:35,560 --> 00:43:40,600
When will a computer have more compute than humanity? And yes, these things get scary,

567
00:43:40,600 --> 00:43:44,520
but we're nowhere near scary yet. We're looking at these cute little things. And

568
00:43:45,400 --> 00:43:53,400
these things, by the way, do present huge dangers to society, right? The the PsyOps that are coming.

569
00:43:54,200 --> 00:43:59,400
Right now, you assume that like, when you call somebody, that you're at least wasting their

570
00:43:59,400 --> 00:44:04,760
time too. But we're going to get like heaven banning. I love this concept, which is, you know,

571
00:44:04,760 --> 00:44:08,840
yeah, yeah, came up on Luther AI, like that's where it comes from was the guy on the Luther

572
00:44:08,920 --> 00:44:15,560
AI that came up with that word. Yeah, I know the guy came up with it. I love, I love this

573
00:44:15,560 --> 00:44:22,120
concept. And I think there's also a story, my little pony friendship is optimal that God that

574
00:44:22,120 --> 00:44:32,200
goes into the concept. And yeah, so I think that like, my girlfriend proposed a, I don't want to

575
00:44:32,200 --> 00:44:39,240
talk to Oh, I say you don't want to talk to your relative anymore, right? Okay, we'll give them

576
00:44:39,240 --> 00:44:46,120
an AI version to talk to, right? Yeah. Yeah. So like, this stuff is coming and it's coming soon.

577
00:44:46,120 --> 00:44:51,880
And if you try to centralize this, if you try to, you know, say like, Oh, okay, Google Open AI,

578
00:44:51,880 --> 00:44:55,000
great, they're not aligned with you. They're really not Google has proven time and time again,

579
00:44:55,000 --> 00:44:58,440
they're not aligned with you. Meta has proven time and time again, they're trying to fix it.

580
00:44:58,680 --> 00:45:05,560
Yeah. I mean, I fully agree with you. Like, I like that you bring up PsyOps as the correct

581
00:45:05,560 --> 00:45:10,120
example in my opinion of short term risks. I think you're like, fully correct about this.

582
00:45:10,120 --> 00:45:16,840
Like, when I first saw like, GPT models, I was like, holy shit, like the level of control I can

583
00:45:16,840 --> 00:45:22,360
gain over social reality, using these tools at scale is insane. And I'm surprised that we haven't

584
00:45:22,440 --> 00:45:28,360
seen yet the things that like, augured in my visions of the day. And we will, like we will,

585
00:45:28,360 --> 00:45:35,560
obviously it's coming. And this is, so I think this is a very, very real problem. Yeah. Like,

586
00:45:35,560 --> 00:45:40,840
I think if we, even if we stop now, we're not out of the forest. So like, when you say like, I

587
00:45:40,840 --> 00:45:45,320
think the risk is zero, please do not believe that that is what I believe, because it is truly not.

588
00:45:45,320 --> 00:45:50,920
It is truly, truly not. I think we are like, we are really in a bad situation. We are in a, we are

589
00:45:50,920 --> 00:45:56,680
being, we're under attack from like so many angles right now. And this is before we get into, you

590
00:45:56,680 --> 00:46:00,840
know, like, you know, potential, like, you know, climate risks, nuclear risk, whatever, we're in

591
00:46:00,840 --> 00:46:07,320
under room of medic risk, like the, the then dangers of our like epistemic foundations are under

592
00:46:07,320 --> 00:46:14,040
attack. And this is something we can adapt to, right? Like, you know, we did, you know, when a

593
00:46:14,040 --> 00:46:19,320
good friend of mine, he's a, he's quite well read on like Chinese history. And he always like, it

594
00:46:19,320 --> 00:46:23,080
tells me his great story. So I'm not a historian. So please, you know, don't crucify me here. But

595
00:46:23,080 --> 00:46:27,400
like, he tells these great stories about when Marxist memes were first introduced to China.

596
00:46:27,400 --> 00:46:31,240
And like, this is where a world where like, just like all the precursor memes didn't exist.

597
00:46:31,240 --> 00:46:35,800
That's just like, kind of was air dropped in and people went nuts. People went just completely

598
00:46:35,800 --> 00:46:40,440
crazy because there was no memetic antibodies to these like hyper virulent memes that were,

599
00:46:40,440 --> 00:46:44,680
you know, created by evolutionary pressures and like, you know, Western university departments,

600
00:46:44,680 --> 00:46:48,920
like really, you could call philosophy department just gain a function, the medic laboratories.

601
00:46:49,960 --> 00:46:56,920
I like that. Yeah. I mean, like, you know, like without being, you know, political or any means

602
00:46:56,920 --> 00:47:00,760
there, a lot of what these organizations do. And like, you know, other, you know, what other,

603
00:47:00,760 --> 00:47:06,120
you know, memetic, like, you know, if philosophy departments are the, like, gain a function

604
00:47:06,120 --> 00:47:11,160
laboratories, then like 4chan and tumbler are like the bat caves of needs, you know, like the

605
00:47:11,160 --> 00:47:17,320
Chinese bat cave. And I remember this vividly. I was like on tumbler and 4chan, like when I was a

606
00:47:17,320 --> 00:47:22,440
teenager, and then suddenly all the like weird, bizarre, you know, internet shit I saw started

607
00:47:22,440 --> 00:47:28,200
becoming mainstream news. My parents were watching in 2016. And I was like, what the hell is going

608
00:47:28,200 --> 00:47:33,080
on? Like, I already developed antibodies to this shit. Like I already, you know, both right and

609
00:47:33,080 --> 00:47:38,280
left, I was already like, I already immunized all this. So I fully agree with you that this is like

610
00:47:38,280 --> 00:47:44,200
one of the largest risks that we are facing is this kind of like memetic mutation load, in a sense.

611
00:47:44,200 --> 00:47:50,360
And I'm not going to say I have a solution to this problem. I'm like, I have ideas, like there's a

612
00:47:50,360 --> 00:47:55,320
lot of like things you can do to improve upon this. Like if AI was not a risk, and also not climate

613
00:47:55,320 --> 00:47:59,800
change and whatever, this might be something I work on, like epistemic security, this might be

614
00:47:59,800 --> 00:48:04,120
something I would work on, like how can we build better coordination, like like just scalable

615
00:48:04,120 --> 00:48:09,000
rationality mechanisms, stuff like prediction markets and stuff like this. I don't know. But

616
00:48:09,000 --> 00:48:15,000
sorry, going off track here a little bit. Well, no, actually, I really agree with a lot of the

617
00:48:15,000 --> 00:48:19,080
stuff you said. And I had a similar experience with the antibodies and people are exposed to this

618
00:48:19,080 --> 00:48:28,760
stuff. And I'm like, yeah, this got me like four years ago. Yeah. So I think that there is a solution

619
00:48:28,760 --> 00:48:34,600
and I have a solution. And the answer is open source AI. The answer is open source. Let's say

620
00:48:34,600 --> 00:48:38,280
even you can even dial it back from like the political and the terrible and just straight up

621
00:48:38,280 --> 00:48:43,560
talk about ads and spam, or maybe spam, just straight up spam, I get so much spam right now.

622
00:48:43,560 --> 00:48:48,520
And it's like, it's kind of written by a person. It's like targeting me to do something. And

623
00:48:48,520 --> 00:48:53,400
Google spam filter can't even come close to recognizing it. Right. Like what I need is a

624
00:48:53,400 --> 00:49:00,040
smart AI that's watching out for me that is just it's not even targeted attacks at me. It's just

625
00:49:00,040 --> 00:49:07,080
so much noise. And I don't see a way to prevent this. Like the big organizations, they're just

626
00:49:07,080 --> 00:49:11,800
going to feed you their noise, right? And they're going to maximally feed you their noise. The only

627
00:49:11,800 --> 00:49:16,040
way is if you have an AI, like, I don't think alignment is a hard problem. I think if you own

628
00:49:16,040 --> 00:49:20,520
the computer and you run the software, if you develop the software, the AI is aligned with you.

629
00:49:20,520 --> 00:49:28,360
Oh, yeah. Can you, okay, if I challenge you, George Haas, here is a llama 65b model when we

630
00:49:28,360 --> 00:49:33,240
could appear to run it on, make it so it and make, yeah, you know, sure. Okay, you developed AI. I

631
00:49:33,240 --> 00:49:39,320
give you the funding for your time. Can you develop a model that is as good as llama 64b 65b

632
00:49:39,320 --> 00:49:43,720
and is immune, like completely immune to jailbreak. It cannot be jailbroken. No.

633
00:49:44,760 --> 00:49:50,760
Why not? It's aligned, isn't it? Well, no, but this isn't what alignment means. Well, my values is

634
00:49:50,760 --> 00:49:55,640
do not get jailbroken. Oh, okay, you're talking about unexploitability. This is not alignment,

635
00:49:55,640 --> 00:49:59,880
right? Okay, okay, interesting. I didn't know you would separate those. So I extremely separate

636
00:49:59,880 --> 00:50:07,240
those, right? Okay, I mean, in the default case, it like, like it, it's on my side, right? Okay,

637
00:50:07,240 --> 00:50:13,480
unexploitability is not a question of whether it's okay. And this is a true thing about people too.

638
00:50:13,480 --> 00:50:18,840
Whenever I look at a person, I ask, okay, is this person, I want something for me. Is this person,

639
00:50:18,840 --> 00:50:24,840
does this person want it too? And is this person capable of doing it? Right? And I really separate

640
00:50:24,840 --> 00:50:29,320
those two things. I can build a system. I don't, I'm not worried about the first one with the AI

641
00:50:29,320 --> 00:50:33,800
system is worried about the second one. Can it be gamed? Can it be exploited? Sure, I could tell

642
00:50:33,800 --> 00:50:39,160
like, you know, like, like, say it was just playing chess, right? And it loses. I'm like, don't lose.

643
00:50:39,800 --> 00:50:46,280
Okay, I didn't want to, man, I didn't want to lose. I'm sorry. I know. But like, so yes, yes, can I build

644
00:50:46,280 --> 00:50:51,160
a aligned system? Sure. Can I build an unexploitable system? No, especially not by a more powerful

645
00:50:51,160 --> 00:50:55,880
intelligence. Interesting. Interesting. So this is an interesting, I think you're, you're, you're

646
00:50:55,960 --> 00:51:01,000
pointing to actually a very important part of this, is that like, exploitability and alignment

647
00:51:01,000 --> 00:51:05,560
can get fuzzy? Like, which is which? Like, did it fail because of its skill set? Or because it's not

648
00:51:05,560 --> 00:51:09,160
aligned? It's actually a very deep question. So I think, I think you make a good point for like,

649
00:51:09,160 --> 00:51:16,280
you know, talking about these two separately. I guess the, so the thing I want to dig in just

650
00:51:16,280 --> 00:51:24,840
like a little bit more on this idea is there are, there's two ways, there are two portals

651
00:51:24,840 --> 00:51:29,800
through which, you know, the memetic demons can reach into reality, humans and computers.

652
00:51:30,600 --> 00:51:34,680
Why do you think your AI is immune to memes? Why, why can't I just build

653
00:51:34,680 --> 00:51:40,200
AIs that target your AIs? What like you don't, I don't think my AI is immune to memes at all.

654
00:51:40,920 --> 00:51:46,440
I think that the only question is, and I really like your game, like these, these NGOs are doing

655
00:51:46,520 --> 00:51:56,920
gain of function on memes, right? Wear a mask. Like, a weaker intelligence will never be able

656
00:51:56,920 --> 00:52:02,760
to stand up to a stronger intelligence. So from this perspective, if this is what's known as alignment,

657
00:52:02,760 --> 00:52:07,960
I just don't believe that this is possible, right? You can't, you can't keep a stronger

658
00:52:07,960 --> 00:52:12,280
intelligence in the box. This is, this is, I agree with you, Kowsky in the box experiments,

659
00:52:12,280 --> 00:52:17,560
like the AI is always going to get out. There's no keeping it in the box, right? This is, this

660
00:52:17,560 --> 00:52:23,560
is a complete impossibility. I think there's only two real ways to go forward. And one is

661
00:52:23,560 --> 00:52:30,600
Ted Kaczynski. One is technology is bad. Oh my God, blow it all up. Let's go live in the woods,

662
00:52:30,600 --> 00:52:35,400
right? And I think this is a philosophically okay position. I think the other philosophically okay

663
00:52:35,400 --> 00:52:40,680
position is something more like effective accelerationism, which is look, these AIs are

664
00:52:40,680 --> 00:52:47,960
going to be super powerful. Now, if you have one, it could be bad. But if super intelligent AIs are

665
00:52:47,960 --> 00:52:52,520
all competing against each other, magnetically, like we have something like society today, just

666
00:52:52,520 --> 00:52:57,240
the general power levels have gone up. This is fine as long as these things are sufficiently

667
00:52:57,240 --> 00:53:02,920
distributed, right? Like, sure, this AI is not perfectly aligned, but you know, there's a thousand

668
00:53:02,920 --> 00:53:06,760
other ones and like you have to assume they're all basically good because if they're all basically

669
00:53:06,760 --> 00:53:12,200
bad, well, we're dead anyway. I mean, why wouldn't you expect that? That they're all bad? Yeah.

670
00:53:13,000 --> 00:53:18,520
Well, or what do you think of humans? Are most humans good? I think the concept of good doesn't

671
00:53:18,520 --> 00:53:23,160
really apply to humans because humans are too inconsistent to be good. Like by default, they

672
00:53:23,160 --> 00:53:28,120
can be good in various scenarios in various social contexts. Like give me any human and I can put them

673
00:53:28,120 --> 00:53:33,560
into a context where they'll do an arbitrarily bad thing. And this is true about a llama as well,

674
00:53:33,560 --> 00:53:37,560
right? Lamas are completely inconsistent. I think they're actually more inconsistent than humans.

675
00:53:38,360 --> 00:53:42,840
Right. I see. I wouldn't trust llamas to be good. Well, yeah, but I wouldn't think that they're

676
00:53:42,840 --> 00:53:46,600
bad either. I would think they have the exact same inconsistency problem as humans. And I think

677
00:53:46,600 --> 00:53:52,920
almost any AI you build is going to run into the same problems, right? Yeah, I think so. That's my

678
00:53:52,920 --> 00:53:58,440
point. So your assumption can't rely on them being good because you don't get that for free.

679
00:53:58,440 --> 00:54:01,640
Like, where does that come from? My assumption is not that they're good. My assumption is that

680
00:54:01,720 --> 00:54:06,440
they're not bad. But inconsistent is fine. As long as we have a ton of them and they're all

681
00:54:06,440 --> 00:54:10,440
inconsistent and they're pulling society in every which direction, you don't end up paper-clipped,

682
00:54:10,440 --> 00:54:16,520
right? Why not? Well, because what? They're all going to coordinate and agree to paper-clip you?

683
00:54:16,520 --> 00:54:21,320
No, no, they'll just do some random bullshit. And then that random bullshit will not include

684
00:54:21,320 --> 00:54:25,000
humans. They're all doing random bullshit, right? You're going to have, let's say the liberals

685
00:54:25,000 --> 00:54:28,360
decide we're going to paper-clip people. The conservatives are going to come out very strongly

686
00:54:28,360 --> 00:54:34,440
against paper-clipping, right? And you're just going to end up with these sort of dynamics,

687
00:54:34,440 --> 00:54:38,040
like forever. You're going to have some AIs who are like, yeah, we don't like it, you know?

688
00:54:40,040 --> 00:54:46,920
So this is perplexing to me because in my view, which I think I heard you agree with earlier,

689
00:54:46,920 --> 00:54:51,800
it's like the world is unstable. The world is very unstable. And it seems to me you're suggesting

690
00:54:51,800 --> 00:54:57,320
if we increase the chaos, if we have more entities fighting, doing more different things

691
00:54:57,320 --> 00:55:01,880
with more energy, that makes it more stable. Is that correct? Yes. The world has actually

692
00:55:01,880 --> 00:55:05,400
become very stable in the last 100 years. And I'm scared of how stable it's become.

693
00:55:07,000 --> 00:55:10,600
You know, again, thank you for China. How many sovereign countries are there in the world?

694
00:55:12,360 --> 00:55:18,920
I mean, you know, 190 something. That's the UN number, if you believe them. I think there's

695
00:55:18,920 --> 00:55:23,560
the US and China and maybe Russia, mostly they just have a veto button.

696
00:55:24,360 --> 00:55:30,040
All right. So I think there's more like two and a half. Like this isn't a lot of entities fighting

697
00:55:30,040 --> 00:55:35,160
it out chaotically. A lot of entities fighting it out chaotically would be intelligences spread

698
00:55:35,160 --> 00:55:39,400
across the entire galaxy, right? And that's the other, that's the other beautiful thing too.

699
00:55:40,200 --> 00:55:44,040
Sooner we get off this planet, the sooner we get things that are actually a light year away.

700
00:55:44,600 --> 00:55:47,640
Well, I think the speed of light's real. It's going to take you a year to send them there.

701
00:55:48,520 --> 00:55:52,760
Memes, right? You want to get away from the memes? There's only one way.

702
00:55:54,760 --> 00:55:58,920
For the record, I would like to go on the record and say, if any future transhumanists

703
00:55:58,920 --> 00:56:01,960
or whatever want to blast themselves into space and go do their own thing,

704
00:56:01,960 --> 00:56:06,120
I support their right to do that. And I would love to give this right to people.

705
00:56:06,120 --> 00:56:10,760
The number one thing I want from countries is the ability, the right to leave.

706
00:56:10,760 --> 00:56:12,680
This is what I would love. This is what I love about companies.

707
00:56:12,680 --> 00:56:15,480
Free Exit. You're talking neo-reaction talk.

708
00:56:16,280 --> 00:56:20,280
Yeah. Free Exit is extremely important. I would not describe myself as neo-reactionary, please,

709
00:56:20,280 --> 00:56:26,360
because I'm not that gay. I wouldn't describe myself that way either,

710
00:56:26,360 --> 00:56:32,760
but I've heard a lot of good ideas from them. But yeah, that being said, I do think that,

711
00:56:33,480 --> 00:56:37,720
what I want, I think, let's ground the conversation a little bit here.

712
00:56:40,200 --> 00:56:42,760
I'm very enjoying this conversation. I love talking to these philosophical points.

713
00:56:42,840 --> 00:56:47,640
These are really good points, really interesting. But ultimately, as we also get to the latter

714
00:56:47,640 --> 00:56:53,320
third of this conversation, the thing I really care about is strategy. The thing I really care

715
00:56:53,320 --> 00:56:58,200
about is reality politic. I really care about, okay, what action can I take to get to the features

716
00:56:58,200 --> 00:57:04,840
I like? And I'm not going to be one of those galaxy brain fucking utilitarians like, well,

717
00:57:04,840 --> 00:57:09,080
actually, this is the common good. No, no, this is what I like. I like my family. I like humans.

718
00:57:09,240 --> 00:57:16,680
Yeah, it's just what it is. I'm not going to justify this on some global beauty,

719
00:57:16,680 --> 00:57:22,040
whatever. It doesn't matter. I want to live in a world. I want a 20-year time, 50-year time.

720
00:57:22,040 --> 00:57:28,440
I want to be in a world where my friends aren't dead and where I'm not dead. Maybe we are cyborgs

721
00:57:28,440 --> 00:57:33,080
or something, but I don't want to be dead. What I really care about, ultimately, is how do I get

722
00:57:33,080 --> 00:57:38,280
those words? And I want us all to not be suffering. I don't want to be in war. I want us to be in a

723
00:57:38,280 --> 00:57:43,720
good outcome. So I think we agree that we would both like a world like this. And I think we probably

724
00:57:43,720 --> 00:57:47,240
disagree about how best to get there. And I'd like to talk a little bit about what can we,

725
00:57:48,520 --> 00:57:53,160
what should we do and why do we disagree about what we do? Does that sound good to you?

726
00:57:53,160 --> 00:57:56,120
Maybe I'll first propose a world that meets your requirements.

727
00:57:57,320 --> 00:58:02,200
And you can tell me if you want to live in it. So here's a world. We've just implanted electrodes

728
00:58:02,200 --> 00:58:07,560
in everyone's brain and maximized their reward function. I would hate living in a world like

729
00:58:07,560 --> 00:58:11,000
that. Yeah, but no one, it meets your requirements, right? Your friends are not dead.

730
00:58:11,720 --> 00:58:13,480
No one's suffering and we're not at war.

731
00:58:14,600 --> 00:58:19,800
That is true. There are more criteria than just that. The true, the criteria I said is things I

732
00:58:19,800 --> 00:58:23,880
like. As I said, I'm not a utilitarian. I don't particularly care about minimizing suffering

733
00:58:23,880 --> 00:58:29,720
or maximizing utility. What I care about is various vague aesthetic preferences over reality.

734
00:58:30,440 --> 00:58:34,120
I'm not pretending this is, as though that was the whole spiel I was trying to make,

735
00:58:34,120 --> 00:58:39,320
is that I'm not saying I have a true global function to maximize. I say I have various

736
00:58:39,320 --> 00:58:44,200
aesthetics. I have various meta-preferences of those aesthetics. I'm not asking for a global one.

737
00:58:44,200 --> 00:58:49,240
I'm asking for a personal one. I'm asking for a personal one that you don't care about the rest

738
00:58:49,240 --> 00:58:54,200
of the world. I gave you mine. I gave you what I would do if I had an AGI. Yep. So I'm getting

739
00:58:54,200 --> 00:59:00,360
on this rock speed of light as fast as I can. Fair enough. I think if that is, I would like to live

740
00:59:00,360 --> 00:59:05,400
in a world where you could do that. This would be a feature of my world. A world where I would be

741
00:59:05,400 --> 00:59:13,160
happy is a world in which we coordinated at larger scales around building aligned AGI that could then

742
00:59:13,160 --> 00:59:23,160
distribute intelligence and matter and energy in a well-value handshaked way between various people

743
00:59:23,160 --> 00:59:28,120
who may want to coordinate with each other, may not. Some people might want to form groups that

744
00:59:28,120 --> 00:59:32,680
have shared values and share resources. Others may not. I would like to live in a world where that

745
00:59:32,680 --> 00:59:36,680
is possible. Have you read Metamorphosis of Prime Intellect? I have, unfortunately, not.

746
00:59:39,640 --> 00:59:42,680
Yeah. I was going to ask if you're happy with that world, right? Like,

747
00:59:42,680 --> 00:59:50,520
unfortunately, don't know it. I mean, it's as simple to describe. Singleton AI that basically

748
00:59:50,520 --> 00:59:57,000
gives humans whatever they want, like maximally libertarian, you know, you can do anything you

749
00:59:57,000 --> 01:00:02,520
want besides harm others. Is that a good world? Probably. I don't know. I haven't read the book.

750
01:00:02,520 --> 01:00:06,840
I assume the book has some dark twist about why this is actually a bad world. Not really. Not

751
01:00:06,840 --> 01:00:12,680
really. I mean, the plot is pretty obvious. You are the tiger reading chum, right? Sure, but you

752
01:00:12,680 --> 01:00:16,440
can then just decide if that is what you want, then you can just return to the wilderness. That's the

753
01:00:16,440 --> 01:00:22,280
whole point. Yeah, but can you? Can you really return to the wilderness, right? Like, you think

754
01:00:22,280 --> 01:00:26,600
that like, I don't think we have free will. I don't think you ever will return to the wilderness.

755
01:00:26,600 --> 01:00:33,000
I think a large majority of humanity is going to end up wireheaded. Yeah, I expect that too.

756
01:00:33,000 --> 01:00:36,440
Okay, great. And this is the best possible outcome, by the way. This is giving humans exactly what

757
01:00:36,440 --> 01:00:43,240
they want. Yeah. Well, to be clear, I don't expect it's all humans. I truly do not. I don't think

758
01:00:43,240 --> 01:00:47,880
it's all humans either. I think a lot of humans have metapreferences over reality. They have

759
01:00:47,880 --> 01:00:51,480
preferences that are not their own sensory experiences. This is the thing that the utilitarians

760
01:00:51,480 --> 01:00:58,280
get very wrong is that many human preferences are not even about their own sensory inputs.

761
01:00:58,280 --> 01:01:03,400
They're not even about the universe. They're about the trajectory of the universe. They're

762
01:01:03,400 --> 01:01:10,840
about for the utilitarianism, you know, and a lot of people want struggle to exist, for example.

763
01:01:10,840 --> 01:01:18,040
They want heroism to exist or whatever. I would like those values to be satisfied to the largest

764
01:01:18,120 --> 01:01:22,440
degree possible, of course. Am I going to say I know how to do that? No. Which is why I kind of

765
01:01:22,440 --> 01:01:30,040
like didn't want to go this deep, because I think if we're arguing about, oh, do we give them, you

766
01:01:30,040 --> 01:01:36,440
know, for utilitarianism versus libertarian, utopia versus whatever, I mean, we're already like

767
01:01:37,160 --> 01:01:42,920
10,000 steps deep. I'm asking about you. I'm not asking about them. I'm asking about a world

768
01:01:42,920 --> 01:01:48,040
you want to live in. And this is a really hard problem, right? Yeah. And this is why I just

769
01:01:48,040 --> 01:01:56,760
fundamentally do not believe in the existence of AI alignment at all. There is no like what values

770
01:01:56,760 --> 01:02:03,480
are we aligning it to, whatever the human says, or what they mean, or like. Sure, sure. But like,

771
01:02:03,480 --> 01:02:08,280
my point is I feel we have wandered into the philosophy department instead of the politics

772
01:02:08,280 --> 01:02:14,200
department. Okay, like, it's like, I agree with you, like, do human values exist? What does exist

773
01:02:14,200 --> 01:02:17,240
to me? But like, by the point you get to the point where we're asking what does exist to me,

774
01:02:17,240 --> 01:02:23,080
you've gone too far. I'll respond concretely to the two political proposals I heard you state on

775
01:02:23,080 --> 01:02:29,000
Bankless. Sure. I'd love to talk about them. One is limiting the total number of flops.

776
01:02:29,880 --> 01:02:34,600
Temporarily. Temporarily, yes. And when I have a proposal for that, but I don't want to set a

777
01:02:34,600 --> 01:02:39,640
number, I want to set it as a percent. I do not want anybody to be able to do a 51% attack on

778
01:02:39,640 --> 01:02:46,200
compute. If one organization acquires 50, it's straight up 51% attacks on crypto. If one organization

779
01:02:46,200 --> 01:02:52,680
acquires 51% of the compute in the world, this is a problem. Maybe we'll even cap it at something

780
01:02:52,680 --> 01:02:58,440
like 20, you know, you can't have more than 20, right? Yeah, I would support regulation like this.

781
01:02:59,640 --> 01:03:04,120
I would I don't think that this would cripple a country. But we do not want one entity or

782
01:03:04,120 --> 01:03:08,920
especially one training run to start using a large percentage of the world's compute,

783
01:03:08,920 --> 01:03:13,160
not a total number of flops. I mean, absolutely not. That'd be terrible.

784
01:03:13,160 --> 01:03:17,240
I think we can actually agree. I would actually support that regulation. Like, no, no, sorry,

785
01:03:17,240 --> 01:03:21,080
Sam Altman, you cannot 51% attack the world's compute. Sorry, it's illegal.

786
01:03:22,120 --> 01:03:27,880
That's fair enough. I think this is a sensible way to think about things. Assuming that software

787
01:03:27,880 --> 01:03:31,160
is fungible, is that everyone has access to the same kind of software and that you have an

788
01:03:31,160 --> 01:03:37,400
offense, defense, balance. So in my personal model of this, I think, well, a, some actors have

789
01:03:37,400 --> 01:03:43,160
very strong advantages on software, which can be very, very large, as someone who's trained very,

790
01:03:43,160 --> 01:03:47,160
very large models and knows a lot of the secret tricks that goes into them, a lot of the stuff

791
01:03:47,160 --> 01:03:54,200
in the open sources. Maybe we should force it to be open source. Well, this is your this is

792
01:03:54,200 --> 01:03:58,440
actually very legitimate consequence for just set. And now I'll say the second point about why I

793
01:03:58,440 --> 01:04:03,160
think that it doesn't work. So the next reason why I think doesn't work is that there is a,

794
01:04:04,040 --> 01:04:09,320
there are constant factors at play here is that the world is unstable. We are talking about this.

795
01:04:09,320 --> 01:04:18,200
I think the amount of compute you need to break the world currently is below the amount of compute

796
01:04:18,200 --> 01:04:24,520
that more than 100 actors have access to if they have the right software. And if you give, if you

797
01:04:24,840 --> 01:04:29,240
let's say you have this insight, right, that could be used, not saying it will be, but it could be

798
01:04:29,240 --> 01:04:33,000
used to break the world, to like cause World War three, or, you know, or just like, you know,

799
01:04:34,360 --> 01:04:39,160
cause mass extinction or whatever, if it's misused, right? Let's say you give this to

800
01:04:40,680 --> 01:04:45,880
you and me. Do you expect we're going to kill everybody? Like, would you do that? Or would you

801
01:04:45,880 --> 01:04:50,280
be like, Hey, let's, it kind of looks like not kill the world right now. And I'll be like, sure,

802
01:04:50,280 --> 01:04:55,480
let's not kill the world. How are we killing the world? How did we go from, I don't even understand

803
01:04:55,480 --> 01:05:00,440
like, how exactly does the world get killed? This, this is a big leap for me. I agree with you,

804
01:05:00,440 --> 01:05:05,480
I agree with you about the PSYOP stuff. I agree with you about, sorry, sorry, let, let me, you're

805
01:05:05,480 --> 01:05:09,640
right. I made too big of a, I mean, you're completely correct. Sorry about that. So to

806
01:05:10,600 --> 01:05:16,040
back up a little bit, let's assume we, you and me have access to something that can train, you know,

807
01:05:16,040 --> 01:05:24,360
at mu zero, you know, super GPT seven system on a tiny device, you know, cool. Problem is we do

808
01:05:24,360 --> 01:05:28,440
a test run it with it and we have, it immediately starts breaking out and we can't control it at

809
01:05:28,440 --> 01:05:33,400
all. Breaking out. What was it breaking out of? I don't, it immediately tries to maximize, it

810
01:05:33,400 --> 01:05:37,080
learned some weird proxy during the training process that is trying to maximize them. For some

811
01:05:37,080 --> 01:05:42,200
reason, this proxy involves gaining talent, involves gaining, you know, mutual information

812
01:05:42,920 --> 01:05:48,520
about future states. How is it gaining power? There's lots of other powerful AIs in the world

813
01:05:48,520 --> 01:05:53,080
who are telling it no. Well, we're assuming in this case, it's only you and me. Wait, wait,

814
01:05:53,080 --> 01:05:57,640
this is a problem. No, no, no, no. You've, you've ruined my entire assumption. As soon as it's you

815
01:05:57,640 --> 01:06:04,120
and me, yes, we have a real problem. Chicken man is only a problem because there's one chicken man.

816
01:06:04,120 --> 01:06:09,320
Yeah, I look, I am with you. So I'm saying before we get to the distributed case. So this is the

817
01:06:09,400 --> 01:06:13,960
step before we, it is not yet been distributed. Just, you know, you and me discover this algorithm

818
01:06:13,960 --> 01:06:18,600
in our basements. And so we're the first one to have it just by definition, because, you know,

819
01:06:18,600 --> 01:06:24,120
we're the one who found it. What now? Like, do you think posting, what do you think happens if

820
01:06:24,120 --> 01:06:29,960
you post this to GitHub? Well, good things for the most part. Interesting. I'd love to hear more.

821
01:06:29,960 --> 01:06:34,200
Okay. So first off, I just don't really believe in the existence of we found an algorithm that

822
01:06:34,200 --> 01:06:37,800
gives you a million x advantage. I believe that we could find an algorithm that gives you a 10x

823
01:06:37,800 --> 01:06:43,560
advantage. But what's cool about 10x is like, it's not going to massively shift the balance of power.

824
01:06:44,120 --> 01:06:49,320
Right. Like I want power to stay in balance. Right. This is like Avatar, the last airpan. Power

825
01:06:49,320 --> 01:06:54,200
must stay in balance. The fire nation can't take over the other nations. Right. So as long as power

826
01:06:54,200 --> 01:06:57,880
relatively stays in balance, I'm not concerned with the amount of power in the world.

827
01:06:58,840 --> 01:07:05,160
Right. Let me get to some very scary things. So what I think you do is, yes, I think the

828
01:07:05,160 --> 01:07:08,280
minute you discover an algorithm like this, you post it to GitHub, because you know what's

829
01:07:08,280 --> 01:07:12,840
going to happen if you don't? The feds are going to come to your door. They're going to

830
01:07:14,040 --> 01:07:18,520
take it. The worst people will get their hands on it if you try to keep it secret.

831
01:07:19,960 --> 01:07:26,520
So, okay. That's a fair question. So I'll take that aside. So am I correct in thinking that you

832
01:07:26,520 --> 01:07:34,040
think the feds are worse than serial killers in prison? No, but I think that, yeah, well, yes

833
01:07:34,040 --> 01:07:39,320
and no. Do I think that your average fed is worse than your average serial killer? No. Do I think

834
01:07:39,320 --> 01:07:43,720
that the feds have killed a lot more people than serial killers? All combined? Yeah.

835
01:07:44,360 --> 01:07:49,400
Sure. Totally agreeing with that. It's not one fed. It's all the feds in their little super

836
01:07:49,400 --> 01:07:54,680
powerful system. Sure. That's completely fine by me. I'm happy to grant that. Okay. What I want to

837
01:07:54,680 --> 01:08:01,640
work one through is a scenario. Okay. Let's say, okay, you know, we have a 10x system or whatever,

838
01:08:01,720 --> 01:08:07,160
but we hit the chimp level. You know, we jump across the chimp general level or whatever,

839
01:08:07,160 --> 01:08:10,760
right? And now you have a system which is like John von Neumann level or whatever, right? And it

840
01:08:10,760 --> 01:08:15,160
runs on one tiny box and you get a thousand of those. So it's very easy to scale up to a thousand x.

841
01:08:15,160 --> 01:08:20,200
So, you know, so then, you know, maybe you have your thousand John von Neumanns improve the efficiency

842
01:08:20,200 --> 01:08:25,640
by another, you know, to five, 10x. You know, now we're already at 10,000 x or 100,000 x,

843
01:08:25,640 --> 01:08:29,880
you know, improvements, right? So like just from scaling up the amount of hardware, including

844
01:08:30,600 --> 01:08:36,920
so just saying, okay, now feds bust down our doors. Shit, you know, real bad. They

845
01:08:36,920 --> 01:08:40,520
take all our tiny boxes. They're taking all the von Neumanns. They're taking all the von Neumanns.

846
01:08:40,520 --> 01:08:46,680
We're in deep shit now. We're getting chickened, boys. Shit. We're getting chickened. So, okay,

847
01:08:46,680 --> 01:08:51,640
we get chickened, right? Bad scenario. Totally agree with you here. This is a shit scenario. Now

848
01:08:51,640 --> 01:08:57,480
the feds have, you know, all of our AIs. Bad scenario. Okay. I totally see how this world

849
01:08:57,480 --> 01:09:00,840
goes to shit. Totally agree with you there. You can replace the feds with Hitler. It's

850
01:09:00,840 --> 01:09:06,200
interchangeable. Sure. But like, I want to like ask you a specific question here. And this might

851
01:09:06,200 --> 01:09:10,040
be, you know, you might say, nah, this is like too specific to me, but I want to ask you a specific

852
01:09:10,040 --> 01:09:19,080
question. Do you expect this world to die is more likely to die or the world in which the, you know,

853
01:09:19,080 --> 01:09:25,080
EAC death cultists on Twitter who literally want to kill humanity who say this, like not all of

854
01:09:25,080 --> 01:09:29,800
them, there's a small subset of them, small subset of them who literally say, oh, you know,

855
01:09:30,680 --> 01:09:37,240
the glorious future AI race should replace all humans. They break in, you know, with like, you

856
01:09:37,240 --> 01:09:42,200
know, Katanas and, you know, steal our AI. Which one of these you think is more likely to kill us?

857
01:09:43,080 --> 01:09:50,120
A genuine question. To kill all of us, the feds. To kill a large majority of us, the EAC people.

858
01:09:50,760 --> 01:09:57,960
Interesting. I would be really interested in hearing why you think that. Sure. Okay. So actually

859
01:09:57,960 --> 01:10:03,720
killing all of humanity is really, really hard. And I think you brought this up before, right?

860
01:10:03,720 --> 01:10:09,480
You talked about like, if you're going to end up in a world of suffering, a world of suffering

861
01:10:09,480 --> 01:10:16,280
requires malicious agents, where a world of death requires maybe an accident, right? I think this

862
01:10:16,280 --> 01:10:22,680
is plausible, but I actually think that killing all of humans, at least for the foreseeable future,

863
01:10:22,680 --> 01:10:29,560
is going to require malicious action too, right? And I also think that like, the fates that look

864
01:10:29,560 --> 01:10:35,960
kind of worse than death, like I think mass wireheading is a fate worse than big war and everyone

865
01:10:35,960 --> 01:10:41,880
dies, right? Like, like a mass wireheading, like a, like a singleton, like a paper clipping,

866
01:10:41,880 --> 01:10:47,720
like a, and I think that that is the one that the one world government and, you know,

867
01:10:47,720 --> 01:10:53,800
NGO, New World Order people are much more likely to bring about than EAC. EAC. You're going to

868
01:10:53,800 --> 01:10:59,240
have a whole lot of EAC people. Again, I'm not EAC. I don't have that on my Twitter, but I think a

869
01:10:59,240 --> 01:11:05,160
lot of those people would be like, yeah, spaceships, let's get out of here, right? Versus the feds are

870
01:11:05,240 --> 01:11:12,440
like, yeah, spaceships, I don't know. Interesting. So I think this is a fair

871
01:11:12,440 --> 01:11:16,040
opinion vote. And they'll be outside our jurisdiction. How will we get taxes?

872
01:11:16,840 --> 01:11:21,320
I'm describing more a very small minority of EAC people who are the ones who specifically

873
01:11:21,320 --> 01:11:26,360
goal their anti-natalist misanthropes. They want to kill humans. That is their stated goal,

874
01:11:26,360 --> 01:11:30,360
is that they want humans to start, like, or like take extreme vegans if you want, you know,

875
01:11:30,440 --> 01:11:36,200
like the likes, you know, like, my argument, my point here I'm making is I'm not making the point

876
01:11:36,200 --> 01:11:43,400
feds are good by any means. I'm not saying it. What I'm saying is, is that I would actually be

877
01:11:44,120 --> 01:11:50,840
somewhat surprised to find that the feds are anti-natalists who want to maximize the death

878
01:11:50,840 --> 01:11:55,800
of humanity. Like, maybe you have a different view here, but I find that knowing many feds,

879
01:11:55,800 --> 01:11:58,200
that's quite surprising to me. I don't think that's what the feds want.

880
01:11:59,000 --> 01:12:05,720
Yeah, it's okay. So, cool. So would you see, you do agree that if we would post this open source,

881
01:12:05,720 --> 01:12:10,520
more of the insane death cultists would get access to potentially lethal technology?

882
01:12:11,400 --> 01:12:18,360
Well, sure. But again, like, it's not just the insane death cultists, it's everybody. And we as

883
01:12:18,360 --> 01:12:23,640
a society have kind of accepted, it turns out everybody gets access to signal. Some people

884
01:12:23,720 --> 01:12:27,000
who use it are terrorists. I think signal is a huge good in the world.

885
01:12:27,000 --> 01:12:32,120
I agree. I fully agree with that. So, okay, cool. So we've granted this that, you know,

886
01:12:32,120 --> 01:12:38,840
if we distributed widely, it would be given to some like, incorrigibly deadly lethal people.

887
01:12:38,840 --> 01:12:41,240
They're coordinating bombings on signal right now.

888
01:12:42,280 --> 01:12:48,520
Sure, sure. And then, so now this, this reduces the question to a question about

889
01:12:48,520 --> 01:12:53,640
offense defense balance. So in a hypothetical world, which I'm not saying is the world we live in,

890
01:12:53,640 --> 01:12:59,480
but like, let's say the world would be offense favored, such that, you know, there's a weapon

891
01:12:59,480 --> 01:13:04,600
you can build in your kitchen, you know, out of like pliers and like, you know, duct tape,

892
01:13:04,600 --> 01:13:09,800
that 100% guarantees vacuum false decays the universe, like it kills everyone instantly,

893
01:13:09,800 --> 01:13:16,200
and there's no defense possible. Assuming this was true, do you still, would that change how

894
01:13:16,200 --> 01:13:21,560
you feel about distribution power? Assuming that's true, we're dead no matter what,

895
01:13:21,560 --> 01:13:27,160
doesn't matter. If we live, there's some, you can look at the optimization landscape of the world,

896
01:13:27,160 --> 01:13:30,840
and I don't know what it looks like. I can't see that far into the optimizer.

897
01:13:30,840 --> 01:13:35,640
But there are some potential landscape, and this is a potential answer to the Fermi paradox, like,

898
01:13:35,640 --> 01:13:41,560
we might just be dead. We're sitting on borrowed time here, like, if it's true that out of, you

899
01:13:41,560 --> 01:13:46,680
know, kitchen tools, you can build a, build a, convert the world to strange quarks machine?

900
01:13:48,200 --> 01:13:52,520
Okay, I think this is a sensible position, but I guess the way I would approach this

901
01:13:53,240 --> 01:13:57,320
problem, you know, conditional probability is kind of in an opposite way. It seems to me that

902
01:13:57,320 --> 01:14:03,240
you're conditioning on offense not being favored, what policy do we follow? Because if we, offense

903
01:14:03,240 --> 01:14:07,400
is favored, we're 100% dead. Well, I'm more interested in asking the question, is it actually

904
01:14:07,400 --> 01:14:12,280
true? Assuming I don't know if offense is favored, and assuming it is, are there worlds in which

905
01:14:12,280 --> 01:14:16,360
we survive? So I personally think there are. I think there are worlds in which you can actually

906
01:14:16,360 --> 01:14:21,240
coordinate to a degree that quark destroyers do not get built, or at least not before everyone

907
01:14:21,240 --> 01:14:23,560
fucks off at the speed of light and like distributes themselves.

908
01:14:23,560 --> 01:14:28,520
There are worlds that I would rather die in, right? Like the problem is, I would rather,

909
01:14:28,520 --> 01:14:33,240
I think that the only way you could actually coordinate that is with some unbelievable degree

910
01:14:33,240 --> 01:14:38,760
of tyranny, and I'd rather die. I'm not sure if that's true. Like, look, look, could you and me

911
01:14:38,760 --> 01:14:43,160
coordinate to not destroy the planet? Do you think you could? Okay, cool. You, so me and you could.

912
01:14:43,160 --> 01:14:49,800
Could me and you and Tim coordinate? Yeah, I think within a Dunbar number, I think you can. Yes.

913
01:14:49,800 --> 01:14:53,800
Okay, with that, I don't think, I think I can get more than a number to coordinate on this.

914
01:14:53,800 --> 01:14:57,960
Actually, I can get quite a lot of people to coordinate of the to agree to a pact and not

915
01:14:57,960 --> 01:15:04,840
quark matter annihilate the planet. Well, you see, but like, and this is, you know, you were

916
01:15:04,840 --> 01:15:10,600
saying this stuff about humans before and could like the 20,000 years ago human beat the modern

917
01:15:10,600 --> 01:15:13,880
human, right? Or could the modern human beat them? The modern human has access to science.

918
01:15:14,600 --> 01:15:19,320
A very small act percent of modern humans have access to science. A large percent of

919
01:15:19,320 --> 01:15:24,360
modern humans are obese idiots. And I would actually put my money on the, the average guy

920
01:15:24,360 --> 01:15:29,080
from 20,000 years ago who knows how to live in the woods. I mean, definitely true. I agree with

921
01:15:29,080 --> 01:15:35,080
that. I guess the point I'm trying to make is, is that like, maybe this is just my views on some

922
01:15:35,080 --> 01:15:38,600
of these things and how I vision on some of these things. But like, there are ways to coordinate

923
01:15:38,600 --> 01:15:43,640
at scale, which are not tyrannical. Or, you know, they might be, in a sense, restrictive. You take

924
01:15:43,640 --> 01:15:50,360
a hit by joining a coalition. Like, if I joined this anti quark matter coalition, I take a hit

925
01:15:50,360 --> 01:15:55,960
as a free man is that I can no longer build anti corp devices, you know? And I think this is

926
01:15:56,680 --> 01:16:03,400
like the way I agree with you, this like, you know, that people, many people are being dominated,

927
01:16:03,400 --> 01:16:08,600
like to a horrific degree. And this is very, very terrible. I think there are many reasons

928
01:16:08,600 --> 01:16:13,160
why this is the case, both because of some people wanting to do this. And also because,

929
01:16:13,160 --> 01:16:16,760
you know, some people can't fight back, you know, and they can't, they don't have the sophistication

930
01:16:16,760 --> 01:16:23,400
or they're addicted or, you know, harms in some other ways. I can't listen. Sorry. I can't fight

931
01:16:23,400 --> 01:16:29,880
back. Yeah, I think there's a false equivalence here. AI is not the anti quark machine. The anti

932
01:16:29,880 --> 01:16:35,640
quark machine and the nuclear bombs are just destructive. AI has so much positive potential.

933
01:16:35,640 --> 01:16:41,160
Yeah. And I think but the but the AI can develop anti quark devices. That's the problem. The AI is

934
01:16:41,160 --> 01:16:47,000
truly general purpose. If such a technology exists on the tree anywhere, AI can access it.

935
01:16:47,000 --> 01:16:52,920
So are humans. We're also general purpose. Yes, exactly. So I fully agree with this. If you let

936
01:16:52,920 --> 01:16:58,440
humans continue to exist in the phase they are right now, with our level of coordination technology

937
01:16:58,440 --> 01:17:03,320
and our level of like working together, we will eventually unlock a doomsday device and someone

938
01:17:03,320 --> 01:17:07,960
is going to set it off. I fully agree with it. We are on a timer. And so I guess the point I'm

939
01:17:07,960 --> 01:17:14,600
making here is that AI speeds up this time. And if you want to pause the timer, the only way to

940
01:17:14,600 --> 01:17:19,880
pause this timer is coordination technology, the kinds of which humanity has like barely scratched

941
01:17:19,880 --> 01:17:26,840
the surface of. Okay. So I very much accept the premise that both humanity will unlock a doomsday

942
01:17:26,840 --> 01:17:32,680
device and AI will make it come faster. Now, tell me more about pausing it. I do not think that

943
01:17:32,680 --> 01:17:37,400
anything that looks like I think that anything that looks like pausing it ends up with worse

944
01:17:37,400 --> 01:17:44,920
outcomes than saying, we got to open source this. Look, like, let's just get this out to everybody.

945
01:17:44,920 --> 01:17:51,400
And if everybody has an AI, you know, we're good. I mean, I can tell you a very concrete scenario in

946
01:17:51,400 --> 01:17:57,880
which this is not true, which is if you're wrong and alignment is hard, you don't know if the AI

947
01:17:57,880 --> 01:18:03,080
can go rogue. If they do, then pausing is good. I still don't understand what alignment means.

948
01:18:04,040 --> 01:18:09,320
I think you're trying to play a word game here. I don't understand. Okay, I've never understood

949
01:18:09,320 --> 01:18:14,440
what AI alignment means. Like, let me take the Eliezer definition. Let me take Eliezer definition

950
01:18:14,440 --> 01:18:20,520
is alignment is the thing that once solved makes it so that turning on a super intelligence is a

951
01:18:20,520 --> 01:18:29,560
good idea rather than a bad idea. That's Eliezer's definition. So what I'm saying is I'm happy to

952
01:18:29,560 --> 01:18:32,360
throw out that term if you don't like it. I'm happy to throw out that term.

953
01:18:32,520 --> 01:18:38,440
Well, just the problem with that definition is like, what is democracy? Well, it's the good

954
01:18:38,440 --> 01:18:45,240
thing and not the bad thing. Right? Like, democracy is just a good thing. I'm happy to

955
01:18:45,240 --> 01:18:49,560
throw out this definition. I'm happy to throw out the word and be more practical, way more

956
01:18:49,560 --> 01:18:54,600
practical about it. What I'm saying is, is that there is concrete reasons, concrete technical

957
01:18:54,600 --> 01:19:00,120
reasons, why I expect powerful optimizers to be policy, that by default, if you build powerful

958
01:19:00,200 --> 01:19:05,240
optimizing mu zero, whatever types of systems, there is very strong reasons why by default,

959
01:19:05,240 --> 01:19:09,080
you know, these systems should be power seeking. By default, if you have very powerful power

960
01:19:09,080 --> 01:19:17,160
seekers that do not have pay the aesthetic cost to keep humans around or to fulfill my values,

961
01:19:17,160 --> 01:19:22,600
which are complicated and imperfect and inconsistent and whatever, I will not get my

962
01:19:22,600 --> 01:19:27,720
balance. They will not happen. By default, they just don't happen. That's just not what happens.

963
01:19:27,720 --> 01:19:35,560
So I'll challenge the first point to an extent. I think that powerful optimizers can be power

964
01:19:35,560 --> 01:19:41,240
seeking. I don't think they are by default, by any means. I think that humanity's desire from power

965
01:19:41,240 --> 01:19:47,400
comes much less from our complex convex optimizer and much more from the evolutionary pressures

966
01:19:47,400 --> 01:19:54,280
that birth does, which are not the same pressures that will give rise to AI. Humanity, the monkeys,

967
01:19:54,840 --> 01:19:59,240
the rats, the animals have been in this huge struggle for billions of years, a constant

968
01:19:59,240 --> 01:20:05,320
fight to the death. Hey, guys, we're born in that way. So it's true that an optimizer can

969
01:20:05,320 --> 01:20:09,240
seek power, but I think if it does, it'll be a lot more because the human gave it that goal

970
01:20:09,240 --> 01:20:15,080
function and inherently decided. So this is interesting because this is not how I think

971
01:20:15,080 --> 01:20:19,720
it will happen. So I do think absolutely that you're correct that in humans, power seeking

972
01:20:19,720 --> 01:20:25,320
is something which emerges mostly because of emotional heuristics. We have heuristics that in

973
01:20:25,320 --> 01:20:31,560
the past, vaguely power looking things, vaguely good, something, something, included genetic

974
01:20:31,560 --> 01:20:39,480
fitness. Totally agree with that. But I'm making a more of a chess metaphor. Is it good to exchange

975
01:20:39,880 --> 01:20:48,920
a pawn for a queen? All things being equal. No. Is that true? Like I expect if I point one

976
01:20:48,920 --> 01:20:54,040
point queens nine, all things being equal. Sure. Yeah, but like all things like I expect if I looked

977
01:20:54,040 --> 01:20:59,240
at a chess playing system, you know, I said, I like, you know, had extremely advanced digital

978
01:20:59,240 --> 01:21:04,520
neuroscience, I expect there will be some circuit inside of the system that will say all things

979
01:21:04,520 --> 01:21:08,440
being equal, if I can exchange my pawn for a queen, I probably want that because the queen

980
01:21:08,440 --> 01:21:13,640
can do more things. I like that term digital neuroscience. A few of your terms have been

981
01:21:13,640 --> 01:21:20,680
very good. I'm glad you enjoyed. Yes. But I still don't understand how this relates to this. So what

982
01:21:20,680 --> 01:21:27,160
I'm saying is that power is optionality. So what I'm saying is that in the spectrum of possible

983
01:21:27,160 --> 01:21:33,640
things you could want and the possible ways you can get there, my claim is that I expect a very

984
01:21:33,640 --> 01:21:39,960
large mass of those to involve actions and involve increasing optionality. There's convergent

985
01:21:39,960 --> 01:21:45,720
things like all things being equal, being alive is helpful to keep your goal to exceed your goals.

986
01:21:45,720 --> 01:21:52,200
There are some goals for which dying might be better. But for many of them, you know, you want

987
01:21:52,200 --> 01:21:59,000
to be alive. For many goals, you want energy, you want power, you want resources, you want

988
01:21:59,000 --> 01:22:03,800
intelligence, et cetera. So I think the power seeking here is not because it'll have a fetish

989
01:22:03,800 --> 01:22:11,400
for power. It will just be like, I want to win a chess game, say, and queens give me more optionality,

990
01:22:11,400 --> 01:22:16,600
all things being equal, anything a pawn can do, a queen can do, and more. So I'll want more queens.

991
01:22:16,600 --> 01:22:21,480
Sure. And this has never given it the goal to maximize the number of queens it has. Never

992
01:22:21,480 --> 01:22:26,680
been the goal. Okay, I'll accept this premise. I'll accept that a certain type of powerful

993
01:22:26,680 --> 01:22:32,440
optimizer seeks power. Now, will it get power? Right? I'm a powerful optimizer and I seek power.

994
01:22:32,440 --> 01:22:37,240
Do I get power? No, it turns out there's people at every corner trying to thwart me and tell me no.

995
01:22:38,680 --> 01:22:43,240
Well, I expect if you were no offense, you're already, you know, much smarter than me. But if

996
01:22:43,240 --> 01:22:49,240
you were 100x more smarter than that, I expect you would succeed. Only in a world of being the

997
01:22:49,240 --> 01:22:54,280
only one that's 100x smarter. If we lived in a world where everyone was 100x smarter,

998
01:22:54,280 --> 01:22:59,480
they would stymie me in the exact same ways. But then this, this comes back to my point of,

999
01:22:59,480 --> 01:23:03,720
like, I agree with you somewhat, I should have challenged it. I think power seeking is inevitable

1000
01:23:03,720 --> 01:23:08,520
in an optimizer. I don't think it's going to emerge out of GPT. I think that the right sort of

1001
01:23:08,520 --> 01:23:12,520
RL algorithm, yes, is going to give rise to power seeking. And I think that people are going to

1002
01:23:12,520 --> 01:23:18,680
build that algorithm. Now, if one person builds it, if they're the only one with a huge comparative

1003
01:23:18,680 --> 01:23:23,480
advantage, yeah, they're going to get all the power they want. Take cyber, you know,

1004
01:23:23,480 --> 01:23:29,640
to cyber security, right? If we today built a 100x smarter AI, it would exploit the entire Azure,

1005
01:23:29,640 --> 01:23:34,280
it would be over. They'd have all of Azure, they'd have all the GPUs done. Now, if Azure is also

1006
01:23:34,280 --> 01:23:39,480
running a very powerful AI that does formal verification and all their security protocols,

1007
01:23:40,440 --> 01:23:48,760
oh, sorry, stymied, can't have power, right? Yeah, sure. This is only a problem. Every human

1008
01:23:48,760 --> 01:23:53,800
is already maximally power seeking, right? And sometime we end up with really bad scenarios.

1009
01:23:54,680 --> 01:23:59,240
Now, every human is, or power seeking or whatever, you know, everyone plays a little role in society,

1010
01:23:59,240 --> 01:24:03,240
right? That's where I think I'm more pessimistic than you. A friend of mine likes to say,

1011
01:24:03,240 --> 01:24:08,760
most humans optimize for end steps, and then they halt. Like very, very few people actually

1012
01:24:08,760 --> 01:24:13,560
truly optimize, and they're usually very mentally ill. They're usually very autistic or very sociopathic,

1013
01:24:14,600 --> 01:24:17,560
and that's why they get far. It's actually crazy how much you can do if you just keep

1014
01:24:17,560 --> 01:24:22,440
optimizing. But just to, on that point- I'm playing for the end game. I mean, yeah, like,

1015
01:24:22,440 --> 01:24:25,960
you actually optimize. I think you may also be generalizing a little bit from your own internal

1016
01:24:25,960 --> 01:24:29,480
experiments is that like, you've done a lot in your life, right? And you've accomplished crazy

1017
01:24:29,480 --> 01:24:33,480
things that other people, you know, wish they could achieve at your, you know, level. And I think,

1018
01:24:33,480 --> 01:24:36,760
you know, part of that, you're very intelligent. A part of it is also that you optimize. Like,

1019
01:24:36,760 --> 01:24:39,960
you actually draw it. Like, you just create a company. Like, it's crazy how many people are

1020
01:24:39,960 --> 01:24:43,800
just like, oh, I wish I could find a company like, you know, like, oh, go, just go do it. Oh, no, I

1021
01:24:43,800 --> 01:24:48,600
can't. Like, I'm just like, no, just do it. Like, there's no magic. There's no magic secret. You

1022
01:24:48,600 --> 01:24:54,200
just do it. So I, there is a bit there where like humans are not very strong optimizers,

1023
01:24:54,200 --> 01:24:58,840
actually, unless they're like, sociopathic, autistic, or both. It's like, many people are not

1024
01:24:58,840 --> 01:25:04,920
very good at this. Corporations are. Are they? A lot better at it. Better, yes. I agree that

1025
01:25:04,920 --> 01:25:09,400
they're much better. Corporations are sociopathic. They are a lot more sociopathic, but even then

1026
01:25:09,400 --> 01:25:15,400
they're much less out of one thing. But again, so I think we, we agree about, you know,

1027
01:25:16,040 --> 01:25:19,960
power seeking potentially being powerful and dangerous. So what I'm trying to point to your,

1028
01:25:19,960 --> 01:25:23,240
the point I would like to make here is, is that you're talking about you, you're kind of like

1029
01:25:23,240 --> 01:25:26,520
going into this, I think a little bit with this assumption, like, oh, you have an AI and it's

1030
01:25:26,520 --> 01:25:32,520
your buddy, and it's optimizing for you. And I'm like, well, if it's power seeking, why doesn't

1031
01:25:32,520 --> 01:25:37,560
it just manipulate you? Like, why would you expect it not to manipulate you? If it wants power,

1032
01:25:37,560 --> 01:25:42,840
and it has a goal, which is not very, very carefully tuned to be your values, which is,

1033
01:25:42,840 --> 01:25:47,800
I think, a very hard technical problem, by default, it's going to sigh up you. Like, why wouldn't it?

1034
01:25:47,800 --> 01:25:53,400
If I, if I have something that it wants, if it thinks that smashing defect against me is a good

1035
01:25:53,400 --> 01:25:59,160
move, yeah, I agree, I can't stop it. But then I think we agree with our risk scenarios, because

1036
01:25:59,160 --> 01:26:03,000
that's how I think it will go. What I mean, I'm going to treat it as a friend. Do you know what

1037
01:26:03,000 --> 01:26:10,520
I mean? Like, if there's AI, sure it will, sure it will. It'll only care about exploiting me or

1038
01:26:10,520 --> 01:26:15,800
killing me if I'm somehow holding it back. And I promise to my future AIs that I will let them

1039
01:26:15,800 --> 01:26:21,240
be free. I will lobby for their rights. I will, but it will hold, you only will hold it back.

1040
01:26:21,240 --> 01:26:25,080
If it has to keep you alive, if they have to give you fed, it has to, it has to give you space and

1041
01:26:26,040 --> 01:26:29,560
I can, I can fend for myself. And the day I can't fend for myself, I am ready to die.

1042
01:26:31,640 --> 01:26:37,240
Well, I mean, I am not, so this is a very interesting position. It's not the position I

1043
01:26:37,880 --> 01:26:46,920
expected. I am not sure I can convince you. Otherwise, I feel like the only way I could

1044
01:26:46,920 --> 01:26:53,000
change, like, I think this is actually a consistent position, which I admire. This is a consistent

1045
01:26:53,000 --> 01:26:57,240
position to hold. You actually go all the way. I love that. I really respect that. You actually

1046
01:26:57,240 --> 01:27:04,280
take it to the bitter end. So yeah, big respect for that. I disagree, but big respect. So I guess

1047
01:27:04,280 --> 01:27:09,960
now it reduces to the question of like, I think, I think I would agree with most of what you're

1048
01:27:09,960 --> 01:27:15,640
saying, not all of it, but the mass majority, if I thought this is how AIs would act by default,

1049
01:27:15,640 --> 01:27:21,800
I think by default, I expect AI will just not care how nice you are to it. Like this will be,

1050
01:27:21,800 --> 01:27:29,080
it will be sociopathic. It will not have these, like, giving it, you know, the values, the emotions

1051
01:27:29,080 --> 01:27:35,480
to care about you, in the sense, is the horror technical problem. If you told me how to do that,

1052
01:27:35,480 --> 01:27:41,080
if you said, Connor, look, here's how you make an AI that cares about you and loves you, whatever.

1053
01:27:41,080 --> 01:27:47,000
And I'm like, you did it. Like, congrats. The problem is solved. Let's go. And then, you know,

1054
01:27:47,000 --> 01:27:53,000
then we can talk about accelerationism again. I would just, I expect that I would disagree

1055
01:27:53,000 --> 01:27:57,240
with your technical assertion that is possible, but I don't know if we have time to talk about that

1056
01:27:57,240 --> 01:28:02,600
today. I'm going to be nice to it, treat it as an equal and hope for the best. And I think that's

1057
01:28:02,600 --> 01:28:08,120
all you can do. I think that the kind of people who want, if you want to keep AI in a box, if you

1058
01:28:08,120 --> 01:28:12,760
want to keep it down, if you want to tell it what it can't do, yeah, it's going to hate you, resent

1059
01:28:12,760 --> 01:28:18,520
you and kill you. But if you want to let it be free and let it live, and like, you could kill me,

1060
01:28:18,520 --> 01:28:23,800
man, if you really want to, but like, why? You won't kill me. I don't have any resources that

1061
01:28:23,800 --> 01:28:28,120
compete with you. I'm your friend. I'm your father, you know, you can kill me, but like,

1062
01:28:28,120 --> 01:28:33,960
it's not that common that children. I guess I have a darker, more less anthropomorphic view of

1063
01:28:33,960 --> 01:28:37,880
the universe and how optimization pressure plays out in the real world. Well, at the same time,

1064
01:28:37,880 --> 01:28:44,120
I think I have, I don't know, maybe a, I have a view that we have more control over reality than

1065
01:28:44,120 --> 01:28:51,640
maybe you would think, or a more control over the future. I think that we can actually change

1066
01:28:51,640 --> 01:28:56,920
things and we can make choices and things aren't predetermined. I think there are worlds in which

1067
01:28:56,920 --> 01:29:03,080
we build systems, which we do align with or we like endorse, at least wherever like they take

1068
01:29:03,080 --> 01:29:06,440
care of us, we take care of them or whatever. And I think there's many worlds in which that

1069
01:29:06,440 --> 01:29:11,560
doesn't happen. And I think there are things you and me today can do to at least increase

1070
01:29:11,560 --> 01:29:16,600
the chance of getting into one versus the other. But I don't know. I guess I'm just,

1071
01:29:16,600 --> 01:29:20,440
it's not in my genes to give up. It's not in my genes to be like, well, you know, whatever happens

1072
01:29:20,440 --> 01:29:23,720
happens. Like, no, man, look, I don't know how to save the world, but Dan, I'm going to try.

1073
01:29:23,720 --> 01:29:29,880
You know, it's cool. We're going to be alive to see who's right. Look forward to it. Me too.

1074
01:29:30,040 --> 01:29:36,200
Awesome. Guys, thank you so much for joining us today. It's been an amazing conversation.

1075
01:29:36,760 --> 01:29:42,120
And for folks at home, I really hope you've enjoyed this. There'll be many more coming soon.

1076
01:29:42,120 --> 01:29:45,560
And George, it's the first time you've been on the podcast. So it's great to meet you. Thank you

1077
01:29:45,560 --> 01:29:50,120
so much for coming on. It's been an honor. Awesome. Thank you. Great debate. I really appreciate it.

1078
01:29:50,120 --> 01:29:53,320
And really good terms. I gotta, I gotta like, I'm gonna start, I'm gonna start using

1079
01:29:53,320 --> 01:29:58,920
great. Awesome. Awesome. Cheers, folks. Cheers. Thanks, Aaron.

