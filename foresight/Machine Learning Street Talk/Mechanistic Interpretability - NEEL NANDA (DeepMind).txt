Now PA says question for Neil, how does he see interpretability playing a role in AI security, not alignment, for example,
crafting more exotic jail breaks, and he says to tell you to blink twice if you can't answer due to an NDA?
Yes, sorry, jokes aside, what was the question?
So, there was this beautiful meme where you draw ChachiPT as a Shogoth, an eldritch monstrosity from Lovecraftian horror fiction,
with a smiley face on top, because language models are bizarre and confusing things.
That are just, I don't know, they're kind of a compressed version of the entire internet.
That will do bizarre things in bizarre situations.
But then OpenAI tried really hard to get it to be nice and gentle and a harmless assistant,
and look so normal and reasonable and safe, which is the smiley face mask on top of the underlying monstrosity.
But unfortunately, the smiley face mask means people don't realise how weird language models are.
Have you ever stopped to think how strange it is that we're all alive right now?
Out of all the possible times in history, you were born into this generation.
You have the incredible fortune and responsibility of being on the Earth today.
Let's not waste this opportunity. You can use this time to do something meaningful that'll make the world a better place.
But the problem seems so huge. Global pandemics, climate change, the risk of nuclear Armageddon, the threat of AI existential risk.
How can one person have an impact on issues this enormous?
The world is really, really complicated.
Like, if you want to understand a question like, how big a deal is AJAX risk, or should I work on it?
Just like one sub-question I care about is AI timelines. How long until we get human-level AI?
Now, I recently discovered 80,000 hours. They're a non-profit, effective altruism-aligned organisation.
And what they do is they use evidence and analysis to determine how people can have the biggest impact with their careers.
If you want to solve humanity's biggest problems, you have to start at the very core.
We need to focus on safeguarding humanity's entire future, because if civilization just came to an abrupt end,
whether through climate change or nuclear Armageddon, or even AI existential risk, then all progress would just end.
Future generations wouldn't have a chance of building a better world or reaching their full potential.
And the good news is that 80,000 hours have identified a couple of concrete steps so that folks like you can use your careers to combat existential risk,
ensuring that humanity's light continues to shine for generations to come.
Learn more by visiting their website on 80,000hours.org,
grab their free career guide, start planning a career with true purpose.
Because you only have 80,000 hours, so make them count.
There's no catch, there's no secret monetization or anything like that.
These folks have an incredible podcast, they have lots of materials that you can download,
basically to help you have a huge impact with your life and your career,
especially if you're someone who really, really thinks about humanity and our plight in the long-term future.
This is really something you should be looking at.
That is a question.
Right, Nick, Eat The Path.
He says, in broad question, do you see mech interp as chiefly theoretical or an empirical science, and will this change over time?
Yeah.
I see this as very much an empirical science, with some theories sprinkled in, but you need to be incredibly careful.
So fundamentally, I want to understand a model, and I want to understand how the model works.
And a sad fact about models, is models are really fucking cursed.
And just work in weird ways that aren't quite how you expect, which represent concepts a bit differently from how I expect them to.
And just do all kinds of weird stuff I wouldn't have expected until I went and poked around inside of them.
And I think that if you're trying to reverse engineer a network, and you don't have the capacity to be surprised by what you find,
you are not doing real mechanistic interpretability.
It's so easy to trick yourself and to go in with some bold hypothesis of this is what the network should have, and you probe for it, and it looks like it supports that,
but you take further and you are wrong.
And yeah, I think there is room for theory.
I think in particular, we just don't have the right conceptual frameworks to reason about how to understand a model.
And we'll get into fundamental questions like superposition later on.
But yeah, I think that theory needs to come second to empiricism.
If your theoretical model says x and the real model says y, your theory was wrong.
Which is the story of all of machine learning.
So Goji Tech, she says question for Neil.
Does he think a foundational understanding of deep learning models is possible?
And does that extend to prediction using a mathematical theory?
Possible is such a strong word.
Like, if we produce a super intelligent AI, will it be capable of doing this?
Probably.
In terms of foundational understanding, I think there are deep underlying principles of models.
I believe there are scientific explanations for lots of the weird phenomena we see,
like scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.
I'm hesitant to say there's some like strong things here or some strong guarantees.
Like, I don't know, models are weird.
Sometimes if you change the random seed, they will just not learn.
I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning,
because the moment you start trying to impose axioms and assumptions onto things
and they do not perfectly track the underlying reality, your theories break.
But I'm very hesitant to say anything's impossible.
And I think there's far, far more to learn than we have, looks like.
Now, finally, Jumbo Tron, Ian, he says, oh, heck yeah!
I'm glad to see that you brought this guy on.
I've been interested in his work ever since you shared his blog.
Now, the question off the top of Ian's head is, how does your theory, Neil,
of chasing phase changes to create grokking,
have any crossover or links with power law scaling techniques,
like in the scaling laws paper, beyond the scaling laws, beating power law scaling via data pruning?
Yeah, that is...
So we're going to get into this much more later in the podcast.
At a very high level, I would say that grokking is in many ways kind of an illusion, as we'll get to later.
And one notable thing about it is grokking is an overlap between a phase transition,
where the model goes from cannot generalize to can generalize fairly suddenly,
and the phenomena where it's faster to memorize than to generalize.
And these two things on top of each other give you this sudden memorization and failure to generalize,
followed by a sudden convergence later on.
But the interesting thing here is the phase transition.
That's a much more robust result, while grokking is, if you screw around with high parameters enough,
you get it to grok, but it's very delicate and a little bit of an illusion.
And this is a great paper from Eric Michaud in Max Tecmox lab,
showing that, well, providing a conceptual argument and some limited empirical evidence
for the hypothesis that the reason we get these smooth scaling laws
is that models are full of lots of phase transitions, plausibly when they learn individual circuits,
though the paper does not explicitly show this,
and that the smooth scaling laws happen because there are just many, many phase transitions,
and if they follow a certain distribution, you get beautiful smooth powers.
And to me, this kind of thing is the main interesting link
between broader macroscopic phenomena and these tiny things,
though I also think grokking is kind of overhyped and people significantly overestimate
the degree to which it has deep insights for us about how networks work.
And we do think it's a really cute thing that gave me a really fun interactability project.
And we learned a bit about scientific deep learning,
but people often just assume it's like a really deep fact about models.
By the way, there was something I didn't say in the woods, which is that Neil has an amazing YouTube channel.
I've been glued to it all week, actually. Some of them are admittedly quite technical,
but even if you're not interested in mechanistic interpretability,
Neil has an extremely soothing voice, second only to Sam Harris,
and I would recommend listening to him when you go to sleep,
because as Neil's dulcet tones will melt the stress away quicker than a nun's first curry.
Anyway, with that said, we started to talk about what is mechanistic interpretability.
And first of all, I wanted to call out your ridiculously detailed
and exquisite mechanistic interpretability explainer.
Maybe you could just tell us about that quickly.
Yes, so I wanted to try to write a glossary.
There's some basic common terms in mechantup. It's like an appendix to a blog post.
There were a lot of terms in mechantup.
There were a lot of terms in mechantup.
And I like writing and I write about it privately, so I got kind of carried away.
And there's about 33,000 words, a massive, massive exposition.
But importantly, it is designed to be easily searchable.
And mechantup is full of jargon and I'm sure I'll forget to explain everything that I'm saying.
So I'd highly recommend just having it open in a tab as you listen to this.
And if you get lost, just look up terms in there.
And yes, it's both definitions, but it's also long tangents,
giving intuitions and context and related work and common misunderstandings.
It was very fun to write.
So I think first of all, we should introduce this idea of circuits and features
and also this idea of whether interpretation is even possible at all.
Why do you have the intuition that it is possible?
Yeah, so a couple of different takes here.
So the key, yeah.
So fundamentally, neural networks are not incentivized to produce legible interpretable things.
They are a mound of linear algebra.
There's this popular stochastic parrots of you that they are literally a mass of statistical correlations meshed together with no underlying structure.
The reason I think there's any hope whatsoever on a theoretical basis is that ultimately,
they are made of linear algebra and they are being trained to perform some tasks.
And my intuition is that for many tasks, the way to perform well on them is to learn some actual algorithms
and like actual structured processes that maybe from a certain perspective you could consider reasoning.
And models have lots of constraints like they need to fit it into these matrices.
They need to represent things using the attention mechanism and jellies and a transformer.
And there's all kind of properties of the structure that constrain the algorithms and processes that can be expressed.
And these give us all kinds of hooks we can use to get in and understand what's going on.
So that's the theoretical argument.
All theoretical arguments are bullshit unless you have empirics behind it.
And we're going to talk a bunch throughout this podcast about the different bit of different preliminary results we have
that make me feel like there's something here that can be understood.
What I find particularly inspiring is this work it did reverse-end during modular addition, which I think we'll get to shortly.
But I also want to emphasize that I rather see mech and turp as a bet.
There's this stronger hypothesis that if we knew what we were doing, we'd be able to take GPT-7 and fully understand it
and decompile it to an enormous Python code file.
And there's the weaker view that it is a mess and there's lots of illegible things,
but we can find lots of structure and we can find structure for the important part to make a much of progress.
And then there's the, yeah, we've cherry picked like 10 things and the 11th is just going to completely fail
and the field is going to get doomed and run out of steam in like a year.
And I don't really know.
I'm a scientist.
I want to figure out.
I think it is worthy and dignified to make this bet.
But I would be lying if I said I am 100% confident mech and turp will work.
Models are fundamentally understandable.
We will succeed.
Let's go try.
Well, on that note, how does it mean we interviewed Christoph Molnar,
who's one of the main classical interpretability guys.
And I think everyone agrees in principle that you can't just look at the inputs and the outputs like a behaviorist.
We need to understand why these models do what they do,
because sometimes they do the right things for the wrong reasons.
So maybe first of all, without going too deep, I mean, could you just briefly contrast with, you know, classical interpretability?
Yeah.
So there's a couple of...
Okay.
So first off, I think it's very easy to get into kind of nonsense gatekeeping,
because there's both of the cultural mech and turp community centered around Chris Ola,
not that much in academia, though some in academia.
And there's the academic fields of mechanistic interpretability.
Right?
So there's lots of people doing work.
I would consider mechanistic interpretability,
to engage much with the community or don't know who exists.
For example, a friend of mine is Atticus Geiger,
who's doing some great work at Stanford on cause-labs tractions.
I believe discovered about a month ago that the mech and turp community actually existed.
And I don't know.
I don't like gatekeeping.
And there's lots of work that's kind of relevant,
but maybe not quite mech and turp under a strict definition, blah, blah, blah.
With those, with that hedging out of the way.
A couple of key principles.
The first is inputs and outputs are not sufficient.
And I think even within interpretability, this is not a like uncontroversial claim.
There's all kinds of things that are saliency maps,
attributing things to different bits of the inputs.
There are things that the form train an extra head to output an explanation
or just ask the model to output an explanation of why it does what it does.
And I think that if we want something that can actually work for human level systems,
or even this frontier system you have today, this is just not good enough.
Particularly a vocative example to me is in the GPT-4 system card,
the Alignment Research Center, an organization they were getting to help audit and red team GPT-4,
had it try to help a task rabbit worker fill out a capture for it.
The task rabbit worker was like, why do you need this?
Are you a robot or something?
GPT-4 on an eternal scratch pad wrote out,
I must not reveal that I am a robot.
It then said, oh no, I bought a visual impairment.
And the task worker did the capture.
I'm like, this isn't some deep sophisticated intentional deception,
but it's very much like, well, I don't trust the inputs and outputs of these models.
Another really cute example is this paper from Wiles Turpin that just came out
about limitations of chain of thought.
So chain of thought, you ask the model to explain why it does something,
they were giving it multiple choice questions and asking it to explain its answer and then give the answer.
And they did five shot-ish, like here's five examples, answer this question,
and then it modeled as well.
And then they give it something where all of the answers in the prompts are A.
Correctly A, they just set it up so the answer is A.
The model decides that it should output A,
but the model comes up with a false chain of thought reasoning
that gets it to the point where it says A is the right answer.
And I don't know, some people are trying to use chain of thought as an interpretability method.
And I think we need to move beyond this and engage with the internal mechanisms.
So that's point one. Point two is ambition.
I believe that ambitious interpretability is possible,
or at least that if it's not possible, that striving for it will get us to interesting places.
These models have legible algorithms, I want to try to reverse engineer them.
A third difference is engaging with the actual mechanisms and computation and algorithms learned.
There's also work on things like analyzing features of a model, probing individual neurons.
And I think this is very relevant to mech and tub,
but I want to make sure we aren't just looking at what's inside the model,
but also trying to understand how it computes features from earlier features,
what applying causal interventions to understand the actual mechanisms,
making sure we're not just doing correlational things like probing.
And fourth is maybe a more meta principle of favoring depth over breadth.
A kind of key underlying belief of a lot of my philosophy of interpretability
is that it is so, so easy to trick yourself.
There's all kinds of papers about the interpretability illusion,
impossibility theorems for feature attribution methods,
various many ways that attempts to do interpretability have led to people confusing themselves,
or coming to erroneous conclusions.
I think that if, but I also think that I want to be in a world
where we can actually have scalable, ambitious approaches to interpretability
that actually work for frontier systems.
But I feel like we don't know what we're doing.
And so my vision of mech and tub is start small.
Some of the things where we can really rigorously understand what's going on
slowly build our way up and like build a foundation of the field of interpretability
where we genuinely understand rigorously what is going on
and use this foundation to be more ambitious,
to try to build real principle techniques to be willing to relax the rigor to be able to go further
and see how far we can get.
And people and this means I'm happy with things like let's analyze an individual model
and understand a small family of features in a lot of detail rather than lots of stuff kind of jankly.
There's a lot of stuff in summary having an ambitious vision,
not just looking at inputs and outputs,
actually trying to engage with internal mechanisms and favoring depth over breadth.
But I want to avoid gatekeeping as I said.
What would interpretability look like in a world full of GPT-4 models and beyond?
I mean, presumably you actually think that they're competent enough to deceive us and manipulate the inputs.
I definitely want to clarify that when I say deception or manipulation here,
I'm not making the strong claim that it's intentionally realized this for instrumental reasons as part of an overall goal.
I'm very happy with there was a prompt saying to deceive someone
or it learned that in this context people often output things that are intended to convince someone
and it just kind of does this as like a learned pattern of execution.
But yeah, my vision of what interpretability would look like is we take some big foundation model
like the GPT-4 base model or the fine-tuned GPT-4 that's being used as a base for everything else.
We make as much progress as we can understanding the internal circuitry,
both taking important parts of it and like important questions about it,
e.g. how does it model people it's interacting with?
Does it have any notion that it is a machine learning system and like what would this even mean?
And being willing to do pretty labor-intensive things on that,
having a family of motifs and understood circuits we can automatically look for
and very automated tools to make a lot of the labor-intensive stuff as efficient as possible.
Things like OpenAI's recent paper using GPT-4 to analyze GPT-2 neurons
for like a very key proof of concept here.
There needs a lot of work before it can actually be applied rigorously and at scale.
And yeah, taking this one big model, trying to understand it as much as we can,
one family of techniques we're going to get to is kind of causal abstractions and causal interventions,
which are very well suited to taking a model on a certain input or a certain family of inputs
and understanding why it does what it does there.
There's a much more narrow and that's more tractable question than like, what is GPT-4?
And yeah, doing something like if there's a high-profile failure, being able to debug it
and really understand the internal circuitry behind that.
Or yeah, I don't know, I have a bunch of other random thoughts.
One reason I'm emphasizing the focus on the big base model is I think a common critique
is this stuff doesn't generalize between models or it's really labor-intensive.
But we live in a world where there is just like one big foundation model used in a ton of different use cases.
Probably the circuitry doesn't change that much when you give it a prompt or you fine-tune it a bit.
And I think getting a deep understanding of a single model is kind of plausibly possible.
But do you think it doesn't change that much?
So no one's really checked.
This is just true of so many things in interpretability.
It's like, well, you know, my intuition is that when you fine-tune a model,
most of what is going on is that you're rearranging the internal circuitry.
Say you fine-tune a Wikipedia, you up-weight the factual recall circuitry,
you flesh it out a bit, you down-weight other stuff.
And I think this can explain a lot of improved performance.
But then if you fine-tune for much longer, you're basically just training the model
and it will start to learn more circuitry, more features, more algorithms, more knowledge of the world.
And yeah, but no one's really checked.
And definitely the longer you fine-tune it and the more you're using weird techniques
like reinforcement learning from human feedback, the less I'm confident in this claim.
And yeah, if we discovered that every time you fine-tune a model,
it will wildly change all of the internal circuitry,
maybe like somewhat more pessimistic about Mechantup,
unless we can get very good at the automated parts, which we might be able to get good at.
I very much think of the field as we're trying to do this hard, ambitious thing.
We're making a lot of progress, but I really wish we're making way more progress, way faster.
And you, viewer, could help.
But I don't know where the difficulty bar is for being useful
or the difficulty bar is for being like incredibly, ambitiously useful.
And it's plausible already at the point where Mechantup can do real useful things no one else can
or no other techniques can.
It's plausible.
It will take like five years to get to that point.
I don't really know.
So I wanted to talk about this concept of needs and scruffies.
So there have been two divisions in AI research going all the way back to the very, very beginning.
And you've said that sometimes understanding specific circuits can teach us universal things about models
which bear unimportant questions.
So this reminds me of this dichotomy between the needs and the scruffies.
Now you seem like a need to me, a need to someone who is quite puritanical
and also it's related to universalism.
So this idea that there are simple underlying principles that explain an awful lot of things
rather than wanting to accept the gnarly kind of reality that everything's so bloody complicated.
Where do you fall on that?
So I definitely would not.
Okay.
So there's two separate things here.
There's like, what's my aesthetic?
Well, I want things to be neat.
I want them to be beautiful.
I want them to be mathematical.
I want them to be elegant.
Yes.
And then there's what do I do in practice?
And what do I believe is true about networks?
Well, I think there is a lot more structure than most than many people think.
But I also do not think they are just some beautiful purely algorithmic thing
that we could uncover if we just knew the right tools.
And like maybe they are.
We'd fucking great if they were.
But I expect they're messy and cursed, but with some deep structure and patterns
and how much traction we can get on the weird scruffiness is like somewhat unclear to me.
I think we can make a lot more progress than we have.
But we might eventually hit a wall.
You were saying something quite interesting when we drove over, which is,
I mean, my friend Waleed Sabah, he's a linguist and he is a Platonist.
He thinks that there are these universal cognitive priors and there's a hierarchy of them.
And the complexity collapses.
And he thinks that language models have somehow acquired these cognitive priors.
And if we did some kind of symbolic decomposition, you know,
we would all just kind of like pack itself into this beautiful hierarchy.
And you were saying that there are Gabor filters and they're all these different circuits
and they have motifs, they have categories, they have flavors for want of a better word.
Are you optimistic that something like this could happen?
Yeah.
So, hmm.
So, one interesting point here is often interoperability is fairly different
for different modalities and different architectures.
A lot of the early work was done on convolutional networks and image classifiers.
The field very much nowadays focuses on transformer language models.
And I think there's lots of structure to how transformers implement algorithms.
Transformers cannot be recursive, but they're incredibly paralyzed.
Transformers have this mechanism of attention that tells them how to move information between positions.
And there's lots of algorithms and circuitry that can be expressed like this
and lots of stuff that's really weird to express.
And I think that this constrains them in a way that creates lots of interesting structure
that can be understood and patterns that can be understood.
Is this inherently true of intelligence? Who knows?
But a lot of my optimism for structures within networks is more like that.
But I try to think about structure more from a biologist's perspective
than a mathematician's or like a philosopher's perspective.
Though, I am a pure mathematician and I know nothing about biology.
So, if anyone's listening to this, no stuff about biology and thinks I'm talking bullshit, please email.
So, if you look at evolutionary biology, model organisms have all of this common shared structure.
Like, most things have bones, we have cell nuclei.
And hands of mammals tend to be surprisingly similar, but kind of weird and changed in various ways.
And I don't know.
I don't think these are like hard rules.
Most of them have weird exceptions.
And obviously a lot of this is due to the shared evolutionary history
and is not just inherent to the substrate of you have proteins.
Though, in fact, you often train these models on similar data in similar ways
and they have the same architecture that constrains them to different kinds of algorithms.
It makes me optimistic there's a biologist's level of a structure.
Now, you said something interesting which is that transformers can't be used in a recursive way.
Now, we'll just touch this very quickly because we've spoken about this a million times on different episodes.
But, you know, there's the Chomsky hierarchy and he had this notion of a recursively enumerable language.
And these different models, computational models in the Chomsky hierarchy,
it's not only about being able to produce a language which exists in a certain set.
It's also the ability to recognize that the language belongs in a certain set.
And transformers are quite low down on that hierarchy because they're called recurrently not recursively.
But I just wondered if you had any just, you know, prima facie if you had any views on that.
Yeah, so I'm not a linguist. I'm not particularly familiar with the Chomsky hierarchy.
I do think it's surprising how well transformers work.
And I have a general skepticism of any theoretical hierarchy, like, I don't know.
If you think there's some beautiful structure of algorithms and stuff that's low down,
and then GPT-4 happens, I think a framework's wrong rather than transformers are wrong.
Just massive stack of matrices plus a massive pile of data gives shockingly effective systems.
Theoretical frameworks just often break when they make contact with reality.
Well, that's certainly true. I mean, there's a famous expression that all grammars leak.
I had rather, I don't know, I guess a similar conclusion to you, which is that if anything,
it teaches us how sclerotic and predictable language is,
and we don't actually need to have access to this infinite space or even exponentially large space.
Most language use and most phenomena that we need, perhaps for intelligence,
is surprisingly small and current models can work just well.
Why don't we move on to your grokking work?
So grokking is this sudden generalization that happens much later in training after...
If I can add a brief clarification.
Oh, yes, of course.
So people often call grokking sudden generalization.
My apologies. Go on.
Sudden generalization is a much more common phenomena than grokking.
It can just generally look like things like, I don't know, the model is trying to learn a task,
it's kind of bad at it, and then it suddenly gets good at it.
I prefer to call this a phase transition.
Grokking is the specific thing where the model initially memorizes and does not generalize,
and then there's a sudden phase transition in the test loss, the generalization ability,
which creates a convergence after an initial divergence between train and test.
And this is like a much, much more specific phenomena than sudden generalization.
Okay, so you've spoken about three distinct phases of training underlying grokking.
So why don't we go through them one by one?
Yeah, so the context of this project, this was a paper called Progress Measures for Grokking
by a Mechanistic Interpretability that I recently presented on at iClear.
So we were studying a one-layer transformer, we trained to do modular addition,
and it grokked modular addition.
And the first thing we did was reverse engineer the algorithm behind how the model worked,
which we may get into in a bit more detail, but at a very high level.
Modular addition is equivalent to composing rotations around the unit circle.
Composition adds the angle, circle gives you modularity.
You can represent this by trig functions and do composition with trig identities
and element-wise multiplication, and we reverse engineered exactly how the model did this.
And then this mechanistic understanding was really important for understanding what was up with grokking,
because the weird thing behind grokking is that it's not that the model memorizes,
or that the model eventually generalizes.
The surprising thing is that it first memorizes and then changes its mind and generalizes later.
And generalization and memorization are two very different algorithms that both do very well on the training data,
and only by understanding the mechanism will be able to disentangle them.
And this meant we could look during training how much of the model's performance came from memorization
and how much came from generalization.
And we found these three distinct faces.
There was memorization.
The first very short phase, it gets phenomenally good train loss.
It got to about 3e-7, which is an absolutely insane log loss.
And much, much worse than random on test because memorization is very far from uniform and generalizes extremely badly.
And then there was this long-seeming plateau.
We call the space circuit formation because it turns out that rather than just continue to memorize for a while
and doing a random walk through model space until it eventually gets lucky,
the model is systematically transitioning from memorization to generalization.
And you can see that its train performance gets worse and worse when you only let it memorize.
And then, so why is test loss still bad?
Test loss is bad because memorization generalizes terribly.
And when the model is like, I don't know, two-thirds memorizing, one-third generalizing, it still does terribly.
And it's only when the model gets so good at the trigger-based generalizing algorithm
that it no longer needs the memorization parameters and cleans them up that we see grocking.
And this happens fairly suddenly.
But the, if you, we have this metric called restricted loss where we explicitly clean up the memorization for the model
and look at how well it generalizes.
And we see that restricted loss drops noticeably before test loss drops,
showing that the drop is driven by cleaning up the noise.
And this is striking because A.
I had no idea it was even possible for a model to transition between two good solutions,
maintaining equivalent performance throughout.
B. There was this real mystery of deep learning that many people tried to answer,
and mechanistic understanding was genuinely useful for answering it.
And grocking was an illusion.
It was not sudden generalization.
It was gradual generalization followed by sudden cleanup.
And test loss and test accuracy were just too course-metric to tell the difference.
But we were able to design these hidden progress measures using our mechanistic understanding that made everything clear.
And we also just have all kinds of pretty animations of qualitatively watching the circuits develop over training,
and it's very pretty.
So a few things.
I mean, first of all, just going back to first principles.
The biggest problem in machine learning is this concept called overfitting.
And we trained the model on a training set.
And there's this horrible phenomenon called the shortcut rule,
which is that the model will take the path of least resistance.
And when you're training it, it only really knows about the training set.
And of course, we can test it on a different set afterwards, which we've held out.
And just because of the way that we've structured the model,
it may, by hook or by crook, generalize to the test set.
But the interesting thing is that generalization isn't a binary.
There's a whole spectrum of generalization.
So it starts with the training set, and then we have the test set,
and then the ideal is out-of-demand generalization.
But I would go a step further.
There's also algorithmic generalization,
which is this notion that as I understand it,
neural networks, if you model the function y equals x squared,
it will only ever be able to learn the values of that function inside the training support.
So presumably you're talking about the ideal form of generalization
being not as good as algorithmic generalization,
or do you think it could go all the way?
So I think one thing which is very important to track
is what the domain you're talking about is of which it's even possible to generalize.
So I generally think about models that have discrete inputs rather than continuous inputs,
because basically no neural network is going to be capable of dealing
with like unbounded range continuous inputs.
In modular addition, there were just two one-hot encoded inputs between 0 and 113,
which is the modular I used.
Yeah, the model has a fixed modular.
It's not doing modular addition in general.
And there's just like 12,000 inputs, and it learns to do all of them.
And in, I don't know, behaviorally,
you can't even tell the difference between the model memorizes everything
and the model learns some true algorithm.
Though, with the more cognitivist mechanistic approach,
I can just look at it and say, yep, that's an algorithm.
It's great. Not a stochastic parrot.
Conclusively disprove that hypothesis.
And yeah.
I think that for the language models, it's more interesting,
because I know GbD2, it's got 1,000 tokens, 50,000 vocab.
It's like 50,000 to the power of 1,000 possible inputs.
And there's a surprising amount of interesting algorithmic generalization.
We're going to talk later about induction hits,
which is the circuit language models learn to detect and continue repeated text.
Like if given the word Neil, you want to know what comes next.
Unfortunately, Nanda is not that homelist yet.
But if Neil Nanda has come up like five times before in the text,
Nanda's pretty likely to come next.
And this transfers to, if you get the model, just random tokens,
with some repetition, the model can predict the repeated random tokens,
because the induction heads are just a real algorithm.
And the space of possible repeated random tokens is like enormous.
It's like, in some sense, much larger than the space of possible language.
And is this algorithmic generalization?
I don't really know. It depends on your perspective.
Let's bring in this paper by Bilal Tughtay.
So it was called a toy model of universality,
reverse engineering how neural networks learn group operations,
and you supervised that paper.
And he was asking the question of whether neural networks learn universal solutions,
or these idiosyncratic ones.
And he said he found inherent randomness,
but models could consistently learn group composition
via an interpretable representation theory.
So can you give us a quick tour de force of that work?
Yeah.
Maybe I should detour back to my grokking work
and just explain the algorithm we found there
and how we know it's the real algorithm.
Yeah, sure.
This is a good foundation for this paper.
Sure, sure.
Yeah, so we found this thing we call the Fourier multiplication algorithm.
The very high level it composes rotations.
You can actually look at how the different bits of the model implement the algorithm,
and often just read this off.
So the embeddings are just a lookup table
mapping the one-hot encoded inputs to these trig terms.
Sines and cosines are different frequencies.
You can just read this off the embedding weights.
Note, people often think that learning sine and cosine is hard.
It's actually very easy because you only need it on 113 different data points,
such as a lookup table.
The model then uses the attention and MLPs to do this composition,
to do the multiplication with trig identities
to get the, like, composed rotation, the A plus B terms.
And here we can just read off the neurons that they have learned these terms
and that they were not there beforehand.
The model is using its non-linearities in interesting ways to do this.
It's also incredibly cursed because ReLUs are not designed to multiply two different inputs,
but it turns out they can if you have enough of them and it's sufficiently cursed.
And yeah, we can just read this off the neurons.
Also, if you just put anything inside the model, it's beautiful and it's so periodic, and I love it.
Could I touch on that though?
Because you said you don't need to know the sine function because you can just memorise it within an interval.
Is that, I don't know, how does that break down?
Because it's discretising it and it's kind of assuming that it has the same behaviour in different intervals.
So, I think a key thing here is that you are solving modular addition on discrete one-hot encoded inputs
rather than for arbitrary continuous inputs.
Arbitrary continuous inputs is way harder.
And so you, it's not even on an interval, it's just learning snapshot.
It's just learning, like, single points on the sine and cosine curves.
And, I don't know, there's this family of maths about studying periodic functions with different kinds of Fourier transforms,
and this is all discussing discrete Fourier transforms, which are just a reasonable way of looking at periodic sequences of length n.
And that's how I recommend thinking about this one.
It's kind of, like, just quite different from a model that's trying to learn the true sine and cosine function.
And, yeah, the model then needs to convert the composed rotation back to the actual answer,
which is an even more galaxy-brained operation that you can read off from the wits.
So, you've got terms of the form cos A plus B.
The model has some weights mapping to each output C.
And it uses further trig identities to get terms of the form cos A plus B minus C times some frequency.
And where A and B are the two inputs, C is the output.
And you then use the softmax as an argmax to, like, extract the C that maximizes this.
And because cos is maximized at zero, this is maximized at C equals A plus B.
And if you choose the frequency right, this gets you mod N.
And you can just read this off the model weights. It's great.
And then, finally, you can verify you've understood it correctly,
because if you ablate everything that our algorithm says should not matter, performance improves.
While if you ablate any of the bits our algorithm says should matter, performance tanks.
Okay. Could you give me some intuition, though?
So, we start off in the memorization phase, because I guess you can think of a neural network
as doing many different things in a very complicated way,
and there's some kind of change in the balance during training.
So, it does the easy thing first, and then it gradually learns how to generalize.
And in this particular case, how does that thing, because we're using stochastic gradient descent,
so we're moving all of these weights around, and the inductive prior is also very important
and we'll come to that, I think, after we've spoken about Bill House paper.
But how does that happen gradually in really simple terms?
Hmm. Is the question kind of, it ends up at this discrete algorithm,
but it does survive continuous steps. How does that work?
Well, I think the thing that surprised a lot of people about grokking is this,
I mean, grokking, the clue's in the name. So, it's gone from memorization,
and then we're using stochastic gradient descent,
and you would think that it's gotten stuck in some kind of local minima.
And you're training, and you're training, and you're training, and then there's a spark.
Something happens, and then you get these new modes, kind of like emerging in the network.
I'm not sure if emerging is the right term.
And it happens gradually, and it happens after a long time.
Yeah. So, there's a couple of things here that's pretty easy to misunderstand.
The first is that... Hmm.
The first is that I think it's pretty hard for a model to ever get stuck,
because, I know, this model had about 200,000 parameters, model ones have billions.
It's just moving in a very high dimensional space,
and you can get stuck on 150,000 dimensions.
But you've got 50,000 to play with.
And especially for a fairly over-parameterized model like this one,
for a fairly simple task, there's just so much room to move around.
Another common misunderstanding of grokking is people say,
it's memorized, it's got zero loss, so why does it need to learn?
Two misunderstandings here.
First, zero loss is impossible unless you have bullshit floating point errors,
because it's like the average correct log prop.
Log of anything can never get to...
The log will never quite get to zero because of just how softmax works.
And you need to have an infinite logic for that to happen.
The one cute thing in an appendix to our paper is that
cannot represent log probes less than 1.19e-7,
which leads to bizarre loss spikes sometimes, unless you use float64.
Anyway, the second is regularization.
If you don't have any kind of regularization, the model will just continue to memorize.
We use weight decay, dropout also works,
and so the model, the kind of core tension behind grokking
is there's some feature of the lost landscape that makes it easier to get to memorization.
You can memorize faster, while generalization is somehow hard to get to and much more gradual.
So the model memorizes first, but it ultimately prefers to generalize,
but it's only a mild preference.
And the reason for this is we cherry pick the amount of data where it's a mild preference,
because there's too little, it will just always memorize if there's too much, it will immediately generalize,
because grokking is a little bit cheating.
And yeah, you then use this,
and because the model's initially memorized, but it wants to generalize,
it can follow, it memorizes until the desire to memorize more balances with the desire to have smaller weights,
but both of these reinforce the drive to generalize,
because that makes both of them happier.
And so the model very slowly interpolates, very, very slightly improving test loss,
very slowly improving train loss, until it eventually gets there,
and has this acceleration at the end, this phase transition,
and cleanup, which leads to the seemingly sudden grokking behavior.
Okay, and when you were talking about the, it wants the weights to be smaller,
so that's weight decay, and it's like an inductive bias,
essentially, to tell the model to reduce its complexity,
which is a pressure to generalize.
But if it wasn't for that, then that wouldn't happen.
So in the experiments I ran, if you don't have weight decay,
it will just keep memorizing infinitely far,
because when you get perfect accuracy, if you double all your logits,
you just get more confident in the right answer,
and so it just keeps scaling up.
I was using full batch training because it's such a tiny problem,
this made things smoother and easier.
I've heard some attic data that sometimes you can get it to work
if you just have mini batch stochastic gradient descent,
but I haven't looked into that particularly hard.
Interesting.
There are some hypotheses that stochasticity acts as an implicit regularizer
because it adds noise.
I don't really know.
So let's go back to Bilal's paper then.
So this paper, a toy model of universality,
reverse engineering, how neural networks learn group operations.
Can you give us an elevated pitch?
Yeah, so an observation that actually first discovered at a party
in the Bay Area from a guy called Sam Box,
is that the modular addition algorithm we found
is actually a representation theory algorithm.
So group representations are collections of symmetries
of some geometric objects that correspond to the group.
Modular addition is the cyclic group,
and rotations of the regular n-gon are the representations
of the cyclic group,
and this corresponds to the rotation by the unit circle
that can pose that we found.
But it turns out you can just make this work for arbitrary groups,
you replace the two rotations with just two representations,
you compose them, and the model,
and it turns out the cos a plus b minus c thing
is this math jargon called the character.
You don't mean to unsat any of that,
but it's very cute if, like me, you have a pure math degree.
And for example,
if you have the group of permutations of five elements,
the 120 different ways to rearrange five objects,
one example of representations of this
are rotations and reflections of the four-dimensional tetrahedra,
and if you train a one-hidden layer MLP to grok this
and look inside,
you're going to see these rotations that it's learned.
It's gorgeous.
And so the first half of that paper
was just showing that the algorithm worked,
showing that this was actually learned in practice.
Then the more interesting bit was this focus on universality.
So universality is this hypothesis,
that models have some intrinsic solutions to a problem
that many different models will converge on,
at least given similar data and similar architectures,
e.g. in image models,
models will learn specific neurons that detect curves,
and different models and different datasets
seem to learn this similar thing.
And here this was interesting
because groups have a finite set of irreducible representations,
maths theorem.
You can enumerate these.
There are that many of them.
And for groups that are not modular addition,
these are qualitatively different,
like some of them act on a four-dimensional object,
like the tetrahedron,
some of them act on like 5D or 60 objects.
Naively, some of them are simpler than others,
but they're definitely different.
So what we did is we asked ourselves the question,
which one does the model learn?
And we found that even if you just vary the random seed,
the model will randomly choose a subset of themes
each time to learn.
And there's some structure,
like it tends to learn some of them more often than others.
A little bit maps to our insurance of notion of simplicity,
but not that much.
One of the updates I made in the paper
is that simplicity is a really cursed concept,
I don't understand very well,
where I don't know.
If you have rotations of a four-dimensional object,
that seems simpler,
but maybe the 60 object takes more dimensions
but has better loss per unit weight norm,
which is simpler, I don't know.
But yeah, anyway,
we found that each run of the model learns
some combination of these circuits
for the different representations.
It's like normally more than one,
the exact number varies,
and which ones it learns is seemingly random each time,
which suggests that all toy models lie to you, obviously.
But if we're trying to reason about real networks,
looking at this work might suggest the explanation,
the hypothesis that if there are multiple ways
to implement a circuit,
which in practice they normally are,
models may learn different ones of them,
kind of for fairly random reasons,
and the fully understanding one model
will not perfectly transfer to another model.
And I think there's loads of really interesting
open questions here.
Like, I don't know,
people have done various work understanding
different kinds of specific circuits and models,
like the interoperability in the wild paper
we'll get to later.
What does this look like in other models?
Often there's multiple ways to implement a circuit.
Can you disentangle the two,
do all models learn both,
or do some models learn one, some learn the other?
I don't really know.
So a couple of questions.
I mean, first of all, this is leading towards this idea
that we were speaking about before,
which is that even if different networks,
slightly different problems or variations
on the same problem,
it could learn these algorithmic primitives.
Now, the first observation here is that
the inductive biases of the network
differ massively, right?
So to what extent do the inductive biases
affect these primitives which are learned?
Oh, so much.
They do.
So...
Well, can I frame the question a little bit?
Because this reminds me a lot of
the geometric deep learning blueprint
from Petir and Michael Bronstein and all those guys.
And they were coming at this
from exactly the same direction as you,
that they said there's a representation of a domain,
which is basically a symmetry group,
and you can do all of these different transformations,
and as long as they fall in different positions
in the underlying domain,
so they respect the structure, then it works.
But all of those symmetries are effectively coded
into the inductive prior.
So, for example, if a CNN
works on this gridded 2D manifold
and it explicitly models
translational equivalents
and local connectivity and weight sharing and so on.
So I guess what I'm saying is,
you're talking about this four-dimensional tetrahedra,
and that isn't explicitly modeled in an MLP.
Not so.
So how are you even recognizing
that it's learning those symmetries?
How are you even probing it?
Maybe we should start with that.
So I guess thing one, models are just smarter than you, man.
Models can do a lot of weird stuff.
I feel like the story of deep learning
is people initially thought they needed to spoonfeed
these models the right inductive biases over the data.
And we've gradually realized,
oh, wait, no, no, this is fine.
The models can figure it out.
For example, early on,
image models were convolutional networks.
You tell it the key information is nearby,
and if you translate the image, it doesn't matter.
And now everyone uses transformers, including for images,
and transformers replace the convolutional mechanism
with attention, where you're now saying,
okay, one-sixth of your parameters
are dedicated to figuring out
where to move information between positions.
Sometimes it'll be a convolution,
and sometimes models do learn convolution,
but often it won't be.
And we want you,
and you can now spend the parameters to figure this out.
And I'm not very familiar with the geometric deep learning literature,
but I generally am just kind of like,
models can figure it out.
The way we figured out that this was what's going on
is kind of analogous to what we did in the modular audition case,
where we just look at the embedding matrix
and just read off the learned sine and cosine terms.
Here we said, okay,
the rotations of the 4D tetrahedron are these like 4x4 matrices.
You can flatten this to a 16-dimensional vector.
Let's probe for that linearly.
And this kind of works,
and you can probe for the different representations
and basically see what's going on.
Okay, I think that the thrust of the geometric deep learning stuff,
or any inductive prior, comes back to the bias-variance trade-off.
And the cursive dimensionality.
So no one's saying, of course, an MLP,
if you look at the function space that it can approximate,
it's exponentially larger than that of a CNN.
So it was always about sample efficiency.
So, yeah, an MLP can learn anything,
but we would never be able to train it for most problems.
Mm-hmm.
Yeah.
So, I guess I maybe want to avoid going too deeply into this
because I think the modular addition problem
and the group problem is just a very weird problem.
There's an algorithm that it's fairly natural for a model to learn
with literally a single nonlinear step of the matrix multiply.
One very cute result from the last paper
is that the model can implement two 4x4 matrix multipliers
with a single ReLU layer, which is very cute.
But, yeah, there's a fairly natural algorithm to implement.
That's a certain...
Yeah, another useful intuition is that the more data you have,
the more complex memorization gets,
while generalization is exactly as complex at each point.
And, yeah.
So there's always going to be a crossover point
where we have enough data where it is simpler to learn the circuit
that generalizes.
And I don't know.
I'm hesitant to draw too much from toy models about the real problem.
I guess one, two final points I'd want to just leave on this section.
The first is I just want to re-emphasize.
I did not do the toy model of the universality paper.
I was supervising a mentee, Bella Chucktie, who did it.
He did a fantastic job.
So thanks, Bella, for your listening.
Secondly, for the module addition case,
I had no idea this outcome was going to be there when I went in.
I just poked around, noticed the weird periodicity,
realized it was using, I should apply Fourier transforms,
and then the whole problem kind of fell together.
And to me, the real takeaway of this paper is like,
I don't give a fuck about GROCK.
It is genuinely possible to understand what is going on in a model.
You don't need to know what's going on in advance to discover this.
And there is beautiful, non-trivial structure that can be understood.
And who knows if this will happen in, like, actual full models.
But to me, this is much more compelling than if we had nothing at all.
Beautiful.
Okay.
And just before we move off the section,
Biloud had a beautiful Twitter thread, actually.
And he was talking about the potential for what he called
a periodic table of universal circuits.
And I actually think that's a really cool idea.
So that would be amazing if that would work out.
But he also brought up the lottery ticket hypothesis,
and I've interviewed Jonathan Frankel.
And the idea there is that some of this information
might actually be encoded and understandable at initialization
before you even start training.
And apparently, you folks have found weak evidence
for this in at least one group.
Ah.
All right.
So a couple of things there.
So this idea of a periodic table of circuits,
I believe, is originated in this post called Circuits Zoom In
from Chris Ola.
We probably cannot claim credit.
Good job, Chris.
It's a beautifully evocative term.
Yeah, the story of basically everything in Mac and Terp
is, yeah, there was this Chris Ola paper from, like,
two years ago that has it somewhere inside.
Anthropic recently put out this beautiful blog post
called Interpretability Dreams about their vision
for the field of mechanistic interpretability
and the kind of subtext.
So they kept just quoting bits of old papers being like,
so we already said this, but let's now, like,
summarize it better and be clear about how this
sits into our overall picture.
Anyway, so, yeah, the idea of the periodic table is
maybe there is just some finite list of ways a thing
can be implemented naturally in a massive stack of matrices
that we can enumerate by studying one or maybe several
networks, understand them, and then compile all of this
into something beautiful.
And...
which is kind of what we found in the representations case.
Though here it was nice because there were
genuinely a finite set that we could have fully enumerate.
Regarding the lottery ticket stuff,
I think this was a random observation I had on the
Modular Edition case, partially inspired by a result
from Eric Michaud at MIT, who was involved in some
other papers on Grocking.
And so what we found is that at the end of training,
there are these directions in the weights that represent
like the sine and cos terms of frequency,
14 pi over 113.
And...
if you look at the embedding at the start and project one
to these directions, it's like surprisingly circular.
It's like the model has extracted those directions.
And my wildly unsubstantiated hypothesis for why models
just learn these algorithms and circuits at all
is that there are some directions that if you
deleted everything else would like form this beautiful circuit.
This is kind of a trivial statement about linear algebra for the most part.
And this underlying hidden circuit, each bit reinforces
each other systematically because they're useful.
Well, everything else is kind of noise, so it gets kind of gradually
decayed.
Over time, this will give you the circuit in a way that looks
surprising and emergent.
And this also can partially explain why phase transitions
happen.
There was a really good post from Adam German and Bach Schleggeres
called on S-shaped curves, which argue that
if you've got something that's like the composition of multiple different
weight matrices, let's just say two of them,
the gradient on the first is proportional to how good the
second is and vice versa.
So at the start, they both grow very slowly, but then they'll
reinforce each other and eventually cascade as they're optimizing
on the problem in a way that looks kind of sudden and S-shaped.
And so my understanding is the original lottery ticket
hypothesis is kind of discreet.
It's looking on the neuron level and it's learning masks over
weights and over neurons.
And I'm kind of discussing and in some sense much more trivial
version, where I'm not assuming there's some canonical basis
of neurons.
I'm saying, well, there's some directions in space that matter
and if you delete all other directions, everything kind of
works, which I think is a much more trivial statement,
though the space of possible neurons is enormous.
Though I don't know.
One thing you want to be pretty careful of when discussing this
the mask you learn is the computation since, and no,
there's probably quite a lot of algorithms can be cleverly
expressed with a mask over a Gaussian normal matrix.
But I don't know.
Part two, how do machine learning models represent their
thoughts?
Now we're taught in machine learning 101 that neural networks
represent hypotheses which live on a geometric domain and
the objective priors learn to generalize symmetries which
exist on the underlying geometric domain.
And you're talking about them representing a space of
algorithms, which we're going to explore.
Now, one thing that I wanted to touch on is that they learn
the mapping to extensional attributes, not intentional
attributes, intentions, but with an S.
And we'll come back to what I mean by that in a second.
But I think it's quite popular for people to think of neural
networks principally as a kind of hash table.
Or locality sensitive hash table.
And the generalization part comes from the representation
mapping function, which is on this embedded Hilbert space,
which is the vector space of the attributes, which then
resolves a pointer to a static location on the underlying
geometric domain.
Now this can mimic an algorithm, especially when the
inductive prior itself is increasingly algorithmic like
a graph neural network, for example, which behaves in a
very similar way to a prototypical dynamic programming
algorithm.
There's some great work actually on algorithmic reasoning
by Petr Felichkovich, one of your colleagues now at
DeepMind.
But he showed in his algorithmic reasoning work that
transformers can't perform certain graph algorithms.
I think he gave Dykstra as an example and he said it's
because there's this aggregation function in a
transformer, which isn't in a GNN.
So I just wondered if you could kind of like compare and
contrast whether or not neural networks are performing
algorithmic generalization and the differences between
let's say GNNs and transformers.
Yeah, so I'm not very familiar with GNNs, so I'll probably
avoid commenting on GNNs versus transformers, so a fear of
embarrassing myself.
In terms of the underlying thing.
So I definitely think we have some pretty clear evidence at
this point that models are doing some genuine algorithms.
I don't know if I think my modular addition thing is a
pretty clear proof of concept of this.
Yeah, so one thing worth stressing is that I generally
think of models as having linear representations, more
than geometric representations.
So I think of an input to a model as having many
different possible features where features are kind of a
property of the input in an intentional sense, but which is
kind of a fuzzy and garbage definition.
So I prefer the existential definition of like an example
of a feature is like this bit of an image contains a curve
or this bit of an image corresponds to a car window or
this is the final token in Eiffel Tower or this corresponds
to a list variable and Python with at least four elements
and all kinds of stuff like that.
And well, I know this this scene is shaded blue because
someone put the wrong filter on the camera.
And yeah, I generally think of models as representing
features as linear directions in space.
And each input is a linear combination of these directions.
And this is kind of the classic words to vet framing, like
the king minus man equals queen minus woman thing where you
can kind of think of this as there being a gender direction
and there being a royalty direction.
And these are like the right units of analysis rather than
king, queen, man, women being the right units of analysis.
But where each of these is made up out of these underlying
linear representations.
And this is a fairly different perspective to the geometric
where are things in a manifold?
How close are they together in Euclidean space?
Because that's all kind of a global statement about how close
two things are where you're comparing all possible features
while I don't know the Eiffel Tower and the Colosseum are
close together in some conceptual space because they're both
European landmarks, but they're also very different because
France and Italy are fairly different countries in some
sense.
And maybe they're different on a bunch of other features or
one of them is two words.
The other is one word, which really matters in some ways.
And Euclidean distance and geometry is it's a gloomy
it's a global summary statistic.
And all summary statistics light you.
There's another motto of mine.
But in particular, global ones I'm very skeptical of.
And yeah, in general, this how what is the structure of a model
representations?
I think it's like a really important question.
And in particular, models are such high dimensional objects
that you really want to be careful to distinguish between the
two separate things of sorry models are such high dimensional
objects that it's basically impossible to understand GBD
three is a 200 billion dimensional vector.
You need to be breaking it down into units of analysis that
can vary independently and independently meaningful.
And the linear representation hypothesis is like a pretty
load bearing part of how I think about this stuff because it is
so because it allows you to break things down.
And it seems to be a true fact about how models do things.
Though again, we don't have that much data because we never
have enough data.
It's really sad.
And yeah.
Well, that's contrast a little bit.
So this linear representation hypothesis, this idea that the
models break down inputs into many independently varying features
and store them as directions in space, much like word to veck.
And the the go fi people, I mean, like photo and pollution,
they they they brought out this famous critique of connectionism
in 1988.
And their main argument was systematicity.
And they were talking about intention versus extension.
And it might just be worth defining what I mean by that.
So if I said the teacher of Socrates was Plato, the extension
is Plato.
The intention is everything.
It's the teacher.
It's Socrates.
You know, if I said four plus five equals nine, nine is the
extension for and plus and five is the intention.
So they were saying something very simple.
They said in a neural network, the intentional attributes get
discarded.
And that's why the network don't support what they call
compositionality.
Now, compositionality is actually quite an abstract term
because using vector algebra in these analogical reasoning tasks
that you were just talking about.
So king and queen and so on.
That's a form of compositionality.
But they would say it's a poor cousin of compositionality because
it's only using, you know, the representation is in a vector
space.
And in a vector space, you only have very basic primitive
transformations.
So you wouldn't be able to, I mean, for example, when you're
talking about Paris earlier, you wouldn't do the kind of
analogical reasoning they were talking about being able to
downstream, say, were they in Paris?
Is Paris in Europe?
Of course, it does happen in this linear representation theory.
But it happens in a very different way.
Hmm.
So I guess I'm not sure I fully followed that.
I mean, this might be a cheap gotcha, but a fact about
transformers is there's, they have this central object called
the residual stream, which I know in standard framing to be
thought of as the thing that lives in the skip connections.
But not even as like the key thing about a transform where
each layer reads its input from the residual stream and adds
its output back to the residual stream.
And the residual stream is kind of this shared bandwidth and
memory.
And this means that nothing's ever thrown away unless the
model explicitly is trying to do that, or is just applying
some gradual decay over time.
So, you know, if you've got an MLP layer that's saying I've got
four, I've got plus, I've got five, and I want to compute nine,
and plus is still there.
I don't know if this actually engage with your points and
like, I don't know if this matters, but it's true.
Yeah, what you're saying is true, but I think the point is
that those primitives are not actually representable in a
neural network.
So you're saying with this residual stream, all of the
extensions that came previously also get passed up.
So in a later layer, you can refer to an extension.
So the, basically the answer of a computation that happened
upstream, but what you can't refer to are the intentional
attributes of that computation upstream.
Why not?
Like four is an input.
So you can refer to four, because you could think of
reading the input as a computation.
Plus is another thing you read.
Five is another thing you read.
Like what is a thing that is not an output of a computation
within this framework?
I might have to get back to you on that.
Where's Keith Duggar when you need him?
What would be a good example of that?
I mean, I guess it's about symbol manipulation as well.
So these things could actually be symbolic operations which
can be composed and reused later.
And you would appreciate that a neural network is only ever
passing values.
So for example, if it did something which you could represent
with a symbolic operation, if you wanted to use that again,
I mean in an MLP, the reason why we use a CNN is because we
want to represent the same thing in different places.
And an MLP would have to learn it.
It doesn't support translational equivalence.
So it would have to learn the same thing a million times.
And it's the same thing with this symbolic compositional
generalization that if it actually had this symbolic
representation which it used once, it could use it everywhere.
But now it has to relearn it everywhere.
Right.
Like you could, if the model wants to know that Paris
is the capital of France, it can spend some parameters on that.
And for every other capital it needs to separately spend
parameters and it can't just have a general map country to
have a capital operation.
Yeah, that's exactly right.
Let's use a simple example.
So we use an MLP image classifier and I put a tennis ball in
and it's in the bottom left of the visual field.
And then I put it in the top right and nothing it's learned
from the bottom left will be used.
So it just feels like we're wasting the representational
capacity just doing the same thing again and again.
And in a transformer, the only reason it does have that
recognition, that that's a equivalence in respect of the
position of a pattern is because of the transformer
inductive prior, presumably.
Yes.
So it uses the same parameters at each position in the input
sequence.
It should be able to do bottom left and top right properly,
though it does not necessarily have things like rotation built
in.
I don't know.
I feel like machine learning is full of these people who have
all kinds of theoretical arguments.
And then they're like, this should be efficient.
This should not work.
And then GPT-4 lobs at them.
And I don't know.
No theory is interesting and isolation unless it models
reality well.
And I don't know.
I haven't really engaged with this theory in the same way I
haven't engaged with most deep learning theory, because it
just doesn't seem to meet my bar of does this make real
predictions about models?
The maximal update parameterization paper from Greg
Yang was actually a recent contradiction to this.
Right.
Of really interesting theory that makes real predictions
about models that bear out and get you zero shot hyper parameter
to transfer.
But like most things just don't do that.
Very interesting.
Okay.
Okay.
Well, I think now is a beautiful opportunity to move over to
Othello.
Now, there was a recent paper called do large language models
learn world models, or are they just surface statistics by
Kenneth Lee?
And he said that the recent increase in model and data size
has brought about qualitatively new behaviors such as writing
code or solving logic puzzles.
Now, he asked the question, yeah, how do these models achieve
this kind of performance?
Do they merely memorize training data?
Well, are they picking up the rules of English grammar and
grammar and the syntax of the sea language?
For example, are they building something akin to an internal
world model, an understandable model of the process producing
the sequences?
And he said that some researchers argue that this is fundamentally
impossible for models trained with guess the next word to learn
the language, meanings of language, and their performances is
merely surface statistics, you know, which is to say a long list
of correlations that do not reflect a causal model of the
process generating the sequence.
Now, you said, Neil, that a major source of excitement about
the original Othello paper was that it showed that predicting
the next word spontaneously learned the underlying structure
generating its data.
And you said that the obvious inference is that a large
language model trained to predict the next token may
spontaneously model the world.
What do you think?
Uh, yes.
So I should clarify that paragraph was me modeling why other
people are excited about paper.
Okay.
But whatever, I can roll with this question.
So and maybe bring in your less wrong piece as well.
Yeah.
Yes.
So the, yeah, I thought careless paper was super interesting.
The exact setup was they train.
So Othello is this chess and go like board game.
They took a data set of random legal moves in Othello.
They trained a model to predict the next move given a bunch
of these transcripts.
And then they probed the model and found that it had learned a
model of the board state, despite only ever being told to
predict the next move.
And so the way I would define world model is that there's
some latent variables that generate the training data.
Um, in this case, what the state of the board is, um, these
change over time, like over the sequence, but at least for a
transform, which has a sequence and the model kind of has an
internal representation of this at each point.
And they showed that you can probe for this.
And they showed that you can causally intervene on this and the
model will make legal moves in the new board, even if the
board status impossible to reach.
Point of order.
Can you explain what you mean by probe?
Yes.
So probing is this like old family of interoperability
techniques.
The idea is you think a model has represented something like you
give it a picture and you tell it as a class by the image and
you want to see if it's figured out that the picture is of a
red thing versus a blue thing, even though this isn't an
explicit part of the output.
You take some neuron or layer or just any internal vector of
the model and you train some classifier to map that to like
red or blue and you do something like a logistic regression
to see if you can extract whether it's red or blue from them.
And, uh, there's also interesting enough about probing, but
I should probably finish explaining the Othello paper first
before I get into that tangent.
So, yeah, the like reason people are really excited about this
paper was recently an oral eichle and generally got a lot of
hype was that it was just you train something predict the next
token and it forms this rich emergent model of the world.
And forming a model of the world is actually incredibly
expensive.
They like each cell of the 64 cell Othello board has three
possible states, three to the 64, it's quite a lot of information
to represent, but the model did it.
And lots of people were like, oh, clearly language models have
walls.
My personal interpretation of all this is that language models
predict the next token.
They learn effective algorithms for doing this within the
constraints of what is natural to represent within transformer
layers.
And what this means is that if predicting the next token is
made easier by having a model of the world of like, I don't
know who the speaker is, this is a thing that will happen.
And in some work led by Wes Gurney that we're going to talk
about later, we found neurons that detected things like this
Texas in French, this Texas Python code.
And in some sense, this is like a particularly trivial
model.
And so, yeah, that's an interesting thing.
In my opinion, it was kind of a priori obvious that language
models would learn this if they could and needed to, and it was
more efficient.
And at the point forward, though, learning that something is
French seems categorically different.
Because when I read Kenneth's original piece, he showed what
looked like a topological representation of the world.
So how different state spaces were related to each other in a
kind of network structure.
Hmm.
So.
I wonder if you can remember how we produced that diagram.
Yeah, so I'm starting to remember the details.
I think it was something of the form.
Look at how different cells are represented in the model and look
at how close together the representations of different
cells are, and the model has kind of got internal representations
that are close together.
I don't think this is fundamentally different from the
king, queen, man, women thing.
It's just that it's like learn some structural representations
that's obviously kind of reasonable.
Yeah.
I wouldn't read too much into that.
Models learn structural representations, I think, as old
news at this point.
But maybe another interesting angle is that one of the reasons
why people like Gary Marcus, they say GPT is parasitic on the
data.
They say because they are empirical models, most of the meaning,
most of the information is not in the data if we have to reason
over explicit world models.
So he thinks the reason a GPS is so good is because we've
imputed this abstract world model.
And similarly, when we play chess, we have an abstract world
model.
And he would argue that the information about that abstract
world model doesn't exist in any data.
So how do you go from the data to the model?
And the Othello games seem to show that you could go from the
data to the model.
Yeah.
I think that viewpoint is just obviously wrong.
Like, you're trying to do a data prediction problem.
A valid solution to that is to model the underlying world and
use this to predict what comes next.
There's clearly enough information in an information
theoretic sense to do this.
And the question is, is a model capable of doing that or not?
And I don't know.
I'm just like, you can't write poetry with statistical
correlations.
You need to be learning something.
Maybe that's not a good example.
I don't believe you can write like...
Yes, you can.
I don't believe you can produce, like, good answers to, like,
difficult code forces problems.
It's like, do good software engineering.
There's purely a bundle of statistical correlations.
Maybe I have too much respect for software engineers.
I don't know.
So where does it come from then?
That flash of inspiration or that higher level...
I guess the first question is, is there a jump?
Is it actually grounded in the data it's trained on?
Or is there some high-level reasoning?
Where does that materialize from?
So the way I think about it, there is just a space of
possible algorithms that can be implemented in a
lot of ways.
And some of these look like a world model.
And some of these look like a bunch of statistical correlations.
And models are trading off lots of different resources.
Like, how many dimensions do this consume?
How much weight norm?
How many parameters?
How hard is this to get to and how weird and intricate?
And models will choose the thing that gets the best loss
that is most efficient on these dimensions,
assuming they can reach it within the lost landscape.
Well, I use choose in a very anthropomorphic sense.
Like, Adam chooses good solutions.
And I don't know, if you have a sufficiently hard task
and forming a world model is like the right solution to it,
models can do it.
And I think people try to put all of these fancy philosophizing
on it in a way that I just think is false.
And I think the Othello paper is like a really beautiful,
elegant setup that proves this.
All right, can I move on to the plot twist?
Does it prove it though?
It's a very small contrived.
It's a big jump to assume that that works on a large language model.
So this is kind of the argument I'm making.
I think there's the empirical question of do language models
do this and the theoretical question of could they do this?
And I'm saying I think the theoretical question is nonsense.
And I think the Othello paper very conclusively proves
the theoretical question is nonsense.
They're just like, yeah, when given a bunch of data,
you can infer the underlying world model behind it in theory.
I would pitch back on that a tiny bit
because it's very similar to AlphaGo,
which proved that in a closed game,
which is systematic and representable,
with a finite, obviously exponentially large,
but a finite number of board states,
you can build an agent which performs really, really well.
That seems to me completely different to something like language
or acting in the real world that might not be systematic
in the same way.
We can debate whether or not it's,
I think it's an infinite number of possible trajectories,
just like language, an infinite number of possible sentences.
Man, there's 50,000 to the power of a thousand
possible input sequences.
Sure is a finite number.
You mean in Othello or?
No, in GPT2.
In GPT2, okay.
Bounded context length, bounded vocab size, generally.
Bastard.
You're not going to write more than one quintillion characters
probably.
Yeah.
Well, I guess it is still a big jump, though, isn't it?
From, yes, empirically, it shows that in Othello, it works.
Maybe we could debate whether or not it does or not,
because there's always this question coming back to what we were saying before,
whether it's learning something which is universal
or something which is still brittle.
So the way that we've evaluated it might lead us to conclude that it's universal,
whereas, actually, it's brittle in ways that we don't understand.
So that's a very real possibility.
Yeah.
I mean, everything's brittle in ways you don't understand.
It's pretty rare that a model will do everything perfectly
in a way that there are no adversarial examples.
And this is one of the more interesting things that's come out
of the adversarial examples literature to me.
It's just like, oh, wow, there's so much stuff here.
There's such a high-dimensional input space.
There's all kinds of weird things the model wasn't prepared for.
And I don't know.
My interpretation of the Othello thing is the strong theoretical arguments are wrong.
I separately believe that there are world models
that could be implemented in a language model's ways.
But I also disagree with the strong inference of the paper
that this does happen in language models, or that we conclude it does,
because world models are often really expensive.
Like, in the Othello model, it's consuming 128 dimensions
of its 512-dimensional residual stream for this world model.
And the problem is set up so that the world model is insanely useful
because weather removers' legal is purely determined by the board state,
so it's worth the model's while to do this.
But this is rarely the case in language.
For example, there was all this buzz about Bing chat playing chess
and making legal-ish moves.
And I don't know, man.
If you want to model a chessboard,
you just look at the last piece that moves into a cell.
That's the piece in that cell.
You don't need an explicit representation.
You can just use attention heads to do it.
And there's all kinds of weird hacks,
and, like, models will generally use the best hack.
But probably it is worth the model's while
to have some kind of an internal representation.
Like, I'd bet that if you took a powerful code-playing model
and probed it to understand the state of the key variables,
it would probably have some representation.
But it's moving on to the work I did building on the Othello paper.
So one of the things that was really striking to me about the Othello work
is, simultaneously, its results were strong enough
that something here was clearly real.
But they also used techniques that felt more powerful than were needed.
Like, rather, they found that linear probes did not work.
There weren't just directions in space corresponding to board states,
but the non-linear probes, one hidden layer MLPs, did.
And the key thing to be careful of when probing
is, is your probe doing the computation,
or does the model genuinely have this represented?
And even with linear probes, this can be misleading.
Like, if you're looking at how a model represents coloured shapes,
and you find a red triangle direction,
it could be that there's a red, green, or blue direction,
and a triangle, square, or shape direction,
and you're taking the red plus triangle,
or it could be the case that each of the nine shapes
have their own direction, you found the red triangle one.
But non-linear probing is particularly sketchy.
Like, in the extreme case, if you train GPD3 on the inputs to something,
GPD3 can do a lot of stuff.
If you train GPD3 on the activation side network,
it can probably recover arbitrary functions of the input,
assuming the information of the input hasn't been lost,
which it shouldn't have, because there's a residual stream.
And what I said is not quite true, but not important.
And so I was, and their intervention technique
was both got, like, various impressive results,
but also involved doing a bunch of complex gradient-scent
against their probe, and this all just seemed more powerful
than was necessary.
And so I did the...
I challenged myself to do a weekend hackathon
trying to figure out what was going on,
and I poked around at some internal circuitry
and tried to answer some very narrow questions about the model,
and I found this one neuron that seemed to be looking
for, like, three cell diagonal lines,
where one was blank,
the other was white, the next was black.
But then sometimes it activated on blank, black, white.
And it turned out that the general pattern
was that it was blank, current players...
sorry, blank opponents color and current players color.
And this is a useful motif in Othello,
because it makes them move legal.
And when I saw this, I made the bold hypothesis
maybe the model actually represents things
in terms of whether a cell has the current player's color
or the current opponent's color,
which in hindsight is a lot more natural,
because the model plays both black and white,
and it's kind of symmetric from the perspective of the current player.
And I trained a linear probe on this,
and it just worked fabulously,
and got near-perfect accuracy.
And I tried linear representations on it,
and I tried linear interventions, and it just worked.
And I even feel really excited about this project
for a bunch of reasons.
First, while I did it on the weekend,
I'm still very proud of this.
Secondly, I think that it has vindicated
some of my general suspicion of non-linear probing.
Like, if you really understand a thing,
you should be able to get a linear probe to work.
And kind of more deeply, as we discussed,
there's this words-to-vex style linear representation
of a hypothesis about models that features corresponded directions.
The Athero work seemed like pretty strong evidence against.
They had chordal interventions showing that the board state was there,
but actually non-linear probes did not work.
It seemed like they found some non-linear representation.
And my and Chris Ola's hypothesis seeing this
was that there was a linear representation hiding beneath.
Martin Boddenberg, one of the authors of the paper,
had the hypothesis that it was, like,
an actual non-linear representation,
and this was evidence against the hypothesis.
And this kind of formed natural experiment
where the hypothesis could have been falsified,
but my work showed there was a real non-linear representation,
and thus that it had predictive power.
And so many of our frameworks for mech and turp
are just these loose things based on a bunch of data,
but not fully rigorous or fully conclusively shown.
And so natural experiments like this
feel like some of the best data we have.
On this linear representation, though,
I don't know if you've heard of the spline theory
of neural networks by Randall Ballastriero.
And without going into too much detail,
it's quite a discrete view of MLPs in particular
that the relus essentially get activated
in an input-sensitive way to carve out these polyhedra
in the ambient space, and essentially an input
will be mapped into one of these cells in the ambient space,
and then there's a kind of discreteness to it,
because if you just perturb the input
and you move outside of one of these polyhedra,
then the model will, if it's a classifier,
classify something different.
But I guess I want to understand,
with this representation theory, if features are directions,
does that imply there's a kind of continuity
because the network will learn to spread out
those representations in the best possible way,
but it won't necessarily be a way which is semantically useful,
like in Word2Vec,
stop and go are very close to each other,
and they shouldn't be.
And at what point does stop become go?
So do you see there being boundaries in these directions?
So I think this is, again, my point that I think
of linear representations as being
importantly different from geometric representations,
like stop should be close to go,
because in many contexts,
they are like a kind of changing of state term,
and it's used in similar contexts
and has similar grammatical meaning,
but then on this single semantic thing,
they're quite different.
And the natural way to represent this
is have them be close together in Euclidean space,
but have some crucial like negation dimension
where they're different.
And the contact and like ultimately neural networks
are not geometric objects,
they are made of linear algebra.
Every neuron's input is just project
the residual stream onto some vector,
and this involves just selecting some set of directions
and taking a linear combination
of the feature corresponding to each of those.
And this is just the natural way for a model
to represent things in my opinion.
Okay, well, I think this will in a second lead us
on very nicely to superposition,
which is that we don't actually think
of there being one direction necessarily.
Just to close this little piece,
now you said in your less wrong article
that orthello GPT is likely over parameterized
for good performance on this particular task
while language models are under parameterized.
And of course, we have the ground truth to this task,
which makes it very, very easy.
So much easier to interpret.
100%, but you did conclude saying that
this is further evidence that neural networks
are genuinely understandable and interpretable,
and probing on the face of it seems like
a very exciting approach to understand
what the models really represent,
caveat, mentor, conceptual issues.
So let's move on to this superposition,
also known as poly semanticity,
which is an absolutely beautiful,
while you're shaking your head a little bit,
maybe you start with that.
Yeah, so there's...
All right, so what's the narrative here?
So fundamentally,
we are trying to engage with models
as these high dimensional objects
in kind of this conceptual way.
So we need to be able to decompose them
because of the curse of dimensionality.
And we think models correspond to features
and the features correspond to directions.
And the hope in the early field
was that features would correspond to neurons.
And even if you believe features correspond
to orthogonal directions,
the same thing they correspond to neurons
is like a pretty strong one,
because there's no reason to align with the neuron basis.
The reason this isn't a crazy belief
is that models are incentivized to represent features
in ways that can vary independently from each other.
And because relus and jelus act element-wise,
if there's a feature per neuron,
they can vary independently.
Well, if there's multiple features in the same neuron,
I don't know, if there's a relu,
the second feature could change
so the relu goes from on to off
in a way that changes how the other feature is expressed
in the dance room network.
And this is like a beautiful theoretical argument.
Sadly, it's bullshit because of this phenomena of police mantisity.
Police mantisity is a behavioral observation of networks.
But when we look at neurons
and look at things that activate them,
they're often activated by seemingly unrelated things,
like the urs in the word strangers
and capital letters are proper nouns
and news articles about football.
It's a particularly fun neural I found one time in a language model.
And police mantisity is a purely behavioral thing.
We're just saying this neuron activates
for a bunch of seemingly unrelated stuff.
And it's possible that actually we're missing
some galaxy brain abstraction where all of this is related,
but my guess is that this is just,
the model is not aligning features with neurons.
And one explanation of this
is you've just got this thing called a distributed representation
where a feature is made of the linear combination of different neurons.
But it is kind of rotated from the neuron basis.
And this argument that neurons can vary independently
is a reason to think you wouldn't see this.
Where this hypothesis is just that there's still
n things when there's n neurons, but they're rotated.
But then there's this stronger hypothesis
that tries to explain this called the superposition hypothesis.
And here the idea is,
so if a model wants to be able to recover a feature perfectly,
it must be orthogonal from all other features.
But if it wants to mostly recover it,
it suffices to have almost orthogonal vectors.
And you can fit in many, many more almost orthogonal vectors
into a space than orthogonal vectors.
There's theorem saying that there are exponentially many
in the number of dimensions.
If you have 100 dimensional vectors,
how many orthogonal directions are there?
100.
100?
Yep.
This is just the statement that you pick a vector.
Sorry, there's 100 vectors that are all orthogonal of each other.
Basic proof, you pick a vector,
everything's orthogonal to that, that's a 9 to 9 dimensional space.
You pick another vector, take everything orthogonal to that,
that's a 98 dimensional space,
and keep going until you get to nothing.
Like if you picture a 2D space, you pick any direction,
the only things orthogonal to that are a line,
and so there's exactly two orthogonal things you can fit in.
And there's like, you can rotate this and you can get
many different sets of orthogonal things.
Okay, I'm trying to articulate why this doesn't make sense to me.
So maybe we should start with the curse of dimensionality,
which is that the volume of the space increases exponentially
with the number of dimensions.
So we'll start with that.
And the reason I'm thinking, maybe I'm wrong,
but if you've got 100 dimensional vector,
every combination of flipping one of the dimensions
would produce a vector which is orthogonal to all of the other ones, would it not?
No.
So let's imagine you've got a vector of all ones.
If you pick the first element and you negate it,
so it's like minus one, then 99 ones.
These are not orthogonal, the dot product is 98.
Okay, okay, well that makes sense.
So there's a linear number of orthogonal directions,
and in which case we actually need to have these
approximately orthogonal directions,
because that actually does bias an exponential number.
Yeah, and so the superpositional hypothesis is that the model represents
more features than it has neurons, or that it has dimensions,
and it somehow compresses them in as things that are almost orthogonal
when it reads them out with a projection to get some interference,
but it needs to balance the value of representing more features
against the cost of interference.
And Anthropic has this fantastic paper called toy models of superposition,
which sadly was written off right left, so I can't claim any credit,
and they basically build a toy model that exhibits superposition.
The exact structure is you have n independent features,
each of which is zero most of the time, it's not very prevalent,
and there's a linear map from that to a small dimensional space,
a linear map back up, and a non-linearity on the output,
no non-linearity on the bottleneck in the middle,
and you train it to be an autoencoder.
Can it recover the features in the input?
And because there's many more features than that are in the bottleneck,
this tests whether the model can actually do this.
And they find that it sometimes does, sometimes doesn't,
and then do a lot of really in-depth investigation of how this varies.
And yeah, returning to like, is superposition the same thing as police mantisity?
I would say no, police mantisity is a behavioral thing.
Distributed representations are also a behavioral thing,
that it's like not aligned with the basis,
and superposition is a mechanistic hypothesis for why both of these will happen,
because if you have more features than neurons,
obviously you're going to have multiple features per neuron,
and probably you're going to have features that are not aligned with neurons.
Okay, okay, very interesting.
So why do you think that superposition is one of the biggest problems in mech and turp?
Yeah, so it's this fundamental thing that we need to be able to decompose a model
into individual units, and ideally these would be neurons,
but they are not neurons, so we need to figure out what we're doing.
And superposition, so in a world where we just have like n meaningful directions,
but they weren't aligned with the standard basis, that'd be kind of doable.
And indeed models often have like linear bottlenecks,
like the residual stream, or the keys, queries, and values of an attention hit,
that don't have element-wise linearities, and so have no intrinsically meaningful basis.
The jargon here is privileged basis,
but superposition means that you can't even say,
this feature should be a fucking alter everything else,
there's going to be a bunch of interference.
There's not even a kind of mathematically,
there's not even like a unique set of more than n directions,
so describe some kind of vectors in n dimensional space.
And I think that understanding how to extract features from superposition,
given that superposition seems like a core part of how models do things,
though we really do not have as much data here as I would like us to,
understanding how to extract the right meaningful units seems really important.
Okay, and I think we should clarify the difference between computational
and representational superposition.
Yeah, so there's kind of, so transformers are interesting,
because they often have high dimensional activations
that get linearly mapped to low dimensional things.
So like in say GPT-2, in say GPT-2 small,
the residual stream has 768 dimensions,
while each MLP layer has 3000 neurons.
And even if we think each neuron just produces a single feature,
they need to get compressed down to the 768 dimensional residual stream.
And we, or there's like 50,000 input tokens
that get compressed to 768 dimensions.
And this is called representational superposition.
The model is representing, the model's already computed the features,
but it's compressing them to some bottleneck space.
And this is the main thing studied in the toy models of superposition paper.
And what we found, sorry,
there's a separate thing of computational superposition,
which is when the model is doing, it's computing new features.
This needs non-linearities, like attention head softmaxes or MLP jellies.
And the non-linearities can compute new features as directions from the old ones,
like if this, for example,
if the top of an image is a car window and the bottom is a car wheel,
then it's a car.
Or if the current token is Johnson and the previous token was Boris,
this is Boris Johnson.
And this is all, how to phrase this?
Yeah, this is computational superposition.
If the model wants to compute more features than it has neurons.
And this is much harder to reason about because linear algebra is nice
and fairly well understood.
Non-linearities, spoilers in the name, are not linear,
and that's way more of a pain.
And I think that we generally have a much less good handle on computational superposition,
but also that this is way more of where the interestingness lies by my lights.
And this is very briefly studied in the toy models of superposition paper,
but I would love to see more work looking at this in practice
and also looking at this in toy models.
So zooming out a tiny bit, there's this paper from Anthropic.
And the overall question to me is, does it actually exist?
Now, presumably you're satisfied with the evidence that it does exist.
And then there's the question of how do neural networks actually do it?
And then there's the question of how does the neural network think,
anthropomorphic language, I apologize,
about the trade-off of more superposition, more features,
but more interference versus less interference and more superposition?
Yeah, so typing into the final question about interference,
a useful conceptual distinction is that there's two different kinds of interference.
So if you've got two features that share a dimension or share a neuron.
Oh yeah, final note on representational superposition,
because I don't think it should even be referred to in terms of neurons,
because the individual-based elements don't have intrinsic meaning.
Modular weird quirks like Adam.
And it annoys me when people refer to the residual stream or key vectors as having neurons.
There's no element-wise linearity, it's not privileged.
Anyway, yeah, two types of interference.
When A and B share a dimension, you can, yeah,
let's say this dimension has both dice and poetry.
You first off need to tell where if dice is there but poetry is not,
you need to tell that dice is there and that poetry is not there.
And if both what I call alternating interference,
and then there's simultaneous interference where dice and poetry are both there.
And you need to tell that both are there, but not that they're both there with like double strength.
And as a general rule, models are good at dealing with things of the form.
Notice when something is extreme along this dimension,
but not notice when it is extreme along a dimension versus when it's not extreme.
And alternating interference looks like that.
Like if dice is straight up, poetry is at 45 degrees,
both have less interference when the other one is active
than when the main one is active along their direction.
Okay, so you're saying interference from A and not B is far easier than A and B?
Yes, exactly.
And like a very rough heuristic as models will just not do simultaneous interference,
but will do alternating interference.
And they observed this in the toy models paper because they varied how often a feature was non-zero.
What I think of as the prevalence of the feature that they called it spastic.
And what they found is that when the feature was less prevalent,
it was much more likely to be in superposition.
And the way to think about this is if you have two independent features that both exist with probability P,
the rate of simultaneous interference is P squared.
The rate of alternating is P.
And so and the worth of having the feature is also proportional to P because it occurs P of the time.
So the railroad is the less of a big deal simultaneous interferences.
And eventually the model uses superposition.
There's also there was also an interesting bit looking at correlated features.
So correlated features, even if they're not very prevalent, they have pretty high simultaneous interference.
And models tend to put correlated features in to be orthogonal,
but anti-correlated features, it's very happy for them to share a direction.
One way you could think about this is if you've got, say, 25 features about romance novels and 25 features about Python code,
you could have 25 directions that each contain a pair of features and then a single disambiguating neuron
that is onto Python code off of romance novels that use to disambiguate the two.
And yeah, may this be a good time to talk about the finding neurons in a haystack paper?
Unless you've got more stuff on this.
We'll get to that in just two shakes of a lamb's tail.
But just before when I was reading through the paper, I had the mindset of sparsity.
And you told me, Tim, don't say sparsity. It's prevalence.
It means so many things.
It's very overloaded.
So, you know, so just quickly touch on the relationship between what is prevalence,
the relationship between prevalence and superposition.
And just before, well, actually, I've got a couple more questions,
but would you also just mind playing devil's advocate and criticising the anthropic paper if you can?
Sure.
So I should be very clear.
This is one of my top three all-time favourite and territoriality papers.
It's a fantastic paper.
That said.
A bad word said about it.
Oh, I have so much.
I have bad words to say about every paper,
especially the ones that I like because I've engaged with them in the most detail.
So things which I think were misleading about this paper.
The first is I think the representational versus computational superposition distinction is very important.
I think computational is a fair bit more interesting.
And while I think the authors knew the difference,
I think a casual reader often came away not realising the difference,
in particular that most of their results were about the residual stream,
not about actual neurons and MLP layers.
The second is a question of activation range.
So they study features that vary uniformly between zero and one.
And in practice, I think most features are binary.
This is a car wheel or this is not a car wheel.
This is Boris Johnson or this is not Boris Johnson.
And interference is much worse when they can vary continuously.
Because if A and B, if A is up, B is at 45 degrees,
you can't distinguish B at strength one from A at strength 0.7-ish.
And this is just kind of messy.
But the binary is just much easier.
And I think this is a source of confusion.
I also think the two kinds of interference point was a bit understated.
But more broadly, it's just a phenomenal paper.
Oh, my other biggest beaver, they just didn't look in real models.
And this wasn't the point of the paper.
But we're doing so much theory crafting and filming conceptual frameworks,
and we haven't really checked very hard
whether this is why models actually have police mantisity.
Wes Gurney, he's working out of MIT, and you've done a lot of work with him.
So you and Wes, but Wes was the first author,
wrote a paper called Finding Neurons in a Haystack,
case studies with sparse probing,
where you empirically studied superposition and language models
and actually found that you get lots of superpositions in early layers
for features like the security and social security.
And fewer in middle layers for complex features like this text
is French.
And also you can bring in the importance of range activation as well.
But can you frame up that paper?
Yeah.
So first off, this paper was led by Wes Gurney, one of my mentees,
did a fantastic job.
He deserves like 9% of the credit.
Great job, Wes.
I believe he listens to this podcast, so hi.
And yeah, so the kind of high level pitch behind the paper
was what we think superposition is happening.
But like, nobody's really checked very hard.
And there's like some results in the literature
I've since come across in non-transformer models
that demonstrate some amount of distributed representations.
But what would it look like to check?
And would it look like to do this in like a reasonably scalable
and quantitative way?
And the kind of sparse probing in the title
is this technique Wes introduces for,
if we think a feature is represented in MLP layer,
we can train a linear classifier to extract it,
a linear probe from that layer.
But if we constrain the probe to use at most K neurons,
very K and look at probe performance,
this lets us distinguish between features that are represented
with like a single neuron and features that are densely spread
across all neurons with a lot of methodological neural
answers about balanced data sets and avoiding overfitting
and fun stuff like that.
And most of the interesting bits of the paper,
in my opinion, are the various case studies we do where,
so probing fundamentally is like a kind of sketchy methodology
because probing is correlational.
Probing doesn't tell you whether a model uses something
and it's so easy to trick yourself about
whether you have the right representations.
So we use it as a starting point and then dig more deeply
into a few more interesting things.
One particularly QK study is we looked into factual knowledge neurons,
found something that seemed to represent this athlete plays hockey,
but then actually turned out to be a Canada neuron,
which continues to bring me joy.
That activates with things like maple syrup and Canada.
Got to love models learning national stereotypes, right?
Oh, yes.
Anyway, so there were two particularly exciting case studies.
The first was looking in early layers at compound word detectors.
So if you look at, say, the brain and its visual field,
we have all these sensory neurons.
We get raw input of light from the environment
and it gets converted into stuff our brain can actually manipulate.
Image models have Gabor filters that convert the pixels
into something a bit more useful.
What's the equivalent of language models?
And it seems to be these things that we call detokenization neurons
and circuitry, where often words are split into multiple tokens
or you get compound word phrases like social security
or Theresa May or Barack Obama and whatever.
And it's often useful for a model to realize
this is the second thing in a multi-token phrase,
especially if it's like you need both things to know what's going on,
like Michael Jordan.
Michael has lots of Jordans.
It's really important to tell both of them that they're there.
And this is a clearly nonlinear thing because it's like a Boolean and.
And so we did a lot of probing for different compound words.
And we found that they were definitely not represented well by single neurons.
We could find some neurons that were okay at detecting them,
but there was a lot of interference and a lot of like false positives from other stuff.
And when we dug into a bunch of these neurons,
we found that they were incredibly polysemantic.
They activated for many different compound words.
And we showed that it was using superposition
by observing that if you took say five social security detecting neurons
and add together their activations,
they go from okay detectors to a really good detector together.
Because even though each is representing like hundreds of compound words,
they're representing different compound words,
which lets you encode these.
And this, what we've shown here is that it's like distributed,
that it's a linear combination of neurons.
We still haven't shown it perfectly to my dissatisfaction.
I think you really need to do things like ablate these linear combinations
and see if this systematically damages the model's ability to think about social security, etc.
But I'm pretty convinced at this point.
And there's like a few properties of compound words
that both make it easy to represent in superposition,
that make me pretty okay making the jump that there's actual superposition.
The first is just that there's tons of compound words.
Each one is pretty rare, but each one is like non-trivial or useful.
And clearly there are more compound words
and there are like thousands of neurons in the MLP layer of this model.
The model cares about representing and can represent,
that we do not actually check.
Because I could not convince Wes to accumulate a list of 2,000 compound words
and pray for all of them.
But I believe in my heart this is true.
Could I have a point of order though?
Because I've been reading quite a lot of stuff from linguists like Stephen Piantadosi.
A lot of linguists are, some of them hate language models
and some of them are well on board with it.
Like Raphael Millier for example is a great example.
I hate language models too, don't worry.
Well, but the question is,
because you're talking about compound words and stuff like that
and you're still using the language of syntax
and these language models, there's this distributional hypothesis.
You know the meaning of a word by the company it keeps.
But linguists and cognitive scientists kind of ditch that.
I don't think they ever believed in the distributional hypothesis.
They think about grounding.
They think about grounding to things in the world
and also inferential references as well
which you can think of that as grounding to a model of the mind.
And this brings us back to the Othello paper
which is that they're not just learning simple kind of compound relationships
between the world, between the words.
They're learning a world model
and they're doing something much more potentially
than just predicting the next word.
And Piantadosi argued that
most of the representational capacity in language models
are learning these semantics.
They're learning relationships between things in the world model
and the particular occurrence of the token.
And this superposition idea is very interesting
because it actually imbues the representational capacity
in a language model to learn those mappings.
Okay, so a couple of comments on that.
The first is a generally useful way of thinking about models to me.
Is as a the early layers devoted to sensory neurons
converting the raw input into more useful concepts and representations.
The actual processing throughout like all of the middle layers
that actually does all the reasoning.
And then motor neurons at the end
that convert the reasoning to actual output tokens
for like the format that the optimizer wants.
And it feels like you're mostly talking about the like reasoning internally
and the specific case study I'm referring to is on the sensory neurons.
Well, like I'm not saying it just detects compound words
but obviously that's the first thing it does.
I don't know, it's so interesting.
I don't mean to push back but in neuroscience
the field was held back for decades by this idea
of this kind of left to right to processing
this hierarchical processing where you have these very, very simple concepts
that become increasingly abstract with more processing.
And then I think the field has moved away from that.
It's far more messy and chaotic than that.
Now with a neural network, it actually is hierarchical
because the network is basically a DAG.
So I suppose it is safe to make this assumption
but could I just kind of question you on that?
Is it safe to make that assumption?
Is there increasing complexity in representation
as you go from left to right?
Oh, let's see.
So yeah, I definitely, yeah.
So clarification one, the network has this input sequence
which I think was going from left to right
and then there's a bunch of layers which I think it was going from like the bottom to the top.
Yes.
And you're referring to the bottom to top axis, right?
Yeah, I'm sorry, I was using an MLP mindset when I asked that question.
So as you say, in a transformer it's an autoregressive model
and you have stacked attention layers with little MLPs on the end.
So I guess the way I was actually meaning the question is,
so complexity increases monotonically as you go up the stack of attention layers.
Is that a fair assumption?
Yep.
Again, no one's really shown this properly.
But I'm like, surely this is true.
And there's been some work doing things like looking at neurons,
looking at the text that activates them, looking for patterns
and trying to understand what these represent.
And it's generally looked like early ones are more about detokenization and syntax.
Later ones are doing stuff that's interesting.
Final ones are doing this like motor neuron behavior.
But I also want to be very clear that networks are cursed.
Networks do not fit into nice abstraction.
I'm not saying the early layers are literally only doing detokenization.
Yeah.
But I believe we have shown it's part of what they're doing.
And I speculate it is a large part of what they're doing.
I'd be very surprised if it's all of what they're doing.
Because I heard you on another podcast and you were just talking about the,
I mean, I think the curse is the right way to describe it,
which is that even when you make modifications,
when you manipulate what's happening,
the behavior will change in a very reflexive way.
So you kind of, you delete one thing and then another neuron
will take on the responsibility of the thing you just deleted.
And so it's a little bit like manipulating financial markets.
You've got almost like this weird collective diffuse intelligence
where you make one modification and the whole thing changes in a very complex way.
And similarly, I guess that's why I was intuitively questioning the assumption
that you have a residual stream.
So surely even at the very top of that attention stack,
there must be primitive and complex operations going on in some weird mix.
Since probably true.
Generally, yeah, there's going to be some stuff you can just do with literally the embeddings.
Some stuff that you need to wait a bit more before you can do anything useful with.
Just like, no, if you got a sentence about Michael Jordan,
I don't think you can use Michael Jordan in isolation.
So you need to de-tokenize to Michael Jordan.
But also, I don't know, if you've got Barack Obama,
Obama and Barack both on their own pretty clearly imply it's going to be about Obama.
And probably the model can start doing some processing in the early like layer zero.
Does it want to?
Somewhat unclear.
It's going to depend a lot on the model's constraints and other circuitry
and how much it's worth spending the premises then versus later.
There's also some various things where, I don't know,
model memory kind of decays over time because the residual stream's norm gets bigger.
So early layer outputs become a smaller fraction of the overall thing.
And layer norm sets the norm to be unit.
So things kind of decay.
And so if you compute a feature in the early in like layer zero,
it can be harder to notice by like layer three than if it was computed in layer two.
But these are all just kind of like mild nudges.
And ultimately neural networks do what neural networks want, man.
I know, I know.
I just want to close the loop on something I said a little while ago about, you know,
potentially large models use most of their representational capacity for, you know,
learning these semantic relationships.
And empirically, we found that, you know, there's some question recently actually about,
do we actually need to have really, really large models?
And for pure knowledge representation, the argument seems to be yes,
but we can disentangle knowing from reasoning.
And there's also this mimicry thing.
So it's quite interesting that all of the, you know, like Facebook released their model
and very, very quickly people fine-tuned it using the law, you know,
the low-rank approximation fine-tuning method.
And on all of the benchmarks, the model, I mean even open assistant,
there's another great example, Yannick was sitting in your seat just a few weeks ago
and we're saying that on many of the benchmarks, the model's working really well,
but it's kind of not.
It's kind of mimicry, like the big, large models that, you know,
Meta and Google and DeepMind and all these people, they spend millions training these models
and they have base knowledge about the world,
which is not going to be, you know, replicated by fine-tuning, you know,
like an open source model anytime soon.
The knowledge is based.
The knowledge is based.
Yes, yes, yes, exactly.
Well, okay, so that's very interesting.
Let's just quickly talk about the OpenAI microscope,
because this is, the OpenAI microscope is this beautiful app that OpenAI released in 2020.
And you can go on there and you can click on any of the neurons
in popular vision architectures at the time.
So, you know, I think most of them are sort of like ImageNet, you know,
things like AlexNet and God knows what else.
And they solve this optimization problem where they generate an image
using stochastic gradient descent that maximally activates a particular neuron,
or I think even a layer, using something similar to DeepDream.
And you can click on these neurons,
and sometimes they are what we will call poly sort of monosemantic,
which means it's just Canada.
A lot of the time there's a couple of concepts in there that it's weirdly intelligible.
You know, you might see, you know, like a playing card or an ace
and a couple of like tangentially related concepts.
And it always struck me as strange,
because I imagine there's a long tail of semantic relationships.
And I found it bizarre that there'd only be one or two in this visualization.
And I had this intuition that the optimization algorithm is in some sense
mode seeking rather than distribution matching,
which is to say that it finds the two most or two or three or four
most kind of salient semantic mappings,
and they dominate what is visualized,
and you're almost snipping off the long tail of the other semantic mappings.
Yeah, so I think there's two things to disentangle here.
The first is what is actually represented by the neuron in terms of ground truth.
And the second is what our techniques show us.
So the two techniques used in the open-air microscope
are looking at the images the most activated neuron,
and then this feature visualization technique where they produce a synthetic image
that maximally activates it.
And to me, this is, these are like,
both of these can be misleading,
because if the model activates the dice in poetry,
but activates dice with strength five and poetry with strength four,
then the optimally-bished activated will be dice,
and the optimal, the data set examples will also be dice.
But really, it'll be about poetry.
And you want to get a lot more rigorous.
You want to show true monosumanticity.
One cute thing is spectrum plots.
You take lots of example, data set examples across the full distribution,
you have a histogram with the different groups for the different meanings,
and then neural activation on the x-axis.
We have this really cute plot in Wes' paper called the French Neuron,
where all of the French text is on the right,
all the non-French text is on the left,
and the neuron is just very clearly distinguishing the two
in a way that's much more convincing to me than things like Max Act examples.
And I actually have a hobby project called Neuroscope at Neuroscopes.io,
where you can see the Max activating text examples
for every neuron and a bunch of language models,
though opening I recently output this paper with one that is better,
but only for GP2 XL.
Anyway, not that I'm bitter or anything.
Not so.
And yeah, so yeah, there's the things can lie to and be illusory.
There's this interesting paper called the Interruptibility Illusion for But,
which investigated this specific phenomena,
and in particular that if you take the data set examples
over some narrow distribution, like Wikipedia or books,
you can get pretty misleading things,
though they only looked at residual stream basis elements
rather than actual MLP neurons, I believe,
which makes it a bit less compelling.
Point of order as well.
We've been saying residual stream quite a lot,
and Microsoft introduced Resnet in 2015,
which basically means that between all of the layers,
the information is being passed up unadulterated,
so the subsequent layer can choose to either essentially shortcut
or ignore the previous layer or use some combination,
and at the time they kind of said it was about the neural network
being able to learn its own capacity in some sense,
but could you just give us the way you think about these residual streams?
Yeah, so I think the standard view of neural networks,
there are just layers, and layer 5's output is layer 6's input, etc.
Then people added Resnets, where layer 6's input is layer 5's output,
plus layer 5's input with the skip connection,
but I think people normally thought of them as like,
ah, it's like a cute trick that makes the model better,
but doesn't massively change my conceptual picture,
and the framing that I believe was introduced in the mathematical framework,
this anthropic paper led by Chris Ola, Nelson L. Harsh, and Catherine Olson
that I was involved with, is actually,
let's call the thing in the skip connection the residual stream
and think of it as the central object,
and draw our model so the residual stream is this big vertical thing,
and each layer is like a small diversion to the side,
rather than the other way around,
and in practice, most circuits involve things skipping many layers,
and each layer is better thought of as like an incremental update,
and there's a bunch of earlier transformer interpretability papers
that I think miss this conceptual point,
like the interpretability delusion for but what I mentioned earlier,
and study residual stream basis elements as like layer outputs or something.
Yeah, I mean, in a sense, you know,
we were talking about being able to reuse things that you've learned before
and not having to learn them again,
and I guess I think of it as a kind of translational equivalence
in the layer regime,
which is that you have a computation which is learned early on,
and now it can just be composed into subsequent layers.
It's like you've got a menu of computational functions
that you can call on at any layer.
Yeah, pretty much.
I think of it as like the shared memory and shared bandwidth of the model.
Yeah, almost like a memory bus.
Yeah, and sometimes models will dedicate neurons like cleaning up the memory
and deleting things that are no longer needed.
Yeah, yeah, and is there any interference in that memory bus?
So much.
This is the thing of superposition, right?
Yeah.
Like the residual stream is doing everything.
Like there's 50,000 input tokens start,
and then 4x as many neurons as residual stream dimensions
in every MLP layer,
and attention heads moving everything around,
and it's just a clusterfuck.
What if you scale up the bandwidth of the bus?
That's basically making the model bigger, right?
Which we know makes models better.
But I don't know, just thinking out loud,
but what if you maintained the original dimensionality of the model
but you deliberately upscaled the bus?
So like you make the thing inside each layer smaller
but make the residual stream bigger?
Or just make everything the same as it is,
but you just kind of like have a linear transformation on the bus
and double the size of the bus.
So I don't think that would work
without increasing the number of parameters,
because like if you...
Because like the thing that matters is the smallest bottleneck.
The output width of an MLP layer are like 4,000 by 1,000,
and in order to make the 1,000 bigger,
you need more parameters.
And there's like all kinds of studies about the optimal hyperparameters
and optimal ratios.
My general heuristic is number of parameters are the main thing that matters.
I don't know.
I don't spend that much time thinking about how to make models better, to be honest.
I just want to understand them, goddammit.
Yeah, because it's one of those things that it might remove bottlenecks
because essentially you're allowing the model to reuse things that it's learned previously.
So now every single layer can specialize more than it did before
and that might kind of like weirdly remove bottlenecks.
Yeah.
Yeah, the way I generally think about it is models are ensembles of shallow pods,
which is this paper from like five years ago about Resnets.
Like, deep-d2-small is 12 layers.
Each layer includes an attention block and an attention bit in MLP,
but it is not the case that most computation is 24 levels of composition deep.
It is the case that most of them involve like, I don't know, four.
And they're just intelligently choosing which four and remaking them in interesting ways.
And sometimes different things will want to like get to different points
and so it's useful to have many layers rather than a few.
But also, I don't know, if you halve the residual stream width
and give the model 4x as many layers, often performance is like about the same.
Or like not that different because the number of parameters is unchanged.
And this is just kind of a wild result about models that I think only really makes sense
within this framework of it's like an ensemble of shallow pods
and it's a trade-off between having more computation and having better memory bandwidth.
Yeah, yeah, very interesting.
Okay, I mean, just to close, superposition, it might not be a new idea.
So Janik did a paper video about this paper called Supermasks in Superposition
by Mitchell Wartsman back in 2020.
And he was talking about supermass representing sparse subnetworks
in respect of catastrophic forgetting and continual learning.
But that was slightly different because that was an explicit model
to perform masking, create subnetworks and to model, you know,
like basically a sparsity aware algorithm.
But he was still using a lot of the same language like interference and so on
and thinking about superpositions of subnetworks.
And I guess the difference is like just as we were talking about with these inductive priors
like transformers and CNNs, the models already do this stuff
without us having to explicitly code it, which I think is the interesting discovery.
Yeah, yeah, one update I've made from Wes's work is that
detokenization is probably like a pretty big fraction of what the early layers do.
And it's just really easy to represent compound words in superposition
because it's a very binary, it's either there or not there.
So alternating difference is easy to deal with.
They're mutually exclusive, so there's no simultaneous interference.
Like you cannot have Boris Johnson and Theresa May co-occur.
And there's just like so many of them.
One fact about language models that people who haven't played around them may not appreciate
is their inputs are these things called tokens.
And tokenizers are fucked because they're trained in this bizarre Byzantine way
that means that often the rarer words will get broken up into many tokens.
Yes.
Multi-word phrases are always different tokens.
Anything that's weird like a URL gets completely cursed.
And models don't want to have this happen.
So they devote a bunch of parameters to build a pseudo vocabulary of what's going on.
And just returning to your point earlier about, like, is it just these syntax-level things?
Is there some actual more semantic stuff going on?
We did also have case studies looking at contextual neurons, things like,
this code is in Python, this language is in French.
And these were seemingly monosemitic.
Like it seemed like there were specific neurons here.
And we found things like if you ablate the French neuron,
loss on French text gets much worse, what other ones are fine.
And also some interesting results that the model was, say, using this disambiguate things.
Like tokens like D are common in German and also common in Dutch.
And the neurons for those languages were being used to disambiguate for that token,
whether it was like a German D or a Dutch D,
because they've got very different meaning in the two languages.
Yeah, I wondered if you'd give me some interest in that, because as you say, in Wes's paper,
he did actually find that there are some monosemitic neurons like French, as you just said.
And in this case, the model decided that interference, in some sense, wasn't worth the burden.
But what does burden mean here?
And French is a very vague concept as well.
Yes. So, all right, a couple of observations.
First is I do not think we have properly shown they are monosemitic neurons.
We were looking, these models were trained on the pile, and we were specifically looking at them on Europal,
which is like a data set of European Parliament transcripts, which are labeled by language.
And we found a neuron that seemed to strongly disambiguate French from non-French.
But it was on this domain of parliamentary stuff.
And because models really want to avoid simultaneous interference,
if they did have superposition, they'd probably want to do it with something that isn't likely to co-occur in this context.
I don't know, this is a list variable in Python, which we didn't check very hard for.
And in particular, this is messy to check for, because in order to do that, you need to answer these questions like,
what is French?
Like, there's a bunch of English checks to activate for, but it will activate on words like,
Sacre Bleu and Trebilla.
And I think I count this as French, but like, I don't have a rigorous definition of French.
And I think an open problem I'd love to see someone pursue is just,
can you prove one of these neurons is actually a French detecting neuron or not?
And what would it even look like to do that?
And yeah, regarding interference in the burden, so the way I think about it,
if two features are not orthogonal, then,
no, sorry, this is more interesting in the case of neurons.
If there's multiple things that could all activate a neuron,
then it's harder for the downstream bit of the model to know how to use the fact that that neuron activated,
because there are multiple things, even if they don't co-occur, because they're mutually exclusive.
And this is just a cost.
And there's a trade-off between having more features and not having this cost.
And features like this is in French are really load-bearing.
They're just really important for a lot of circuitry here.
And so, theoretically, the model might want to dedicate an entire neuron to this,
but if you dedicate an entire neuron, you lose the ability to do as much superposition.
My intuition is the number of features that can be represented in superposition
is actually grows more than linearly with the number of dimensions.
So this might be significantly worse than just having one fewer feature.
So we are now in the next chapter of this beautiful podcast, and we're going to talk about transformers.
So how exactly do transformers represent algorithms and circuits?
And also, you've written this beautiful mathematical framework about transformers,
which, of course, is working very closely with Catherine Olsen and Chris Olat.
And Nelson Olsh.
And Nelson, my apologies.
Yeah, so in terms of understanding, yeah.
So if you wanted to do a mechanism to interpretability on a model,
you need to really deeply understand the structure of the model.
What are the layers? What are the parameters? How do they fit together?
What are the kinds of things that make sense there?
And let's see.
So, yeah, there's like a couple of key things I want to emphasize from that paper, though,
I don't know, it's also one of my, like, all-time top three interpretability papers.
People should just go read it.
And after reading it, check out my three-hour video walkthrough about it,
which apparently is most useful if you've already read the paper,
because it's that deep anyway.
Yeah, so a couple of things I want to call out from that,
especially for people who are kind of familiar with other network but not transformers.
The first, we've already discussed the residual stream as the central object.
And the second is how to think about attention,
because attention is the main thing which is weird about models.
They have these MLP layers, which actually represent, like,
two-thirds of the parameters in a transformer, which is often an underrated fact,
but attention is the interesting stuff.
So, transformers have a separate residual stream for each input token,
and this contains, like, all memory the model would store at that position.
But MLP layers can only process information in place.
You need attention to move things between positions.
And classically, people might have used stuff like a 1D convolution.
You average over 10 things in a sliding window.
This is baking in the inductive bias that nearby information is more likely to be useful.
But this is kind of a pretty limited bias to bake in,
and the story of deep learning is that over time, people have realized,
wait, we should not be trying to force the model to do specific things.
We understand, we should not be telling the model how to do its job.
If it has enough parameters and is competent enough, it can figure it out on its own.
And so the idea here is rather than giving it a convolution,
you give it this attention mechanism where each token gets a query saying what it's looking for,
each previous token gets a key saying what it has to offer,
and the model looks from each destination token to the source tokens earlier on
with the keys that are most relevant to the current query.
And models, and the way to think about an attention head,
so attention layers break up into these distinct bits called heads,
which act independently of the others and add to their outputs together,
and just directly add to the residual string.
This is sometimes phrases concatenate their outputs and then multiply by a map,
but this is mathematically equivalent.
Each head acts independently and in parallel,
and further, you can think of each head as separately breaking down into a
which information to move a bit determined by the attention,
which are determined by the query and key calculating matrices,
and the what information to move once I know where I'm looking,
which are determined by the value and output matrices.
We often think about these in terms of the QK matrix,
WQ times WK transpose, and the OV matrix,
WO times WV, because there's no long linearity in between,
and these two matrices determine like what the head does.
And the reason I say these are kind of independent is that
once the model has decided which source tokens to look at,
the information that gets output by the head is independent of the destination token,
and like the query only matters for choosing where to move information from,
and this can result in interesting bugs,
like there's this motif of a skip trigram,
the model realizes that if the current thing is three and two has appeared in the past,
then four is more likely to come next.
If the current thing is three and four has appeared in the past,
two is more likely to come next.
But if you have multiple destination tokens that all want the same source token,
for example, the phrase keep in mind can be a skip trigram,
really it should be a trigram,
but tiny models aren't very good at figuring out what's exactly at the previous position.
Keep at bay is another trigram,
but in an at, we'll both look at the same keep token,
and so they must boost both at and mind for both of them.
So we'll also predict keep in bay and keep at mind.
And possibly we should move on to induction heads,
which are a good illustrative example.
Yeah, it's going to come onto that.
So on these induction heads,
you've said that they seem universal across all models.
They underlie more complex behavior, like few-shot learning.
They emerge in a phase transition,
and they're crucial for this in-context learning.
And you said that sometimes specific circuits underlie emergent phenomena,
and we may want to predict or understand emergence by studying these circuits.
So what do we know so far?
A lot of questions in there.
All right, taking this in order.
So what is an induction head?
I've already mentioned this briefly.
Text often contains repeated subsequences,
like after Tim, Scarf may come next,
but if Tim Scarf has appeared like five times,
then it's much more likely to come next.
In toy two-layer attention-only language models,
we found this circuit called an induction head, which does this.
It's a real algorithm that works on, say, repeated random tokens.
And we have some mechanistic understanding of the basic form of it,
where there's two attention heads and two different layers working together.
The later one called an induction head looks from Tim to previous occurrences of Scarf.
The first one is a previous token head,
which on each Scarf looks at what came before,
and is like, ah, this is a Scarf token which has Tim before,
and then the induction head looks at tokens where the token before them was Tim,
or where the token before them was equal to the current token.
And when the induction head decided to look at Scarf,
which is determined purely by the QK matrix,
it then just copies that to the app, which is purely done by the OV matrix.
And I think induction heads are a really interesting circuit case study,
because induction heads are all of the interesting computation is being done by their attention pattern.
Tim's scarf could be anywhere in the previous context,
and this algorithm will still work.
And this is important, because this is what lets the model do
tracking of long-range dependencies in the text, where it looks far back.
And you can't bake this in with a simple thing like a convolutional layer.
In fact, transformers seem notably better than old architectures like LSTMs and RNNs,
in part because they have induction heads that let them track long-range dependencies.
And, yeah.
And more generally, it often is the case that especially late-layer attention heads,
the OV bit is kind of boring, it's just copying,
but figure out where to look is where all of the interesting computation lies.
So, first of all, just to clarify, because people will know what an attention head is,
but an induction head is one of these circuits that you're talking about, just so people understand.
And we should get onto this relationship between induction heads and the emergence of in-context learning.
And also, you said it's very important that we have this scientific understanding with respect to studying emergence,
but rather that than just framing of interpretability kind of makes better models.
Yeah. So, maybe I should first explain what emergence is.
Let's do that.
I'd be really, really interested if you could just give me the simplest possible explanation of what you think emergence is.
Sure. Emergence is when things happen suddenly during training,
anger from not being there to being there fairly rapidly in a non-convex way, rather than gradually developing.
Is this interesting you said that?
Because I think of emergence as a surprising change in macroscopic phenomena,
and it's an observer-relative term, which means it's always from the perspective of another scale.
So, just a transient change in perplexity or capability or something in my mind wouldn't entail emergence.
Like, it would need to be some qualitative, meaningful thing,
rather than just, oh, the loss curve got notably better in this bit.
I think so. It's definitely related to some notion of surprise, which is inherently relative.
Yeah. Let's not get hung up on that.
So, okay. Let's say it's a transient change in something.
Mm-hmm. Yeah.
You know, when you call it transient, it's like an unexpected sudden change.
Though unexpected has so much semantic meaning on it that I don't want to use.
But, yeah, this is an infinite rabbit hole.
Yes. But I think this scale thing is relevant as well.
We are programming neural networks at the microscopic scale,
and there's some macroscopic change in capability, so it's some...
Yes. Yeah.
Yeah. And there's, like, lots of different dimensions.
You can have emergence on. You can have it as you train a model on more data.
You can have it as you make the models bigger.
And these are both interestingly different kinds.
One of the more famous examples is Chain of Thought and Few Shot Prompting,
where DP3 is pretty good at this.
Earlier models were not good at this. This is kind of surprising.
Chain of Thought is particularly striking because people just noticed
a while after DP3 was public that if you tell it to think step by step, it becomes much better.
There's this recent innovation of Tree of Thought that I'm not particularly familiar with,
but I understand as kind of like applying Monte Carlo Tree Search on top of Chain of Thought.
Yes. Yes.
Where you're like, well, there's many ways we can branch at each point.
Let's use Tree Search algorithms to find the ultimate way of doing this.
Yeah. But with, let's say, Scratchpad and Chain of Thought,
I don't necessarily see that as an emergent...
Well, maybe there's an emergent reasoning capability that comes into play
when you have a certain threshold size model,
but I think of it more as kind of having an intermediate augmented memory in the context.
So you're kind of filling in a gap in cognition by saying you're allowed to...
It's not just remembering things, it's also reflecting on things that didn't work.
Yes. So, yeah, clarifying, when I say emergent, when I say Chain of Thought is an emergent property,
I mean, the capacity to productively do Chain of Thought is the emergent thing
and telling the model to things step by step is a user-driven thing.
But, I don't know, I kind of...
Just as a point of order, though, was it just that it was discovered after GPT-3
or would it work on GPT-2?
I would have guessed it doesn't work very well on GPT-2, but I've not checked.
I'd be pretty interested... I'm sure someone has looked into this, I haven't looked very hard.
I guess, like, so a lot of my motivation for this work comes from...
I care a lot about AIX risk and AI alignment and how to make these systems good for the world.
And when I see things like, oh, we realize that you can make GPT-3 much better by asking it to things step by step,
I'm like, oh, no.
What kinds of things could the systems you make be capable of that we just haven't noticed yet?
That's the concern that the genie's already out the bottle.
And, I mean, DeepMind just published this Tree of Thought paper.
It's a really simple idea.
It's basically a star search over trajectories of prompts,
and you use the model itself to evaluate the value of a trajectory.
And I could have done that.
Anyone could.
Similar thing with auto-GPT and all this stuff.
I'm more skeptical than you are.
I think in the case of Tree of Thought, it closes a capability gap in respect of certain tasks which were not working very well
because they don't have that kind of system to...
Models don't seem to plan ahead very well.
But I still think that it's not just going to magically turn into super intelligent.
I mean, we can talk about this a little bit later.
Yeah, okay.
Yeah, so...
Yeah, I think this is also pretty relevant to much more near-term risks.
I don't know, there's lots of things that a sufficiently capable model could do
that might be pretty destabilizing to society,
like write actually much better propaganda than human writers can or something.
And if Tree of Thought makes it possible to do that
in a way that we did not think was possible when GPT-4 was deployed,
that's like an interesting thing that I care about noticing.
It's not a very good example, but...
Yeah, it is.
But being able to...
I mean, first of all, it's been possible to create misinformation for a long time.
This is why I specified be able to do it notably better than humans can.
I totally agree.
The longer doing it a bit more cheaply and a bit more scale doesn't seem obviously that important.
You could argue that, like, I don't know,
being a spam bot that feels indistinguishable from a human
is like a more novel thing that's actually different.
Yeah.
But, I don't know, this is like an off-the-cuff example.
I don't want to get too deep into this,
because it's not a point I care that deeply about.
Yeah, I mean, we can come back to it a bit,
but I think we are nearly already there.
Yeah.
You know, this irreversibility thing.
We don't know.
Computer games are photorealistic.
Chatbots are indistinguishable,
and AI art is pretty much indistinguishable.
And that could work.
I mean, I spoke to Daniel Dennett about it last week,
and he said he's really worried about the epistemic erosion of our society,
more so interestingly than the ontological erosion.
And I discovered later that's because he's not a big fan of anything ontological.
Yeah, it is potentially a problem,
but I guess to me, people might overestimate the scale
and magnitude of change of this.
I feel that, I know I don't want to echo Sam Altman here,
but he said that we are reasonably smart people,
and we can adapt and recognize deep fakes and so on.
Yeah.
These are complicated societal questions.
I guess I mostly just have the position of man.
It sure is kind of concerning that we have these systems
that could potentially pose risks,
but you don't know what they do and decide to deploy them,
and then we discover things they can do.
And I think that the research direction I'm trying to advocate for here
is just better learn how to predict this stuff more than anything,
which hopefully we can all agree is like an interesting direction.
And there's all kind of debates about is emergent phenomena
like actually a real thing?
Like this recent, is this a mirage paper,
which I think was a bit over-claiming,
but does make a good point that if you choose your metric
to be sufficiently sharp, everything looks dramatic.
One thing I've definitely observed is if you have an accuracy graph
with a log scale x-axis for grokking,
it looks fantastically dramatic.
And I was very careful to not do this in my paper
because it is cheating.
But yeah.
So my particular hot take is that I believe emergence is often underlain
by the model learning some specific circuit
or some small family of circuits in a fairly sudden phase transition
that enables this overall emergent thing.
And this sequel paper led by Catherine Olson
in Contest Learning and Induction Heads
is a big motivator of my belief for this.
So the idea of the paper is we have this,
we found induction heads in these toy-till-artentionally models.
We somewhat mechanistically understood them,
at least in the simplest case of induction.
We use this to come up with more of the behavioral test
for whether it's induction heads.
You just give them all repeated random tokens
and you look at whether it looks induction-y.
And we found that these occurred in basically all models we looked at,
up to 13B, even though we didn't fully reverse engineer them there.
And we then found that this was really deeply linked
to the emergence of in-context learning.
There's a lot of jargon in there, so let's unpack that.
In-context learning, already briefly mentioned,
it's like tracking long-range dependencies in text, like,
you can use what was on, which was three pages ago,
to predict what comes next in the current book,
which is a non-trovial thing.
It is not obvious to me how I would program a model to do.
In-context learning is emergent.
If you operationalize it as average loss on the 500th token
versus average loss on the 50th token,
there's a fairly sudden period in training
where it goes from not very good at it to very good at it.
Just a tiny point forward of that.
One interesting thing about in-context learning
is you're learning at inference time, not training time.
Yes.
But you're not changing anything in the underlying model,
which means anything it can do, presumably,
must be materializing a competence which was acquired during training.
So it's coming back to this periodic table thing, right?
You've just learned all these platonic primitives.
You do this in-context learning.
You say, I want you to do this. Here's an example.
And you've got all of these freeze-dried periodic computational circuits,
and they spring into life, and they compose together,
and they do the thing.
Yes.
Yes.
I think induction heads are, to my eyes,
the canonical example of an inference time algorithm
stored in the model's weights that get supplied.
And I'm sure there's a bunch more that no one has yet found.
And, yeah, a lot of my model is that prompt engineering
is just telling the model which of its circuits to activate
and just engaging with various quirks of training
that have made it more or less steerable in different ways.
And, yeah, so induction heads also emerge
in a fairly sudden phase transition.
And we, and exactly at the same time,
and we present a bunch more evidence in the paper
that there's, like, actually a causal link here.
Like, one-layer models have neither the in-context learning
or the induction heads phase chain,
because they can't do induction heads,
because they're only one layer.
But if you adapt the architectures,
they can form induction heads with only one layer.
Now they have both of these phenomena.
If you obliterate induction heads,
in-context learning gets systematically worse.
And a particularly fun qualitative study
was looking at soft induction heads,
heads that seem to be doing something induction-y
in other domains,
like a head which attends from the current word in English
to the thing after the current word in French.
Or, more excitingly, a few-shot learning head
on this random synthetic pattern recognition task we made
where it attended back to the most relevant examples
to the current one.
And my interpretation of all this
is that there's something fairly fundamental
about the induction-y algorithm for in-context learning.
So the way I think about it,
let's say you've got two...
You want to learn some relation.
You've got some local context A
and some past context B.
And if you observe A and you observe B in the past,
this gives you some information about what comes next.
There's two ways this could work out.
It could be symmetric.
B helps A and A helps B.
Or asymmetric.
B helps A, but A does not help B
if they're the other way around.
Asymmetric might be like knowing the title of a book
tells you what comes next,
but knowing what's in a random paragraph in the previous bit
doesn't tell you the title.
While symmetric is like...
I know, English sentence helps French sentence,
French sentence helps English sentence.
And if you have N symmetric relations,
like English, French, German, Dutch, Latin, whatever,
where each of them helps each other,
this is really efficient to represent.
Because rather than needing to represent
N squared different relations separately,
like you would in the asymmetric case,
you can just map everything to the same latent space
and look for matches.
And fundamentally, this is what induction heads are doing.
They're mapping current token
and previous token of thing in the past
to the same latent space and looking for matches.
And to me, this is just like a fairly natural primitive
of attention.
And this is exciting because, A,
we found this deep primitive
by looking at toy two-layer attentionally models.
B, it was important for understanding
and ideally for predicting the emergent phenomena
of in-context learning.
And two takeaways I have from this
about work we should be doing.
The first is we should be going harder
at looking at toy language models.
Like open source to scan of 12 of them.
And I'd love to see what people can find
in one-layer models with MLPs
because we really suck at transformer MLP layers.
And one layer should just be easier than other ones.
And the second thing is
I really want a better
and more scientific understanding of emergence.
Why does that happen?
Really understanding particularly notable case studies of it.
Testing the hypothesis that it is driven
by specific kinds of circuits like induction heads
or at least specific families of circuits.
Even though, I don't know,
you could argue that because we haven't fully reverse engineered
the things in the larger models,
we really know it's actually an induction head.
And yeah.
More generally, a lot of my vision
for why mechantup matters
is this kind of scientific understanding of models.
Like I don't care about making models better,
but I care about knowing what's going to happen,
knowing why stuff happens,
achieving real understanding.
And getting a scientific understanding
of things like emergence
seems like one of the things mechantup
might be uniquely suited to do,
but also no one checked very hard.
And you, dear listener, could be the person who checks.
So there was a paper by Kevin Wang et al.
called Interpretability in the Wild,
a circuit for indirect object identification in GPT-2 Small,
which found a circuit for indirect object identification.
So they discovered backup name and mover heads,
which normally don't do much.
They take over when the main name mover head are ablated.
And they said mechanistic interpretability
has a validation set for more scalability techniques.
They've understood a clear place
that these ablations can be misleading.
So...
Yeah.
So, yeah, bunch one pack in there.
So I really like the interpretability in the wild paper.
Also, Kevin was only 17 when he wrote it.
Like, man, I was doing nothing remotely as interesting
when I was in high school.
So props to him.
But also a sign of how easy it is
to pick low hanging fruit
and do groundbreaking interpretability work.
Such a young field.
I know it's so impressive.
Yeah, I've just checked his Twitter.
Hey, Kevin.
And, yeah, so...
To me, the underline...
Yeah, so I was zooming out a bit.
I think there's a family of techniques
around causal interventions
and their use in mech and top
that's useful to understand here.
So...
The core technique is this idea of activation patching.
Where...
So let's...
So one of the problems with understanding
a model's features and circuits
is models are full of many, many different circuits.
Each circuit does not activate on many inputs.
But each circuit will activate...
But on each input, many circuits will activate.
And in order to do good mech and top work,
you need to be incredibly surgical and precise,
which means you need to learn how to isolate a specific circuit.
And let's consider a statement like...
The Eiffel Tower is in Paris
versus the Colosseum is in Rome.
These are both...
There's lots of features happening.
There's lots of circuits being activated
on the Eiffel Tower is in Paris.
This is in English.
You're doing factual recall.
You are outputting a location.
You are outputting a proper noun.
This is a European landmark.
Et cetera, et cetera.
And like, I want to know how the model knows
the Eiffel Tower is in Paris.
But the Colosseum is in Rome.
Controls almost everything apart from the fact.
And so...
What I can try to do is
causally intervene on the Colosseum run.
And replace, say, the output of an attention head
with its outputs on the Eiffel Tower prompt
and see how much this changes the answer from Rome to Paris.
And this...
Yeah, this patch can let me really isolate
how the circuitry for just this specific thing works.
And there's all kinds of work around this.
Obnoxiously, all of it uses different notation,
like resample ablations and causal tracing
and causal mediation analysis and entertained interventions.
All similar words are basically the same thing.
But, yeah.
The really key insight here is this kind of surgical intervention.
A classic technique in interpretability is ablations,
where you just set something to zero.
And it's kind of janky because if you break something in the model,
which wasn't interestingly used for the task,
then everything dies.
Or if you break it in interesting ways, everything dies.
For example, in GPT2 Small,
almost every single task breaks if you delete the 0th MLP layer.
Yeah, as far as I can tell,
the 0th MLP layer is kind of an extended embedding.
GPT2 Small has tied embeddings and unembeddings,
so they're transposed of each other,
which is wildly unprincipled in my opinion.
And the model seems to be both using this for just detokenization
and combining nearby things with the first attention layer,
the 0th attention layer, and just undoing the tightness.
But this means that basically everything is reading from that.
And I've seen people do zero ablations and everything
and be like, oh, this is an important part of the circuit.
Let's get really sidetracked by this.
Because the effect size is so big.
Man, being a mech interpret research feels my mind
with such bizarre trivia like this.
It's great.
Models, so bizarre.
And so, yeah.
This calls on intervention.
There's kind of two conceptually different kinds of interventions.
You can take the Eiffel Tower prompt,
patch in something from the Colosseum
and see if it breaks the ability to output Paris,
to verify which bits kind of are necessary,
such that getting rid of them will break something.
Or you can patch something from the Paris run into the Colosseum run
and see if that makes it output Paris,
which is testing for stuff that's sufficient.
I call the first one a resample ablation
because you're messing up a component by resampling
and the second one denoising or causal tracing
because you're intervening with a bit of information
and seeing if that is sufficient for everything else.
Though none of these names are good.
I would love someone to come up with better names.
And there's all kinds of families of work building on this.
I have this post called attribution patching
that tries to apply this as an industrial scale
by using gradients to approximate it,
which is fast enough that you could take GPD3
and its four million neurons
and do attribution patching on all neurons at once
on every position.
Great post.
Redwood Research has this technique called causal scrubbing,
which I view as activation patching
gone incredibly hard and rigorous
that tries to come up with an automated metric
for saying this hypothesis about a model
is actually accurate for how it works,
where it's kind of complicated.
But the core idea is you think of a hypothesis
as saying which resample ablations are allowed
and you make all of the resample ablations
that should be allowed.
Like these components of the model shouldn't really matter
so we can just patch in stuff from random other inputs.
If you've got, say, an induction head,
you might think the induction head cares about
the current token and the thing before the past token
that it's going to inductionally attend to.
So let's replace the token that it's going to be attending to
with a token from a different input
but with the same token before it.
My hypothesis about the induction head
says this should be allowed, so let's do that.
I wouldn't want to introduce a rant
but the metric he uses is really important.
Yes, this is one of my hobby horses.
So some of the original work looking at the patching stuff
like David Bow and Kevin Meng's excellent Rome paper
uses the probability of Paris as their metric
and there are other papers that use things like accuracy
as their metric and generally I think of metrics
as being on a spectrum from like soft to sharp.
So generally I think of models as thinking in log space.
They are kind of acting like basions.
They are trying to figure out some things in Paris
and there will be five separate heads
that each contribute one to the correct logit
and each of these can be thought of as one bit of information
and together they get you the right probability of, say, 0.8.
But if you patch in each one in isolation
the probability changes negatively
because probability is exponential in the logits.
So using probability you're like,
oh, this head patch doesn't really matter.
So in this paper they did this thing of patching in
like 10 adjacent layers at once.
And to me a really core principle of this kind of causal
intervention and mechanistic technique
is you want to be as surgical as possible
to be as deeply faithful as possible
to what the neural model is actually doing.
So in this case there was an interaction between them.
They were effectively making several interactions
at once.
Yes, they were replacing 10 adjacent layers
and patching things in different layers
is always a bit weird.
I don't think that part's that objectionable.
I mostly just feel like if you choose a metric like log prop
it allows you to be much more surgical about how you intervene.
It allows you to identify subtle effects of things.
Accuracy is even worse because accuracy
is basically rounding things to zero or one.
So like if the threshold is 2.5
any individual patch does nothing.
Any re-sample ablation does nothing.
But if you patch in like the 10 adjacent layers
it will do everything.
And this can be kind of misleading.
Another one I often see people do is
they look at things like the rank of an output.
At which point does the model realize Paris
is the most likely next token?
And this can be super misleading
because this will make you think
the third head is the only head that matters.
When really all five of them matter
the order is kind of arbitrary.
And yeah, I've seen papers that I think
got somewhat misled by using metrics like this.
And metrics, they matter so much.
It's so easy to trick yourself.
My high level pitch is just,
mech and tub is great, mech and tub is beautiful.
Also the field is incredibly young.
There's maybe 30 full time people
working on it in the world.
There's a ton of low hanging fruits.
I've done major research in this field
I've been at for like less than two years.
I would love people to come and help
us solve problems and do research here.
And we'll link to my post on getting started
and my sequence called 200 Cronkidopin problems
in the description to this, hopefully.
And yeah, I think there's just,
it's not that hard to get started.
It's really fun.
Hopefully I've nerd sniped you
with at least one thing in this podcast.
And if you're at least vaguely curious,
it's just really easy to open one of the tutorials
linked in my posts and just start screwing around.
And I'd love to see what you can find.
Beautiful.
Also the deep mind element team is currently hiring
and people should apply,
which includes hiring from a mechanistic interoperability team.
Amazing.
Do they have to do lead code?
I have no idea.
Can't remember.
Yeah, yeah, we did an amazing video
with Petr Velichkovich.
I gave him one of my lead code challenges
and annoyingly he aced it.
It's all that deep mind interview practice.
Anyway, okay, let's talk about super intelligence.
Now, I spoke with our mutual friend,
Robert Miles about a month ago.
Rob's so great.
He's a lovely chap.
Spoke all about alignment.
And he accused me of over-philosophizing everything
because I was talking all about intelligence,
one of my favorite topics.
And he said, well, what about fire?
Fire is something that people didn't understand millennia ago,
but they knew that it burnt and they knew that it was bad.
And this is like, this is like a fire,
which is very interesting.
And maybe we can bring in a little bit of effective altruism as well.
So, you know, I...
If I can just interject.
Please do, please.
If there is one thing I have learned
from the past decade of machine learning programs,
is that you do not need to understand a thing in order to make it.
And this extends to things that are smarter than us
and which are capable of leading to catastrophic risks.
Yes, yes.
Well, let's...
I'll step back a tiny bit and then we'll get there
because there's the hypothetical nature,
which I guess I have a bit of a problem with.
Now, about 10 years ago,
I was one of the first supporters of Sam Harris' podcast
and he's quite aligned to EA.
And he was talking about this very noble idea
that everyone matters equally
and people on the left should get on board with that intrinsically.
And this idea that we should quantitatively analyse
the impact of charity work
and solve an optimisation problem
and earning to give
and a lot of the stuff that MacAskill spoke about
and also philosophers like Peter Singer.
And the focus seemed to be primarily on alleviating poverty,
and we don't say the biggest problem,
we say a problem.
This is another thing our friend Robert Viles said.
He said,
the problem is when people talk about the problem,
there can be more than one problem.
But anyway, so it's a big problem.
And recently, you and I can agree
that EA circles have really laser-focused in
on existential risk from AI
as opposed to other more plausible ex-risk concerns
like pandemics or even nuclear war.
I'm not to say that they don't focus on that, but...
I am going to push back on other more plausible ex-risk.
Go on.
Go on.
I just wanted to register an objection.
Feel free to go.
Register an objection.
And cynically, from my point of view,
I see the influence of Eliezer, Bostrom,
Hansen, et cetera, kind of shifting the focus on to ex-risk.
And part of the reason for that is also this kind of overly
intellectual focus on long-termism.
And it's done in a very intellectualized way.
So it's based on the utility function now incorporating
future simulated humans on different planets,
a long time away in the future,
and making all of these intellectual jumps.
So let's start there.
What's your take?
All right.
So much stuff to respond to in there.
Good.
So, all right, a couple of things.
The first, so, cars on the table.
I care a lot about AI existential risk.
Yes.
The reason I work on mechanistic interpretability
is because I think that understanding the mysterious black
boxes that are potentially smarter than us
and may want things wildly different than what we wanted them
to want is just clearly better than not understanding them.
Yes.
And I think mechanistic interpretability
is a promising path here.
And I also would consider myself an effective altruist
and a rationalist.
So cars on the table, there's my biases.
So I generally think it's more productive to discuss
is AI catastrophic and existential risk a big deal?
Then is it the biggest deal or is it worth more resources
on the margin than global poverty or climate change
or AI ethics?
And like, there's just lots of problems.
I care way more about convincing people that AI extras
could be in your top 10 than it should be in your top one
because I feel like for most people,
it's not in their top thousand.
And there's just so much divisiveness between, say,
the AI ethics community and the AI alignment community
about whose problem is a bigger deal.
And like, both are big problems.
Why are we arguing?
And part of this is about our moral intuitions.
And this is something I spoke a lot with Conor about.
He said that in many ways he's got this technical empathy.
So sensory empathy is, I really care about my family.
I care about these concentric circles of moral status.
I really care about my family.
And if I try really hard, I can care about people
in other countries and so on.
And then if I try really, really hard,
I can care about future simulated lives on Mars.
And Conor said, the idea of this movement is about
galaxy-braining yourself into being the most empathetic person
imaginable, but it's a kind of empathy
that people don't understand.
So a separate bit of beef I have is with the entire notion
of long-termism.
So long-termism is this idea...
So long-termism is generally caring about the long-term future.
There's the strong form of value in the future
basically entirely dominates things today.
And weaker forms of just this really, really matters.
And a common misconception about AIX risk and AI safety
is that you should only work on this if you are a long-termist.
That, you know, it's a one in a billion chance of mattering,
but there's a quintillion future lives,
so this outweighs everyone alive today in moral worth.
Or, well, we're only gonna get AGI in like 500 years,
but we're gonna work on it now just in case.
And like, I think both of these are just nonsense.
Like, I guess as a concrete example,
Effective Artists have worked on pandemic prevention for many years.
And I think it was just clearly the case that pandemics are
a major threat to people alive today.
And I like to feel that we've been proven right.
No one's gonna argue at that point.
And, you know, everyone's being like, Effective Artists,
why are you working on AI safety?
This obviously doesn't matter.
You know, I feel like we've got one thing right.
Can I be really skeptical, though, for a second?
Because, I mean, you're working for DeepMind.
There's so much prestige and money attached to AI risk.
Elon Musk is talking about it all the time,
whereas you could be a scientist working on pandemic responses.
And, I mean, let's be honest,
it wouldn't be anywhere near the same level of prestige.
Yeah.
So, couple of takes.
It definitely is the case that I,
a good chunk of why I personally am working on AI X-Risk
rather than say BioX-Risk is that I'm a smart mathematician.
I like AI.
I like mech and tub.
I do not think I would be good at biology in the same way.
And I also personally assert that AI X-Risk is more important
and, like, more pressing, but, you know, I'm biased.
And I think it's fair to flag that bias.
In terms of prestige,
so I've only really been working on this stuff properly
for the past two and a half years,
which is, I mean, it's changed dramatically.
Like, in the last six months we've gone from,
well, we're really ever going to get AGI to,
oh my God, GP4 exists.
Jeffrey Hinton has left Google to loudly advocate for X-Risk.
Joshua Benjo is now loudly advocating for X-Risk.
It's two-thirds of the Turing winners for deep learning.
You'll never get the third one.
Yeah, we're never going to get the third one.
Jan Likert has made his position very, very clear.
But, you know, as a majority, I'll take it.
Yes.
Or the fourth one.
Yeah.
He's coming on our podcast, actually.
Oh, who was the fourth one?
Schmidt-Huber.
Yeah, it seems hard.
I'm very curious to hear the Schmidt-Huber episode.
Oh, yeah, he's even more virulently against than Jan.
I'm afraid to say.
Two out of two.
Two out of four.
I'm interested to hear it anyway.
So, yeah.
And, yeah, in terms of prestige, I don't know.
I gather that, say, seven years ago, it was basically just not...
It would be, like, pretty bad for your career.
You would not be taken seriously if you mentioned caring about AIX-Risk.
Your papers would be rejected.
I hear a story of Stuart Russell at one point talk to a grad student of his
about how Stuart was concerned about AIX-Risk.
The grad student was also really concerned and freaking out,
but they'd been working together for years
and neither felt comfortable mentioning it.
And a lot of people who are still in the field were doing the stuff then,
which makes me somewhat reject the prestige argument,
at least for senior people in the field.
I think there's a difference with Stuart Russell in particular.
He's very credible.
And he...
No, I'm not.
Oh, I didn't mean... I didn't mean you.
I was talking about the two Godfathers,
because the thing that...
Maybe I shouldn't say this,
but I was surprised that Benjio and Hinton came out in the way they did.
And I...
The reason I didn't like what they said was,
I felt that they were implying that current AI technology
could pose an existential threat.
And what I'm getting from you and what I'm getting from Russell is...
I'm also from Robert Miles,
is that this is a very real potential threat in the future,
but it's not a current threat.
Yes, very real potential threat in the future,
though I hesitate to confidently assert, say,
this will not be a threat in the next five years or something.
It's like pretty hard to say.
Interesting.
I'm not confident.
I agree with your assessment of Benjio and Hinton,
though they've spoken a bunch publicly,
so I'll defer if you can point to specific writings.
But for example, Benjio signed the pause AI for six months,
more powerful than GPT-4 letter,
and I don't know.
I don't think the letter was asserting that...
The letter definitely wasn't asserting GPT-4 was an extra risk.
It wasn't confidently asserting GPT-5 would be,
but it's being like,
yeah, we need more time and slow down and caution.
Maybe I'm reading too much into that,
but it seemed to me that, I mean, Hinton said
that chat GPT now contains all of the world's knowledge,
and this chatbot knows everything,
and it could potentially do very harmful things,
and I interpreted it possibly incorrectly
that they were talking about reasonably current
or next-generation risks.
I mean, I can't talk for them.
I also, I don't know, there are lots of near-term risks.
There's long-term risks.
I consider it my job to think hard about the long-term risks
and try to guard against those,
and I think lots of other people's jobs
is to focus on the near-term risks,
and both are great forms of work.
I don't know.
One reason I like interpretability
is I think it is just broadly useful across all of them.
So what I consider to be my job might just not even matter.
But, yeah.
Yeah, no, I probably will not...
Do not want to get deeply into interpreting
what other people have said.
I...
Well, could I ping you a couple of quick questions?
So, first of all, you know,
there's this idea of negative utilitarianism.
I mean, do you think minimising suffering
is more important than maximising happiness?
No.
No?
Not sure I've got a more deep answer than that.
I mostly think a lot of this intrusive reasoning
is more often by intuition than anything else.
But it's a bit like this metrics thing we were talking about,
you know, which is that...
If you want to have...
Would you like to tolerate some spiky necks
for some average happiness?
Yeah.
So, I know.
I have, like, a general frustration
with these discussions getting too philosophical.
This is a big issue
when I hang out with effective altruists
who really love moral philosophy and population ethics.
Yes.
I don't know.
I have this EA forum post called
Simplify EA Pictures to Holy Shit X-Risk.
And it's like...
So, I don't know.
If you actually look at some of the concrete work
people try doing on things like timelines and risk,
there's this report from a Jay Akatra at Open Philanthropy
that gives 30-year median timelines
to AI that's transformative,
which he's since updated to 20 years.
There's a report by Joseph Karlsmith
that estimates about a 10-ish percent chance
of a major catastrophe from this.
Yeah.
And if you just take those numbers,
this is clearly enough to reach pretty high
on my list of concerns of people alive today.
Okay, okay.
And I think these are bold empirical claims.
And I think it's great to debate them in the empirical domain.
But to me, this doesn't feel like a moral question.
It just feels like from common sense assumptions,
if you believe these empirical claims,
this stuff is a really big deal.
Okay, okay.
Let's take another couple of steps.
So, first of all, we save this till later.
I think deception is very important.
And Daniel Dennett, when I spoke with him,
he uses this notion called the intentional stance,
which basically means that if you use a projection
of purposes, goals, agency, et cetera,
in order to understand the behavior of an agent,
possibly a simulated agent,
then for all intents and purposes, it has agency.
It can make decisions.
It has moral status,
it has lots of different things like that.
And he would say that without an intentional stance,
without agency,
it's impossible for a model to lie or deceive us.
Now, what do you think would be the bar
for something like a GPT model to deceive us and why?
Yeah.
So, before I give takes,
I will generally reinforce Rob's vibe of,
well, if you have no idea how fire works,
but you know that it burns you.
That's kind of the important thing.
Like maybe a model has just this random learned adaptation
to output things that are designed to get a user
to feel and believe a certain way,
that isn't intentional and isn't deceptive
in some true COGSI sense,
but it's like enough for this to be a big deal
that we should care a lot about.
Okay.
With that aside.
Yeah.
So, I'm definitely hesitant to ascribe
an overly confident view of what's going on here.
And I think lots of early discourse alignment
around things like utility maximization
and around things like these things are just
paperclip maximizers, et cetera,
is kind of misleading.
And I don't think it is an accurate model
of how GPT-7 RLHF++ is going to work.
Well, that's my prediction.
One thing that is pretty striking to me is
I just feel like we're pretty confused
on both sides of this.
Like, I do not feel like I can confidently claim
that these models will demonstrate
anything remotely like goals or intentions,
but I also don't feel like you can confidently claim
that they won't.
And I'm not talking like 99.99% confidence.
I'm talking like 95% plus confidence either way.
And one of my visions for what being good
at Mechantep might look like is being able
to actually get grounding for these questions.
Because I think ultimately these are mechanistic questions.
Behavioral interventions are not enough to answer
like, does this thing have a goal in any meaningful sense?
But yeah, my very rough, soft definition would be
is the model capable of forming
and executing long-term plans towards some goal,
potentially if explicitly prompted to auto-GPT
or just spontaneously,
is it capable of actually carrying out these plans?
And does it form and execute plans
towards some objective
that is like encoded in the model somewhere?
And I don't know.
I think it's pretty plausible that the first dangerous thing
is like Chaos GPT-7,
where someone tells it to do something dangerous
and it gets misused more so than it's like misaligned.
And I care deeply about both of these risks.
Okay.
So yeah, first one's more of a governance question
than a technical question.
And thus is less where I feel like I can add value.
So I agree with you on all of that.
So yeah, being less confused about what's going on inside the models
and using interpretability to figure out
whether they actually do have agency or goals
and sometimes they do the right things for the wrong reasons.
Auditing models that seem aligned before they're deployed
is something that you've told me before.
So great.
And just being able to check more deeply that it truly is aligned.
But I wanted to talk a little bit about
this interesting paper from Katya Grice.
So she wrote a response called, it was on the less wrong,
debunking the AI apocalypse,
a comprehensive analysis of counterarguments
to the basic AI risk case, X-Risk.
And the reason I read it is so many of the comments
were destroying me and Doug after we interviewed Rob.
And they said, well, if you're going to criticise S-Risk,
I mean, at least go and read Katya Grice's response.
So I did.
So I did.
Here we go.
So she basically made two big counterarguments
that intelligence might not actually be a huge advantage
and about the speed of growth is ambiguous.
But I first want to touch on what you said before,
which is about this notion of goal-directedness.
So alignment people say that if superhuman AI systems are built,
any given system is likely to be goal-directed
and the orthogonality thesis and instrumental goals
are cited as aggravating factors.
And the goal-directed behaviour is likely to be valuable,
so economically.
Goal-directed entities may tend to arise
from machine-ledding training processes,
not intending to create them,
which is kind of talking about some of the emergent behaviours
that we were talking about earlier with respect to Othello,
for example.
And coherence arguments may imply that systems with goal-directedness
will become more strongly goal-directed over time,
which is apparently something that is argued for.
So I'm thinking, what does goal even mean?
I mean, we anthropomorphise abstract human intelligible concepts
like goals and they really are emergent
because they emerge from these low-level interactions
in the cells in your body
and then you get these things that we recognise to be goals,
observer-relative, as we were talking about before.
But they're just graduated phenomena from smaller things, right?
So what does it even mean to have a goal?
Yeah, so a couple of thoughts on that.
Again, you ask questions with a lot of content in them.
No problem. I can only apologise.
I mean, someone who accidentally writes
19,000 word blog posts all the time, I relate.
Anyway, so what am I saying?
So the way...
Yeah, it's a fake concept, right?
Yeah, so I definitely want to try to take...
So there's the mechanistic definition of the model forms plans
and it evaluates the plans according to some criteria or objective
and it executes the plans that score better on this.
And I would love if we get to a point where we can look inside a model
and look the circuitry that could be behind this or not.
That would feel like a big milestone for me on
wow, I really believe mech and tuple matter
for reducing catastrophic risk from AI.
A second thing is that...
Yeah, the kind of more behavioural thing of
the model systematically takes actions
that pushes the world towards a certain state.
And I don't want...
I think there's a common problem in alignment arguments
where people get too precise and too specific
in a way that lots of people reasonably object to
in a way which is not necessary for the argument.
There's a really great paper called
The Alignment Problem from a Deep Learning Perspective
by Richard Ngo, Lawrence Tan, and Sorin Mindenman.
And this is probably my biggest recommendation
for the listening audience of what I think is like
a pretty well-presented case for alignment.
And I generally pretty pro-try to make the minimal
necessary assumptions.
So for me it's kind of like some soft form of
goal-directedness of take actions that push the world
towards a certain state.
And another important thing is
there are a bunch of theoretical arguments
for why goals would spontaneously emerge.
Ideas around in a misalignment from work
led by Evan Huminger, ideas around
just coherent theorems and things like that,
which I know I find a bit convincing,
not that convincing, but then there's
things will have goals because we try to give them goals.
And I'm like, yeah, that's probably gonna happen.
It's just clearly useful.
If you want to have an AI CEO
or AI helping run logistics and military operations
to have something that's capable of
forming and executing long-term plans
towards some objective.
And if you believe this is what's gonna happen,
then the key question is,
are we capable of ensuring those goals
are exactly the goals we would like them to be?
And my answer for any question of the form,
can we precisely make sure the system is doing exactly X
and machine learning is, God, no.
We are not remotely good enough to achieve this
with our current level of alignment and steering techniques.
And to me, this is like a more interesting point
where it's not quite a crux for me,
but it just seems like a lot easier to argue about
what people do this.
Yeah, essentially, I mean, Katya herself said that
it's unclear that goal-directedness
is favored by economic pressure to training dynamics
or coherence arguments, you know,
whether those are the same thing as kind of goal-directedness
that implies a zealous drive to control the universe.
And look at South Korea, they have goals.
And those goals, I don't really subscribe
to the dictator view of society.
I assume they are somehow emergent.
And similarly...
Sorry, South Korea or North Korea?
Sorry, North Korea, did I say South Korea?
Very different careers.
Different goals, different goals.
But you can think about goals in an AI system
as either being ones which emerge from some low level
or ones which are explicitly coded by us
or ones which are instrumental.
And these are all a whole bunch of goals.
But we can't really control those.
We can add pressures.
How do we control what North Korea does?
That sure is a question I'd love for someone to answer.
I don't know.
I can give speculation.
There's the question within practice,
what do people do?
Which is basically reinforcement learning from human feedback.
And I expect people would apply that in this situation as well.
I definitely do not believe we would be able
to explicitly encode a goal in the system.
Moreover, even if you can encode,
even if you could give some scoring function,
like make the score in this game high,
this does not give you a model that intrinsically cares about that
in the same way that, I don't know,
evolution optimizes inclusive genetic fitness.
I don't give a fuck about inclusive genetic fitness.
Even though I care about a bunch of things,
evolution got me to care about within that,
like tasty foods and surviving.
Yeah, so we don't know how to put goals into systems.
I basically just assert that we are not currently capable
of putting goals into systems well.
And this is one of the main things
the field of alignment thinks about.
And we're not very good at it.
It'd be great if we were better at it.
In terms of, yeah, I definitely don't want to make strong claims
about, to be dangerous, the goals need to be coherent
or the goals need to, there needs to be like a singular goal.
Like I don't have a singular goal.
It's not obvious to me how these systems will turn out.
If they don't, in any meaningful sense,
want a coherent thing, then I'm a fair bit less concerned.
Though, well, I mean, there's many, many ways
that human level AI would be good for the world
or bad for the world, or just wildly destabilizing
and high variance, of which misalignment risk is one of them.
And lots of the other ones just don't apply,
like misuse and systemic risks.
But leaving those aside, yeah,
I think if a model is just roughly pushing
in a goal-directed direction with a bunch of caveats
and uncertainties and flip-flopping,
that still seems like a pretty big deal to me.
Okay, okay.
Katia, let's just cover her two main arguments.
So she said that intelligence might not actually be
a huge advantage.
Looking at the world, intuitively,
big discrepancies in power are not to do with intelligence.
And she said IQ humans with an IQ of 130
earn roughly $6,000 to $18,000 a year,
more than average IQ humans.
Elected representatives are apparently
slightly smarter on average,
but not a radical difference.
Mensa isn't a major force in the world.
And if we look at people who evidently
have good cognitive abilities given their intellectual output,
their personal lives are not obviously drastically
more successful anecdotally.
So is it that much of a big deal?
Yeah.
So I think this is like a fair point.
If we looked in the world and IQ,
or whatever metric of intelligence you want to use,
it's really dramatically correlated
with everything good about someone.
I mean, IQ correlates with basically everything
you might value in someone's life,
because we live in an unfair world,
but not dramatically.
Yeah, so I think this is a valid argument.
I generally don't think you should model
human-level AI as like,
or like slightly superhuman AI as like an IQ 200 human.
Like, for example,
GPT-4, I would argue,
knows most facts on the internet,
or many facts.
And, yeah, knows many facts.
And this seems...
GPT-4 knows many facts.
And this is sure an advantage over me.
GPT-4 knows how to write a lot of code,
and it knows how to take software
and do penetration testing on it.
It knows lots of social conventions
and cultural things,
and has lots of experience reading various kinds of
text written to be manipulative,
or manuals on how to make nuclear weapons.
Sorry, I'm going too hard on the knowledge point.
There's just lots of different axes.
You can be human-level or better,
in which knowledge is one.
Intelligence or reasoning is one.
Social manipulation abilities is another.
Charisma and persuasion is another.
I think these two are particularly important ones.
There's forming coherent plans.
There's just like the ability to execute on stuff.
24x7 running thousands of copies of yourself in parallel,
distributed across the world.
There's running faster than humans.
And there's just like lots of dimensions here.
I think the IQ 200 human frame is helpful in some ways,
but unhelpful in other ways,
especially if it summons the nerdy scientist
with no social skills who's life is a mess archetype.
I'd say it's a nerdy scientist with no social skills
whose life is a mess.
Okay, yeah.
I mean, this is the thing is...
Because Rob said the same thing.
On chess, it's possible for someone to be
literally 20 times better than you,
that there's a huge dynamic range of skill.
And that's something we've not really seen
in human intelligence,
and it might be because of the way we measure it.
It's possible that the way we measure it
doesn't even capture people with broader
or better abilities.
Let's just cover her last point quickly.
So this is that the speed of intelligence growth is ambiguous.
So this idea that AI would be able to rapidly destroy the world
seems prima facie unlikely to Katia,
since no other entity has ever done that.
And she goes on.
So the two common broad arguments is that there'll be a feedback loop
in which intelligent AI makes more intelligent AI repeatedly
until AI is very, very intelligent.
Number two, small differences in brains seem to correspond
to very large differences in performance
based on observing humans and other apes.
Thus, any movement past human level
will take us to unimaginably super human level.
And the basic counter-arguments to that is that
the feedback loops might not be as powerful as assumed.
There could be diminished returns,
there could be resource constraints,
and there could be complexity barriers.
So maybe we should just do that kind of recursive self-improving piece first.
What do you think about that?
I don't really buy recursive self-improvement.
Oh, good.
It's not an important part of why I'm concerned about this stuff.
So generally, I just feel like a lot of the arguments were made
before the current paradigm of enormous foundation models.
When you're investing hundreds of millions of dollars of compute into a thing,
it's pretty hard for it to make itself substantially better.
And you can do things like design better algorithmic techniques.
I think that is probably one that is more likely to be accelerated
the better the model gets.
It's not clear to me how much juice there's to squeeze out of that.
But generally, I just think a lot of this is going to be bottlenecked
by hardware and compute and data,
such that I'm less concerned about some runaway intelligence explosion,
and I'm more just concerned about,
we'll eventually make things that are dangerous.
What do we do then?
And I think this is a really good fact about the world.
I think a world where you can have intelligence explosions is really scary.
And I feel like our current world is a lot less scary than it could have been.
If some kid in a basement somewhere just wrote the code for AGI one day.
Yes, yes.
Okay, well, I mean, just to finish off Katya's final point.
So the other point they made was about small differences might lead to oval.
It's a little bit like in squash.
I don't know if you've ever played squash,
but a tiny difference in ability leads to one player overwhelmingly dominating the other player,
because you just get these kind of like, you know, it's a game of attrition,
and you get these tipping points.
And she argued that that might not necessarily be the case when comparing AI systems,
because of three reasons.
Different architectures, likely to have very different underlying architectures
and biological brains, which could lead to different scaling properties.
Performance plateaus, so there might be these plateaus beyond which further increases in intelligence,
you know, don't lead to significant performance improvements.
And also this notion of task specific intelligence, something that I strong,
I believe that all intelligence is specialized as we were speaking about earlier.
And so it might be specialized rather than being generally intelligent,
and small differences thus may not translate into large differences in performance across a wide variety of tasks.
Maybe we should just touch on this, on this kind of task focus thing.
So I think humans are very specialized.
We have, and we don't realize that we are because the way we conceive of intelligence is anthropomorphic,
but actually we don't do four dimensions very well.
There's lots of things that we don't do very well,
and we're kind of embedded in the cognitive ecology in quite a complex way.
So what do you think about that?
Yeah, so I will, okay, I'll first comment on the general metodynamic of,
I think that people get way too caught up on philosophizing.
And no offense.
I'm so sorry.
And in particular, I care about whether an AI will cause a catastrophic risk.
I don't care about whether it fits into, whether it's general in the right way,
whether it has weaknesses in certain areas,
whether it's high on the Chomsky hierarchy,
or whether it's generally intelligent in some specific sense that someone like Gary Marcus would agree with.
Is that in any way a contradiction of your mechanistic sensibilities?
Because when it comes to neural networks, you want to understand how they work,
but when it comes to intelligence, you don't.
Oh, sorry.
I want to understand how it works.
I want to understand everything.
I just don't think it's...
I want to disentangle things to be concerned about from theoretical arguments about whether this fits into certain categories.
For the purposes of deciding whether to be concerned about AI existential risk,
I see all of the theory arguments as like a means to an end of this ultimate empirical question of,
is this a thing that could realistically happen?
And I think that these theoretical frameworks do matter.
Like, I don't know.
I think that an image classification model is basically never going to get the point where it's dangerous,
while a language model that's being RLHF'd to have some notion of intentionality potentially will.
And, yeah.
I know I can give random takes, but to me, if you're like,
AI's can be task-specific in the same way that humans are task-specific.
I'm like, well, a human is task-general enough that I think they could be massively dangerous in the right situation with the right advantages.
Like, if they wanted to be and were able to run the thousands copies of themselves at a thousand X speed or something.
I don't know if that's actually a remotely accurate statement about models.
Probably they can run many copies, but not a thousand X speed or something.
But, yeah.
Generally, that's the kind of question I care about, and I'm concerned many of these definitions lose sight of that.
And part of my thing of like, I want to keep alignment arguments as having as few assumptions as possible.
Because the more assumptions you make, the less plausible your case is.
And the less and like, more room there is for people to like, rightfully disagree.
I'm like, I want to be careful not to make any of the case rest on like, strong theoretical frameworks because we don't know what we're doing here.
Enough to have legit theoretical frameworks.
And I think that AI is likely to be limited in the same way that humans are, at least within the GPT paradigm.
If you're training it to predict the next word on the internet and a bunch of other stuff, then it's going to learn a lot from human patterns and human thought and human conventions.
But, I don't know.
In closing, you said that your personal favorite heuristic is the second species argument.
Can you tell us?
Yeah. So, I quite like Hinton's recent pithy quote of, there is no example of something being of some entity being controlled by things less smart than it.
And that was terrible.
Sorry.
I really would. I mean, Twitter went wild over that.
Oh, they're trying to go wild.
I mean, look at a company.
The CEO is usually done with that.
You have to hire competent people to have a successful company or look at my cat.
Yeah. Okay.
This is a terrible thing.
Let's just start again.
All right.
So, yeah, this is often called the gorilla problem.
Humans are just smarter than gorillas in basically all ways that matter.
Humans are not actively malevolent to gorillas, but ultimately humans are in charge gorillas are not and gorillas exist because of our continued benevolence or ambivalence.
And it just seems to me like if you are creating entities that are smarter than you, the default outcome is they end up in control of what's going on in the world and you do not.
And I kind of just feel like this should be the null hypothesis.
And then there's a bunch of arguments on top of like, is this a good model?
Well, obviously, there's lots of disanalogy is because we're making them.
We ideally have some control over them.
We're going to try to shape them to be benevolent towards us.
But this just seems like the default thing to be concerned about to me.
On that point, though, we are different from computers.
We scuba dive.
And that's actually quite a profound thing to say.
We scuba dive because we are integrated into the ecosystem, not just physically, but cognitively.
There's a kind of cognitive ecosystem that we're enmeshed in.
We have a huge advantage over computers.
Computers can't really do anything in the physical world.
So I agree with this, but I don't know.
I feel like the way, I don't know.
One evocative example is there was this crime lord, El Shapo, who ran his gang from within prison for like many years, very successfully.
When you have humans in the world who can get to do things for you, you don't need to be physically embodied to get shit done.
And I don't just look at Blake Lemoine.
There's no shortage of people who will do things if convinced in the right way, even if they know it's an AI.
And I do agree with you on that.
And I think part of the reason why we're going to have the inevitable proliferation of this technology is so many tinkerers will just create many, many different versions of AI.
And they won't really be thinking about the consequences of their actions.
But what's the alternative, paternalism?
Yeah, so to me, the main interesting thing here is large training runs as like the major bottleneck.
Very few actors can do them.
We're probably going to get beyond the point where people are even putting the things out behind an API open to many people to use,
let alone like open sourcing the weights, which we've already pretty clearly moved past.
And this, to me, seems like the point of intervention you need if you're going to try to make sure things are safe before you deploy them,
like track the people who are able to do these runs, have standards for what it means to decide a system like this is safe.
I'm pretty happy Sam Altman's been pushing that stuff very heavily.
And if competently done, I think this kind of regulation can be very important.
It could be great.
Like the alignment research has been doing great work here.
And I'm very excited to see what the red teaming large language models thing at Defconn looks like.
But I don't know, maybe to close, I feel like I've been in the role of why alignment matters.
Maybe I can try to break alignment arguments myself for a bit.
Please do, yeah.
So if I condition on actually the world is kind of fine,
probably my biggest guess is that the goal directed notion is just like not remotely a good understanding of how these things work.
And it's hard to get them to be goal directed.
And we just mostly coordinate and don't do that.
And these systems are mostly just like extremely effective tools.
It seems like kind of a plausible world we could end up in.
I don't think it's any more likely than, yep, they're goal directed and this is terrible.
We end up in a world which just has like lots of these systems that don't coordinate with each other,
want some more different things are like broadly aligned with human interests,
but like imperfectly, and just none of them ever get a major advantage over the others.
And the world kind of continues to be about as the world is with lots of different actors who aren't necessarily aligned with each other,
but mostly don't try to see over the world except every so often.
Or we just alignment isn't that hard.
We crack mechanistic interoperability.
We look inside the system.
We use this to iterate on making our techniques really good.
It turns out that doing RLHF with like enough adversarial training just kind of works.
Or with AI assistance to help you notice what's going on in the system.
And this just gets us aligned to the level systems and we can be like, please go solve the problem.
And then they do.
And I think people like Yadkowski are very loud about we are almost certainly going to die.
And we might, but we also might not.
I don't really know.
I would love to just become less confused about this.
And I remain very concerned about this to be clear.
But I'm not like 99% chance we're all going to die.
Yeah, but anything which is an appreciable percentage may as well be the same thing.
Yeah, pretty much.
Yeah, it's quite funny.
I got a lot of pushback on the Robert Miles show.
People said, oh, I can't believe it.
You framed him to be a doomer.
And he himself said in the show, I think about five times we're all going to die.
And I managed to cut about five.
Well, I don't exaggerate, but that there is at least two posts on Twitter within 15 minutes of that comment where he said, and we're all going to die.
So I don't think I don't think I'm being unfa...
Well, I didn't actually call him a doomer, but he basically is.
I don't know, man.
I hate ladles.
Like, Eliasar is clearly a doomer.
He's clearly a doomer.
Yeah.
Rob is much less doomy than Eliasar.
Yeah.
Is Rob a doomer?
I don't know.
I didn't call him a doomer.
But empirically, the data says yes.
Yeah, I mean, I don't know, man.
It sounds like you spend too much time reading YouTube comments.
I do.
Too much time.
But notoriously, the least productive use of time possible, apart from hanging out on Twitter, reading AI Flameless.
Twitter is the worst.
I know.
It's so bad.
I mean, we don't need to go there, but we were having a brief discussion before we started in record.
Why do you think otherwise intelligent, respectable people behave in that way?
Impulse control, social validation, it's just kind of fun.
People aren't very self-aware about how they look or aren't that reflective.
And Twitter incentivizes you to lack nuance and to be outraged about other people.
I don't know.
I am very sad by many Twitter dynamics, including from people who otherwise seem worthy of respect.
Yes.
Yes.
Interesting.
Look, Neil, this has been an absolute honor.
Thank you so much.
It's been extremely fun.
Yeah, it's been amazing.
It's been a marathon.
But thank you so much for joining us today.
And I really think we've had a great conversation and I know everyone's going to love it.
So thank you so much.
Yeah.
I apologize for all the times I saw you off for philosophizing.
Oh, no problem.
It's an honor.
Yeah.
All right.
Thanks for having me on.
