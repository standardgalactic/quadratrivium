WEBVTT

00:00.000 --> 00:02.000
Welcome to MLS T.

00:02.000 --> 00:06.800
Today we are extremely excited to have two distinguished guests join us.

00:06.800 --> 00:09.000
The first guest is Ellie Pavlik.

00:09.000 --> 00:13.600
She is an assistant professor of computer science at Brown University

00:13.600 --> 00:16.000
and a research scientist at Google AI.

00:16.000 --> 00:21.800
Her work focuses on building better computational models of natural language semantics and pragmatics,

00:21.800 --> 00:26.400
aiming to help computers understand language the way humans do.

00:26.400 --> 00:33.000
Our second guest, the legendary Raphael Millier, is the 2020 Robert A. Burt Presidential Scholar

00:33.000 --> 00:37.200
in Science and Neuroscience in the Center for Science and Society

00:37.200 --> 00:40.800
and a lecturer in the Philosophy Department at Columbia University.

00:40.800 --> 00:45.800
Raphael completed his D-Phil in Philosophy from the University of Oxford,

00:45.800 --> 00:52.000
where his work centered on self-consciousness and his main interests lie in the philosophy of artificial intelligence,

00:52.000 --> 00:55.000
cognitive science and the mind.

00:55.000 --> 00:57.400
Now, this is an interesting experiment for us.

00:57.400 --> 01:02.800
We've decided to get these two heavy weights, just one-on-one having a conversation with each other

01:02.800 --> 01:05.400
and we're hosting it here on MLS T.

01:05.400 --> 01:08.000
They spoke about compositionality and grounding,

01:08.000 --> 01:13.000
compositional generalization benchmarks, mechanistic understanding in language models,

01:13.000 --> 01:16.600
variable binding in transformers, language and vision models,

01:16.600 --> 01:18.600
compositional behavior in humans,

01:18.600 --> 01:22.200
compositional reasoning and negation in language models,

01:22.200 --> 01:25.200
variable binding in reinforcement learning and transformers,

01:25.200 --> 01:28.600
the difference between instruction tuning and RLHF

01:28.600 --> 01:33.200
and the benefits of RLHF, referential grounding and language models.

01:33.200 --> 01:39.400
And yeah, the Chomsky skepticism, of course, you know, our friend Stephen Piantodosi's paper,

01:39.400 --> 01:44.600
inductive biases in language learning, language models in different languages

01:44.600 --> 01:49.400
and indeed the future of academic work in language models.

01:49.600 --> 01:53.800
Now, the audio from Raphael in particular wasn't as good as it could be.

01:53.800 --> 01:56.600
I've done my very best to process it and improve it.

01:56.600 --> 02:01.800
Just to help folks follow along, I've kind of generated some subtitles.

02:01.800 --> 02:04.600
The subtitles on Google are absolutely rubbish

02:04.600 --> 02:08.800
and speech recognition technologies come along so far in the last few years.

02:08.800 --> 02:12.800
So, I've generated some better subtitles using another service

02:12.800 --> 02:14.600
and I've superimposed it on the top.

02:14.600 --> 02:17.000
I've also superimposed some descriptive titles on the top

02:17.000 --> 02:19.600
just to help you folks follow along at home.

02:19.600 --> 02:24.600
So, anyway, without any further ado, I give you Ellie Pavlik and Raphael Billier.

02:24.600 --> 02:25.200
Enjoy.

02:25.200 --> 02:26.000
Hi, Ellie.

02:26.000 --> 02:27.800
I know Raphael. Hi, how's it going?

02:27.800 --> 02:30.000
I guess you talked to Tim before,

02:30.000 --> 02:34.400
so maybe you have a kind of more of a context of the previous conversation that started this one.

02:34.400 --> 02:39.200
So, I'll let you decide where we're beginning, what the first topic is.

02:39.200 --> 02:44.600
Yes, so we had a chat back a few weeks or half a year ago

02:44.600 --> 02:47.800
when I came on the podcast and we had to get it short and you thought,

02:47.800 --> 02:52.600
maybe if I come back home, we should do it as a discussion.

02:52.600 --> 02:55.600
And I think you had to chat with you.

02:55.600 --> 02:58.200
So, Tim thought that maybe some of the topics we could discuss

02:58.200 --> 03:02.000
would be equal positionality and brown links with both versus your dads.

03:02.000 --> 03:03.600
Seems natural, yeah.

03:03.600 --> 03:04.200
Yeah.

03:04.200 --> 03:09.800
So, I don't know whether we should try to disagree more than actually do

03:09.800 --> 03:12.000
because I think we were aligned with a lot of the topics,

03:12.000 --> 03:13.600
but I'm sure there are some topics.

03:13.600 --> 03:21.600
Yeah, but I mean, I also feel like for both of them, I know what I currently think,

03:21.600 --> 03:25.800
but I also am pretty prepared to just have to renege in a couple of years

03:25.800 --> 03:26.800
and be like, I was wrong.

03:26.800 --> 03:29.000
So, I feel like we can see both sides.

03:29.000 --> 03:32.400
So, we can definitely simulate some disagree or we don't have to disagree,

03:32.400 --> 03:35.800
but we can argue both sides of whichever issue.

03:35.800 --> 03:37.000
I don't know where do you want to start?

03:37.000 --> 03:40.600
Do you want to talk grounding or do you want to talk compositionality?

03:40.600 --> 03:44.000
I think I have more immediate questions with respect to compositionality.

03:44.000 --> 03:47.800
So, one thing I was wondering is how do you see the progress

03:47.800 --> 03:50.400
on compositional generalization benchmarks?

03:50.400 --> 03:55.000
So, just for the listener of yours,

03:55.000 --> 03:58.000
it's really hard to assess whether large language models

03:58.600 --> 04:05.600
on huge corpora actually acquire the capacity to generalize compositionally properly

04:05.600 --> 04:09.600
because you can never really know what's in the training data with your models.

04:09.600 --> 04:14.600
Before, you can never really know whether they're the memorized structures and so on.

04:14.600 --> 04:22.600
So, what strategies to use, synthetic datasets, such that you try them on a test set

04:22.600 --> 04:25.000
and on a trans set and then when you test them,

04:25.000 --> 04:30.000
the only way that you can achieve this goal is generalizing perfectly.

04:30.000 --> 04:36.000
And so, there has been some, the initial results from some of these datasets and benchmarks,

04:36.000 --> 04:42.000
like Scars, Cards and others, were a little bit mixed with LSTMs and formers

04:42.000 --> 04:47.000
and lately they have been a part of the steady improvements

04:47.000 --> 04:52.000
past due to tweaks in the architecture and I remember having a discussion with Tal,

04:52.000 --> 04:57.000
Linsen, when he organized this composition of the workshop that he was speaking of.

04:57.000 --> 05:02.000
And one of the, one of the contentious points is whether we could call these tweaks

05:02.000 --> 05:04.000
or whether these are significant changes.

05:04.000 --> 05:09.000
And that's always the crux of the debate, I think, including also with things like

05:09.000 --> 05:16.000
Paul Smolensky, Paul Smolensky's approach that has an explicit transfer product of presentations is

05:16.000 --> 05:21.000
how much of a hand engineer tweak you make in the architecture

05:21.000 --> 05:27.000
to solve this composition generalization problem and how much do we need.

05:27.000 --> 05:30.000
So, I was just wondering where you step on that.

05:30.000 --> 05:37.000
That's a super interesting, I mean, so, yeah, so on the composition generalization test,

05:37.000 --> 05:41.000
I mean, I probably have like an unsatisfying middle ground opinion.

05:41.000 --> 05:42.000
I'm curious where your thoughts are.

05:42.000 --> 05:48.000
So I think, I think we're both pretty interested in the kind of mechanistic stuff right now.

05:48.000 --> 05:54.000
So I guess for the audience, this is this idea that like this idea of trying to kind of understand

05:54.000 --> 05:57.000
what the models are doing kind of under the hood.

05:57.000 --> 06:00.000
So when we think about the, I guess, compositional generalization tasks,

06:00.000 --> 06:05.000
it's like we have some inputs, what are the training, like what is the training model it gets

06:05.000 --> 06:08.000
and then what are the outputs that it produces.

06:08.000 --> 06:14.000
And that's really the data that we're basing our claim about whether it's compositional or not on.

06:14.000 --> 06:20.000
And the kind of mechanistic or the other approaches to try to characterize

06:20.000 --> 06:23.000
what actually is the process it used to get from the inputs to the outputs.

06:23.000 --> 06:27.000
I really like Chris Ola, who I think coined the term mechanistic,

06:27.000 --> 06:30.000
uses the phrase of kind of trying to understand the source code of the model.

06:30.000 --> 06:37.000
So it's like you have, you want that kind of something like a kind of human understandable description

06:37.000 --> 06:41.000
of the algorithm that it's running under the hood.

06:41.000 --> 06:46.000
And so like I've just, I've been super hung up on that.

06:46.000 --> 06:53.000
Like I think all of my projects, all of like what my students are working on are some flavor of that

06:53.000 --> 06:57.000
because to me, I think the reason it's so, I think it's interesting,

06:57.000 --> 07:03.000
but it also just feels like the questions about things like compositionality are almost stuck right now

07:03.000 --> 07:05.000
without that level of description.

07:05.000 --> 07:08.000
So if we're just looking at what are the inputs and what are the outputs,

07:08.000 --> 07:12.000
I just feel like we're, it's just going around in circles with people like,

07:12.000 --> 07:14.000
like we're not really making progress on the issue.

07:14.000 --> 07:19.000
There's kind of people who are inclined to agree and inclined to disagree.

07:19.000 --> 07:27.000
And so from my perspective, like, okay, so if we have the model that's doing this kind of, I guess,

07:27.000 --> 07:30.000
quasi generalization, it's like succeeding on some cases,

07:30.000 --> 07:34.000
not perfectly compositionally generalizing in the kind of really abstract case

07:34.000 --> 07:37.000
that those data sets tend to be going for.

07:37.000 --> 07:43.000
But it's doing something in between and we're trying to figure out whether that counts as compositional or not.

07:43.000 --> 07:48.000
It seems like that just hinges on what it's actually doing under the hood and how it's doing it.

07:48.000 --> 07:56.000
So I guess I'm like, I'm like basically like neutral on or not paying attention

07:56.000 --> 07:58.000
to compositional generalization data sets right now.

07:58.000 --> 08:01.000
And I recognize like a very fair criticism.

08:01.000 --> 08:04.000
And I feel like when I've talked to people like tall, probably have this word.

08:04.000 --> 08:06.000
It's like you can't just sidestep the issue.

08:06.000 --> 08:09.000
It feels like being overly generous to the neural networks, right?

08:09.000 --> 08:10.000
It's like they're not doing that well.

08:10.000 --> 08:12.000
And then you like change the game a little bit, right?

08:12.000 --> 08:14.000
You're like, oh, well, that's not even the metric we care about.

08:14.000 --> 08:16.000
That's not really what I would see as the goal.

08:16.000 --> 08:20.000
It's just like in the immediate term, it seems like first we want to characterize what's happening under the hood.

08:20.000 --> 08:24.000
And then we can come back to those data sets and understand them much more in depth.

08:24.000 --> 08:29.000
And then when we see how they're solving it or not solving it, we can like,

08:29.000 --> 08:35.000
it gives us a much more concrete thing to analyze and try to ask whether that counts as compositional or whether that's at all human like,

08:35.000 --> 08:37.000
if that's what we care about.

08:37.000 --> 08:44.000
Like it, like, I guess to me it feels like a dead end if we're not allowed to comment on the procedure that happened in between.

08:44.000 --> 08:49.000
And right now we can't comment on the procedure that happened in between inputs and outputs because we just don't know what's happening there.

08:49.000 --> 08:54.000
So I don't know, I guess that's my current take on compositional generalization data sets is I'm like,

08:54.000 --> 08:56.000
now is not the right time for them.

08:56.000 --> 09:01.000
We'll come back to them later, which I recognize seems like a dodge, but it's not meant as a dodge.

09:01.000 --> 09:04.000
It's like, basically, we'll come back to this later.

09:04.000 --> 09:08.000
Yeah, but I'm curious what you think about them.

09:08.000 --> 09:10.000
Yeah, I mean, I'm very sympathetic to that view.

09:10.000 --> 09:19.000
And as you know, I'm super interesting to mechanistic that visibility just to looking at positionality.

09:19.000 --> 09:29.000
And I guess in the background, there is lurks this debate about whether humans themselves have something like a perfectly compositional language of thoughts or something else.

09:29.000 --> 09:30.000
Right.

09:30.000 --> 09:31.000
Absolutely.

09:32.000 --> 09:42.000
And so perhaps, you know, we might, we might learn some things about computations implementing a human cognition by looking at this imperfect completion of systems.

09:42.000 --> 09:56.000
Absolutely. I mean, I think that's, that would be really my, we just like so right now, and this is a gross oversimplification of where the two viewpoints are, but like, the two really concrete like

09:56.000 --> 10:08.000
kind of options on the table for like what a system can be is like this pure symbolic language of thought and that the language of thought it would be something like humans in their heads have something like a Python program language,

10:08.000 --> 10:20.000
like a perfect kind of formal system for reasoning over symbols in this compositional way, or it's like this loose, or like these associations, these idiomatic kind of just things back together.

10:20.000 --> 10:31.000
And like, I think most people would assume or something in between and there's not a really good proposal for what the in between is right like there's all kinds of ways of being in between those two things.

10:31.000 --> 10:38.000
And so I feel like whatever we find in the neural networks gives us some kind of concrete proposal like here's an example of an in between.

10:38.000 --> 10:39.000
It's not the language of thought thing.

10:39.000 --> 10:44.000
They're often also obviously not just giant lookup tables right they're doing something more than that.

10:44.000 --> 10:50.000
So whatever they're doing in between it's at least like a candidate for what could be happening right that's pretty exciting.

10:50.000 --> 10:58.000
And it might not be the right one but at least it's something because I haven't seen like a really satisfying candidate for what the in between is right.

10:58.000 --> 11:02.000
Like a lot of the debate still seems to kind of put these two strawmen up against each other.

11:03.000 --> 11:04.000
Yeah, I agree.

11:04.000 --> 11:22.000
And I think some of you with some of the people who are pushing back against this kind of really symbolic thing with the thoughts architecture that Paul Smolensky, the self-faith that you need to build into your connections model, some more explicit compositional structure with these

11:22.000 --> 11:33.000
kinds of products, but these these vectors, rodent field of vectors that are never formal, but you can combine with transfer part of operations and there are other vector symbolic architectures like this that do it like that.

11:33.000 --> 11:38.000
And my qualm is always with with this and I have to talk about this.

11:38.000 --> 11:51.000
I think, you know, he's open to the possibility that perhaps transformers, you're handling compositionality in the way transformers handle these might turn out to be enough perhaps but my calm is that if you if you build, you know, if you kind of hard code.

11:52.000 --> 12:07.000
Into your network architecture, or you do some future engineering to the input vectors being your first and all that seems just to be ad hoc to me right so you need to specify what are the rules what are the fillers and that's that seems like an

12:07.000 --> 12:14.000
Inclosable model for how it works unless you want to say that they are in its academic concepts and we just do a lot.

12:14.000 --> 12:15.000
I mean, a lot of people do.

12:15.000 --> 12:16.000
Yeah.

12:16.000 --> 12:17.000
Right.

12:17.000 --> 12:26.000
That's like a claim that I think a large fraction of cognitive science is very happy to say is the case is that there's an inventory of innate atomic concepts.

12:26.000 --> 12:27.000
Right.

12:27.000 --> 12:30.000
But but yes, I agree.

12:30.000 --> 12:47.000
I think maybe it's the computer scientists got or something something about that is unsatisfying or at least you want a story of where those came from, which still seems like you somehow it needs to emerge or come from data or come from some kind of pressure other than just we got lucky and they were there.

12:47.000 --> 12:49.000
I changed the right inventory of concepts.

12:49.000 --> 12:59.000
I mean, there's some daylight between the core knowledge knew that, you know, there are some some basic concepts and the four orient view that all atomic concepts must be in it because right.

12:59.000 --> 13:01.000
Yes, which is born with this.

13:01.000 --> 13:02.000
That's part of it.

13:02.000 --> 13:03.000
And they're on the ball.

13:03.000 --> 13:04.000
Yeah.

13:04.000 --> 13:05.000
Right.

13:05.000 --> 13:15.000
And it seems that if you if you want to handle all compositionality with these ad hoc world and filler vectors or something along these lines, then.

13:16.000 --> 13:34.000
It seems that you're going in that direction. And what I'm excited about which forms formers is that it looks like they can do something that in my mind, if you look at the kind of secretive research researcher looks quite like variable binding by reading and writing information to sub spaces of the main embedding

13:34.000 --> 13:37.000
space and using that as a virtual content principle memory.

13:37.000 --> 13:41.000
But it's fuzzy right it's not just implementing a classical symbolic.

13:41.000 --> 14:02.000
That's always the charge that I think that the classes are never against the connections is that oh if you're if your model is working well and if it's if it's a good model of human behavior and then presumably is just implementing classical symbolic architecture and it's right, right, right, right, right.

14:02.000 --> 14:23.000
I mean, almost I, I'm teaching a class this spring on language processing of humans and machines and we were talking about this question of implementing a classical architecture and it's actually quite hard to tell even in the historic debates on this like what what

14:23.000 --> 14:37.000
people are even claiming about this right like there's like simultaneously claims that like oh it'll like sure it can implement it but also nothing it ever does whatever count as an implementation like I just can't even tell what the what actually the consensus is.

14:37.000 --> 14:47.000
But I agree I think we'll get something that the ideal is that when we look into the transformers will find something that preserves the really necessary pieces of it.

14:47.000 --> 15:02.000
But is different enough to be interesting I mean, I don't know I guess it in some ways if what we did if what we ended up finding was something that was identical to the classical architecture that would be like, I think a huge win for classical architecture.

15:02.000 --> 15:09.000
People right that the only way these transformers a were able to solve it was by learning to implement the thing they said that you would need all along.

15:09.000 --> 15:20.000
But I think it's unlikely that we would find that right like I don't really know exactly how that would even work on so we'll get something like this fuzzy version, which, which I think would just be fascinating.

15:20.000 --> 15:36.000
I mean, I'm curious so the variable binding stuff so I'm generally take the position that the transformers these other logical networks will be able to implement like these core kind of symbolic operations will be able to replicate this kind of behavior.

15:37.000 --> 15:43.000
And then when we kind of dig under the hood will be able to find these clever implementations of these things.

15:43.000 --> 15:50.000
But variable binding is one of these things where we've actually really struggled to find good evidence of it.

15:50.000 --> 16:11.000
And I guess particularly so in language models it almost seems like you have to say it's there by like by Fiat because of some of these the ability of the models to do this compositional generalization stuff like they, they, I think Tom McCoy had a really cool paper where

16:11.000 --> 16:23.000
I was showing that they could generate some like syntactic structures that were unattested in training. So for something like that you're like well I guess to do that you would have to have something like a abstract filler and roll binding.

16:23.000 --> 16:29.000
But especially what we've played around the language and vision models and maybe it's just that clip kind of sucks.

16:30.000 --> 16:41.000
I guess clip is the one of the pre trained link image and text models that kind of is trained to map images to to text captions.

16:41.000 --> 16:51.000
But we've had several projects of trying to show, I guess clip and then the image generation models based on top of it like Dolly and state things like that.

16:52.000 --> 17:07.000
Like we just can't get any evidence that it's doing anything clever or abstracting away from the from the structure so that's where we'll look for stuff like a red cube in front of a blue cube and then blue cube in front of a red cube or things like the, I guess the famous on Twitter like

17:08.000 --> 17:21.000
astronaut riding a horse a horse riding an astronaut. And like, even with really controlled cases and doing it in large days that like we just can't get it to do anything that gives us any data to point to to be like look it's doing okay.

17:21.000 --> 17:24.000
But maybe it's just a clip problem maybe just sex.

17:24.000 --> 17:40.000
I think it's, I think it's a clip problem so that's that's, I wanted to talk about this because you have this really interesting paper about it and I think we can't generalize any finding about vision language models based on contrastive learning clip to say, language models train or to

17:40.000 --> 17:59.000
say, there's this recent paper from Stanford configurably first author is like which which confirms my suspicions basically the ideas that click treats ends up treating text like just by the bags of words, because in order to just to fulfill the contrastive learning objectives of bringing

18:00.000 --> 18:23.000
the captions and measures that go together and further away in the space the ones that don't go together. You don't need to actually induce much about syntax, right, just keywords is sufficient and and generally also in the captions themselves you don't have a ton of information about, say, like their, their relative positioning of different objects or how many of, you know, it's not like you say, they are key forks and one and one knife or just say

18:24.000 --> 18:38.000
something in the caption. Right, right, right. And if you look at all the models like party from Google, other image nourishment models that is a pre trained as a text and currently is a pre thin language model, they actually do way better at things like much better.

18:38.000 --> 18:42.000
Yeah, yeah, yeah. Yeah. Yeah.

18:42.000 --> 19:01.000
Yeah. And I think that surprised me in trying to do so we did the red cube blue cube kind of stuff because that's like super abstract and it's easy to do. But one of my other students Charlie Lovering is working on a project with some of the image generation models to do to try to kind of more systematically look at the, you know, horse riding an astronaut kind of example.

19:01.000 --> 19:21.000
And one thing that struck me with it is just how like how not uniformly distributed the world is right like it's just like finding examples of relations, like where you want to have like argument one relation argument to that can actually exist in both directions.

19:22.000 --> 19:37.000
And that's a thing that a human could visualize. We like, it's really, really hard to do like we're restricted to a really simple set of relations because we like spit out a whole ton of things and tried flipping the order and you're like, yeah, this isn't a thing I could actually imagine or would expect a human to be able to

19:38.000 --> 19:53.000
to realize in any kind of way. And so that I think getting at kind of revisiting our assumptions about how compositional humans are I think that's like kind of relevant data to have is like how often are we actually forced to combine completely

19:53.000 --> 20:08.000
novel elements in a purely abstract way without anything that we can relate it to. Like what we think we're our kind of working theory for what the model is doing for these kinds of things is like if you ask for a horse riding an astronaut, it's like finding the most similar thing it's seen.

20:08.000 --> 20:18.000
Right. It's like, well, here's an example of like, you know, a teddy bear riding a puppy dog or something and then morphs it into a horse ride, like some other type of thing.

20:18.000 --> 20:25.000
And it's like, that's not actually an absurd model to assume might be underlying some kind of compositional behavior in humans too.

20:25.000 --> 20:39.000
Yeah, it might be of the this might be for with respect to language models that friends all these kinds of effects on reasoning where you know you test them on syllogism solving the Western selection tasks you get you get these effects that are similar.

20:40.000 --> 20:49.000
You know, also fun in humans where if you're feeling the story of the task with plausible details that could apply to realize that they did much better.

20:49.000 --> 20:50.000
Right.

20:50.000 --> 20:53.000
You just use this like farfetched, you know, abstracted novels.

20:53.000 --> 20:55.000
Right.

20:55.000 --> 21:08.000
Right. Yeah, and that's that's been shown like I know this is, I don't remember the authors but there's pretty classic study about the kind of logical syllogisms right like if you ask people to reason about these just like, you know, famous modus ponens or whatever.

21:08.000 --> 21:24.000
When you ask them like a not a like people suck. Yeah, they don't know how to do it but if you put it in realistic real world scenario like you're at a party you can have ice cream and cake or whatever then people do totally fine and I think it.

21:24.000 --> 21:33.000
It wouldn't be surprising that we might have these that we would have evolved to be able to reason about realistic scenes and not abstract ones.

21:33.000 --> 21:50.000
Yeah, and the, the, I think the skill to be able to reason over the most, the more abstract examples of including with images and those just in a possible way is something probably require through training and people of, you know, who have two children,

21:50.000 --> 22:05.000
for example, are probably familiar with that you kind of need to some prompt engineering as a word to, to get them to actually, you know, draft certain concepts or how to, you know, teaching logic, even to the grads.

22:05.000 --> 22:19.000
Yeah, you get a lot of resistance from this content effects and to be prime institutions in the right way. And I would I wonder whether you know with the astronauts running a horse examples, whether it would be interesting to try that with with small children,

22:19.000 --> 22:24.000
and see whether, you know, whether they actually do well at that test.

22:24.000 --> 22:34.000
Yeah. Yeah, I mean, I would have to assume they would right so that's where like my intuitions are strongly like but humans are quite compositional like you would.

22:34.000 --> 22:39.000
You would have to assume that they would know how to.

22:39.000 --> 22:51.000
They wouldn't just draw a horse riding, or if you asked for a horse, they wouldn't just draw astronaut riding a horse, right. They might like giggle and be like that's silly courses don't write something right but.

22:51.000 --> 23:08.000
Yeah, I mean, I guess so I guess some counter examples to both of our intuitions so that there's some of this variable binding stuff but with the image models and I'm kind of like I'm willing to give the models a pass at least in the immediate term while we figure out what's going on because

23:08.000 --> 23:17.000
and like yeah I think some of this perfectly abstract compositionality might be like a tall order and not something we would have to do.

23:17.000 --> 23:34.000
But the other thing that's always been weird to me so like I'm channeling Roman Feynman my colleague here who's like much more language of thought tradition and you know he'll say that you know of course humans are like you know better at like he's aware of all the data that people

23:34.000 --> 23:48.000
aren't as good at logical reasoning in some settings but they can do this other sense and things like that but it's like that doesn't undermine that humans have logical capacities like we use negation freely all the time like we don't really struggle with that.

23:48.000 --> 24:02.000
But the language models actually still kind of suck at negation like it's pretty easy to just write something in a slightly odd way and get them to ignore that you negated something I think even fairly recently I guess I haven't tried this in the past month so

24:02.000 --> 24:19.000
maybe it's fixed but like I asked something like asked GPT or chat GPT I think for like a recipe that like uses tofu and nutritional yeast but isn't vegetarian or something like that and it just spits out some vegetarian recipes or something

24:19.000 --> 24:35.000
like you know you sugar and lemon but not a dessert and it's like have you tried lemon bars and stuff so it's like it just kind of ignores it and that's weird to me that seems like for a language model doing a language modeling task like that's relevant like

24:35.000 --> 24:55.000
this isn't like a super trick out of distribution thing so that's kind of like I guess a thorn that like I feel like that's where I say like I have a bit of a caveat where I wouldn't be surprised if a couple years from now I have to be like yeah I was wrong the models are not at all like human like or something

24:55.000 --> 25:08.000
I'm hoping that's not the case but these kind of data points are like yeah that's it's frustrating how bad they are with negation or other kind of basic things like that.

25:08.000 --> 25:27.000
Yeah I mean it's always a moving target of course because people maybe said GPT-4 can handle that for example but I agree it's definitely very unsatisfactory to still his failures as some compositing of the problems.

25:27.000 --> 25:43.000
Yeah like we have some I guess so on one of these projects with Roman in this undergrad we're working with Alyssa we and this is just super frustrating data for me although actually it is getting better with the bigger models so maybe it is.

25:43.000 --> 26:06.000
But for GPT-3 it was like we have this very basic task that they had run on humans so you say something like it gives a little scenario and then you say you know it's like some scenario about scientists running experiments on rats and it's like the scientists saw that none of the rats liked the food or something.

26:06.000 --> 26:19.000
And then it's like now that they knew that some of the rats liked the food and they like did human reading time and saw that people showed a slow down and were surprised by the word some in that case because it's a blatant contradiction to what was just said.

26:20.000 --> 26:44.000
And when we use like GPT-3 and predicts surprise there's like no surprise whatsoever no suggestion of any up being at all bothered by the word some in that context which is weird because that's just a straight up language modeling task it's just what is the probability of this word in context which the human data was very clearly like it's low and the model was like it's fine.

26:44.000 --> 26:57.000
And then but then we had we tried with GPT-4 and then the numbers it looked better but it was kind of messy because we can't get perplexities of GPT-4 so you had to ask it to fill in the blank is a little bit different so it's hard to read.

26:57.000 --> 27:00.000
API was just the Chesapeake version.

27:00.000 --> 27:05.000
The with GPT-3 we had the API.

27:05.000 --> 27:09.000
Because now GPT-4 is available by the API as well.

27:09.000 --> 27:12.000
Yeah but it doesn't we need to check.

27:12.000 --> 27:13.000
Okay.

27:13.000 --> 27:17.000
Yeah we need to check last we checked we couldn't get perplexities out like we had the API but.

27:17.000 --> 27:18.000
I see.

27:18.000 --> 27:22.000
Yeah but yeah it's one of these weird things.

27:22.000 --> 27:34.000
Yeah and so like maybe GPT-4 is better but it wasn't as clean of us comparison to the humans and and it was even for a model like GPT-3 like it was surprising that it was so bad at that and like I don't know.

27:34.000 --> 27:45.000
So I feel like there's a few of these data points that are like this story isn't a slam dunk like there's some of these things that really should be easy for a model with basic structure.

27:45.000 --> 27:46.000
You know.

27:46.000 --> 27:53.000
Yeah it's surprising to me because there are you know this that is been telling others right that's even way smaller models.

27:53.000 --> 28:04.000
Can exhibit the right behavior in terms of surprise all when you look at things like subject development agreement field of gap dependencies you know I don't stress on these and.

28:04.000 --> 28:11.000
You know the right range of some tactics and they exhibit the same patterns of humans in terms of being surprised when the.

28:11.000 --> 28:12.000
Right.

28:12.000 --> 28:13.000
Subject things like that.

28:13.000 --> 28:16.000
Even if you're very not structured because of the structures to.

28:16.000 --> 28:17.000
Right.

28:17.000 --> 28:20.000
So why is negation right.

28:20.000 --> 28:31.000
Totally it seems much much simpler right like it's not like if anything I feel like negation plus these quantifier terms like you could just enumerate a table and say these things.

28:31.000 --> 28:39.000
Can go together and can't go together like it's I mean there's like a little more semantic cushion around it you have to know who the who's being modified or whatever but like.

28:39.000 --> 28:44.000
I'm quite sure models can do all of that so I was very surprised it might be just that it's a really.

28:45.000 --> 28:53.000
Infrequent thing in the training data in the input but then you have this poverty of the stimulus argument that you have to account for right is like.

28:53.000 --> 29:08.000
I mean maybe it is that this kind of entailment relation is just not frequent on the Internet but it is in kids input or I you know I don't know or maybe it is that the models are just not the right models for this task I don't want to believe that though I think there's

29:08.000 --> 29:10.000
I think there's must be some other reason.

29:10.000 --> 29:24.000
Yeah, I mean there was this paper by Will Merrill that shows that entailment semantics can be induced by an idealized ideal language model on synthetic data up to a certain sentence length.

29:25.000 --> 29:32.000
In real in real world scenarios there's only given the size of the three.

29:32.000 --> 29:39.000
I think perfect in terms of that it's going to be induced for me to this formal truth to send this is the four words.

29:39.000 --> 29:42.000
But if you want perfect absolutely you know perfect.

29:42.000 --> 29:43.000
Right, right, right.

29:43.000 --> 29:49.000
You exactly know whether or not sometimes in the next intelligent right which I don't know that I believe in entailment.

29:50.000 --> 29:56.000
It's like the logic stuff you're like okay we can come up with these like toy domains in which we all agree about entailment.

29:56.000 --> 30:16.000
But like during my PhD I did a lot of work on entailment and just trying to collect entailment judgments on humans is a nightmare right like they do not behave to the point that you have like that's what I feel like that was like a switching point for me where I was like okay maybe maybe I should accept that we have to trust them.

30:16.000 --> 30:17.000
But entailment is entailed.

30:17.000 --> 30:22.000
And as you idealize Gressian speakers as well humans are not.

30:22.000 --> 30:23.000
Yeah, no, no.

30:23.000 --> 30:31.000
But it's it's intriguing because it does suggest that I mean you can learn something about entailment semantics from distributional information.

30:31.000 --> 30:35.000
Even you know entailments in a loser sense like in the non perfect right sense.

30:35.000 --> 30:40.000
So again, why is negation so hard is is.

30:40.000 --> 30:45.000
Yeah, yeah, I guess I haven't seen a really good study on just.

30:47.000 --> 30:57.000
The distribution of negation in the like in a models training corpus like how is it used in what context because I don't.

30:57.000 --> 31:00.000
Um.

31:00.000 --> 31:11.000
Yeah, like it might just be that it's actually it's just not distributed the way we kind of think it's distributed it just functions very differently in written text in general.

31:11.000 --> 31:20.000
Yeah, like I do think that perhaps kids would learn it like kids get a very different input of negation than what I would imagine is on the internet.

31:20.000 --> 31:21.000
Right.

31:21.000 --> 31:26.000
Like I in academic writing I use negation only in the most convoluted ways like.

31:26.000 --> 31:38.000
Like it with like these triple negations that are right you know like while it is not unreasonable to assume that such is not the case right like that's the way I would use negation I wouldn't say like that's not a dog.

31:38.000 --> 31:40.000
Yeah, no one writes that.

31:40.000 --> 31:47.000
Yeah, and also it's interesting because in terms of the text that's actually generated language models.

31:47.000 --> 31:59.000
I don't think you know even with you three in my experience you see negation error is like I can even remember when when they generate text they use negation properly it's more when the policy comes somehow.

31:59.000 --> 32:04.000
Sometimes they know of negation as if they try to maximize the relevance of every word.

32:04.000 --> 32:09.000
So if you mentioned if you say I want you to give me recipes but not in the paprika.

32:09.000 --> 32:13.000
They see the word paprika and then there's like I need to maximize the you know.

32:13.000 --> 32:16.000
Yeah, yeah, but right.

32:16.000 --> 32:19.000
And it might be.

32:19.000 --> 32:31.000
I so when I was playing around with the recipes and stuff it did seem to change a lot I mean it was anecdotal but on on the wording so like if you marked it a lot more right like.

32:32.000 --> 32:45.000
Like if you said something like a recipe with these things but include me it does fine right so if you say not vegetarian it gets confused also if you'd say like and not vegetarian versus but not vegetarian and like where you put like.

32:45.000 --> 32:57.000
Like where you whether you front loaded or back loaded certain information it made a difference and so you could imagine there's these distributional signals where it's like when people are saying don't do this thing they market in a few ways right they don't just like.

32:57.000 --> 33:03.000
Slip in the negation but otherwise have the sentence read exactly like it would in the positive case.

33:03.000 --> 33:11.000
Like this probably a reason we have but as a conjunction and not and right is just to like help emphasize so that people don't miss that negation piece of it.

33:11.000 --> 33:12.000
Right.

33:12.000 --> 33:22.000
So like you could imagine something like that that it's like the model has very little incentive to emphasize the negation unless there's other signals that you really are right.

33:22.000 --> 33:26.000
So you wouldn't like be deathly allergic to Paprika and then just like slip it in.

33:26.000 --> 33:32.000
Give me a recipe for chicken Paprikash without Paprika or something and then just like yeah.

33:32.000 --> 33:33.000
Yeah I know that.

33:33.000 --> 33:40.000
So so so one other thing that this article was writing about and viable binding that I wanted to just keep up there's this.

33:40.000 --> 33:45.000
Preprints that I don't think I've got published.

33:45.000 --> 33:52.000
And on our size that's an alleges that you know they looked at viable binding interest formers.

33:52.000 --> 34:03.000
And I can't exactly remember the methods, but the alleged that they found that transformers can't really do viable binding unless it's by using the output as an external memory.

34:03.000 --> 34:12.000
And it gives to some of the discussion that we had at the conference that I co-organized with Dave Cheneze and a few signals.

34:12.000 --> 34:22.000
Nick Shea made a somewhat similar claim that you know the kinds of what he calls non content specific reasoning that transformers can do.

34:23.000 --> 34:27.000
He's always propped up by reasoning on the output.

34:27.000 --> 34:40.000
So using using the generated work, like saying China China China from saying you just reason step by step and use use the steps in the generator steps to crutch to solve problems.

34:40.000 --> 34:46.000
And maybe there is also this paper is very you've you've written structural and sub routine.

34:47.000 --> 35:01.000
Yeah, I haven't had chance to read that one but but I just wanted to ask you because I was striking to me that you can do some things zero shots with kind of language models that seem to fly in the face of that kind of thing.

35:01.000 --> 35:08.000
So for example, would you be for you can you can tell it's behave like a python shell.

35:09.000 --> 35:25.000
And then define a function with a bunch of variables and then a little bit later in the interaction just, you know, call that function for specific values and and say, you know, and because it's been having like a python shell it just has to give the answer zero shot it's not able to do some

35:26.000 --> 35:39.000
and it can do this pretty reliably and there's some family modes but the fact that you can do it that's all just me that you can has to be able to internally right make variables.

35:39.000 --> 35:45.000
Absolutely. Yeah, I yeah I love that example that you gave in the workshop.

35:45.000 --> 35:53.000
So at this philosophy of deep learning workshop because you had that like you had it behave like a python shell and do this Fibonacci sequence.

35:54.000 --> 36:02.000
The language model like we just ask the language model to predict or to tell you the, you know, seven hundredth Fibonacci number it messed it up right.

36:02.000 --> 36:21.000
Yeah, that was like super interesting and yeah I guess the thing we like fight with with these models is that they're, you know, they're like we do know that they have that they've trained over the whole Internet and so they've learned to like kind of sub spaces right and they're like drawing from these different

36:21.000 --> 36:32.000
domains and it's makes it really hard to interact with them because they can always from anthropomorphizing but they can always pull this like oh sorry I didn't understand the question game right like I thought I was being my red

36:32.000 --> 36:43.000
itself but it turns out you wanted me to be my New York time self or something like and it could always pull that so it could be like oh yeah no I know how but you have to ask me to act like a python shell because people on the Internet don't know the Fibonacci number.

36:44.000 --> 37:00.000
And this is a little bit of like for the for the critics of large language models this is a frustrating game to play because like like this is always a move that the proponents of language models can make is like oh you didn't ask it right like it knows how but it

37:01.000 --> 37:15.000
which is really why I feel like the mechanistic stuff is so important because if we know more about it then this becomes like less of it feels less like you're sidestepping the criticism so we can just say what actually happened but I do think that right now that is the case right like it could be that they

37:15.000 --> 37:21.000
have the ability to find variables and do this stuff but just they

37:22.000 --> 37:34.000
have deemed that that's not the right way to solve the task in the average case or in a typical case like maybe negation is not important for the typical thing but if it if they're acting as a python shell then of course negation is important right.

37:34.000 --> 37:44.000
Yeah, and I'm really interested in the role of our life chef fun tuning in that context so we have put money for feedback because it looks like it's it's vast including the zero shot capacity of the model.

37:45.000 --> 38:03.000
And I think there's some evidence that it ends up you know, condensing the probability mass of, you know the distribution of over over the next word for to much more narrow range, such that you know, just a few words will have a high probability for certain for some problems,

38:03.000 --> 38:12.000
because it's essentially enabling the model to to latch on to the right task right away right instead of having to do this. I mean still, as I said, it's still resolved.

38:12.000 --> 38:26.000
Oh, sorry, I didn't get your question right. But if you ask a question zero shot to valley lad, jpd3 without the life chef, it will weigh more often just have no clue what you're really asking it and you have to like

38:26.000 --> 38:31.000
totally. Yeah, yeah. Yeah, yeah, I like to use I like that.

38:32.000 --> 38:48.000
Open AI recently re released their like the old school jpd3 because you can see what a big difference even the instruction tuning makes like my favorite is if you ask something like write a report on the war in Ukraine or something.

38:49.000 --> 39:04.000
It'll like follow it up as though it's like in an email and it'll be like, please include additional like details on budgets and blah, blah, blah. Please get it to me by Monday regards or something like rather than writing a report it like gives you a list of other tasks to do.

39:05.000 --> 39:15.000
But yeah, so there's clearly like, like the RLHF clearly improves your shower at least in even instruction tuning something I'm, I don't know, maybe you've thought about this more.

39:15.000 --> 39:33.000
I know we talked about this like briefly before like just I guess I'm, I'm still getting my head around what RLHF is or in particular how it's like, if it is profoundly different from other types of training because I feel like in some cases like I've heard a lot

39:33.000 --> 39:43.000
of people kind of crediting RLHF as like possibly like, oh, we have all these problems with, with our language models, but maybe RLHF alleviates them.

39:44.000 --> 39:58.000
And I can never tell if there's like a genuine feeling that it does or if it's kind of a this is a new kid on the block and we're not sure so to kind of hedge or future proof whatever claims we're making we add this caveat that's like maybe RLHF alleviates it.

39:59.000 --> 40:10.000
Or maybe it's that like RL in general RL is associated with like grounded and maybe more cognitive like possible learning in certain domains and so people hear the RL part and think that it's somehow better.

40:10.000 --> 40:22.000
But I'm not quite sure if RLHF is like deeply different or if you could induce the exact same behavior through something more like an instruction tuning setup.

40:23.000 --> 40:36.000
Like I genuinely don't know which of those things is going on and I haven't heard like I know there are people trying to come up with a fine tuning variant that otherwise behaves the same as RLHF because RLHF is unstable and people don't like it.

40:37.000 --> 40:48.000
Brown has a lot of people who work on RL and they look at RLHF and they're like this isn't even really RL because it's weird and it just seems like you kind of like folded more language models on top of each other.

40:49.000 --> 41:00.000
So, so yeah I just I don't know how I feel about it and my gut is to be like no it's not special it's like the same stuff in a new package but that's based on absolutely nothing except vibes so I don't know it.

41:01.000 --> 41:10.000
Yeah, I mean I do show my intuition with respect to the difference between RLHF and instruction tuning because it seems you know if you look at what people did with the Lamar model from the time I.

41:10.000 --> 41:21.000
There's a generated a bunch of of you know input out compares with GP4 or something to create what is it? Vecunia or a pack out.

41:22.000 --> 41:25.000
Yeah, so yeah, so that this camelid animals.

41:26.000 --> 41:29.000
So they yeah so they generated essentially the instruction tuning.

41:30.000 --> 41:48.000
Transat from models that have benefited from RLHF and then you kind of get the benefits of RLHF for free right or for $300 and so it seems that actually this works pretty well right so right so maybe I don't know I mean it could be.

41:49.000 --> 42:02.000
I'm just pure speculation but like it that could be the case and RLHF is special it could be that RLHF allows you to optimize for a function that you can't directly optimize for with next word prediction but if you have an RLHF.

42:02.000 --> 42:08.000
RLHF trained model you can then distill that function like the know and then directly something like that.

42:09.000 --> 42:20.000
I want to say that RLHF doesn't even optimize for a different or special or function that like you could just take the data you get from RLHF and just.

42:22.000 --> 42:25.000
Use it differently and like fine tune on it or something.

42:26.000 --> 42:37.000
I mean that must not be the case so I guess I'm really purely speculating but that I think that's the intuition that I'm just like deeply wanting right now is like is there something special going on or is it just.

42:38.000 --> 42:44.000
We found a different way of getting somewhere like we kind of stumbled upon it and we can actually get the same effect.

42:45.000 --> 42:49.000
And it's I feel like it matters it matters from an engineer standpoint but it also matters because I just hear RLHF.

42:50.000 --> 42:55.000
Actually I would like to talk to you about that because you like mentioned it in your grounding paper or like I've heard it from a lot of people.

42:56.000 --> 43:11.000
Like at Federico had a paper on kind of disassociating language and thought and had all these different criticisms of large language models and the things they can't do and then at the end it's like but maybe RLHF solves all of this and I was like whoa that's like a huge.

43:11.000 --> 43:17.000
But I don't know and so and I and I've seen similar things elsewhere and so I think to me I'm like.

43:18.000 --> 43:21.000
I'm just deeply curious if that's the case like I think I'm lacking that intuition.

43:22.000 --> 43:26.000
Yeah, no absolutely I mean that's first of all let's see best transitions through the topic.

43:27.000 --> 43:28.000
Yeah, so nicely done.

43:29.000 --> 43:30.000
Yeah.

43:31.000 --> 43:42.000
So, just to follow from the audience, the, so if we're going to call them we wrote this paper and essentially we said well people use the notion of grounding in different ways in the literature.

43:43.000 --> 43:47.000
And then of this goes back to how not simple grounding problem from the 1990.

43:48.000 --> 43:59.000
90 and in which he argues that symbolic AI models lack the capacity to intrinsically many forms of patients and out this.

44:01.000 --> 44:12.000
Because you know that the semantic interpretation of their representations is provided externally by the programmers so CHRT and you for example it's capable school program that could manipulate locks in the box world.

44:13.000 --> 44:23.000
If you can link can connect its linguistics presentations to virtual objects but that connections provided as I thought can externally by physically programmers.

44:24.000 --> 44:30.000
And the problem is how do we get models that actually he should be grounded.

44:31.000 --> 44:42.000
Presentations of me because he guidance where here the key notion of grounding is referential so how do we get representations that are actually making reference to the the objects of the other worlds that they are about.

44:43.000 --> 44:57.000
And that program has created your marriage with recent connections wills and so on and I need them a lot of with that support back and so we, we were trying to pick about different notions of grounding said the referential national

44:57.000 --> 45:13.000
grounding is the most important one and then say well in light of that, can we say that mangos will also in a text only achieve some form of referential grounding, and we do mention our other chef because we've basically the argument is going from the perhaps the most plausible and convincing

45:13.000 --> 45:18.000
for the most people to something that's a bit more speculative so with our own chef.

45:18.000 --> 45:25.000
So the premise and she is that we, we've next for prediction that's a that's an intro linguistic function right so you could eat in the next word this is trained.

45:26.000 --> 45:42.000
And that doesn't seem to give you quite the, at least intuitively, the right kind of, of normativity for for representations of the world the reference of music items, such that you could have the possibility of misrepresenting something.

45:43.000 --> 45:50.000
So, in other words, you don't have anything to give you the right kind of world involving function is just, you know, whether you write a role about the next talking.

45:51.000 --> 46:08.000
And our chef, on the other hand, because you get this explicit feedback from humans, including feedback about truthfulness honesty, the three h's of our chef is helpfulness, homelessness and honesty right so that is one of these which is a novelive component that's about that's an epistemic in all to

46:09.000 --> 46:17.000
whether you're, if you're, you know, answering questions about capitals of the world, whether you're rather long about the state of the world when you say that Paris is the capital of France.

46:18.000 --> 46:35.000
So that seems to be well involving in the right way, but then, you know, we had this conversation a few weeks back in, and as you rightly pointed out, it seems that, you know, you could get that the right kind of for the public function without this explicit feedback from humans.

46:36.000 --> 46:52.000
And we do actually think that that's the case. So, and going down from slightly less consensual claim, we think that you can get in context learning and in context learning you have a fusion, when you have a fuchsia swamp where you have several examples of successful

46:52.000 --> 47:10.000
problems that you can ask, if it's a future prompt on worldly facts, you also get this implicit feedback in the prompt about what's right of log in the world and if you're seeing projects learning as, you know, inducing a function, optimizing for a function that's not just

47:10.000 --> 47:24.000
the next for prediction, then you can also see that as providing a well developed function, but then you can go even further and say, well, so why not look at pre training and I've told the awesome context windows in there that will include discussions of what the facts where, in order to do the next

47:24.000 --> 47:35.000
for prediction, which is the proximate function, the model might have to induce a more complicated ultimate function that's about the real world. So I think that's where your intuition is.

47:35.000 --> 47:50.000
Yeah, right. Yeah, I think exactly like an, and this is this. Yeah, I love, I mean, I love the paper and I like this idea of the world evolving function because I think that is the, that's kind of the intuition that we all have.

47:50.000 --> 48:09.000
Right, like there does seem to be something that's not like I think most people's gut instinct is like next word prediction over text isn't enough, right. And then the challenge is figuring out like what like where is the line like what is the what actually is the problem with it right and that's

48:09.000 --> 48:24.000
and that's where I'm kind of stuck because I think my like in my heart I kind of don't want language model next word prediction to count like I don't want that to be the whole thing.

48:24.000 --> 48:36.000
And then I just like and a lot of our own work though we seem to be arguing the opposite like I'm not quite sure where I fall on this issue and so it's like yeah you want something like this like this world of our function or something about the learning mechanism right the learning mechanism doesn't

48:36.000 --> 48:57.000
feel like enough you're like no obviously people do more right then predict the thing that happens next but then like formalizing what that difference is I just I haven't been able to convince myself like I haven't been able to come up with a thing that I'm like yeah that's the thing I totally buy it like sometimes it gets on this issue of like goals and I guess this

48:57.000 --> 49:15.000
actually came up a bit in the philosophy of deep learning debate like you know and when I talked to a lot of cognitive scientists and people who don't like the next word patient language while it's like you know people have goals but that feels like a weird because you can say that the language

49:15.000 --> 49:36.000
has a goal and the goal is to predict the next word and like so now are we just like making a judgment about what goals count as good ones and not good ones there's a similar yet like this kind of having the person give the feedback who is sufficiently tied to the world and now it seems

49:36.000 --> 49:56.000
like we've like outsourced the question of whether the model is grounded to something about the trainer and the goals of which maybe actually is fine maybe philosophically that's fine you're like to fall along this causal chain you have to have kind of inherited from somebody who's also on the causal chain so I mean I don't know because like yeah if you had like somebody giving

49:56.000 --> 50:17.000
RLHF feedback who is like intentionally misleading right or just confused and misinformed like any of these types of things like I don't know how that muddies the analysis of whether the model is now world involved or grounded

50:17.000 --> 50:18.000
right

50:18.000 --> 50:19.000
referentially grounded

50:19.000 --> 50:33.000
yeah totally agree on the last point I think if the whole argument hinges also on the assumption that the crowd workers actually know how to run the outputs or do it right to a large extent

50:33.000 --> 50:34.000
right

50:34.000 --> 50:46.000
and there's this and I guess this is a classic philosophical qualm too right like cause we can like humans could be totally wrong about stuff right so like science progresses right so right now we might be seeing

50:46.000 --> 51:04.000
like oh the I forgot one of the classic examples the like a lot of things about like viruses and diseases right like we had theories about what this different diseases were back in the day and then we learned stuff and it's like an entirely new thing now right

51:04.000 --> 51:15.000
but you wouldn't say that the people historically had like an ungrounded meaningless notion of that thing right so we so like you need to account for the fact that we could be teaching the model something that is

51:15.000 --> 51:26.000
ultimately wrong because we haven't learned that in fact you know that I don't I can't yeah but I can't think of a good example

51:26.000 --> 51:40.000
the thing is theories of representations of representation will account for the fact that you can misrepresent things right so just because you have a you know a linguistic like lexical concept that is

51:40.000 --> 51:55.000
not refraction be grounded doesn't mean that this precludes the possibility I have you know something going on right misrepresenting right so like thinking

51:56.000 --> 52:13.000
yeah and I guess this is kind of it's like so if the model thinks that the capital of France is Berlin quote thinks that and produces that as output I guess we need to differentiate between the case where it's just wrong and ungrounded and

52:13.000 --> 52:27.000
learning loose associations versus it was quote mistaken right but like that because I could not know the capital of the city and like that it's true that I don't have quite the right concept of that thing but it's not the same as me being a parrot that's

52:27.000 --> 52:38.000
just spitting out it's right and so I don't think we quite quite right now I don't know how we're drawing that line within the models of them just producing wrong stuff versus

52:38.000 --> 52:54.000
yeah I mean certainly with I think perhaps with things like early Jeff you can draw that line potentially but with retraining I mean and I show you intuition that you know since it seems you know we don't really want to do the line at early

52:54.000 --> 53:05.000
Jeff it just it just seems like the low hunting fruit because that's what's going to come in last people but that's already you know it's already shouldn't become a racial statement to say like not going to do it all trained and

53:05.000 --> 53:20.000
text only can achieve the first one but then we would want to go further like you and but then it's tricky so so here is the interesting that I have and I've been maybe overly impressed by these recent papers that look at income

53:20.000 --> 53:35.000
learning and show that it's this wonderful Google and others that show the same thing essentially that they functionally equivalent to right fine tuning with gradient descent, even though you're not actually addressing the weights and what's

53:35.000 --> 53:48.000
tuning on what what's what's the what's the during gradient descent virtually on is not next for prediction, it's whatever function is connected to the specified by the future, that the future prompt.

53:49.000 --> 54:02.000
And so that really is the thing that can be okay so so so you could get the work involving stuff from that and if you get if you can get it at different time within complex learning, then presumably you could also get it at pre training time.

54:02.000 --> 54:08.000
If you say you have a window context window that's a bunch of capital questions that was a good front.

54:09.000 --> 54:20.000
Yeah, you have some future like stuff in the training data that presumably would allow the model when it's not at the beginning when it's totally random but when it starts learning everything last at some point.

54:20.000 --> 54:23.000
Right, you know, you might not get that.

54:23.000 --> 54:27.000
Yeah, yeah, and I think this is where.

54:27.000 --> 54:44.000
Like I agree with that and I agree with kind of calling out RLHF as possibly the a different point because it's like it's a good thing to ground to as or not ground to to overloading the word ground it's good like data point right to use and then trying to peel away like what is the minimal

54:44.000 --> 54:52.000
thing like what about RLHF gives it that and but like that's exactly that logic you just laid out is what I.

54:52.000 --> 55:00.000
I think I accept right now or feel somewhat forced to accept right based on this because you're like yeah that seems correct.

55:00.000 --> 55:14.000
But then I have to go back to putting on the hat of somebody somebody who does not believe these language models because and I think it is important to point out that when we're talking about language models being grounded or having meaning it's not the same as saying they are.

55:15.000 --> 55:24.000
conscious and intelligent right like but sometimes but that's that's this kind of elephant in the room where it's like where are we going next and so I think when people are looking at.

55:24.000 --> 55:36.000
These language models and they don't want to acknowledge that they can be referentially grounded because that seems like a step along the way to claiming that they are like human level cognition and all of these other ways.

55:37.000 --> 55:46.000
It's so deeply unsatisfying it's like wait no like you've missed the point like now we're saying that just having a few examples during pre training of someone listing off countries who wrote that down.

55:47.000 --> 55:59.000
In you know good faith just listing the capitals of countries that's enough and now the language model counts even though it's just doing next word prediction whether like that seems insane right and so I kind of like I feel like I just go between these two positions of being like.

56:00.000 --> 56:08.000
Like right now based on everything I feel kind of forced to accept like no the language models I would say they're referentially grounded I can't find a good case to make for why they're not.

56:08.000 --> 56:12.000
And at the same time I'm like do do we really want to say that that seems bizarre right like.

56:12.000 --> 56:23.000
But when we get our for our hand forced by this the kind of all of nothing's thinking that you see sometimes in these debates where it's like.

56:23.000 --> 56:28.000
At the high level it's like either the stochastic power so it's like human like.

56:28.000 --> 56:31.000
You know AGI with with.

56:31.000 --> 56:35.000
Understanding consciousness and for just whatever you would be like yeah yeah yeah.

56:35.000 --> 56:38.000
And there's this huge you know.

56:38.000 --> 56:44.000
Space in between possibilities that we could explore where you look at different capacities in the case by case based and say that.

56:44.000 --> 56:47.000
And then within each capacity that grounding.

56:47.000 --> 56:49.000
It's also spectrum right it's not like.

56:49.000 --> 56:54.000
You know we want to say you get a few examples of question answers about capitals and then.

56:54.000 --> 56:58.000
That's it you're you're you have human like referential grinding on everything.

56:58.000 --> 57:00.000
You check that box.

57:00.000 --> 57:03.000
You're good yeah right so.

57:03.000 --> 57:12.000
So presumably you know you could say well you know that that gets you you know your foot on the lighter of a personal grounding in a tiny tiny tiny domain and.

57:12.000 --> 57:15.000
And that's still you know 30 feet.

57:15.000 --> 57:21.000
Interesting yeah right that said I mean there's something that I think is really interesting from your work on.

57:21.000 --> 57:29.000
You know isomorphisms between say in color terms and the color space and stuff you've been working on with grounding that.

57:29.000 --> 57:36.000
Maybe suggests that sometimes when you get a toe hold on grounding in a specific domain.

57:36.000 --> 57:43.000
You could leverage the isomorphism between language in the world to get a little bit more for free.

57:43.000 --> 57:49.000
Right right right right right you definitely could.

57:49.000 --> 57:56.000
Imagine that and I guess this is the project that you and I start working together is the kind of what kind of power of analogy.

57:56.000 --> 58:01.000
Reasoning do these models have because yeah you could imagine something like this like.

58:01.000 --> 58:07.000
With the toe hold and really strong reasoning by analogy capability you could get a lot.

58:07.000 --> 58:13.000
Out of that but I also agree I think there's just this big middle ground like it's not like you could.

58:14.000 --> 58:15.000
Like.

58:15.000 --> 58:17.000
Learning that the.

58:17.000 --> 58:21.000
The meaning of the capital cities or something shouldn't.

58:21.000 --> 58:25.000
Be enough that now by reasoning by analogy you can infer like.

58:25.000 --> 58:28.000
Everything the whole world like that that seems.

58:28.000 --> 58:35.000
I would if somebody could spell out a mechanism via which that would happen sure but I can't imagine what that would be.

58:35.000 --> 58:38.000
Yeah yeah.

58:39.000 --> 58:43.000
I was I was going to make a complete.

58:43.000 --> 58:47.000
Detour but I remembered we didn't talk about the Chomsky stuff when we're talking about.

58:47.000 --> 58:50.000
Yeah so I wanted to hear your thoughts on that.

58:50.000 --> 58:55.000
The thoughts but I would like to hear your thoughts on yeah so.

58:55.000 --> 58:59.000
So just for context but Chomsky.

58:59.000 --> 59:03.000
Steve has been.

59:03.000 --> 59:08.000
Writing about statistical models of language learning for decades but he recently.

59:08.000 --> 59:19.000
Let on the records saying as you would expect that he's totally skeptical you can anything whatsoever from language models about human language acquisition human condition in general.

59:19.000 --> 59:26.000
And he co wrote this op-ed in New York Times making that claim.

59:26.000 --> 59:32.000
Even though he told me he would have he signed on it but would have made a point of all the differently.

59:32.000 --> 59:40.000
Because it goes in different directions I think you know I have some process about how the op-ed is believe no different directions I think his point is more.

59:40.000 --> 59:43.000
His corporate is simpler is just.

59:43.000 --> 59:48.000
You know like with bottles.

59:48.000 --> 59:54.000
And then he makes analogy with with a theory of physics that would say anything goes and.

59:54.000 --> 01:00:00.000
And that wouldn't be a good theory of physics and he's he's he's very impressed by this particular paper that.

01:00:00.000 --> 01:00:04.000
From Bowers colleagues I think that that's a big step.

01:00:04.000 --> 01:00:09.000
Learning supposedly possible languages and showing that they have some visual scan.

01:00:09.000 --> 01:00:13.000
Learn such languages and things that's that's that's it all months on and forth.

01:00:13.000 --> 01:00:15.000
Yeah.

01:00:15.000 --> 01:00:17.000
And then there's tip that I don't see.

01:00:17.000 --> 01:00:18.000
I wrote this paper.

01:00:18.000 --> 01:00:26.000
That's saying, you know, taking the completely opposite stance saying language models refused the whole program and programming linguistics.

01:00:26.000 --> 01:00:30.000
And I, I mean, so I started between.

01:00:30.000 --> 01:00:37.000
As often in these discussions that's the running thread but I think that there's always room for positions in between the two extremes.

01:00:37.000 --> 01:00:38.000
Yes.

01:00:38.000 --> 01:00:46.000
So, I think there was this interesting, interesting discussion that talent in others.

01:00:46.000 --> 01:00:56.000
In a trader where there are two versions of the cover to the stimulus arguments that is used to justify the claim that there is something like the next year universal grandma in humans.

01:00:56.000 --> 01:01:03.000
So there's a strong version that jumps key himself did defend in the past and that's people still defend neighboring very popular grandma.

01:01:03.000 --> 01:01:09.000
You know, generally grandma textbooks, which is syntax is just unlearnable from data.

01:01:09.000 --> 01:01:20.000
Period. Like no more of the devil will get you to learn to structure because having discovered cursive infinity productive system is not something you can learn from detail.

01:01:20.000 --> 01:01:21.000
Interesting.

01:01:21.000 --> 01:01:28.000
So, so I think there is a good argument and see it makes a good argument for that being somewhat refuted by language models.

01:01:28.000 --> 01:01:34.000
And they discover in so far as they can, you know, there is a lot of work showing that it can induce that structure.

01:01:34.000 --> 01:01:41.000
Whether it's, you know, no, no, it's, it's the, it's the verb agreements to get dependencies.

01:01:41.000 --> 01:01:45.000
You can even decodes proxies from nations of baritone.

01:01:45.000 --> 01:01:48.000
So, so that's a strong graph.

01:01:48.000 --> 01:02:04.000
And then there is the more in my mind more reasonable developmental version, which is, well, with children can do this constraint generalizations on certain syntactic phenomena and if they get dependencies from few examples from this improvisate most.

01:02:04.000 --> 01:02:12.000
And it seems like this is hard to come for if they don't have, you know, strong enough in that device to make this deductive inferences.

01:02:12.000 --> 01:02:20.000
And one way that you could have this kind of device would be to have a nice grammatical stripes.

01:02:20.000 --> 01:02:26.000
And there I think the evidence with language models is way less clear and a way more tentative.

01:02:26.000 --> 01:02:38.000
There, there is a little bit of, so the problem being that the large number of the giant ones that actually give you for that and from order of orders of magnitude more words than children's children do.

01:02:38.000 --> 01:02:45.000
There are a few papers in one from from Jengen colleagues from 231 and there is more recent one for the office.

01:02:45.000 --> 01:02:55.000
Training models with between 100 million and sorry, 10 million and 100 billion words, which is what a child would get by the age of six, eight, 10 years old.

01:02:56.000 --> 01:03:05.000
And they show that actually you can get a lot of paper with a title like this. It was like even when trained on a normal amount of data language model still replicate.

01:03:05.000 --> 01:03:20.000
Yeah. Yeah. And so, and so what one of these papers shows that what you get when you train them, you keep training them on X for words is mostly he mostly pertains to things like common sense knowledge about the world and all that semantic stuff, but not so much the syntax stuff.

01:03:20.000 --> 01:03:22.000
Maybe negation would be an exception.

01:03:22.000 --> 01:03:26.000
But yeah, that's what I said. Yeah.

01:03:26.000 --> 01:03:34.000
And there is this this also this project and I'm very excited to see the results of the baby a length challenge from Alex. Yeah, yeah.

01:03:34.000 --> 01:03:43.000
And so this is the same kind of challenge they use this the credit is copies that's copies of child directed speech that's actually recorded from real real life.

01:03:43.000 --> 01:03:55.000
And so this is the same kind of child's the kind of word stick here and sentence stick here. And there's a hard version of the challenge where you have to train language model on only 10 million words, I think at an easier version of 100 million words.

01:03:55.000 --> 01:04:01.000
And people will submit their contributions and I think the results will be more supportive of the store.

01:04:01.000 --> 01:04:16.000
But yeah, I think, I think that kind of projects could, in my opinion, constrain type of this in theoretical linguistics and maybe go against the week version of the progressives to us and say, look, like maybe you don't need

01:04:16.000 --> 01:04:21.000
to think that's syntactic in a knowledge.

01:04:21.000 --> 01:04:26.000
But even if that's the case, I think something, you know,

01:04:26.000 --> 01:04:39.000
that doesn't mean that there is no amount of of in its structure or inductive bias whatsoever because after all, you know, language models are not typically rising they have inductive bias is just almost having the two biases.

01:04:39.000 --> 01:04:48.000
And even if these are not language specific, I think that's a kitty front before Chemsky would say we say we need this language specific knowledge.

01:04:48.000 --> 01:05:04.000
There is a sense in which the moderates empiricists meets halfway with the moderates nativists to say there is some like there are some inductive biases that not that they're like general that don't in general and then language specific and we can be happy with that so.

01:05:04.000 --> 01:05:23.000
Right. Yeah, that's a really, yeah, that's a great point and I think I agree with that like I um yeah I mean I also just in general probably just a general of sciences like the putting something up as a choice between two extremes is like always detrimental right

01:05:23.000 --> 01:05:29.000
like and sometimes they can prevent people from working on the problem because they're like afraid of pissing off half of the field.

01:05:29.000 --> 01:05:49.000
But yeah, I so I had a, I like this I like one thing I was thinking about or was thought about with reading Steve's responses, again the kind of just him on the mechanistic stuff like to me, it seems like the answer kind of hinges I guess this is a different version of

01:05:49.000 --> 01:06:00.000
question but it seems like a lot of it hinges on what the model is doing internally to process these to process the language like whether it represents something that looks like Trump skin syntax or not.

01:06:01.000 --> 01:06:19.000
But I guess this is kind of a different question so there's there's the kind of what you were describing which I think is definitely like one of the important ways to be think about this problem from kind of a linguistics perspective which is like take the blank slate language model and say how much data doesn't need to learn because it's definitely a

01:06:19.000 --> 01:06:32.000
super significant finding if you could replicate if like a transformer randomly initialized transformer trained on a realistic amount of child directed speech learn syntax like that's a huge finding right.

01:06:32.000 --> 01:06:38.000
There's also this kind of like take the giant pre trained language model.

01:06:39.000 --> 01:06:57.000
trained on a way more data than human has but then you can kind of use that as the starting point like that's the innate structure that a human has right, which could be a ton of language specific and syntactic structure, and then that's kind of what.

01:06:58.000 --> 01:07:13.000
And then from that point on then it's quite efficient it's the problem so that's kind of what my first time I read Steve's papers like well we can't really say because we don't know whether the model has solved the language problem by learning some like nice looking

01:07:13.000 --> 01:07:21.000
formal syntactic abstract syntactic structure under the hood or not although I think a lot of data suggests that it has.

01:07:21.000 --> 01:07:38.000
But I guess it's it's kind of hard question to answer because you can't do something like you can't then take that as the starting point of a language learner because it's already learned a lot of language like you would want some other way of like inducing that structure, but then stripping out the ability to actually generate

01:07:39.000 --> 01:07:48.000
like you would want to like find a way of like pre training the model getting all the innate syntactic structure but then somehow re initializing a large part of it so that.

01:07:49.000 --> 01:07:57.000
Child would have to relearn and then use that as to ask whether what the language while has learned is something akin to the innate structure that Chomsky would have wanted I also.

01:07:58.000 --> 01:08:06.000
I mean I guess something you could do because you mentioned like learning on learnable languages is like try to transfer a pre trained language model to one of these.

01:08:07.000 --> 01:08:26.000
Unlearnable languages because I like I always think of the I've been thinking of the pre trained language models as kind of the like that's whatever is in doubt by evolution or something that's the starting yeah yeah right and so but yeah I recognize that it's kind of impossible to use as a model of a.

01:08:27.000 --> 01:08:37.000
Pre linguistic child because it has way too much language to do any interesting experiments on its language learning but you could ask whether you could transfer to one of these unlearnable languages and whether it basically.

01:08:38.000 --> 01:08:46.000
Needs to unlearn and relearn as basically like if it can transfer too well to one of these other languages that I guess would be a data point against.

01:08:47.000 --> 01:08:57.000
That structure being the kind of structure that kids have born in whereas if it like their models are very powerful but what it might need to do is basically unlearn everything and then relearn the new language from scratch.

01:08:58.000 --> 01:09:08.000
And then that would suggest that it's like not yeah it's not actually very learnable to the models once they've been pre trained with this strong bias for other structures.

01:09:09.000 --> 01:09:16.000
Yeah although you need to show that humans that this this made of languages are actually unknowable.

01:09:17.000 --> 01:09:24.000
And that's how to do because you can try with adults and then it's like an L2 problem like second language but if you want to try with the first language.

01:09:25.000 --> 01:09:29.000
If that's how you said it I'm the you know if you think of pre training as like evolution.

01:09:30.000 --> 01:09:40.000
And then you could say well whatever is going to be pre training could be the innate structure of Chomsky and yeah so then you would need to actually you know it's an impossible experiment to do you need to be very unethical.

01:09:41.000 --> 01:09:43.000
You have to have a child grow up in this.

01:09:44.000 --> 01:09:49.000
So because like what is the data that those languages unlearnable is that no language has the features that this language has.

01:09:50.000 --> 01:09:57.000
I think so they have this weird yeah I haven't done a deep dive and I think it's very controversial so a lot of people were convinced by that paper.

01:09:57.000 --> 01:10:03.000
Chomsky really latched onto that as you know hard evidence that we can learn nothing from language models.

01:10:04.000 --> 01:10:05.000
Right.

01:10:06.000 --> 01:10:21.000
Yeah I'm speaking out of my domain but I so I would defer to someone who studies this but my I see why it's controversial right because there's like perhaps lots of explanations for wide languages share common features other than those are the only possible features for the human mind to.

01:10:22.000 --> 01:10:23.000
Yeah.

01:10:24.000 --> 01:10:25.000
Yeah.

01:10:25.000 --> 01:10:26.000
Interesting.

01:10:26.000 --> 01:10:27.000
Yeah.

01:10:27.000 --> 01:10:44.000
And also you know there is some evidence I think you know that some people who are not very changing the mistakes always point to evidence across languages where it seems that some of the features of generative grammar seem to work better for some dominant languages than others that some languages.

01:10:45.000 --> 01:10:46.000
Right.

01:10:46.000 --> 01:10:55.000
And then the standard move sometimes is to just add some kind of ethnic thing to the German grammar to compensate for that but then it becomes a little less elegant and.

01:10:56.000 --> 01:10:57.000
Yeah.

01:10:57.000 --> 01:10:58.000
You know.

01:10:58.000 --> 01:10:59.000
Yeah.

01:10:59.000 --> 01:11:00.000
Yeah.

01:11:00.000 --> 01:11:01.000
Yeah.

01:11:01.000 --> 01:11:02.000
Yeah.

01:11:02.000 --> 01:11:03.000
Yeah.

01:11:03.000 --> 01:11:05.000
Yeah I mean I think it's kind of an un.

01:11:06.000 --> 01:11:25.000
Transiential point but I guess there isn't as much work on these large language models in other languages right like I mean they're there are multi-lingual models and stuff but that's definitely some data points that would be good to know like we talk about transformers having some they do have inductive biases and things like I don't know how well they work for.

01:11:26.000 --> 01:11:38.000
Other languages I've never seen a good controlled experiment because just the quantity and quality of data we have in different languages is like so varied that you just can't say anything when the models are better or worse.

01:11:39.000 --> 01:11:51.000
But that definitely seems like from the perspective of asking what do these models mean for the Chomsky and program or universal grammar and stuff like it seems like we need data on how they work in.

01:11:52.000 --> 01:11:53.000
Cross-lingually.

01:11:53.000 --> 01:11:54.000
Yeah.

01:11:54.000 --> 01:12:04.000
There is this work by Stephanie Chan goes from deep mind that we said that how the data distributional properties that probably sort of the trading data influences.

01:12:05.000 --> 01:12:13.000
I think in contact starting in transformers but so they look at that we've made up better sense that different distributional distributional which is.

01:12:13.000 --> 01:12:21.000
And they find that the typical distributional properties of languages that seem to actually be there is work stream that is you know there's a zip.

01:12:22.000 --> 01:12:34.000
Zips will apply across basically every human language and it seems that there is something in common in the distributional properties of all human languages that transformers can watch on.

01:12:35.000 --> 01:12:36.000
Yeah.

01:12:36.000 --> 01:12:37.000
Yeah.

01:12:38.000 --> 01:12:39.000
Interesting.

01:12:39.000 --> 01:12:40.000
Interesting.

01:12:43.000 --> 01:12:44.000
Yeah.

01:12:44.000 --> 01:12:45.000
Yeah.

01:12:45.000 --> 01:12:54.000
Luckily there's still lots of stuff to think about like sometimes it's like oh all the problems are being solved these models keep coming on doing all the tests and it's like oh god no there's so many.

01:12:55.000 --> 01:12:57.000
Like I think we still have careers ahead of us that's good.

01:12:58.000 --> 01:12:59.000
Yeah.

01:12:59.000 --> 01:13:11.000
In the way because you know people freak out about the fact that all the big problems now are being tried by industry and before it's taken away from academia but I think all of the a lot of interesting problems actually things you can only do on small models anyway.

01:13:11.000 --> 01:13:12.000
Like we can.

01:13:12.000 --> 01:13:13.000
Oh yeah totally.

01:13:13.000 --> 01:13:15.000
And so in a way there is a nice division of labor.

01:13:16.000 --> 01:13:29.000
Yeah I actually yeah I was like I work at Google 20% and I've said a couple times like all my most interesting projects are the ones at Brown because like I actually think the small toy data things like that's where I feel like I'm learning stuff and like it makes sense.

01:13:30.000 --> 01:13:32.000
That's how things work like it's so exciting.

01:13:32.000 --> 01:13:37.000
So yeah I feel good about academic work on this stuff right now.

01:13:37.000 --> 01:13:41.000
With the synoptimistic notes to end on.

01:13:41.000 --> 01:13:43.000
Yeah thanks a lot.

01:13:43.000 --> 01:13:45.000
This is fun.

01:13:45.000 --> 01:13:46.000
This is a lot of fun.

01:13:46.000 --> 01:13:51.000
Yeah we killed I guess I have my timer going almost 75 minutes.

01:13:51.000 --> 01:13:52.000
It's pretty impressive.

01:13:52.000 --> 01:13:53.000
Perfect.

01:13:54.000 --> 01:13:55.000
Hi Tim.

01:13:56.000 --> 01:13:57.000
Hi Tim.

