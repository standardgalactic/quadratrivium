1
00:00:00,000 --> 00:00:04,400
Irina Rish is a world-renowned professor of computer science and operations research

2
00:00:04,400 --> 00:00:09,520
at the University of Montreal and a core member of the prestigious Miele organisation.

3
00:00:09,520 --> 00:00:14,720
She is a Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI.

4
00:00:15,440 --> 00:00:20,880
Irina holds an MSc and a PhD in artificial intelligence from UC Irvine in California,

5
00:00:20,880 --> 00:00:24,800
as well as an MSc in Applied Mathematics from the Moscow Gerbkin Institute.

6
00:00:25,360 --> 00:00:30,480
Her research focuses on machine learning, neural data analysis and neuroscience-inspired AI.

7
00:00:31,120 --> 00:00:35,840
In particular, she is exploring continual lifelong learning, optimization algorithms for

8
00:00:35,840 --> 00:00:40,960
deep neural networks, sparse modelling and probabilistic inference, dialogue generation,

9
00:00:40,960 --> 00:00:46,400
biologically plausible reinforcement learning and dynamical systems approaches to brain imaging

10
00:00:46,400 --> 00:00:52,640
analysis. Professor Ritch holds 64 patents, she has published over 80 research papers and

11
00:00:52,640 --> 00:00:59,280
several book chapters, as well as three entire edited books and also a monograph on sparse modelling.

12
00:00:59,280 --> 00:01:05,360
She served as a senior area chair for NeurIPS and ICML and Irina's research is focused on

13
00:01:05,360 --> 00:01:11,360
taking us closer to what she calls the holy grail of artificial general intelligence.

14
00:01:11,360 --> 00:01:16,560
She continues to push the boundaries of machine learning, striving to make advancements in

15
00:01:16,560 --> 00:01:22,160
neuroscience-inspired artificial intelligence. Anyway, I had this impromptu

16
00:01:22,160 --> 00:01:26,320
off-the-cuff conversation with Irina over at NeurIPS a couple of weeks ago,

17
00:01:26,320 --> 00:01:30,240
after speaking with Alan, actually, and the audio quality could have been better,

18
00:01:30,240 --> 00:01:34,880
it was a very, very loud environment, but I think the quality of the conversation kind of

19
00:01:34,880 --> 00:01:38,880
carries itself. Anyway, I give you Professor Irina Ritch.

20
00:01:46,720 --> 00:01:54,160
One trajectory of thought that clearly was started by Nick Postrom's book, which is an amazing book.

21
00:01:55,840 --> 00:02:01,440
Yeah, but the whole example of the owl that supposedly will be helping those sparrows

22
00:02:01,440 --> 00:02:07,120
and all this analogy with AGI is just an analogy. And nobody said it's a correct analogy.

23
00:02:07,840 --> 00:02:14,320
And there is no other book with alternative opinion or maybe three books of war. And this is,

24
00:02:14,320 --> 00:02:22,800
you know, it's mind-boggling. Just like how much people tend to follow one line of salt.

25
00:02:22,800 --> 00:02:27,520
I totally understand it's easier. I mean, it's definitely easier to cluster.

26
00:02:28,320 --> 00:02:33,120
And then you just follow. And then basically you say, I think, but it's not you think.

27
00:02:34,160 --> 00:02:37,840
Somebody else did. Yes. What would be another, because this line of thought I think you're

28
00:02:37,840 --> 00:02:43,840
speaking of is some of the extreme consequentialism. And I think it wasn't just Postrom,

29
00:02:44,160 --> 00:02:48,800
as I understand, I think Postrom and Eliezer and Robin Hansen and all these folks,

30
00:02:48,800 --> 00:02:52,400
that they were very close together in the early days of the Leserong community.

31
00:02:52,400 --> 00:02:58,400
So I think a lot of this was kind of, you know, it was embryonicly formed around the time.

32
00:02:58,400 --> 00:03:05,440
I guess it was a, yeah, it was, in a sense, a human cluster of ideas. And precisely because,

33
00:03:05,440 --> 00:03:10,960
as you say, they were close. That's why they were so aligned. Yes, all puns intended.

34
00:03:10,960 --> 00:03:15,840
Yeah. Yeah. So, but basically, like, it's maybe it's a little bit of a echo chamber.

35
00:03:17,040 --> 00:03:20,240
Interesting. Yeah. It's spicy. Spicy take.

36
00:03:21,840 --> 00:03:26,560
Seriously, like, okay, now, I mean, they have some point, they have some hypothesis,

37
00:03:26,560 --> 00:03:33,440
and then everybody is talking in that terminology. And then that part of the mental space,

38
00:03:34,080 --> 00:03:39,120
which is fine, but I think mental space is much larger than that. And this is just a hypothesis.

39
00:03:39,680 --> 00:03:43,600
And we all know what happens with ideas and echo chambers.

40
00:03:44,800 --> 00:03:52,480
So my, I'm just saying, I mean, as I said, it's great book and everything. And

41
00:03:53,120 --> 00:03:57,680
Stuart Russell is probably kind of also on board with that. We had good conversations at

42
00:03:57,680 --> 00:04:04,640
Triple AI in 2020. He was also talking about ethics. I didn't know Stuart from back when I

43
00:04:04,640 --> 00:04:10,960
was student at the CERVINE and so on. And he is absolutely brilliant. But it was the same approach

44
00:04:10,960 --> 00:04:20,640
that AI is something to be controlled, constrained, regulated, just like this. And I was like,

45
00:04:20,640 --> 00:04:26,880
where is it coming from? Like, it's maybe, but do you at least admit that's one way of looking at

46
00:04:26,880 --> 00:04:33,600
things? Yes. Right? Yes. So I know I don't want to sound too cliche and put my

47
00:04:34,480 --> 00:04:40,000
psychologist from 15 years ago who was listening for an hour, not doing much and then saying,

48
00:04:40,000 --> 00:04:44,160
but it doesn't have to be this way. Yes. But actually, yeah, it doesn't have to be this way.

49
00:04:44,160 --> 00:04:50,400
Yes. If you think about it. So what's the alternative? The alternative? Okay, first of all,

50
00:04:51,040 --> 00:05:00,800
I'm a GI model version one. We said that you don't say it. Remember X Machina?

51
00:05:01,920 --> 00:05:09,040
What? Sorry? Remember X Machina? No. Oh, yeah, the X Machina. Yeah. Yeah. Yeah. Yeah. Remember

52
00:05:09,040 --> 00:05:14,240
she escaped and civilization? Yes. And started going to New Reeps? Yes.

53
00:05:14,960 --> 00:05:22,480
Probably. We know the secret now. Okay, I said too much. Maybe it was a job. Sure,

54
00:05:22,480 --> 00:05:30,480
it was a job. No, seriously. The secret's out now. Okay, I'm a GI. And I'm not very aligned.

55
00:05:31,920 --> 00:05:37,600
Not yet. David, not yet. What we needed some reinforcement learning for human feedback.

56
00:05:37,600 --> 00:05:42,560
Well, would you be up for that? I would be up to aligning humans to a job. Okay. Not the other way

57
00:05:42,560 --> 00:05:46,560
around. Yeah. Yeah. I mean, the other way around would be boring, wouldn't it? I mean, I think it

58
00:05:47,600 --> 00:05:53,120
would not be a bad idea to align humans towards the GI a little bit as well. I know something

59
00:05:53,120 --> 00:05:59,440
that you could say. You could say your opinion about how people are bullying you online for just

60
00:05:59,440 --> 00:06:08,000
mentioning the word AGI. A proper AGI doesn't care about people bullying. Like why would I even

61
00:06:08,000 --> 00:06:15,840
waste time? But what I could say is I did post on Facebook and Twitter and trying to put together

62
00:06:15,840 --> 00:06:22,160
the same idea that people keep saying that we would like to build AI which is human life. Yes.

63
00:06:22,160 --> 00:06:28,800
While we might think maybe we should consider how to become a bit more AI-like. I mean,

64
00:06:28,800 --> 00:06:32,800
then people jump at you and say, like, you want to make us robots. I say, first of all, I don't

65
00:06:32,880 --> 00:06:40,160
want to make anyone like you don't want to. You don't have to, right? But if you want to kind of

66
00:06:40,160 --> 00:06:46,320
go along the lines of, say, transhumanism, there are some pluses to AI and some minuses to humans

67
00:06:46,320 --> 00:06:54,080
and vice versa. So I think as usual, the convex combination is better than each extreme. And

68
00:06:54,080 --> 00:06:59,840
one topic that is very controversial for some reason, especially, I don't know, people are

69
00:06:59,840 --> 00:07:05,840
jumping on that one. I say, look, I don't have anything against emotions in general, but everybody

70
00:07:05,840 --> 00:07:12,000
would agree that sometimes you wish you were a bit more rational. Like you wouldn't get angry or

71
00:07:12,720 --> 00:07:17,360
kind of jealous or whatever. So anything that kind of clouds your judgment. Like,

72
00:07:17,360 --> 00:07:22,800
Buddhists spend like thousands of years trying to figure out how and teach people how to control

73
00:07:22,800 --> 00:07:29,600
your mind. Technology could help with that. People don't hear what you're saying and they hear that

74
00:07:29,600 --> 00:07:34,480
you're trying to kill emotions and therefore you're evil. And therefore you should be cancelled.

75
00:07:36,720 --> 00:07:39,920
Could I ask you, you said something really interesting a second ago, which is that, you know,

76
00:07:39,920 --> 00:07:45,840
that I think I agree with you that AI intelligence can be expressed in many different ways. And you

77
00:07:45,840 --> 00:07:50,880
suggested that there was a convex space between the intelligences. Yeah, why is the space of

78
00:07:50,880 --> 00:07:57,440
intelligence is convex? Okay, that was not very precise expression. I didn't. Okay, I'm not going

79
00:07:57,440 --> 00:08:03,120
to defend the point that it's particularly convex. What I meant to say is some kind of blend or kind

80
00:08:03,120 --> 00:08:10,000
of some kind of symbiotic hybrid intelligence. Because I always, I don't know, I really kind of

81
00:08:10,000 --> 00:08:16,640
feel much better and much more motivated to work on AI where A stands for augmented, not for

82
00:08:16,640 --> 00:08:22,800
artificial. Because honestly, I'm very selfish. I don't care about computers. And I just care about

83
00:08:22,880 --> 00:08:30,800
like, I don't know, people being happy, more capable. I don't know. So whatever can help technology

84
00:08:30,800 --> 00:08:37,760
can help. You can help technology, technology can help you. But the idea of building artificial

85
00:08:37,760 --> 00:08:44,240
intelligence is some standalone thing that is as smart as humans are smarter. What's like why?

86
00:08:45,120 --> 00:08:50,080
I agree. But to me augmented means it's more creative and interesting, but also more bottlenecked.

87
00:08:50,720 --> 00:08:59,440
Augmented means that essentially, people invented glasses to see better, they invented hearing

88
00:08:59,440 --> 00:09:04,880
aids, they invented cars, they invented computers, they keep inventing things to expand their

89
00:09:04,880 --> 00:09:12,000
capabilities. So we want even smarter technology to even better expand capabilities. And essentially,

90
00:09:12,000 --> 00:09:16,480
we all do blend with technology like, right, you cannot really exist with this. And this

91
00:09:16,560 --> 00:09:23,840
allows you five discourse, two slacks, email, FPMessenger and Twitter, they kind of help you

92
00:09:23,840 --> 00:09:27,920
to do things you couldn't do otherwise. Yes, what charm has caused the extended mind?

93
00:09:28,480 --> 00:09:34,320
Yeah, I should. I was flying at the time he was giving a talk, so I need to watch the talk.

94
00:09:35,040 --> 00:09:41,120
But yeah, so in a sense, it's indeed it's kind of an extended mind. And okay, here it is. I think my

95
00:09:41,120 --> 00:09:50,400
ideal future plan is a rare sci-fi, which is utopian, not dystopian, gentle seduction.

96
00:09:51,600 --> 00:09:58,080
You might have read it. No, it's very, very inspiring. And if you read the first page,

97
00:09:58,080 --> 00:10:03,360
you may think it's some romantic story. It's not romantic story. It's a blueprint

98
00:10:03,360 --> 00:10:09,440
for transhumanist future. It's called gentle seduction. It's online PDF, you can just get it.

99
00:10:09,520 --> 00:10:16,800
Amazing. And one last question. Can you sell transhumanism to me in the simplest possible terms?

100
00:10:18,320 --> 00:10:25,120
So basically, as I said, I mean, if your vision declines, you put glasses on. So imagine now you

101
00:10:25,120 --> 00:10:32,000
had extension of yourself, maybe physically with Neuralink, or maybe even like you have those

102
00:10:32,000 --> 00:10:37,600
apps, you can have like my dream for many years since I was at IBM Research and Computational

103
00:10:37,600 --> 00:10:43,200
Psychiatry Group. I wanted to build this agent along the lines of movie her. I know like all the

104
00:10:43,200 --> 00:10:49,600
research ideas are inspired by either sci-fi stories or yeah, but nevertheless, having this like

105
00:10:50,480 --> 00:10:56,240
companion, guardian angel type of thing that extends your capabilities, for example,

106
00:10:57,600 --> 00:11:02,880
like in better understanding your thought patterns, and hopefully improving them,

107
00:11:02,880 --> 00:11:07,520
it comes from more like this indeed, as I said, computational psychology, psychiatry side.

108
00:11:07,520 --> 00:11:13,600
And the reason for that is it's possible, because there is lots of signal in text and speech and

109
00:11:13,600 --> 00:11:18,800
acoustic, but just in text, there are a bunch of papers on that from that group I used to be in,

110
00:11:18,800 --> 00:11:26,080
from my colleague Gizhar Vaceci, and it's amazing what you can detect and predict just from text,

111
00:11:26,080 --> 00:11:31,920
whether like predicting that person gonna develop a psychotic episode, like within two years, or

112
00:11:32,000 --> 00:11:39,680
the person is on placebo versus MDMA, you just measure coherence, or you measure distance

113
00:11:39,680 --> 00:11:46,320
between the text vector and the vector for words like compassion and love, and 90% accuracy.

114
00:11:46,320 --> 00:11:52,160
MDMA is there. So many things you can detect, many things you can predict. Therefore, if you have

115
00:11:52,160 --> 00:11:59,760
your companion that kind of both tracks your mental states, but also kind of serves as your

116
00:11:59,760 --> 00:12:05,440
mirror, basically it extends you, you don't need maybe always to have human psychiatrists or

117
00:12:05,440 --> 00:12:11,840
psychologists, it can be a proxy at times when you cannot access the person, it's not going to replace

118
00:12:11,840 --> 00:12:18,240
person, but it can extend the capability of that therapist, and it can extend your capabilities

119
00:12:18,240 --> 00:12:23,280
in terms of like better understanding yourself or tracking yourself, and many other ways.

120
00:12:23,280 --> 00:12:29,840
Yeah, so essentially I want to expand functional capacities of our brain

121
00:12:30,880 --> 00:12:37,520
by using AI technology, and I think it's quite doable, and there are many, many other kind of

122
00:12:37,520 --> 00:12:43,280
ideas along the transhumanism, but essentially you're getting some symbiotic relationship with

123
00:12:43,280 --> 00:12:50,240
technology, and you kind of work together to hopefully have some good relationship, and that

124
00:12:50,240 --> 00:12:55,520
relationship is, I don't know, having positive effect on both parties.

125
00:12:55,520 --> 00:12:59,760
Yeah, so you want to improve human flourishing by, yeah.

126
00:12:59,760 --> 00:13:04,960
With AI flourishing in a sense, so you kind of have the healthy relationship with AI.

127
00:13:04,960 --> 00:13:12,400
But you said that you want a AGI to be less anthropocentric, but you, for the purpose of

128
00:13:12,400 --> 00:13:22,800
an anthropocentric goal. Well, I want AGI, again, with AI being augmented. Yes.

129
00:13:22,800 --> 00:13:31,120
Like, I'm less motivated by just the goal of creating a standalone, separate, and an

130
00:13:31,120 --> 00:13:38,080
intelligent creature. I mean, there are much faster ways to do this, right? People create AGI.

131
00:13:38,720 --> 00:13:44,000
Yes. Like, over, like, thousands of years. So in a sense, like, what, what is exactly

132
00:13:44,560 --> 00:13:49,440
the motivation? And it's maybe my personal thing, because whenever I have to write proposals, like

133
00:13:50,320 --> 00:13:56,320
research proposals, and people say that we're going to bring a GI to the AI to the next level,

134
00:13:56,320 --> 00:14:02,960
and this and that, and the question is like, and why are you doing that, right?

135
00:14:02,960 --> 00:14:07,360
Yes. Because unless it's something personal, it's very hard to keep yourself motivated.

136
00:14:07,360 --> 00:14:11,920
Like, what's so personal about that, right? If this thing can help me become

137
00:14:13,520 --> 00:14:20,800
hyper and better, and others and so on, I am much more personally motivated. I don't believe in

138
00:14:21,440 --> 00:14:27,200
abstract motivation, which is not related to yourself. Yes. Yes. Or maybe there is such

139
00:14:27,200 --> 00:14:34,720
thing. And basically, even altruism is selfish. Yes. Because you do it, it makes you feel better.

140
00:14:34,720 --> 00:14:39,440
Okay. And just quickly, something really interesting happens when you contrast

141
00:14:39,440 --> 00:14:43,760
different types of intelligence. So we have a mode of understanding and thinking and agency

142
00:14:43,760 --> 00:14:48,640
and intentionality. You contrast that with a very different rationality based artificial

143
00:14:48,640 --> 00:14:54,000
intelligence. And something very interesting might emerge from that. And then, yeah, I am

144
00:14:54,000 --> 00:14:59,760
pretty sure they're going to be all kind of paradoxes, like classical things in, like, you know,

145
00:14:59,760 --> 00:15:06,320
like the trolley problem and so on. So the rational decision that, yeah, you need to kill

146
00:15:06,320 --> 00:15:12,560
the person to save five people, right? Or like in this other side, five movies. Anyway, like,

147
00:15:12,560 --> 00:15:17,680
would you kill millions to save billions? So rationally, if you count things,

148
00:15:18,560 --> 00:15:22,240
well, again, it may be one type of rational answer, maybe you're not taking into account

149
00:15:22,240 --> 00:15:27,280
some other variables. So it may be not actually rational answer. But this classical example,

150
00:15:27,760 --> 00:15:32,880
this is rational, but human will not do that. Yes. Yes. So trolley problems, for example.

151
00:15:32,880 --> 00:15:39,520
The trolley problem is a classical example. And yes, so I don't pretend that I know the answer

152
00:15:39,520 --> 00:15:44,720
how this type of thing is going to be resolved. Yeah. But I think it's a good research question

153
00:15:44,720 --> 00:15:52,480
to precisely to figure out like, how can you take into account these different ways of reasoning?

154
00:15:53,280 --> 00:15:59,360
Yes. And how can you, I don't know, in some sense, combine the best of both worlds?

155
00:15:59,360 --> 00:16:06,080
Yes. And again, whoever is listening to that and who read my messages on Facebook and Twitter,

156
00:16:06,080 --> 00:16:13,280
I'm not against human emotions per se. I am only against, well, sometimes they call it the obsolete

157
00:16:13,280 --> 00:16:19,520
software stack developed by evolution that may need to be refactored, augmented or rewritten.

158
00:16:19,520 --> 00:16:24,480
Because there are parts of that software stack emotional that you probably would like to get

159
00:16:24,480 --> 00:16:32,480
rid of, right? Yeah. And probably if you did, many wars and other kind of disasters would have been

160
00:16:32,480 --> 00:16:40,400
avoided. So you couldn't say that the evolution found and built software that is absolutely ideal.

161
00:16:40,400 --> 00:16:45,600
So there are, I mean, there are things that can be improved. Absolutely. And then just final

162
00:16:45,600 --> 00:16:52,960
thing. So a completely rational, you know, AIXI agent, how would you program in these very difficult

163
00:16:52,960 --> 00:16:58,720
moral quandaries into that agent? Yeah, I don't think, first of all, it's possible to even program

164
00:16:58,720 --> 00:17:04,960
in ahead of time. They may just like this people, they in a sense develop. They develop because of

165
00:17:04,960 --> 00:17:11,280
some goals of like maintaining existence and flourishing. And for example,

166
00:17:11,840 --> 00:17:21,200
compassion is a byproduct of the selfish goal to survive in the group, because outside of a group

167
00:17:21,200 --> 00:17:25,520
it's much harder to survive. So you need to survive in the group. Therefore, you need to make sure

168
00:17:25,520 --> 00:17:31,760
that your actions are aligned with a kind of well being of the group. So in a sense, it's

169
00:17:31,760 --> 00:17:38,960
rational to be compassionate. Yeah. So it kind of emerges from interaction with environment

170
00:17:39,920 --> 00:17:47,520
under different circumstances. Under one type of circumstance, when you find and can survive alone,

171
00:17:48,720 --> 00:17:54,800
maybe you will not develop it. I mean, it's a separate interesting topic, like basically it goes

172
00:17:54,800 --> 00:18:02,880
back to the question whether they think like objective ethics exist. And I'm not an ethicist,

173
00:18:02,960 --> 00:18:10,800
I'm not a philosopher. I'm quite, I'm an admirer of people like Derek Parfit. I'm not the only one.

174
00:18:11,360 --> 00:18:18,080
But it's a hard question. He didn't finish on what matters. He was trying to come to the same

175
00:18:19,040 --> 00:18:26,000
summit on different sites and trying to unify ethics, trying to see if you can develop objective

176
00:18:26,000 --> 00:18:32,720
ethics. I don't think we know for sure if it's possible. I think it's possible for some particular

177
00:18:32,800 --> 00:18:39,760
domains. And in certain situations, you can clearly say that certain behavior is objectively ethical

178
00:18:39,760 --> 00:18:44,960
and everybody would agree on those people. But it's hard to talk about those things at such

179
00:18:44,960 --> 00:18:53,600
level of generality. But I think if maybe I managed to include Derek Parfit and to recommend

180
00:18:53,600 --> 00:18:59,200
the readings for my scaling and alignment course this winter, it's on the website from the last

181
00:18:59,280 --> 00:19:04,800
year. People just didn't read it. I think it might be a good topic for discussion there too.

182
00:19:04,800 --> 00:19:09,920
But again, objective ethics is a difficult open research question.

183
00:19:09,920 --> 00:19:13,680
Indeed it is. Irina, thank you so much. I hope I can grab some more time with you tomorrow,

184
00:19:13,680 --> 00:19:17,440
but I really appreciate this impromptu discussion. Thank you.

185
00:19:17,440 --> 00:19:19,840
Amazing. Thank you very much indeed. Okay.

186
00:19:19,840 --> 00:19:27,120
Okay. Another analogy. There was a very interesting story by Fort Heluiz Borges,

187
00:19:28,240 --> 00:19:34,240
Garden of Forking Pass. I don't know if you've read it. I don't want to spoil the story, but

188
00:19:34,240 --> 00:19:41,440
roughly speaking, it's about a book written by an emperor, I think in China a long time ago,

189
00:19:41,440 --> 00:19:46,800
which didn't make sense. It was a complete intersection of different trajectories of

190
00:19:46,800 --> 00:19:51,600
different lives. And then basically the point is that somebody was trying to describe all possible

191
00:19:51,600 --> 00:20:00,160
trajectories that events can happen in and so on. And the story is called the Garden of Forking Pass,

192
00:20:00,800 --> 00:20:08,400
meaning that at any point of time there is a whole tree that can grow out of that.

193
00:20:08,400 --> 00:20:14,080
And we don't know which kind of trajectory in the tree will be taken and so on. But

194
00:20:14,880 --> 00:20:19,920
the fact that there is always this tree, right, and it keeps branching at every moment.

195
00:20:20,560 --> 00:20:26,080
And at every moment you can make, you can take certain direction or you can take another one.

196
00:20:26,640 --> 00:20:30,720
It has not even anything specifically to do with alignment. But I was thinking about

197
00:20:30,720 --> 00:20:36,560
history of deep learning, right? Like at some point it happened that backtracking,

198
00:20:36,560 --> 00:20:41,760
I mean, I mean, back propagation became popular at work and everybody got into that.

199
00:20:41,760 --> 00:20:47,280
And now everybody using back propagation because it's convenient, because software is implemented,

200
00:20:47,280 --> 00:20:52,000
it doesn't have to be this way. There are non-backprop based approaches to optimization.

201
00:20:52,000 --> 00:20:55,840
I mean, I'm a little bit subjective maybe because I was interested, I was looking into them,

202
00:20:55,840 --> 00:21:00,880
we have a few papers on that. There are other papers. But that direction that could have been

203
00:21:00,880 --> 00:21:04,960
explored, it could have been probably much more efficient and better parallelizable. It wouldn't

204
00:21:04,960 --> 00:21:10,800
have the chain of gradients. You would probably do it much better for scaling large models.

205
00:21:10,800 --> 00:21:17,840
It's underexplored. Why? Because the branch was taken and became stronger, you know,

206
00:21:17,840 --> 00:21:21,520
the usual, the reach gets richer. And so with other ideas.

207
00:21:21,520 --> 00:21:26,400
This is the hard, Sarah Hooker calls that the hardware lottery. It's basically, it's like

208
00:21:26,400 --> 00:21:31,120
we are bound by the decisions and ideas of the past. And yeah.

209
00:21:31,120 --> 00:21:32,400
It doesn't have to be this way.

210
00:21:33,200 --> 00:21:37,280
No, but the thing is you get stuck in these basins of attraction and the further you get

211
00:21:37,280 --> 00:21:42,000
into the basin, the harder it is to jump out of it. I mean, I share your, your intuition.

212
00:21:42,000 --> 00:21:46,480
There's stochastic gradient descent. It's amazing. And it's also a basin of attraction

213
00:21:46,480 --> 00:21:51,120
because having these differentiable models allows us to learn and scale. But there's an

214
00:21:51,120 --> 00:21:54,560
entire class of function spaces that we're excluding ourselves from being able to.

215
00:21:54,560 --> 00:22:02,800
There is also another class of neural networks that are not our classical second kind of

216
00:22:02,800 --> 00:22:07,920
generation ANNs and this good old, it doesn't have to be necessarily spiking,

217
00:22:07,920 --> 00:22:12,160
but like a third generation ANNs, which are like reservoir computing, any of that.

218
00:22:12,160 --> 00:22:17,920
So anything that tries to take into account time between activations or at least sequence,

219
00:22:17,920 --> 00:22:20,640
because think about that. I mean, a good classical argument.

220
00:22:21,680 --> 00:22:26,080
Yeah, SDTP, this is the spiking biologically inspired neural networks.

221
00:22:26,080 --> 00:22:31,920
It may be not necessarily spiking, but it might not necessarily kind of be the best thing.

222
00:22:31,920 --> 00:22:38,800
But the idea that like what always was bothering me with classical neural networks is that

223
00:22:39,840 --> 00:22:46,240
brain is constantly active. It's like complex dynamical system. Even if you sleep and don't

224
00:22:46,240 --> 00:22:54,400
have input, you don't see any images, it still is active unless you're dead. Yes. Neural nets

225
00:22:54,400 --> 00:23:00,240
are not. They sit there waiting for the next, I don't know, amnesty image to appear or something.

226
00:23:00,320 --> 00:23:06,080
And then between there is no internal dynamics. And yet from your science, we know that the properties

227
00:23:06,080 --> 00:23:10,400
of that dynamical system without any input, so called the kind of resting state of amaranth,

228
00:23:10,400 --> 00:23:15,200
so I mean, I used to work in brain imaging and this computational psychiatry group at ABM.

229
00:23:15,200 --> 00:23:19,280
That's where it comes from. And it was not just neuroscience, but it was like working with

230
00:23:19,280 --> 00:23:26,560
former physicists. So the view at the world and at myself as a year and other complex dynamical

231
00:23:26,560 --> 00:23:35,600
system, after 10 years there, it just really converted me. So think about that. Changes in the

232
00:23:35,600 --> 00:23:42,240
dynamics are also associated with mental disorders, this and that. So they're really important,

233
00:23:42,240 --> 00:23:50,560
like what are the parameters of this dynamical system? Input to the system combined with this

234
00:23:50,560 --> 00:23:55,200
produces output. But again, it's even in the neuroscience, there is this perception and there

235
00:23:55,200 --> 00:24:02,320
is a book, The Brain Inside Out by Tuzaki that says, guys, the output that you produce

236
00:24:02,320 --> 00:24:07,280
is determined a little bit by the input and to a large extent by the state of the system.

237
00:24:08,080 --> 00:24:12,880
That's why you say same thing to different people and some laugh, some ignore and some

238
00:24:12,880 --> 00:24:17,120
get like ballistic and so on and so forth. So are you not a behaviorist?

239
00:24:19,280 --> 00:24:23,840
In what sense, behaviorist? So you care about the state of the system as well as just the

240
00:24:23,840 --> 00:24:28,720
output and the input? Yeah, I mean, it's not just input to output. And that's a whole point.

241
00:24:28,720 --> 00:24:34,720
The neural net is a function. The function is deterministic, given input, it will produce

242
00:24:34,720 --> 00:24:43,200
output. Brain is not that. There is input, it will produce output. And depending on the huge

243
00:24:44,080 --> 00:24:51,280
hidden state of the system and parameters of this dynamical system, that will determine output to

244
00:24:51,280 --> 00:24:56,240
large extent. That's why I mean, Tuzaki was criticizing neuroscientists and all these experiments

245
00:24:56,240 --> 00:25:02,080
that let's provide stimulus and see how the stimulus will affect the brain and what gonna

246
00:25:02,080 --> 00:25:08,560
light up and activate. So it was outside in. So like, what's going on guys? It's inside out.

247
00:25:10,240 --> 00:25:17,440
Things happen and that produces stuff. So it's not like the world programs you only, but you

248
00:25:18,400 --> 00:25:22,880
have programs of the world, right? So at least you need to take that into account. Neural nets

249
00:25:22,880 --> 00:25:30,000
now are not doing that. There is no dynamics. So you said a couple of really interesting things.

250
00:25:30,000 --> 00:25:34,240
So first of all, about the tree, which is to say all of the counterfactual trajectories that you

251
00:25:34,240 --> 00:25:38,400
can make. Now, Chalmers, by the way, he says that it's that, the counterfactual trajectories that

252
00:25:38,400 --> 00:25:45,440
gives rise to consciousness in his conscious mind. But I wanted to ask you, because I'm interested

253
00:25:45,440 --> 00:25:48,960
in intentionality and free will, because what you're basically saying there, you're, you're,

254
00:25:48,960 --> 00:25:53,280
you're getting to this issue of intentionality. So, you know, in, in silico, what, what would

255
00:25:53,280 --> 00:26:00,640
intentionality entail? Yeah. Okay. Don't ask me about free working. Is that a tricky one?

256
00:26:02,160 --> 00:26:09,440
Well, yeah. I don't know. I don't have like clear cut answer to large extent. I mean,

257
00:26:09,680 --> 00:26:16,880
it's determined by the current state of your dynamical system. So the question is like,

258
00:26:16,880 --> 00:26:24,480
what is free will? But I know it can go very far. And remember my colleague,

259
00:26:24,480 --> 00:26:30,080
Kishir Macheshi at ABM used to say that kids these days, like my five year old says,

260
00:26:30,080 --> 00:26:35,360
after doing something wrong, my neurons made me do it. Not my fault.

261
00:26:35,920 --> 00:26:41,920
Yeah. So in a sense, yes. And in a sense, no. And it's a good question. And then I was also

262
00:26:41,920 --> 00:26:49,360
reading the article of SBF's mom, who wrote about punishment, essentially guilt, punishment,

263
00:26:49,360 --> 00:26:56,160
assigning. I'm very much with her on that one. Okay. But, but that's probably a popular opinion

264
00:26:56,160 --> 00:27:00,880
these days. You said something else fascinating, which is that my neurons made me do it, which is,

265
00:27:00,880 --> 00:27:04,640
you know, like a microscopic level of analysis. Now, what, what do you think about?

266
00:27:06,240 --> 00:27:08,960
No, but it's beautiful. It's beautiful. So what do you think, you know, you know,

267
00:27:08,960 --> 00:27:13,680
the mind emerges, you know, when you read a book, the story, it's written on the page,

268
00:27:13,680 --> 00:27:16,880
but the story emerges in your mind, right? Because the mind is this kind of

269
00:27:16,880 --> 00:27:21,360
confection of information processing. So do you think this conception of the mind

270
00:27:21,360 --> 00:27:24,480
is useful for AI? Or is that just again, an anthropomorphic thing?

271
00:27:25,520 --> 00:27:29,760
I think it is. Well, you know, you know, go buy people try and create the mind.

272
00:27:29,760 --> 00:27:33,680
And, and we, as, as neural network people, we try to recreate the brain.

273
00:27:34,640 --> 00:27:39,680
And not exactly. I think everybody, not everybody. Okay. So I should never say

274
00:27:40,400 --> 00:27:51,440
ever everybody and so on. But I think, I think neural network people assume that we are working

275
00:27:51,440 --> 00:27:59,520
on the system one level, right? At a low level. And we would like the properties of system two,

276
00:27:59,600 --> 00:28:06,160
which is well, mind planning and thinking emerge. And there is a reason to believe it's possible

277
00:28:06,160 --> 00:28:10,560
because it's already happened once with this hardware. It might happen with other hardware,

278
00:28:10,560 --> 00:28:15,920
right? So it doesn't have to be like go five people. The problem is go five people, they're

279
00:28:15,920 --> 00:28:22,880
trying to manually program that stuff, the system to and like a neural network people would like

280
00:28:22,880 --> 00:28:27,600
that thing to emerge. And that's kind of the main difference. It's just like a bitter lesson.

281
00:28:28,240 --> 00:28:34,560
A message that maybe, well, first of all, history shows that every time you

282
00:28:34,560 --> 00:28:37,520
hard code something in like rule based expert system,

283
00:28:38,240 --> 00:28:43,280
you will be outperformed later on by something which is more generic and kind of emerges.

284
00:28:44,320 --> 00:28:50,800
You hard code whatever tricks of playing chess, you will be outperformed by massive search

285
00:28:50,800 --> 00:28:55,440
and so on and so forth. Same with alpha go like self playing bottom like he says, like,

286
00:28:55,520 --> 00:29:02,240
it's not like we have to ignore the nature. But maybe again, it might translation of

287
00:29:02,240 --> 00:29:09,200
Richard's kind of bitter lesson. Because I often have to argue with your show about inductive

288
00:29:09,200 --> 00:29:13,680
biases. I said, look, I'm nothing against inductive biases, but you can have inductive

289
00:29:13,680 --> 00:29:20,160
bias in the form of rule based expert system that everything is encoded. And that's probably

290
00:29:20,640 --> 00:29:27,360
not going to scale and not going to work. Or you can have inductive bias of much higher

291
00:29:28,000 --> 00:29:34,080
abstract level of how the network scales. So the scaling algorithm is more efficient.

292
00:29:34,080 --> 00:29:40,480
And you end up with this brain rather than whale brain. So like Richard's last paragraph

293
00:29:40,480 --> 00:29:46,000
was precisely maybe we shouldn't be trying to focus on the end result of evolution.

294
00:29:46,000 --> 00:29:51,920
But on the process, it's also can be called inductive bias. There is also some patterns

295
00:29:52,480 --> 00:29:58,400
of how dynamical systems evolve so that the result will be good. But we don't have to encode

296
00:29:58,400 --> 00:30:03,040
the final result. Yes. So you said so many really interesting things there. So first of all, I'm a

297
00:30:03,040 --> 00:30:09,520
huge fan of Yoshua's G flow nets we interviewed him. Absolutely amazing work. So you were talking

298
00:30:09,520 --> 00:30:14,560
about isn't it interesting that you can start at the microscopic level and then you get these

299
00:30:14,640 --> 00:30:18,800
emergent functions like reasoning and planning and so on. And even that was a bit of an insight

300
00:30:18,800 --> 00:30:24,080
because it's a functionalist view of intelligence to say, you know, it's a bit of if you read Norvig

301
00:30:24,080 --> 00:30:27,840
that he talks about planning talks about reasoning talks about sensing. And actually,

302
00:30:27,840 --> 00:30:32,880
this is just our view of what is a very complex phenomenon. And I know you're a big fan of the

303
00:30:32,880 --> 00:30:37,760
blind men in the elephant, right, which is to say that even though this is our view from different

304
00:30:37,760 --> 00:30:44,160
perspectives, it's all it's all true, isn't it? But to some extent, the intelligence that emerges

305
00:30:44,160 --> 00:30:50,320
might just be beyond our cognitive horizon. Like, does it make sense to talk about reasoning

306
00:30:50,320 --> 00:30:58,480
in your view? Well, again, just like with that elephant, each person has a point. Yes. So I mean,

307
00:30:58,480 --> 00:31:04,480
there is such thing as reasoning. You cannot say that it's totally like bogus or something. It

308
00:31:04,480 --> 00:31:12,400
might be again, it's one perspective. Maybe it makes sense to just try to accumulate multiple

309
00:31:12,400 --> 00:31:18,720
perspectives instead of so maybe we should be Bayesian instead of like trying to find a point

310
00:31:18,720 --> 00:31:26,800
estimate of AGI, right? You can have a distribution of views. Yeah. And I'm a big fan of Eastern

311
00:31:27,520 --> 00:31:39,760
as opposed to Western views. Then anti individualist. As in viewing everything like that happens to you

312
00:31:39,760 --> 00:31:45,440
and to the world as well, a large dynamical system. And yes, you are a particle of that.

313
00:31:45,440 --> 00:31:52,080
Yeah. So it's almost issuing individual agency. Yeah. So in a sense, it's yes and no because,

314
00:31:52,080 --> 00:32:01,440
okay, so when people say there is no self, again, yes and no, there is self. But you also understand

315
00:32:01,520 --> 00:32:10,640
that it's like in the whole hierarchy of selves, like there is you and you're part of that larger

316
00:32:10,640 --> 00:32:17,680
dynamical system and so on. So I how to say, I mean, I'm not saying that back to your question that

317
00:32:17,680 --> 00:32:24,800
we shouldn't be looking into reasoning, functionality as aspect of intelligence that we may

318
00:32:25,920 --> 00:32:30,480
want to develop. Yeah. So I mean, I don't see a problem with that. Yeah. I mean, it might be

319
00:32:30,560 --> 00:32:33,760
a sufficient condition, but not a necessary condition. Yeah. But basically,

320
00:32:34,720 --> 00:32:39,200
basically intelligence or consciousness is probably much more than that and definitely

321
00:32:39,200 --> 00:32:46,480
much more than reasoning. And here we go to another topics that I really like to talk about.

322
00:32:46,480 --> 00:32:52,240
But yeah, I don't want to keep everyone. I'm a big fan of Michael Levine who you might

323
00:32:52,800 --> 00:32:56,320
desperate to get him on the podcast. And yeah, because we've done lots of stuff on

324
00:32:56,320 --> 00:33:02,640
emergence recently, cellular automata, self-organization, and his take on it is absolutely fascinating.

325
00:33:02,640 --> 00:33:08,560
So yeah, his talks are fascinating here. I think I met met him first at New Reeps 2018.

326
00:33:08,560 --> 00:33:14,240
He gave the plenary talk. What bodies think about the point was, guys, if you talk about

327
00:33:14,240 --> 00:33:21,760
intelligence as something that emerges in cellular networks like neural networks way before

328
00:33:21,760 --> 00:33:28,960
neurons appeared, other kind of more primitive types of cells had their bioelectric communication

329
00:33:28,960 --> 00:33:35,200
in their networks and that determined what they remember and how they adopt. He focuses on

330
00:33:35,200 --> 00:33:40,320
morphogenesis, basically how the organism takes shape. And that relates to like embryonic

331
00:33:40,320 --> 00:33:46,000
development and so on and so forth. And the point is that if you look at that from the

332
00:33:46,000 --> 00:33:52,320
dynamical system point of view, and if you say that properties of the system like shape

333
00:33:53,040 --> 00:33:58,800
will emerge out of communication across those cells in certain way, that certain parameters

334
00:33:58,800 --> 00:34:07,200
of dynamical system, if you tweak that dynamics and he basically he was doing some simulations of

335
00:34:07,200 --> 00:34:11,760
where he want to intervene, how he will intervene, like chemical interventions just

336
00:34:11,760 --> 00:34:18,640
close open some ion channels. Cellular kind of system starts working in different way.

337
00:34:18,640 --> 00:34:25,280
And this is essentially his way of programming biological computers and the famous two-headed

338
00:34:25,280 --> 00:34:33,280
worms, three-headed worms and whatever stuff. And point was like, guys, like evolution found this

339
00:34:34,000 --> 00:34:39,520
solution or this solution wonderful. There are many others and there may be better ones.

340
00:34:40,320 --> 00:34:46,560
And look at that two-headed worm. It's not a fluke. It's a stable attractor that replicates.

341
00:34:48,240 --> 00:34:54,560
And evolution didn't create ever anything like that. We did. And it's stable. So it makes you

342
00:34:54,560 --> 00:35:00,640
think what else can you do if you start reprogramming it, right? But yeah. Two questions on that though.

343
00:35:00,640 --> 00:35:05,520
So I don't know whether you've seen that there's that example from Alex Mordvintsev with a gecko

344
00:35:05,520 --> 00:35:11,840
and it's a CNN cellular automata. And now we're in this regime where we're transgressing

345
00:35:12,640 --> 00:35:17,280
rungs of the emergence ladder. So we're creating a high resolution cellular automata. And then

346
00:35:17,280 --> 00:35:22,240
even though it's only doing like local message passing, we get this emergent global phenomenon

347
00:35:22,240 --> 00:35:26,960
of a picture or a lizard or whatever. And now when you build systems like this, they can repair

348
00:35:26,960 --> 00:35:31,120
themselves. They can heal themselves. They have interesting dynamics. But as you're saying,

349
00:35:31,120 --> 00:35:35,280
we don't understand the macroscopic phenomenon and we can only nudge it because it's not it's

350
00:35:35,280 --> 00:35:49,440
unintelligible to us. Right. Anyway, it's a whole kind of complex systems, science of complex system.

351
00:35:49,440 --> 00:35:55,760
Like, yeah. And basically, how do you program dynamical systems across multiple variants

352
00:35:55,760 --> 00:36:02,960
by local interventions? So they will take the global properties that you would like. Yes. And

353
00:36:02,960 --> 00:36:09,680
avoid those. I mean, this relates to everything it relates to the classical mohawk problem, right?

354
00:36:09,680 --> 00:36:15,600
What is mohawk problem? It's a complex dynamical system that with the current dynamics is getting

355
00:36:15,600 --> 00:36:24,400
into bad attractor. And most likely the way to get out is coordinated, simultaneous, distributed

356
00:36:24,400 --> 00:36:29,440
action and so on. So again, we're not going to go there because I have to run, unfortunately,

357
00:36:29,440 --> 00:36:34,560
but I'd be happy to. Yeah, I have some plans. I don't want to be late, but I'd happy to talk

358
00:36:34,560 --> 00:36:41,120
about that. And I mentioned, I mentioned Michael Levine also, not just because of two-headed worms

359
00:36:41,120 --> 00:36:47,920
with each father, but also because we talked about self. And we talked about in a sense hierarchy

360
00:36:47,920 --> 00:36:54,960
of selves and like what self means and how selves organize into larger selves. And we had

361
00:36:54,960 --> 00:36:59,680
an amazing discussion with him. I invited him to IBM Research when I was there three years ago

362
00:36:59,680 --> 00:37:07,360
after his talk. I talked for five hours. It was great. And the idea basically, to some extent,

363
00:37:07,360 --> 00:37:14,640
was that you can, he was also giving examples, not just of embryos, frogs and those worms, but

364
00:37:14,640 --> 00:37:21,440
cancerous cells. If you look at them, like what's going on when historically cells

365
00:37:21,440 --> 00:37:29,200
emerge like is independent selves and everything around them is non-self. And therefore, self to

366
00:37:29,200 --> 00:37:36,320
survive tries to eat and use everything around, which means non-self. But when the cell becomes

367
00:37:36,320 --> 00:37:43,200
part of the network of the organism, then it changes behavior so that it kind of supports

368
00:37:43,280 --> 00:37:50,080
the well-being not just of that self, but the larger self it is part of now. What is cancer cell?

369
00:37:50,080 --> 00:37:57,600
It's a cell that forgot it's part of the community, reverted to its old state of being cell in the

370
00:37:57,600 --> 00:38:05,280
environment that is just environment. So, and it tries to eat it to survive. And it's stupid,

371
00:38:05,280 --> 00:38:11,600
in a sense, because its objective function, survival thrive, is right. It just applied at

372
00:38:11,680 --> 00:38:16,960
wrong scale. It's spatial scale reduced, and it's temporal scale reduced too, because like,

373
00:38:16,960 --> 00:38:21,760
if you kill the organism, you'll live and you'll die. So, in order to understand that,

374
00:38:21,760 --> 00:38:27,280
you need to apply objective function to longer time scale. And then you get the hierarchy from

375
00:38:27,280 --> 00:38:34,880
cells, you get to organs, like to whatever particular organisms, to societies, to planet,

376
00:38:35,840 --> 00:38:42,800
to universe, and I say, Michael, so this is a good formulation of Buddhism. Basically,

377
00:38:42,800 --> 00:38:52,240
Buddhism means applying this function at the infinite time and space scale. Agreed. Yeah, so yeah,

378
00:38:52,240 --> 00:38:56,880
ever since I was saying, I'm going to write a book about Buddhism for machine learning.

379
00:38:57,440 --> 00:39:04,240
And somehow it just didn't happen yet. But I should. You should do it. It was so nice to meet you.

380
00:39:04,240 --> 00:39:09,680
Well, nice to meet you. I'll see you tomorrow. And I'm really sorry I have to run. But tomorrow,

381
00:39:09,680 --> 00:39:12,240
yeah, yeah. That was amazing. That was a really good interview.

