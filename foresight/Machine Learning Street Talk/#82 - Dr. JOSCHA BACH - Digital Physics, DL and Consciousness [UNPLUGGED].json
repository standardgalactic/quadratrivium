{"text": " Welcome back to Street Talk, just a little bit of housekeeping before we kick off today. Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine. Now, their main goal is to raise funds for Ukraine, both from the folks attending the conference and also from companies sponsoring the conference. And it's not too late to sponsor the conference and support it, so please do if you possibly can. Now, all of the funds that they raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling, Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD, Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to raise $100,000 and they really, really need the support of the AI community to club together and to just donate anything that you can. So, we'll link to the conference in the video and the podcast description. Please donate if you can and also share the links on your socials. Now, I'm at New Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me and if you want to record any spicy takes on artificial general intelligence, then let's do it. Today is a conversation with Yoshua Bach, who's one of our most requested guests ever. We recorded the conversation back in April, which gives you a bit of an indication of our backlog. I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray and as you can imagine, we've been working around the clock coding, just trying to get that business off the ground. But when we've made our millions, we'll devote all of our time to producing amazing content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way soon. I hope you enjoy the show today. Peace out. Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing on cognitive architectures, models of mental representation, emotion, motivation and sociality. Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast, have been watched over two million times so far, which is just absolutely unreal. Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your most important views in our shared space, to the extent that we can keep up with you, of course. Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics, free will and determinism, large statistical models and indeed whether they're AGI or a parlor trick or something more esoteric. Now, when people talk about God or consciousness or any other complex phenomena, it relates to everyone and it means something different to everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion, which is to say extremely low information content. Now, the topics we're discussing today are very complex and often we're reaching for the best language to use to conduct the conversation. So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing computer science many years ago, interestingly, the theory of computation wasn't even on the curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain everything like we're five. What does computation mean to you? I think that computation is far easier than most people think. It means that you have a causal structure where every transition can be decomposed into individual steps. And when we talk about computational models, we decompose the world into states and transitions between the states. And then it turns out that there is a certain minimal system that is able to execute everything. And this can be described in many ways. The most famous one is probably the Turing machine and many other ways in which you can describe the Turing machine. For instance, you can just do the Turing machine by doing search and replace on strings. And this is how the Lambda calculus is defined. And all the programming languages and the Lambda calculus and the Turing machine turn out to have the same power. That is, if you can compute something with one of these paradigms, you can compute it with the others. As long as you don't run into resource constraints, so as long as it still fits into memory and you don't care about speed, they all have the same power. But in practice, of course, every system is limited. So we don't run things forever. We want them to give us a result after a certain time. So what matters is what can be efficiently computed, not what is reachable at all. Awesome. And there are, and we're going to get into this a bit, but there are some possible loopholes, at least in, you know, let's say whether or not the universe is limited in certain ways that the definitions of like Turing machines are. And one I wanted to ask you about specifically is Penrose's claims. And so he claims that what Godel's work in fact proves is that the human mind can understand truths that are not provable. So specifically one can show that, you know, given Godel's sentence is necessarily true given given a mathematical analysis, even though it can't be proven within the formal system that it's that it's defined. And Penrose claims that this capability to understand, if you will, to mathematically understand is in fact non computational, at least in part. And so if he's right, then our brains might be what Turing referred to as Oracle machines. These are computers that have access to a non computable Oracle or function that they can then utilize those oracles in order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this possibility? And if not, what is your response to Penrose's arguments? I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth realist. That is, he thought that truth really exists out there. That it's the thing that is eternal in some sense. He had this very strong intuition and mathematics classically is also formalized in this way. The difference between mathematics and computation, at least in the standard sense in which we normally teach mathematics at school is that mathematics has no states. Everything in mathematics just is eternally. It's a single state. And if you want to go through a sequence of states, you put an index into the formula. But still, everything is there at the same time. The index is just a way to access this thing. And this way of having mathematics stateless is very elegant because it allows us to define functions that have infinitely many arguments. If you would have a state machine that tries to consume infinitely many arguments, it would never finish before it goes to the next step. And the same thing in the middle of the function, if you compute something, if it's stateless, you can just compute all the indices all at once, even if it's infinitely many in classical mathematics. In a computational system, you would have to do this maybe one after the other. And if you do it at parallel, you will have lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So in the classical mathematics, you can chain infinitely many steps and functions and exchange infinitely many arguments. But of course, mathematicians never did this a practice. It's just a specification. This is how they like to write things down. When they want to calculate it, they still have to go down and do it sequentially step by step. Just mathematics is defined in such a way as if you could upload this to some supernatural being or some grad student who is going to do the infinitely many calculations. And Goethe took this specification of mathematics and he found out that when you have this stateless mathematics, you can, for instance, define self referential statements that change that choose value depending on the statement itself. And this recurrence leads can lead to a contradiction in the state itself. So you basically get two statements which say I am wrong. And if by referring to itself, it changes its own truth value. So if mathematics is stateless, you will now run into a conflict. In a computational system, that's not a big problem. Your computer is not going to crash. If you write it down the right bay, it just happens is that your truth value fluctuates in every execution step. It's not going to converge. But this is not the real truth. Truth is something that doesn't change when you call the function again. So what's going on here? And I think what Goethe has discovered is that classical mathematics doesn't work. What you cannot build is any kind of mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of universe that runs the semantics of the classical mathematics without crashing. Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction in there. Like you either believe mathematics, and thus you believe Goethe's proof of some specific limitations on computational systems, right? Or you believe that somehow mathematics is flawed, in which case you can't trust the proof that mathematics is flawed. I think Goethe's conclusion was that there is something fundamentally going wrong, that there might be an inability of mathematics to describe reality. And if you believe that truth is real and it exists independently of the procedure by which you calculate it, then this seems to be plausible. And it was also the conclusions a lot of philosophers have drawn from this, which basically read Goethe's proof and concluded that mathematicians have admitted that their arcane techniques are important to describe reality. And therefore, philosophers who don't understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead, what turns out is that if you just skip or if you drop the original classical notation or as understanding of mathematics and replace it by computation, basically we say, truth is what you calculate with the following procedure. And you can define any kind of procedure that you want. You just have to make sure that it converges to some kind of value in the way that you want. Then you resolve your problem. It's just that you lose your notion that truth is independent of that procedure. And so in some sense, the classical mathematics is a specification that cannot be computed. From the perspective of computer scientists, this happens all the time. Some customer wants you to build something that cannot be built, and you have just proven that it cannot be built. Right? But it doesn't mean that you cannot build something useful. And I think that Penrose believed that our brain is actually doing these infinite things. And it's not. When we reason about infinity, we are not actually reasoning about infinitely many steps. What we do is we create a symbol, and then we do very finite computations over that symbol. But we cannot construct infinity. We cannot build it. We cannot go there from scratch and write down some clever automaton that produces an infinity for you. I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean, I don't know that much about physics, but the chapters were really interesting. They're talking about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions, calculus, matrix theory, and even computation. I mean, almost all of the discussion was on mathematical modeling at different levels of description or emergence, if you will. And in machine learning and AI, we are forever challenged by trying to get machines to model physical reality at different levels of description using an interoperable set of tools. So it seems increasingly true that we need machines that can learn descriptions and concepts at multiple levels if we're ever going to have AGI capable of understanding the world and learning novel semantic models. All of machine learning models today work by chopping up a Euclidean space into what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go far beyond this. It's got to be able to learn novel geometries beyond even what humans could have come up with and the ability to reason topologically and algebraically over those geometries. Something which I think you would agree is not happening with the current deep learning systems. Well, let's start out with the notion of geometry first. If you read Penrose's book, what you find is that this entire universe is geometric, which means it's made of continuous spaces in which things are happening. And if we actually look into the world deeply, quantum mechanics is not a geometric theory. The geometry only emerges approximately at the level of the space-time description. And it seems that geometry is actually the domain of too many parts to count. In reality, all the objects that we describe as surfaces, if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn are made out of things that have a finite resolution. And if we look into our computer programs, you can create stuff that looks continuous to us, but there's nothing continuous inside of our computer programs. And it turns out that the assumption of continuity requires that we partition the space into infinitely many parts. So now we are again running against that thing which G\u00fcrl has shown us as difficult. And it's not a big problem in practice because in practice, we never need to do these infinitely many things to produce a computer game with an arbitrary fidelity. We can make something that looks like space. But the space that we think in and so on is an approximation that our brain has discovered. It's a set of operators that converge in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is made of too many parts to count for almost everywhere where you look, you need to find these operators that converge in the limit. And the set of operators that happens to converge in the limit and is still computable. This is what we call geometry. And to use these uncomputable geometric approximations for macroscopic physics like Newtonian mechanics is completely fine. You're just going to compute it up to a certain digit and then this is good. But it's a problem for foundational physics. Because if it turns out that you cannot take a language that actually computes infinities, if you cannot construct your language, then you cannot write a universe in it. So our universe is not written in continuous language, but Penrose universe is. This doesn't mean that geometry is full. We need this to describe the world of too many parts to count. But we do this via computational approximations. Our brain does the same. So let me ask you this then because we come across kind of the infinities a couple of times. And I know that you placed an emphasis on constructive mathematics. So of course, you and all of us, you know, except let's say the existence of potential infinities, you know, algorithms that you can sit there and just keep calculating for as long as you want and get kind of more digits. But it's really around actual infinities that we seem to be running into problems. So let me ask this this first question here, really leading up to some computational questions, which is, can the universe, can our actual universe that we're in right now be actually infinite in spatial extent? A problem is that it can have unboundedness in the sense that you have a computation that doesn't stop giving your results. But you cannot take the last result of such a computation and go to the next step. You cannot have a computation that relies on knowing the last digit of pi before it goes to the next step. In the sense that you don't have an infinity. But the infinities are about the conclusion of such a function. It means that you actually run this function to the end and then do something with the result. Unboundedness is different in the sense that you will always get something new that you didn't expect that they cannot predict. But it's just going on and on without this end. And I think it's completely conceivable that our universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there is anything that gives you the result of an infinite computation. Because if that was the case, then it could not be expressed in any language. It also means if something cannot be expressed in any language, that you cannot actually properly think about it. Because when you think you need to think in some kind of language, not in English, but in some kind of language of sort or in a mathematical language that doesn't have contradictions. And what Goethe has shown is that the language that he hoped to reason in about infinities breaks that it has contradictions in it. That at some point, it blows itself apart. So the languages that we can build are only those in which we have to assume that infinities cannot be built. So infinity, in this sense, is meaningless. Because we cannot make it in any kind of language. So the thing is, though, I'm not limiting what the universe is capable of based on human mental and linguistic limitations or even mathematical limitations. I'm asking you if it's possible for this universe that we're in to ontically be right now actually infinite in spatial extent. The thing is that you try to make a reference to something that you cannot observe, that cannot conceive of other than making a model in some kind of language. And to have that model make sense, the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing. And we can construct delusional things. We can construct languages that have bugs that we cannot see. But if we use a language that has bugs in it that we cannot see and we cannot repair them, then this means that the stuff that we express in the language is not meaningful. Right? We have to use a different language that has maybe the same expressive power but doesn't have these bugs. But now if you try to think about the universe in the language that allows you to imagine that the universe is literally infinite, rather than very, very, very big and much bigger than you can imagine and not ending, which is for all means and purposes almost the same thing. Right? Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot properly express the idea in your own mind without running into contradictions that the universe is infinite in the sense that such a universe could exist. Okay, so you're basically following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot express this. That's my issue. Okay, fine. So you're basically saying that the English that I used just a minute or so ago just is not coherent or not conceivable. It's not something that you want to. But the underlying thing behind the English, right? English is not designed to be coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to express things vaguely and not break. But if you think really, really deeply and really exactly, then the question is, what kind of model is your mind building? At which point is there just some kind of noisy nabler that you're pointing at without actually decomposing it and anything that would make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to ontically exist in some sense, if we just deny all that, so we're really just stuck with, all right, we've got finite everything, discrete everything. There's no such thing as a continuum. There's no such thing as actual infinite spatial extent, etc. That's really the world that you're proposing here, right? That everything is constructed from at the end of the day, finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library of functions in a way, and these functions are doing jobs. And on the bouts of the box, you write down what these functions are doing. And you construct a box that this, in this box, there is an infinity between, for instance, a continuum between two points. And then you open up the box and look at what's actually inside of the box. And you realize it's just a lot of small steps. And it's designed in such a way that you can, if you want to have more steps, it's going to give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down on the box. But if you look very closely, realize, oh no, the thing that is written down on the box that you have written down on the box cannot actually be in there. You can prove that it cannot be in there. It must be something else that's in there that is doing most of the work of what you've written down. So what you should actually be doing, I think, if you are interested in how things actually work, write on the box what it's actually doing, which means it's going to subdivide or any interval with any resolution you want as long as you can afford it. Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that all of the standard models for physics that we have today, they do have in them these continuous, you know, for example, symmetries that are rotational symmetry or things like that. They're built off of positing continuums with continuous waves, lots of continuities and infinities, at least in the mathematical descriptions. Except for quantum mechanics, right? Right. And I think based on what you've been saying, you would say that those are artifacts or properties of our mathematical descriptions of reality, but they're not actually extant in reality. And my mystery there is why do those continuous and mathematical maybe flawed and inconsistent with infinities all over the place descriptions work so well for describing phenomenon at different levels? If everything at the end of the day, you know, if we just looked at high enough energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees. You know, why does all this continuous infinity based mathematics work so well? What is the explanation for the unreasonable effectiveness of that kind of mathematics? The easiest answer is that the world in which we live in is made of extremely small parts. And we could not exist if that world was not made of that many small parts. So for instance, you want to have a momentum for particles that are almost continuous. So you can address the space with high resolution because the momentum is what tells you where information comes from in the universe, basically the direction of where from which information reaches you and so on. If that would be very coarse, then the complexity that you could build would probably be far lower. And we consist of so many parts that when you look down, it's uncountably many for all practical purposes. So the mathematics that we need to describe the world that we are in that we need to model are mostly not in the realm of countable numbers. The countable numbers only play a role when we are looking at very few microscopic things. As soon as we leave this domain of a few apples on our table, we almost instantly drop in this realm where we just need to switch to a continuous description of things. And this is completely fine for most of our history. When we did physics, we never zoomed in that heart. And even now, when we really need to zoom at the level where the plank length matters and the resolution of the universe becomes visible. And it's of course not some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer have space. I wanted to move matters back over to some of the happenings in the world of large language models and deep learning and so on. And first, quick fire question. I honestly, you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's article. So the first question is, are you a symbolist or a connectionist? I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove that these algorithms do not converge to what we want them to converge to. It's maybe not elegant, but it works. And the solution to problems with deep learning so far has always been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant, more sparse and so on. But in the past, all these elegant sparse models have been left in the dust by just using more deep learning. And we can also see when we zoom out a little bit that there is not an obvious limit to deep learning itself, because deep learning is not just the algorithms. Deep learning is a programming paradigm. It's differentiable programming. It basically means that you express everything with approximately continuous numbers, and you use algorithms that converge business at certain ranges. And when it doesn't converge, then you just tweak it and you introduce a different architecture, which is some kind of discrete operations that you do on these continuous numbers and so on. You just patch it, you write your programs slightly differently, and you can automate the search for the program. And the people who do deep learning are not also docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use a Python script in here rather than just a TensorFlow. This is not what's happening. It's also not that they are constrained to any kind of thing that will use whatever is working. And what we see is that the end-to-end train systems are going more and more powerful, and rather than sitting there by hand and tinkering and finding a solution, we can just use a system that is tinkering automatically through a dramatically larger space than we would ever be able to explore by trying all sorts of algorithms. So when we look at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's actually giving as arguments, the arguments are not very good. He gives us an example, the NetHack challenge. NetHack is a game which has a very large horizon because you basically have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead with what you're doing. And so it's something that is difficult to discover this right solution with a deep learning model that has no prior ideas about what it's doing. Because it takes us very, very long until you get the necessary feedback to learn about your actions. And people are relatively good at learning this because they have so many ideas about what the situation is that they're in. There's so many priors from our world interaction and from other games that we have played that we can bring to the tasks. So the current winner of this is the symbolic solution. And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead of deep learning things. In a single case, not like he has a big array of tasks where they are superior, it's just two students who have written a program that is made of lots and lots of events. This is just a big hack. This is not some symbolic learning algorithm that does something novel, hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my god, deep learning models are limited and we need to replace them with more scripts? This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind of two competing camps and they maybe go after each other with some. No, they don't. This is only on Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that he believed you need to use this argument, all the other arguments are impure and flawed. His brand is to build systems that work. And if one of his people comes up with something that works better than what he came up with, you probably praise him for that and let him go on. Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent, they can strangle off, you know, resources that maybe we like, we shouldn't be investing all our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily down, down deep learning. And I think that's kind of the problem that, that these paradigms cause. But I want to get back to something you said, which is a good point. It's, I think that's an important point. I think that in absolute terms, the other approaches get more money than they did before. It's not that we have a funding stop, as we had at some point, a return funding stop for Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky was wrong. People found a way around this. But at this time, there was so little funding that this cutoff mattered. And at the moment, if you want to do something that has AI and Adline, the chance that you get it funded and whether what paradigm you're doing is greater than ever. So the absolute amount of funds that goes into any kind of paradigm that you want to work on is greater than ever. And the reason why the majority of funds goes into very few paradigms is because these are the things that work in industrial applications. There is no other algorithm that is able to learn from scratch how to translate between arbitrary languages and generate stories and draw pre pictures for you. This is the only game in town at the moment, the only class of algorithm that converges over all these many domains. And people are looking for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly where things already were. You have hardware that works, you have libraries that work and so on. It's hard to get out of that bubble. That is true. And it's always good to push for alternatives and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something politically wrong. I suspect that by and large, the forces of the markets and the forces of the academic researchers that want to explore alternative are pushing in the right direction already. Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent that, let's say, what deep learning is doing is this this differentiable program search, if you will. And a question I have about that is if we imagine the space of all possible programs, that, you know, requiring that we're doing a differentiable search is certainly going to skew that sample space that may even cut off programs in that space that can't be discovered easily by differentiable search. So I'm wondering, doesn't that leave open the possibility that other algorithms that are more discreet in nature, say evolutionary algorithms or discrete program search or whatever, they may have access to a different subspace of the space of all programs that aren't easily accessible by differentiable paradigms. Is that true? The question is, how do you find it? How do you find these algorithms to manipulate the discrete things? I agree that when you have a perceptual model that is modeling everything with chains or sums over real numbers, and a few non-algebraic throne, and you get characteristic artifacts. For instance, in the generative models, you often have the problem, and you try to model a person with glasses or without glasses, that because the model thinks that these features are somewhat continuous, you often run into the situation that you get areas in the generative model, where the glasses are half materialized, and it looks always very weird. And you have these strange things where reality has a discontinuity, but your model has permissible states where you are in the middle of the discontinuity, and you try to generate something that cannot exist. You want your model to be structured such a way ideally that every model configuration corresponds to a world configuration. And this is not necessarily the case with many of the deep learning models. And what the deep learning models, as you train them harder, typically tend to do is that they squeeze these impermeasurable areas until you are very unlikely to end up in them. And it's probably possible to get them to implement filters and all sorts of tricks. But what you can also do is you can combine this with some kind of discrete machine. And then what you do is you learn how to use this. So this deep learning network is not interacting with the world directly, but it learns how to use an architecture that does that. So for instance, instead of training a neural network to do numerical calculations, you can train it to use a numerical calculator. And in this way, it can become very sparse again. Right? So there's not an obvious limit to that I can see where I can prove to the deep learning people, oh, here's where you should stop deep learning, because they can just combine their deep learning approach with other approaches and use the deep learning system to remote control this. And it turns out when we reason and so on, even when we do discrete reasoning, that the steps that we assemble it to each other are heuristics that require some kind of probabilistic element. Right? So when we form a sort that when the sort is made of very discrete elements, the search for that sort is some kind of deep learning process that is happening. Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course, we can combine this and we can get the neural network to learn how to perform the discrete operations. There's a certain thing that I would like to see, which is something like a more sparse language of thought. When we are looking at deep learning models, there's a phenomenon that people are sometimes observing, which they call grocking. That is, you train the model and your model gets better and better. And then it overfits, which means it gets very good at the training data, but it gets very bad on the real world at things that it hasn't seen before, like a person in psychedelics was able to explain everything in the past, but is no longer able to perform well in the future because they're overfitting. They basically fit the curve too closely to the data that I've seen. And there are many tricks in deep learning to go around this overfitting to make sure that this doesn't happen. And people try to avoid it. And then what they discovered is when you take this overfit model, you train it more and more and more and more. At some point, it sometimes clicks and it gets much better than ever before. And there is a question if there's something that we're doing wrong in deep learning. For instance, when you think about how people learn, they learn very different from GPT-3. People first learn by pointing at stuff that thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them, or that they find pleasant and so on. They, that they can feel that they can, they have contrast on it that are salient to them. And so you start out with learning these semantics based on the saliency and relevance that you have. And then when you learn language, you learn basic syntax, how to put things together. And in the long tail of the syntax, you learn style, how to express things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right? And then you learn syntax as the regularities in the style. And the semantics is the long tail of that. And to make that happen, you need to learn much, much more. You need to have more training data and so on. Maybe there's a way in which we can reverse the order and basically get it to start out with relevance, to build a curriculum where you first get very sparse regularities, where it clicks into place. You always make sure that you can handle it with very limited resources and only see the style and the niceties and the nuances as the far extensions of these very sparse concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking paper was very interesting. And a lot of these large language model fans always cite that very, very quickly when you have a conversation with them. But there is a problem with machine learning in general, which is that there is, as you said, there's a spectrum of correlations and almost all of them are spurious. And on one side of that spectrum, you have the idealized features you actually want it to learn, which will generalize after distribution. And then, of course, if you go down that spectrum, you pick up on all sorts of very spurious correlations that just happen to generalize very well. And if you tell the models not to use those spurious correlations, that the performance of the model will go down. But I want to just move a little bit over to Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of large language models for arithmetic tasks are linearly correlated to the term frequency and the training corpus, suggesting that they are memorizing the data set, which presumably you would agree with. And Google has recently released this 540 billion parameter language model called PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks, such as the conceptual combinations task, which is one of them, which tests for compositionality, which we'll talk about in a minute. But compositionality is when you can take constituents from the prompt and compose them together to form the answer. Now, it's tempting to jump to the conclusion that these models are starting to magically reason at scale along the lines that you were just discussing. But I still think there's plenty of opportunities for shortcut learning, you know, by which I mean these spurious correlations, given the brittle interface of an autoregressive GPT style language model with these human designed benchmarks. Would you agree with that? Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the prof here in Britain realized that I was bored in class. So he took me out of the class and in his lab, and he gave me the task to discover grammatical structure and an unknown language from scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked was English, was just unknown to the computer, but was the easiest one to get a corpus for, and they gave me the largest computer they had. It has two gigabytes of RAM, and I did in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM statistics don't work because of too many words in between. So unlike vision tasks where confnets have a useful prior by thinking that adjacent pixels also relate to symbolically related information, right? So adjacency in images is a very good predictor for thematic relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language processing for that reason, because you cannot use direct adjacency very well. And so I realized I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered pairs of words and tried to find correlations between pairs and then find a mutual information tree that would give me the best prediction over the structure of the sentence for all the sentences that I would have in my corpus. And indeed this correlated to structure. And I realized this is going to not just give me grammar, but it's also going to give me semantics if I can more deep statistics. But I will need something not just ordered pairs, but I need to have something like force order models. But to do the statistics, even in memory, with clever and memory compression and many tricks that I did, I could not do full statistics on this. So what I realized I had to do was that I do multiple passes. And at first I discard almost all the information. I only pick out the most salient things. And then my time was over in this lab. And I went back to Germany and never reviewed this area of research again. But what I had realized is to make progress and need to make statistics over what I need to make the statistics over. And the very principled base, I need to learn what I have to learn. And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this. They did statistics over the statistics. And it's still not the right solution, I think. It's not the way in which our brain is doing it. It's some brute force shortcut. Well, for instance, the individual attention heads are not correlated with each other, but in reality, they are. We have this in reality, our attention heads are integrated into one model of what's going on. And it's not that we have an attention net on every layer that just pays attention to what's happening in the lower layer. It's much more clever in our own mind. And this thing is active. We single out things in reality to research, which book we need to take out of the shelf to update our working memory context so we are able to interpret the current sentence that we don't understand. And so we always go for saliency when we read something that doesn't make sense. At least my mind works like this. I discard it. I will not stop until it makes sense, or I will have to go to some preliminary. I will not accept some kind of vague statistical approximation of what I read. Keep this as an intermediary stage in my mind until the hope that eventually converges. It's a completely different learning paradigm. When we teach our children arithmetic, it's not that we show them lots of very long mass textbooks and hope that initially it will not make any sense to them. But as they reread them again and again with many samples, eventually it will click and they will converge on arithmetic. No, this is not how it works. You start this giving them extremely simple things and say, in these extremely simple things, there is structure that you can fully understand. Now go and find the structure that you fully understand. Once you've done it, you make this a little bit more complicated for you. This is probably the paradigm that you could be exploring. I know, but the problem is it's incredibly deceptive when you have something which appears intelligence. Of course, the boundary of our perception of intelligence is a receding one. But I wanted to just get on to there are some incredible generative visual models like Dali and the disco diffusion. These models, I think, are going to revolutionize the creative profession. I've been playing with disco diffusion all day today. I've already ordered some prints to go on my wall. It's incredible. The two obvious settings where large language models might be successful are coding and information retrieval in my opinion. But let's take pause for thought. I've played with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch between the process of generating the code and then debugging and running the code, which has euphemistically been framed as prompt engineering, or another term which I've just invented, retrospective development. I think it's easier to start again from scratch than fix broken code from a large language model. I mean, this is quite interesting. At Google, it already takes months to get any code checked into their mono repo because it's basically a bureaucracy because they needed to have gatekeeping after they decided to use a mono repo. Could you imagine how much bureaucracy there'd be if they allowed people to start checking in code, which was generated from an algorithm? Anyway, I think there's an exciting possible future for using these systems for information retrieval rather than the way that we go through and prune the results on a Google search. These models might just answer directly, but I hasten to think what that search UI would look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you were looking for. Do you think these models would vitiate or spoil our society, or do you think they would actually enrich it? It's very hard to say. I think that from some perspective, our society is already maximally spoiled. Humans, as they live today, are basically locusts with opposable thumbs. This is not going to go on forever, this technological society. Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what. And what basically should make us content is that the Titanic was the only place in the universe that has internet. And we are born on it, and we wouldn't have been born if there was no Titanic. We would not have been born in a sustainable ancestral society. In some sense, our society needs to reinvent itself. It's not really working right now. We don't know what the future is going to look like, and if it's going to be very technological, or if limit certain things, no idea what's going to happen. But if we think about how our current approaches work, if you want just to make programming better, I suspect that these tools can help. But they will be much more useful if you do not have to have this battle between a machine that doesn't really understand what you want. And instead, you have something that is working next to you. It's like, imagine you were working for some corporation and the corporation introduces some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns out that the planning tool itself makes you more productive, but it makes work much less fun. It's still rational to use it. And everybody will hate it, but by and large, it will be used if it makes people 30 more productive. And everybody will feel there might be a must be a better solution, something that feels more organic. And so it could be that Codex is in this category that it makes mediocre programmers much more productive at producing boilerplate. But it's not just this, it's often able to find solutions very quickly, but you need to use a lot of Stack Overflow before you understand the new language or before you tease this new algorithm or part that you want to understand and so on. It just when it doesn't probably turn you into a better programmer, if that is your goal. But for your employer, maybe they don't care whether you're a better programmer, they just want you to turn out these pages of code and then they run this against the verifier and against the unit test and then are done, go to the next thing, right? So maybe it's not that important. But the systems that we would want, what would they look like? I think they need to know what they're doing. You want to have a program that is not just able to reproduce something very well in a given context, you want to understand the context as deeply as you do or better. So it needs to understand what kind of world it's operating in in the moment. And what itself is, what is it that it can do? What is that what needs to learn still? In some sense, you want systems that are sentient. And it's self like, oh my God, but it just means you have a learning system that's general enough to model in principle the entire universe. And this is not as outrageous as it sounds because Delhi is already dealing with two modalities, language and images. And we will get to video and we will get to audio connected and you see them early steps in this direction with the Socratic model of people, for instance. So I think that's almost inevitable that this generality will happen. And you will have to add a system to work in real time so it can discover itself. I think I think there's something really magic, though, about the creative process here. And also the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this thing called Pick Breeder. And you could essentially distribute the selection of these images created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful images. They were they were so incredibly, incredibly diverse and interesting. So it's not that the algorithms were intelligent, there was something magic about the externalized process. And what's really interesting about these models like Dali, for example, is that creativity has been distilled down to a raw idea in your head, right? So for example, I might decide to mix the style of two artists and combine them with a new subject. And I want a black cat in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day. And the technical process is now done for you. The only limit is your imagination. So just like Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3 was intelligent is because it can't be used non interactively. The magic must happen when it's used by humans interactively. Well, you can basically build a machine that is generating prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that is used to prompt GPT3 into generating a story about a robot who sees these things and interact with them. And then you take the output of the generative model and translate this using text to motor module. And in this way, you close the loop. And I just used it as a thought experiment to think about the limitations of embodiment for such systems. Second is essentially doing that. So somebody has made this happen. And even with the language model, it works to some degree. And we know that we don't want to do this with natural language because natural language is a crutch. These systems make up for this, but you're just using more natural language faster than people could use it. But there is some language of thought that we are using that is not learned, but discovered by our own mind that we converge on, that is much more efficient. And this language of thought seems to be able to bottom out and perceptual distributed representations that are unprincipled, like these neural networks are in a sense unprincipled, but they don't break. Then there is something that is vague and ambiguous and has small contradictions in it. But at some level, it also is able to emulate very principal logic very well and becomes very sparse and very powerful in expressing things concisely. And this very concise language of thought is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language model and division model using embedding spaces. And these embedding spaces basically project all the concepts into some high dimensional manifold and find similarities between them. And Gary Marcus points out that there is an issue with compositionality in this. So you need to find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy to do with the grammar. And it's much harder to do this with a deep learning system that needs to discover this in a way and structure this space in the right way. It's not impossible. So when Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong. But I think he is right in the sense that this is something that is much, much harder for the current approaches. They need dramatic training data than a human being. And the algorithms are not doing this naturally. So there are probably ways in which we could make this happen much more elegantly and quickly and converge, for instance, on models for arithmetic. That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael Millier, you know, about compositionality of these large generative vision models. Because usually compositionality is referred to in respect of language models. I think Raphael said that the assessment of the claim is complicated by the fact that people differ in their understanding of what compositionality means. But if language is compositional, as you say, and thought is language as argued by the proponents of this language of thought hypothesis, I think Raphael said that he thought language itself should be compositional in a similar sense. And perhaps by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure, or the grammar, or the constituents. It's much easier to go the other way around, would you agree? The issue is that language of thought is executable. And natural language is not. We execute natural language by translating it into our mind in something that we can execute. And the reason about code, you might use natural language to support your reasoning. But the code that you build in your mind is filled in some kind of abstract syntax tree that you can actually execute in your mind to some degree. And then you get a sense of the output. So you entrain your own brain with an executable structure. And this executable structure has properties that are quite similar to the ones that the compiler has in your computer. So you can anticipate what the compiler is going to do with your code. You're not going to do this with all the depths that your compiler do it, you might still have to run your code, but you will find when you want to experience programming, your stuff will usually run. So our language of thought can do this, it can execute stuff. And it's not just a machine neural network that guesses what the outcome is going to be and is right some of the time. But it gets pretty good at figuring this out. And this means that it has to build this compositional structure that has some verifiable properties. And we observe ourselves operating on this verification process, right? When we do introspection for the program, we observe ourselves how we direct our attention on making proofs. And this attentional algorithm that works in real time, that is making changes on your mental models and then predicts the outcome of these changes and compares this with what your mental computations give you and then fixes your models of how your own thinking process in this domain works and so on. You can observe yourself doing that. And it's nothing where I would say a given approach or the given approaches that we have will never get there. But there seem to be ways in which we have just barely scratched the surface in what you need to be doing to make these models sample efficient and sparse and more adequate to model domains you're interested in. Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean, I agree with the prognosticators. And I do think that these large language models and these visual generative models will be revolutionary for some domains. But you know, I think you really need to have a human guiding the creative process, which is a huge limitation. But I think it could also potentially hint at what intelligence actually is, right? I think intelligence might be this externalized process in a cybernetic sense, if you like, this idea of intelligence being fully embedded in an algorithm in a single agent might be the wrong way to think about it. I think that humans, by and large, are very confused. Very often you need a human to guide a human, right? And then you ask yourself, if you do this recursively, does this society know where it's going? Or is this at some level confusion at all levels that is balancing each other? So there seem to be very few people with a plan right now. And it's quite apparent that we see that in the sciences, we see this in politics, we need an art and literature. It is that humans have a higher degree of sentience. But by and large, very few people have a principal plan on how to build a sustainable, harmonic world. And if you set an AI system to this task, it might make more progress on it. It's just that Dalit is not operating in real time on the universe that it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art. It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly doing skills, using skills that I didn't have. And I suppose that you had the same impression when you were generating things with your diffusion model that you're going to put up on your walls, right? You did that using this amazing tool that was empowering you to think that you otherwise never could do. But you are the creative nexus. And to make an artist, a digital artist, you would need to create an autonomous creative nexus in a way, a creative entity, something that reflects on the world because art is about capturing conscious states. So we would need to build a system that has a story about itself and that is reacting towards own interactions with the world and that would need to be human. It would need to be consistent. Something that is an intelligent entity that is creatively interacting with the world. I think we could totally build an AI artist franchise right now that would have a huge following. But what it need to have is an identity that is not fake, that it's actually built from its interactions with the world in real time. Well, I think we've got a lovely segue there because you said that art is about representing our conscious states. In a way, I disagree with you because you could say, well, it's very reductionist. I've just put a prompt in there and I've created art. Well, I think it is art. But how much of a representation of my conscious state is it? I think Douglas Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low on time. I mean, you said actually that you've spent much of your life thinking about what consciousness is. And you said that you thought it was very mysterious, but you now think that it's a riddle that can be solved, right? So on your recent theory of everything interview with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains are mechanistic and that the elements of consciousness are magical somehow. But you said it had an a causal structure, but not the way physics is built. But it was a story which the physical system tells to itself. You said that the organism is a coherent and consistent pattern, which is state building at least at some level of analysis and that consciousness allows organisms to coordinate their cells to succeed in their niches. And then you spoke of information processing over cells. Now, what model of I should say like what measure of consciousness do you think you most align with? There is only one theory that offers a measure of consciousness and that is integrated information theory, where you actually put a number on it. And it's not clear what that number means. It's not that there is some kind of scalar that measures this. And people we think of consciousness as something that is more qualitative than quantitative. Either somebody is conscious or somebody is unconscious. And when you are conscious, you can have a lack of acuity, you can be addled in your brain and you can be drifting in and out of consciousness. But it's still a qualitative thing of whether you have that or not. And this qualitative thing seems to be simple, probably much simpler than people expected to be. The hard thing might be perception. And consciousness is on top of perception is a certain way to deal with our attention. So I think a very important aspect of consciousness is reflexive attention, that we notice ourselves attending to something, and we reflect on that and integrate this in our model. The conundrum is understanding consciousness if you go right into the history of everything starting with Leibniz and many others. Leibniz says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is made of lots of mechanical parts and somehow the thing is feeling and perceiving things. And we blow the mill up so large that we can walk into it or we would today zoom into it until we see all the parts and we just see these pushing and pulling parts and nothing of them can ever explain a perception or a feeling. And it is a very strong intuition that also drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty obvious that these mechanical phenomena are insufficient to explain what's going on. It's not an obvious connection. So people become dualist. There are somehow two completely separate domains. And I think in a way this dualism is correct, but not in the sense that the mental states are ontologically existing. They exist as if. There is no organism. There is only this connection of cells and this collection of cells is acting in a coherent way, which means we can compress it. We can model it using a very low-dimensional, much circular function than look at all the cells in general. And the organism is only approximating this function, but what makes the organism more powerful than a collection of cells is exactly that function, this structure that we project into it. And the interesting thing is that by the information processing within the organism, the organism can discover that function by itself and use it to drive its own behavior. So while the organism is not a person, it's not even an organism, it is very useful for the organism to behave as if it was an organism and also to have an idea of what it would be like to be a person that interacts, for instance, with the social world. So it creates a simulation of that. And it's often not even a simulation, it's often just a simulacrum. That's what makes it a causal. The difference between a simulation and the reality is that the simulation is modeling some aspects of the dynamics of a domain on a different causal substrate, on a different causal footing. So you have a computer game in which you can shoot a gun, but there is no proper physics in the game that would recreate what's happened in the real world when you shoot a gun. Instead, it is using a different causal structure of your software program to give you something that gives you good enough dynamics so you can interact with the world and experience these causal structures. You can make a different decision, you make a different move in the game, and as a result, the game behaves as if you would expect it because it's imitating the same causal structure using this different substrate. In the simulacrum, you don't have the causal structure. Like a movie doesn't have causal structure. It only gives you a sequence of observables. And our own mental model of ourselves is a mixture of simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like it does this thing magically. And sometimes we have a causal model, but this causal model is not the real deal. It's just this simplified geometric simulation of how the world works. It's a game engine that our brain is producing to anticipate what happens in the physical world. Yeah, so that very strongly resonates with me. And another person I very much respect whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not sure how much you're familiar with his free energy principle and his thoughts on consciousness, but I'd like to put forward to you one of his more recent definitions, if you will, or proposals to explain consciousness. And I get your opinion on it. This is from his 2018 article titled, am I self conscious or does self organization entail self consciousness? And what he says here is the proposal on offer here is that the mind comes into being when self evidencing has a temporal thickness or counterfactual depth, which grounds inferences about the consequences of my action. On this view, consciousness is nothing more than inference about my future, namely the self evidencing consequences of what I could do. Does that align pretty closely with your your view? No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea, but most of the free energy principle comes down to predictive coding, which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely on predictive coding. It's only trying to predict the future from the past. And the future is the next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can go with this, and you can go very far. But you need far far more samples than an organism does. So there are players in us that go beyond predictive coding, maybe we converge towards this over many generations in the evolutionary process. So I don't think it's a stupid idea that Carl Friston proposes, but we are born with additional loss functions that let us converge much, much faster on something that is useful to the organism. And if we think about consciousness, he has a point about agency in there. Agency means that you have a controller that is able to control the future. Took me a while to understand this, but when I go up, we talked about BDI agents, and they seem to be quite complicated and convoluted, put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions, and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal agent. So that has enough agency, it doesn't want anything. It just acts on the present frame by doing the obvious thing. But imagine that you give the thermostat the ability to integrate the expected temperature differences, the differences over the future, when it does x now or y now or does it a moment later, right? So suddenly you have a branching reality. And in this branching reality, you can make decisions and you will have preferences based on this integrated expected reward. So just by giving the thermostat the ability to model the future, you turn it into an agent. This is sufficient. And if you make this model deeper and deeper, it's going to get better and better at it. And at a certain depth, the thermostat is going to discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that the sensor operates differently when it's closer to the heating element and so on and so on, right? So it becomes aware of how it functions. It might even become aware of the way in which its modeling and reasoning process works and to improve it or to account for its inefficiencies in certain ways. And this is also what we do with our own cell. But this model of the self is not identical to our consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience of a now. There is an experience of a perspective that we are having, right? And this is what's absent in the description of first. And he is missing the core point of what it means for something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a first person perspective. It means that it experiences a reality. And this is not described in this Friston quote. We explicitly asked him this question actually when we talked to him last time. And his response there was that this concept of feel like is really something that would need to be coded into the generative model that this agent has about the world. Number one, it has to have a generative model as we've just been discussing. It has to be able to entertain counterfactual, you know, possibilities for the predictive coding, right? And he's saying that these feel like concepts would literally be encoded in that generative model as hypotheses that we recognize, you know, so things like I'm feeling pain, for example, would be a concept within that model. And he says there's actually evidence from, you know, treating patients with chronic pain and this sort of thing that that's actually exactly what's happening in the mind that that the feeling of pain is actually a concept that's built in as a slot, if you will, into this generative model. I mean, what do you think about that proposal? The semantics of pain are given by the avoidance that you don't want to experience the pain usually. And it could be that you cultivate the pain and use it to make something happening on the next level, but it requires that you are then building a multi-level control structure, if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain, I think, without an action tendency, without something that modulates what you are doing. So your, your cognition is embedded into this engine. And to build such an engine that does it, that causally changes how you operate is not that hard. But when you live inside of such an engine, it feels very strange that there is something that is happening that somehow depends on what you are thinking, but you cannot control it. It controls you. It's upstream from you. You are downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's something that is a representation that you can now control and be able to get there. But it's not easy and you're not meant to get there because it means that we can immunize ourselves to pain and sacrifice the organism to our intellectual interests. What's crucial about feelings when you look at them introspectively is that feelings are essentially geometric. I don't know if you noticed that. So for instance, we notice feelings typically in our body. And that's because I think that the feelings play out in a space. And the only space that we have always instantiated in our mind is the body map. So they're being projected into the space to make them distinct. And when we look at the semantics of the feeling, we noticed that they are contracting or expanding or they are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry plus valence, the stuff that is going to push your behaviors in a certain direction. So these are basically the interactions of some deep learning system that is producing continuous geometric representations as perceived from an analytic engine. It's an interface between two parts of your mind, between the analytic attention control that is reflecting on the operations that your mind is doing while it's optimizing its attention. And the underlying system that represents the state of the organism entails where you should be going and makes this visible to you. It is a system that is not able to speak to you, uses geometry. And these geometrical features, this is what we call feelings. So that's a very interesting connection. And I think Jeff Hawkins of Nemento would be quite interested in that as well, because some of what he discussed with us was that, in his view, the evolution of, let's say, abstract thinking and whatnot actually came from systems that evolved to operate in just simple three-dimensional kind of motion, and that eventually those were reutilized by the evolutionary process to start engaging in abstract thinking, which he views as movement through an abstract space. And so I think there's a lot of connection here to what you're saying about feeling, which is that, again, in a sense, our mind has reutilized this three, three plus one d movement mapping capability that it needed in order to survive in a three plus one d environment, physical environment, and it's reutilized those for mapping feelings, it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract space. Is that a fair connection? Yes, but I don't think that it's because it's borrowed from the world in which we interact, but because it's the only game in town, it's the only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually talk about multi-dimensional numbers, about things that are not just a scalar in a single dimension, but features that are related. And sometimes you can take these features that you measure continuously because they have too many steps to meaningfully discretize them. So what it does is you discover that you can rotate something. And this is when you get a space in the sense as we have a space to which we are moving. And these spaces which you can rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is constrained to certain mathematical paradigms, which you can derive from number theory, compressed principles. And our brain is discovering a useful set of functions to model anything, a set of useful computational primitives. And we can probably give our deep learning systems a library of predefined primitives to speed up their convergence. That's also the reason why there is useful transfer learning between different domains. You can train a vision model and use it as a pre-training for audio. And it's not because it's the same thing, but because it has learned useful computational primitives that it can apply across domains. But there is geometry in the audio signal. So this is very interesting territory. I hope you'll come back to dive into this a bit more deeply when we have more time and a better connection, because I agree with you. Some very fascinating math here. Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest that we've had right from the very beginning. So it's an honor to finally get you on the show. And I hope we can get you back soon for a longer conversation. Thank you so much. Likewise. I enjoyed this very much. Let's meet again soon.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.24, "text": " Welcome back to Street Talk, just a little bit of housekeeping before we kick off today.", "tokens": [50364, 4027, 646, 281, 7638, 8780, 11, 445, 257, 707, 857, 295, 48033, 949, 321, 4437, 766, 965, 13, 50576], "temperature": 0.0, "avg_logprob": -0.1731542934070934, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.07340539991855621}, {"id": 1, "seek": 0, "start": 4.24, "end": 10.16, "text": " Polina Silivadov is one of the organisers for a charity AI conference called AI Helps Ukraine.", "tokens": [50576, 3635, 1426, 6943, 592, 1573, 85, 307, 472, 295, 264, 15223, 433, 337, 257, 16863, 7318, 7586, 1219, 7318, 6128, 1878, 14081, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1731542934070934, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.07340539991855621}, {"id": 2, "seek": 0, "start": 10.8, "end": 15.6, "text": " Now, their main goal is to raise funds for Ukraine, both from the folks attending the", "tokens": [50904, 823, 11, 641, 2135, 3387, 307, 281, 5300, 8271, 337, 14081, 11, 1293, 490, 264, 4024, 15862, 264, 51144], "temperature": 0.0, "avg_logprob": -0.1731542934070934, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.07340539991855621}, {"id": 3, "seek": 0, "start": 15.6, "end": 20.080000000000002, "text": " conference and also from companies sponsoring the conference. And it's not too late to sponsor", "tokens": [51144, 7586, 293, 611, 490, 3431, 30311, 264, 7586, 13, 400, 309, 311, 406, 886, 3469, 281, 16198, 51368], "temperature": 0.0, "avg_logprob": -0.1731542934070934, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.07340539991855621}, {"id": 4, "seek": 0, "start": 20.080000000000002, "end": 24.96, "text": " the conference and support it, so please do if you possibly can. Now, all of the funds that they", "tokens": [51368, 264, 7586, 293, 1406, 309, 11, 370, 1767, 360, 498, 291, 6264, 393, 13, 823, 11, 439, 295, 264, 8271, 300, 436, 51612], "temperature": 0.0, "avg_logprob": -0.1731542934070934, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.07340539991855621}, {"id": 5, "seek": 2496, "start": 24.96, "end": 29.36, "text": " raise will go to Ukraine Medical Support, which is a Canadian non-profit organisation", "tokens": [50364, 5300, 486, 352, 281, 14081, 15896, 18073, 11, 597, 307, 257, 12641, 2107, 12, 14583, 18641, 50584], "temperature": 0.0, "avg_logprob": -0.13781487008799678, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.35969728231430054}, {"id": 6, "seek": 2496, "start": 30.0, "end": 34.96, "text": " which is specialising in humanitarian aid for Ukraine. Now, they have some of the world's", "tokens": [50616, 597, 307, 2121, 3436, 294, 25096, 9418, 337, 14081, 13, 823, 11, 436, 362, 512, 295, 264, 1002, 311, 50864], "temperature": 0.0, "avg_logprob": -0.13781487008799678, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.35969728231430054}, {"id": 7, "seek": 2496, "start": 34.96, "end": 41.760000000000005, "text": " leading AI experts keynoting at this event, so Yoshua Benjo, Timnick Gabru, Max Welling,", "tokens": [50864, 5775, 7318, 8572, 2141, 2247, 278, 412, 341, 2280, 11, 370, 38949, 4398, 3964, 5134, 11, 7172, 77, 618, 11995, 894, 11, 7402, 1042, 278, 11, 51204], "temperature": 0.0, "avg_logprob": -0.13781487008799678, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.35969728231430054}, {"id": 8, "seek": 2496, "start": 41.760000000000005, "end": 47.28, "text": " Regina Basile, Alexei Efros and also one of our own personal favourites here on MLSD,", "tokens": [51204, 48407, 5859, 794, 11, 5202, 17067, 31840, 2635, 293, 611, 472, 295, 527, 1065, 2973, 8182, 3324, 510, 322, 376, 19198, 35, 11, 51480], "temperature": 0.0, "avg_logprob": -0.13781487008799678, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.35969728231430054}, {"id": 9, "seek": 2496, "start": 47.28, "end": 52.72, "text": " Professor Michael Bronstein, the one and only. Now, the conference is online pretty much from", "tokens": [51480, 8419, 5116, 19544, 9089, 11, 264, 472, 293, 787, 13, 823, 11, 264, 7586, 307, 2950, 1238, 709, 490, 51752], "temperature": 0.0, "avg_logprob": -0.13781487008799678, "compression_ratio": 1.5257731958762886, "no_speech_prob": 0.35969728231430054}, {"id": 10, "seek": 5272, "start": 52.72, "end": 58.72, "text": " now until the 6th of December and it's being hosted from Mela in Montreal. Their goal is to", "tokens": [50364, 586, 1826, 264, 1386, 392, 295, 7687, 293, 309, 311, 885, 19204, 490, 376, 4053, 294, 34180, 13, 6710, 3387, 307, 281, 50664], "temperature": 0.0, "avg_logprob": -0.09962555826926718, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.07997458428144455}, {"id": 11, "seek": 5272, "start": 58.72, "end": 65.2, "text": " raise $100,000 and they really, really need the support of the AI community to club together and", "tokens": [50664, 5300, 1848, 6879, 11, 1360, 293, 436, 534, 11, 534, 643, 264, 1406, 295, 264, 7318, 1768, 281, 6482, 1214, 293, 50988], "temperature": 0.0, "avg_logprob": -0.09962555826926718, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.07997458428144455}, {"id": 12, "seek": 5272, "start": 65.2, "end": 70.8, "text": " to just donate anything that you can. So, we'll link to the conference in the video and the podcast", "tokens": [50988, 281, 445, 17751, 1340, 300, 291, 393, 13, 407, 11, 321, 603, 2113, 281, 264, 7586, 294, 264, 960, 293, 264, 7367, 51268], "temperature": 0.0, "avg_logprob": -0.09962555826926718, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.07997458428144455}, {"id": 13, "seek": 5272, "start": 70.8, "end": 77.75999999999999, "text": " description. Please donate if you can and also share the links on your socials. Now, I'm at New", "tokens": [51268, 3855, 13, 2555, 17751, 498, 291, 393, 293, 611, 2073, 264, 6123, 322, 428, 2093, 82, 13, 823, 11, 286, 478, 412, 1873, 51616], "temperature": 0.0, "avg_logprob": -0.09962555826926718, "compression_ratio": 1.5298804780876494, "no_speech_prob": 0.07997458428144455}, {"id": 14, "seek": 7776, "start": 77.76, "end": 84.56, "text": " Europe this week in New Orleans, so I'll be walking around with a camera. Please just bump into me", "tokens": [50364, 3315, 341, 1243, 294, 1873, 24715, 11, 370, 286, 603, 312, 4494, 926, 365, 257, 2799, 13, 2555, 445, 9961, 666, 385, 50704], "temperature": 0.0, "avg_logprob": -0.12772878709730212, "compression_ratio": 1.52, "no_speech_prob": 0.16561457514762878}, {"id": 15, "seek": 7776, "start": 84.56, "end": 91.28, "text": " and if you want to record any spicy takes on artificial general intelligence, then let's do it.", "tokens": [50704, 293, 498, 291, 528, 281, 2136, 604, 9127, 2516, 322, 11677, 2674, 7599, 11, 550, 718, 311, 360, 309, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12772878709730212, "compression_ratio": 1.52, "no_speech_prob": 0.16561457514762878}, {"id": 16, "seek": 7776, "start": 92.72, "end": 98.24000000000001, "text": " Today is a conversation with Yoshua Bach, who's one of our most requested guests ever.", "tokens": [51112, 2692, 307, 257, 3761, 365, 38949, 4398, 30920, 11, 567, 311, 472, 295, 527, 881, 16436, 9804, 1562, 13, 51388], "temperature": 0.0, "avg_logprob": -0.12772878709730212, "compression_ratio": 1.52, "no_speech_prob": 0.16561457514762878}, {"id": 17, "seek": 7776, "start": 98.24000000000001, "end": 102.80000000000001, "text": " We recorded the conversation back in April, which gives you a bit of an indication of our backlog.", "tokens": [51388, 492, 8287, 264, 3761, 646, 294, 6929, 11, 597, 2709, 291, 257, 857, 295, 364, 18877, 295, 527, 47364, 13, 51616], "temperature": 0.0, "avg_logprob": -0.12772878709730212, "compression_ratio": 1.52, "no_speech_prob": 0.16561457514762878}, {"id": 18, "seek": 10280, "start": 102.8, "end": 109.36, "text": " I can only apologise about the backlog. Keith and I recently started a new venture called X-Ray", "tokens": [50364, 286, 393, 787, 50128, 466, 264, 47364, 13, 20613, 293, 286, 3938, 1409, 257, 777, 18474, 1219, 1783, 12, 35737, 50692], "temperature": 0.0, "avg_logprob": -0.08536317944526672, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.07864111661911011}, {"id": 19, "seek": 10280, "start": 109.36, "end": 115.03999999999999, "text": " and as you can imagine, we've been working around the clock coding, just trying to get that business", "tokens": [50692, 293, 382, 291, 393, 3811, 11, 321, 600, 668, 1364, 926, 264, 7830, 17720, 11, 445, 1382, 281, 483, 300, 1606, 50976], "temperature": 0.0, "avg_logprob": -0.08536317944526672, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.07864111661911011}, {"id": 20, "seek": 10280, "start": 115.03999999999999, "end": 120.08, "text": " off the ground. But when we've made our millions, we'll devote all of our time to producing amazing", "tokens": [50976, 766, 264, 2727, 13, 583, 562, 321, 600, 1027, 527, 6803, 11, 321, 603, 23184, 439, 295, 527, 565, 281, 10501, 2243, 51228], "temperature": 0.0, "avg_logprob": -0.08536317944526672, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.07864111661911011}, {"id": 21, "seek": 10280, "start": 120.08, "end": 124.88, "text": " content on MLSD. So, yeah, please bear with us loads and loads of cool content coming your way", "tokens": [51228, 2701, 322, 376, 19198, 35, 13, 407, 11, 1338, 11, 1767, 6155, 365, 505, 12668, 293, 12668, 295, 1627, 2701, 1348, 428, 636, 51468], "temperature": 0.0, "avg_logprob": -0.08536317944526672, "compression_ratio": 1.5393700787401574, "no_speech_prob": 0.07864111661911011}, {"id": 22, "seek": 12488, "start": 124.96, "end": 128.48, "text": " soon. I hope you enjoy the show today. Peace out.", "tokens": [50368, 2321, 13, 286, 1454, 291, 2103, 264, 855, 965, 13, 13204, 484, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11853576016116452, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.04816373437643051}, {"id": 23, "seek": 12488, "start": 128.48, "end": 135.84, "text": " Dr Yoshua Bach is our most requested guest ever. Yoshua Bach is a cognitive scientist focusing", "tokens": [50544, 2491, 38949, 4398, 30920, 307, 527, 881, 16436, 8341, 1562, 13, 38949, 4398, 30920, 307, 257, 15605, 12662, 8416, 50912], "temperature": 0.0, "avg_logprob": -0.11853576016116452, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.04816373437643051}, {"id": 24, "seek": 12488, "start": 135.84, "end": 143.76, "text": " on cognitive architectures, models of mental representation, emotion, motivation and sociality.", "tokens": [50912, 322, 15605, 6331, 1303, 11, 5245, 295, 4973, 10290, 11, 8913, 11, 12335, 293, 2093, 507, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11853576016116452, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.04816373437643051}, {"id": 25, "seek": 12488, "start": 143.76, "end": 149.76, "text": " Yoshua's interview on Lex's podcast, he did two interviews on Lex's podcast,", "tokens": [51308, 38949, 4398, 311, 4049, 322, 24086, 311, 7367, 11, 415, 630, 732, 12318, 322, 24086, 311, 7367, 11, 51608], "temperature": 0.0, "avg_logprob": -0.11853576016116452, "compression_ratio": 1.6173469387755102, "no_speech_prob": 0.04816373437643051}, {"id": 26, "seek": 14976, "start": 149.76, "end": 154.23999999999998, "text": " have been watched over two million times so far, which is just absolutely unreal.", "tokens": [50364, 362, 668, 6337, 670, 732, 2459, 1413, 370, 1400, 11, 597, 307, 445, 3122, 25754, 13, 50588], "temperature": 0.0, "avg_logprob": -0.08059040574002857, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.24950174987316132}, {"id": 27, "seek": 14976, "start": 154.88, "end": 159.67999999999998, "text": " Now Yoshua, I've watched many of your interviews and I still don't feel that I have a firm grasp on", "tokens": [50620, 823, 38949, 4398, 11, 286, 600, 6337, 867, 295, 428, 12318, 293, 286, 920, 500, 380, 841, 300, 286, 362, 257, 6174, 21743, 322, 50860], "temperature": 0.0, "avg_logprob": -0.08059040574002857, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.24950174987316132}, {"id": 28, "seek": 14976, "start": 159.67999999999998, "end": 164.48, "text": " some of your views. So today, if you don't mind, I hope we can do a tour de force over some of your", "tokens": [50860, 512, 295, 428, 6809, 13, 407, 965, 11, 498, 291, 500, 380, 1575, 11, 286, 1454, 321, 393, 360, 257, 3512, 368, 3464, 670, 512, 295, 428, 51100], "temperature": 0.0, "avg_logprob": -0.08059040574002857, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.24950174987316132}, {"id": 29, "seek": 14976, "start": 164.48, "end": 169.12, "text": " most important views in our shared space, to the extent that we can keep up with you, of course.", "tokens": [51100, 881, 1021, 6809, 294, 527, 5507, 1901, 11, 281, 264, 8396, 300, 321, 393, 1066, 493, 365, 291, 11, 295, 1164, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08059040574002857, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.24950174987316132}, {"id": 30, "seek": 14976, "start": 169.68, "end": 176.39999999999998, "text": " Now, for example, we'd like to discuss Godel and computation, consciousness, digital physics,", "tokens": [51360, 823, 11, 337, 1365, 11, 321, 1116, 411, 281, 2248, 1265, 338, 293, 24903, 11, 10081, 11, 4562, 10649, 11, 51696], "temperature": 0.0, "avg_logprob": -0.08059040574002857, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.24950174987316132}, {"id": 31, "seek": 17640, "start": 176.4, "end": 182.0, "text": " free will and determinism, large statistical models and indeed whether they're AGI or a", "tokens": [50364, 1737, 486, 293, 15957, 1434, 11, 2416, 22820, 5245, 293, 6451, 1968, 436, 434, 316, 26252, 420, 257, 50644], "temperature": 0.0, "avg_logprob": -0.09437673068740993, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.03439415246248245}, {"id": 32, "seek": 17640, "start": 182.0, "end": 186.88, "text": " parlor trick or something more esoteric. Now, when people talk about God or consciousness", "tokens": [50644, 971, 6746, 4282, 420, 746, 544, 785, 21585, 299, 13, 823, 11, 562, 561, 751, 466, 1265, 420, 10081, 50888], "temperature": 0.0, "avg_logprob": -0.09437673068740993, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.03439415246248245}, {"id": 33, "seek": 17640, "start": 186.88, "end": 192.4, "text": " or any other complex phenomena, it relates to everyone and it means something different to", "tokens": [50888, 420, 604, 661, 3997, 22004, 11, 309, 16155, 281, 1518, 293, 309, 1355, 746, 819, 281, 51164], "temperature": 0.0, "avg_logprob": -0.09437673068740993, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.03439415246248245}, {"id": 34, "seek": 17640, "start": 192.4, "end": 198.08, "text": " everyone. It's ineffable and every conversation sounds like a typical post ketamine discussion,", "tokens": [51164, 1518, 13, 467, 311, 7167, 602, 712, 293, 633, 3761, 3263, 411, 257, 7476, 2183, 14979, 18929, 5017, 11, 51448], "temperature": 0.0, "avg_logprob": -0.09437673068740993, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.03439415246248245}, {"id": 35, "seek": 17640, "start": 198.08, "end": 203.36, "text": " which is to say extremely low information content. Now, the topics we're discussing today are very", "tokens": [51448, 597, 307, 281, 584, 4664, 2295, 1589, 2701, 13, 823, 11, 264, 8378, 321, 434, 10850, 965, 366, 588, 51712], "temperature": 0.0, "avg_logprob": -0.09437673068740993, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.03439415246248245}, {"id": 36, "seek": 20336, "start": 203.36, "end": 208.48000000000002, "text": " complex and often we're reaching for the best language to use to conduct the conversation.", "tokens": [50364, 3997, 293, 2049, 321, 434, 9906, 337, 264, 1151, 2856, 281, 764, 281, 6018, 264, 3761, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11715058957116078, "compression_ratio": 1.5439739413680782, "no_speech_prob": 0.0064253550954163074}, {"id": 37, "seek": 20336, "start": 208.48000000000002, "end": 213.52, "text": " So I hope we do well today. Anyway, Dr. Yoshua Barker is an absolute honor to finally welcome you", "tokens": [50620, 407, 286, 1454, 321, 360, 731, 965, 13, 5684, 11, 2491, 13, 38949, 4398, 36275, 260, 307, 364, 8236, 5968, 281, 2721, 2928, 291, 50872], "temperature": 0.0, "avg_logprob": -0.11715058957116078, "compression_ratio": 1.5439739413680782, "no_speech_prob": 0.0064253550954163074}, {"id": 38, "seek": 20336, "start": 213.52, "end": 221.76000000000002, "text": " to MLST. Thank you very much. I'm glad to be on the show. Amazing. Well, when I started doing", "tokens": [50872, 281, 21601, 6840, 13, 1044, 291, 588, 709, 13, 286, 478, 5404, 281, 312, 322, 264, 855, 13, 14165, 13, 1042, 11, 562, 286, 1409, 884, 51284], "temperature": 0.0, "avg_logprob": -0.11715058957116078, "compression_ratio": 1.5439739413680782, "no_speech_prob": 0.0064253550954163074}, {"id": 39, "seek": 20336, "start": 221.76000000000002, "end": 227.20000000000002, "text": " computer science many years ago, interestingly, the theory of computation wasn't even on the", "tokens": [51284, 3820, 3497, 867, 924, 2057, 11, 25873, 11, 264, 5261, 295, 24903, 2067, 380, 754, 322, 264, 51556], "temperature": 0.0, "avg_logprob": -0.11715058957116078, "compression_ratio": 1.5439739413680782, "no_speech_prob": 0.0064253550954163074}, {"id": 40, "seek": 20336, "start": 227.20000000000002, "end": 231.76000000000002, "text": " curricula. And I was wondering whether you thought it should be. I mean, presumably you think it's", "tokens": [51556, 13179, 3780, 13, 400, 286, 390, 6359, 1968, 291, 1194, 309, 820, 312, 13, 286, 914, 11, 26742, 291, 519, 309, 311, 51784], "temperature": 0.0, "avg_logprob": -0.11715058957116078, "compression_ratio": 1.5439739413680782, "no_speech_prob": 0.0064253550954163074}, {"id": 41, "seek": 23176, "start": 231.76, "end": 236.56, "text": " extremely relevant for AGI. Now, we want this to be as pedagogical as possible. So please explain", "tokens": [50364, 4664, 7340, 337, 316, 26252, 13, 823, 11, 321, 528, 341, 281, 312, 382, 5670, 31599, 804, 382, 1944, 13, 407, 1767, 2903, 50604], "temperature": 0.0, "avg_logprob": -0.08858211085481464, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0022508606780320406}, {"id": 42, "seek": 23176, "start": 236.56, "end": 243.2, "text": " everything like we're five. What does computation mean to you? I think that computation is", "tokens": [50604, 1203, 411, 321, 434, 1732, 13, 708, 775, 24903, 914, 281, 291, 30, 286, 519, 300, 24903, 307, 50936], "temperature": 0.0, "avg_logprob": -0.08858211085481464, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0022508606780320406}, {"id": 43, "seek": 23176, "start": 243.2, "end": 248.79999999999998, "text": " far easier than most people think. It means that you have a causal structure where every", "tokens": [50936, 1400, 3571, 813, 881, 561, 519, 13, 467, 1355, 300, 291, 362, 257, 38755, 3877, 689, 633, 51216], "temperature": 0.0, "avg_logprob": -0.08858211085481464, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0022508606780320406}, {"id": 44, "seek": 23176, "start": 249.76, "end": 255.04, "text": " transition can be decomposed into individual steps. And when we talk about computational models,", "tokens": [51264, 6034, 393, 312, 22867, 1744, 666, 2609, 4439, 13, 400, 562, 321, 751, 466, 28270, 5245, 11, 51528], "temperature": 0.0, "avg_logprob": -0.08858211085481464, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0022508606780320406}, {"id": 45, "seek": 23176, "start": 255.04, "end": 260.64, "text": " we decompose the world into states and transitions between the states. And then it turns out that", "tokens": [51528, 321, 22867, 541, 264, 1002, 666, 4368, 293, 23767, 1296, 264, 4368, 13, 400, 550, 309, 4523, 484, 300, 51808], "temperature": 0.0, "avg_logprob": -0.08858211085481464, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0022508606780320406}, {"id": 46, "seek": 26064, "start": 260.71999999999997, "end": 266.96, "text": " there is a certain minimal system that is able to execute everything. And this can be described in", "tokens": [50368, 456, 307, 257, 1629, 13206, 1185, 300, 307, 1075, 281, 14483, 1203, 13, 400, 341, 393, 312, 7619, 294, 50680], "temperature": 0.0, "avg_logprob": -0.09357902791240427, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.00520428828895092}, {"id": 47, "seek": 26064, "start": 266.96, "end": 272.24, "text": " many ways. The most famous one is probably the Turing machine and many other ways in which you", "tokens": [50680, 867, 2098, 13, 440, 881, 4618, 472, 307, 1391, 264, 314, 1345, 3479, 293, 867, 661, 2098, 294, 597, 291, 50944], "temperature": 0.0, "avg_logprob": -0.09357902791240427, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.00520428828895092}, {"id": 48, "seek": 26064, "start": 272.24, "end": 278.47999999999996, "text": " can describe the Turing machine. For instance, you can just do the Turing machine by doing", "tokens": [50944, 393, 6786, 264, 314, 1345, 3479, 13, 1171, 5197, 11, 291, 393, 445, 360, 264, 314, 1345, 3479, 538, 884, 51256], "temperature": 0.0, "avg_logprob": -0.09357902791240427, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.00520428828895092}, {"id": 49, "seek": 26064, "start": 279.52, "end": 283.52, "text": " search and replace on strings. And this is how the Lambda calculus is defined.", "tokens": [51308, 3164, 293, 7406, 322, 13985, 13, 400, 341, 307, 577, 264, 45691, 33400, 307, 7642, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09357902791240427, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.00520428828895092}, {"id": 50, "seek": 26064, "start": 284.24, "end": 288.88, "text": " And all the programming languages and the Lambda calculus and the Turing machine", "tokens": [51544, 400, 439, 264, 9410, 8650, 293, 264, 45691, 33400, 293, 264, 314, 1345, 3479, 51776], "temperature": 0.0, "avg_logprob": -0.09357902791240427, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.00520428828895092}, {"id": 51, "seek": 28888, "start": 288.88, "end": 293.12, "text": " turn out to have the same power. That is, if you can compute something with one of these paradigms,", "tokens": [50364, 1261, 484, 281, 362, 264, 912, 1347, 13, 663, 307, 11, 498, 291, 393, 14722, 746, 365, 472, 295, 613, 13480, 328, 2592, 11, 50576], "temperature": 0.0, "avg_logprob": -0.06678580443064372, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.008177218958735466}, {"id": 52, "seek": 28888, "start": 293.12, "end": 296.96, "text": " you can compute it with the others. As long as you don't run into resource constraints,", "tokens": [50576, 291, 393, 14722, 309, 365, 264, 2357, 13, 1018, 938, 382, 291, 500, 380, 1190, 666, 7684, 18491, 11, 50768], "temperature": 0.0, "avg_logprob": -0.06678580443064372, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.008177218958735466}, {"id": 53, "seek": 28888, "start": 296.96, "end": 301.52, "text": " so as long as it still fits into memory and you don't care about speed, they all have the same", "tokens": [50768, 370, 382, 938, 382, 309, 920, 9001, 666, 4675, 293, 291, 500, 380, 1127, 466, 3073, 11, 436, 439, 362, 264, 912, 50996], "temperature": 0.0, "avg_logprob": -0.06678580443064372, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.008177218958735466}, {"id": 54, "seek": 28888, "start": 301.52, "end": 308.0, "text": " power. But in practice, of course, every system is limited. So we don't run things forever. We", "tokens": [50996, 1347, 13, 583, 294, 3124, 11, 295, 1164, 11, 633, 1185, 307, 5567, 13, 407, 321, 500, 380, 1190, 721, 5680, 13, 492, 51320], "temperature": 0.0, "avg_logprob": -0.06678580443064372, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.008177218958735466}, {"id": 55, "seek": 28888, "start": 308.0, "end": 312.88, "text": " want them to give us a result after a certain time. So what matters is what can be efficiently", "tokens": [51320, 528, 552, 281, 976, 505, 257, 1874, 934, 257, 1629, 565, 13, 407, 437, 7001, 307, 437, 393, 312, 19621, 51564], "temperature": 0.0, "avg_logprob": -0.06678580443064372, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.008177218958735466}, {"id": 56, "seek": 31288, "start": 312.88, "end": 319.44, "text": " computed, not what is reachable at all. Awesome. And there are, and we're going to get into this", "tokens": [50364, 40610, 11, 406, 437, 307, 2524, 712, 412, 439, 13, 10391, 13, 400, 456, 366, 11, 293, 321, 434, 516, 281, 483, 666, 341, 50692], "temperature": 0.0, "avg_logprob": -0.1309337019920349, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.02366783656179905}, {"id": 57, "seek": 31288, "start": 319.44, "end": 324.8, "text": " a bit, but there are some possible loopholes, at least in, you know, let's say whether or not", "tokens": [50692, 257, 857, 11, 457, 456, 366, 512, 1944, 450, 5317, 7456, 11, 412, 1935, 294, 11, 291, 458, 11, 718, 311, 584, 1968, 420, 406, 50960], "temperature": 0.0, "avg_logprob": -0.1309337019920349, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.02366783656179905}, {"id": 58, "seek": 31288, "start": 324.8, "end": 330.71999999999997, "text": " the universe is limited in certain ways that the definitions of like Turing machines are.", "tokens": [50960, 264, 6445, 307, 5567, 294, 1629, 2098, 300, 264, 21988, 295, 411, 314, 1345, 8379, 366, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1309337019920349, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.02366783656179905}, {"id": 59, "seek": 31288, "start": 330.71999999999997, "end": 337.04, "text": " And one I wanted to ask you about specifically is Penrose's claims. And so he claims that", "tokens": [51256, 400, 472, 286, 1415, 281, 1029, 291, 466, 4682, 307, 10571, 37841, 311, 9441, 13, 400, 370, 415, 9441, 300, 51572], "temperature": 0.0, "avg_logprob": -0.1309337019920349, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.02366783656179905}, {"id": 60, "seek": 33704, "start": 337.84000000000003, "end": 344.56, "text": " what Godel's work in fact proves is that the human mind can understand truths that are not", "tokens": [50404, 437, 1265, 338, 311, 589, 294, 1186, 25019, 307, 300, 264, 1952, 1575, 393, 1223, 30079, 300, 366, 406, 50740], "temperature": 0.0, "avg_logprob": -0.11076996061537, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003944994881749153}, {"id": 61, "seek": 33704, "start": 344.56, "end": 351.6, "text": " provable. So specifically one can show that, you know, given Godel's sentence is necessarily true", "tokens": [50740, 1439, 712, 13, 407, 4682, 472, 393, 855, 300, 11, 291, 458, 11, 2212, 1265, 338, 311, 8174, 307, 4725, 2074, 51092], "temperature": 0.0, "avg_logprob": -0.11076996061537, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003944994881749153}, {"id": 62, "seek": 33704, "start": 351.6, "end": 357.44, "text": " given given a mathematical analysis, even though it can't be proven within the formal system that", "tokens": [51092, 2212, 2212, 257, 18894, 5215, 11, 754, 1673, 309, 393, 380, 312, 12785, 1951, 264, 9860, 1185, 300, 51384], "temperature": 0.0, "avg_logprob": -0.11076996061537, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003944994881749153}, {"id": 63, "seek": 33704, "start": 357.44, "end": 363.12, "text": " it's that it's defined. And Penrose claims that this capability to understand, if you will, to", "tokens": [51384, 309, 311, 300, 309, 311, 7642, 13, 400, 10571, 37841, 9441, 300, 341, 13759, 281, 1223, 11, 498, 291, 486, 11, 281, 51668], "temperature": 0.0, "avg_logprob": -0.11076996061537, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.003944994881749153}, {"id": 64, "seek": 36312, "start": 363.12, "end": 369.68, "text": " mathematically understand is in fact non computational, at least in part. And so if he's right, then", "tokens": [50364, 44003, 1223, 307, 294, 1186, 2107, 28270, 11, 412, 1935, 294, 644, 13, 400, 370, 498, 415, 311, 558, 11, 550, 50692], "temperature": 0.0, "avg_logprob": -0.09962821274660946, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.002631534356623888}, {"id": 65, "seek": 36312, "start": 369.68, "end": 376.72, "text": " our brains might be what Turing referred to as Oracle machines. These are computers that have", "tokens": [50692, 527, 15442, 1062, 312, 437, 314, 1345, 10839, 281, 382, 25654, 8379, 13, 1981, 366, 10807, 300, 362, 51044], "temperature": 0.0, "avg_logprob": -0.09962821274660946, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.002631534356623888}, {"id": 66, "seek": 36312, "start": 376.72, "end": 384.08, "text": " access to a non computable Oracle or function that they can then utilize those oracles in", "tokens": [51044, 2105, 281, 257, 2107, 2807, 712, 25654, 420, 2445, 300, 436, 393, 550, 16117, 729, 420, 9918, 294, 51412], "temperature": 0.0, "avg_logprob": -0.09962821274660946, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.002631534356623888}, {"id": 67, "seek": 36312, "start": 384.08, "end": 389.44, "text": " order to perform hyper computation, essentially. So I'm asking, I'm curious, are you open to this", "tokens": [51412, 1668, 281, 2042, 9848, 24903, 11, 4476, 13, 407, 286, 478, 3365, 11, 286, 478, 6369, 11, 366, 291, 1269, 281, 341, 51680], "temperature": 0.0, "avg_logprob": -0.09962821274660946, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.002631534356623888}, {"id": 68, "seek": 38944, "start": 389.44, "end": 394.08, "text": " possibility? And if not, what is your response to Penrose's arguments?", "tokens": [50364, 7959, 30, 400, 498, 406, 11, 437, 307, 428, 4134, 281, 10571, 37841, 311, 12869, 30, 50596], "temperature": 0.0, "avg_logprob": -0.1370647975376674, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001454838435165584}, {"id": 69, "seek": 38944, "start": 395.04, "end": 400.24, "text": " I suspect that Goedl has been misunderstood by a lot of philosophers. Goedl was a truth", "tokens": [50644, 286, 9091, 300, 1037, 292, 75, 575, 668, 33870, 538, 257, 688, 295, 36839, 13, 1037, 292, 75, 390, 257, 3494, 50904], "temperature": 0.0, "avg_logprob": -0.1370647975376674, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001454838435165584}, {"id": 70, "seek": 38944, "start": 400.24, "end": 405.92, "text": " realist. That is, he thought that truth really exists out there. That it's the thing that is", "tokens": [50904, 957, 468, 13, 663, 307, 11, 415, 1194, 300, 3494, 534, 8198, 484, 456, 13, 663, 309, 311, 264, 551, 300, 307, 51188], "temperature": 0.0, "avg_logprob": -0.1370647975376674, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001454838435165584}, {"id": 71, "seek": 38944, "start": 405.92, "end": 410.96, "text": " eternal in some sense. He had this very strong intuition and mathematics classically is also", "tokens": [51188, 14503, 294, 512, 2020, 13, 634, 632, 341, 588, 2068, 24002, 293, 18666, 1508, 984, 307, 611, 51440], "temperature": 0.0, "avg_logprob": -0.1370647975376674, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001454838435165584}, {"id": 72, "seek": 38944, "start": 410.96, "end": 415.76, "text": " formalized in this way. The difference between mathematics and computation, at least in the", "tokens": [51440, 9860, 1602, 294, 341, 636, 13, 440, 2649, 1296, 18666, 293, 24903, 11, 412, 1935, 294, 264, 51680], "temperature": 0.0, "avg_logprob": -0.1370647975376674, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001454838435165584}, {"id": 73, "seek": 41576, "start": 415.76, "end": 420.08, "text": " standard sense in which we normally teach mathematics at school is that mathematics has no", "tokens": [50364, 3832, 2020, 294, 597, 321, 5646, 2924, 18666, 412, 1395, 307, 300, 18666, 575, 572, 50580], "temperature": 0.0, "avg_logprob": -0.11119321127918279, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.0037064712960273027}, {"id": 74, "seek": 41576, "start": 420.08, "end": 427.52, "text": " states. Everything in mathematics just is eternally. It's a single state. And if you want to go through", "tokens": [50580, 4368, 13, 5471, 294, 18666, 445, 307, 10533, 379, 13, 467, 311, 257, 2167, 1785, 13, 400, 498, 291, 528, 281, 352, 807, 50952], "temperature": 0.0, "avg_logprob": -0.11119321127918279, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.0037064712960273027}, {"id": 75, "seek": 41576, "start": 427.52, "end": 433.12, "text": " a sequence of states, you put an index into the formula. But still, everything is there at the", "tokens": [50952, 257, 8310, 295, 4368, 11, 291, 829, 364, 8186, 666, 264, 8513, 13, 583, 920, 11, 1203, 307, 456, 412, 264, 51232], "temperature": 0.0, "avg_logprob": -0.11119321127918279, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.0037064712960273027}, {"id": 76, "seek": 41576, "start": 433.12, "end": 437.76, "text": " same time. The index is just a way to access this thing. And this way of having mathematics", "tokens": [51232, 912, 565, 13, 440, 8186, 307, 445, 257, 636, 281, 2105, 341, 551, 13, 400, 341, 636, 295, 1419, 18666, 51464], "temperature": 0.0, "avg_logprob": -0.11119321127918279, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.0037064712960273027}, {"id": 77, "seek": 41576, "start": 437.76, "end": 442.96, "text": " stateless is very elegant because it allows us to define functions that have infinitely many", "tokens": [51464, 2219, 4272, 307, 588, 21117, 570, 309, 4045, 505, 281, 6964, 6828, 300, 362, 36227, 867, 51724], "temperature": 0.0, "avg_logprob": -0.11119321127918279, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.0037064712960273027}, {"id": 78, "seek": 44296, "start": 442.96, "end": 447.76, "text": " arguments. If you would have a state machine that tries to consume infinitely many arguments,", "tokens": [50364, 12869, 13, 759, 291, 576, 362, 257, 1785, 3479, 300, 9898, 281, 14732, 36227, 867, 12869, 11, 50604], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 79, "seek": 44296, "start": 447.76, "end": 452.08, "text": " it would never finish before it goes to the next step. And the same thing in the middle of the", "tokens": [50604, 309, 576, 1128, 2413, 949, 309, 1709, 281, 264, 958, 1823, 13, 400, 264, 912, 551, 294, 264, 2808, 295, 264, 50820], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 80, "seek": 44296, "start": 452.08, "end": 456.32, "text": " function, if you compute something, if it's stateless, you can just compute all the indices", "tokens": [50820, 2445, 11, 498, 291, 14722, 746, 11, 498, 309, 311, 2219, 4272, 11, 291, 393, 445, 14722, 439, 264, 43840, 51032], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 81, "seek": 44296, "start": 456.32, "end": 461.84, "text": " all at once, even if it's infinitely many in classical mathematics. In a computational system,", "tokens": [51032, 439, 412, 1564, 11, 754, 498, 309, 311, 36227, 867, 294, 13735, 18666, 13, 682, 257, 28270, 1185, 11, 51308], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 82, "seek": 44296, "start": 461.84, "end": 466.47999999999996, "text": " you would have to do this maybe one after the other. And if you do it at parallel, you will have", "tokens": [51308, 291, 576, 362, 281, 360, 341, 1310, 472, 934, 264, 661, 13, 400, 498, 291, 360, 309, 412, 8952, 11, 291, 486, 362, 51540], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 83, "seek": 44296, "start": 466.47999999999996, "end": 471.91999999999996, "text": " lots of CPUs running at parallel. So you run into limits. And the same thing with the output. So", "tokens": [51540, 3195, 295, 13199, 82, 2614, 412, 8952, 13, 407, 291, 1190, 666, 10406, 13, 400, 264, 912, 551, 365, 264, 5598, 13, 407, 51812], "temperature": 0.0, "avg_logprob": -0.09242987806779625, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.014061279594898224}, {"id": 84, "seek": 47192, "start": 472.0, "end": 476.88, "text": " in the classical mathematics, you can chain infinitely many steps and functions and exchange", "tokens": [50368, 294, 264, 13735, 18666, 11, 291, 393, 5021, 36227, 867, 4439, 293, 6828, 293, 7742, 50612], "temperature": 0.0, "avg_logprob": -0.10738687061128162, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001151267089881003}, {"id": 85, "seek": 47192, "start": 476.88, "end": 481.6, "text": " infinitely many arguments. But of course, mathematicians never did this a practice. It's", "tokens": [50612, 36227, 867, 12869, 13, 583, 295, 1164, 11, 32811, 2567, 1128, 630, 341, 257, 3124, 13, 467, 311, 50848], "temperature": 0.0, "avg_logprob": -0.10738687061128162, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001151267089881003}, {"id": 86, "seek": 47192, "start": 481.6, "end": 486.16, "text": " just a specification. This is how they like to write things down. When they want to calculate it,", "tokens": [50848, 445, 257, 31256, 13, 639, 307, 577, 436, 411, 281, 2464, 721, 760, 13, 1133, 436, 528, 281, 8873, 309, 11, 51076], "temperature": 0.0, "avg_logprob": -0.10738687061128162, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001151267089881003}, {"id": 87, "seek": 47192, "start": 486.16, "end": 492.08000000000004, "text": " they still have to go down and do it sequentially step by step. Just mathematics is defined in such", "tokens": [51076, 436, 920, 362, 281, 352, 760, 293, 360, 309, 5123, 3137, 1823, 538, 1823, 13, 1449, 18666, 307, 7642, 294, 1270, 51372], "temperature": 0.0, "avg_logprob": -0.10738687061128162, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001151267089881003}, {"id": 88, "seek": 47192, "start": 492.08000000000004, "end": 497.84000000000003, "text": " a way as if you could upload this to some supernatural being or some grad student who is", "tokens": [51372, 257, 636, 382, 498, 291, 727, 6580, 341, 281, 512, 25678, 885, 420, 512, 2771, 3107, 567, 307, 51660], "temperature": 0.0, "avg_logprob": -0.10738687061128162, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001151267089881003}, {"id": 89, "seek": 49784, "start": 497.84, "end": 504.15999999999997, "text": " going to do the infinitely many calculations. And Goethe took this specification of mathematics", "tokens": [50364, 516, 281, 360, 264, 36227, 867, 20448, 13, 400, 1037, 302, 675, 1890, 341, 31256, 295, 18666, 50680], "temperature": 0.0, "avg_logprob": -0.1360619395386939, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0051385872066020966}, {"id": 90, "seek": 49784, "start": 504.15999999999997, "end": 508.15999999999997, "text": " and he found out that when you have this stateless mathematics, you can, for instance,", "tokens": [50680, 293, 415, 1352, 484, 300, 562, 291, 362, 341, 2219, 4272, 18666, 11, 291, 393, 11, 337, 5197, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1360619395386939, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0051385872066020966}, {"id": 91, "seek": 49784, "start": 508.15999999999997, "end": 513.92, "text": " define self referential statements that change that choose value depending on the statement itself.", "tokens": [50880, 6964, 2698, 2864, 2549, 12363, 300, 1319, 300, 2826, 2158, 5413, 322, 264, 5629, 2564, 13, 51168], "temperature": 0.0, "avg_logprob": -0.1360619395386939, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0051385872066020966}, {"id": 92, "seek": 49784, "start": 514.56, "end": 519.68, "text": " And this recurrence leads can lead to a contradiction in the state itself. So you basically", "tokens": [51200, 400, 341, 18680, 10760, 6689, 393, 1477, 281, 257, 34937, 294, 264, 1785, 2564, 13, 407, 291, 1936, 51456], "temperature": 0.0, "avg_logprob": -0.1360619395386939, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0051385872066020966}, {"id": 93, "seek": 49784, "start": 519.68, "end": 525.1999999999999, "text": " get two statements which say I am wrong. And if by referring to itself, it changes its own", "tokens": [51456, 483, 732, 12363, 597, 584, 286, 669, 2085, 13, 400, 498, 538, 13761, 281, 2564, 11, 309, 2962, 1080, 1065, 51732], "temperature": 0.0, "avg_logprob": -0.1360619395386939, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0051385872066020966}, {"id": 94, "seek": 52520, "start": 525.2, "end": 531.0400000000001, "text": " truth value. So if mathematics is stateless, you will now run into a conflict. In a computational", "tokens": [50364, 3494, 2158, 13, 407, 498, 18666, 307, 2219, 4272, 11, 291, 486, 586, 1190, 666, 257, 6596, 13, 682, 257, 28270, 50656], "temperature": 0.0, "avg_logprob": -0.13190406055773718, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.004904743749648333}, {"id": 95, "seek": 52520, "start": 531.0400000000001, "end": 534.8000000000001, "text": " system, that's not a big problem. Your computer is not going to crash. If you write it down the", "tokens": [50656, 1185, 11, 300, 311, 406, 257, 955, 1154, 13, 2260, 3820, 307, 406, 516, 281, 8252, 13, 759, 291, 2464, 309, 760, 264, 50844], "temperature": 0.0, "avg_logprob": -0.13190406055773718, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.004904743749648333}, {"id": 96, "seek": 52520, "start": 534.8000000000001, "end": 539.5200000000001, "text": " right bay, it just happens is that your truth value fluctuates in every execution step. It's not", "tokens": [50844, 558, 13642, 11, 309, 445, 2314, 307, 300, 428, 3494, 2158, 23448, 27710, 294, 633, 15058, 1823, 13, 467, 311, 406, 51080], "temperature": 0.0, "avg_logprob": -0.13190406055773718, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.004904743749648333}, {"id": 97, "seek": 52520, "start": 539.5200000000001, "end": 545.36, "text": " going to converge. But this is not the real truth. Truth is something that doesn't change", "tokens": [51080, 516, 281, 41881, 13, 583, 341, 307, 406, 264, 957, 3494, 13, 20522, 307, 746, 300, 1177, 380, 1319, 51372], "temperature": 0.0, "avg_logprob": -0.13190406055773718, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.004904743749648333}, {"id": 98, "seek": 52520, "start": 545.36, "end": 552.5600000000001, "text": " when you call the function again. So what's going on here? And I think what Goethe has", "tokens": [51372, 562, 291, 818, 264, 2445, 797, 13, 407, 437, 311, 516, 322, 510, 30, 400, 286, 519, 437, 1037, 302, 675, 575, 51732], "temperature": 0.0, "avg_logprob": -0.13190406055773718, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.004904743749648333}, {"id": 99, "seek": 55256, "start": 552.64, "end": 557.76, "text": " discovered is that classical mathematics doesn't work. What you cannot build is any kind of", "tokens": [50368, 6941, 307, 300, 13735, 18666, 1177, 380, 589, 13, 708, 291, 2644, 1322, 307, 604, 733, 295, 50624], "temperature": 0.0, "avg_logprob": -0.08763191916725853, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.0017273660050705075}, {"id": 100, "seek": 55256, "start": 557.76, "end": 562.56, "text": " mathematics doesn't allow you to build a machine, a hypothetical abstract machine, any kind of", "tokens": [50624, 18666, 1177, 380, 2089, 291, 281, 1322, 257, 3479, 11, 257, 33053, 12649, 3479, 11, 604, 733, 295, 50864], "temperature": 0.0, "avg_logprob": -0.08763191916725853, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.0017273660050705075}, {"id": 101, "seek": 55256, "start": 562.56, "end": 566.4799999999999, "text": " universe that runs the semantics of the classical mathematics without crashing.", "tokens": [50864, 6445, 300, 6676, 264, 4361, 45298, 295, 264, 13735, 18666, 1553, 26900, 13, 51060], "temperature": 0.0, "avg_logprob": -0.08763191916725853, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.0017273660050705075}, {"id": 102, "seek": 55256, "start": 567.3599999999999, "end": 574.0799999999999, "text": " Yeah, but it kind of seems like, okay, we're going to believe Goethe's use of mathematics", "tokens": [51104, 865, 11, 457, 309, 733, 295, 2544, 411, 11, 1392, 11, 321, 434, 516, 281, 1697, 1037, 302, 675, 311, 764, 295, 18666, 51440], "temperature": 0.0, "avg_logprob": -0.08763191916725853, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.0017273660050705075}, {"id": 103, "seek": 55256, "start": 574.0799999999999, "end": 579.8399999999999, "text": " to prove that mathematics is flawed. Like there seems to be almost an inherent contradiction", "tokens": [51440, 281, 7081, 300, 18666, 307, 38823, 13, 1743, 456, 2544, 281, 312, 1920, 364, 26387, 34937, 51728], "temperature": 0.0, "avg_logprob": -0.08763191916725853, "compression_ratio": 1.8630705394190872, "no_speech_prob": 0.0017273660050705075}, {"id": 104, "seek": 57984, "start": 579.84, "end": 585.36, "text": " in there. Like you either believe mathematics, and thus you believe Goethe's proof of some", "tokens": [50364, 294, 456, 13, 1743, 291, 2139, 1697, 18666, 11, 293, 8807, 291, 1697, 1037, 302, 675, 311, 8177, 295, 512, 50640], "temperature": 0.0, "avg_logprob": -0.08724706823175604, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.0006771219195798039}, {"id": 105, "seek": 57984, "start": 585.36, "end": 591.76, "text": " specific limitations on computational systems, right? Or you believe that somehow mathematics", "tokens": [50640, 2685, 15705, 322, 28270, 3652, 11, 558, 30, 1610, 291, 1697, 300, 6063, 18666, 50960], "temperature": 0.0, "avg_logprob": -0.08724706823175604, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.0006771219195798039}, {"id": 106, "seek": 57984, "start": 591.76, "end": 595.52, "text": " is flawed, in which case you can't trust the proof that mathematics is flawed.", "tokens": [50960, 307, 38823, 11, 294, 597, 1389, 291, 393, 380, 3361, 264, 8177, 300, 18666, 307, 38823, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08724706823175604, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.0006771219195798039}, {"id": 107, "seek": 57984, "start": 597.2, "end": 602.08, "text": " I think Goethe's conclusion was that there is something fundamentally going wrong, that there", "tokens": [51232, 286, 519, 1037, 302, 675, 311, 10063, 390, 300, 456, 307, 746, 17879, 516, 2085, 11, 300, 456, 51476], "temperature": 0.0, "avg_logprob": -0.08724706823175604, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.0006771219195798039}, {"id": 108, "seek": 57984, "start": 602.08, "end": 607.76, "text": " might be an inability of mathematics to describe reality. And if you believe that truth is real", "tokens": [51476, 1062, 312, 364, 33162, 295, 18666, 281, 6786, 4103, 13, 400, 498, 291, 1697, 300, 3494, 307, 957, 51760], "temperature": 0.0, "avg_logprob": -0.08724706823175604, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.0006771219195798039}, {"id": 109, "seek": 60776, "start": 607.76, "end": 612.8, "text": " and it exists independently of the procedure by which you calculate it, then this seems to be", "tokens": [50364, 293, 309, 8198, 21761, 295, 264, 10747, 538, 597, 291, 8873, 309, 11, 550, 341, 2544, 281, 312, 50616], "temperature": 0.0, "avg_logprob": -0.10230088720516282, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0014094110811129212}, {"id": 110, "seek": 60776, "start": 612.8, "end": 617.52, "text": " plausible. And it was also the conclusions a lot of philosophers have drawn from this,", "tokens": [50616, 39925, 13, 400, 309, 390, 611, 264, 22865, 257, 688, 295, 36839, 362, 10117, 490, 341, 11, 50852], "temperature": 0.0, "avg_logprob": -0.10230088720516282, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0014094110811129212}, {"id": 111, "seek": 60776, "start": 617.52, "end": 622.0, "text": " which basically read Goethe's proof and concluded that mathematicians have admitted that their", "tokens": [50852, 597, 1936, 1401, 1037, 302, 675, 311, 8177, 293, 22960, 300, 32811, 2567, 362, 14920, 300, 641, 51076], "temperature": 0.0, "avg_logprob": -0.10230088720516282, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0014094110811129212}, {"id": 112, "seek": 60776, "start": 622.96, "end": 627.92, "text": " arcane techniques are important to describe reality. And therefore, philosophers who don't", "tokens": [51124, 10346, 1929, 7512, 366, 1021, 281, 6786, 4103, 13, 400, 4412, 11, 36839, 567, 500, 380, 51372], "temperature": 0.0, "avg_logprob": -0.10230088720516282, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0014094110811129212}, {"id": 113, "seek": 60776, "start": 627.92, "end": 634.48, "text": " understand mathematics have a clear advantage. Of course, this is not the conclusion. Instead,", "tokens": [51372, 1223, 18666, 362, 257, 1850, 5002, 13, 2720, 1164, 11, 341, 307, 406, 264, 10063, 13, 7156, 11, 51700], "temperature": 0.0, "avg_logprob": -0.10230088720516282, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0014094110811129212}, {"id": 114, "seek": 63448, "start": 634.48, "end": 640.88, "text": " what turns out is that if you just skip or if you drop the original classical notation or", "tokens": [50364, 437, 4523, 484, 307, 300, 498, 291, 445, 10023, 420, 498, 291, 3270, 264, 3380, 13735, 24657, 420, 50684], "temperature": 0.0, "avg_logprob": -0.1176515875510799, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.0013884094078093767}, {"id": 115, "seek": 63448, "start": 640.88, "end": 645.9200000000001, "text": " as understanding of mathematics and replace it by computation, basically we say,", "tokens": [50684, 382, 3701, 295, 18666, 293, 7406, 309, 538, 24903, 11, 1936, 321, 584, 11, 50936], "temperature": 0.0, "avg_logprob": -0.1176515875510799, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.0013884094078093767}, {"id": 116, "seek": 63448, "start": 645.9200000000001, "end": 650.5600000000001, "text": " truth is what you calculate with the following procedure. And you can define any kind of", "tokens": [50936, 3494, 307, 437, 291, 8873, 365, 264, 3480, 10747, 13, 400, 291, 393, 6964, 604, 733, 295, 51168], "temperature": 0.0, "avg_logprob": -0.1176515875510799, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.0013884094078093767}, {"id": 117, "seek": 63448, "start": 650.5600000000001, "end": 654.4, "text": " procedure that you want. You just have to make sure that it converges to some kind of value in", "tokens": [51168, 10747, 300, 291, 528, 13, 509, 445, 362, 281, 652, 988, 300, 309, 9652, 2880, 281, 512, 733, 295, 2158, 294, 51360], "temperature": 0.0, "avg_logprob": -0.1176515875510799, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.0013884094078093767}, {"id": 118, "seek": 63448, "start": 654.4, "end": 659.12, "text": " the way that you want. Then you resolve your problem. It's just that you lose your notion that", "tokens": [51360, 264, 636, 300, 291, 528, 13, 1396, 291, 14151, 428, 1154, 13, 467, 311, 445, 300, 291, 3624, 428, 10710, 300, 51596], "temperature": 0.0, "avg_logprob": -0.1176515875510799, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.0013884094078093767}, {"id": 119, "seek": 65912, "start": 659.12, "end": 664.32, "text": " truth is independent of that procedure. And so in some sense, the classical mathematics is a", "tokens": [50364, 3494, 307, 6695, 295, 300, 10747, 13, 400, 370, 294, 512, 2020, 11, 264, 13735, 18666, 307, 257, 50624], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 120, "seek": 65912, "start": 664.32, "end": 668.24, "text": " specification that cannot be computed. From the perspective of computer scientists,", "tokens": [50624, 31256, 300, 2644, 312, 40610, 13, 3358, 264, 4585, 295, 3820, 7708, 11, 50820], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 121, "seek": 65912, "start": 668.24, "end": 672.88, "text": " this happens all the time. Some customer wants you to build something that cannot be built,", "tokens": [50820, 341, 2314, 439, 264, 565, 13, 2188, 5474, 2738, 291, 281, 1322, 746, 300, 2644, 312, 3094, 11, 51052], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 122, "seek": 65912, "start": 672.88, "end": 676.4, "text": " and you have just proven that it cannot be built. Right? But it doesn't mean that you", "tokens": [51052, 293, 291, 362, 445, 12785, 300, 309, 2644, 312, 3094, 13, 1779, 30, 583, 309, 1177, 380, 914, 300, 291, 51228], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 123, "seek": 65912, "start": 676.4, "end": 682.16, "text": " cannot build something useful. And I think that Penrose believed that our brain is actually doing", "tokens": [51228, 2644, 1322, 746, 4420, 13, 400, 286, 519, 300, 10571, 37841, 7847, 300, 527, 3567, 307, 767, 884, 51516], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 124, "seek": 65912, "start": 682.16, "end": 688.0, "text": " these infinite things. And it's not. When we reason about infinity, we are not actually reasoning", "tokens": [51516, 613, 13785, 721, 13, 400, 309, 311, 406, 13, 1133, 321, 1778, 466, 13202, 11, 321, 366, 406, 767, 21577, 51808], "temperature": 0.0, "avg_logprob": -0.11908143413953545, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0023959914688020945}, {"id": 125, "seek": 68800, "start": 688.0, "end": 693.28, "text": " about infinitely many steps. What we do is we create a symbol, and then we do very finite", "tokens": [50364, 466, 36227, 867, 4439, 13, 708, 321, 360, 307, 321, 1884, 257, 5986, 11, 293, 550, 321, 360, 588, 19362, 50628], "temperature": 0.0, "avg_logprob": -0.11433542096937024, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0011851184535771608}, {"id": 126, "seek": 68800, "start": 693.28, "end": 698.24, "text": " computations over that symbol. But we cannot construct infinity. We cannot build it. We", "tokens": [50628, 2807, 763, 670, 300, 5986, 13, 583, 321, 2644, 7690, 13202, 13, 492, 2644, 1322, 309, 13, 492, 50876], "temperature": 0.0, "avg_logprob": -0.11433542096937024, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0011851184535771608}, {"id": 127, "seek": 68800, "start": 698.24, "end": 704.32, "text": " cannot go there from scratch and write down some clever automaton that produces an infinity for you.", "tokens": [50876, 2644, 352, 456, 490, 8459, 293, 2464, 760, 512, 13494, 3553, 25781, 300, 14725, 364, 13202, 337, 291, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11433542096937024, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0011851184535771608}, {"id": 128, "seek": 68800, "start": 705.44, "end": 709.84, "text": " I was recently browsing Penrose's book, The Road to Reality, and I would say that, I mean,", "tokens": [51236, 286, 390, 3938, 38602, 10571, 37841, 311, 1446, 11, 440, 11507, 281, 33822, 11, 293, 286, 576, 584, 300, 11, 286, 914, 11, 51456], "temperature": 0.0, "avg_logprob": -0.11433542096937024, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0011851184535771608}, {"id": 129, "seek": 68800, "start": 709.84, "end": 714.8, "text": " I don't know that much about physics, but the chapters were really interesting. They're talking", "tokens": [51456, 286, 500, 380, 458, 300, 709, 466, 10649, 11, 457, 264, 20013, 645, 534, 1880, 13, 814, 434, 1417, 51704], "temperature": 0.0, "avg_logprob": -0.11433542096937024, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0011851184535771608}, {"id": 130, "seek": 71480, "start": 714.8, "end": 721.04, "text": " about surfaces and manifolds and symmetries and fiber bundles and gauges and wave functions,", "tokens": [50364, 466, 16130, 293, 8173, 31518, 293, 14232, 302, 2244, 293, 12874, 13882, 904, 293, 5959, 39064, 293, 5772, 6828, 11, 50676], "temperature": 0.0, "avg_logprob": -0.062191009521484375, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.007816783152520657}, {"id": 131, "seek": 71480, "start": 721.04, "end": 725.92, "text": " calculus, matrix theory, and even computation. I mean, almost all of the discussion was on", "tokens": [50676, 33400, 11, 8141, 5261, 11, 293, 754, 24903, 13, 286, 914, 11, 1920, 439, 295, 264, 5017, 390, 322, 50920], "temperature": 0.0, "avg_logprob": -0.062191009521484375, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.007816783152520657}, {"id": 132, "seek": 71480, "start": 725.92, "end": 730.8, "text": " mathematical modeling at different levels of description or emergence, if you will. And", "tokens": [50920, 18894, 15983, 412, 819, 4358, 295, 3855, 420, 36211, 11, 498, 291, 486, 13, 400, 51164], "temperature": 0.0, "avg_logprob": -0.062191009521484375, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.007816783152520657}, {"id": 133, "seek": 71480, "start": 730.8, "end": 735.8399999999999, "text": " in machine learning and AI, we are forever challenged by trying to get machines to model", "tokens": [51164, 294, 3479, 2539, 293, 7318, 11, 321, 366, 5680, 17737, 538, 1382, 281, 483, 8379, 281, 2316, 51416], "temperature": 0.0, "avg_logprob": -0.062191009521484375, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.007816783152520657}, {"id": 134, "seek": 71480, "start": 735.8399999999999, "end": 741.3599999999999, "text": " physical reality at different levels of description using an interoperable set of tools. So it seems", "tokens": [51416, 4001, 4103, 412, 819, 4358, 295, 3855, 1228, 364, 728, 7192, 712, 992, 295, 3873, 13, 407, 309, 2544, 51692], "temperature": 0.0, "avg_logprob": -0.062191009521484375, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.007816783152520657}, {"id": 135, "seek": 74136, "start": 741.36, "end": 746.48, "text": " increasingly true that we need machines that can learn descriptions and concepts at multiple levels", "tokens": [50364, 12980, 2074, 300, 321, 643, 8379, 300, 393, 1466, 24406, 293, 10392, 412, 3866, 4358, 50620], "temperature": 0.0, "avg_logprob": -0.065723332491788, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.011627290397882462}, {"id": 136, "seek": 74136, "start": 746.48, "end": 750.64, "text": " if we're ever going to have AGI capable of understanding the world and learning novel", "tokens": [50620, 498, 321, 434, 1562, 516, 281, 362, 316, 26252, 8189, 295, 3701, 264, 1002, 293, 2539, 7613, 50828], "temperature": 0.0, "avg_logprob": -0.065723332491788, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.011627290397882462}, {"id": 137, "seek": 74136, "start": 750.64, "end": 756.48, "text": " semantic models. All of machine learning models today work by chopping up a Euclidean space into", "tokens": [50828, 47982, 5245, 13, 1057, 295, 3479, 2539, 5245, 965, 589, 538, 35205, 493, 257, 462, 1311, 31264, 282, 1901, 666, 51120], "temperature": 0.0, "avg_logprob": -0.065723332491788, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.011627290397882462}, {"id": 138, "seek": 74136, "start": 756.48, "end": 762.48, "text": " what is effectively a locality-sensitive lookup table. Very big one. And we need AGIs that can go", "tokens": [51120, 437, 307, 8659, 257, 1628, 1860, 12, 82, 34465, 574, 1010, 3199, 13, 4372, 955, 472, 13, 400, 321, 643, 316, 26252, 82, 300, 393, 352, 51420], "temperature": 0.0, "avg_logprob": -0.065723332491788, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.011627290397882462}, {"id": 139, "seek": 74136, "start": 762.48, "end": 768.0, "text": " far beyond this. It's got to be able to learn novel geometries beyond even what humans could", "tokens": [51420, 1400, 4399, 341, 13, 467, 311, 658, 281, 312, 1075, 281, 1466, 7613, 12956, 2244, 4399, 754, 437, 6255, 727, 51696], "temperature": 0.0, "avg_logprob": -0.065723332491788, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.011627290397882462}, {"id": 140, "seek": 76800, "start": 768.0, "end": 772.48, "text": " have come up with and the ability to reason topologically and algebraically over those", "tokens": [50364, 362, 808, 493, 365, 293, 264, 3485, 281, 1778, 1192, 17157, 293, 21989, 984, 670, 729, 50588], "temperature": 0.0, "avg_logprob": -0.09548447682307316, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.005381316877901554}, {"id": 141, "seek": 76800, "start": 772.48, "end": 777.04, "text": " geometries. Something which I think you would agree is not happening with the current deep learning", "tokens": [50588, 12956, 2244, 13, 6595, 597, 286, 519, 291, 576, 3986, 307, 406, 2737, 365, 264, 2190, 2452, 2539, 50816], "temperature": 0.0, "avg_logprob": -0.09548447682307316, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.005381316877901554}, {"id": 142, "seek": 76800, "start": 777.04, "end": 784.64, "text": " systems. Well, let's start out with the notion of geometry first. If you read Penrose's book,", "tokens": [50816, 3652, 13, 1042, 11, 718, 311, 722, 484, 365, 264, 10710, 295, 18426, 700, 13, 759, 291, 1401, 10571, 37841, 311, 1446, 11, 51196], "temperature": 0.0, "avg_logprob": -0.09548447682307316, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.005381316877901554}, {"id": 143, "seek": 76800, "start": 784.64, "end": 789.04, "text": " what you find is that this entire universe is geometric, which means it's made of", "tokens": [51196, 437, 291, 915, 307, 300, 341, 2302, 6445, 307, 33246, 11, 597, 1355, 309, 311, 1027, 295, 51416], "temperature": 0.0, "avg_logprob": -0.09548447682307316, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.005381316877901554}, {"id": 144, "seek": 76800, "start": 789.04, "end": 795.68, "text": " continuous spaces in which things are happening. And if we actually look into the world deeply,", "tokens": [51416, 10957, 7673, 294, 597, 721, 366, 2737, 13, 400, 498, 321, 767, 574, 666, 264, 1002, 8760, 11, 51748], "temperature": 0.0, "avg_logprob": -0.09548447682307316, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.005381316877901554}, {"id": 145, "seek": 79568, "start": 796.3199999999999, "end": 801.52, "text": " quantum mechanics is not a geometric theory. The geometry only emerges approximately", "tokens": [50396, 13018, 12939, 307, 406, 257, 33246, 5261, 13, 440, 18426, 787, 38965, 10447, 50656], "temperature": 0.0, "avg_logprob": -0.07804630798043556, "compression_ratio": 1.7164750957854407, "no_speech_prob": 0.0038840605411678553}, {"id": 146, "seek": 79568, "start": 801.52, "end": 807.04, "text": " at the level of the space-time description. And it seems that geometry is actually the", "tokens": [50656, 412, 264, 1496, 295, 264, 1901, 12, 3766, 3855, 13, 400, 309, 2544, 300, 18426, 307, 767, 264, 50932], "temperature": 0.0, "avg_logprob": -0.07804630798043556, "compression_ratio": 1.7164750957854407, "no_speech_prob": 0.0038840605411678553}, {"id": 147, "seek": 79568, "start": 807.04, "end": 812.0, "text": " domain of too many parts to count. In reality, all the objects that we describe as surfaces,", "tokens": [50932, 9274, 295, 886, 867, 3166, 281, 1207, 13, 682, 4103, 11, 439, 264, 6565, 300, 321, 6786, 382, 16130, 11, 51180], "temperature": 0.0, "avg_logprob": -0.07804630798043556, "compression_ratio": 1.7164750957854407, "no_speech_prob": 0.0038840605411678553}, {"id": 148, "seek": 79568, "start": 812.0, "end": 817.52, "text": " if you zoom in, are made of discrete parts like atoms and particles and so on. And these in turn", "tokens": [51180, 498, 291, 8863, 294, 11, 366, 1027, 295, 27706, 3166, 411, 16871, 293, 10007, 293, 370, 322, 13, 400, 613, 294, 1261, 51456], "temperature": 0.0, "avg_logprob": -0.07804630798043556, "compression_ratio": 1.7164750957854407, "no_speech_prob": 0.0038840605411678553}, {"id": 149, "seek": 79568, "start": 817.52, "end": 823.04, "text": " are made out of things that have a finite resolution. And if we look into our computer", "tokens": [51456, 366, 1027, 484, 295, 721, 300, 362, 257, 19362, 8669, 13, 400, 498, 321, 574, 666, 527, 3820, 51732], "temperature": 0.0, "avg_logprob": -0.07804630798043556, "compression_ratio": 1.7164750957854407, "no_speech_prob": 0.0038840605411678553}, {"id": 150, "seek": 82304, "start": 823.04, "end": 827.4399999999999, "text": " programs, you can create stuff that looks continuous to us, but there's nothing continuous", "tokens": [50364, 4268, 11, 291, 393, 1884, 1507, 300, 1542, 10957, 281, 505, 11, 457, 456, 311, 1825, 10957, 50584], "temperature": 0.0, "avg_logprob": -0.13161863864046855, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0011149866040796041}, {"id": 151, "seek": 82304, "start": 827.4399999999999, "end": 834.64, "text": " inside of our computer programs. And it turns out that the assumption of continuity requires", "tokens": [50584, 1854, 295, 527, 3820, 4268, 13, 400, 309, 4523, 484, 300, 264, 15302, 295, 23807, 7029, 50944], "temperature": 0.0, "avg_logprob": -0.13161863864046855, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0011149866040796041}, {"id": 152, "seek": 82304, "start": 834.64, "end": 840.16, "text": " that we partition the space into infinitely many parts. So now we are again running against", "tokens": [50944, 300, 321, 24808, 264, 1901, 666, 36227, 867, 3166, 13, 407, 586, 321, 366, 797, 2614, 1970, 51220], "temperature": 0.0, "avg_logprob": -0.13161863864046855, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0011149866040796041}, {"id": 153, "seek": 82304, "start": 840.16, "end": 846.0799999999999, "text": " that thing which G\u00fcrl has shown us as difficult. And it's not a big problem in practice because", "tokens": [51220, 300, 551, 597, 460, 1655, 75, 575, 4898, 505, 382, 2252, 13, 400, 309, 311, 406, 257, 955, 1154, 294, 3124, 570, 51516], "temperature": 0.0, "avg_logprob": -0.13161863864046855, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0011149866040796041}, {"id": 154, "seek": 82304, "start": 846.0799999999999, "end": 850.7199999999999, "text": " in practice, we never need to do these infinitely many things to produce a computer game with an", "tokens": [51516, 294, 3124, 11, 321, 1128, 643, 281, 360, 613, 36227, 867, 721, 281, 5258, 257, 3820, 1216, 365, 364, 51748], "temperature": 0.0, "avg_logprob": -0.13161863864046855, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0011149866040796041}, {"id": 155, "seek": 85072, "start": 850.72, "end": 857.28, "text": " arbitrary fidelity. We can make something that looks like space. But the space that we think in", "tokens": [50364, 23211, 46404, 13, 492, 393, 652, 746, 300, 1542, 411, 1901, 13, 583, 264, 1901, 300, 321, 519, 294, 50692], "temperature": 0.0, "avg_logprob": -0.08226061703865989, "compression_ratio": 1.9328063241106719, "no_speech_prob": 0.0029806422535330057}, {"id": 156, "seek": 85072, "start": 857.28, "end": 862.5600000000001, "text": " and so on is an approximation that our brain has discovered. It's a set of operators that converge", "tokens": [50692, 293, 370, 322, 307, 364, 28023, 300, 527, 3567, 575, 6941, 13, 467, 311, 257, 992, 295, 19077, 300, 41881, 50956], "temperature": 0.0, "avg_logprob": -0.08226061703865989, "compression_ratio": 1.9328063241106719, "no_speech_prob": 0.0029806422535330057}, {"id": 157, "seek": 85072, "start": 862.5600000000001, "end": 868.08, "text": " in the limit. But the limit doesn't exist. It just, it happens that when you live in a world that is", "tokens": [50956, 294, 264, 4948, 13, 583, 264, 4948, 1177, 380, 2514, 13, 467, 445, 11, 309, 2314, 300, 562, 291, 1621, 294, 257, 1002, 300, 307, 51232], "temperature": 0.0, "avg_logprob": -0.08226061703865989, "compression_ratio": 1.9328063241106719, "no_speech_prob": 0.0029806422535330057}, {"id": 158, "seek": 85072, "start": 868.08, "end": 872.96, "text": " made of too many parts to count for almost everywhere where you look, you need to find these operators", "tokens": [51232, 1027, 295, 886, 867, 3166, 281, 1207, 337, 1920, 5315, 689, 291, 574, 11, 291, 643, 281, 915, 613, 19077, 51476], "temperature": 0.0, "avg_logprob": -0.08226061703865989, "compression_ratio": 1.9328063241106719, "no_speech_prob": 0.0029806422535330057}, {"id": 159, "seek": 85072, "start": 872.96, "end": 877.12, "text": " that converge in the limit. And the set of operators that happens to converge in the limit", "tokens": [51476, 300, 41881, 294, 264, 4948, 13, 400, 264, 992, 295, 19077, 300, 2314, 281, 41881, 294, 264, 4948, 51684], "temperature": 0.0, "avg_logprob": -0.08226061703865989, "compression_ratio": 1.9328063241106719, "no_speech_prob": 0.0029806422535330057}, {"id": 160, "seek": 87712, "start": 877.12, "end": 883.28, "text": " and is still computable. This is what we call geometry. And to use these uncomputable geometric", "tokens": [50364, 293, 307, 920, 2807, 712, 13, 639, 307, 437, 321, 818, 18426, 13, 400, 281, 764, 613, 8585, 2582, 712, 33246, 50672], "temperature": 0.0, "avg_logprob": -0.10739095495381487, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.030195504426956177}, {"id": 161, "seek": 87712, "start": 883.28, "end": 888.48, "text": " approximations for macroscopic physics like Newtonian mechanics is completely fine. You're", "tokens": [50672, 8542, 763, 337, 7912, 38006, 299, 10649, 411, 19541, 952, 12939, 307, 2584, 2489, 13, 509, 434, 50932], "temperature": 0.0, "avg_logprob": -0.10739095495381487, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.030195504426956177}, {"id": 162, "seek": 87712, "start": 888.48, "end": 892.48, "text": " just going to compute it up to a certain digit and then this is good. But it's a problem for", "tokens": [50932, 445, 516, 281, 14722, 309, 493, 281, 257, 1629, 14293, 293, 550, 341, 307, 665, 13, 583, 309, 311, 257, 1154, 337, 51132], "temperature": 0.0, "avg_logprob": -0.10739095495381487, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.030195504426956177}, {"id": 163, "seek": 87712, "start": 892.48, "end": 898.16, "text": " foundational physics. Because if it turns out that you cannot take a language that actually", "tokens": [51132, 32195, 10649, 13, 1436, 498, 309, 4523, 484, 300, 291, 2644, 747, 257, 2856, 300, 767, 51416], "temperature": 0.0, "avg_logprob": -0.10739095495381487, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.030195504426956177}, {"id": 164, "seek": 87712, "start": 898.16, "end": 902.88, "text": " computes infinities, if you cannot construct your language, then you cannot write a universe in it.", "tokens": [51416, 715, 1819, 7193, 1088, 11, 498, 291, 2644, 7690, 428, 2856, 11, 550, 291, 2644, 2464, 257, 6445, 294, 309, 13, 51652], "temperature": 0.0, "avg_logprob": -0.10739095495381487, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.030195504426956177}, {"id": 165, "seek": 90288, "start": 903.2, "end": 907.92, "text": " So our universe is not written in continuous language, but Penrose universe is.", "tokens": [50380, 407, 527, 6445, 307, 406, 3720, 294, 10957, 2856, 11, 457, 10571, 37841, 6445, 307, 13, 50616], "temperature": 0.0, "avg_logprob": -0.1485821960169241, "compression_ratio": 1.6107142857142858, "no_speech_prob": 0.0008829267462715507}, {"id": 166, "seek": 90288, "start": 909.84, "end": 913.84, "text": " This doesn't mean that geometry is full. We need this to describe the world of too many parts to", "tokens": [50712, 639, 1177, 380, 914, 300, 18426, 307, 1577, 13, 492, 643, 341, 281, 6786, 264, 1002, 295, 886, 867, 3166, 281, 50912], "temperature": 0.0, "avg_logprob": -0.1485821960169241, "compression_ratio": 1.6107142857142858, "no_speech_prob": 0.0008829267462715507}, {"id": 167, "seek": 90288, "start": 913.84, "end": 918.08, "text": " count. But we do this via computational approximations. Our brain does the same.", "tokens": [50912, 1207, 13, 583, 321, 360, 341, 5766, 28270, 8542, 763, 13, 2621, 3567, 775, 264, 912, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1485821960169241, "compression_ratio": 1.6107142857142858, "no_speech_prob": 0.0008829267462715507}, {"id": 168, "seek": 90288, "start": 919.28, "end": 924.96, "text": " So let me ask you this then because we come across kind of the infinities a couple of times. And I", "tokens": [51184, 407, 718, 385, 1029, 291, 341, 550, 570, 321, 808, 2108, 733, 295, 264, 7193, 1088, 257, 1916, 295, 1413, 13, 400, 286, 51468], "temperature": 0.0, "avg_logprob": -0.1485821960169241, "compression_ratio": 1.6107142857142858, "no_speech_prob": 0.0008829267462715507}, {"id": 169, "seek": 90288, "start": 924.96, "end": 929.68, "text": " know that you placed an emphasis on constructive mathematics. So of course, you and all of us,", "tokens": [51468, 458, 300, 291, 7074, 364, 16271, 322, 30223, 18666, 13, 407, 295, 1164, 11, 291, 293, 439, 295, 505, 11, 51704], "temperature": 0.0, "avg_logprob": -0.1485821960169241, "compression_ratio": 1.6107142857142858, "no_speech_prob": 0.0008829267462715507}, {"id": 170, "seek": 92968, "start": 930.2399999999999, "end": 935.28, "text": " you know, except let's say the existence of potential infinities, you know, algorithms that", "tokens": [50392, 291, 458, 11, 3993, 718, 311, 584, 264, 9123, 295, 3995, 7193, 1088, 11, 291, 458, 11, 14642, 300, 50644], "temperature": 0.0, "avg_logprob": -0.1225031736855195, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.0006666745757684112}, {"id": 171, "seek": 92968, "start": 935.28, "end": 939.8399999999999, "text": " you can sit there and just keep calculating for as long as you want and get kind of more digits.", "tokens": [50644, 291, 393, 1394, 456, 293, 445, 1066, 28258, 337, 382, 938, 382, 291, 528, 293, 483, 733, 295, 544, 27011, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1225031736855195, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.0006666745757684112}, {"id": 172, "seek": 92968, "start": 939.8399999999999, "end": 946.0, "text": " But it's really around actual infinities that we seem to be running into problems. So let me ask", "tokens": [50872, 583, 309, 311, 534, 926, 3539, 7193, 1088, 300, 321, 1643, 281, 312, 2614, 666, 2740, 13, 407, 718, 385, 1029, 51180], "temperature": 0.0, "avg_logprob": -0.1225031736855195, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.0006666745757684112}, {"id": 173, "seek": 92968, "start": 946.0, "end": 951.12, "text": " this this first question here, really leading up to some computational questions, which is,", "tokens": [51180, 341, 341, 700, 1168, 510, 11, 534, 5775, 493, 281, 512, 28270, 1651, 11, 597, 307, 11, 51436], "temperature": 0.0, "avg_logprob": -0.1225031736855195, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.0006666745757684112}, {"id": 174, "seek": 92968, "start": 951.92, "end": 958.7199999999999, "text": " can the universe, can our actual universe that we're in right now be actually infinite", "tokens": [51476, 393, 264, 6445, 11, 393, 527, 3539, 6445, 300, 321, 434, 294, 558, 586, 312, 767, 13785, 51816], "temperature": 0.0, "avg_logprob": -0.1225031736855195, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.0006666745757684112}, {"id": 175, "seek": 95872, "start": 958.72, "end": 959.84, "text": " in spatial extent?", "tokens": [50364, 294, 23598, 8396, 30, 50420], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 176, "seek": 95872, "start": 962.32, "end": 966.48, "text": " A problem is that it can have unboundedness in the sense that you have a computation that", "tokens": [50544, 316, 1154, 307, 300, 309, 393, 362, 517, 18767, 292, 1287, 294, 264, 2020, 300, 291, 362, 257, 24903, 300, 50752], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 177, "seek": 95872, "start": 966.48, "end": 973.12, "text": " doesn't stop giving your results. But you cannot take the last result of such a computation and go", "tokens": [50752, 1177, 380, 1590, 2902, 428, 3542, 13, 583, 291, 2644, 747, 264, 1036, 1874, 295, 1270, 257, 24903, 293, 352, 51084], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 178, "seek": 95872, "start": 973.12, "end": 978.08, "text": " to the next step. You cannot have a computation that relies on knowing the last digit of pi", "tokens": [51084, 281, 264, 958, 1823, 13, 509, 2644, 362, 257, 24903, 300, 30910, 322, 5276, 264, 1036, 14293, 295, 3895, 51332], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 179, "seek": 95872, "start": 978.08, "end": 982.96, "text": " before it goes to the next step. In the sense that you don't have an infinity. But the infinities", "tokens": [51332, 949, 309, 1709, 281, 264, 958, 1823, 13, 682, 264, 2020, 300, 291, 500, 380, 362, 364, 13202, 13, 583, 264, 7193, 1088, 51576], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 180, "seek": 95872, "start": 982.96, "end": 987.12, "text": " are about the conclusion of such a function. It means that you actually run this function to the", "tokens": [51576, 366, 466, 264, 10063, 295, 1270, 257, 2445, 13, 467, 1355, 300, 291, 767, 1190, 341, 2445, 281, 264, 51784], "temperature": 0.0, "avg_logprob": -0.11778470848788734, "compression_ratio": 1.9839357429718876, "no_speech_prob": 0.0006878061685711145}, {"id": 181, "seek": 98712, "start": 987.12, "end": 991.92, "text": " end and then do something with the result. Unboundedness is different in the sense that", "tokens": [50364, 917, 293, 550, 360, 746, 365, 264, 1874, 13, 1156, 18767, 292, 1287, 307, 819, 294, 264, 2020, 300, 50604], "temperature": 0.0, "avg_logprob": -0.1018081012072864, "compression_ratio": 1.7953667953667953, "no_speech_prob": 0.003940049093216658}, {"id": 182, "seek": 98712, "start": 991.92, "end": 996.08, "text": " you will always get something new that you didn't expect that they cannot predict.", "tokens": [50604, 291, 486, 1009, 483, 746, 777, 300, 291, 994, 380, 2066, 300, 436, 2644, 6069, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1018081012072864, "compression_ratio": 1.7953667953667953, "no_speech_prob": 0.003940049093216658}, {"id": 183, "seek": 98712, "start": 996.08, "end": 1002.4, "text": " But it's just going on and on without this end. And I think it's completely conceivable that our", "tokens": [50812, 583, 309, 311, 445, 516, 322, 293, 322, 1553, 341, 917, 13, 400, 286, 519, 309, 311, 2584, 10413, 34376, 300, 527, 51128], "temperature": 0.0, "avg_logprob": -0.1018081012072864, "compression_ratio": 1.7953667953667953, "no_speech_prob": 0.003940049093216658}, {"id": 184, "seek": 98712, "start": 1002.4, "end": 1009.36, "text": " universe is in this class of systems in the sense that it doesn't end. But it doesn't mean that there", "tokens": [51128, 6445, 307, 294, 341, 1508, 295, 3652, 294, 264, 2020, 300, 309, 1177, 380, 917, 13, 583, 309, 1177, 380, 914, 300, 456, 51476], "temperature": 0.0, "avg_logprob": -0.1018081012072864, "compression_ratio": 1.7953667953667953, "no_speech_prob": 0.003940049093216658}, {"id": 185, "seek": 98712, "start": 1009.36, "end": 1015.04, "text": " is anything that gives you the result of an infinite computation. Because if that was the case,", "tokens": [51476, 307, 1340, 300, 2709, 291, 264, 1874, 295, 364, 13785, 24903, 13, 1436, 498, 300, 390, 264, 1389, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1018081012072864, "compression_ratio": 1.7953667953667953, "no_speech_prob": 0.003940049093216658}, {"id": 186, "seek": 101504, "start": 1015.04, "end": 1020.16, "text": " then it could not be expressed in any language. It also means if something cannot be expressed", "tokens": [50364, 550, 309, 727, 406, 312, 12675, 294, 604, 2856, 13, 467, 611, 1355, 498, 746, 2644, 312, 12675, 50620], "temperature": 0.0, "avg_logprob": -0.12261087704548794, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.010962571948766708}, {"id": 187, "seek": 101504, "start": 1020.16, "end": 1025.36, "text": " in any language, that you cannot actually properly think about it. Because when you think you need", "tokens": [50620, 294, 604, 2856, 11, 300, 291, 2644, 767, 6108, 519, 466, 309, 13, 1436, 562, 291, 519, 291, 643, 50880], "temperature": 0.0, "avg_logprob": -0.12261087704548794, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.010962571948766708}, {"id": 188, "seek": 101504, "start": 1025.36, "end": 1029.92, "text": " to think in some kind of language, not in English, but in some kind of language of sort or in a", "tokens": [50880, 281, 519, 294, 512, 733, 295, 2856, 11, 406, 294, 3669, 11, 457, 294, 512, 733, 295, 2856, 295, 1333, 420, 294, 257, 51108], "temperature": 0.0, "avg_logprob": -0.12261087704548794, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.010962571948766708}, {"id": 189, "seek": 101504, "start": 1029.92, "end": 1035.04, "text": " mathematical language that doesn't have contradictions. And what Goethe has shown is that the language", "tokens": [51108, 18894, 2856, 300, 1177, 380, 362, 15858, 15607, 13, 400, 437, 1037, 302, 675, 575, 4898, 307, 300, 264, 2856, 51364], "temperature": 0.0, "avg_logprob": -0.12261087704548794, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.010962571948766708}, {"id": 190, "seek": 101504, "start": 1035.04, "end": 1042.08, "text": " that he hoped to reason in about infinities breaks that it has contradictions in it. That at some", "tokens": [51364, 300, 415, 19737, 281, 1778, 294, 466, 7193, 1088, 9857, 300, 309, 575, 15858, 15607, 294, 309, 13, 663, 412, 512, 51716], "temperature": 0.0, "avg_logprob": -0.12261087704548794, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.010962571948766708}, {"id": 191, "seek": 104208, "start": 1042.08, "end": 1047.84, "text": " point, it blows itself apart. So the languages that we can build are only those in which we have", "tokens": [50364, 935, 11, 309, 18458, 2564, 4936, 13, 407, 264, 8650, 300, 321, 393, 1322, 366, 787, 729, 294, 597, 321, 362, 50652], "temperature": 0.0, "avg_logprob": -0.1232168960571289, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.002322820480912924}, {"id": 192, "seek": 104208, "start": 1047.84, "end": 1052.0, "text": " to assume that infinities cannot be built. So infinity, in this sense, is meaningless.", "tokens": [50652, 281, 6552, 300, 7193, 1088, 2644, 312, 3094, 13, 407, 13202, 11, 294, 341, 2020, 11, 307, 33232, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1232168960571289, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.002322820480912924}, {"id": 193, "seek": 104208, "start": 1052.56, "end": 1055.04, "text": " Because we cannot make it in any kind of language.", "tokens": [50888, 1436, 321, 2644, 652, 309, 294, 604, 733, 295, 2856, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1232168960571289, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.002322820480912924}, {"id": 194, "seek": 104208, "start": 1056.0, "end": 1062.48, "text": " So the thing is, though, I'm not limiting what the universe is capable of based on human mental", "tokens": [51060, 407, 264, 551, 307, 11, 1673, 11, 286, 478, 406, 22083, 437, 264, 6445, 307, 8189, 295, 2361, 322, 1952, 4973, 51384], "temperature": 0.0, "avg_logprob": -0.1232168960571289, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.002322820480912924}, {"id": 195, "seek": 104208, "start": 1062.48, "end": 1069.52, "text": " and linguistic limitations or even mathematical limitations. I'm asking you if it's possible", "tokens": [51384, 293, 43002, 15705, 420, 754, 18894, 15705, 13, 286, 478, 3365, 291, 498, 309, 311, 1944, 51736], "temperature": 0.0, "avg_logprob": -0.1232168960571289, "compression_ratio": 1.6653543307086613, "no_speech_prob": 0.002322820480912924}, {"id": 196, "seek": 106952, "start": 1069.52, "end": 1076.48, "text": " for this universe that we're in to ontically be right now actually infinite in spatial extent.", "tokens": [50364, 337, 341, 6445, 300, 321, 434, 294, 281, 6592, 984, 312, 558, 586, 767, 13785, 294, 23598, 8396, 13, 50712], "temperature": 0.0, "avg_logprob": -0.10207181391508682, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0005613576504401863}, {"id": 197, "seek": 106952, "start": 1079.36, "end": 1084.6399999999999, "text": " The thing is that you try to make a reference to something that you cannot observe, that cannot", "tokens": [50856, 440, 551, 307, 300, 291, 853, 281, 652, 257, 6408, 281, 746, 300, 291, 2644, 11441, 11, 300, 2644, 51120], "temperature": 0.0, "avg_logprob": -0.10207181391508682, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0005613576504401863}, {"id": 198, "seek": 106952, "start": 1084.6399999999999, "end": 1090.8799999999999, "text": " conceive of other than making a model in some kind of language. And to have that model make sense,", "tokens": [51120, 48605, 295, 661, 813, 1455, 257, 2316, 294, 512, 733, 295, 2856, 13, 400, 281, 362, 300, 2316, 652, 2020, 11, 51432], "temperature": 0.0, "avg_logprob": -0.10207181391508682, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0005613576504401863}, {"id": 199, "seek": 106952, "start": 1090.8799999999999, "end": 1096.32, "text": " the language needs to work. Right? Otherwise, you are just maybe in some kind of delusional thing.", "tokens": [51432, 264, 2856, 2203, 281, 589, 13, 1779, 30, 10328, 11, 291, 366, 445, 1310, 294, 512, 733, 295, 1103, 301, 1966, 551, 13, 51704], "temperature": 0.0, "avg_logprob": -0.10207181391508682, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0005613576504401863}, {"id": 200, "seek": 109632, "start": 1096.96, "end": 1101.2, "text": " And we can construct delusional things. We can construct languages that have bugs that we cannot", "tokens": [50396, 400, 321, 393, 7690, 1103, 301, 1966, 721, 13, 492, 393, 7690, 8650, 300, 362, 15120, 300, 321, 2644, 50608], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 201, "seek": 109632, "start": 1101.2, "end": 1106.48, "text": " see. But if we use a language that has bugs in it that we cannot see and we cannot repair them,", "tokens": [50608, 536, 13, 583, 498, 321, 764, 257, 2856, 300, 575, 15120, 294, 309, 300, 321, 2644, 536, 293, 321, 2644, 10535, 552, 11, 50872], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 202, "seek": 109632, "start": 1106.48, "end": 1110.96, "text": " then this means that the stuff that we express in the language is not meaningful. Right? We have to", "tokens": [50872, 550, 341, 1355, 300, 264, 1507, 300, 321, 5109, 294, 264, 2856, 307, 406, 10995, 13, 1779, 30, 492, 362, 281, 51096], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 203, "seek": 109632, "start": 1110.96, "end": 1115.04, "text": " use a different language that has maybe the same expressive power but doesn't have these bugs.", "tokens": [51096, 764, 257, 819, 2856, 300, 575, 1310, 264, 912, 40189, 1347, 457, 1177, 380, 362, 613, 15120, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 204, "seek": 109632, "start": 1115.6, "end": 1121.2, "text": " But now if you try to think about the universe in the language that allows you to imagine that", "tokens": [51328, 583, 586, 498, 291, 853, 281, 519, 466, 264, 6445, 294, 264, 2856, 300, 4045, 291, 281, 3811, 300, 51608], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 205, "seek": 109632, "start": 1121.2, "end": 1125.9199999999998, "text": " the universe is literally infinite, rather than very, very, very big and much bigger than you", "tokens": [51608, 264, 6445, 307, 3736, 13785, 11, 2831, 813, 588, 11, 588, 11, 588, 955, 293, 709, 3801, 813, 291, 51844], "temperature": 0.0, "avg_logprob": -0.11354688773477885, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.003323446260765195}, {"id": 206, "seek": 112592, "start": 1125.92, "end": 1131.3600000000001, "text": " can imagine and not ending, which is for all means and purposes almost the same thing. Right?", "tokens": [50364, 393, 3811, 293, 406, 8121, 11, 597, 307, 337, 439, 1355, 293, 9932, 1920, 264, 912, 551, 13, 1779, 30, 50636], "temperature": 0.0, "avg_logprob": -0.15065956115722656, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0016203314298763871}, {"id": 207, "seek": 112592, "start": 1132.48, "end": 1138.5600000000002, "text": " Then if you do this other thing, then your thought doesn't mean anything. So it's basically you cannot", "tokens": [50692, 1396, 498, 291, 360, 341, 661, 551, 11, 550, 428, 1194, 1177, 380, 914, 1340, 13, 407, 309, 311, 1936, 291, 2644, 50996], "temperature": 0.0, "avg_logprob": -0.15065956115722656, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0016203314298763871}, {"id": 208, "seek": 112592, "start": 1138.5600000000002, "end": 1143.44, "text": " properly express the idea in your own mind without running into contradictions that the", "tokens": [50996, 6108, 5109, 264, 1558, 294, 428, 1065, 1575, 1553, 2614, 666, 15858, 15607, 300, 264, 51240], "temperature": 0.0, "avg_logprob": -0.15065956115722656, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0016203314298763871}, {"id": 209, "seek": 112592, "start": 1143.44, "end": 1149.1200000000001, "text": " universe is infinite in the sense that such a universe could exist. Okay, so you're basically", "tokens": [51240, 6445, 307, 13785, 294, 264, 2020, 300, 1270, 257, 6445, 727, 2514, 13, 1033, 11, 370, 291, 434, 1936, 51524], "temperature": 0.0, "avg_logprob": -0.15065956115722656, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0016203314298763871}, {"id": 210, "seek": 112592, "start": 1149.1200000000001, "end": 1152.8000000000002, "text": " following that. That's the issue. Basically, I cannot think that the universe is infinite. I cannot", "tokens": [51524, 3480, 300, 13, 663, 311, 264, 2734, 13, 8537, 11, 286, 2644, 519, 300, 264, 6445, 307, 13785, 13, 286, 2644, 51708], "temperature": 0.0, "avg_logprob": -0.15065956115722656, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.0016203314298763871}, {"id": 211, "seek": 115280, "start": 1152.8, "end": 1158.72, "text": " express this. That's my issue. Okay, fine. So you're basically saying that the English that I", "tokens": [50364, 5109, 341, 13, 663, 311, 452, 2734, 13, 1033, 11, 2489, 13, 407, 291, 434, 1936, 1566, 300, 264, 3669, 300, 286, 50660], "temperature": 0.0, "avg_logprob": -0.15362606048583985, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.002395029878243804}, {"id": 212, "seek": 115280, "start": 1158.72, "end": 1166.0, "text": " used just a minute or so ago just is not coherent or not conceivable. It's not something that you", "tokens": [50660, 1143, 445, 257, 3456, 420, 370, 2057, 445, 307, 406, 36239, 420, 406, 10413, 34376, 13, 467, 311, 406, 746, 300, 291, 51024], "temperature": 0.0, "avg_logprob": -0.15362606048583985, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.002395029878243804}, {"id": 213, "seek": 115280, "start": 1166.0, "end": 1168.96, "text": " want to. But the underlying thing behind the English, right? English is not designed to be", "tokens": [51024, 528, 281, 13, 583, 264, 14217, 551, 2261, 264, 3669, 11, 558, 30, 3669, 307, 406, 4761, 281, 312, 51172], "temperature": 0.0, "avg_logprob": -0.15362606048583985, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.002395029878243804}, {"id": 214, "seek": 115280, "start": 1168.96, "end": 1174.72, "text": " coherent. It's designed to be disambiguating. It's designed to be unprincipled to allow us to", "tokens": [51172, 36239, 13, 467, 311, 4761, 281, 312, 717, 2173, 16397, 990, 13, 467, 311, 4761, 281, 312, 517, 1424, 21961, 15551, 281, 2089, 505, 281, 51460], "temperature": 0.0, "avg_logprob": -0.15362606048583985, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.002395029878243804}, {"id": 215, "seek": 115280, "start": 1174.72, "end": 1180.3999999999999, "text": " express things vaguely and not break. But if you think really, really deeply and really exactly,", "tokens": [51460, 5109, 721, 13501, 48863, 293, 406, 1821, 13, 583, 498, 291, 519, 534, 11, 534, 8760, 293, 534, 2293, 11, 51744], "temperature": 0.0, "avg_logprob": -0.15362606048583985, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.002395029878243804}, {"id": 216, "seek": 118040, "start": 1180.4, "end": 1184.72, "text": " then the question is, what kind of model is your mind building? At which point is there just some", "tokens": [50364, 550, 264, 1168, 307, 11, 437, 733, 295, 2316, 307, 428, 1575, 2390, 30, 1711, 597, 935, 307, 456, 445, 512, 50580], "temperature": 0.0, "avg_logprob": -0.17614096783577127, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.001454426208510995}, {"id": 217, "seek": 118040, "start": 1184.72, "end": 1190.8000000000002, "text": " kind of noisy nabler that you're pointing at without actually decomposing it and anything that would", "tokens": [50580, 733, 295, 24518, 297, 455, 1918, 300, 291, 434, 12166, 412, 1553, 767, 22867, 6110, 309, 293, 1340, 300, 576, 50884], "temperature": 0.0, "avg_logprob": -0.17614096783577127, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.001454426208510995}, {"id": 218, "seek": 118040, "start": 1190.8000000000002, "end": 1201.1200000000001, "text": " make sense? Okay. And so the lack of really the ability to conceive or for actual infinities to", "tokens": [50884, 652, 2020, 30, 1033, 13, 400, 370, 264, 5011, 295, 534, 264, 3485, 281, 48605, 420, 337, 3539, 7193, 1088, 281, 51400], "temperature": 0.0, "avg_logprob": -0.17614096783577127, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.001454426208510995}, {"id": 219, "seek": 118040, "start": 1201.1200000000001, "end": 1206.24, "text": " ontically exist in some sense, if we just deny all that, so we're really just stuck with,", "tokens": [51400, 6592, 984, 2514, 294, 512, 2020, 11, 498, 321, 445, 15744, 439, 300, 11, 370, 321, 434, 534, 445, 5541, 365, 11, 51656], "temperature": 0.0, "avg_logprob": -0.17614096783577127, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.001454426208510995}, {"id": 220, "seek": 120624, "start": 1206.88, "end": 1212.16, "text": " all right, we've got finite everything, discrete everything. There's no such thing as a continuum.", "tokens": [50396, 439, 558, 11, 321, 600, 658, 19362, 1203, 11, 27706, 1203, 13, 821, 311, 572, 1270, 551, 382, 257, 36120, 13, 50660], "temperature": 0.0, "avg_logprob": -0.17786089026409646, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.006287824362516403}, {"id": 221, "seek": 120624, "start": 1212.8, "end": 1219.04, "text": " There's no such thing as actual infinite spatial extent, etc. That's really the world that you're", "tokens": [50692, 821, 311, 572, 1270, 551, 382, 3539, 13785, 23598, 8396, 11, 5183, 13, 663, 311, 534, 264, 1002, 300, 291, 434, 51004], "temperature": 0.0, "avg_logprob": -0.17786089026409646, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.006287824362516403}, {"id": 222, "seek": 120624, "start": 1219.6, "end": 1223.6, "text": " proposing here, right? That everything is constructed from at the end of the day,", "tokens": [51032, 29939, 510, 11, 558, 30, 663, 1203, 307, 17083, 490, 412, 264, 917, 295, 264, 786, 11, 51232], "temperature": 0.0, "avg_logprob": -0.17786089026409646, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.006287824362516403}, {"id": 223, "seek": 120624, "start": 1223.6, "end": 1229.36, "text": " finite, discrete kind of elements. So if we... Yeah, you can imagine that your mind is a library", "tokens": [51232, 19362, 11, 27706, 733, 295, 4959, 13, 407, 498, 321, 485, 865, 11, 291, 393, 3811, 300, 428, 1575, 307, 257, 6405, 51520], "temperature": 0.0, "avg_logprob": -0.17786089026409646, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.006287824362516403}, {"id": 224, "seek": 120624, "start": 1229.36, "end": 1233.44, "text": " of functions in a way, and these functions are doing jobs. And on the bouts of the box,", "tokens": [51520, 295, 6828, 294, 257, 636, 11, 293, 613, 6828, 366, 884, 4782, 13, 400, 322, 264, 272, 7711, 295, 264, 2424, 11, 51724], "temperature": 0.0, "avg_logprob": -0.17786089026409646, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.006287824362516403}, {"id": 225, "seek": 123344, "start": 1233.44, "end": 1238.24, "text": " you write down what these functions are doing. And you construct a box that this, in this box,", "tokens": [50364, 291, 2464, 760, 437, 613, 6828, 366, 884, 13, 400, 291, 7690, 257, 2424, 300, 341, 11, 294, 341, 2424, 11, 50604], "temperature": 0.0, "avg_logprob": -0.08772044372558593, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.012418515048921108}, {"id": 226, "seek": 123344, "start": 1238.24, "end": 1244.3200000000002, "text": " there is an infinity between, for instance, a continuum between two points. And then you open", "tokens": [50604, 456, 307, 364, 13202, 1296, 11, 337, 5197, 11, 257, 36120, 1296, 732, 2793, 13, 400, 550, 291, 1269, 50908], "temperature": 0.0, "avg_logprob": -0.08772044372558593, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.012418515048921108}, {"id": 227, "seek": 123344, "start": 1244.3200000000002, "end": 1249.92, "text": " up the box and look at what's actually inside of the box. And you realize it's just a lot of small", "tokens": [50908, 493, 264, 2424, 293, 574, 412, 437, 311, 767, 1854, 295, 264, 2424, 13, 400, 291, 4325, 309, 311, 445, 257, 688, 295, 1359, 51188], "temperature": 0.0, "avg_logprob": -0.08772044372558593, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.012418515048921108}, {"id": 228, "seek": 123344, "start": 1249.92, "end": 1254.3200000000002, "text": " steps. And it's designed in such a way that you can, if you want to have more steps, it's going to", "tokens": [51188, 4439, 13, 400, 309, 311, 4761, 294, 1270, 257, 636, 300, 291, 393, 11, 498, 291, 528, 281, 362, 544, 4439, 11, 309, 311, 516, 281, 51408], "temperature": 0.0, "avg_logprob": -0.08772044372558593, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.012418515048921108}, {"id": 229, "seek": 123344, "start": 1254.3200000000002, "end": 1259.3600000000001, "text": " give you more steps if you zoom in, right? And it's totally doing, apparently, what's written down", "tokens": [51408, 976, 291, 544, 4439, 498, 291, 8863, 294, 11, 558, 30, 400, 309, 311, 3879, 884, 11, 7970, 11, 437, 311, 3720, 760, 51660], "temperature": 0.0, "avg_logprob": -0.08772044372558593, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.012418515048921108}, {"id": 230, "seek": 125936, "start": 1259.36, "end": 1264.08, "text": " on the box. But if you look very closely, realize, oh no, the thing that is written down on the box", "tokens": [50364, 322, 264, 2424, 13, 583, 498, 291, 574, 588, 8185, 11, 4325, 11, 1954, 572, 11, 264, 551, 300, 307, 3720, 760, 322, 264, 2424, 50600], "temperature": 0.0, "avg_logprob": -0.08092509546587544, "compression_ratio": 2.024793388429752, "no_speech_prob": 0.24751675128936768}, {"id": 231, "seek": 125936, "start": 1264.08, "end": 1268.08, "text": " that you have written down on the box cannot actually be in there. You can prove that it cannot", "tokens": [50600, 300, 291, 362, 3720, 760, 322, 264, 2424, 2644, 767, 312, 294, 456, 13, 509, 393, 7081, 300, 309, 2644, 50800], "temperature": 0.0, "avg_logprob": -0.08092509546587544, "compression_ratio": 2.024793388429752, "no_speech_prob": 0.24751675128936768}, {"id": 232, "seek": 125936, "start": 1268.08, "end": 1272.0, "text": " be in there. It must be something else that's in there that is doing most of the work of what you've", "tokens": [50800, 312, 294, 456, 13, 467, 1633, 312, 746, 1646, 300, 311, 294, 456, 300, 307, 884, 881, 295, 264, 589, 295, 437, 291, 600, 50996], "temperature": 0.0, "avg_logprob": -0.08092509546587544, "compression_ratio": 2.024793388429752, "no_speech_prob": 0.24751675128936768}, {"id": 233, "seek": 125936, "start": 1272.0, "end": 1276.0, "text": " written down. So what you should actually be doing, I think, if you are interested in how things", "tokens": [50996, 3720, 760, 13, 407, 437, 291, 820, 767, 312, 884, 11, 286, 519, 11, 498, 291, 366, 3102, 294, 577, 721, 51196], "temperature": 0.0, "avg_logprob": -0.08092509546587544, "compression_ratio": 2.024793388429752, "no_speech_prob": 0.24751675128936768}, {"id": 234, "seek": 125936, "start": 1276.0, "end": 1282.32, "text": " actually work, write on the box what it's actually doing, which means it's going to subdivide or", "tokens": [51196, 767, 589, 11, 2464, 322, 264, 2424, 437, 309, 311, 767, 884, 11, 597, 1355, 309, 311, 516, 281, 45331, 482, 420, 51512], "temperature": 0.0, "avg_logprob": -0.08092509546587544, "compression_ratio": 2.024793388429752, "no_speech_prob": 0.24751675128936768}, {"id": 235, "seek": 128232, "start": 1282.32, "end": 1286.32, "text": " any interval with any resolution you want as long as you can afford it.", "tokens": [50364, 604, 15035, 365, 604, 8669, 291, 528, 382, 938, 382, 291, 393, 6157, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14639600118001303, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.021936213597655296}, {"id": 236, "seek": 128232, "start": 1287.28, "end": 1293.9199999999998, "text": " Okay. One mystery, if you will, for me, and I'm hoping you can help me understand this, is that", "tokens": [50612, 1033, 13, 1485, 11422, 11, 498, 291, 486, 11, 337, 385, 11, 293, 286, 478, 7159, 291, 393, 854, 385, 1223, 341, 11, 307, 300, 50944], "temperature": 0.0, "avg_logprob": -0.14639600118001303, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.021936213597655296}, {"id": 237, "seek": 128232, "start": 1294.72, "end": 1301.9199999999998, "text": " all of the standard models for physics that we have today, they do have in them these continuous,", "tokens": [50984, 439, 295, 264, 3832, 5245, 337, 10649, 300, 321, 362, 965, 11, 436, 360, 362, 294, 552, 613, 10957, 11, 51344], "temperature": 0.0, "avg_logprob": -0.14639600118001303, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.021936213597655296}, {"id": 238, "seek": 128232, "start": 1301.9199999999998, "end": 1307.4399999999998, "text": " you know, for example, symmetries that are rotational symmetry or things like that. They're", "tokens": [51344, 291, 458, 11, 337, 1365, 11, 14232, 302, 2244, 300, 366, 45420, 25440, 420, 721, 411, 300, 13, 814, 434, 51620], "temperature": 0.0, "avg_logprob": -0.14639600118001303, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.021936213597655296}, {"id": 239, "seek": 130744, "start": 1307.44, "end": 1315.44, "text": " built off of positing continuums with continuous waves, lots of continuities and infinities,", "tokens": [50364, 3094, 766, 295, 1366, 1748, 2993, 8099, 365, 10957, 9417, 11, 3195, 295, 2993, 1088, 293, 7193, 1088, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1072844386100769, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.016652369871735573}, {"id": 240, "seek": 130744, "start": 1315.44, "end": 1320.48, "text": " at least in the mathematical descriptions. Except for quantum mechanics, right?", "tokens": [50764, 412, 1935, 294, 264, 18894, 24406, 13, 16192, 337, 13018, 12939, 11, 558, 30, 51016], "temperature": 0.0, "avg_logprob": -0.1072844386100769, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.016652369871735573}, {"id": 241, "seek": 130744, "start": 1320.48, "end": 1325.52, "text": " Right. And I think based on what you've been saying, you would say that those are artifacts or", "tokens": [51016, 1779, 13, 400, 286, 519, 2361, 322, 437, 291, 600, 668, 1566, 11, 291, 576, 584, 300, 729, 366, 24617, 420, 51268], "temperature": 0.0, "avg_logprob": -0.1072844386100769, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.016652369871735573}, {"id": 242, "seek": 130744, "start": 1325.52, "end": 1332.72, "text": " properties of our mathematical descriptions of reality, but they're not actually extant in", "tokens": [51268, 7221, 295, 527, 18894, 24406, 295, 4103, 11, 457, 436, 434, 406, 767, 1279, 394, 294, 51628], "temperature": 0.0, "avg_logprob": -0.1072844386100769, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.016652369871735573}, {"id": 243, "seek": 133272, "start": 1332.72, "end": 1342.88, "text": " reality. And my mystery there is why do those continuous and mathematical maybe flawed and", "tokens": [50364, 4103, 13, 400, 452, 11422, 456, 307, 983, 360, 729, 10957, 293, 18894, 1310, 38823, 293, 50872], "temperature": 0.0, "avg_logprob": -0.1074971075980894, "compression_ratio": 1.5, "no_speech_prob": 0.0020504812709987164}, {"id": 244, "seek": 133272, "start": 1342.88, "end": 1351.1200000000001, "text": " inconsistent with infinities all over the place descriptions work so well for describing phenomenon", "tokens": [50872, 36891, 365, 7193, 1088, 439, 670, 264, 1081, 24406, 589, 370, 731, 337, 16141, 14029, 51284], "temperature": 0.0, "avg_logprob": -0.1074971075980894, "compression_ratio": 1.5, "no_speech_prob": 0.0020504812709987164}, {"id": 245, "seek": 133272, "start": 1351.1200000000001, "end": 1356.08, "text": " at different levels? If everything at the end of the day, you know, if we just looked at high enough", "tokens": [51284, 412, 819, 4358, 30, 759, 1203, 412, 264, 917, 295, 264, 786, 11, 291, 458, 11, 498, 321, 445, 2956, 412, 1090, 1547, 51532], "temperature": 0.0, "avg_logprob": -0.1074971075980894, "compression_ratio": 1.5, "no_speech_prob": 0.0020504812709987164}, {"id": 246, "seek": 135608, "start": 1356.08, "end": 1362.8, "text": " energy and small enough resolution, we'd see kind of the grid and, you know, all the discrete", "tokens": [50364, 2281, 293, 1359, 1547, 8669, 11, 321, 1116, 536, 733, 295, 264, 10748, 293, 11, 291, 458, 11, 439, 264, 27706, 50700], "temperature": 0.0, "avg_logprob": -0.1097374928148487, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.019704995676875114}, {"id": 247, "seek": 135608, "start": 1362.8, "end": 1370.48, "text": " effects and rotation happening kind of in little tiny, very small but not infinitesimal degrees.", "tokens": [50700, 5065, 293, 12447, 2737, 733, 295, 294, 707, 5870, 11, 588, 1359, 457, 406, 7193, 3324, 10650, 5310, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1097374928148487, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.019704995676875114}, {"id": 248, "seek": 135608, "start": 1370.48, "end": 1376.0, "text": " You know, why does all this continuous infinity based mathematics work so well? What is the", "tokens": [51084, 509, 458, 11, 983, 775, 439, 341, 10957, 13202, 2361, 18666, 589, 370, 731, 30, 708, 307, 264, 51360], "temperature": 0.0, "avg_logprob": -0.1097374928148487, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.019704995676875114}, {"id": 249, "seek": 135608, "start": 1376.0, "end": 1380.1599999999999, "text": " explanation for the unreasonable effectiveness of that kind of mathematics?", "tokens": [51360, 10835, 337, 264, 41730, 21208, 295, 300, 733, 295, 18666, 30, 51568], "temperature": 0.0, "avg_logprob": -0.1097374928148487, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.019704995676875114}, {"id": 250, "seek": 138016, "start": 1380.96, "end": 1388.0800000000002, "text": " The easiest answer is that the world in which we live in is made of extremely small parts.", "tokens": [50404, 440, 12889, 1867, 307, 300, 264, 1002, 294, 597, 321, 1621, 294, 307, 1027, 295, 4664, 1359, 3166, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19147021511951126, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0010480869095772505}, {"id": 251, "seek": 138016, "start": 1388.0800000000002, "end": 1394.16, "text": " And we could not exist if that world was not made of that many small parts. So for instance,", "tokens": [50760, 400, 321, 727, 406, 2514, 498, 300, 1002, 390, 406, 1027, 295, 300, 867, 1359, 3166, 13, 407, 337, 5197, 11, 51064], "temperature": 0.0, "avg_logprob": -0.19147021511951126, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0010480869095772505}, {"id": 252, "seek": 138016, "start": 1394.16, "end": 1399.68, "text": " you want to have a momentum for particles that are almost continuous. So you can address the", "tokens": [51064, 291, 528, 281, 362, 257, 11244, 337, 10007, 300, 366, 1920, 10957, 13, 407, 291, 393, 2985, 264, 51340], "temperature": 0.0, "avg_logprob": -0.19147021511951126, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0010480869095772505}, {"id": 253, "seek": 138016, "start": 1399.68, "end": 1404.48, "text": " space with high resolution because the momentum is what tells you where information comes from", "tokens": [51340, 1901, 365, 1090, 8669, 570, 264, 11244, 307, 437, 5112, 291, 689, 1589, 1487, 490, 51580], "temperature": 0.0, "avg_logprob": -0.19147021511951126, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0010480869095772505}, {"id": 254, "seek": 140448, "start": 1404.96, "end": 1410.16, "text": " in the universe, basically the direction of where from which information reaches you and so on.", "tokens": [50388, 294, 264, 6445, 11, 1936, 264, 3513, 295, 689, 490, 597, 1589, 14235, 291, 293, 370, 322, 13, 50648], "temperature": 0.0, "avg_logprob": -0.08746218681335449, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.027563614770770073}, {"id": 255, "seek": 140448, "start": 1410.16, "end": 1415.92, "text": " If that would be very coarse, then the complexity that you could build would probably be far lower.", "tokens": [50648, 759, 300, 576, 312, 588, 39312, 11, 550, 264, 14024, 300, 291, 727, 1322, 576, 1391, 312, 1400, 3126, 13, 50936], "temperature": 0.0, "avg_logprob": -0.08746218681335449, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.027563614770770073}, {"id": 256, "seek": 140448, "start": 1415.92, "end": 1422.16, "text": " And we consist of so many parts that when you look down, it's uncountably many for all practical", "tokens": [50936, 400, 321, 4603, 295, 370, 867, 3166, 300, 562, 291, 574, 760, 11, 309, 311, 6219, 792, 1188, 867, 337, 439, 8496, 51248], "temperature": 0.0, "avg_logprob": -0.08746218681335449, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.027563614770770073}, {"id": 257, "seek": 140448, "start": 1422.16, "end": 1427.2, "text": " purposes. So the mathematics that we need to describe the world that we are in that we need", "tokens": [51248, 9932, 13, 407, 264, 18666, 300, 321, 643, 281, 6786, 264, 1002, 300, 321, 366, 294, 300, 321, 643, 51500], "temperature": 0.0, "avg_logprob": -0.08746218681335449, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.027563614770770073}, {"id": 258, "seek": 140448, "start": 1427.2, "end": 1432.88, "text": " to model are mostly not in the realm of countable numbers. The countable numbers only play a role", "tokens": [51500, 281, 2316, 366, 5240, 406, 294, 264, 15355, 295, 1207, 712, 3547, 13, 440, 1207, 712, 3547, 787, 862, 257, 3090, 51784], "temperature": 0.0, "avg_logprob": -0.08746218681335449, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.027563614770770073}, {"id": 259, "seek": 143288, "start": 1432.96, "end": 1439.3600000000001, "text": " when we are looking at very few microscopic things. As soon as we leave this domain of a few apples", "tokens": [50368, 562, 321, 366, 1237, 412, 588, 1326, 47897, 721, 13, 1018, 2321, 382, 321, 1856, 341, 9274, 295, 257, 1326, 16814, 50688], "temperature": 0.0, "avg_logprob": -0.11165438416183636, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0020811944268643856}, {"id": 260, "seek": 143288, "start": 1439.3600000000001, "end": 1446.16, "text": " on our table, we almost instantly drop in this realm where we just need to switch to a continuous", "tokens": [50688, 322, 527, 3199, 11, 321, 1920, 13518, 3270, 294, 341, 15355, 689, 321, 445, 643, 281, 3679, 281, 257, 10957, 51028], "temperature": 0.0, "avg_logprob": -0.11165438416183636, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0020811944268643856}, {"id": 261, "seek": 143288, "start": 1446.16, "end": 1452.0800000000002, "text": " description of things. And this is completely fine for most of our history. When we did physics,", "tokens": [51028, 3855, 295, 721, 13, 400, 341, 307, 2584, 2489, 337, 881, 295, 527, 2503, 13, 1133, 321, 630, 10649, 11, 51324], "temperature": 0.0, "avg_logprob": -0.11165438416183636, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0020811944268643856}, {"id": 262, "seek": 143288, "start": 1452.0800000000002, "end": 1458.88, "text": " we never zoomed in that heart. And even now, when we really need to zoom at the level where", "tokens": [51324, 321, 1128, 8863, 292, 294, 300, 1917, 13, 400, 754, 586, 11, 562, 321, 534, 643, 281, 8863, 412, 264, 1496, 689, 51664], "temperature": 0.0, "avg_logprob": -0.11165438416183636, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.0020811944268643856}, {"id": 263, "seek": 145888, "start": 1458.88, "end": 1464.64, "text": " the plank length matters and the resolution of the universe becomes visible. And it's of course not", "tokens": [50364, 264, 27861, 4641, 7001, 293, 264, 8669, 295, 264, 6445, 3643, 8974, 13, 400, 309, 311, 295, 1164, 406, 50652], "temperature": 0.0, "avg_logprob": -0.1348910131374327, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.0016975787002593279}, {"id": 264, "seek": 145888, "start": 1464.64, "end": 1469.3600000000001, "text": " some Euclidean lattice, some grid that you can see. It's just that at this level, you no longer", "tokens": [50652, 512, 462, 1311, 31264, 282, 34011, 11, 512, 10748, 300, 291, 393, 536, 13, 467, 311, 445, 300, 412, 341, 1496, 11, 291, 572, 2854, 50888], "temperature": 0.0, "avg_logprob": -0.1348910131374327, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.0016975787002593279}, {"id": 265, "seek": 145888, "start": 1469.3600000000001, "end": 1476.88, "text": " have space. I wanted to move matters back over to some of the happenings in the world of large", "tokens": [50888, 362, 1901, 13, 286, 1415, 281, 1286, 7001, 646, 670, 281, 512, 295, 264, 1051, 1109, 294, 264, 1002, 295, 2416, 51264], "temperature": 0.0, "avg_logprob": -0.1348910131374327, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.0016975787002593279}, {"id": 266, "seek": 145888, "start": 1476.88, "end": 1483.0400000000002, "text": " language models and deep learning and so on. And first, quick fire question. I honestly,", "tokens": [51264, 2856, 5245, 293, 2452, 2539, 293, 370, 322, 13, 400, 700, 11, 1702, 2610, 1168, 13, 286, 6095, 11, 51572], "temperature": 0.0, "avg_logprob": -0.1348910131374327, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.0016975787002593279}, {"id": 267, "seek": 145888, "start": 1483.0400000000002, "end": 1488.0, "text": " you're a bit of an enigma to me, Joshua, because obviously I've read some of your research and", "tokens": [51572, 291, 434, 257, 857, 295, 364, 465, 16150, 281, 385, 11, 24005, 11, 570, 2745, 286, 600, 1401, 512, 295, 428, 2132, 293, 51820], "temperature": 0.0, "avg_logprob": -0.1348910131374327, "compression_ratio": 1.674911660777385, "no_speech_prob": 0.0016975787002593279}, {"id": 268, "seek": 148800, "start": 1488.0, "end": 1494.32, "text": " you seem like a hybrid guy to me. You know Ben Goetzel very well, for example, but you're also", "tokens": [50364, 291, 1643, 411, 257, 13051, 2146, 281, 385, 13, 509, 458, 3964, 1037, 10074, 338, 588, 731, 11, 337, 1365, 11, 457, 291, 434, 611, 50680], "temperature": 0.0, "avg_logprob": -0.1570555500148498, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0018330945167690516}, {"id": 269, "seek": 148800, "start": 1494.32, "end": 1501.2, "text": " hugely into the hype train on the connectionism. For example, you criticised Gary Marcus's", "tokens": [50680, 27417, 666, 264, 24144, 3847, 322, 264, 4984, 1434, 13, 1171, 1365, 11, 291, 7850, 2640, 13788, 26574, 311, 51024], "temperature": 0.0, "avg_logprob": -0.1570555500148498, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0018330945167690516}, {"id": 270, "seek": 148800, "start": 1501.2, "end": 1504.64, "text": " article. So the first question is, are you a symbolist or a connectionist?", "tokens": [51024, 7222, 13, 407, 264, 700, 1168, 307, 11, 366, 291, 257, 5986, 468, 420, 257, 4984, 468, 30, 51196], "temperature": 0.0, "avg_logprob": -0.1570555500148498, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0018330945167690516}, {"id": 271, "seek": 148800, "start": 1505.44, "end": 1512.16, "text": " I'm neither. The thing is that I hate deep learning as the best of us. Deep learning is ugly. It's", "tokens": [51236, 286, 478, 9662, 13, 440, 551, 307, 300, 286, 4700, 2452, 2539, 382, 264, 1151, 295, 505, 13, 14895, 2539, 307, 12246, 13, 467, 311, 51572], "temperature": 0.0, "avg_logprob": -0.1570555500148498, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0018330945167690516}, {"id": 272, "seek": 151216, "start": 1512.16, "end": 1519.28, "text": " brutalist. It's a few very simple algorithms that are blown up to the max. But I cannot prove", "tokens": [50364, 17878, 468, 13, 467, 311, 257, 1326, 588, 2199, 14642, 300, 366, 16479, 493, 281, 264, 11469, 13, 583, 286, 2644, 7081, 50720], "temperature": 0.0, "avg_logprob": -0.08086653070135431, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.03460186347365379}, {"id": 273, "seek": 151216, "start": 1519.28, "end": 1525.8400000000001, "text": " that these algorithms do not converge to what we want them to converge to. It's maybe not", "tokens": [50720, 300, 613, 14642, 360, 406, 41881, 281, 437, 321, 528, 552, 281, 41881, 281, 13, 467, 311, 1310, 406, 51048], "temperature": 0.0, "avg_logprob": -0.08086653070135431, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.03460186347365379}, {"id": 274, "seek": 151216, "start": 1525.8400000000001, "end": 1531.28, "text": " elegant, but it works. And the solution to problems with deep learning so far has always", "tokens": [51048, 21117, 11, 457, 309, 1985, 13, 400, 264, 3827, 281, 2740, 365, 2452, 2539, 370, 1400, 575, 1009, 51320], "temperature": 0.0, "avg_logprob": -0.08086653070135431, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.03460186347365379}, {"id": 275, "seek": 151216, "start": 1531.28, "end": 1538.88, "text": " been to use more deep learning, not less. So what upsets me about Gary Marcus argument is not that", "tokens": [51320, 668, 281, 764, 544, 2452, 2539, 11, 406, 1570, 13, 407, 437, 15497, 1385, 385, 466, 13788, 26574, 6770, 307, 406, 300, 51700], "temperature": 0.0, "avg_logprob": -0.08086653070135431, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.03460186347365379}, {"id": 276, "seek": 153888, "start": 1538.96, "end": 1544.0, "text": " I'm not sympathetic to what he's trying to push it. I'd like to build models that are more elegant,", "tokens": [50368, 286, 478, 406, 36032, 281, 437, 415, 311, 1382, 281, 2944, 309, 13, 286, 1116, 411, 281, 1322, 5245, 300, 366, 544, 21117, 11, 50620], "temperature": 0.0, "avg_logprob": -0.08079957962036133, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.0007095137261785567}, {"id": 277, "seek": 153888, "start": 1544.0, "end": 1552.16, "text": " more sparse and so on. But in the past, all these elegant sparse models have been left in the dust", "tokens": [50620, 544, 637, 11668, 293, 370, 322, 13, 583, 294, 264, 1791, 11, 439, 613, 21117, 637, 11668, 5245, 362, 668, 1411, 294, 264, 8634, 51028], "temperature": 0.0, "avg_logprob": -0.08079957962036133, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.0007095137261785567}, {"id": 278, "seek": 153888, "start": 1552.16, "end": 1557.3600000000001, "text": " by just using more deep learning. And we can also see when we zoom out a little bit that there is", "tokens": [51028, 538, 445, 1228, 544, 2452, 2539, 13, 400, 321, 393, 611, 536, 562, 321, 8863, 484, 257, 707, 857, 300, 456, 307, 51288], "temperature": 0.0, "avg_logprob": -0.08079957962036133, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.0007095137261785567}, {"id": 279, "seek": 153888, "start": 1557.3600000000001, "end": 1562.8000000000002, "text": " not an obvious limit to deep learning itself, because deep learning is not just the algorithms.", "tokens": [51288, 406, 364, 6322, 4948, 281, 2452, 2539, 2564, 11, 570, 2452, 2539, 307, 406, 445, 264, 14642, 13, 51560], "temperature": 0.0, "avg_logprob": -0.08079957962036133, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.0007095137261785567}, {"id": 280, "seek": 153888, "start": 1562.8000000000002, "end": 1567.6000000000001, "text": " Deep learning is a programming paradigm. It's differentiable programming. It basically means", "tokens": [51560, 14895, 2539, 307, 257, 9410, 24709, 13, 467, 311, 819, 9364, 9410, 13, 467, 1936, 1355, 51800], "temperature": 0.0, "avg_logprob": -0.08079957962036133, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.0007095137261785567}, {"id": 281, "seek": 156760, "start": 1567.6, "end": 1574.1599999999999, "text": " that you express everything with approximately continuous numbers, and you use algorithms that", "tokens": [50364, 300, 291, 5109, 1203, 365, 10447, 10957, 3547, 11, 293, 291, 764, 14642, 300, 50692], "temperature": 0.0, "avg_logprob": -0.13898850431536683, "compression_ratio": 1.76, "no_speech_prob": 0.0028002201579511166}, {"id": 282, "seek": 156760, "start": 1574.1599999999999, "end": 1582.1599999999999, "text": " converge business at certain ranges. And when it doesn't converge, then you just tweak it and you", "tokens": [50692, 41881, 1606, 412, 1629, 22526, 13, 400, 562, 309, 1177, 380, 41881, 11, 550, 291, 445, 29879, 309, 293, 291, 51092], "temperature": 0.0, "avg_logprob": -0.13898850431536683, "compression_ratio": 1.76, "no_speech_prob": 0.0028002201579511166}, {"id": 283, "seek": 156760, "start": 1582.1599999999999, "end": 1587.9199999999998, "text": " introduce a different architecture, which is some kind of discrete operations that you do on these", "tokens": [51092, 5366, 257, 819, 9482, 11, 597, 307, 512, 733, 295, 27706, 7705, 300, 291, 360, 322, 613, 51380], "temperature": 0.0, "avg_logprob": -0.13898850431536683, "compression_ratio": 1.76, "no_speech_prob": 0.0028002201579511166}, {"id": 284, "seek": 156760, "start": 1587.9199999999998, "end": 1592.9599999999998, "text": " continuous numbers and so on. You just patch it, you write your programs slightly differently,", "tokens": [51380, 10957, 3547, 293, 370, 322, 13, 509, 445, 9972, 309, 11, 291, 2464, 428, 4268, 4748, 7614, 11, 51632], "temperature": 0.0, "avg_logprob": -0.13898850431536683, "compression_ratio": 1.76, "no_speech_prob": 0.0028002201579511166}, {"id": 285, "seek": 156760, "start": 1592.9599999999998, "end": 1597.52, "text": " and you can automate the search for the program. And the people who do deep learning are not also", "tokens": [51632, 293, 291, 393, 31605, 264, 3164, 337, 264, 1461, 13, 400, 264, 561, 567, 360, 2452, 2539, 366, 406, 611, 51860], "temperature": 0.0, "avg_logprob": -0.13898850431536683, "compression_ratio": 1.76, "no_speech_prob": 0.0028002201579511166}, {"id": 286, "seek": 159752, "start": 1597.52, "end": 1602.08, "text": " docs in the sense that they say, oh my God, symbolic structures are not allowed. I cannot use", "tokens": [50364, 45623, 294, 264, 2020, 300, 436, 584, 11, 1954, 452, 1265, 11, 25755, 9227, 366, 406, 4350, 13, 286, 2644, 764, 50592], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 287, "seek": 159752, "start": 1602.08, "end": 1607.6, "text": " a Python script in here rather than just a TensorFlow. This is not what's happening. It's", "tokens": [50592, 257, 15329, 5755, 294, 510, 2831, 813, 445, 257, 37624, 13, 639, 307, 406, 437, 311, 2737, 13, 467, 311, 50868], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 288, "seek": 159752, "start": 1607.6, "end": 1612.4, "text": " also not that they are constrained to any kind of thing that will use whatever is working.", "tokens": [50868, 611, 406, 300, 436, 366, 38901, 281, 604, 733, 295, 551, 300, 486, 764, 2035, 307, 1364, 13, 51108], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 289, "seek": 159752, "start": 1612.4, "end": 1616.96, "text": " And what we see is that the end-to-end train systems are going more and more powerful,", "tokens": [51108, 400, 437, 321, 536, 307, 300, 264, 917, 12, 1353, 12, 521, 3847, 3652, 366, 516, 544, 293, 544, 4005, 11, 51336], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 290, "seek": 159752, "start": 1616.96, "end": 1621.2, "text": " and rather than sitting there by hand and tinkering and finding a solution,", "tokens": [51336, 293, 2831, 813, 3798, 456, 538, 1011, 293, 256, 475, 1794, 293, 5006, 257, 3827, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 291, "seek": 159752, "start": 1621.84, "end": 1626.72, "text": " we can just use a system that is tinkering automatically through a dramatically larger", "tokens": [51580, 321, 393, 445, 764, 257, 1185, 300, 307, 256, 475, 1794, 6772, 807, 257, 17548, 4833, 51824], "temperature": 0.0, "avg_logprob": -0.10441489446730841, "compression_ratio": 1.7643097643097643, "no_speech_prob": 0.006686200387775898}, {"id": 292, "seek": 162672, "start": 1626.72, "end": 1632.72, "text": " space than we would ever be able to explore by trying all sorts of algorithms. So when we look", "tokens": [50364, 1901, 813, 321, 576, 1562, 312, 1075, 281, 6839, 538, 1382, 439, 7527, 295, 14642, 13, 407, 562, 321, 574, 50664], "temperature": 0.0, "avg_logprob": -0.15186810289692676, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.0059094806201756}, {"id": 293, "seek": 162672, "start": 1632.72, "end": 1638.48, "text": " at Gary Marcus' articles like his deep learning is hitting a wall and so on, and you look what he's", "tokens": [50664, 412, 13788, 26574, 6, 11290, 411, 702, 2452, 2539, 307, 8850, 257, 2929, 293, 370, 322, 11, 293, 291, 574, 437, 415, 311, 50952], "temperature": 0.0, "avg_logprob": -0.15186810289692676, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.0059094806201756}, {"id": 294, "seek": 162672, "start": 1638.48, "end": 1642.88, "text": " actually giving as arguments, the arguments are not very good. He gives us an example,", "tokens": [50952, 767, 2902, 382, 12869, 11, 264, 12869, 366, 406, 588, 665, 13, 634, 2709, 505, 364, 1365, 11, 51172], "temperature": 0.0, "avg_logprob": -0.15186810289692676, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.0059094806201756}, {"id": 295, "seek": 162672, "start": 1642.88, "end": 1649.6000000000001, "text": " the NetHack challenge. NetHack is a game which has a very large horizon because you basically", "tokens": [51172, 264, 6188, 39, 501, 3430, 13, 6188, 39, 501, 307, 257, 1216, 597, 575, 257, 588, 2416, 18046, 570, 291, 1936, 51508], "temperature": 0.0, "avg_logprob": -0.15186810289692676, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.0059094806201756}, {"id": 296, "seek": 162672, "start": 1649.6000000000001, "end": 1655.1200000000001, "text": " have only one life. You need to explore a very deep labyrinth and you need to plan pretty far ahead", "tokens": [51508, 362, 787, 472, 993, 13, 509, 643, 281, 6839, 257, 588, 2452, 287, 46800, 392, 293, 291, 643, 281, 1393, 1238, 1400, 2286, 51784], "temperature": 0.0, "avg_logprob": -0.15186810289692676, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.0059094806201756}, {"id": 297, "seek": 165512, "start": 1655.12, "end": 1661.12, "text": " with what you're doing. And so it's something that is difficult to discover this right solution", "tokens": [50364, 365, 437, 291, 434, 884, 13, 400, 370, 309, 311, 746, 300, 307, 2252, 281, 4411, 341, 558, 3827, 50664], "temperature": 0.0, "avg_logprob": -0.0834155913886674, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.003819506848230958}, {"id": 298, "seek": 165512, "start": 1661.12, "end": 1667.6799999999998, "text": " with a deep learning model that has no prior ideas about what it's doing. Because it takes us", "tokens": [50664, 365, 257, 2452, 2539, 2316, 300, 575, 572, 4059, 3487, 466, 437, 309, 311, 884, 13, 1436, 309, 2516, 505, 50992], "temperature": 0.0, "avg_logprob": -0.0834155913886674, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.003819506848230958}, {"id": 299, "seek": 165512, "start": 1667.6799999999998, "end": 1672.0, "text": " very, very long until you get the necessary feedback to learn about your actions. And people", "tokens": [50992, 588, 11, 588, 938, 1826, 291, 483, 264, 4818, 5824, 281, 1466, 466, 428, 5909, 13, 400, 561, 51208], "temperature": 0.0, "avg_logprob": -0.0834155913886674, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.003819506848230958}, {"id": 300, "seek": 165512, "start": 1672.0, "end": 1676.4799999999998, "text": " are relatively good at learning this because they have so many ideas about what the situation is that", "tokens": [51208, 366, 7226, 665, 412, 2539, 341, 570, 436, 362, 370, 867, 3487, 466, 437, 264, 2590, 307, 300, 51432], "temperature": 0.0, "avg_logprob": -0.0834155913886674, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.003819506848230958}, {"id": 301, "seek": 165512, "start": 1676.4799999999998, "end": 1681.6799999999998, "text": " they're in. There's so many priors from our world interaction and from other games that we have played", "tokens": [51432, 436, 434, 294, 13, 821, 311, 370, 867, 1790, 830, 490, 527, 1002, 9285, 293, 490, 661, 2813, 300, 321, 362, 3737, 51692], "temperature": 0.0, "avg_logprob": -0.0834155913886674, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.003819506848230958}, {"id": 302, "seek": 168168, "start": 1681.68, "end": 1686.96, "text": " that we can bring to the tasks. So the current winner of this is the symbolic solution.", "tokens": [50364, 300, 321, 393, 1565, 281, 264, 9608, 13, 407, 264, 2190, 8507, 295, 341, 307, 264, 25755, 3827, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1312082307832735, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.027917388826608658}, {"id": 303, "seek": 168168, "start": 1688.0800000000002, "end": 1694.48, "text": " And the symbolic solution that Gary Marcus gives as a proof that symbolic methods are still ahead", "tokens": [50684, 400, 264, 25755, 3827, 300, 13788, 26574, 2709, 382, 257, 8177, 300, 25755, 7150, 366, 920, 2286, 51004], "temperature": 0.0, "avg_logprob": -0.1312082307832735, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.027917388826608658}, {"id": 304, "seek": 168168, "start": 1694.48, "end": 1700.3200000000002, "text": " of deep learning things. In a single case, not like he has a big array of tasks where they are", "tokens": [51004, 295, 2452, 2539, 721, 13, 682, 257, 2167, 1389, 11, 406, 411, 415, 575, 257, 955, 10225, 295, 9608, 689, 436, 366, 51296], "temperature": 0.0, "avg_logprob": -0.1312082307832735, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.027917388826608658}, {"id": 305, "seek": 168168, "start": 1700.3200000000002, "end": 1706.0, "text": " superior, it's just two students who have written a program that is made of lots and lots of events.", "tokens": [51296, 13028, 11, 309, 311, 445, 732, 1731, 567, 362, 3720, 257, 1461, 300, 307, 1027, 295, 3195, 293, 3195, 295, 3931, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1312082307832735, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.027917388826608658}, {"id": 306, "seek": 168168, "start": 1706.0, "end": 1710.96, "text": " This is just a big hack. This is not some symbolic learning algorithm that does something novel,", "tokens": [51580, 639, 307, 445, 257, 955, 10339, 13, 639, 307, 406, 512, 25755, 2539, 9284, 300, 775, 746, 7613, 11, 51828], "temperature": 0.0, "avg_logprob": -0.1312082307832735, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.027917388826608658}, {"id": 307, "seek": 171096, "start": 1710.96, "end": 1716.88, "text": " hybrid or whatever. No, this is just a script. And is Gary Marcus seriously proposing, oh my", "tokens": [50364, 13051, 420, 2035, 13, 883, 11, 341, 307, 445, 257, 5755, 13, 400, 307, 13788, 26574, 6638, 29939, 11, 1954, 452, 50660], "temperature": 0.0, "avg_logprob": -0.1940982012244744, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.011678915470838547}, {"id": 308, "seek": 171096, "start": 1716.88, "end": 1720.88, "text": " god, deep learning models are limited and we need to replace them with more scripts?", "tokens": [50660, 3044, 11, 2452, 2539, 5245, 366, 5567, 293, 321, 643, 281, 7406, 552, 365, 544, 23294, 30, 50860], "temperature": 0.0, "avg_logprob": -0.1940982012244744, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.011678915470838547}, {"id": 309, "seek": 171096, "start": 1721.76, "end": 1729.04, "text": " This is not a good argument. Yeah. So I think maybe, and look, I get that there are these kind", "tokens": [50904, 639, 307, 406, 257, 665, 6770, 13, 865, 13, 407, 286, 519, 1310, 11, 293, 574, 11, 286, 483, 300, 456, 366, 613, 733, 51268], "temperature": 0.0, "avg_logprob": -0.1940982012244744, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.011678915470838547}, {"id": 310, "seek": 171096, "start": 1729.04, "end": 1734.08, "text": " of two competing camps and they maybe go after each other with some. No, they don't. This is only on", "tokens": [51268, 295, 732, 15439, 16573, 293, 436, 1310, 352, 934, 1184, 661, 365, 512, 13, 883, 11, 436, 500, 380, 13, 639, 307, 787, 322, 51520], "temperature": 0.0, "avg_logprob": -0.1940982012244744, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.011678915470838547}, {"id": 311, "seek": 171096, "start": 1734.08, "end": 1739.76, "text": " Twitter. There is, there are no competing camps. It's Yandekun is not also docs in the sense that", "tokens": [51520, 5794, 13, 821, 307, 11, 456, 366, 572, 15439, 16573, 13, 467, 311, 398, 474, 916, 409, 307, 406, 611, 45623, 294, 264, 2020, 300, 51804], "temperature": 0.0, "avg_logprob": -0.1940982012244744, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.011678915470838547}, {"id": 312, "seek": 173976, "start": 1739.76, "end": 1743.44, "text": " he believed you need to use this argument, all the other arguments are impure and flawed.", "tokens": [50364, 415, 7847, 291, 643, 281, 764, 341, 6770, 11, 439, 264, 661, 12869, 366, 704, 540, 293, 38823, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1662378229646601, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0026719083543866873}, {"id": 313, "seek": 173976, "start": 1744.8799999999999, "end": 1750.4, "text": " His brand is to build systems that work. And if one of his people comes up with something that", "tokens": [50620, 2812, 3360, 307, 281, 1322, 3652, 300, 589, 13, 400, 498, 472, 295, 702, 561, 1487, 493, 365, 746, 300, 50896], "temperature": 0.0, "avg_logprob": -0.1662378229646601, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0026719083543866873}, {"id": 314, "seek": 173976, "start": 1750.4, "end": 1755.68, "text": " works better than what he came up with, you probably praise him for that and let him go on.", "tokens": [50896, 1985, 1101, 813, 437, 415, 1361, 493, 365, 11, 291, 1391, 13286, 796, 337, 300, 293, 718, 796, 352, 322, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1662378229646601, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0026719083543866873}, {"id": 315, "seek": 173976, "start": 1757.12, "end": 1764.08, "text": " Yeah, sure. But there's absolutely, however, there is, you know, there is, let's say, momentum", "tokens": [51232, 865, 11, 988, 13, 583, 456, 311, 3122, 11, 4461, 11, 456, 307, 11, 291, 458, 11, 456, 307, 11, 718, 311, 584, 11, 11244, 51580], "temperature": 0.0, "avg_logprob": -0.1662378229646601, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0026719083543866873}, {"id": 316, "seek": 173976, "start": 1764.08, "end": 1769.28, "text": " and hardware lotteries and paradigms that kind of reinforce themselves. And to an extent,", "tokens": [51580, 293, 8837, 688, 391, 530, 293, 13480, 328, 2592, 300, 733, 295, 22634, 2969, 13, 400, 281, 364, 8396, 11, 51840], "temperature": 0.0, "avg_logprob": -0.1662378229646601, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.0026719083543866873}, {"id": 317, "seek": 176928, "start": 1769.36, "end": 1774.3999999999999, "text": " they can strangle off, you know, resources that maybe we like, we shouldn't be investing all", "tokens": [50368, 436, 393, 1056, 7846, 766, 11, 291, 458, 11, 3593, 300, 1310, 321, 411, 11, 321, 4659, 380, 312, 10978, 439, 50620], "temperature": 0.0, "avg_logprob": -0.0950976503573782, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.0038240207359194756}, {"id": 318, "seek": 176928, "start": 1774.3999999999999, "end": 1780.6399999999999, "text": " our eggs in one basket. We shouldn't be pouring, you know, the 99% of research funding necessarily", "tokens": [50620, 527, 6466, 294, 472, 8390, 13, 492, 4659, 380, 312, 20450, 11, 291, 458, 11, 264, 11803, 4, 295, 2132, 6137, 4725, 50932], "temperature": 0.0, "avg_logprob": -0.0950976503573782, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.0038240207359194756}, {"id": 319, "seek": 176928, "start": 1780.6399999999999, "end": 1786.24, "text": " down, down deep learning. And I think that's kind of the problem that, that these paradigms cause.", "tokens": [50932, 760, 11, 760, 2452, 2539, 13, 400, 286, 519, 300, 311, 733, 295, 264, 1154, 300, 11, 300, 613, 13480, 328, 2592, 3082, 13, 51212], "temperature": 0.0, "avg_logprob": -0.0950976503573782, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.0038240207359194756}, {"id": 320, "seek": 176928, "start": 1786.24, "end": 1792.0, "text": " But I want to get back to something you said, which is a good point. It's, I think that's an", "tokens": [51212, 583, 286, 528, 281, 483, 646, 281, 746, 291, 848, 11, 597, 307, 257, 665, 935, 13, 467, 311, 11, 286, 519, 300, 311, 364, 51500], "temperature": 0.0, "avg_logprob": -0.0950976503573782, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.0038240207359194756}, {"id": 321, "seek": 176928, "start": 1792.0, "end": 1796.8, "text": " important point. I think that in absolute terms, the other approaches get more money than they did", "tokens": [51500, 1021, 935, 13, 286, 519, 300, 294, 8236, 2115, 11, 264, 661, 11587, 483, 544, 1460, 813, 436, 630, 51740], "temperature": 0.0, "avg_logprob": -0.0950976503573782, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.0038240207359194756}, {"id": 322, "seek": 179680, "start": 1796.8, "end": 1802.24, "text": " before. It's not that we have a funding stop, as we had at some point, a return funding stop for", "tokens": [50364, 949, 13, 467, 311, 406, 300, 321, 362, 257, 6137, 1590, 11, 382, 321, 632, 412, 512, 935, 11, 257, 2736, 6137, 1590, 337, 50636], "temperature": 0.0, "avg_logprob": -0.18232211303710938, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.037864647805690765}, {"id": 323, "seek": 179680, "start": 1802.24, "end": 1807.28, "text": " Neural Networks. And Marvin Minsky wrote a book where he saw he had proven that the Neural Networks", "tokens": [50636, 1734, 1807, 12640, 82, 13, 400, 48722, 376, 44153, 4114, 257, 1446, 689, 415, 1866, 415, 632, 12785, 300, 264, 1734, 1807, 12640, 82, 50888], "temperature": 0.0, "avg_logprob": -0.18232211303710938, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.037864647805690765}, {"id": 324, "seek": 179680, "start": 1807.28, "end": 1812.3999999999999, "text": " cannot converge over multiple layers, press up drones cannot earn X or and so on. Right. Minsky", "tokens": [50888, 2644, 41881, 670, 3866, 7914, 11, 1886, 493, 23823, 2644, 6012, 1783, 420, 293, 370, 322, 13, 1779, 13, 376, 44153, 51144], "temperature": 0.0, "avg_logprob": -0.18232211303710938, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.037864647805690765}, {"id": 325, "seek": 179680, "start": 1812.3999999999999, "end": 1817.44, "text": " was wrong. People found a way around this. But at this time, there was so little funding that", "tokens": [51144, 390, 2085, 13, 3432, 1352, 257, 636, 926, 341, 13, 583, 412, 341, 565, 11, 456, 390, 370, 707, 6137, 300, 51396], "temperature": 0.0, "avg_logprob": -0.18232211303710938, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.037864647805690765}, {"id": 326, "seek": 179680, "start": 1817.44, "end": 1823.2, "text": " this cutoff mattered. And at the moment, if you want to do something that has AI and Adline,", "tokens": [51396, 341, 1723, 4506, 44282, 13, 400, 412, 264, 1623, 11, 498, 291, 528, 281, 360, 746, 300, 575, 7318, 293, 1999, 1889, 11, 51684], "temperature": 0.0, "avg_logprob": -0.18232211303710938, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.037864647805690765}, {"id": 327, "seek": 182320, "start": 1823.2, "end": 1827.44, "text": " the chance that you get it funded and whether what paradigm you're doing is greater than ever.", "tokens": [50364, 264, 2931, 300, 291, 483, 309, 14385, 293, 1968, 437, 24709, 291, 434, 884, 307, 5044, 813, 1562, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 328, "seek": 182320, "start": 1827.44, "end": 1831.76, "text": " So the absolute amount of funds that goes into any kind of paradigm that you want to work on", "tokens": [50576, 407, 264, 8236, 2372, 295, 8271, 300, 1709, 666, 604, 733, 295, 24709, 300, 291, 528, 281, 589, 322, 50792], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 329, "seek": 182320, "start": 1831.76, "end": 1836.96, "text": " is greater than ever. And the reason why the majority of funds goes into very few paradigms is", "tokens": [50792, 307, 5044, 813, 1562, 13, 400, 264, 1778, 983, 264, 6286, 295, 8271, 1709, 666, 588, 1326, 13480, 328, 2592, 307, 51052], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 330, "seek": 182320, "start": 1836.96, "end": 1842.0800000000002, "text": " because these are the things that work in industrial applications. There is no other", "tokens": [51052, 570, 613, 366, 264, 721, 300, 589, 294, 9987, 5821, 13, 821, 307, 572, 661, 51308], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 331, "seek": 182320, "start": 1842.0800000000002, "end": 1846.24, "text": " algorithm that is able to learn from scratch how to translate between arbitrary languages and", "tokens": [51308, 9284, 300, 307, 1075, 281, 1466, 490, 8459, 577, 281, 13799, 1296, 23211, 8650, 293, 51516], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 332, "seek": 182320, "start": 1846.24, "end": 1851.6000000000001, "text": " generate stories and draw pre pictures for you. This is the only game in town at the moment,", "tokens": [51516, 8460, 3676, 293, 2642, 659, 5242, 337, 291, 13, 639, 307, 264, 787, 1216, 294, 3954, 412, 264, 1623, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08550714477290952, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.004263186827301979}, {"id": 333, "seek": 185160, "start": 1851.6, "end": 1856.32, "text": " the only class of algorithm that converges over all these many domains. And people are looking", "tokens": [50364, 264, 787, 1508, 295, 9284, 300, 9652, 2880, 670, 439, 613, 867, 25514, 13, 400, 561, 366, 1237, 50600], "temperature": 0.0, "avg_logprob": -0.10321595846128857, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.005548943765461445}, {"id": 334, "seek": 185160, "start": 1856.32, "end": 1860.8, "text": " for better alternatives. And yes, we are in a bubble, because of course, they're looking mostly", "tokens": [50600, 337, 1101, 20478, 13, 400, 2086, 11, 321, 366, 294, 257, 12212, 11, 570, 295, 1164, 11, 436, 434, 1237, 5240, 50824], "temperature": 0.0, "avg_logprob": -0.10321595846128857, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.005548943765461445}, {"id": 335, "seek": 185160, "start": 1860.8, "end": 1865.52, "text": " where things already were. You have hardware that works, you have libraries that work and so on.", "tokens": [50824, 689, 721, 1217, 645, 13, 509, 362, 8837, 300, 1985, 11, 291, 362, 15148, 300, 589, 293, 370, 322, 13, 51060], "temperature": 0.0, "avg_logprob": -0.10321595846128857, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.005548943765461445}, {"id": 336, "seek": 185160, "start": 1865.52, "end": 1870.48, "text": " It's hard to get out of that bubble. That is true. And it's always good to push for alternatives", "tokens": [51060, 467, 311, 1152, 281, 483, 484, 295, 300, 12212, 13, 663, 307, 2074, 13, 400, 309, 311, 1009, 665, 281, 2944, 337, 20478, 51308], "temperature": 0.0, "avg_logprob": -0.10321595846128857, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.005548943765461445}, {"id": 337, "seek": 185160, "start": 1870.48, "end": 1877.4399999999998, "text": " and so on. But I don't think that we should be in a panic and say, Oh, my God, there is something", "tokens": [51308, 293, 370, 322, 13, 583, 286, 500, 380, 519, 300, 321, 820, 312, 294, 257, 14783, 293, 584, 11, 876, 11, 452, 1265, 11, 456, 307, 746, 51656], "temperature": 0.0, "avg_logprob": -0.10321595846128857, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.005548943765461445}, {"id": 338, "seek": 187744, "start": 1877.44, "end": 1883.44, "text": " politically wrong. I suspect that by and large, the forces of the markets and the forces of the", "tokens": [50364, 21154, 2085, 13, 286, 9091, 300, 538, 293, 2416, 11, 264, 5874, 295, 264, 8383, 293, 264, 5874, 295, 264, 50664], "temperature": 0.0, "avg_logprob": -0.0857477356902266, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.03511713445186615}, {"id": 339, "seek": 187744, "start": 1883.44, "end": 1888.3200000000002, "text": " academic researchers that want to explore alternative are pushing in the right direction already.", "tokens": [50664, 7778, 10309, 300, 528, 281, 6839, 8535, 366, 7380, 294, 264, 558, 3513, 1217, 13, 50908], "temperature": 0.0, "avg_logprob": -0.0857477356902266, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.03511713445186615}, {"id": 340, "seek": 187744, "start": 1889.8400000000001, "end": 1894.16, "text": " Yeah, I mean, fair enough. And, you know, you could be right. And there may not be that much", "tokens": [50984, 865, 11, 286, 914, 11, 3143, 1547, 13, 400, 11, 291, 458, 11, 291, 727, 312, 558, 13, 400, 456, 815, 406, 312, 300, 709, 51200], "temperature": 0.0, "avg_logprob": -0.0857477356902266, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.03511713445186615}, {"id": 341, "seek": 187744, "start": 1894.16, "end": 1900.88, "text": " of an imbalance. But I want to get back to one technical thing you said. Yes, it seems apparent", "tokens": [51200, 295, 364, 43007, 13, 583, 286, 528, 281, 483, 646, 281, 472, 6191, 551, 291, 848, 13, 1079, 11, 309, 2544, 18335, 51536], "temperature": 0.0, "avg_logprob": -0.0857477356902266, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.03511713445186615}, {"id": 342, "seek": 187744, "start": 1900.88, "end": 1906.3200000000002, "text": " that, let's say, what deep learning is doing is this this differentiable program search,", "tokens": [51536, 300, 11, 718, 311, 584, 11, 437, 2452, 2539, 307, 884, 307, 341, 341, 819, 9364, 1461, 3164, 11, 51808], "temperature": 0.0, "avg_logprob": -0.0857477356902266, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.03511713445186615}, {"id": 343, "seek": 190632, "start": 1906.3999999999999, "end": 1911.76, "text": " if you will. And a question I have about that is if we imagine the space of all possible programs,", "tokens": [50368, 498, 291, 486, 13, 400, 257, 1168, 286, 362, 466, 300, 307, 498, 321, 3811, 264, 1901, 295, 439, 1944, 4268, 11, 50636], "temperature": 0.0, "avg_logprob": -0.10183413523548054, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00043052606633864343}, {"id": 344, "seek": 190632, "start": 1912.8, "end": 1917.76, "text": " that, you know, requiring that we're doing a differentiable search is certainly going to skew", "tokens": [50688, 300, 11, 291, 458, 11, 24165, 300, 321, 434, 884, 257, 819, 9364, 3164, 307, 3297, 516, 281, 8756, 86, 50936], "temperature": 0.0, "avg_logprob": -0.10183413523548054, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00043052606633864343}, {"id": 345, "seek": 190632, "start": 1917.76, "end": 1925.36, "text": " that sample space that may even cut off programs in that space that can't be discovered easily by", "tokens": [50936, 300, 6889, 1901, 300, 815, 754, 1723, 766, 4268, 294, 300, 1901, 300, 393, 380, 312, 6941, 3612, 538, 51316], "temperature": 0.0, "avg_logprob": -0.10183413523548054, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00043052606633864343}, {"id": 346, "seek": 190632, "start": 1925.36, "end": 1930.08, "text": " differentiable search. So I'm wondering, doesn't that leave open the possibility that other", "tokens": [51316, 819, 9364, 3164, 13, 407, 286, 478, 6359, 11, 1177, 380, 300, 1856, 1269, 264, 7959, 300, 661, 51552], "temperature": 0.0, "avg_logprob": -0.10183413523548054, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00043052606633864343}, {"id": 347, "seek": 190632, "start": 1930.08, "end": 1935.6799999999998, "text": " algorithms that are more discreet in nature, say evolutionary algorithms or discrete program", "tokens": [51552, 14642, 300, 366, 544, 2983, 4751, 294, 3687, 11, 584, 27567, 14642, 420, 27706, 1461, 51832], "temperature": 0.0, "avg_logprob": -0.10183413523548054, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00043052606633864343}, {"id": 348, "seek": 193568, "start": 1935.68, "end": 1941.6000000000001, "text": " search or whatever, they may have access to a different subspace of the space of all programs", "tokens": [50364, 3164, 420, 2035, 11, 436, 815, 362, 2105, 281, 257, 819, 2090, 17940, 295, 264, 1901, 295, 439, 4268, 50660], "temperature": 0.0, "avg_logprob": -0.10938038060694565, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.0003301090036984533}, {"id": 349, "seek": 193568, "start": 1941.6000000000001, "end": 1946.0, "text": " that aren't easily accessible by differentiable paradigms. Is that true?", "tokens": [50660, 300, 3212, 380, 3612, 9515, 538, 819, 9364, 13480, 328, 2592, 13, 1119, 300, 2074, 30, 50880], "temperature": 0.0, "avg_logprob": -0.10938038060694565, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.0003301090036984533}, {"id": 350, "seek": 193568, "start": 1948.72, "end": 1952.96, "text": " The question is, how do you find it? How do you find these algorithms to manipulate the", "tokens": [51016, 440, 1168, 307, 11, 577, 360, 291, 915, 309, 30, 1012, 360, 291, 915, 613, 14642, 281, 20459, 264, 51228], "temperature": 0.0, "avg_logprob": -0.10938038060694565, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.0003301090036984533}, {"id": 351, "seek": 193568, "start": 1952.96, "end": 1958.88, "text": " discrete things? I agree that when you have a perceptual model that is modeling everything", "tokens": [51228, 27706, 721, 30, 286, 3986, 300, 562, 291, 362, 257, 43276, 901, 2316, 300, 307, 15983, 1203, 51524], "temperature": 0.0, "avg_logprob": -0.10938038060694565, "compression_ratio": 1.6350710900473933, "no_speech_prob": 0.0003301090036984533}, {"id": 352, "seek": 195888, "start": 1959.5200000000002, "end": 1966.5600000000002, "text": " with chains or sums over real numbers, and a few non-algebraic throne, and you get", "tokens": [50396, 365, 12626, 420, 34499, 670, 957, 3547, 11, 293, 257, 1326, 2107, 12, 304, 19983, 299, 17678, 11, 293, 291, 483, 50748], "temperature": 0.0, "avg_logprob": -0.1867297876661069, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.037870850414037704}, {"id": 353, "seek": 195888, "start": 1966.5600000000002, "end": 1970.3200000000002, "text": " characteristic artifacts. For instance, in the generative models, you often have the problem,", "tokens": [50748, 16282, 24617, 13, 1171, 5197, 11, 294, 264, 1337, 1166, 5245, 11, 291, 2049, 362, 264, 1154, 11, 50936], "temperature": 0.0, "avg_logprob": -0.1867297876661069, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.037870850414037704}, {"id": 354, "seek": 195888, "start": 1970.3200000000002, "end": 1975.2800000000002, "text": " and you try to model a person with glasses or without glasses, that because the model thinks", "tokens": [50936, 293, 291, 853, 281, 2316, 257, 954, 365, 10812, 420, 1553, 10812, 11, 300, 570, 264, 2316, 7309, 51184], "temperature": 0.0, "avg_logprob": -0.1867297876661069, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.037870850414037704}, {"id": 355, "seek": 195888, "start": 1975.2800000000002, "end": 1980.24, "text": " that these features are somewhat continuous, you often run into the situation that you get areas", "tokens": [51184, 300, 613, 4122, 366, 8344, 10957, 11, 291, 2049, 1190, 666, 264, 2590, 300, 291, 483, 3179, 51432], "temperature": 0.0, "avg_logprob": -0.1867297876661069, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.037870850414037704}, {"id": 356, "seek": 195888, "start": 1980.24, "end": 1985.0400000000002, "text": " in the generative model, where the glasses are half materialized, and it looks always very weird.", "tokens": [51432, 294, 264, 1337, 1166, 2316, 11, 689, 264, 10812, 366, 1922, 2527, 1602, 11, 293, 309, 1542, 1009, 588, 3657, 13, 51672], "temperature": 0.0, "avg_logprob": -0.1867297876661069, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.037870850414037704}, {"id": 357, "seek": 198504, "start": 1985.04, "end": 1991.2, "text": " And you have these strange things where reality has a discontinuity, but your model has permissible", "tokens": [50364, 400, 291, 362, 613, 5861, 721, 689, 4103, 575, 257, 31420, 21757, 11, 457, 428, 2316, 575, 4784, 41073, 50672], "temperature": 0.0, "avg_logprob": -0.11348859786987305, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.0015008143382146955}, {"id": 358, "seek": 198504, "start": 1991.2, "end": 1997.12, "text": " states where you are in the middle of the discontinuity, and you try to generate something", "tokens": [50672, 4368, 689, 291, 366, 294, 264, 2808, 295, 264, 31420, 21757, 11, 293, 291, 853, 281, 8460, 746, 50968], "temperature": 0.0, "avg_logprob": -0.11348859786987305, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.0015008143382146955}, {"id": 359, "seek": 198504, "start": 1997.12, "end": 2001.2, "text": " that cannot exist. You want your model to be structured such a way ideally that every", "tokens": [50968, 300, 2644, 2514, 13, 509, 528, 428, 2316, 281, 312, 18519, 1270, 257, 636, 22915, 300, 633, 51172], "temperature": 0.0, "avg_logprob": -0.11348859786987305, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.0015008143382146955}, {"id": 360, "seek": 198504, "start": 2001.2, "end": 2007.44, "text": " model configuration corresponds to a world configuration. And this is not necessarily the", "tokens": [51172, 2316, 11694, 23249, 281, 257, 1002, 11694, 13, 400, 341, 307, 406, 4725, 264, 51484], "temperature": 0.0, "avg_logprob": -0.11348859786987305, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.0015008143382146955}, {"id": 361, "seek": 198504, "start": 2007.44, "end": 2012.08, "text": " case with many of the deep learning models. And what the deep learning models, as you train them", "tokens": [51484, 1389, 365, 867, 295, 264, 2452, 2539, 5245, 13, 400, 437, 264, 2452, 2539, 5245, 11, 382, 291, 3847, 552, 51716], "temperature": 0.0, "avg_logprob": -0.11348859786987305, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.0015008143382146955}, {"id": 362, "seek": 201208, "start": 2012.08, "end": 2017.36, "text": " harder, typically tend to do is that they squeeze these impermeasurable areas until you are very", "tokens": [50364, 6081, 11, 5850, 3928, 281, 360, 307, 300, 436, 13578, 613, 10100, 1398, 296, 25863, 3179, 1826, 291, 366, 588, 50628], "temperature": 0.0, "avg_logprob": -0.08156320878437587, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.00818418338894844}, {"id": 363, "seek": 201208, "start": 2017.36, "end": 2022.3999999999999, "text": " unlikely to end up in them. And it's probably possible to get them to implement filters and", "tokens": [50628, 17518, 281, 917, 493, 294, 552, 13, 400, 309, 311, 1391, 1944, 281, 483, 552, 281, 4445, 15995, 293, 50880], "temperature": 0.0, "avg_logprob": -0.08156320878437587, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.00818418338894844}, {"id": 364, "seek": 201208, "start": 2022.3999999999999, "end": 2028.08, "text": " all sorts of tricks. But what you can also do is you can combine this with some kind of discrete", "tokens": [50880, 439, 7527, 295, 11733, 13, 583, 437, 291, 393, 611, 360, 307, 291, 393, 10432, 341, 365, 512, 733, 295, 27706, 51164], "temperature": 0.0, "avg_logprob": -0.08156320878437587, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.00818418338894844}, {"id": 365, "seek": 201208, "start": 2028.08, "end": 2033.6, "text": " machine. And then what you do is you learn how to use this. So this deep learning network is not", "tokens": [51164, 3479, 13, 400, 550, 437, 291, 360, 307, 291, 1466, 577, 281, 764, 341, 13, 407, 341, 2452, 2539, 3209, 307, 406, 51440], "temperature": 0.0, "avg_logprob": -0.08156320878437587, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.00818418338894844}, {"id": 366, "seek": 201208, "start": 2034.1599999999999, "end": 2039.28, "text": " interacting with the world directly, but it learns how to use an architecture that does that.", "tokens": [51468, 18017, 365, 264, 1002, 3838, 11, 457, 309, 27152, 577, 281, 764, 364, 9482, 300, 775, 300, 13, 51724], "temperature": 0.0, "avg_logprob": -0.08156320878437587, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.00818418338894844}, {"id": 367, "seek": 203928, "start": 2040.08, "end": 2044.6399999999999, "text": " So for instance, instead of training a neural network to do numerical calculations,", "tokens": [50404, 407, 337, 5197, 11, 2602, 295, 3097, 257, 18161, 3209, 281, 360, 29054, 20448, 11, 50632], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 368, "seek": 203928, "start": 2044.6399999999999, "end": 2050.16, "text": " you can train it to use a numerical calculator. And in this way, it can become very sparse again.", "tokens": [50632, 291, 393, 3847, 309, 281, 764, 257, 29054, 24993, 13, 400, 294, 341, 636, 11, 309, 393, 1813, 588, 637, 11668, 797, 13, 50908], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 369, "seek": 203928, "start": 2050.72, "end": 2056.08, "text": " Right? So there's not an obvious limit to that I can see where I can prove to the deep learning", "tokens": [50936, 1779, 30, 407, 456, 311, 406, 364, 6322, 4948, 281, 300, 286, 393, 536, 689, 286, 393, 7081, 281, 264, 2452, 2539, 51204], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 370, "seek": 203928, "start": 2056.08, "end": 2060.08, "text": " people, oh, here's where you should stop deep learning, because they can just combine their", "tokens": [51204, 561, 11, 1954, 11, 510, 311, 689, 291, 820, 1590, 2452, 2539, 11, 570, 436, 393, 445, 10432, 641, 51404], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 371, "seek": 203928, "start": 2060.08, "end": 2064.56, "text": " deep learning approach with other approaches and use the deep learning system to remote control", "tokens": [51404, 2452, 2539, 3109, 365, 661, 11587, 293, 764, 264, 2452, 2539, 1185, 281, 8607, 1969, 51628], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 372, "seek": 203928, "start": 2064.56, "end": 2068.4, "text": " this. And it turns out when we reason and so on, even when we do discrete reasoning,", "tokens": [51628, 341, 13, 400, 309, 4523, 484, 562, 321, 1778, 293, 370, 322, 11, 754, 562, 321, 360, 27706, 21577, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10778021067380905, "compression_ratio": 1.8581081081081081, "no_speech_prob": 0.003171556629240513}, {"id": 373, "seek": 206840, "start": 2068.4, "end": 2074.0, "text": " that the steps that we assemble it to each other are heuristics that require some kind", "tokens": [50364, 300, 264, 4439, 300, 321, 22364, 309, 281, 1184, 661, 366, 415, 374, 6006, 300, 3651, 512, 733, 50644], "temperature": 0.0, "avg_logprob": -0.1306198652800139, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0014061041874811053}, {"id": 374, "seek": 206840, "start": 2074.0, "end": 2078.7200000000003, "text": " of probabilistic element. Right? So when we form a sort that when the sort is made of very", "tokens": [50644, 295, 31959, 3142, 4478, 13, 1779, 30, 407, 562, 321, 1254, 257, 1333, 300, 562, 264, 1333, 307, 1027, 295, 588, 50880], "temperature": 0.0, "avg_logprob": -0.1306198652800139, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0014061041874811053}, {"id": 375, "seek": 206840, "start": 2078.7200000000003, "end": 2084.64, "text": " discrete elements, the search for that sort is some kind of deep learning process that is happening.", "tokens": [50880, 27706, 4959, 11, 264, 3164, 337, 300, 1333, 307, 512, 733, 295, 2452, 2539, 1399, 300, 307, 2737, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1306198652800139, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0014061041874811053}, {"id": 376, "seek": 206840, "start": 2084.64, "end": 2091.76, "text": " Right? And when we make the pool, we do this, we emulate a discrete reasoning. But of course,", "tokens": [51176, 1779, 30, 400, 562, 321, 652, 264, 7005, 11, 321, 360, 341, 11, 321, 45497, 257, 27706, 21577, 13, 583, 295, 1164, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1306198652800139, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0014061041874811053}, {"id": 377, "seek": 206840, "start": 2091.76, "end": 2096.88, "text": " we can combine this and we can get the neural network to learn how to perform the discrete", "tokens": [51532, 321, 393, 10432, 341, 293, 321, 393, 483, 264, 18161, 3209, 281, 1466, 577, 281, 2042, 264, 27706, 51788], "temperature": 0.0, "avg_logprob": -0.1306198652800139, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0014061041874811053}, {"id": 378, "seek": 209688, "start": 2096.88, "end": 2102.4, "text": " operations. There's a certain thing that I would like to see, which is something like a more sparse", "tokens": [50364, 7705, 13, 821, 311, 257, 1629, 551, 300, 286, 576, 411, 281, 536, 11, 597, 307, 746, 411, 257, 544, 637, 11668, 50640], "temperature": 0.0, "avg_logprob": -0.13858315924636455, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006685667205601931}, {"id": 379, "seek": 209688, "start": 2102.4, "end": 2109.6, "text": " language of thought. When we are looking at deep learning models, there's a phenomenon that people", "tokens": [50640, 2856, 295, 1194, 13, 1133, 321, 366, 1237, 412, 2452, 2539, 5245, 11, 456, 311, 257, 14029, 300, 561, 51000], "temperature": 0.0, "avg_logprob": -0.13858315924636455, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006685667205601931}, {"id": 380, "seek": 209688, "start": 2109.6, "end": 2115.52, "text": " are sometimes observing, which they call grocking. That is, you train the model and your model gets", "tokens": [51000, 366, 2171, 22107, 11, 597, 436, 818, 290, 17799, 278, 13, 663, 307, 11, 291, 3847, 264, 2316, 293, 428, 2316, 2170, 51296], "temperature": 0.0, "avg_logprob": -0.13858315924636455, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006685667205601931}, {"id": 381, "seek": 209688, "start": 2115.52, "end": 2120.32, "text": " better and better. And then it overfits, which means it gets very good at the training data,", "tokens": [51296, 1101, 293, 1101, 13, 400, 550, 309, 670, 69, 1208, 11, 597, 1355, 309, 2170, 588, 665, 412, 264, 3097, 1412, 11, 51536], "temperature": 0.0, "avg_logprob": -0.13858315924636455, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006685667205601931}, {"id": 382, "seek": 209688, "start": 2120.32, "end": 2124.96, "text": " but it gets very bad on the real world at things that it hasn't seen before, like a person in", "tokens": [51536, 457, 309, 2170, 588, 1578, 322, 264, 957, 1002, 412, 721, 300, 309, 6132, 380, 1612, 949, 11, 411, 257, 954, 294, 51768], "temperature": 0.0, "avg_logprob": -0.13858315924636455, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006685667205601931}, {"id": 383, "seek": 212496, "start": 2124.96, "end": 2129.92, "text": " psychedelics was able to explain everything in the past, but is no longer able to perform well in", "tokens": [50364, 47732, 1167, 390, 1075, 281, 2903, 1203, 294, 264, 1791, 11, 457, 307, 572, 2854, 1075, 281, 2042, 731, 294, 50612], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 384, "seek": 212496, "start": 2129.92, "end": 2133.92, "text": " the future because they're overfitting. They basically fit the curve too closely to the data", "tokens": [50612, 264, 2027, 570, 436, 434, 670, 69, 2414, 13, 814, 1936, 3318, 264, 7605, 886, 8185, 281, 264, 1412, 50812], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 385, "seek": 212496, "start": 2133.92, "end": 2138.8, "text": " that I've seen. And there are many tricks in deep learning to go around this overfitting to make", "tokens": [50812, 300, 286, 600, 1612, 13, 400, 456, 366, 867, 11733, 294, 2452, 2539, 281, 352, 926, 341, 670, 69, 2414, 281, 652, 51056], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 386, "seek": 212496, "start": 2138.8, "end": 2143.44, "text": " sure that this doesn't happen. And people try to avoid it. And then what they discovered is when", "tokens": [51056, 988, 300, 341, 1177, 380, 1051, 13, 400, 561, 853, 281, 5042, 309, 13, 400, 550, 437, 436, 6941, 307, 562, 51288], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 387, "seek": 212496, "start": 2143.44, "end": 2147.2, "text": " you take this overfit model, you train it more and more and more and more. At some point, it", "tokens": [51288, 291, 747, 341, 670, 6845, 2316, 11, 291, 3847, 309, 544, 293, 544, 293, 544, 293, 544, 13, 1711, 512, 935, 11, 309, 51476], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 388, "seek": 212496, "start": 2147.2, "end": 2153.04, "text": " sometimes clicks and it gets much better than ever before. And there is a question if there's", "tokens": [51476, 2171, 18521, 293, 309, 2170, 709, 1101, 813, 1562, 949, 13, 400, 456, 307, 257, 1168, 498, 456, 311, 51768], "temperature": 0.0, "avg_logprob": -0.09737286360367485, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.003592804539948702}, {"id": 389, "seek": 215304, "start": 2153.12, "end": 2158.96, "text": " something that we're doing wrong in deep learning. For instance, when you think about how people", "tokens": [50368, 746, 300, 321, 434, 884, 2085, 294, 2452, 2539, 13, 1171, 5197, 11, 562, 291, 519, 466, 577, 561, 50660], "temperature": 0.0, "avg_logprob": -0.14536068582127237, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0027564316987991333}, {"id": 390, "seek": 215304, "start": 2158.96, "end": 2165.12, "text": " learn, they learn very different from GPT-3. People first learn by pointing at stuff that", "tokens": [50660, 1466, 11, 436, 1466, 588, 819, 490, 26039, 51, 12, 18, 13, 3432, 700, 1466, 538, 12166, 412, 1507, 300, 50968], "temperature": 0.0, "avg_logprob": -0.14536068582127237, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0027564316987991333}, {"id": 391, "seek": 215304, "start": 2165.12, "end": 2169.2, "text": " thinks that are relevant to them, that they can eat, that they can hurt, that can hurt them,", "tokens": [50968, 7309, 300, 366, 7340, 281, 552, 11, 300, 436, 393, 1862, 11, 300, 436, 393, 4607, 11, 300, 393, 4607, 552, 11, 51172], "temperature": 0.0, "avg_logprob": -0.14536068582127237, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0027564316987991333}, {"id": 392, "seek": 215304, "start": 2169.2, "end": 2174.0, "text": " or that they find pleasant and so on. They, that they can feel that they can, they have contrast", "tokens": [51172, 420, 300, 436, 915, 16232, 293, 370, 322, 13, 814, 11, 300, 436, 393, 841, 300, 436, 393, 11, 436, 362, 8712, 51412], "temperature": 0.0, "avg_logprob": -0.14536068582127237, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0027564316987991333}, {"id": 393, "seek": 215304, "start": 2174.0, "end": 2178.64, "text": " on it that are salient to them. And so you start out with learning these semantics based on the", "tokens": [51412, 322, 309, 300, 366, 1845, 1196, 281, 552, 13, 400, 370, 291, 722, 484, 365, 2539, 613, 4361, 45298, 2361, 322, 264, 51644], "temperature": 0.0, "avg_logprob": -0.14536068582127237, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0027564316987991333}, {"id": 394, "seek": 217864, "start": 2178.64, "end": 2184.0, "text": " saliency and relevance that you have. And then when you learn language, you learn basic syntax,", "tokens": [50364, 1845, 7848, 293, 32684, 300, 291, 362, 13, 400, 550, 562, 291, 1466, 2856, 11, 291, 1466, 3875, 28431, 11, 50632], "temperature": 0.0, "avg_logprob": -0.09809832345871698, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.007010053843259811}, {"id": 395, "seek": 217864, "start": 2184.0, "end": 2188.72, "text": " how to put things together. And in the long tail of the syntax, you learn style, how to express", "tokens": [50632, 577, 281, 829, 721, 1214, 13, 400, 294, 264, 938, 6838, 295, 264, 28431, 11, 291, 1466, 3758, 11, 577, 281, 5109, 50868], "temperature": 0.0, "avg_logprob": -0.09809832345871698, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.007010053843259811}, {"id": 396, "seek": 217864, "start": 2188.72, "end": 2195.68, "text": " things with new ones and so on. And with GPT-3, it's the opposite. You first learn style, right?", "tokens": [50868, 721, 365, 777, 2306, 293, 370, 322, 13, 400, 365, 26039, 51, 12, 18, 11, 309, 311, 264, 6182, 13, 509, 700, 1466, 3758, 11, 558, 30, 51216], "temperature": 0.0, "avg_logprob": -0.09809832345871698, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.007010053843259811}, {"id": 397, "seek": 217864, "start": 2195.68, "end": 2200.8799999999997, "text": " And then you learn syntax as the regularities in the style. And the semantics is the long tail of", "tokens": [51216, 400, 550, 291, 1466, 28431, 382, 264, 3890, 1088, 294, 264, 3758, 13, 400, 264, 4361, 45298, 307, 264, 938, 6838, 295, 51476], "temperature": 0.0, "avg_logprob": -0.09809832345871698, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.007010053843259811}, {"id": 398, "seek": 217864, "start": 2200.8799999999997, "end": 2206.3199999999997, "text": " that. And to make that happen, you need to learn much, much more. You need to have more training", "tokens": [51476, 300, 13, 400, 281, 652, 300, 1051, 11, 291, 643, 281, 1466, 709, 11, 709, 544, 13, 509, 643, 281, 362, 544, 3097, 51748], "temperature": 0.0, "avg_logprob": -0.09809832345871698, "compression_ratio": 1.9015748031496063, "no_speech_prob": 0.007010053843259811}, {"id": 399, "seek": 220632, "start": 2206.32, "end": 2212.2400000000002, "text": " data and so on. Maybe there's a way in which we can reverse the order and basically get it to", "tokens": [50364, 1412, 293, 370, 322, 13, 2704, 456, 311, 257, 636, 294, 597, 321, 393, 9943, 264, 1668, 293, 1936, 483, 309, 281, 50660], "temperature": 0.0, "avg_logprob": -0.14272389860234708, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0131930997595191}, {"id": 400, "seek": 220632, "start": 2212.2400000000002, "end": 2217.2000000000003, "text": " start out with relevance, to build a curriculum where you first get very sparse regularities,", "tokens": [50660, 722, 484, 365, 32684, 11, 281, 1322, 257, 14302, 689, 291, 700, 483, 588, 637, 11668, 3890, 1088, 11, 50908], "temperature": 0.0, "avg_logprob": -0.14272389860234708, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0131930997595191}, {"id": 401, "seek": 220632, "start": 2217.2000000000003, "end": 2222.56, "text": " where it clicks into place. You always make sure that you can handle it with very limited resources", "tokens": [50908, 689, 309, 18521, 666, 1081, 13, 509, 1009, 652, 988, 300, 291, 393, 4813, 309, 365, 588, 5567, 3593, 51176], "temperature": 0.0, "avg_logprob": -0.14272389860234708, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0131930997595191}, {"id": 402, "seek": 220632, "start": 2222.56, "end": 2228.6400000000003, "text": " and only see the style and the niceties and the nuances as the far extensions of these very sparse", "tokens": [51176, 293, 787, 536, 264, 3758, 293, 264, 6201, 43469, 293, 264, 38775, 382, 264, 1400, 25129, 295, 613, 588, 637, 11668, 51480], "temperature": 0.0, "avg_logprob": -0.14272389860234708, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0131930997595191}, {"id": 403, "seek": 220632, "start": 2228.6400000000003, "end": 2234.32, "text": " concise models that have very big predictive power. Yeah. I mean, on that, I mean, the Grocking", "tokens": [51480, 44882, 5245, 300, 362, 588, 955, 35521, 1347, 13, 865, 13, 286, 914, 11, 322, 300, 11, 286, 914, 11, 264, 460, 17799, 278, 51764], "temperature": 0.0, "avg_logprob": -0.14272389860234708, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0131930997595191}, {"id": 404, "seek": 223432, "start": 2234.32, "end": 2239.2000000000003, "text": " paper was very interesting. And a lot of these large language model fans always cite that very,", "tokens": [50364, 3035, 390, 588, 1880, 13, 400, 257, 688, 295, 613, 2416, 2856, 2316, 4499, 1009, 37771, 300, 588, 11, 50608], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 405, "seek": 223432, "start": 2239.2000000000003, "end": 2243.28, "text": " very quickly when you have a conversation with them. But there is a problem with machine learning", "tokens": [50608, 588, 2661, 562, 291, 362, 257, 3761, 365, 552, 13, 583, 456, 307, 257, 1154, 365, 3479, 2539, 50812], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 406, "seek": 223432, "start": 2243.28, "end": 2247.84, "text": " in general, which is that there is, as you said, there's a spectrum of correlations and almost", "tokens": [50812, 294, 2674, 11, 597, 307, 300, 456, 307, 11, 382, 291, 848, 11, 456, 311, 257, 11143, 295, 13983, 763, 293, 1920, 51040], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 407, "seek": 223432, "start": 2247.84, "end": 2254.0, "text": " all of them are spurious. And on one side of that spectrum, you have the idealized features you", "tokens": [51040, 439, 295, 552, 366, 637, 24274, 13, 400, 322, 472, 1252, 295, 300, 11143, 11, 291, 362, 264, 7157, 1602, 4122, 291, 51348], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 408, "seek": 223432, "start": 2254.0, "end": 2258.0, "text": " actually want it to learn, which will generalize after distribution. And then, of course, if you", "tokens": [51348, 767, 528, 309, 281, 1466, 11, 597, 486, 2674, 1125, 934, 7316, 13, 400, 550, 11, 295, 1164, 11, 498, 291, 51548], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 409, "seek": 223432, "start": 2258.0, "end": 2263.6800000000003, "text": " go down that spectrum, you pick up on all sorts of very spurious correlations that just happen", "tokens": [51548, 352, 760, 300, 11143, 11, 291, 1888, 493, 322, 439, 7527, 295, 588, 637, 24274, 13983, 763, 300, 445, 1051, 51832], "temperature": 0.0, "avg_logprob": -0.05463390139972463, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.007480241358280182}, {"id": 410, "seek": 226368, "start": 2263.68, "end": 2268.3999999999996, "text": " to generalize very well. And if you tell the models not to use those spurious correlations,", "tokens": [50364, 281, 2674, 1125, 588, 731, 13, 400, 498, 291, 980, 264, 5245, 406, 281, 764, 729, 637, 24274, 13983, 763, 11, 50600], "temperature": 0.0, "avg_logprob": -0.11280732541470914, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0054579684510827065}, {"id": 411, "seek": 226368, "start": 2268.3999999999996, "end": 2273.2799999999997, "text": " that the performance of the model will go down. But I want to just move a little bit over to", "tokens": [50600, 300, 264, 3389, 295, 264, 2316, 486, 352, 760, 13, 583, 286, 528, 281, 445, 1286, 257, 707, 857, 670, 281, 50844], "temperature": 0.0, "avg_logprob": -0.11280732541470914, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0054579684510827065}, {"id": 412, "seek": 226368, "start": 2274.7999999999997, "end": 2279.68, "text": " Yasaman Rezegi's paper. I don't know whether you saw that, but she showed that the performance of", "tokens": [50920, 30557, 6147, 1300, 89, 1146, 72, 311, 3035, 13, 286, 500, 380, 458, 1968, 291, 1866, 300, 11, 457, 750, 4712, 300, 264, 3389, 295, 51164], "temperature": 0.0, "avg_logprob": -0.11280732541470914, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0054579684510827065}, {"id": 413, "seek": 226368, "start": 2279.68, "end": 2285.04, "text": " large language models for arithmetic tasks are linearly correlated to the term frequency and", "tokens": [51164, 2416, 2856, 5245, 337, 42973, 9608, 366, 43586, 38574, 281, 264, 1433, 7893, 293, 51432], "temperature": 0.0, "avg_logprob": -0.11280732541470914, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0054579684510827065}, {"id": 414, "seek": 226368, "start": 2285.04, "end": 2290.16, "text": " the training corpus, suggesting that they are memorizing the data set, which presumably you", "tokens": [51432, 264, 3097, 1181, 31624, 11, 18094, 300, 436, 366, 10560, 3319, 264, 1412, 992, 11, 597, 26742, 291, 51688], "temperature": 0.0, "avg_logprob": -0.11280732541470914, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0054579684510827065}, {"id": 415, "seek": 229016, "start": 2290.16, "end": 2295.8399999999997, "text": " would agree with. And Google has recently released this 540 billion parameter language model called", "tokens": [50364, 576, 3986, 365, 13, 400, 3329, 575, 3938, 4736, 341, 1025, 5254, 5218, 13075, 2856, 2316, 1219, 50648], "temperature": 0.0, "avg_logprob": -0.09107430775960286, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.022405922412872314}, {"id": 416, "seek": 229016, "start": 2295.8399999999997, "end": 2301.8399999999997, "text": " PAM, which interestingly does extremely well on, for example, some of the Google big bench tasks,", "tokens": [50648, 430, 2865, 11, 597, 25873, 775, 4664, 731, 322, 11, 337, 1365, 11, 512, 295, 264, 3329, 955, 10638, 9608, 11, 50948], "temperature": 0.0, "avg_logprob": -0.09107430775960286, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.022405922412872314}, {"id": 417, "seek": 229016, "start": 2301.8399999999997, "end": 2307.12, "text": " such as the conceptual combinations task, which is one of them, which tests for compositionality,", "tokens": [50948, 1270, 382, 264, 24106, 21267, 5633, 11, 597, 307, 472, 295, 552, 11, 597, 6921, 337, 12686, 1860, 11, 51212], "temperature": 0.0, "avg_logprob": -0.09107430775960286, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.022405922412872314}, {"id": 418, "seek": 229016, "start": 2307.12, "end": 2311.8399999999997, "text": " which we'll talk about in a minute. But compositionality is when you can take constituents from", "tokens": [51212, 597, 321, 603, 751, 466, 294, 257, 3456, 13, 583, 12686, 1860, 307, 562, 291, 393, 747, 30847, 490, 51448], "temperature": 0.0, "avg_logprob": -0.09107430775960286, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.022405922412872314}, {"id": 419, "seek": 229016, "start": 2311.8399999999997, "end": 2316.3999999999996, "text": " the prompt and compose them together to form the answer. Now, it's tempting to jump to the", "tokens": [51448, 264, 12391, 293, 35925, 552, 1214, 281, 1254, 264, 1867, 13, 823, 11, 309, 311, 37900, 281, 3012, 281, 264, 51676], "temperature": 0.0, "avg_logprob": -0.09107430775960286, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.022405922412872314}, {"id": 420, "seek": 231640, "start": 2316.48, "end": 2321.36, "text": " conclusion that these models are starting to magically reason at scale along the lines that", "tokens": [50368, 10063, 300, 613, 5245, 366, 2891, 281, 39763, 1778, 412, 4373, 2051, 264, 3876, 300, 50612], "temperature": 0.0, "avg_logprob": -0.09488090836858175, "compression_ratio": 1.536, "no_speech_prob": 0.018913256004452705}, {"id": 421, "seek": 231640, "start": 2321.36, "end": 2326.32, "text": " you were just discussing. But I still think there's plenty of opportunities for shortcut learning,", "tokens": [50612, 291, 645, 445, 10850, 13, 583, 286, 920, 519, 456, 311, 7140, 295, 4786, 337, 24822, 2539, 11, 50860], "temperature": 0.0, "avg_logprob": -0.09488090836858175, "compression_ratio": 1.536, "no_speech_prob": 0.018913256004452705}, {"id": 422, "seek": 231640, "start": 2326.32, "end": 2331.12, "text": " you know, by which I mean these spurious correlations, given the brittle interface of an", "tokens": [50860, 291, 458, 11, 538, 597, 286, 914, 613, 637, 24274, 13983, 763, 11, 2212, 264, 49325, 9226, 295, 364, 51100], "temperature": 0.0, "avg_logprob": -0.09488090836858175, "compression_ratio": 1.536, "no_speech_prob": 0.018913256004452705}, {"id": 423, "seek": 231640, "start": 2331.12, "end": 2337.6, "text": " autoregressive GPT style language model with these human designed benchmarks. Would you agree with that?", "tokens": [51100, 1476, 418, 3091, 488, 26039, 51, 3758, 2856, 2316, 365, 613, 1952, 4761, 43751, 13, 6068, 291, 3986, 365, 300, 30, 51424], "temperature": 0.0, "avg_logprob": -0.09488090836858175, "compression_ratio": 1.536, "no_speech_prob": 0.018913256004452705}, {"id": 424, "seek": 233760, "start": 2337.6, "end": 2347.6, "text": " Yeah. When I started my own career in computer science in the 90s, I was in New Zealand, and the", "tokens": [50364, 865, 13, 1133, 286, 1409, 452, 1065, 3988, 294, 3820, 3497, 294, 264, 4289, 82, 11, 286, 390, 294, 1873, 13883, 11, 293, 264, 50864], "temperature": 0.0, "avg_logprob": -0.1306590255425901, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.01852371357381344}, {"id": 425, "seek": 233760, "start": 2347.6, "end": 2353.52, "text": " prof here in Britain realized that I was bored in class. So he took me out of the class and in", "tokens": [50864, 1740, 510, 294, 12960, 5334, 300, 286, 390, 13521, 294, 1508, 13, 407, 415, 1890, 385, 484, 295, 264, 1508, 293, 294, 51160], "temperature": 0.0, "avg_logprob": -0.1306590255425901, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.01852371357381344}, {"id": 426, "seek": 233760, "start": 2353.52, "end": 2358.48, "text": " his lab, and he gave me the task to discover grammatical structure and an unknown language from", "tokens": [51160, 702, 2715, 11, 293, 415, 2729, 385, 264, 5633, 281, 4411, 17570, 267, 804, 3877, 293, 364, 9841, 2856, 490, 51408], "temperature": 0.0, "avg_logprob": -0.1306590255425901, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.01852371357381344}, {"id": 427, "seek": 233760, "start": 2358.48, "end": 2364.08, "text": " scratch and left me pretty much to my own devices on how to do this. So the unknown language I picked", "tokens": [51408, 8459, 293, 1411, 385, 1238, 709, 281, 452, 1065, 5759, 322, 577, 281, 360, 341, 13, 407, 264, 9841, 2856, 286, 6183, 51688], "temperature": 0.0, "avg_logprob": -0.1306590255425901, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.01852371357381344}, {"id": 428, "seek": 236408, "start": 2364.08, "end": 2368.64, "text": " was English, was just unknown to the computer, but was the easiest one to get a corpus for,", "tokens": [50364, 390, 3669, 11, 390, 445, 9841, 281, 264, 3820, 11, 457, 390, 264, 12889, 472, 281, 483, 257, 1181, 31624, 337, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1706452109596946, "compression_ratio": 1.578397212543554, "no_speech_prob": 0.022609161213040352}, {"id": 429, "seek": 236408, "start": 2368.64, "end": 2372.56, "text": " and they gave me the largest computer they had. It has two gigabytes of RAM, and I did", "tokens": [50592, 293, 436, 2729, 385, 264, 6443, 3820, 436, 632, 13, 467, 575, 732, 42741, 295, 14561, 11, 293, 286, 630, 50788], "temperature": 0.0, "avg_logprob": -0.1706452109596946, "compression_ratio": 1.578397212543554, "no_speech_prob": 0.022609161213040352}, {"id": 430, "seek": 236408, "start": 2372.56, "end": 2377.6, "text": " in memory compression with C and so on, and tried to do statistics, and I quickly realized NREM", "tokens": [50788, 294, 4675, 19355, 365, 383, 293, 370, 322, 11, 293, 3031, 281, 360, 12523, 11, 293, 286, 2661, 5334, 426, 3850, 44, 51040], "temperature": 0.0, "avg_logprob": -0.1706452109596946, "compression_ratio": 1.578397212543554, "no_speech_prob": 0.022609161213040352}, {"id": 431, "seek": 236408, "start": 2377.6, "end": 2384.56, "text": " statistics don't work because of too many words in between. So unlike vision tasks where", "tokens": [51040, 12523, 500, 380, 589, 570, 295, 886, 867, 2283, 294, 1296, 13, 407, 8343, 5201, 9608, 689, 51388], "temperature": 0.0, "avg_logprob": -0.1706452109596946, "compression_ratio": 1.578397212543554, "no_speech_prob": 0.022609161213040352}, {"id": 432, "seek": 236408, "start": 2384.56, "end": 2390.4, "text": " confnets have a useful prior by thinking that adjacent pixels also relate to symbolically", "tokens": [51388, 1497, 77, 1385, 362, 257, 4420, 4059, 538, 1953, 300, 24441, 18668, 611, 10961, 281, 5986, 984, 51680], "temperature": 0.0, "avg_logprob": -0.1706452109596946, "compression_ratio": 1.578397212543554, "no_speech_prob": 0.022609161213040352}, {"id": 433, "seek": 239040, "start": 2390.4, "end": 2394.7200000000003, "text": " related information, right? So adjacency in images is a very good predictor for thematic", "tokens": [50364, 4077, 1589, 11, 558, 30, 407, 22940, 3020, 294, 5267, 307, 257, 588, 665, 6069, 284, 337, 552, 2399, 50580], "temperature": 0.0, "avg_logprob": -0.11775265942822706, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.014276611618697643}, {"id": 434, "seek": 239040, "start": 2394.7200000000003, "end": 2399.92, "text": " relatedness. It doesn't really work in NLP. So the transformer was discovered in natural language", "tokens": [50580, 4077, 1287, 13, 467, 1177, 380, 534, 589, 294, 426, 45196, 13, 407, 264, 31782, 390, 6941, 294, 3303, 2856, 50840], "temperature": 0.0, "avg_logprob": -0.11775265942822706, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.014276611618697643}, {"id": 435, "seek": 239040, "start": 2399.92, "end": 2406.56, "text": " processing for that reason, because you cannot use direct adjacency very well. And so I realized", "tokens": [50840, 9007, 337, 300, 1778, 11, 570, 291, 2644, 764, 2047, 22940, 3020, 588, 731, 13, 400, 370, 286, 5334, 51172], "temperature": 0.0, "avg_logprob": -0.11775265942822706, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.014276611618697643}, {"id": 436, "seek": 239040, "start": 2406.56, "end": 2413.36, "text": " I cannot use NREM, which depend on direct adjacency between words. And so I first of all used ordered", "tokens": [51172, 286, 2644, 764, 426, 3850, 44, 11, 597, 5672, 322, 2047, 22940, 3020, 1296, 2283, 13, 400, 370, 286, 700, 295, 439, 1143, 8866, 51512], "temperature": 0.0, "avg_logprob": -0.11775265942822706, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.014276611618697643}, {"id": 437, "seek": 239040, "start": 2413.36, "end": 2418.96, "text": " pairs of words and tried to find correlations between pairs and then find a mutual information", "tokens": [51512, 15494, 295, 2283, 293, 3031, 281, 915, 13983, 763, 1296, 15494, 293, 550, 915, 257, 16917, 1589, 51792], "temperature": 0.0, "avg_logprob": -0.11775265942822706, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.014276611618697643}, {"id": 438, "seek": 241896, "start": 2418.96, "end": 2424.16, "text": " tree that would give me the best prediction over the structure of the sentence for all the", "tokens": [50364, 4230, 300, 576, 976, 385, 264, 1151, 17630, 670, 264, 3877, 295, 264, 8174, 337, 439, 264, 50624], "temperature": 0.0, "avg_logprob": -0.11411897712778822, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.0020186284091323614}, {"id": 439, "seek": 241896, "start": 2424.16, "end": 2429.12, "text": " sentences that I would have in my corpus. And indeed this correlated to structure. And I realized", "tokens": [50624, 16579, 300, 286, 576, 362, 294, 452, 1181, 31624, 13, 400, 6451, 341, 38574, 281, 3877, 13, 400, 286, 5334, 50872], "temperature": 0.0, "avg_logprob": -0.11411897712778822, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.0020186284091323614}, {"id": 440, "seek": 241896, "start": 2429.12, "end": 2433.68, "text": " this is going to not just give me grammar, but it's also going to give me semantics", "tokens": [50872, 341, 307, 516, 281, 406, 445, 976, 385, 22317, 11, 457, 309, 311, 611, 516, 281, 976, 385, 4361, 45298, 51100], "temperature": 0.0, "avg_logprob": -0.11411897712778822, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.0020186284091323614}, {"id": 441, "seek": 241896, "start": 2433.68, "end": 2439.28, "text": " if I can more deep statistics. But I will need something not just ordered pairs,", "tokens": [51100, 498, 286, 393, 544, 2452, 12523, 13, 583, 286, 486, 643, 746, 406, 445, 8866, 15494, 11, 51380], "temperature": 0.0, "avg_logprob": -0.11411897712778822, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.0020186284091323614}, {"id": 442, "seek": 241896, "start": 2439.28, "end": 2444.48, "text": " but I need to have something like force order models. But to do the statistics, even in memory,", "tokens": [51380, 457, 286, 643, 281, 362, 746, 411, 3464, 1668, 5245, 13, 583, 281, 360, 264, 12523, 11, 754, 294, 4675, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11411897712778822, "compression_ratio": 1.8401639344262295, "no_speech_prob": 0.0020186284091323614}, {"id": 443, "seek": 244448, "start": 2444.48, "end": 2448.96, "text": " with clever and memory compression and many tricks that I did, I could not do full statistics on", "tokens": [50364, 365, 13494, 293, 4675, 19355, 293, 867, 11733, 300, 286, 630, 11, 286, 727, 406, 360, 1577, 12523, 322, 50588], "temperature": 0.0, "avg_logprob": -0.10706715834768195, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0216099563986063}, {"id": 444, "seek": 244448, "start": 2448.96, "end": 2456.8, "text": " this. So what I realized I had to do was that I do multiple passes. And at first I discard almost", "tokens": [50588, 341, 13, 407, 437, 286, 5334, 286, 632, 281, 360, 390, 300, 286, 360, 3866, 11335, 13, 400, 412, 700, 286, 31597, 1920, 50980], "temperature": 0.0, "avg_logprob": -0.10706715834768195, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0216099563986063}, {"id": 445, "seek": 244448, "start": 2456.8, "end": 2461.68, "text": " all the information. I only pick out the most salient things. And then my time was over in", "tokens": [50980, 439, 264, 1589, 13, 286, 787, 1888, 484, 264, 881, 1845, 1196, 721, 13, 400, 550, 452, 565, 390, 670, 294, 51224], "temperature": 0.0, "avg_logprob": -0.10706715834768195, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0216099563986063}, {"id": 446, "seek": 244448, "start": 2461.68, "end": 2467.28, "text": " this lab. And I went back to Germany and never reviewed this area of research again. But what", "tokens": [51224, 341, 2715, 13, 400, 286, 1437, 646, 281, 7244, 293, 1128, 18429, 341, 1859, 295, 2132, 797, 13, 583, 437, 51504], "temperature": 0.0, "avg_logprob": -0.10706715834768195, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0216099563986063}, {"id": 447, "seek": 244448, "start": 2467.28, "end": 2470.96, "text": " I had realized is to make progress and need to make statistics over what I need to make the", "tokens": [51504, 286, 632, 5334, 307, 281, 652, 4205, 293, 643, 281, 652, 12523, 670, 437, 286, 643, 281, 652, 264, 51688], "temperature": 0.0, "avg_logprob": -0.10706715834768195, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0216099563986063}, {"id": 448, "seek": 247096, "start": 2470.96, "end": 2476.32, "text": " statistics over. And the very principled base, I need to learn what I have to learn.", "tokens": [50364, 12523, 670, 13, 400, 264, 588, 3681, 15551, 3096, 11, 286, 643, 281, 1466, 437, 286, 362, 281, 1466, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13409467603339523, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0021474116947501898}, {"id": 449, "seek": 247096, "start": 2477.6, "end": 2483.6, "text": " And they didn't pay attention to this domain at all. And I also missed the 2017 transformer paper", "tokens": [50696, 400, 436, 994, 380, 1689, 3202, 281, 341, 9274, 412, 439, 13, 400, 286, 611, 6721, 264, 6591, 31782, 3035, 50996], "temperature": 0.0, "avg_logprob": -0.13409467603339523, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0021474116947501898}, {"id": 450, "seek": 247096, "start": 2483.6, "end": 2488.32, "text": " and its relevance. It was only when GPT-2 came out that I realized, oh my god, they did this.", "tokens": [50996, 293, 1080, 32684, 13, 467, 390, 787, 562, 26039, 51, 12, 17, 1361, 484, 300, 286, 5334, 11, 1954, 452, 3044, 11, 436, 630, 341, 13, 51232], "temperature": 0.0, "avg_logprob": -0.13409467603339523, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0021474116947501898}, {"id": 451, "seek": 247096, "start": 2488.32, "end": 2494.88, "text": " They did statistics over the statistics. And it's still not the right solution, I think. It's not", "tokens": [51232, 814, 630, 12523, 670, 264, 12523, 13, 400, 309, 311, 920, 406, 264, 558, 3827, 11, 286, 519, 13, 467, 311, 406, 51560], "temperature": 0.0, "avg_logprob": -0.13409467603339523, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0021474116947501898}, {"id": 452, "seek": 247096, "start": 2494.88, "end": 2498.8, "text": " the way in which our brain is doing it. It's some brute force shortcut. Well, for instance,", "tokens": [51560, 264, 636, 294, 597, 527, 3567, 307, 884, 309, 13, 467, 311, 512, 47909, 3464, 24822, 13, 1042, 11, 337, 5197, 11, 51756], "temperature": 0.0, "avg_logprob": -0.13409467603339523, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0021474116947501898}, {"id": 453, "seek": 249880, "start": 2498.88, "end": 2502.0, "text": " the individual attention heads are not correlated with each other, but in reality,", "tokens": [50368, 264, 2609, 3202, 8050, 366, 406, 38574, 365, 1184, 661, 11, 457, 294, 4103, 11, 50524], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 454, "seek": 249880, "start": 2502.0, "end": 2507.6800000000003, "text": " they are. We have this in reality, our attention heads are integrated into one model of what's", "tokens": [50524, 436, 366, 13, 492, 362, 341, 294, 4103, 11, 527, 3202, 8050, 366, 10919, 666, 472, 2316, 295, 437, 311, 50808], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 455, "seek": 249880, "start": 2507.6800000000003, "end": 2513.6000000000004, "text": " going on. And it's not that we have an attention net on every layer that just pays attention to", "tokens": [50808, 516, 322, 13, 400, 309, 311, 406, 300, 321, 362, 364, 3202, 2533, 322, 633, 4583, 300, 445, 10604, 3202, 281, 51104], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 456, "seek": 249880, "start": 2513.6000000000004, "end": 2519.1200000000003, "text": " what's happening in the lower layer. It's much more clever in our own mind. And this thing is", "tokens": [51104, 437, 311, 2737, 294, 264, 3126, 4583, 13, 467, 311, 709, 544, 13494, 294, 527, 1065, 1575, 13, 400, 341, 551, 307, 51380], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 457, "seek": 249880, "start": 2519.1200000000003, "end": 2523.36, "text": " active. We single out things in reality to research, which book we need to take out of the", "tokens": [51380, 4967, 13, 492, 2167, 484, 721, 294, 4103, 281, 2132, 11, 597, 1446, 321, 643, 281, 747, 484, 295, 264, 51592], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 458, "seek": 249880, "start": 2523.36, "end": 2527.84, "text": " shelf to update our working memory context so we are able to interpret the current sentence", "tokens": [51592, 15222, 281, 5623, 527, 1364, 4675, 4319, 370, 321, 366, 1075, 281, 7302, 264, 2190, 8174, 51816], "temperature": 0.0, "avg_logprob": -0.1091969758272171, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.01910189539194107}, {"id": 459, "seek": 252784, "start": 2527.84, "end": 2532.8, "text": " that we don't understand. And so we always go for saliency when we read something that doesn't", "tokens": [50364, 300, 321, 500, 380, 1223, 13, 400, 370, 321, 1009, 352, 337, 1845, 7848, 562, 321, 1401, 746, 300, 1177, 380, 50612], "temperature": 0.0, "avg_logprob": -0.08954788105828422, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.004975941963493824}, {"id": 460, "seek": 252784, "start": 2532.8, "end": 2541.04, "text": " make sense. At least my mind works like this. I discard it. I will not stop until it makes sense,", "tokens": [50612, 652, 2020, 13, 1711, 1935, 452, 1575, 1985, 411, 341, 13, 286, 31597, 309, 13, 286, 486, 406, 1590, 1826, 309, 1669, 2020, 11, 51024], "temperature": 0.0, "avg_logprob": -0.08954788105828422, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.004975941963493824}, {"id": 461, "seek": 252784, "start": 2541.04, "end": 2546.48, "text": " or I will have to go to some preliminary. I will not accept some kind of vague statistical", "tokens": [51024, 420, 286, 486, 362, 281, 352, 281, 512, 28817, 13, 286, 486, 406, 3241, 512, 733, 295, 24247, 22820, 51296], "temperature": 0.0, "avg_logprob": -0.08954788105828422, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.004975941963493824}, {"id": 462, "seek": 252784, "start": 2546.48, "end": 2551.36, "text": " approximation of what I read. Keep this as an intermediary stage in my mind until the hope", "tokens": [51296, 28023, 295, 437, 286, 1401, 13, 5527, 341, 382, 364, 15184, 822, 3233, 294, 452, 1575, 1826, 264, 1454, 51540], "temperature": 0.0, "avg_logprob": -0.08954788105828422, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.004975941963493824}, {"id": 463, "seek": 252784, "start": 2551.36, "end": 2556.48, "text": " that eventually converges. It's a completely different learning paradigm. When we teach our", "tokens": [51540, 300, 4728, 9652, 2880, 13, 467, 311, 257, 2584, 819, 2539, 24709, 13, 1133, 321, 2924, 527, 51796], "temperature": 0.0, "avg_logprob": -0.08954788105828422, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.004975941963493824}, {"id": 464, "seek": 255648, "start": 2556.48, "end": 2561.76, "text": " children arithmetic, it's not that we show them lots of very long mass textbooks and hope that", "tokens": [50364, 2227, 42973, 11, 309, 311, 406, 300, 321, 855, 552, 3195, 295, 588, 938, 2758, 33587, 293, 1454, 300, 50628], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 465, "seek": 255648, "start": 2561.76, "end": 2566.4, "text": " initially it will not make any sense to them. But as they reread them again and again with many", "tokens": [50628, 9105, 309, 486, 406, 652, 604, 2020, 281, 552, 13, 583, 382, 436, 46453, 345, 552, 797, 293, 797, 365, 867, 50860], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 466, "seek": 255648, "start": 2566.4, "end": 2571.28, "text": " samples, eventually it will click and they will converge on arithmetic. No, this is not how it", "tokens": [50860, 10938, 11, 4728, 309, 486, 2052, 293, 436, 486, 41881, 322, 42973, 13, 883, 11, 341, 307, 406, 577, 309, 51104], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 467, "seek": 255648, "start": 2571.28, "end": 2575.44, "text": " works. You start this giving them extremely simple things and say, in these extremely simple things,", "tokens": [51104, 1985, 13, 509, 722, 341, 2902, 552, 4664, 2199, 721, 293, 584, 11, 294, 613, 4664, 2199, 721, 11, 51312], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 468, "seek": 255648, "start": 2575.44, "end": 2579.6, "text": " there is structure that you can fully understand. Now go and find the structure that you fully", "tokens": [51312, 456, 307, 3877, 300, 291, 393, 4498, 1223, 13, 823, 352, 293, 915, 264, 3877, 300, 291, 4498, 51520], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 469, "seek": 255648, "start": 2579.6, "end": 2584.4, "text": " understand. Once you've done it, you make this a little bit more complicated for you. This is", "tokens": [51520, 1223, 13, 3443, 291, 600, 1096, 309, 11, 291, 652, 341, 257, 707, 857, 544, 6179, 337, 291, 13, 639, 307, 51760], "temperature": 0.0, "avg_logprob": -0.10592562502080743, "compression_ratio": 1.8976897689768977, "no_speech_prob": 0.013194521889090538}, {"id": 470, "seek": 258440, "start": 2584.4, "end": 2592.32, "text": " probably the paradigm that you could be exploring. I know, but the problem is it's incredibly", "tokens": [50364, 1391, 264, 24709, 300, 291, 727, 312, 12736, 13, 286, 458, 11, 457, 264, 1154, 307, 309, 311, 6252, 50760], "temperature": 0.0, "avg_logprob": -0.15928703546524048, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0008258149609901011}, {"id": 471, "seek": 258440, "start": 2592.32, "end": 2595.6800000000003, "text": " deceptive when you have something which appears intelligence. Of course, the", "tokens": [50760, 368, 1336, 488, 562, 291, 362, 746, 597, 7038, 7599, 13, 2720, 1164, 11, 264, 50928], "temperature": 0.0, "avg_logprob": -0.15928703546524048, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0008258149609901011}, {"id": 472, "seek": 258440, "start": 2596.4, "end": 2602.0, "text": " boundary of our perception of intelligence is a receding one. But I wanted to just get on to", "tokens": [50964, 12866, 295, 527, 12860, 295, 7599, 307, 257, 850, 9794, 472, 13, 583, 286, 1415, 281, 445, 483, 322, 281, 51244], "temperature": 0.0, "avg_logprob": -0.15928703546524048, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0008258149609901011}, {"id": 473, "seek": 258440, "start": 2602.64, "end": 2609.2000000000003, "text": " there are some incredible generative visual models like Dali and the disco diffusion.", "tokens": [51276, 456, 366, 512, 4651, 1337, 1166, 5056, 5245, 411, 413, 5103, 293, 264, 3622, 25242, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15928703546524048, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0008258149609901011}, {"id": 474, "seek": 260920, "start": 2609.68, "end": 2614.24, "text": " These models, I think, are going to revolutionize the creative profession. I've been", "tokens": [50388, 1981, 5245, 11, 286, 519, 11, 366, 516, 281, 8894, 1125, 264, 5880, 7032, 13, 286, 600, 668, 50616], "temperature": 0.0, "avg_logprob": -0.08621066700328481, "compression_ratio": 1.6089965397923875, "no_speech_prob": 0.005396069027483463}, {"id": 475, "seek": 260920, "start": 2614.24, "end": 2618.96, "text": " playing with disco diffusion all day today. I've already ordered some prints to go on my wall.", "tokens": [50616, 2433, 365, 3622, 25242, 439, 786, 965, 13, 286, 600, 1217, 8866, 512, 22305, 281, 352, 322, 452, 2929, 13, 50852], "temperature": 0.0, "avg_logprob": -0.08621066700328481, "compression_ratio": 1.6089965397923875, "no_speech_prob": 0.005396069027483463}, {"id": 476, "seek": 260920, "start": 2618.96, "end": 2625.6, "text": " It's incredible. The two obvious settings where large language models might be successful", "tokens": [50852, 467, 311, 4651, 13, 440, 732, 6322, 6257, 689, 2416, 2856, 5245, 1062, 312, 4406, 51184], "temperature": 0.0, "avg_logprob": -0.08621066700328481, "compression_ratio": 1.6089965397923875, "no_speech_prob": 0.005396069027483463}, {"id": 477, "seek": 260920, "start": 2626.3999999999996, "end": 2631.68, "text": " are coding and information retrieval in my opinion. But let's take pause for thought. I've played", "tokens": [51224, 366, 17720, 293, 1589, 19817, 3337, 294, 452, 4800, 13, 583, 718, 311, 747, 10465, 337, 1194, 13, 286, 600, 3737, 51488], "temperature": 0.0, "avg_logprob": -0.08621066700328481, "compression_ratio": 1.6089965397923875, "no_speech_prob": 0.005396069027483463}, {"id": 478, "seek": 260920, "start": 2631.68, "end": 2637.6, "text": " with Codex and I'm resolutely sure that I wouldn't want to use it. I think code and knowledge are", "tokens": [51488, 365, 15549, 87, 293, 286, 478, 725, 2553, 988, 300, 286, 2759, 380, 528, 281, 764, 309, 13, 286, 519, 3089, 293, 3601, 366, 51784], "temperature": 0.0, "avg_logprob": -0.08621066700328481, "compression_ratio": 1.6089965397923875, "no_speech_prob": 0.005396069027483463}, {"id": 479, "seek": 263760, "start": 2637.6, "end": 2643.12, "text": " a different ballgame to art, which I think will be amazing. With Codex, there's an impedance mismatch", "tokens": [50364, 257, 819, 2594, 15038, 281, 1523, 11, 597, 286, 519, 486, 312, 2243, 13, 2022, 15549, 87, 11, 456, 311, 364, 36264, 23220, 852, 50640], "temperature": 0.0, "avg_logprob": -0.0887980242387964, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.010264856740832329}, {"id": 480, "seek": 263760, "start": 2643.12, "end": 2647.68, "text": " between the process of generating the code and then debugging and running the code, which has", "tokens": [50640, 1296, 264, 1399, 295, 17746, 264, 3089, 293, 550, 45592, 293, 2614, 264, 3089, 11, 597, 575, 50868], "temperature": 0.0, "avg_logprob": -0.0887980242387964, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.010264856740832329}, {"id": 481, "seek": 263760, "start": 2647.68, "end": 2652.7999999999997, "text": " euphemistically been framed as prompt engineering, or another term which I've just invented,", "tokens": [50868, 2228, 41245, 20458, 668, 30420, 382, 12391, 7043, 11, 420, 1071, 1433, 597, 286, 600, 445, 14479, 11, 51124], "temperature": 0.0, "avg_logprob": -0.0887980242387964, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.010264856740832329}, {"id": 482, "seek": 263760, "start": 2652.7999999999997, "end": 2658.56, "text": " retrospective development. I think it's easier to start again from scratch than fix broken code", "tokens": [51124, 34997, 488, 3250, 13, 286, 519, 309, 311, 3571, 281, 722, 797, 490, 8459, 813, 3191, 5463, 3089, 51412], "temperature": 0.0, "avg_logprob": -0.0887980242387964, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.010264856740832329}, {"id": 483, "seek": 263760, "start": 2658.56, "end": 2663.12, "text": " from a large language model. I mean, this is quite interesting. At Google, it already takes", "tokens": [51412, 490, 257, 2416, 2856, 2316, 13, 286, 914, 11, 341, 307, 1596, 1880, 13, 1711, 3329, 11, 309, 1217, 2516, 51640], "temperature": 0.0, "avg_logprob": -0.0887980242387964, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.010264856740832329}, {"id": 484, "seek": 266312, "start": 2663.12, "end": 2668.3199999999997, "text": " months to get any code checked into their mono repo because it's basically a bureaucracy because", "tokens": [50364, 2493, 281, 483, 604, 3089, 10033, 666, 641, 35624, 49040, 570, 309, 311, 1936, 257, 44671, 570, 50624], "temperature": 0.0, "avg_logprob": -0.08730572920579177, "compression_ratio": 1.6416382252559727, "no_speech_prob": 0.00391833670437336}, {"id": 485, "seek": 266312, "start": 2668.3199999999997, "end": 2673.8399999999997, "text": " they needed to have gatekeeping after they decided to use a mono repo. Could you imagine", "tokens": [50624, 436, 2978, 281, 362, 8539, 25769, 934, 436, 3047, 281, 764, 257, 35624, 49040, 13, 7497, 291, 3811, 50900], "temperature": 0.0, "avg_logprob": -0.08730572920579177, "compression_ratio": 1.6416382252559727, "no_speech_prob": 0.00391833670437336}, {"id": 486, "seek": 266312, "start": 2673.8399999999997, "end": 2678.88, "text": " how much bureaucracy there'd be if they allowed people to start checking in code, which was generated", "tokens": [50900, 577, 709, 44671, 456, 1116, 312, 498, 436, 4350, 561, 281, 722, 8568, 294, 3089, 11, 597, 390, 10833, 51152], "temperature": 0.0, "avg_logprob": -0.08730572920579177, "compression_ratio": 1.6416382252559727, "no_speech_prob": 0.00391833670437336}, {"id": 487, "seek": 266312, "start": 2678.88, "end": 2683.12, "text": " from an algorithm? Anyway, I think there's an exciting possible future for using these systems", "tokens": [51152, 490, 364, 9284, 30, 5684, 11, 286, 519, 456, 311, 364, 4670, 1944, 2027, 337, 1228, 613, 3652, 51364], "temperature": 0.0, "avg_logprob": -0.08730572920579177, "compression_ratio": 1.6416382252559727, "no_speech_prob": 0.00391833670437336}, {"id": 488, "seek": 266312, "start": 2683.12, "end": 2689.44, "text": " for information retrieval rather than the way that we go through and prune the results on a Google", "tokens": [51364, 337, 1589, 19817, 3337, 2831, 813, 264, 636, 300, 321, 352, 807, 293, 582, 2613, 264, 3542, 322, 257, 3329, 51680], "temperature": 0.0, "avg_logprob": -0.08730572920579177, "compression_ratio": 1.6416382252559727, "no_speech_prob": 0.00391833670437336}, {"id": 489, "seek": 268944, "start": 2689.44, "end": 2695.28, "text": " search. These models might just answer directly, but I hasten to think what that search UI would", "tokens": [50364, 3164, 13, 1981, 5245, 1062, 445, 1867, 3838, 11, 457, 286, 6581, 268, 281, 519, 437, 300, 3164, 15682, 576, 50656], "temperature": 0.0, "avg_logprob": -0.07558088302612305, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.07340400665998459}, {"id": 490, "seek": 268944, "start": 2695.28, "end": 2701.12, "text": " look like. Would its output be sclerotic or unadaptable? Would it be relevant to the query that", "tokens": [50656, 574, 411, 13, 6068, 1080, 5598, 312, 795, 1918, 9411, 420, 517, 345, 2796, 712, 30, 6068, 309, 312, 7340, 281, 264, 14581, 300, 50948], "temperature": 0.0, "avg_logprob": -0.07558088302612305, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.07340400665998459}, {"id": 491, "seek": 268944, "start": 2701.12, "end": 2707.92, "text": " I put in? Would its output even be true? Perhaps it will ask you to select what kind of truth you", "tokens": [50948, 286, 829, 294, 30, 6068, 1080, 5598, 754, 312, 2074, 30, 10517, 309, 486, 1029, 291, 281, 3048, 437, 733, 295, 3494, 291, 51288], "temperature": 0.0, "avg_logprob": -0.07558088302612305, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.07340400665998459}, {"id": 492, "seek": 268944, "start": 2707.92, "end": 2713.6, "text": " were looking for. Do you think these models would vitiate or spoil our society, or do you think they", "tokens": [51288, 645, 1237, 337, 13, 1144, 291, 519, 613, 5245, 576, 371, 8707, 473, 420, 18630, 527, 4086, 11, 420, 360, 291, 519, 436, 51572], "temperature": 0.0, "avg_logprob": -0.07558088302612305, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.07340400665998459}, {"id": 493, "seek": 271360, "start": 2713.6, "end": 2719.52, "text": " would actually enrich it? It's very hard to say. I think that from some perspective,", "tokens": [50364, 576, 767, 18849, 309, 30, 467, 311, 588, 1152, 281, 584, 13, 286, 519, 300, 490, 512, 4585, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10885436318137429, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.012428038753569126}, {"id": 494, "seek": 271360, "start": 2719.52, "end": 2725.8399999999997, "text": " our society is already maximally spoiled. Humans, as they live today, are basically", "tokens": [50660, 527, 4086, 307, 1217, 5138, 379, 32439, 13, 35809, 11, 382, 436, 1621, 965, 11, 366, 1936, 50976], "temperature": 0.0, "avg_logprob": -0.10885436318137429, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.012428038753569126}, {"id": 495, "seek": 271360, "start": 2725.8399999999997, "end": 2731.04, "text": " locusts with opposable thumbs. This is not going to go on forever, this technological society.", "tokens": [50976, 1628, 381, 82, 365, 4665, 712, 8838, 13, 639, 307, 406, 516, 281, 352, 322, 5680, 11, 341, 18439, 4086, 13, 51236], "temperature": 0.0, "avg_logprob": -0.10885436318137429, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.012428038753569126}, {"id": 496, "seek": 271360, "start": 2731.04, "end": 2736.48, "text": " Here are, it seems to me, on some kind of Titanic that is going to hit the iceberg no matter what.", "tokens": [51236, 1692, 366, 11, 309, 2544, 281, 385, 11, 322, 512, 733, 295, 42183, 300, 307, 516, 281, 2045, 264, 38880, 572, 1871, 437, 13, 51508], "temperature": 0.0, "avg_logprob": -0.10885436318137429, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.012428038753569126}, {"id": 497, "seek": 271360, "start": 2737.2, "end": 2742.08, "text": " And what basically should make us content is that the Titanic was the only place in the", "tokens": [51544, 400, 437, 1936, 820, 652, 505, 2701, 307, 300, 264, 42183, 390, 264, 787, 1081, 294, 264, 51788], "temperature": 0.0, "avg_logprob": -0.10885436318137429, "compression_ratio": 1.6245487364620939, "no_speech_prob": 0.012428038753569126}, {"id": 498, "seek": 274208, "start": 2742.16, "end": 2748.08, "text": " universe that has internet. And we are born on it, and we wouldn't have been born if there was no", "tokens": [50368, 6445, 300, 575, 4705, 13, 400, 321, 366, 4232, 322, 309, 11, 293, 321, 2759, 380, 362, 668, 4232, 498, 456, 390, 572, 50664], "temperature": 0.0, "avg_logprob": -0.11809584924152919, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.008049032650887966}, {"id": 499, "seek": 274208, "start": 2748.08, "end": 2754.88, "text": " Titanic. We would not have been born in a sustainable ancestral society. In some sense,", "tokens": [50664, 42183, 13, 492, 576, 406, 362, 668, 4232, 294, 257, 11235, 40049, 4086, 13, 682, 512, 2020, 11, 51004], "temperature": 0.0, "avg_logprob": -0.11809584924152919, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.008049032650887966}, {"id": 500, "seek": 274208, "start": 2754.88, "end": 2758.96, "text": " our society needs to reinvent itself. It's not really working right now. We don't know", "tokens": [51004, 527, 4086, 2203, 281, 33477, 2564, 13, 467, 311, 406, 534, 1364, 558, 586, 13, 492, 500, 380, 458, 51208], "temperature": 0.0, "avg_logprob": -0.11809584924152919, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.008049032650887966}, {"id": 501, "seek": 274208, "start": 2758.96, "end": 2762.56, "text": " what the future is going to look like, and if it's going to be very technological,", "tokens": [51208, 437, 264, 2027, 307, 516, 281, 574, 411, 11, 293, 498, 309, 311, 516, 281, 312, 588, 18439, 11, 51388], "temperature": 0.0, "avg_logprob": -0.11809584924152919, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.008049032650887966}, {"id": 502, "seek": 274208, "start": 2762.56, "end": 2769.36, "text": " or if limit certain things, no idea what's going to happen. But if we think about how our", "tokens": [51388, 420, 498, 4948, 1629, 721, 11, 572, 1558, 437, 311, 516, 281, 1051, 13, 583, 498, 321, 519, 466, 577, 527, 51728], "temperature": 0.0, "avg_logprob": -0.11809584924152919, "compression_ratio": 1.6920152091254752, "no_speech_prob": 0.008049032650887966}, {"id": 503, "seek": 276936, "start": 2769.44, "end": 2775.6800000000003, "text": " current approaches work, if you want just to make programming better, I suspect that these tools can", "tokens": [50368, 2190, 11587, 589, 11, 498, 291, 528, 445, 281, 652, 9410, 1101, 11, 286, 9091, 300, 613, 3873, 393, 50680], "temperature": 0.0, "avg_logprob": -0.14839688572314902, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.005999954883009195}, {"id": 504, "seek": 276936, "start": 2775.6800000000003, "end": 2781.36, "text": " help. But they will be much more useful if you do not have to have this battle between a machine", "tokens": [50680, 854, 13, 583, 436, 486, 312, 709, 544, 4420, 498, 291, 360, 406, 362, 281, 362, 341, 4635, 1296, 257, 3479, 50964], "temperature": 0.0, "avg_logprob": -0.14839688572314902, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.005999954883009195}, {"id": 505, "seek": 276936, "start": 2781.36, "end": 2787.28, "text": " that doesn't really understand what you want. And instead, you have something that is working next", "tokens": [50964, 300, 1177, 380, 534, 1223, 437, 291, 528, 13, 400, 2602, 11, 291, 362, 746, 300, 307, 1364, 958, 51260], "temperature": 0.0, "avg_logprob": -0.14839688572314902, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.005999954883009195}, {"id": 506, "seek": 276936, "start": 2787.28, "end": 2793.6800000000003, "text": " to you. It's like, imagine you were working for some corporation and the corporation introduces", "tokens": [51260, 281, 291, 13, 467, 311, 411, 11, 3811, 291, 645, 1364, 337, 512, 22197, 293, 264, 22197, 31472, 51580], "temperature": 0.0, "avg_logprob": -0.14839688572314902, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.005999954883009195}, {"id": 507, "seek": 276936, "start": 2793.6800000000003, "end": 2798.1600000000003, "text": " some kind of planning tool that requires to do to jump through all sorts of hopes. And it turns", "tokens": [51580, 512, 733, 295, 5038, 2290, 300, 7029, 281, 360, 281, 3012, 807, 439, 7527, 295, 13681, 13, 400, 309, 4523, 51804], "temperature": 0.0, "avg_logprob": -0.14839688572314902, "compression_ratio": 1.712280701754386, "no_speech_prob": 0.005999954883009195}, {"id": 508, "seek": 279816, "start": 2798.24, "end": 2802.96, "text": " out that the planning tool itself makes you more productive, but it makes work much less fun.", "tokens": [50368, 484, 300, 264, 5038, 2290, 2564, 1669, 291, 544, 13304, 11, 457, 309, 1669, 589, 709, 1570, 1019, 13, 50604], "temperature": 0.0, "avg_logprob": -0.12596911650437576, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.006899597588926554}, {"id": 509, "seek": 279816, "start": 2804.16, "end": 2807.52, "text": " It's still rational to use it. And everybody will hate it, but", "tokens": [50664, 467, 311, 920, 15090, 281, 764, 309, 13, 400, 2201, 486, 4700, 309, 11, 457, 50832], "temperature": 0.0, "avg_logprob": -0.12596911650437576, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.006899597588926554}, {"id": 510, "seek": 279816, "start": 2808.64, "end": 2813.6, "text": " by and large, it will be used if it makes people 30 more productive. And everybody will feel there", "tokens": [50888, 538, 293, 2416, 11, 309, 486, 312, 1143, 498, 309, 1669, 561, 2217, 544, 13304, 13, 400, 2201, 486, 841, 456, 51136], "temperature": 0.0, "avg_logprob": -0.12596911650437576, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.006899597588926554}, {"id": 511, "seek": 279816, "start": 2813.6, "end": 2818.64, "text": " might be a must be a better solution, something that feels more organic. And so it could be that", "tokens": [51136, 1062, 312, 257, 1633, 312, 257, 1101, 3827, 11, 746, 300, 3417, 544, 10220, 13, 400, 370, 309, 727, 312, 300, 51388], "temperature": 0.0, "avg_logprob": -0.12596911650437576, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.006899597588926554}, {"id": 512, "seek": 279816, "start": 2818.64, "end": 2823.44, "text": " Codex is in this category that it makes mediocre programmers much more productive at producing", "tokens": [51388, 15549, 87, 307, 294, 341, 7719, 300, 309, 1669, 45415, 41504, 709, 544, 13304, 412, 10501, 51628], "temperature": 0.0, "avg_logprob": -0.12596911650437576, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.006899597588926554}, {"id": 513, "seek": 282344, "start": 2823.44, "end": 2828.2400000000002, "text": " boilerplate. But it's not just this, it's often able to find solutions very quickly,", "tokens": [50364, 39228, 37008, 13, 583, 309, 311, 406, 445, 341, 11, 309, 311, 2049, 1075, 281, 915, 6547, 588, 2661, 11, 50604], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 514, "seek": 282344, "start": 2828.2400000000002, "end": 2832.48, "text": " but you need to use a lot of Stack Overflow before you understand the new language or before you", "tokens": [50604, 457, 291, 643, 281, 764, 257, 688, 295, 37649, 4886, 10565, 949, 291, 1223, 264, 777, 2856, 420, 949, 291, 50816], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 515, "seek": 282344, "start": 2833.12, "end": 2837.76, "text": " tease this new algorithm or part that you want to understand and so on. It just when it doesn't", "tokens": [50848, 30444, 341, 777, 9284, 420, 644, 300, 291, 528, 281, 1223, 293, 370, 322, 13, 467, 445, 562, 309, 1177, 380, 51080], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 516, "seek": 282344, "start": 2837.76, "end": 2842.08, "text": " probably turn you into a better programmer, if that is your goal. But for your employer,", "tokens": [51080, 1391, 1261, 291, 666, 257, 1101, 32116, 11, 498, 300, 307, 428, 3387, 13, 583, 337, 428, 16205, 11, 51296], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 517, "seek": 282344, "start": 2842.08, "end": 2846.08, "text": " maybe they don't care whether you're a better programmer, they just want you to turn out these", "tokens": [51296, 1310, 436, 500, 380, 1127, 1968, 291, 434, 257, 1101, 32116, 11, 436, 445, 528, 291, 281, 1261, 484, 613, 51496], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 518, "seek": 282344, "start": 2846.08, "end": 2850.7200000000003, "text": " pages of code and then they run this against the verifier and against the unit test and then are", "tokens": [51496, 7183, 295, 3089, 293, 550, 436, 1190, 341, 1970, 264, 1306, 9902, 293, 1970, 264, 4985, 1500, 293, 550, 366, 51728], "temperature": 0.0, "avg_logprob": -0.11009378575566989, "compression_ratio": 1.8476821192052981, "no_speech_prob": 0.11743413656949997}, {"id": 519, "seek": 285072, "start": 2850.72, "end": 2856.3999999999996, "text": " done, go to the next thing, right? So maybe it's not that important. But the systems that we would", "tokens": [50364, 1096, 11, 352, 281, 264, 958, 551, 11, 558, 30, 407, 1310, 309, 311, 406, 300, 1021, 13, 583, 264, 3652, 300, 321, 576, 50648], "temperature": 0.0, "avg_logprob": -0.12831925564124935, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.01770731434226036}, {"id": 520, "seek": 285072, "start": 2856.3999999999996, "end": 2860.64, "text": " want, what would they look like? I think they need to know what they're doing. You want to have a", "tokens": [50648, 528, 11, 437, 576, 436, 574, 411, 30, 286, 519, 436, 643, 281, 458, 437, 436, 434, 884, 13, 509, 528, 281, 362, 257, 50860], "temperature": 0.0, "avg_logprob": -0.12831925564124935, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.01770731434226036}, {"id": 521, "seek": 285072, "start": 2860.64, "end": 2865.68, "text": " program that is not just able to reproduce something very well in a given context, you want to", "tokens": [50860, 1461, 300, 307, 406, 445, 1075, 281, 29501, 746, 588, 731, 294, 257, 2212, 4319, 11, 291, 528, 281, 51112], "temperature": 0.0, "avg_logprob": -0.12831925564124935, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.01770731434226036}, {"id": 522, "seek": 285072, "start": 2865.68, "end": 2871.3599999999997, "text": " understand the context as deeply as you do or better. So it needs to understand what kind of", "tokens": [51112, 1223, 264, 4319, 382, 8760, 382, 291, 360, 420, 1101, 13, 407, 309, 2203, 281, 1223, 437, 733, 295, 51396], "temperature": 0.0, "avg_logprob": -0.12831925564124935, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.01770731434226036}, {"id": 523, "seek": 285072, "start": 2871.3599999999997, "end": 2876.16, "text": " world it's operating in in the moment. And what itself is, what is it that it can do? What is", "tokens": [51396, 1002, 309, 311, 7447, 294, 294, 264, 1623, 13, 400, 437, 2564, 307, 11, 437, 307, 309, 300, 309, 393, 360, 30, 708, 307, 51636], "temperature": 0.0, "avg_logprob": -0.12831925564124935, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.01770731434226036}, {"id": 524, "seek": 287616, "start": 2876.16, "end": 2881.44, "text": " that what needs to learn still? In some sense, you want systems that are sentient. And it's self", "tokens": [50364, 300, 437, 2203, 281, 1466, 920, 30, 682, 512, 2020, 11, 291, 528, 3652, 300, 366, 2279, 1196, 13, 400, 309, 311, 2698, 50628], "temperature": 0.0, "avg_logprob": -0.16576932157788957, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.022269830107688904}, {"id": 525, "seek": 287616, "start": 2881.44, "end": 2886.16, "text": " like, oh my God, but it just means you have a learning system that's general enough to model", "tokens": [50628, 411, 11, 1954, 452, 1265, 11, 457, 309, 445, 1355, 291, 362, 257, 2539, 1185, 300, 311, 2674, 1547, 281, 2316, 50864], "temperature": 0.0, "avg_logprob": -0.16576932157788957, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.022269830107688904}, {"id": 526, "seek": 287616, "start": 2886.16, "end": 2891.3599999999997, "text": " in principle the entire universe. And this is not as outrageous as it sounds because", "tokens": [50864, 294, 8665, 264, 2302, 6445, 13, 400, 341, 307, 406, 382, 38685, 382, 309, 3263, 570, 51124], "temperature": 0.0, "avg_logprob": -0.16576932157788957, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.022269830107688904}, {"id": 527, "seek": 287616, "start": 2892.48, "end": 2899.2, "text": " Delhi is already dealing with two modalities, language and images. And we will get to video", "tokens": [51180, 26680, 307, 1217, 6260, 365, 732, 1072, 16110, 11, 2856, 293, 5267, 13, 400, 321, 486, 483, 281, 960, 51516], "temperature": 0.0, "avg_logprob": -0.16576932157788957, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.022269830107688904}, {"id": 528, "seek": 287616, "start": 2899.2, "end": 2904.3999999999996, "text": " and we will get to audio connected and you see them early steps in this direction with the Socratic", "tokens": [51516, 293, 321, 486, 483, 281, 6278, 4582, 293, 291, 536, 552, 2440, 4439, 294, 341, 3513, 365, 264, 407, 10757, 2399, 51776], "temperature": 0.0, "avg_logprob": -0.16576932157788957, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.022269830107688904}, {"id": 529, "seek": 290440, "start": 2904.4, "end": 2909.92, "text": " model of people, for instance. So I think that's almost inevitable that this generality will happen.", "tokens": [50364, 2316, 295, 561, 11, 337, 5197, 13, 407, 286, 519, 300, 311, 1920, 21451, 300, 341, 1337, 1860, 486, 1051, 13, 50640], "temperature": 0.0, "avg_logprob": -0.15050396692185175, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.004749750252813101}, {"id": 530, "seek": 290440, "start": 2909.92, "end": 2913.52, "text": " And you will have to add a system to work in real time so it can discover itself.", "tokens": [50640, 400, 291, 486, 362, 281, 909, 257, 1185, 281, 589, 294, 957, 565, 370, 309, 393, 4411, 2564, 13, 50820], "temperature": 0.0, "avg_logprob": -0.15050396692185175, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.004749750252813101}, {"id": 531, "seek": 290440, "start": 2915.6, "end": 2922.48, "text": " I think I think there's something really magic, though, about the creative process here. And also", "tokens": [50924, 286, 519, 286, 519, 456, 311, 746, 534, 5585, 11, 1673, 11, 466, 264, 5880, 1399, 510, 13, 400, 611, 51268], "temperature": 0.0, "avg_logprob": -0.15050396692185175, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.004749750252813101}, {"id": 532, "seek": 290440, "start": 2923.2000000000003, "end": 2927.92, "text": " the prompt engineering is another thing we can talk about. But Kenneth Stanley once made this", "tokens": [51304, 264, 12391, 7043, 307, 1071, 551, 321, 393, 751, 466, 13, 583, 33735, 28329, 1564, 1027, 341, 51540], "temperature": 0.0, "avg_logprob": -0.15050396692185175, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.004749750252813101}, {"id": 533, "seek": 290440, "start": 2927.92, "end": 2933.84, "text": " thing called Pick Breeder. And you could essentially distribute the selection of these images", "tokens": [51540, 551, 1219, 14129, 7090, 10020, 13, 400, 291, 727, 4476, 20594, 264, 9450, 295, 613, 5267, 51836], "temperature": 0.0, "avg_logprob": -0.15050396692185175, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.004749750252813101}, {"id": 534, "seek": 293384, "start": 2933.84, "end": 2940.48, "text": " created with CPPNs, Compositional Pattern Producing Networks. And you would just get these beautiful", "tokens": [50364, 2942, 365, 383, 17755, 45, 82, 11, 6620, 329, 2628, 34367, 77, 11793, 2175, 12640, 82, 13, 400, 291, 576, 445, 483, 613, 2238, 50696], "temperature": 0.0, "avg_logprob": -0.11613955668040685, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.0010877939639613032}, {"id": 535, "seek": 293384, "start": 2940.48, "end": 2946.32, "text": " images. They were they were so incredibly, incredibly diverse and interesting. So it's", "tokens": [50696, 5267, 13, 814, 645, 436, 645, 370, 6252, 11, 6252, 9521, 293, 1880, 13, 407, 309, 311, 50988], "temperature": 0.0, "avg_logprob": -0.11613955668040685, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.0010877939639613032}, {"id": 536, "seek": 293384, "start": 2946.32, "end": 2951.92, "text": " not that the algorithms were intelligent, there was something magic about the externalized process.", "tokens": [50988, 406, 300, 264, 14642, 645, 13232, 11, 456, 390, 746, 5585, 466, 264, 8320, 1602, 1399, 13, 51268], "temperature": 0.0, "avg_logprob": -0.11613955668040685, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.0010877939639613032}, {"id": 537, "seek": 293384, "start": 2951.92, "end": 2955.84, "text": " And what's really interesting about these models like Dali, for example, is that creativity has", "tokens": [51268, 400, 437, 311, 534, 1880, 466, 613, 5245, 411, 413, 5103, 11, 337, 1365, 11, 307, 300, 12915, 575, 51464], "temperature": 0.0, "avg_logprob": -0.11613955668040685, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.0010877939639613032}, {"id": 538, "seek": 293384, "start": 2955.84, "end": 2961.92, "text": " been distilled down to a raw idea in your head, right? So for example, I might decide to mix", "tokens": [51464, 668, 1483, 6261, 760, 281, 257, 8936, 1558, 294, 428, 1378, 11, 558, 30, 407, 337, 1365, 11, 286, 1062, 4536, 281, 2890, 51768], "temperature": 0.0, "avg_logprob": -0.11613955668040685, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.0010877939639613032}, {"id": 539, "seek": 296192, "start": 2961.92, "end": 2966.08, "text": " the style of two artists and combine them with a new subject. And I want a black cat", "tokens": [50364, 264, 3758, 295, 732, 6910, 293, 10432, 552, 365, 257, 777, 3983, 13, 400, 286, 528, 257, 2211, 3857, 50572], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 540, "seek": 296192, "start": 2966.7200000000003, "end": 2971.6, "text": " in front of Royal Holloway University in the style of cyberpunk. I've been doing that all day.", "tokens": [50604, 294, 1868, 295, 12717, 46731, 320, 3535, 294, 264, 3758, 295, 13411, 27133, 13, 286, 600, 668, 884, 300, 439, 786, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 541, "seek": 296192, "start": 2971.6, "end": 2975.92, "text": " And the technical process is now done for you. The only limit is your imagination. So just like", "tokens": [50848, 400, 264, 6191, 1399, 307, 586, 1096, 337, 291, 13, 440, 787, 4948, 307, 428, 12938, 13, 407, 445, 411, 51064], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 542, "seek": 296192, "start": 2975.92, "end": 2980.48, "text": " Kenneth Stanley's Pick Breeder, creativity itself has now become this uber efficient and", "tokens": [51064, 33735, 28329, 311, 14129, 7090, 10020, 11, 12915, 2564, 575, 586, 1813, 341, 344, 607, 7148, 293, 51292], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 543, "seek": 296192, "start": 2980.48, "end": 2986.32, "text": " externalized process. I think it's unreal. But the thing is, like the reason I never thought GPT3", "tokens": [51292, 8320, 1602, 1399, 13, 286, 519, 309, 311, 25754, 13, 583, 264, 551, 307, 11, 411, 264, 1778, 286, 1128, 1194, 26039, 51, 18, 51584], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 544, "seek": 296192, "start": 2986.32, "end": 2991.76, "text": " was intelligent is because it can't be used non interactively. The magic must happen when it's", "tokens": [51584, 390, 13232, 307, 570, 309, 393, 380, 312, 1143, 2107, 4648, 3413, 13, 440, 5585, 1633, 1051, 562, 309, 311, 51856], "temperature": 0.0, "avg_logprob": -0.08259898073533002, "compression_ratio": 1.586894586894587, "no_speech_prob": 0.004431781824678183}, {"id": 545, "seek": 299176, "start": 2991.76, "end": 2998.8, "text": " used by humans interactively. Well, you can basically build a machine that is generating", "tokens": [50364, 1143, 538, 6255, 4648, 3413, 13, 1042, 11, 291, 393, 1936, 1322, 257, 3479, 300, 307, 17746, 50716], "temperature": 0.0, "avg_logprob": -0.06851379163972623, "compression_ratio": 1.6875, "no_speech_prob": 0.0016733435913920403}, {"id": 546, "seek": 299176, "start": 2998.8, "end": 3007.44, "text": " prompts for GPT3. So in principle, you can build a robot that has a vision to text module. And that", "tokens": [50716, 41095, 337, 26039, 51, 18, 13, 407, 294, 8665, 11, 291, 393, 1322, 257, 7881, 300, 575, 257, 5201, 281, 2487, 10088, 13, 400, 300, 51148], "temperature": 0.0, "avg_logprob": -0.06851379163972623, "compression_ratio": 1.6875, "no_speech_prob": 0.0016733435913920403}, {"id": 547, "seek": 299176, "start": 3007.44, "end": 3012.88, "text": " is used to prompt GPT3 into generating a story about a robot who sees these things and interact", "tokens": [51148, 307, 1143, 281, 12391, 26039, 51, 18, 666, 17746, 257, 1657, 466, 257, 7881, 567, 8194, 613, 721, 293, 4648, 51420], "temperature": 0.0, "avg_logprob": -0.06851379163972623, "compression_ratio": 1.6875, "no_speech_prob": 0.0016733435913920403}, {"id": 548, "seek": 299176, "start": 3012.88, "end": 3019.44, "text": " with them. And then you take the output of the generative model and translate this using text", "tokens": [51420, 365, 552, 13, 400, 550, 291, 747, 264, 5598, 295, 264, 1337, 1166, 2316, 293, 13799, 341, 1228, 2487, 51748], "temperature": 0.0, "avg_logprob": -0.06851379163972623, "compression_ratio": 1.6875, "no_speech_prob": 0.0016733435913920403}, {"id": 549, "seek": 301944, "start": 3019.44, "end": 3027.68, "text": " to motor module. And in this way, you close the loop. And I just used it as a thought experiment", "tokens": [50364, 281, 5932, 10088, 13, 400, 294, 341, 636, 11, 291, 1998, 264, 6367, 13, 400, 286, 445, 1143, 309, 382, 257, 1194, 5120, 50776], "temperature": 0.0, "avg_logprob": -0.17676499911717006, "compression_ratio": 1.75, "no_speech_prob": 0.0060924626886844635}, {"id": 550, "seek": 301944, "start": 3027.68, "end": 3033.92, "text": " to think about the limitations of embodiment for such systems. Second is essentially doing that.", "tokens": [50776, 281, 519, 466, 264, 15705, 295, 28935, 2328, 337, 1270, 3652, 13, 5736, 307, 4476, 884, 300, 13, 51088], "temperature": 0.0, "avg_logprob": -0.17676499911717006, "compression_ratio": 1.75, "no_speech_prob": 0.0060924626886844635}, {"id": 551, "seek": 301944, "start": 3034.7200000000003, "end": 3040.2400000000002, "text": " So somebody has made this happen. And even with the language model, it works to some degree.", "tokens": [51128, 407, 2618, 575, 1027, 341, 1051, 13, 400, 754, 365, 264, 2856, 2316, 11, 309, 1985, 281, 512, 4314, 13, 51404], "temperature": 0.0, "avg_logprob": -0.17676499911717006, "compression_ratio": 1.75, "no_speech_prob": 0.0060924626886844635}, {"id": 552, "seek": 301944, "start": 3040.2400000000002, "end": 3044.48, "text": " And we know that we don't want to do this with natural language because natural language is a", "tokens": [51404, 400, 321, 458, 300, 321, 500, 380, 528, 281, 360, 341, 365, 3303, 2856, 570, 3303, 2856, 307, 257, 51616], "temperature": 0.0, "avg_logprob": -0.17676499911717006, "compression_ratio": 1.75, "no_speech_prob": 0.0060924626886844635}, {"id": 553, "seek": 301944, "start": 3044.48, "end": 3049.28, "text": " crutch. These systems make up for this, but you're just using more natural language faster than", "tokens": [51616, 941, 9349, 13, 1981, 3652, 652, 493, 337, 341, 11, 457, 291, 434, 445, 1228, 544, 3303, 2856, 4663, 813, 51856], "temperature": 0.0, "avg_logprob": -0.17676499911717006, "compression_ratio": 1.75, "no_speech_prob": 0.0060924626886844635}, {"id": 554, "seek": 304928, "start": 3049.28, "end": 3054.96, "text": " people could use it. But there is some language of thought that we are using that is not learned,", "tokens": [50364, 561, 727, 764, 309, 13, 583, 456, 307, 512, 2856, 295, 1194, 300, 321, 366, 1228, 300, 307, 406, 3264, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1426776885986328, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.00439620204269886}, {"id": 555, "seek": 304928, "start": 3054.96, "end": 3059.84, "text": " but discovered by our own mind that we converge on, that is much more efficient. And this language", "tokens": [50648, 457, 6941, 538, 527, 1065, 1575, 300, 321, 41881, 322, 11, 300, 307, 709, 544, 7148, 13, 400, 341, 2856, 50892], "temperature": 0.0, "avg_logprob": -0.1426776885986328, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.00439620204269886}, {"id": 556, "seek": 304928, "start": 3059.84, "end": 3064.88, "text": " of thought seems to be able to bottom out and perceptual distributed representations that are", "tokens": [50892, 295, 1194, 2544, 281, 312, 1075, 281, 2767, 484, 293, 43276, 901, 12631, 33358, 300, 366, 51144], "temperature": 0.0, "avg_logprob": -0.1426776885986328, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.00439620204269886}, {"id": 557, "seek": 304928, "start": 3064.88, "end": 3069.2000000000003, "text": " unprincipled, like these neural networks are in a sense unprincipled, but they don't break.", "tokens": [51144, 517, 1424, 21961, 15551, 11, 411, 613, 18161, 9590, 366, 294, 257, 2020, 517, 1424, 21961, 15551, 11, 457, 436, 500, 380, 1821, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1426776885986328, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.00439620204269886}, {"id": 558, "seek": 304928, "start": 3069.2000000000003, "end": 3073.84, "text": " Then there is something that is vague and ambiguous and has small contradictions in it.", "tokens": [51360, 1396, 456, 307, 746, 300, 307, 24247, 293, 39465, 293, 575, 1359, 15858, 15607, 294, 309, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1426776885986328, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.00439620204269886}, {"id": 559, "seek": 307384, "start": 3073.84, "end": 3079.84, "text": " But at some level, it also is able to emulate very principal logic very well and becomes very", "tokens": [50364, 583, 412, 512, 1496, 11, 309, 611, 307, 1075, 281, 45497, 588, 9716, 9952, 588, 731, 293, 3643, 588, 50664], "temperature": 0.0, "avg_logprob": -0.1381668468098064, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.001925610820762813}, {"id": 560, "seek": 307384, "start": 3079.84, "end": 3086.0, "text": " sparse and very powerful in expressing things concisely. And this very concise language of thought", "tokens": [50664, 637, 11668, 293, 588, 4005, 294, 22171, 721, 1588, 271, 736, 13, 400, 341, 588, 44882, 2856, 295, 1194, 50972], "temperature": 0.0, "avg_logprob": -0.1381668468098064, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.001925610820762813}, {"id": 561, "seek": 307384, "start": 3086.0, "end": 3092.88, "text": " is so don't see it in our models. What Dali is doing, Dali 2, is that it combines the language", "tokens": [50972, 307, 370, 500, 380, 536, 309, 294, 527, 5245, 13, 708, 413, 5103, 307, 884, 11, 413, 5103, 568, 11, 307, 300, 309, 29520, 264, 2856, 51316], "temperature": 0.0, "avg_logprob": -0.1381668468098064, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.001925610820762813}, {"id": 562, "seek": 307384, "start": 3092.88, "end": 3098.56, "text": " model and division model using embedding spaces. And these embedding spaces basically project all", "tokens": [51316, 2316, 293, 10044, 2316, 1228, 12240, 3584, 7673, 13, 400, 613, 12240, 3584, 7673, 1936, 1716, 439, 51600], "temperature": 0.0, "avg_logprob": -0.1381668468098064, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.001925610820762813}, {"id": 563, "seek": 309856, "start": 3098.56, "end": 3103.36, "text": " the concepts into some high dimensional manifold and find similarities between them.", "tokens": [50364, 264, 10392, 666, 512, 1090, 18795, 47138, 293, 915, 24197, 1296, 552, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11618078739271251, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.027570992708206177}, {"id": 564, "seek": 309856, "start": 3104.48, "end": 3110.96, "text": " And Gary Marcus points out that there is an issue with compositionality in this. So you need to", "tokens": [50660, 400, 13788, 26574, 2793, 484, 300, 456, 307, 364, 2734, 365, 10199, 2628, 507, 294, 341, 13, 407, 291, 643, 281, 50984], "temperature": 0.0, "avg_logprob": -0.11618078739271251, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.027570992708206177}, {"id": 565, "seek": 309856, "start": 3110.96, "end": 3116.48, "text": " find the semantic structure of a sentence that is made of a hierarchy of concepts. And this is easy", "tokens": [50984, 915, 264, 47982, 3877, 295, 257, 8174, 300, 307, 1027, 295, 257, 22333, 295, 10392, 13, 400, 341, 307, 1858, 51260], "temperature": 0.0, "avg_logprob": -0.11618078739271251, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.027570992708206177}, {"id": 566, "seek": 309856, "start": 3116.48, "end": 3121.12, "text": " to do with the grammar. And it's much harder to do this with a deep learning system that needs to", "tokens": [51260, 281, 360, 365, 264, 22317, 13, 400, 309, 311, 709, 6081, 281, 360, 341, 365, 257, 2452, 2539, 1185, 300, 2203, 281, 51492], "temperature": 0.0, "avg_logprob": -0.11618078739271251, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.027570992708206177}, {"id": 567, "seek": 309856, "start": 3121.12, "end": 3126.96, "text": " discover this in a way and structure this space in the right way. It's not impossible. So when", "tokens": [51492, 4411, 341, 294, 257, 636, 293, 3877, 341, 1901, 294, 264, 558, 636, 13, 467, 311, 406, 6243, 13, 407, 562, 51784], "temperature": 0.0, "avg_logprob": -0.11618078739271251, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.027570992708206177}, {"id": 568, "seek": 312696, "start": 3126.96, "end": 3131.76, "text": " Gary Marcus says these models cannot do this and cannot learn it, he is probably wrong.", "tokens": [50364, 13788, 26574, 1619, 613, 5245, 2644, 360, 341, 293, 2644, 1466, 309, 11, 415, 307, 1391, 2085, 13, 50604], "temperature": 0.0, "avg_logprob": -0.13509706422394396, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0009682986419647932}, {"id": 569, "seek": 312696, "start": 3132.32, "end": 3137.68, "text": " But I think he is right in the sense that this is something that is much, much harder for the", "tokens": [50632, 583, 286, 519, 415, 307, 558, 294, 264, 2020, 300, 341, 307, 746, 300, 307, 709, 11, 709, 6081, 337, 264, 50900], "temperature": 0.0, "avg_logprob": -0.13509706422394396, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0009682986419647932}, {"id": 570, "seek": 312696, "start": 3137.68, "end": 3142.64, "text": " current approaches. They need dramatic training data than a human being. And the algorithms are", "tokens": [50900, 2190, 11587, 13, 814, 643, 12023, 3097, 1412, 813, 257, 1952, 885, 13, 400, 264, 14642, 366, 51148], "temperature": 0.0, "avg_logprob": -0.13509706422394396, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0009682986419647932}, {"id": 571, "seek": 312696, "start": 3142.64, "end": 3148.7200000000003, "text": " not doing this naturally. So there are probably ways in which we could make this happen much more", "tokens": [51148, 406, 884, 341, 8195, 13, 407, 456, 366, 1391, 2098, 294, 597, 321, 727, 652, 341, 1051, 709, 544, 51452], "temperature": 0.0, "avg_logprob": -0.13509706422394396, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0009682986419647932}, {"id": 572, "seek": 312696, "start": 3148.7200000000003, "end": 3152.88, "text": " elegantly and quickly and converge, for instance, on models for arithmetic.", "tokens": [51452, 14459, 3627, 293, 2661, 293, 41881, 11, 337, 5197, 11, 322, 5245, 337, 42973, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13509706422394396, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0009682986419647932}, {"id": 573, "seek": 315288, "start": 3153.36, "end": 3159.52, "text": " That's right. I mean, I remember I read a really good Twitter thread, I think it was by Raphael", "tokens": [50388, 663, 311, 558, 13, 286, 914, 11, 286, 1604, 286, 1401, 257, 534, 665, 5794, 7207, 11, 286, 519, 309, 390, 538, 49690, 338, 50696], "temperature": 0.0, "avg_logprob": -0.16000487123216903, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.010009350255131721}, {"id": 574, "seek": 315288, "start": 3159.52, "end": 3163.92, "text": " Millier, you know, about compositionality of these large generative vision models. Because usually", "tokens": [50696, 7190, 811, 11, 291, 458, 11, 466, 10199, 2628, 507, 295, 613, 2416, 1337, 1166, 5201, 5245, 13, 1436, 2673, 50916], "temperature": 0.0, "avg_logprob": -0.16000487123216903, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.010009350255131721}, {"id": 575, "seek": 315288, "start": 3163.92, "end": 3170.7200000000003, "text": " compositionality is referred to in respect of language models. I think Raphael said that the", "tokens": [50916, 10199, 2628, 507, 307, 10839, 281, 294, 3104, 295, 2856, 5245, 13, 286, 519, 49690, 338, 848, 300, 264, 51256], "temperature": 0.0, "avg_logprob": -0.16000487123216903, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.010009350255131721}, {"id": 576, "seek": 315288, "start": 3170.7200000000003, "end": 3175.12, "text": " assessment of the claim is complicated by the fact that people differ in their understanding of what", "tokens": [51256, 9687, 295, 264, 3932, 307, 6179, 538, 264, 1186, 300, 561, 743, 294, 641, 3701, 295, 437, 51476], "temperature": 0.0, "avg_logprob": -0.16000487123216903, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.010009350255131721}, {"id": 577, "seek": 315288, "start": 3175.12, "end": 3180.56, "text": " compositionality means. But if language is compositional, as you say, and thought is language", "tokens": [51476, 10199, 2628, 507, 1355, 13, 583, 498, 2856, 307, 10199, 2628, 11, 382, 291, 584, 11, 293, 1194, 307, 2856, 51748], "temperature": 0.0, "avg_logprob": -0.16000487123216903, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.010009350255131721}, {"id": 578, "seek": 318056, "start": 3180.72, "end": 3185.2, "text": " as argued by the proponents of this language of thought hypothesis, I think Raphael said", "tokens": [50372, 382, 20219, 538, 264, 2365, 40496, 295, 341, 2856, 295, 1194, 17291, 11, 286, 519, 49690, 338, 848, 50596], "temperature": 0.0, "avg_logprob": -0.09208107816761937, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0064587462693452835}, {"id": 579, "seek": 318056, "start": 3185.2, "end": 3190.24, "text": " that he thought language itself should be compositional in a similar sense. And perhaps", "tokens": [50596, 300, 415, 1194, 2856, 2564, 820, 312, 10199, 2628, 294, 257, 2531, 2020, 13, 400, 4317, 50848], "temperature": 0.0, "avg_logprob": -0.09208107816761937, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0064587462693452835}, {"id": 580, "seek": 318056, "start": 3190.24, "end": 3196.24, "text": " by extension, visual imagery should be compositional. So I think Gary was arguing in a nutshell that", "tokens": [50848, 538, 10320, 11, 5056, 24340, 820, 312, 10199, 2628, 13, 407, 286, 519, 13788, 390, 19697, 294, 257, 37711, 300, 51148], "temperature": 0.0, "avg_logprob": -0.09208107816761937, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0064587462693452835}, {"id": 581, "seek": 318056, "start": 3196.24, "end": 3201.2799999999997, "text": " it's hard to go from the image, or let's say the utterance, if it's NLP, to the structure,", "tokens": [51148, 309, 311, 1152, 281, 352, 490, 264, 3256, 11, 420, 718, 311, 584, 264, 17567, 719, 11, 498, 309, 311, 426, 45196, 11, 281, 264, 3877, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09208107816761937, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0064587462693452835}, {"id": 582, "seek": 318056, "start": 3201.2799999999997, "end": 3205.92, "text": " or the grammar, or the constituents. It's much easier to go the other way around, would you agree?", "tokens": [51400, 420, 264, 22317, 11, 420, 264, 30847, 13, 467, 311, 709, 3571, 281, 352, 264, 661, 636, 926, 11, 576, 291, 3986, 30, 51632], "temperature": 0.0, "avg_logprob": -0.09208107816761937, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0064587462693452835}, {"id": 583, "seek": 320592, "start": 3206.88, "end": 3210.96, "text": " The issue is that language of thought is executable. And natural language is not.", "tokens": [50412, 440, 2734, 307, 300, 2856, 295, 1194, 307, 7568, 712, 13, 400, 3303, 2856, 307, 406, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11357544018672062, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.005300601478666067}, {"id": 584, "seek": 320592, "start": 3212.0, "end": 3217.2000000000003, "text": " We execute natural language by translating it into our mind in something that we can execute.", "tokens": [50668, 492, 14483, 3303, 2856, 538, 35030, 309, 666, 527, 1575, 294, 746, 300, 321, 393, 14483, 13, 50928], "temperature": 0.0, "avg_logprob": -0.11357544018672062, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.005300601478666067}, {"id": 585, "seek": 320592, "start": 3217.84, "end": 3222.2400000000002, "text": " And the reason about code, you might use natural language to support your reasoning.", "tokens": [50960, 400, 264, 1778, 466, 3089, 11, 291, 1062, 764, 3303, 2856, 281, 1406, 428, 21577, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11357544018672062, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.005300601478666067}, {"id": 586, "seek": 320592, "start": 3222.2400000000002, "end": 3227.28, "text": " But the code that you build in your mind is filled in some kind of abstract syntax tree that you", "tokens": [51180, 583, 264, 3089, 300, 291, 1322, 294, 428, 1575, 307, 6412, 294, 512, 733, 295, 12649, 28431, 4230, 300, 291, 51432], "temperature": 0.0, "avg_logprob": -0.11357544018672062, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.005300601478666067}, {"id": 587, "seek": 320592, "start": 3227.28, "end": 3232.48, "text": " can actually execute in your mind to some degree. And then you get a sense of the output. So you", "tokens": [51432, 393, 767, 14483, 294, 428, 1575, 281, 512, 4314, 13, 400, 550, 291, 483, 257, 2020, 295, 264, 5598, 13, 407, 291, 51692], "temperature": 0.0, "avg_logprob": -0.11357544018672062, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.005300601478666067}, {"id": 588, "seek": 323248, "start": 3232.48, "end": 3238.16, "text": " entrain your own brain with an executable structure. And this executable structure has", "tokens": [50364, 948, 7146, 428, 1065, 3567, 365, 364, 7568, 712, 3877, 13, 400, 341, 7568, 712, 3877, 575, 50648], "temperature": 0.0, "avg_logprob": -0.11941015278851544, "compression_ratio": 1.8616600790513833, "no_speech_prob": 0.006097091361880302}, {"id": 589, "seek": 323248, "start": 3238.16, "end": 3243.76, "text": " properties that are quite similar to the ones that the compiler has in your computer. So you", "tokens": [50648, 7221, 300, 366, 1596, 2531, 281, 264, 2306, 300, 264, 31958, 575, 294, 428, 3820, 13, 407, 291, 50928], "temperature": 0.0, "avg_logprob": -0.11941015278851544, "compression_ratio": 1.8616600790513833, "no_speech_prob": 0.006097091361880302}, {"id": 590, "seek": 323248, "start": 3243.76, "end": 3247.68, "text": " can anticipate what the compiler is going to do with your code. You're not going to do this with", "tokens": [50928, 393, 21685, 437, 264, 31958, 307, 516, 281, 360, 365, 428, 3089, 13, 509, 434, 406, 516, 281, 360, 341, 365, 51124], "temperature": 0.0, "avg_logprob": -0.11941015278851544, "compression_ratio": 1.8616600790513833, "no_speech_prob": 0.006097091361880302}, {"id": 591, "seek": 323248, "start": 3247.68, "end": 3252.32, "text": " all the depths that your compiler do it, you might still have to run your code, but you will find", "tokens": [51124, 439, 264, 28439, 300, 428, 31958, 360, 309, 11, 291, 1062, 920, 362, 281, 1190, 428, 3089, 11, 457, 291, 486, 915, 51356], "temperature": 0.0, "avg_logprob": -0.11941015278851544, "compression_ratio": 1.8616600790513833, "no_speech_prob": 0.006097091361880302}, {"id": 592, "seek": 323248, "start": 3252.32, "end": 3258.16, "text": " when you want to experience programming, your stuff will usually run. So our language of thought", "tokens": [51356, 562, 291, 528, 281, 1752, 9410, 11, 428, 1507, 486, 2673, 1190, 13, 407, 527, 2856, 295, 1194, 51648], "temperature": 0.0, "avg_logprob": -0.11941015278851544, "compression_ratio": 1.8616600790513833, "no_speech_prob": 0.006097091361880302}, {"id": 593, "seek": 325816, "start": 3258.16, "end": 3262.8799999999997, "text": " can do this, it can execute stuff. And it's not just a machine neural network that guesses", "tokens": [50364, 393, 360, 341, 11, 309, 393, 14483, 1507, 13, 400, 309, 311, 406, 445, 257, 3479, 18161, 3209, 300, 42703, 50600], "temperature": 0.0, "avg_logprob": -0.11772186065388617, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.022275537252426147}, {"id": 594, "seek": 325816, "start": 3262.8799999999997, "end": 3268.08, "text": " what the outcome is going to be and is right some of the time. But it gets pretty good at", "tokens": [50600, 437, 264, 9700, 307, 516, 281, 312, 293, 307, 558, 512, 295, 264, 565, 13, 583, 309, 2170, 1238, 665, 412, 50860], "temperature": 0.0, "avg_logprob": -0.11772186065388617, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.022275537252426147}, {"id": 595, "seek": 325816, "start": 3268.08, "end": 3273.3599999999997, "text": " figuring this out. And this means that it has to build this compositional structure that has some", "tokens": [50860, 15213, 341, 484, 13, 400, 341, 1355, 300, 309, 575, 281, 1322, 341, 10199, 2628, 3877, 300, 575, 512, 51124], "temperature": 0.0, "avg_logprob": -0.11772186065388617, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.022275537252426147}, {"id": 596, "seek": 325816, "start": 3273.3599999999997, "end": 3278.8799999999997, "text": " verifiable properties. And we observe ourselves operating on this verification process, right?", "tokens": [51124, 1306, 30876, 7221, 13, 400, 321, 11441, 4175, 7447, 322, 341, 30206, 1399, 11, 558, 30, 51400], "temperature": 0.0, "avg_logprob": -0.11772186065388617, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.022275537252426147}, {"id": 597, "seek": 325816, "start": 3278.8799999999997, "end": 3284.72, "text": " When we do introspection for the program, we observe ourselves how we direct our attention on", "tokens": [51400, 1133, 321, 360, 560, 2635, 19997, 337, 264, 1461, 11, 321, 11441, 4175, 577, 321, 2047, 527, 3202, 322, 51692], "temperature": 0.0, "avg_logprob": -0.11772186065388617, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.022275537252426147}, {"id": 598, "seek": 328472, "start": 3284.72, "end": 3292.08, "text": " making proofs. And this attentional algorithm that works in real time, that is making changes on", "tokens": [50364, 1455, 8177, 82, 13, 400, 341, 3202, 304, 9284, 300, 1985, 294, 957, 565, 11, 300, 307, 1455, 2962, 322, 50732], "temperature": 0.0, "avg_logprob": -0.10226332813227942, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.02063268981873989}, {"id": 599, "seek": 328472, "start": 3292.08, "end": 3298.3199999999997, "text": " your mental models and then predicts the outcome of these changes and compares this with what your", "tokens": [50732, 428, 4973, 5245, 293, 550, 6069, 82, 264, 9700, 295, 613, 2962, 293, 38334, 341, 365, 437, 428, 51044], "temperature": 0.0, "avg_logprob": -0.10226332813227942, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.02063268981873989}, {"id": 600, "seek": 328472, "start": 3298.3199999999997, "end": 3303.2, "text": " mental computations give you and then fixes your models of how your own thinking process in this", "tokens": [51044, 4973, 2807, 763, 976, 291, 293, 550, 32539, 428, 5245, 295, 577, 428, 1065, 1953, 1399, 294, 341, 51288], "temperature": 0.0, "avg_logprob": -0.10226332813227942, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.02063268981873989}, {"id": 601, "seek": 328472, "start": 3303.2, "end": 3308.8799999999997, "text": " domain works and so on. You can observe yourself doing that. And it's nothing where I would say", "tokens": [51288, 9274, 1985, 293, 370, 322, 13, 509, 393, 11441, 1803, 884, 300, 13, 400, 309, 311, 1825, 689, 286, 576, 584, 51572], "temperature": 0.0, "avg_logprob": -0.10226332813227942, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.02063268981873989}, {"id": 602, "seek": 328472, "start": 3308.8799999999997, "end": 3313.8399999999997, "text": " a given approach or the given approaches that we have will never get there. But there seem to be", "tokens": [51572, 257, 2212, 3109, 420, 264, 2212, 11587, 300, 321, 362, 486, 1128, 483, 456, 13, 583, 456, 1643, 281, 312, 51820], "temperature": 0.0, "avg_logprob": -0.10226332813227942, "compression_ratio": 1.8097014925373134, "no_speech_prob": 0.02063268981873989}, {"id": 603, "seek": 331384, "start": 3313.84, "end": 3318.4, "text": " ways in which we have just barely scratched the surface in what you need to be doing to make", "tokens": [50364, 2098, 294, 597, 321, 362, 445, 10268, 40513, 264, 3753, 294, 437, 291, 643, 281, 312, 884, 281, 652, 50592], "temperature": 0.0, "avg_logprob": -0.12259637741815477, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.005585650447756052}, {"id": 604, "seek": 331384, "start": 3318.4, "end": 3322.7200000000003, "text": " these models sample efficient and sparse and more adequate to model domains you're interested in.", "tokens": [50592, 613, 5245, 6889, 7148, 293, 637, 11668, 293, 544, 20927, 281, 2316, 25514, 291, 434, 3102, 294, 13, 50808], "temperature": 0.0, "avg_logprob": -0.12259637741815477, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.005585650447756052}, {"id": 605, "seek": 331384, "start": 3325.1200000000003, "end": 3330.48, "text": " Cool. Okay. Well, I mean, just to finish like the discussion about the OpenAI stuff, I mean,", "tokens": [50928, 8561, 13, 1033, 13, 1042, 11, 286, 914, 11, 445, 281, 2413, 411, 264, 5017, 466, 264, 7238, 48698, 1507, 11, 286, 914, 11, 51196], "temperature": 0.0, "avg_logprob": -0.12259637741815477, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.005585650447756052}, {"id": 606, "seek": 331384, "start": 3330.48, "end": 3335.28, "text": " I agree with the prognosticators. And I do think that these large language models and", "tokens": [51196, 286, 3986, 365, 264, 447, 4568, 19634, 3391, 13, 400, 286, 360, 519, 300, 613, 2416, 2856, 5245, 293, 51436], "temperature": 0.0, "avg_logprob": -0.12259637741815477, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.005585650447756052}, {"id": 607, "seek": 331384, "start": 3335.28, "end": 3338.7200000000003, "text": " these visual generative models will be revolutionary for some domains. But", "tokens": [51436, 613, 5056, 1337, 1166, 5245, 486, 312, 22687, 337, 512, 25514, 13, 583, 51608], "temperature": 0.0, "avg_logprob": -0.12259637741815477, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.005585650447756052}, {"id": 608, "seek": 333872, "start": 3339.2799999999997, "end": 3344.72, "text": " you know, I think you really need to have a human guiding the creative process, which is a huge", "tokens": [50392, 291, 458, 11, 286, 519, 291, 534, 643, 281, 362, 257, 1952, 25061, 264, 5880, 1399, 11, 597, 307, 257, 2603, 50664], "temperature": 0.0, "avg_logprob": -0.20689735412597657, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.00504053570330143}, {"id": 609, "seek": 333872, "start": 3344.72, "end": 3349.7599999999998, "text": " limitation. But I think it could also potentially hint at what intelligence actually is, right?", "tokens": [50664, 27432, 13, 583, 286, 519, 309, 727, 611, 7263, 12075, 412, 437, 7599, 767, 307, 11, 558, 30, 50916], "temperature": 0.0, "avg_logprob": -0.20689735412597657, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.00504053570330143}, {"id": 610, "seek": 333872, "start": 3349.7599999999998, "end": 3355.3599999999997, "text": " I think intelligence might be this externalized process in a cybernetic sense, if you like,", "tokens": [50916, 286, 519, 7599, 1062, 312, 341, 8320, 1602, 1399, 294, 257, 13411, 77, 3532, 2020, 11, 498, 291, 411, 11, 51196], "temperature": 0.0, "avg_logprob": -0.20689735412597657, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.00504053570330143}, {"id": 611, "seek": 333872, "start": 3355.3599999999997, "end": 3360.16, "text": " this idea of intelligence being fully embedded in an algorithm in a single agent might be", "tokens": [51196, 341, 1558, 295, 7599, 885, 4498, 16741, 294, 364, 9284, 294, 257, 2167, 9461, 1062, 312, 51436], "temperature": 0.0, "avg_logprob": -0.20689735412597657, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.00504053570330143}, {"id": 612, "seek": 333872, "start": 3361.04, "end": 3362.7999999999997, "text": " the wrong way to think about it.", "tokens": [51480, 264, 2085, 636, 281, 519, 466, 309, 13, 51568], "temperature": 0.0, "avg_logprob": -0.20689735412597657, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.00504053570330143}, {"id": 613, "seek": 336280, "start": 3363.44, "end": 3368.48, "text": " I think that humans, by and large, are very confused. Very often you need a human to guide a", "tokens": [50396, 286, 519, 300, 6255, 11, 538, 293, 2416, 11, 366, 588, 9019, 13, 4372, 2049, 291, 643, 257, 1952, 281, 5934, 257, 50648], "temperature": 0.0, "avg_logprob": -0.15916558901468914, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.029742972925305367}, {"id": 614, "seek": 336280, "start": 3368.48, "end": 3374.1600000000003, "text": " human, right? And then you ask yourself, if you do this recursively, does this society know where", "tokens": [50648, 1952, 11, 558, 30, 400, 550, 291, 1029, 1803, 11, 498, 291, 360, 341, 20560, 3413, 11, 775, 341, 4086, 458, 689, 50932], "temperature": 0.0, "avg_logprob": -0.15916558901468914, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.029742972925305367}, {"id": 615, "seek": 336280, "start": 3374.1600000000003, "end": 3379.36, "text": " it's going? Or is this at some level confusion at all levels that is balancing each other?", "tokens": [50932, 309, 311, 516, 30, 1610, 307, 341, 412, 512, 1496, 15075, 412, 439, 4358, 300, 307, 22495, 1184, 661, 30, 51192], "temperature": 0.0, "avg_logprob": -0.15916558901468914, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.029742972925305367}, {"id": 616, "seek": 336280, "start": 3379.92, "end": 3385.1200000000003, "text": " So there seem to be very few people with a plan right now. And it's quite apparent that we see", "tokens": [51220, 407, 456, 1643, 281, 312, 588, 1326, 561, 365, 257, 1393, 558, 586, 13, 400, 309, 311, 1596, 18335, 300, 321, 536, 51480], "temperature": 0.0, "avg_logprob": -0.15916558901468914, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.029742972925305367}, {"id": 617, "seek": 336280, "start": 3385.1200000000003, "end": 3389.92, "text": " that in the sciences, we see this in politics, we need an art and literature. It is that humans", "tokens": [51480, 300, 294, 264, 17677, 11, 321, 536, 341, 294, 7341, 11, 321, 643, 364, 1523, 293, 10394, 13, 467, 307, 300, 6255, 51720], "temperature": 0.0, "avg_logprob": -0.15916558901468914, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.029742972925305367}, {"id": 618, "seek": 338992, "start": 3389.92, "end": 3395.44, "text": " have a higher degree of sentience. But by and large, very few people have a principal plan on", "tokens": [50364, 362, 257, 2946, 4314, 295, 2279, 1182, 13, 583, 538, 293, 2416, 11, 588, 1326, 561, 362, 257, 9716, 1393, 322, 50640], "temperature": 0.0, "avg_logprob": -0.12913388353053148, "compression_ratio": 1.6088435374149659, "no_speech_prob": 0.01470670010894537}, {"id": 619, "seek": 338992, "start": 3395.44, "end": 3401.84, "text": " how to build a sustainable, harmonic world. And if you set an AI system to this task, it might", "tokens": [50640, 577, 281, 1322, 257, 11235, 11, 32270, 1002, 13, 400, 498, 291, 992, 364, 7318, 1185, 281, 341, 5633, 11, 309, 1062, 50960], "temperature": 0.0, "avg_logprob": -0.12913388353053148, "compression_ratio": 1.6088435374149659, "no_speech_prob": 0.01470670010894537}, {"id": 620, "seek": 338992, "start": 3401.84, "end": 3407.04, "text": " make more progress on it. It's just that Dalit is not operating in real time on the universe that", "tokens": [50960, 652, 544, 4205, 322, 309, 13, 467, 311, 445, 300, 17357, 270, 307, 406, 7447, 294, 957, 565, 322, 264, 6445, 300, 51220], "temperature": 0.0, "avg_logprob": -0.12913388353053148, "compression_ratio": 1.6088435374149659, "no_speech_prob": 0.01470670010894537}, {"id": 621, "seek": 338992, "start": 3407.04, "end": 3411.84, "text": " it's entangled with and neither is GPT-3. Both of them are in some sense, fancy autocomplete", "tokens": [51220, 309, 311, 948, 39101, 365, 293, 9662, 307, 26039, 51, 12, 18, 13, 6767, 295, 552, 366, 294, 512, 2020, 11, 10247, 45833, 298, 17220, 51460], "temperature": 0.0, "avg_logprob": -0.12913388353053148, "compression_ratio": 1.6088435374149659, "no_speech_prob": 0.01470670010894537}, {"id": 622, "seek": 338992, "start": 3411.84, "end": 3419.52, "text": " algorithms. But this fancy autocomplete is able to do autocompletions that are far beyond the", "tokens": [51460, 14642, 13, 583, 341, 10247, 45833, 298, 17220, 307, 1075, 281, 360, 45833, 298, 14657, 626, 300, 366, 1400, 4399, 264, 51844], "temperature": 0.0, "avg_logprob": -0.12913388353053148, "compression_ratio": 1.6088435374149659, "no_speech_prob": 0.01470670010894537}, {"id": 623, "seek": 341952, "start": 3419.52, "end": 3426.88, "text": " autocompletion abilities of humans in almost every context. And so I don't see Dalit yet as art.", "tokens": [50364, 45833, 298, 14657, 313, 11582, 295, 6255, 294, 1920, 633, 4319, 13, 400, 370, 286, 500, 380, 536, 17357, 270, 1939, 382, 1523, 13, 50732], "temperature": 0.0, "avg_logprob": -0.09693223527334269, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0019239785615354776}, {"id": 624, "seek": 341952, "start": 3426.88, "end": 3436.16, "text": " It's a very strange sense when someone at OpenAI let me throw trumps at Dalit too. And I got images", "tokens": [50732, 467, 311, 257, 588, 5861, 2020, 562, 1580, 412, 7238, 48698, 718, 385, 3507, 504, 16951, 412, 17357, 270, 886, 13, 400, 286, 658, 5267, 51196], "temperature": 0.0, "avg_logprob": -0.09693223527334269, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0019239785615354776}, {"id": 625, "seek": 341952, "start": 3436.16, "end": 3442.24, "text": " back. I had a sense of ownership. I had the sense that I was doing that, even though it was clearly", "tokens": [51196, 646, 13, 286, 632, 257, 2020, 295, 15279, 13, 286, 632, 264, 2020, 300, 286, 390, 884, 300, 11, 754, 1673, 309, 390, 4448, 51500], "temperature": 0.0, "avg_logprob": -0.09693223527334269, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0019239785615354776}, {"id": 626, "seek": 341952, "start": 3442.24, "end": 3446.88, "text": " doing skills, using skills that I didn't have. And I suppose that you had the same impression", "tokens": [51500, 884, 3942, 11, 1228, 3942, 300, 286, 994, 380, 362, 13, 400, 286, 7297, 300, 291, 632, 264, 912, 9995, 51732], "temperature": 0.0, "avg_logprob": -0.09693223527334269, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0019239785615354776}, {"id": 627, "seek": 344688, "start": 3446.88, "end": 3451.04, "text": " when you were generating things with your diffusion model that you're going to put up on your walls,", "tokens": [50364, 562, 291, 645, 17746, 721, 365, 428, 25242, 2316, 300, 291, 434, 516, 281, 829, 493, 322, 428, 7920, 11, 50572], "temperature": 0.0, "avg_logprob": -0.0828765920690588, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.011675583198666573}, {"id": 628, "seek": 344688, "start": 3451.04, "end": 3456.4, "text": " right? You did that using this amazing tool that was empowering you to think that you otherwise", "tokens": [50572, 558, 30, 509, 630, 300, 1228, 341, 2243, 2290, 300, 390, 28261, 291, 281, 519, 300, 291, 5911, 50840], "temperature": 0.0, "avg_logprob": -0.0828765920690588, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.011675583198666573}, {"id": 629, "seek": 344688, "start": 3456.4, "end": 3462.48, "text": " never could do. But you are the creative nexus. And to make an artist, a digital artist, you would", "tokens": [50840, 1128, 727, 360, 13, 583, 291, 366, 264, 5880, 408, 32618, 13, 400, 281, 652, 364, 5748, 11, 257, 4562, 5748, 11, 291, 576, 51144], "temperature": 0.0, "avg_logprob": -0.0828765920690588, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.011675583198666573}, {"id": 630, "seek": 344688, "start": 3462.48, "end": 3468.6400000000003, "text": " need to create an autonomous creative nexus in a way, a creative entity, something that reflects", "tokens": [51144, 643, 281, 1884, 364, 23797, 5880, 408, 32618, 294, 257, 636, 11, 257, 5880, 13977, 11, 746, 300, 18926, 51452], "temperature": 0.0, "avg_logprob": -0.0828765920690588, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.011675583198666573}, {"id": 631, "seek": 344688, "start": 3468.6400000000003, "end": 3473.52, "text": " on the world because art is about capturing conscious states. So we would need to build a", "tokens": [51452, 322, 264, 1002, 570, 1523, 307, 466, 23384, 6648, 4368, 13, 407, 321, 576, 643, 281, 1322, 257, 51696], "temperature": 0.0, "avg_logprob": -0.0828765920690588, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.011675583198666573}, {"id": 632, "seek": 347352, "start": 3473.52, "end": 3478.96, "text": " system that has a story about itself and that is reacting towards own interactions with the world", "tokens": [50364, 1185, 300, 575, 257, 1657, 466, 2564, 293, 300, 307, 25817, 3030, 1065, 13280, 365, 264, 1002, 50636], "temperature": 0.0, "avg_logprob": -0.11302024013591262, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.010152561590075493}, {"id": 633, "seek": 347352, "start": 3478.96, "end": 3484.64, "text": " and that would need to be human. It would need to be consistent. Something that is an intelligent", "tokens": [50636, 293, 300, 576, 643, 281, 312, 1952, 13, 467, 576, 643, 281, 312, 8398, 13, 6595, 300, 307, 364, 13232, 50920], "temperature": 0.0, "avg_logprob": -0.11302024013591262, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.010152561590075493}, {"id": 634, "seek": 347352, "start": 3484.64, "end": 3490.16, "text": " entity that is creatively interacting with the world. I think we could totally build an AI", "tokens": [50920, 13977, 300, 307, 43750, 18017, 365, 264, 1002, 13, 286, 519, 321, 727, 3879, 1322, 364, 7318, 51196], "temperature": 0.0, "avg_logprob": -0.11302024013591262, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.010152561590075493}, {"id": 635, "seek": 347352, "start": 3490.16, "end": 3495.2, "text": " artist franchise right now that would have a huge following. But what it need to have is an identity", "tokens": [51196, 5748, 16222, 558, 586, 300, 576, 362, 257, 2603, 3480, 13, 583, 437, 309, 643, 281, 362, 307, 364, 6575, 51448], "temperature": 0.0, "avg_logprob": -0.11302024013591262, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.010152561590075493}, {"id": 636, "seek": 347352, "start": 3495.2, "end": 3500.0, "text": " that is not fake, that it's actually built from its interactions with the world in real time.", "tokens": [51448, 300, 307, 406, 7592, 11, 300, 309, 311, 767, 3094, 490, 1080, 13280, 365, 264, 1002, 294, 957, 565, 13, 51688], "temperature": 0.0, "avg_logprob": -0.11302024013591262, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.010152561590075493}, {"id": 637, "seek": 350000, "start": 3500.96, "end": 3507.84, "text": " Well, I think we've got a lovely segue there because you said that art is about", "tokens": [50412, 1042, 11, 286, 519, 321, 600, 658, 257, 7496, 33850, 456, 570, 291, 848, 300, 1523, 307, 466, 50756], "temperature": 0.0, "avg_logprob": -0.10021300151430328, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.0029672090895473957}, {"id": 638, "seek": 350000, "start": 3507.84, "end": 3512.0, "text": " representing our conscious states. In a way, I disagree with you because you could say,", "tokens": [50756, 13460, 527, 6648, 4368, 13, 682, 257, 636, 11, 286, 14091, 365, 291, 570, 291, 727, 584, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10021300151430328, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.0029672090895473957}, {"id": 639, "seek": 350000, "start": 3512.0, "end": 3516.0, "text": " well, it's very reductionist. I've just put a prompt in there and I've created art. Well,", "tokens": [50964, 731, 11, 309, 311, 588, 11004, 468, 13, 286, 600, 445, 829, 257, 12391, 294, 456, 293, 286, 600, 2942, 1523, 13, 1042, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10021300151430328, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.0029672090895473957}, {"id": 640, "seek": 350000, "start": 3516.0, "end": 3521.36, "text": " I think it is art. But how much of a representation of my conscious state is it? I think Douglas", "tokens": [51164, 286, 519, 309, 307, 1523, 13, 583, 577, 709, 295, 257, 10290, 295, 452, 6648, 1785, 307, 309, 30, 286, 519, 23010, 51432], "temperature": 0.0, "avg_logprob": -0.10021300151430328, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.0029672090895473957}, {"id": 641, "seek": 350000, "start": 3521.36, "end": 3526.72, "text": " Hofstadter would say it wasn't. But over to the matter of consciousness because we're a bit low", "tokens": [51432, 37379, 48299, 391, 576, 584, 309, 2067, 380, 13, 583, 670, 281, 264, 1871, 295, 10081, 570, 321, 434, 257, 857, 2295, 51700], "temperature": 0.0, "avg_logprob": -0.10021300151430328, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.0029672090895473957}, {"id": 642, "seek": 352672, "start": 3526.72, "end": 3530.8799999999997, "text": " on time. I mean, you said actually that you've spent much of your life thinking about what", "tokens": [50364, 322, 565, 13, 286, 914, 11, 291, 848, 767, 300, 291, 600, 4418, 709, 295, 428, 993, 1953, 466, 437, 50572], "temperature": 0.0, "avg_logprob": -0.09351017740037706, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.020838353782892227}, {"id": 643, "seek": 352672, "start": 3530.8799999999997, "end": 3537.04, "text": " consciousness is. And you said that you thought it was very mysterious, but you now think that", "tokens": [50572, 10081, 307, 13, 400, 291, 848, 300, 291, 1194, 309, 390, 588, 13831, 11, 457, 291, 586, 519, 300, 50880], "temperature": 0.0, "avg_logprob": -0.09351017740037706, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.020838353782892227}, {"id": 644, "seek": 352672, "start": 3537.04, "end": 3541.8399999999997, "text": " it's a riddle that can be solved, right? So on your recent theory of everything interview", "tokens": [50880, 309, 311, 257, 3973, 2285, 300, 393, 312, 13041, 11, 558, 30, 407, 322, 428, 5162, 5261, 295, 1203, 4049, 51120], "temperature": 0.0, "avg_logprob": -0.09351017740037706, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.020838353782892227}, {"id": 645, "seek": 352672, "start": 3541.8399999999997, "end": 3547.3599999999997, "text": " with Donald Hoffman, actually, you said that it was virtual, not a physical thing, that brains", "tokens": [51120, 365, 8632, 29135, 1601, 11, 767, 11, 291, 848, 300, 309, 390, 6374, 11, 406, 257, 4001, 551, 11, 300, 15442, 51396], "temperature": 0.0, "avg_logprob": -0.09351017740037706, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.020838353782892227}, {"id": 646, "seek": 352672, "start": 3547.3599999999997, "end": 3553.52, "text": " are mechanistic and that the elements of consciousness are magical somehow. But you said", "tokens": [51396, 366, 4236, 3142, 293, 300, 264, 4959, 295, 10081, 366, 12066, 6063, 13, 583, 291, 848, 51704], "temperature": 0.0, "avg_logprob": -0.09351017740037706, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.020838353782892227}, {"id": 647, "seek": 355352, "start": 3553.52, "end": 3559.2, "text": " it had an a causal structure, but not the way physics is built. But it was a story", "tokens": [50364, 309, 632, 364, 257, 38755, 3877, 11, 457, 406, 264, 636, 10649, 307, 3094, 13, 583, 309, 390, 257, 1657, 50648], "temperature": 0.0, "avg_logprob": -0.0837982177734375, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0049523767083883286}, {"id": 648, "seek": 355352, "start": 3559.2, "end": 3564.24, "text": " which the physical system tells to itself. You said that the organism is a coherent", "tokens": [50648, 597, 264, 4001, 1185, 5112, 281, 2564, 13, 509, 848, 300, 264, 24128, 307, 257, 36239, 50900], "temperature": 0.0, "avg_logprob": -0.0837982177734375, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0049523767083883286}, {"id": 649, "seek": 355352, "start": 3564.24, "end": 3569.68, "text": " and consistent pattern, which is state building at least at some level of analysis and that", "tokens": [50900, 293, 8398, 5102, 11, 597, 307, 1785, 2390, 412, 1935, 412, 512, 1496, 295, 5215, 293, 300, 51172], "temperature": 0.0, "avg_logprob": -0.0837982177734375, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0049523767083883286}, {"id": 650, "seek": 355352, "start": 3569.68, "end": 3575.28, "text": " consciousness allows organisms to coordinate their cells to succeed in their niches. And then", "tokens": [51172, 10081, 4045, 22110, 281, 15670, 641, 5438, 281, 7754, 294, 641, 25570, 279, 13, 400, 550, 51452], "temperature": 0.0, "avg_logprob": -0.0837982177734375, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0049523767083883286}, {"id": 651, "seek": 355352, "start": 3575.28, "end": 3582.32, "text": " you spoke of information processing over cells. Now, what model of I should say like what measure", "tokens": [51452, 291, 7179, 295, 1589, 9007, 670, 5438, 13, 823, 11, 437, 2316, 295, 286, 820, 584, 411, 437, 3481, 51804], "temperature": 0.0, "avg_logprob": -0.0837982177734375, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0049523767083883286}, {"id": 652, "seek": 358232, "start": 3582.32, "end": 3588.6400000000003, "text": " of consciousness do you think you most align with? There is only one theory that offers a", "tokens": [50364, 295, 10081, 360, 291, 519, 291, 881, 7975, 365, 30, 821, 307, 787, 472, 5261, 300, 7736, 257, 50680], "temperature": 0.0, "avg_logprob": -0.12555612431894433, "compression_ratio": 1.8951612903225807, "no_speech_prob": 0.010805412195622921}, {"id": 653, "seek": 358232, "start": 3588.6400000000003, "end": 3592.88, "text": " measure of consciousness and that is integrated information theory, where you actually put a", "tokens": [50680, 3481, 295, 10081, 293, 300, 307, 10919, 1589, 5261, 11, 689, 291, 767, 829, 257, 50892], "temperature": 0.0, "avg_logprob": -0.12555612431894433, "compression_ratio": 1.8951612903225807, "no_speech_prob": 0.010805412195622921}, {"id": 654, "seek": 358232, "start": 3592.88, "end": 3600.2400000000002, "text": " number on it. And it's not clear what that number means. It's not that there is some kind of scalar", "tokens": [50892, 1230, 322, 309, 13, 400, 309, 311, 406, 1850, 437, 300, 1230, 1355, 13, 467, 311, 406, 300, 456, 307, 512, 733, 295, 39684, 51260], "temperature": 0.0, "avg_logprob": -0.12555612431894433, "compression_ratio": 1.8951612903225807, "no_speech_prob": 0.010805412195622921}, {"id": 655, "seek": 358232, "start": 3600.2400000000002, "end": 3606.32, "text": " that measures this. And people we think of consciousness as something that is more qualitative", "tokens": [51260, 300, 8000, 341, 13, 400, 561, 321, 519, 295, 10081, 382, 746, 300, 307, 544, 31312, 51564], "temperature": 0.0, "avg_logprob": -0.12555612431894433, "compression_ratio": 1.8951612903225807, "no_speech_prob": 0.010805412195622921}, {"id": 656, "seek": 358232, "start": 3606.32, "end": 3611.28, "text": " than quantitative. Either somebody is conscious or somebody is unconscious. And when you are", "tokens": [51564, 813, 27778, 13, 13746, 2618, 307, 6648, 420, 2618, 307, 18900, 13, 400, 562, 291, 366, 51812], "temperature": 0.0, "avg_logprob": -0.12555612431894433, "compression_ratio": 1.8951612903225807, "no_speech_prob": 0.010805412195622921}, {"id": 657, "seek": 361128, "start": 3611.28, "end": 3616.2400000000002, "text": " conscious, you can have a lack of acuity, you can be addled in your brain and you can be", "tokens": [50364, 6648, 11, 291, 393, 362, 257, 5011, 295, 696, 21757, 11, 291, 393, 312, 909, 1493, 294, 428, 3567, 293, 291, 393, 312, 50612], "temperature": 0.0, "avg_logprob": -0.10058609044776773, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017535818042233586}, {"id": 658, "seek": 361128, "start": 3616.2400000000002, "end": 3622.2400000000002, "text": " drifting in and out of consciousness. But it's still a qualitative thing of whether you have", "tokens": [50612, 37973, 294, 293, 484, 295, 10081, 13, 583, 309, 311, 920, 257, 31312, 551, 295, 1968, 291, 362, 50912], "temperature": 0.0, "avg_logprob": -0.10058609044776773, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017535818042233586}, {"id": 659, "seek": 361128, "start": 3622.2400000000002, "end": 3628.8, "text": " that or not. And this qualitative thing seems to be simple, probably much simpler than people", "tokens": [50912, 300, 420, 406, 13, 400, 341, 31312, 551, 2544, 281, 312, 2199, 11, 1391, 709, 18587, 813, 561, 51240], "temperature": 0.0, "avg_logprob": -0.10058609044776773, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017535818042233586}, {"id": 660, "seek": 361128, "start": 3628.8, "end": 3634.96, "text": " expected to be. The hard thing might be perception. And consciousness is on top of", "tokens": [51240, 5176, 281, 312, 13, 440, 1152, 551, 1062, 312, 12860, 13, 400, 10081, 307, 322, 1192, 295, 51548], "temperature": 0.0, "avg_logprob": -0.10058609044776773, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017535818042233586}, {"id": 661, "seek": 361128, "start": 3634.96, "end": 3640.48, "text": " perception is a certain way to deal with our attention. So I think a very important aspect", "tokens": [51548, 12860, 307, 257, 1629, 636, 281, 2028, 365, 527, 3202, 13, 407, 286, 519, 257, 588, 1021, 4171, 51824], "temperature": 0.0, "avg_logprob": -0.10058609044776773, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017535818042233586}, {"id": 662, "seek": 364048, "start": 3640.48, "end": 3646.0, "text": " of consciousness is reflexive attention, that we notice ourselves attending to something,", "tokens": [50364, 295, 10081, 307, 23802, 488, 3202, 11, 300, 321, 3449, 4175, 15862, 281, 746, 11, 50640], "temperature": 0.0, "avg_logprob": -0.135374567243788, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0012829299084842205}, {"id": 663, "seek": 364048, "start": 3646.0, "end": 3652.4, "text": " and we reflect on that and integrate this in our model. The conundrum is understanding consciousness", "tokens": [50640, 293, 321, 5031, 322, 300, 293, 13365, 341, 294, 527, 2316, 13, 440, 416, 997, 6247, 307, 3701, 10081, 50960], "temperature": 0.0, "avg_logprob": -0.135374567243788, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0012829299084842205}, {"id": 664, "seek": 364048, "start": 3652.4, "end": 3657.68, "text": " if you go right into the history of everything starting with Leibniz and many others. Leibniz", "tokens": [50960, 498, 291, 352, 558, 666, 264, 2503, 295, 1203, 2891, 365, 1456, 897, 77, 590, 293, 867, 2357, 13, 1456, 897, 77, 590, 51224], "temperature": 0.0, "avg_logprob": -0.135374567243788, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0012829299084842205}, {"id": 665, "seek": 364048, "start": 3657.68, "end": 3665.68, "text": " says this idea of imagine you could have a mill and this mill, this is your mind. And the mill is", "tokens": [51224, 1619, 341, 1558, 295, 3811, 291, 727, 362, 257, 1728, 293, 341, 1728, 11, 341, 307, 428, 1575, 13, 400, 264, 1728, 307, 51624], "temperature": 0.0, "avg_logprob": -0.135374567243788, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0012829299084842205}, {"id": 666, "seek": 366568, "start": 3665.68, "end": 3671.68, "text": " made of lots of mechanical parts and somehow the thing is feeling and perceiving things.", "tokens": [50364, 1027, 295, 3195, 295, 12070, 3166, 293, 6063, 264, 551, 307, 2633, 293, 9016, 2123, 721, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14917711370131548, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.04018549993634224}, {"id": 667, "seek": 366568, "start": 3672.3199999999997, "end": 3678.96, "text": " And we blow the mill up so large that we can walk into it or we would today zoom into it until", "tokens": [50696, 400, 321, 6327, 264, 1728, 493, 370, 2416, 300, 321, 393, 1792, 666, 309, 420, 321, 576, 965, 8863, 666, 309, 1826, 51028], "temperature": 0.0, "avg_logprob": -0.14917711370131548, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.04018549993634224}, {"id": 668, "seek": 366568, "start": 3678.96, "end": 3683.8399999999997, "text": " we see all the parts and we just see these pushing and pulling parts and nothing of them", "tokens": [51028, 321, 536, 439, 264, 3166, 293, 321, 445, 536, 613, 7380, 293, 8407, 3166, 293, 1825, 295, 552, 51272], "temperature": 0.0, "avg_logprob": -0.14917711370131548, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.04018549993634224}, {"id": 669, "seek": 366568, "start": 3683.8399999999997, "end": 3689.7599999999998, "text": " can ever explain a perception or a feeling. And it is a very strong intuition that also", "tokens": [51272, 393, 1562, 2903, 257, 12860, 420, 257, 2633, 13, 400, 309, 307, 257, 588, 2068, 24002, 300, 611, 51568], "temperature": 0.0, "avg_logprob": -0.14917711370131548, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.04018549993634224}, {"id": 670, "seek": 368976, "start": 3689.76, "end": 3696.4, "text": " drives the Chinese rule when many other thinkers will get attracted to this. And it seems pretty", "tokens": [50364, 11754, 264, 4649, 4978, 562, 867, 661, 37895, 486, 483, 15912, 281, 341, 13, 400, 309, 2544, 1238, 50696], "temperature": 0.0, "avg_logprob": -0.12070214604756918, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.010161042213439941}, {"id": 671, "seek": 368976, "start": 3696.4, "end": 3701.1200000000003, "text": " obvious that these mechanical phenomena are insufficient to explain what's going on. It's", "tokens": [50696, 6322, 300, 613, 12070, 22004, 366, 41709, 281, 2903, 437, 311, 516, 322, 13, 467, 311, 50932], "temperature": 0.0, "avg_logprob": -0.12070214604756918, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.010161042213439941}, {"id": 672, "seek": 368976, "start": 3701.1200000000003, "end": 3706.48, "text": " not an obvious connection. So people become dualist. There are somehow two completely separate", "tokens": [50932, 406, 364, 6322, 4984, 13, 407, 561, 1813, 11848, 468, 13, 821, 366, 6063, 732, 2584, 4994, 51200], "temperature": 0.0, "avg_logprob": -0.12070214604756918, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.010161042213439941}, {"id": 673, "seek": 368976, "start": 3706.48, "end": 3711.92, "text": " domains. And I think in a way this dualism is correct, but not in the sense that the mental", "tokens": [51200, 25514, 13, 400, 286, 519, 294, 257, 636, 341, 11848, 1434, 307, 3006, 11, 457, 406, 294, 264, 2020, 300, 264, 4973, 51472], "temperature": 0.0, "avg_logprob": -0.12070214604756918, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.010161042213439941}, {"id": 674, "seek": 371192, "start": 3711.92, "end": 3720.56, "text": " states are ontologically existing. They exist as if. There is no organism. There is only this", "tokens": [50364, 4368, 366, 6592, 17157, 6741, 13, 814, 2514, 382, 498, 13, 821, 307, 572, 24128, 13, 821, 307, 787, 341, 50796], "temperature": 0.0, "avg_logprob": -0.13240448328164908, "compression_ratio": 1.8699186991869918, "no_speech_prob": 0.06088503822684288}, {"id": 675, "seek": 371192, "start": 3720.56, "end": 3726.2400000000002, "text": " connection of cells and this collection of cells is acting in a coherent way, which means we can", "tokens": [50796, 4984, 295, 5438, 293, 341, 5765, 295, 5438, 307, 6577, 294, 257, 36239, 636, 11, 597, 1355, 321, 393, 51080], "temperature": 0.0, "avg_logprob": -0.13240448328164908, "compression_ratio": 1.8699186991869918, "no_speech_prob": 0.06088503822684288}, {"id": 676, "seek": 371192, "start": 3726.2400000000002, "end": 3730.96, "text": " compress it. We can model it using a very low-dimensional, much circular function", "tokens": [51080, 14778, 309, 13, 492, 393, 2316, 309, 1228, 257, 588, 2295, 12, 18759, 11, 709, 16476, 2445, 51316], "temperature": 0.0, "avg_logprob": -0.13240448328164908, "compression_ratio": 1.8699186991869918, "no_speech_prob": 0.06088503822684288}, {"id": 677, "seek": 371192, "start": 3730.96, "end": 3735.6, "text": " than look at all the cells in general. And the organism is only approximating this function,", "tokens": [51316, 813, 574, 412, 439, 264, 5438, 294, 2674, 13, 400, 264, 24128, 307, 787, 8542, 990, 341, 2445, 11, 51548], "temperature": 0.0, "avg_logprob": -0.13240448328164908, "compression_ratio": 1.8699186991869918, "no_speech_prob": 0.06088503822684288}, {"id": 678, "seek": 371192, "start": 3735.6, "end": 3740.7200000000003, "text": " but what makes the organism more powerful than a collection of cells is exactly that function,", "tokens": [51548, 457, 437, 1669, 264, 24128, 544, 4005, 813, 257, 5765, 295, 5438, 307, 2293, 300, 2445, 11, 51804], "temperature": 0.0, "avg_logprob": -0.13240448328164908, "compression_ratio": 1.8699186991869918, "no_speech_prob": 0.06088503822684288}, {"id": 679, "seek": 374072, "start": 3740.72, "end": 3746.08, "text": " this structure that we project into it. And the interesting thing is that by the information", "tokens": [50364, 341, 3877, 300, 321, 1716, 666, 309, 13, 400, 264, 1880, 551, 307, 300, 538, 264, 1589, 50632], "temperature": 0.0, "avg_logprob": -0.09261429637943933, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.0029328842647373676}, {"id": 680, "seek": 374072, "start": 3746.08, "end": 3750.64, "text": " processing within the organism, the organism can discover that function by itself and use it to", "tokens": [50632, 9007, 1951, 264, 24128, 11, 264, 24128, 393, 4411, 300, 2445, 538, 2564, 293, 764, 309, 281, 50860], "temperature": 0.0, "avg_logprob": -0.09261429637943933, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.0029328842647373676}, {"id": 681, "seek": 374072, "start": 3750.64, "end": 3756.8799999999997, "text": " drive its own behavior. So while the organism is not a person, it's not even an organism,", "tokens": [50860, 3332, 1080, 1065, 5223, 13, 407, 1339, 264, 24128, 307, 406, 257, 954, 11, 309, 311, 406, 754, 364, 24128, 11, 51172], "temperature": 0.0, "avg_logprob": -0.09261429637943933, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.0029328842647373676}, {"id": 682, "seek": 374072, "start": 3757.52, "end": 3763.04, "text": " it is very useful for the organism to behave as if it was an organism and also to have an idea", "tokens": [51204, 309, 307, 588, 4420, 337, 264, 24128, 281, 15158, 382, 498, 309, 390, 364, 24128, 293, 611, 281, 362, 364, 1558, 51480], "temperature": 0.0, "avg_logprob": -0.09261429637943933, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.0029328842647373676}, {"id": 683, "seek": 374072, "start": 3763.04, "end": 3766.7999999999997, "text": " of what it would be like to be a person that interacts, for instance, with the social world.", "tokens": [51480, 295, 437, 309, 576, 312, 411, 281, 312, 257, 954, 300, 43582, 11, 337, 5197, 11, 365, 264, 2093, 1002, 13, 51668], "temperature": 0.0, "avg_logprob": -0.09261429637943933, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.0029328842647373676}, {"id": 684, "seek": 376680, "start": 3767.52, "end": 3772.1600000000003, "text": " So it creates a simulation of that. And it's often not even a simulation, it's often just a", "tokens": [50400, 407, 309, 7829, 257, 16575, 295, 300, 13, 400, 309, 311, 2049, 406, 754, 257, 16575, 11, 309, 311, 2049, 445, 257, 50632], "temperature": 0.0, "avg_logprob": -0.08982510720529864, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.00529461307451129}, {"id": 685, "seek": 376680, "start": 3772.1600000000003, "end": 3777.76, "text": " simulacrum. That's what makes it a causal. The difference between a simulation and the reality", "tokens": [50632, 1034, 425, 326, 6247, 13, 663, 311, 437, 1669, 309, 257, 38755, 13, 440, 2649, 1296, 257, 16575, 293, 264, 4103, 50912], "temperature": 0.0, "avg_logprob": -0.08982510720529864, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.00529461307451129}, {"id": 686, "seek": 376680, "start": 3777.76, "end": 3784.8, "text": " is that the simulation is modeling some aspects of the dynamics of a domain on a different causal", "tokens": [50912, 307, 300, 264, 16575, 307, 15983, 512, 7270, 295, 264, 15679, 295, 257, 9274, 322, 257, 819, 38755, 51264], "temperature": 0.0, "avg_logprob": -0.08982510720529864, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.00529461307451129}, {"id": 687, "seek": 376680, "start": 3784.8, "end": 3790.1600000000003, "text": " substrate, on a different causal footing. So you have a computer game in which you can shoot a gun,", "tokens": [51264, 27585, 11, 322, 257, 819, 38755, 45959, 13, 407, 291, 362, 257, 3820, 1216, 294, 597, 291, 393, 3076, 257, 3874, 11, 51532], "temperature": 0.0, "avg_logprob": -0.08982510720529864, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.00529461307451129}, {"id": 688, "seek": 379016, "start": 3790.16, "end": 3796.64, "text": " but there is no proper physics in the game that would recreate what's happened in the real world", "tokens": [50364, 457, 456, 307, 572, 2296, 10649, 294, 264, 1216, 300, 576, 25833, 437, 311, 2011, 294, 264, 957, 1002, 50688], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 689, "seek": 379016, "start": 3796.64, "end": 3801.52, "text": " when you shoot a gun. Instead, it is using a different causal structure of your software", "tokens": [50688, 562, 291, 3076, 257, 3874, 13, 7156, 11, 309, 307, 1228, 257, 819, 38755, 3877, 295, 428, 4722, 50932], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 690, "seek": 379016, "start": 3801.52, "end": 3806.08, "text": " program to give you something that gives you good enough dynamics so you can interact with the world", "tokens": [50932, 1461, 281, 976, 291, 746, 300, 2709, 291, 665, 1547, 15679, 370, 291, 393, 4648, 365, 264, 1002, 51160], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 691, "seek": 379016, "start": 3806.08, "end": 3810.24, "text": " and experience these causal structures. You can make a different decision, you make a different", "tokens": [51160, 293, 1752, 613, 38755, 9227, 13, 509, 393, 652, 257, 819, 3537, 11, 291, 652, 257, 819, 51368], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 692, "seek": 379016, "start": 3810.24, "end": 3815.12, "text": " move in the game, and as a result, the game behaves as if you would expect it because it's", "tokens": [51368, 1286, 294, 264, 1216, 11, 293, 382, 257, 1874, 11, 264, 1216, 36896, 382, 498, 291, 576, 2066, 309, 570, 309, 311, 51612], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 693, "seek": 379016, "start": 3815.12, "end": 3819.44, "text": " imitating the same causal structure using this different substrate. In the simulacrum,", "tokens": [51612, 566, 16350, 264, 912, 38755, 3877, 1228, 341, 819, 27585, 13, 682, 264, 1034, 425, 326, 6247, 11, 51828], "temperature": 0.0, "avg_logprob": -0.09747906336708674, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.006671522743999958}, {"id": 694, "seek": 381944, "start": 3819.52, "end": 3823.28, "text": " you don't have the causal structure. Like a movie doesn't have causal structure. It only", "tokens": [50368, 291, 500, 380, 362, 264, 38755, 3877, 13, 1743, 257, 3169, 1177, 380, 362, 38755, 3877, 13, 467, 787, 50556], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 695, "seek": 381944, "start": 3823.28, "end": 3828.96, "text": " gives you a sequence of observables. And our own mental model of ourselves is a mixture of", "tokens": [50556, 2709, 291, 257, 8310, 295, 9951, 2965, 13, 400, 527, 1065, 4973, 2316, 295, 4175, 307, 257, 9925, 295, 50840], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 696, "seek": 381944, "start": 3828.96, "end": 3835.36, "text": " simulation and simulacrum. So we sometimes create a sequence without causal structure. It looks like", "tokens": [50840, 16575, 293, 1034, 425, 326, 6247, 13, 407, 321, 2171, 1884, 257, 8310, 1553, 38755, 3877, 13, 467, 1542, 411, 51160], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 697, "seek": 381944, "start": 3835.36, "end": 3839.92, "text": " it does this thing magically. And sometimes we have a causal model, but this causal model", "tokens": [51160, 309, 775, 341, 551, 39763, 13, 400, 2171, 321, 362, 257, 38755, 2316, 11, 457, 341, 38755, 2316, 51388], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 698, "seek": 381944, "start": 3839.92, "end": 3844.56, "text": " is not the real deal. It's just this simplified geometric simulation of how the world works.", "tokens": [51388, 307, 406, 264, 957, 2028, 13, 467, 311, 445, 341, 26335, 33246, 16575, 295, 577, 264, 1002, 1985, 13, 51620], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 699, "seek": 381944, "start": 3844.56, "end": 3849.04, "text": " It's a game engine that our brain is producing to anticipate what happens in the physical world.", "tokens": [51620, 467, 311, 257, 1216, 2848, 300, 527, 3567, 307, 10501, 281, 21685, 437, 2314, 294, 264, 4001, 1002, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10469635640542338, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.0029333222191780806}, {"id": 700, "seek": 384944, "start": 3849.92, "end": 3856.64, "text": " Yeah, so that very strongly resonates with me. And another person I very much respect", "tokens": [50388, 865, 11, 370, 300, 588, 10613, 41051, 365, 385, 13, 400, 1071, 954, 286, 588, 709, 3104, 50724], "temperature": 0.0, "avg_logprob": -0.10807411193847656, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.0004441669734660536}, {"id": 701, "seek": 384944, "start": 3856.64, "end": 3862.7200000000003, "text": " whose opinion resonates with me regarding consciousness is Carl Friston. And I'm not", "tokens": [50724, 6104, 4800, 41051, 365, 385, 8595, 10081, 307, 14256, 1526, 47345, 13, 400, 286, 478, 406, 51028], "temperature": 0.0, "avg_logprob": -0.10807411193847656, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.0004441669734660536}, {"id": 702, "seek": 384944, "start": 3862.7200000000003, "end": 3867.92, "text": " sure how much you're familiar with his free energy principle and his thoughts on consciousness,", "tokens": [51028, 988, 577, 709, 291, 434, 4963, 365, 702, 1737, 2281, 8665, 293, 702, 4598, 322, 10081, 11, 51288], "temperature": 0.0, "avg_logprob": -0.10807411193847656, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.0004441669734660536}, {"id": 703, "seek": 384944, "start": 3867.92, "end": 3873.04, "text": " but I'd like to put forward to you one of his more recent definitions, if you will,", "tokens": [51288, 457, 286, 1116, 411, 281, 829, 2128, 281, 291, 472, 295, 702, 544, 5162, 21988, 11, 498, 291, 486, 11, 51544], "temperature": 0.0, "avg_logprob": -0.10807411193847656, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.0004441669734660536}, {"id": 704, "seek": 384944, "start": 3873.04, "end": 3877.68, "text": " or proposals to explain consciousness. And I get your opinion on it. This is from his", "tokens": [51544, 420, 20198, 281, 2903, 10081, 13, 400, 286, 483, 428, 4800, 322, 309, 13, 639, 307, 490, 702, 51776], "temperature": 0.0, "avg_logprob": -0.10807411193847656, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.0004441669734660536}, {"id": 705, "seek": 387768, "start": 3878.3199999999997, "end": 3886.56, "text": " 2018 article titled, am I self conscious or does self organization entail self consciousness?", "tokens": [50396, 6096, 7222, 19841, 11, 669, 286, 2698, 6648, 420, 775, 2698, 4475, 948, 864, 2698, 10081, 30, 50808], "temperature": 0.0, "avg_logprob": -0.08807831406593322, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0032723830081522465}, {"id": 706, "seek": 387768, "start": 3886.56, "end": 3892.48, "text": " And what he says here is the proposal on offer here is that the mind comes into being when", "tokens": [50808, 400, 437, 415, 1619, 510, 307, 264, 11494, 322, 2626, 510, 307, 300, 264, 1575, 1487, 666, 885, 562, 51104], "temperature": 0.0, "avg_logprob": -0.08807831406593322, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0032723830081522465}, {"id": 707, "seek": 387768, "start": 3892.48, "end": 3899.9199999999996, "text": " self evidencing has a temporal thickness or counterfactual depth, which grounds inferences", "tokens": [51104, 2698, 43699, 2175, 575, 257, 30881, 14855, 420, 5682, 44919, 901, 7161, 11, 597, 19196, 13596, 2667, 51476], "temperature": 0.0, "avg_logprob": -0.08807831406593322, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0032723830081522465}, {"id": 708, "seek": 387768, "start": 3899.9199999999996, "end": 3906.7999999999997, "text": " about the consequences of my action. On this view, consciousness is nothing more than inference", "tokens": [51476, 466, 264, 10098, 295, 452, 3069, 13, 1282, 341, 1910, 11, 10081, 307, 1825, 544, 813, 38253, 51820], "temperature": 0.0, "avg_logprob": -0.08807831406593322, "compression_ratio": 1.6415929203539823, "no_speech_prob": 0.0032723830081522465}, {"id": 709, "seek": 390680, "start": 3906.8, "end": 3914.0800000000004, "text": " about my future, namely the self evidencing consequences of what I could do. Does that align", "tokens": [50364, 466, 452, 2027, 11, 20926, 264, 2698, 43699, 2175, 10098, 295, 437, 286, 727, 360, 13, 4402, 300, 7975, 50728], "temperature": 0.0, "avg_logprob": -0.15493569374084473, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.0010161030804738402}, {"id": 710, "seek": 390680, "start": 3914.7200000000003, "end": 3916.8, "text": " pretty closely with your your view?", "tokens": [50760, 1238, 8185, 365, 428, 428, 1910, 30, 50864], "temperature": 0.0, "avg_logprob": -0.15493569374084473, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.0010161030804738402}, {"id": 711, "seek": 390680, "start": 3917.6800000000003, "end": 3923.92, "text": " No, I don't think it's sufficient and also don't think it's necessary. I like Friston's idea,", "tokens": [50908, 883, 11, 286, 500, 380, 519, 309, 311, 11563, 293, 611, 500, 380, 519, 309, 311, 4818, 13, 286, 411, 1526, 47345, 311, 1558, 11, 51220], "temperature": 0.0, "avg_logprob": -0.15493569374084473, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.0010161030804738402}, {"id": 712, "seek": 390680, "start": 3923.92, "end": 3927.92, "text": " but most of the free energy principle comes down to predictive coding,", "tokens": [51220, 457, 881, 295, 264, 1737, 2281, 8665, 1487, 760, 281, 35521, 17720, 11, 51420], "temperature": 0.0, "avg_logprob": -0.15493569374084473, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.0010161030804738402}, {"id": 713, "seek": 390680, "start": 3927.92, "end": 3935.44, "text": " which is in some sense, radically tested with GPT-3. GPT-3 is trained in some sense entirely", "tokens": [51420, 597, 307, 294, 512, 2020, 11, 35508, 8246, 365, 26039, 51, 12, 18, 13, 26039, 51, 12, 18, 307, 8895, 294, 512, 2020, 7696, 51796], "temperature": 0.0, "avg_logprob": -0.15493569374084473, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.0010161030804738402}, {"id": 714, "seek": 393544, "start": 3935.44, "end": 3940.32, "text": " on predictive coding. It's only trying to predict the future from the past. And the future is the", "tokens": [50364, 322, 35521, 17720, 13, 467, 311, 787, 1382, 281, 6069, 264, 2027, 490, 264, 1791, 13, 400, 264, 2027, 307, 264, 50608], "temperature": 0.0, "avg_logprob": -0.10709975698719854, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.008442926220595837}, {"id": 715, "seek": 393544, "start": 3940.32, "end": 3946.0, "text": " next token based on the tokens that it has seen so far. And GPT-3 radically tries how far you can", "tokens": [50608, 958, 14862, 2361, 322, 264, 22667, 300, 309, 575, 1612, 370, 1400, 13, 400, 26039, 51, 12, 18, 35508, 9898, 577, 1400, 291, 393, 50892], "temperature": 0.0, "avg_logprob": -0.10709975698719854, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.008442926220595837}, {"id": 716, "seek": 393544, "start": 3946.0, "end": 3952.08, "text": " go with this, and you can go very far. But you need far far more samples than an organism does.", "tokens": [50892, 352, 365, 341, 11, 293, 291, 393, 352, 588, 1400, 13, 583, 291, 643, 1400, 1400, 544, 10938, 813, 364, 24128, 775, 13, 51196], "temperature": 0.0, "avg_logprob": -0.10709975698719854, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.008442926220595837}, {"id": 717, "seek": 393544, "start": 3952.96, "end": 3958.0, "text": " So there are players in us that go beyond predictive coding, maybe we converge towards", "tokens": [51240, 407, 456, 366, 4150, 294, 505, 300, 352, 4399, 35521, 17720, 11, 1310, 321, 41881, 3030, 51492], "temperature": 0.0, "avg_logprob": -0.10709975698719854, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.008442926220595837}, {"id": 718, "seek": 393544, "start": 3958.0, "end": 3963.28, "text": " this over many generations in the evolutionary process. So I don't think it's a stupid idea", "tokens": [51492, 341, 670, 867, 10593, 294, 264, 27567, 1399, 13, 407, 286, 500, 380, 519, 309, 311, 257, 6631, 1558, 51756], "temperature": 0.0, "avg_logprob": -0.10709975698719854, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.008442926220595837}, {"id": 719, "seek": 396328, "start": 3963.28, "end": 3970.32, "text": " that Carl Friston proposes, but we are born with additional loss functions that let us", "tokens": [50364, 300, 14256, 1526, 47345, 2365, 4201, 11, 457, 321, 366, 4232, 365, 4497, 4470, 6828, 300, 718, 505, 50716], "temperature": 0.0, "avg_logprob": -0.12544413849159522, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004465359263122082}, {"id": 720, "seek": 396328, "start": 3970.32, "end": 3975.6800000000003, "text": " converge much, much faster on something that is useful to the organism. And if we think about", "tokens": [50716, 41881, 709, 11, 709, 4663, 322, 746, 300, 307, 4420, 281, 264, 24128, 13, 400, 498, 321, 519, 466, 50984], "temperature": 0.0, "avg_logprob": -0.12544413849159522, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004465359263122082}, {"id": 721, "seek": 396328, "start": 3975.6800000000003, "end": 3982.1600000000003, "text": " consciousness, he has a point about agency in there. Agency means that you have a controller", "tokens": [50984, 10081, 11, 415, 575, 257, 935, 466, 7934, 294, 456, 13, 21649, 1355, 300, 291, 362, 257, 10561, 51308], "temperature": 0.0, "avg_logprob": -0.12544413849159522, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004465359263122082}, {"id": 722, "seek": 396328, "start": 3982.1600000000003, "end": 3987.6000000000004, "text": " that is able to control the future. Took me a while to understand this, but when I go up,", "tokens": [51308, 300, 307, 1075, 281, 1969, 264, 2027, 13, 38288, 385, 257, 1339, 281, 1223, 341, 11, 457, 562, 286, 352, 493, 11, 51580], "temperature": 0.0, "avg_logprob": -0.12544413849159522, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004465359263122082}, {"id": 723, "seek": 396328, "start": 3987.6000000000004, "end": 3991.28, "text": " we talked about BDI agents, and they seem to be quite complicated and convoluted,", "tokens": [51580, 321, 2825, 466, 363, 3085, 12554, 11, 293, 436, 1643, 281, 312, 1596, 6179, 293, 3754, 2308, 292, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12544413849159522, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004465359263122082}, {"id": 724, "seek": 399128, "start": 3991.6000000000004, "end": 3996.1600000000003, "text": " put a lot of quote there to make a BDI agent, but there's beliefs, desires, and intentions,", "tokens": [50380, 829, 257, 688, 295, 6513, 456, 281, 652, 257, 363, 3085, 9461, 11, 457, 456, 311, 13585, 11, 18005, 11, 293, 19354, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1792178196189678, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0030745440162718296}, {"id": 725, "seek": 399128, "start": 3996.1600000000003, "end": 4002.7200000000003, "text": " and so on. But if we think about what actually is a minimal agent, a thermostat is not a minimal", "tokens": [50608, 293, 370, 322, 13, 583, 498, 321, 519, 466, 437, 767, 307, 257, 13206, 9461, 11, 257, 8810, 39036, 307, 406, 257, 13206, 50936], "temperature": 0.0, "avg_logprob": -0.1792178196189678, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0030745440162718296}, {"id": 726, "seek": 399128, "start": 4002.7200000000003, "end": 4006.7200000000003, "text": " agent. So that has enough agency, it doesn't want anything. It just acts on the present", "tokens": [50936, 9461, 13, 407, 300, 575, 1547, 7934, 11, 309, 1177, 380, 528, 1340, 13, 467, 445, 10672, 322, 264, 1974, 51136], "temperature": 0.0, "avg_logprob": -0.1792178196189678, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0030745440162718296}, {"id": 727, "seek": 399128, "start": 4006.7200000000003, "end": 4012.2400000000002, "text": " frame by doing the obvious thing. But imagine that you give the thermostat the ability to", "tokens": [51136, 3920, 538, 884, 264, 6322, 551, 13, 583, 3811, 300, 291, 976, 264, 8810, 39036, 264, 3485, 281, 51412], "temperature": 0.0, "avg_logprob": -0.1792178196189678, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0030745440162718296}, {"id": 728, "seek": 399128, "start": 4012.2400000000002, "end": 4017.6800000000003, "text": " integrate the expected temperature differences, the differences over the future, when it does x now", "tokens": [51412, 13365, 264, 5176, 4292, 7300, 11, 264, 7300, 670, 264, 2027, 11, 562, 309, 775, 2031, 586, 51684], "temperature": 0.0, "avg_logprob": -0.1792178196189678, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0030745440162718296}, {"id": 729, "seek": 401768, "start": 4017.68, "end": 4023.12, "text": " or y now or does it a moment later, right? So suddenly you have a branching reality. And in", "tokens": [50364, 420, 288, 586, 420, 775, 309, 257, 1623, 1780, 11, 558, 30, 407, 5800, 291, 362, 257, 9819, 278, 4103, 13, 400, 294, 50636], "temperature": 0.0, "avg_logprob": -0.09684233916433234, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.0035931335296481848}, {"id": 730, "seek": 401768, "start": 4023.12, "end": 4028.0, "text": " this branching reality, you can make decisions and you will have preferences based on this", "tokens": [50636, 341, 9819, 278, 4103, 11, 291, 393, 652, 5327, 293, 291, 486, 362, 21910, 2361, 322, 341, 50880], "temperature": 0.0, "avg_logprob": -0.09684233916433234, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.0035931335296481848}, {"id": 731, "seek": 401768, "start": 4028.0, "end": 4033.68, "text": " integrated expected reward. So just by giving the thermostat the ability to model the future,", "tokens": [50880, 10919, 5176, 7782, 13, 407, 445, 538, 2902, 264, 8810, 39036, 264, 3485, 281, 2316, 264, 2027, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09684233916433234, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.0035931335296481848}, {"id": 732, "seek": 401768, "start": 4033.68, "end": 4039.12, "text": " you turn it into an agent. This is sufficient. And if you make this model deeper and deeper,", "tokens": [51164, 291, 1261, 309, 666, 364, 9461, 13, 639, 307, 11563, 13, 400, 498, 291, 652, 341, 2316, 7731, 293, 7731, 11, 51436], "temperature": 0.0, "avg_logprob": -0.09684233916433234, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.0035931335296481848}, {"id": 733, "seek": 401768, "start": 4039.12, "end": 4043.52, "text": " it's going to get better and better at it. And at a certain depth, the thermostat is going to", "tokens": [51436, 309, 311, 516, 281, 483, 1101, 293, 1101, 412, 309, 13, 400, 412, 257, 1629, 7161, 11, 264, 8810, 39036, 307, 516, 281, 51656], "temperature": 0.0, "avg_logprob": -0.09684233916433234, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.0035931335296481848}, {"id": 734, "seek": 404352, "start": 4043.6, "end": 4049.04, "text": " discover itself. It's going to discover the idiosyncrasies of its sensors. And notice that", "tokens": [50368, 4411, 2564, 13, 467, 311, 516, 281, 4411, 264, 4496, 2717, 34015, 3906, 530, 295, 1080, 14840, 13, 400, 3449, 300, 50640], "temperature": 0.0, "avg_logprob": -0.10643742448192532, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.010321741923689842}, {"id": 735, "seek": 404352, "start": 4049.04, "end": 4053.36, "text": " the sensor operates differently when it's closer to the heating element and so on and so on, right?", "tokens": [50640, 264, 10200, 22577, 7614, 562, 309, 311, 4966, 281, 264, 15082, 4478, 293, 370, 322, 293, 370, 322, 11, 558, 30, 50856], "temperature": 0.0, "avg_logprob": -0.10643742448192532, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.010321741923689842}, {"id": 736, "seek": 404352, "start": 4053.36, "end": 4058.48, "text": " So it becomes aware of how it functions. It might even become aware of the way in which its modeling", "tokens": [50856, 407, 309, 3643, 3650, 295, 577, 309, 6828, 13, 467, 1062, 754, 1813, 3650, 295, 264, 636, 294, 597, 1080, 15983, 51112], "temperature": 0.0, "avg_logprob": -0.10643742448192532, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.010321741923689842}, {"id": 737, "seek": 404352, "start": 4058.48, "end": 4066.0, "text": " and reasoning process works and to improve it or to account for its inefficiencies in certain ways.", "tokens": [51112, 293, 21577, 1399, 1985, 293, 281, 3470, 309, 420, 281, 2696, 337, 1080, 7167, 3341, 31294, 294, 1629, 2098, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10643742448192532, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.010321741923689842}, {"id": 738, "seek": 404352, "start": 4066.0, "end": 4072.64, "text": " And this is also what we do with our own cell. But this model of the self is not identical to our", "tokens": [51488, 400, 341, 307, 611, 437, 321, 360, 365, 527, 1065, 2815, 13, 583, 341, 2316, 295, 264, 2698, 307, 406, 14800, 281, 527, 51820], "temperature": 0.0, "avg_logprob": -0.10643742448192532, "compression_ratio": 1.7340425531914894, "no_speech_prob": 0.010321741923689842}, {"id": 739, "seek": 407264, "start": 4072.64, "end": 4078.48, "text": " consciousness. Our consciousness is a feeling of what it's like in the moment. It's the experience", "tokens": [50364, 10081, 13, 2621, 10081, 307, 257, 2633, 295, 437, 309, 311, 411, 294, 264, 1623, 13, 467, 311, 264, 1752, 50656], "temperature": 0.0, "avg_logprob": -0.1260144441024117, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.007448361720889807}, {"id": 740, "seek": 407264, "start": 4078.48, "end": 4083.92, "text": " of a now. There is an experience of a perspective that we are having, right? And this is what's", "tokens": [50656, 295, 257, 586, 13, 821, 307, 364, 1752, 295, 257, 4585, 300, 321, 366, 1419, 11, 558, 30, 400, 341, 307, 437, 311, 50928], "temperature": 0.0, "avg_logprob": -0.1260144441024117, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.007448361720889807}, {"id": 741, "seek": 407264, "start": 4083.92, "end": 4088.48, "text": " absent in the description of first. And he is missing the core point of what it means for", "tokens": [50928, 25185, 294, 264, 3855, 295, 700, 13, 400, 415, 307, 5361, 264, 4965, 935, 295, 437, 309, 1355, 337, 51156], "temperature": 0.0, "avg_logprob": -0.1260144441024117, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.007448361720889807}, {"id": 742, "seek": 407264, "start": 4088.48, "end": 4092.24, "text": " something to be conscious. It doesn't mean that it has a self. It doesn't even just mean it has a", "tokens": [51156, 746, 281, 312, 6648, 13, 467, 1177, 380, 914, 300, 309, 575, 257, 2698, 13, 467, 1177, 380, 754, 445, 914, 309, 575, 257, 51344], "temperature": 0.0, "avg_logprob": -0.1260144441024117, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.007448361720889807}, {"id": 743, "seek": 407264, "start": 4092.24, "end": 4099.04, "text": " first person perspective. It means that it experiences a reality. And this is not described", "tokens": [51344, 700, 954, 4585, 13, 467, 1355, 300, 309, 5235, 257, 4103, 13, 400, 341, 307, 406, 7619, 51684], "temperature": 0.0, "avg_logprob": -0.1260144441024117, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.007448361720889807}, {"id": 744, "seek": 409904, "start": 4099.12, "end": 4106.16, "text": " in this Friston quote. We explicitly asked him this question actually when we talked to him", "tokens": [50368, 294, 341, 1526, 47345, 6513, 13, 492, 20803, 2351, 796, 341, 1168, 767, 562, 321, 2825, 281, 796, 50720], "temperature": 0.0, "avg_logprob": -0.14676791630434186, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.006188967730849981}, {"id": 745, "seek": 409904, "start": 4106.16, "end": 4114.48, "text": " last time. And his response there was that this concept of feel like is really something that", "tokens": [50720, 1036, 565, 13, 400, 702, 4134, 456, 390, 300, 341, 3410, 295, 841, 411, 307, 534, 746, 300, 51136], "temperature": 0.0, "avg_logprob": -0.14676791630434186, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.006188967730849981}, {"id": 746, "seek": 409904, "start": 4114.48, "end": 4121.12, "text": " would need to be coded into the generative model that this agent has about the world. Number one,", "tokens": [51136, 576, 643, 281, 312, 34874, 666, 264, 1337, 1166, 2316, 300, 341, 9461, 575, 466, 264, 1002, 13, 5118, 472, 11, 51468], "temperature": 0.0, "avg_logprob": -0.14676791630434186, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.006188967730849981}, {"id": 747, "seek": 409904, "start": 4121.12, "end": 4125.04, "text": " it has to have a generative model as we've just been discussing. It has to be able to entertain", "tokens": [51468, 309, 575, 281, 362, 257, 1337, 1166, 2316, 382, 321, 600, 445, 668, 10850, 13, 467, 575, 281, 312, 1075, 281, 7655, 51664], "temperature": 0.0, "avg_logprob": -0.14676791630434186, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.006188967730849981}, {"id": 748, "seek": 412504, "start": 4125.04, "end": 4130.56, "text": " counterfactual, you know, possibilities for the predictive coding, right? And he's saying that", "tokens": [50364, 5682, 44919, 901, 11, 291, 458, 11, 12178, 337, 264, 35521, 17720, 11, 558, 30, 400, 415, 311, 1566, 300, 50640], "temperature": 0.0, "avg_logprob": -0.08037325600597346, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.001000407850369811}, {"id": 749, "seek": 412504, "start": 4130.56, "end": 4136.96, "text": " these feel like concepts would literally be encoded in that generative model as hypotheses", "tokens": [50640, 613, 841, 411, 10392, 576, 3736, 312, 2058, 12340, 294, 300, 1337, 1166, 2316, 382, 49969, 50960], "temperature": 0.0, "avg_logprob": -0.08037325600597346, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.001000407850369811}, {"id": 750, "seek": 412504, "start": 4136.96, "end": 4143.68, "text": " that we recognize, you know, so things like I'm feeling pain, for example, would be a concept", "tokens": [50960, 300, 321, 5521, 11, 291, 458, 11, 370, 721, 411, 286, 478, 2633, 1822, 11, 337, 1365, 11, 576, 312, 257, 3410, 51296], "temperature": 0.0, "avg_logprob": -0.08037325600597346, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.001000407850369811}, {"id": 751, "seek": 412504, "start": 4143.68, "end": 4148.8, "text": " within that model. And he says there's actually evidence from, you know, treating patients with", "tokens": [51296, 1951, 300, 2316, 13, 400, 415, 1619, 456, 311, 767, 4467, 490, 11, 291, 458, 11, 15083, 4209, 365, 51552], "temperature": 0.0, "avg_logprob": -0.08037325600597346, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.001000407850369811}, {"id": 752, "seek": 412504, "start": 4148.8, "end": 4153.6, "text": " chronic pain and this sort of thing that that's actually exactly what's happening in the mind", "tokens": [51552, 14493, 1822, 293, 341, 1333, 295, 551, 300, 300, 311, 767, 2293, 437, 311, 2737, 294, 264, 1575, 51792], "temperature": 0.0, "avg_logprob": -0.08037325600597346, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.001000407850369811}, {"id": 753, "seek": 415360, "start": 4154.160000000001, "end": 4160.08, "text": " that that the feeling of pain is actually a concept that's built in as a slot, if you will,", "tokens": [50392, 300, 300, 264, 2633, 295, 1822, 307, 767, 257, 3410, 300, 311, 3094, 294, 382, 257, 14747, 11, 498, 291, 486, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09502674488539106, "compression_ratio": 1.6376146788990826, "no_speech_prob": 7.141439709812403e-05}, {"id": 754, "seek": 415360, "start": 4160.08, "end": 4165.92, "text": " into this generative model. I mean, what do you think about that proposal?", "tokens": [50688, 666, 341, 1337, 1166, 2316, 13, 286, 914, 11, 437, 360, 291, 519, 466, 300, 11494, 30, 50980], "temperature": 0.0, "avg_logprob": -0.09502674488539106, "compression_ratio": 1.6376146788990826, "no_speech_prob": 7.141439709812403e-05}, {"id": 755, "seek": 415360, "start": 4167.52, "end": 4174.72, "text": " The semantics of pain are given by the avoidance that you don't want to experience the pain usually.", "tokens": [51060, 440, 4361, 45298, 295, 1822, 366, 2212, 538, 264, 5042, 719, 300, 291, 500, 380, 528, 281, 1752, 264, 1822, 2673, 13, 51420], "temperature": 0.0, "avg_logprob": -0.09502674488539106, "compression_ratio": 1.6376146788990826, "no_speech_prob": 7.141439709812403e-05}, {"id": 756, "seek": 415360, "start": 4175.6, "end": 4180.160000000001, "text": " And it could be that you cultivate the pain and use it to make something happening on the", "tokens": [51464, 400, 309, 727, 312, 300, 291, 33341, 264, 1822, 293, 764, 309, 281, 652, 746, 2737, 322, 264, 51692], "temperature": 0.0, "avg_logprob": -0.09502674488539106, "compression_ratio": 1.6376146788990826, "no_speech_prob": 7.141439709812403e-05}, {"id": 757, "seek": 418016, "start": 4180.24, "end": 4184.72, "text": " next level, but it requires that you are then building a multi-level control structure,", "tokens": [50368, 958, 1496, 11, 457, 309, 7029, 300, 291, 366, 550, 2390, 257, 4825, 12, 12418, 1969, 3877, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1638073670236688, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0016481186030432582}, {"id": 758, "seek": 418016, "start": 4184.72, "end": 4191.68, "text": " if you want to use pain productively, some artists are maybe doing. But the, you cannot have pain,", "tokens": [50592, 498, 291, 528, 281, 764, 1822, 1674, 3413, 11, 512, 6910, 366, 1310, 884, 13, 583, 264, 11, 291, 2644, 362, 1822, 11, 50940], "temperature": 0.0, "avg_logprob": -0.1638073670236688, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0016481186030432582}, {"id": 759, "seek": 418016, "start": 4191.68, "end": 4196.08, "text": " I think, without an action tendency, without something that modulates what you are doing.", "tokens": [50940, 286, 519, 11, 1553, 364, 3069, 18187, 11, 1553, 746, 300, 1072, 26192, 437, 291, 366, 884, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1638073670236688, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0016481186030432582}, {"id": 760, "seek": 418016, "start": 4196.96, "end": 4202.72, "text": " So your, your cognition is embedded into this engine. And to build such an engine that does it,", "tokens": [51204, 407, 428, 11, 428, 46905, 307, 16741, 666, 341, 2848, 13, 400, 281, 1322, 1270, 364, 2848, 300, 775, 309, 11, 51492], "temperature": 0.0, "avg_logprob": -0.1638073670236688, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0016481186030432582}, {"id": 761, "seek": 418016, "start": 4202.72, "end": 4208.48, "text": " that causally changes how you operate is not that hard. But when you live inside of such an engine,", "tokens": [51492, 300, 3302, 379, 2962, 577, 291, 9651, 307, 406, 300, 1152, 13, 583, 562, 291, 1621, 1854, 295, 1270, 364, 2848, 11, 51780], "temperature": 0.0, "avg_logprob": -0.1638073670236688, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0016481186030432582}, {"id": 762, "seek": 420848, "start": 4208.48, "end": 4213.2, "text": " it feels very strange that there is something that is happening that somehow depends on what", "tokens": [50364, 309, 3417, 588, 5861, 300, 456, 307, 746, 300, 307, 2737, 300, 6063, 5946, 322, 437, 50600], "temperature": 0.0, "avg_logprob": -0.0768326784657166, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.003271674970164895}, {"id": 763, "seek": 420848, "start": 4213.2, "end": 4217.839999999999, "text": " you are thinking, but you cannot control it. It controls you. It's upstream from you. You are", "tokens": [50600, 291, 366, 1953, 11, 457, 291, 2644, 1969, 309, 13, 467, 9003, 291, 13, 467, 311, 33915, 490, 291, 13, 509, 366, 50832], "temperature": 0.0, "avg_logprob": -0.0768326784657166, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.003271674970164895}, {"id": 764, "seek": 420848, "start": 4217.839999999999, "end": 4223.919999999999, "text": " downstream from it. And when you get upstream of your own pain, the pain stops being pain. It's", "tokens": [50832, 30621, 490, 309, 13, 400, 562, 291, 483, 33915, 295, 428, 1065, 1822, 11, 264, 1822, 10094, 885, 1822, 13, 467, 311, 51136], "temperature": 0.0, "avg_logprob": -0.0768326784657166, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.003271674970164895}, {"id": 765, "seek": 420848, "start": 4223.919999999999, "end": 4229.5199999999995, "text": " something that is a representation that you can now control and be able to get there. But it's", "tokens": [51136, 746, 300, 307, 257, 10290, 300, 291, 393, 586, 1969, 293, 312, 1075, 281, 483, 456, 13, 583, 309, 311, 51416], "temperature": 0.0, "avg_logprob": -0.0768326784657166, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.003271674970164895}, {"id": 766, "seek": 420848, "start": 4229.5199999999995, "end": 4234.5599999999995, "text": " not easy and you're not meant to get there because it means that we can immunize ourselves to pain", "tokens": [51416, 406, 1858, 293, 291, 434, 406, 4140, 281, 483, 456, 570, 309, 1355, 300, 321, 393, 13154, 1125, 4175, 281, 1822, 51668], "temperature": 0.0, "avg_logprob": -0.0768326784657166, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.003271674970164895}, {"id": 767, "seek": 423456, "start": 4234.56, "end": 4239.84, "text": " and sacrifice the organism to our intellectual interests. What's crucial about feelings when", "tokens": [50364, 293, 11521, 264, 24128, 281, 527, 12576, 8847, 13, 708, 311, 11462, 466, 6640, 562, 50628], "temperature": 0.0, "avg_logprob": -0.06966762883322579, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.0166354738175869}, {"id": 768, "seek": 423456, "start": 4239.84, "end": 4244.400000000001, "text": " you look at them introspectively is that feelings are essentially geometric. I don't know if you", "tokens": [50628, 291, 574, 412, 552, 560, 28713, 3413, 307, 300, 6640, 366, 4476, 33246, 13, 286, 500, 380, 458, 498, 291, 50856], "temperature": 0.0, "avg_logprob": -0.06966762883322579, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.0166354738175869}, {"id": 769, "seek": 423456, "start": 4244.400000000001, "end": 4250.320000000001, "text": " noticed that. So for instance, we notice feelings typically in our body. And that's because I think", "tokens": [50856, 5694, 300, 13, 407, 337, 5197, 11, 321, 3449, 6640, 5850, 294, 527, 1772, 13, 400, 300, 311, 570, 286, 519, 51152], "temperature": 0.0, "avg_logprob": -0.06966762883322579, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.0166354738175869}, {"id": 770, "seek": 423456, "start": 4250.320000000001, "end": 4255.68, "text": " that the feelings play out in a space. And the only space that we have always instantiated in our", "tokens": [51152, 300, 264, 6640, 862, 484, 294, 257, 1901, 13, 400, 264, 787, 1901, 300, 321, 362, 1009, 9836, 72, 770, 294, 527, 51420], "temperature": 0.0, "avg_logprob": -0.06966762883322579, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.0166354738175869}, {"id": 771, "seek": 423456, "start": 4255.68, "end": 4261.04, "text": " mind is the body map. So they're being projected into the space to make them distinct. And when we", "tokens": [51420, 1575, 307, 264, 1772, 4471, 13, 407, 436, 434, 885, 26231, 666, 264, 1901, 281, 652, 552, 10644, 13, 400, 562, 321, 51688], "temperature": 0.0, "avg_logprob": -0.06966762883322579, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.0166354738175869}, {"id": 772, "seek": 426104, "start": 4261.04, "end": 4267.04, "text": " look at the semantics of the feeling, we noticed that they are contracting or expanding or they", "tokens": [50364, 574, 412, 264, 4361, 45298, 295, 264, 2633, 11, 321, 5694, 300, 436, 366, 36095, 420, 14702, 420, 436, 50664], "temperature": 0.0, "avg_logprob": -0.10276271048046294, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.007221653591841459}, {"id": 773, "seek": 426104, "start": 4267.04, "end": 4272.08, "text": " are light or they're heavy and so on. This is all movement of stuff in space. It's all geometry", "tokens": [50664, 366, 1442, 420, 436, 434, 4676, 293, 370, 322, 13, 639, 307, 439, 3963, 295, 1507, 294, 1901, 13, 467, 311, 439, 18426, 50916], "temperature": 0.0, "avg_logprob": -0.10276271048046294, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.007221653591841459}, {"id": 774, "seek": 426104, "start": 4272.72, "end": 4276.72, "text": " plus valence, the stuff that is going to push your behaviors in a certain direction.", "tokens": [50948, 1804, 1323, 655, 11, 264, 1507, 300, 307, 516, 281, 2944, 428, 15501, 294, 257, 1629, 3513, 13, 51148], "temperature": 0.0, "avg_logprob": -0.10276271048046294, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.007221653591841459}, {"id": 775, "seek": 426104, "start": 4277.28, "end": 4284.16, "text": " So these are basically the interactions of some deep learning system that is producing", "tokens": [51176, 407, 613, 366, 1936, 264, 13280, 295, 512, 2452, 2539, 1185, 300, 307, 10501, 51520], "temperature": 0.0, "avg_logprob": -0.10276271048046294, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.007221653591841459}, {"id": 776, "seek": 428416, "start": 4284.16, "end": 4291.04, "text": " continuous geometric representations as perceived from an analytic engine. It's an interface", "tokens": [50364, 10957, 33246, 33358, 382, 19049, 490, 364, 40358, 2848, 13, 467, 311, 364, 9226, 50708], "temperature": 0.0, "avg_logprob": -0.12636840466371516, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.39010369777679443}, {"id": 777, "seek": 428416, "start": 4291.04, "end": 4296.88, "text": " between two parts of your mind, between the analytic attention control that is reflecting", "tokens": [50708, 1296, 732, 3166, 295, 428, 1575, 11, 1296, 264, 40358, 3202, 1969, 300, 307, 23543, 51000], "temperature": 0.0, "avg_logprob": -0.12636840466371516, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.39010369777679443}, {"id": 778, "seek": 428416, "start": 4296.88, "end": 4301.5199999999995, "text": " on the operations that your mind is doing while it's optimizing its attention. And the underlying", "tokens": [51000, 322, 264, 7705, 300, 428, 1575, 307, 884, 1339, 309, 311, 40425, 1080, 3202, 13, 400, 264, 14217, 51232], "temperature": 0.0, "avg_logprob": -0.12636840466371516, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.39010369777679443}, {"id": 779, "seek": 428416, "start": 4301.5199999999995, "end": 4306.72, "text": " system that represents the state of the organism entails where you should be going and makes this", "tokens": [51232, 1185, 300, 8855, 264, 1785, 295, 264, 24128, 50133, 689, 291, 820, 312, 516, 293, 1669, 341, 51492], "temperature": 0.0, "avg_logprob": -0.12636840466371516, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.39010369777679443}, {"id": 780, "seek": 428416, "start": 4306.72, "end": 4313.12, "text": " visible to you. It is a system that is not able to speak to you, uses geometry. And these", "tokens": [51492, 8974, 281, 291, 13, 467, 307, 257, 1185, 300, 307, 406, 1075, 281, 1710, 281, 291, 11, 4960, 18426, 13, 400, 613, 51812], "temperature": 0.0, "avg_logprob": -0.12636840466371516, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.39010369777679443}, {"id": 781, "seek": 431416, "start": 4314.16, "end": 4319.12, "text": " geometrical features, this is what we call feelings. So that's a very interesting connection.", "tokens": [50364, 12956, 15888, 4122, 11, 341, 307, 437, 321, 818, 6640, 13, 407, 300, 311, 257, 588, 1880, 4984, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14976424048928655, "compression_ratio": 1.5301724137931034, "no_speech_prob": 0.0006562264752574265}, {"id": 782, "seek": 431416, "start": 4319.12, "end": 4325.5199999999995, "text": " And I think Jeff Hawkins of Nemento would be quite interested in that as well, because", "tokens": [50612, 400, 286, 519, 7506, 9325, 10277, 295, 426, 1712, 78, 576, 312, 1596, 3102, 294, 300, 382, 731, 11, 570, 50932], "temperature": 0.0, "avg_logprob": -0.14976424048928655, "compression_ratio": 1.5301724137931034, "no_speech_prob": 0.0006562264752574265}, {"id": 783, "seek": 431416, "start": 4326.96, "end": 4334.639999999999, "text": " some of what he discussed with us was that, in his view, the evolution of, let's say,", "tokens": [51004, 512, 295, 437, 415, 7152, 365, 505, 390, 300, 11, 294, 702, 1910, 11, 264, 9303, 295, 11, 718, 311, 584, 11, 51388], "temperature": 0.0, "avg_logprob": -0.14976424048928655, "compression_ratio": 1.5301724137931034, "no_speech_prob": 0.0006562264752574265}, {"id": 784, "seek": 431416, "start": 4334.639999999999, "end": 4340.4, "text": " abstract thinking and whatnot actually came from systems that evolved to operate in just", "tokens": [51388, 12649, 1953, 293, 25882, 767, 1361, 490, 3652, 300, 14178, 281, 9651, 294, 445, 51676], "temperature": 0.0, "avg_logprob": -0.14976424048928655, "compression_ratio": 1.5301724137931034, "no_speech_prob": 0.0006562264752574265}, {"id": 785, "seek": 434040, "start": 4340.4, "end": 4347.839999999999, "text": " simple three-dimensional kind of motion, and that eventually those were reutilized by the", "tokens": [50364, 2199, 1045, 12, 18759, 733, 295, 5394, 11, 293, 300, 4728, 729, 645, 319, 20835, 1602, 538, 264, 50736], "temperature": 0.0, "avg_logprob": -0.14174493536891708, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0008829775615595281}, {"id": 786, "seek": 434040, "start": 4347.839999999999, "end": 4354.24, "text": " evolutionary process to start engaging in abstract thinking, which he views as movement", "tokens": [50736, 27567, 1399, 281, 722, 11268, 294, 12649, 1953, 11, 597, 415, 6809, 382, 3963, 51056], "temperature": 0.0, "avg_logprob": -0.14174493536891708, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0008829775615595281}, {"id": 787, "seek": 434040, "start": 4354.24, "end": 4358.0, "text": " through an abstract space. And so I think there's a lot of connection here to what you're saying", "tokens": [51056, 807, 364, 12649, 1901, 13, 400, 370, 286, 519, 456, 311, 257, 688, 295, 4984, 510, 281, 437, 291, 434, 1566, 51244], "temperature": 0.0, "avg_logprob": -0.14174493536891708, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0008829775615595281}, {"id": 788, "seek": 434040, "start": 4358.0, "end": 4363.92, "text": " about feeling, which is that, again, in a sense, our mind has reutilized this", "tokens": [51244, 466, 2633, 11, 597, 307, 300, 11, 797, 11, 294, 257, 2020, 11, 527, 1575, 575, 319, 20835, 1602, 341, 51540], "temperature": 0.0, "avg_logprob": -0.14174493536891708, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0008829775615595281}, {"id": 789, "seek": 436392, "start": 4364.64, "end": 4371.12, "text": " three, three plus one d movement mapping capability that it needed in order to survive in a three", "tokens": [50400, 1045, 11, 1045, 1804, 472, 274, 3963, 18350, 13759, 300, 309, 2978, 294, 1668, 281, 7867, 294, 257, 1045, 50724], "temperature": 0.0, "avg_logprob": -0.11929123053389988, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.006796504370868206}, {"id": 790, "seek": 436392, "start": 4371.12, "end": 4378.24, "text": " plus one d environment, physical environment, and it's reutilized those for mapping feelings,", "tokens": [50724, 1804, 472, 274, 2823, 11, 4001, 2823, 11, 293, 309, 311, 319, 20835, 1602, 729, 337, 18350, 6640, 11, 51080], "temperature": 0.0, "avg_logprob": -0.11929123053389988, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.006796504370868206}, {"id": 791, "seek": 436392, "start": 4378.24, "end": 4384.64, "text": " it's reutilized them for mapping to abstract thinking is like a form of motion in an abstract", "tokens": [51080, 309, 311, 319, 20835, 1602, 552, 337, 18350, 281, 12649, 1953, 307, 411, 257, 1254, 295, 5394, 294, 364, 12649, 51400], "temperature": 0.0, "avg_logprob": -0.11929123053389988, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.006796504370868206}, {"id": 792, "seek": 436392, "start": 4384.64, "end": 4391.76, "text": " space. Is that a fair connection? Yes, but I don't think that it's because it's", "tokens": [51400, 1901, 13, 1119, 300, 257, 3143, 4984, 30, 1079, 11, 457, 286, 500, 380, 519, 300, 309, 311, 570, 309, 311, 51756], "temperature": 0.0, "avg_logprob": -0.11929123053389988, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.006796504370868206}, {"id": 793, "seek": 439176, "start": 4391.76, "end": 4397.76, "text": " borrowed from the world in which we interact, but because it's the only game in town, it's the", "tokens": [50364, 26805, 490, 264, 1002, 294, 597, 321, 4648, 11, 457, 570, 309, 311, 264, 787, 1216, 294, 3954, 11, 309, 311, 264, 50664], "temperature": 0.0, "avg_logprob": -0.14710435500511757, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.009856265969574451}, {"id": 794, "seek": 439176, "start": 4397.76, "end": 4404.4800000000005, "text": " only mathematics that can deal with multi-dimensional numbers. So when we talk about spaces, we actually", "tokens": [50664, 787, 18666, 300, 393, 2028, 365, 4825, 12, 18759, 3547, 13, 407, 562, 321, 751, 466, 7673, 11, 321, 767, 51000], "temperature": 0.0, "avg_logprob": -0.14710435500511757, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.009856265969574451}, {"id": 795, "seek": 439176, "start": 4404.4800000000005, "end": 4409.04, "text": " talk about multi-dimensional numbers, about things that are not just a scalar in a single", "tokens": [51000, 751, 466, 4825, 12, 18759, 3547, 11, 466, 721, 300, 366, 406, 445, 257, 39684, 294, 257, 2167, 51228], "temperature": 0.0, "avg_logprob": -0.14710435500511757, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.009856265969574451}, {"id": 796, "seek": 439176, "start": 4409.04, "end": 4414.72, "text": " dimension, but features that are related. And sometimes you can take these features that you", "tokens": [51228, 10139, 11, 457, 4122, 300, 366, 4077, 13, 400, 2171, 291, 393, 747, 613, 4122, 300, 291, 51512], "temperature": 0.0, "avg_logprob": -0.14710435500511757, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.009856265969574451}, {"id": 797, "seek": 439176, "start": 4414.72, "end": 4418.96, "text": " measure continuously because they have too many steps to meaningfully discretize them.", "tokens": [51512, 3481, 15684, 570, 436, 362, 886, 867, 4439, 281, 3620, 2277, 25656, 1125, 552, 13, 51724], "temperature": 0.0, "avg_logprob": -0.14710435500511757, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.009856265969574451}, {"id": 798, "seek": 441896, "start": 4419.76, "end": 4426.16, "text": " So what it does is you discover that you can rotate something. And this is when you get a", "tokens": [50404, 407, 437, 309, 775, 307, 291, 4411, 300, 291, 393, 13121, 746, 13, 400, 341, 307, 562, 291, 483, 257, 50724], "temperature": 0.0, "avg_logprob": -0.2091151855804108, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.001000264659523964}, {"id": 799, "seek": 441896, "start": 4426.16, "end": 4431.2, "text": " space in the sense as we have a space to which we are moving. And these spaces which you can", "tokens": [50724, 1901, 294, 264, 2020, 382, 321, 362, 257, 1901, 281, 597, 321, 366, 2684, 13, 400, 613, 7673, 597, 291, 393, 50976], "temperature": 0.0, "avg_logprob": -0.2091151855804108, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.001000264659523964}, {"id": 800, "seek": 441896, "start": 4431.2, "end": 4439.76, "text": " rotate things only exist in 2D and 4D and 8D. And so the geometry that we're talking about is", "tokens": [50976, 13121, 721, 787, 2514, 294, 568, 35, 293, 1017, 35, 293, 1649, 35, 13, 400, 370, 264, 18426, 300, 321, 434, 1417, 466, 307, 51404], "temperature": 0.0, "avg_logprob": -0.2091151855804108, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.001000264659523964}, {"id": 801, "seek": 441896, "start": 4439.76, "end": 4445.12, "text": " constrained to certain mathematical paradigms, which you can derive from number theory,", "tokens": [51404, 38901, 281, 1629, 18894, 13480, 328, 2592, 11, 597, 291, 393, 28446, 490, 1230, 5261, 11, 51672], "temperature": 0.0, "avg_logprob": -0.2091151855804108, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.001000264659523964}, {"id": 802, "seek": 444512, "start": 4445.12, "end": 4452.08, "text": " compressed principles. And our brain is discovering a useful set of functions to model", "tokens": [50364, 30353, 9156, 13, 400, 527, 3567, 307, 24773, 257, 4420, 992, 295, 6828, 281, 2316, 50712], "temperature": 0.0, "avg_logprob": -0.11742088029969414, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.025152618065476418}, {"id": 803, "seek": 444512, "start": 4452.08, "end": 4458.0, "text": " anything, a set of useful computational primitives. And we can probably give our deep learning systems", "tokens": [50712, 1340, 11, 257, 992, 295, 4420, 28270, 2886, 38970, 13, 400, 321, 393, 1391, 976, 527, 2452, 2539, 3652, 51008], "temperature": 0.0, "avg_logprob": -0.11742088029969414, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.025152618065476418}, {"id": 804, "seek": 444512, "start": 4458.0, "end": 4462.88, "text": " a library of predefined primitives to speed up their convergence. That's also the reason why", "tokens": [51008, 257, 6405, 295, 659, 37716, 2886, 38970, 281, 3073, 493, 641, 32181, 13, 663, 311, 611, 264, 1778, 983, 51252], "temperature": 0.0, "avg_logprob": -0.11742088029969414, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.025152618065476418}, {"id": 805, "seek": 444512, "start": 4462.88, "end": 4468.0, "text": " there is useful transfer learning between different domains. You can train a vision model", "tokens": [51252, 456, 307, 4420, 5003, 2539, 1296, 819, 25514, 13, 509, 393, 3847, 257, 5201, 2316, 51508], "temperature": 0.0, "avg_logprob": -0.11742088029969414, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.025152618065476418}, {"id": 806, "seek": 444512, "start": 4469.76, "end": 4474.8, "text": " and use it as a pre-training for audio. And it's not because it's the same thing, but because it", "tokens": [51596, 293, 764, 309, 382, 257, 659, 12, 17227, 1760, 337, 6278, 13, 400, 309, 311, 406, 570, 309, 311, 264, 912, 551, 11, 457, 570, 309, 51848], "temperature": 0.0, "avg_logprob": -0.11742088029969414, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.025152618065476418}, {"id": 807, "seek": 447480, "start": 4474.8, "end": 4480.4800000000005, "text": " has learned useful computational primitives that it can apply across domains. But there is geometry", "tokens": [50364, 575, 3264, 4420, 28270, 2886, 38970, 300, 309, 393, 3079, 2108, 25514, 13, 583, 456, 307, 18426, 50648], "temperature": 0.0, "avg_logprob": -0.10296877543131511, "compression_ratio": 1.5305164319248827, "no_speech_prob": 0.000606998975854367}, {"id": 808, "seek": 447480, "start": 4480.4800000000005, "end": 4487.76, "text": " in the audio signal. So this is very interesting territory. I hope you'll come back to dive into", "tokens": [50648, 294, 264, 6278, 6358, 13, 407, 341, 307, 588, 1880, 11360, 13, 286, 1454, 291, 603, 808, 646, 281, 9192, 666, 51012], "temperature": 0.0, "avg_logprob": -0.10296877543131511, "compression_ratio": 1.5305164319248827, "no_speech_prob": 0.000606998975854367}, {"id": 809, "seek": 447480, "start": 4487.76, "end": 4492.64, "text": " this a bit more deeply when we have more time and a better connection, because I agree with you.", "tokens": [51012, 341, 257, 857, 544, 8760, 562, 321, 362, 544, 565, 293, 257, 1101, 4984, 11, 570, 286, 3986, 365, 291, 13, 51256], "temperature": 0.0, "avg_logprob": -0.10296877543131511, "compression_ratio": 1.5305164319248827, "no_speech_prob": 0.000606998975854367}, {"id": 810, "seek": 447480, "start": 4493.92, "end": 4496.16, "text": " Some very fascinating math here.", "tokens": [51320, 2188, 588, 10343, 5221, 510, 13, 51432], "temperature": 0.0, "avg_logprob": -0.10296877543131511, "compression_ratio": 1.5305164319248827, "no_speech_prob": 0.000606998975854367}, {"id": 811, "seek": 449616, "start": 4496.32, "end": 4506.16, "text": " Fantastic. Well, Dr. Yoshua Bak, it's, as I said, you've by far the most requested guest", "tokens": [50372, 21320, 13, 1042, 11, 2491, 13, 38949, 4398, 12063, 11, 309, 311, 11, 382, 286, 848, 11, 291, 600, 538, 1400, 264, 881, 16436, 8341, 50864], "temperature": 0.0, "avg_logprob": -0.16165663401285807, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.03391652926802635}, {"id": 812, "seek": 449616, "start": 4507.04, "end": 4511.04, "text": " that we've had right from the very beginning. So it's an honor to finally get you on the show.", "tokens": [50908, 300, 321, 600, 632, 558, 490, 264, 588, 2863, 13, 407, 309, 311, 364, 5968, 281, 2721, 483, 291, 322, 264, 855, 13, 51108], "temperature": 0.0, "avg_logprob": -0.16165663401285807, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.03391652926802635}, {"id": 813, "seek": 449616, "start": 4511.04, "end": 4514.48, "text": " And I hope we can get you back soon for a longer conversation. Thank you so much.", "tokens": [51108, 400, 286, 1454, 321, 393, 483, 291, 646, 2321, 337, 257, 2854, 3761, 13, 1044, 291, 370, 709, 13, 51280], "temperature": 0.0, "avg_logprob": -0.16165663401285807, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.03391652926802635}, {"id": 814, "seek": 449616, "start": 4515.12, "end": 4518.4, "text": " Likewise. I enjoyed this very much. Let's meet again soon.", "tokens": [51312, 30269, 13, 286, 4626, 341, 588, 709, 13, 961, 311, 1677, 797, 2321, 13, 51476], "temperature": 0.0, "avg_logprob": -0.16165663401285807, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.03391652926802635}], "language": "en"}