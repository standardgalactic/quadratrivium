start	end	text
0	4400	Today we have the privilege of speaking with Professor Chris Bishop,
4400	7920	a luminary in the field of artificial intelligence
7920	11760	and machine learning. Chris is a technical fellow
11760	15600	and director at Microsoft Research, AI for Science,
15600	20160	in Cambridge. He's also honorary professor of computer science
20160	24560	at the University of Edinburgh and fellow of Darwin College,
24560	28800	Cambridge. Hi, nice to meet you Tim. This is the new book on
28800	32960	Deep Learning Foundations and Concepts published with my son Hugh.
32960	36560	What proper have you got? Ethanol. I don't know if I'll use it but
36560	39360	we're going to talk about invariance. That's wonderful, that's wonderful.
39360	43520	Because you ought to get a little bit techy at some point. Oh yeah, our audience loves that.
43520	47600	In 2004 he was elected fellow of the Royal Academy of Engineering,
47600	52320	in 2007 he was elected fellow of the Royal Society of Edinburgh
52320	57600	and in 2017 he was elected fellow of the Royal Society.
57600	61200	Chris was a founding member of the UK AI Council
61200	65680	and in 2019 he was appointed to the Prime Minister's Council
65680	72080	for Science and Technology. At Microsoft Research, Chris oversees a global portfolio
72080	76880	of industrial research and development with a strong focus on machine learning
76880	81840	and the natural sciences. Chris obtained a BA in physics from Oxford
81840	85840	and a PhD in theoretical physics from the University of Edinburgh
85840	90720	with a thesis on quantum field theory. Chris's contributions to the field of
90720	96560	machine learning have been truly remarkable. He's authored one of the main
96560	100400	textbooks in the field which is Pattern Recognition and Machine Learning
100400	105360	or PRML. It has served as an essential reference for
105360	108480	countless students and researchers around the world.
108480	112160	Chris explained in the interview how it steered the field towards a more
112160	117280	probabilistic perspective at the time and he also mentioned his first textbook
117280	121600	Neural Networks for Pattern Recognition and its role in promoting neural
121600	125040	networks as a powerful tool for machine learning.
125040	129040	So this is the new textbook, Deep Learning, Foundations and Concepts
129040	132800	and one of the things that we're proud of with this book is the production values.
132800	136320	We really worked with the publisher to ensure the book would be
136320	139440	produced to a high physical quality and in particular it's produced with what
139440	142080	are called stitched signatures. So if you look down the edge there you'll see the
142080	147120	pages are not simply glued in. Instead this uses an offset printing
147120	151360	technique where 16 pages are printed on a big sheet of paper on both sides.
151360	155120	Some of the pages are turned upside down and then the page of the paper is
155120	158720	folded and then folded and then folded again and trimmed and the resulting
158720	162320	set is called a signature and actually stitched in with cord. And the point
162320	166560	about that is it allows the book to open flat so it means that the book is easy
166560	169920	to read and it means it should last a long time.
169920	172320	What are your favourite figures in the book Chris?
172320	175440	Well the ones produced by my son of course are the best. I mean here's a nice
175440	179760	picture of the transformer architecture which is this is GPT so
179760	182720	you could say it's one of the most important figures in the book I suppose
182720	185840	and I just love the way he's done this.
185840	188960	How did you do the research for this?
188960	194240	So that's a great question. I think you know one of the big
194240	196800	challenges with writing a book like this is knowing what to include and
196800	200080	what not to include and with literally thousands of papers being
200080	203120	published every month it can be overwhelming for the authors never
203120	205760	mind the readers. So I think the value we add in the book
205760	209120	is trying to distill out what we think of as the core concept.
209120	213600	So part of this was really looking at key papers in the field
213600	219040	seeing what relatively recent ideas there are but also trying to focus down on
219040	222080	techniques and ideas that we believe will actually stand the test of time.
222080	225440	We don't have this book to go out of date in a year or two we want it to have
225440	229440	have lasting value and of course it's quite possible there'll be a breakthrough
229440	232080	next week and that it will turn out to be a very important
232080	236400	new architecture but for the most part many of the core concepts actually go
236400	239760	back a long way and so what we've really done is taken some
239760	242960	some of the foundations of the field and brought them into the modern deep learning
242960	246800	era but the idea of probabilities the idea of gradient based methods and so on
246800	249440	those have been around for decades and they're just as applicable today as
249760	252560	they ever were. One of the things I really like actually is the
252560	256560	chapter on convolutional networks. My son Hugh did a lot of this
256560	260560	chapter he works on using techniques like convolutional neural
260560	264320	nets as part of his work on autonomous vehicles and I think there's a really
264320	267680	nice description here of convolutional networks
267680	271040	really from the ground up explaining the the basic concepts and
271040	274960	but also motivating them not just saying this is how a convolutional network is
274960	278320	built but why is it built this way how do we actually motivate it so that's
278320	281280	one of my favorite chapters as well. Yeah it's been a very interesting career
281280	285280	and at this stage of the career I can now finally look back and make sense of it
285280	287680	but at the time it felt like a bit of a random walk
287680	292480	so actually when I was a teenager I went to see 2001 A Space Odyssey it was
292480	295760	actually very inspired by that rather abstract concept of an
295760	298720	artificial intelligence very different from the usual sort of Hollywood
298720	302000	the trail of robots so I was very interested in the idea of artificial
302000	306320	intelligence from a young age but I was very uninspired by the field of AI
306320	309440	at the time which was very much sort of rule based and and didn't seem to be on
309440	312480	a path through intelligence and then I did a PhD in quantum field theory which
312480	315760	is a very hot field at the time gauge field theory
315760	319920	at Edinburgh University had a wonderful time at the end of my PhD though I wanted
319920	322560	to do something a bit more practical a bit more useful
322560	326480	and so I went into the fusion program I'm a big fan of nuclear fusion
326480	329840	it was sort of 30 years away then and it's kind of still 30 years away now but
329840	333120	I'm still a big believer but I went to work on
333920	337920	talk about physics essentially theoretical physics of of plasmas trying to
337920	341040	trying to understand the instabilities and control them
341040	344400	so I was working very happily as a theoretical physicist having a having a
344400	348080	great time and after about 10 years or so as a theoretical physicist
348080	352080	Jeff Hinton published the backprop paper and it came to my attention
352080	355920	and I found that very inspiring because there I saw a very very different
355920	361600	approach to towards intelligence and so I started by applying neural
361680	365360	networks to data from the fusion program because it was big data
365360	368560	in its day I was I was working there to the the jet
368560	372080	tokamak and they had many many high resolution diagnostics I had lots of
372080	374800	data to play with and I became more and more fascinated by
374800	378400	neural networks and then I did a sort of completely crazy thing I walked away
378400	381440	from a very respectable career as a theoretical physicist
381440	384720	and went full-time into the field of neural nets which at the time was not
384720	387760	really a respectable field I would say it's not wasn't mainstream computer
387760	390560	science it certainly wasn't physics it wasn't really anything
390560	393600	but I just found it very inspiring and I was particularly inspired by the work
393600	396720	of Jeff Hinton and so I've been in that field for
396720	400160	you know three and a half decades now and of course
400160	403600	recent history suggests that was probably a good career move
403600	407360	and now most recently I've brought the two ends of my career together because
407360	410720	I'm now very excited about the impact that neural nets and machine learning
410720	413280	are having on the natural sciences including physics.
413280	417600	Hinton is a famous connectionist so he believes that knowledge is
417600	422160	sub-symbolic and I speaking with Nick Chater the other week he had a
422160	425600	book called The Mind is Flat which is talking about the inscrutability of
425600	429120	our brains. How do you feel that things have changed?
429120	432880	I mean you were talking about a convergence of these different ideas in AI.
432880	435520	I think one thing that's very interesting is that there has been a lot of
435520	438560	discussion let's say from 2012 onwards when deep
438560	442320	learning was clearly being very successful a lot of discussion that it was
442320	446080	missing the sort of symbolic approach that we somehow to find a way to combine
446080	449360	this connectionist approach to use that sort of probably rather dated term now
449360	451760	but that sort of you know that neural net approach
451760	455040	with the more traditional symbolic approach and I think what we've seen
455040	458160	with models like GPT-4 for example that it's perfectly capable of
458160	461520	reasoning at a more symbolic level not at the level of a human being of course
461520	465040	but it can do that that kind of more abstract higher level
465040	468160	reasoning and so I think what we're seeing with neural
468160	470880	nets is rather like the human brain. The human brain doesn't have a
470880	474960	connectionist neural net piece then some other machinery that does symbolic
474960	478480	reasoning that that that same substrate is capable of all of these
478480	481280	different kinds of reasoning and these different kinds of intelligence
481280	484080	and we're starting to see that emerge now with neural net so I think
484080	488080	I think for me the discussion of should we somehow combine symbolic reasoning
488080	491520	with with connectionism no that that to me that's a piece of history
491520	494960	it's about how can we how can we expand on the capabilities of neural nets.
494960	498160	Yeah that's so interesting I remember there was a paper by Polition I think it
498160	502000	was the the connectionist critique in 1988
502000	505520	and I was quite sold on this idea of you know systematicity and
505520	510080	productivity and so on and even now folks from that school of thought
510080	513680	think that our brains are Turing machines this ability to address
513680	518400	potential infinity and I guess what I'm getting from
518400	521120	what you're saying is that the distinction isn't really there anymore you
521120	524080	can do that kind of reasoning with neural networks.
524080	527760	Well I take a very simple view which is that neural nets in that since 2012 in
527760	531280	particular have been shown to be spectacularly capable
531280	534720	and there's no end in sight the rate of progress is faster now than ever
534720	538160	so it seems very straight nobody imagines that that machine learning and
538160	541040	deep learning has suddenly ended at you know whatever the time is today you
541040	543840	know this is this is this is the beginning of an S-curve
543840	547840	so the idea that we would worry so much about the limitations of neural
547840	550800	networks and what they can't do I think we just you put the word yet at the end
550800	554480	of it your neural networks can't do x y and z yet but but I don't think
554480	558400	any sense we've hit the buffers of of what neural nets can do and it's by
558400	561920	far the most successful of the most rapidly advancing technology we have
561920	565360	so to me you should look for the keys under the lamppost we have this powerful
565360	569040	technology that's getting better by the week why would we not see how far we
569040	571360	can push it rather than worry about its limitations.
571360	575600	Absolutely now Professor Bishop you are incredibly famous
575600	579040	for your book PRML but of course it wasn't your first book as you were just
579040	582160	speaking to but what was I mean could you just tell us about your your
582160	585360	motivations and just the thought process behind that book?
585360	588880	Yes so as you said it wasn't my first book my first book is published in 1995
588880	592160	Neural Networks for Pattern Recognition and that book had a very specific
592160	595440	motivation which is that I was a newcomer to the field I mentioned earlier
595440	597760	that I got excited about backprop and and sort of
597760	600560	transition from theoretical physics into machine learning.
600560	603520	That was my way of learning about the field you know if you're a university
603520	606880	professor a great way to learn about something is to teach a course on it
606880	609520	because it forces you to think about it very carefully you're going to get
609520	613280	tricky questions from smart students and you're very motivated to to really
613280	617280	understand it and so for me the analogue of that was was writing a book.
617280	621760	PRML was rather different by the time we got to published in 2006 and by then
621760	625120	the field was much larger its sense it was much more mature as a much more
625120	628880	established and respected field there were many courses on machine learning
628880	632560	the goal there was very different I simply wanted to write the as it were
632560	635440	the book that everybody would use to learn about the field so it was trying to
635440	639920	be comprehensive but trying to be to explain the concepts as clearly as
639920	645440	possible and so really that was the goal the goal was to in a sense you know
645440	648800	replace the earlier neural nets for pattern recognition book which was
648800	652800	which serves an important role in its day I think but really try to produce a
652800	656640	single coherent text where people could learn about the different topics in
656640	659600	you know with a shared notation and hopefully trying to explain things as
659600	662320	clearly as I could. We know in theoretical physics you know you can
662320	665200	you can write down an equation but solving it may be extremely difficult
665200	668800	you have to resort to approximations but it's still nice to have that that
668800	673360	north star that compass that guides you and so for me I try to think of machine
673360	677040	learning in similar terms there are some some foundations that that really
677040	681440	don't change much over time that are that are very good guiding principles and
681440	684560	we're dealing with data we're dealing with uncertainty we want to be
684560	689040	quantitative so you're led very naturally indeed uniquely into probability theory
689040	692160	and if you apply probability theory consistently that is the Bayesian
692160	696160	framework so for me the Bayesian framework is a very natural
696160	699600	bedrock on which you can build and think about machine learning now just as
699600	702640	with theoretical physics you can't often just solve things exactly and
702640	706080	certainly the Bayesian paradigm calls for
706080	709920	integration or marginalization of all possible values of the parameters in
709920	712960	your neural network well you always operate with a fixed
712960	716400	computational budget right it may be a huge one but it be always constrained
716400	720320	by by computational budget and should you spend that budget doing a very
720320	723680	thorough Bayesian marginalization over a small neural network
723680	726960	or should you take the same number of compute cycles and train a very much
726960	730320	larger network and if you have plenty of data to train the larger network then
730320	733600	the latter seems to be much more effective in a practical sense
733600	737280	so while from a practical point of view the Bayesian approach still has
737280	741920	certain applications in in various domains for the most part it's not the
741920	745520	framework we'd want to use in in sort of mainstream
745520	749200	machine learning today we're much more interested in scale and making point
749200	753040	estimates in using stagastic gradient send and so on so I still
753040	756720	think that students should learn the basic ideas of of Bayesian inference
756720	759360	because really they have to learn you have to learn about probability I don't
759360	762400	think you can be in machine learning and not understand probability
762400	765520	and then once you understand probability and you apply it uniformly that
765520	769600	that really is the Bayesian framework so I think it's the foundation
769600	773120	but then you're led to make approximations and in particular you make
773120	776400	point estimates so in practice you don't actually execute the full Bayesian
776400	780560	paradigm yeah I agree that um Bayesian reasoning is
780560	784400	it's beautiful and it's the continuation even of sort of propositional
784400	787520	logic in the domain of uncertainty it's fundamental
787520	792400	but there is this question of the world is a very gnarly place
792400	797200	and folks argue that the brain is a kind of Bayesian inference machine
797200	802560	but it can't it can't possibly be solving the intractable Bayesian problem
802560	806400	and therein lies the question so there are many hybrids or
806400	810080	even deep learning approaches could be seen as some kind of a continuation or
810080	814480	somewhere on the spectrum between maximum likelihood point estimation and
814480	818160	Bayesian models I mean how do you think about that
818160	820560	spectrum I think that's a great that's a great question I
820560	823920	think you're spot on there if you look back to a time when
823920	826720	there are a lot of competitions here's a data set we're going to hold out the
826720	829520	test set you've got to score as high as you can on the test set
829520	834000	and what approach should you use the winner always is an ensemble you
834000	837520	should try 10 different things preferably diverse and then combine
837520	841120	them suitably maybe taking an average or some smarter combination
841120	845280	and that ensemble will always outperform any one single model
845280	848480	so if you're not constrained by compute and in some of those competitions you
848480	852080	weren't then the ensemble always wins and you can think about that ensemble is
852080	855200	like as you say a sort of rough and ready approximation
855200	859600	to a full marginalization of all of the uncertainty in the predictions that you
859600	862080	might make and so I think there's a little glimmer of
862080	864800	sort of Bayesian approaches coming through there but again
864800	868080	you know in the modern era you're probably better off training one single
868080	871120	large model than 10 smaller ones and averaging
871120	875680	so it's so I think knowing about the Bayesian paradigm and understanding
875680	879040	where you can learn from it is still valuable today
879040	883840	but nevertheless it's unlikely in most applications that you're going to want
883840	886960	to apply the full Bayesian machinery because it's just so computational
886960	890400	expensive fascinating I mean just one more thing on this
890400	894320	do you think of large you know let's say large language models but large deep
894320	897680	learning models do you think of them as one model or do you think of
897680	901200	them as an inscrutable bundle of models because we're kind of getting into the
901200	905280	no-free lunch theorem here coming from the Bayesian world we
905280	909520	design models you know using principles and with neural networks we just
909520	912960	train these big black boxes so do you think of them as one model or lots of
912960	916000	models I certainly I always think of them as a
916000	918960	single model I've never thought thought of them as separate models unless you
918960	921520	unless you explicitly construct a mixture of experts or something like that
921600	925280	you have an internal an internal structure I guess
925280	928080	everything is sort of very distributed and somehow sort of holographic and
928080	932240	overlapping and you know a remarkable thing about GPT-4 is that
932240	936160	you know you often see people when they first they first use it they'll ask
936160	938400	some question how tall is the Eiffel Tower and it probably gets the right
938400	941600	answer and you know it's like oh that's kind of interesting and you're sort of
941600	944480	a little bit disappointed in this technology but it's like being given
944480	948000	the keys to a very expensive sports car and you notice the cup holders and you
948000	950800	notice that it can can support a cup rather nicely you don't realize you
950800	953680	need to start the engine and and drive off in it to really get the full
953680	956320	experience and so until you realize that actually you can you can have a
956320	959840	conversation it can it can write poetry it can explain jokes it can write code
959840	962880	it can do so many many different things and that all those capabilities
962880	966400	embedded in the same model and and what is I think a really interesting
966400	970960	lesson of the last few years is that models like GPT-4 outperform the
970960	974720	specialist models so for example in my lab we had a project for many years which
974720	978160	essentially said the following it said well you know this is Microsoft
978240	981440	world's biggest software company we have lots of source code we could use source
981440	984880	code as training data for machine learning we've added all sorts of things you
984880	988560	know spot bugs do water complete you know all kinds of things you could do if
988560	992320	you had a good model of source code and the project was reasonably successful it
992320	995760	was you know it worked reasonably well but what we've learned is that when you
995760	1000640	build one gigantic model that that yes it sees source code it sees scientific
1000640	1004400	papers it sees wikipedia it sees many many different things in some way it
1004400	1008480	becomes better at writing source code than a model specifically for writing
1008480	1012400	source code and there are even even in ablation studies where people have a
1012400	1015520	model that's trained to solve maths problems and it does reasonably well
1015520	1019040	and now you give it some apparently irrelevant information let's say from
1019040	1022400	wikipedia but with anything to do with maths stripped out and you find it
1022400	1025120	actually does better at the maths so I think there are things here that we
1025120	1028960	don't really understand but the general lesson I think is fairly clear
1028960	1032720	that when you have a larger very general model it can outperform
1032720	1036320	a specific model which I think is very interesting I guess the reason I was
1036320	1040320	talking about the no free lunch theorem is it feels to me as you say that
1040320	1044800	models behave quite differently in an input sensitive way
1044800	1047520	so you ask them about this particular thing and it's almost like it's a
1047520	1051200	different model because different parts of the model get activated
1051200	1055920	and then there's this question of well is the no free lunch theorem violated
1055920	1059840	can there be such a thing as a general foundational agent
1059840	1064560	that could in robotics just do really well in any game or any environment
1064560	1069200	or do you think do you think there's still some need for specialization
1069200	1074640	another great question so I think these really open research questions honestly
1074640	1078560	I'm not sure anybody really knows but I think one of the lessons is that the
1078560	1082160	general can be more powerful than the specific so clearly one of the research
1082160	1084560	frontiers we should push on is greater and greater
1085520	1089840	and see so you know GBT4 can't ride a bicycle but if we have models that can
1089840	1093040	can do robotics should they be separate and distinct models or if we somehow
1093040	1095840	combined everything into a single model would it be more powerful
1095840	1098640	and there's a decent chance that the latter would be true that it would be
1098640	1100960	more powerful so certainly that's one research frontier
1100960	1104640	we should push on an area I'm very interested in these days is
1104640	1108400	is deep learning for for science for scientific discovery and science
1108400	1112480	amongst other things involves very precise detailed numerical calculations
1112480	1116000	now if you want to multiply some numbers together GBT4 would be a terrible way of
1116000	1118560	doing it it might give you the wrong answer and even if it gets the right
1118560	1122160	answer you're burning a tremendous amount of compute cycles to do something you
1122160	1124960	could do with the far fewer compute cycles
1124960	1129440	so there will still as far as I can see in certain domains be a role for
1129440	1132640	specialist models but even then I can see them being
1132640	1135280	integrated with things like large language models
1135280	1140320	partly to provide human interface because one of the one of the things about
1140320	1143360	language models is they they're so easy to interact with you don't have to be a
1143360	1146480	computer program you just have a a natural conversation with them
1146480	1150720	but also the other remarkable thing about the large language models
1150720	1153520	I think there are two remarkable things the first of all is that they're so good
1153520	1156800	at human language maybe that's not too surprising because they're sort of
1156800	1161840	designed to do that but by virtue of being forced to
1161840	1165920	effectively compress human language they become reasoning engines and that's a
1165920	1169600	remarkable discovery right that is a big surprise certainly to me I
1169600	1172000	think to many people perhaps to everybody in the field
1172000	1175600	that they can function as reasoning engines and so even if you're
1175600	1178960	let's say doing some specialist scientific calculations you might still
1178960	1183120	think about a large language model as as a kind of a
1183120	1186560	co-pilot for the scientist helping the scientist reason over
1186560	1191120	what increasingly consists of massive massively complex spaces
1191120	1194080	very high dimensionality many different modalities of data
1194080	1196960	it's harder and harder for humans to sort of wrap their head around this and this
1196960	1200480	is where I think a large language model can can can be valuable
1200480	1203680	but I still see it calling on specialist tools in the foreseeable future
1203680	1206800	because you were talking about statistical generalization but you could
1206800	1210800	argue that language models can't do let's say they can't compute the
1210800	1213920	nth digit of pi because they don't have an expandable memory they're not
1213920	1216560	Turing machine so that that's a computational limitation but
1216560	1219680	but they might be able to do this statistical generalization
1219680	1223440	as we were talking about even though it might in fact be a weird form of
1223440	1228240	specialization in terms of an ensemble of methods of models inside
1228240	1231360	a large language model but on the on the language thing and the reasoning this
1231360	1235120	this is fascinating so I think that language
1235120	1239120	is a bunch of memetically embedded programs
1239120	1244480	so we we play the language game and we establish cognitive categories we
1244480	1246560	embed them and share them socially and it's like
1246560	1249680	there's a little simulation out there and I'm using that to think
1249680	1252640	but the question always is to what extent
1252640	1256960	and is that that's a bunch of processing that previous humans have done
1256960	1260480	and we can use it but can the language model create
1260480	1264400	new programs like that this is I think part of a fascinating and broader
1264400	1268160	discussion so I do hear a lot of oh it can't do x y and z
1268160	1271120	often that's true and I've always put the word yet at the end of it because I
1271120	1273600	don't know any law or physics and it can't do certain there are some things
1273600	1276320	which perhaps the current architectures provably can't do
1276320	1279360	but but there's lots of exploration in different architectures there's a lot of
1279360	1282400	scope for for for expanding and generalizing neural net so I
1282400	1284800	always think of it can't do a certain thing yet
1284800	1288480	but a lot of the questions or a lot of the comments about
1288480	1292640	the limitations of models I have a have a hypothesis on this I mean
1292640	1296000	let me test this out on you I may be I may be way shorter the mark on this one
1296000	1299520	but a lot of the a lot of the critique of what models
1299520	1303600	seemingly can't do or especially when it's oh they will never be able to do this
1303600	1307120	they cannot be creative or they cannot reason or they cannot whatever
1307200	1311360	I wonder if a lot of this comes to to a much more fundamental point that's not
1311360	1313600	actually a technical one it's really to do with the human
1313600	1316720	the human journey over the last few thousand years because we've
1316720	1320080	you know a few thousand years ago I guess most humans would have perceived
1320080	1322640	humanity as the center of the universe they were the earth's center of the
1322640	1326000	universe the universe was created for the benefit of humanity
1326000	1329200	we had this very arrogant view of our own importance and what we've learned over
1329200	1331680	the centuries especially from fields like astronomy
1331680	1335520	is of course you know that the the entirety of humanity's existence
1335520	1339760	is a brief blink of the eye compared to the distance of the the whole universe
1339760	1342960	and and our physical place in the universe in terms of length scale we're on a
1342960	1345760	little speck of dust orbiting an insignificant star in a rather boring
1345760	1350160	galaxy in this colossal universe and and so I think it's natural for us as
1350160	1353840	humans to sort of continue to cling to the things that we feel make us special
1353840	1356080	and we're certainly not the fastest creatures on earth we're not the
1356080	1358720	strongest but it's our brains that seem to make us
1358720	1362080	unique we are the most intelligent creatures by far on earth
1362080	1365440	and so we think of our of our intelligence as being the very special
1365440	1367840	thing yes okay we get it that we're just living in a
1367840	1370800	sporing corner of the universe but nevertheless it's our brains that make
1370800	1374080	us special so let me tell you a little story which is
1374080	1377040	because I work for Microsoft I was very privileged to have early access to
1377040	1380400	GPT-4 and it was still a highly-tented highly-secret
1380400	1384880	project and so I was exposed to GPT-4 at a time when
1384880	1388160	I could only discuss it with a very small number of very specific colleagues
1388160	1390880	and for everybody else I couldn't couldn't even talk about it
1390880	1396080	and it was quite a shocking moment the the ability to
1396080	1399680	understand and generate language sort of didn't come as so much of a surprise
1399680	1402720	because of course I'd been following GPT-2 and GPT-3 and
1402720	1405280	you know knew this technically was getting better
1405280	1409600	but this ability to reason there was a sort of visceral reaction I had which
1409600	1414000	took me right back to that film 2001 that sense of I was engaging with something
1414000	1416960	which you know my colleague Sebastian Bubeck called it the sparks of
1416960	1420160	artificial intelligence so nobody in that nobody's claiming GPT-4 is
1420160	1423040	anywhere close to human intelligence or anything like that but there was just
1423040	1426800	the first glimpse of something it was the first time in my life that I'd
1426800	1429680	interacted with something that wasn't a human being
1429680	1434240	that had a glimmer of this this higher level of intelligence
1434240	1440080	and and realizing this may be the dawn of a of a new era that may be
1440080	1443600	even more significant than the 2012 moment of the dawn of deep learning
1443600	1447840	there was something very special going on and I wonder if part of the reaction
1447920	1451840	that we have to these models is a little bit of that sense of that threat to
1451840	1455120	the specialness that we feel as humans now maybe completely wrong this is purely
1455120	1458960	speculation but you know it's interesting that we
1458960	1461360	talk about people use phrases like stochastic
1461360	1464400	parody it's just regurgitating stuff that it that it's seen before
1464400	1467120	some people claim or you know of course it hallucinates sometimes it comes up
1467120	1470320	with stuff that's just wrong or doesn't make sense
1470320	1474480	but but think about the following imagine there was a very very smart
1474480	1478160	physics student went to you went to a top university worked really hard for
1478160	1481680	four years what would they do they would they would read books they would
1481680	1484960	read papers listen to lectures have discussions with their professors and
1484960	1487520	with other students and then they sit their final exam
1487520	1491760	and they get 95% in their final exam and they come top of the year
1491760	1496000	we don't say huh well 95% of the time there are stochastic parrot
1496000	1499600	regurgitating Einstein and Maxwell and the other 5% of the time they're
1499600	1502080	hallucinating no we say congratulations you have a
1502080	1505520	first-class honors degree you've graduated with honors this is this is a
1505520	1511200	you know a wonderful achievement so it's interesting that we do seem to view
1511200	1515280	the the capabilities of of neural nets with it with almost a different ruler
1515280	1518960	to that of humans and while nobody's suggesting that current models are
1518960	1522480	anywhere close to humans on many axes of intelligence
1522480	1526560	nevertheless i see the first sparks of of artificial intelligence
1526560	1531040	and just one final comment the term AI artificial intelligence has been very
1531040	1534320	popular for many years i used to hate it i used to always say that's machine
1534320	1536640	learning none of these systems are intelligent they're very good at
1536640	1540080	recognizing cats and images there's nothing really intelligent about this
1540080	1544480	in in in in one sense and yet now i find for the first
1544480	1546480	time i feel comfortable talking about artificial
1546480	1549600	intelligence because i think we've taken the first baby steps towards what i
1549600	1552960	think of as true artificial intelligence i still think that
1552960	1556800	agency and creativity are the distinguishing feature
1556800	1560400	not necessarily that we are and biological beings
1560400	1564640	it's more to do with we are independent agents and we are
1564640	1568320	sampling random things from our local worlds and we're combining them
1568320	1572080	together in in interesting ways and in doing so
1572080	1575760	intelligence is about the process of building models
1575760	1579520	and sharing models and embedding models in in our culture
1579520	1582720	so it feels to me that gpt was building models
1582720	1587840	at the time it was trained and and and that's all it's doing
1587840	1590880	i can imagine a world where there were lots of gpt's
1590880	1595120	we all had gpt in our pockets and maybe then it would be much more like
1595120	1598640	biomimetic intelligence i think there are lots of interesting
1598640	1603680	points that you touched on there tim so i think one thing is in terms of
1603680	1607040	creativity you know are these systems creative it's certainly true they only
1607040	1610080	exist because of humans they're created by humans
1610080	1613200	and and we should acknowledge that but i don't think it means they're
1613200	1617520	intrinsically not creative if i asked an artist to
1617520	1620800	paint me a picture of some people walking on the beach with a sunset or
1620800	1623280	whatever and they came back a few days later with some
1623280	1626480	beautiful picture i might hate it they may have used very vivid
1626480	1629840	colors i might like pale pastel colors but that's a matter of opinion
1629840	1632800	but i wouldn't deny that there was creativity there
1632800	1636560	but their expertise came because well they went they perhaps had some
1636560	1640000	intrinsic ability in some sense but they went to art school they study the
1640000	1642640	work of other artists they practice they got better
1642640	1647520	and and and that creativity owes a lot to what went before but i don't think it
1647520	1650240	diminishes that in the same way a physics student who can
1650240	1653200	explain the theory of relativity you have to say well you didn't invent the
1653200	1656080	theory of relativity you know einstein invented that you only learned it from
1656080	1659760	einstein but it doesn't diminish the the fact
1659760	1662400	that they have understanding the fact that they convey it and the fact they
1662400	1665200	can potentially think in new ways and be creative
1665200	1671680	so i'm i'm less convinced about discussions about the limitations of
1671680	1675600	of of the technology in general of where it can go i don't particularly see any
1675600	1678960	limitations the brain is a machine that uses this
1678960	1681840	a term used earlier connectionist approach it uses these fine-grained
1681840	1685040	neural nets and and so there are similarities to the
1685040	1687920	technology that we have now there are also huge differences
1687920	1691120	some of those differences point to the artificial neural nets being much more
1691120	1694000	powerful than biological neural nets and hinted made a strong
1694000	1696320	point of this lately and i think it's a very interesting
1696320	1700880	perspective so i would be the first to say
1700880	1705120	yes the technologies we have on many axes are a long way short of humans on
1705120	1708320	many axes the much better gbt4 can create text
1708320	1712480	much better than any human i mean to produce a page of coherent text that's
1712480	1715760	correctly punctuated in good grammar and so on in a few seconds there aren't
1715760	1720000	any people that can do that i think so on an increasing number of axes
1720000	1723840	systems clearly outperform humans and on others there's still a very long
1723840	1727840	way to go but i think one of the nice things about technologies like this
1727840	1731680	generative ai technologies whether it's you know saura for creating videos or
1731680	1735920	gbt4 or whatever it might be is they do rely on the prompt there is a clear
1735920	1740480	role they are co-pilots as we say they they they sit there and do nothing
1740480	1745040	and you use them as a sort of a cognitive amplifier you have an idea
1745040	1748400	sort of half-baked and then you can engage in a conversation and
1748400	1751520	sure enough it can come up with a different way of thinking say hey that's
1751520	1755680	really good i like that idea now let's take that work that back in try again
1755680	1759120	and so it becomes now a companion a co-pilot something that
1759120	1762880	enhances your your cognitive ability but the human is still very much in the
1762880	1765920	loop and playing a key part and actually initiating the process
1765920	1768800	and of course finally at the end of the day you're the one that selects the
1768800	1772240	you know the 10 video clips you pick the one that you like and so the human is
1772240	1774800	very much involved in the loop throughout so i think that's a very nice
1774800	1778400	feature of this technology i completely agree with that so
1778400	1783440	at the moment a is are embedded in the cognitive nexus of humans so we have the
1783440	1786720	agency and we drive these things and and they
1786720	1790240	help us think and also i agree with you that it doesn't
1790240	1793120	make sense to think of these things as limited forms of computation we should
1793120	1796880	think of the collective intelligence so we are touring machines and we are
1796880	1799840	driving these things and we are sharing information so when you look at
1799840	1804880	the entire system it is a new type of memetic intelligence in fact
1804960	1809040	you know to a certain extent GPT-4 isn't running on Microsoft servers
1809040	1812640	it's in all of us right and that's that's a wonderful way
1812640	1819120	um to think about it but to me the extent to which it is constraining our
1819120	1824000	agency and creativity is what i'm fascinated by so GPT says unraveling
1824000	1826800	the mysteries and you know the intricate
1826800	1830240	dance of x y z and all of these weird motifs and
1830240	1834720	constructions and maybe that's just the way that our
1834720	1840000	LHF has constrained the model or maybe it speaks to the constraining
1840000	1843200	forces in general of having these low entropy models that kind of you know
1843200	1846560	snip off a lot of the interesting pathways so
1846560	1852000	we are very creative GPT-4 resists creativity a little bit is it a problem
1852000	1855200	well i think there's some design choices there so you talked about reinforcement
1855200	1858000	learning through human feedback is part of that alignment process we ought to
1858000	1861760	create this technology in a way that does good to minimize harm
1861760	1866160	and so naturally we do constrain it so for sure it's true that our constrained
1866160	1870000	GPT-4 behave in you might say less creative ways but perhaps in more
1870000	1873200	helpful and beneficial ways and it's appropriate that we should do that
1873200	1876720	and perhaps we lose a little bit of the creativity
1876720	1880800	in the process and so there's there's a balance there's a there's a there's a
1880800	1884560	choice to be made a design choice in how we want to create the technology and
1884560	1887440	we should be very deliberate about that and not not apologetic for that i think
1887440	1890480	it's good that we are that we are making those design choices
1890480	1894880	but people sometimes have an intuition that it's not creative
1894880	1898240	and contrast that to i'm using DaVinci Resolve
1898240	1902160	and i'm using all of these nodes and i have all of these filters and
1902160	1906080	processing transforms the difference seems to be that
1906080	1910720	i'm designing the architecture so i'm using cognitive primitives
1910720	1914880	and i'm composing them together in a new way and by tweaking the parameters on
1914880	1917760	the filters i'm going off-peast a little bit i'm
1917760	1922160	doing i'm creating the structure myself whereas in neural networks the
1922160	1925280	structure is implicit i don't know what the structure is
1925280	1927920	well i think you're talking about you're contrasting two different kinds of tools
1927920	1931600	there so the video editing tool is designed so that it follows your
1931600	1935200	instructions very precisely and you prefer one tool over another
1935200	1938720	perhaps because the interface is easier to use you get the results faster but
1938720	1941840	you have in your you've done the creativity you've designed this to
1941840	1945600	video edit that you want that you want to have and now the tool is to try to get
1945600	1948560	you to that as fast as possible as accurately as possible
1948560	1951520	but sometimes we need more than that sometimes you know if you've got right
1951520	1953280	as block and you don't know where to begin
1953280	1956960	having a tool like GPT-4 could be very powerful you're not you're not
1956960	1960400	delegating the entire process to the technology you're working
1960400	1963600	with it as a as a co-pilot as an assistant
1963600	1967440	that can for sure help you with that creative process it will come up with
1967440	1970800	with crazy things and most of them you may not like but maybe one of them
1970800	1973680	you don't like it either but it causes you to think about something that you
1973680	1977520	would otherwise not have thought of and so the two working together
1977520	1981760	can surely be more creative so i think certainly as a working in unison with
1981760	1984240	humans it certainly enhances creativity so that's
1984240	1987680	certainly my experience i think there's no doubt about that but also if you
1987680	1990640	think about let's take a simple example that i think most people relate to which
1990640	1993840	is which is image generation you're giving a talk and you want some
1993840	1998640	image to illustrate the talk and you know you could go to stock images and
1998720	2003200	you know it's a fixed set and you know you can't easily adjust it or you
2003200	2007520	or you go to editing images yourself that's a sort of slow and painful process
2007520	2011120	but now you can just with a simple prompt you know you can get a a bunch of
2011120	2014080	examples and if one of those isn't quite what you like you can
2014080	2017920	alter the prompt and and fine tune it and it now becomes that
2017920	2022160	that process which is a creative process and you can still say the human is in
2022160	2026000	the driving seat but the overall creativity is certainly enhanced
2026000	2030000	and when you take a text prompt and and the machine produces this beautiful
2030000	2034560	photorealistic image i mean how many of us weren't absolutely blown away
2034560	2037760	by the incredible advances in generative AI of the last
2037760	2042560	you know the last decade why would you not call that creative if a human being
2042560	2046160	did it you would call it creative why why are we not allowing the machine
2046160	2049520	to be described as creative that's the piece that i don't that i don't quite
2049520	2053280	understand so you could argue that creativity is just pure novelty of the
2053280	2056160	artifact so it's just how much entropy is in the artifact
2056160	2060960	but you could you could think of GPT4 pros as being a kind of
2060960	2063840	category so there's a lot of variants in there
2063840	2067760	but there are also certain motifs and and now when people see the motifs they
2067760	2071040	say oh i've seen that a million times before so i did think it was novel and
2071040	2075680	interesting and now i don't and but this is the thing so now when i'm
2075680	2079600	writing blog posts and stuff like that i'm deliberately trying to do something
2079600	2083040	genuinely creative to me you know it's it's almost like the intrinsic
2083040	2087040	creativity isn't important i don't want people to think that i use GPT4
2087040	2091360	so that's driving it do you see what i mean yeah so in clearly creativity is
2091360	2095680	about novelty and novelty is you know what we desire here
2095680	2101440	but whether that novelty has value or not that's a subjective opinion in your
2101440	2104800	case it's whether it's achieving the the goals that you desire
2104800	2108000	so i think there is no doubt that it's even if you say
2108000	2110960	we're just taking existing ideas and combining them in new ways
2110960	2114400	everything that humans do or i think builds on the work of
2114400	2118000	their own previous experience and on the work of others and i think that's
2118000	2121360	absolutely fine that's a wonderful thing about the humanity is that we
2121360	2124880	from generation to generation we build upon the work of what's gone before
2124880	2128400	and the machines that we build now are heavily dependent on
2128400	2131760	the creativity and the work of humans before because they learn from humans
2131760	2134640	they're designed by humans and i think that's absolutely fine it's a
2134640	2137280	wonderful thing and they add to the sum total of human
2137280	2139360	creativity and that that's a wonderful thing
2139360	2145920	Chris you wrote a really beautiful book and you wrote it with your son
2145920	2151120	Hugh and there was a picture of Hugh i think in the introduction of of PRML
2151120	2155200	and i guess part of what i want to understand is is deep learning is a
2155200	2158400	huge field i mean what was the thought process and
2158400	2162400	how did you decide what to tackle and what not to tackle
2162400	2165840	great questions there's an interesting story behind the the new deep learning
2165840	2170800	book which is that PRML was written in 2006
2170800	2174400	it predates the deep learning revolution and what has constantly surprised
2174400	2178480	me is just how popular it's remained in spite of the fact that in one sense it's
2178480	2181280	massively out of date because it does has no mention
2181280	2184720	of the most important thing in the field of machine learning and so i've long
2184720	2188080	felt it was time to update the book produce the second edition add some
2188080	2192400	material on deep learning but life is busy and you know
2192400	2196000	anybody who's ever written a book will tell you that it takes way more effort
2196000	2199200	than you can possibly imagine if you've not actually had that experience
2199200	2203360	and so i never really got around to doing it and along came the covid pandemic
2203360	2207120	and we all went into lockdown and i feel like it was one of the very
2207120	2210320	privileged people in that lockdown we were we were locked down together as a
2210320	2214960	family in in Cambridge and you know when you're
2214960	2218480	locked down at home for several months you kind of need a project and
2218480	2221120	and i thought this would be a great time to think about a second edition of the
2221120	2224480	PRML book because you know what what else you're going to do in lockdown and it
2224480	2227760	became a project with my son because he was he was with me
2227760	2232320	by this time he he'd gained a lot of experience master's degree in machine
2232320	2235520	learning and been working in autonomous vehicle technology
2235520	2238160	and in a sense he had a lot more practical hands-on experience with deep
2238160	2242160	learning than than i did at that point and so we started this as a joint
2242160	2246000	project but we very quickly realized that what was needed was not
2246000	2250240	a couple of extra chapters on PRML but rather the whole field had changed so
2250240	2253440	much and also we didn't want to write a book we
2253440	2256800	were just accumulated more and more material it would just become a huge
2256800	2260960	a huge tome the value of a book i think is
2260960	2265360	is in the distillation is in the way it draws your attention to a subset of
2265360	2268160	specific things this is the small set of things that you really need to
2268160	2271200	understand and then you're quick to go off into the field
2271200	2274720	so what we omitted was almost as important as what we what we added
2274720	2278800	and we very quickly realized this was a this was a new book so we we we called
2278800	2281680	the book deep learning foundations and concepts
2281680	2285120	and we made a lot of progress but then of course the the lockdowns
2285120	2289600	ended i started a new team called ai for science at microsoft
2289600	2293440	hu started at wave technologies building the the core machine learning
2293440	2297440	technology for their autonomous vehicles and we were all just far too busy
2297440	2300880	and then the next thing that happened was the chat gpt moment
2300880	2304000	we're you know in a space of a few weeks a hundred million people were using
2304000	2307440	this and suddenly ai machine learning was in the
2307440	2310400	in the consciousness of the the general public
2310400	2314320	and we realized that if ever there was a time to finish this book it had to be
2314320	2318480	now and so we had a just a really big push to
2318480	2322800	to get the book finished and available for for new eurips in 2023
2322800	2326720	and we made it just you know at the last minute as you do
2326720	2329680	and the book was on display at new eurips there and
2329680	2333600	hu and i spent the week going around the conference together
2333600	2337600	talking to folks at posters and and just had a great time so it was actually a
2337600	2340800	huge privilege to to be able to write the book with my son
2340800	2344480	yeah that's fantastic um what was your favorite chapter
2344480	2347920	and i mean are there any um things that you felt were
2347920	2351360	remissions that you would have liked to do but you just had to draw a line under
2351360	2354000	it yeah in terms of uh favorite chapters i mean
2354000	2357200	of course the things the the the more recent architectures were
2357200	2360080	particularly interesting i very much enjoyed writing the the diffusion
2360080	2364080	chapter and hu had a lot of input into that chapter of course transformers as
2364080	2368560	well and just understanding how to how to
2368560	2372320	integrate the the sort of the different generative frameworks how to bring
2372320	2375120	think about gans and how to think about variational order encoders and you know
2375120	2377840	how to think about normalizing flows and so on how to think about those under
2377840	2380640	one umbrella and present them in a more coherent way
2380640	2384000	so that was that was part of the interesting free the learning experience
2384000	2386800	i always enjoy learning new things i learned things writing that book and
2386800	2390720	that and i think you did as well and so in a sense that was that was the
2390720	2393680	favorite part of the book the things where i where i learned new things or
2393680	2397120	new ways of looking at things i already knew about the real decision process is
2397120	2400080	what to put in what not to put in while keeping the size of the book under
2400080	2403040	control because i think it's something like it's thousands of papers a month
2403040	2406720	now published in machine learning uh it's overwhelming for the beginner so
2406720	2410240	really the goal of the book is to still out those few core concepts which means
2410240	2413600	there are always things oh should we have added this should we have added that
2413600	2416800	what we wanted to do was to avoid adding the latest sort of
2416800	2420080	architecture that might be very hot at the minute that could easily disappear
2420080	2424160	three months down the line so i hope we've resisted that that temptation
2424160	2427680	but there are areas where you know perhaps when we at some point if we get
2427680	2430480	around to a second edition we might think about including reinforcement
2430480	2433760	learning is something which is of growing importance and
2433760	2436960	would be lovely to have a chapter on reinforcement learning that integrates
2436960	2439520	well with the rest of the book there are books on reinforcement learning there
2439520	2442000	are review articles there's plenty of place to go learn about them
2442000	2445120	there's something that sort of integrated with the book i think could be
2445120	2449680	could be valuable so that is something we might we might visit in the future
2449680	2453040	but for the moment we've just focused on what we think are the
2453040	2457280	core principles that any any newcomer to the field whether a master student
2457280	2460160	whether they're somebody who's self-taught a practitioner coming into
2460160	2463440	the field wanting to understand the basics of the field and so the goal was
2463440	2466720	to try to keep the book as it were as short as possible but no shorter
2466720	2470080	looking back on on your last couple of books as well in in retrospect
2470640	2473360	which bits are you are you most kind of proud of and
2473360	2476720	which bits do you do you kind of feel that when you did make the decision at
2476720	2480480	the time perhaps you've you've you've mispredicted how successful something
2480480	2484320	might be very interesting so the thing i'm most proud of actually
2484320	2488000	is the very first book called neural networks for pattern recognition
2488000	2491760	and the reason is because i think that the book was quite influential in
2491760	2494800	steering the field towards a more probabilistic more statistical
2494800	2498320	perspective of machine learning it perhaps hard for people to appreciate
2498320	2502960	today but it wasn't always that way when i first went into machine learning
2502960	2506480	a lot of it was inspired by neurobiology which is which is fine
2506480	2510160	but it lacked sort of mathematical rigor it lacked any mathematical foundation
2510160	2513520	and so there was a lot of trying to learn a bit more about the brain and then
2513520	2517120	try to copy that in the algorithms and see if that worked better or not
2517120	2521200	and there was a lot of trial and error still a lot of empirical trial and error
2521200	2524240	in machine learning of course but at least we have that that sort of bedrock
2524240	2527840	of probability theory and so i think that the book was the first one to
2527840	2531120	really address machine learning and neural networks from a statistical from
2531120	2534160	a probabilistic perspective and i think in that respect the book was very
2534160	2537920	influential the field was much smaller than today we take we take that as
2537920	2541200	obvious but i think in terms of the thing i'm most proud of it's probably
2541200	2545280	the influence of that that first book back in back in 1995
2545280	2548960	in terms of things i look back on that i might do differently
2548960	2552960	i suppose when i look at if i look at prml for example and i look at the
2552960	2556320	trajectory of the field we've seen that neural networks were
2556320	2560480	were all the rage in the mid mid 1980s to mid 1990s
2560480	2563680	and then they kind of got overtaken by other techniques and then we had this
2563680	2566960	sort of Cambrian explosion of you know support vector machines and
2566960	2569840	Gaussian process and Bayesian methods and graphical models and
2569840	2573680	and all the rest of it and and i think one thing that one thing that i think
2573680	2577040	Jeff Hinton really got right is we really understood that neural networks
2577040	2579920	were the way the way forward and he really stuck to that
2580000	2584240	perspective sort of through thick and thin i got kind of distracted
2584240	2587200	particularly we talked earlier about Bayesian methods and how beautiful and
2587200	2590960	how elegant they are and a theoretical physicist it's very appealing to think
2590960	2594320	of everything from a Bayesian perspective but really what we've seen
2594320	2598480	today is that the the practical tool that's giving us these
2598480	2602480	extraordinary advances is neural networks and most of those ideas
2602480	2606320	go back to the to the mid 1980s to the idea of gradient descent and
2606320	2609600	and so on a few new a few new tweaks you know we have GPUs we have
2609600	2612880	reluers we have a few but essentially most of the ideas were
2612880	2616240	were were were still were around back in the
2616240	2620080	back in the late 1980s we didn't really understand
2620080	2623760	the incredible scale at which you need to use them but they only really work
2623760	2627120	when you have this gargantuan scale of data and compute
2627120	2630240	and of course we didn't really have GPUs or know how to use them back then
2630240	2633680	so there were some key developments that sort of unlocked this and made it
2633680	2636160	possible but i think perhaps if i did something
2636160	2639280	differently with the amazing benefit of hindsight other than sort of
2639280	2642480	investing in certain stocks and whatever and all the other things you could do
2642480	2645520	if you had perfect hindsight i think the other thing i would do is probably
2645520	2648480	just stay really focused on neural networks because eventually there
2648480	2651760	that's the technology that came good but i always come back to probability
2651760	2655520	theory it's very much a unifying idea so for let me just give you a specific
2655520	2658880	example from prml actually there were two different technologies one called
2658880	2662400	hidden markoff models that were all the rage and speech recognition back then
2662400	2665520	another technique called kalman filters that have been used for many years to
2665520	2670160	to guide spacecraft track aircraft on radar and all sorts of things
2670160	2672880	it turns out they're essentially the same algorithm
2672880	2677280	and not only are they the same algorithm but they can be derived from the most
2677280	2680160	beautifully simple principle you just take the sum and product rule of
2680160	2683440	probabilities and then you take the idea that a joint probability distribution
2683440	2686640	has a factorization described by a directed graph
2686640	2690160	and if you want to so when i was preparing prml
2690240	2693120	i looked over a bunch of books called kalman filters an introduction to
2693120	2696160	kalman filters and they become chapter after chapter at the
2696160	2699120	forward and then chapter after chapter at the reverse equations and so on it
2699120	2703200	very very complex and very very heavy going but you can derive
2703200	2707040	the kalman filter and get the hidden markoff model for free in almost a few
2707040	2710000	lines of of algebra just starting from probability theory
2710000	2713360	and this idea of factorization it's sort of deep mathematical principle that
2713360	2716400	operates there and you discover the message passing algorithm and if it's a
2716400	2718960	tree structure graph it's exact and you have two passes
2719280	2723280	it's very beautiful very elegant so i love the fact we're exploring
2723280	2726480	all these many different frontiers but i love the fact we have some at least
2726480	2729360	some compass to guide us as we as we engage in the
2729360	2732080	exploration of this combinatorially vast space
2732080	2735840	yeah it's so interesting my co-host Dr Keith Duggar he always says that he
2735840	2738240	doesn't need to remember all of the different statistical quantities
2738240	2741920	because he can re-derive them from first principles it's that nice
2741920	2745680	but we should move on to AI for science so you're leading this
2745680	2748320	initiative at Microsoft Research can you tell us about that
2748320	2751280	yes so at a personal level of course this brings back my
2751280	2756800	my earlier interest in theoretical physics and chemistry and and biology
2756800	2759440	and that brings it together with with machine learning
2759440	2763600	and what many people realized a few years ago
2763600	2769280	was that of the many areas that machine learning would impact the scientific
2769280	2773200	the area of scientific discovery would be i think in my view the most important
2773200	2777760	the reason i say that is because it's actually scientific discovery
2777760	2781760	that really has allowed humans to go on that trajectory the last few thousand
2781760	2784880	years not just understanding our place in the universe but to be much more in
2784880	2788560	control of our own destiny to double our lifespan to cure many
2788560	2790640	diseases to give us much higher standards of
2790640	2793840	living to give us a much brighter outlook for the
2793840	2796960	future than humans humans have traditionally enjoyed
2796960	2800880	and and that's come through scientific discovery and then the application
2800880	2803920	of that knowledge and understanding of the world in the form of technologies
2804000	2808560	agriculture industrial and so on and so i can't think of any more
2808560	2813360	important application for AI but what's really interesting is it's very clear
2813360	2816800	that many areas of scientific discovery are being disrupted and when i say
2816800	2819600	disrupted i'll just give you one simple example
2819600	2823280	the ability of neural nets machine learning models to
2823280	2828000	act as emulators for previously were very expensive numerical stimulators
2828000	2831600	very often gives you a factor of a thousand acceleration
2831600	2834240	you know we can forecast the weather a thousand times faster with the same
2834240	2837600	accuracy than we could a few years ago prior to the use of deep learning
2837600	2840560	now if that were the only thing that was happening
2840560	2843600	that alone would be a disruption that alone would be worth setting up a team
2843600	2846560	on AI for science i think actually it's only scratching the surface
2846560	2850720	but anytime something that's very core very important gets a thousand times
2850720	2854160	faster it means you can do things that would take
2854160	2858320	years in a few tens of hours it that really is a disruption it really is
2858320	2862400	transformational so a couple of years ago i pitched to
2862400	2866800	our chief technology officer to say look this is a really important field
2866800	2870560	i'm happy to step down from my role as the lab director of MSR
2870560	2874240	in in europe and instead i'd like to lead a new team
2874240	2878000	focusing on AI for science and met with enormous enthusiasm
2878000	2881200	and so we've been growing and building that team it's very interesting team
2881200	2884240	it's very multinational we have people on on many different continents in
2884240	2887440	different countries we've opened new labs in in in
2887440	2890480	in amsterdam and in in berlin we have teams in
2890480	2894800	in beijing and in shanghai and folks in in seattle as well
2894800	2899600	and so very very multidisciplinary very multinational
2899600	2904160	but with with one thing in common this real excitement and passion for what
2904160	2908480	machine learning and AI is going to do to really transform and accelerate our
2908480	2911680	ability to do scientific discovery you were talking about inductive
2911680	2915440	priors just a second ago and i guess i first learned about this
2915440	2918400	with the art of you know designing inductive priors and machine learning
2918400	2921280	from max welling's group they were saying that you know the
2921280	2927040	remarkable thing is that you can using principles let's say from physics
2927040	2930800	we can design these inductive priors and we can reduce the size of the
2930800	2933520	hypothesis class that that we're approximating
2933520	2937120	and because we know the target function is inside that class we are not
2937120	2939360	introducing any approximation error and we
2939360	2942880	we are kind of overcoming some of the curses in in in machine learning by
2942880	2946080	making the problem tractable which which is amazing but
2946080	2949840	that's speaking to this kind of principled approach of imbuing
2949840	2953360	domain knowledge into these systems it's really interesting actually max and i
2953360	2956160	have a similar trajectory you both did phd's in theoretical physics and then
2956160	2959680	moved into machine learning and i think we both feel there's a very
2959680	2963440	important role for inductive bias to play in the use of
2963440	2966880	machine learning in the scientific domain i think i'm sure everybody is
2966880	2970720	familiar with the the blog called the bitter lesson by rich sudden
2970720	2974240	and if any if anybody watching this is is not familiar they should
2974240	2977520	immediately after this video go and read that blog it's a very short blog
2977520	2980960	and without giving too much of a spoiler he essentially says that
2980960	2985120	every attempt by people to improve the performance of machine learning by
2985120	2988960	building in prior knowledge building in what we call inductive biases
2988960	2992640	into the models it produces some improvement and then but very quickly
2992640	2995520	it's overtaken by somebody else who just has more data
2995520	2999200	and and that indeed is a bitter lesson and and it's a wonderful blog and
2999200	3002160	people should i i've read it many times i think people should you know probably
3002160	3005600	read that once a month and and it's it's very inspiring
3005600	3008640	but i think there may be exceptions and i think the scientific domain
3008640	3012400	is one where inductive biases for the foreseeable future will be
3012400	3015280	extremely important sort of almost contrary to the bitter lesson
3015280	3019280	and a couple of reasons for this one is that the the inductive biases we have
3019280	3024640	a not not of the kind let's say let's say linguistics or something which is
3024720	3029600	any domain where which is based on human expertise acquired through experience
3029600	3033680	because a person who's had a lot of experience over a number of years and
3033680	3038160	formulated some sort of rules of thumb that guide them that's exactly what
3038160	3042000	machine learning is very good at processing very large amounts of data
3042000	3045840	and and inducing the the the rules as it were the patterns
3045840	3050160	within that data so i think that kind of inductive bias is
3050160	3053440	typically harmful and and i think the bitter lesson will certainly apply
3053440	3056560	there but in the scientific domain it's rather different first of all the
3056560	3059360	inductive biases we have are very rigorous we have
3059360	3062960	the idea of conservation of energy conservation of momentum we have
3062960	3067040	symmetries if i have a molecule in a vacuum it has a certain energy
3067040	3070080	if i rotate the molecule the representation of the coordinates of all the
3070080	3073520	atoms changes wildly in the computer but the energy is the same
3073520	3077440	so we have this very rigorous inductive bias we also know that the world at the
3077440	3081280	atomic level is described exquisitely well by by Schrodinger's equation
3081440	3084320	sprinkling a few relativistic effects and you've got an amazingly accurate
3084320	3088160	description of the world but it's way too complex to just solve it directly
3088160	3091840	or is exponentially costly in the number of electrons but nevertheless we have
3091840	3097040	this bedrock of of really understanding the laws that govern the universe
3097040	3100880	and so and so i think that's the first the first thing we have very rigorous
3100880	3104160	priors that we believe in deeply it's not that we think conservation of energy
3104160	3106560	doesn't work we know that we know that it's true
3106560	3110000	the second thing is that we're operating in a data scarce regime so large
3110000	3113920	language models are able to use very large quantities of internet scale
3113920	3117760	quantities of human created data whether it's in the form of you know
3117760	3122320	whether it's wikipedia or whether it's just scientific papers or any of the
3122320	3126240	output of humans almost is potentially material on which which large models
3126240	3130640	can feed they're in a very data rich regime and can go to scale
3130640	3134480	and and so the bitter lesson i think really kicks in there in the scientific
3134480	3137920	domain the data might come from simulations which are
3137920	3141840	computational and expensive or it might come from lab experiments which are
3141840	3146240	which are expensive and the data is is limited so we're operating
3146240	3149840	usually in a data scarce regime so we have relatively limited data
3149840	3153040	and we have very rigorous prior knowledge and so the balance between
3153040	3157520	the data and the inductive bias is very different because of course the no
3157520	3161280	free lunch theorem says you can't learn purely from data you have to have some
3161280	3165680	form of inductive bias and in the case of a transformer it's a
3165680	3168960	very lightweight form of inductive bias we believe there's a there's a deep
3168960	3172400	hierarchy there's some you know data dependent
3172400	3177120	self-attention but but really that's it and the rest is determined from the data
3177120	3181040	in science there's much more scope for bringing in these inductive biases there's
3181040	3184400	much more need to bring in the inductive biases and that also
3184400	3188960	incidentally again in my personal very biased opinion makes the
3188960	3192800	application of machine learning and ai to the sciences the most exciting
3192800	3195920	frontier of AI machine learning because it's the one that's
3195920	3200320	richest in terms of the creativity and also in terms of the need to bring in
3200320	3204160	some of that beautiful mathematics that that underpins the universe
3204160	3207680	yeah so so fascinating I mean could we just linger just just for a second on
3207680	3211920	that so rich Sutton in his bitter lesson essay he explicitly called out
3211920	3215840	symmetries as being you know he was warning against human designed
3215840	3220800	artifacts in in these models and I mean max welling as you say famously
3220800	3224560	built these gauge equivariant neural networks bringing in his
3224560	3228960	physics knowledge and so I'm just trying to understand the spectrum between
3228960	3233920	high-resolution physical priors and the kind of macroscopic human knowledge
3233920	3236960	that that we learn which is presumably brittle
3236960	3241520	is it just that we think that these physical priors are fundamental
3241520	3245440	and that's that's a that's a perfectly acceptable way to constrain
3245440	3249440	the search space but these high-level priors are brittle
3249440	3252640	yes I think I think the the the prior knowledge that comes from human
3252640	3256560	experience is is is more of that brittle kind
3256560	3260080	because the machine can see far more examples than a human can in a
3260080	3264160	lifetime and can can do a more systematic job
3264160	3267600	of looking across all of that data we're not
3267600	3270720	not subject to say recency bias and those sorts of things
3270720	3273760	so I think that kind of prior knowledge is is one where
3273760	3278640	where scale and data will will win whereas the the prior knowledge that we
3278640	3282000	have from the physical laws in a sense is much more rigorous and symmetry
3282000	3285040	is is is very powerful it's sometimes said that
3285040	3287760	physics more or less is symmetry that's almost
3287760	3291440	yes right so conservation conservation laws arise from
3291440	3294160	symmetry you know translation in variance in
3294160	3297840	spacetime gives you conservation of energy and momentum
3297840	3301760	and you know gauge symmetry of the electromagnetic field gives you charge
3301760	3306160	conservation and so on and and so these are very very rigorous laws that apply
3306160	3309200	from symmetry but you know even if you take a data-driven approach
3309200	3312320	people often use data augmentation if you know that an object doesn't depend
3312320	3314720	his identity doesn't depend on where it is in the image you might
3314720	3317600	you know make lots of random translations of your data to augment
3317600	3321680	your data so data augmentation can be a data-driven way of building in those
3321680	3324320	symmetries but now when we have very rich prior
3324320	3327360	knowledge I'll come back to Schrodinger's equation it describes the world with
3327360	3331280	exquisite precision at the atomic level but solving it is very very expensive
3331280	3335280	and so what we can do is we can cache those computations we call it the fifth
3335280	3338480	paradigm of scientific discovery which is a rather fancy term but the idea is
3338480	3342960	very simple is that instead of taking a conventional numerical
3342960	3346560	solver and using it to solve something like Schrodinger's equation or something
3346560	3350000	called density functional theory instead of solving that directly to solve your
3350000	3353840	problem instead you use that simulator to
3353840	3356960	generate training data and use that training data to train a
3356960	3360880	machine learning emulator and then that machine learning emulator
3360880	3364400	can now emulate the simulator but typically three or four
3364400	3368800	orders of magnitude faster so provided you use it a lot and you amortize the one
3368800	3372080	off cost of generating the training data and doing the training if you're going
3372080	3376960	to use it many many times overall it becomes dramatically faster dramatically
3376960	3380560	more efficient than using the simulator and that that's just one of the
3380560	3384080	breakthroughs we're seeing in this space so first of all um there's there's a
3384080	3387680	spectrum as you say of we could just train on lots of data or we could
3387680	3392240	augment the data or we could make a simulator for the data and then we can
3392240	3396000	train a machine learning model and as we were just speaking to
3396000	3399120	these inductive priors they are so high resolution
3399120	3404080	that we are not restricting the target function that that that we want to
3404080	3406960	learn and we can make quite a principled argument about that
3406960	3410880	but the one question to me is there's a kind of
3410880	3414800	I don't know whether it's best to frame it as exploration versus exploitation but
3414800	3418000	there needs to be some amount of going off-piste
3418000	3422640	so we define the structure and we we essentially build a generative model
3422640	3425200	and we can generate a whole bunch of trajectories
3425200	3428880	but could it ever be the case that we wouldn't have enough
3428880	3433280	variance to find something interesting there's a very interesting question
3433280	3437200	about the the overall scientific method of formulating hypotheses running
3437200	3440400	tests evaluating those hypotheses refining the hypotheses running more
3440400	3443040	experiments and so on that that scientific loop
3443040	3446160	I think machine learning will have an important role to play there because
3446160	3449760	data is becoming very high dimensional very high throughput humans can't
3449760	3452720	analyze this data anymore a human can't directly look at the output of the
3452720	3456160	large hadron collider with its you know petabytes a second or whatever it is
3456160	3461120	pouring off we we need machines to help us but again I think the human
3461120	3464480	rises to the level of the conductor of the orchestra as it were they no longer
3464480	3468160	have to do things by hand machines are helping to accelerate that
3468160	3471200	and and I think the machines can help accelerate the creative process
3471200	3475200	potentially by pointing to anomalies or highlighting patterns in the data and
3475200	3478480	so on but very much with the human scientist in the loop
3478480	3481520	but but even coming down from those sort of lofty more sort of philosophical
3481520	3485280	considerations just to the the practicalities when we talk about discovery
3485280	3487840	we're also interested just the very practical method of how we how do we
3487840	3490880	discover a new drug or how do we discover a new material
3490880	3494560	so scientific discovery also means that that that that that very pragmatic
3494560	3498080	near-term approach and there we're seeing really dramatic
3498080	3501680	acceleration through the the concept of this emulator
3501680	3505120	inner ability to explore the combinatorically vast space of new
3505120	3509120	molecules and new materials exploring those spaces efficiently to find
3509120	3513520	potential candidates that might be new drugs or new
3513520	3516800	new materials for batteries or other other forms of green energy
3516800	3520880	so that that alone is a very exciting frontier I think
3520880	3525440	it's so interesting so searching these space I mean drug discovery is an
3525440	3528720	interesting one I think you spoke about sustainability as well as
3528720	3532960	another application you can speak to but how do you identify an
3532960	3537120	interesting drug so the drug discovery process
3537120	3540480	starts first of all with the disease and trying to
3540480	3543360	first of all deciding we want to go tackle a particular disease
3543360	3547600	and then finding a suitable target so the the standard so-called small molecule
3547600	3550400	paradigm which is where most drugs are today
3550400	3554640	they're small synthetic organic molecules that bind with a particular
3554640	3557600	protein so pharma companies will will will
3557600	3561360	spend a lot of time identifying targets so say a protein that has a particular
3561360	3565360	region with a molecule combined can combined to
3565360	3568320	and therefore can influence the behavior of that protein switching on or
3568320	3571680	switching off some part of that disease pathway and breaking the chain of
3571680	3575040	disease so the challenge then is to find a small
3575040	3577840	molecule that first of all has the property that it binds with the target
3577840	3581520	protein that's the first step but there are many other things that it has to do
3581520	3585520	it has to be absorbed into the body it has to be metabolized and excreted it
3585520	3588400	mustn't and particularly mustn't be toxic it mustn't bind to anything
3588400	3591520	many other proteins in the body and cause bad things to happen
3591520	3595120	so what you have is a very large space of molecules usually estimated around
3595120	3599280	10 to the power 60 potential drug-like molecules
3599280	3602400	and out of that enormous space of 10 to the 60 you're trying to find
3602400	3605680	an example that meets all of these many many criteria
3605680	3610080	and so one approach is to generate a lot of candidates but in computationally
3610080	3613520	and then screen them one by one for different properties that screening
3613520	3617680	process the more that can be done in silico rather than
3617680	3620960	in a wet lab the faster it can be done and the
3620960	3624960	the larger the search space can be and therefore the bigger the fraction of
3624960	3628960	that space of possibilities you can explore hopefully thereby increasing
3628960	3632080	the chances of finding a good candidate because many attempts to find a drug for
3632080	3635600	a disease simply fail nothing nothing eventually comes of it
3635600	3638320	so increasing the probability of success increasing the speed of that
3638320	3641440	discovery process so in all of that there are many places
3641520	3644640	where machine learning could could be disruptive
3644640	3648880	so on that process of I guess you're describing
3648880	3653120	you generate candidates and then you almost discriminate interesting ones
3653120	3657840	and then you rinse and repeat in a kind of iterative process
3657840	3661840	let me give you a concrete example so we've done some work looking at
3661840	3666000	tuberculosis so tuberculosis kills something like 1.3 million people very
3666000	3669520	sadly back in 2022 which is the last year we have we have data
3669520	3672320	and I might seem surprising because we have we have antibiotics we have drugs
3672320	3674880	for tuberculosis why are so many people dying
3674880	3678560	and one core reason is that the the bacterium is evolving to develop drug
3678560	3681840	resistance and so there's a search on for new for new drugs
3681840	3684960	so maybe I'll just take a moment to explain some of the architecture and
3684960	3688400	get into a little bit of the sort of the techie details of this
3688400	3692320	so we wanted a way of finding we know what the target is we've been told what
3692320	3695280	the target protein is the target has a region called a pocket and we're
3695280	3698320	looking for molecules that are bind tightly with that pocket region on the
3698320	3702240	protein and and so the way the way we approached
3702240	3705760	this was first of all build a language model but not a language model
3705760	3708560	for human language but for the language of molecules
3708560	3711440	so we first of all take there's a representation representation called
3711440	3714480	smiles it's a way of taking a it's an acronym but just a way of
3714480	3717600	taking a molecule and describing as a one-dimensional string
3717600	3720560	and so you first of all take a large database of I don't know 10 million
3720560	3723920	molecules it represented the smile strings and you treat them like the
3723920	3727840	tokens for a for a transformer model and by getting it to predict the next token
3727840	3732240	the next element of the smile string you build a transformer based language
3732240	3734720	model that can speak the language of molecules
3734720	3738160	so it can it can run generatively and it can create new
3738160	3742080	create new molecules as output so you can think of that as kind of like a
3742080	3745600	foundation language model but speaking the language of molecules
3745600	3749120	now we want to generate molecules but not just any molecules we want molecules
3749120	3752800	which bind with a particular target protein so we have the target protein
3752800	3755280	in particular it's the pocket region that we're interested in
3755280	3758480	so we can give it the amino acid sequence of the protein as input
3758480	3762080	but we need more than that we need the geometry of the of the pocket and this
3762080	3764560	is where some of those inductive biases come in so we
3764560	3768400	we need to have representations of the geometry of the atoms in that form that
3768400	3770960	pocket but a way that represents these
3770960	3775120	equivalences and so they're encoded as input to a transformer model that learns
3775120	3779040	a representation for the protein pocket and the final piece we need as you said
3779040	3782160	we want to do this iteratively we want to take a good molecule and make it a
3782160	3785520	better molecule rather than just searching blindly across a space of
3785520	3788480	10 to the 60 possibilities and so the other thing we want to
3788480	3792720	provide as input is a molecule a descriptor of a
3792720	3796880	a known small molecule that does bind with the pocket already
3796880	3800400	and but we want to do this in a way that creates variability and we actually use
3800400	3803840	a variational autoencoder to create that representation and
3803840	3807600	that's the an encoder that trun translates the molecule into a latent
3807600	3810480	space and we can sample from that latent space
3810480	3813840	and then the this language model the smiles
3813840	3817680	language model can attend to both the output of the variational autoencoder
3817680	3820640	and the output of the protein encoder using cross-attention
3820640	3824160	and so what we've done there is I think rather tastefully combined
3824160	3828720	some elements from you know states of the arts at modern deep learning
3828720	3832800	the result then can be can be trained end to end using a database
3832800	3836400	of known other proteins that are known to bind efficiently to
3836560	3840480	small molecules and once the system is trained we can now
3840480	3843840	provide as input the known target for tuberculosis
3843840	3847600	and some known molecules that bind with this and then we can
3847600	3850720	iteratively refine those molecules at the output we get molecules that have
3850720	3853600	better binding efficiency and we're able to increase the binding
3853600	3857600	effectiveness by two orders of magnitude and so we now have states of the art
3857600	3861200	molecules in terms of binding efficiency to this
3861200	3865840	to this target protein of course we can't do the wet lab experiments ourselves
3865840	3869440	we partner with an organization called giddy the global health drug discovery
3869440	3872880	institute they've synthesized the molecules that we've we've
3872880	3876400	generated and measured their their binding efficacy
3876400	3880560	and so we're very very excited about this and of course the next stage now is to
3880560	3884400	take that as a starting point and further refine and optimize those molecules and
3884400	3887360	and try to address all those other requirements that we have for before a
3887360	3890080	drug can actually be tested on humans in terms of its
3890080	3892720	toxicity and metabolism and all and all the other things
3892720	3897440	but i think it's just a a very a very nice example of almost like a first step
3897440	3901360	in using modern deep learning architecture to accelerate the process of drug
3901360	3904640	discovery and already we have i think really quite a
3904640	3908400	spectacular success given that we're we're kind of newcomers to this
3908400	3912080	to this field partnering with experts domain experts with the wet lab
3912080	3916240	experience and the wet lab capability to me this is the beginning of a very
3916240	3919280	exciting journey that sounds incredible
3919280	3923360	is there any kind of representational transfer between the the models so for
3923360	3927840	example you're talking about this this geometric
3927840	3932240	prior model and generating tokens to go into the language model
3932240	3935600	because just using language models by the way is a fascinating approach i
3935600	3939760	spoke with christian sogeddy and he was doing mathematical conjecturing
3939760	3943600	just using language models you know just just taking mathematical
3943600	3947120	constructions and putting them into language and they used to use graph neural
3947120	3950480	networks for this and so i guess the question is
3950480	3954800	could you kind of bootstrap it with a you know with an inductive
3954800	3960720	principled model and then kind of just train using the language model afterwards
3960720	3963440	i think i think the general principle there's a very powerful one so the idea
3963440	3966400	of borrowing strength from other domains and i think we're seeing this time and
3966400	3971040	time again in deep learning that that the machine learning models are able to
3971040	3974720	extract some general patterns from even from one domain and translate them into
3974720	3977680	completely different domain we talked earlier about large language models
3977680	3981600	being getting better at writing code if they've also got exposure to
3981600	3984880	to poetry or something is seemingly quite irrelevant there's some
3984880	3988000	there's something quite deep and subtle going on there but perhaps in a less
3988000	3990960	subtle way it's clear there's a sort of a language of
3990960	3995040	molecules there's a language of materials and that by
3995040	3998640	building models that have a broader exposure to that language
3998640	4003040	they almost invariably will become better at the specific tasks that we want to
4003040	4006160	to apply them to so i think there is a general principle at heart there
4006160	4009600	yeah it's so interesting because i i used to think that
4009600	4012880	that perhaps the drawback of these inductive prior models is that
4012880	4017520	it was one inductive prior per model but this ability potentially to
4017520	4021680	bootstrap a foundational model that can do all of the things
4021680	4025040	that's really interesting i think the most powerful inductive biases and the
4025040	4027920	ones we focus on are really those very general ones where
4027920	4031920	symmetry says just very fundamental properties of the universe and we
4032080	4035040	want we want those really baked into the models i think
4035040	4039600	the the the sort of intuitions we have about more specific domains i think
4039600	4042560	they can perhaps lead us astray because they're based on
4042560	4045440	our experience of much more limited domains i think this is where the
4045440	4048400	machines can be can be much better at
4048400	4052240	processing and interpreting large volumes of data and drawing regularities
4052240	4055200	out of that out of data in a more systematic way
4055200	4059840	okay okay and just before we we leave this this is a bit of a galaxy brain
4059840	4063040	question and and that that's parlance that all the kids are using these days
4063040	4067520	by the way but how fundamental is is our physical
4067520	4070960	knowledge you know the question is like we are we're designing these inductive
4070960	4074800	priors as if they are fundamental but folks like Steven Wolfram for
4074800	4077840	example argue that there's there's a deeper
4077840	4081040	ontological reality you know might be a graph cellular automaton or something
4081040	4084080	like that and is that something you think
4084080	4087680	about the kind of the gap between our models and what reality is
4087680	4090640	so i think first of all one of the greatest scientific discoveries of all
4090640	4094000	time is the fact the universe can be described by simple laws that that is
4094000	4096640	not obvious a priori that itself is perhaps the most
4096640	4099760	profound discovery you know really going back to Newton but we found it time
4099760	4102720	and time again what we've also found is that the
4102720	4105200	our understanding of the universe as it exists today
4105200	4108800	has has it's almost like onions we're peeling way layers of onions
4108800	4112000	you know Newton if you want to navigate a spacecraft to Jupiter you still use
4112000	4115920	Newton's laws of motion and Newton's law of gravity it's just fine
4115920	4119040	it doesn't mean we believe it's exact description of nature we've now got
4119040	4122400	deeper descriptions of nature we understand relativity for example
4122400	4125600	general relativity tells us that actually Newton's second law of
4125600	4128240	motion or Newton's Newton's law of gravity rather is just an
4128240	4130080	approximation the inverse square law is a pretty good
4130080	4133360	approximation but we've got a much better description now
4133360	4138080	but but it's it's it's hard to say that we've we've found the ultimate answer
4138080	4141120	it's rather that human knowledge is or just always
4141120	4144800	stands on that that edge of what we don't understand and scientific discovery
4144800	4147760	is always about exploring the things we don't understand
4147760	4152160	working out whether you know whether the laws actually do hold
4152160	4155440	and the anomaly we see in the data is is because of some
4155440	4158480	phenomenon that we haven't yet observed I mean this is how Neptune was
4158480	4161840	discovered by by seeing that the planets were not behaving as they
4161840	4164480	should do according to Newton's laws Newton's laws were just fine there's
4164480	4168880	just another planet perturbing them or is the procession of the perihelion of
4168880	4171680	Mercury because because there's another no it's because
4171680	4174320	actually Newton's law of gravity isn't quite right we need
4174320	4178160	relativity to understand that so I think scientific
4178160	4182640	exploration as far as I can tell has no particular end in sight it's
4182640	4186080	rather that we have things that we understand and there are new frontiers
4186080	4188560	you know when I was when I was a teenager getting
4188560	4192880	excited by physics I love reading about relativity and quantum physics but it's
4192880	4195520	kind of depressing because I thought you know it's kind of born
4195520	4198720	you know 50 years too late or whatever you know all the exciting stuff happened
4198720	4202400	at the beginning of the 20th century it's kind of all been done
4202400	4205920	but now we have you know dark matter and dark energy and we realize that most of
4205920	4208720	the universe isn't sitting on the periodic table that I learned about in
4208720	4212720	schools and actually I needn't have worried you know
4212720	4217520	I think it was at Vannevar Bush who called it the endless frontier
4217520	4220720	the you know science is an endless frontier there is just there is
4220720	4224560	always more to explore and always more to learn so whether the particular ideas
4224560	4227520	you alluded to have substance I don't know at the end of the day the scientific
4227520	4230560	method will tell us if they have predictive capabilities they can predict
4230560	4232720	new phenomena that we weren't aware of before
4232720	4238240	then you know then they have they have credence as far as a scientist
4238240	4241600	is concerned but ultimately you know we still stick to the
4241600	4244640	scientific method it's about our ability to make predictions that are
4244640	4247520	testable experimentally and if they stand up to the test of
4247520	4250560	experiment then we give more weight to those to those hypotheses and
4250560	4253120	eventually they're elevated to the stages of theory
4253120	4257120	I often wonder about the horizon of our cognition
4257120	4261440	you know what we are capable of understanding and we tend to understand
4261440	4265520	things using high-level metaphors information is a great example of that
4265520	4268080	so a lot of people talk about the universe as
4268080	4271840	information this agential view is quite interesting so
4271840	4276480	modeling everything as agents and it might well be possible that the
4276480	4279920	universe is just so strange and alien that we could never possibly
4279920	4282720	understand it so there's a bit of an interplay between
4282720	4287840	our kind of intelligibility and and our models and what it is
4287840	4291360	the universe clearly is completely unintelligible in the sense of
4291360	4295040	nobody can really think about quantum physics
4295040	4297920	it completely defies our everyday intuitions that we learn at this sort
4297920	4301440	of macroscopic level so I think we have to accept already that the
4301440	4304800	universe is described mathematically that's our precise description
4304800	4307760	and then we have kind of metaphors about waves and particles and so on but
4307760	4310480	they none of them none of them really work properly they're just
4310880	4314400	crutches to lean on but ultimately it's a mathematical description
4314400	4317920	but that that is that is also very interesting the fact that the world is
4317920	4321280	described by mathematics that by making little marks on a piece of paper you
4321280	4323760	can discover a new planet that's quite incredible
4323760	4327440	shifting over to deep learning a little bit more more broadly
4327440	4331200	and we were touching on this already but the landscape is dominated by
4331200	4335680	transformers architectures what what are your broad thoughts about that
4335680	4339600	like any field I think machine learning has its sort of its fads and its waves
4339600	4342640	something works really well and then everybody latches onto that and makes
4342640	4344720	use of that and that that's all well and good
4344720	4348480	I'd be kind of surprised if the transformer is the last word in deep
4348480	4352640	learning if that's the the the the the architecture we use forever more
4352640	4355360	but it clearly works very well and we haven't reached the end of its
4355360	4358320	capabilities by any means so it makes a lot of sense to
4358320	4362320	exploit the transformer architecture in applications and see how much we can
4362320	4365600	gain from that at the same time there's clearly opportunities to think about
4365600	4368880	the limitations of transformers the computational costs can we do the same
4368880	4372560	thing you know with better scaling if you want longer context windows and all
4372560	4375600	the rest so there's plenty of interesting research I think to be done in
4375600	4378080	in new architectures as well so I think we need both
4378080	4381920	so you know here's another galaxy brain question why does deep learning work
4381920	4385120	you know because on on the face of it it shouldn't work it shouldn't train it
4385120	4388720	shouldn't generalize and they've been an absolutely
4388720	4393760	remarkable success why is that so I think first of all at one level you
4393760	4396240	could say well we understand why they work we're fitting
4396240	4399440	nonlinear functions we're kind of doing curve fitting in high-dimensional space
4399440	4403200	we need some some generalization and it comes out to no free lunch theorems of
4403200	4406320	inductive biases perhaps it's smoothness continuity perhaps it's something
4406320	4410640	more more constraining than that so at one level it's sort of not surprising I
4410640	4414000	can I can fit a polynomial to a bunch of data points and by gradient
4414000	4417360	methods and I can make good predictions for sort of intermediate points
4417360	4420320	just we're just generalizing that to more data and higher dimensions
4420320	4423600	so so one level I say no it's not at all surprising they work
4423600	4427040	at a different level of course the fact they work so well is remarkable
4427040	4430960	but the way in which they work is very interesting so one thing which
4430960	4435600	if we go back to the earlier years of machine learning and certainly back to
4435600	4438320	the world of statistics the idea that you would fit models that
4438320	4441280	have way more parameters than the number of data points would be
4441280	4444880	clearly insane to any self-respecting statistician we never would have and
4444880	4448800	perhaps that's nobody why nobody really tried it very much and yet we have
4448800	4452480	these odd phenomena whereby you know the training error goes to zero and yet the
4452480	4455440	test error continues to come down even though the training error is already
4455440	4460000	at zero something about stochastic gradient descent
4460000	4462880	the actual training process clearly is important there it's not just
4462880	4465440	here's a cost function we find the global minimum it's a property of the
4465440	4468640	global minimum no there are many many global minimum that all
4468640	4472480	have zero error some solutions will clearly overfit others generalize
4472480	4474640	well and so there's something about the training
4474640	4478160	process that we need to understand so I think there's a lot of research to be
4478160	4481360	to be done in why do they work so well I think it's an open question
4481360	4484400	we can describe the model we can say lots of things about the model we can
4484400	4487600	say because it has this and this and that number of layers
4487600	4491200	therefore the structure of the space has this and that properties and it divides
4491200	4493760	it up into such and such regions and so on
4493760	4497280	those are true I don't know whether that gives us real insights into why it's
4497280	4500640	working I think there are some some very much open questions there it strikes
4500640	4503600	me a little bit like neuroscience you know we have the human brain it does
4503600	4506640	these amazing things and we can get more and more and
4506640	4509760	richer and richer data about which neurons are firing and when and how the
4509760	4512240	firings are correlated we can learn something about the
4512240	4515760	the underlying machinery this is a bit like neuroscience except we can put a
4515760	4519520	probe in every neuron in the you know artificial brain and gather very very
4519520	4522000	rich information so again I think there's a very
4522000	4525520	interesting research frontier of getting better understanding of why are
4525520	4528880	they able to generalize so well and why do we have these
4528880	4532480	strange phenomena with these seemingly over parameterized models that don't
4532480	4535520	overfit but rather have very good generalization lots of research to be
4535520	4539040	done and just to linger on that that observation you made that you
4539040	4542800	can train a deep learning model and after the
4542800	4547600	training error has converged the test loss continues to improve I mean that
4547600	4550560	just seems it just doesn't make sense
4550560	4554160	I mean how and that there's grocking as well which is another
4554160	4557440	it's almost like we were saying with physics that outside of the
4557440	4560320	the machinations of the optimization algorithm
4560320	4563360	stuff is happening that we don't understand well you can tell stories
4563360	4565360	right you can say there's a there's a big space
4565360	4568000	each point of the space is setting for all the parameters of the model so the
4568000	4570880	sort of the weight space of the model and maybe you started off somewhere near
4570880	4573680	the origin with some little random initialization and you follow some
4573680	4576480	trajectory that's defined by stochastic gradient descent
4576480	4579920	and there are lots and lots of places in this space all of which have zero
4579920	4582480	training error so and they're connected so there's some
4582480	4585360	sort of manifold of zero training error and you're starting off at the origin
4585360	4588800	and stochastic gradient descent is somehow not taking you at a random way
4588800	4591360	maybe it's taking you to something like you know the nearest
4591360	4593920	point on this manifold or something and that maybe that's some kind of
4594000	4597600	regularization and maybe that place has
4597600	4600480	certain smoothness properties that lead to good generalization
4600480	4603360	so you can kind of tell these stories I think the challenge is to take the
4603360	4607120	stories and make them predictive so I think when we have a theory of
4607120	4610000	what's going on we'll know we have a theory because it can predict new
4610000	4613120	things not just tell stories about what we've already discovered empirically
4613120	4618000	but really become predictive I think that's still a very much an open question
4618000	4623120	so what do you think about the intelligibility of neural networks in
4623200	4626960	terms of things like bias and fairness and safety
4626960	4632240	because you could just think of these things as inscrutable
4632240	4637440	bags of neurons and but we need to have some guardrails don't we
4637440	4640800	well we absolutely need to create technology that's beneficial to humanity
4640800	4643760	there's no question about that and there are mechanisms for doing that to
4643760	4647440	align the systems whether it's through you know human feedback
4647440	4650800	whether it's external guardrails that are providing more conventional sort of
4650800	4654720	checks on how things are being used that's
4654720	4658080	clearly necessary and I find it very encouraging that so much
4658080	4661680	energy and effort is going into this and yes there'll be bumps in the
4661680	4665840	road and missteps on the way for sure but overall we seem to be heading in a
4665840	4668960	very good direction but I think the fact that there is a lot of attention
4668960	4672160	being paid to the potential risks associated with this
4672160	4676400	very powerful and very general new technology gives me hope that we will
4676400	4681440	avoid most of the the biggest risks. Can you give me a specific example of an
4681440	4685200	emulator? Yes I can so one very nice example actually
4685200	4689360	it was the final project I worked on when I was working in the fusion program so I
4689360	4692800	was using fusion as a sort of springboard to get into machine learning
4692800	4696720	and we wanted to do real-time feedback control of a fusion experiment
4696720	4699520	I think called a tokamak very high-temperature plasma we wanted to
4699520	4704480	use neural nets to do non-linear feedback so the challenge there was
4704480	4709040	to take a plasma it's like a donut shaped ring of hot plasma
4709040	4711600	and it was known that if you could change the cross-sectional shape you could
4711600	4714640	improve its performance so there's an experiment called a compass compact
4714640	4717120	assembly at Cullum in in Oxfordshire and the
4717120	4721120	experiment's designed to produce very interesting exotic cross-sectional
4721120	4724320	shapes to to explore the performance so we wanted to use a neural net to do that
4724320	4727360	feedback control. Now the good news is we had a great
4727360	4730560	piece of inductive bias I think called the grad shafranoff equation it's a
4730560	4733600	second-order elliptic partial differential equation but the point is it
4733600	4738320	describes the boundary of the plasma very accurately right so you make a bunch
4738320	4741520	of measurements from hundreds of little pickup coils around the plasma
4741520	4744560	and those are boundary conditions you solve the grad shafranoff equation you
4744560	4747440	know the shape of the plasma and the goal was to
4747440	4751120	decide ahead of the time that you wanted to create a circular plasma
4751120	4756640	and then change its shape and and and then make corrections if the shape
4756640	4759840	wasn't quite the one you wanted you would change the the big control coil
4759840	4764400	currents and an alternate shape. The problem was the grad shafranoff equation
4764400	4767840	on a state-of-the-art workstation of the day would take two or three minutes to
4767840	4770960	solve whereas we had to do feedback on a sort of 20
4770960	4774000	kilohertz frequency or something it was about something like six orders of
4774000	4777920	magnitude too slow so what we did instead was we we solved the
4777920	4782000	grad shafranoff equation many times on the on the workstation
4782000	4786000	over a period of you know days and weeks until we built up a large database
4786000	4789520	of known solutions along with their magnetic measurements
4789520	4792640	and then we trained a neural network just a simple two-layer neural network
4792640	4796240	back in the day with probably only a few thousand parameters i mean
4796240	4799440	miniscule by modern standards but it was trained to take the magnetic
4799440	4802240	measurements and predict the shape and we could put that into a standard
4802240	4806000	feedback loop and and we're in a bit of a race with
4806000	4809120	another organization that was doing a similar thing a different fusion lab
4809120	4813120	that was working on the same project and so that was very motivating and i'm
4813120	4816160	pleased to say we got there first and we did the world's first ever real-time
4816240	4820160	feedback control of a tokamak plasma using a neural network
4820160	4823920	but as a beautiful example of a of an emulator we could get five or six orders
4823920	4827920	of magnitude speed up not by solving the equation directly
4827920	4832640	to do feedback control but by using the numerical solver to generate training
4832640	4835360	data and using the training data to train the emulator
4835360	4838560	and then the emulator and even then it was still
4838560	4842960	quite demanding for the silicon of the day there was no processor fast enough so
4842960	4846560	we actually built a physical implementation of the
4846560	4850080	neural net believe it or not so it was a hybrid
4850080	4854000	analog digital system had an analog signal pathway with analog
4854000	4857520	sigmoidal units but the weights were set using digitally set
4857520	4860400	resistors so we could take the numerical output of the the emulator
4860400	4864960	downloaded into this bespoke hardware physical neural network
4864960	4867920	and do real-time feedback control so i was pretty pretty excited about that
4867920	4871440	project that's fascinating what do you think about
4871440	4875520	control now do you have any opinions on you know model predictive control and
4875520	4879280	control is a super important area different both the both the control
4879280	4882560	problem and the overall planning problem i think
4882560	4886400	despite all the remarkable advances in gbt4 the world of instantiated
4886400	4890960	ai and robotics and so on is still a very very wide open frontier
4890960	4895040	we don't we don't really have robots that can even yet drive a car through
4895040	4898720	central london that's still a a major challenge that
4898720	4901280	we're seeing some very remarkable progress recently
4901280	4905120	yeah i mean more broadly i've been speaking with some neuroscientists and
4905120	4908320	they say that we have the matrix in in our heads so we're always running
4908320	4912160	simulations and presumably in the future this will be a
4912160	4915680	principled way of building agents so the agents will run
4915680	4919760	counterfactual simulations and select trajectories which look like good ones
4919760	4923600	and then the process will will iterate i think this is this is very powerful i
4923600	4927040	mean the the idea of sort of type one and type two fast learning slow learning
4927040	4929840	the idea that we simulate the world and we compare the simulation with the
4929840	4932560	reality and we can learn from our own simulators and so on
4932560	4936000	we don't we don't quite know what best to do with that but it feels such a
4936000	4939360	powerful and compelling concept and we we think something like that is going on
4939360	4941760	in the brain that again that feels like a
4941760	4945360	an area that's ripe for exploration and i think in some form
4945360	4949120	some kind of you know model prediction and simulation of the world feels like
4949120	4953120	it will be increasingly a part of ai systems as we go forward
4953120	4956480	i mean for me the takeaway in all of this is just what an amazing time to be in
4956480	4959040	this field there are so many fascinating things to work on
4959040	4962080	professor bishop it's been an honor to have you on mlst thank you so much
4962080	4966480	well thank you i've enjoyed it thank you amazing
