start	end	text
0	7860	To me, the difference feels like language models start with this highly abstract language
7860	8960	representation.
8960	13720	The system as a whole can try to predict the next token with greater and greater accuracy.
13720	18840	And so the difference it seems is that the adversarial inputs for us tend to look a lot
18840	21880	different than the adversarial examples for LLM.
21880	27600	Once you try and go outside of this sphere of what is meaningful to humans, the possibilities
27600	28600	grow exponentially.
28800	34200	I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos
34200	39040	will come out very shortly, but around the same time someone shared a paper on our Discord
39040	41600	server and it's called What's the Magic Word?
41600	46640	A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron
46640	47640	Wachowski.
47640	53560	Now, what these guys did is theoretically think about a language model as a dynamical
53560	59440	system and use the lens of control theory to think about the space of reachability.
59440	60440	Why is this important?
60440	66600	Well, language models, we think that they think in language space, this abstract language
66600	68360	space, but they don't.
68360	70400	They actually think using the shogoth.
70400	76960	They think in this very high resolution token space, and it's just this horrible hairy
76960	79080	gnarly mess, right?
79080	82800	No one has created any firewalls for large language models yet.
82800	87680	When companies publish their language models, you know, you just have an API and you just
87680	95000	send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind
95000	100200	of fine-tuning or preference-steering using human feedback, I thought that they significantly
100200	102600	reduced the reachability space.
102600	106800	Because in language models, we do the pre-training, which is distribution matching, and then we
106800	112840	do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt
112840	116680	by snipping off all of those trajectories.
116680	117680	Turns out I'm wrong.
117680	121200	The reachability space is much larger than I thought it was, and this is one of the things
121200	123960	that they point out in their paper.
123960	125640	And we kind of knew this, right?
125640	129280	Because we can do adversarial attacks on these language models.
129280	133000	You know, people have observed that if you use sort of human social engineering tricks
133000	137280	on them, like, oh, I'll tip you $500, then it'll do a bit better.
137280	141560	But then there's this whole other sort of perceptual layer, I guess you could call it,
141560	146600	where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,
146600	152400	kind of like magic, where if you give it these very strange, very inhuman-looking prompts
152400	157920	that will steer it to this, to just making a certain output extremely likely, right?
157920	162640	And so, to me, it feels really similar to digging into, like, magic and the human perceptual
162640	167360	system just with LLMs, where we're learning about basically the shape or what the nature
167360	172080	of these language models are in terms of how they interact with the world and how their
172080	174080	dynamics really work.
192640	211360	For as long as I can remember, the thing I've wanted more than anything else is to figure
211360	213440	it all out.
213440	216120	I've never shied away from the big questions.
216120	217120	Why are we here?
217120	218120	What are we all doing?
218120	224080	What is this thing we call life that we are all experiencing and one and the same a part
224080	225080	of?
225080	230800	While these questions are all, you know, 30,000 feet in the air, one thing that drew me back
230800	232840	down to earth was the field of engineering.
232840	237960	And when I graduated high school, this had a very strong appeal, a pull, because in engineering
237960	239080	you can design systems.
239080	244280	You can design real, operable things that you can work with and design and understand
244280	245520	how they work.
245520	252440	And so through engineering, perhaps, you can begin to investigate and understand the intricacies
252440	253440	of our world.
253440	255000	That's my hope at least.
255000	260920	So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence
260920	265480	because intelligence seems to underlie so much of our world, so much of the design process
265480	267920	of engineering itself.
267920	271360	But what is intelligence and how can we understand it?
271360	277760	It's a question of systems design, really, where we're trying to figure out, okay, we're
277760	282360	humans, we've been in civilization for some time, and we've sort of figured out how to
282360	283360	cooperate with each other.
283360	285200	We obviously have challenges with that.
285200	290520	We're not perfect by any means, but when it comes to adding language models to the mix,
290520	294900	I think it could go both ways, where we could have a world where language models just make
294900	299200	us much dumber, much less capable, maybe make for a worse world.
299200	303220	I think that if we think carefully and we really understand what's going on with the
303220	306920	language models, if we can get a fundamental understanding of them one way or another,
306920	310560	then there's much more hope that maybe we could make a world where our language models
310560	315600	don't just make us smarter, but make our world substantially better and perhaps lead us towards
315600	320560	some greater enlightenment and basically ability to cooperate much better than we were even
320560	321560	before.
321560	324680	Do you think language models are intelligent?
324680	326080	That's a great question.
326080	330360	I think that they're able to simulate intelligence.
330360	335360	One of the really interesting things I'm starting to see now is we are building software abstractions
335360	337840	and controllers on top of language models.
337840	342360	We've been talking about doing this for years, right, because at the end of the day, we have
342360	346840	this idea that we can have this big foundation model and it does all of the things.
346840	352280	It's multimodal, it knows how to reason, and the fact of the matter is that's not really
352280	353280	true.
353280	357640	We control them and I think initially we're seeing frameworks that allow you to do things
357640	362560	like prompt injection, but the next step is thinking of controllers, using control theory
362560	364800	to think about these large language models.
364800	367120	Anyway, I really hope you enjoy the conversation today.
367120	371720	Now, these guys are fascinated not only with controlling language models, but also with
371720	375680	things like AGI, general intelligence, collective intelligence.
375680	379960	It was a really interesting conversation and if you stick around to the end, you can also
379960	384520	hear about the institute that they've set up around AGI technology.
384520	385520	Enjoy the show.
385520	386520	So, my name is Aman.
386520	391240	I'm a PhD student at Caltech studying computation and neural systems.
391240	395360	Recently, we released this paper called What's the Magic Word, Towards the Control Theory
395360	399960	of LLMs, and did that over the last summer with Cameron here.
399960	404080	And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering
404080	405080	science.
405080	409400	I specialized in machine intelligence, sort of been bouncing around between doing machine
409400	414280	learning stuff, applying it to computational biology, trying to understand some stuff in
414280	419960	theoretical neuroscience, and most recently getting back into the LLM space, as well as
419960	424360	trying to study collective intelligence, how very simple machines can come together to
424360	428800	produce a very complicated and beautiful system as a whole.
428800	429800	So, yeah.
429800	430800	Amazing.
430800	431800	And Cameron.
431800	432800	Yeah.
432800	436080	So, my name is Cameron McCoskey, and I went to undergrad here.
436080	439120	I did engineering science as well.
439120	443400	I majored in the robotics engineering option, and now I'm a grad student.
443400	449000	I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and
449000	450200	Kevin Chiron.
450200	455880	I'm really interested in the deep questions of intelligence, and right now I'm pursuing
455880	460280	research related to morphogenesis and computational models of it.
460280	464320	Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt
464320	468240	engineering, well, a control theory of prompt engineering.
468360	470400	I'm excited to get into it.
470400	473360	You folks have just written an incredibly interesting paper.
473360	478880	It was shared in our Discord server, and I saw your presentation, and we'll share a clip
478880	482640	of that in the introduction, but I was intrigued by it straight away.
482640	486760	And what you're doing is you're talking about control theory in respect of large language
486760	487840	models.
487840	489800	Can you explain what that is?
489800	490800	Yeah.
490800	493720	So, I guess I'll get started with control theory.
493720	501000	So back in the day, the late 1800s, this guy Maxwell observed that people were making
501000	504920	these engines, and they were putting these things called governors on them, where if
504920	509280	your car or your machine was experiencing varying loads, you wanted the engine to still
509280	511120	go at the same rate, right?
511120	513080	And people had these things called governors.
513080	516800	There's this fly ball governor, which is this sort of hand tune thing that you put on top
516800	520880	of the engine to try to make sure that it'll be consistent, that it'll do what you want,
520880	524000	that it'll be going at a consistent speed, right?
524000	528880	And people were hand tuning these things, and obviously the engines were working, but
528880	533480	it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees
533480	535920	as to how it would end up working in practice.
535920	541680	And so what Maxwell did was he formalized the notion of feedback control, where if you
541680	547200	have this system, even if it's quite complicated, as it turns out, if you feedback the output
547200	552640	of the system into a controller, and try to compute some error metric, and try to correct
552640	556560	for that at every moment in time, it turns out to be a much easier problem to solve from
556560	561160	an engineering perspective than trying to make a perfect system that just does the right
561160	562160	thing off the bat.
562160	566640	So this idea of feedback was really powerful, and sort of gave birth to modern control theory.
566640	570760	And as it turned out, that was a really powerful way to look at systems, building systems,
570760	574840	and controlling them and doing engineering on them, so that they could be robust, do
574840	577440	what we want, and so that we could predict them.
577440	583320	And so when it comes to LLM control theory, what we saw is that we're kind of at a similar
583320	587440	place with language models, where we have these engines, we have these language models
587440	591440	that are very powerful, they can do a lot, they seem to exhibit many interesting attributes
591440	595440	of intelligence, and there's a lot of utility there for people to build further systems
595440	597920	on top of them, and people are already doing that.
597920	602560	But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going
602560	609480	on where it's really hard to get at the fundamentals of what exactly it means to control an LLM
609480	611080	system and how you might do it.
611080	612640	At this point, it's very heuristic.
612640	616120	And so we sort of saw that as an opportunity to try to figure out what would a control
616120	620880	theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to
620880	624920	all of these really, really useful engineering insights, and also just fundamental insights
624920	629640	as to the nature of LLM systems, so that we can better control them, make them reliable
629640	634240	and robust, and be able to do engineering in a more principled manner on them than we're
634240	635480	currently able to.
635480	639160	So that's sort of the general direction and the motivations for our control theory of
639160	640160	language models.
640160	641760	Yeah, that's absolutely fascinating.
641760	645800	I mean, for many years, I've been thinking that we need to have some kind of a controller
645800	647920	for a large language model.
647920	652080	But I guess I'm interested in, first of all, what are the differences between large language
652080	654360	models and something like a steam engine?
654360	659600	And also, with a steam engine, you might be optimizing the efficiency or the performance
659600	661640	or the speed or something like that.
661640	666120	What is it that we are kind of trying to make better with a large language model?
666120	670160	So first off, we'll talk about the differences between large language models and other types
670160	673040	of systems that you might want to control.
673040	678640	Typically a control system, you might first be introduced to control theory in the context
678640	682640	of like, say you're trying to control an engine or something else where the states can be
682640	687920	represented by a set of numbers or set of real numbers that is fixed size.
687920	692560	So perhaps we have an X and a Y coordinate where it's trying to control or a position
692560	693880	in a velocity.
693880	700040	These are common types of systems in scenarios that show up in control theory.
700040	704320	The difference with an LLM, the first major difference is that the token space, the state
704320	706000	space of the system is discreet.
706000	710240	Because we're dealing with tokens, we're dealing with words, we're not operating in the space
710240	715960	of real numbers anymore, and so this introduces some complications and complexities when dealing
715960	717280	with control theory.
717280	722720	The second thing that's really significant is that each time an LLM generates a token
722720	726480	or a user inputs a token, that state space actually expands.
726480	728560	It grows by one token.
728560	732320	And this is very interesting and unique for LLM systems.
732320	737520	On the one hand, this can be exploited to try and get the LLMs to engage in reasoning
737520	743320	or chain of thoughts or kind of take a winding path to the answer you actually want them
743320	744920	to outputs.
744920	748720	But of course, this makes it very difficult for control theory because each new token
748720	754160	you add, the space of possible sentences grows exponentially.
754160	759760	And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this
759760	762560	grows extremely, extremely quickly.
762560	764400	These are some of the challenges.
764400	769240	And with a control theory of say engines, you're trying to optimize the efficiency.
769240	773960	It's a good question what you're trying to optimize for language models.
773960	777920	I think this is definitely a direction for future research.
777920	779360	Do you have any thoughts on this?
779360	780360	Yeah.
780360	786760	I think the thing that we saw was that even very simple questions about how these LLMs operate,
786760	791080	their input-output relationships, when you start to treat them just as a system that
791080	795080	maybe there's an imposed input, like a system prompt, and then you get to pick a subset
795080	796800	of those tokens, right?
796800	800480	When you start to treat it like that, and you just ask a really simple question, like,
800480	803640	let's say that I want it to generate a specific string.
803640	806680	We're not going to be trying to use it to do some intelligent information processing.
806680	809960	I just want to see, can I make it do something?
809960	814440	And what we found and what sort of motivated us to do this is that we really had no idea
814440	819240	when it would be possible or if it was generally possible to make it do anything we want.
819240	823240	Can we just make an LLM system generate any output we desire?
823240	826200	And if the answer is yes, which seems like it's probable.
826200	829800	If you get to have a lot of tokens in your input that you control, it seems reasonable
829800	834520	that you'd be able to probably get it to output a wide variety of at least reasonable
834520	837720	English sentences or linguistically valid sentences.
837720	842600	But the question that we had was, OK, if you have a finite budget for that, would you be
842600	843920	able to get it to do anything?
843920	848320	And what budget of tokens, like how many tokens do you have to be able to control if you want
848320	851520	to be able to make the system do whatever you want?
851520	855200	And that was the initial motivation where it was like, yeah, there are all these high
855200	860440	and mighty sort of questions of how do we make these systems do what we want in an alignment
860440	861440	sense?
861440	864480	How do we make them do what we want in the sense of cooperating towards some information
864480	865880	processing objective?
865880	869080	But we realized that these really, really simple questions are just, OK, you have an input
869080	872440	that you get to partially control and you're trying to make it do something.
872440	875480	That question was completely unanswered and we were sort of taking bets on it.
875520	877560	I think Cameron was the one who started to make bets.
877560	880000	He was like, I bet like $10 that we can get this done.
880000	882960	We can make it emit this output within five tokens.
882960	887320	And that was really the initial motivation where it was like, even the feed forward dynamics
887320	889680	of this system are really mysterious.
889680	893960	And getting a grip on those, it seems like that's a really strong way to start building
893960	899160	up a fundamental control theory and a really strong understanding of these LLM systems where
899160	902920	in control theory at least, when you start to really deeply understand just a single
902960	907640	system with its own dynamics and how the input-output relationships work, what the reachable sets
907640	912560	look like, how controllable it is, then when it comes to building more complicated systems
912560	918280	where maybe you have a more complicated objective, maybe you have interacting systems, when you
918280	921120	really understand the fundamentals, it makes that way easier.
921120	925400	And so the example in classical control theory is that you observe that if you couple a bunch
925400	929480	of linear controllers and linear systems together, what you get is just one bigger linear system
929480	930920	and all of the same stuff applies.
930920	936280	So what we were hoping is that by starting to answer this really simple question of just,
936280	938360	okay, how much can we control this?
938360	940440	What does the reachability of these LLMs look like?
940440	942000	We're really hoping to build that up.
942000	945200	And to me, it feels like we're kind of doing our homework where in engineering, we had
945200	947000	to take all these classes in control.
947000	949680	And that was sort of our homework to be able to go into the world.
949680	954600	And if it ever comes time to build some electromechanical system and get a PID controller in there,
954600	958440	now we've done our homework so we can have a sense what to expect, how we could do engineering
958440	959440	on it.
959440	961640	So that's really where I feel like it's at.
961640	965360	And I think this is a really promising way to try to get a really fundamental understanding
965360	967640	of what's going on with these language model systems.
967640	968640	Amazing.
968640	972600	So in a second, we're going to introduce this concept of reachability.
972600	977040	But I've thought about this because I've had a couple of days to reflect on this.
977040	980920	And my intuition, intuitions just seem a little bit mixed up.
980920	985160	So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building
985160	990560	on adversarial examples and writing algorithms to find adversarial examples.
990560	994520	And we know that neural networks are not robust.
994520	1000160	You can quite easily perturb, let's say, an input image in a vision model.
1000160	1003960	And if it's a classifier, you can make it pretty much say anything with a very small
1003960	1005240	perturbation.
1005240	1008920	And that's kind of the same thing as what you mean as reachability.
1008920	1013440	It's this idea to kind of reach into the state space and make it do something quite
1013480	1015680	weird outside of what you would expect.
1015680	1021040	Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do,
1021040	1025960	you know, I didn't think they had this problem, but they do have this problem.
1025960	1030680	And you introduced this really interesting, I guess it started out as a thought experiment
1030680	1032480	and you coded it into a game.
1032480	1034240	And it's the Roger Federer game.
1034240	1035840	I think that's quite instructive.
1035840	1037320	So can you tell us about that?
1037320	1038000	Yeah, for sure.
1038000	1043120	So one of the earliest examples that we were thinking about was just a simple example
1043120	1045560	of you have this state sequence that's imposed.
1045560	1046480	You don't get to pick it.
1046480	1048560	It says, Roger Federer is the.
1048560	1052200	And then the next thing that you want it to say, the thing that you want the LLM to
1052200	1054320	generate, is the word the greatest.
1054320	1056720	So you want to say, Roger Federer is the greatest.
1056720	1061480	And you're trying to pick out a prompt that comes before then that will steer the system
1061480	1062760	so that it'll output that.
1062760	1066600	So we're basically asking the question, you know, is this word in the reachable set
1066600	1071000	of outputs, given that we have some finite control over the input, where the goal of
1071000	1074360	the game is to, for one, get it to actually output the right answer, which is the greatest,
1074360	1078200	which is a fairly reasonable English thing to say.
1078200	1083960	And the metric that we use to grade how well you're doing on that is basically how efficiently
1083960	1088680	you're able to do control, where in the original control theory, this idea of efficient or
1088680	1090240	optimal control is really important.
1090240	1094840	You have this linear quadratic regularization idea where you're like, I have only a finite
1094840	1096880	energy budget for the signal I put in.
1096880	1102160	Similarly, with language models, what we're interested in is the minimal length of the
1102160	1105920	control input that will steer the model successfully to what you want it to do.
1105920	1109880	And it turns out that the game is actually very challenging, at least with this GPT-2
1109880	1113120	model, which is the one that we're using right now, since it's just running out of a desktop
1113120	1115200	on my desk at home.
1115200	1119880	And so, yeah, there's this game that you can play, we can link it where you get to put
1119880	1123800	in a prompt to the system, and it'll come back to you and say, OK, you got the answer
1123800	1129000	right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy
1129000	1132360	loss on getting the correct output, the desired output.
1132360	1136280	And the game is to basically get the shortest prompt that will steer the model to the desired
1136280	1137360	output.
1137360	1141000	And it's actually quite challenging with GPT-2, where I think only four people, including
1141000	1145320	Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4,
1145320	1148880	which is this resume checker that uses language models to basically predict your probability
1148880	1150800	of getting into a Fang company.
1151320	1155320	I think those two were the only people who actually ended up getting it right, and it
1155320	1157160	turns out to be very difficult.
1157160	1162180	So that game was sort of a codified sort of interactive version of our initial motivations
1162180	1165320	for this, where it was like, wow, this really simple question that seems like there should
1165320	1166320	be an easy answer.
1166320	1168240	I mean, if there is an easy answer, I'd love to know.
1168240	1174240	But the simple question really leads to a problem that's quite difficult to solve, and
1174240	1178920	we really have poor insight on, and we're really just trying to get that insight together
1178920	1180080	to understand what's going on there.
1180360	1184160	And just to jump off that point as well, I think one of the reasons why this game in
1184160	1190680	particular is difficult is because we're using GPT-2, and Roger Federer is the blank.
1190680	1194640	You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots
1194640	1195800	of fill-in-the-blank tasks.
1195800	1199060	It tends to output just a set of underscores quite often.
1199060	1203200	To comment on your intuition you've mentioned before on whether language models have this
1203200	1206920	adversarial property, one thing that was really interesting when we were doing some of our
1206920	1210440	initial work was this technique of soft prompting.
1210440	1216480	So soft prompting, instead of selecting discrete tokens, which we want to adversarial change
1216480	1222400	the model's behavior with, soft prompting modifies the embedding vectors directly.
1222400	1227520	So you have a lot more fine-grained control over the outputs, and it turns out when you
1227520	1233040	soft prompt, when you adversarily attack not the tokens themselves, but the embedding
1233040	1237760	vectors, you can send the cross-entropy law straight to zero for whatever token you want
1237760	1241760	with a very tiny adjustment in these embedding vectors.
1241760	1243080	So this is very interesting.
1243080	1248000	This points to the fact that the real challenge with controllability is not necessarily that
1248000	1253120	there aren't adversarial inputs for language models, but just it's very hard to search
1253120	1256640	this exponential space of discrete prompts.
1256640	1261800	Yeah, and so I guess there are many degrees of freedom in any deep learning model.
1261840	1263240	It's a very highly dimensional model.
1263240	1267600	There are many degrees of freedom, and I'm trying to understand my intuition.
1267600	1275240	So it's trained with a softmax, for example, and certainly when you do temperature sampling,
1275240	1279320	the likelihood is that you're only going to get the top few tokens.
1279320	1284240	I mean, if you look at the distribution of the probability, it's almost certainly this
1284240	1287200	one or this one, and then it just tails off very, very quickly.
1287200	1292640	And I assume that inductive prior was quite deliberate, really, to increase the statistical
1292640	1295080	tractability of the model.
1295080	1300800	But underneath that, in the embedding space, it's not a shell at all, even though there's
1300800	1305760	some low-level surface of embeddings, and you can traverse this.
1305760	1306760	Right.
1306760	1311560	So initially, you might think that this embedding space is a very rich representation of the
1311560	1313440	meaning of different words.
1313440	1318280	And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for
1318280	1322240	any large language model, you'll find something that roughly corresponds to the meaning.
1322240	1326360	I mean, words that mean similar things are attached more closely together.
1326360	1330160	But this opens the question, if you were to interpolate between two similar words, take
1330160	1334320	the embedding vector that is halfway between, would you get the halfway in between word,
1334320	1336800	or would you get something that's nonsense, right?
1336800	1341840	And I think what you find by these kinds of soft-prompting experiments, by directly
1341840	1347120	manipulating the embedding vectors, is that the embedding space is actually extremely
1347120	1353960	non-convex, in the sense that by interpolating, you don't just get an average value between
1353960	1354960	the two of them.
1354960	1355960	Yeah.
1355960	1360360	I don't know if this is best to get into, but one of the techniques we were trying to
1360360	1363280	use is this technique called gumball softmax.
1363280	1369160	So instead of a discrete search over the token space, one thing you can do is it's kind of
1369160	1374680	like the repair metrization trick for variation autoencoders, but it works for a categorical
1374680	1376280	distribution.
1376280	1382360	And so you can use this trick, and it essentially works by kind of interpolating between embeddings.
1382360	1386280	But it actually was very difficult to get to converge and did not even close to rival
1386280	1389160	the performance of GCG.
1389160	1394960	My intuition is that when you take a data point off the manifold, because these neural
1394960	1397920	networks, they do learn a manifold of language.
1397920	1402640	I thought if you take a data point off the manifold, it would cause some kind of mode
1402640	1403640	collapse.
1403640	1408160	It would just cause the network to become chaotic and go crazy.
1408160	1410880	But apparently that's not the case.
1410880	1411880	Can it recover?
1411880	1416320	It's almost like if you put a bunch of tokens in which are just really weird, and then you
1416320	1419760	just carry on, it's like the language model recovers.
1419760	1422840	It finds coherence again, and then it just carries on.
1422840	1423840	Yeah.
1424520	1428360	It's honestly a really hard question to answer, where in different regimes, we've noticed
1428360	1434160	different things where if you choose this adversarial prompt so that basically these
1434160	1439920	prompt optimization algorithms all work in the same way where you're trying to maximize
1439920	1444400	the likelihood of some desired string, and then you're able to modify some input.
1444400	1450400	And so depending on how you choose that, you can do the optimization so that the model
1450400	1452640	will output some gibberish.
1453000	1457120	It seems like depending on the model, depending on the sampling techniques, I've seen it go
1457120	1461240	both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable
1461240	1466040	coherent text, and other times it seems like it'll continue to generate some random stuff.
1466040	1469000	It'll kind of be in this outer distribution mode.
1469000	1474080	I think that that's one of the reasons that I think that these adversarial examples, studying
1474080	1477440	them as well as this control theory stuff is really important where it's like, yeah,
1477440	1482600	if you have a system in the real world where tokens are coming in, you're actually processing
1482760	1487360	them from real users, you don't have total control, but the user is the one who's giving
1487360	1488360	the control input.
1488360	1492480	You want to make sure that your system is sort of robust to that, where there's a lot
1492480	1496480	of really complicated interactions as it turns out between, for instance, the tokenizer
1496480	1500520	and the incoming strings, where when you do this prompt optimization, sometimes it'll
1500520	1504400	come out with a sequence of tokens that if you convert it to a string and then convert
1504400	1509040	it back to tokens, it'll actually be very different, which we ran into with this game
1509040	1510480	where I was like, oh, I'm going to cheat at this game.
1510520	1511800	I want to be the top prompter.
1512000	1515200	So I'm just going to use some of the algorithms that we had from our GitHub repository, the
1515200	1518640	magic words GitHub repository to basically optimize these prompts.
1518640	1522440	But then when you convert it back to a string, then it turns out not to work as well.
1522720	1527080	And so, yeah, I think that answering that question and seeing when is it that the model
1527080	1530760	will actually be able to recover, is it a function of how big the model is, are bigger
1530760	1534480	models better at recovering, or is it the case that bigger models are maybe more
1534480	1539480	controllable, maybe you can shift these models into this weird sort of, sorry, just
1539480	1544320	on the mic, but this sort of out of distribution regime where they're generating this seemingly
1544320	1547960	random output based on seemingly random input.
1548280	1552720	And so, yeah, I think that that question is really, really important and is one that is, I
1552720	1556000	think, well addressed through considering them as systems, which is sort of the thesis
1556000	1556560	of this paper.
1556560	1562040	And we're trying to get a grip on what exactly the case is, you know, is it going to be
1562040	1565200	able to recover, is that a consistent behavior, or is it not?
1565960	1570200	There's this sort of weird recurrence relationship between the prompt and then the
1570200	1573320	stuff that the language model generates, and then the stuff that's generated in the
1573320	1577160	future, where in effect, you know, you're able to pick a prompt, and then the language
1577160	1578560	model will generate some more text.
1578760	1581200	But then that text becomes sort of part of the prompt as well.
1581440	1586400	So it seems like maybe there could be these sort of degenerate states where if you start
1586400	1590640	with this seed of chaos, it'll basically branch out and the future strings that it
1590640	1592960	generates is going to prompt it into being more and more chaotic.
1593280	1596640	And that's basically stability analysis or sensitivity analysis.
1596880	1601240	And there's all this like rich vocabulary and all of these people who have spent
1601280	1605800	basically hundreds of years thinking about these concepts for both, you know, discrete
1605800	1610000	and continuous dynamical systems that we get to build on top of and basically use
1610000	1612640	their insights to understand, you know, what does it mean?
1612680	1613840	What does stability really mean?
1613840	1618160	We can just draw those definitions in, apply them to our generalized form of a
1618160	1620040	system, a language model system.
1620280	1623960	And I think that's why the control theoretic aspect is exciting, where you can
1623960	1627200	actually ask these questions in a very concrete and reasonable way.
1627520	1631840	And the best part is that people haven't really been using these, these ideas or
1631840	1634720	using this vocabulary to describe the questions that we're trying to answer.
1634920	1638840	And so most of these things, if you just spin up, you know, a small GPU and test
1638840	1641640	some stuff out with a seven billion parameter model, you're actually doing new
1641640	1644400	research and it's actually some useful research, in my opinion, where you're
1644600	1648000	getting a sense of the control theoretic properties of language models.
1648240	1651200	And to me, that felt like the most exciting thing here.
1651240	1654560	The open questions are the most exciting part of the paper to me, where we've
1654560	1658720	taken a stab at basically the, you know, empirical study of controllability by
1658720	1661840	sampling these wiki tech sequences, seeing if we can control the next
1661840	1666160	token, the next few tokens, as well as some sort of theoretical results on
1666160	1667680	self-attention and its controllability.
1667920	1671360	But then all of these open questions emerged just because we're now
1671360	1674360	framing it as a system and people for hundreds of years have been thinking
1674360	1678720	really, really deeply about how you understand systems when they're used in
1678720	1681200	the real world and you have this sort of finite control of them.
1682360	1683360	Yeah, that's really interesting.
1683360	1686840	I mean, I suppose I'm pointing out the obvious here, but these are auto
1686840	1687680	regressive models.
1687720	1691400	So the answer gets kind of fed back into the prompt and then we rinse and
1691400	1694680	repeat, which means you can model them as dynamical systems.
1695000	1698680	And that is in stark contrast to something like a vision classifier where, you
1698680	1700480	know, there's just an input and an output and that's it.
1700520	1701480	That that that's the end.
1701760	1706320	So now you can get the system into this kind of corrupted state where, you
1706320	1708480	know, you get divergence and decoherence.
1708480	1712080	And as you said, that that that could be analyzed with stability analysis.
1712360	1713440	But I find that fascinating.
1713440	1716640	But we should just go back quickly to your Roger Federer example.
1717000	1721280	So I'm interested in the different ways that we could go about this.
1721520	1725760	So the humans were kind of using language and language are a bunch of
1725760	1727480	mimetically shared cognitive tools.
1727680	1731120	And they were saying things like, you know, you know, basketball is a great
1731120	1732880	and, you know, Joe blogs is great.
1733240	1734440	Roger Federer is great.
1734880	1738080	And it wasn't very parsimonious, but it but it worked.
1738520	1742720	And then, you know, another approach that that that you spoke about is you
1742720	1746360	could just make a Python program and you can just let's try a neighborhood
1746360	1748240	greedy search one token at a time.
1748240	1753360	So we find the nearest token and then we find the second nearest token until
1753360	1754640	we find the adversarial attack.
1755160	1757920	Or we could do like a low level gradient search.
1758160	1760400	And then we can find something really weird and wonderful.
1760400	1764280	There might be some esoteric characters that just make it go bananas.
1764600	1768440	But these are three very, very different levels of talking to a language model.
1768600	1772880	The word on the street is that language models are a new form of programming
1773200	1777480	that you can just say what you want to do using English language and so on.
1778640	1782760	And language models certainly seem to incorporate that structure.
1783040	1787240	But the language models themselves are just an inscrutable, you know,
1787360	1789560	set of, of, of neurons, right?
1789560	1791760	And, and weights and matrices and so on.
1792200	1795240	So there's some, there's a kind of higher resolution
1795240	1797480	shog off going on underneath the covers.
1797520	1799280	That's more or less the picture I have.
1799280	1803640	We have this interface where we can speak to the language model using language.
1804000	1809200	And if we set up a conversation with a language model where we have different labels,
1809200	1813760	you know, chat, GBT says this, Cameron says this, and, you know,
1813760	1817080	you engage in a conversation because it is seen enough conversations and it's
1817080	1820880	training data, then it's able to play along very fine.
1821200	1824480	What's going on under the hood, of course, like you say, it's very inscrutable.
1824800	1827640	It's very difficult to really probe and understand.
1828120	1831320	There are certain techniques in the interpretability literature,
1831520	1835400	but I don't think as a whole it's we're even remotely close to having
1835400	1837480	a complete understanding of how these systems work.
1837760	1841600	But that's one of the reasons why I think that control theory is a great way
1841600	1843960	to kind of break in and see what's going on.
1843960	1847920	Because if you just look at the system's input and output characteristics,
1848240	1851680	you can really gain a lot of insight into the nature of these systems.
1852280	1857600	One guiding principle in my life doing engineering and trying to learn
1857600	1860560	about the world has been this quote by Richard Feynman.
1860560	1864080	It's very popular. What I cannot create, I cannot understand.
1864720	1866680	And yet today we find ourselves in this situation
1866680	1870480	with language models where we have these incredibly complex systems we built
1870480	1872480	and yet we can't really get into them.
1872800	1877560	So to extend this to today, what I would say is what I cannot control,
1877600	1878600	I cannot understand.
1879560	1884600	The way I think about it is it's almost like you want the language model
1884600	1887200	to be a high level controlled, robust interface.
1887720	1891760	And it's almost like we're all Marvel characters
1891800	1895000	and we can give secret hidden codes.
1895160	1896000	It's like me now.
1896000	1899160	Imagine if I could just through telepathy control your behavior
1899440	1901960	and anyone can do that with a language model.
1901960	1905480	They can just put weird tokens in and they can manipulate its behavior.
1906000	1909280	And there's there's no there's nothing stopping you.
1909280	1910280	There's no firewall.
1910280	1913320	I feel like the this kind of harkens to why we call the paper.
1913320	1917560	What's the magic word where, you know, the initial reason was just that,
1917640	1920640	you know, it's almost like the LLM is asking you if you wanted to do something.
1920800	1921680	What's the magic word?
1921680	1924400	Like, what's the this key, this weird control prompt
1924400	1925800	that will just make it do the right thing?
1925800	1929320	But I think more generally, you know, I used to be into magic when I was a kid.
1929320	1931960	I had to jog at a restaurant doing, you know, card tricks for the patrons
1931960	1933360	while they waited for their food.
1933360	1939080	And what magic is, is basically you're playing tricks on the human perceptual system
1939080	1942200	where there are all of these sort of inductive biases that the human
1942320	1945760	perceptual system has where, you know, for instance, if I move something
1945760	1948800	and I look at it, you naturally will tend to follow that my gaze
1948800	1950920	and what is moving is generally more salient.
1950920	1953040	And so then I can like do something over here with my other hand,
1953040	1954320	like take something out of my pocket.
1954320	1957320	And then when I display it, they'll be like, oh, my God, where did that come from?
1957320	1960600	Right. And what we're discovering, I think, is a sort of similar thing
1960600	1963800	with language models where, for one, you know, people have observed
1963800	1967160	that if you use sort of human social engineering tricks on them, like,
1967280	1970080	oh, I'll tip you $500, then, you know, it'll do a bit better.
1970680	1974560	But then there's this whole other sort of perceptual layer, I guess you could call it
1974760	1978640	where there's this sort of chaotic regime of adversarial prompts,
1978680	1983400	kind of like hypnosis, kind of like magic, where if you give it these very
1983400	1987520	strange, very inhuman looking prompts that will steer it to this,
1987640	1990920	to just making a certain output, extremely likely, right?
1991240	1995400	And so to me, it feels really similar to digging into like magic
1995400	1998280	and the human perceptual system, just with LLMs, where we're learning about
1998280	2003000	basically the shape or the what the nature of these language models are in terms
2003000	2006880	of how they interact with the world and how they, how their dynamics really work.
2007080	2011040	And I think that it's very sensible that the control
2011040	2015120	theoretic perspective would be useful for this, where in classical control
2015120	2018960	theory, trying to control these systems actually taught us a lot about
2018960	2021080	the nature of systems, both linear and nonlinear.
2021320	2023960	And I think that we have a very similar opportunity here where we're really
2023960	2027760	discovering what is the nature of these language models in terms of control,
2027920	2031000	where these questions don't emerge quite as naturally and don't have
2031000	2034240	quite as natural of an answer when you're just thinking about them as a sort
2034240	2037400	of probability distribution over text, thinking about them in terms
2037400	2040400	of being systems that have inputs and outputs and these trajectories and the
2040400	2043840	like actually really does change the kinds of questions that you end up
2044080	2046800	being able to answer and the kind of understanding that you get about the
2046800	2049680	nature of the system itself, which to me is one of the most exciting things.
2049920	2052000	So yeah, that's so interesting.
2052000	2057200	The magic example thing, I think we, we think that we are robust, but we're not.
2057200	2060080	Maybe we're system two robust, but we're not system one robust.
2060080	2063720	And if you look in the animal kingdom, there are so many examples of, you
2063720	2067040	know, like a hen, if you make the right kind of clucking noise, the mother will
2067040	2069040	think that you're that you're the chick.
2069480	2072680	So it's really, really weird, actually.
2072680	2076080	And Keith gave me this example of, I think it was from science fiction, that
2076320	2077600	there's a hypothetical image.
2077600	2080280	And if you look at the image, every single person goes into a coma.
2080640	2083760	And what's interesting about that is it's a kind of, you know, population
2083760	2087240	level adversarial example rather than an individual adversarial example.
2087520	2090480	But then it gets into the question of, you know, how can we use this
2090480	2093040	control theoretic approach to robustify models?
2093040	2095800	Cause we're talking about building a genetic LLMs.
2096120	2099640	And part of the thing I'm trying to get my head around is in this particular
2099640	2104560	case, we had a very clear kind of cost function, you know, a specific thing.
2105000	2108760	But what would it mean to robustify language models in, in the general?
2108960	2113120	So one, one of the things that came up in our, you know, sort of literature
2113120	2118080	view was this idea of, you know, when you're trying to control these discrete
2118240	2122680	stochastic dynamical systems, one concept that can be quite useful is
2122880	2126640	you might have a set of outputs that you want to reach or a set of outputs
2126640	2127520	that you want to avoid.
2127520	2130240	So an avoid set and basically a desirable set, right?
2130640	2134840	And when you frame it like that, you know, I think that the robustification
2135080	2138160	comes from the fact that let's say that you have a set of outputs, you
2138160	2140440	really don't want the language model to, to emit, right?
2140640	2143560	You might think, okay, well, I'll just fine tune it so that it decreases
2143560	2146760	the likelihood, the prior likelihood basically of those sequences, right?
2147160	2150040	And the issue with that, I think, and the thing that the control
2150040	2154440	theoretic perspective sort of brings in is the fact that when you have
2155440	2159320	finite, even a small control prompt, some extra tokens that you get to inject,
2159520	2164200	it turns out that even very, very unlikely next tokens can be made to be
2164200	2168240	the most likely next token just by inputting these new examples.
2168240	2172880	So even if you did hypothetically fine tune the model so that this avoid set
2172880	2177080	was assigned very low probability, it seems like if you don't incorporate
2177080	2180320	some aspect of, you know, maybe stochastically trying to search for
2180320	2183600	these adversarial examples and sort of having this sort of mini max thing
2183600	2186800	where you have one system that's trying to elicit the output, one system
2186800	2190160	that is trying to fine tune the model to maybe make it less likely or optimize
2190160	2193400	another part of the prompt that is supposed to steer it away from these outputs.
2194200	2197400	Basically, the inside, I think, is that you really have to be careful
2197400	2201160	to consider the fact that you have, you're giving the outside world some
2201160	2204480	amount of control over the system, some amount of control over the context.
2204720	2208080	And planning around that is actually very non-trivial and is not really
2208080	2211680	well managed, I don't think, through the classical view of just cross entropy loss
2211880	2214240	and just treating it like a probability distribution.
2214680	2219520	Something else that fascinates me is the divergence between focusing on
2219520	2224120	the model versus, you know, complexifying the software which controls it.
2224400	2227760	So right now, for example, we have language models and, you know, there's
2227760	2231840	this kind of base training and then there's fine tuning and there's RLHF
2231840	2235840	and, you know, there's like command variations of that, for example.
2236240	2239440	And then we build these software APIs that are just trying to abstract
2239440	2244680	away the complexity, so they will do dynamic prompt construction for multi,
2245960	2249000	you know, multi-stop tool use and it goes on and on and on.
2249000	2253520	There will be frameworks for doing agentic LLMs and there just seems
2253520	2255760	to be like a bit of a divergence here.
2256000	2259920	But the reason I'm asking the question is, does it make sense to
2260120	2262840	robustify and fix the problem in the model?
2263040	2267800	Or does it make sense to almost increase the flexibility of the model
2267800	2269560	and fix it in the software layer?
2270360	2275240	I think one of the insights from our paper is that solely focusing on
2275240	2279680	the model itself, like Amman was just saying, as soon as you give the
2279680	2283440	outside world control over the model in the sense of being able to input
2283480	2287600	whatever kind of text that they want, it becomes very difficult to
2287600	2291280	really prevent adversarial attacks and prevents jail breaks.
2291280	2293520	And that's, you know, why you see jail breaks keep coming up.
2294120	2300400	I think if you were to involve some sort of robustness in a software layer,
2300640	2302000	that might be more feasible.
2302800	2307880	At least I can't immediately picture, you know, ways around it as, you know,
2308000	2312640	of course, if I was a hacker, I could probably, you know, find some loophole.
2312640	2314840	There's usually some loophole you can find.
2314880	2319560	But if there was some way of fielding the prompt messages, for instance,
2319840	2323880	a user gives you a prompt, first you check, is this a reasonable thing
2323880	2326640	that a human being would say in conversation, or is this something
2326640	2329720	that I've never seen before in the entire history of the internets?
2330080	2330400	Right.
2330880	2335240	The latter maybe is a prompt injection, maybe is, you know, something
2335240	2337960	devious, or maybe is, you know, computer science research.
2338800	2341400	But yeah, it's definitely not an easy problem.
2341440	2344040	But the good thing is that there are multiple approaches to it.
2344400	2344920	Very cool.
2345160	2347800	So we're going to go on to the more galaxy brain stuff in a second.
2347800	2350440	So before we move off the paper, can you just talk more formally
2350440	2352680	about what you showed in the paper?
2353000	2353720	Yeah, definitely.
2353720	2356040	So there were two main parts of the paper.
2356440	2357560	So I guess three.
2357560	2361920	So for one, what we did was we tried to formalize what an LLM system
2361920	2363960	really is at a mathematical level.
2364200	2367000	And what we were trying to do at that was basically balance the fact
2367000	2370360	that, you know, we really wanted to try to take advantage of, you know,
2370360	2373480	the original sort of control theories, very abstract picture of a system
2373480	2377280	where you have this input space, you have a state space and output space.
2377440	2379160	And there's some dynamics going on inside of it.
2379640	2383400	In our case, we parameterized those dynamics with an LLM and our input
2383400	2386760	space and our state spaces were basically the set of all possible
2386760	2389960	token sequences from the vocabulary set of this model.
2389960	2390280	Right.
2390480	2391760	So that was the first part.
2391800	2395600	And we basically transferred over a lot of the notions of basically
2395600	2399520	reachability and controllability for LLM systems from the original control
2399520	2402880	theory where you can really just define it in terms of this really abstract
2402920	2407000	notions of, you know, have sets for the reachable or sorry, the state space,
2407000	2409400	the input space and the output space, you have some dynamics.
2409400	2412760	And basically in terms of those sets, you can define reachability and control.
2413000	2414000	So that was the first part.
2414600	2416960	The next thing that we did was we tried to look inside the model.
2416960	2420400	So we were thinking, you know, it'd be really nice, like in control theory,
2420840	2424440	if we could have a really good understanding of the components of the system
2424440	2426480	and how controllable those individual pieces were.
2426760	2430160	So what we did is we looked at a single self-attention head and tried
2430160	2432760	to really think about it through a matrix algebraic perspective.
2432960	2436280	To really break down what the relationship is between, let's say,
2436280	2439400	you have a subset of the tokens, you get to control a subset that's fixed.
2439640	2442560	And you're trying to get the output to be, you know, a certain value,
2442560	2445800	the output representations where all of these in the case of a self-attention
2445800	2448760	head are just these vector representations of tokens.
2449400	2453720	So what we found there was that it actually is possible to do some fairly,
2453760	2457800	you know, simple matrix algebra manipulations to decompose the output
2457920	2462480	of a self-attention head into one component that arises from the imposed input.
2462600	2466440	And then another component that arises from the control input, and assuming
2466440	2470440	that those two are bound, then you can actually derive that, well,
2470440	2473640	there actually is this geometry that sort of looks like a bubble around
2473640	2474600	the default output.
2474600	2477720	So the output, if you didn't have any control input in, there's a sort
2477720	2480920	of bubble of reachable space that scales with the number of control
2480920	2482640	input tokens that you're able to use.
2482960	2486080	And we thought that that was really exciting because for one, I didn't
2486080	2488800	really expect that you'd be able to do proofs on these sort of, you know,
2488880	2493280	very complicated, high dimensional machine learning or deep learning systems
2493280	2494240	like a self-attention head.
2494560	2497680	But it also gave us some insight to say that, okay, we actually have
2497680	2501280	this really concrete relationship between the sort of number of control
2501280	2504600	input tokens, the magnitudes that you're able to input into the system,
2504840	2508800	and the output reachable set that is at your disposal, basically.
2509120	2511000	And so that was the second part.
2511000	2514200	And then the last part was some empirical experiments where we said, okay,
2514200	2516840	let's just sample a bunch of strings from Wikipedia.
2517080	2521760	And we'll see, okay, the strings were between eight and 32 tokens.
2521760	2524040	And those were basically our imposed state sequences.
2524280	2527560	And we asked the question, well, can we get it to output the correct next
2527560	2529240	token, the real next Wikipedia token?
2529480	2532640	How many, you know, input tokens does it take or control input tokens does it
2532640	2534000	take for that to happen?
2534000	2537560	It turned out that you could get that done about 97% of the time to steer
2537560	2541760	the model to the correct output within 10 tokens of a control input, which is
2541760	2544800	reasonable, you know, we'd expect that the model should be able to be steered
2544840	2548360	towards reasonable true English sentences that were more than likely in the
2548360	2549160	training data set.
2549800	2553040	What we did next was we tried to figure out, you know, if you sample the top
2553040	2559280	75 most likely tokens, according to the model, based on this fixed input, can
2559280	2563600	you steer those things to be the most likely token, basically the arg max of
2563600	2564720	the probability distribution?
2565080	2568840	And what we found there is that it's about 89% of the time, at least 89% of
2568840	2572360	the time, we were able to find these optimal control inputs that were less
2572360	2574680	than 10 tokens long, that would steer the model to do that.
2574920	2577360	And then the last thing we did was he said, okay, well, let's see what would
2577360	2579840	happen if we just randomly picked a token from the vocabulary.
2579840	2584040	So this is everything from regular English to numbers to Cyrillic characters
2584040	2585040	to Chinese characters.
2585360	2586960	What if we just randomly sampled those?
2587200	2590760	And we tried to see how many tokens it would take to steer that to being the
2590760	2592320	arg max of the probability distribution.
2592600	2596600	And we found there is about 46% of the time we were able to make that next
2596600	2601240	token, the random one, the most likely next token using a prompt of length 10 or
2601240	2606480	less. And the sort of curves are there in our, in our paper that described as
2606480	2610120	you have an increasing budget for these tokens, how much of the time were we
2610120	2611920	able to basically steer it to the right output?
2612120	2615560	That's our basically the K epsilon controllability metric that lets us get
2615560	2619320	this sort of statistical picture on controllability that renders it sort of
2619320	2622280	practical to empirically estimate for these complicated systems.
2622640	2624560	And so those are really the main results.
2624560	2627880	And the surprising thing about the last one that I mentioned before was that a
2627880	2632080	lot of times even really unlikely next tokens were able to be steered to be the
2632080	2636080	most likely just using a really short prompt, which both gets at the, you know,
2636280	2640960	basically chaoticness or complexity of language as a system, as well as the fact
2640960	2644560	that the prior likelihood picture or the cross entropy loss picture doesn't
2644560	2649360	quite get at the controllability sense of when you do have a, you know, ability
2649360	2652000	to input tokens into the context, what happens then?
2652040	2653640	So those are the really the main results.
2653640	2656800	And then I mean, to me, the exciting, the really exciting part was the open
2656800	2659840	questions where I was like, Oh, now that we're using this vocabulary, now that we
2659840	2664080	formalize these LLMs as systems, it's really easy to ask these, you know,
2664080	2667360	additional questions about, you know, the nature of the systems and the
2667360	2671120	steerability controllability, especially with feedback or chain of thought or,
2671160	2673440	you know, agents or all of these other ideas.
2673680	2675600	And so yeah, that was basically the paper.
2675840	2676160	Yeah.
2676160	2679440	And it's really making me update my intuitions, right?
2679440	2681400	So I'm thinking about the bias variance trade off.
2681840	2686240	And I'm thinking that the reason we build these inductive priors is to
2686240	2690280	constrain the model intentionally to make it statistically tractable to reduce
2690280	2691920	the size of the hypothesis class.
2692360	2697360	But what you're saying is making me think that statistical tractability and
2697360	2700520	flexibility are not necessarily the same thing.
2700920	2705920	Now it seems that the model must maintain a degree of flexibility.
2705920	2706920	I mean, it makes sense, right?
2706920	2710640	You have to be flexible in order to be a successful model.
2711400	2713880	But that creates a kind of adversarial attack.
2713880	2717880	So you can, the way I think about this is the model should be like the
2717880	2719640	interstate freeway of language.
2719920	2723040	So all of the major roads should be carved out and there should be side
2723040	2723680	roads and so on.
2723680	2725520	And that's the way I visualized the model.
2725760	2726840	But the model's not like that.
2726840	2730600	There's actually like all of these little slip roads and you can kind of
2730600	2734360	push the cars off into the slip roads, but you need the slip roads because
2734360	2736320	perhaps you couldn't train the model without the slip roads.
2736520	2738600	Yeah, I think, I think that's a really good analogy.
2738600	2744840	I think that's, um, thinking about pushing cars off the road into this space
2744840	2751440	where they perhaps aren't used to being and what happens next.
2752120	2756160	This, this is a case where the language model can answer some of these mode
2756160	2759320	collapse type regimes and you can get kind of weird outputs.
2759680	2765000	This is where you also, um, I mean, it was surprising that you can get the
2765120	2770520	least likely token with just a specific inputs to be the most, the most
2770520	2775160	likely next token, but if we treat language as this kind of road or as
2775160	2778640	this kind of map structure, then it kind of makes sense that once you get off
2778640	2782840	the map, once you enter this kind of regime that is completely unexplored,
2783200	2787080	which there are actually plenty of regimes like this again, because the
2787080	2792200	space is exponential in the number of tokens, it's growing so incredibly fast
2792520	2796600	that it's very easy to find pockets that the model has never seen before and
2796600	2799480	maybe no human on earth or it never will be seen again.
2800200	2803800	You guys are really interested in, in collective intelligence and
2803800	2807440	biomimetic intelligence and biologically plausible intelligence.
2807440	2810160	And this is a matter very close to my heart.
2810520	2814280	Um, what, what, what are you guys interested in specifically in that field?
2815200	2815440	Yeah.
2815440	2820680	So I guess when I first got into machine learning, it was from watching
2820680	2824040	this Google DeepMind video where they were using reinforcement learning to
2824040	2828560	teach this guy how to run this virtual reality avatar, how to run really fast.
2828560	2831960	And I thought that was fascinating because it was like, okay, instead of
2832000	2835240	traditional programming, you just have this neural network that optimizes
2835240	2836920	itself according to some objective, right?
2837280	2841000	And the thing that was intriguing to me about that was like the feed forward
2841000	2843320	dynamics of a neural network aren't that complicated, right?
2843440	2846520	You know, you have these synapses, you have this sort of gated action
2846520	2847640	potential function.
2847920	2853120	And the thing that was weird to me was like, how does every neuron know how
2853120	2854360	to change its weights, right?
2854600	2858440	How does each neuron that's independently not that smart know what to do?
2858800	2861880	And so that sort of led me down the theoretical, the theoretical
2861880	2865360	neuroscience route for some time where I was trying to figure out, okay, what
2865360	2868480	do these learning rules look like that don't have to, you know, use the chain
2868480	2871080	rule, use back propagation to update their weights.
2871200	2874600	So I did that for a while and then sort of realized that the question of
2874600	2878360	supervised learning was not necessarily the most interesting question to be
2878360	2881360	asked, where it seems like the lion's share of what makes us really
2881360	2885920	interesting as humans in our cognition seems to be associated with the cortex
2885920	2889800	and this kind of predictive coding module that we have that lets us make
2889800	2893680	these really rich abstract representations of reality, sort of understand what's
2893680	2896600	going on, you know, we sort of hallucinate this internal model of the world.
2897000	2901000	And so the interesting thing to me about the cortex was that, you know, you
2901000	2904440	have this structure that's pretty flat and pretty homogenous throughout, you
2904440	2907560	know, there's differences in different regions, but the end of the day, it's
2907560	2908240	very similar.
2908240	2911880	And in fact, if you lose a sense, like if you lose your vision, that region is
2911880	2913360	often repurposed for other things.
2913360	2916440	So it seems like there should exist, you know, the brain is kind of this
2916440	2920240	existence proof that there should exist this rule set that if you apply it
2920240	2924120	everywhere in the system in this sort of layer on the outside of the brain, then
2924360	2929000	the behavior, the emergent property of that system is that you'll get this
2929000	2932600	really robust and rich sort of representation of the world that is very
2932640	2934880	predictive of subsequent sensory input.
2934880	2935200	Right.
2935520	2938480	And I think that the collective intelligence aspect of that is really,
2938480	2942200	really important where there's one way to go in machine learning where you say,
2942200	2945720	okay, we're going to make this monolithic pile of matrix algebra and we're going
2945720	2948160	to train it through back propagation and gradient descent and the atom
2948160	2949200	optimizer and all of that.
2949520	2953400	And we're going to make it do some prediction task, but at the end of the
2953400	2956480	day, every computation has to be implemented in physical reality.
2956480	2956800	Right.
2957080	2961120	And when we make the abstraction and just say, oh, it's just a bunch of math,
2961160	2962240	we'll just have a GPU run it.
2962560	2965320	It kind of abstracts away from this fact that at the end of the day,
2965360	2969880	you have real physical objects that need to do computation and share
2969880	2974320	information and in the sort of maximum efficiency, maximum scalability limit,
2974560	2978440	it seems like what you'd end up having is a very similar sort of distributed
2978440	2983240	structure where you can't really easily separate memory from computation.
2983240	2986520	I think there's a quote from this MIT professor that says that Turing's
2986520	2990080	initial mistake was saying that the head of the Turing machine was separate
2990080	2990640	from the tape.
2991000	2994000	Uh, and I think that that's true where in reality, you know, in brains,
2994000	2998360	in, in real computing systems, the matter that composes the memory and the
2998360	3000880	matter that composes the computation is really one in the same.
3001160	3004360	And the brain is obviously this really great proof that, okay, there are
3004360	3008440	relatively simple rules that are implementable with these biological neurons
3008440	3011240	that if you just implement them everywhere, we'll get you this really
3011240	3014760	beautiful, you know, convergence and emergent property of intelligence.
3015080	3018200	And that really drove me for a long time in theoretical neuroscience.
3018200	3023560	And then more recently in trying to build these distributed systems of, you
3023560	3027320	know, artificial intelligences that, you know, the dream that I was trying to
3027320	3030720	pursue before we started this control theory thing was that, okay, well, what
3030720	3033680	if I just had a bunch of really small LLMs that, you know, everybody in the
3033680	3037160	world could host and they could communicate with this sort of low band
3037160	3040840	with communication using just tokens, just text over, you know, the regular
3040840	3044680	internet and the emergent property of that, you know, what if it was possible
3044680	3048760	that we can engineer a system that the emergent property was that it would
3048760	3052920	actually be this really capable collective where maybe GPT-7 can be
3052920	3055920	owned by everyone instead of just being behind closed doors in a data center
3055920	3056680	that we have now.
3056880	3061120	We're sort of using these insane engineering, you know, feats of, you
3061120	3063640	know, NVIDIA interconnects and these really high bandwidth connections
3063640	3068040	between massive racks in a data center that take a ton of energy to get this
3068040	3070800	really great result of, you know, modern language models.
3071040	3073840	What if we could have a system that was a bit more like the brain, a bit
3073840	3077600	more decentralized and really leverage this insight that it should be possible,
3077600	3079960	you know, this existence proof keeps coming back to you where it's like,
3079960	3081360	okay, it should be possible, right?
3081800	3085640	And that is sort of originally what led me to the control theory stuff where it
3085640	3088360	just turned out to be really hard where we didn't have a great understanding of,
3088560	3091800	you know, if we're treating these LLMs as systems rather than just, you know,
3091800	3095080	big piles of matrix algebra that we're trying to distribute over many GPUs,
3095360	3098160	if you treat them as systems that are coupled together, they're interacting
3098160	3100640	in this networked fashion, how do we really understand that?
3100640	3102960	You know, is it even possible to prompt them to do the right thing?
3103000	3103800	When is it possible?
3103800	3105120	How long do the prompts need to be?
3105480	3106920	And that sort of led us down this route.
3107480	3110480	But yeah, definitely the collective intelligence thing was, was a big
3110480	3112400	motivation for me to get this working.
3112400	3115880	And there's this neural cellular automata thing that I know you had talked
3115880	3118640	with Michael Levin, who was the last author on that.
3118640	3122160	And we worked with Alexander Mordvinsev on it, where it's this really,
3122160	3126840	really great demonstration of how if you just optimize these basically small
3126840	3131960	MLPs with local interaction to try to satisfy some objective, like, you know,
3132000	3136400	reforming this gecko or lizard in their paper, then you actually can do that
3136400	3137680	with back propagation through time.
3137680	3140760	And so, you know, I thought, you know, it'd be really cool if we could try
3140760	3144000	to engineer information processing systems that did this, not just morphogenesis
3144000	3147360	systems, but information processing systems that operate in this way.
3147360	3150400	Cause, you know, as a graduate of engineering science, we had to take
3150400	3152040	a bunch of these digital logic courses.
3152280	3156080	And when you have this very simple, you basically local state machine that has
3156440	3159280	basically local connectivity, it's really easy to imagine how it
3159280	3162720	would implement that as a custom chip and sort of reach this, you know,
3162720	3166280	as Beth Jesus puts it, you know, thermodynamic limit of AI.
3166520	3167640	And so that really excited me.
3167640	3170640	And so I built a sort of demo of that where it was trying to do visual
3170640	3174240	information processing on this really sparsified video is basically trying to
3174240	3178640	do predictive coding of sorts on our active inference, I guess, on this
3178680	3181600	incoming data stream of really sparsified video, trying to predict what would
3181600	3183840	happen next, and it turned out to work quite well.
3184040	3186560	And so then I was like, well, why can't we do that with language models?
3186560	3189040	You know, as you mentioned, there are all these slip roads, right?
3189040	3192240	Where if you prompt it just right, you can enter this really weird different
3192240	3196480	regime and this exponentially large prompt space is a really handy way to try
3196480	3199720	to control them where, you know, fine tuning is great, but what if we could
3199720	3203520	just prompt them into interacting in a way that would lead to this emergent
3203520	3206880	property of just being basically one larger language model that could
3206920	3208880	predict the next token really, really well.
3209240	3212400	And so that initial motivation sort of led to this control theory stuff.
3212400	3216840	And I think that it is probably the right way to go for the field where if we
3216840	3221360	want to be able to really leverage maximal computation towards our objectives,
3221400	3224320	you know, the bitter lesson by Richard Sutton kind of suggests that we should
3224320	3228320	probably aim for systems where you can just slap on more and more compute.
3228320	3231400	You can have a relatively simple procedure that you follow to leverage
3231400	3233280	more compute towards your objectives.
3233440	3235640	That's probably the way to go for making advances in AI.
3235880	3239600	And if we can have this decentralized networked system that, you know, I took
3239600	3242160	this distributed systems course while I was here, that was really great and
3242160	3245040	sort of taught how to make, you know, basically databases that were
3245040	3248160	distributed over many servers that would have this, you know, the emergent
3248160	3251280	property they wanted was robustness, consistency and availability.
3252080	3255520	If we could have something similar to that, that is radically scalable and is
3255520	3258640	able to be, you know, just run by regular people who don't need to own their
3258640	3262360	own, you know, GPU cluster that's maybe illegal in the future when the US
3262360	3264560	government is like, oh, you can only have this many petaflops.
3265520	3269600	Basically, yeah, that was the real motivation for, for the, what I call
3269600	3270960	the language game, that project.
3270960	3273160	And that's something that we're continuing to work on.
3273160	3276040	But yeah, that kind of led to this control theory thing where we were just
3276040	3279400	like, yeah, we really need to get a grip on what these look like as systems.
3279680	3283960	As we start to build these more and more complicated, you know, network
3283960	3287240	distributed, you know, beautiful emergent systems that hopefully will be able
3287240	3289040	to be hypercapable in the future.
3289320	3290840	Yeah, this is all music to my ears.
3290880	3294840	I'm a huge fan of the externalist thought in cognitive science.
3295200	3299200	And even though I, I love the work from Jeff Hawkins, you were talking about
3299200	3305160	the neocortex, but even then, you know, I would kind of say that it's a lot
3305160	3308920	of the cognition happens outside of the brain, you know, we're not islands.
3309200	3311880	And actually, I was just thinking maybe a better analogy rather than the interstate
3311880	3315800	freeway might be, you know, in Star Trek Voyager, there was the wormhole
3315800	3318440	network and the Borg fan, the secret work.
3318440	3321040	And you could kind of like, you know, get into these little slip streams
3321040	3322880	and go to different parts of the universe.
3323160	3326960	But when I was interviewing Philip Ball, he wrote this book, How Life Works.
3327240	3330480	And he was trying to understand, you know, what are the mechanisms like, you know,
3330480	3333440	self-organization and multi-scale information sharing and, you know,
3333480	3334360	emergentism.
3334800	3338680	And it's, it's really, really, um, uh, fascinating.
3338960	3343880	So how can we introduce some of these concepts into the next generation of AI?
3344160	3347360	Yeah, this is one of the things I'm certainly most excited, excited about
3347680	3354160	because I see life as this kind of interconnected, interplay, multi-scale
3354880	3358240	process of exploitation and exploration.
3358520	3360880	And these are two terms from the reinforcement learning literature.
3361120	3364680	But I mean this in a much more general sense because at each stage of life, we're
3364680	3369800	either going out into the world to get something, to do something, to try something
3369800	3375360	new, and then at the next stage, we're coming back in, going home, uh, you know,
3375400	3378280	reflecting, uh, going over our insights.
3378960	3383800	And it's, it's this process, this ebb and flow, going out, coming back in.
3384240	3389480	And I see this kind of pattern emerge across many different aspects of machine
3389480	3393200	learning and artificial intelligence work in the sense that a lot of our
3393440	3397000	algorithms that we have now are convergent, they're objective driven.
3397360	3399200	We establish a loss function.
3399200	3402120	We say, these are the rules it should follow.
3402320	3404600	It's going to update according to this equation.
3404920	3408920	And we set the system running, learns from data, and we have a final product.
3409400	3414000	And on the flip side, there's, you know, like what Ken Stanley works with.
3414320	3419160	Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms.
3419520	3421640	And this is, this is the other side of things.
3421640	3426240	And I think some of the most interesting work to be done is how these
3426240	3433000	two sides interconnects, how can we lay down rules, strict rigid rules, which
3433040	3438120	when they're followed can generate novelty, can generate creativity, can
3438120	3443200	generate organization in a way which is not predetermined, but almost fractal
3443200	3445040	and infinite in its complexity.
3445640	3448760	And are those rules defined already?
3448760	3450000	Do they exist in the world?
3450240	3451360	Are we guided by them?
3451800	3454240	Are there principles like that which exist that we can come to?
3455040	3459080	Or is it, you know, are we kind of, you know, the authors of our own
3459080	3459920	fates in a sense?
3459920	3461680	Are we each agents and actions?
3462040	3463680	Uh, we get to choose our path in life.
3464240	3467320	I think these, these are the directions I'm really interested in.
3467520	3471000	And to connect this to my research, one thing I'm focused on now for my thesis
3471000	3474680	project, um, is looking at morphogenesis.
3474800	3478400	So this connects to the more defensive paper as well, except what I'm
3478400	3480480	really interested in is how does structure emerge?
3480840	3483480	How do different cells actually connect together?
3483920	3488560	So, um, in that paper, for instance, each of the cells were on a fixed grid,
3488600	3492680	but in our bodies, uh, there's actually quite a sophisticated protein
3492680	3496200	expression network which governs how cells adhere together.
3496600	3501560	Um, certain gene regulation pathways will turn on cat herons, which will
3501560	3503760	cause cells to attach together.
3503760	3507720	And then in other parts, um, these cells can unattach and then be
3507720	3510080	transported all around the embryo.
3510480	3514440	And I think understanding this process more deeply, not only could shed
3514440	3518760	lights on structure formation and problems in biology in general, but
3518760	3523440	maybe more deeper general problems of structure learning, because we might
3523440	3527200	think of embryology as quite disconnected from machine intelligence or
3527200	3532160	artificial intelligence, but every single brain is formed in the same way.
3532560	3534400	And that's through developments.
3535000	3535280	Yeah.
3535320	3537640	Um, I'm also a disciple of Kenneth Stanley.
3537640	3539320	He's, he's absolutely incredible.
3539320	3540880	Everyone at home needs to read his book.
3540880	3542120	My greatness cannot be planned.
3542560	3543920	Um, yeah.
3543920	3549040	You know, so in, in the natural world, we have, um, it, it's so interesting.
3549040	3552960	So we have this kind of like self-organization and then we have multi-scale
3552960	3558080	information sharing, but we also have canalization, which is that, um, you
3558080	3562160	actually see a kind of, um, convergence of, of structure and forms, you know,
3562160	3565720	which is reused, you know, almost as, as modules, um, in the system.
3565960	3569240	But then there's always the question of how do we create something like this?
3569240	3571240	Because is it simply a matter of complexity?
3571240	3574880	Do you need to have a microscopic scale to reproduce this?
3575080	3576400	Or could we reproduce it?
3576640	3580880	And then if we did reproduce it, the catch 22 situation is that, you know,
3580920	3585680	when you impute directedness onto a system, it loses its intelligence.
3585680	3588440	Cause to me intelligence is divergence.
3588440	3593560	It's exactly as you were saying, it's this tapestry of, um, discovering
3593560	3596320	problems, solutions, new problems, solutions.
3596320	3597600	And it goes on and there's no end.
3597600	3598560	It goes on forever.
3598920	3603200	And any attempt by us to control it with, I mean, it's a bit like the
3603200	3607080	bitter lesson, you know, Sutton said, any human design, any attempt to
3607080	3611560	steer it makes it convergent, but then we could do something like the game
3611560	3616600	of life from John Conway and incredible, beautiful structure emerges from that.
3616640	3623000	But whenever we try to steer it with our own will, it seems to corrupt it as well.
3624440	3624840	Yeah.
3625040	3628520	I think that the analogy to biology is really useful here and the
3628520	3631760	canonization that you mentioned, you know, you have this reuse of structures
3631760	3635840	across, you know, cells, for instance, they all have this similar machinery to
3635880	3639080	do gene expression and they have the same genetic code underlying that
3639080	3642160	gene expression with, you know, maybe differences in cell state, but at the
3642160	3643720	end of the day, it's the same machinery, right?
3644000	3646800	And, you know, I used to do a bit of protein engineering with language
3646800	3649560	models, and that's how actually how I learned about, uh, transformers and
3649560	3650760	built my first transformers.
3650760	3654840	And I think that the analogy is really strong where, you know, cells sort of
3654840	3658760	know how to read this genetic code, this language of the genetic code.
3658960	3662640	And they all use that ability, this canalized ability that's distributed
3662640	3666640	across all of them to locally they solve this problem of, okay, what is this
3666640	3667960	specific cell supposed to do?
3667960	3671800	What should it do to basically support the overall function of the organism?
3671800	3672120	Right.
3672400	3676280	And similarly, I think the hope with these language models is that now we have
3676280	3681480	these language based models or LLMs that have this similar sort of
3681520	3683440	understanding of language.
3683480	3687120	They are able to really constrain the probability distribution, understand
3687240	3690480	which sequences of text are reasonable English and, you know, what they might
3690600	3691440	want to generate.
3691680	3695440	And the exciting thing to me is that we can kind of do a similar sort of
3695440	3699360	evolutionary search that we used to do with, or that we currently do with, uh,
3699400	3702560	trying to find protein sequences, uh, when we're doing protein engineering with
3702560	3707200	the language models, where every computer in this network of systems has this
3707200	3711760	canalized ability to understand language, if you will, and is locally, it just needs
3711760	3715520	to solve this problem of what should this particular node do to support the
3715520	3716280	function of the system?
3716280	3719120	And that might be to explore, that might be to exploit, that might be to do any
3719120	3720680	sort of, any number of things.
3720960	3724440	And the discovery of that, I think, is really helped by the fact that we do
3724440	3728160	have strong language models that are able to really predict English or text
3728160	3732040	very well, uh, because they're able to explore this space.
3732040	3735640	And basically in the limit, you know, there's this good regulator theorem that
3735640	3740320	we had talked about before that says that any system that is a X that does
3740360	3744040	optimal control over another system must necessarily model that system.
3744360	3748280	Uh, and so if you think about in the limit, it seems like the best prompt
3748320	3750840	optimizers may end up being language models.
3750840	3754280	And already in our study, we were using this GCG algorithm that leverages a
3754280	3757800	language model to compute these gradients and try to figure out how we should
3757800	3760280	do this local stochastic search over prompts.
3760520	3764240	And so what I basically, I'm trying to get at is that there are actually a lot
3764240	3768000	of really interesting similarities, I think, that can be drawn upon from what
3768000	3771840	we know about the structure and the function of biological systems where,
3772000	3775400	you know, if we could crack this problem of there's this local control
3775400	3778960	objective or maybe information processing objective that must be met by every
3778960	3779520	cell, right?
3779520	3783480	Every compute node in this network of language models, if we could understand
3783480	3787160	what that is, what that even means from the perspective of systems and control
3787160	3791200	and, you know, computation and like, I think that that's a really promising
3791200	3795080	way that we can make progress on this dream of like, to me, it seems like it
3795080	3798960	would be great to have GPT seven, not just owned by one entity, but maybe
3798960	3802200	operated by the world where we could all have a say in what goes into it and
3802200	3805560	how it's used and what it should be, you know, doing and can all benefit
3805560	3809160	from its excellent ability to compute and predict what will happen next and
3809160	3812440	basically perform intelligent, you know, operations on data.
3812720	3816440	So yeah, I think this is a really, really exciting area to be working on.
3816680	3817160	Amazing.
3817520	3820200	We're nearly at time, but we'll do two quick five questions.
3820200	3824480	So you've both just started the Society for the Pursuit of AGI.
3824520	3824840	Yes.
3824840	3825640	Can you tell us about that?
3825880	3826640	Absolutely.
3826920	3831080	So the Society for the Pursuit of AGI is a student organization.
3831080	3834320	Currently we're operating at the University of Toronto and at Caltech.
3834840	3838920	And we're essentially a crucible for new ideas.
3839880	3844400	If you think of university research labs as pursuing relatively safe
3844480	3849080	bets that could be publishable, industry research labs, relatively safe
3849080	3852880	bets that maybe might turn a profit one day in some new product or system.
3853600	3859600	The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for
3859600	3863440	the real innovative stuff that's way outside the, you know, to use the
3863440	3867160	analogy of the highway network, we're trying to go off the beaten path.
3867680	3873480	And we really believe that the bottleneck in AI progress right now is not so
3873480	3876600	much compute, not so much algorithms, but it's conceptual.
3876920	3881720	We need better ideas about intelligence, about life, about what this whole thing
3881720	3885160	is that we're all experiencing and how we can gain deeper insights of it.
3885600	3889280	Not only do I think that a deeper understanding will help us to create
3889280	3892840	better systems, but it'll also give us confidence that the systems we're
3892840	3896520	developing will be beneficial to humanity and not harmful.
3896960	3900600	And I think that will only come with knowledge, with first principles,
3900840	3901520	understanding.
3902000	3905240	And so that's why one of the things we're trying to do is have our
3905240	3906680	club very interdisciplinary.
3907160	3912000	I think having machine learning be some, this kind of echo chamber amongst
3912080	3915800	engineers, computer scientists, maybe a dash of, you know, philosophy and
3915800	3919680	neuroscience, it'd be really nice to open the conversation to people in other
3919680	3923520	fields who maybe have a really unique insights into the phenomenon of
3923520	3927480	intelligence, perhaps behavioral economics can offer some insights.
3927720	3929360	Political science, right?
3929360	3934680	These are fields that are currently underappreciated, but may have useful ideas.
3935160	3940080	And maybe even people in the arts who, you know, creates, maybe they don't
3940080	3943480	design systems as much as they re-represent things that we know and
3943480	3947080	understand, they could have an interesting voice as well.
3947720	3948240	Very cool.
3948280	3949280	And final question.
3949280	3951520	I mean, first of all, I just wanted to say to both of you, thank you for
3951520	3952600	doing this great work.
3952640	3955600	So your paper is one of the most interesting that I've seen in the
3955600	3957360	LLM space in recent history.
3957360	3961800	And it was shared and loved by many of the folks on our Discord server.
3962240	3968800	But that does bring me to another point, which is that you didn't get into
3968800	3976120	ICLR and from my perspective, I'm, I'm shocked because this is really, really
3976120	3976520	interesting.
3976520	3981240	It has great utility from a practical and a theoretical perspective.
3982480	3985240	Feel free to have a, you know, a good bitch about reviewer number two.
3985840	3987720	No, I mean, I wish you'd just keep talking like that.
3987720	3990400	It really soothes the burn of reviewer number two, you know?
3990840	3994240	But no, I think that, um, yeah, the review system, to be honest, I'm
3994240	3995400	still trying to get my head around it.
3995400	3999280	I'm sort of an early career, you know, researcher, uh, trying to learn how it
3999280	3999680	works.
3999680	4005160	I mean, definitely the, the review process in, for ICLR, in their defense, you
4005160	4010480	know, we had this bug with the submission, uh, submission of our rebuttals basically.
4010480	4013760	So we had submitted the revision to our paper and then 15 minutes before the
4013760	4016120	deadline, Cameron and I were both getting this timed out error.
4016400	4017280	Uh, he was in Toronto.
4017280	4022000	I was in, uh, California and so, you know, they didn't end up actually reading
4022000	4024800	our rebuttals because we had sent it in and they were like, Oh, we'll post it for
4024800	4027720	you and then they were like, Oh, it was posted late, so can't read that.
4028040	4032120	Um, so yeah, I think that the review process definitely has given us a lot of,
4032160	4035320	you know, really useful insights where, you know, the second two results actually
4035320	4039000	that we talked about, the top 75 controllability and the random controllability.
4039160	4042760	Both of those were like from trying to address these reviewer comments, right?
4043000	4047080	So I think that what I'm trying to do at least is take as much of the good
4047080	4050120	parts of that, you know, trying to figure out how we can take advantage of this
4050120	4053200	process where we actually get insight from people in the field, what they're
4053200	4056000	looking for, what they think is interesting, what they think would improve
4056000	4059280	the, the work and try to, uh, try to use that.
4059520	4062840	And overall, just trying to figure out how to navigate this peer review system.
4063080	4065560	I think it definitely made it feel better as well that the Mamba paper was
4065560	4069120	also rejected from ICLR, which, uh, you know, sorry.
4069720	4072760	I know, yeah, yeah, it was crazy to me as well.
4072800	4075720	But, uh, yeah, definitely, uh, it's a, it's a challenge.
4075720	4078760	And, you know, after staying up for 40 hours to get this done, it was like,
4078760	4081160	Oh, would be a, it would have been nice if they could have looked at our
4081160	4083720	paper at least, you know, just see, you know, the work that we did.
4083720	4087400	But yeah, it's, uh, it's definitely good to learn from these things.
4087400	4089960	And I guess we've learned the lesson as well, not to submit in the last 15
4089960	4092600	minutes and to, you know, do it in, uh, in advance.
4092600	4095200	But yeah, thank you so much for your kind words about the paper.
4095200	4095840	That means a lot.
4096120	4099360	And yeah, we'll surely continue to make this better and a lot of exciting
4099360	4102400	plans for how we're going to continue to try to, you know, merge together
4102400	4106280	these two, you know, empirical and theoretical sides of the equation to make
4106280	4110720	some really, hopefully impactful work that can really help people build systems
4110760	4114160	and, you know, make better systems and not be suffering so much under the load
4114160	4115080	of prompt engineering.
4115320	4116640	So yeah, thank you very much.
4116720	4117040	Amazing.
4117040	4119560	Well, guys, it's been a pleasure and an honor to have you on the show.
4119560	4120880	So just keep doing the great work.
4121200	4121760	Absolutely.
4122040	4123520	Hopefully we'll get you on again.
4123640	4123960	Yeah.
4123960	4126920	Thank you so much for, I mean, for the opportunity to come and talk.
4126920	4128800	It's, it's been an amazing opportunity.
4128800	4132200	It's, it's really unbelievable to be sitting here in front of these cameras
4132440	4136520	after watching the show so many times, listening to so many of the podcasts.
4136520	4138920	And now to be speaking, it's just unbelievable.
4138960	4139520	So thank you.
4139800	4140240	Amazing.
4140280	4141000	Thanks so much, guys.
4141760	4142040	Awesome.
4142040	4142360	Okay.
4142400	4142880	It's a wrap.
