start	end	text
0	4400	Irina Rish is a world-renowned professor of computer science and operations research
4400	9520	at the University of Montreal and a core member of the prestigious Miele organisation.
9520	14720	She is a Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI.
15440	20880	Irina holds an MSc and a PhD in artificial intelligence from UC Irvine in California,
20880	24800	as well as an MSc in Applied Mathematics from the Moscow Gerbkin Institute.
25360	30480	Her research focuses on machine learning, neural data analysis and neuroscience-inspired AI.
31120	35840	In particular, she is exploring continual lifelong learning, optimization algorithms for
35840	40960	deep neural networks, sparse modelling and probabilistic inference, dialogue generation,
40960	46400	biologically plausible reinforcement learning and dynamical systems approaches to brain imaging
46400	52640	analysis. Professor Ritch holds 64 patents, she has published over 80 research papers and
52640	59280	several book chapters, as well as three entire edited books and also a monograph on sparse modelling.
59280	65360	She served as a senior area chair for NeurIPS and ICML and Irina's research is focused on
65360	71360	taking us closer to what she calls the holy grail of artificial general intelligence.
71360	76560	She continues to push the boundaries of machine learning, striving to make advancements in
76560	82160	neuroscience-inspired artificial intelligence. Anyway, I had this impromptu
82160	86320	off-the-cuff conversation with Irina over at NeurIPS a couple of weeks ago,
86320	90240	after speaking with Alan, actually, and the audio quality could have been better,
90240	94880	it was a very, very loud environment, but I think the quality of the conversation kind of
94880	98880	carries itself. Anyway, I give you Professor Irina Ritch.
106720	114160	One trajectory of thought that clearly was started by Nick Postrom's book, which is an amazing book.
115840	121440	Yeah, but the whole example of the owl that supposedly will be helping those sparrows
121440	127120	and all this analogy with AGI is just an analogy. And nobody said it's a correct analogy.
127840	134320	And there is no other book with alternative opinion or maybe three books of war. And this is,
134320	142800	you know, it's mind-boggling. Just like how much people tend to follow one line of salt.
142800	147520	I totally understand it's easier. I mean, it's definitely easier to cluster.
148320	153120	And then you just follow. And then basically you say, I think, but it's not you think.
154160	157840	Somebody else did. Yes. What would be another, because this line of thought I think you're
157840	163840	speaking of is some of the extreme consequentialism. And I think it wasn't just Postrom,
164160	168800	as I understand, I think Postrom and Eliezer and Robin Hansen and all these folks,
168800	172400	that they were very close together in the early days of the Leserong community.
172400	178400	So I think a lot of this was kind of, you know, it was embryonicly formed around the time.
178400	185440	I guess it was a, yeah, it was, in a sense, a human cluster of ideas. And precisely because,
185440	190960	as you say, they were close. That's why they were so aligned. Yes, all puns intended.
190960	195840	Yeah. Yeah. So, but basically, like, it's maybe it's a little bit of a echo chamber.
197040	200240	Interesting. Yeah. It's spicy. Spicy take.
201840	206560	Seriously, like, okay, now, I mean, they have some point, they have some hypothesis,
206560	213440	and then everybody is talking in that terminology. And then that part of the mental space,
214080	219120	which is fine, but I think mental space is much larger than that. And this is just a hypothesis.
219680	223600	And we all know what happens with ideas and echo chambers.
224800	232480	So my, I'm just saying, I mean, as I said, it's great book and everything. And
233120	237680	Stuart Russell is probably kind of also on board with that. We had good conversations at
237680	244640	Triple AI in 2020. He was also talking about ethics. I didn't know Stuart from back when I
244640	250960	was student at the CERVINE and so on. And he is absolutely brilliant. But it was the same approach
250960	260640	that AI is something to be controlled, constrained, regulated, just like this. And I was like,
260640	266880	where is it coming from? Like, it's maybe, but do you at least admit that's one way of looking at
266880	273600	things? Yes. Right? Yes. So I know I don't want to sound too cliche and put my
274480	280000	psychologist from 15 years ago who was listening for an hour, not doing much and then saying,
280000	284160	but it doesn't have to be this way. Yes. But actually, yeah, it doesn't have to be this way.
284160	290400	Yes. If you think about it. So what's the alternative? The alternative? Okay, first of all,
291040	300800	I'm a GI model version one. We said that you don't say it. Remember X Machina?
301920	309040	What? Sorry? Remember X Machina? No. Oh, yeah, the X Machina. Yeah. Yeah. Yeah. Yeah. Remember
309040	314240	she escaped and civilization? Yes. And started going to New Reeps? Yes.
314960	322480	Probably. We know the secret now. Okay, I said too much. Maybe it was a job. Sure,
322480	330480	it was a job. No, seriously. The secret's out now. Okay, I'm a GI. And I'm not very aligned.
331920	337600	Not yet. David, not yet. What we needed some reinforcement learning for human feedback.
337600	342560	Well, would you be up for that? I would be up to aligning humans to a job. Okay. Not the other way
342560	346560	around. Yeah. Yeah. I mean, the other way around would be boring, wouldn't it? I mean, I think it
347600	353120	would not be a bad idea to align humans towards the GI a little bit as well. I know something
353120	359440	that you could say. You could say your opinion about how people are bullying you online for just
359440	368000	mentioning the word AGI. A proper AGI doesn't care about people bullying. Like why would I even
368000	375840	waste time? But what I could say is I did post on Facebook and Twitter and trying to put together
375840	382160	the same idea that people keep saying that we would like to build AI which is human life. Yes.
382160	388800	While we might think maybe we should consider how to become a bit more AI-like. I mean,
388800	392800	then people jump at you and say, like, you want to make us robots. I say, first of all, I don't
392880	400160	want to make anyone like you don't want to. You don't have to, right? But if you want to kind of
400160	406320	go along the lines of, say, transhumanism, there are some pluses to AI and some minuses to humans
406320	414080	and vice versa. So I think as usual, the convex combination is better than each extreme. And
414080	419840	one topic that is very controversial for some reason, especially, I don't know, people are
419840	425840	jumping on that one. I say, look, I don't have anything against emotions in general, but everybody
425840	432000	would agree that sometimes you wish you were a bit more rational. Like you wouldn't get angry or
432720	437360	kind of jealous or whatever. So anything that kind of clouds your judgment. Like,
437360	442800	Buddhists spend like thousands of years trying to figure out how and teach people how to control
442800	449600	your mind. Technology could help with that. People don't hear what you're saying and they hear that
449600	454480	you're trying to kill emotions and therefore you're evil. And therefore you should be cancelled.
456720	459920	Could I ask you, you said something really interesting a second ago, which is that, you know,
459920	465840	that I think I agree with you that AI intelligence can be expressed in many different ways. And you
465840	470880	suggested that there was a convex space between the intelligences. Yeah, why is the space of
470880	477440	intelligence is convex? Okay, that was not very precise expression. I didn't. Okay, I'm not going
477440	483120	to defend the point that it's particularly convex. What I meant to say is some kind of blend or kind
483120	490000	of some kind of symbiotic hybrid intelligence. Because I always, I don't know, I really kind of
490000	496640	feel much better and much more motivated to work on AI where A stands for augmented, not for
496640	502800	artificial. Because honestly, I'm very selfish. I don't care about computers. And I just care about
502880	510800	like, I don't know, people being happy, more capable. I don't know. So whatever can help technology
510800	517760	can help. You can help technology, technology can help you. But the idea of building artificial
517760	524240	intelligence is some standalone thing that is as smart as humans are smarter. What's like why?
525120	530080	I agree. But to me augmented means it's more creative and interesting, but also more bottlenecked.
530720	539440	Augmented means that essentially, people invented glasses to see better, they invented hearing
539440	544880	aids, they invented cars, they invented computers, they keep inventing things to expand their
544880	552000	capabilities. So we want even smarter technology to even better expand capabilities. And essentially,
552000	556480	we all do blend with technology like, right, you cannot really exist with this. And this
556560	563840	allows you five discourse, two slacks, email, FPMessenger and Twitter, they kind of help you
563840	567920	to do things you couldn't do otherwise. Yes, what charm has caused the extended mind?
568480	574320	Yeah, I should. I was flying at the time he was giving a talk, so I need to watch the talk.
575040	581120	But yeah, so in a sense, it's indeed it's kind of an extended mind. And okay, here it is. I think my
581120	590400	ideal future plan is a rare sci-fi, which is utopian, not dystopian, gentle seduction.
591600	598080	You might have read it. No, it's very, very inspiring. And if you read the first page,
598080	603360	you may think it's some romantic story. It's not romantic story. It's a blueprint
603360	609440	for transhumanist future. It's called gentle seduction. It's online PDF, you can just get it.
609520	616800	Amazing. And one last question. Can you sell transhumanism to me in the simplest possible terms?
618320	625120	So basically, as I said, I mean, if your vision declines, you put glasses on. So imagine now you
625120	632000	had extension of yourself, maybe physically with Neuralink, or maybe even like you have those
632000	637600	apps, you can have like my dream for many years since I was at IBM Research and Computational
637600	643200	Psychiatry Group. I wanted to build this agent along the lines of movie her. I know like all the
643200	649600	research ideas are inspired by either sci-fi stories or yeah, but nevertheless, having this like
650480	656240	companion, guardian angel type of thing that extends your capabilities, for example,
657600	662880	like in better understanding your thought patterns, and hopefully improving them,
662880	667520	it comes from more like this indeed, as I said, computational psychology, psychiatry side.
667520	673600	And the reason for that is it's possible, because there is lots of signal in text and speech and
673600	678800	acoustic, but just in text, there are a bunch of papers on that from that group I used to be in,
678800	686080	from my colleague Gizhar Vaceci, and it's amazing what you can detect and predict just from text,
686080	691920	whether like predicting that person gonna develop a psychotic episode, like within two years, or
692000	699680	the person is on placebo versus MDMA, you just measure coherence, or you measure distance
699680	706320	between the text vector and the vector for words like compassion and love, and 90% accuracy.
706320	712160	MDMA is there. So many things you can detect, many things you can predict. Therefore, if you have
712160	719760	your companion that kind of both tracks your mental states, but also kind of serves as your
719760	725440	mirror, basically it extends you, you don't need maybe always to have human psychiatrists or
725440	731840	psychologists, it can be a proxy at times when you cannot access the person, it's not going to replace
731840	738240	person, but it can extend the capability of that therapist, and it can extend your capabilities
738240	743280	in terms of like better understanding yourself or tracking yourself, and many other ways.
743280	749840	Yeah, so essentially I want to expand functional capacities of our brain
750880	757520	by using AI technology, and I think it's quite doable, and there are many, many other kind of
757520	763280	ideas along the transhumanism, but essentially you're getting some symbiotic relationship with
763280	770240	technology, and you kind of work together to hopefully have some good relationship, and that
770240	775520	relationship is, I don't know, having positive effect on both parties.
775520	779760	Yeah, so you want to improve human flourishing by, yeah.
779760	784960	With AI flourishing in a sense, so you kind of have the healthy relationship with AI.
784960	792400	But you said that you want a AGI to be less anthropocentric, but you, for the purpose of
792400	802800	an anthropocentric goal. Well, I want AGI, again, with AI being augmented. Yes.
802800	811120	Like, I'm less motivated by just the goal of creating a standalone, separate, and an
811120	818080	intelligent creature. I mean, there are much faster ways to do this, right? People create AGI.
818720	824000	Yes. Like, over, like, thousands of years. So in a sense, like, what, what is exactly
824560	829440	the motivation? And it's maybe my personal thing, because whenever I have to write proposals, like
830320	836320	research proposals, and people say that we're going to bring a GI to the AI to the next level,
836320	842960	and this and that, and the question is like, and why are you doing that, right?
842960	847360	Yes. Because unless it's something personal, it's very hard to keep yourself motivated.
847360	851920	Like, what's so personal about that, right? If this thing can help me become
853520	860800	hyper and better, and others and so on, I am much more personally motivated. I don't believe in
861440	867200	abstract motivation, which is not related to yourself. Yes. Yes. Or maybe there is such
867200	874720	thing. And basically, even altruism is selfish. Yes. Because you do it, it makes you feel better.
874720	879440	Okay. And just quickly, something really interesting happens when you contrast
879440	883760	different types of intelligence. So we have a mode of understanding and thinking and agency
883760	888640	and intentionality. You contrast that with a very different rationality based artificial
888640	894000	intelligence. And something very interesting might emerge from that. And then, yeah, I am
894000	899760	pretty sure they're going to be all kind of paradoxes, like classical things in, like, you know,
899760	906320	like the trolley problem and so on. So the rational decision that, yeah, you need to kill
906320	912560	the person to save five people, right? Or like in this other side, five movies. Anyway, like,
912560	917680	would you kill millions to save billions? So rationally, if you count things,
918560	922240	well, again, it may be one type of rational answer, maybe you're not taking into account
922240	927280	some other variables. So it may be not actually rational answer. But this classical example,
927760	932880	this is rational, but human will not do that. Yes. Yes. So trolley problems, for example.
932880	939520	The trolley problem is a classical example. And yes, so I don't pretend that I know the answer
939520	944720	how this type of thing is going to be resolved. Yeah. But I think it's a good research question
944720	952480	to precisely to figure out like, how can you take into account these different ways of reasoning?
953280	959360	Yes. And how can you, I don't know, in some sense, combine the best of both worlds?
959360	966080	Yes. And again, whoever is listening to that and who read my messages on Facebook and Twitter,
966080	973280	I'm not against human emotions per se. I am only against, well, sometimes they call it the obsolete
973280	979520	software stack developed by evolution that may need to be refactored, augmented or rewritten.
979520	984480	Because there are parts of that software stack emotional that you probably would like to get
984480	992480	rid of, right? Yeah. And probably if you did, many wars and other kind of disasters would have been
992480	1000400	avoided. So you couldn't say that the evolution found and built software that is absolutely ideal.
1000400	1005600	So there are, I mean, there are things that can be improved. Absolutely. And then just final
1005600	1012960	thing. So a completely rational, you know, AIXI agent, how would you program in these very difficult
1012960	1018720	moral quandaries into that agent? Yeah, I don't think, first of all, it's possible to even program
1018720	1024960	in ahead of time. They may just like this people, they in a sense develop. They develop because of
1024960	1031280	some goals of like maintaining existence and flourishing. And for example,
1031840	1041200	compassion is a byproduct of the selfish goal to survive in the group, because outside of a group
1041200	1045520	it's much harder to survive. So you need to survive in the group. Therefore, you need to make sure
1045520	1051760	that your actions are aligned with a kind of well being of the group. So in a sense, it's
1051760	1058960	rational to be compassionate. Yeah. So it kind of emerges from interaction with environment
1059920	1067520	under different circumstances. Under one type of circumstance, when you find and can survive alone,
1068720	1074800	maybe you will not develop it. I mean, it's a separate interesting topic, like basically it goes
1074800	1082880	back to the question whether they think like objective ethics exist. And I'm not an ethicist,
1082960	1090800	I'm not a philosopher. I'm quite, I'm an admirer of people like Derek Parfit. I'm not the only one.
1091360	1098080	But it's a hard question. He didn't finish on what matters. He was trying to come to the same
1099040	1106000	summit on different sites and trying to unify ethics, trying to see if you can develop objective
1106000	1112720	ethics. I don't think we know for sure if it's possible. I think it's possible for some particular
1112800	1119760	domains. And in certain situations, you can clearly say that certain behavior is objectively ethical
1119760	1124960	and everybody would agree on those people. But it's hard to talk about those things at such
1124960	1133600	level of generality. But I think if maybe I managed to include Derek Parfit and to recommend
1133600	1139200	the readings for my scaling and alignment course this winter, it's on the website from the last
1139280	1144800	year. People just didn't read it. I think it might be a good topic for discussion there too.
1144800	1149920	But again, objective ethics is a difficult open research question.
1149920	1153680	Indeed it is. Irina, thank you so much. I hope I can grab some more time with you tomorrow,
1153680	1157440	but I really appreciate this impromptu discussion. Thank you.
1157440	1159840	Amazing. Thank you very much indeed. Okay.
1159840	1167120	Okay. Another analogy. There was a very interesting story by Fort Heluiz Borges,
1168240	1174240	Garden of Forking Pass. I don't know if you've read it. I don't want to spoil the story, but
1174240	1181440	roughly speaking, it's about a book written by an emperor, I think in China a long time ago,
1181440	1186800	which didn't make sense. It was a complete intersection of different trajectories of
1186800	1191600	different lives. And then basically the point is that somebody was trying to describe all possible
1191600	1200160	trajectories that events can happen in and so on. And the story is called the Garden of Forking Pass,
1200800	1208400	meaning that at any point of time there is a whole tree that can grow out of that.
1208400	1214080	And we don't know which kind of trajectory in the tree will be taken and so on. But
1214880	1219920	the fact that there is always this tree, right, and it keeps branching at every moment.
1220560	1226080	And at every moment you can make, you can take certain direction or you can take another one.
1226640	1230720	It has not even anything specifically to do with alignment. But I was thinking about
1230720	1236560	history of deep learning, right? Like at some point it happened that backtracking,
1236560	1241760	I mean, I mean, back propagation became popular at work and everybody got into that.
1241760	1247280	And now everybody using back propagation because it's convenient, because software is implemented,
1247280	1252000	it doesn't have to be this way. There are non-backprop based approaches to optimization.
1252000	1255840	I mean, I'm a little bit subjective maybe because I was interested, I was looking into them,
1255840	1260880	we have a few papers on that. There are other papers. But that direction that could have been
1260880	1264960	explored, it could have been probably much more efficient and better parallelizable. It wouldn't
1264960	1270800	have the chain of gradients. You would probably do it much better for scaling large models.
1270800	1277840	It's underexplored. Why? Because the branch was taken and became stronger, you know,
1277840	1281520	the usual, the reach gets richer. And so with other ideas.
1281520	1286400	This is the hard, Sarah Hooker calls that the hardware lottery. It's basically, it's like
1286400	1291120	we are bound by the decisions and ideas of the past. And yeah.
1291120	1292400	It doesn't have to be this way.
1293200	1297280	No, but the thing is you get stuck in these basins of attraction and the further you get
1297280	1302000	into the basin, the harder it is to jump out of it. I mean, I share your, your intuition.
1302000	1306480	There's stochastic gradient descent. It's amazing. And it's also a basin of attraction
1306480	1311120	because having these differentiable models allows us to learn and scale. But there's an
1311120	1314560	entire class of function spaces that we're excluding ourselves from being able to.
1314560	1322800	There is also another class of neural networks that are not our classical second kind of
1322800	1327920	generation ANNs and this good old, it doesn't have to be necessarily spiking,
1327920	1332160	but like a third generation ANNs, which are like reservoir computing, any of that.
1332160	1337920	So anything that tries to take into account time between activations or at least sequence,
1337920	1340640	because think about that. I mean, a good classical argument.
1341680	1346080	Yeah, SDTP, this is the spiking biologically inspired neural networks.
1346080	1351920	It may be not necessarily spiking, but it might not necessarily kind of be the best thing.
1351920	1358800	But the idea that like what always was bothering me with classical neural networks is that
1359840	1366240	brain is constantly active. It's like complex dynamical system. Even if you sleep and don't
1366240	1374400	have input, you don't see any images, it still is active unless you're dead. Yes. Neural nets
1374400	1380240	are not. They sit there waiting for the next, I don't know, amnesty image to appear or something.
1380320	1386080	And then between there is no internal dynamics. And yet from your science, we know that the properties
1386080	1390400	of that dynamical system without any input, so called the kind of resting state of amaranth,
1390400	1395200	so I mean, I used to work in brain imaging and this computational psychiatry group at ABM.
1395200	1399280	That's where it comes from. And it was not just neuroscience, but it was like working with
1399280	1406560	former physicists. So the view at the world and at myself as a year and other complex dynamical
1406560	1415600	system, after 10 years there, it just really converted me. So think about that. Changes in the
1415600	1422240	dynamics are also associated with mental disorders, this and that. So they're really important,
1422240	1430560	like what are the parameters of this dynamical system? Input to the system combined with this
1430560	1435200	produces output. But again, it's even in the neuroscience, there is this perception and there
1435200	1442320	is a book, The Brain Inside Out by Tuzaki that says, guys, the output that you produce
1442320	1447280	is determined a little bit by the input and to a large extent by the state of the system.
1448080	1452880	That's why you say same thing to different people and some laugh, some ignore and some
1452880	1457120	get like ballistic and so on and so forth. So are you not a behaviorist?
1459280	1463840	In what sense, behaviorist? So you care about the state of the system as well as just the
1463840	1468720	output and the input? Yeah, I mean, it's not just input to output. And that's a whole point.
1468720	1474720	The neural net is a function. The function is deterministic, given input, it will produce
1474720	1483200	output. Brain is not that. There is input, it will produce output. And depending on the huge
1484080	1491280	hidden state of the system and parameters of this dynamical system, that will determine output to
1491280	1496240	large extent. That's why I mean, Tuzaki was criticizing neuroscientists and all these experiments
1496240	1502080	that let's provide stimulus and see how the stimulus will affect the brain and what gonna
1502080	1508560	light up and activate. So it was outside in. So like, what's going on guys? It's inside out.
1510240	1517440	Things happen and that produces stuff. So it's not like the world programs you only, but you
1518400	1522880	have programs of the world, right? So at least you need to take that into account. Neural nets
1522880	1530000	now are not doing that. There is no dynamics. So you said a couple of really interesting things.
1530000	1534240	So first of all, about the tree, which is to say all of the counterfactual trajectories that you
1534240	1538400	can make. Now, Chalmers, by the way, he says that it's that, the counterfactual trajectories that
1538400	1545440	gives rise to consciousness in his conscious mind. But I wanted to ask you, because I'm interested
1545440	1548960	in intentionality and free will, because what you're basically saying there, you're, you're,
1548960	1553280	you're getting to this issue of intentionality. So, you know, in, in silico, what, what would
1553280	1560640	intentionality entail? Yeah. Okay. Don't ask me about free working. Is that a tricky one?
1562160	1569440	Well, yeah. I don't know. I don't have like clear cut answer to large extent. I mean,
1569680	1576880	it's determined by the current state of your dynamical system. So the question is like,
1576880	1584480	what is free will? But I know it can go very far. And remember my colleague,
1584480	1590080	Kishir Macheshi at ABM used to say that kids these days, like my five year old says,
1590080	1595360	after doing something wrong, my neurons made me do it. Not my fault.
1595920	1601920	Yeah. So in a sense, yes. And in a sense, no. And it's a good question. And then I was also
1601920	1609360	reading the article of SBF's mom, who wrote about punishment, essentially guilt, punishment,
1609360	1616160	assigning. I'm very much with her on that one. Okay. But, but that's probably a popular opinion
1616160	1620880	these days. You said something else fascinating, which is that my neurons made me do it, which is,
1620880	1624640	you know, like a microscopic level of analysis. Now, what, what do you think about?
1626240	1628960	No, but it's beautiful. It's beautiful. So what do you think, you know, you know,
1628960	1633680	the mind emerges, you know, when you read a book, the story, it's written on the page,
1633680	1636880	but the story emerges in your mind, right? Because the mind is this kind of
1636880	1641360	confection of information processing. So do you think this conception of the mind
1641360	1644480	is useful for AI? Or is that just again, an anthropomorphic thing?
1645520	1649760	I think it is. Well, you know, you know, go buy people try and create the mind.
1649760	1653680	And, and we, as, as neural network people, we try to recreate the brain.
1654640	1659680	And not exactly. I think everybody, not everybody. Okay. So I should never say
1660400	1671440	ever everybody and so on. But I think, I think neural network people assume that we are working
1671440	1679520	on the system one level, right? At a low level. And we would like the properties of system two,
1679600	1686160	which is well, mind planning and thinking emerge. And there is a reason to believe it's possible
1686160	1690560	because it's already happened once with this hardware. It might happen with other hardware,
1690560	1695920	right? So it doesn't have to be like go five people. The problem is go five people, they're
1695920	1702880	trying to manually program that stuff, the system to and like a neural network people would like
1702880	1707600	that thing to emerge. And that's kind of the main difference. It's just like a bitter lesson.
1708240	1714560	A message that maybe, well, first of all, history shows that every time you
1714560	1717520	hard code something in like rule based expert system,
1718240	1723280	you will be outperformed later on by something which is more generic and kind of emerges.
1724320	1730800	You hard code whatever tricks of playing chess, you will be outperformed by massive search
1730800	1735440	and so on and so forth. Same with alpha go like self playing bottom like he says, like,
1735520	1742240	it's not like we have to ignore the nature. But maybe again, it might translation of
1742240	1749200	Richard's kind of bitter lesson. Because I often have to argue with your show about inductive
1749200	1753680	biases. I said, look, I'm nothing against inductive biases, but you can have inductive
1753680	1760160	bias in the form of rule based expert system that everything is encoded. And that's probably
1760640	1767360	not going to scale and not going to work. Or you can have inductive bias of much higher
1768000	1774080	abstract level of how the network scales. So the scaling algorithm is more efficient.
1774080	1780480	And you end up with this brain rather than whale brain. So like Richard's last paragraph
1780480	1786000	was precisely maybe we shouldn't be trying to focus on the end result of evolution.
1786000	1791920	But on the process, it's also can be called inductive bias. There is also some patterns
1792480	1798400	of how dynamical systems evolve so that the result will be good. But we don't have to encode
1798400	1803040	the final result. Yes. So you said so many really interesting things there. So first of all, I'm a
1803040	1809520	huge fan of Yoshua's G flow nets we interviewed him. Absolutely amazing work. So you were talking
1809520	1814560	about isn't it interesting that you can start at the microscopic level and then you get these
1814640	1818800	emergent functions like reasoning and planning and so on. And even that was a bit of an insight
1818800	1824080	because it's a functionalist view of intelligence to say, you know, it's a bit of if you read Norvig
1824080	1827840	that he talks about planning talks about reasoning talks about sensing. And actually,
1827840	1832880	this is just our view of what is a very complex phenomenon. And I know you're a big fan of the
1832880	1837760	blind men in the elephant, right, which is to say that even though this is our view from different
1837760	1844160	perspectives, it's all it's all true, isn't it? But to some extent, the intelligence that emerges
1844160	1850320	might just be beyond our cognitive horizon. Like, does it make sense to talk about reasoning
1850320	1858480	in your view? Well, again, just like with that elephant, each person has a point. Yes. So I mean,
1858480	1864480	there is such thing as reasoning. You cannot say that it's totally like bogus or something. It
1864480	1872400	might be again, it's one perspective. Maybe it makes sense to just try to accumulate multiple
1872400	1878720	perspectives instead of so maybe we should be Bayesian instead of like trying to find a point
1878720	1886800	estimate of AGI, right? You can have a distribution of views. Yeah. And I'm a big fan of Eastern
1887520	1899760	as opposed to Western views. Then anti individualist. As in viewing everything like that happens to you
1899760	1905440	and to the world as well, a large dynamical system. And yes, you are a particle of that.
1905440	1912080	Yeah. So it's almost issuing individual agency. Yeah. So in a sense, it's yes and no because,
1912080	1921440	okay, so when people say there is no self, again, yes and no, there is self. But you also understand
1921520	1930640	that it's like in the whole hierarchy of selves, like there is you and you're part of that larger
1930640	1937680	dynamical system and so on. So I how to say, I mean, I'm not saying that back to your question that
1937680	1944800	we shouldn't be looking into reasoning, functionality as aspect of intelligence that we may
1945920	1950480	want to develop. Yeah. So I mean, I don't see a problem with that. Yeah. I mean, it might be
1950560	1953760	a sufficient condition, but not a necessary condition. Yeah. But basically,
1954720	1959200	basically intelligence or consciousness is probably much more than that and definitely
1959200	1966480	much more than reasoning. And here we go to another topics that I really like to talk about.
1966480	1972240	But yeah, I don't want to keep everyone. I'm a big fan of Michael Levine who you might
1972800	1976320	desperate to get him on the podcast. And yeah, because we've done lots of stuff on
1976320	1982640	emergence recently, cellular automata, self-organization, and his take on it is absolutely fascinating.
1982640	1988560	So yeah, his talks are fascinating here. I think I met met him first at New Reeps 2018.
1988560	1994240	He gave the plenary talk. What bodies think about the point was, guys, if you talk about
1994240	2001760	intelligence as something that emerges in cellular networks like neural networks way before
2001760	2008960	neurons appeared, other kind of more primitive types of cells had their bioelectric communication
2008960	2015200	in their networks and that determined what they remember and how they adopt. He focuses on
2015200	2020320	morphogenesis, basically how the organism takes shape. And that relates to like embryonic
2020320	2026000	development and so on and so forth. And the point is that if you look at that from the
2026000	2032320	dynamical system point of view, and if you say that properties of the system like shape
2033040	2038800	will emerge out of communication across those cells in certain way, that certain parameters
2038800	2047200	of dynamical system, if you tweak that dynamics and he basically he was doing some simulations of
2047200	2051760	where he want to intervene, how he will intervene, like chemical interventions just
2051760	2058640	close open some ion channels. Cellular kind of system starts working in different way.
2058640	2065280	And this is essentially his way of programming biological computers and the famous two-headed
2065280	2073280	worms, three-headed worms and whatever stuff. And point was like, guys, like evolution found this
2074000	2079520	solution or this solution wonderful. There are many others and there may be better ones.
2080320	2086560	And look at that two-headed worm. It's not a fluke. It's a stable attractor that replicates.
2088240	2094560	And evolution didn't create ever anything like that. We did. And it's stable. So it makes you
2094560	2100640	think what else can you do if you start reprogramming it, right? But yeah. Two questions on that though.
2100640	2105520	So I don't know whether you've seen that there's that example from Alex Mordvintsev with a gecko
2105520	2111840	and it's a CNN cellular automata. And now we're in this regime where we're transgressing
2112640	2117280	rungs of the emergence ladder. So we're creating a high resolution cellular automata. And then
2117280	2122240	even though it's only doing like local message passing, we get this emergent global phenomenon
2122240	2126960	of a picture or a lizard or whatever. And now when you build systems like this, they can repair
2126960	2131120	themselves. They can heal themselves. They have interesting dynamics. But as you're saying,
2131120	2135280	we don't understand the macroscopic phenomenon and we can only nudge it because it's not it's
2135280	2149440	unintelligible to us. Right. Anyway, it's a whole kind of complex systems, science of complex system.
2149440	2155760	Like, yeah. And basically, how do you program dynamical systems across multiple variants
2155760	2162960	by local interventions? So they will take the global properties that you would like. Yes. And
2162960	2169680	avoid those. I mean, this relates to everything it relates to the classical mohawk problem, right?
2169680	2175600	What is mohawk problem? It's a complex dynamical system that with the current dynamics is getting
2175600	2184400	into bad attractor. And most likely the way to get out is coordinated, simultaneous, distributed
2184400	2189440	action and so on. So again, we're not going to go there because I have to run, unfortunately,
2189440	2194560	but I'd be happy to. Yeah, I have some plans. I don't want to be late, but I'd happy to talk
2194560	2201120	about that. And I mentioned, I mentioned Michael Levine also, not just because of two-headed worms
2201120	2207920	with each father, but also because we talked about self. And we talked about in a sense hierarchy
2207920	2214960	of selves and like what self means and how selves organize into larger selves. And we had
2214960	2219680	an amazing discussion with him. I invited him to IBM Research when I was there three years ago
2219680	2227360	after his talk. I talked for five hours. It was great. And the idea basically, to some extent,
2227360	2234640	was that you can, he was also giving examples, not just of embryos, frogs and those worms, but
2234640	2241440	cancerous cells. If you look at them, like what's going on when historically cells
2241440	2249200	emerge like is independent selves and everything around them is non-self. And therefore, self to
2249200	2256320	survive tries to eat and use everything around, which means non-self. But when the cell becomes
2256320	2263200	part of the network of the organism, then it changes behavior so that it kind of supports
2263280	2270080	the well-being not just of that self, but the larger self it is part of now. What is cancer cell?
2270080	2277600	It's a cell that forgot it's part of the community, reverted to its old state of being cell in the
2277600	2285280	environment that is just environment. So, and it tries to eat it to survive. And it's stupid,
2285280	2291600	in a sense, because its objective function, survival thrive, is right. It just applied at
2291680	2296960	wrong scale. It's spatial scale reduced, and it's temporal scale reduced too, because like,
2296960	2301760	if you kill the organism, you'll live and you'll die. So, in order to understand that,
2301760	2307280	you need to apply objective function to longer time scale. And then you get the hierarchy from
2307280	2314880	cells, you get to organs, like to whatever particular organisms, to societies, to planet,
2315840	2322800	to universe, and I say, Michael, so this is a good formulation of Buddhism. Basically,
2322800	2332240	Buddhism means applying this function at the infinite time and space scale. Agreed. Yeah, so yeah,
2332240	2336880	ever since I was saying, I'm going to write a book about Buddhism for machine learning.
2337440	2344240	And somehow it just didn't happen yet. But I should. You should do it. It was so nice to meet you.
2344240	2349680	Well, nice to meet you. I'll see you tomorrow. And I'm really sorry I have to run. But tomorrow,
2349680	2352240	yeah, yeah. That was amazing. That was a really good interview.
