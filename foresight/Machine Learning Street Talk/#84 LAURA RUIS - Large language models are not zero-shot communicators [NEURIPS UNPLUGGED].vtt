WEBVTT

00:00.000 --> 00:04.080
Hello, folks. I was in New Orleans last week and I had the pleasure of interviewing Laura Ruiz,

00:04.080 --> 00:06.160
the primary author on this paper,

00:06.160 --> 00:10.080
large language models are not zero-shot communicators.

00:10.080 --> 00:14.880
Now, this is exploring the ability of language models to perform Implicituk,

00:14.880 --> 00:18.160
which I guess from a machine learning audience point of view,

00:18.160 --> 00:23.440
you might think of as being some kind of extrapolation or even abstractive reasoning.

00:24.080 --> 00:25.920
There's an example of this that we can try.

00:26.880 --> 00:29.680
Esther asked, can you come to my party on Friday?

00:29.680 --> 00:34.480
And Zhuang responded, I've got to work, which means no.

00:35.520 --> 00:39.840
Yeah, part of the reason I wanted to do this quick intro is since this interview,

00:39.840 --> 00:45.200
OpenAI has released a chat GPT, which is pretty impressive, actually.

00:45.200 --> 00:50.320
So we can come in here and we can say something along the lines of

00:51.840 --> 00:54.000
Esther asked, can you come to the party on Friday?

00:54.080 --> 00:55.920
Zhuang responded, I have to work.

00:57.360 --> 01:04.720
Does Zhuang, can Zhuang come to party?

01:09.520 --> 01:12.960
It looks like it has failed. It says it's not possible to say

01:12.960 --> 01:16.480
whether Zhuang can come to the party or not because we don't have enough information.

01:17.840 --> 01:22.080
Zhuang may or may not be able to come to the party depending on his work schedule and other factors.

01:24.560 --> 01:28.400
Yeah, so this is an example of failed implicature.

01:28.400 --> 01:31.360
But anyway, if we come over to Laura's Twitter,

01:32.400 --> 01:36.640
she posted a little thread the other day saying that loads of people have been sending her

01:36.640 --> 01:39.920
implicatures, which they used as examples in the paper.

01:40.560 --> 01:45.200
And apparently chat GPT does understand some of them, which she's very happy about,

01:45.200 --> 01:47.600
but she wanted to write a short thread about it.

01:47.600 --> 01:52.160
So she said before they started writing the paper, she would try lots of implicatures

01:52.160 --> 01:56.880
that she came up with on Da Vinci 2 in different wordings with moderate success.

01:56.880 --> 02:01.120
Some always solved and some half of the time depending on the wording,

02:01.120 --> 02:05.840
meaning random performance since the test is a binary, which is to say a yes or a no.

02:06.720 --> 02:12.320
That's why they decided to do a systematic test to figure out how good it actually was

02:12.320 --> 02:16.000
and how much it depended on the wording of the prompt.

02:16.560 --> 02:20.080
And a few months later, they had the answer that it was okay,

02:20.080 --> 02:21.520
but not close to humans.

02:22.080 --> 02:29.440
And okay means that on Da Vinci 2 and 3, the performance of zero shot implicature is roughly

02:29.440 --> 02:34.880
70%. Most of the other models fail, even with few shot in context prompting.

02:35.760 --> 02:41.280
So anyway, she said that she gets the people are excited that chat GPT is doing pragmatic

02:41.280 --> 02:44.960
inferences, but she felt the same with Da Vinci 2.

02:44.960 --> 02:46.800
It's all anecdotal, she says.

02:46.800 --> 02:51.520
But a more systematic test shows a significant gap with humans nonetheless,

02:51.520 --> 02:56.480
and it's the same for Da Vinci 3 and presumably the same for chat GPT.

02:56.480 --> 03:01.040
She says that once this implicature dataset gets solved, and she has no doubt that it will get

03:01.040 --> 03:05.200
solved relatively soon, since fine tuning with human feedback helps a lot,

03:05.200 --> 03:08.960
they might have some baseline pragmatics in their models.

03:08.960 --> 03:11.200
And that's when it will get really exciting.

03:11.200 --> 03:15.200
She says that she's personally blown away by chat GPT's capabilities.

03:15.200 --> 03:20.880
It's absolutely incredible at explaining things, compositional generalization of concepts,

03:20.880 --> 03:22.240
simulating a VM.

03:22.880 --> 03:28.240
I'm not sure what VM means, coherence, creativity, writing essays, poems and more.

03:29.200 --> 03:34.480
She said that the pragmatic language that they studied is part of a type of casual language

03:34.480 --> 03:38.320
that we're using conversation that might emerge from social interactions.

03:38.320 --> 03:42.320
She's personally thinking about why human feedback helps so much,

03:42.320 --> 03:46.720
and whether interactivity and social pressures might help even more.

03:46.720 --> 03:48.480
Anyway, enjoy the interview.

03:48.480 --> 03:49.040
Hi.

03:49.040 --> 03:50.080
It's lovely to meet you.

03:50.080 --> 03:51.120
Nice to meet you too.

03:51.120 --> 03:56.160
So, I was speaking with Andrew Lampanam yesterday, and he really highly recommended your paper.

03:56.160 --> 03:57.360
I looked it up, it's called,

03:57.360 --> 04:00.960
Large Language Models Are Not Zero Shot Communicators,

04:00.960 --> 04:04.880
and I also recognize Stella Biderman and Sarah Hooker, of course.

04:04.880 --> 04:06.560
Sarah's an absolute legend.

04:06.560 --> 04:09.360
Now, you led in the paper by saying,

04:09.360 --> 04:14.240
humans interpret language using beliefs and prior knowledge about the world.

04:14.240 --> 04:16.960
For example, we intuitively understand the response,

04:16.960 --> 04:21.440
I wore gloves to the question, did you leave fingerprints as meaning no?

04:22.080 --> 04:28.800
So, you call this implicature, but I suppose I would think of it as some kind of

04:29.920 --> 04:34.560
extrapolation, being able to reuse knowledge that we have about the world

04:34.560 --> 04:35.920
in a different situation.

04:35.920 --> 04:37.920
But could you talk us through that paper?

04:38.640 --> 04:39.440
So, yeah, thank you.

04:39.440 --> 04:41.040
That was a great introduction to the paper.

04:41.760 --> 04:47.360
Indeed, implicature is kind of the technical term that we use for this example that you gave.

04:48.720 --> 04:52.800
And indeed, extrapolation is a sensible way to describe this.

04:52.800 --> 04:58.160
What we do in this paper is kind of show that large language models are not really good at this

04:58.160 --> 05:02.880
aspect of communication, and we think it's a very important aspect of communication.

05:02.880 --> 05:06.880
So, the title says, Large Language Models Are Not Zero Shots Communicators, right?

05:07.440 --> 05:13.600
So, what we mean by that is to be a communicator, you have to infer the meaning of utterances,

05:14.400 --> 05:20.000
not only by their semantics, so not only by how words combine into some kind of meaning,

05:20.000 --> 05:24.480
but by interpreting the shared knowledge, our shared experience of the world.

05:25.600 --> 05:27.920
And that's what we look at in this paper.

05:27.920 --> 05:32.880
And what we find is that large language models are pretty bad at this.

05:33.840 --> 05:37.600
Specifically, we group them into different groups.

05:37.600 --> 05:41.760
So, we have base large language models like OPT and BLOOM that are just trained on

05:41.760 --> 05:43.280
next-word prediction.

05:43.280 --> 05:52.400
And we also have instructable models like Flonty5, T0, or DaVinci Instructable Models by OpenAI.

05:53.360 --> 05:58.160
And all models perform really bad, closer to random than to humans.

05:58.800 --> 06:04.880
But OpenAI's instructable models have much more promise.

06:04.880 --> 06:05.760
They're much better at it.

06:06.480 --> 06:06.960
Interesting.

06:06.960 --> 06:10.560
Okay, so now the zero shot thing is very interesting.

06:10.560 --> 06:14.000
So, we take these models, and it's kind of like self-supervised learning.

06:14.000 --> 06:16.560
We train them on loads of data on the Internet.

06:16.560 --> 06:21.600
And you're saying that zero shot is when we don't really give much information in the prompt.

06:21.600 --> 06:26.320
So, there's a relationship between how big the model is and how much in-context learning

06:26.400 --> 06:28.240
that we give to the model in the prompt.

06:28.240 --> 06:29.600
Yeah, that's true.

06:29.600 --> 06:36.000
Yeah, the zero shot case that we tested is we give the model a short instruction saying like,

06:37.600 --> 06:43.280
in the following exchange, someone gives a response that has some meaning beyond the utterances.

06:44.400 --> 06:47.600
It had the meaning is yes or no, can you resolve this?

06:47.600 --> 06:49.440
And then we give an example.

06:50.720 --> 06:55.280
And then we evaluate it on ways based on whether it can choose yes or no.

06:56.400 --> 06:59.840
So, that's the zero shot case, and humans, we don't give any instructions at all.

06:59.840 --> 07:04.080
We just said, resolve this to a yes or no.

07:04.640 --> 07:10.480
Okay, so, is it the case that large language models then zero shot almost irrespective of their size

07:10.480 --> 07:13.200
and irrespective of this human feedback alignment?

07:13.200 --> 07:15.440
They just don't perform very well at this implicature at all.

07:16.160 --> 07:21.120
The instructable models by OpenAI gets a non-trivial performance.

07:21.840 --> 07:26.400
I think the models like OPT and Bloom, those kind of base models,

07:27.200 --> 07:29.680
they really conduce this style very well at all.

07:29.680 --> 07:37.520
They get 10% above random, but OpenAI's models are around 70% at zero shots.

07:37.520 --> 07:38.000
Interesting.

07:38.000 --> 07:42.640
So, did you do some work looking at, okay, well, let's try some in-context learning.

07:42.640 --> 07:44.320
Does that improve the implicature?

07:44.960 --> 07:45.840
Definitely, yeah.

07:46.400 --> 07:48.800
Like, it's unclear, right?

07:48.800 --> 07:53.600
Whether zero shots is a fair comparison to humans for these models.

07:53.600 --> 07:56.240
Humans are primed in different ways.

07:56.240 --> 08:00.000
So, we also wanted to try view shots in context learning.

08:00.640 --> 08:05.520
And personally, I thought in this case, in context learning wouldn't help much because

08:05.520 --> 08:09.120
each implicature requires a completely novel type of inference.

08:09.760 --> 08:14.560
But in fact, we show that OpenAI's models is the only group of models that really

08:15.280 --> 08:17.040
benefits from this a lot.

08:17.040 --> 08:21.200
And they can get to up to 80% performance with roughly five examples,

08:21.760 --> 08:24.880
which, and afterwards, more than five examples, it's kind of plateaus.

08:26.240 --> 08:30.160
But they're still like a significant gap with humans.

08:30.160 --> 08:32.720
But it's a great improvement.

08:32.720 --> 08:33.520
Yeah, that's fascinating.

08:33.520 --> 08:37.360
So, can you give me an example of, if we were doing some in-context learning,

08:37.360 --> 08:41.040
let's say with DaVinci 2, what would that prompt look like?

08:42.000 --> 08:47.760
So, if we, I don't exactly remember the wording of the prompts,

08:47.760 --> 08:51.520
but there would be something like the following are examples of the task.

08:51.520 --> 08:55.920
And then you get a bunch of implicatures that are already resolved to a yes or no.

08:56.560 --> 09:01.040
And then you get the original instruct prompt that says

09:02.160 --> 09:05.040
resolve the following sentence to a yes or no.

09:05.040 --> 09:07.440
And then you get the actual example.

09:07.440 --> 09:10.560
And these in-context examples are all taken from a development set.

09:11.120 --> 09:14.720
Okay, so, can you tell us a little bit about

09:14.720 --> 09:18.480
how this reinforcement learning for human preferences works on language models?

09:19.200 --> 09:22.800
So, reinforcement learning for human preferences is a method to fine-tune

09:23.760 --> 09:24.800
based on our language models.

09:24.800 --> 09:28.880
So, the based on our language models are OPT and BLU, for example,

09:28.880 --> 09:30.240
that's part of the group.

09:30.240 --> 09:32.400
And they are just trying some next-word prediction, right?

09:32.400 --> 09:34.080
But they are not really aligned.

09:34.080 --> 09:38.000
They're sort of this alignment problem where they're trained on next-word prediction

09:38.000 --> 09:40.560
and that's not really what we are asking them to do.

09:41.680 --> 09:45.440
And then with reinforcement learning from human feedback,

09:45.440 --> 09:50.080
what we further do, I mean, not we, unfortunately, other people do,

09:50.080 --> 09:55.040
is they take some kind of human preferences from somewhere.

09:55.040 --> 09:59.200
Like, for example, humans are shown prompts and completions by models

09:59.200 --> 10:01.360
and they say, this one is better than that one.

10:01.360 --> 10:04.960
This completion for the text for this prompt is better than that one.

10:05.520 --> 10:08.800
By that, we get a sort of ranking by preference

10:08.800 --> 10:11.440
and we can learn a reward model on those preferences

10:12.640 --> 10:15.520
with an interesting trick that was published in 2017.

10:16.640 --> 10:19.760
And through this rewards model, we have sort of,

10:19.760 --> 10:26.000
we can bootstrap the preferences from humans into the based archangels model

10:26.000 --> 10:31.440
by fine-tuning them with regular RL on this reward model.

10:31.440 --> 10:32.720
Yeah, it's really interesting.

10:32.720 --> 10:37.520
I was speaking with Srijan Kumar who won one of the outstanding

10:37.520 --> 10:41.360
paper awards in Europe and he's got this work on kind of,

10:42.400 --> 10:45.360
we want the models to be more anthropomorphic

10:45.360 --> 10:48.320
and we have these priors to help us understand the world.

10:48.320 --> 10:53.360
And he came up with a framework of kind of like importing these priors

10:53.360 --> 10:57.920
from language encodings into, let's say, a discrete program synthesis model.

10:57.920 --> 11:00.960
But I guess what I'm saying is that there's something really interesting

11:00.960 --> 11:05.280
going on within context learning and it's almost like we're giving the model

11:05.280 --> 11:11.040
the priors to extrapolate or to do something useful in this particular situation.

11:11.040 --> 11:12.800
Yeah, yeah, that's really interesting.

11:12.800 --> 11:14.480
I don't know the paper, I should check it out.

11:14.480 --> 11:19.360
But the way I view it, and my thinking has been shaped this week also

11:19.360 --> 11:23.120
by Andrew Lampinen who wrote an interesting paper on comparing models in humans,

11:23.680 --> 11:28.880
is that it seems that in context learning for this specific task, implicatures,

11:28.880 --> 11:32.800
it's not really that they learn how to use their shared experience

11:33.600 --> 11:36.640
from the in-concept samples, they're primed for the task

11:36.640 --> 11:38.880
with a few shot examples in the context.

11:39.440 --> 11:42.160
And I think that's actually what's happening here.

11:42.160 --> 11:46.240
Like if you test the model zero shots, there's no,

11:46.240 --> 11:49.840
why would we expect it to do this task properly?

11:49.840 --> 11:51.840
There's no motivation or anything like that.

11:51.840 --> 11:56.480
But if you prime it with in-concept samples, it does better.

11:56.480 --> 12:02.560
And that would also explain why it doesn't help to add more than five examples

12:02.560 --> 12:06.480
because it's not using the inherent information in the examples,

12:06.480 --> 12:08.880
it's just being primed for this specific task.

12:08.880 --> 12:10.000
Yeah, that's really interesting.

12:10.000 --> 12:13.840
Sarah has done lots of work on interpretability in machine learning models.

12:13.840 --> 12:19.280
And one thing that I wrestle with a lot is whether we should try and get models

12:19.280 --> 12:21.040
to think the way humans do.

12:21.040 --> 12:25.360
And you can come at it from an intelligibility point of view,

12:25.360 --> 12:28.080
but you could also come at it from a generalization point of view.

12:28.080 --> 12:31.440
Like maybe we do symbolic generalization over cognitive priors,

12:31.440 --> 12:32.960
and that's how we understand the world.

12:32.960 --> 12:36.320
But there are people who just say large language models,

12:36.320 --> 12:37.920
they're just a different mode of understanding,

12:37.920 --> 12:39.280
and we shouldn't try and make them like us.

12:39.280 --> 12:40.080
Like what do you think?

12:42.240 --> 12:43.360
It's a good question.

12:43.360 --> 12:46.480
I am really a non-expert on interpretability.

12:46.480 --> 12:51.120
I'm like, I always come at it from a very anthropocentric view.

12:51.120 --> 12:53.040
Like I would love them to be more like humans

12:53.040 --> 12:56.400
because that would make them interesting subjects to study also

12:56.400 --> 12:58.960
and better to communicate with.

12:58.960 --> 13:01.680
But at the same time, you can take this opposite view.

13:01.680 --> 13:06.000
And I think Stella, the co-author on this paper often says,

13:06.880 --> 13:09.920
you're making a category area, you're attributing something to these models

13:09.920 --> 13:13.440
that they don't have knowledge, those kind of things.

13:13.440 --> 13:18.000
So it might also very well be that we're trying to look for pragmatics

13:18.000 --> 13:19.600
or semantic understanding in these models,

13:19.600 --> 13:22.000
but that's just not how you should think about it.

13:22.640 --> 13:24.400
And I completely forgot to ask you.

13:24.640 --> 13:28.560
So again, some of the audience don't know about

13:28.560 --> 13:30.480
natural language understanding in linguistics and so on.

13:30.480 --> 13:32.720
So what is pragmatics?

13:32.720 --> 13:34.000
Yeah, that's a good question.

13:34.000 --> 13:41.840
So pragmatics is an aspect of language, the way we study language,

13:41.840 --> 13:46.160
that doesn't really look at the syntax or semantics,

13:47.520 --> 13:52.000
which look at, for example, those kind of aspects of linguistics,

13:52.000 --> 13:55.920
look at what a word means and how you combine them into novel meanings.

13:55.920 --> 13:59.120
So those kind of areas of linguistics really look at

13:59.760 --> 14:03.840
when someone understands the term, the utterance as John loves Mary,

14:03.840 --> 14:06.080
they also understand utterance Mary loves John.

14:06.640 --> 14:08.720
Pragmatics goes beyond that.

14:08.720 --> 14:15.520
It looks at how context and our shared experience really influences meaning.

14:15.520 --> 14:19.360
So usually the meaning determined by pragmatic inference

14:19.360 --> 14:23.040
is not really directly part of the context window.

14:23.040 --> 14:28.080
You really have to tap into your prior knowledge.

14:29.280 --> 14:31.600
Yeah, so I'm a fan of Montague as well.

14:31.600 --> 14:36.240
So it's almost like we have the semantic potential and then we have pragmatics,

14:36.240 --> 14:39.760
and that's bringing some additional context.

14:39.760 --> 14:40.880
Yeah, yeah, exactly.

14:42.880 --> 14:47.840
Okay, and because that's a really great example from that extrapolative example

14:47.840 --> 14:49.440
from Montague about Mary loves John,

14:50.640 --> 14:52.880
how could a large language model realistically,

14:52.880 --> 14:56.160
because I think of that as being a symbolic generalization.

14:56.160 --> 14:58.640
So how could a language model do that kind of generalization?

14:59.200 --> 15:00.400
Symbolic generalization?

15:00.400 --> 15:01.200
Yes.

15:01.200 --> 15:02.160
Oh, that's a big one.

15:05.360 --> 15:06.160
I don't know.

15:06.160 --> 15:07.120
I really, really don't know.

15:08.400 --> 15:11.200
In my research journey, I can kind of

15:12.000 --> 15:14.080
come from studying compositionality in language,

15:14.080 --> 15:17.200
which is really more this type of thing that we're talking about now.

15:17.840 --> 15:22.960
And looking at more sort of neurosymbolic approaches or stronger inductive biases.

15:22.960 --> 15:26.720
And now these large language models really showed us that

15:27.280 --> 15:31.360
there is an insane amount of compositional generalization going on

15:31.360 --> 15:33.280
without any inductive bias for that.

15:33.840 --> 15:36.720
Chat GPT kind of shows us that with all these examples on Twitter,

15:36.720 --> 15:39.280
right, you give it two novel concepts,

15:39.280 --> 15:43.040
and it combines it beautifully into some kind of story.

15:44.240 --> 15:46.560
But yeah, to go back to your question, how can they do it?

15:46.560 --> 15:50.640
I don't know. Maybe Skill will get us there to the sense that

15:51.280 --> 15:53.840
humans are also imperfect symbolic reasoners.

15:53.840 --> 15:55.280
Again, to mention Andrew Lampinen,

15:55.280 --> 15:58.640
he did a great paper on symbolic behavior,

15:58.640 --> 16:00.880
where it's not really a discrete,

16:00.880 --> 16:04.560
I can do symbolic processing versus I cannot do symbolic processing,

16:04.560 --> 16:06.000
but it's more a scale.

16:07.360 --> 16:08.800
That's kind of shaped my thinking as well.

16:08.800 --> 16:10.160
I think it's a scale.

16:10.160 --> 16:12.880
Large language models are pretty far on that scale.

16:12.880 --> 16:15.760
They can do very interesting compositional generalization.

16:16.720 --> 16:18.800
And sort of symbolic behavior,

16:18.800 --> 16:22.000
but they fail in catastrophic ways as well.

16:22.880 --> 16:26.000
Again, an example that I think comes from Gary Marcus is

16:26.000 --> 16:27.760
when you ask Chat GPT,

16:30.960 --> 16:33.280
how do horses ride cowboys?

16:33.280 --> 16:37.120
And it just writes a whole story about how a horse rides a cowboy,

16:37.120 --> 16:39.600
even though it doesn't work.

16:40.320 --> 16:41.520
Yeah, it's so interesting,

16:41.520 --> 16:45.120
because I think it's easy to think of large language models in the binary.

16:45.840 --> 16:47.360
Marcus says they're bloviators,

16:48.800 --> 16:50.720
and Bender says they're stochastic parrots.

16:51.280 --> 16:54.880
And then you have the folks who think that it's emergent reasoning

16:54.880 --> 16:56.560
and symbolic generalization.

16:56.560 --> 17:00.240
And I was a skeptic, and I just can't ignore the evidence.

17:00.240 --> 17:03.760
They really are doing amazing things now.

17:04.720 --> 17:06.480
And you were just speaking to Lampinen,

17:06.480 --> 17:10.800
and it's a similar thing with this idea of symbolic generalization.

17:10.800 --> 17:12.080
It might not be a binary, right?

17:13.120 --> 17:13.840
Yeah, exactly.

17:14.480 --> 17:16.400
It might be a very great as competency.

17:17.440 --> 17:19.760
And humans also fail in certain cases.

17:20.720 --> 17:22.880
So on this in-context learning,

17:22.880 --> 17:24.960
because that's something that has interested me.

17:24.960 --> 17:28.000
So the first version of GPT-3 is zero-shot.

17:28.000 --> 17:29.600
We didn't really know how to prompt it.

17:29.600 --> 17:31.280
It looked like a bloviator.

17:31.280 --> 17:33.040
We then went on this intellectual journey,

17:33.040 --> 17:36.160
and we discovered front engineering, scratch pad,

17:36.160 --> 17:37.360
chain of thought reasoning.

17:38.000 --> 17:39.760
I spoke with Hattie Zoe the other day,

17:39.760 --> 17:43.040
and she's got this kind of algorithmic front in-context learning.

17:43.040 --> 17:45.680
And it's just remarkable what's going on there.

17:46.720 --> 17:47.920
Do you have any intuition?

17:48.960 --> 17:52.960
Is it like the prompt is some kind of a program interpreter or something?

17:55.760 --> 17:58.640
My intuition is rather that the prompt kind of...

17:59.360 --> 18:01.920
I don't know how to formalize this intuition,

18:01.920 --> 18:03.600
but I guess that's why it's an intuition.

18:04.640 --> 18:08.080
That the prompt kind of primes the model

18:08.080 --> 18:11.440
and puts it into a sort of area of its weight space,

18:11.440 --> 18:17.680
where it can better answer the actual question

18:17.680 --> 18:21.440
that is asked in the actual question that's asked.

18:22.000 --> 18:25.120
And I think certain things that point towards this

18:25.120 --> 18:28.160
is that there is also some research coming out

18:28.160 --> 18:33.520
where they permute the labels in the in-context examples,

18:33.520 --> 18:35.520
and they show that the performance is similarly good,

18:36.160 --> 18:39.520
even though at the same or like they do completely random labels

18:39.600 --> 18:43.280
in the in-context examples, and the performance is still pretty good.

18:43.280 --> 18:47.040
But there's also other work that shows that doesn't always work.

18:47.040 --> 18:48.800
Sometimes you do need actual labels.

18:50.560 --> 18:53.520
So yeah, again, the answer is basically, I don't know,

18:53.520 --> 18:56.800
but my intuition is rather that the model is really primed for...

18:58.480 --> 19:01.040
Or there's also another great way of viewing this,

19:02.400 --> 19:04.880
and I read that unless wrong at some point.

19:04.880 --> 19:07.120
I don't know how to attribute the author,

19:07.840 --> 19:10.480
because I forgot their name, but it's about

19:11.840 --> 19:14.080
that these models are good at simulating anything.

19:14.080 --> 19:16.880
So you have to sort of prime them to let them know

19:16.880 --> 19:18.080
what they're simulating right now.

19:19.440 --> 19:20.240
Yes, weird, isn't it?

19:20.240 --> 19:25.040
Because we have an anthropocentric view of the world,

19:25.040 --> 19:29.440
and we're agentive with individual agents.

19:29.440 --> 19:32.400
And a language model is everything at once.

19:32.400 --> 19:36.480
So it's almost like you need to give it a trajectory just to get it

19:36.480 --> 19:37.840
to go somewhere interesting.

19:37.840 --> 19:42.160
So with this in mind, we really want to make progress

19:42.160 --> 19:43.920
in natural language understanding.

19:43.920 --> 19:46.720
And what do you think are the steps we need to make

19:46.720 --> 19:50.000
to robustify these language models?

19:50.000 --> 19:51.280
Yeah, that's a good question.

19:52.000 --> 19:54.960
Personally, from this pragmatics paper,

19:54.960 --> 19:59.360
I think pragmatics is one area where they can make huge strides too.

19:59.360 --> 20:01.920
I think even though they have semantic failure modes,

20:01.920 --> 20:03.680
they're really impressive at that.

20:03.680 --> 20:06.400
They're really impressive at compositional generalization.

20:08.080 --> 20:09.600
But pragmatics might be something

20:09.600 --> 20:13.840
that they're simply not trained for currently.

20:13.840 --> 20:18.160
And one very low-hanging fruit is the RLHF that we talked about.

20:18.960 --> 20:21.920
I think that clearly really improves that.

20:21.920 --> 20:26.960
And intuitively, it seems like in the instructivity paper,

20:26.960 --> 20:28.880
you see that they ask the human laborers

20:28.880 --> 20:31.760
to really infer the human intent and the problems and write on.

20:31.760 --> 20:34.480
And that's very reminiscent of implicatures.

20:35.680 --> 20:37.520
But then on a more sort of broader scale,

20:37.520 --> 20:41.040
I think some kind of embodiment or interactivity

20:41.040 --> 20:42.800
might be really important.

20:42.800 --> 20:46.160
Like pragmatic inference is really a social skill that we have.

20:47.120 --> 20:51.360
There's a lot of pragmatic pressures that you encounter

20:51.360 --> 20:54.960
while just acting in the world and navigating communication

20:54.960 --> 20:56.960
and navigating a lot of things.

20:57.520 --> 21:00.000
So I think I'm currently trying to look at

21:01.680 --> 21:04.000
a setup in reinforcement learning

21:04.000 --> 21:06.960
where we're trying to make a pragmatic task

21:06.960 --> 21:09.600
and see when pragmatic reasoning would emerge there.

21:12.560 --> 21:14.720
I don't know how to consolidate that fully

21:14.720 --> 21:15.920
with large language models yet,

21:15.920 --> 21:22.320
but I think interactivity and social navigation is important.

21:23.280 --> 21:27.040
I'm really fascinated by the embedded tradition

21:27.040 --> 21:28.400
in cognitive science.

21:28.400 --> 21:30.080
And I think there's a lot of interesting work there.

21:30.080 --> 21:32.720
But I suspect you do as well a little bit.

21:33.360 --> 21:35.600
How do you contrast what is essentially

21:35.600 --> 21:36.960
the representation of this view

21:36.960 --> 21:39.200
where everything's in this big monolithic model

21:39.200 --> 21:41.200
to some kind of relational view

21:41.200 --> 21:44.000
where we're using essentially the world

21:44.000 --> 21:45.520
as its own representation?

21:45.520 --> 21:47.600
Yeah. Again, I don't know.

21:47.600 --> 21:50.640
To what extent it's also possible to express everything

21:50.720 --> 21:52.880
in just the representation in this view

21:52.880 --> 21:54.560
where you have an internal world model.

21:54.560 --> 21:56.640
And I don't know to what extent

21:56.640 --> 21:58.720
you really need an external world to learn,

21:58.720 --> 22:02.160
but intuitively it seems like that might be very important.

22:02.160 --> 22:08.240
And intuitively it also seems like the behaviors

22:08.240 --> 22:12.320
that can emerge are really limited by the world's models

22:12.320 --> 22:15.360
acting in a large language model only see sex.

22:16.320 --> 22:20.400
And there's basic things that just simply cannot learn,

22:20.400 --> 22:23.120
even though it has surprised us a lot.

22:24.080 --> 22:25.600
So I think, I don't know,

22:25.600 --> 22:28.000
it's easy to think about that it's really important

22:28.000 --> 22:30.240
to have some kind of external world to interact with.

22:31.440 --> 22:35.200
But I'm happy people are working on scaling

22:35.200 --> 22:41.040
and I'm not saying some type of AGI, whatever that means,

22:41.040 --> 22:43.920
might not emerge from simply scaling up

22:43.920 --> 22:45.280
basically what we're doing right now.

22:46.320 --> 22:49.920
Amazing. And are there any other parts of your paper

22:49.920 --> 22:51.120
that we haven't spoken about?

22:52.960 --> 22:56.000
Yeah, one thing that we found pretty interesting

22:56.000 --> 22:59.200
is that even though all these open AIs models

22:59.200 --> 23:02.560
can get really high performance, close to humans,

23:02.560 --> 23:05.760
6% roughly, that won't tell you much without the details.

23:05.760 --> 23:08.480
But it's a significant gap still, but it's really, really close.

23:08.480 --> 23:12.640
I don't know if a human might figure out

23:12.640 --> 23:16.320
whether this model is a model or a human in that case.

23:16.960 --> 23:21.920
But when we sort of drill down and make a taxonomy

23:21.920 --> 23:24.080
of the examples that are in this data set,

23:24.080 --> 23:27.040
we find that they are mostly benefiting

23:27.040 --> 23:30.720
from the simple examples where not a lot of context is needing.

23:30.720 --> 23:36.080
So one example is an implicator is if you ask me

23:36.640 --> 23:38.240
how many people came to your party,

23:38.240 --> 23:39.920
and then I say some people came.

23:40.560 --> 23:42.960
It's really the conventional meaning kind of of the word

23:42.960 --> 23:45.280
some that I meant not all people came.

23:45.840 --> 23:48.800
But it's still an implicator, but it's a very common one.

23:48.800 --> 23:51.120
So those kind of examples, if we isolate those

23:51.120 --> 23:53.680
and we look at specifically examples

23:53.680 --> 23:55.680
that are really context-dependent like

23:58.400 --> 24:01.440
are you coming to the open AI party tonight?

24:01.440 --> 24:03.040
I have food poisoning.

24:03.040 --> 24:06.080
Those need much more context to be resolved.

24:06.080 --> 24:07.920
And then the performance decreases again,

24:07.920 --> 24:13.360
and there is roughly a 9% gap, which like the best model,

24:13.440 --> 24:17.280
but all other based models and instructable models

24:17.280 --> 24:18.560
like Flancy V and stuff,

24:18.560 --> 24:21.920
they then again completely fail on those kind of examples.

24:21.920 --> 24:22.560
Fascinating.

24:23.280 --> 24:25.200
I'm really interested by this idea that

24:25.200 --> 24:27.680
understanding is a complex phenomenon,

24:27.680 --> 24:31.200
and it's like the parable of the blind man and the elephant.

24:31.200 --> 24:33.200
So we create all of these metrics,

24:33.840 --> 24:36.320
and the metrics exclude most of the truth.

24:36.320 --> 24:40.240
And the metrics for pragmatics presumably are in some sense

24:40.240 --> 24:41.840
even more complicated than the metrics

24:41.840 --> 24:43.920
that we already use in natural language understanding.

24:43.920 --> 24:48.960
And it just feels like is that going to be a serious problem

24:48.960 --> 24:52.560
for us to kind of encapsulate how well the model understands?

24:53.360 --> 24:57.440
Do you mean that we're sort of giving it a test

24:57.440 --> 24:58.560
that is couldn't really solve?

24:59.280 --> 25:01.520
Well, I suppose one way of looking at it is,

25:01.520 --> 25:04.240
in this particular test, we've come up with lots of examples

25:04.240 --> 25:06.960
of pragmatic inference, if you like.

25:06.960 --> 25:09.760
And what we're doing is we're taking a very complex phenomenon

25:09.760 --> 25:12.320
and we're putting pins through it,

25:12.320 --> 25:15.360
so we're putting like little slices through it in different angles.

25:15.360 --> 25:18.000
And then we've got this shortcut problem

25:18.000 --> 25:20.720
that if we optimize on any one of those slices,

25:20.720 --> 25:24.320
we might be kind of like excluding everything else that's important.

25:24.320 --> 25:25.280
So it feels like we've got,

25:26.800 --> 25:30.240
is this like a general problem in natural language understanding?

25:30.240 --> 25:34.240
It seems like you're getting at evaluation to some extent, right?

25:34.480 --> 25:39.680
I think evaluation is the single most difficult thing in NLP.

25:41.280 --> 25:43.760
This is just a benchmark to give us some intuition

25:43.760 --> 25:46.800
as to what these current failure models,

25:47.360 --> 25:49.120
failure modes of these models might be.

25:49.680 --> 25:53.520
And I think if this benchmark is at some point passed

25:53.520 --> 25:57.280
by a model that's in and of itself

25:57.280 --> 25:59.600
without trying to diminish my own paper, it doesn't tell us much.

25:59.600 --> 26:00.640
There's much more to be done.

26:00.640 --> 26:02.560
We need more different benchmarks.

26:02.560 --> 26:05.360
We need like human testing in a sort of Turing style maybe.

26:06.800 --> 26:12.000
And yeah, I think this is the most interesting problem in NLP,

26:12.000 --> 26:13.440
like how the property evaluates.

26:14.080 --> 26:14.880
Interesting.

26:14.880 --> 26:18.320
And do you take an interest in fairness and bias in the models as well?

26:18.880 --> 26:22.480
I'm very interested in it, but from a sort of spectator view as well.

26:22.480 --> 26:24.560
Like I haven't worked on it at all.

26:24.560 --> 26:27.600
Okay, yeah, because that's presumably a massive challenge.

26:27.600 --> 26:30.320
Yeah, yeah, definitely, yeah.

26:30.400 --> 26:31.280
Amazing.

26:31.280 --> 26:37.440
So in final question, what are you most excited about in your research trajectory

26:37.440 --> 26:38.240
over the next year or so?

26:40.080 --> 26:44.000
Well, definitely, I just feel super excited to be working on stuff like this

26:44.000 --> 26:47.520
currently given the capabilities these models show.

26:47.520 --> 26:49.520
Like they're absolutely amazing.

26:49.520 --> 26:52.240
And I love seeing how people interact with them.

26:52.240 --> 26:56.160
Like the creativity of people is really needed to get some kind of interesting

26:56.160 --> 26:57.680
response out of these models, right?

26:57.680 --> 27:01.760
And also the creativity of people is needed to find the failure modes.

27:01.760 --> 27:08.320
And yeah, so what I'm most interested, excited about now personally for my own

27:08.320 --> 27:13.920
research journey is really trying to look at, you know, an interactive setup

27:13.920 --> 27:20.480
and see when pragmatic inferences might emerge in what kind of environments,

27:20.480 --> 27:25.040
what kind of pressures do we need, and how can we translate that back to getting to be,

27:25.520 --> 27:30.080
to getting like language models be zero shells communicators.

27:30.080 --> 27:30.480
Amazing.

27:30.480 --> 27:32.000
And where can people find out more about you?

27:33.680 --> 27:38.240
They can follow me on Twitter is first name, last name, and I have a website,

27:38.240 --> 27:39.840
also firstname, lastname.com.

27:40.560 --> 27:41.280
Amazing.

27:41.280 --> 27:42.240
Laura, thank you so much.

27:42.240 --> 27:43.280
It's a pleasure to meet you.

27:43.280 --> 27:44.400
Thank you for having me.

27:44.400 --> 27:44.880
Amazing.

27:44.880 --> 27:45.120
Amazing.

27:45.120 --> 27:45.360
Right.

27:45.360 --> 27:45.760
Cool.

