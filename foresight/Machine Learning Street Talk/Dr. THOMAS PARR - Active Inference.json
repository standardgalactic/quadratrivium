{"text": " So, welcome back to MLST. Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo, and Professor Carl Friston. Now the book is Active Inference, the free energy principle in mind, brain, and behavior. So the book, from a pedagogical perspective, it's describing active inference from the high road and the low road. And the high road is a little bit kind of helicopter view, so it's saying, OK, we've got these biological organisms or these living systems, and what do they do in order to be living systems? Well, they resist entropic forces acting on them by minimizing their free energy. So it goes into the how question, but it also goes into the why question from a helicopter view. The low road of active inference is far more mechanistic, far more mathematical, and obviously both all of the roads lead to Rome, if you like. But the low road is talking about things like Bayesian mechanics, there's a primer on probability theory, talking about things like variational inference, which is the way that we solve these intractable optimization problems in active inference, and also talking about framing active inference as a process theory, which is the latest incarnation of the description of active inference. So Professor Carl Friston wrote a preface for the book. He said, active inference is a way of understanding sentient behavior. The very fact that you are reading these lines means that you are engaging in active inference, namely actively sampling the world in a particular way, because you believe you will learn something. You are palpating. This is beautiful, by the way. Friston uses the most beautiful language, it's his signature, if you like, it's his calling card. He said, you are palpating this page with your eyes, simply because this is the kind of action that will resolve uncertainty about what you're going to do next, indeed what these words convey. In short, he said, active inference puts action into perception, whereby perception is treated as perceptual inference or hypothesis testing. Active inference goes even further and considers planning as inference, that is inferring what you're going to do next to resolve uncertainty about your lived world. So I'm about to show you a sneaky clip of Professor Friston that we filmed in January. I might publish the full show on MLSD in the future, but I just want to take this as an opportunity to thank you so much for all of our Patreon supporters. Honestly, it means so much to me because the last few months I've just been, you know, trying to make this activity of mine, this passion of mine, a full-time job. And it's not just because you love the show and you want to support me, you get early access to content, you can join our private Patreon Discord. We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you also get early access to lots of our content. So, you know, please check that out. But in the meantime, here's a little sneaky clip from Professor Friston. The neural network is a generative model of the way in which its content work was generated. And its only job is effectively to learn to be a good model of the content that it has to assimilate. If you put agency into the mix, you get to active inference. And now that we've got a generative model that now has to decide which data to go and solicit. And that's actually quite a key move and also quite a thematic move. So we're moving from perception machines. We're moving from sort of neural networks in the service of, say, face recognition into a much more natural science problem of how would you then choose which data in a smart way you go and solicit in order to build the best models of the causes of the data that you are in charge of gathering. Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging at the Queen Square Institute of Neurology at University College London and a practice in clinician. Now, one of the reviews from the book was from Andy Clark. He said, it should have been impossible. A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual landscape from the formal schemas and some of the neurobiology and then garnished with practical recipes for active model design. Philosophically astute and scientifically compelling, this book is essential reading for anyone interested in minds, brains and action. Well, I mean, thank you very much for having me on. So I'm Thomas Parr. I'm both a clinician and a theoretical neuroscientist. So I've been working in active inference for a number of years now since I did my PhD back in 2016 with Carl at the theoretical neurobiology group at Queen Square. And I'm now based at Oxford where I split my time between research and clinical practice. So tell me about the first time you met Carl. The first time I met Carl, I was considering, so I was a medical student at the time at UCL and I was considering doing a PhD. And I remember arranging to meet with him and obviously being a relatively nerve-wracking experience meeting one of the most famous neuroscientists in the world. I remember discussing with him about it and saying, you know, this is what I'm interested in. Would you consider supervising my PhD if I were to get the funding for it? And I remember he said, yes, all right, then, anything else. And I asked, do you want to see my CV or anything like that? And he said, no, I'll only forget it. Yes. That was my first encounter with Carl. But since then, he's always been immensely supportive and has been, you know, exactly the sort of mentor that I think anybody would want to be able to develop a skill set and sort of proceed in science. I come from a machine learning background. And since discovering active inference and Carl's work, it's really broadened my horizons. And at the moment, there's an obsession with things like chat GPT. And I just wondered in your own articulation, how would you kind of pose the work that you do in relation to that kind of technology? It's a good question. And I suppose there are many levels at which it could be answered, aren't there? I guess thinking about something like chat GPT in that style of technology, it's clearly been very, very effective at what it does. But it's worth thinking about what is it that it does? And I think chat GPT is an excellent example because so many people are familiar with it. It has such impressive results in terms of being able to simulate very effectively what it's like to have a conversation. But ultimately, it is like most deep learning architectures. It's a form of function approximation. It's a form of being able to capture very well the output that would be expected under some set of conditions given some input. So you give it some text and it knows which text to predict. And it's very good at that. But in a sense, that's where it stops. It doesn't necessarily do anything else. That's very different to what you and I do when we engage with the world around us, when we want to learn about the world around us, when we want to form our own beliefs about what's going on. And those are the things that I think it doesn't have in the same way. It certainly can't act and go and seek out specific exchanges, specific conversations that it might want to learn from. Whereas you or I might do that if we wanted to know about something specifically, we'd go and look for information about that thing. And I think that's where active inference and the idea of having a generative world model and understanding of what's there in your world that you can alter yourself, that you can change is very different to a lot of more passive artificial intelligence. Probably the point where things become closer is in fields like robotics, where you have to account for both of those things. You have to model a world that has yourself in it, where your actions affect the data that you get in. And I think that's probably where more of the convergence is likely to happen. Yes. So you're describing the difference, I guess, between an observational system and an interactive system. So in an interactive system, an agent can seek information and change or bend the environment to suit its will. Just to linger on this for a second, though, there are folks who do argue that neural networks are more than hash tables, because I think of them the same way you do. They essentially learn a function. And if you densely sample it enough, just like a hash table, it can go and retrieve what that function says given a certain input. But there are folks who say, no, no, no, these models learn a world model. So is given as an example, or with SORA, they say it's learned Navier stokes. It's a really good question. And I think there are some open questions here, and I wouldn't claim to have all the answers to this one. I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly it has captured something about the statistics of language. It's uncovered something about the hidden causes. So you could argue there is potentially an element of world modeling in there that is left implicit. I think it would be very difficult to pull that out or to sort of see that with any transparency with something like chat GPT. And so if it does have something of that sort, probably it's the methods that neuroscientists have been using for years to understand the brain that might help to try and pull out those same things in those sorts of architectures. Maybe some sorts of deep learning and neural network models are very good at picking up regularities in terms of dynamics as well and being able to predict trajectories. And I think it's important to say that describing something as a function approximator is not to criticize or belittle it. It's a very important thing to be able to do. And it may also be very important in certain types of inference. So for instance, things like variational autoencoders are based upon often deep learning neural network architectures. But the function that is learned is the one that maps from the data I've got coming in to the posterior beliefs or the parameters of the posterior beliefs that I would arrive at were I to perform inference of the sort we might do in active inference. So you've written an absolutely beautiful book on active inference. And active inference, in my view, it's a theory of agency, which is to say it describes what an agent does. And I'm fascinated by agency. But could you just start by, I mean, from your perspective, could you introduce the book and tell us about your experience writing it? Of course. So the active inference book that we've written is a collaboration between myself, Giovanni Pazzullo is based in Rome and Carl Friston, who has to take credit for development of active inference in the first place. And the book sort of rose out of our sense that there wasn't a unified book out there or a resource out there to help people learn about what is ultimately a very interdisciplinary field. And so we've all had experience with students coming to us asking for resources, asking what they need to read. And it may be we refer them to a little bit of neuroscience work, a little bit of machine learning, textbooks or specific pages on variational inference or whatever else, giving people introductions to or places they can learn about the maths they need to be able to do it. But then also the biology, the underlying psychology, the long sort of tradition of previous scientists who worked in related areas. And so the book was an attempt to try and provide a place that people could find all of that, or at least references to all the relevant things they needed for that, to stick to the same sort of notation, which is one of the things that's often very difficult, and the same formalisms and try and introduce everything in a very systematic way to people. So I'm pleased here that you found it useful, and I hope other people will as well. The experience of writing it, I mean, so that took place over several years, partly because the pandemic got in the way in the middle. So Giovanni and I were passing notes between one another over email and weren't able to sort of meet in person to discuss it during that time period. But I think we're all quite proud of the result that we've got out of that, and people seem to have responded quite well to it. You start off by talking about what you call a high road and a low road to active inference. Can you sketch that out? Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it, because as I say, it's very multidisciplinary. There are lots of ways into active inference, and one of the things that's most difficult for people who are getting into the field for the first time is knowing where to start. Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas like that, or do they start from a physics-based perspective and start working their way towards something that looks like sentience? And there are lots of different, lots of alternative ways people get into it. The fact that you become interested via machine learning, the fact that other people have become interested through biology, I developed an interest through neuroscience while I was at medical school. And the high road and the low road was a way of just trying to acknowledge that difference or that difficulty of knowing where to begin, and saying that that's okay, there are lots of different roads, but they ultimately end up in the same place. The idea of the low road was to say, well, let's just take observations and psychology sort of development of a number of ideas that are built up over time that come to the idea that we're using internal models to explain our world, that the brain is using something like Bayesian inference, or at least can be described as using Bayesian inference, and go from there through the advances that lead you to active inference, the idea that it's not just a passive process that you're also inferring what I'm going to do. And furthermore, that when we're doing inference, we're changing our beliefs to reflect what's in the world around us and to explain our sensory data. But actually, when we're acting in the world, we can also change the world itself to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the world to also changing the world to reflect our beliefs. And that fascinating move that actually both can be seen as optimization of exactly the same objective that they have the same goal, that in both cases, it's really just improving the fit between us and our world. So that's the sort of low road perspective. The high road perspective was the idea of saying, well, let's start from the minimum number of assumptions we can, let's start from first principles. And that takes a much more physics based approach. It says, if you have a creature that is interacting with its world, then there are a number of things you've already committed to, and that includes things like the persistence of that creature over a reasonable length of time, the maintenance of a boundary between that creature and its world, and that sort of self world distinction. And once you've committed to those things, you can then start to write down the constraints that those imply in terms of the physical dynamics of that system. And you can start to interpret those dynamics in terms of the functions they might be optimizing, much like, much like if you were to write down the equations that underpin Newtonian dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's following the same sort of logic to then get to flows on free energy functions, where free energy is just a measure of that fit between us and our world. And so both roads ultimately end up leading to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we live in, we have to be able to change our beliefs, to reflect what's going on around us and change the world through our dynamical flows on a free energy functional, to best fit with the sorts of creatures we are. When we first started looking at the free energy principle, we were talking about things. It was known as a theory of every thing, every space thing, which is to say, roughly speaking, if a thing exists, what must the thing do to continue to exist? And just their continued existence resisting entropic forces is what defines them, which gets us into the second law of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to resist entropic forces? And I think there's a development in how a lot of these ideas are presented over time, which you expect and hope for in science. And I think we've often taken different perspectives at different points in time as to how we explain these ideas. And resisting entropic forces is an idea that I think most people find relatively intuitive. So the idea that the physical systems will tend to increase their entropy over time, at least close systems, so that over time things will gradually dissipate, things that are highly structured and highly ordered and can only exist in a very small number of configurations and more likely to change into something that can exist in many different configurations than they are to go in the opposite direction. But anything that persists over time and maintains its form clearly resists that process of decay, at least to some extent, or at least for some period of time. However, the opposite is also true. We're also not creatures that tend towards a zero entropy state. We don't end up in a single configuration. We have to be flexible. We have to change in various ways throughout our lifetime or even throughout our daily routine. So it's not quite as simple as just saying you have to resist entropic change. It's more to say that entropic change or the amount of entropy that you expect to develop has to be bounded both from above and below, that there is a sort of optimum level to be at. And that optimum probably varies from different, well, from person to person, from creature to creature, from thing to thing. You could imagine a rock that doesn't need to do much. Its interface with the environment is quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist, we have many more ways of interfacing with the environment and we need to plan many more steps ahead. So is that just a pure continuum between rocks and people? I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy will depend very much on what you are. And as you say, you could imagine a whole scale of things in between. I mean, in a way that as you've highlighted with the rock, some of the most boring things are the things with the, sorry, I shouldn't say that, poor geologists who might find rocks very interesting. And I'm sure are very complex, but from a sort of behavioral perspective, clearly things like us are much more interesting to study than things like a rock. And part of that is that we actually have a higher degree of entropy in how we live our daily lives compared to things like, I almost said organisms like rocks, but things like rocks that are not behaving. The reason I was thinking about this is the second law of thermodynamics was conceived, I don't know, 150 years ago or something like that. And many people at the time thought that it was an affront on free will. I think the religious people at the time were aghast at the idea that things were mapped out in this way. It's always worth saying in this discussion that obviously the tendency for entropy to increase from a physical perspective generally relates to closed systems of which we are not. And as soon as you start talking about different compartments and interactions between them, you also introduce the idea of several coupled systems. And so you can start to ask questions about the overall entropy or the entropy of specific parts of that system. And agents and worlds are two compartments and systems that exchange things with one another. And so are not closed systems almost by definition that a closed system, again, from a sort of neuroscience standpoint is not necessarily a very interesting system. So probably that deals with a large part of that. The question of free will is always an interesting one and always a thorny one that I'm not going to claim to have any expertise on or be able to answer. But I think it probably tackles a slightly different thing from a cognitive science perspective, which is whether or not we believe that the actions we're taking are actions that we've chosen. And that probably comes back into another aspect of active inference, which is that idea that the way we're regulating our worlds, the way we're perhaps changing the entropy of our environment depends upon our own choices about it, our inferences about which one we're going to do next. And that feeds into things like we've spoken about free energy, that that quantity that we use to both choose our actions, an act in the world around us while also drawing inferences. But we can also talk about things like expected free energy, which is a way of evaluating our future state and what would be a good trajectory or a good way for the world to play out. And their entropy has a completely different meaning and there are different sorts of entropy. So for instance, if I were choosing between several different eye movements I could make while looking around this room, the best eye movements I might choose are those for which I'm least certain about what I would see. In other words, the highest entropy distribution. So once you start planning in the future and once you start selecting things to resolve your uncertainty and to be more confident about the world around you, you actually end up seeking out entropy, which it seems to then very much contradict some of the other ideas that we were talking about, the idea that we're constantly resisting it. But actually it's by seeking out the things that we're least certain about that we can start to resolve that uncertainty and start to become more confident and more certain about the world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what you were saying just a second ago about this description of how agents operate, it's very principled. We were talking about this balancing epistemic foraging versus sticking with what you know. And more broadly speaking, thinking of agency as this sophisticated cognition of having preferences and bending the environment and so on. And I guess where I was going before is it's tempting to think that this erodes free will. And I think of them quite adjacently in my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't matter that it's predetermined. For me, free will, I'll try not to use the word free will, but thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant. It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency is better to think of than free will, if that makes sense. Yeah. And I think that's probably right. And the experience of and the inference of agency as well, I think is part of that. There's a potential link that you can draw here also to the idea of chaotic, dynamical systems of which we essentially are examples. And the idea of chaos in that setting is that if you start from two ever so slightly different initial conditions, your path and your future may unfold in a completely different way. And I think that fits very nicely with what you're saying about distinguishing my agency from somebody else's because you don't see it as if I were, you know, the time going to behave in exactly the same way somebody else's. And part of the reason for that is that you end up starting from a slightly different perspective to where they are, and that might lead to wildly different futures for both of you. So something I think about a lot is whether agents are ontologically real or whether they are an instrumental fiction. And I think part of the complexity, especially with active inference and the free energy principle is this hierarchical nesting. So we can think of agents inside agents inside agents. And I guess the first question is, are they real and does it matter? Define real for me. Well, one argument would be that they are epiphenomenal, that they themselves don't affect the system that they are. Is this a good way to think about it? It is a very difficult question to try and contend with, isn't it? Because I think there are so many words that come up here that are kind of laden with different semantics or different meanings depending upon who you speak to and which camp they come from in the sort of philosophical world. And that's why I sort of asked you to define real. And it's really difficult to define what real means in that setting, isn't it? And I guess coming back to your original question there, for me, does it matter if they're a sort of real thing or not? Probably not. It matters whether it's useful. And I guess that sort of brings me to a point about one of the things I find quite appealing about active inference as a way of doing science. And I think, you know, having had an interest in things like neuroscience and psychology for some time, I often found it quite frustrating to understand what people meant and the different language they used in psychology to understand different aspects of cognitive function. And I think, you know, it's worth acknowledging that actually lots of people mean completely different things when they say attention. And some people say attention to mean the sort of overt process of looking at something and paying attention to it. Other people use it to talk about that the differences in gain in different sensory channels that they're trying to pay attention to or not, you know, am I paying attention to colors versus something else? And that's just turning up the volume of different pathways in your brain. And I'm sure there are a world of other things that people mean by it as well. But the idea of then trying to commit to a mathematical description of these things means that a lot of that ambiguity just disappears, that if you put a word to a particular mathematical quantity, as long as you define what that mathematical quantity is and how it interacts with other things, then a lot of that ambiguity just isn't there. And it forces you to commit to your assumptions in a much more specific way. And so that's why I come back to say, does it necessarily matter if an agent is real or not? I don't really know what that means. But if an agent is just a description of something that is separated from its environment that persists for a certain length of time, that has a dynamical structure that can be written down and a set of variables that can be partitioned off from another bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one. Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up in an intelligent way that explains what things do and what they don't do. And I guess the ontological statement, maybe we can park that to one side, because as you say, from a semantics point of view, people have very relativistic understandings of things. And there's always the philosophical turtles all the way down. Well, is it really real? Is it really real? But one thing that is interesting, though, about active inference is that it's quite mathematically abstract. So when we were saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just talking about the statistical independence between states. And those don't necessarily correspond to physical things. So I guess it could be applied to almost anything. It could be applied to culture or memes or language or something like that. And it has been. Yes, indeed. Yeah, it's a good point. And then you end up sort of dragged into the questions of what is physical. What does that mean? Is physical just an expression of dynamics that evolve in time? Because I mean, even committing to a temporal dimension tells you something about the world you're living in. Are the boundaries that we're talking about, are the partitions, are they spatial in nature or not? And, you know, I remember there was an article a little while back that sort of made a lot of argument about this as to whether the partitions that divide creatures from their environments are equivalent to statements of conditional independence of the sort that are seen in machine learning or various other things. And arguing that there's something inherently different about a physical boundary. For me, I was never completely convinced by that, but partly because you have to then define what you mean by a physical boundary. And I suspect it's the same sort of boundary, it's the same sort of conditional dependencies and independences. But where those have specific semantics, whether those be temporal, whether they be something where, you know, you can actually define a proper spatial metric underneath the things that you're separating out. And clearly, that sort of boundary is very important. But for me, that is just another form of the same sort of boundary. And as you say, you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical, whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose, even though mathematically, you could apply it first to other geometries, that would be quite easy, because they have certain mathematical properties in terms of like, you know, being locally connected and measure spaces and all that kind of stuff. But if you did say, okay, I want to have an agent that represents a meme. How would that act? I don't know, you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge is defining the boundary. I think the boundary is a very difficult thing to define sometimes when you're dealing with something non spatial. That boundary, though, might be reflected in the interactions between a meme and a community that that engage with it. It might be to do with the expression of a meme in different parts of, I don't know, a network of some sort or social network. I don't know how easy it would be. I've not tried to do it in that context. And I think with many of these things, you never really know until you've had to go at doing it. But I suppose the key things I would be thinking about are, is there a clean way of defining a boundary for a meme? Is there something that the meme is doing to the outside world? Is there something that the outside world is doing to the meme? And I think if you're able to define those things convincingly, then perhaps there is a form of agent that may be non physical, if that's how you choose to define it. But then I'm not sure what physical means in this setting. Is there also an account of saying, well, actually, if you can write down the dynamics of how a meme propagates through a network, is that any different writing down the dynamics of another sort of physical system? Yes, possibly not. But it is really interesting to me that something like language could be seen as a life as a super organism, or even something like religion. And it seems to tick all of the boxes that we talk about with a gentleness in physical agents, which is to say, let's say a religion or even nationalism, you could say that the state of the Netherlands has certain objectives. And clearly, there's a two way process here. So the state affects our behavior. And we, our collective behavior influences the state. But this then, I think the reason why people don't like to think in this way is we have psychological priors. So we are biased towards seeing a gentleness in individual humans, but we tend not to think of non physical or diffuse things as being agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole issue about the language that we use, that it comes laden with lots of prior beliefs about what it means, which may vary from person to person. And there comes a point where you say, how important is it that I commit to using this particular word to mean this particular thing in this setting? But again, in your example of taking a nation or nation state as being a form of organism at a higher level or form of agent, if you can show that there is a way of summarizing the dynamics of that system, maybe some high order summary of the behavior of people in that system, voting intentions, I don't know, you might then be able to show that it behaves in exactly the same way mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday. And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have built these abstract models to reason about how we think. So we do planning, and we do reasoning, and we have perception, and we do this, and we do that. And also, we try and generate explanations for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly incoherent and inconsistent. And he was talking all about how the brain is actually a kind of predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see things that aren't there. And I think you speak about this in your book that there was a really big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of prediction machine, rather than our brain just kind of like building a simulacrum of the world around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling us to our world that without prediction, you know, if you're purely simulating what might be going on without actually then correcting your simulation based upon what's actually going on or the input you're getting from the world, then you're not going to get very far. So prediction is just an efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want to call it a simulation? My simulation, my internal simulation of what's going on outside. And once you cease to have that constraint, once the world ceases to constrain the simulation, that's the point at which you start, as you say, hallucinating, seeing things that aren't there and developing beliefs that just bear no relationship to or little relationship to reality. Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have a self model, I have a model of your mind, and I observe behavior and I kind of impute onto you a model and I can generate explanations. So as I say, Thomas did that because he must have wanted to do this. And I guess you could argue that all of this is just a confabulation. It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist. But then there's the question of, well, it's not that it doesn't exist. It's just that your mind is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and the idea of deep learning neural networks with multiple layers. And I think you're right that depth is an important part of our generative models as well, of our brains models of the world. And part of that comes from the fact that the world actually does separate out into a whole different series of temporal scales of things that happen slowly, that contextualize things that happen more quickly, that contextualize things that are even faster than that. And so one good example of depth might be that if you're reading a book, then you have to bear in mind which page you're on within that page, which sentence or which paragraph you want, within paragraph, which sentence, within the sentence, which word, within the word, which letter. And by combining your predictions sort of both down the system that way, but then updating your predictions all the way back up again, you start to be able to make inferences about the overall narrative that you're reading. The other thing you mentioned that I thought was interesting was the idea of confabulation and of how we come to beliefs about other people's behavior. And I think the same thing is also true about our own behavior and sort of making an inference about what we've done. And this comes all the way back to the sense of agency again, doesn't it? It comes back to the idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this, because I had this goal in mind. And to come back to the other question, is that real? Or is it simply an inference about what I've done? I would suggest that it's certainly an inference about what I've done, whether or not it's real. Giovanni and I put together some simulations and some theoretical work a couple of years ago after a discussion at a conference about or a workshop about machine understanding, suggesting that machine intelligence is one thing, but actually understanding why you've come to a particular conclusion. ChatGPT being able to explain to you why it came up with a specific sequence of words or why a convolutional neural network classified an image in a particular way is one of the big issues really, and there are solutions coming up, but it's one of the big issues in the deep learning community as to how you have that transparency in terms of what the models are doing and why they're doing it. Giovanni and I put together some work following that, looking at understanding of our own actions from an active inference perspective, and there it was very much framed as I have a series of hypotheses of things I might do, of reasons why I might do that. And then after observing myself behaving in a particular way, I can then use my own behavior as data that I then have to come up with an explanation for. And it's very interesting to see what happens if you start depriving that of aspects of its behavior and to see the confabulations that result from that. I can't remember where it came from originally, the idea of hallucinations being a perception generally being effectively a constrained hallucination, where you take your hallucination, your simulation of what's going on, and then you fix it to what's actually coming in. But you could argue that actually a lot of our understanding about what we're doing is also just a constrained confabulation in exactly the same way. Yes, which is very ironic because people diminish GPT and because they say it's just confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument about how the brain works, and even our communication now on conditioning your simulator. So the semantics are drawn by your own model in simulation of the world, rather than being the simulacrum of mine. You spoke about machine understanding, I mean, there's this Chinese Rem argument. And we're in a really interesting time now because we have artifacts that behave in a way which is isomorphic in many ways. And it's so tempting to say, well, we're different. And you could make the ontological argument, but this psychological argument is a big one as well, which is we're different because we have beliefs, motives, volition, desires, we have all of these things. But as we were just saying before, this is all post hoc confabulated. We actually don't have consistent beliefs and desires. It's just a fiction. Was it a fiction or is it a plausible explanation? Well, I guess the thing that breaks it for me is the incoherence and inconsistency, because you would think that we would be fully fledged human agents if we had consistent beliefs and desires. And it's not to say that we don't because it feels like some of our goals are they grounded in some way, like we need to eat food. But we think of ourselves as being unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental to eating food. And I guess those things in particular might be confabulatory. Yes. So on the volition thing, that's something that really interests me. An active inference agent is we draw a boundary around a thing and it can act in the environment and it has preferences. And essentially, it has a generative model where it can produce these plans, these policies, if you like, and at the end of every single plan is an end state. So it's got all of these different goals in mind, if you like. And in the real world, real in big air quotes, these things emerge. But when we design these agents, we need to somehow impute the preferences onto them. So it feels like they have less agency if we impute the preferences. Would you agree with that? Interesting question. And a very relevant question in the current number of industry related applications of active inference. I think we were speaking about earlier, there are a number of companies now that have been set up looking at use of active inference based principles for various problems. Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well. And the issue there is very much, it's a different kind of issue to the biological issue of describing how things work. And it's the issue of saying, if I now want to design an agent to behave in a particular way, as you say, am I taking some agency away from that? There are a couple of things to think about there. I suppose one is thinking about do biological agents actually select their own preferences to begin with? And I think most people would probably say they don't most of the time. There may be certain circumstances where they do or where a particular preference might be conditionally dependent upon the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's not that I'm actually selecting this is what I want to want. There is a famous quote here, but I can't remember what it is. I don't know whether you do. No. No, it's escaped me about wanting what you want or wanting what you do or something along those lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us effectively through a process of evolution, natural selection, previous experience that has affected what is a good set of states to occupy. And those will often be a good set of states that help my survival, that help the persistence of the species that I'm a part of. And arguably, the same thing is true when you as a designer of a particular algorithm or an agent are giving it a set of preferences. From its perspective, it's never selected them anyway. And that's the same as you or I not necessarily having selected our preferences. There's one additional element that I think is interesting to think about. And one of my colleagues and collaborators, Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own preferences, of actually saying, let's create an agent that isn't given preferences to begin with, but is allowed to learn as it behaves what sort of goal states it ends up in. And there you get some very interesting results. So she showed that these sorts of agents may end up doing things that you just don't want them to do, that they end up forming a particular pattern of being or a particular way of being that you as a designer might never have envisaged. For example, in an environment with lots of potential holes that it can fall into, some of these agents just become hole dwellers. They just decide, I found that the first few times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe that agency is the ability to sort of disagree with what you as a designer might expect or want from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a discussion about cybernetics and externalism. But so what you're describing there is the reason why AI systems today are not sophisticated is because they are convergent. And that's usually because they don't actually have any agency. So one of the hallmarks of the physical real systems in the real world is that they have these divergent properties. And that's because you have lots of independent agents following their own directiveness doing epistemic foraging. So interesting stepping stones get discovered. And sometimes those stepping stones aren't what the designer of the system would have liked, as you just said. So there's an interesting kind of paradox there of how much agency do you want to imbue in the agents. But the other paradox is the physical and social embeddedness. Because as you just said, cynically, we don't have as much agency as we think we do, because we're embedded in the dynamics around us. And being part of this overall system means that our agency is defined not just by our boundary, but it's by the history of the system. It's the history of us sharing information of all of the things around us. And all of these things inform what we do and what our preferences are. And then you say, well, we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out of water. It's not embedded in the ways that things that emerged in that system were in the first place. But this does get us onto this discussion of externalism. So part of the fiction of how we think about cognition is that we think of ourselves as islands that don't share information dynamically with the outside world. And of course, active inference is a way of bridging these two schools of thought. So can you kind of bring that in? I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness. It's the idea that our brains and our internal state evolves in such a way that reflects beliefs about what's outside. And I think that's one of the key things that you have to have for any sort of intelligent system. And that doesn't necessarily exist with other approaches that exist in neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much being, the aboutness is the key thing that what's happening in my head is a reflection or is a description in some way is about what's happening outside my head. And maybe that's the link with this sort of externalism. But it's not just unidirectional either. It's the fact that I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing the outside world to change it to fit with the beliefs I have about how it should be. Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around us. And we influence the world around us. And that's essentially what active inference is. I guess it might be useful just to sketch out the cognitive science idea of an activism or cybernetic. So there were folks who really railed against this idea of representationalism, which is this idea of model building in principle. And active inference is an integrated approach where we allow some model building, but we also think of the world itself as being its own best representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in the distinction between the sort of inactivists, radical inactivists, the sort of different levels of stance you can take on this. And I think it comes down to that, that from an active inference perspective, both your representations, if that's the right word, the beliefs you have about the world, whether or not that meets the criteria for representation from an inactivist perspective is very important. But it is only important in terms of how you act. If your beliefs did not affect how you acted, clearly natural selection would not have selected you to form those beliefs. I think it's the simple way of putting it. So let's talk about some of the kind of the mathematical underpinnings here. So I think probably one of the main concepts we should start on is this idea of surprise. And maybe we can talk about it in general terms, and then we can move on to Bayesian surprise. So why is surprise so important in the free energy principle? Well, it's central to it. It is the key thing that matters. And we talk about the free energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes, what do we mean by surprise? And it's another one of those things like the high road and the low road that you can approach from several different angles or several different lines of attack. If you were modeling something, if you were a Bayesian, so if you took a particular stance on probability theory and wanted to know, given my model, given my hypothesis, what's the evidence for it? What you would normally do is calculate something known as a marginal likelihood, which is just a measure of the fit between your model and the data that you have that you're trying to explain. That fit trades off various different things. So it can trade off how accurately your model is explaining the data against how far you've had to deviate from your prior beliefs or from your initial assumptions in order to arrive at that explanation. So that marginal likelihood, that evidence is effectively just the negative or the inverse of surprise. So that that's one perspective on it, the better the fit, the simpler and most accurate my explanation for something, the less surprised I will be by it. Another perspective on surprise is just this more colloquial sense. It's the idea that, given what I would predict, how far out of that prediction is it? One could take a more biological perspective on it and say, imagine we are, well, we are homeostatic systems that have some set points. We want to keep our temperature within a certain range, our blood pressure within a certain range, our heart rate within a certain range. If we find ourselves deviating from that, that is effectively a surprise because we're not where we expect to be. And so we enact various changes to bring those parameters back in range. So we might, if our blood pressure is too low, we might increase our heart rate to bring our blood pressure back up to the range we expect it to be in. And that is, in a sense, what active inference is all about. It's just this idea of keeping things within that minimally surprising range. But of course, once you put dynamics on it, once you start unfolding that in time, you end up having to not just deal with how surprising things are now, but you've got to try and anticipate surprise and behave in such a way that you allostatically control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure, etc., but also your extraceptive sensations, your vision, your audition, and the like. And there's almost no end to the perspective you could take on surprise. Another perspective on it is that it's a reflective of, in a physical system, the improbability of being in a particular state. From a lot of physics perspectives, improbability is also associated with energy. It takes energy to bring things into less probable states. And without inputting energy into a system, it will generally end up in its most probable state in the absence of that. You think of things like Boltzmann's equation and the relationship there between energy and probability. And that also has a link then to the idea of either a Hamiltonian or indeed a steady state distribution, which is just what is the distribution things will end up in if left to their own devices for a certain amount of time until things have probabilistically converged. And that means that if I would construct a probability distribution over where things will be at a long point of time in the future, there will come a point at which that probability won't change any further. And the tendency of physical systems to go to those more probable states is exactly the same as the tendency to avoid surprising states. And again, we could sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully that sort of explains why it's such an important thing that underpins so much of what we do. We're either trying to sort of evolve as a physical system towards more probable states. Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within the right set points. Or we are more colloquially just trying to avoid things that are different to what we predict. Or we are statisticians trying to fit our model to the world as best we can. And all of those things come under the same umbrella of surprise. Free energy comes in because surprise is not a trivial thing to compute. Mathematically, it's often either intractable mathematically or computationally. And so it's just not efficient to be able to calculate. But free energy is a way of then approximating that surprise. It's a way of coming up with something that is close enough to it. Or even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy, then that limits how high your surprise can be. The key additional thing in free energy is that the distance between that bound, your free energy and your surprise depends on how good your beliefs about the world are. And that's where perception comes in. That by getting the best beliefs you possibly can, you minimize the distance between your free energy and which is up a bounding of surprise and the surprise itself. So then any further reduction in free energy, you would expect to also result in a decrease. Sorry, any further decrease in free energy would also result in a further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all, what struck me is that we're using the language of things like statistical mechanics and Bayesian statistics and information theory, things like entropy and so on. And we're interchangeably kind of speaking about the same thing from the perspective of different disciplines, which I find very, very interesting. And on the surprise thing, even though in this formalism, we are minimizing surprise, I think there's an interesting perspective that sometimes surprise is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when something surprising happens that the weights get updated because it's information. Or people on YouTube, my videos are that they get more views when they have a cash value, which means they have information content, which means that, you know, they're actually surprising your predictive model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles. You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So there's this interesting juxtaposition between actually seeking out surprise, even though you can think of our brains overall as minimizing surprise. And what was the other thing I was going to say? Yeah, you were just getting onto variational inference, which is really interesting. So there's a couple of intractable statistical quantities in this mixture that we're talking about. I think it's the log model evidence and the Bayesian posterior. And we can't represent those things directly. So we have to put a proxy in there, which kind of captures most of the information, but it's still possible to deal with it. So how does this variational inference work? Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian inference is. I suppose we've been talking about it quite a lot without necessarily defining it. And many of you listeners, I'm sure, will know already. But the idea is actually relatively straightforward and well-established and quite widely used. And it's the idea that if I have some beliefs about things that are in my world that I can't directly observe, I may have a sense of what's plausible to begin with. And that's what we refer to as a prior probability. I then also need to have a model that says, given the world is this way, what would I expect to actually observe? So for instance, given where you are relative to me, I can predict a certain pattern on my retina. And if you were somewhere else, I would expect a different pattern on my retina. So I might have a prior range of plausibilities as to where you are relative to me. And then I have a model that explains how I'm going to generate some data based upon that. And Bayesian inference basically takes those two things and inverts them using Bayes' theorem and effectively just flips both of them round. So you now say instead of a distribution of where you are relative to me, I'm now talking about a distribution of all the possible things that I could see on my retina. And instead of predicting the distribution on the retina given where you are, I now want to know the distribution of where you are given what's on my retina. And Bayesian inference, much like active inference, is full of all these interesting inversions where you sort of flip things round from how they initially appeared. But the problem is calculating those two things, calculating the flipped model. So the distribution of all the things on my retina here would now be my model evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina is my posterior distribution. But those things are not always straightforward to calculate. And so variational inference takes that problem and makes it into an optimization problem. It writes down a function that quantifies how far am I away from my, or what would be the true posterior if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior probability. So come up with a function that represents a probability distribution that's easy to characterize, something like a Gaussian distribution where I know I just need my mean and my variance. And then just changes that mean and variance until you minimize this function that represents that discrepancy, minimize this free energy, also sometimes known as an evidence lower bound, in which case you maximize it. And interestingly, once you've maximized your evidence lower bound or minimized your free energy, you end up with a situation where the free energy starts to approximate your log model evidence or your negative log surprise. And your approximate posterior distribution, your variational distribution starts to look much more like your exact posterior probability distribution. So it's another one of those interesting scenarios where doing one thing optimizing one quantity ends up having a dual purpose. And in active inference, the only additional thing you throw into that is that you want to then also change your data itself. So you do the third thing you act on the world to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting to machine learning again. So in machine learning, we also have these big parameterized models and we do stochastic gradient descent. And some might think of deep learning, because obviously you can think of everything as a Bayesian. So you can think of machine learning as being maximum likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not something like maximum likelihood estimation? It's an interesting question. And there are a couple of answers you could give again, some of which are more technical, but some of which are some of which are slightly more intuitive. And I think one of the more intuitive answers is that by having an expression of plausibility of things in advance, you just maintain things within a plausible region. So maximum likelihood for those who are unaware is where you essentially throw away that prior probability, where you throw away any prior plausibility as to as to what the state of the world might be. And you just try and find the value that would maximize your likelihood, which is your prediction of how things would be under some hypothesis or under some parameter setting. And I think the first thing to say is if you throw away that prior information, then you end up potentially coming up with quite implausible solutions. That's particularly relevant if you're dealing with what's known as an inverse problem. So where there are multiple different things that could have caused the same outcome. An example that's often given is that for any given shadow, there's almost an infinite number of things, configurations of the sun and the shape of the thing that's casting the shadow that could lead to exactly the same shadow. And so maximum likelihood approach just won't be able to tell the difference between all of those things. However, if you have some prior on top of that, if you have some statement of the plausible things that might cause it, you can come up with a much better estimate of those sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood estimate, you're throwing away all uncertainty about the solution. So you're coming up with a point estimate and you're saying this is the most likely thing, but you're ignoring all of your uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to the problem of overfitting, where you start to become very confident about what you can see from a relatively small sample of things and you can end up with all of these well-described in the media scenarios of complete misclassifications based upon that sort of overconfidence just because all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about what a free energy is. So free energy is our measure of our marginal likelihood that we're using when we're doing Bayesian inference. And one way of separating out what a free energy looks like is to have our complexity, which is effectively how far we needed to deviate from our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit our model. Accuracy is common to both maximum likelihood type approaches because we're trying to find the value that most accurately predicts our data and also to Bayesian approaches. Both want to do that. But what's thrown away in the maximum likelihood type approach is the complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor, the idea that the simplest explanation is a priori more likely that you get from a Bayesian approach that you throw away when you're dealing with maximum likelihood estimation. I wondered to what extent does the active part play a role here. So even in machine learning, there's something called active learning, where you dynamically retrain the model, or there's something called machine teaching, where you dynamically select more salient data to train the model, and the model gets much better. And in things like Bayesian optimization, for example, by maintaining this distribution of all of your uncertainty in a principled way, you can go and seek and find more information to kind of improve your knowledge on subsequent steps. So I guess it's sort of bringing in this idea of it's not just what happens now, it's about how can I improve my knowledge of the world over several steps. Yes, and that reminds me about the point you were making earlier, that sometimes we actually do things to surprise ourselves, which seems very counter-intuitive in the context of the idea that we're trying to minimize surprises as our sole objective in life. And sometimes people talk about this in terms of a dark room problem, the idea that actually if all you want to do is minimize your surprise, you just go into a room, turn off the lights and stay there because you're not going to experience anything that's going to surprise you. I mean, the answer to this problem is that actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the sort of organism that would be is, again, probably not a very interesting one. And that what we predict, what we'd be surprised by might be permanently staying in a dark room. But it goes even further than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable world where I know what's going to happen next. The best way of doing that is to actually gather as much information as you can about the world around you. So the first thing you do really is you turn on the light and see what the room looks like, because that might then predict all the sorts of things that could fall on you in that room and could potentially cause surprise. And by knowing about it, you mitigate the surprise that you might get in the future. And as you say, you can only really do that if you know what you're certain about. And so if you take a maximum likelihood approach, if you work based on point estimates and you have no measure of your uncertainty, then there's no way you can possibly know what you're uncertain about to be able to resolve that uncertainty. So this brings me on to causality. We know that predictive systems, which are aware of causal relationships, work better. But if we just bring it back to physics first, I mean, to you, what do you think causality is? It is a tricky issue as to what causality is. And I think whether it exists or not is really a matter of how you define it, isn't it? And some would define that purely in terms of conditional dependencies, that the behavior of one thing is conditionally dependent upon something else, and therefore you could say that the one thing causes the other. But as we know from Bayes' theorem, that's not quite good enough, because you can swap any conditional relationship around through that process of inverting your model. Sometimes that causality is written into the dynamics of a model. So this would be the approach used in things like dynamic causal modeling of brain data, where you might say that the current neural activity in one area of the brain affects maybe the rate of change of neural activity in another part of the brain. And it's the way in which those dynamics are written in, the fact that it's one affects the rate of change of the other, that gives it that causal flavor and a very directed perspective on it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and a lot of his work on causality. There's a lot of detail about the notion of an intervention. And I suppose you can think of this in terms of how you might establish causation in a clinical context. If you were to run a trial to try and establish whether one thing's caused another, you need to make sure you're not inadvertently capturing a correlation or a conditional dependence that could go either way, or a common cause of both things that depends upon something else. And typically the way you do that is you intervene on the system. You randomize at the beginning to make sure that people are assigned to different treatment groups at random, so that you break that dependency upon something prior to it. And then anything that happens going forward is going to depend on the intervention that you're doing. So I think that's probably the key thing that gives you causality or perhaps defines causality. It's the idea that an intervention is what will change it. If you intervene in one thing, that should then in a way that doesn't necessarily match its natural distribution if you hadn't intervened at all, and then see what the effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study his book, The Book of Why. It's one thing that we've really dropped the ball on, actually. But I suppose one way to think about it is if you go back to the core physical, in physics, there's a whole bunch of equations to describe the world we live in. And those equations don't have, they don't say anything about causality, and they're even reversible. And then you can think, okay, well, maybe it's a little bit like the free energy principle. It's a lens, like really, there's only dynamics. But when you look at these dynamical systems, then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality, and it's something which is statistically efficacious to build it into our models. But where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern of how one thing reacts to another. So if you imagine you've got the classic physics example, billiard balls bouncing into one another, how do you know that the collision of one ball with another is causative of the subsequent motion of the second ball? And you could argue that that's due to a particular pattern of which variables affect which other variables and the particular exchange between them. And this comes back quite nicely to things like the physics perspective on the free energy principle, the idea that actually one could see the location of a particular ball as being, you know, maybe it's internal state, and then the action that that then causes is perhaps the, or in fact, you could say that the action is the position of the ball, the force that results from that action is the sensory state of the next ball, which then changes its velocity to then change its action relative to something else. You can sort of rearrange those labels slightly, but there is a directional element to it. And in that sort of pattern of causation, you really do expect the position of one ball to have an effect on the rate of change, or in fact, even the rate of rate of change of the second ball, which again, I think brings us back to those kinds of dynamical descriptions of causality where one thing might affect how another thing changes. So you almost get it from the dynamics itself. But again, to some extent, it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose cause is a hypothesis as to a particular configuration of things. But then you've got to write down what does that hypothesis mean? What's my model of what a causation involves? Yes, yes. I mean, we were just talking about, you know, build building these models. And one of the bright differences from machine learning is that we need to build a generative model by hand. So we have to define these these variables, and some of them are presumably observed, and some of them are not observed. They're inferred. And that process seems like you would need to have a lot of domain expertise. And it seems like something which is at least has a degree of subjectivity. I mean, we were just talking about causality, for example, there are many ways you could model the risk of cancer from smoking. It seems like there are many, many different ways of building those models. So that subjectivity is interesting. I mean, are there principled ways of building these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to which model minimizes the surprise the best. And but there are interesting questions amongst that. So how do you actually choose the space of models that you want to compare? So you're right to say that that that often there is some specific prior information that's put into models and active inference. And very often we do end up sort of building models by hand to demonstrate a specific outcome or a specific cognitive function. But there's no reason why it has to be that way. You can build models through exposure to data, where where the models are selecting the data to best build themselves. But the question is how you do that, how you start to add on additional things, how you start to change the structure of your model. But there's a lot of ongoing research into that. And I think there are now methods that are coming out that will allow you to allow an active inference model to build itself. And the way it will do that will be sort of adding on additional states and potential causes, adjusting beliefs about the mappings and the distributions and the parameters of given this than that, adding an additional paths that different or different transitions that systems will pursue. So it's a fascinating area. I think it's one that's still a growing area. But it's this idea of structure learning of comparing each alternative model based upon its free energy or model evidence or surprise as a way of minimizing that by being able to better predict things. Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all, via abduction, we can select relevant models to explain behavior, you know, what we observe. But we also have the ability to create models. In fact, I think of intelligence as the ability to create models. So we experience something. And I now construct a model to explain this and similar experiences in experience space. But in a machine, it's really difficult. So in machine learning, there's this bias variance trade off. So we deliberately reduce the size of the approximation space to make it computationally tractable. And when we're talking about building these models, just from observational data, it feels like there's an exponential blow up of possible models. So I can imagine there might be a whole bunch of heuristics around library learning or having modules. So these modules have worked well over there. So we'll try composing together known modules rather than starting from scratch every single time. I mean, what kind of work is being done there? I mean, I think I think you're right about, you know, it's not going to be worth starting from scratch every time. You can sort of build models by saying, okay, let's start with something very simple with a sort of known structure. And I think it's sensible to use some priors in that rather than starting from complete, completely nothing, because there are some things that we know about in the world. And there's no point hiding that from the models we're trying to build. And that might be a simple structural thing like things evolve in time. So one thing is conditioned upon the next is conditioned upon the next. And things now will influence the data I observe things well in the past might not anymore. But then then there's the question of, well, how can a model then grow? What are the things that you can add to it or subtract from it? And subtraction is another key element. Because you could take this whole problem from the other direction, and you could say, well, let's start with a model that just has everything in it and take away bits until we've got the model that's relevant to where we are at the moment. And we know that during development, there's a lot of synaptic pruning that goes on and removal of synapses that we have when we're much younger compared to compared to as you get older. So what can you add on? Well, it depends what your model looks like. So if your model says there's a set of states that can evolve over time, there are a set of outcomes that are generated, well, we know what the outcomes are, we know what the data are, because we know what our sensory organs are. So it's the states that are going to change so do we add in more states? Do we allow them to take more alternative values? Do we allow their transitions to change in more than one different way? Which ones can I change? Which ones can I not change? And it's really just asking these questions that helps you to grow your model. So you say, well, let's try it. If I allow this state to take additional values, if it's not providing a sufficiently good explanation for how things are at the moment. And if that improves your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need to include additional state factors? So you could either say there is one sort of state of the world that can take multiple different values, or you could actually this is contextualized by something completely separate. So where am I along an x coordinate? You also need to know where you are along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in a model? How do you build a model almost gives you the answers to the ways or the directions in which you can grow it. The other thing you can then do when you're trying to work out how to grow it is to say, well, let's treat this as the same sort of problem as exploring my world, selecting actions that will then give me more information about the world. You could say, well, actually, now let's treat my exploration of model space as being a similar process of exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping between what I'm predicting or what's in my world and what I'm currently predicting? Yes, it rather brings me back to our comments about the space or the manifold that the models sit on, whether they would have a kind of contiguity or whether they would have a gradient. I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether it's worth bringing in. Obviously, you're a neuroscientist and the way brains work, we must do this. Of course, there's this idea of nativism. Some psychologists think that we have these models built in from birth and then the other school of thought is that we're just a complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical thing that just builds models on the fly. But perhaps one difference at least between brains and machines is the multi-modality, which is to say we have so many different senses that creates a gradient or that makes it tractable. Because when a model from a particular sensation and starts predicting well, we can rapidly optimise and go in the right direction. Because the problem seems to be that there are so many directions where we can go in, doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the search space, so we've wasted our time. Yeah, I think that's a really good point, absolutely. As soon as you know how one thing works or how vision works, I suppose vision and proprioception is a good example, isn't it? If I recognise where my hand is and I can make a good estimate of that visually, then that helps me tune my joint position sense as to where my arm might be. And it's always fascinating to see situations where that breaks down, so there are a number of conditions where if you lose your joint position sense, you're perfectly okay holding your arm out like that until you close your eyes, at which point you start getting all these interesting twitches and changes. So yes, the multimodality I think probably is a really key thing that really does help constrain the other senses because you're just getting more information about each thing. Maybe we should just talk about chapter 10 in general, because that was kind of like the homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together a lot of the themes that had been discussed earlier on, but to also make the point that, well, I'll come back to one of the things you said earlier was about how it seems we're talking about lots of different things from different perspectives, but actually they're really the same thing. So we talked about how surprise is also a measure of steady state of energies of various sorts of of statistics and model comparison of homeostatic set points, you know, that all of these things can be seen through the same lens. But again, taking one of those inversions, you can invert that lens and say, well, actually, you can start from the same thing and now project back into all of these different fields. And I think that's a useful thing to do because I think it helps foster multidisciplinary work, helps to engage people from different fields and areas, and helps us know what's happening elsewhere so that you're not just duplicating everything that people have already done. So I think it's really important to have those connections to different areas. And the chapter 10 from the book was an aim to try and connect to those different areas, whether it be to things you've spoken about, like cybernetics and inactivism, and just to try and understand the relationship between each of them. Well, I mean, quite a lot of people use this as a model of, you know, just things like sentience and consciousness in general. And I often speak about the strange bedfellows of the free energy principle. So, you know, there are, you know, autopoietic and activists and phenomenologists and, you know, people talking about sentience and consciousness, you know, obviously you're a clinician, you know, you're working in a hospital. So it's just this incredible conflation of different people together, and they all bring their own lexicon with them. But maybe we should just get on to this kind of sentience and consciousness thing, because that seems quite mysterious. We almost come back to one of the themes we've spoken about a few times, which is that the specific words we use for things in the effect that different people, that has on different people. So some people, I think, would probably get very angry with the idea of using sentience to describe some of the sort of simulations and models that we would develop. But that comes down to what you mean by sentience. And I think one of the key things for sentience is the aboutness we were talking about before. The idea that our brains or any sentient system really is trying to try not to anthropomorphise too much, but it's almost impossible to do in this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system are reflective of what's going on external to it, and that you can now start to see those dynamics as being optimization of beliefs. And those beliefs are about what's happening in the outside world and about how I'm affecting the outside world. And I think that probably gets to the root of at least a definition of sentience and one that I'd be happy with, which is just the dynamics of beliefs about what's external to us and how we want to change it. And there are very few things other than that sort of inferential formalism that give you that. Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists, so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are those who believe that these kind of qualities that we're speaking about, certainly with conscious experience, for example, that it's not reducible to these kind of simple explanations that we're talking about, that it has a different character. David Chalmers talks about a philosophical zombie. So for example, you might behave just like a real human being, but you could be divorced of conscious experience. So he says that you can think of behavior, dynamics, and function, and conscious experience as something entirely different. But as an observer, you would never know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions like consciousness as well, I mean, I think it does become very, very difficult, because once you're putting forward or advocating a theoretical framework that seems like it's supposed to have all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be able to ask the right questions or to be able to articulate your hypotheses. So if you think that consciousness is based upon the idea of having some sense of trajectory of temporal extent and different worlds I can choose between or different futures I can choose between, that might be a key part of it. But for some people, that's not what they mean by consciousness. I found in a particular reading books by people like Anil Seth on this sort of topic, I found one of the interesting comparisons being the questions about consciousness versus questions about life. And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious, just because we've had so much of an understanding of the processes involved in life, the dynamics of life and the way biology works, it's still much more to go. But the question of what life is just doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions that were being posed. And perhaps we'll see the same thing with questions like consciousness. Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting thought experiment just to get someone to explain just an everyday thing, you know, like what happens when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and incomplete the explanations are. And it's the same thing with life, it's the same thing of consciousness, it's the same thing of causality, agency, intelligence, all of these different things. And I guess most people don't spend time digging into their understandings of these things and realizing how incoherent and incomplete they are. Life is quite an interesting one in particular, because I think one of the achievements of active inference is blurring the definition of or the demarcation between things which are and are not alive. For example, the orthopedic anactivists, they think of biology as being instrumental. And what the, you know, free energy principle does in my opinion, is it removes the need for this, it almost removes the need for biology entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that point, though, I think many of our ideas about the world are quite incoherent. Yeah. And I think it's interesting that, you know, one of the things that you're saying, and I would agree with you as one of the big advantages of active inference-based formalisms, you'll probably find some people will say, that's a problem with it, that actually there is a clean distinction in their mind between these different things. But then I think the challenge is to work out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't exist in somebody else's mind. And so getting people to try and or trying to support people to be able to express that in a very precise mathematical hypothesis, I think is quite a useful way of trying to explore those problems. Because clearly, for some people, there is something that's getting at it that is not quite explaining. And it's interesting to try and explore that and to work out what that thing is. Indeed, indeed. And just final question, what was your experience writing a book? And would you recommend it to other people? I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of the time compared to, you know, I think anyone who's had some experience of writing papers will often find that at the point where you're ready to submit it, you're just sick of it and want to see the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments writing a book, it obviously takes you much longer. So you end up being almost more sick of it at various times. But it's quite fun as a collaborative project. It's quite interesting to get other people's perspectives on it. And I was lucky to have great collaborators to write it with. And I think it really is a good way of organizing your thoughts in a slightly more holistic way than you would while focusing on a very specific topic in a research paper. And I've also just enjoyed the response I've had from people who've read it, some of whom have picked out a number of errors, not many. But generally, everybody's been very supportive of that and people seem to have responded well to it, which I think is always encouraging. And that's what we hope should happen. Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.44, "text": " So, welcome back to MLST.", "tokens": [50364, 407, 11, 2928, 646, 281, 21601, 6840, 13, 50436], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 1, "seek": 0, "start": 1.44, "end": 7.32, "text": " Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo,", "tokens": [50436, 2692, 11, 321, 434, 516, 281, 312, 1417, 466, 341, 1446, 538, 2491, 13, 8500, 47890, 11, 47089, 35832, 430, 9112, 858, 78, 11, 50730], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 2, "seek": 0, "start": 7.32, "end": 9.32, "text": " and Professor Carl Friston.", "tokens": [50730, 293, 8419, 14256, 1526, 47345, 13, 50830], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 3, "seek": 0, "start": 9.32, "end": 15.280000000000001, "text": " Now the book is Active Inference, the free energy principle in mind, brain, and behavior.", "tokens": [50830, 823, 264, 1446, 307, 26635, 682, 5158, 11, 264, 1737, 2281, 8665, 294, 1575, 11, 3567, 11, 293, 5223, 13, 51128], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 4, "seek": 0, "start": 15.280000000000001, "end": 19.68, "text": " So the book, from a pedagogical perspective, it's describing active inference from the", "tokens": [51128, 407, 264, 1446, 11, 490, 257, 5670, 31599, 804, 4585, 11, 309, 311, 16141, 4967, 38253, 490, 264, 51348], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 5, "seek": 0, "start": 19.68, "end": 22.400000000000002, "text": " high road and the low road.", "tokens": [51348, 1090, 3060, 293, 264, 2295, 3060, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 6, "seek": 0, "start": 22.400000000000002, "end": 26.92, "text": " And the high road is a little bit kind of helicopter view, so it's saying, OK, we've", "tokens": [51484, 400, 264, 1090, 3060, 307, 257, 707, 857, 733, 295, 19803, 1910, 11, 370, 309, 311, 1566, 11, 2264, 11, 321, 600, 51710], "temperature": 0.0, "avg_logprob": -0.2351597872647372, "compression_ratio": 1.5672727272727274, "no_speech_prob": 0.004749365616589785}, {"id": 7, "seek": 2692, "start": 26.92, "end": 33.64, "text": " got these biological organisms or these living systems, and what do they do in order to be", "tokens": [50364, 658, 613, 13910, 22110, 420, 613, 2647, 3652, 11, 293, 437, 360, 436, 360, 294, 1668, 281, 312, 50700], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 8, "seek": 2692, "start": 33.64, "end": 34.64, "text": " living systems?", "tokens": [50700, 2647, 3652, 30, 50750], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 9, "seek": 2692, "start": 34.64, "end": 40.56, "text": " Well, they resist entropic forces acting on them by minimizing their free energy.", "tokens": [50750, 1042, 11, 436, 4597, 948, 39173, 5874, 6577, 322, 552, 538, 46608, 641, 1737, 2281, 13, 51046], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 10, "seek": 2692, "start": 40.56, "end": 44.8, "text": " So it goes into the how question, but it also goes into the why question from a helicopter", "tokens": [51046, 407, 309, 1709, 666, 264, 577, 1168, 11, 457, 309, 611, 1709, 666, 264, 983, 1168, 490, 257, 19803, 51258], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 11, "seek": 2692, "start": 44.8, "end": 45.8, "text": " view.", "tokens": [51258, 1910, 13, 51308], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 12, "seek": 2692, "start": 45.8, "end": 51.08, "text": " The low road of active inference is far more mechanistic, far more mathematical, and obviously", "tokens": [51308, 440, 2295, 3060, 295, 4967, 38253, 307, 1400, 544, 4236, 3142, 11, 1400, 544, 18894, 11, 293, 2745, 51572], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 13, "seek": 2692, "start": 51.08, "end": 54.64, "text": " both all of the roads lead to Rome, if you like.", "tokens": [51572, 1293, 439, 295, 264, 11344, 1477, 281, 12043, 11, 498, 291, 411, 13, 51750], "temperature": 0.0, "avg_logprob": -0.15061045147123792, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.004740833770483732}, {"id": 14, "seek": 5464, "start": 54.64, "end": 58.2, "text": " But the low road is talking about things like Bayesian mechanics, there's a primer", "tokens": [50364, 583, 264, 2295, 3060, 307, 1417, 466, 721, 411, 7840, 42434, 12939, 11, 456, 311, 257, 12595, 50542], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 15, "seek": 5464, "start": 58.2, "end": 62.52, "text": " on probability theory, talking about things like variational inference, which is the way", "tokens": [50542, 322, 8482, 5261, 11, 1417, 466, 721, 411, 3034, 1478, 38253, 11, 597, 307, 264, 636, 50758], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 16, "seek": 5464, "start": 62.52, "end": 69.32, "text": " that we solve these intractable optimization problems in active inference, and also talking", "tokens": [50758, 300, 321, 5039, 613, 560, 1897, 712, 19618, 2740, 294, 4967, 38253, 11, 293, 611, 1417, 51098], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 17, "seek": 5464, "start": 69.32, "end": 74.52, "text": " about framing active inference as a process theory, which is the latest incarnation of", "tokens": [51098, 466, 28971, 4967, 38253, 382, 257, 1399, 5261, 11, 597, 307, 264, 6792, 49988, 295, 51358], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 18, "seek": 5464, "start": 74.52, "end": 77.44, "text": " the description of active inference.", "tokens": [51358, 264, 3855, 295, 4967, 38253, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 19, "seek": 5464, "start": 77.44, "end": 80.84, "text": " So Professor Carl Friston wrote a preface for the book.", "tokens": [51504, 407, 8419, 14256, 1526, 47345, 4114, 257, 659, 2868, 337, 264, 1446, 13, 51674], "temperature": 0.0, "avg_logprob": -0.10712183136300943, "compression_ratio": 1.8851063829787233, "no_speech_prob": 0.0003796672390308231}, {"id": 20, "seek": 8084, "start": 80.84, "end": 85.04, "text": " He said, active inference is a way of understanding sentient behavior.", "tokens": [50364, 634, 848, 11, 4967, 38253, 307, 257, 636, 295, 3701, 2279, 1196, 5223, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 21, "seek": 8084, "start": 85.04, "end": 91.76, "text": " The very fact that you are reading these lines means that you are engaging in active inference,", "tokens": [50574, 440, 588, 1186, 300, 291, 366, 3760, 613, 3876, 1355, 300, 291, 366, 11268, 294, 4967, 38253, 11, 50910], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 22, "seek": 8084, "start": 91.76, "end": 98.52000000000001, "text": " namely actively sampling the world in a particular way, because you believe you will learn something.", "tokens": [50910, 20926, 13022, 21179, 264, 1002, 294, 257, 1729, 636, 11, 570, 291, 1697, 291, 486, 1466, 746, 13, 51248], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 23, "seek": 8084, "start": 98.52000000000001, "end": 100.64, "text": " You are palpating.", "tokens": [51248, 509, 366, 3984, 79, 990, 13, 51354], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 24, "seek": 8084, "start": 100.64, "end": 102.04, "text": " This is beautiful, by the way.", "tokens": [51354, 639, 307, 2238, 11, 538, 264, 636, 13, 51424], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 25, "seek": 8084, "start": 102.04, "end": 107.28, "text": " Friston uses the most beautiful language, it's his signature, if you like, it's his", "tokens": [51424, 1526, 47345, 4960, 264, 881, 2238, 2856, 11, 309, 311, 702, 13397, 11, 498, 291, 411, 11, 309, 311, 702, 51686], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 26, "seek": 8084, "start": 107.28, "end": 108.52000000000001, "text": " calling card.", "tokens": [51686, 5141, 2920, 13, 51748], "temperature": 0.0, "avg_logprob": -0.14625793753318417, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.27686992287635803}, {"id": 27, "seek": 10852, "start": 108.52, "end": 113.03999999999999, "text": " He said, you are palpating this page with your eyes, simply because this is the kind", "tokens": [50364, 634, 848, 11, 291, 366, 3984, 79, 990, 341, 3028, 365, 428, 2575, 11, 2935, 570, 341, 307, 264, 733, 50590], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 28, "seek": 10852, "start": 113.03999999999999, "end": 118.47999999999999, "text": " of action that will resolve uncertainty about what you're going to do next, indeed what", "tokens": [50590, 295, 3069, 300, 486, 14151, 15697, 466, 437, 291, 434, 516, 281, 360, 958, 11, 6451, 437, 50862], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 29, "seek": 10852, "start": 118.47999999999999, "end": 120.8, "text": " these words convey.", "tokens": [50862, 613, 2283, 16965, 13, 50978], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 30, "seek": 10852, "start": 120.8, "end": 127.47999999999999, "text": " In short, he said, active inference puts action into perception, whereby perception is treated", "tokens": [50978, 682, 2099, 11, 415, 848, 11, 4967, 38253, 8137, 3069, 666, 12860, 11, 36998, 12860, 307, 8668, 51312], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 31, "seek": 10852, "start": 127.47999999999999, "end": 131.12, "text": " as perceptual inference or hypothesis testing.", "tokens": [51312, 382, 43276, 901, 38253, 420, 17291, 4997, 13, 51494], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 32, "seek": 10852, "start": 131.12, "end": 136.68, "text": " Active inference goes even further and considers planning as inference, that is inferring what", "tokens": [51494, 26635, 38253, 1709, 754, 3052, 293, 33095, 5038, 382, 38253, 11, 300, 307, 13596, 2937, 437, 51772], "temperature": 0.0, "avg_logprob": -0.12764398256937662, "compression_ratio": 1.7510204081632652, "no_speech_prob": 0.02093775011599064}, {"id": 33, "seek": 13668, "start": 136.68, "end": 141.52, "text": " you're going to do next to resolve uncertainty about your lived world.", "tokens": [50364, 291, 434, 516, 281, 360, 958, 281, 14151, 15697, 466, 428, 5152, 1002, 13, 50606], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 34, "seek": 13668, "start": 141.52, "end": 146.44, "text": " So I'm about to show you a sneaky clip of Professor Friston that we filmed in January.", "tokens": [50606, 407, 286, 478, 466, 281, 855, 291, 257, 39518, 7353, 295, 8419, 1526, 47345, 300, 321, 15133, 294, 7061, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 35, "seek": 13668, "start": 146.44, "end": 151.12, "text": " I might publish the full show on MLSD in the future, but I just want to take this as an", "tokens": [50852, 286, 1062, 11374, 264, 1577, 855, 322, 376, 19198, 35, 294, 264, 2027, 11, 457, 286, 445, 528, 281, 747, 341, 382, 364, 51086], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 36, "seek": 13668, "start": 151.12, "end": 154.08, "text": " opportunity to thank you so much for all of our Patreon supporters.", "tokens": [51086, 2650, 281, 1309, 291, 370, 709, 337, 439, 295, 527, 15692, 17683, 13, 51234], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 37, "seek": 13668, "start": 154.08, "end": 159.76000000000002, "text": " Honestly, it means so much to me because the last few months I've just been, you know,", "tokens": [51234, 12348, 11, 309, 1355, 370, 709, 281, 385, 570, 264, 1036, 1326, 2493, 286, 600, 445, 668, 11, 291, 458, 11, 51518], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 38, "seek": 13668, "start": 159.76000000000002, "end": 164.88, "text": " trying to make this activity of mine, this passion of mine, a full-time job.", "tokens": [51518, 1382, 281, 652, 341, 5191, 295, 3892, 11, 341, 5418, 295, 3892, 11, 257, 1577, 12, 3766, 1691, 13, 51774], "temperature": 0.0, "avg_logprob": -0.14135239201207314, "compression_ratio": 1.6006711409395973, "no_speech_prob": 0.5249924659729004}, {"id": 39, "seek": 16488, "start": 164.88, "end": 167.96, "text": " And it's not just because you love the show and you want to support me, you get early", "tokens": [50364, 400, 309, 311, 406, 445, 570, 291, 959, 264, 855, 293, 291, 528, 281, 1406, 385, 11, 291, 483, 2440, 50518], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 40, "seek": 16488, "start": 167.96, "end": 171.68, "text": " access to content, you can join our private Patreon Discord.", "tokens": [50518, 2105, 281, 2701, 11, 291, 393, 3917, 527, 4551, 15692, 32623, 13, 50704], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 41, "seek": 16488, "start": 171.68, "end": 175.92, "text": " We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you", "tokens": [50704, 492, 362, 3228, 12, 23188, 356, 5498, 689, 321, 11, 291, 458, 11, 751, 466, 439, 7527, 295, 4974, 1507, 293, 291, 50916], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 42, "seek": 16488, "start": 175.92, "end": 178.16, "text": " also get early access to lots of our content.", "tokens": [50916, 611, 483, 2440, 2105, 281, 3195, 295, 527, 2701, 13, 51028], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 43, "seek": 16488, "start": 178.16, "end": 179.76, "text": " So, you know, please check that out.", "tokens": [51028, 407, 11, 291, 458, 11, 1767, 1520, 300, 484, 13, 51108], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 44, "seek": 16488, "start": 179.76, "end": 184.44, "text": " But in the meantime, here's a little sneaky clip from Professor Friston.", "tokens": [51108, 583, 294, 264, 14991, 11, 510, 311, 257, 707, 39518, 7353, 490, 8419, 1526, 47345, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 45, "seek": 16488, "start": 184.44, "end": 192.96, "text": " The neural network is a generative model of the way in which its content work was generated.", "tokens": [51342, 440, 18161, 3209, 307, 257, 1337, 1166, 2316, 295, 264, 636, 294, 597, 1080, 2701, 589, 390, 10833, 13, 51768], "temperature": 0.0, "avg_logprob": -0.15707119750976561, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.02206331118941307}, {"id": 46, "seek": 19296, "start": 192.96, "end": 199.08, "text": " And its only job is effectively to learn to be a good model of the content that it has", "tokens": [50364, 400, 1080, 787, 1691, 307, 8659, 281, 1466, 281, 312, 257, 665, 2316, 295, 264, 2701, 300, 309, 575, 50670], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 47, "seek": 19296, "start": 199.08, "end": 202.12, "text": " to assimilate.", "tokens": [50670, 281, 8249, 48104, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 48, "seek": 19296, "start": 202.12, "end": 205.28, "text": " If you put agency into the mix, you get to active inference.", "tokens": [50822, 759, 291, 829, 7934, 666, 264, 2890, 11, 291, 483, 281, 4967, 38253, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 49, "seek": 19296, "start": 205.28, "end": 212.20000000000002, "text": " And now that we've got a generative model that now has to decide which data to go and", "tokens": [50980, 400, 586, 300, 321, 600, 658, 257, 1337, 1166, 2316, 300, 586, 575, 281, 4536, 597, 1412, 281, 352, 293, 51326], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 50, "seek": 19296, "start": 212.20000000000002, "end": 213.20000000000002, "text": " solicit.", "tokens": [51326, 23665, 270, 13, 51376], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 51, "seek": 19296, "start": 213.20000000000002, "end": 218.0, "text": " And that's actually quite a key move and also quite a thematic move.", "tokens": [51376, 400, 300, 311, 767, 1596, 257, 2141, 1286, 293, 611, 1596, 257, 552, 2399, 1286, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 52, "seek": 19296, "start": 218.0, "end": 220.12, "text": " So we're moving from perception machines.", "tokens": [51616, 407, 321, 434, 2684, 490, 12860, 8379, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1753060531616211, "compression_ratio": 1.6283185840707965, "no_speech_prob": 0.0854249894618988}, {"id": 53, "seek": 22012, "start": 220.12, "end": 225.8, "text": " We're moving from sort of neural networks in the service of, say, face recognition into", "tokens": [50364, 492, 434, 2684, 490, 1333, 295, 18161, 9590, 294, 264, 2643, 295, 11, 584, 11, 1851, 11150, 666, 50648], "temperature": 0.0, "avg_logprob": -0.15282203091515434, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.002955019474029541}, {"id": 54, "seek": 22012, "start": 225.8, "end": 235.68, "text": " a much more natural science problem of how would you then choose which data in a smart", "tokens": [50648, 257, 709, 544, 3303, 3497, 1154, 295, 577, 576, 291, 550, 2826, 597, 1412, 294, 257, 4069, 51142], "temperature": 0.0, "avg_logprob": -0.15282203091515434, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.002955019474029541}, {"id": 55, "seek": 22012, "start": 235.68, "end": 243.04000000000002, "text": " way you go and solicit in order to build the best models of the causes of the data that", "tokens": [51142, 636, 291, 352, 293, 23665, 270, 294, 1668, 281, 1322, 264, 1151, 5245, 295, 264, 7700, 295, 264, 1412, 300, 51510], "temperature": 0.0, "avg_logprob": -0.15282203091515434, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.002955019474029541}, {"id": 56, "seek": 22012, "start": 243.04000000000002, "end": 245.24, "text": " you are in charge of gathering.", "tokens": [51510, 291, 366, 294, 4602, 295, 13519, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15282203091515434, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.002955019474029541}, {"id": 57, "seek": 24524, "start": 245.56, "end": 250.48000000000002, "text": " Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging", "tokens": [50380, 2491, 13, 8500, 47890, 307, 257, 2183, 2595, 20946, 17912, 412, 264, 1042, 1102, 20764, 337, 10294, 1734, 7052, 332, 3568, 50626], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 58, "seek": 24524, "start": 250.48000000000002, "end": 254.96, "text": " at the Queen Square Institute of Neurology at University College London and a practice", "tokens": [50626, 412, 264, 10077, 16463, 9446, 295, 1734, 7052, 4987, 88, 412, 3535, 6745, 7042, 293, 257, 3124, 50850], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 59, "seek": 24524, "start": 254.96, "end": 255.96, "text": " in clinician.", "tokens": [50850, 294, 45962, 13, 50900], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 60, "seek": 24524, "start": 255.96, "end": 259.24, "text": " Now, one of the reviews from the book was from Andy Clark.", "tokens": [50900, 823, 11, 472, 295, 264, 10229, 490, 264, 1446, 390, 490, 13285, 18572, 13, 51064], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 61, "seek": 24524, "start": 259.24, "end": 261.08, "text": " He said, it should have been impossible.", "tokens": [51064, 634, 848, 11, 309, 820, 362, 668, 6243, 13, 51156], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 62, "seek": 24524, "start": 261.08, "end": 266.2, "text": " A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual", "tokens": [51156, 316, 26787, 5261, 295, 993, 293, 1575, 9897, 484, 294, 1266, 21117, 20013, 47626, 264, 24106, 51412], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 63, "seek": 24524, "start": 266.2, "end": 271.28000000000003, "text": " landscape from the formal schemas and some of the neurobiology and then garnished with", "tokens": [51412, 9661, 490, 264, 9860, 22627, 296, 293, 512, 295, 264, 16499, 5614, 1793, 293, 550, 25067, 4729, 365, 51666], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 64, "seek": 24524, "start": 271.28000000000003, "end": 273.92, "text": " practical recipes for active model design.", "tokens": [51666, 8496, 13035, 337, 4967, 2316, 1715, 13, 51798], "temperature": 0.0, "avg_logprob": -0.174875119837319, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.1750841736793518}, {"id": 65, "seek": 27392, "start": 273.92, "end": 278.08000000000004, "text": " Philosophically astute and scientifically compelling, this book is essential reading", "tokens": [50364, 31182, 5317, 984, 5357, 1169, 293, 39719, 20050, 11, 341, 1446, 307, 7115, 3760, 50572], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 66, "seek": 27392, "start": 278.08000000000004, "end": 281.40000000000003, "text": " for anyone interested in minds, brains and action.", "tokens": [50572, 337, 2878, 3102, 294, 9634, 11, 15442, 293, 3069, 13, 50738], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 67, "seek": 27392, "start": 281.40000000000003, "end": 283.96000000000004, "text": " Well, I mean, thank you very much for having me on.", "tokens": [50738, 1042, 11, 286, 914, 11, 1309, 291, 588, 709, 337, 1419, 385, 322, 13, 50866], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 68, "seek": 27392, "start": 283.96000000000004, "end": 285.92, "text": " So I'm Thomas Parr.", "tokens": [50866, 407, 286, 478, 8500, 47890, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 69, "seek": 27392, "start": 285.92, "end": 291.76, "text": " I'm both a clinician and a theoretical neuroscientist.", "tokens": [50964, 286, 478, 1293, 257, 45962, 293, 257, 20864, 28813, 5412, 468, 13, 51256], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 70, "seek": 27392, "start": 291.76, "end": 297.16, "text": " So I've been working in active inference for a number of years now since I did my PhD", "tokens": [51256, 407, 286, 600, 668, 1364, 294, 4967, 38253, 337, 257, 1230, 295, 924, 586, 1670, 286, 630, 452, 14476, 51526], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 71, "seek": 27392, "start": 297.16, "end": 303.88, "text": " back in 2016 with Carl at the theoretical neurobiology group at Queen Square.", "tokens": [51526, 646, 294, 6549, 365, 14256, 412, 264, 20864, 16499, 5614, 1793, 1594, 412, 10077, 16463, 13, 51862], "temperature": 0.0, "avg_logprob": -0.18411570397492882, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0024915062822401524}, {"id": 72, "seek": 30388, "start": 303.88, "end": 310.76, "text": " And I'm now based at Oxford where I split my time between research and clinical practice.", "tokens": [50364, 400, 286, 478, 586, 2361, 412, 24786, 689, 286, 7472, 452, 565, 1296, 2132, 293, 9115, 3124, 13, 50708], "temperature": 0.0, "avg_logprob": -0.18641798231336806, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.00752637255936861}, {"id": 73, "seek": 30388, "start": 310.76, "end": 313.88, "text": " So tell me about the first time you met Carl.", "tokens": [50708, 407, 980, 385, 466, 264, 700, 565, 291, 1131, 14256, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18641798231336806, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.00752637255936861}, {"id": 74, "seek": 30388, "start": 313.88, "end": 319.71999999999997, "text": " The first time I met Carl, I was considering, so I was a medical student at the time at", "tokens": [50864, 440, 700, 565, 286, 1131, 14256, 11, 286, 390, 8079, 11, 370, 286, 390, 257, 4625, 3107, 412, 264, 565, 412, 51156], "temperature": 0.0, "avg_logprob": -0.18641798231336806, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.00752637255936861}, {"id": 75, "seek": 30388, "start": 319.71999999999997, "end": 323.6, "text": " UCL and I was considering doing a PhD.", "tokens": [51156, 14079, 43, 293, 286, 390, 8079, 884, 257, 14476, 13, 51350], "temperature": 0.0, "avg_logprob": -0.18641798231336806, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.00752637255936861}, {"id": 76, "seek": 30388, "start": 323.6, "end": 327.32, "text": " And I remember arranging to meet with him and obviously being a relatively nerve-wracking", "tokens": [51350, 400, 286, 1604, 5539, 9741, 281, 1677, 365, 796, 293, 2745, 885, 257, 7226, 16355, 12, 7449, 14134, 51536], "temperature": 0.0, "avg_logprob": -0.18641798231336806, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.00752637255936861}, {"id": 77, "seek": 32732, "start": 327.32, "end": 333.76, "text": " experience meeting one of the most famous neuroscientists in the world.", "tokens": [50364, 1752, 3440, 472, 295, 264, 881, 4618, 28813, 5412, 1751, 294, 264, 1002, 13, 50686], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 78, "seek": 32732, "start": 333.76, "end": 337.36, "text": " I remember discussing with him about it and saying, you know, this is what I'm interested", "tokens": [50686, 286, 1604, 10850, 365, 796, 466, 309, 293, 1566, 11, 291, 458, 11, 341, 307, 437, 286, 478, 3102, 50866], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 79, "seek": 32732, "start": 337.36, "end": 338.71999999999997, "text": " in.", "tokens": [50866, 294, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 80, "seek": 32732, "start": 338.71999999999997, "end": 343.76, "text": " Would you consider supervising my PhD if I were to get the funding for it?", "tokens": [50934, 6068, 291, 1949, 37971, 3436, 452, 14476, 498, 286, 645, 281, 483, 264, 6137, 337, 309, 30, 51186], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 81, "seek": 32732, "start": 343.76, "end": 348.84, "text": " And I remember he said, yes, all right, then, anything else.", "tokens": [51186, 400, 286, 1604, 415, 848, 11, 2086, 11, 439, 558, 11, 550, 11, 1340, 1646, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 82, "seek": 32732, "start": 348.84, "end": 351.4, "text": " And I asked, do you want to see my CV or anything like that?", "tokens": [51440, 400, 286, 2351, 11, 360, 291, 528, 281, 536, 452, 22995, 420, 1340, 411, 300, 30, 51568], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 83, "seek": 32732, "start": 351.4, "end": 353.88, "text": " And he said, no, I'll only forget it.", "tokens": [51568, 400, 415, 848, 11, 572, 11, 286, 603, 787, 2870, 309, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 84, "seek": 32732, "start": 353.88, "end": 354.88, "text": " Yes.", "tokens": [51692, 1079, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2421606655778556, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.4323660135269165}, {"id": 85, "seek": 35488, "start": 355.56, "end": 359.0, "text": " That was my first encounter with Carl.", "tokens": [50398, 663, 390, 452, 700, 8593, 365, 14256, 13, 50570], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 86, "seek": 35488, "start": 359.0, "end": 365.04, "text": " But since then, he's always been immensely supportive and has been, you know, exactly", "tokens": [50570, 583, 1670, 550, 11, 415, 311, 1009, 668, 38674, 14435, 293, 575, 668, 11, 291, 458, 11, 2293, 50872], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 87, "seek": 35488, "start": 365.04, "end": 371.36, "text": " the sort of mentor that I think anybody would want to be able to develop a skill set and", "tokens": [50872, 264, 1333, 295, 14478, 300, 286, 519, 4472, 576, 528, 281, 312, 1075, 281, 1499, 257, 5389, 992, 293, 51188], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 88, "seek": 35488, "start": 371.36, "end": 372.76, "text": " sort of proceed in science.", "tokens": [51188, 1333, 295, 8991, 294, 3497, 13, 51258], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 89, "seek": 35488, "start": 372.76, "end": 375.52, "text": " I come from a machine learning background.", "tokens": [51258, 286, 808, 490, 257, 3479, 2539, 3678, 13, 51396], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 90, "seek": 35488, "start": 375.52, "end": 381.44, "text": " And since discovering active inference and Carl's work, it's really broadened my horizons.", "tokens": [51396, 400, 1670, 24773, 4967, 38253, 293, 14256, 311, 589, 11, 309, 311, 534, 4152, 5320, 452, 7937, 892, 13, 51692], "temperature": 0.0, "avg_logprob": -0.16673460214034372, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.002661139704287052}, {"id": 91, "seek": 38144, "start": 381.48, "end": 386.12, "text": " And at the moment, there's an obsession with things like chat GPT.", "tokens": [50366, 400, 412, 264, 1623, 11, 456, 311, 364, 30521, 365, 721, 411, 5081, 26039, 51, 13, 50598], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 92, "seek": 38144, "start": 386.12, "end": 392.12, "text": " And I just wondered in your own articulation, how would you kind of pose the work that you", "tokens": [50598, 400, 286, 445, 17055, 294, 428, 1065, 15228, 2776, 11, 577, 576, 291, 733, 295, 10774, 264, 589, 300, 291, 50898], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 93, "seek": 38144, "start": 392.12, "end": 394.72, "text": " do in relation to that kind of technology?", "tokens": [50898, 360, 294, 9721, 281, 300, 733, 295, 2899, 30, 51028], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 94, "seek": 38144, "start": 394.72, "end": 396.12, "text": " It's a good question.", "tokens": [51028, 467, 311, 257, 665, 1168, 13, 51098], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 95, "seek": 38144, "start": 396.12, "end": 402.12, "text": " And I suppose there are many levels at which it could be answered, aren't there?", "tokens": [51098, 400, 286, 7297, 456, 366, 867, 4358, 412, 597, 309, 727, 312, 10103, 11, 3212, 380, 456, 30, 51398], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 96, "seek": 38144, "start": 402.12, "end": 406.56, "text": " I guess thinking about something like chat GPT in that style of technology, it's clearly", "tokens": [51398, 286, 2041, 1953, 466, 746, 411, 5081, 26039, 51, 294, 300, 3758, 295, 2899, 11, 309, 311, 4448, 51620], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 97, "seek": 38144, "start": 406.56, "end": 410.44, "text": " been very, very effective at what it does.", "tokens": [51620, 668, 588, 11, 588, 4942, 412, 437, 309, 775, 13, 51814], "temperature": 0.0, "avg_logprob": -0.154239401353144, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.012482807040214539}, {"id": 98, "seek": 41044, "start": 410.44, "end": 413.56, "text": " But it's worth thinking about what is it that it does?", "tokens": [50364, 583, 309, 311, 3163, 1953, 466, 437, 307, 309, 300, 309, 775, 30, 50520], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 99, "seek": 41044, "start": 413.56, "end": 417.08, "text": " And I think chat GPT is an excellent example because so many people are familiar with it.", "tokens": [50520, 400, 286, 519, 5081, 26039, 51, 307, 364, 7103, 1365, 570, 370, 867, 561, 366, 4963, 365, 309, 13, 50696], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 100, "seek": 41044, "start": 417.08, "end": 423.88, "text": " It has such impressive results in terms of being able to simulate very effectively what", "tokens": [50696, 467, 575, 1270, 8992, 3542, 294, 2115, 295, 885, 1075, 281, 27817, 588, 8659, 437, 51036], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 101, "seek": 41044, "start": 423.88, "end": 426.96, "text": " it's like to have a conversation.", "tokens": [51036, 309, 311, 411, 281, 362, 257, 3761, 13, 51190], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 102, "seek": 41044, "start": 426.96, "end": 431.88, "text": " But ultimately, it is like most deep learning architectures.", "tokens": [51190, 583, 6284, 11, 309, 307, 411, 881, 2452, 2539, 6331, 1303, 13, 51436], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 103, "seek": 41044, "start": 431.88, "end": 433.96, "text": " It's a form of function approximation.", "tokens": [51436, 467, 311, 257, 1254, 295, 2445, 28023, 13, 51540], "temperature": 0.0, "avg_logprob": -0.12855808386641943, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.002217242494225502}, {"id": 104, "seek": 43396, "start": 433.96, "end": 442.12, "text": " It's a form of being able to capture very well the output that would be expected under", "tokens": [50364, 467, 311, 257, 1254, 295, 885, 1075, 281, 7983, 588, 731, 264, 5598, 300, 576, 312, 5176, 833, 50772], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 105, "seek": 43396, "start": 442.12, "end": 446.0, "text": " some set of conditions given some input.", "tokens": [50772, 512, 992, 295, 4487, 2212, 512, 4846, 13, 50966], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 106, "seek": 43396, "start": 446.0, "end": 452.03999999999996, "text": " So you give it some text and it knows which text to predict.", "tokens": [50966, 407, 291, 976, 309, 512, 2487, 293, 309, 3255, 597, 2487, 281, 6069, 13, 51268], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 107, "seek": 43396, "start": 452.03999999999996, "end": 453.03999999999996, "text": " And it's very good at that.", "tokens": [51268, 400, 309, 311, 588, 665, 412, 300, 13, 51318], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 108, "seek": 43396, "start": 453.03999999999996, "end": 455.35999999999996, "text": " But in a sense, that's where it stops.", "tokens": [51318, 583, 294, 257, 2020, 11, 300, 311, 689, 309, 10094, 13, 51434], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 109, "seek": 43396, "start": 455.35999999999996, "end": 458.52, "text": " It doesn't necessarily do anything else.", "tokens": [51434, 467, 1177, 380, 4725, 360, 1340, 1646, 13, 51592], "temperature": 0.0, "avg_logprob": -0.10616346053135248, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.08172967284917831}, {"id": 110, "seek": 45852, "start": 458.52, "end": 464.12, "text": " That's very different to what you and I do when we engage with the world around us, when", "tokens": [50364, 663, 311, 588, 819, 281, 437, 291, 293, 286, 360, 562, 321, 4683, 365, 264, 1002, 926, 505, 11, 562, 50644], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 111, "seek": 45852, "start": 464.12, "end": 468.03999999999996, "text": " we want to learn about the world around us, when we want to form our own beliefs about", "tokens": [50644, 321, 528, 281, 1466, 466, 264, 1002, 926, 505, 11, 562, 321, 528, 281, 1254, 527, 1065, 13585, 466, 50840], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 112, "seek": 45852, "start": 468.03999999999996, "end": 469.12, "text": " what's going on.", "tokens": [50840, 437, 311, 516, 322, 13, 50894], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 113, "seek": 45852, "start": 469.12, "end": 473.03999999999996, "text": " And those are the things that I think it doesn't have in the same way.", "tokens": [50894, 400, 729, 366, 264, 721, 300, 286, 519, 309, 1177, 380, 362, 294, 264, 912, 636, 13, 51090], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 114, "seek": 45852, "start": 473.03999999999996, "end": 479.91999999999996, "text": " It certainly can't act and go and seek out specific exchanges, specific conversations", "tokens": [51090, 467, 3297, 393, 380, 605, 293, 352, 293, 8075, 484, 2685, 27374, 11, 2685, 7315, 51434], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 115, "seek": 45852, "start": 479.91999999999996, "end": 482.35999999999996, "text": " that it might want to learn from.", "tokens": [51434, 300, 309, 1062, 528, 281, 1466, 490, 13, 51556], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 116, "seek": 45852, "start": 482.35999999999996, "end": 485.44, "text": " Whereas you or I might do that if we wanted to know about something specifically, we'd", "tokens": [51556, 13813, 291, 420, 286, 1062, 360, 300, 498, 321, 1415, 281, 458, 466, 746, 4682, 11, 321, 1116, 51710], "temperature": 0.0, "avg_logprob": -0.12186594332678843, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.11012130975723267}, {"id": 117, "seek": 48544, "start": 485.44, "end": 488.6, "text": " go and look for information about that thing.", "tokens": [50364, 352, 293, 574, 337, 1589, 466, 300, 551, 13, 50522], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 118, "seek": 48544, "start": 488.6, "end": 493.56, "text": " And I think that's where active inference and the idea of having a generative world", "tokens": [50522, 400, 286, 519, 300, 311, 689, 4967, 38253, 293, 264, 1558, 295, 1419, 257, 1337, 1166, 1002, 50770], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 119, "seek": 48544, "start": 493.56, "end": 498.56, "text": " model and understanding of what's there in your world that you can alter yourself, that", "tokens": [50770, 2316, 293, 3701, 295, 437, 311, 456, 294, 428, 1002, 300, 291, 393, 11337, 1803, 11, 300, 51020], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 120, "seek": 48544, "start": 498.56, "end": 504.24, "text": " you can change is very different to a lot of more passive artificial intelligence.", "tokens": [51020, 291, 393, 1319, 307, 588, 819, 281, 257, 688, 295, 544, 14975, 11677, 7599, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 121, "seek": 48544, "start": 504.24, "end": 508.64, "text": " Probably the point where things become closer is in fields like robotics, where you have", "tokens": [51304, 9210, 264, 935, 689, 721, 1813, 4966, 307, 294, 7909, 411, 34145, 11, 689, 291, 362, 51524], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 122, "seek": 48544, "start": 508.64, "end": 510.08, "text": " to account for both of those things.", "tokens": [51524, 281, 2696, 337, 1293, 295, 729, 721, 13, 51596], "temperature": 0.0, "avg_logprob": -0.12780738870302835, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.026925861835479736}, {"id": 123, "seek": 51008, "start": 510.08, "end": 515.84, "text": " You have to model a world that has yourself in it, where your actions affect the data", "tokens": [50364, 509, 362, 281, 2316, 257, 1002, 300, 575, 1803, 294, 309, 11, 689, 428, 5909, 3345, 264, 1412, 50652], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 124, "seek": 51008, "start": 515.84, "end": 516.84, "text": " that you get in.", "tokens": [50652, 300, 291, 483, 294, 13, 50702], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 125, "seek": 51008, "start": 516.84, "end": 520.56, "text": " And I think that's probably where more of the convergence is likely to happen.", "tokens": [50702, 400, 286, 519, 300, 311, 1391, 689, 544, 295, 264, 32181, 307, 3700, 281, 1051, 13, 50888], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 126, "seek": 51008, "start": 520.56, "end": 521.56, "text": " Yes.", "tokens": [50888, 1079, 13, 50938], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 127, "seek": 51008, "start": 521.56, "end": 528.64, "text": " So you're describing the difference, I guess, between an observational system and an interactive", "tokens": [50938, 407, 291, 434, 16141, 264, 2649, 11, 286, 2041, 11, 1296, 364, 9951, 1478, 1185, 293, 364, 15141, 51292], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 128, "seek": 51008, "start": 528.64, "end": 529.64, "text": " system.", "tokens": [51292, 1185, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 129, "seek": 51008, "start": 529.64, "end": 536.0, "text": " So in an interactive system, an agent can seek information and change or bend the environment", "tokens": [51342, 407, 294, 364, 15141, 1185, 11, 364, 9461, 393, 8075, 1589, 293, 1319, 420, 11229, 264, 2823, 51660], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 130, "seek": 51008, "start": 536.0, "end": 537.52, "text": " to suit its will.", "tokens": [51660, 281, 5722, 1080, 486, 13, 51736], "temperature": 0.0, "avg_logprob": -0.15457486634207243, "compression_ratio": 1.651639344262295, "no_speech_prob": 0.034895628690719604}, {"id": 131, "seek": 53752, "start": 537.8, "end": 543.04, "text": " Just to linger on this for a second, though, there are folks who do argue that neural networks", "tokens": [50378, 1449, 281, 45657, 322, 341, 337, 257, 1150, 11, 1673, 11, 456, 366, 4024, 567, 360, 9695, 300, 18161, 9590, 50640], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 132, "seek": 53752, "start": 543.04, "end": 547.56, "text": " are more than hash tables, because I think of them the same way you do.", "tokens": [50640, 366, 544, 813, 22019, 8020, 11, 570, 286, 519, 295, 552, 264, 912, 636, 291, 360, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 133, "seek": 53752, "start": 547.56, "end": 550.24, "text": " They essentially learn a function.", "tokens": [50866, 814, 4476, 1466, 257, 2445, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 134, "seek": 53752, "start": 550.24, "end": 555.3199999999999, "text": " And if you densely sample it enough, just like a hash table, it can go and retrieve what", "tokens": [51000, 400, 498, 291, 24505, 736, 6889, 309, 1547, 11, 445, 411, 257, 22019, 3199, 11, 309, 393, 352, 293, 30254, 437, 51254], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 135, "seek": 53752, "start": 555.3199999999999, "end": 557.88, "text": " that function says given a certain input.", "tokens": [51254, 300, 2445, 1619, 2212, 257, 1629, 4846, 13, 51382], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 136, "seek": 53752, "start": 557.88, "end": 563.0, "text": " But there are folks who say, no, no, no, these models learn a world model.", "tokens": [51382, 583, 456, 366, 4024, 567, 584, 11, 572, 11, 572, 11, 572, 11, 613, 5245, 1466, 257, 1002, 2316, 13, 51638], "temperature": 0.0, "avg_logprob": -0.1528933363140754, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.34005191922187805}, {"id": 137, "seek": 56300, "start": 563.0, "end": 569.12, "text": " So is given as an example, or with SORA, they say it's learned Navier stokes.", "tokens": [50364, 407, 307, 2212, 382, 364, 1365, 11, 420, 365, 318, 36860, 11, 436, 584, 309, 311, 3264, 9219, 811, 342, 8606, 13, 50670], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 138, "seek": 56300, "start": 569.12, "end": 570.6, "text": " It's a really good question.", "tokens": [50670, 467, 311, 257, 534, 665, 1168, 13, 50744], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 139, "seek": 56300, "start": 570.6, "end": 574.24, "text": " And I think there are some open questions here, and I wouldn't claim to have all the answers", "tokens": [50744, 400, 286, 519, 456, 366, 512, 1269, 1651, 510, 11, 293, 286, 2759, 380, 3932, 281, 362, 439, 264, 6338, 50926], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 140, "seek": 56300, "start": 574.24, "end": 575.96, "text": " to this one.", "tokens": [50926, 281, 341, 472, 13, 51012], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 141, "seek": 56300, "start": 575.96, "end": 582.16, "text": " I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly", "tokens": [51012, 286, 519, 281, 312, 1075, 11, 797, 11, 281, 747, 5081, 26039, 51, 11, 281, 312, 1075, 281, 976, 264, 1867, 309, 775, 11, 4448, 51322], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 142, "seek": 56300, "start": 582.16, "end": 584.88, "text": " it has captured something about the statistics of language.", "tokens": [51322, 309, 575, 11828, 746, 466, 264, 12523, 295, 2856, 13, 51458], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 143, "seek": 56300, "start": 584.88, "end": 589.72, "text": " It's uncovered something about the hidden causes.", "tokens": [51458, 467, 311, 37729, 746, 466, 264, 7633, 7700, 13, 51700], "temperature": 0.0, "avg_logprob": -0.2285243515419749, "compression_ratio": 1.656, "no_speech_prob": 0.20024432241916656}, {"id": 144, "seek": 58972, "start": 589.72, "end": 594.24, "text": " So you could argue there is potentially an element of world modeling in there that is", "tokens": [50364, 407, 291, 727, 9695, 456, 307, 7263, 364, 4478, 295, 1002, 15983, 294, 456, 300, 307, 50590], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 145, "seek": 58972, "start": 594.24, "end": 596.24, "text": " left implicit.", "tokens": [50590, 1411, 26947, 13, 50690], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 146, "seek": 58972, "start": 596.24, "end": 601.28, "text": " I think it would be very difficult to pull that out or to sort of see that with any transparency", "tokens": [50690, 286, 519, 309, 576, 312, 588, 2252, 281, 2235, 300, 484, 420, 281, 1333, 295, 536, 300, 365, 604, 17131, 50942], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 147, "seek": 58972, "start": 601.28, "end": 603.48, "text": " with something like chat GPT.", "tokens": [50942, 365, 746, 411, 5081, 26039, 51, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 148, "seek": 58972, "start": 603.48, "end": 608.64, "text": " And so if it does have something of that sort, probably it's the methods that neuroscientists", "tokens": [51052, 400, 370, 498, 309, 775, 362, 746, 295, 300, 1333, 11, 1391, 309, 311, 264, 7150, 300, 28813, 5412, 1751, 51310], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 149, "seek": 58972, "start": 608.64, "end": 611.96, "text": " have been using for years to understand the brain that might help to try and pull out", "tokens": [51310, 362, 668, 1228, 337, 924, 281, 1223, 264, 3567, 300, 1062, 854, 281, 853, 293, 2235, 484, 51476], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 150, "seek": 58972, "start": 611.96, "end": 616.96, "text": " those same things in those sorts of architectures.", "tokens": [51476, 729, 912, 721, 294, 729, 7527, 295, 6331, 1303, 13, 51726], "temperature": 0.0, "avg_logprob": -0.10760741277572212, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.0013474844163283706}, {"id": 151, "seek": 61696, "start": 616.96, "end": 621.9200000000001, "text": " Maybe some sorts of deep learning and neural network models are very good at picking up", "tokens": [50364, 2704, 512, 7527, 295, 2452, 2539, 293, 18161, 3209, 5245, 366, 588, 665, 412, 8867, 493, 50612], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 152, "seek": 61696, "start": 621.9200000000001, "end": 627.84, "text": " regularities in terms of dynamics as well and being able to predict trajectories.", "tokens": [50612, 3890, 1088, 294, 2115, 295, 15679, 382, 731, 293, 885, 1075, 281, 6069, 18257, 2083, 13, 50908], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 153, "seek": 61696, "start": 627.84, "end": 635.84, "text": " And I think it's important to say that describing something as a function approximator is not", "tokens": [50908, 400, 286, 519, 309, 311, 1021, 281, 584, 300, 16141, 746, 382, 257, 2445, 8542, 1639, 307, 406, 51308], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 154, "seek": 61696, "start": 635.84, "end": 637.36, "text": " to criticize or belittle it.", "tokens": [51308, 281, 31010, 420, 989, 703, 309, 13, 51384], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 155, "seek": 61696, "start": 637.36, "end": 640.48, "text": " It's a very important thing to be able to do.", "tokens": [51384, 467, 311, 257, 588, 1021, 551, 281, 312, 1075, 281, 360, 13, 51540], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 156, "seek": 61696, "start": 640.48, "end": 644.2, "text": " And it may also be very important in certain types of inference.", "tokens": [51540, 400, 309, 815, 611, 312, 588, 1021, 294, 1629, 3467, 295, 38253, 13, 51726], "temperature": 0.0, "avg_logprob": -0.14115678270657858, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.13509748876094818}, {"id": 157, "seek": 64420, "start": 644.2, "end": 649.2800000000001, "text": " So for instance, things like variational autoencoders are based upon often deep learning", "tokens": [50364, 407, 337, 5197, 11, 721, 411, 3034, 1478, 8399, 22660, 378, 433, 366, 2361, 3564, 2049, 2452, 2539, 50618], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 158, "seek": 64420, "start": 649.2800000000001, "end": 651.6, "text": " neural network architectures.", "tokens": [50618, 18161, 3209, 6331, 1303, 13, 50734], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 159, "seek": 64420, "start": 651.6, "end": 655.8000000000001, "text": " But the function that is learned is the one that maps from the data I've got coming in", "tokens": [50734, 583, 264, 2445, 300, 307, 3264, 307, 264, 472, 300, 11317, 490, 264, 1412, 286, 600, 658, 1348, 294, 50944], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 160, "seek": 64420, "start": 655.8000000000001, "end": 659.8000000000001, "text": " to the posterior beliefs or the parameters of the posterior beliefs that I would arrive", "tokens": [50944, 281, 264, 33529, 13585, 420, 264, 9834, 295, 264, 33529, 13585, 300, 286, 576, 8881, 51144], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 161, "seek": 64420, "start": 659.8000000000001, "end": 665.0400000000001, "text": " at were I to perform inference of the sort we might do in active inference.", "tokens": [51144, 412, 645, 286, 281, 2042, 38253, 295, 264, 1333, 321, 1062, 360, 294, 4967, 38253, 13, 51406], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 162, "seek": 64420, "start": 665.0400000000001, "end": 670.2800000000001, "text": " So you've written an absolutely beautiful book on active inference.", "tokens": [51406, 407, 291, 600, 3720, 364, 3122, 2238, 1446, 322, 4967, 38253, 13, 51668], "temperature": 0.0, "avg_logprob": -0.12637373413702455, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0032098775263875723}, {"id": 163, "seek": 67028, "start": 670.36, "end": 678.8, "text": " And active inference, in my view, it's a theory of agency, which is to say it describes what", "tokens": [50368, 400, 4967, 38253, 11, 294, 452, 1910, 11, 309, 311, 257, 5261, 295, 7934, 11, 597, 307, 281, 584, 309, 15626, 437, 50790], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 164, "seek": 67028, "start": 678.8, "end": 679.8, "text": " an agent does.", "tokens": [50790, 364, 9461, 775, 13, 50840], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 165, "seek": 67028, "start": 679.8, "end": 681.12, "text": " And I'm fascinated by agency.", "tokens": [50840, 400, 286, 478, 24597, 538, 7934, 13, 50906], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 166, "seek": 67028, "start": 681.12, "end": 684.76, "text": " But could you just start by, I mean, from your perspective, could you introduce the", "tokens": [50906, 583, 727, 291, 445, 722, 538, 11, 286, 914, 11, 490, 428, 4585, 11, 727, 291, 5366, 264, 51088], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 167, "seek": 67028, "start": 684.76, "end": 687.0799999999999, "text": " book and tell us about your experience writing it?", "tokens": [51088, 1446, 293, 980, 505, 466, 428, 1752, 3579, 309, 30, 51204], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 168, "seek": 67028, "start": 687.0799999999999, "end": 688.0799999999999, "text": " Of course.", "tokens": [51204, 2720, 1164, 13, 51254], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 169, "seek": 67028, "start": 688.0799999999999, "end": 693.4, "text": " So the active inference book that we've written is a collaboration between myself, Giovanni", "tokens": [51254, 407, 264, 4967, 38253, 1446, 300, 321, 600, 3720, 307, 257, 9363, 1296, 2059, 11, 47089, 35832, 51520], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 170, "seek": 67028, "start": 693.4, "end": 698.8399999999999, "text": " Pazzullo is based in Rome and Carl Friston, who has to take credit for development of", "tokens": [51520, 430, 9112, 858, 78, 307, 2361, 294, 12043, 293, 14256, 1526, 47345, 11, 567, 575, 281, 747, 5397, 337, 3250, 295, 51792], "temperature": 0.0, "avg_logprob": -0.19768673578898113, "compression_ratio": 1.5841924398625429, "no_speech_prob": 0.23030561208724976}, {"id": 171, "seek": 69884, "start": 698.84, "end": 702.08, "text": " active inference in the first place.", "tokens": [50364, 4967, 38253, 294, 264, 700, 1081, 13, 50526], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 172, "seek": 69884, "start": 702.08, "end": 711.52, "text": " And the book sort of rose out of our sense that there wasn't a unified book out there", "tokens": [50526, 400, 264, 1446, 1333, 295, 10895, 484, 295, 527, 2020, 300, 456, 2067, 380, 257, 26787, 1446, 484, 456, 50998], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 173, "seek": 69884, "start": 711.52, "end": 716.2, "text": " or a resource out there to help people learn about what is ultimately a very interdisciplinary", "tokens": [50998, 420, 257, 7684, 484, 456, 281, 854, 561, 1466, 466, 437, 307, 6284, 257, 588, 38280, 51232], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 174, "seek": 69884, "start": 716.2, "end": 718.12, "text": " field.", "tokens": [51232, 2519, 13, 51328], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 175, "seek": 69884, "start": 718.12, "end": 722.8000000000001, "text": " And so we've all had experience with students coming to us asking for resources, asking", "tokens": [51328, 400, 370, 321, 600, 439, 632, 1752, 365, 1731, 1348, 281, 505, 3365, 337, 3593, 11, 3365, 51562], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 176, "seek": 69884, "start": 722.8000000000001, "end": 724.2, "text": " what they need to read.", "tokens": [51562, 437, 436, 643, 281, 1401, 13, 51632], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 177, "seek": 69884, "start": 724.2, "end": 728.12, "text": " And it may be we refer them to a little bit of neuroscience work, a little bit of machine", "tokens": [51632, 400, 309, 815, 312, 321, 2864, 552, 281, 257, 707, 857, 295, 42762, 589, 11, 257, 707, 857, 295, 3479, 51828], "temperature": 0.0, "avg_logprob": -0.13187321412910535, "compression_ratio": 1.7459016393442623, "no_speech_prob": 0.004104692488908768}, {"id": 178, "seek": 72812, "start": 728.4, "end": 734.44, "text": " learning, textbooks or specific pages on variational inference or whatever else, giving", "tokens": [50378, 2539, 11, 33587, 420, 2685, 7183, 322, 3034, 1478, 38253, 420, 2035, 1646, 11, 2902, 50680], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 179, "seek": 72812, "start": 734.44, "end": 739.84, "text": " people introductions to or places they can learn about the maths they need to be able", "tokens": [50680, 561, 48032, 281, 420, 3190, 436, 393, 1466, 466, 264, 36287, 436, 643, 281, 312, 1075, 50950], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 180, "seek": 72812, "start": 739.84, "end": 740.84, "text": " to do it.", "tokens": [50950, 281, 360, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 181, "seek": 72812, "start": 740.84, "end": 744.84, "text": " But then also the biology, the underlying psychology, the long sort of tradition of", "tokens": [51000, 583, 550, 611, 264, 14956, 11, 264, 14217, 15105, 11, 264, 938, 1333, 295, 6994, 295, 51200], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 182, "seek": 72812, "start": 744.84, "end": 749.32, "text": " previous scientists who worked in related areas.", "tokens": [51200, 3894, 7708, 567, 2732, 294, 4077, 3179, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 183, "seek": 72812, "start": 749.32, "end": 752.52, "text": " And so the book was an attempt to try and provide a place that people could find all", "tokens": [51424, 400, 370, 264, 1446, 390, 364, 5217, 281, 853, 293, 2893, 257, 1081, 300, 561, 727, 915, 439, 51584], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 184, "seek": 72812, "start": 752.52, "end": 756.84, "text": " of that, or at least references to all the relevant things they needed for that, to stick", "tokens": [51584, 295, 300, 11, 420, 412, 1935, 15400, 281, 439, 264, 7340, 721, 436, 2978, 337, 300, 11, 281, 2897, 51800], "temperature": 0.0, "avg_logprob": -0.1336407916886466, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.04528193175792694}, {"id": 185, "seek": 75684, "start": 756.9200000000001, "end": 762.12, "text": " to the same sort of notation, which is one of the things that's often very difficult,", "tokens": [50368, 281, 264, 912, 1333, 295, 24657, 11, 597, 307, 472, 295, 264, 721, 300, 311, 2049, 588, 2252, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1552744787566516, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.001883463584817946}, {"id": 186, "seek": 75684, "start": 762.12, "end": 767.8000000000001, "text": " and the same formalisms and try and introduce everything in a very systematic way to people.", "tokens": [50628, 293, 264, 912, 9860, 13539, 293, 853, 293, 5366, 1203, 294, 257, 588, 27249, 636, 281, 561, 13, 50912], "temperature": 0.0, "avg_logprob": -0.1552744787566516, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.001883463584817946}, {"id": 187, "seek": 75684, "start": 767.8000000000001, "end": 774.9200000000001, "text": " So I'm pleased here that you found it useful, and I hope other people will as well.", "tokens": [50912, 407, 286, 478, 10587, 510, 300, 291, 1352, 309, 4420, 11, 293, 286, 1454, 661, 561, 486, 382, 731, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1552744787566516, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.001883463584817946}, {"id": 188, "seek": 75684, "start": 774.9200000000001, "end": 781.88, "text": " The experience of writing it, I mean, so that took place over several years, partly because", "tokens": [51268, 440, 1752, 295, 3579, 309, 11, 286, 914, 11, 370, 300, 1890, 1081, 670, 2940, 924, 11, 17031, 570, 51616], "temperature": 0.0, "avg_logprob": -0.1552744787566516, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.001883463584817946}, {"id": 189, "seek": 75684, "start": 781.88, "end": 785.24, "text": " the pandemic got in the way in the middle.", "tokens": [51616, 264, 5388, 658, 294, 264, 636, 294, 264, 2808, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1552744787566516, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.001883463584817946}, {"id": 190, "seek": 78524, "start": 785.24, "end": 790.6, "text": " So Giovanni and I were passing notes between one another over email and weren't able to", "tokens": [50364, 407, 47089, 35832, 293, 286, 645, 8437, 5570, 1296, 472, 1071, 670, 3796, 293, 4999, 380, 1075, 281, 50632], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 191, "seek": 78524, "start": 790.6, "end": 793.0, "text": " sort of meet in person to discuss it during that time period.", "tokens": [50632, 1333, 295, 1677, 294, 954, 281, 2248, 309, 1830, 300, 565, 2896, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 192, "seek": 78524, "start": 795.16, "end": 801.32, "text": " But I think we're all quite proud of the result that we've got out of that, and people seem", "tokens": [50860, 583, 286, 519, 321, 434, 439, 1596, 4570, 295, 264, 1874, 300, 321, 600, 658, 484, 295, 300, 11, 293, 561, 1643, 51168], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 193, "seek": 78524, "start": 801.32, "end": 802.92, "text": " to have responded quite well to it.", "tokens": [51168, 281, 362, 15806, 1596, 731, 281, 309, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 194, "seek": 78524, "start": 802.92, "end": 808.2, "text": " You start off by talking about what you call a high road and a low road to active inference.", "tokens": [51248, 509, 722, 766, 538, 1417, 466, 437, 291, 818, 257, 1090, 3060, 293, 257, 2295, 3060, 281, 4967, 38253, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 195, "seek": 78524, "start": 808.2, "end": 809.4, "text": " Can you sketch that out?", "tokens": [51512, 1664, 291, 12325, 300, 484, 30, 51572], "temperature": 0.0, "avg_logprob": -0.1306259608504796, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.0005121122812852263}, {"id": 196, "seek": 80940, "start": 810.28, "end": 816.92, "text": " Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it,", "tokens": [50408, 1079, 11, 293, 286, 519, 341, 390, 472, 295, 47089, 35832, 311, 588, 1481, 3487, 466, 577, 281, 5366, 309, 11, 50740], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 197, "seek": 80940, "start": 816.92, "end": 819.64, "text": " because as I say, it's very multidisciplinary.", "tokens": [50740, 570, 382, 286, 584, 11, 309, 311, 588, 2120, 40920, 24560, 13, 50876], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 198, "seek": 80940, "start": 819.64, "end": 823.64, "text": " There are lots of ways into active inference, and one of the things that's most difficult", "tokens": [50876, 821, 366, 3195, 295, 2098, 666, 4967, 38253, 11, 293, 472, 295, 264, 721, 300, 311, 881, 2252, 51076], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 199, "seek": 80940, "start": 823.64, "end": 827.3199999999999, "text": " for people who are getting into the field for the first time is knowing where to start.", "tokens": [51076, 337, 561, 567, 366, 1242, 666, 264, 2519, 337, 264, 700, 565, 307, 5276, 689, 281, 722, 13, 51260], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 200, "seek": 80940, "start": 827.3199999999999, "end": 833.0799999999999, "text": " Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas", "tokens": [51260, 1144, 436, 722, 6260, 365, 264, 7840, 42434, 3567, 11, 18900, 38253, 11, 293, 6128, 76, 71, 4837, 89, 952, 3487, 51548], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 201, "seek": 80940, "start": 833.0799999999999, "end": 839.0799999999999, "text": " like that, or do they start from a physics-based perspective and start working their way towards", "tokens": [51548, 411, 300, 11, 420, 360, 436, 722, 490, 257, 10649, 12, 6032, 4585, 293, 722, 1364, 641, 636, 3030, 51848], "temperature": 0.0, "avg_logprob": -0.10121723081244798, "compression_ratio": 1.7040816326530612, "no_speech_prob": 0.006180813070386648}, {"id": 202, "seek": 83940, "start": 839.8, "end": 841.56, "text": " something that looks like sentience?", "tokens": [50384, 746, 300, 1542, 411, 2279, 1182, 30, 50472], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 203, "seek": 83940, "start": 842.6, "end": 846.12, "text": " And there are lots of different, lots of alternative ways people get into it.", "tokens": [50524, 400, 456, 366, 3195, 295, 819, 11, 3195, 295, 8535, 2098, 561, 483, 666, 309, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 204, "seek": 83940, "start": 846.12, "end": 849.8, "text": " The fact that you become interested via machine learning, the fact that other people", "tokens": [50700, 440, 1186, 300, 291, 1813, 3102, 5766, 3479, 2539, 11, 264, 1186, 300, 661, 561, 50884], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 205, "seek": 83940, "start": 849.8, "end": 854.36, "text": " have become interested through biology, I developed an interest through neuroscience", "tokens": [50884, 362, 1813, 3102, 807, 14956, 11, 286, 4743, 364, 1179, 807, 42762, 51112], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 206, "seek": 83940, "start": 855.72, "end": 856.92, "text": " while I was at medical school.", "tokens": [51180, 1339, 286, 390, 412, 4625, 1395, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 207, "seek": 83940, "start": 859.24, "end": 861.9599999999999, "text": " And the high road and the low road was a way of just trying to acknowledge", "tokens": [51356, 400, 264, 1090, 3060, 293, 264, 2295, 3060, 390, 257, 636, 295, 445, 1382, 281, 10692, 51492], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 208, "seek": 83940, "start": 862.76, "end": 868.52, "text": " that difference or that difficulty of knowing where to begin, and saying that that's okay,", "tokens": [51532, 300, 2649, 420, 300, 10360, 295, 5276, 689, 281, 1841, 11, 293, 1566, 300, 300, 311, 1392, 11, 51820], "temperature": 0.0, "avg_logprob": -0.1111520874166043, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.0011219590669497848}, {"id": 209, "seek": 86852, "start": 868.6, "end": 872.12, "text": " there are lots of different roads, but they ultimately end up in the same place.", "tokens": [50368, 456, 366, 3195, 295, 819, 11344, 11, 457, 436, 6284, 917, 493, 294, 264, 912, 1081, 13, 50544], "temperature": 0.0, "avg_logprob": -0.056109192445106114, "compression_ratio": 1.64, "no_speech_prob": 0.00039229990215972066}, {"id": 210, "seek": 86852, "start": 873.0799999999999, "end": 877.64, "text": " The idea of the low road was to say, well, let's just take observations and psychology", "tokens": [50592, 440, 1558, 295, 264, 2295, 3060, 390, 281, 584, 11, 731, 11, 718, 311, 445, 747, 18163, 293, 15105, 50820], "temperature": 0.0, "avg_logprob": -0.056109192445106114, "compression_ratio": 1.64, "no_speech_prob": 0.00039229990215972066}, {"id": 211, "seek": 86852, "start": 877.64, "end": 880.76, "text": " sort of development of a number of ideas that are built up over time", "tokens": [50820, 1333, 295, 3250, 295, 257, 1230, 295, 3487, 300, 366, 3094, 493, 670, 565, 50976], "temperature": 0.0, "avg_logprob": -0.056109192445106114, "compression_ratio": 1.64, "no_speech_prob": 0.00039229990215972066}, {"id": 212, "seek": 86852, "start": 882.4399999999999, "end": 887.3199999999999, "text": " that come to the idea that we're using internal models to explain our world,", "tokens": [51060, 300, 808, 281, 264, 1558, 300, 321, 434, 1228, 6920, 5245, 281, 2903, 527, 1002, 11, 51304], "temperature": 0.0, "avg_logprob": -0.056109192445106114, "compression_ratio": 1.64, "no_speech_prob": 0.00039229990215972066}, {"id": 213, "seek": 86852, "start": 887.3199999999999, "end": 892.36, "text": " that the brain is using something like Bayesian inference, or at least can be described as using", "tokens": [51304, 300, 264, 3567, 307, 1228, 746, 411, 7840, 42434, 38253, 11, 420, 412, 1935, 393, 312, 7619, 382, 1228, 51556], "temperature": 0.0, "avg_logprob": -0.056109192445106114, "compression_ratio": 1.64, "no_speech_prob": 0.00039229990215972066}, {"id": 214, "seek": 89236, "start": 892.36, "end": 899.4, "text": " Bayesian inference, and go from there through the advances that lead you to active inference,", "tokens": [50364, 7840, 42434, 38253, 11, 293, 352, 490, 456, 807, 264, 25297, 300, 1477, 291, 281, 4967, 38253, 11, 50716], "temperature": 0.0, "avg_logprob": -0.0694669427223576, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.02582021802663803}, {"id": 215, "seek": 89236, "start": 899.4, "end": 904.52, "text": " the idea that it's not just a passive process that you're also inferring what I'm going to do.", "tokens": [50716, 264, 1558, 300, 309, 311, 406, 445, 257, 14975, 1399, 300, 291, 434, 611, 13596, 2937, 437, 286, 478, 516, 281, 360, 13, 50972], "temperature": 0.0, "avg_logprob": -0.0694669427223576, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.02582021802663803}, {"id": 216, "seek": 89236, "start": 905.64, "end": 910.04, "text": " And furthermore, that when we're doing inference, we're changing our beliefs", "tokens": [51028, 400, 3052, 3138, 11, 300, 562, 321, 434, 884, 38253, 11, 321, 434, 4473, 527, 13585, 51248], "temperature": 0.0, "avg_logprob": -0.0694669427223576, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.02582021802663803}, {"id": 217, "seek": 89236, "start": 910.76, "end": 914.84, "text": " to reflect what's in the world around us and to explain our sensory data.", "tokens": [51284, 281, 5031, 437, 311, 294, 264, 1002, 926, 505, 293, 281, 2903, 527, 27233, 1412, 13, 51488], "temperature": 0.0, "avg_logprob": -0.0694669427223576, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.02582021802663803}, {"id": 218, "seek": 89236, "start": 915.48, "end": 918.84, "text": " But actually, when we're acting in the world, we can also change the world itself", "tokens": [51520, 583, 767, 11, 562, 321, 434, 6577, 294, 264, 1002, 11, 321, 393, 611, 1319, 264, 1002, 2564, 51688], "temperature": 0.0, "avg_logprob": -0.0694669427223576, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.02582021802663803}, {"id": 219, "seek": 91884, "start": 918.9200000000001, "end": 925.4, "text": " to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the", "tokens": [50368, 281, 1101, 27956, 365, 527, 13585, 13, 407, 309, 311, 300, 1286, 490, 17491, 4473, 527, 13585, 281, 5031, 264, 50692], "temperature": 0.0, "avg_logprob": -0.10242814563569569, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0024876475799828768}, {"id": 220, "seek": 91884, "start": 925.4, "end": 931.08, "text": " world to also changing the world to reflect our beliefs. And that fascinating move that", "tokens": [50692, 1002, 281, 611, 4473, 264, 1002, 281, 5031, 527, 13585, 13, 400, 300, 10343, 1286, 300, 50976], "temperature": 0.0, "avg_logprob": -0.10242814563569569, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0024876475799828768}, {"id": 221, "seek": 91884, "start": 931.08, "end": 935.72, "text": " actually both can be seen as optimization of exactly the same objective that they have the", "tokens": [50976, 767, 1293, 393, 312, 1612, 382, 19618, 295, 2293, 264, 912, 10024, 300, 436, 362, 264, 51208], "temperature": 0.0, "avg_logprob": -0.10242814563569569, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0024876475799828768}, {"id": 222, "seek": 91884, "start": 935.72, "end": 941.8000000000001, "text": " same goal, that in both cases, it's really just improving the fit between us and our world.", "tokens": [51208, 912, 3387, 11, 300, 294, 1293, 3331, 11, 309, 311, 534, 445, 11470, 264, 3318, 1296, 505, 293, 527, 1002, 13, 51512], "temperature": 0.0, "avg_logprob": -0.10242814563569569, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0024876475799828768}, {"id": 223, "seek": 91884, "start": 943.0, "end": 947.72, "text": " So that's the sort of low road perspective. The high road perspective was the idea of saying,", "tokens": [51572, 407, 300, 311, 264, 1333, 295, 2295, 3060, 4585, 13, 440, 1090, 3060, 4585, 390, 264, 1558, 295, 1566, 11, 51808], "temperature": 0.0, "avg_logprob": -0.10242814563569569, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0024876475799828768}, {"id": 224, "seek": 94772, "start": 948.36, "end": 952.9200000000001, "text": " well, let's start from the minimum number of assumptions we can, let's start from first", "tokens": [50396, 731, 11, 718, 311, 722, 490, 264, 7285, 1230, 295, 17695, 321, 393, 11, 718, 311, 722, 490, 700, 50624], "temperature": 0.0, "avg_logprob": -0.07591523115451519, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.0009620590135455132}, {"id": 225, "seek": 94772, "start": 952.9200000000001, "end": 960.36, "text": " principles. And that takes a much more physics based approach. It says, if you have a creature", "tokens": [50624, 9156, 13, 400, 300, 2516, 257, 709, 544, 10649, 2361, 3109, 13, 467, 1619, 11, 498, 291, 362, 257, 12797, 50996], "temperature": 0.0, "avg_logprob": -0.07591523115451519, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.0009620590135455132}, {"id": 226, "seek": 94772, "start": 960.36, "end": 966.36, "text": " that is interacting with its world, then there are a number of things you've already committed to,", "tokens": [50996, 300, 307, 18017, 365, 1080, 1002, 11, 550, 456, 366, 257, 1230, 295, 721, 291, 600, 1217, 7784, 281, 11, 51296], "temperature": 0.0, "avg_logprob": -0.07591523115451519, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.0009620590135455132}, {"id": 227, "seek": 94772, "start": 966.36, "end": 971.0, "text": " and that includes things like the persistence of that creature over a reasonable length of time,", "tokens": [51296, 293, 300, 5974, 721, 411, 264, 37617, 295, 300, 12797, 670, 257, 10585, 4641, 295, 565, 11, 51528], "temperature": 0.0, "avg_logprob": -0.07591523115451519, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.0009620590135455132}, {"id": 228, "seek": 94772, "start": 971.0, "end": 974.9200000000001, "text": " the maintenance of a boundary between that creature and its world, and that sort of", "tokens": [51528, 264, 11258, 295, 257, 12866, 1296, 300, 12797, 293, 1080, 1002, 11, 293, 300, 1333, 295, 51724], "temperature": 0.0, "avg_logprob": -0.07591523115451519, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.0009620590135455132}, {"id": 229, "seek": 97492, "start": 974.92, "end": 981.88, "text": " self world distinction. And once you've committed to those things, you can then start to write down", "tokens": [50364, 2698, 1002, 16844, 13, 400, 1564, 291, 600, 7784, 281, 729, 721, 11, 291, 393, 550, 722, 281, 2464, 760, 50712], "temperature": 0.0, "avg_logprob": -0.06490474997214901, "compression_ratio": 1.9489361702127659, "no_speech_prob": 0.00205593672581017}, {"id": 230, "seek": 97492, "start": 981.88, "end": 986.52, "text": " the constraints that those imply in terms of the physical dynamics of that system.", "tokens": [50712, 264, 18491, 300, 729, 33616, 294, 2115, 295, 264, 4001, 15679, 295, 300, 1185, 13, 50944], "temperature": 0.0, "avg_logprob": -0.06490474997214901, "compression_ratio": 1.9489361702127659, "no_speech_prob": 0.00205593672581017}, {"id": 231, "seek": 97492, "start": 986.52, "end": 990.76, "text": " And you can start to interpret those dynamics in terms of the functions they might be", "tokens": [50944, 400, 291, 393, 722, 281, 7302, 729, 15679, 294, 2115, 295, 264, 6828, 436, 1062, 312, 51156], "temperature": 0.0, "avg_logprob": -0.06490474997214901, "compression_ratio": 1.9489361702127659, "no_speech_prob": 0.00205593672581017}, {"id": 232, "seek": 97492, "start": 990.76, "end": 996.92, "text": " optimizing, much like, much like if you were to write down the equations that underpin Newtonian", "tokens": [51156, 40425, 11, 709, 411, 11, 709, 411, 498, 291, 645, 281, 2464, 760, 264, 11787, 300, 833, 17836, 19541, 952, 51464], "temperature": 0.0, "avg_logprob": -0.06490474997214901, "compression_ratio": 1.9489361702127659, "no_speech_prob": 0.00205593672581017}, {"id": 233, "seek": 97492, "start": 996.92, "end": 1001.88, "text": " dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's", "tokens": [51464, 15679, 11, 291, 393, 2464, 552, 760, 294, 2115, 295, 641, 12867, 322, 18484, 952, 6828, 13, 400, 309, 311, 51712], "temperature": 0.0, "avg_logprob": -0.06490474997214901, "compression_ratio": 1.9489361702127659, "no_speech_prob": 0.00205593672581017}, {"id": 234, "seek": 100188, "start": 1001.88, "end": 1006.6, "text": " following the same sort of logic to then get to flows on free energy functions, where free energy", "tokens": [50364, 3480, 264, 912, 1333, 295, 9952, 281, 550, 483, 281, 12867, 322, 1737, 2281, 6828, 11, 689, 1737, 2281, 50600], "temperature": 0.0, "avg_logprob": -0.05713531594527395, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.003508983412757516}, {"id": 235, "seek": 100188, "start": 1006.6, "end": 1012.12, "text": " is just a measure of that fit between us and our world. And so both roads ultimately end up leading", "tokens": [50600, 307, 445, 257, 3481, 295, 300, 3318, 1296, 505, 293, 527, 1002, 13, 400, 370, 1293, 11344, 6284, 917, 493, 5775, 50876], "temperature": 0.0, "avg_logprob": -0.05713531594527395, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.003508983412757516}, {"id": 236, "seek": 100188, "start": 1012.12, "end": 1020.6, "text": " to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we", "tokens": [50876, 281, 341, 2689, 35795, 11, 597, 307, 300, 281, 312, 364, 9461, 294, 527, 13401, 11, 294, 264, 1333, 295, 13401, 321, 51300], "temperature": 0.0, "avg_logprob": -0.05713531594527395, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.003508983412757516}, {"id": 237, "seek": 100188, "start": 1020.6, "end": 1027.16, "text": " live in, we have to be able to change our beliefs, to reflect what's going on around us and change", "tokens": [51300, 1621, 294, 11, 321, 362, 281, 312, 1075, 281, 1319, 527, 13585, 11, 281, 5031, 437, 311, 516, 322, 926, 505, 293, 1319, 51628], "temperature": 0.0, "avg_logprob": -0.05713531594527395, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.003508983412757516}, {"id": 238, "seek": 102716, "start": 1027.24, "end": 1034.8400000000001, "text": " the world through our dynamical flows on a free energy functional, to best fit with the sorts", "tokens": [50368, 264, 1002, 807, 527, 5999, 804, 12867, 322, 257, 1737, 2281, 11745, 11, 281, 1151, 3318, 365, 264, 7527, 50748], "temperature": 0.0, "avg_logprob": -0.08148709069127621, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0017370189307257533}, {"id": 239, "seek": 102716, "start": 1034.8400000000001, "end": 1039.4, "text": " of creatures we are. When we first started looking at the free energy principle, we were talking", "tokens": [50748, 295, 12281, 321, 366, 13, 1133, 321, 700, 1409, 1237, 412, 264, 1737, 2281, 8665, 11, 321, 645, 1417, 50976], "temperature": 0.0, "avg_logprob": -0.08148709069127621, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0017370189307257533}, {"id": 240, "seek": 102716, "start": 1039.4, "end": 1047.4, "text": " about things. It was known as a theory of every thing, every space thing, which is to say, roughly", "tokens": [50976, 466, 721, 13, 467, 390, 2570, 382, 257, 5261, 295, 633, 551, 11, 633, 1901, 551, 11, 597, 307, 281, 584, 11, 9810, 51376], "temperature": 0.0, "avg_logprob": -0.08148709069127621, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0017370189307257533}, {"id": 241, "seek": 102716, "start": 1047.4, "end": 1055.16, "text": " speaking, if a thing exists, what must the thing do to continue to exist? And just their continued", "tokens": [51376, 4124, 11, 498, 257, 551, 8198, 11, 437, 1633, 264, 551, 360, 281, 2354, 281, 2514, 30, 400, 445, 641, 7014, 51764], "temperature": 0.0, "avg_logprob": -0.08148709069127621, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0017370189307257533}, {"id": 242, "seek": 105516, "start": 1055.16, "end": 1061.64, "text": " existence resisting entropic forces is what defines them, which gets us into the second law", "tokens": [50364, 9123, 43940, 948, 39173, 5874, 307, 437, 23122, 552, 11, 597, 2170, 505, 666, 264, 1150, 2101, 50688], "temperature": 0.0, "avg_logprob": -0.06910681951613654, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.003287853905931115}, {"id": 243, "seek": 105516, "start": 1061.64, "end": 1066.28, "text": " of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to", "tokens": [50688, 295, 8810, 35483, 13, 823, 11, 300, 3263, 411, 1596, 257, 5861, 551, 281, 584, 13, 1545, 360, 721, 643, 281, 50920], "temperature": 0.0, "avg_logprob": -0.06910681951613654, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.003287853905931115}, {"id": 244, "seek": 105516, "start": 1066.28, "end": 1071.64, "text": " resist entropic forces? And I think there's a development in how a lot of these ideas are", "tokens": [50920, 4597, 948, 39173, 5874, 30, 400, 286, 519, 456, 311, 257, 3250, 294, 577, 257, 688, 295, 613, 3487, 366, 51188], "temperature": 0.0, "avg_logprob": -0.06910681951613654, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.003287853905931115}, {"id": 245, "seek": 105516, "start": 1071.64, "end": 1077.88, "text": " presented over time, which you expect and hope for in science. And I think we've often taken", "tokens": [51188, 8212, 670, 565, 11, 597, 291, 2066, 293, 1454, 337, 294, 3497, 13, 400, 286, 519, 321, 600, 2049, 2726, 51500], "temperature": 0.0, "avg_logprob": -0.06910681951613654, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.003287853905931115}, {"id": 246, "seek": 105516, "start": 1077.88, "end": 1083.8000000000002, "text": " different perspectives at different points in time as to how we explain these ideas.", "tokens": [51500, 819, 16766, 412, 819, 2793, 294, 565, 382, 281, 577, 321, 2903, 613, 3487, 13, 51796], "temperature": 0.0, "avg_logprob": -0.06910681951613654, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.003287853905931115}, {"id": 247, "seek": 108380, "start": 1084.76, "end": 1091.0, "text": " And resisting entropic forces is an idea that I think most people find relatively intuitive.", "tokens": [50412, 400, 43940, 948, 39173, 5874, 307, 364, 1558, 300, 286, 519, 881, 561, 915, 7226, 21769, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1424867923443134, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010010527912527323}, {"id": 248, "seek": 108380, "start": 1091.6399999999999, "end": 1099.96, "text": " So the idea that the physical systems will tend to increase their entropy over time,", "tokens": [50756, 407, 264, 1558, 300, 264, 4001, 3652, 486, 3928, 281, 3488, 641, 30867, 670, 565, 11, 51172], "temperature": 0.0, "avg_logprob": -0.1424867923443134, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010010527912527323}, {"id": 249, "seek": 108380, "start": 1101.6399999999999, "end": 1108.2, "text": " at least close systems, so that over time things will gradually dissipate, things that are highly", "tokens": [51256, 412, 1935, 1998, 3652, 11, 370, 300, 670, 565, 721, 486, 13145, 29544, 473, 11, 721, 300, 366, 5405, 51584], "temperature": 0.0, "avg_logprob": -0.1424867923443134, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010010527912527323}, {"id": 250, "seek": 108380, "start": 1108.2, "end": 1112.2, "text": " structured and highly ordered and can only exist in a very small number of configurations and", "tokens": [51584, 18519, 293, 5405, 8866, 293, 393, 787, 2514, 294, 257, 588, 1359, 1230, 295, 31493, 293, 51784], "temperature": 0.0, "avg_logprob": -0.1424867923443134, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010010527912527323}, {"id": 251, "seek": 111220, "start": 1112.28, "end": 1117.24, "text": " more likely to change into something that can exist in many different configurations than they", "tokens": [50368, 544, 3700, 281, 1319, 666, 746, 300, 393, 2514, 294, 867, 819, 31493, 813, 436, 50616], "temperature": 0.0, "avg_logprob": -0.06390980232593625, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.001117815962061286}, {"id": 252, "seek": 111220, "start": 1117.24, "end": 1123.64, "text": " are to go in the opposite direction. But anything that persists over time and maintains its form", "tokens": [50616, 366, 281, 352, 294, 264, 6182, 3513, 13, 583, 1340, 300, 868, 1751, 670, 565, 293, 33385, 1080, 1254, 50936], "temperature": 0.0, "avg_logprob": -0.06390980232593625, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.001117815962061286}, {"id": 253, "seek": 111220, "start": 1123.64, "end": 1129.64, "text": " clearly resists that process of decay, at least to some extent, or at least for some period of time.", "tokens": [50936, 4448, 725, 1751, 300, 1399, 295, 21039, 11, 412, 1935, 281, 512, 8396, 11, 420, 412, 1935, 337, 512, 2896, 295, 565, 13, 51236], "temperature": 0.0, "avg_logprob": -0.06390980232593625, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.001117815962061286}, {"id": 254, "seek": 111220, "start": 1131.32, "end": 1136.44, "text": " However, the opposite is also true. We're also not creatures that tend towards a zero entropy", "tokens": [51320, 2908, 11, 264, 6182, 307, 611, 2074, 13, 492, 434, 611, 406, 12281, 300, 3928, 3030, 257, 4018, 30867, 51576], "temperature": 0.0, "avg_logprob": -0.06390980232593625, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.001117815962061286}, {"id": 255, "seek": 113644, "start": 1136.44, "end": 1142.8400000000001, "text": " state. We don't end up in a single configuration. We have to be flexible. We have to change in", "tokens": [50364, 1785, 13, 492, 500, 380, 917, 493, 294, 257, 2167, 11694, 13, 492, 362, 281, 312, 11358, 13, 492, 362, 281, 1319, 294, 50684], "temperature": 0.0, "avg_logprob": -0.0737123435802674, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.03319841995835304}, {"id": 256, "seek": 113644, "start": 1142.8400000000001, "end": 1151.3200000000002, "text": " various ways throughout our lifetime or even throughout our daily routine. So it's not quite", "tokens": [50684, 3683, 2098, 3710, 527, 11364, 420, 754, 3710, 527, 5212, 9927, 13, 407, 309, 311, 406, 1596, 51108], "temperature": 0.0, "avg_logprob": -0.0737123435802674, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.03319841995835304}, {"id": 257, "seek": 113644, "start": 1151.3200000000002, "end": 1157.96, "text": " as simple as just saying you have to resist entropic change. It's more to say that entropic", "tokens": [51108, 382, 2199, 382, 445, 1566, 291, 362, 281, 4597, 948, 39173, 1319, 13, 467, 311, 544, 281, 584, 300, 948, 39173, 51440], "temperature": 0.0, "avg_logprob": -0.0737123435802674, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.03319841995835304}, {"id": 258, "seek": 113644, "start": 1157.96, "end": 1162.76, "text": " change or the amount of entropy that you expect to develop has to be bounded both from above", "tokens": [51440, 1319, 420, 264, 2372, 295, 30867, 300, 291, 2066, 281, 1499, 575, 281, 312, 37498, 1293, 490, 3673, 51680], "temperature": 0.0, "avg_logprob": -0.0737123435802674, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.03319841995835304}, {"id": 259, "seek": 116276, "start": 1162.76, "end": 1168.6, "text": " and below, that there is a sort of optimum level to be at. And that optimum probably varies from", "tokens": [50364, 293, 2507, 11, 300, 456, 307, 257, 1333, 295, 39326, 1496, 281, 312, 412, 13, 400, 300, 39326, 1391, 21716, 490, 50656], "temperature": 0.0, "avg_logprob": -0.07754991656151887, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0007549852598458529}, {"id": 260, "seek": 116276, "start": 1168.6, "end": 1173.56, "text": " different, well, from person to person, from creature to creature, from thing to thing.", "tokens": [50656, 819, 11, 731, 11, 490, 954, 281, 954, 11, 490, 12797, 281, 12797, 11, 490, 551, 281, 551, 13, 50904], "temperature": 0.0, "avg_logprob": -0.07754991656151887, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0007549852598458529}, {"id": 261, "seek": 116276, "start": 1174.12, "end": 1179.24, "text": " You could imagine a rock that doesn't need to do much. Its interface with the environment is", "tokens": [50932, 509, 727, 3811, 257, 3727, 300, 1177, 380, 643, 281, 360, 709, 13, 6953, 9226, 365, 264, 2823, 307, 51188], "temperature": 0.0, "avg_logprob": -0.07754991656151887, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0007549852598458529}, {"id": 262, "seek": 116276, "start": 1179.24, "end": 1187.16, "text": " quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist,", "tokens": [51188, 1596, 26703, 5717, 505, 382, 12554, 13, 492, 366, 6252, 16950, 13, 407, 337, 505, 281, 2354, 281, 2514, 11, 51584], "temperature": 0.0, "avg_logprob": -0.07754991656151887, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0007549852598458529}, {"id": 263, "seek": 116276, "start": 1187.16, "end": 1191.08, "text": " we have many more ways of interfacing with the environment and we need to plan", "tokens": [51584, 321, 362, 867, 544, 2098, 295, 14510, 5615, 365, 264, 2823, 293, 321, 643, 281, 1393, 51780], "temperature": 0.0, "avg_logprob": -0.07754991656151887, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0007549852598458529}, {"id": 264, "seek": 119108, "start": 1191.08, "end": 1196.52, "text": " many more steps ahead. So is that just a pure continuum between rocks and people?", "tokens": [50364, 867, 544, 4439, 2286, 13, 407, 307, 300, 445, 257, 6075, 36120, 1296, 10989, 293, 561, 30, 50636], "temperature": 0.0, "avg_logprob": -0.09779450062955364, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0013260864652693272}, {"id": 265, "seek": 119108, "start": 1199.1599999999999, "end": 1207.8799999999999, "text": " I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy", "tokens": [50768, 286, 914, 11, 294, 8665, 11, 2086, 13, 286, 914, 11, 264, 10710, 295, 300, 37617, 11, 295, 300, 7335, 295, 30867, 51204], "temperature": 0.0, "avg_logprob": -0.09779450062955364, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0013260864652693272}, {"id": 266, "seek": 119108, "start": 1207.8799999999999, "end": 1211.72, "text": " will depend very much on what you are. And as you say, you could imagine a whole scale of", "tokens": [51204, 486, 5672, 588, 709, 322, 437, 291, 366, 13, 400, 382, 291, 584, 11, 291, 727, 3811, 257, 1379, 4373, 295, 51396], "temperature": 0.0, "avg_logprob": -0.09779450062955364, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0013260864652693272}, {"id": 267, "seek": 119108, "start": 1211.72, "end": 1217.96, "text": " things in between. I mean, in a way that as you've highlighted with the rock,", "tokens": [51396, 721, 294, 1296, 13, 286, 914, 11, 294, 257, 636, 300, 382, 291, 600, 17173, 365, 264, 3727, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09779450062955364, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.0013260864652693272}, {"id": 268, "seek": 121796, "start": 1217.96, "end": 1221.56, "text": " some of the most boring things are the things with the, sorry, I shouldn't say that,", "tokens": [50364, 512, 295, 264, 881, 9989, 721, 366, 264, 721, 365, 264, 11, 2597, 11, 286, 4659, 380, 584, 300, 11, 50544], "temperature": 0.0, "avg_logprob": -0.11929954100992078, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0024254624731838703}, {"id": 269, "seek": 121796, "start": 1221.56, "end": 1228.6000000000001, "text": " poor geologists who might find rocks very interesting. And I'm sure are very complex, but", "tokens": [50544, 4716, 1519, 12256, 567, 1062, 915, 10989, 588, 1880, 13, 400, 286, 478, 988, 366, 588, 3997, 11, 457, 50896], "temperature": 0.0, "avg_logprob": -0.11929954100992078, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0024254624731838703}, {"id": 270, "seek": 121796, "start": 1230.76, "end": 1235.72, "text": " from a sort of behavioral perspective, clearly things like us are much more interesting to study", "tokens": [51004, 490, 257, 1333, 295, 19124, 4585, 11, 4448, 721, 411, 505, 366, 709, 544, 1880, 281, 2979, 51252], "temperature": 0.0, "avg_logprob": -0.11929954100992078, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0024254624731838703}, {"id": 271, "seek": 121796, "start": 1235.72, "end": 1240.6000000000001, "text": " than things like a rock. And part of that is that we actually have a higher degree of entropy in", "tokens": [51252, 813, 721, 411, 257, 3727, 13, 400, 644, 295, 300, 307, 300, 321, 767, 362, 257, 2946, 4314, 295, 30867, 294, 51496], "temperature": 0.0, "avg_logprob": -0.11929954100992078, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0024254624731838703}, {"id": 272, "seek": 121796, "start": 1240.6000000000001, "end": 1247.64, "text": " how we live our daily lives compared to things like, I almost said organisms like rocks,", "tokens": [51496, 577, 321, 1621, 527, 5212, 2909, 5347, 281, 721, 411, 11, 286, 1920, 848, 22110, 411, 10989, 11, 51848], "temperature": 0.0, "avg_logprob": -0.11929954100992078, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0024254624731838703}, {"id": 273, "seek": 124764, "start": 1247.64, "end": 1253.0800000000002, "text": " but things like rocks that are not behaving. The reason I was thinking about this is the", "tokens": [50364, 457, 721, 411, 10989, 300, 366, 406, 35263, 13, 440, 1778, 286, 390, 1953, 466, 341, 307, 264, 50636], "temperature": 0.0, "avg_logprob": -0.06824377289524784, "compression_ratio": 1.7, "no_speech_prob": 0.0004711398796644062}, {"id": 274, "seek": 124764, "start": 1253.0800000000002, "end": 1258.5200000000002, "text": " second law of thermodynamics was conceived, I don't know, 150 years ago or something like that.", "tokens": [50636, 1150, 2101, 295, 8810, 35483, 390, 34898, 11, 286, 500, 380, 458, 11, 8451, 924, 2057, 420, 746, 411, 300, 13, 50908], "temperature": 0.0, "avg_logprob": -0.06824377289524784, "compression_ratio": 1.7, "no_speech_prob": 0.0004711398796644062}, {"id": 275, "seek": 124764, "start": 1258.5200000000002, "end": 1263.72, "text": " And many people at the time thought that it was an affront on free will. I think the religious", "tokens": [50908, 400, 867, 561, 412, 264, 565, 1194, 300, 309, 390, 364, 2096, 10001, 322, 1737, 486, 13, 286, 519, 264, 7185, 51168], "temperature": 0.0, "avg_logprob": -0.06824377289524784, "compression_ratio": 1.7, "no_speech_prob": 0.0004711398796644062}, {"id": 276, "seek": 124764, "start": 1263.72, "end": 1269.5600000000002, "text": " people at the time were aghast at the idea that things were mapped out in this way.", "tokens": [51168, 561, 412, 264, 565, 645, 623, 71, 525, 412, 264, 1558, 300, 721, 645, 33318, 484, 294, 341, 636, 13, 51460], "temperature": 0.0, "avg_logprob": -0.06824377289524784, "compression_ratio": 1.7, "no_speech_prob": 0.0004711398796644062}, {"id": 277, "seek": 124764, "start": 1269.5600000000002, "end": 1275.5600000000002, "text": " It's always worth saying in this discussion that obviously the tendency for entropy to increase", "tokens": [51460, 467, 311, 1009, 3163, 1566, 294, 341, 5017, 300, 2745, 264, 18187, 337, 30867, 281, 3488, 51760], "temperature": 0.0, "avg_logprob": -0.06824377289524784, "compression_ratio": 1.7, "no_speech_prob": 0.0004711398796644062}, {"id": 278, "seek": 127556, "start": 1275.56, "end": 1282.52, "text": " from a physical perspective generally relates to closed systems of which we are not. And as soon", "tokens": [50364, 490, 257, 4001, 4585, 5101, 16155, 281, 5395, 3652, 295, 597, 321, 366, 406, 13, 400, 382, 2321, 50712], "temperature": 0.0, "avg_logprob": -0.05461154332975062, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.004093346185982227}, {"id": 279, "seek": 127556, "start": 1282.52, "end": 1287.6399999999999, "text": " as you start talking about different compartments and interactions between them, you also introduce", "tokens": [50712, 382, 291, 722, 1417, 466, 819, 18113, 1117, 293, 13280, 1296, 552, 11, 291, 611, 5366, 50968], "temperature": 0.0, "avg_logprob": -0.05461154332975062, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.004093346185982227}, {"id": 280, "seek": 127556, "start": 1287.6399999999999, "end": 1293.56, "text": " the idea of several coupled systems. And so you can start to ask questions about the overall entropy", "tokens": [50968, 264, 1558, 295, 2940, 29482, 3652, 13, 400, 370, 291, 393, 722, 281, 1029, 1651, 466, 264, 4787, 30867, 51264], "temperature": 0.0, "avg_logprob": -0.05461154332975062, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.004093346185982227}, {"id": 281, "seek": 127556, "start": 1293.56, "end": 1301.72, "text": " or the entropy of specific parts of that system. And agents and worlds are two compartments and", "tokens": [51264, 420, 264, 30867, 295, 2685, 3166, 295, 300, 1185, 13, 400, 12554, 293, 13401, 366, 732, 18113, 1117, 293, 51672], "temperature": 0.0, "avg_logprob": -0.05461154332975062, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.004093346185982227}, {"id": 282, "seek": 130172, "start": 1301.72, "end": 1307.16, "text": " systems that exchange things with one another. And so are not closed systems almost by definition", "tokens": [50364, 3652, 300, 7742, 721, 365, 472, 1071, 13, 400, 370, 366, 406, 5395, 3652, 1920, 538, 7123, 50636], "temperature": 0.0, "avg_logprob": -0.0814874200259938, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0015919337747618556}, {"id": 283, "seek": 130172, "start": 1307.16, "end": 1313.0, "text": " that a closed system, again, from a sort of neuroscience standpoint is not necessarily", "tokens": [50636, 300, 257, 5395, 1185, 11, 797, 11, 490, 257, 1333, 295, 42762, 15827, 307, 406, 4725, 50928], "temperature": 0.0, "avg_logprob": -0.0814874200259938, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0015919337747618556}, {"id": 284, "seek": 130172, "start": 1313.0, "end": 1320.84, "text": " a very interesting system. So probably that deals with a large part of that. The question of free", "tokens": [50928, 257, 588, 1880, 1185, 13, 407, 1391, 300, 11215, 365, 257, 2416, 644, 295, 300, 13, 440, 1168, 295, 1737, 51320], "temperature": 0.0, "avg_logprob": -0.0814874200259938, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0015919337747618556}, {"id": 285, "seek": 130172, "start": 1320.84, "end": 1325.72, "text": " will is always an interesting one and always a thorny one that I'm not going to claim to have any", "tokens": [51320, 486, 307, 1009, 364, 1880, 472, 293, 1009, 257, 11588, 1634, 472, 300, 286, 478, 406, 516, 281, 3932, 281, 362, 604, 51564], "temperature": 0.0, "avg_logprob": -0.0814874200259938, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0015919337747618556}, {"id": 286, "seek": 132572, "start": 1325.8, "end": 1332.92, "text": " expertise on or be able to answer. But I think it probably tackles a slightly", "tokens": [50368, 11769, 322, 420, 312, 1075, 281, 1867, 13, 583, 286, 519, 309, 1391, 9426, 904, 257, 4748, 50724], "temperature": 0.0, "avg_logprob": -0.06465310928149101, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.00448921974748373}, {"id": 287, "seek": 132572, "start": 1332.92, "end": 1339.4, "text": " different thing from a cognitive science perspective, which is whether or not we believe", "tokens": [50724, 819, 551, 490, 257, 15605, 3497, 4585, 11, 597, 307, 1968, 420, 406, 321, 1697, 51048], "temperature": 0.0, "avg_logprob": -0.06465310928149101, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.00448921974748373}, {"id": 288, "seek": 132572, "start": 1339.4, "end": 1344.92, "text": " that the actions we're taking are actions that we've chosen. And that probably comes back into", "tokens": [51048, 300, 264, 5909, 321, 434, 1940, 366, 5909, 300, 321, 600, 8614, 13, 400, 300, 1391, 1487, 646, 666, 51324], "temperature": 0.0, "avg_logprob": -0.06465310928149101, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.00448921974748373}, {"id": 289, "seek": 132572, "start": 1344.92, "end": 1350.44, "text": " another aspect of active inference, which is that idea that the way we're regulating our", "tokens": [51324, 1071, 4171, 295, 4967, 38253, 11, 597, 307, 300, 1558, 300, 264, 636, 321, 434, 46715, 527, 51600], "temperature": 0.0, "avg_logprob": -0.06465310928149101, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.00448921974748373}, {"id": 290, "seek": 135044, "start": 1350.44, "end": 1357.16, "text": " worlds, the way we're perhaps changing the entropy of our environment depends upon our own", "tokens": [50364, 13401, 11, 264, 636, 321, 434, 4317, 4473, 264, 30867, 295, 527, 2823, 5946, 3564, 527, 1065, 50700], "temperature": 0.0, "avg_logprob": -0.10845750101496664, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003334322012960911}, {"id": 291, "seek": 135044, "start": 1358.68, "end": 1365.16, "text": " choices about it, our inferences about which one we're going to do next. And that feeds into things", "tokens": [50776, 7994, 466, 309, 11, 527, 13596, 2667, 466, 597, 472, 321, 434, 516, 281, 360, 958, 13, 400, 300, 23712, 666, 721, 51100], "temperature": 0.0, "avg_logprob": -0.10845750101496664, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003334322012960911}, {"id": 292, "seek": 135044, "start": 1365.16, "end": 1372.6000000000001, "text": " like we've spoken about free energy, that that quantity that we use to both choose our actions,", "tokens": [51100, 411, 321, 600, 10759, 466, 1737, 2281, 11, 300, 300, 11275, 300, 321, 764, 281, 1293, 2826, 527, 5909, 11, 51472], "temperature": 0.0, "avg_logprob": -0.10845750101496664, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003334322012960911}, {"id": 293, "seek": 135044, "start": 1373.48, "end": 1378.04, "text": " an act in the world around us while also drawing inferences. But we can also talk about things", "tokens": [51516, 364, 605, 294, 264, 1002, 926, 505, 1339, 611, 6316, 13596, 2667, 13, 583, 321, 393, 611, 751, 466, 721, 51744], "temperature": 0.0, "avg_logprob": -0.10845750101496664, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003334322012960911}, {"id": 294, "seek": 137804, "start": 1378.04, "end": 1383.6399999999999, "text": " like expected free energy, which is a way of evaluating our future state and what would be a", "tokens": [50364, 411, 5176, 1737, 2281, 11, 597, 307, 257, 636, 295, 27479, 527, 2027, 1785, 293, 437, 576, 312, 257, 50644], "temperature": 0.0, "avg_logprob": -0.07277743793228297, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0011171238729730248}, {"id": 295, "seek": 137804, "start": 1383.6399999999999, "end": 1389.3999999999999, "text": " good trajectory or a good way for the world to play out. And their entropy has a completely", "tokens": [50644, 665, 21512, 420, 257, 665, 636, 337, 264, 1002, 281, 862, 484, 13, 400, 641, 30867, 575, 257, 2584, 50932], "temperature": 0.0, "avg_logprob": -0.07277743793228297, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0011171238729730248}, {"id": 296, "seek": 137804, "start": 1389.3999999999999, "end": 1395.56, "text": " different meaning and there are different sorts of entropy. So for instance, if I were choosing", "tokens": [50932, 819, 3620, 293, 456, 366, 819, 7527, 295, 30867, 13, 407, 337, 5197, 11, 498, 286, 645, 10875, 51240], "temperature": 0.0, "avg_logprob": -0.07277743793228297, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0011171238729730248}, {"id": 297, "seek": 137804, "start": 1395.56, "end": 1399.24, "text": " between several different eye movements I could make while looking around this room,", "tokens": [51240, 1296, 2940, 819, 3313, 9981, 286, 727, 652, 1339, 1237, 926, 341, 1808, 11, 51424], "temperature": 0.0, "avg_logprob": -0.07277743793228297, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0011171238729730248}, {"id": 298, "seek": 137804, "start": 1401.32, "end": 1406.84, "text": " the best eye movements I might choose are those for which I'm least certain about what I would see.", "tokens": [51528, 264, 1151, 3313, 9981, 286, 1062, 2826, 366, 729, 337, 597, 286, 478, 1935, 1629, 466, 437, 286, 576, 536, 13, 51804], "temperature": 0.0, "avg_logprob": -0.07277743793228297, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.0011171238729730248}, {"id": 299, "seek": 140684, "start": 1406.84, "end": 1412.28, "text": " In other words, the highest entropy distribution. So once you start planning in the future and once", "tokens": [50364, 682, 661, 2283, 11, 264, 6343, 30867, 7316, 13, 407, 1564, 291, 722, 5038, 294, 264, 2027, 293, 1564, 50636], "temperature": 0.0, "avg_logprob": -0.07881936749208321, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0012138050515204668}, {"id": 300, "seek": 140684, "start": 1412.28, "end": 1416.9199999999998, "text": " you start selecting things to resolve your uncertainty and to be more confident about the", "tokens": [50636, 291, 722, 18182, 721, 281, 14151, 428, 15697, 293, 281, 312, 544, 6679, 466, 264, 50868], "temperature": 0.0, "avg_logprob": -0.07881936749208321, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0012138050515204668}, {"id": 301, "seek": 140684, "start": 1416.9199999999998, "end": 1422.6799999999998, "text": " world around you, you actually end up seeking out entropy, which it seems to then very much", "tokens": [50868, 1002, 926, 291, 11, 291, 767, 917, 493, 11670, 484, 30867, 11, 597, 309, 2544, 281, 550, 588, 709, 51156], "temperature": 0.0, "avg_logprob": -0.07881936749208321, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0012138050515204668}, {"id": 302, "seek": 140684, "start": 1422.6799999999998, "end": 1426.1999999999998, "text": " contradict some of the other ideas that we were talking about, the idea that we're constantly", "tokens": [51156, 28900, 512, 295, 264, 661, 3487, 300, 321, 645, 1417, 466, 11, 264, 1558, 300, 321, 434, 6460, 51332], "temperature": 0.0, "avg_logprob": -0.07881936749208321, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0012138050515204668}, {"id": 303, "seek": 140684, "start": 1426.1999999999998, "end": 1432.1999999999998, "text": " resisting it. But actually it's by seeking out the things that we're least certain about that we", "tokens": [51332, 43940, 309, 13, 583, 767, 309, 311, 538, 11670, 484, 264, 721, 300, 321, 434, 1935, 1629, 466, 300, 321, 51632], "temperature": 0.0, "avg_logprob": -0.07881936749208321, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0012138050515204668}, {"id": 304, "seek": 143220, "start": 1432.2, "end": 1437.8, "text": " can start to resolve that uncertainty and start to become more confident and more certain about the", "tokens": [50364, 393, 722, 281, 14151, 300, 15697, 293, 722, 281, 1813, 544, 6679, 293, 544, 1629, 466, 264, 50644], "temperature": 0.0, "avg_logprob": -0.07969505678523671, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0013450741535052657}, {"id": 305, "seek": 143220, "start": 1437.8, "end": 1442.92, "text": " world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what", "tokens": [50644, 1002, 926, 505, 13, 1079, 11, 4597, 30867, 538, 11670, 309, 484, 13, 663, 311, 257, 857, 295, 257, 26221, 13, 583, 754, 437, 50900], "temperature": 0.0, "avg_logprob": -0.07969505678523671, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0013450741535052657}, {"id": 306, "seek": 143220, "start": 1442.92, "end": 1449.56, "text": " you were saying just a second ago about this description of how agents operate, it's very", "tokens": [50900, 291, 645, 1566, 445, 257, 1150, 2057, 466, 341, 3855, 295, 577, 12554, 9651, 11, 309, 311, 588, 51232], "temperature": 0.0, "avg_logprob": -0.07969505678523671, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0013450741535052657}, {"id": 307, "seek": 143220, "start": 1449.56, "end": 1455.32, "text": " principled. We were talking about this balancing epistemic foraging versus sticking with what you", "tokens": [51232, 3681, 15551, 13, 492, 645, 1417, 466, 341, 22495, 2388, 468, 3438, 337, 3568, 5717, 13465, 365, 437, 291, 51520], "temperature": 0.0, "avg_logprob": -0.07969505678523671, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0013450741535052657}, {"id": 308, "seek": 145532, "start": 1455.32, "end": 1463.08, "text": " know. And more broadly speaking, thinking of agency as this sophisticated cognition of", "tokens": [50364, 458, 13, 400, 544, 19511, 4124, 11, 1953, 295, 7934, 382, 341, 16950, 46905, 295, 50752], "temperature": 0.0, "avg_logprob": -0.12281248542699921, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.4028310775756836}, {"id": 309, "seek": 145532, "start": 1463.08, "end": 1467.3999999999999, "text": " having preferences and bending the environment and so on. And I guess where I was going before", "tokens": [50752, 1419, 21910, 293, 22487, 264, 2823, 293, 370, 322, 13, 400, 286, 2041, 689, 286, 390, 516, 949, 50968], "temperature": 0.0, "avg_logprob": -0.12281248542699921, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.4028310775756836}, {"id": 310, "seek": 145532, "start": 1467.3999999999999, "end": 1475.1599999999999, "text": " is it's tempting to think that this erodes free will. And I think of them quite adjacently in", "tokens": [50968, 307, 309, 311, 37900, 281, 519, 300, 341, 1189, 4789, 1737, 486, 13, 400, 286, 519, 295, 552, 1596, 22940, 2276, 294, 51356], "temperature": 0.0, "avg_logprob": -0.12281248542699921, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.4028310775756836}, {"id": 311, "seek": 145532, "start": 1475.1599999999999, "end": 1481.48, "text": " my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't", "tokens": [51356, 452, 1575, 13, 759, 1340, 11, 286, 2041, 286, 576, 818, 2059, 257, 1737, 486, 13147, 11607, 468, 11, 597, 1355, 309, 1177, 380, 51672], "temperature": 0.0, "avg_logprob": -0.12281248542699921, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.4028310775756836}, {"id": 312, "seek": 148148, "start": 1481.56, "end": 1489.8, "text": " matter that it's predetermined. For me, free will, I'll try not to use the word free will, but", "tokens": [50368, 1871, 300, 309, 311, 3852, 35344, 2001, 13, 1171, 385, 11, 1737, 486, 11, 286, 603, 853, 406, 281, 764, 264, 1349, 1737, 486, 11, 457, 50780], "temperature": 0.0, "avg_logprob": -0.11570014758985869, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004752122797071934}, {"id": 313, "seek": 148148, "start": 1489.8, "end": 1494.3600000000001, "text": " thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant.", "tokens": [50780, 1953, 295, 7934, 294, 341, 16950, 636, 11, 1968, 309, 311, 3852, 35344, 2001, 420, 406, 307, 28682, 13, 51008], "temperature": 0.0, "avg_logprob": -0.11570014758985869, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004752122797071934}, {"id": 314, "seek": 148148, "start": 1494.3600000000001, "end": 1502.3600000000001, "text": " It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency", "tokens": [51008, 467, 311, 264, 3997, 15679, 300, 11365, 16423, 452, 7934, 490, 2618, 1646, 311, 13, 407, 286, 519, 7934, 51408], "temperature": 0.0, "avg_logprob": -0.11570014758985869, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004752122797071934}, {"id": 315, "seek": 148148, "start": 1502.3600000000001, "end": 1507.96, "text": " is better to think of than free will, if that makes sense. Yeah. And I think that's probably right.", "tokens": [51408, 307, 1101, 281, 519, 295, 813, 1737, 486, 11, 498, 300, 1669, 2020, 13, 865, 13, 400, 286, 519, 300, 311, 1391, 558, 13, 51688], "temperature": 0.0, "avg_logprob": -0.11570014758985869, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004752122797071934}, {"id": 316, "seek": 150796, "start": 1508.2, "end": 1512.6000000000001, "text": " And the experience of and the inference of agency as well, I think is part of that.", "tokens": [50376, 400, 264, 1752, 295, 293, 264, 38253, 295, 7934, 382, 731, 11, 286, 519, 307, 644, 295, 300, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09201168432468321, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.011290761642158031}, {"id": 317, "seek": 150796, "start": 1513.8, "end": 1517.8, "text": " There's a potential link that you can draw here also to the idea of chaotic,", "tokens": [50656, 821, 311, 257, 3995, 2113, 300, 291, 393, 2642, 510, 611, 281, 264, 1558, 295, 27013, 11, 50856], "temperature": 0.0, "avg_logprob": -0.09201168432468321, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.011290761642158031}, {"id": 318, "seek": 150796, "start": 1517.8, "end": 1527.0, "text": " dynamical systems of which we essentially are examples. And the idea of chaos in that setting", "tokens": [50856, 5999, 804, 3652, 295, 597, 321, 4476, 366, 5110, 13, 400, 264, 1558, 295, 14158, 294, 300, 3287, 51316], "temperature": 0.0, "avg_logprob": -0.09201168432468321, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.011290761642158031}, {"id": 319, "seek": 150796, "start": 1527.0, "end": 1533.96, "text": " is that if you start from two ever so slightly different initial conditions, your path and your", "tokens": [51316, 307, 300, 498, 291, 722, 490, 732, 1562, 370, 4748, 819, 5883, 4487, 11, 428, 3100, 293, 428, 51664], "temperature": 0.0, "avg_logprob": -0.09201168432468321, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.011290761642158031}, {"id": 320, "seek": 153396, "start": 1533.96, "end": 1539.0, "text": " future may unfold in a completely different way. And I think that fits very nicely with what you're", "tokens": [50364, 2027, 815, 17980, 294, 257, 2584, 819, 636, 13, 400, 286, 519, 300, 9001, 588, 9594, 365, 437, 291, 434, 50616], "temperature": 0.0, "avg_logprob": -0.08342941752019918, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.017475305125117302}, {"id": 321, "seek": 153396, "start": 1539.0, "end": 1545.0, "text": " saying about distinguishing my agency from somebody else's because you don't see it as if I were,", "tokens": [50616, 1566, 466, 11365, 3807, 452, 7934, 490, 2618, 1646, 311, 570, 291, 500, 380, 536, 309, 382, 498, 286, 645, 11, 50916], "temperature": 0.0, "avg_logprob": -0.08342941752019918, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.017475305125117302}, {"id": 322, "seek": 153396, "start": 1545.8, "end": 1551.8, "text": " you know, the time going to behave in exactly the same way somebody else's. And part of the", "tokens": [50956, 291, 458, 11, 264, 565, 516, 281, 15158, 294, 2293, 264, 912, 636, 2618, 1646, 311, 13, 400, 644, 295, 264, 51256], "temperature": 0.0, "avg_logprob": -0.08342941752019918, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.017475305125117302}, {"id": 323, "seek": 153396, "start": 1551.8, "end": 1556.68, "text": " reason for that is that you end up starting from a slightly different perspective to where they are,", "tokens": [51256, 1778, 337, 300, 307, 300, 291, 917, 493, 2891, 490, 257, 4748, 819, 4585, 281, 689, 436, 366, 11, 51500], "temperature": 0.0, "avg_logprob": -0.08342941752019918, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.017475305125117302}, {"id": 324, "seek": 153396, "start": 1556.68, "end": 1559.88, "text": " and that might lead to wildly different futures for both of you.", "tokens": [51500, 293, 300, 1062, 1477, 281, 34731, 819, 26071, 337, 1293, 295, 291, 13, 51660], "temperature": 0.0, "avg_logprob": -0.08342941752019918, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.017475305125117302}, {"id": 325, "seek": 155988, "start": 1560.44, "end": 1569.48, "text": " So something I think about a lot is whether agents are ontologically real or whether they", "tokens": [50392, 407, 746, 286, 519, 466, 257, 688, 307, 1968, 12554, 366, 6592, 17157, 957, 420, 1968, 436, 50844], "temperature": 0.0, "avg_logprob": -0.13928020431334714, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.002551352372393012}, {"id": 326, "seek": 155988, "start": 1569.48, "end": 1574.7600000000002, "text": " are an instrumental fiction. And I think part of the complexity, especially with active inference", "tokens": [50844, 366, 364, 17388, 13266, 13, 400, 286, 519, 644, 295, 264, 14024, 11, 2318, 365, 4967, 38253, 51108], "temperature": 0.0, "avg_logprob": -0.13928020431334714, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.002551352372393012}, {"id": 327, "seek": 155988, "start": 1574.7600000000002, "end": 1580.92, "text": " and the free energy principle is this hierarchical nesting. So we can think of agents inside agents", "tokens": [51108, 293, 264, 1737, 2281, 8665, 307, 341, 35250, 804, 297, 8714, 13, 407, 321, 393, 519, 295, 12554, 1854, 12554, 51416], "temperature": 0.0, "avg_logprob": -0.13928020431334714, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.002551352372393012}, {"id": 328, "seek": 155988, "start": 1580.92, "end": 1587.8000000000002, "text": " inside agents. And I guess the first question is, are they real and does it matter?", "tokens": [51416, 1854, 12554, 13, 400, 286, 2041, 264, 700, 1168, 307, 11, 366, 436, 957, 293, 775, 309, 1871, 30, 51760], "temperature": 0.0, "avg_logprob": -0.13928020431334714, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.002551352372393012}, {"id": 329, "seek": 158780, "start": 1588.6, "end": 1590.84, "text": " Define real for me.", "tokens": [50404, 9548, 533, 957, 337, 385, 13, 50516], "temperature": 0.0, "avg_logprob": -0.19863136394603834, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0020754665601998568}, {"id": 330, "seek": 158780, "start": 1596.52, "end": 1606.6, "text": " Well, one argument would be that they are epiphenomenal, that they themselves don't affect", "tokens": [50800, 1042, 11, 472, 6770, 576, 312, 300, 436, 366, 2388, 647, 2932, 4726, 304, 11, 300, 436, 2969, 500, 380, 3345, 51304], "temperature": 0.0, "avg_logprob": -0.19863136394603834, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0020754665601998568}, {"id": 331, "seek": 158780, "start": 1606.6, "end": 1610.52, "text": " the system that they are. Is this a good way to think about it?", "tokens": [51304, 264, 1185, 300, 436, 366, 13, 1119, 341, 257, 665, 636, 281, 519, 466, 309, 30, 51500], "temperature": 0.0, "avg_logprob": -0.19863136394603834, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0020754665601998568}, {"id": 332, "seek": 158780, "start": 1613.96, "end": 1617.48, "text": " It is a very difficult question to try and contend with, isn't it? Because I think there", "tokens": [51672, 467, 307, 257, 588, 2252, 1168, 281, 853, 293, 660, 521, 365, 11, 1943, 380, 309, 30, 1436, 286, 519, 456, 51848], "temperature": 0.0, "avg_logprob": -0.19863136394603834, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0020754665601998568}, {"id": 333, "seek": 161748, "start": 1617.48, "end": 1622.68, "text": " are so many words that come up here that are kind of laden with different semantics or different", "tokens": [50364, 366, 370, 867, 2283, 300, 808, 493, 510, 300, 366, 733, 295, 6632, 268, 365, 819, 4361, 45298, 420, 819, 50624], "temperature": 0.0, "avg_logprob": -0.08850029240483823, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0001719392166705802}, {"id": 334, "seek": 161748, "start": 1622.68, "end": 1629.96, "text": " meanings depending upon who you speak to and which camp they come from in the sort of philosophical", "tokens": [50624, 28138, 5413, 3564, 567, 291, 1710, 281, 293, 597, 2255, 436, 808, 490, 294, 264, 1333, 295, 25066, 50988], "temperature": 0.0, "avg_logprob": -0.08850029240483823, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0001719392166705802}, {"id": 335, "seek": 161748, "start": 1629.96, "end": 1638.3600000000001, "text": " world. And that's why I sort of asked you to define real. And it's really difficult to define", "tokens": [50988, 1002, 13, 400, 300, 311, 983, 286, 1333, 295, 2351, 291, 281, 6964, 957, 13, 400, 309, 311, 534, 2252, 281, 6964, 51408], "temperature": 0.0, "avg_logprob": -0.08850029240483823, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0001719392166705802}, {"id": 336, "seek": 161748, "start": 1638.3600000000001, "end": 1644.84, "text": " what real means in that setting, isn't it? And I guess coming back to your original question there,", "tokens": [51408, 437, 957, 1355, 294, 300, 3287, 11, 1943, 380, 309, 30, 400, 286, 2041, 1348, 646, 281, 428, 3380, 1168, 456, 11, 51732], "temperature": 0.0, "avg_logprob": -0.08850029240483823, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0001719392166705802}, {"id": 337, "seek": 164484, "start": 1644.84, "end": 1649.9599999999998, "text": " for me, does it matter if they're a sort of real thing or not? Probably not. It matters", "tokens": [50364, 337, 385, 11, 775, 309, 1871, 498, 436, 434, 257, 1333, 295, 957, 551, 420, 406, 30, 9210, 406, 13, 467, 7001, 50620], "temperature": 0.0, "avg_logprob": -0.07741119748070127, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0001641450508031994}, {"id": 338, "seek": 164484, "start": 1649.9599999999998, "end": 1655.9599999999998, "text": " whether it's useful. And I guess that sort of brings me to a point about one of the things I", "tokens": [50620, 1968, 309, 311, 4420, 13, 400, 286, 2041, 300, 1333, 295, 5607, 385, 281, 257, 935, 466, 472, 295, 264, 721, 286, 50920], "temperature": 0.0, "avg_logprob": -0.07741119748070127, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0001641450508031994}, {"id": 339, "seek": 164484, "start": 1655.9599999999998, "end": 1661.3999999999999, "text": " find quite appealing about active inference as a way of doing science. And I think,", "tokens": [50920, 915, 1596, 23842, 466, 4967, 38253, 382, 257, 636, 295, 884, 3497, 13, 400, 286, 519, 11, 51192], "temperature": 0.0, "avg_logprob": -0.07741119748070127, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0001641450508031994}, {"id": 340, "seek": 164484, "start": 1663.72, "end": 1667.9599999999998, "text": " you know, having had an interest in things like neuroscience and psychology for some time,", "tokens": [51308, 291, 458, 11, 1419, 632, 364, 1179, 294, 721, 411, 42762, 293, 15105, 337, 512, 565, 11, 51520], "temperature": 0.0, "avg_logprob": -0.07741119748070127, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0001641450508031994}, {"id": 341, "seek": 164484, "start": 1667.9599999999998, "end": 1671.9599999999998, "text": " I often found it quite frustrating to understand what people meant and the different language", "tokens": [51520, 286, 2049, 1352, 309, 1596, 16522, 281, 1223, 437, 561, 4140, 293, 264, 819, 2856, 51720], "temperature": 0.0, "avg_logprob": -0.07741119748070127, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0001641450508031994}, {"id": 342, "seek": 167196, "start": 1671.96, "end": 1675.8, "text": " they used in psychology to understand different aspects of cognitive function.", "tokens": [50364, 436, 1143, 294, 15105, 281, 1223, 819, 7270, 295, 15605, 2445, 13, 50556], "temperature": 0.0, "avg_logprob": -0.07287189524660828, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017516881925985217}, {"id": 343, "seek": 167196, "start": 1677.32, "end": 1681.4, "text": " And I think, you know, it's worth acknowledging that actually lots of people mean completely", "tokens": [50632, 400, 286, 519, 11, 291, 458, 11, 309, 311, 3163, 30904, 300, 767, 3195, 295, 561, 914, 2584, 50836], "temperature": 0.0, "avg_logprob": -0.07287189524660828, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017516881925985217}, {"id": 344, "seek": 167196, "start": 1681.4, "end": 1688.52, "text": " different things when they say attention. And some people say attention to mean the sort of", "tokens": [50836, 819, 721, 562, 436, 584, 3202, 13, 400, 512, 561, 584, 3202, 281, 914, 264, 1333, 295, 51192], "temperature": 0.0, "avg_logprob": -0.07287189524660828, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017516881925985217}, {"id": 345, "seek": 167196, "start": 1688.52, "end": 1693.64, "text": " overt process of looking at something and paying attention to it. Other people use it to talk", "tokens": [51192, 17038, 1399, 295, 1237, 412, 746, 293, 6229, 3202, 281, 309, 13, 5358, 561, 764, 309, 281, 751, 51448], "temperature": 0.0, "avg_logprob": -0.07287189524660828, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017516881925985217}, {"id": 346, "seek": 167196, "start": 1693.64, "end": 1699.72, "text": " about that the differences in gain in different sensory channels that they're trying to pay", "tokens": [51448, 466, 300, 264, 7300, 294, 6052, 294, 819, 27233, 9235, 300, 436, 434, 1382, 281, 1689, 51752], "temperature": 0.0, "avg_logprob": -0.07287189524660828, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0017516881925985217}, {"id": 347, "seek": 169972, "start": 1699.72, "end": 1704.1200000000001, "text": " attention to or not, you know, am I paying attention to colors versus something else?", "tokens": [50364, 3202, 281, 420, 406, 11, 291, 458, 11, 669, 286, 6229, 3202, 281, 4577, 5717, 746, 1646, 30, 50584], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 348, "seek": 169972, "start": 1704.1200000000001, "end": 1707.16, "text": " And that's just turning up the volume of different pathways in your brain.", "tokens": [50584, 400, 300, 311, 445, 6246, 493, 264, 5523, 295, 819, 22988, 294, 428, 3567, 13, 50736], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 349, "seek": 169972, "start": 1708.3600000000001, "end": 1712.04, "text": " And I'm sure there are a world of other things that people mean by it as well.", "tokens": [50796, 400, 286, 478, 988, 456, 366, 257, 1002, 295, 661, 721, 300, 561, 914, 538, 309, 382, 731, 13, 50980], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 350, "seek": 169972, "start": 1712.84, "end": 1716.04, "text": " But the idea of then trying to commit to a mathematical description of these things", "tokens": [51020, 583, 264, 1558, 295, 550, 1382, 281, 5599, 281, 257, 18894, 3855, 295, 613, 721, 51180], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 351, "seek": 169972, "start": 1716.6000000000001, "end": 1721.32, "text": " means that a lot of that ambiguity just disappears, that if you put a word to a", "tokens": [51208, 1355, 300, 257, 688, 295, 300, 46519, 445, 25527, 11, 300, 498, 291, 829, 257, 1349, 281, 257, 51444], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 352, "seek": 169972, "start": 1721.32, "end": 1725.32, "text": " particular mathematical quantity, as long as you define what that mathematical quantity is", "tokens": [51444, 1729, 18894, 11275, 11, 382, 938, 382, 291, 6964, 437, 300, 18894, 11275, 307, 51644], "temperature": 0.0, "avg_logprob": -0.08081131487821056, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.0014312434941530228}, {"id": 353, "seek": 172532, "start": 1725.32, "end": 1730.6, "text": " and how it interacts with other things, then a lot of that ambiguity just isn't there.", "tokens": [50364, 293, 577, 309, 43582, 365, 661, 721, 11, 550, 257, 688, 295, 300, 46519, 445, 1943, 380, 456, 13, 50628], "temperature": 0.0, "avg_logprob": -0.07168323343450372, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007711305283010006}, {"id": 354, "seek": 172532, "start": 1730.6, "end": 1736.2, "text": " And it forces you to commit to your assumptions in a much more specific way.", "tokens": [50628, 400, 309, 5874, 291, 281, 5599, 281, 428, 17695, 294, 257, 709, 544, 2685, 636, 13, 50908], "temperature": 0.0, "avg_logprob": -0.07168323343450372, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007711305283010006}, {"id": 355, "seek": 172532, "start": 1737.1599999999999, "end": 1741.96, "text": " And so that's why I come back to say, does it necessarily matter if an agent is real or not?", "tokens": [50956, 400, 370, 300, 311, 983, 286, 808, 646, 281, 584, 11, 775, 309, 4725, 1871, 498, 364, 9461, 307, 957, 420, 406, 30, 51196], "temperature": 0.0, "avg_logprob": -0.07168323343450372, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007711305283010006}, {"id": 356, "seek": 172532, "start": 1741.96, "end": 1747.6399999999999, "text": " I don't really know what that means. But if an agent is just a description of something that is", "tokens": [51196, 286, 500, 380, 534, 458, 437, 300, 1355, 13, 583, 498, 364, 9461, 307, 445, 257, 3855, 295, 746, 300, 307, 51480], "temperature": 0.0, "avg_logprob": -0.07168323343450372, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007711305283010006}, {"id": 357, "seek": 172532, "start": 1747.6399999999999, "end": 1752.28, "text": " separated from its environment that persists for a certain length of time, that has a dynamical", "tokens": [51480, 12005, 490, 1080, 2823, 300, 868, 1751, 337, 257, 1629, 4641, 295, 565, 11, 300, 575, 257, 5999, 804, 51712], "temperature": 0.0, "avg_logprob": -0.07168323343450372, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007711305283010006}, {"id": 358, "seek": 175228, "start": 1752.36, "end": 1756.6, "text": " structure that can be written down and a set of variables that can be partitioned off from another", "tokens": [50368, 3877, 300, 393, 312, 3720, 760, 293, 257, 992, 295, 9102, 300, 393, 312, 24808, 292, 766, 490, 1071, 50580], "temperature": 0.0, "avg_logprob": -0.08978183295137139, "compression_ratio": 1.6, "no_speech_prob": 0.008518301881849766}, {"id": 359, "seek": 175228, "start": 1756.6, "end": 1763.24, "text": " bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one.", "tokens": [50580, 857, 295, 264, 1002, 13, 1171, 385, 11, 300, 311, 957, 1547, 281, 312, 4420, 13, 400, 370, 300, 311, 689, 286, 1116, 352, 365, 300, 472, 13, 50912], "temperature": 0.0, "avg_logprob": -0.08978183295137139, "compression_ratio": 1.6, "no_speech_prob": 0.008518301881849766}, {"id": 360, "seek": 175228, "start": 1763.8799999999999, "end": 1771.8, "text": " Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up", "tokens": [50944, 1079, 11, 2086, 13, 639, 307, 10343, 13, 407, 309, 311, 257, 18894, 5261, 300, 1032, 977, 264, 1002, 493, 51340], "temperature": 0.0, "avg_logprob": -0.08978183295137139, "compression_ratio": 1.6, "no_speech_prob": 0.008518301881849766}, {"id": 361, "seek": 175228, "start": 1771.8, "end": 1775.72, "text": " in an intelligent way that explains what things do and what they don't do.", "tokens": [51340, 294, 364, 13232, 636, 300, 13948, 437, 721, 360, 293, 437, 436, 500, 380, 360, 13, 51536], "temperature": 0.0, "avg_logprob": -0.08978183295137139, "compression_ratio": 1.6, "no_speech_prob": 0.008518301881849766}, {"id": 362, "seek": 177572, "start": 1776.3600000000001, "end": 1782.1200000000001, "text": " And I guess the ontological statement, maybe we can park that to one side,", "tokens": [50396, 400, 286, 2041, 264, 6592, 4383, 5629, 11, 1310, 321, 393, 3884, 300, 281, 472, 1252, 11, 50684], "temperature": 0.0, "avg_logprob": -0.09136491775512695, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.12465254962444305}, {"id": 363, "seek": 177572, "start": 1782.76, "end": 1787.32, "text": " because as you say, from a semantics point of view, people have very relativistic", "tokens": [50716, 570, 382, 291, 584, 11, 490, 257, 4361, 45298, 935, 295, 1910, 11, 561, 362, 588, 21960, 3142, 50944], "temperature": 0.0, "avg_logprob": -0.09136491775512695, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.12465254962444305}, {"id": 364, "seek": 177572, "start": 1787.32, "end": 1793.08, "text": " understandings of things. And there's always the philosophical turtles all the way down. Well,", "tokens": [50944, 1223, 1109, 295, 721, 13, 400, 456, 311, 1009, 264, 25066, 32422, 439, 264, 636, 760, 13, 1042, 11, 51232], "temperature": 0.0, "avg_logprob": -0.09136491775512695, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.12465254962444305}, {"id": 365, "seek": 177572, "start": 1793.08, "end": 1797.64, "text": " is it really real? Is it really real? But one thing that is interesting, though,", "tokens": [51232, 307, 309, 534, 957, 30, 1119, 309, 534, 957, 30, 583, 472, 551, 300, 307, 1880, 11, 1673, 11, 51460], "temperature": 0.0, "avg_logprob": -0.09136491775512695, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.12465254962444305}, {"id": 366, "seek": 177572, "start": 1797.64, "end": 1803.56, "text": " about active inference is that it's quite mathematically abstract. So when we were", "tokens": [51460, 466, 4967, 38253, 307, 300, 309, 311, 1596, 44003, 12649, 13, 407, 562, 321, 645, 51756], "temperature": 0.0, "avg_logprob": -0.09136491775512695, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.12465254962444305}, {"id": 367, "seek": 180356, "start": 1803.56, "end": 1809.56, "text": " saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just", "tokens": [50364, 1566, 11, 307, 309, 957, 30, 467, 1177, 380, 754, 1715, 473, 11, 307, 309, 4001, 30, 407, 337, 1365, 11, 257, 12866, 307, 445, 50664], "temperature": 0.0, "avg_logprob": -0.10757231712341309, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.02018141560256481}, {"id": 368, "seek": 180356, "start": 1809.56, "end": 1816.52, "text": " talking about the statistical independence between states. And those don't necessarily", "tokens": [50664, 1417, 466, 264, 22820, 14640, 1296, 4368, 13, 400, 729, 500, 380, 4725, 51012], "temperature": 0.0, "avg_logprob": -0.10757231712341309, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.02018141560256481}, {"id": 369, "seek": 180356, "start": 1816.52, "end": 1821.96, "text": " correspond to physical things. So I guess it could be applied to almost anything. It could be applied", "tokens": [51012, 6805, 281, 4001, 721, 13, 407, 286, 2041, 309, 727, 312, 6456, 281, 1920, 1340, 13, 467, 727, 312, 6456, 51284], "temperature": 0.0, "avg_logprob": -0.10757231712341309, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.02018141560256481}, {"id": 370, "seek": 180356, "start": 1821.96, "end": 1827.6399999999999, "text": " to culture or memes or language or something like that. And it has been. Yes, indeed.", "tokens": [51284, 281, 3713, 420, 29730, 420, 2856, 420, 746, 411, 300, 13, 400, 309, 575, 668, 13, 1079, 11, 6451, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10757231712341309, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.02018141560256481}, {"id": 371, "seek": 182764, "start": 1828.5200000000002, "end": 1833.64, "text": " Yeah, it's a good point. And then you end up sort of dragged into the questions of what is", "tokens": [50408, 865, 11, 309, 311, 257, 665, 935, 13, 400, 550, 291, 917, 493, 1333, 295, 25717, 666, 264, 1651, 295, 437, 307, 50664], "temperature": 0.0, "avg_logprob": -0.09412347286119374, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.010983501560986042}, {"id": 372, "seek": 182764, "start": 1833.64, "end": 1839.0, "text": " physical. What does that mean? Is physical just an expression of dynamics that evolve in time?", "tokens": [50664, 4001, 13, 708, 775, 300, 914, 30, 1119, 4001, 445, 364, 6114, 295, 15679, 300, 16693, 294, 565, 30, 50932], "temperature": 0.0, "avg_logprob": -0.09412347286119374, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.010983501560986042}, {"id": 373, "seek": 182764, "start": 1839.0, "end": 1842.68, "text": " Because I mean, even committing to a temporal dimension tells you something about the world", "tokens": [50932, 1436, 286, 914, 11, 754, 26659, 281, 257, 30881, 10139, 5112, 291, 746, 466, 264, 1002, 51116], "temperature": 0.0, "avg_logprob": -0.09412347286119374, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.010983501560986042}, {"id": 374, "seek": 182764, "start": 1842.68, "end": 1848.76, "text": " you're living in. Are the boundaries that we're talking about, are the partitions,", "tokens": [51116, 291, 434, 2647, 294, 13, 2014, 264, 13180, 300, 321, 434, 1417, 466, 11, 366, 264, 644, 2451, 11, 51420], "temperature": 0.0, "avg_logprob": -0.09412347286119374, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.010983501560986042}, {"id": 375, "seek": 182764, "start": 1848.76, "end": 1855.96, "text": " are they spatial in nature or not? And, you know, I remember there was an article a little", "tokens": [51420, 366, 436, 23598, 294, 3687, 420, 406, 30, 400, 11, 291, 458, 11, 286, 1604, 456, 390, 364, 7222, 257, 707, 51780], "temperature": 0.0, "avg_logprob": -0.09412347286119374, "compression_ratio": 1.652014652014652, "no_speech_prob": 0.010983501560986042}, {"id": 376, "seek": 185596, "start": 1855.96, "end": 1862.04, "text": " while back that sort of made a lot of argument about this as to whether the partitions that", "tokens": [50364, 1339, 646, 300, 1333, 295, 1027, 257, 688, 295, 6770, 466, 341, 382, 281, 1968, 264, 644, 2451, 300, 50668], "temperature": 0.0, "avg_logprob": -0.06690222455054215, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0005396056803874671}, {"id": 377, "seek": 185596, "start": 1862.04, "end": 1867.16, "text": " divide creatures from their environments are equivalent to statements of conditional independence", "tokens": [50668, 9845, 12281, 490, 641, 12388, 366, 10344, 281, 12363, 295, 27708, 14640, 50924], "temperature": 0.0, "avg_logprob": -0.06690222455054215, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0005396056803874671}, {"id": 378, "seek": 185596, "start": 1867.16, "end": 1872.2, "text": " of the sort that are seen in machine learning or various other things. And arguing that there's", "tokens": [50924, 295, 264, 1333, 300, 366, 1612, 294, 3479, 2539, 420, 3683, 661, 721, 13, 400, 19697, 300, 456, 311, 51176], "temperature": 0.0, "avg_logprob": -0.06690222455054215, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0005396056803874671}, {"id": 379, "seek": 185596, "start": 1872.2, "end": 1879.72, "text": " something inherently different about a physical boundary. For me, I was never completely convinced", "tokens": [51176, 746, 27993, 819, 466, 257, 4001, 12866, 13, 1171, 385, 11, 286, 390, 1128, 2584, 12561, 51552], "temperature": 0.0, "avg_logprob": -0.06690222455054215, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0005396056803874671}, {"id": 380, "seek": 185596, "start": 1879.72, "end": 1883.8, "text": " by that, but partly because you have to then define what you mean by a physical boundary.", "tokens": [51552, 538, 300, 11, 457, 17031, 570, 291, 362, 281, 550, 6964, 437, 291, 914, 538, 257, 4001, 12866, 13, 51756], "temperature": 0.0, "avg_logprob": -0.06690222455054215, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0005396056803874671}, {"id": 381, "seek": 188380, "start": 1883.8, "end": 1887.96, "text": " And I suspect it's the same sort of boundary, it's the same sort of conditional", "tokens": [50364, 400, 286, 9091, 309, 311, 264, 912, 1333, 295, 12866, 11, 309, 311, 264, 912, 1333, 295, 27708, 50572], "temperature": 0.0, "avg_logprob": -0.06162678970480865, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.00011907804582733661}, {"id": 382, "seek": 188380, "start": 1887.96, "end": 1894.68, "text": " dependencies and independences. But where those have specific semantics, whether those be temporal,", "tokens": [50572, 36606, 293, 4819, 2667, 13, 583, 689, 729, 362, 2685, 4361, 45298, 11, 1968, 729, 312, 30881, 11, 50908], "temperature": 0.0, "avg_logprob": -0.06162678970480865, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.00011907804582733661}, {"id": 383, "seek": 188380, "start": 1894.68, "end": 1900.84, "text": " whether they be something where, you know, you can actually define a proper spatial metric", "tokens": [50908, 1968, 436, 312, 746, 689, 11, 291, 458, 11, 291, 393, 767, 6964, 257, 2296, 23598, 20678, 51216], "temperature": 0.0, "avg_logprob": -0.06162678970480865, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.00011907804582733661}, {"id": 384, "seek": 188380, "start": 1900.84, "end": 1906.84, "text": " underneath the things that you're separating out. And clearly, that sort of boundary is very", "tokens": [51216, 7223, 264, 721, 300, 291, 434, 29279, 484, 13, 400, 4448, 11, 300, 1333, 295, 12866, 307, 588, 51516], "temperature": 0.0, "avg_logprob": -0.06162678970480865, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.00011907804582733661}, {"id": 385, "seek": 188380, "start": 1906.84, "end": 1911.6399999999999, "text": " important. But for me, that is just another form of the same sort of boundary. And as you say,", "tokens": [51516, 1021, 13, 583, 337, 385, 11, 300, 307, 445, 1071, 1254, 295, 264, 912, 1333, 295, 12866, 13, 400, 382, 291, 584, 11, 51756], "temperature": 0.0, "avg_logprob": -0.06162678970480865, "compression_ratio": 1.8393574297188755, "no_speech_prob": 0.00011907804582733661}, {"id": 386, "seek": 191164, "start": 1911.64, "end": 1917.16, "text": " you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical,", "tokens": [50364, 291, 393, 3079, 2293, 264, 912, 1333, 295, 3487, 281, 721, 300, 366, 406, 23598, 11, 406, 1333, 295, 4001, 11, 50640], "temperature": 0.0, "avg_logprob": -0.10245763494613323, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00525651266798377}, {"id": 387, "seek": 191164, "start": 1917.16, "end": 1927.24, "text": " whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing", "tokens": [50640, 2035, 300, 1062, 914, 13, 1079, 13, 1079, 13, 1436, 562, 286, 562, 286, 7179, 365, 14256, 1036, 565, 11, 286, 390, 12417, 51144], "temperature": 0.0, "avg_logprob": -0.10245763494613323, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00525651266798377}, {"id": 388, "seek": 191164, "start": 1927.24, "end": 1934.2800000000002, "text": " him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose,", "tokens": [51144, 796, 322, 341, 1558, 295, 257, 2107, 4001, 9461, 11, 293, 415, 390, 1596, 31606, 281, 264, 1558, 13, 400, 286, 7297, 11, 51496], "temperature": 0.0, "avg_logprob": -0.10245763494613323, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00525651266798377}, {"id": 389, "seek": 191164, "start": 1934.2800000000002, "end": 1939.16, "text": " even though mathematically, you could apply it first to other geometries, that would be", "tokens": [51496, 754, 1673, 44003, 11, 291, 727, 3079, 309, 700, 281, 661, 12956, 2244, 11, 300, 576, 312, 51740], "temperature": 0.0, "avg_logprob": -0.10245763494613323, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00525651266798377}, {"id": 390, "seek": 193916, "start": 1939.24, "end": 1943.16, "text": " quite easy, because they have certain mathematical properties in terms of like, you know,", "tokens": [50368, 1596, 1858, 11, 570, 436, 362, 1629, 18894, 7221, 294, 2115, 295, 411, 11, 291, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10459504502542903, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.001511847018264234}, {"id": 391, "seek": 193916, "start": 1943.16, "end": 1947.4, "text": " being locally connected and measure spaces and all that kind of stuff. But if you did say,", "tokens": [50564, 885, 16143, 4582, 293, 3481, 7673, 293, 439, 300, 733, 295, 1507, 13, 583, 498, 291, 630, 584, 11, 50776], "temperature": 0.0, "avg_logprob": -0.10459504502542903, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.001511847018264234}, {"id": 392, "seek": 193916, "start": 1947.4, "end": 1955.8000000000002, "text": " okay, I want to have an agent that represents a meme. How would that act? I don't know,", "tokens": [50776, 1392, 11, 286, 528, 281, 362, 364, 9461, 300, 8855, 257, 21701, 13, 1012, 576, 300, 605, 30, 286, 500, 380, 458, 11, 51196], "temperature": 0.0, "avg_logprob": -0.10459504502542903, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.001511847018264234}, {"id": 393, "seek": 193916, "start": 1955.8000000000002, "end": 1966.0400000000002, "text": " you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge", "tokens": [51196, 291, 483, 666, 15983, 4759, 11, 500, 380, 291, 30, 286, 7297, 291, 360, 13, 286, 519, 264, 15983, 3430, 51708], "temperature": 0.0, "avg_logprob": -0.10459504502542903, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.001511847018264234}, {"id": 394, "seek": 196604, "start": 1966.04, "end": 1971.08, "text": " is defining the boundary. I think the boundary is a very difficult thing to define sometimes", "tokens": [50364, 307, 17827, 264, 12866, 13, 286, 519, 264, 12866, 307, 257, 588, 2252, 551, 281, 6964, 2171, 50616], "temperature": 0.0, "avg_logprob": -0.08102523140285327, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03147835657000542}, {"id": 395, "seek": 196604, "start": 1971.08, "end": 1975.72, "text": " when you're dealing with something non spatial. That boundary, though, might be reflected in", "tokens": [50616, 562, 291, 434, 6260, 365, 746, 2107, 23598, 13, 663, 12866, 11, 1673, 11, 1062, 312, 15502, 294, 50848], "temperature": 0.0, "avg_logprob": -0.08102523140285327, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03147835657000542}, {"id": 396, "seek": 196604, "start": 1975.72, "end": 1984.44, "text": " the interactions between a meme and a community that that engage with it. It might be to do with", "tokens": [50848, 264, 13280, 1296, 257, 21701, 293, 257, 1768, 300, 300, 4683, 365, 309, 13, 467, 1062, 312, 281, 360, 365, 51284], "temperature": 0.0, "avg_logprob": -0.08102523140285327, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03147835657000542}, {"id": 397, "seek": 196604, "start": 1984.44, "end": 1989.56, "text": " the expression of a meme in different parts of, I don't know, a network of some sort or social", "tokens": [51284, 264, 6114, 295, 257, 21701, 294, 819, 3166, 295, 11, 286, 500, 380, 458, 11, 257, 3209, 295, 512, 1333, 420, 2093, 51540], "temperature": 0.0, "avg_logprob": -0.08102523140285327, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03147835657000542}, {"id": 398, "seek": 196604, "start": 1989.56, "end": 1995.48, "text": " network. I don't know how easy it would be. I've not tried to do it in that context. And I think", "tokens": [51540, 3209, 13, 286, 500, 380, 458, 577, 1858, 309, 576, 312, 13, 286, 600, 406, 3031, 281, 360, 309, 294, 300, 4319, 13, 400, 286, 519, 51836], "temperature": 0.0, "avg_logprob": -0.08102523140285327, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.03147835657000542}, {"id": 399, "seek": 199548, "start": 1995.48, "end": 1999.24, "text": " with many of these things, you never really know until you've had to go at doing it. But", "tokens": [50364, 365, 867, 295, 613, 721, 11, 291, 1128, 534, 458, 1826, 291, 600, 632, 281, 352, 412, 884, 309, 13, 583, 50552], "temperature": 0.0, "avg_logprob": -0.06721140089489165, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0015869562048465014}, {"id": 400, "seek": 199548, "start": 2001.4, "end": 2006.28, "text": " I suppose the key things I would be thinking about are, is there a clean way of defining", "tokens": [50660, 286, 7297, 264, 2141, 721, 286, 576, 312, 1953, 466, 366, 11, 307, 456, 257, 2541, 636, 295, 17827, 50904], "temperature": 0.0, "avg_logprob": -0.06721140089489165, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0015869562048465014}, {"id": 401, "seek": 199548, "start": 2006.28, "end": 2011.24, "text": " a boundary for a meme? Is there something that the meme is doing to the outside world?", "tokens": [50904, 257, 12866, 337, 257, 21701, 30, 1119, 456, 746, 300, 264, 21701, 307, 884, 281, 264, 2380, 1002, 30, 51152], "temperature": 0.0, "avg_logprob": -0.06721140089489165, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0015869562048465014}, {"id": 402, "seek": 199548, "start": 2011.96, "end": 2014.44, "text": " Is there something that the outside world is doing to the meme?", "tokens": [51188, 1119, 456, 746, 300, 264, 2380, 1002, 307, 884, 281, 264, 21701, 30, 51312], "temperature": 0.0, "avg_logprob": -0.06721140089489165, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0015869562048465014}, {"id": 403, "seek": 199548, "start": 2016.68, "end": 2021.8, "text": " And I think if you're able to define those things convincingly, then perhaps there is a form of", "tokens": [51424, 400, 286, 519, 498, 291, 434, 1075, 281, 6964, 729, 721, 24823, 356, 11, 550, 4317, 456, 307, 257, 1254, 295, 51680], "temperature": 0.0, "avg_logprob": -0.06721140089489165, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0015869562048465014}, {"id": 404, "seek": 202180, "start": 2021.8799999999999, "end": 2029.96, "text": " agent that may be non physical, if that's how you choose to define it. But then I'm not sure what", "tokens": [50368, 9461, 300, 815, 312, 2107, 4001, 11, 498, 300, 311, 577, 291, 2826, 281, 6964, 309, 13, 583, 550, 286, 478, 406, 988, 437, 50772], "temperature": 0.0, "avg_logprob": -0.07377743948073615, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.0009266508277505636}, {"id": 405, "seek": 202180, "start": 2029.96, "end": 2033.56, "text": " physical means in this setting. Is there also an account of saying, well, actually,", "tokens": [50772, 4001, 1355, 294, 341, 3287, 13, 1119, 456, 611, 364, 2696, 295, 1566, 11, 731, 11, 767, 11, 50952], "temperature": 0.0, "avg_logprob": -0.07377743948073615, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.0009266508277505636}, {"id": 406, "seek": 202180, "start": 2033.56, "end": 2036.6, "text": " if you can write down the dynamics of how a meme propagates through a network,", "tokens": [50952, 498, 291, 393, 2464, 760, 264, 15679, 295, 577, 257, 21701, 12425, 1024, 807, 257, 3209, 11, 51104], "temperature": 0.0, "avg_logprob": -0.07377743948073615, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.0009266508277505636}, {"id": 407, "seek": 202180, "start": 2037.32, "end": 2040.68, "text": " is that any different writing down the dynamics of another sort of physical system?", "tokens": [51140, 307, 300, 604, 819, 3579, 760, 264, 15679, 295, 1071, 1333, 295, 4001, 1185, 30, 51308], "temperature": 0.0, "avg_logprob": -0.07377743948073615, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.0009266508277505636}, {"id": 408, "seek": 202180, "start": 2041.8, "end": 2049.08, "text": " Yes, possibly not. But it is really interesting to me that something like language could be seen", "tokens": [51364, 1079, 11, 6264, 406, 13, 583, 309, 307, 534, 1880, 281, 385, 300, 746, 411, 2856, 727, 312, 1612, 51728], "temperature": 0.0, "avg_logprob": -0.07377743948073615, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.0009266508277505636}, {"id": 409, "seek": 204908, "start": 2049.08, "end": 2054.44, "text": " as a life as a super organism, or even something like religion. And it seems to tick all of the", "tokens": [50364, 382, 257, 993, 382, 257, 1687, 24128, 11, 420, 754, 746, 411, 7561, 13, 400, 309, 2544, 281, 5204, 439, 295, 264, 50632], "temperature": 0.0, "avg_logprob": -0.10891197573754095, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.020903736352920532}, {"id": 410, "seek": 204908, "start": 2054.44, "end": 2061.64, "text": " boxes that we talk about with a gentleness in physical agents, which is to say, let's say", "tokens": [50632, 9002, 300, 321, 751, 466, 365, 257, 16108, 45887, 294, 4001, 12554, 11, 597, 307, 281, 584, 11, 718, 311, 584, 50992], "temperature": 0.0, "avg_logprob": -0.10891197573754095, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.020903736352920532}, {"id": 411, "seek": 204908, "start": 2061.64, "end": 2067.56, "text": " a religion or even nationalism, you could say that the state of the Netherlands has certain", "tokens": [50992, 257, 7561, 420, 754, 39186, 11, 291, 727, 584, 300, 264, 1785, 295, 264, 20873, 575, 1629, 51288], "temperature": 0.0, "avg_logprob": -0.10891197573754095, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.020903736352920532}, {"id": 412, "seek": 204908, "start": 2067.56, "end": 2075.88, "text": " objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,", "tokens": [51288, 15961, 13, 400, 4448, 11, 456, 311, 257, 732, 636, 1399, 510, 13, 407, 264, 1785, 11807, 527, 5223, 13, 400, 321, 11, 51704], "temperature": 0.0, "avg_logprob": -0.10891197573754095, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.020903736352920532}, {"id": 413, "seek": 207588, "start": 2076.36, "end": 2083.0, "text": " our collective behavior influences the state. But this then, I think the reason why people don't", "tokens": [50388, 527, 12590, 5223, 21222, 264, 1785, 13, 583, 341, 550, 11, 286, 519, 264, 1778, 983, 561, 500, 380, 50720], "temperature": 0.0, "avg_logprob": -0.08275000945381496, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004190857522189617}, {"id": 414, "seek": 207588, "start": 2083.0, "end": 2089.0, "text": " like to think in this way is we have psychological priors. So we are biased towards seeing a", "tokens": [50720, 411, 281, 519, 294, 341, 636, 307, 321, 362, 14346, 1790, 830, 13, 407, 321, 366, 28035, 3030, 2577, 257, 51020], "temperature": 0.0, "avg_logprob": -0.08275000945381496, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004190857522189617}, {"id": 415, "seek": 207588, "start": 2089.0, "end": 2095.96, "text": " gentleness in individual humans, but we tend not to think of non physical or diffuse things as being", "tokens": [51020, 16108, 45887, 294, 2609, 6255, 11, 457, 321, 3928, 406, 281, 519, 295, 2107, 4001, 420, 42165, 721, 382, 885, 51368], "temperature": 0.0, "avg_logprob": -0.08275000945381496, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004190857522189617}, {"id": 416, "seek": 207588, "start": 2095.96, "end": 2101.48, "text": " agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole", "tokens": [51368, 12554, 13, 1079, 11, 286, 519, 300, 311, 1391, 558, 13, 400, 797, 11, 309, 1333, 295, 5607, 505, 646, 281, 341, 1379, 51644], "temperature": 0.0, "avg_logprob": -0.08275000945381496, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.004190857522189617}, {"id": 417, "seek": 210148, "start": 2101.48, "end": 2107.48, "text": " issue about the language that we use, that it comes laden with lots of prior beliefs about what it", "tokens": [50364, 2734, 466, 264, 2856, 300, 321, 764, 11, 300, 309, 1487, 6632, 268, 365, 3195, 295, 4059, 13585, 466, 437, 309, 50664], "temperature": 0.0, "avg_logprob": -0.1061971083931301, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.002329791197553277}, {"id": 418, "seek": 210148, "start": 2107.48, "end": 2116.36, "text": " means, which may vary from person to person. And there comes a point where you say, how important", "tokens": [50664, 1355, 11, 597, 815, 10559, 490, 954, 281, 954, 13, 400, 456, 1487, 257, 935, 689, 291, 584, 11, 577, 1021, 51108], "temperature": 0.0, "avg_logprob": -0.1061971083931301, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.002329791197553277}, {"id": 419, "seek": 210148, "start": 2116.36, "end": 2120.92, "text": " is it that I commit to using this particular word to mean this particular thing in this setting?", "tokens": [51108, 307, 309, 300, 286, 5599, 281, 1228, 341, 1729, 1349, 281, 914, 341, 1729, 551, 294, 341, 3287, 30, 51336], "temperature": 0.0, "avg_logprob": -0.1061971083931301, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.002329791197553277}, {"id": 420, "seek": 210148, "start": 2121.72, "end": 2129.88, "text": " But again, in your example of taking a nation or nation state as being a form of organism at a", "tokens": [51376, 583, 797, 11, 294, 428, 1365, 295, 1940, 257, 4790, 420, 4790, 1785, 382, 885, 257, 1254, 295, 24128, 412, 257, 51784], "temperature": 0.0, "avg_logprob": -0.1061971083931301, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.002329791197553277}, {"id": 421, "seek": 212988, "start": 2130.12, "end": 2137.4, "text": " higher level or form of agent, if you can show that there is a way of summarizing the dynamics of", "tokens": [50376, 2946, 1496, 420, 1254, 295, 9461, 11, 498, 291, 393, 855, 300, 456, 307, 257, 636, 295, 14611, 3319, 264, 15679, 295, 50740], "temperature": 0.0, "avg_logprob": -0.09456460193921161, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.002590569667518139}, {"id": 422, "seek": 212988, "start": 2137.4, "end": 2143.96, "text": " that system, maybe some high order summary of the behavior of people in that system, voting", "tokens": [50740, 300, 1185, 11, 1310, 512, 1090, 1668, 12691, 295, 264, 5223, 295, 561, 294, 300, 1185, 11, 10419, 51068], "temperature": 0.0, "avg_logprob": -0.09456460193921161, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.002590569667518139}, {"id": 423, "seek": 212988, "start": 2143.96, "end": 2150.36, "text": " intentions, I don't know, you might then be able to show that it behaves in exactly the same way", "tokens": [51068, 19354, 11, 286, 500, 380, 458, 11, 291, 1062, 550, 312, 1075, 281, 855, 300, 309, 36896, 294, 2293, 264, 912, 636, 51388], "temperature": 0.0, "avg_logprob": -0.09456460193921161, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.002590569667518139}, {"id": 424, "seek": 212988, "start": 2150.36, "end": 2157.48, "text": " mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've", "tokens": [51388, 44003, 382, 5346, 1951, 300, 1185, 13, 865, 13, 407, 341, 5607, 385, 322, 257, 707, 857, 886, 13, 286, 600, 51744], "temperature": 0.0, "avg_logprob": -0.09456460193921161, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.002590569667518139}, {"id": 425, "seek": 215748, "start": 2157.48, "end": 2162.04, "text": " been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.", "tokens": [50364, 668, 3760, 341, 1446, 1219, 440, 13719, 307, 36172, 538, 9449, 761, 1001, 78, 293, 286, 478, 4124, 281, 796, 322, 6984, 13, 50592], "temperature": 0.0, "avg_logprob": -0.08591916826036242, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.015003404580056667}, {"id": 426, "seek": 215748, "start": 2162.04, "end": 2167.72, "text": " And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey", "tokens": [50592, 400, 702, 2135, 747, 307, 300, 11, 286, 2041, 291, 727, 818, 796, 257, 4984, 468, 11, 415, 311, 1855, 365, 28721, 50876], "temperature": 0.0, "avg_logprob": -0.08591916826036242, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.015003404580056667}, {"id": 427, "seek": 215748, "start": 2167.72, "end": 2175.08, "text": " Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have", "tokens": [50876, 389, 12442, 13, 400, 702, 2135, 747, 307, 300, 456, 307, 572, 7161, 281, 264, 1575, 13, 407, 337, 924, 11, 41562, 362, 51244], "temperature": 0.0, "avg_logprob": -0.08591916826036242, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.015003404580056667}, {"id": 428, "seek": 215748, "start": 2175.08, "end": 2183.2400000000002, "text": " built these abstract models to reason about how we think. So we do planning, and we do reasoning,", "tokens": [51244, 3094, 613, 12649, 5245, 281, 1778, 466, 577, 321, 519, 13, 407, 321, 360, 5038, 11, 293, 321, 360, 21577, 11, 51652], "temperature": 0.0, "avg_logprob": -0.08591916826036242, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.015003404580056667}, {"id": 429, "seek": 218324, "start": 2183.24, "end": 2188.4399999999996, "text": " and we have perception, and we do this, and we do that. And also, we try and generate explanations", "tokens": [50364, 293, 321, 362, 12860, 11, 293, 321, 360, 341, 11, 293, 321, 360, 300, 13, 400, 611, 11, 321, 853, 293, 8460, 28708, 50624], "temperature": 0.0, "avg_logprob": -0.08544591393801246, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.07417259365320206}, {"id": 430, "seek": 218324, "start": 2188.4399999999996, "end": 2194.7599999999998, "text": " for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly", "tokens": [50624, 337, 527, 5223, 13, 407, 321, 360, 341, 733, 295, 2183, 16708, 1497, 455, 2776, 13, 583, 562, 291, 2979, 309, 11, 309, 311, 6252, 50940], "temperature": 0.0, "avg_logprob": -0.08544591393801246, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.07417259365320206}, {"id": 431, "seek": 218324, "start": 2194.7599999999998, "end": 2201.9599999999996, "text": " incoherent and inconsistent. And he was talking all about how the brain is actually a kind of", "tokens": [50940, 834, 78, 511, 317, 293, 36891, 13, 400, 415, 390, 1417, 439, 466, 577, 264, 3567, 307, 767, 257, 733, 295, 51300], "temperature": 0.0, "avg_logprob": -0.08544591393801246, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.07417259365320206}, {"id": 432, "seek": 218324, "start": 2201.9599999999996, "end": 2209.64, "text": " predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see", "tokens": [51300, 35521, 1185, 11, 558, 30, 407, 321, 362, 613, 588, 637, 11668, 834, 78, 511, 317, 15743, 11, 293, 321, 2171, 536, 51684], "temperature": 0.0, "avg_logprob": -0.08544591393801246, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.07417259365320206}, {"id": 433, "seek": 220964, "start": 2209.64, "end": 2214.8399999999997, "text": " things that aren't there. And I think you speak about this in your book that there was a really", "tokens": [50364, 721, 300, 3212, 380, 456, 13, 400, 286, 519, 291, 1710, 466, 341, 294, 428, 1446, 300, 456, 390, 257, 534, 50624], "temperature": 0.0, "avg_logprob": -0.07495706863985717, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.007603728212416172}, {"id": 434, "seek": 220964, "start": 2214.8399999999997, "end": 2220.7599999999998, "text": " big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of", "tokens": [50624, 955, 5513, 13, 286, 519, 291, 10839, 281, 309, 382, 264, 6128, 2592, 12, 39, 1971, 952, 1558, 300, 264, 3567, 307, 257, 733, 295, 50920], "temperature": 0.0, "avg_logprob": -0.07495706863985717, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.007603728212416172}, {"id": 435, "seek": 220964, "start": 2220.7599999999998, "end": 2226.44, "text": " prediction machine, rather than our brain just kind of like building a simulacrum of the world", "tokens": [50920, 17630, 3479, 11, 2831, 813, 527, 3567, 445, 733, 295, 411, 2390, 257, 1034, 425, 326, 6247, 295, 264, 1002, 51204], "temperature": 0.0, "avg_logprob": -0.07495706863985717, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.007603728212416172}, {"id": 436, "seek": 220964, "start": 2226.44, "end": 2233.0, "text": " around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction", "tokens": [51204, 926, 505, 13, 286, 914, 11, 577, 360, 291, 519, 466, 300, 382, 257, 28813, 5412, 468, 30, 865, 11, 286, 914, 11, 286, 519, 17630, 51532], "temperature": 0.0, "avg_logprob": -0.07495706863985717, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.007603728212416172}, {"id": 437, "seek": 220964, "start": 2233.0, "end": 2239.48, "text": " has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling", "tokens": [51532, 575, 281, 312, 257, 2141, 644, 295, 309, 13, 400, 264, 1778, 309, 311, 257, 2141, 644, 295, 309, 307, 300, 309, 311, 257, 636, 295, 37447, 51856], "temperature": 0.0, "avg_logprob": -0.07495706863985717, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.007603728212416172}, {"id": 438, "seek": 223948, "start": 2239.48, "end": 2245.64, "text": " us to our world that without prediction, you know, if you're purely simulating what might be going on", "tokens": [50364, 505, 281, 527, 1002, 300, 1553, 17630, 11, 291, 458, 11, 498, 291, 434, 17491, 1034, 12162, 437, 1062, 312, 516, 322, 50672], "temperature": 0.0, "avg_logprob": -0.08226991332737746, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.002091035945340991}, {"id": 439, "seek": 223948, "start": 2245.64, "end": 2249.96, "text": " without actually then correcting your simulation based upon what's actually going on or the input", "tokens": [50672, 1553, 767, 550, 47032, 428, 16575, 2361, 3564, 437, 311, 767, 516, 322, 420, 264, 4846, 50888], "temperature": 0.0, "avg_logprob": -0.08226991332737746, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.002091035945340991}, {"id": 440, "seek": 223948, "start": 2249.96, "end": 2255.2400000000002, "text": " you're getting from the world, then you're not going to get very far. So prediction is just an", "tokens": [50888, 291, 434, 1242, 490, 264, 1002, 11, 550, 291, 434, 406, 516, 281, 483, 588, 1400, 13, 407, 17630, 307, 445, 364, 51152], "temperature": 0.0, "avg_logprob": -0.08226991332737746, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.002091035945340991}, {"id": 441, "seek": 223948, "start": 2255.2400000000002, "end": 2260.52, "text": " efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want", "tokens": [51152, 7148, 636, 295, 6260, 365, 264, 2734, 295, 577, 360, 286, 5623, 452, 13585, 30, 1012, 360, 286, 5623, 498, 291, 528, 51416], "temperature": 0.0, "avg_logprob": -0.08226991332737746, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.002091035945340991}, {"id": 442, "seek": 223948, "start": 2260.52, "end": 2264.52, "text": " to call it a simulation? My simulation, my internal simulation of what's going on outside.", "tokens": [51416, 281, 818, 309, 257, 16575, 30, 1222, 16575, 11, 452, 6920, 16575, 295, 437, 311, 516, 322, 2380, 13, 51616], "temperature": 0.0, "avg_logprob": -0.08226991332737746, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.002091035945340991}, {"id": 443, "seek": 226452, "start": 2265.24, "end": 2271.4, "text": " And once you cease to have that constraint, once the world ceases to constrain the simulation,", "tokens": [50400, 400, 1564, 291, 27887, 281, 362, 300, 25534, 11, 1564, 264, 1002, 1769, 1957, 281, 1817, 7146, 264, 16575, 11, 50708], "temperature": 0.0, "avg_logprob": -0.16209964752197265, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.009457727894186974}, {"id": 444, "seek": 226452, "start": 2271.4, "end": 2275.48, "text": " that's the point at which you start, as you say, hallucinating, seeing things that aren't there", "tokens": [50708, 300, 311, 264, 935, 412, 597, 291, 722, 11, 382, 291, 584, 11, 35212, 8205, 11, 2577, 721, 300, 3212, 380, 456, 50912], "temperature": 0.0, "avg_logprob": -0.16209964752197265, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.009457727894186974}, {"id": 445, "seek": 226452, "start": 2276.36, "end": 2280.92, "text": " and developing beliefs that just bear no relationship to or little relationship to reality.", "tokens": [50956, 293, 6416, 13585, 300, 445, 6155, 572, 2480, 281, 420, 707, 2480, 281, 4103, 13, 51184], "temperature": 0.0, "avg_logprob": -0.16209964752197265, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.009457727894186974}, {"id": 446, "seek": 226452, "start": 2282.2, "end": 2289.16, "text": " Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a", "tokens": [51248, 865, 11, 1880, 13, 407, 286, 914, 11, 472, 551, 341, 9449, 761, 1001, 31686, 320, 390, 1566, 390, 300, 321, 536, 257, 51596], "temperature": 0.0, "avg_logprob": -0.16209964752197265, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.009457727894186974}, {"id": 447, "seek": 228916, "start": 2289.16, "end": 2295.64, "text": " complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have", "tokens": [50364, 3997, 1185, 293, 321, 6878, 437, 8033, 19027, 3093, 5498, 264, 21935, 21033, 13, 400, 300, 307, 286, 362, 50688], "temperature": 0.0, "avg_logprob": -0.11939147146124589, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.12186353653669357}, {"id": 448, "seek": 228916, "start": 2296.3599999999997, "end": 2303.08, "text": " a self model, I have a model of your mind, and I observe behavior and I kind of impute", "tokens": [50724, 257, 2698, 2316, 11, 286, 362, 257, 2316, 295, 428, 1575, 11, 293, 286, 11441, 5223, 293, 286, 733, 295, 704, 1169, 51060], "temperature": 0.0, "avg_logprob": -0.11939147146124589, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.12186353653669357}, {"id": 449, "seek": 228916, "start": 2303.96, "end": 2310.2, "text": " onto you a model and I can generate explanations. So as I say, Thomas did that because he must", "tokens": [51104, 3911, 291, 257, 2316, 293, 286, 393, 8460, 28708, 13, 407, 382, 286, 584, 11, 8500, 630, 300, 570, 415, 1633, 51416], "temperature": 0.0, "avg_logprob": -0.11939147146124589, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.12186353653669357}, {"id": 450, "seek": 228916, "start": 2310.2, "end": 2316.2799999999997, "text": " have wanted to do this. And I guess you could argue that all of this is just a confabulation.", "tokens": [51416, 362, 1415, 281, 360, 341, 13, 400, 286, 2041, 291, 727, 9695, 300, 439, 295, 341, 307, 445, 257, 1497, 455, 2776, 13, 51720], "temperature": 0.0, "avg_logprob": -0.11939147146124589, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.12186353653669357}, {"id": 451, "seek": 231628, "start": 2316.28, "end": 2324.0400000000004, "text": " It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.", "tokens": [50364, 467, 311, 445, 364, 17388, 13266, 13, 467, 311, 257, 636, 337, 505, 281, 2903, 5223, 11, 457, 309, 1177, 380, 534, 2514, 13, 50752], "temperature": 0.0, "avg_logprob": -0.07453058333623977, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0014853711472824216}, {"id": 452, "seek": 231628, "start": 2324.0400000000004, "end": 2328.92, "text": " But then there's the question of, well, it's not that it doesn't exist. It's just that your mind", "tokens": [50752, 583, 550, 456, 311, 264, 1168, 295, 11, 731, 11, 309, 311, 406, 300, 309, 1177, 380, 2514, 13, 467, 311, 445, 300, 428, 1575, 50996], "temperature": 0.0, "avg_logprob": -0.07453058333623977, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0014853711472824216}, {"id": 453, "seek": 231628, "start": 2329.48, "end": 2335.96, "text": " is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as", "tokens": [51024, 307, 14036, 40128, 694, 3545, 3997, 13, 407, 309, 311, 406, 300, 264, 1575, 307, 20488, 13, 286, 4382, 281, 519, 295, 309, 382, 51348], "temperature": 0.0, "avg_logprob": -0.07453058333623977, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0014853711472824216}, {"id": 454, "seek": 231628, "start": 2335.96, "end": 2341.48, "text": " the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an", "tokens": [51348, 264, 1575, 575, 370, 709, 7161, 300, 309, 311, 4399, 527, 15605, 18046, 13, 400, 7161, 11, 286, 519, 11, 307, 364, 51624], "temperature": 0.0, "avg_logprob": -0.07453058333623977, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0014853711472824216}, {"id": 455, "seek": 234148, "start": 2341.48, "end": 2348.12, "text": " interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and", "tokens": [50364, 1880, 10710, 382, 731, 13, 286, 914, 11, 309, 311, 264, 1558, 300, 1487, 833, 257, 688, 295, 3479, 2539, 293, 50696], "temperature": 0.0, "avg_logprob": -0.07105657187375156, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.009241115301847458}, {"id": 456, "seek": 234148, "start": 2348.12, "end": 2354.28, "text": " the idea of deep learning neural networks with multiple layers. And I think you're right that", "tokens": [50696, 264, 1558, 295, 2452, 2539, 18161, 9590, 365, 3866, 7914, 13, 400, 286, 519, 291, 434, 558, 300, 51004], "temperature": 0.0, "avg_logprob": -0.07105657187375156, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.009241115301847458}, {"id": 457, "seek": 234148, "start": 2354.28, "end": 2359.88, "text": " depth is an important part of our generative models as well, of our brains models of the world.", "tokens": [51004, 7161, 307, 364, 1021, 644, 295, 527, 1337, 1166, 5245, 382, 731, 11, 295, 527, 15442, 5245, 295, 264, 1002, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07105657187375156, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.009241115301847458}, {"id": 458, "seek": 234148, "start": 2360.84, "end": 2367.72, "text": " And part of that comes from the fact that the world actually does separate out into a whole", "tokens": [51332, 400, 644, 295, 300, 1487, 490, 264, 1186, 300, 264, 1002, 767, 775, 4994, 484, 666, 257, 1379, 51676], "temperature": 0.0, "avg_logprob": -0.07105657187375156, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.009241115301847458}, {"id": 459, "seek": 236772, "start": 2367.72, "end": 2372.7599999999998, "text": " different series of temporal scales of things that happen slowly, that contextualize things that", "tokens": [50364, 819, 2638, 295, 30881, 17408, 295, 721, 300, 1051, 5692, 11, 300, 35526, 1125, 721, 300, 50616], "temperature": 0.0, "avg_logprob": -0.07467604129113883, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.058714356273412704}, {"id": 460, "seek": 236772, "start": 2372.7599999999998, "end": 2377.9599999999996, "text": " happen more quickly, that contextualize things that are even faster than that. And so one good", "tokens": [50616, 1051, 544, 2661, 11, 300, 35526, 1125, 721, 300, 366, 754, 4663, 813, 300, 13, 400, 370, 472, 665, 50876], "temperature": 0.0, "avg_logprob": -0.07467604129113883, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.058714356273412704}, {"id": 461, "seek": 236772, "start": 2377.9599999999996, "end": 2382.7599999999998, "text": " example of depth might be that if you're reading a book, then you have to bear in mind which page", "tokens": [50876, 1365, 295, 7161, 1062, 312, 300, 498, 291, 434, 3760, 257, 1446, 11, 550, 291, 362, 281, 6155, 294, 1575, 597, 3028, 51116], "temperature": 0.0, "avg_logprob": -0.07467604129113883, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.058714356273412704}, {"id": 462, "seek": 236772, "start": 2382.7599999999998, "end": 2387.16, "text": " you're on within that page, which sentence or which paragraph you want, within paragraph,", "tokens": [51116, 291, 434, 322, 1951, 300, 3028, 11, 597, 8174, 420, 597, 18865, 291, 528, 11, 1951, 18865, 11, 51336], "temperature": 0.0, "avg_logprob": -0.07467604129113883, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.058714356273412704}, {"id": 463, "seek": 236772, "start": 2387.16, "end": 2392.8399999999997, "text": " which sentence, within the sentence, which word, within the word, which letter. And by combining", "tokens": [51336, 597, 8174, 11, 1951, 264, 8174, 11, 597, 1349, 11, 1951, 264, 1349, 11, 597, 5063, 13, 400, 538, 21928, 51620], "temperature": 0.0, "avg_logprob": -0.07467604129113883, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.058714356273412704}, {"id": 464, "seek": 239284, "start": 2392.84, "end": 2397.7200000000003, "text": " your predictions sort of both down the system that way, but then updating your predictions", "tokens": [50364, 428, 21264, 1333, 295, 1293, 760, 264, 1185, 300, 636, 11, 457, 550, 25113, 428, 21264, 50608], "temperature": 0.0, "avg_logprob": -0.07110523736035382, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.013847067020833492}, {"id": 465, "seek": 239284, "start": 2397.7200000000003, "end": 2402.6800000000003, "text": " all the way back up again, you start to be able to make inferences about the overall narrative", "tokens": [50608, 439, 264, 636, 646, 493, 797, 11, 291, 722, 281, 312, 1075, 281, 652, 13596, 2667, 466, 264, 4787, 9977, 50856], "temperature": 0.0, "avg_logprob": -0.07110523736035382, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.013847067020833492}, {"id": 466, "seek": 239284, "start": 2402.6800000000003, "end": 2411.4, "text": " that you're reading. The other thing you mentioned that I thought was interesting was the idea of", "tokens": [50856, 300, 291, 434, 3760, 13, 440, 661, 551, 291, 2835, 300, 286, 1194, 390, 1880, 390, 264, 1558, 295, 51292], "temperature": 0.0, "avg_logprob": -0.07110523736035382, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.013847067020833492}, {"id": 467, "seek": 239284, "start": 2411.4, "end": 2417.4, "text": " confabulation and of how we come to beliefs about other people's behavior. And I think the same", "tokens": [51292, 1497, 455, 2776, 293, 295, 577, 321, 808, 281, 13585, 466, 661, 561, 311, 5223, 13, 400, 286, 519, 264, 912, 51592], "temperature": 0.0, "avg_logprob": -0.07110523736035382, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.013847067020833492}, {"id": 468, "seek": 239284, "start": 2417.4, "end": 2421.8, "text": " thing is also true about our own behavior and sort of making an inference about what we've done.", "tokens": [51592, 551, 307, 611, 2074, 466, 527, 1065, 5223, 293, 1333, 295, 1455, 364, 38253, 466, 437, 321, 600, 1096, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07110523736035382, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.013847067020833492}, {"id": 469, "seek": 242180, "start": 2422.44, "end": 2427.1600000000003, "text": " And this comes all the way back to the sense of agency again, doesn't it? It comes back to the", "tokens": [50396, 400, 341, 1487, 439, 264, 636, 646, 281, 264, 2020, 295, 7934, 797, 11, 1177, 380, 309, 30, 467, 1487, 646, 281, 264, 50632], "temperature": 0.0, "avg_logprob": -0.07747172837210174, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00235972972586751}, {"id": 470, "seek": 242180, "start": 2427.1600000000003, "end": 2431.96, "text": " idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,", "tokens": [50632, 1558, 300, 286, 669, 13596, 2937, 11, 286, 478, 35263, 294, 341, 636, 337, 341, 1778, 11, 570, 286, 600, 8614, 281, 360, 341, 11, 50872], "temperature": 0.0, "avg_logprob": -0.07747172837210174, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00235972972586751}, {"id": 471, "seek": 242180, "start": 2431.96, "end": 2439.4, "text": " because I had this goal in mind. And to come back to the other question, is that real? Or is it", "tokens": [50872, 570, 286, 632, 341, 3387, 294, 1575, 13, 400, 281, 808, 646, 281, 264, 661, 1168, 11, 307, 300, 957, 30, 1610, 307, 309, 51244], "temperature": 0.0, "avg_logprob": -0.07747172837210174, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00235972972586751}, {"id": 472, "seek": 242180, "start": 2439.4, "end": 2445.5600000000004, "text": " simply an inference about what I've done? I would suggest that it's certainly an inference about", "tokens": [51244, 2935, 364, 38253, 466, 437, 286, 600, 1096, 30, 286, 576, 3402, 300, 309, 311, 3297, 364, 38253, 466, 51552], "temperature": 0.0, "avg_logprob": -0.07747172837210174, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.00235972972586751}, {"id": 473, "seek": 244556, "start": 2445.56, "end": 2453.08, "text": " what I've done, whether or not it's real. Giovanni and I put together some simulations", "tokens": [50364, 437, 286, 600, 1096, 11, 1968, 420, 406, 309, 311, 957, 13, 47089, 35832, 293, 286, 829, 1214, 512, 35138, 50740], "temperature": 0.0, "avg_logprob": -0.09266939163208007, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03615444898605347}, {"id": 474, "seek": 244556, "start": 2453.08, "end": 2458.68, "text": " and some theoretical work a couple of years ago after a discussion at a conference about or a", "tokens": [50740, 293, 512, 20864, 589, 257, 1916, 295, 924, 2057, 934, 257, 5017, 412, 257, 7586, 466, 420, 257, 51020], "temperature": 0.0, "avg_logprob": -0.09266939163208007, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03615444898605347}, {"id": 475, "seek": 244556, "start": 2458.68, "end": 2464.2, "text": " workshop about machine understanding, suggesting that machine intelligence is one thing, but", "tokens": [51020, 13541, 466, 3479, 3701, 11, 18094, 300, 3479, 7599, 307, 472, 551, 11, 457, 51296], "temperature": 0.0, "avg_logprob": -0.09266939163208007, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03615444898605347}, {"id": 476, "seek": 244556, "start": 2464.2, "end": 2469.64, "text": " actually understanding why you've come to a particular conclusion. ChatGPT being able to", "tokens": [51296, 767, 3701, 983, 291, 600, 808, 281, 257, 1729, 10063, 13, 27503, 38, 47, 51, 885, 1075, 281, 51568], "temperature": 0.0, "avg_logprob": -0.09266939163208007, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.03615444898605347}, {"id": 477, "seek": 246964, "start": 2469.64, "end": 2475.7999999999997, "text": " explain to you why it came up with a specific sequence of words or why a convolutional neural", "tokens": [50364, 2903, 281, 291, 983, 309, 1361, 493, 365, 257, 2685, 8310, 295, 2283, 420, 983, 257, 45216, 304, 18161, 50672], "temperature": 0.0, "avg_logprob": -0.08590987149406881, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.006807879079133272}, {"id": 478, "seek": 246964, "start": 2475.7999999999997, "end": 2482.2799999999997, "text": " network classified an image in a particular way is one of the big issues really, and there are", "tokens": [50672, 3209, 20627, 364, 3256, 294, 257, 1729, 636, 307, 472, 295, 264, 955, 2663, 534, 11, 293, 456, 366, 50996], "temperature": 0.0, "avg_logprob": -0.08590987149406881, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.006807879079133272}, {"id": 479, "seek": 246964, "start": 2482.2799999999997, "end": 2486.8399999999997, "text": " solutions coming up, but it's one of the big issues in the deep learning community as to how", "tokens": [50996, 6547, 1348, 493, 11, 457, 309, 311, 472, 295, 264, 955, 2663, 294, 264, 2452, 2539, 1768, 382, 281, 577, 51224], "temperature": 0.0, "avg_logprob": -0.08590987149406881, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.006807879079133272}, {"id": 480, "seek": 246964, "start": 2486.8399999999997, "end": 2490.6, "text": " you have that transparency in terms of what the models are doing and why they're doing it.", "tokens": [51224, 291, 362, 300, 17131, 294, 2115, 295, 437, 264, 5245, 366, 884, 293, 983, 436, 434, 884, 309, 13, 51412], "temperature": 0.0, "avg_logprob": -0.08590987149406881, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.006807879079133272}, {"id": 481, "seek": 246964, "start": 2493.3199999999997, "end": 2496.8399999999997, "text": " Giovanni and I put together some work following that, looking at", "tokens": [51548, 47089, 35832, 293, 286, 829, 1214, 512, 589, 3480, 300, 11, 1237, 412, 51724], "temperature": 0.0, "avg_logprob": -0.08590987149406881, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.006807879079133272}, {"id": 482, "seek": 249684, "start": 2497.56, "end": 2501.56, "text": " understanding of our own actions from an active inference perspective, and there it was very much", "tokens": [50400, 3701, 295, 527, 1065, 5909, 490, 364, 4967, 38253, 4585, 11, 293, 456, 309, 390, 588, 709, 50600], "temperature": 0.0, "avg_logprob": -0.073198110407049, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.004785915371030569}, {"id": 483, "seek": 249684, "start": 2501.56, "end": 2506.76, "text": " framed as I have a series of hypotheses of things I might do, of reasons why I might do that.", "tokens": [50600, 30420, 382, 286, 362, 257, 2638, 295, 49969, 295, 721, 286, 1062, 360, 11, 295, 4112, 983, 286, 1062, 360, 300, 13, 50860], "temperature": 0.0, "avg_logprob": -0.073198110407049, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.004785915371030569}, {"id": 484, "seek": 249684, "start": 2507.56, "end": 2512.2000000000003, "text": " And then after observing myself behaving in a particular way, I can then use my own behavior", "tokens": [50900, 400, 550, 934, 22107, 2059, 35263, 294, 257, 1729, 636, 11, 286, 393, 550, 764, 452, 1065, 5223, 51132], "temperature": 0.0, "avg_logprob": -0.073198110407049, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.004785915371030569}, {"id": 485, "seek": 249684, "start": 2512.2000000000003, "end": 2518.2000000000003, "text": " as data that I then have to come up with an explanation for. And it's very interesting to", "tokens": [51132, 382, 1412, 300, 286, 550, 362, 281, 808, 493, 365, 364, 10835, 337, 13, 400, 309, 311, 588, 1880, 281, 51432], "temperature": 0.0, "avg_logprob": -0.073198110407049, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.004785915371030569}, {"id": 486, "seek": 249684, "start": 2518.2000000000003, "end": 2523.08, "text": " see what happens if you start depriving that of aspects of its behavior and to see the confabulations", "tokens": [51432, 536, 437, 2314, 498, 291, 722, 27095, 798, 300, 295, 7270, 295, 1080, 5223, 293, 281, 536, 264, 1497, 455, 4136, 51676], "temperature": 0.0, "avg_logprob": -0.073198110407049, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.004785915371030569}, {"id": 487, "seek": 252308, "start": 2523.08, "end": 2528.6, "text": " that result from that. I can't remember where it came from originally, the idea of hallucinations", "tokens": [50364, 300, 1874, 490, 300, 13, 286, 393, 380, 1604, 689, 309, 1361, 490, 7993, 11, 264, 1558, 295, 35212, 10325, 50640], "temperature": 0.0, "avg_logprob": -0.07894093990325927, "compression_ratio": 1.8271604938271604, "no_speech_prob": 0.028189783915877342}, {"id": 488, "seek": 252308, "start": 2528.6, "end": 2537.96, "text": " being a perception generally being effectively a constrained hallucination, where you take your", "tokens": [50640, 885, 257, 12860, 5101, 885, 8659, 257, 38901, 35212, 2486, 11, 689, 291, 747, 428, 51108], "temperature": 0.0, "avg_logprob": -0.07894093990325927, "compression_ratio": 1.8271604938271604, "no_speech_prob": 0.028189783915877342}, {"id": 489, "seek": 252308, "start": 2537.96, "end": 2541.7999999999997, "text": " hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.", "tokens": [51108, 35212, 2486, 11, 428, 16575, 295, 437, 311, 516, 322, 11, 293, 550, 291, 3191, 309, 281, 437, 311, 767, 1348, 294, 13, 51300], "temperature": 0.0, "avg_logprob": -0.07894093990325927, "compression_ratio": 1.8271604938271604, "no_speech_prob": 0.028189783915877342}, {"id": 490, "seek": 252308, "start": 2542.92, "end": 2546.44, "text": " But you could argue that actually a lot of our understanding about what we're doing is also", "tokens": [51356, 583, 291, 727, 9695, 300, 767, 257, 688, 295, 527, 3701, 466, 437, 321, 434, 884, 307, 611, 51532], "temperature": 0.0, "avg_logprob": -0.07894093990325927, "compression_ratio": 1.8271604938271604, "no_speech_prob": 0.028189783915877342}, {"id": 491, "seek": 252308, "start": 2546.44, "end": 2549.72, "text": " just a constrained confabulation in exactly the same way.", "tokens": [51532, 445, 257, 38901, 1497, 455, 2776, 294, 2293, 264, 912, 636, 13, 51696], "temperature": 0.0, "avg_logprob": -0.07894093990325927, "compression_ratio": 1.8271604938271604, "no_speech_prob": 0.028189783915877342}, {"id": 492, "seek": 254972, "start": 2550.68, "end": 2556.2, "text": " Yes, which is very ironic because people diminish GPT and because they say it's just", "tokens": [50412, 1079, 11, 597, 307, 588, 33719, 570, 561, 48696, 26039, 51, 293, 570, 436, 584, 309, 311, 445, 50688], "temperature": 0.0, "avg_logprob": -0.1191481113433838, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.005061781499534845}, {"id": 493, "seek": 254972, "start": 2556.2, "end": 2563.7999999999997, "text": " confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument", "tokens": [50688, 1497, 455, 12162, 11, 9735, 264, 659, 443, 11058, 28813, 5412, 1751, 295, 264, 786, 360, 1936, 652, 264, 912, 6770, 51068], "temperature": 0.0, "avg_logprob": -0.1191481113433838, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.005061781499534845}, {"id": 494, "seek": 254972, "start": 2563.7999999999997, "end": 2569.48, "text": " about how the brain works, and even our communication now on conditioning your simulator.", "tokens": [51068, 466, 577, 264, 3567, 1985, 11, 293, 754, 527, 6101, 586, 322, 21901, 428, 32974, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1191481113433838, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.005061781499534845}, {"id": 495, "seek": 254972, "start": 2569.48, "end": 2573.48, "text": " So the semantics are drawn by your own model in simulation of the world,", "tokens": [51352, 407, 264, 4361, 45298, 366, 10117, 538, 428, 1065, 2316, 294, 16575, 295, 264, 1002, 11, 51552], "temperature": 0.0, "avg_logprob": -0.1191481113433838, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.005061781499534845}, {"id": 496, "seek": 254972, "start": 2573.48, "end": 2578.04, "text": " rather than being the simulacrum of mine. You spoke about machine understanding,", "tokens": [51552, 2831, 813, 885, 264, 1034, 425, 326, 6247, 295, 3892, 13, 509, 7179, 466, 3479, 3701, 11, 51780], "temperature": 0.0, "avg_logprob": -0.1191481113433838, "compression_ratio": 1.6311787072243347, "no_speech_prob": 0.005061781499534845}, {"id": 497, "seek": 257804, "start": 2578.04, "end": 2584.04, "text": " I mean, there's this Chinese Rem argument. And we're in a really interesting time now because", "tokens": [50364, 286, 914, 11, 456, 311, 341, 4649, 4080, 6770, 13, 400, 321, 434, 294, 257, 534, 1880, 565, 586, 570, 50664], "temperature": 0.0, "avg_logprob": -0.14483609360255553, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.005304361693561077}, {"id": 498, "seek": 257804, "start": 2584.68, "end": 2591.88, "text": " we have artifacts that behave in a way which is isomorphic in many ways.", "tokens": [50696, 321, 362, 24617, 300, 15158, 294, 257, 636, 597, 307, 307, 32702, 299, 294, 867, 2098, 13, 51056], "temperature": 0.0, "avg_logprob": -0.14483609360255553, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.005304361693561077}, {"id": 499, "seek": 257804, "start": 2592.44, "end": 2600.68, "text": " And it's so tempting to say, well, we're different. And you could make the ontological argument,", "tokens": [51084, 400, 309, 311, 370, 37900, 281, 584, 11, 731, 11, 321, 434, 819, 13, 400, 291, 727, 652, 264, 6592, 4383, 6770, 11, 51496], "temperature": 0.0, "avg_logprob": -0.14483609360255553, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.005304361693561077}, {"id": 500, "seek": 257804, "start": 2600.68, "end": 2606.2799999999997, "text": " but this psychological argument is a big one as well, which is we're different because we have", "tokens": [51496, 457, 341, 14346, 6770, 307, 257, 955, 472, 382, 731, 11, 597, 307, 321, 434, 819, 570, 321, 362, 51776], "temperature": 0.0, "avg_logprob": -0.14483609360255553, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.005304361693561077}, {"id": 501, "seek": 260628, "start": 2607.0800000000004, "end": 2611.4, "text": " beliefs, motives, volition, desires, we have all of these things.", "tokens": [50404, 13585, 11, 39812, 11, 1996, 849, 11, 18005, 11, 321, 362, 439, 295, 613, 721, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12260967619875644, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.011039607226848602}, {"id": 502, "seek": 260628, "start": 2611.4, "end": 2615.1600000000003, "text": " But as we were just saying before, this is all post hoc confabulated.", "tokens": [50620, 583, 382, 321, 645, 445, 1566, 949, 11, 341, 307, 439, 2183, 16708, 1497, 455, 6987, 13, 50808], "temperature": 0.0, "avg_logprob": -0.12260967619875644, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.011039607226848602}, {"id": 503, "seek": 260628, "start": 2615.1600000000003, "end": 2619.5600000000004, "text": " We actually don't have consistent beliefs and desires. It's just a fiction.", "tokens": [50808, 492, 767, 500, 380, 362, 8398, 13585, 293, 18005, 13, 467, 311, 445, 257, 13266, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12260967619875644, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.011039607226848602}, {"id": 504, "seek": 260628, "start": 2621.2400000000002, "end": 2623.96, "text": " Was it a fiction or is it a plausible explanation?", "tokens": [51112, 3027, 309, 257, 13266, 420, 307, 309, 257, 39925, 10835, 30, 51248], "temperature": 0.0, "avg_logprob": -0.12260967619875644, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.011039607226848602}, {"id": 505, "seek": 260628, "start": 2625.48, "end": 2630.92, "text": " Well, I guess the thing that breaks it for me is the incoherence and inconsistency,", "tokens": [51324, 1042, 11, 286, 2041, 264, 551, 300, 9857, 309, 337, 385, 307, 264, 834, 78, 511, 655, 293, 22039, 468, 3020, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12260967619875644, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.011039607226848602}, {"id": 506, "seek": 263092, "start": 2631.0, "end": 2638.04, "text": " because you would think that we would be fully fledged human agents if we had consistent beliefs", "tokens": [50368, 570, 291, 576, 519, 300, 321, 576, 312, 4498, 24114, 3004, 1952, 12554, 498, 321, 632, 8398, 13585, 50720], "temperature": 0.0, "avg_logprob": -0.08028845951474946, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.015671679750084877}, {"id": 507, "seek": 263092, "start": 2638.04, "end": 2642.92, "text": " and desires. And it's not to say that we don't because it feels like some of our goals are", "tokens": [50720, 293, 18005, 13, 400, 309, 311, 406, 281, 584, 300, 321, 500, 380, 570, 309, 3417, 411, 512, 295, 527, 5493, 366, 50964], "temperature": 0.0, "avg_logprob": -0.08028845951474946, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.015671679750084877}, {"id": 508, "seek": 263092, "start": 2645.0, "end": 2651.56, "text": " they grounded in some way, like we need to eat food. But we think of ourselves as being", "tokens": [51068, 436, 23535, 294, 512, 636, 11, 411, 321, 643, 281, 1862, 1755, 13, 583, 321, 519, 295, 4175, 382, 885, 51396], "temperature": 0.0, "avg_logprob": -0.08028845951474946, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.015671679750084877}, {"id": 509, "seek": 263092, "start": 2651.56, "end": 2657.32, "text": " unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental", "tokens": [51396, 3845, 382, 6255, 11, 570, 321, 362, 2946, 1496, 5493, 293, 13585, 300, 3212, 380, 4725, 17388, 51684], "temperature": 0.0, "avg_logprob": -0.08028845951474946, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.015671679750084877}, {"id": 510, "seek": 265732, "start": 2657.7200000000003, "end": 2662.28, "text": " to eating food. And I guess those things in particular might be confabulatory.", "tokens": [50384, 281, 3936, 1755, 13, 400, 286, 2041, 729, 721, 294, 1729, 1062, 312, 1497, 455, 425, 4745, 13, 50612], "temperature": 0.0, "avg_logprob": -0.147742627614952, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.006111690774559975}, {"id": 511, "seek": 265732, "start": 2663.1600000000003, "end": 2667.7200000000003, "text": " Yes. So on the volition thing, that's something that really interests me.", "tokens": [50656, 1079, 13, 407, 322, 264, 1996, 849, 551, 11, 300, 311, 746, 300, 534, 8847, 385, 13, 50884], "temperature": 0.0, "avg_logprob": -0.147742627614952, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.006111690774559975}, {"id": 512, "seek": 265732, "start": 2670.44, "end": 2677.0800000000004, "text": " An active inference agent is we draw a boundary around a thing and it can act in the environment", "tokens": [51020, 1107, 4967, 38253, 9461, 307, 321, 2642, 257, 12866, 926, 257, 551, 293, 309, 393, 605, 294, 264, 2823, 51352], "temperature": 0.0, "avg_logprob": -0.147742627614952, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.006111690774559975}, {"id": 513, "seek": 265732, "start": 2677.0800000000004, "end": 2682.6000000000004, "text": " and it has preferences. And essentially, it has a generative model where it can produce these", "tokens": [51352, 293, 309, 575, 21910, 13, 400, 4476, 11, 309, 575, 257, 1337, 1166, 2316, 689, 309, 393, 5258, 613, 51628], "temperature": 0.0, "avg_logprob": -0.147742627614952, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.006111690774559975}, {"id": 514, "seek": 268260, "start": 2682.6, "end": 2688.36, "text": " plans, these policies, if you like, and at the end of every single plan is an end state.", "tokens": [50364, 5482, 11, 613, 7657, 11, 498, 291, 411, 11, 293, 412, 264, 917, 295, 633, 2167, 1393, 307, 364, 917, 1785, 13, 50652], "temperature": 0.0, "avg_logprob": -0.08877756284630817, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.008819755166769028}, {"id": 515, "seek": 268260, "start": 2689.48, "end": 2696.36, "text": " So it's got all of these different goals in mind, if you like. And in the real world,", "tokens": [50708, 407, 309, 311, 658, 439, 295, 613, 819, 5493, 294, 1575, 11, 498, 291, 411, 13, 400, 294, 264, 957, 1002, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08877756284630817, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.008819755166769028}, {"id": 516, "seek": 268260, "start": 2697.24, "end": 2704.2799999999997, "text": " real in big air quotes, these things emerge. But when we design these agents, we need to", "tokens": [51096, 957, 294, 955, 1988, 19963, 11, 613, 721, 21511, 13, 583, 562, 321, 1715, 613, 12554, 11, 321, 643, 281, 51448], "temperature": 0.0, "avg_logprob": -0.08877756284630817, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.008819755166769028}, {"id": 517, "seek": 268260, "start": 2704.2799999999997, "end": 2711.08, "text": " somehow impute the preferences onto them. So it feels like they have less agency if we", "tokens": [51448, 6063, 704, 1169, 264, 21910, 3911, 552, 13, 407, 309, 3417, 411, 436, 362, 1570, 7934, 498, 321, 51788], "temperature": 0.0, "avg_logprob": -0.08877756284630817, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.008819755166769028}, {"id": 518, "seek": 271108, "start": 2711.88, "end": 2717.56, "text": " impute the preferences. Would you agree with that? Interesting question.", "tokens": [50404, 704, 1169, 264, 21910, 13, 6068, 291, 3986, 365, 300, 30, 14711, 1168, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13728352115578848, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.0007912229630164802}, {"id": 519, "seek": 271108, "start": 2719.7999999999997, "end": 2729.7999999999997, "text": " And a very relevant question in the current number of industry related applications of", "tokens": [50800, 400, 257, 588, 7340, 1168, 294, 264, 2190, 1230, 295, 3518, 4077, 5821, 295, 51300], "temperature": 0.0, "avg_logprob": -0.13728352115578848, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.0007912229630164802}, {"id": 520, "seek": 271108, "start": 2729.7999999999997, "end": 2734.36, "text": " active inference. I think we were speaking about earlier, there are a number of companies now", "tokens": [51300, 4967, 38253, 13, 286, 519, 321, 645, 4124, 466, 3071, 11, 456, 366, 257, 1230, 295, 3431, 586, 51528], "temperature": 0.0, "avg_logprob": -0.13728352115578848, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.0007912229630164802}, {"id": 521, "seek": 271108, "start": 2734.36, "end": 2739.7999999999997, "text": " that have been set up looking at use of active inference based principles for various problems.", "tokens": [51528, 300, 362, 668, 992, 493, 1237, 412, 764, 295, 4967, 38253, 2361, 9156, 337, 3683, 2740, 13, 51800], "temperature": 0.0, "avg_logprob": -0.13728352115578848, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.0007912229630164802}, {"id": 522, "seek": 273980, "start": 2740.6800000000003, "end": 2745.4, "text": " Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.", "tokens": [50408, 44031, 411, 12226, 301, 300, 321, 7179, 466, 949, 293, 10061, 6483, 7318, 300, 286, 360, 512, 589, 365, 382, 731, 13, 50644], "temperature": 0.0, "avg_logprob": -0.12511664564891528, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010041907662525773}, {"id": 523, "seek": 273980, "start": 2747.1600000000003, "end": 2752.92, "text": " And the issue there is very much, it's a different kind of issue to the biological", "tokens": [50732, 400, 264, 2734, 456, 307, 588, 709, 11, 309, 311, 257, 819, 733, 295, 2734, 281, 264, 13910, 51020], "temperature": 0.0, "avg_logprob": -0.12511664564891528, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010041907662525773}, {"id": 524, "seek": 273980, "start": 2752.92, "end": 2758.6800000000003, "text": " issue of describing how things work. And it's the issue of saying, if I now want to design an", "tokens": [51020, 2734, 295, 16141, 577, 721, 589, 13, 400, 309, 311, 264, 2734, 295, 1566, 11, 498, 286, 586, 528, 281, 1715, 364, 51308], "temperature": 0.0, "avg_logprob": -0.12511664564891528, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010041907662525773}, {"id": 525, "seek": 273980, "start": 2758.6800000000003, "end": 2763.1600000000003, "text": " agent to behave in a particular way, as you say, am I taking some agency away from that?", "tokens": [51308, 9461, 281, 15158, 294, 257, 1729, 636, 11, 382, 291, 584, 11, 669, 286, 1940, 512, 7934, 1314, 490, 300, 30, 51532], "temperature": 0.0, "avg_logprob": -0.12511664564891528, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010041907662525773}, {"id": 526, "seek": 276316, "start": 2763.16, "end": 2769.3999999999996, "text": " There are a couple of things to think about there. I suppose one is thinking about", "tokens": [50364, 821, 366, 257, 1916, 295, 721, 281, 519, 466, 456, 13, 286, 7297, 472, 307, 1953, 466, 50676], "temperature": 0.0, "avg_logprob": -0.1433109913057494, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.032226014882326126}, {"id": 527, "seek": 276316, "start": 2770.8399999999997, "end": 2775.24, "text": " do biological agents actually select their own preferences to begin with?", "tokens": [50748, 360, 13910, 12554, 767, 3048, 641, 1065, 21910, 281, 1841, 365, 30, 50968], "temperature": 0.0, "avg_logprob": -0.1433109913057494, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.032226014882326126}, {"id": 528, "seek": 276316, "start": 2777.08, "end": 2781.72, "text": " And I think most people would probably say they don't most of the time. There may be certain", "tokens": [51060, 400, 286, 519, 881, 561, 576, 1391, 584, 436, 500, 380, 881, 295, 264, 565, 13, 821, 815, 312, 1629, 51292], "temperature": 0.0, "avg_logprob": -0.1433109913057494, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.032226014882326126}, {"id": 529, "seek": 276316, "start": 2781.72, "end": 2786.3599999999997, "text": " circumstances where they do or where a particular preference might be conditionally dependent upon", "tokens": [51292, 9121, 689, 436, 360, 420, 689, 257, 1729, 17502, 1062, 312, 4188, 379, 12334, 3564, 51524], "temperature": 0.0, "avg_logprob": -0.1433109913057494, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.032226014882326126}, {"id": 530, "seek": 276316, "start": 2787.24, "end": 2792.2799999999997, "text": " the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's", "tokens": [51568, 264, 5633, 286, 478, 294, 11, 264, 9005, 286, 478, 294, 11, 1968, 286, 478, 412, 589, 420, 412, 1280, 420, 2035, 1646, 13, 583, 309, 311, 51820], "temperature": 0.0, "avg_logprob": -0.1433109913057494, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.032226014882326126}, {"id": 531, "seek": 279228, "start": 2792.28, "end": 2797.1600000000003, "text": " not that I'm actually selecting this is what I want to want. There is a famous quote here,", "tokens": [50364, 406, 300, 286, 478, 767, 18182, 341, 307, 437, 286, 528, 281, 528, 13, 821, 307, 257, 4618, 6513, 510, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12649003319118335, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0008437489741481841}, {"id": 532, "seek": 279228, "start": 2797.1600000000003, "end": 2800.28, "text": " but I can't remember what it is. I don't know whether you do. No.", "tokens": [50608, 457, 286, 393, 380, 1604, 437, 309, 307, 13, 286, 500, 380, 458, 1968, 291, 360, 13, 883, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12649003319118335, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0008437489741481841}, {"id": 533, "seek": 279228, "start": 2803.96, "end": 2811.0, "text": " No, it's escaped me about wanting what you want or wanting what you do or something along those", "tokens": [50948, 883, 11, 309, 311, 20397, 385, 466, 7935, 437, 291, 528, 420, 7935, 437, 291, 360, 420, 746, 2051, 729, 51300], "temperature": 0.0, "avg_logprob": -0.12649003319118335, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0008437489741481841}, {"id": 534, "seek": 279228, "start": 2811.0, "end": 2817.1600000000003, "text": " lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us", "tokens": [51300, 3876, 13, 5684, 11, 264, 935, 286, 478, 1455, 307, 300, 11, 281, 512, 8396, 11, 527, 21910, 366, 2212, 281, 505, 51608], "temperature": 0.0, "avg_logprob": -0.12649003319118335, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0008437489741481841}, {"id": 535, "seek": 281716, "start": 2817.16, "end": 2822.3599999999997, "text": " effectively through a process of evolution, natural selection, previous experience that has", "tokens": [50364, 8659, 807, 257, 1399, 295, 9303, 11, 3303, 9450, 11, 3894, 1752, 300, 575, 50624], "temperature": 0.0, "avg_logprob": -0.07643381754557292, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.006868736352771521}, {"id": 536, "seek": 281716, "start": 2822.3599999999997, "end": 2829.0, "text": " affected what is a good set of states to occupy. And those will often be a good set of states that", "tokens": [50624, 8028, 437, 307, 257, 665, 992, 295, 4368, 281, 30645, 13, 400, 729, 486, 2049, 312, 257, 665, 992, 295, 4368, 300, 50956], "temperature": 0.0, "avg_logprob": -0.07643381754557292, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.006868736352771521}, {"id": 537, "seek": 281716, "start": 2829.0, "end": 2838.92, "text": " help my survival, that help the persistence of the species that I'm a part of. And arguably,", "tokens": [50956, 854, 452, 12559, 11, 300, 854, 264, 37617, 295, 264, 6172, 300, 286, 478, 257, 644, 295, 13, 400, 26771, 11, 51452], "temperature": 0.0, "avg_logprob": -0.07643381754557292, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.006868736352771521}, {"id": 538, "seek": 281716, "start": 2838.92, "end": 2845.24, "text": " the same thing is true when you as a designer of a particular algorithm or an agent are giving it", "tokens": [51452, 264, 912, 551, 307, 2074, 562, 291, 382, 257, 11795, 295, 257, 1729, 9284, 420, 364, 9461, 366, 2902, 309, 51768], "temperature": 0.0, "avg_logprob": -0.07643381754557292, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.006868736352771521}, {"id": 539, "seek": 284524, "start": 2845.3199999999997, "end": 2850.12, "text": " a set of preferences. From its perspective, it's never selected them anyway. And that's the same", "tokens": [50368, 257, 992, 295, 21910, 13, 3358, 1080, 4585, 11, 309, 311, 1128, 8209, 552, 4033, 13, 400, 300, 311, 264, 912, 50608], "temperature": 0.0, "avg_logprob": -0.10653566657950025, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0015025116736069322}, {"id": 540, "seek": 284524, "start": 2850.12, "end": 2856.2, "text": " as you or I not necessarily having selected our preferences. There's one additional element that", "tokens": [50608, 382, 291, 420, 286, 406, 4725, 1419, 8209, 527, 21910, 13, 821, 311, 472, 4497, 4478, 300, 50912], "temperature": 0.0, "avg_logprob": -0.10653566657950025, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0015025116736069322}, {"id": 541, "seek": 284524, "start": 2856.2, "end": 2863.9599999999996, "text": " I think is interesting to think about. And one of my colleagues and collaborators,", "tokens": [50912, 286, 519, 307, 1880, 281, 519, 466, 13, 400, 472, 295, 452, 7734, 293, 39789, 11, 51300], "temperature": 0.0, "avg_logprob": -0.10653566657950025, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0015025116736069322}, {"id": 542, "seek": 284524, "start": 2863.9599999999996, "end": 2868.6, "text": " Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own", "tokens": [51300, 6966, 318, 1805, 327, 11, 575, 1096, 257, 688, 295, 1880, 589, 322, 341, 11, 597, 307, 264, 1558, 295, 2539, 428, 1065, 51532], "temperature": 0.0, "avg_logprob": -0.10653566657950025, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0015025116736069322}, {"id": 543, "seek": 284524, "start": 2868.6, "end": 2874.2799999999997, "text": " preferences, of actually saying, let's create an agent that isn't given preferences to begin with,", "tokens": [51532, 21910, 11, 295, 767, 1566, 11, 718, 311, 1884, 364, 9461, 300, 1943, 380, 2212, 21910, 281, 1841, 365, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10653566657950025, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0015025116736069322}, {"id": 544, "seek": 287428, "start": 2874.28, "end": 2880.6000000000004, "text": " but is allowed to learn as it behaves what sort of goal states it ends up in.", "tokens": [50364, 457, 307, 4350, 281, 1466, 382, 309, 36896, 437, 1333, 295, 3387, 4368, 309, 5314, 493, 294, 13, 50680], "temperature": 0.0, "avg_logprob": -0.06273794174194336, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.001427146722562611}, {"id": 545, "seek": 287428, "start": 2881.88, "end": 2888.28, "text": " And there you get some very interesting results. So she showed that these sorts of agents", "tokens": [50744, 400, 456, 291, 483, 512, 588, 1880, 3542, 13, 407, 750, 4712, 300, 613, 7527, 295, 12554, 51064], "temperature": 0.0, "avg_logprob": -0.06273794174194336, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.001427146722562611}, {"id": 546, "seek": 287428, "start": 2889.6400000000003, "end": 2893.7200000000003, "text": " may end up doing things that you just don't want them to do, that they end up forming a", "tokens": [51132, 815, 917, 493, 884, 721, 300, 291, 445, 500, 380, 528, 552, 281, 360, 11, 300, 436, 917, 493, 15745, 257, 51336], "temperature": 0.0, "avg_logprob": -0.06273794174194336, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.001427146722562611}, {"id": 547, "seek": 287428, "start": 2893.7200000000003, "end": 2898.0400000000004, "text": " particular pattern of being or a particular way of being that you as a designer might never have", "tokens": [51336, 1729, 5102, 295, 885, 420, 257, 1729, 636, 295, 885, 300, 291, 382, 257, 11795, 1062, 1128, 362, 51552], "temperature": 0.0, "avg_logprob": -0.06273794174194336, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.001427146722562611}, {"id": 548, "seek": 287428, "start": 2898.0400000000004, "end": 2903.1600000000003, "text": " envisaged. For example, in an environment with lots of potential holes that it can fall into,", "tokens": [51552, 2267, 271, 2980, 13, 1171, 1365, 11, 294, 364, 2823, 365, 3195, 295, 3995, 8118, 300, 309, 393, 2100, 666, 11, 51808], "temperature": 0.0, "avg_logprob": -0.06273794174194336, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.001427146722562611}, {"id": 549, "seek": 290316, "start": 2903.16, "end": 2908.2799999999997, "text": " some of these agents just become hole dwellers. They just decide, I found that the first few", "tokens": [50364, 512, 295, 613, 12554, 445, 1813, 5458, 24355, 433, 13, 814, 445, 4536, 11, 286, 1352, 300, 264, 700, 1326, 50620], "temperature": 0.0, "avg_logprob": -0.09248753844714555, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0006375053781084716}, {"id": 550, "seek": 290316, "start": 2908.2799999999997, "end": 2912.2799999999997, "text": " times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature", "tokens": [50620, 1413, 286, 630, 341, 5633, 11, 286, 5696, 666, 264, 5458, 13, 407, 286, 600, 3047, 286, 478, 1391, 264, 1333, 295, 12797, 50820], "temperature": 0.0, "avg_logprob": -0.09248753844714555, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0006375053781084716}, {"id": 551, "seek": 290316, "start": 2912.2799999999997, "end": 2918.52, "text": " that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe", "tokens": [50820, 300, 5902, 2647, 294, 257, 5458, 13, 407, 300, 311, 257, 2590, 689, 291, 393, 976, 309, 257, 1629, 7934, 13, 400, 1310, 51132], "temperature": 0.0, "avg_logprob": -0.09248753844714555, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0006375053781084716}, {"id": 552, "seek": 290316, "start": 2918.52, "end": 2924.52, "text": " that agency is the ability to sort of disagree with what you as a designer might expect or want", "tokens": [51132, 300, 7934, 307, 264, 3485, 281, 1333, 295, 14091, 365, 437, 291, 382, 257, 11795, 1062, 2066, 420, 528, 51432], "temperature": 0.0, "avg_logprob": -0.09248753844714555, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0006375053781084716}, {"id": 553, "seek": 290316, "start": 2924.52, "end": 2930.12, "text": " from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a", "tokens": [51432, 490, 309, 13, 1079, 13, 639, 307, 370, 1880, 13, 286, 914, 11, 321, 434, 1242, 257, 707, 857, 666, 11, 321, 603, 362, 257, 51712], "temperature": 0.0, "avg_logprob": -0.09248753844714555, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0006375053781084716}, {"id": 554, "seek": 293012, "start": 2930.12, "end": 2939.0, "text": " discussion about cybernetics and externalism. But so what you're describing there is the reason", "tokens": [50364, 5017, 466, 13411, 7129, 1167, 293, 8320, 1434, 13, 583, 370, 437, 291, 434, 16141, 456, 307, 264, 1778, 50808], "temperature": 0.0, "avg_logprob": -0.0764258905898693, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.005690852180123329}, {"id": 555, "seek": 293012, "start": 2939.0, "end": 2946.68, "text": " why AI systems today are not sophisticated is because they are convergent. And that's usually", "tokens": [50808, 983, 7318, 3652, 965, 366, 406, 16950, 307, 570, 436, 366, 9652, 6930, 13, 400, 300, 311, 2673, 51192], "temperature": 0.0, "avg_logprob": -0.0764258905898693, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.005690852180123329}, {"id": 556, "seek": 293012, "start": 2946.68, "end": 2953.24, "text": " because they don't actually have any agency. So one of the hallmarks of the physical real", "tokens": [51192, 570, 436, 500, 380, 767, 362, 604, 7934, 13, 407, 472, 295, 264, 6500, 37307, 295, 264, 4001, 957, 51520], "temperature": 0.0, "avg_logprob": -0.0764258905898693, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.005690852180123329}, {"id": 557, "seek": 293012, "start": 2953.24, "end": 2959.4, "text": " systems in the real world is that they have these divergent properties. And that's because you have", "tokens": [51520, 3652, 294, 264, 957, 1002, 307, 300, 436, 362, 613, 18558, 6930, 7221, 13, 400, 300, 311, 570, 291, 362, 51828], "temperature": 0.0, "avg_logprob": -0.0764258905898693, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.005690852180123329}, {"id": 558, "seek": 295940, "start": 2959.48, "end": 2964.6, "text": " lots of independent agents following their own directiveness doing epistemic foraging. So", "tokens": [50368, 3195, 295, 6695, 12554, 3480, 641, 1065, 2047, 8477, 884, 2388, 468, 3438, 337, 3568, 13, 407, 50624], "temperature": 0.0, "avg_logprob": -0.06693940549283414, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.009215585887432098}, {"id": 559, "seek": 295940, "start": 2964.6, "end": 2968.76, "text": " interesting stepping stones get discovered. And sometimes those stepping stones aren't what the", "tokens": [50624, 1880, 16821, 14083, 483, 6941, 13, 400, 2171, 729, 16821, 14083, 3212, 380, 437, 264, 50832], "temperature": 0.0, "avg_logprob": -0.06693940549283414, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.009215585887432098}, {"id": 560, "seek": 295940, "start": 2968.76, "end": 2974.12, "text": " designer of the system would have liked, as you just said. So there's an interesting kind of paradox", "tokens": [50832, 11795, 295, 264, 1185, 576, 362, 4501, 11, 382, 291, 445, 848, 13, 407, 456, 311, 364, 1880, 733, 295, 26221, 51100], "temperature": 0.0, "avg_logprob": -0.06693940549283414, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.009215585887432098}, {"id": 561, "seek": 295940, "start": 2974.12, "end": 2981.1600000000003, "text": " there of how much agency do you want to imbue in the agents. But the other paradox is the physical", "tokens": [51100, 456, 295, 577, 709, 7934, 360, 291, 528, 281, 566, 65, 622, 294, 264, 12554, 13, 583, 264, 661, 26221, 307, 264, 4001, 51452], "temperature": 0.0, "avg_logprob": -0.06693940549283414, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.009215585887432098}, {"id": 562, "seek": 295940, "start": 2981.1600000000003, "end": 2987.48, "text": " and social embeddedness. Because as you just said, cynically, we don't have as much agency as we", "tokens": [51452, 293, 2093, 16741, 1287, 13, 1436, 382, 291, 445, 848, 11, 28365, 984, 11, 321, 500, 380, 362, 382, 709, 7934, 382, 321, 51768], "temperature": 0.0, "avg_logprob": -0.06693940549283414, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.009215585887432098}, {"id": 563, "seek": 298748, "start": 2987.48, "end": 2992.76, "text": " think we do, because we're embedded in the dynamics around us. And being part of this", "tokens": [50364, 519, 321, 360, 11, 570, 321, 434, 16741, 294, 264, 15679, 926, 505, 13, 400, 885, 644, 295, 341, 50628], "temperature": 0.0, "avg_logprob": -0.057208929306421526, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.00996966939419508}, {"id": 564, "seek": 298748, "start": 2992.76, "end": 2999.96, "text": " overall system means that our agency is defined not just by our boundary, but it's by the history", "tokens": [50628, 4787, 1185, 1355, 300, 527, 7934, 307, 7642, 406, 445, 538, 527, 12866, 11, 457, 309, 311, 538, 264, 2503, 50988], "temperature": 0.0, "avg_logprob": -0.057208929306421526, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.00996966939419508}, {"id": 565, "seek": 298748, "start": 2999.96, "end": 3005.16, "text": " of the system. It's the history of us sharing information of all of the things around us.", "tokens": [50988, 295, 264, 1185, 13, 467, 311, 264, 2503, 295, 505, 5414, 1589, 295, 439, 295, 264, 721, 926, 505, 13, 51248], "temperature": 0.0, "avg_logprob": -0.057208929306421526, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.00996966939419508}, {"id": 566, "seek": 298748, "start": 3005.16, "end": 3010.68, "text": " And all of these things inform what we do and what our preferences are. And then you say, well,", "tokens": [51248, 400, 439, 295, 613, 721, 1356, 437, 321, 360, 293, 437, 527, 21910, 366, 13, 400, 550, 291, 584, 11, 731, 11, 51524], "temperature": 0.0, "avg_logprob": -0.057208929306421526, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.00996966939419508}, {"id": 567, "seek": 298748, "start": 3010.68, "end": 3015.96, "text": " we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out", "tokens": [51524, 321, 393, 445, 3270, 257, 3360, 777, 9461, 294, 264, 1185, 13, 400, 309, 1177, 380, 1596, 589, 570, 309, 311, 257, 3506, 484, 51788], "temperature": 0.0, "avg_logprob": -0.057208929306421526, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.00996966939419508}, {"id": 568, "seek": 301596, "start": 3016.04, "end": 3021.64, "text": " of water. It's not embedded in the ways that things that emerged in that system were in the", "tokens": [50368, 295, 1281, 13, 467, 311, 406, 16741, 294, 264, 2098, 300, 721, 300, 20178, 294, 300, 1185, 645, 294, 264, 50648], "temperature": 0.0, "avg_logprob": -0.054907952231922366, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.009679638780653477}, {"id": 569, "seek": 301596, "start": 3021.64, "end": 3029.08, "text": " first place. But this does get us onto this discussion of externalism. So part of the fiction", "tokens": [50648, 700, 1081, 13, 583, 341, 775, 483, 505, 3911, 341, 5017, 295, 8320, 1434, 13, 407, 644, 295, 264, 13266, 51020], "temperature": 0.0, "avg_logprob": -0.054907952231922366, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.009679638780653477}, {"id": 570, "seek": 301596, "start": 3029.08, "end": 3037.96, "text": " of how we think about cognition is that we think of ourselves as islands that don't share information", "tokens": [51020, 295, 577, 321, 519, 466, 46905, 307, 300, 321, 519, 295, 4175, 382, 17402, 300, 500, 380, 2073, 1589, 51464], "temperature": 0.0, "avg_logprob": -0.054907952231922366, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.009679638780653477}, {"id": 571, "seek": 301596, "start": 3037.96, "end": 3042.76, "text": " dynamically with the outside world. And of course, active inference is a way of bridging", "tokens": [51464, 43492, 365, 264, 2380, 1002, 13, 400, 295, 1164, 11, 4967, 38253, 307, 257, 636, 295, 16362, 3249, 51704], "temperature": 0.0, "avg_logprob": -0.054907952231922366, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.009679638780653477}, {"id": 572, "seek": 304276, "start": 3042.76, "end": 3045.4, "text": " these two schools of thought. So can you kind of bring that in?", "tokens": [50364, 613, 732, 4656, 295, 1194, 13, 407, 393, 291, 733, 295, 1565, 300, 294, 30, 50496], "temperature": 0.0, "avg_logprob": -0.09561548427659639, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.006357020698487759}, {"id": 573, "seek": 304276, "start": 3047.7200000000003, "end": 3052.0400000000004, "text": " I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on", "tokens": [50612, 286, 914, 11, 286, 519, 291, 600, 1217, 1096, 309, 294, 257, 2020, 13, 286, 478, 406, 988, 437, 1646, 456, 307, 337, 385, 281, 584, 322, 50828], "temperature": 0.0, "avg_logprob": -0.09561548427659639, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.006357020698487759}, {"id": 574, "seek": 304276, "start": 3052.0400000000004, "end": 3061.1600000000003, "text": " that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.", "tokens": [50828, 300, 13, 286, 603, 853, 452, 1151, 13, 407, 2086, 11, 286, 914, 11, 4967, 38253, 307, 466, 11, 731, 11, 309, 311, 466, 466, 1287, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09561548427659639, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.006357020698487759}, {"id": 575, "seek": 304276, "start": 3061.1600000000003, "end": 3068.36, "text": " It's the idea that our brains and our internal state evolves in such a way that reflects beliefs", "tokens": [51284, 467, 311, 264, 1558, 300, 527, 15442, 293, 527, 6920, 1785, 43737, 294, 1270, 257, 636, 300, 18926, 13585, 51644], "temperature": 0.0, "avg_logprob": -0.09561548427659639, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.006357020698487759}, {"id": 576, "seek": 306836, "start": 3068.36, "end": 3073.88, "text": " about what's outside. And I think that's one of the key things that you have to have for any sort", "tokens": [50364, 466, 437, 311, 2380, 13, 400, 286, 519, 300, 311, 472, 295, 264, 2141, 721, 300, 291, 362, 281, 362, 337, 604, 1333, 50640], "temperature": 0.0, "avg_logprob": -0.12106644976270067, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.010055331513285637}, {"id": 577, "seek": 306836, "start": 3073.88, "end": 3079.32, "text": " of intelligent system. And that doesn't necessarily exist with other approaches that exist in", "tokens": [50640, 295, 13232, 1185, 13, 400, 300, 1177, 380, 4725, 2514, 365, 661, 11587, 300, 2514, 294, 50912], "temperature": 0.0, "avg_logprob": -0.12106644976270067, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.010055331513285637}, {"id": 578, "seek": 306836, "start": 3079.32, "end": 3085.96, "text": " neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much", "tokens": [50912, 42762, 420, 11677, 7599, 13, 467, 307, 300, 11, 293, 286, 603, 445, 7149, 300, 11, 309, 311, 588, 709, 51244], "temperature": 0.0, "avg_logprob": -0.12106644976270067, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.010055331513285637}, {"id": 579, "seek": 306836, "start": 3085.96, "end": 3093.32, "text": " being, the aboutness is the key thing that what's happening in my head is a reflection or is a", "tokens": [51244, 885, 11, 264, 466, 1287, 307, 264, 2141, 551, 300, 437, 311, 2737, 294, 452, 1378, 307, 257, 12914, 420, 307, 257, 51612], "temperature": 0.0, "avg_logprob": -0.12106644976270067, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.010055331513285637}, {"id": 580, "seek": 309332, "start": 3093.32, "end": 3100.28, "text": " description in some way is about what's happening outside my head. And maybe that's the link with", "tokens": [50364, 3855, 294, 512, 636, 307, 466, 437, 311, 2737, 2380, 452, 1378, 13, 400, 1310, 300, 311, 264, 2113, 365, 50712], "temperature": 0.0, "avg_logprob": -0.08257440944294353, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.007600360084325075}, {"id": 581, "seek": 309332, "start": 3100.28, "end": 3105.0800000000004, "text": " this sort of externalism. But it's not just unidirectional either. It's the fact that", "tokens": [50712, 341, 1333, 295, 8320, 1434, 13, 583, 309, 311, 406, 445, 517, 327, 621, 41048, 2139, 13, 467, 311, 264, 1186, 300, 50952], "temperature": 0.0, "avg_logprob": -0.08257440944294353, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.007600360084325075}, {"id": 582, "seek": 309332, "start": 3106.52, "end": 3110.1200000000003, "text": " I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing", "tokens": [51024, 286, 478, 15745, 13585, 466, 437, 311, 2737, 294, 264, 2380, 1002, 11, 457, 286, 478, 611, 264, 472, 40396, 51204], "temperature": 0.0, "avg_logprob": -0.08257440944294353, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.007600360084325075}, {"id": 583, "seek": 309332, "start": 3110.1200000000003, "end": 3115.1600000000003, "text": " the outside world to change it to fit with the beliefs I have about how it should be.", "tokens": [51204, 264, 2380, 1002, 281, 1319, 309, 281, 3318, 365, 264, 13585, 286, 362, 466, 577, 309, 820, 312, 13, 51456], "temperature": 0.0, "avg_logprob": -0.08257440944294353, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.007600360084325075}, {"id": 584, "seek": 311516, "start": 3115.16, "end": 3127.64, "text": " Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around", "tokens": [50364, 1079, 13, 1079, 13, 407, 456, 311, 257, 733, 295, 2316, 13, 407, 321, 2642, 613, 13180, 13, 400, 321, 2316, 264, 1002, 926, 50988], "temperature": 0.0, "avg_logprob": -0.17393913821897644, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.02218901552259922}, {"id": 585, "seek": 311516, "start": 3127.64, "end": 3134.52, "text": " us. And we influence the world around us. And that's essentially what active inference is.", "tokens": [50988, 505, 13, 400, 321, 6503, 264, 1002, 926, 505, 13, 400, 300, 311, 4476, 437, 4967, 38253, 307, 13, 51332], "temperature": 0.0, "avg_logprob": -0.17393913821897644, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.02218901552259922}, {"id": 586, "seek": 311516, "start": 3134.52, "end": 3141.48, "text": " I guess it might be useful just to sketch out the cognitive science idea of an activism or", "tokens": [51332, 286, 2041, 309, 1062, 312, 4420, 445, 281, 12325, 484, 264, 15605, 3497, 1558, 295, 364, 29040, 420, 51680], "temperature": 0.0, "avg_logprob": -0.17393913821897644, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.02218901552259922}, {"id": 587, "seek": 314148, "start": 3141.48, "end": 3147.64, "text": " cybernetic. So there were folks who really railed against this idea of representationalism,", "tokens": [50364, 13411, 77, 3532, 13, 407, 456, 645, 4024, 567, 534, 8765, 292, 1970, 341, 1558, 295, 2906, 1478, 1434, 11, 50672], "temperature": 0.0, "avg_logprob": -0.07501175138685438, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0031198870856314898}, {"id": 588, "seek": 314148, "start": 3147.64, "end": 3154.52, "text": " which is this idea of model building in principle. And active inference is an integrated approach", "tokens": [50672, 597, 307, 341, 1558, 295, 2316, 2390, 294, 8665, 13, 400, 4967, 38253, 307, 364, 10919, 3109, 51016], "temperature": 0.0, "avg_logprob": -0.07501175138685438, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0031198870856314898}, {"id": 589, "seek": 314148, "start": 3154.52, "end": 3160.12, "text": " where we allow some model building, but we also think of the world itself as being its own best", "tokens": [51016, 689, 321, 2089, 512, 2316, 2390, 11, 457, 321, 611, 519, 295, 264, 1002, 2564, 382, 885, 1080, 1065, 1151, 51296], "temperature": 0.0, "avg_logprob": -0.07501175138685438, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0031198870856314898}, {"id": 590, "seek": 314148, "start": 3160.12, "end": 3167.4, "text": " representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in", "tokens": [51296, 33358, 13, 1012, 360, 321, 733, 295, 7283, 729, 732, 3487, 30, 1079, 13, 400, 286, 19367, 11, 286, 478, 1009, 2731, 294, 51660], "temperature": 0.0, "avg_logprob": -0.07501175138685438, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0031198870856314898}, {"id": 591, "seek": 316740, "start": 3167.4, "end": 3171.8, "text": " the distinction between the sort of inactivists, radical inactivists, the sort of different levels", "tokens": [50364, 264, 16844, 1296, 264, 1333, 295, 294, 23397, 1751, 11, 12001, 294, 23397, 1751, 11, 264, 1333, 295, 819, 4358, 50584], "temperature": 0.0, "avg_logprob": -0.07199943607503717, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.0037701197434216738}, {"id": 592, "seek": 316740, "start": 3171.8, "end": 3181.4, "text": " of stance you can take on this. And I think it comes down to that, that from an active inference", "tokens": [50584, 295, 21033, 291, 393, 747, 322, 341, 13, 400, 286, 519, 309, 1487, 760, 281, 300, 11, 300, 490, 364, 4967, 38253, 51064], "temperature": 0.0, "avg_logprob": -0.07199943607503717, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.0037701197434216738}, {"id": 593, "seek": 316740, "start": 3181.4, "end": 3187.2400000000002, "text": " perspective, both your representations, if that's the right word, the beliefs you have about the world,", "tokens": [51064, 4585, 11, 1293, 428, 33358, 11, 498, 300, 311, 264, 558, 1349, 11, 264, 13585, 291, 362, 466, 264, 1002, 11, 51356], "temperature": 0.0, "avg_logprob": -0.07199943607503717, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.0037701197434216738}, {"id": 594, "seek": 316740, "start": 3187.2400000000002, "end": 3191.64, "text": " whether or not that meets the criteria for representation from an inactivist perspective", "tokens": [51356, 1968, 420, 406, 300, 13961, 264, 11101, 337, 10290, 490, 364, 294, 23397, 468, 4585, 51576], "temperature": 0.0, "avg_logprob": -0.07199943607503717, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.0037701197434216738}, {"id": 595, "seek": 319164, "start": 3192.6, "end": 3199.08, "text": " is very important. But it is only important in terms of how you act. If your beliefs did not", "tokens": [50412, 307, 588, 1021, 13, 583, 309, 307, 787, 1021, 294, 2115, 295, 577, 291, 605, 13, 759, 428, 13585, 630, 406, 50736], "temperature": 0.0, "avg_logprob": -0.09003705539922605, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.005876544397324324}, {"id": 596, "seek": 319164, "start": 3199.08, "end": 3203.96, "text": " affect how you acted, clearly natural selection would not have selected you to form those beliefs.", "tokens": [50736, 3345, 577, 291, 20359, 11, 4448, 3303, 9450, 576, 406, 362, 8209, 291, 281, 1254, 729, 13585, 13, 50980], "temperature": 0.0, "avg_logprob": -0.09003705539922605, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.005876544397324324}, {"id": 597, "seek": 319164, "start": 3203.96, "end": 3209.48, "text": " I think it's the simple way of putting it. So let's talk about some of the kind of", "tokens": [50980, 286, 519, 309, 311, 264, 2199, 636, 295, 3372, 309, 13, 407, 718, 311, 751, 466, 512, 295, 264, 733, 295, 51256], "temperature": 0.0, "avg_logprob": -0.09003705539922605, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.005876544397324324}, {"id": 598, "seek": 319164, "start": 3210.2799999999997, "end": 3216.68, "text": " the mathematical underpinnings here. So I think probably one of the main concepts we", "tokens": [51296, 264, 18894, 833, 17836, 24451, 510, 13, 407, 286, 519, 1391, 472, 295, 264, 2135, 10392, 321, 51616], "temperature": 0.0, "avg_logprob": -0.09003705539922605, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.005876544397324324}, {"id": 599, "seek": 321668, "start": 3216.68, "end": 3222.04, "text": " should start on is this idea of surprise. And maybe we can talk about it in general terms,", "tokens": [50364, 820, 722, 322, 307, 341, 1558, 295, 6365, 13, 400, 1310, 321, 393, 751, 466, 309, 294, 2674, 2115, 11, 50632], "temperature": 0.0, "avg_logprob": -0.07192804887122714, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001894734101369977}, {"id": 600, "seek": 321668, "start": 3222.04, "end": 3228.68, "text": " and then we can move on to Bayesian surprise. So why is surprise so important in the free energy", "tokens": [50632, 293, 550, 321, 393, 1286, 322, 281, 7840, 42434, 6365, 13, 407, 983, 307, 6365, 370, 1021, 294, 264, 1737, 2281, 50964], "temperature": 0.0, "avg_logprob": -0.07192804887122714, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001894734101369977}, {"id": 601, "seek": 321668, "start": 3228.68, "end": 3236.7599999999998, "text": " principle? Well, it's central to it. It is the key thing that matters. And we talk about the free", "tokens": [50964, 8665, 30, 1042, 11, 309, 311, 5777, 281, 309, 13, 467, 307, 264, 2141, 551, 300, 7001, 13, 400, 321, 751, 466, 264, 1737, 51368], "temperature": 0.0, "avg_logprob": -0.07192804887122714, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001894734101369977}, {"id": 602, "seek": 321668, "start": 3236.7599999999998, "end": 3243.8799999999997, "text": " energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,", "tokens": [51368, 2281, 8665, 13, 583, 294, 257, 2020, 11, 1737, 2281, 307, 534, 456, 382, 257, 29690, 337, 6365, 13, 407, 2086, 11, 51724], "temperature": 0.0, "avg_logprob": -0.07192804887122714, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001894734101369977}, {"id": 603, "seek": 324388, "start": 3243.88, "end": 3250.04, "text": " what do we mean by surprise? And it's another one of those things like the high road and", "tokens": [50364, 437, 360, 321, 914, 538, 6365, 30, 400, 309, 311, 1071, 472, 295, 729, 721, 411, 264, 1090, 3060, 293, 50672], "temperature": 0.0, "avg_logprob": -0.08644954931168329, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0032024767715483904}, {"id": 604, "seek": 324388, "start": 3250.04, "end": 3253.56, "text": " the low road that you can approach from several different angles or several different lines of", "tokens": [50672, 264, 2295, 3060, 300, 291, 393, 3109, 490, 2940, 819, 14708, 420, 2940, 819, 3876, 295, 50848], "temperature": 0.0, "avg_logprob": -0.08644954931168329, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0032024767715483904}, {"id": 605, "seek": 324388, "start": 3253.56, "end": 3265.0, "text": " attack. If you were modeling something, if you were a Bayesian, so if you took a particular", "tokens": [50848, 2690, 13, 759, 291, 645, 15983, 746, 11, 498, 291, 645, 257, 7840, 42434, 11, 370, 498, 291, 1890, 257, 1729, 51420], "temperature": 0.0, "avg_logprob": -0.08644954931168329, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0032024767715483904}, {"id": 606, "seek": 324388, "start": 3265.0, "end": 3269.7200000000003, "text": " stance on probability theory and wanted to know, given my model, given my hypothesis,", "tokens": [51420, 21033, 322, 8482, 5261, 293, 1415, 281, 458, 11, 2212, 452, 2316, 11, 2212, 452, 17291, 11, 51656], "temperature": 0.0, "avg_logprob": -0.08644954931168329, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0032024767715483904}, {"id": 607, "seek": 326972, "start": 3269.7999999999997, "end": 3274.3599999999997, "text": " what's the evidence for it? What you would normally do is calculate something known as", "tokens": [50368, 437, 311, 264, 4467, 337, 309, 30, 708, 291, 576, 5646, 360, 307, 8873, 746, 2570, 382, 50596], "temperature": 0.0, "avg_logprob": -0.06119370180017808, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.031055117025971413}, {"id": 608, "seek": 326972, "start": 3274.3599999999997, "end": 3281.9599999999996, "text": " a marginal likelihood, which is just a measure of the fit between your model and the data that", "tokens": [50596, 257, 16885, 22119, 11, 597, 307, 445, 257, 3481, 295, 264, 3318, 1296, 428, 2316, 293, 264, 1412, 300, 50976], "temperature": 0.0, "avg_logprob": -0.06119370180017808, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.031055117025971413}, {"id": 609, "seek": 326972, "start": 3281.9599999999996, "end": 3293.48, "text": " you have that you're trying to explain. That fit trades off various different things. So it can", "tokens": [50976, 291, 362, 300, 291, 434, 1382, 281, 2903, 13, 663, 3318, 21287, 766, 3683, 819, 721, 13, 407, 309, 393, 51552], "temperature": 0.0, "avg_logprob": -0.06119370180017808, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.031055117025971413}, {"id": 610, "seek": 326972, "start": 3293.48, "end": 3298.7599999999998, "text": " trade off how accurately your model is explaining the data against how far you've had to deviate", "tokens": [51552, 4923, 766, 577, 20095, 428, 2316, 307, 13468, 264, 1412, 1970, 577, 1400, 291, 600, 632, 281, 1905, 13024, 51816], "temperature": 0.0, "avg_logprob": -0.06119370180017808, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.031055117025971413}, {"id": 611, "seek": 329876, "start": 3298.76, "end": 3303.88, "text": " from your prior beliefs or from your initial assumptions in order to arrive at that explanation.", "tokens": [50364, 490, 428, 4059, 13585, 420, 490, 428, 5883, 17695, 294, 1668, 281, 8881, 412, 300, 10835, 13, 50620], "temperature": 0.0, "avg_logprob": -0.07323505282402039, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0007979145157150924}, {"id": 612, "seek": 329876, "start": 3306.0400000000004, "end": 3312.6800000000003, "text": " So that marginal likelihood, that evidence is effectively just the negative or the inverse", "tokens": [50728, 407, 300, 16885, 22119, 11, 300, 4467, 307, 8659, 445, 264, 3671, 420, 264, 17340, 51060], "temperature": 0.0, "avg_logprob": -0.07323505282402039, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0007979145157150924}, {"id": 613, "seek": 329876, "start": 3312.6800000000003, "end": 3319.48, "text": " of surprise. So that that's one perspective on it, the better the fit, the simpler and most", "tokens": [51060, 295, 6365, 13, 407, 300, 300, 311, 472, 4585, 322, 309, 11, 264, 1101, 264, 3318, 11, 264, 18587, 293, 881, 51400], "temperature": 0.0, "avg_logprob": -0.07323505282402039, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0007979145157150924}, {"id": 614, "seek": 329876, "start": 3319.48, "end": 3325.4, "text": " accurate my explanation for something, the less surprised I will be by it. Another perspective", "tokens": [51400, 8559, 452, 10835, 337, 746, 11, 264, 1570, 6100, 286, 486, 312, 538, 309, 13, 3996, 4585, 51696], "temperature": 0.0, "avg_logprob": -0.07323505282402039, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.0007979145157150924}, {"id": 615, "seek": 332540, "start": 3325.4, "end": 3331.56, "text": " on surprise is just this more colloquial sense. It's the idea that, given what I would predict,", "tokens": [50364, 322, 6365, 307, 445, 341, 544, 1263, 29826, 831, 2020, 13, 467, 311, 264, 1558, 300, 11, 2212, 437, 286, 576, 6069, 11, 50672], "temperature": 0.0, "avg_logprob": -0.07979857403299083, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.008288200944662094}, {"id": 616, "seek": 332540, "start": 3331.56, "end": 3338.12, "text": " how far out of that prediction is it? One could take a more biological perspective on it and say,", "tokens": [50672, 577, 1400, 484, 295, 300, 17630, 307, 309, 30, 1485, 727, 747, 257, 544, 13910, 4585, 322, 309, 293, 584, 11, 51000], "temperature": 0.0, "avg_logprob": -0.07979857403299083, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.008288200944662094}, {"id": 617, "seek": 332540, "start": 3338.12, "end": 3343.56, "text": " imagine we are, well, we are homeostatic systems that have some set points. We want to keep our", "tokens": [51000, 3811, 321, 366, 11, 731, 11, 321, 366, 1280, 555, 2399, 3652, 300, 362, 512, 992, 2793, 13, 492, 528, 281, 1066, 527, 51272], "temperature": 0.0, "avg_logprob": -0.07979857403299083, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.008288200944662094}, {"id": 618, "seek": 332540, "start": 3343.56, "end": 3347.1600000000003, "text": " temperature within a certain range, our blood pressure within a certain range, our heart rate", "tokens": [51272, 4292, 1951, 257, 1629, 3613, 11, 527, 3390, 3321, 1951, 257, 1629, 3613, 11, 527, 1917, 3314, 51452], "temperature": 0.0, "avg_logprob": -0.07979857403299083, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.008288200944662094}, {"id": 619, "seek": 332540, "start": 3347.1600000000003, "end": 3352.92, "text": " within a certain range. If we find ourselves deviating from that, that is effectively a surprise", "tokens": [51452, 1951, 257, 1629, 3613, 13, 759, 321, 915, 4175, 31219, 990, 490, 300, 11, 300, 307, 8659, 257, 6365, 51740], "temperature": 0.0, "avg_logprob": -0.07979857403299083, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.008288200944662094}, {"id": 620, "seek": 335292, "start": 3352.92, "end": 3357.64, "text": " because we're not where we expect to be. And so we enact various changes to bring", "tokens": [50364, 570, 321, 434, 406, 689, 321, 2066, 281, 312, 13, 400, 370, 321, 25909, 3683, 2962, 281, 1565, 50600], "temperature": 0.0, "avg_logprob": -0.08887852283946254, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.002762523479759693}, {"id": 621, "seek": 335292, "start": 3358.84, "end": 3364.84, "text": " those parameters back in range. So we might, if our blood pressure is too low, we might increase", "tokens": [50660, 729, 9834, 646, 294, 3613, 13, 407, 321, 1062, 11, 498, 527, 3390, 3321, 307, 886, 2295, 11, 321, 1062, 3488, 50960], "temperature": 0.0, "avg_logprob": -0.08887852283946254, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.002762523479759693}, {"id": 622, "seek": 335292, "start": 3364.84, "end": 3368.52, "text": " our heart rate to bring our blood pressure back up to the range we expect it to be in.", "tokens": [50960, 527, 1917, 3314, 281, 1565, 527, 3390, 3321, 646, 493, 281, 264, 3613, 321, 2066, 309, 281, 312, 294, 13, 51144], "temperature": 0.0, "avg_logprob": -0.08887852283946254, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.002762523479759693}, {"id": 623, "seek": 335292, "start": 3369.4, "end": 3376.2000000000003, "text": " And that is, in a sense, what active inference is all about. It's just this idea of keeping things", "tokens": [51188, 400, 300, 307, 11, 294, 257, 2020, 11, 437, 4967, 38253, 307, 439, 466, 13, 467, 311, 445, 341, 1558, 295, 5145, 721, 51528], "temperature": 0.0, "avg_logprob": -0.08887852283946254, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.002762523479759693}, {"id": 624, "seek": 335292, "start": 3376.2000000000003, "end": 3382.04, "text": " within that minimally surprising range. But of course, once you put dynamics on it, once you", "tokens": [51528, 1951, 300, 4464, 379, 8830, 3613, 13, 583, 295, 1164, 11, 1564, 291, 829, 15679, 322, 309, 11, 1564, 291, 51820], "temperature": 0.0, "avg_logprob": -0.08887852283946254, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.002762523479759693}, {"id": 625, "seek": 338204, "start": 3382.04, "end": 3387.88, "text": " start unfolding that in time, you end up having to not just deal with how surprising things are now,", "tokens": [50364, 722, 44586, 300, 294, 565, 11, 291, 917, 493, 1419, 281, 406, 445, 2028, 365, 577, 8830, 721, 366, 586, 11, 50656], "temperature": 0.0, "avg_logprob": -0.08788282029769, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0017286645015701652}, {"id": 626, "seek": 338204, "start": 3387.88, "end": 3393.56, "text": " but you've got to try and anticipate surprise and behave in such a way that you allostatically", "tokens": [50656, 457, 291, 600, 658, 281, 853, 293, 21685, 6365, 293, 15158, 294, 1270, 257, 636, 300, 291, 439, 555, 5030, 50940], "temperature": 0.0, "avg_logprob": -0.08788282029769, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0017286645015701652}, {"id": 627, "seek": 338204, "start": 3393.56, "end": 3399.96, "text": " control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,", "tokens": [50940, 1969, 428, 27233, 15743, 11, 1293, 428, 560, 12080, 5250, 488, 15743, 411, 1917, 3314, 293, 3390, 3321, 11, 51260], "temperature": 0.0, "avg_logprob": -0.08788282029769, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0017286645015701652}, {"id": 628, "seek": 339996, "start": 3399.96, "end": 3409.16, "text": " etc., but also your extraceptive sensations, your vision, your audition, and the like.", "tokens": [50364, 5183, 7933, 457, 611, 428, 2857, 1336, 488, 36642, 11, 428, 5201, 11, 428, 20015, 11, 293, 264, 411, 13, 50824], "temperature": 0.0, "avg_logprob": -0.12138041897096495, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.18786181509494781}, {"id": 629, "seek": 339996, "start": 3412.04, "end": 3416.12, "text": " And there's almost no end to the perspective you could take on surprise. Another perspective", "tokens": [50968, 400, 456, 311, 1920, 572, 917, 281, 264, 4585, 291, 727, 747, 322, 6365, 13, 3996, 4585, 51172], "temperature": 0.0, "avg_logprob": -0.12138041897096495, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.18786181509494781}, {"id": 630, "seek": 339996, "start": 3416.12, "end": 3424.28, "text": " on it is that it's a reflective of, in a physical system, the improbability of being in a particular", "tokens": [51172, 322, 309, 307, 300, 309, 311, 257, 28931, 295, 11, 294, 257, 4001, 1185, 11, 264, 2530, 65, 2310, 295, 885, 294, 257, 1729, 51580], "temperature": 0.0, "avg_logprob": -0.12138041897096495, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.18786181509494781}, {"id": 631, "seek": 342428, "start": 3424.28, "end": 3431.0, "text": " state. From a lot of physics perspectives, improbability is also associated with energy.", "tokens": [50364, 1785, 13, 3358, 257, 688, 295, 10649, 16766, 11, 2530, 65, 2310, 307, 611, 6615, 365, 2281, 13, 50700], "temperature": 0.0, "avg_logprob": -0.08614533035843461, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.03587046638131142}, {"id": 632, "seek": 342428, "start": 3431.0, "end": 3438.52, "text": " It takes energy to bring things into less probable states. And without inputting energy into a system,", "tokens": [50700, 467, 2516, 2281, 281, 1565, 721, 666, 1570, 21759, 4368, 13, 400, 1553, 4846, 783, 2281, 666, 257, 1185, 11, 51076], "temperature": 0.0, "avg_logprob": -0.08614533035843461, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.03587046638131142}, {"id": 633, "seek": 342428, "start": 3438.52, "end": 3443.6400000000003, "text": " it will generally end up in its most probable state in the absence of that.", "tokens": [51076, 309, 486, 5101, 917, 493, 294, 1080, 881, 21759, 1785, 294, 264, 17145, 295, 300, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08614533035843461, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.03587046638131142}, {"id": 634, "seek": 342428, "start": 3445.32, "end": 3449.32, "text": " You think of things like Boltzmann's equation and the relationship there between energy and", "tokens": [51416, 509, 519, 295, 721, 411, 37884, 89, 14912, 311, 5367, 293, 264, 2480, 456, 1296, 2281, 293, 51616], "temperature": 0.0, "avg_logprob": -0.08614533035843461, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.03587046638131142}, {"id": 635, "seek": 344932, "start": 3449.32, "end": 3458.36, "text": " probability. And that also has a link then to the idea of either a Hamiltonian or indeed a", "tokens": [50364, 8482, 13, 400, 300, 611, 575, 257, 2113, 550, 281, 264, 1558, 295, 2139, 257, 18484, 952, 420, 6451, 257, 50816], "temperature": 0.0, "avg_logprob": -0.0959857702255249, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.009113126434385777}, {"id": 636, "seek": 344932, "start": 3458.36, "end": 3463.4, "text": " steady state distribution, which is just what is the distribution things will end up in if left", "tokens": [50816, 13211, 1785, 7316, 11, 597, 307, 445, 437, 307, 264, 7316, 721, 486, 917, 493, 294, 498, 1411, 51068], "temperature": 0.0, "avg_logprob": -0.0959857702255249, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.009113126434385777}, {"id": 637, "seek": 344932, "start": 3463.4, "end": 3469.88, "text": " to their own devices for a certain amount of time until things have probabilistically converged.", "tokens": [51068, 281, 641, 1065, 5759, 337, 257, 1629, 2372, 295, 565, 1826, 721, 362, 31959, 20458, 9652, 3004, 13, 51392], "temperature": 0.0, "avg_logprob": -0.0959857702255249, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.009113126434385777}, {"id": 638, "seek": 344932, "start": 3469.88, "end": 3475.0, "text": " And that means that if I would construct a probability distribution over where things", "tokens": [51392, 400, 300, 1355, 300, 498, 286, 576, 7690, 257, 8482, 7316, 670, 689, 721, 51648], "temperature": 0.0, "avg_logprob": -0.0959857702255249, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.009113126434385777}, {"id": 639, "seek": 347500, "start": 3475.0, "end": 3479.16, "text": " will be at a long point of time in the future, there will come a point at which that probability", "tokens": [50364, 486, 312, 412, 257, 938, 935, 295, 565, 294, 264, 2027, 11, 456, 486, 808, 257, 935, 412, 597, 300, 8482, 50572], "temperature": 0.0, "avg_logprob": -0.0790200902704607, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.007133133709430695}, {"id": 640, "seek": 347500, "start": 3479.16, "end": 3485.88, "text": " won't change any further. And the tendency of physical systems to go to those more probable", "tokens": [50572, 1582, 380, 1319, 604, 3052, 13, 400, 264, 18187, 295, 4001, 3652, 281, 352, 281, 729, 544, 21759, 50908], "temperature": 0.0, "avg_logprob": -0.0790200902704607, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.007133133709430695}, {"id": 641, "seek": 347500, "start": 3485.88, "end": 3494.44, "text": " states is exactly the same as the tendency to avoid surprising states. And again, we could", "tokens": [50908, 4368, 307, 2293, 264, 912, 382, 264, 18187, 281, 5042, 8830, 4368, 13, 400, 797, 11, 321, 727, 51336], "temperature": 0.0, "avg_logprob": -0.0790200902704607, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.007133133709430695}, {"id": 642, "seek": 347500, "start": 3494.44, "end": 3498.92, "text": " sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully", "tokens": [51336, 1333, 295, 352, 322, 337, 257, 1339, 11, 457, 286, 1582, 380, 322, 1333, 295, 661, 2098, 295, 24106, 3319, 309, 13, 583, 4696, 51560], "temperature": 0.0, "avg_logprob": -0.0790200902704607, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.007133133709430695}, {"id": 643, "seek": 347500, "start": 3498.92, "end": 3504.44, "text": " that sort of explains why it's such an important thing that underpins so much of what we do.", "tokens": [51560, 300, 1333, 295, 13948, 983, 309, 311, 1270, 364, 1021, 551, 300, 833, 79, 1292, 370, 709, 295, 437, 321, 360, 13, 51836], "temperature": 0.0, "avg_logprob": -0.0790200902704607, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.007133133709430695}, {"id": 644, "seek": 350500, "start": 3505.24, "end": 3509.64, "text": " We're either trying to sort of evolve as a physical system towards more probable states.", "tokens": [50376, 492, 434, 2139, 1382, 281, 1333, 295, 16693, 382, 257, 4001, 1185, 3030, 544, 21759, 4368, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09512901306152344, "compression_ratio": 1.75, "no_speech_prob": 0.0006981529295444489}, {"id": 645, "seek": 350500, "start": 3511.0, "end": 3516.6, "text": " Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within", "tokens": [50664, 1610, 321, 366, 1280, 555, 2399, 420, 439, 555, 2399, 22110, 1382, 281, 6909, 527, 6920, 9834, 1951, 50944], "temperature": 0.0, "avg_logprob": -0.09512901306152344, "compression_ratio": 1.75, "no_speech_prob": 0.0006981529295444489}, {"id": 646, "seek": 350500, "start": 3516.6, "end": 3523.64, "text": " the right set points. Or we are more colloquially just trying to avoid things that are different to", "tokens": [50944, 264, 558, 992, 2793, 13, 1610, 321, 366, 544, 1263, 29826, 2270, 445, 1382, 281, 5042, 721, 300, 366, 819, 281, 51296], "temperature": 0.0, "avg_logprob": -0.09512901306152344, "compression_ratio": 1.75, "no_speech_prob": 0.0006981529295444489}, {"id": 647, "seek": 350500, "start": 3523.64, "end": 3530.52, "text": " what we predict. Or we are statisticians trying to fit our model to the world as best we can.", "tokens": [51296, 437, 321, 6069, 13, 1610, 321, 366, 29588, 2567, 1382, 281, 3318, 527, 2316, 281, 264, 1002, 382, 1151, 321, 393, 13, 51640], "temperature": 0.0, "avg_logprob": -0.09512901306152344, "compression_ratio": 1.75, "no_speech_prob": 0.0006981529295444489}, {"id": 648, "seek": 353052, "start": 3530.52, "end": 3533.16, "text": " And all of those things come under the same umbrella of surprise.", "tokens": [50364, 400, 439, 295, 729, 721, 808, 833, 264, 912, 21925, 295, 6365, 13, 50496], "temperature": 0.0, "avg_logprob": -0.08710574120590367, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.002055257325991988}, {"id": 649, "seek": 353052, "start": 3534.92, "end": 3539.16, "text": " Free energy comes in because surprise is not a trivial thing to compute.", "tokens": [50584, 11551, 2281, 1487, 294, 570, 6365, 307, 406, 257, 26703, 551, 281, 14722, 13, 50796], "temperature": 0.0, "avg_logprob": -0.08710574120590367, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.002055257325991988}, {"id": 650, "seek": 353052, "start": 3541.64, "end": 3547.08, "text": " Mathematically, it's often either intractable mathematically or computationally. And so it's", "tokens": [50920, 15776, 40197, 11, 309, 311, 2049, 2139, 560, 1897, 712, 44003, 420, 24903, 379, 13, 400, 370, 309, 311, 51192], "temperature": 0.0, "avg_logprob": -0.08710574120590367, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.002055257325991988}, {"id": 651, "seek": 353052, "start": 3547.08, "end": 3552.12, "text": " just not efficient to be able to calculate. But free energy is a way of then approximating", "tokens": [51192, 445, 406, 7148, 281, 312, 1075, 281, 8873, 13, 583, 1737, 2281, 307, 257, 636, 295, 550, 8542, 990, 51444], "temperature": 0.0, "avg_logprob": -0.08710574120590367, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.002055257325991988}, {"id": 652, "seek": 353052, "start": 3552.12, "end": 3557.24, "text": " that surprise. It's a way of coming up with something that is close enough to it. Or", "tokens": [51444, 300, 6365, 13, 467, 311, 257, 636, 295, 1348, 493, 365, 746, 300, 307, 1998, 1547, 281, 309, 13, 1610, 51700], "temperature": 0.0, "avg_logprob": -0.08710574120590367, "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.002055257325991988}, {"id": 653, "seek": 355724, "start": 3558.04, "end": 3562.8399999999997, "text": " even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,", "tokens": [50404, 754, 544, 13402, 382, 364, 6597, 5472, 322, 6365, 13, 407, 498, 291, 434, 412, 264, 12437, 935, 295, 428, 1737, 2281, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08654582765367295, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.004031132906675339}, {"id": 654, "seek": 355724, "start": 3564.04, "end": 3573.16, "text": " then that limits how high your surprise can be. The key additional thing in free energy is that", "tokens": [50704, 550, 300, 10406, 577, 1090, 428, 6365, 393, 312, 13, 440, 2141, 4497, 551, 294, 1737, 2281, 307, 300, 51160], "temperature": 0.0, "avg_logprob": -0.08654582765367295, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.004031132906675339}, {"id": 655, "seek": 355724, "start": 3573.16, "end": 3578.7599999999998, "text": " the distance between that bound, your free energy and your surprise depends on how good your beliefs", "tokens": [51160, 264, 4560, 1296, 300, 5472, 11, 428, 1737, 2281, 293, 428, 6365, 5946, 322, 577, 665, 428, 13585, 51440], "temperature": 0.0, "avg_logprob": -0.08654582765367295, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.004031132906675339}, {"id": 656, "seek": 355724, "start": 3578.7599999999998, "end": 3584.12, "text": " about the world are. And that's where perception comes in. That by getting the best beliefs you", "tokens": [51440, 466, 264, 1002, 366, 13, 400, 300, 311, 689, 12860, 1487, 294, 13, 663, 538, 1242, 264, 1151, 13585, 291, 51708], "temperature": 0.0, "avg_logprob": -0.08654582765367295, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.004031132906675339}, {"id": 657, "seek": 358412, "start": 3584.12, "end": 3590.04, "text": " possibly can, you minimize the distance between your free energy and which is up a bounding of", "tokens": [50364, 6264, 393, 11, 291, 17522, 264, 4560, 1296, 428, 1737, 2281, 293, 597, 307, 493, 257, 5472, 278, 295, 50660], "temperature": 0.0, "avg_logprob": -0.1134261483544702, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.004005471244454384}, {"id": 658, "seek": 358412, "start": 3590.04, "end": 3594.92, "text": " surprise and the surprise itself. So then any further reduction in free energy, you would expect", "tokens": [50660, 6365, 293, 264, 6365, 2564, 13, 407, 550, 604, 3052, 11004, 294, 1737, 2281, 11, 291, 576, 2066, 50904], "temperature": 0.0, "avg_logprob": -0.1134261483544702, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.004005471244454384}, {"id": 659, "seek": 358412, "start": 3594.92, "end": 3599.72, "text": " to also result in a decrease. Sorry, any further decrease in free energy would also result in a", "tokens": [50904, 281, 611, 1874, 294, 257, 11514, 13, 4919, 11, 604, 3052, 11514, 294, 1737, 2281, 576, 611, 1874, 294, 257, 51144], "temperature": 0.0, "avg_logprob": -0.1134261483544702, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.004005471244454384}, {"id": 660, "seek": 358412, "start": 3599.72, "end": 3604.68, "text": " further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,", "tokens": [51144, 3052, 11514, 294, 6365, 13, 286, 914, 11, 456, 311, 257, 1326, 721, 300, 13159, 385, 13, 286, 914, 11, 700, 295, 439, 11, 51392], "temperature": 0.0, "avg_logprob": -0.1134261483544702, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.004005471244454384}, {"id": 661, "seek": 358412, "start": 3604.68, "end": 3611.88, "text": " what struck me is that we're using the language of things like statistical mechanics and Bayesian", "tokens": [51392, 437, 13159, 385, 307, 300, 321, 434, 1228, 264, 2856, 295, 721, 411, 22820, 12939, 293, 7840, 42434, 51752], "temperature": 0.0, "avg_logprob": -0.1134261483544702, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.004005471244454384}, {"id": 662, "seek": 361188, "start": 3612.28, "end": 3619.0, "text": " statistics and information theory, things like entropy and so on. And we're interchangeably", "tokens": [50384, 12523, 293, 1589, 5261, 11, 721, 411, 30867, 293, 370, 322, 13, 400, 321, 434, 30358, 1188, 50720], "temperature": 0.0, "avg_logprob": -0.11098326645888291, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004537464585155249}, {"id": 663, "seek": 361188, "start": 3620.12, "end": 3624.04, "text": " kind of speaking about the same thing from the perspective of different disciplines,", "tokens": [50776, 733, 295, 4124, 466, 264, 912, 551, 490, 264, 4585, 295, 819, 21919, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11098326645888291, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004537464585155249}, {"id": 664, "seek": 361188, "start": 3624.04, "end": 3631.96, "text": " which I find very, very interesting. And on the surprise thing, even though in this formalism,", "tokens": [50972, 597, 286, 915, 588, 11, 588, 1880, 13, 400, 322, 264, 6365, 551, 11, 754, 1673, 294, 341, 9860, 1434, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11098326645888291, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004537464585155249}, {"id": 665, "seek": 361188, "start": 3631.96, "end": 3638.04, "text": " we are minimizing surprise, I think there's an interesting perspective that sometimes surprise", "tokens": [51368, 321, 366, 46608, 6365, 11, 286, 519, 456, 311, 364, 1880, 4585, 300, 2171, 6365, 51672], "temperature": 0.0, "avg_logprob": -0.11098326645888291, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004537464585155249}, {"id": 666, "seek": 363804, "start": 3638.04, "end": 3644.92, "text": " is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when", "tokens": [50364, 307, 437, 321, 528, 13, 407, 337, 1365, 11, 264, 24122, 9284, 11, 264, 14426, 46, 9284, 11, 309, 311, 787, 562, 50708], "temperature": 0.0, "avg_logprob": -0.10548602782928192, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.4609244167804718}, {"id": 667, "seek": 363804, "start": 3644.92, "end": 3651.0, "text": " something surprising happens that the weights get updated because it's information. Or people on", "tokens": [50708, 746, 8830, 2314, 300, 264, 17443, 483, 10588, 570, 309, 311, 1589, 13, 1610, 561, 322, 51012], "temperature": 0.0, "avg_logprob": -0.10548602782928192, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.4609244167804718}, {"id": 668, "seek": 363804, "start": 3651.0, "end": 3657.64, "text": " YouTube, my videos are that they get more views when they have a cash value, which means they", "tokens": [51012, 3088, 11, 452, 2145, 366, 300, 436, 483, 544, 6809, 562, 436, 362, 257, 6388, 2158, 11, 597, 1355, 436, 51344], "temperature": 0.0, "avg_logprob": -0.10548602782928192, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.4609244167804718}, {"id": 669, "seek": 363804, "start": 3657.64, "end": 3661.96, "text": " have information content, which means that, you know, they're actually surprising your predictive", "tokens": [51344, 362, 1589, 2701, 11, 597, 1355, 300, 11, 291, 458, 11, 436, 434, 767, 8830, 428, 35521, 51560], "temperature": 0.0, "avg_logprob": -0.10548602782928192, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.4609244167804718}, {"id": 670, "seek": 363804, "start": 3661.96, "end": 3666.36, "text": " model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.", "tokens": [51560, 2316, 13, 2754, 30406, 46487, 2904, 1146, 1321, 1143, 281, 7647, 466, 309, 11, 415, 848, 11, 291, 362, 281, 5588, 264, 9530, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10548602782928192, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.4609244167804718}, {"id": 671, "seek": 366636, "start": 3666.36, "end": 3670.52, "text": " You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So", "tokens": [50364, 509, 458, 11, 291, 362, 281, 360, 437, 264, 9530, 500, 380, 2066, 13, 10328, 11, 456, 311, 406, 364, 21549, 13, 407, 50572], "temperature": 0.0, "avg_logprob": -0.07238568641521313, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.0012120268074795604}, {"id": 672, "seek": 366636, "start": 3670.52, "end": 3675.1600000000003, "text": " there's this interesting juxtaposition between actually seeking out surprise, even though you", "tokens": [50572, 456, 311, 341, 1880, 3649, 734, 569, 5830, 1296, 767, 11670, 484, 6365, 11, 754, 1673, 291, 50804], "temperature": 0.0, "avg_logprob": -0.07238568641521313, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.0012120268074795604}, {"id": 673, "seek": 366636, "start": 3675.1600000000003, "end": 3680.2000000000003, "text": " can think of our brains overall as minimizing surprise. And what was the other thing I was", "tokens": [50804, 393, 519, 295, 527, 15442, 4787, 382, 46608, 6365, 13, 400, 437, 390, 264, 661, 551, 286, 390, 51056], "temperature": 0.0, "avg_logprob": -0.07238568641521313, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.0012120268074795604}, {"id": 674, "seek": 366636, "start": 3680.2000000000003, "end": 3684.1200000000003, "text": " going to say? Yeah, you were just getting onto variational inference, which is really interesting.", "tokens": [51056, 516, 281, 584, 30, 865, 11, 291, 645, 445, 1242, 3911, 3034, 1478, 38253, 11, 597, 307, 534, 1880, 13, 51252], "temperature": 0.0, "avg_logprob": -0.07238568641521313, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.0012120268074795604}, {"id": 675, "seek": 366636, "start": 3684.1200000000003, "end": 3690.04, "text": " So there's a couple of intractable statistical quantities in this mixture that we're talking", "tokens": [51252, 407, 456, 311, 257, 1916, 295, 560, 1897, 712, 22820, 22927, 294, 341, 9925, 300, 321, 434, 1417, 51548], "temperature": 0.0, "avg_logprob": -0.07238568641521313, "compression_ratio": 1.6654929577464788, "no_speech_prob": 0.0012120268074795604}, {"id": 676, "seek": 369004, "start": 3690.04, "end": 3696.52, "text": " about. I think it's the log model evidence and the Bayesian posterior. And we can't represent", "tokens": [50364, 466, 13, 286, 519, 309, 311, 264, 3565, 2316, 4467, 293, 264, 7840, 42434, 33529, 13, 400, 321, 393, 380, 2906, 50688], "temperature": 0.0, "avg_logprob": -0.08424408682461443, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03622201830148697}, {"id": 677, "seek": 369004, "start": 3696.52, "end": 3701.48, "text": " those things directly. So we have to put a proxy in there, which kind of captures most of the", "tokens": [50688, 729, 721, 3838, 13, 407, 321, 362, 281, 829, 257, 29690, 294, 456, 11, 597, 733, 295, 27986, 881, 295, 264, 50936], "temperature": 0.0, "avg_logprob": -0.08424408682461443, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03622201830148697}, {"id": 678, "seek": 369004, "start": 3701.48, "end": 3706.36, "text": " information, but it's still possible to deal with it. So how does this variational inference work?", "tokens": [50936, 1589, 11, 457, 309, 311, 920, 1944, 281, 2028, 365, 309, 13, 407, 577, 775, 341, 3034, 1478, 38253, 589, 30, 51180], "temperature": 0.0, "avg_logprob": -0.08424408682461443, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03622201830148697}, {"id": 679, "seek": 369004, "start": 3707.32, "end": 3712.44, "text": " Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian", "tokens": [51228, 865, 13, 407, 286, 7297, 1310, 264, 700, 551, 281, 519, 466, 11, 1673, 11, 307, 445, 281, 20928, 437, 7840, 42434, 51484], "temperature": 0.0, "avg_logprob": -0.08424408682461443, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03622201830148697}, {"id": 680, "seek": 369004, "start": 3712.44, "end": 3716.92, "text": " inference is. I suppose we've been talking about it quite a lot without necessarily defining it.", "tokens": [51484, 38253, 307, 13, 286, 7297, 321, 600, 668, 1417, 466, 309, 1596, 257, 688, 1553, 4725, 17827, 309, 13, 51708], "temperature": 0.0, "avg_logprob": -0.08424408682461443, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03622201830148697}, {"id": 681, "seek": 371692, "start": 3716.92, "end": 3724.28, "text": " And many of you listeners, I'm sure, will know already. But the idea is actually relatively", "tokens": [50364, 400, 867, 295, 291, 23274, 11, 286, 478, 988, 11, 486, 458, 1217, 13, 583, 264, 1558, 307, 767, 7226, 50732], "temperature": 0.0, "avg_logprob": -0.07912611961364746, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0022437218576669693}, {"id": 682, "seek": 371692, "start": 3724.28, "end": 3728.52, "text": " straightforward and well-established and quite widely used. And it's the idea that if I have", "tokens": [50732, 15325, 293, 731, 12, 33542, 4173, 293, 1596, 13371, 1143, 13, 400, 309, 311, 264, 1558, 300, 498, 286, 362, 50944], "temperature": 0.0, "avg_logprob": -0.07912611961364746, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0022437218576669693}, {"id": 683, "seek": 371692, "start": 3728.52, "end": 3735.64, "text": " some beliefs about things that are in my world that I can't directly observe, I may have a sense", "tokens": [50944, 512, 13585, 466, 721, 300, 366, 294, 452, 1002, 300, 286, 393, 380, 3838, 11441, 11, 286, 815, 362, 257, 2020, 51300], "temperature": 0.0, "avg_logprob": -0.07912611961364746, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0022437218576669693}, {"id": 684, "seek": 371692, "start": 3735.64, "end": 3741.08, "text": " of what's plausible to begin with. And that's what we refer to as a prior probability. I then also", "tokens": [51300, 295, 437, 311, 39925, 281, 1841, 365, 13, 400, 300, 311, 437, 321, 2864, 281, 382, 257, 4059, 8482, 13, 286, 550, 611, 51572], "temperature": 0.0, "avg_logprob": -0.07912611961364746, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.0022437218576669693}, {"id": 685, "seek": 374108, "start": 3741.08, "end": 3748.6, "text": " need to have a model that says, given the world is this way, what would I expect to actually observe?", "tokens": [50364, 643, 281, 362, 257, 2316, 300, 1619, 11, 2212, 264, 1002, 307, 341, 636, 11, 437, 576, 286, 2066, 281, 767, 11441, 30, 50740], "temperature": 0.0, "avg_logprob": -0.06341205835342408, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.1384323388338089}, {"id": 686, "seek": 374108, "start": 3749.16, "end": 3756.92, "text": " So for instance, given where you are relative to me, I can predict a certain pattern on my retina.", "tokens": [50768, 407, 337, 5197, 11, 2212, 689, 291, 366, 4972, 281, 385, 11, 286, 393, 6069, 257, 1629, 5102, 322, 452, 1533, 1426, 13, 51156], "temperature": 0.0, "avg_logprob": -0.06341205835342408, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.1384323388338089}, {"id": 687, "seek": 374108, "start": 3757.56, "end": 3761.56, "text": " And if you were somewhere else, I would expect a different pattern on my retina. So I might have", "tokens": [51188, 400, 498, 291, 645, 4079, 1646, 11, 286, 576, 2066, 257, 819, 5102, 322, 452, 1533, 1426, 13, 407, 286, 1062, 362, 51388], "temperature": 0.0, "avg_logprob": -0.06341205835342408, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.1384323388338089}, {"id": 688, "seek": 374108, "start": 3761.56, "end": 3765.72, "text": " a prior range of plausibilities as to where you are relative to me. And then I have a model that", "tokens": [51388, 257, 4059, 3613, 295, 34946, 8261, 382, 281, 689, 291, 366, 4972, 281, 385, 13, 400, 550, 286, 362, 257, 2316, 300, 51596], "temperature": 0.0, "avg_logprob": -0.06341205835342408, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.1384323388338089}, {"id": 689, "seek": 374108, "start": 3765.72, "end": 3770.84, "text": " explains how I'm going to generate some data based upon that. And Bayesian inference basically", "tokens": [51596, 13948, 577, 286, 478, 516, 281, 8460, 512, 1412, 2361, 3564, 300, 13, 400, 7840, 42434, 38253, 1936, 51852], "temperature": 0.0, "avg_logprob": -0.06341205835342408, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.1384323388338089}, {"id": 690, "seek": 377084, "start": 3770.84, "end": 3777.7200000000003, "text": " takes those two things and inverts them using Bayes' theorem and effectively just flips both of them", "tokens": [50364, 2516, 729, 732, 721, 293, 28653, 1373, 552, 1228, 7840, 279, 6, 20904, 293, 8659, 445, 40249, 1293, 295, 552, 50708], "temperature": 0.0, "avg_logprob": -0.07011243275233678, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.001682812231592834}, {"id": 691, "seek": 377084, "start": 3777.7200000000003, "end": 3783.88, "text": " round. So you now say instead of a distribution of where you are relative to me, I'm now talking", "tokens": [50708, 3098, 13, 407, 291, 586, 584, 2602, 295, 257, 7316, 295, 689, 291, 366, 4972, 281, 385, 11, 286, 478, 586, 1417, 51016], "temperature": 0.0, "avg_logprob": -0.07011243275233678, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.001682812231592834}, {"id": 692, "seek": 377084, "start": 3783.88, "end": 3789.6400000000003, "text": " about a distribution of all the possible things that I could see on my retina. And instead of", "tokens": [51016, 466, 257, 7316, 295, 439, 264, 1944, 721, 300, 286, 727, 536, 322, 452, 1533, 1426, 13, 400, 2602, 295, 51304], "temperature": 0.0, "avg_logprob": -0.07011243275233678, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.001682812231592834}, {"id": 693, "seek": 377084, "start": 3789.6400000000003, "end": 3797.0, "text": " predicting the distribution on the retina given where you are, I now want to know the distribution", "tokens": [51304, 32884, 264, 7316, 322, 264, 1533, 1426, 2212, 689, 291, 366, 11, 286, 586, 528, 281, 458, 264, 7316, 51672], "temperature": 0.0, "avg_logprob": -0.07011243275233678, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.001682812231592834}, {"id": 694, "seek": 379700, "start": 3797.0, "end": 3803.4, "text": " of where you are given what's on my retina. And Bayesian inference, much like active inference,", "tokens": [50364, 295, 689, 291, 366, 2212, 437, 311, 322, 452, 1533, 1426, 13, 400, 7840, 42434, 38253, 11, 709, 411, 4967, 38253, 11, 50684], "temperature": 0.0, "avg_logprob": -0.059659844353085474, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0024915377143770456}, {"id": 695, "seek": 379700, "start": 3803.4, "end": 3807.64, "text": " is full of all these interesting inversions where you sort of flip things round from how", "tokens": [50684, 307, 1577, 295, 439, 613, 1880, 21378, 626, 689, 291, 1333, 295, 7929, 721, 3098, 490, 577, 50896], "temperature": 0.0, "avg_logprob": -0.059659844353085474, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0024915377143770456}, {"id": 696, "seek": 379700, "start": 3807.64, "end": 3814.28, "text": " they initially appeared. But the problem is calculating those two things, calculating the", "tokens": [50896, 436, 9105, 8516, 13, 583, 264, 1154, 307, 28258, 729, 732, 721, 11, 28258, 264, 51228], "temperature": 0.0, "avg_logprob": -0.059659844353085474, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0024915377143770456}, {"id": 697, "seek": 379700, "start": 3814.28, "end": 3819.64, "text": " flipped model. So the distribution of all the things on my retina here would now be my model", "tokens": [51228, 26273, 2316, 13, 407, 264, 7316, 295, 439, 264, 721, 322, 452, 1533, 1426, 510, 576, 586, 312, 452, 2316, 51496], "temperature": 0.0, "avg_logprob": -0.059659844353085474, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0024915377143770456}, {"id": 698, "seek": 381964, "start": 3819.64, "end": 3828.3599999999997, "text": " evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina", "tokens": [50364, 4467, 11, 452, 17340, 6365, 13, 400, 264, 7316, 295, 689, 291, 366, 4972, 281, 437, 311, 322, 452, 1533, 1426, 50800], "temperature": 0.0, "avg_logprob": -0.06158964493695428, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.008132698014378548}, {"id": 699, "seek": 381964, "start": 3829.24, "end": 3833.7999999999997, "text": " is my posterior distribution. But those things are not always straightforward to calculate.", "tokens": [50844, 307, 452, 33529, 7316, 13, 583, 729, 721, 366, 406, 1009, 15325, 281, 8873, 13, 51072], "temperature": 0.0, "avg_logprob": -0.06158964493695428, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.008132698014378548}, {"id": 700, "seek": 381964, "start": 3834.3599999999997, "end": 3840.2, "text": " And so variational inference takes that problem and makes it into an optimization problem. It", "tokens": [51100, 400, 370, 3034, 1478, 38253, 2516, 300, 1154, 293, 1669, 309, 666, 364, 19618, 1154, 13, 467, 51392], "temperature": 0.0, "avg_logprob": -0.06158964493695428, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.008132698014378548}, {"id": 701, "seek": 381964, "start": 3840.2, "end": 3848.12, "text": " writes down a function that quantifies how far am I away from my, or what would be the true posterior", "tokens": [51392, 13657, 760, 257, 2445, 300, 4426, 11221, 577, 1400, 669, 286, 1314, 490, 452, 11, 420, 437, 576, 312, 264, 2074, 33529, 51788], "temperature": 0.0, "avg_logprob": -0.06158964493695428, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.008132698014378548}, {"id": 702, "seek": 384812, "start": 3848.12, "end": 3855.48, "text": " if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior", "tokens": [50364, 498, 286, 1116, 1143, 1900, 7840, 279, 13, 400, 550, 309, 1619, 11, 731, 11, 718, 311, 13075, 1125, 512, 30874, 33529, 50732], "temperature": 0.0, "avg_logprob": -0.08336108338599112, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0010677339741960168}, {"id": 703, "seek": 384812, "start": 3855.48, "end": 3860.12, "text": " probability. So come up with a function that represents a probability distribution that's", "tokens": [50732, 8482, 13, 407, 808, 493, 365, 257, 2445, 300, 8855, 257, 8482, 7316, 300, 311, 50964], "temperature": 0.0, "avg_logprob": -0.08336108338599112, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0010677339741960168}, {"id": 704, "seek": 384812, "start": 3860.12, "end": 3864.52, "text": " easy to characterize, something like a Gaussian distribution where I know I just need my mean", "tokens": [50964, 1858, 281, 38463, 11, 746, 411, 257, 39148, 7316, 689, 286, 458, 286, 445, 643, 452, 914, 51184], "temperature": 0.0, "avg_logprob": -0.08336108338599112, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0010677339741960168}, {"id": 705, "seek": 384812, "start": 3864.52, "end": 3870.92, "text": " and my variance. And then just changes that mean and variance until you minimize this function", "tokens": [51184, 293, 452, 21977, 13, 400, 550, 445, 2962, 300, 914, 293, 21977, 1826, 291, 17522, 341, 2445, 51504], "temperature": 0.0, "avg_logprob": -0.08336108338599112, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0010677339741960168}, {"id": 706, "seek": 384812, "start": 3870.92, "end": 3876.3599999999997, "text": " that represents that discrepancy, minimize this free energy, also sometimes known as an evidence", "tokens": [51504, 300, 8855, 300, 2983, 265, 6040, 1344, 11, 17522, 341, 1737, 2281, 11, 611, 2171, 2570, 382, 364, 4467, 51776], "temperature": 0.0, "avg_logprob": -0.08336108338599112, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0010677339741960168}, {"id": 707, "seek": 387636, "start": 3876.36, "end": 3882.1200000000003, "text": " lower bound, in which case you maximize it. And interestingly, once you've maximized your", "tokens": [50364, 3126, 5472, 11, 294, 597, 1389, 291, 19874, 309, 13, 400, 25873, 11, 1564, 291, 600, 5138, 1602, 428, 50652], "temperature": 0.0, "avg_logprob": -0.08215555191040039, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.0021743273828178644}, {"id": 708, "seek": 387636, "start": 3882.1200000000003, "end": 3886.44, "text": " evidence lower bound or minimized your free energy, you end up with a situation where", "tokens": [50652, 4467, 3126, 5472, 420, 4464, 1602, 428, 1737, 2281, 11, 291, 917, 493, 365, 257, 2590, 689, 50868], "temperature": 0.0, "avg_logprob": -0.08215555191040039, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.0021743273828178644}, {"id": 709, "seek": 387636, "start": 3887.7200000000003, "end": 3894.2000000000003, "text": " the free energy starts to approximate your log model evidence or your negative log surprise.", "tokens": [50932, 264, 1737, 2281, 3719, 281, 30874, 428, 3565, 2316, 4467, 420, 428, 3671, 3565, 6365, 13, 51256], "temperature": 0.0, "avg_logprob": -0.08215555191040039, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.0021743273828178644}, {"id": 710, "seek": 387636, "start": 3895.48, "end": 3901.2400000000002, "text": " And your approximate posterior distribution, your variational distribution starts to look", "tokens": [51320, 400, 428, 30874, 33529, 7316, 11, 428, 3034, 1478, 7316, 3719, 281, 574, 51608], "temperature": 0.0, "avg_logprob": -0.08215555191040039, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.0021743273828178644}, {"id": 711, "seek": 390124, "start": 3901.24, "end": 3908.7599999999998, "text": " much more like your exact posterior probability distribution. So it's another one of those", "tokens": [50364, 709, 544, 411, 428, 1900, 33529, 8482, 7316, 13, 407, 309, 311, 1071, 472, 295, 729, 50740], "temperature": 0.0, "avg_logprob": -0.05040372983373777, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.004174201749265194}, {"id": 712, "seek": 390124, "start": 3908.7599999999998, "end": 3913.7999999999997, "text": " interesting scenarios where doing one thing optimizing one quantity ends up having a dual", "tokens": [50740, 1880, 15077, 689, 884, 472, 551, 40425, 472, 11275, 5314, 493, 1419, 257, 11848, 50992], "temperature": 0.0, "avg_logprob": -0.05040372983373777, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.004174201749265194}, {"id": 713, "seek": 390124, "start": 3913.7999999999997, "end": 3918.04, "text": " purpose. And in active inference, the only additional thing you throw into that is that you", "tokens": [50992, 4334, 13, 400, 294, 4967, 38253, 11, 264, 787, 4497, 551, 291, 3507, 666, 300, 307, 300, 291, 51204], "temperature": 0.0, "avg_logprob": -0.05040372983373777, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.004174201749265194}, {"id": 714, "seek": 390124, "start": 3918.04, "end": 3925.3999999999996, "text": " want to then also change your data itself. So you do the third thing you act on the world", "tokens": [51204, 528, 281, 550, 611, 1319, 428, 1412, 2564, 13, 407, 291, 360, 264, 2636, 551, 291, 605, 322, 264, 1002, 51572], "temperature": 0.0, "avg_logprob": -0.05040372983373777, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.004174201749265194}, {"id": 715, "seek": 390124, "start": 3925.3999999999996, "end": 3930.3599999999997, "text": " to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting", "tokens": [51572, 281, 550, 19719, 2293, 264, 912, 10024, 13, 440, 1880, 551, 11, 286, 2041, 11, 307, 445, 8712, 278, 51820], "temperature": 0.0, "avg_logprob": -0.05040372983373777, "compression_ratio": 1.7255639097744362, "no_speech_prob": 0.004174201749265194}, {"id": 716, "seek": 393036, "start": 3930.36, "end": 3935.8, "text": " to machine learning again. So in machine learning, we also have these big parameterized models and we", "tokens": [50364, 281, 3479, 2539, 797, 13, 407, 294, 3479, 2539, 11, 321, 611, 362, 613, 955, 13075, 1602, 5245, 293, 321, 50636], "temperature": 0.0, "avg_logprob": -0.08329064891023456, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.00851812120527029}, {"id": 717, "seek": 393036, "start": 3935.8, "end": 3941.1600000000003, "text": " do stochastic gradient descent. And some might think of deep learning, because obviously you", "tokens": [50636, 360, 342, 8997, 2750, 16235, 23475, 13, 400, 512, 1062, 519, 295, 2452, 2539, 11, 570, 2745, 291, 50904], "temperature": 0.0, "avg_logprob": -0.08329064891023456, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.00851812120527029}, {"id": 718, "seek": 393036, "start": 3941.1600000000003, "end": 3945.2400000000002, "text": " can think of everything as a Bayesian. So you can think of machine learning as being maximum", "tokens": [50904, 393, 519, 295, 1203, 382, 257, 7840, 42434, 13, 407, 291, 393, 519, 295, 3479, 2539, 382, 885, 6674, 51108], "temperature": 0.0, "avg_logprob": -0.08329064891023456, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.00851812120527029}, {"id": 719, "seek": 393036, "start": 3945.2400000000002, "end": 3952.92, "text": " likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not", "tokens": [51108, 22119, 35701, 13, 1545, 307, 309, 300, 321, 352, 1577, 7840, 42434, 562, 321, 360, 4967, 38253, 30, 1545, 406, 51492], "temperature": 0.0, "avg_logprob": -0.08329064891023456, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.00851812120527029}, {"id": 720, "seek": 393036, "start": 3952.92, "end": 3958.6800000000003, "text": " something like maximum likelihood estimation? It's an interesting question. And there are a couple", "tokens": [51492, 746, 411, 6674, 22119, 35701, 30, 467, 311, 364, 1880, 1168, 13, 400, 456, 366, 257, 1916, 51780], "temperature": 0.0, "avg_logprob": -0.08329064891023456, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.00851812120527029}, {"id": 721, "seek": 395868, "start": 3958.68, "end": 3964.12, "text": " of answers you could give again, some of which are more technical, but some of which are", "tokens": [50364, 295, 6338, 291, 727, 976, 797, 11, 512, 295, 597, 366, 544, 6191, 11, 457, 512, 295, 597, 366, 50636], "temperature": 0.0, "avg_logprob": -0.09226020177205403, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0019275745144113898}, {"id": 722, "seek": 395868, "start": 3967.24, "end": 3972.44, "text": " some of which are slightly more intuitive. And I think one of the more intuitive answers is that", "tokens": [50792, 512, 295, 597, 366, 4748, 544, 21769, 13, 400, 286, 519, 472, 295, 264, 544, 21769, 6338, 307, 300, 51052], "temperature": 0.0, "avg_logprob": -0.09226020177205403, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0019275745144113898}, {"id": 723, "seek": 395868, "start": 3972.44, "end": 3978.3599999999997, "text": " by having an expression of plausibility of things in advance, you just maintain things", "tokens": [51052, 538, 1419, 364, 6114, 295, 34946, 2841, 295, 721, 294, 7295, 11, 291, 445, 6909, 721, 51348], "temperature": 0.0, "avg_logprob": -0.09226020177205403, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0019275745144113898}, {"id": 724, "seek": 395868, "start": 3978.3599999999997, "end": 3984.12, "text": " within a plausible region. So maximum likelihood for those who are unaware is where you essentially", "tokens": [51348, 1951, 257, 39925, 4458, 13, 407, 6674, 22119, 337, 729, 567, 366, 32065, 307, 689, 291, 4476, 51636], "temperature": 0.0, "avg_logprob": -0.09226020177205403, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0019275745144113898}, {"id": 725, "seek": 398412, "start": 3984.2, "end": 3989.56, "text": " throw away that prior probability, where you throw away any prior plausibility as to as to", "tokens": [50368, 3507, 1314, 300, 4059, 8482, 11, 689, 291, 3507, 1314, 604, 4059, 34946, 2841, 382, 281, 382, 281, 50636], "temperature": 0.0, "avg_logprob": -0.06691468897319976, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.013710124418139458}, {"id": 726, "seek": 398412, "start": 3989.56, "end": 3996.44, "text": " what the state of the world might be. And you just try and find the value that would maximize", "tokens": [50636, 437, 264, 1785, 295, 264, 1002, 1062, 312, 13, 400, 291, 445, 853, 293, 915, 264, 2158, 300, 576, 19874, 50980], "temperature": 0.0, "avg_logprob": -0.06691468897319976, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.013710124418139458}, {"id": 727, "seek": 398412, "start": 3996.44, "end": 4001.48, "text": " your likelihood, which is your prediction of how things would be under some hypothesis or under", "tokens": [50980, 428, 22119, 11, 597, 307, 428, 17630, 295, 577, 721, 576, 312, 833, 512, 17291, 420, 833, 51232], "temperature": 0.0, "avg_logprob": -0.06691468897319976, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.013710124418139458}, {"id": 728, "seek": 398412, "start": 4001.48, "end": 4013.08, "text": " some parameter setting. And I think the first thing to say is if you throw away that prior", "tokens": [51232, 512, 13075, 3287, 13, 400, 286, 519, 264, 700, 551, 281, 584, 307, 498, 291, 3507, 1314, 300, 4059, 51812], "temperature": 0.0, "avg_logprob": -0.06691468897319976, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.013710124418139458}, {"id": 729, "seek": 401308, "start": 4013.08, "end": 4017.96, "text": " information, then you end up potentially coming up with quite implausible solutions.", "tokens": [50364, 1589, 11, 550, 291, 917, 493, 7263, 1348, 493, 365, 1596, 8484, 8463, 964, 6547, 13, 50608], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 730, "seek": 401308, "start": 4019.0, "end": 4023.3199999999997, "text": " That's particularly relevant if you're dealing with what's known as an inverse problem. So where", "tokens": [50660, 663, 311, 4098, 7340, 498, 291, 434, 6260, 365, 437, 311, 2570, 382, 364, 17340, 1154, 13, 407, 689, 50876], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 731, "seek": 401308, "start": 4023.3199999999997, "end": 4028.84, "text": " there are multiple different things that could have caused the same outcome. An example that's", "tokens": [50876, 456, 366, 3866, 819, 721, 300, 727, 362, 7008, 264, 912, 9700, 13, 1107, 1365, 300, 311, 51152], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 732, "seek": 401308, "start": 4028.84, "end": 4033.72, "text": " often given is that for any given shadow, there's almost an infinite number of things, configurations", "tokens": [51152, 2049, 2212, 307, 300, 337, 604, 2212, 8576, 11, 456, 311, 1920, 364, 13785, 1230, 295, 721, 11, 31493, 51396], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 733, "seek": 401308, "start": 4033.72, "end": 4038.04, "text": " of the sun and the shape of the thing that's casting the shadow that could lead to exactly the", "tokens": [51396, 295, 264, 3295, 293, 264, 3909, 295, 264, 551, 300, 311, 17301, 264, 8576, 300, 727, 1477, 281, 2293, 264, 51612], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 734, "seek": 401308, "start": 4038.04, "end": 4042.52, "text": " same shadow. And so maximum likelihood approach just won't be able to tell the difference between", "tokens": [51612, 912, 8576, 13, 400, 370, 6674, 22119, 3109, 445, 1582, 380, 312, 1075, 281, 980, 264, 2649, 1296, 51836], "temperature": 0.0, "avg_logprob": -0.06395528762321162, "compression_ratio": 1.7678018575851393, "no_speech_prob": 0.010268017649650574}, {"id": 735, "seek": 404252, "start": 4042.52, "end": 4047.88, "text": " all of those things. However, if you have some prior on top of that, if you have some statement", "tokens": [50364, 439, 295, 729, 721, 13, 2908, 11, 498, 291, 362, 512, 4059, 322, 1192, 295, 300, 11, 498, 291, 362, 512, 5629, 50632], "temperature": 0.0, "avg_logprob": -0.03683192913348858, "compression_ratio": 1.7671232876712328, "no_speech_prob": 0.0019148803548887372}, {"id": 736, "seek": 404252, "start": 4047.88, "end": 4052.7599999999998, "text": " of the plausible things that might cause it, you can come up with a much better estimate of those", "tokens": [50632, 295, 264, 39925, 721, 300, 1062, 3082, 309, 11, 291, 393, 808, 493, 365, 257, 709, 1101, 12539, 295, 729, 50876], "temperature": 0.0, "avg_logprob": -0.03683192913348858, "compression_ratio": 1.7671232876712328, "no_speech_prob": 0.0019148803548887372}, {"id": 737, "seek": 404252, "start": 4052.7599999999998, "end": 4063.8, "text": " sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood", "tokens": [50876, 7527, 295, 721, 13, 3996, 636, 295, 1237, 412, 309, 307, 300, 562, 291, 434, 6260, 365, 257, 6674, 22119, 51428], "temperature": 0.0, "avg_logprob": -0.03683192913348858, "compression_ratio": 1.7671232876712328, "no_speech_prob": 0.0019148803548887372}, {"id": 738, "seek": 404252, "start": 4063.8, "end": 4069.16, "text": " estimate, you're throwing away all uncertainty about the solution. So you're coming up with a", "tokens": [51428, 12539, 11, 291, 434, 10238, 1314, 439, 15697, 466, 264, 3827, 13, 407, 291, 434, 1348, 493, 365, 257, 51696], "temperature": 0.0, "avg_logprob": -0.03683192913348858, "compression_ratio": 1.7671232876712328, "no_speech_prob": 0.0019148803548887372}, {"id": 739, "seek": 406916, "start": 4069.16, "end": 4073.3199999999997, "text": " point estimate and you're saying this is the most likely thing, but you're ignoring all of your", "tokens": [50364, 935, 12539, 293, 291, 434, 1566, 341, 307, 264, 881, 3700, 551, 11, 457, 291, 434, 26258, 439, 295, 428, 50572], "temperature": 0.0, "avg_logprob": -0.10181004513976394, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.03329673781991005}, {"id": 740, "seek": 406916, "start": 4073.3199999999997, "end": 4079.08, "text": " uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to", "tokens": [50572, 15697, 466, 309, 13, 400, 286, 519, 300, 307, 294, 2564, 257, 7226, 5795, 551, 281, 360, 293, 393, 1477, 281, 50860], "temperature": 0.0, "avg_logprob": -0.10181004513976394, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.03329673781991005}, {"id": 741, "seek": 406916, "start": 4079.08, "end": 4083.8799999999997, "text": " the problem of overfitting, where you start to become very confident about what you can see from", "tokens": [50860, 264, 1154, 295, 670, 69, 2414, 11, 689, 291, 722, 281, 1813, 588, 6679, 466, 437, 291, 393, 536, 490, 51100], "temperature": 0.0, "avg_logprob": -0.10181004513976394, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.03329673781991005}, {"id": 742, "seek": 406916, "start": 4083.8799999999997, "end": 4094.92, "text": " a relatively small sample of things and you can end up with all of these well-described in the media", "tokens": [51100, 257, 7226, 1359, 6889, 295, 721, 293, 291, 393, 917, 493, 365, 439, 295, 613, 731, 12, 14792, 18732, 294, 264, 3021, 51652], "temperature": 0.0, "avg_logprob": -0.10181004513976394, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.03329673781991005}, {"id": 743, "seek": 409492, "start": 4094.92, "end": 4100.52, "text": " scenarios of complete misclassifications based upon that sort of overconfidence just because", "tokens": [50364, 15077, 295, 3566, 3346, 11665, 7833, 2361, 3564, 300, 1333, 295, 670, 47273, 445, 570, 50644], "temperature": 0.0, "avg_logprob": -0.0663231417190197, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0012705300468951464}, {"id": 744, "seek": 409492, "start": 4100.52, "end": 4107.96, "text": " all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about", "tokens": [50644, 439, 264, 15697, 307, 2780, 13, 316, 544, 6191, 636, 295, 1237, 412, 309, 11, 286, 519, 11, 307, 498, 291, 519, 466, 51016], "temperature": 0.0, "avg_logprob": -0.0663231417190197, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0012705300468951464}, {"id": 745, "seek": 409492, "start": 4108.68, "end": 4116.12, "text": " what a free energy is. So free energy is our measure of our marginal likelihood that we're", "tokens": [51052, 437, 257, 1737, 2281, 307, 13, 407, 1737, 2281, 307, 527, 3481, 295, 527, 16885, 22119, 300, 321, 434, 51424], "temperature": 0.0, "avg_logprob": -0.0663231417190197, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0012705300468951464}, {"id": 746, "seek": 409492, "start": 4116.12, "end": 4122.76, "text": " using when we're doing Bayesian inference. And one way of separating out what a free energy", "tokens": [51424, 1228, 562, 321, 434, 884, 7840, 42434, 38253, 13, 400, 472, 636, 295, 29279, 484, 437, 257, 1737, 2281, 51756], "temperature": 0.0, "avg_logprob": -0.0663231417190197, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0012705300468951464}, {"id": 747, "seek": 412276, "start": 4122.76, "end": 4129.72, "text": " looks like is to have our complexity, which is effectively how far we needed to deviate from", "tokens": [50364, 1542, 411, 307, 281, 362, 527, 14024, 11, 597, 307, 8659, 577, 1400, 321, 2978, 281, 1905, 13024, 490, 50712], "temperature": 0.0, "avg_logprob": -0.08611928584963777, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.0014605523319914937}, {"id": 748, "seek": 412276, "start": 4129.72, "end": 4135.0, "text": " our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit", "tokens": [50712, 527, 4059, 17695, 281, 808, 493, 365, 364, 10835, 11, 293, 527, 14170, 11, 597, 307, 577, 731, 321, 393, 3318, 50976], "temperature": 0.0, "avg_logprob": -0.08611928584963777, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.0014605523319914937}, {"id": 749, "seek": 412276, "start": 4135.0, "end": 4142.52, "text": " our model. Accuracy is common to both maximum likelihood type approaches because we're trying", "tokens": [50976, 527, 2316, 13, 5725, 374, 2551, 307, 2689, 281, 1293, 6674, 22119, 2010, 11587, 570, 321, 434, 1382, 51352], "temperature": 0.0, "avg_logprob": -0.08611928584963777, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.0014605523319914937}, {"id": 750, "seek": 412276, "start": 4142.52, "end": 4150.12, "text": " to find the value that most accurately predicts our data and also to Bayesian approaches.", "tokens": [51352, 281, 915, 264, 2158, 300, 881, 20095, 6069, 82, 527, 1412, 293, 611, 281, 7840, 42434, 11587, 13, 51732], "temperature": 0.0, "avg_logprob": -0.08611928584963777, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.0014605523319914937}, {"id": 751, "seek": 415012, "start": 4150.12, "end": 4156.2, "text": " Both want to do that. But what's thrown away in the maximum likelihood type approach is the", "tokens": [50364, 6767, 528, 281, 360, 300, 13, 583, 437, 311, 11732, 1314, 294, 264, 6674, 22119, 2010, 3109, 307, 264, 50668], "temperature": 0.0, "avg_logprob": -0.08081991564143788, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004001096356660128}, {"id": 752, "seek": 415012, "start": 4156.2, "end": 4163.8, "text": " complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,", "tokens": [50668, 14024, 857, 11, 264, 577, 1400, 360, 291, 1905, 13024, 490, 428, 1790, 830, 13, 407, 456, 311, 364, 294, 23018, 26191, 335, 311, 30478, 11, 51048], "temperature": 0.0, "avg_logprob": -0.08081991564143788, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004001096356660128}, {"id": 753, "seek": 415012, "start": 4163.8, "end": 4170.2, "text": " the idea that the simplest explanation is a priori more likely that you get from a Bayesian", "tokens": [51048, 264, 1558, 300, 264, 22811, 10835, 307, 257, 4059, 72, 544, 3700, 300, 291, 483, 490, 257, 7840, 42434, 51368], "temperature": 0.0, "avg_logprob": -0.08081991564143788, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004001096356660128}, {"id": 754, "seek": 415012, "start": 4170.2, "end": 4174.2, "text": " approach that you throw away when you're dealing with maximum likelihood estimation.", "tokens": [51368, 3109, 300, 291, 3507, 1314, 562, 291, 434, 6260, 365, 6674, 22119, 35701, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08081991564143788, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.004001096356660128}, {"id": 755, "seek": 417420, "start": 4175.16, "end": 4181.48, "text": " I wondered to what extent does the active part play a role here. So even in machine learning,", "tokens": [50412, 286, 17055, 281, 437, 8396, 775, 264, 4967, 644, 862, 257, 3090, 510, 13, 407, 754, 294, 3479, 2539, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1326283086644541, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.004723474849015474}, {"id": 756, "seek": 417420, "start": 4183.0, "end": 4187.8, "text": " there's something called active learning, where you dynamically retrain the model,", "tokens": [50804, 456, 311, 746, 1219, 4967, 2539, 11, 689, 291, 43492, 1533, 7146, 264, 2316, 11, 51044], "temperature": 0.0, "avg_logprob": -0.1326283086644541, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.004723474849015474}, {"id": 757, "seek": 417420, "start": 4187.8, "end": 4192.28, "text": " or there's something called machine teaching, where you dynamically select more salient data", "tokens": [51044, 420, 456, 311, 746, 1219, 3479, 4571, 11, 689, 291, 43492, 3048, 544, 1845, 1196, 1412, 51268], "temperature": 0.0, "avg_logprob": -0.1326283086644541, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.004723474849015474}, {"id": 758, "seek": 417420, "start": 4192.28, "end": 4197.88, "text": " to train the model, and the model gets much better. And in things like Bayesian optimization,", "tokens": [51268, 281, 3847, 264, 2316, 11, 293, 264, 2316, 2170, 709, 1101, 13, 400, 294, 721, 411, 7840, 42434, 19618, 11, 51548], "temperature": 0.0, "avg_logprob": -0.1326283086644541, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.004723474849015474}, {"id": 759, "seek": 417420, "start": 4197.88, "end": 4203.8, "text": " for example, by maintaining this distribution of all of your uncertainty in a principled way,", "tokens": [51548, 337, 1365, 11, 538, 14916, 341, 7316, 295, 439, 295, 428, 15697, 294, 257, 3681, 15551, 636, 11, 51844], "temperature": 0.0, "avg_logprob": -0.1326283086644541, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.004723474849015474}, {"id": 760, "seek": 420380, "start": 4203.88, "end": 4209.72, "text": " you can go and seek and find more information to kind of improve your knowledge on subsequent steps.", "tokens": [50368, 291, 393, 352, 293, 8075, 293, 915, 544, 1589, 281, 733, 295, 3470, 428, 3601, 322, 19962, 4439, 13, 50660], "temperature": 0.0, "avg_logprob": -0.118470337277367, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.005586742423474789}, {"id": 761, "seek": 420380, "start": 4209.72, "end": 4215.24, "text": " So I guess it's sort of bringing in this idea of it's not just what happens now,", "tokens": [50660, 407, 286, 2041, 309, 311, 1333, 295, 5062, 294, 341, 1558, 295, 309, 311, 406, 445, 437, 2314, 586, 11, 50936], "temperature": 0.0, "avg_logprob": -0.118470337277367, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.005586742423474789}, {"id": 762, "seek": 420380, "start": 4215.24, "end": 4221.0, "text": " it's about how can I improve my knowledge of the world over several steps.", "tokens": [50936, 309, 311, 466, 577, 393, 286, 3470, 452, 3601, 295, 264, 1002, 670, 2940, 4439, 13, 51224], "temperature": 0.0, "avg_logprob": -0.118470337277367, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.005586742423474789}, {"id": 763, "seek": 420380, "start": 4221.72, "end": 4225.64, "text": " Yes, and that reminds me about the point you were making earlier, that sometimes we actually do", "tokens": [51260, 1079, 11, 293, 300, 12025, 385, 466, 264, 935, 291, 645, 1455, 3071, 11, 300, 2171, 321, 767, 360, 51456], "temperature": 0.0, "avg_logprob": -0.118470337277367, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.005586742423474789}, {"id": 764, "seek": 420380, "start": 4225.64, "end": 4233.64, "text": " things to surprise ourselves, which seems very counter-intuitive in the context of the idea that", "tokens": [51456, 721, 281, 6365, 4175, 11, 597, 2544, 588, 5682, 12, 686, 48314, 294, 264, 4319, 295, 264, 1558, 300, 51856], "temperature": 0.0, "avg_logprob": -0.118470337277367, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.005586742423474789}, {"id": 765, "seek": 423364, "start": 4233.64, "end": 4241.4800000000005, "text": " we're trying to minimize surprises as our sole objective in life. And sometimes people talk about", "tokens": [50364, 321, 434, 1382, 281, 17522, 22655, 382, 527, 12321, 10024, 294, 993, 13, 400, 2171, 561, 751, 466, 50756], "temperature": 0.0, "avg_logprob": -0.10749808510581216, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.000960079487413168}, {"id": 766, "seek": 423364, "start": 4241.4800000000005, "end": 4246.04, "text": " this in terms of a dark room problem, the idea that actually if all you want to do is minimize", "tokens": [50756, 341, 294, 2115, 295, 257, 2877, 1808, 1154, 11, 264, 1558, 300, 767, 498, 439, 291, 528, 281, 360, 307, 17522, 50984], "temperature": 0.0, "avg_logprob": -0.10749808510581216, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.000960079487413168}, {"id": 767, "seek": 423364, "start": 4246.04, "end": 4250.04, "text": " your surprise, you just go into a room, turn off the lights and stay there because you're not going", "tokens": [50984, 428, 6365, 11, 291, 445, 352, 666, 257, 1808, 11, 1261, 766, 264, 5811, 293, 1754, 456, 570, 291, 434, 406, 516, 51184], "temperature": 0.0, "avg_logprob": -0.10749808510581216, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.000960079487413168}, {"id": 768, "seek": 423364, "start": 4250.04, "end": 4260.12, "text": " to experience anything that's going to surprise you. I mean, the answer to this problem is that", "tokens": [51184, 281, 1752, 1340, 300, 311, 516, 281, 6365, 291, 13, 286, 914, 11, 264, 1867, 281, 341, 1154, 307, 300, 51688], "temperature": 0.0, "avg_logprob": -0.10749808510581216, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.000960079487413168}, {"id": 769, "seek": 426012, "start": 4260.12, "end": 4268.599999999999, "text": " actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the", "tokens": [50364, 767, 11, 382, 22110, 11, 382, 12281, 11, 321, 500, 380, 2066, 281, 312, 17491, 294, 257, 2877, 1808, 13, 400, 264, 50788], "temperature": 0.0, "avg_logprob": -0.08787972450256348, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.002784468699246645}, {"id": 770, "seek": 426012, "start": 4268.599999999999, "end": 4275.32, "text": " sort of organism that would be is, again, probably not a very interesting one. And that what we predict,", "tokens": [50788, 1333, 295, 24128, 300, 576, 312, 307, 11, 797, 11, 1391, 406, 257, 588, 1880, 472, 13, 400, 300, 437, 321, 6069, 11, 51124], "temperature": 0.0, "avg_logprob": -0.08787972450256348, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.002784468699246645}, {"id": 771, "seek": 426012, "start": 4275.32, "end": 4281.08, "text": " what we'd be surprised by might be permanently staying in a dark room. But it goes even further", "tokens": [51124, 437, 321, 1116, 312, 6100, 538, 1062, 312, 24042, 7939, 294, 257, 2877, 1808, 13, 583, 309, 1709, 754, 3052, 51412], "temperature": 0.0, "avg_logprob": -0.08787972450256348, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.002784468699246645}, {"id": 772, "seek": 426012, "start": 4281.08, "end": 4287.16, "text": " than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable", "tokens": [51412, 813, 300, 13, 400, 498, 291, 584, 11, 767, 11, 286, 478, 46608, 452, 6365, 670, 565, 11, 286, 528, 281, 312, 294, 257, 27737, 51716], "temperature": 0.0, "avg_logprob": -0.08787972450256348, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.002784468699246645}, {"id": 773, "seek": 428716, "start": 4287.16, "end": 4291.8, "text": " world where I know what's going to happen next. The best way of doing that is to actually gather", "tokens": [50364, 1002, 689, 286, 458, 437, 311, 516, 281, 1051, 958, 13, 440, 1151, 636, 295, 884, 300, 307, 281, 767, 5448, 50596], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 774, "seek": 428716, "start": 4291.8, "end": 4296.04, "text": " as much information as you can about the world around you. So the first thing you do really is", "tokens": [50596, 382, 709, 1589, 382, 291, 393, 466, 264, 1002, 926, 291, 13, 407, 264, 700, 551, 291, 360, 534, 307, 50808], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 775, "seek": 428716, "start": 4296.04, "end": 4299.639999999999, "text": " you turn on the light and see what the room looks like, because that might then predict all the sorts", "tokens": [50808, 291, 1261, 322, 264, 1442, 293, 536, 437, 264, 1808, 1542, 411, 11, 570, 300, 1062, 550, 6069, 439, 264, 7527, 50988], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 776, "seek": 428716, "start": 4299.639999999999, "end": 4303.72, "text": " of things that could fall on you in that room and could potentially cause surprise. And by knowing", "tokens": [50988, 295, 721, 300, 727, 2100, 322, 291, 294, 300, 1808, 293, 727, 7263, 3082, 6365, 13, 400, 538, 5276, 51192], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 777, "seek": 428716, "start": 4303.72, "end": 4309.639999999999, "text": " about it, you mitigate the surprise that you might get in the future. And as you say, you can only", "tokens": [51192, 466, 309, 11, 291, 27336, 264, 6365, 300, 291, 1062, 483, 294, 264, 2027, 13, 400, 382, 291, 584, 11, 291, 393, 787, 51488], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 778, "seek": 428716, "start": 4309.639999999999, "end": 4314.12, "text": " really do that if you know what you're certain about. And so if you take a maximum likelihood", "tokens": [51488, 534, 360, 300, 498, 291, 458, 437, 291, 434, 1629, 466, 13, 400, 370, 498, 291, 747, 257, 6674, 22119, 51712], "temperature": 0.0, "avg_logprob": -0.048819690510846565, "compression_ratio": 1.8810289389067525, "no_speech_prob": 0.10657977312803268}, {"id": 779, "seek": 431412, "start": 4314.2, "end": 4318.5199999999995, "text": " approach, if you work based on point estimates and you have no measure of your uncertainty,", "tokens": [50368, 3109, 11, 498, 291, 589, 2361, 322, 935, 20561, 293, 291, 362, 572, 3481, 295, 428, 15697, 11, 50584], "temperature": 0.0, "avg_logprob": -0.07985186576843262, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.010634364560246468}, {"id": 780, "seek": 431412, "start": 4319.32, "end": 4323.5599999999995, "text": " then there's no way you can possibly know what you're uncertain about to be able to resolve", "tokens": [50624, 550, 456, 311, 572, 636, 291, 393, 6264, 458, 437, 291, 434, 11308, 466, 281, 312, 1075, 281, 14151, 50836], "temperature": 0.0, "avg_logprob": -0.07985186576843262, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.010634364560246468}, {"id": 781, "seek": 431412, "start": 4323.5599999999995, "end": 4330.599999999999, "text": " that uncertainty. So this brings me on to causality. We know that predictive systems,", "tokens": [50836, 300, 15697, 13, 407, 341, 5607, 385, 322, 281, 3302, 1860, 13, 492, 458, 300, 35521, 3652, 11, 51188], "temperature": 0.0, "avg_logprob": -0.07985186576843262, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.010634364560246468}, {"id": 782, "seek": 431412, "start": 4330.599999999999, "end": 4336.12, "text": " which are aware of causal relationships, work better. But if we just bring it back to physics", "tokens": [51188, 597, 366, 3650, 295, 38755, 6159, 11, 589, 1101, 13, 583, 498, 321, 445, 1565, 309, 646, 281, 10649, 51464], "temperature": 0.0, "avg_logprob": -0.07985186576843262, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.010634364560246468}, {"id": 783, "seek": 433612, "start": 4336.12, "end": 4341.08, "text": " first, I mean, to you, what do you think causality is?", "tokens": [50364, 700, 11, 286, 914, 11, 281, 291, 11, 437, 360, 291, 519, 3302, 1860, 307, 30, 50612], "temperature": 0.0, "avg_logprob": -0.12045108440310456, "compression_ratio": 1.6782178217821782, "no_speech_prob": 0.23583070933818817}, {"id": 784, "seek": 433612, "start": 4342.2, "end": 4347.16, "text": " It is a tricky issue as to what causality is. And I think whether it exists or not is really a", "tokens": [50668, 467, 307, 257, 12414, 2734, 382, 281, 437, 3302, 1860, 307, 13, 400, 286, 519, 1968, 309, 8198, 420, 406, 307, 534, 257, 50916], "temperature": 0.0, "avg_logprob": -0.12045108440310456, "compression_ratio": 1.6782178217821782, "no_speech_prob": 0.23583070933818817}, {"id": 785, "seek": 433612, "start": 4347.16, "end": 4354.84, "text": " matter of how you define it, isn't it? And some would define that purely in terms of conditional", "tokens": [50916, 1871, 295, 577, 291, 6964, 309, 11, 1943, 380, 309, 30, 400, 512, 576, 6964, 300, 17491, 294, 2115, 295, 27708, 51300], "temperature": 0.0, "avg_logprob": -0.12045108440310456, "compression_ratio": 1.6782178217821782, "no_speech_prob": 0.23583070933818817}, {"id": 786, "seek": 433612, "start": 4354.84, "end": 4361.32, "text": " dependencies, that the behavior of one thing is conditionally dependent upon something else,", "tokens": [51300, 36606, 11, 300, 264, 5223, 295, 472, 551, 307, 4188, 379, 12334, 3564, 746, 1646, 11, 51624], "temperature": 0.0, "avg_logprob": -0.12045108440310456, "compression_ratio": 1.6782178217821782, "no_speech_prob": 0.23583070933818817}, {"id": 787, "seek": 436132, "start": 4361.4, "end": 4366.04, "text": " and therefore you could say that the one thing causes the other. But as we know from Bayes'", "tokens": [50368, 293, 4412, 291, 727, 584, 300, 264, 472, 551, 7700, 264, 661, 13, 583, 382, 321, 458, 490, 7840, 279, 6, 50600], "temperature": 0.0, "avg_logprob": -0.06582264298374213, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0040547107346355915}, {"id": 788, "seek": 436132, "start": 4366.04, "end": 4370.599999999999, "text": " theorem, that's not quite good enough, because you can swap any conditional relationship around", "tokens": [50600, 20904, 11, 300, 311, 406, 1596, 665, 1547, 11, 570, 291, 393, 18135, 604, 27708, 2480, 926, 50828], "temperature": 0.0, "avg_logprob": -0.06582264298374213, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0040547107346355915}, {"id": 789, "seek": 436132, "start": 4372.44, "end": 4379.0, "text": " through that process of inverting your model. Sometimes that causality is written into the", "tokens": [50920, 807, 300, 1399, 295, 28653, 783, 428, 2316, 13, 4803, 300, 3302, 1860, 307, 3720, 666, 264, 51248], "temperature": 0.0, "avg_logprob": -0.06582264298374213, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0040547107346355915}, {"id": 790, "seek": 436132, "start": 4379.0, "end": 4385.48, "text": " dynamics of a model. So this would be the approach used in things like dynamic causal", "tokens": [51248, 15679, 295, 257, 2316, 13, 407, 341, 576, 312, 264, 3109, 1143, 294, 721, 411, 8546, 38755, 51572], "temperature": 0.0, "avg_logprob": -0.06582264298374213, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0040547107346355915}, {"id": 791, "seek": 436132, "start": 4385.48, "end": 4390.5199999999995, "text": " modeling of brain data, where you might say that the current neural activity in one area of the", "tokens": [51572, 15983, 295, 3567, 1412, 11, 689, 291, 1062, 584, 300, 264, 2190, 18161, 5191, 294, 472, 1859, 295, 264, 51824], "temperature": 0.0, "avg_logprob": -0.06582264298374213, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0040547107346355915}, {"id": 792, "seek": 439052, "start": 4390.52, "end": 4395.160000000001, "text": " brain affects maybe the rate of change of neural activity in another part of the brain.", "tokens": [50364, 3567, 11807, 1310, 264, 3314, 295, 1319, 295, 18161, 5191, 294, 1071, 644, 295, 264, 3567, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09448499789183167, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013601023238152266}, {"id": 793, "seek": 439052, "start": 4395.8, "end": 4399.4800000000005, "text": " And it's the way in which those dynamics are written in, the fact that it's one affects the", "tokens": [50628, 400, 309, 311, 264, 636, 294, 597, 729, 15679, 366, 3720, 294, 11, 264, 1186, 300, 309, 311, 472, 11807, 264, 50812], "temperature": 0.0, "avg_logprob": -0.09448499789183167, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013601023238152266}, {"id": 794, "seek": 439052, "start": 4399.4800000000005, "end": 4407.400000000001, "text": " rate of change of the other, that gives it that causal flavor and a very directed perspective on", "tokens": [50812, 3314, 295, 1319, 295, 264, 661, 11, 300, 2709, 309, 300, 38755, 6813, 293, 257, 588, 12898, 4585, 322, 51208], "temperature": 0.0, "avg_logprob": -0.09448499789183167, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013601023238152266}, {"id": 795, "seek": 439052, "start": 4407.400000000001, "end": 4415.4800000000005, "text": " it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and", "tokens": [51208, 309, 13, 9210, 264, 589, 300, 307, 881, 13914, 322, 341, 307, 1237, 412, 561, 411, 36521, 64, 24639, 293, 51612], "temperature": 0.0, "avg_logprob": -0.09448499789183167, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013601023238152266}, {"id": 796, "seek": 441548, "start": 4415.5599999999995, "end": 4421.799999999999, "text": " a lot of his work on causality. There's a lot of detail about the notion of an intervention.", "tokens": [50368, 257, 688, 295, 702, 589, 322, 3302, 1860, 13, 821, 311, 257, 688, 295, 2607, 466, 264, 10710, 295, 364, 13176, 13, 50680], "temperature": 0.0, "avg_logprob": -0.05870370864868164, "compression_ratio": 1.72, "no_speech_prob": 0.05676402524113655}, {"id": 797, "seek": 441548, "start": 4422.36, "end": 4427.719999999999, "text": " And I suppose you can think of this in terms of how you might establish causation in a clinical", "tokens": [50708, 400, 286, 7297, 291, 393, 519, 295, 341, 294, 2115, 295, 577, 291, 1062, 8327, 3302, 399, 294, 257, 9115, 50976], "temperature": 0.0, "avg_logprob": -0.05870370864868164, "compression_ratio": 1.72, "no_speech_prob": 0.05676402524113655}, {"id": 798, "seek": 441548, "start": 4427.719999999999, "end": 4433.0, "text": " context. If you were to run a trial to try and establish whether one thing's caused another,", "tokens": [50976, 4319, 13, 759, 291, 645, 281, 1190, 257, 7308, 281, 853, 293, 8327, 1968, 472, 551, 311, 7008, 1071, 11, 51240], "temperature": 0.0, "avg_logprob": -0.05870370864868164, "compression_ratio": 1.72, "no_speech_prob": 0.05676402524113655}, {"id": 799, "seek": 441548, "start": 4433.0, "end": 4437.4, "text": " you need to make sure you're not inadvertently capturing a correlation or a conditional dependence", "tokens": [51240, 291, 643, 281, 652, 988, 291, 434, 406, 49152, 2276, 23384, 257, 20009, 420, 257, 27708, 31704, 51460], "temperature": 0.0, "avg_logprob": -0.05870370864868164, "compression_ratio": 1.72, "no_speech_prob": 0.05676402524113655}, {"id": 800, "seek": 441548, "start": 4437.4, "end": 4443.0, "text": " that could go either way, or a common cause of both things that depends upon something else.", "tokens": [51460, 300, 727, 352, 2139, 636, 11, 420, 257, 2689, 3082, 295, 1293, 721, 300, 5946, 3564, 746, 1646, 13, 51740], "temperature": 0.0, "avg_logprob": -0.05870370864868164, "compression_ratio": 1.72, "no_speech_prob": 0.05676402524113655}, {"id": 801, "seek": 444300, "start": 4443.88, "end": 4449.8, "text": " And typically the way you do that is you intervene on the system. You randomize at the beginning", "tokens": [50408, 400, 5850, 264, 636, 291, 360, 300, 307, 291, 30407, 322, 264, 1185, 13, 509, 4974, 1125, 412, 264, 2863, 50704], "temperature": 0.0, "avg_logprob": -0.1233440027004335, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016012463020160794}, {"id": 802, "seek": 444300, "start": 4449.8, "end": 4454.04, "text": " to make sure that people are assigned to different treatment groups at random,", "tokens": [50704, 281, 652, 988, 300, 561, 366, 13279, 281, 819, 5032, 3935, 412, 4974, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1233440027004335, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016012463020160794}, {"id": 803, "seek": 444300, "start": 4454.04, "end": 4460.2, "text": " so that you break that dependency upon something prior to it. And then anything that happens going", "tokens": [50916, 370, 300, 291, 1821, 300, 33621, 3564, 746, 4059, 281, 309, 13, 400, 550, 1340, 300, 2314, 516, 51224], "temperature": 0.0, "avg_logprob": -0.1233440027004335, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016012463020160794}, {"id": 804, "seek": 444300, "start": 4460.2, "end": 4467.88, "text": " forward is going to depend on the intervention that you're doing. So I think that's probably the", "tokens": [51224, 2128, 307, 516, 281, 5672, 322, 264, 13176, 300, 291, 434, 884, 13, 407, 286, 519, 300, 311, 1391, 264, 51608], "temperature": 0.0, "avg_logprob": -0.1233440027004335, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0016012463020160794}, {"id": 805, "seek": 446788, "start": 4467.96, "end": 4473.72, "text": " key thing that gives you causality or perhaps defines causality. It's the idea that an intervention", "tokens": [50368, 2141, 551, 300, 2709, 291, 3302, 1860, 420, 4317, 23122, 3302, 1860, 13, 467, 311, 264, 1558, 300, 364, 13176, 50656], "temperature": 0.0, "avg_logprob": -0.10176512452422595, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00426887720823288}, {"id": 806, "seek": 446788, "start": 4473.72, "end": 4478.04, "text": " is what will change it. If you intervene in one thing, that should then in a way that doesn't", "tokens": [50656, 307, 437, 486, 1319, 309, 13, 759, 291, 30407, 294, 472, 551, 11, 300, 820, 550, 294, 257, 636, 300, 1177, 380, 50872], "temperature": 0.0, "avg_logprob": -0.10176512452422595, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00426887720823288}, {"id": 807, "seek": 446788, "start": 4478.04, "end": 4483.24, "text": " necessarily match its natural distribution if you hadn't intervened at all, and then see what the", "tokens": [50872, 4725, 2995, 1080, 3303, 7316, 498, 291, 8782, 380, 17104, 292, 412, 439, 11, 293, 550, 536, 437, 264, 51132], "temperature": 0.0, "avg_logprob": -0.10176512452422595, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00426887720823288}, {"id": 808, "seek": 446788, "start": 4483.24, "end": 4490.52, "text": " effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study", "tokens": [51132, 1802, 307, 13, 1079, 11, 2086, 13, 286, 914, 11, 293, 538, 264, 636, 11, 36521, 64, 24639, 307, 534, 1880, 13, 286, 528, 281, 2979, 51496], "temperature": 0.0, "avg_logprob": -0.10176512452422595, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00426887720823288}, {"id": 809, "seek": 446788, "start": 4490.52, "end": 4496.52, "text": " his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.", "tokens": [51496, 702, 1446, 11, 440, 9476, 295, 1545, 13, 467, 311, 472, 551, 300, 321, 600, 534, 8119, 264, 2594, 322, 11, 767, 13, 51796], "temperature": 0.0, "avg_logprob": -0.10176512452422595, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00426887720823288}, {"id": 810, "seek": 449652, "start": 4496.52, "end": 4503.160000000001, "text": " But I suppose one way to think about it is if you go back to the core physical, in physics,", "tokens": [50364, 583, 286, 7297, 472, 636, 281, 519, 466, 309, 307, 498, 291, 352, 646, 281, 264, 4965, 4001, 11, 294, 10649, 11, 50696], "temperature": 0.0, "avg_logprob": -0.10756879017270844, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.001407635398209095}, {"id": 811, "seek": 449652, "start": 4503.160000000001, "end": 4507.240000000001, "text": " there's a whole bunch of equations to describe the world we live in. And those equations don't", "tokens": [50696, 456, 311, 257, 1379, 3840, 295, 11787, 281, 6786, 264, 1002, 321, 1621, 294, 13, 400, 729, 11787, 500, 380, 50900], "temperature": 0.0, "avg_logprob": -0.10756879017270844, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.001407635398209095}, {"id": 812, "seek": 449652, "start": 4507.240000000001, "end": 4512.76, "text": " have, they don't say anything about causality, and they're even reversible. And then you can think,", "tokens": [50900, 362, 11, 436, 500, 380, 584, 1340, 466, 3302, 1860, 11, 293, 436, 434, 754, 44788, 13, 400, 550, 291, 393, 519, 11, 51176], "temperature": 0.0, "avg_logprob": -0.10756879017270844, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.001407635398209095}, {"id": 813, "seek": 449652, "start": 4512.76, "end": 4517.0, "text": " okay, well, maybe it's a little bit like the free energy principle. It's a lens,", "tokens": [51176, 1392, 11, 731, 11, 1310, 309, 311, 257, 707, 857, 411, 264, 1737, 2281, 8665, 13, 467, 311, 257, 6765, 11, 51388], "temperature": 0.0, "avg_logprob": -0.10756879017270844, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.001407635398209095}, {"id": 814, "seek": 449652, "start": 4517.0, "end": 4521.56, "text": " like really, there's only dynamics. But when you look at these dynamical systems,", "tokens": [51388, 411, 534, 11, 456, 311, 787, 15679, 13, 583, 562, 291, 574, 412, 613, 5999, 804, 3652, 11, 51616], "temperature": 0.0, "avg_logprob": -0.10756879017270844, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.001407635398209095}, {"id": 815, "seek": 452156, "start": 4521.56, "end": 4528.6, "text": " then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,", "tokens": [50364, 550, 15501, 21511, 11, 293, 4079, 493, 300, 5021, 11, 291, 393, 584, 11, 1392, 11, 586, 321, 600, 658, 3302, 1860, 11, 50716], "temperature": 0.0, "avg_logprob": -0.09908885955810547, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0008001651149243116}, {"id": 816, "seek": 452156, "start": 4528.6, "end": 4534.360000000001, "text": " and it's something which is statistically efficacious to build it into our models. But", "tokens": [50716, 293, 309, 311, 746, 597, 307, 36478, 4703, 22641, 281, 1322, 309, 666, 527, 5245, 13, 583, 51004], "temperature": 0.0, "avg_logprob": -0.09908885955810547, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0008001651149243116}, {"id": 817, "seek": 452156, "start": 4536.4400000000005, "end": 4542.4400000000005, "text": " where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular", "tokens": [51108, 689, 775, 309, 808, 490, 30, 1042, 11, 309, 1487, 490, 505, 11, 1177, 380, 309, 30, 467, 311, 257, 17291, 281, 2903, 257, 1729, 51408], "temperature": 0.0, "avg_logprob": -0.09908885955810547, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0008001651149243116}, {"id": 818, "seek": 452156, "start": 4542.4400000000005, "end": 4551.080000000001, "text": " pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern", "tokens": [51408, 5102, 295, 8546, 13, 1079, 13, 400, 321, 1062, 13596, 3302, 399, 2361, 3564, 11, 797, 11, 257, 1729, 5102, 51840], "temperature": 0.0, "avg_logprob": -0.09908885955810547, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0008001651149243116}, {"id": 819, "seek": 455108, "start": 4551.08, "end": 4555.88, "text": " of how one thing reacts to another. So if you imagine you've got the classic physics example,", "tokens": [50364, 295, 577, 472, 551, 33305, 281, 1071, 13, 407, 498, 291, 3811, 291, 600, 658, 264, 7230, 10649, 1365, 11, 50604], "temperature": 0.0, "avg_logprob": -0.06382348236528415, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0023776497691869736}, {"id": 820, "seek": 455108, "start": 4555.88, "end": 4563.16, "text": " billiard balls bouncing into one another, how do you know that the collision of one ball with", "tokens": [50604, 2961, 72, 515, 9803, 27380, 666, 472, 1071, 11, 577, 360, 291, 458, 300, 264, 24644, 295, 472, 2594, 365, 50968], "temperature": 0.0, "avg_logprob": -0.06382348236528415, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0023776497691869736}, {"id": 821, "seek": 455108, "start": 4563.16, "end": 4570.28, "text": " another is causative of the subsequent motion of the second ball? And you could argue that that's", "tokens": [50968, 1071, 307, 3302, 1166, 295, 264, 19962, 5394, 295, 264, 1150, 2594, 30, 400, 291, 727, 9695, 300, 300, 311, 51324], "temperature": 0.0, "avg_logprob": -0.06382348236528415, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0023776497691869736}, {"id": 822, "seek": 455108, "start": 4570.28, "end": 4575.16, "text": " due to a particular pattern of which variables affect which other variables and the particular", "tokens": [51324, 3462, 281, 257, 1729, 5102, 295, 597, 9102, 3345, 597, 661, 9102, 293, 264, 1729, 51568], "temperature": 0.0, "avg_logprob": -0.06382348236528415, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0023776497691869736}, {"id": 823, "seek": 455108, "start": 4575.16, "end": 4580.28, "text": " exchange between them. And this comes back quite nicely to things like the physics perspective", "tokens": [51568, 7742, 1296, 552, 13, 400, 341, 1487, 646, 1596, 9594, 281, 721, 411, 264, 10649, 4585, 51824], "temperature": 0.0, "avg_logprob": -0.06382348236528415, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0023776497691869736}, {"id": 824, "seek": 458028, "start": 4580.679999999999, "end": 4586.44, "text": " on the free energy principle, the idea that actually one could see the location of a particular", "tokens": [50384, 322, 264, 1737, 2281, 8665, 11, 264, 1558, 300, 767, 472, 727, 536, 264, 4914, 295, 257, 1729, 50672], "temperature": 0.0, "avg_logprob": -0.11352675755818685, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.0016407539369538426}, {"id": 825, "seek": 458028, "start": 4586.44, "end": 4594.5199999999995, "text": " ball as being, you know, maybe it's internal state, and then the action that that then causes", "tokens": [50672, 2594, 382, 885, 11, 291, 458, 11, 1310, 309, 311, 6920, 1785, 11, 293, 550, 264, 3069, 300, 300, 550, 7700, 51076], "temperature": 0.0, "avg_logprob": -0.11352675755818685, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.0016407539369538426}, {"id": 826, "seek": 458028, "start": 4594.5199999999995, "end": 4601.0, "text": " is perhaps the, or in fact, you could say that the action is the position of the ball, the force", "tokens": [51076, 307, 4317, 264, 11, 420, 294, 1186, 11, 291, 727, 584, 300, 264, 3069, 307, 264, 2535, 295, 264, 2594, 11, 264, 3464, 51400], "temperature": 0.0, "avg_logprob": -0.11352675755818685, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.0016407539369538426}, {"id": 827, "seek": 458028, "start": 4601.0, "end": 4606.84, "text": " that results from that action is the sensory state of the next ball, which then changes its", "tokens": [51400, 300, 3542, 490, 300, 3069, 307, 264, 27233, 1785, 295, 264, 958, 2594, 11, 597, 550, 2962, 1080, 51692], "temperature": 0.0, "avg_logprob": -0.11352675755818685, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.0016407539369538426}, {"id": 828, "seek": 460684, "start": 4606.84, "end": 4613.96, "text": " velocity to then change its action relative to something else. You can sort of rearrange those", "tokens": [50364, 9269, 281, 550, 1319, 1080, 3069, 4972, 281, 746, 1646, 13, 509, 393, 1333, 295, 39568, 729, 50720], "temperature": 0.0, "avg_logprob": -0.058033934506503015, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0020270913373678923}, {"id": 829, "seek": 460684, "start": 4613.96, "end": 4619.8, "text": " labels slightly, but there is a directional element to it. And in that sort of pattern of", "tokens": [50720, 16949, 4748, 11, 457, 456, 307, 257, 42242, 4478, 281, 309, 13, 400, 294, 300, 1333, 295, 5102, 295, 51012], "temperature": 0.0, "avg_logprob": -0.058033934506503015, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0020270913373678923}, {"id": 830, "seek": 460684, "start": 4619.8, "end": 4625.24, "text": " causation, you really do expect the position of one ball to have an effect on the rate of change,", "tokens": [51012, 3302, 399, 11, 291, 534, 360, 2066, 264, 2535, 295, 472, 2594, 281, 362, 364, 1802, 322, 264, 3314, 295, 1319, 11, 51284], "temperature": 0.0, "avg_logprob": -0.058033934506503015, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0020270913373678923}, {"id": 831, "seek": 460684, "start": 4625.24, "end": 4629.72, "text": " or in fact, even the rate of rate of change of the second ball, which again, I think brings us", "tokens": [51284, 420, 294, 1186, 11, 754, 264, 3314, 295, 3314, 295, 1319, 295, 264, 1150, 2594, 11, 597, 797, 11, 286, 519, 5607, 505, 51508], "temperature": 0.0, "avg_logprob": -0.058033934506503015, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0020270913373678923}, {"id": 832, "seek": 460684, "start": 4629.72, "end": 4635.4800000000005, "text": " back to those kinds of dynamical descriptions of causality where one thing might affect how", "tokens": [51508, 646, 281, 729, 3685, 295, 5999, 804, 24406, 295, 3302, 1860, 689, 472, 551, 1062, 3345, 577, 51796], "temperature": 0.0, "avg_logprob": -0.058033934506503015, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0020270913373678923}, {"id": 833, "seek": 463548, "start": 4635.48, "end": 4641.16, "text": " another thing changes. So you almost get it from the dynamics itself. But again, to some extent,", "tokens": [50364, 1071, 551, 2962, 13, 407, 291, 1920, 483, 309, 490, 264, 15679, 2564, 13, 583, 797, 11, 281, 512, 8396, 11, 50648], "temperature": 0.0, "avg_logprob": -0.07839035193125407, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.001366900047287345}, {"id": 834, "seek": 463548, "start": 4641.16, "end": 4645.799999999999, "text": " it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose", "tokens": [50648, 309, 1487, 646, 281, 4361, 45298, 11, 1177, 380, 309, 30, 467, 1487, 646, 281, 437, 360, 321, 914, 538, 3082, 30, 1042, 11, 286, 7297, 50880], "temperature": 0.0, "avg_logprob": -0.07839035193125407, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.001366900047287345}, {"id": 835, "seek": 463548, "start": 4645.799999999999, "end": 4649.959999999999, "text": " cause is a hypothesis as to a particular configuration of things. But then you've got to", "tokens": [50880, 3082, 307, 257, 17291, 382, 281, 257, 1729, 11694, 295, 721, 13, 583, 550, 291, 600, 658, 281, 51088], "temperature": 0.0, "avg_logprob": -0.07839035193125407, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.001366900047287345}, {"id": 836, "seek": 463548, "start": 4649.959999999999, "end": 4654.839999999999, "text": " write down what does that hypothesis mean? What's my model of what a causation involves?", "tokens": [51088, 2464, 760, 437, 775, 300, 17291, 914, 30, 708, 311, 452, 2316, 295, 437, 257, 3302, 399, 11626, 30, 51332], "temperature": 0.0, "avg_logprob": -0.07839035193125407, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.001366900047287345}, {"id": 837, "seek": 463548, "start": 4655.5599999999995, "end": 4661.719999999999, "text": " Yes, yes. I mean, we were just talking about, you know, build building these models. And one of", "tokens": [51368, 1079, 11, 2086, 13, 286, 914, 11, 321, 645, 445, 1417, 466, 11, 291, 458, 11, 1322, 2390, 613, 5245, 13, 400, 472, 295, 51676], "temperature": 0.0, "avg_logprob": -0.07839035193125407, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.001366900047287345}, {"id": 838, "seek": 466172, "start": 4661.72, "end": 4668.52, "text": " the bright differences from machine learning is that we need to build a generative model by hand.", "tokens": [50364, 264, 4730, 7300, 490, 3479, 2539, 307, 300, 321, 643, 281, 1322, 257, 1337, 1166, 2316, 538, 1011, 13, 50704], "temperature": 0.0, "avg_logprob": -0.09429013598096239, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0033742543309926987}, {"id": 839, "seek": 466172, "start": 4669.56, "end": 4674.52, "text": " So we have to define these these variables, and some of them are presumably observed, and some of", "tokens": [50756, 407, 321, 362, 281, 6964, 613, 613, 9102, 11, 293, 512, 295, 552, 366, 26742, 13095, 11, 293, 512, 295, 51004], "temperature": 0.0, "avg_logprob": -0.09429013598096239, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0033742543309926987}, {"id": 840, "seek": 466172, "start": 4674.52, "end": 4683.320000000001, "text": " them are not observed. They're inferred. And that process seems like you would need to have a lot", "tokens": [51004, 552, 366, 406, 13095, 13, 814, 434, 13596, 986, 13, 400, 300, 1399, 2544, 411, 291, 576, 643, 281, 362, 257, 688, 51444], "temperature": 0.0, "avg_logprob": -0.09429013598096239, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0033742543309926987}, {"id": 841, "seek": 466172, "start": 4683.320000000001, "end": 4690.4400000000005, "text": " of domain expertise. And it seems like something which is at least has a degree of subjectivity.", "tokens": [51444, 295, 9274, 11769, 13, 400, 309, 2544, 411, 746, 597, 307, 412, 1935, 575, 257, 4314, 295, 3983, 4253, 13, 51800], "temperature": 0.0, "avg_logprob": -0.09429013598096239, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0033742543309926987}, {"id": 842, "seek": 469044, "start": 4690.44, "end": 4694.759999999999, "text": " I mean, we were just talking about causality, for example, there are many ways you could model", "tokens": [50364, 286, 914, 11, 321, 645, 445, 1417, 466, 3302, 1860, 11, 337, 1365, 11, 456, 366, 867, 2098, 291, 727, 2316, 50580], "temperature": 0.0, "avg_logprob": -0.06426578379691915, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0013096813345327973}, {"id": 843, "seek": 469044, "start": 4695.32, "end": 4701.16, "text": " the risk of cancer from smoking. It seems like there are many, many different ways of building", "tokens": [50608, 264, 3148, 295, 5592, 490, 14055, 13, 467, 2544, 411, 456, 366, 867, 11, 867, 819, 2098, 295, 2390, 50900], "temperature": 0.0, "avg_logprob": -0.06426578379691915, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0013096813345327973}, {"id": 844, "seek": 469044, "start": 4701.16, "end": 4707.96, "text": " those models. So that subjectivity is interesting. I mean, are there principled ways of building", "tokens": [50900, 729, 5245, 13, 407, 300, 3983, 4253, 307, 1880, 13, 286, 914, 11, 366, 456, 3681, 15551, 2098, 295, 2390, 51240], "temperature": 0.0, "avg_logprob": -0.06426578379691915, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0013096813345327973}, {"id": 845, "seek": 469044, "start": 4707.96, "end": 4713.799999999999, "text": " these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to", "tokens": [51240, 613, 5245, 30, 1079, 13, 400, 294, 257, 2020, 11, 309, 439, 1487, 646, 281, 264, 912, 551, 797, 11, 309, 1487, 646, 281, 51532], "temperature": 0.0, "avg_logprob": -0.06426578379691915, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.0013096813345327973}, {"id": 846, "seek": 471380, "start": 4713.8, "end": 4722.28, "text": " which model minimizes the surprise the best. And but there are interesting questions amongst that.", "tokens": [50364, 597, 2316, 4464, 5660, 264, 6365, 264, 1151, 13, 400, 457, 456, 366, 1880, 1651, 12918, 300, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11871565219967864, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.07505395263433456}, {"id": 847, "seek": 471380, "start": 4722.28, "end": 4728.84, "text": " So how do you actually choose the space of models that you want to compare? So you're right to say", "tokens": [50788, 407, 577, 360, 291, 767, 2826, 264, 1901, 295, 5245, 300, 291, 528, 281, 6794, 30, 407, 291, 434, 558, 281, 584, 51116], "temperature": 0.0, "avg_logprob": -0.11871565219967864, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.07505395263433456}, {"id": 848, "seek": 471380, "start": 4728.84, "end": 4734.52, "text": " that that that often there is some specific prior information that's put into models and active", "tokens": [51116, 300, 300, 300, 2049, 456, 307, 512, 2685, 4059, 1589, 300, 311, 829, 666, 5245, 293, 4967, 51400], "temperature": 0.0, "avg_logprob": -0.11871565219967864, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.07505395263433456}, {"id": 849, "seek": 471380, "start": 4734.52, "end": 4739.0, "text": " inference. And very often we do end up sort of building models by hand to demonstrate a specific", "tokens": [51400, 38253, 13, 400, 588, 2049, 321, 360, 917, 493, 1333, 295, 2390, 5245, 538, 1011, 281, 11698, 257, 2685, 51624], "temperature": 0.0, "avg_logprob": -0.11871565219967864, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.07505395263433456}, {"id": 850, "seek": 473900, "start": 4739.72, "end": 4744.76, "text": " outcome or a specific cognitive function. But there's no reason why it has to be that way.", "tokens": [50400, 9700, 420, 257, 2685, 15605, 2445, 13, 583, 456, 311, 572, 1778, 983, 309, 575, 281, 312, 300, 636, 13, 50652], "temperature": 0.0, "avg_logprob": -0.0647546307424481, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.013433098793029785}, {"id": 851, "seek": 473900, "start": 4744.76, "end": 4754.04, "text": " You can build models through exposure to data, where where the models are selecting the data to", "tokens": [50652, 509, 393, 1322, 5245, 807, 10420, 281, 1412, 11, 689, 689, 264, 5245, 366, 18182, 264, 1412, 281, 51116], "temperature": 0.0, "avg_logprob": -0.0647546307424481, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.013433098793029785}, {"id": 852, "seek": 473900, "start": 4754.04, "end": 4759.24, "text": " best build themselves. But the question is how you do that, how you start to add on additional", "tokens": [51116, 1151, 1322, 2969, 13, 583, 264, 1168, 307, 577, 291, 360, 300, 11, 577, 291, 722, 281, 909, 322, 4497, 51376], "temperature": 0.0, "avg_logprob": -0.0647546307424481, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.013433098793029785}, {"id": 853, "seek": 473900, "start": 4759.24, "end": 4764.2, "text": " things, how you start to change the structure of your model. But there's a lot of ongoing research", "tokens": [51376, 721, 11, 577, 291, 722, 281, 1319, 264, 3877, 295, 428, 2316, 13, 583, 456, 311, 257, 688, 295, 10452, 2132, 51624], "temperature": 0.0, "avg_logprob": -0.0647546307424481, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.013433098793029785}, {"id": 854, "seek": 476420, "start": 4764.2, "end": 4769.0, "text": " into that. And I think there are now methods that are coming out that will allow you to allow an", "tokens": [50364, 666, 300, 13, 400, 286, 519, 456, 366, 586, 7150, 300, 366, 1348, 484, 300, 486, 2089, 291, 281, 2089, 364, 50604], "temperature": 0.0, "avg_logprob": -0.12220087169129172, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.01320446003228426}, {"id": 855, "seek": 476420, "start": 4769.0, "end": 4774.5199999999995, "text": " active inference model to build itself. And the way it will do that will be sort of adding on", "tokens": [50604, 4967, 38253, 2316, 281, 1322, 2564, 13, 400, 264, 636, 309, 486, 360, 300, 486, 312, 1333, 295, 5127, 322, 50880], "temperature": 0.0, "avg_logprob": -0.12220087169129172, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.01320446003228426}, {"id": 856, "seek": 476420, "start": 4774.5199999999995, "end": 4781.24, "text": " additional states and potential causes, adjusting beliefs about the mappings and the distributions", "tokens": [50880, 4497, 4368, 293, 3995, 7700, 11, 23559, 13585, 466, 264, 463, 28968, 293, 264, 37870, 51216], "temperature": 0.0, "avg_logprob": -0.12220087169129172, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.01320446003228426}, {"id": 857, "seek": 476420, "start": 4781.24, "end": 4789.4, "text": " and the parameters of given this than that, adding an additional paths that different", "tokens": [51216, 293, 264, 9834, 295, 2212, 341, 813, 300, 11, 5127, 364, 4497, 14518, 300, 819, 51624], "temperature": 0.0, "avg_logprob": -0.12220087169129172, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.01320446003228426}, {"id": 858, "seek": 478940, "start": 4790.12, "end": 4798.04, "text": " or different transitions that systems will pursue. So it's a fascinating area. I think", "tokens": [50400, 420, 819, 23767, 300, 3652, 486, 12392, 13, 407, 309, 311, 257, 10343, 1859, 13, 286, 519, 50796], "temperature": 0.0, "avg_logprob": -0.13198478801830396, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.004122802522033453}, {"id": 859, "seek": 478940, "start": 4798.04, "end": 4803.879999999999, "text": " it's one that's still a growing area. But it's this idea of structure learning of comparing", "tokens": [50796, 309, 311, 472, 300, 311, 920, 257, 4194, 1859, 13, 583, 309, 311, 341, 1558, 295, 3877, 2539, 295, 15763, 51088], "temperature": 0.0, "avg_logprob": -0.13198478801830396, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.004122802522033453}, {"id": 860, "seek": 478940, "start": 4803.879999999999, "end": 4808.839999999999, "text": " each alternative model based upon its free energy or model evidence or surprise as a way of", "tokens": [51088, 1184, 8535, 2316, 2361, 3564, 1080, 1737, 2281, 420, 2316, 4467, 420, 6365, 382, 257, 636, 295, 51336], "temperature": 0.0, "avg_logprob": -0.13198478801830396, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.004122802522033453}, {"id": 861, "seek": 478940, "start": 4810.5199999999995, "end": 4813.08, "text": " minimizing that by being able to better predict things.", "tokens": [51420, 46608, 300, 538, 885, 1075, 281, 1101, 6069, 721, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13198478801830396, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.004122802522033453}, {"id": 862, "seek": 481308, "start": 4813.96, "end": 4822.12, "text": " Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,", "tokens": [50408, 865, 11, 286, 914, 11, 300, 311, 746, 300, 321, 6255, 11, 321, 1643, 281, 360, 534, 731, 13, 407, 321, 393, 11, 700, 295, 439, 11, 50816], "temperature": 0.0, "avg_logprob": -0.18809560139973958, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.02082691341638565}, {"id": 863, "seek": 481308, "start": 4822.12, "end": 4830.76, "text": " via abduction, we can select relevant models to explain behavior, you know, what we observe.", "tokens": [50816, 5766, 410, 40335, 11, 321, 393, 3048, 7340, 5245, 281, 2903, 5223, 11, 291, 458, 11, 437, 321, 11441, 13, 51248], "temperature": 0.0, "avg_logprob": -0.18809560139973958, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.02082691341638565}, {"id": 864, "seek": 481308, "start": 4830.76, "end": 4836.12, "text": " But we also have the ability to create models. In fact, I think of intelligence as the ability", "tokens": [51248, 583, 321, 611, 362, 264, 3485, 281, 1884, 5245, 13, 682, 1186, 11, 286, 519, 295, 7599, 382, 264, 3485, 51516], "temperature": 0.0, "avg_logprob": -0.18809560139973958, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.02082691341638565}, {"id": 865, "seek": 483612, "start": 4836.12, "end": 4842.76, "text": " to create models. So we experience something. And I now construct a model to explain this", "tokens": [50364, 281, 1884, 5245, 13, 407, 321, 1752, 746, 13, 400, 286, 586, 7690, 257, 2316, 281, 2903, 341, 50696], "temperature": 0.0, "avg_logprob": -0.09910542790482683, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.14277179539203644}, {"id": 866, "seek": 483612, "start": 4842.76, "end": 4851.24, "text": " and similar experiences in experience space. But in a machine, it's really difficult. So in", "tokens": [50696, 293, 2531, 5235, 294, 1752, 1901, 13, 583, 294, 257, 3479, 11, 309, 311, 534, 2252, 13, 407, 294, 51120], "temperature": 0.0, "avg_logprob": -0.09910542790482683, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.14277179539203644}, {"id": 867, "seek": 483612, "start": 4851.24, "end": 4856.28, "text": " machine learning, there's this bias variance trade off. So we deliberately reduce the size", "tokens": [51120, 3479, 2539, 11, 456, 311, 341, 12577, 21977, 4923, 766, 13, 407, 321, 23506, 5407, 264, 2744, 51372], "temperature": 0.0, "avg_logprob": -0.09910542790482683, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.14277179539203644}, {"id": 868, "seek": 483612, "start": 4856.28, "end": 4862.2, "text": " of the approximation space to make it computationally tractable. And when we're talking about", "tokens": [51372, 295, 264, 28023, 1901, 281, 652, 309, 24903, 379, 24207, 712, 13, 400, 562, 321, 434, 1417, 466, 51668], "temperature": 0.0, "avg_logprob": -0.09910542790482683, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.14277179539203644}, {"id": 869, "seek": 486220, "start": 4862.2, "end": 4867.72, "text": " building these models, just from observational data, it feels like there's an exponential", "tokens": [50364, 2390, 613, 5245, 11, 445, 490, 9951, 1478, 1412, 11, 309, 3417, 411, 456, 311, 364, 21510, 50640], "temperature": 0.0, "avg_logprob": -0.07446659261530096, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.010339781641960144}, {"id": 870, "seek": 486220, "start": 4867.72, "end": 4872.44, "text": " blow up of possible models. So I can imagine there might be a whole bunch of heuristics around", "tokens": [50640, 6327, 493, 295, 1944, 5245, 13, 407, 286, 393, 3811, 456, 1062, 312, 257, 1379, 3840, 295, 415, 374, 6006, 926, 50876], "temperature": 0.0, "avg_logprob": -0.07446659261530096, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.010339781641960144}, {"id": 871, "seek": 486220, "start": 4872.44, "end": 4877.5599999999995, "text": " library learning or having modules. So these modules have worked well over there. So we'll", "tokens": [50876, 6405, 2539, 420, 1419, 16679, 13, 407, 613, 16679, 362, 2732, 731, 670, 456, 13, 407, 321, 603, 51132], "temperature": 0.0, "avg_logprob": -0.07446659261530096, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.010339781641960144}, {"id": 872, "seek": 486220, "start": 4877.5599999999995, "end": 4882.5199999999995, "text": " try composing together known modules rather than starting from scratch every single time. I mean,", "tokens": [51132, 853, 715, 6110, 1214, 2570, 16679, 2831, 813, 2891, 490, 8459, 633, 2167, 565, 13, 286, 914, 11, 51380], "temperature": 0.0, "avg_logprob": -0.07446659261530096, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.010339781641960144}, {"id": 873, "seek": 486220, "start": 4882.5199999999995, "end": 4886.679999999999, "text": " what kind of work is being done there? I mean, I think I think you're right about, you know,", "tokens": [51380, 437, 733, 295, 589, 307, 885, 1096, 456, 30, 286, 914, 11, 286, 519, 286, 519, 291, 434, 558, 466, 11, 291, 458, 11, 51588], "temperature": 0.0, "avg_logprob": -0.07446659261530096, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.010339781641960144}, {"id": 874, "seek": 488668, "start": 4886.68, "end": 4893.72, "text": " it's not going to be worth starting from scratch every time. You can sort of build models by saying,", "tokens": [50364, 309, 311, 406, 516, 281, 312, 3163, 2891, 490, 8459, 633, 565, 13, 509, 393, 1333, 295, 1322, 5245, 538, 1566, 11, 50716], "temperature": 0.0, "avg_logprob": -0.08319300574225348, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.008988570421934128}, {"id": 875, "seek": 488668, "start": 4893.72, "end": 4898.84, "text": " okay, let's start with something very simple with a sort of known structure. And I think it's", "tokens": [50716, 1392, 11, 718, 311, 722, 365, 746, 588, 2199, 365, 257, 1333, 295, 2570, 3877, 13, 400, 286, 519, 309, 311, 50972], "temperature": 0.0, "avg_logprob": -0.08319300574225348, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.008988570421934128}, {"id": 876, "seek": 488668, "start": 4898.84, "end": 4904.280000000001, "text": " sensible to use some priors in that rather than starting from complete, completely nothing,", "tokens": [50972, 25380, 281, 764, 512, 1790, 830, 294, 300, 2831, 813, 2891, 490, 3566, 11, 2584, 1825, 11, 51244], "temperature": 0.0, "avg_logprob": -0.08319300574225348, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.008988570421934128}, {"id": 877, "seek": 488668, "start": 4904.280000000001, "end": 4907.72, "text": " because there are some things that we know about in the world. And there's no point hiding that", "tokens": [51244, 570, 456, 366, 512, 721, 300, 321, 458, 466, 294, 264, 1002, 13, 400, 456, 311, 572, 935, 10596, 300, 51416], "temperature": 0.0, "avg_logprob": -0.08319300574225348, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.008988570421934128}, {"id": 878, "seek": 488668, "start": 4907.72, "end": 4913.64, "text": " from the models we're trying to build. And that might be a simple structural thing like things", "tokens": [51416, 490, 264, 5245, 321, 434, 1382, 281, 1322, 13, 400, 300, 1062, 312, 257, 2199, 15067, 551, 411, 721, 51712], "temperature": 0.0, "avg_logprob": -0.08319300574225348, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.008988570421934128}, {"id": 879, "seek": 491364, "start": 4913.64, "end": 4917.4800000000005, "text": " evolve in time. So one thing is conditioned upon the next is conditioned upon the next.", "tokens": [50364, 16693, 294, 565, 13, 407, 472, 551, 307, 35833, 3564, 264, 958, 307, 35833, 3564, 264, 958, 13, 50556], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 880, "seek": 491364, "start": 4917.4800000000005, "end": 4922.12, "text": " And things now will influence the data I observe things well in the past might not anymore.", "tokens": [50556, 400, 721, 586, 486, 6503, 264, 1412, 286, 11441, 721, 731, 294, 264, 1791, 1062, 406, 3602, 13, 50788], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 881, "seek": 491364, "start": 4925.72, "end": 4930.52, "text": " But then then there's the question of, well, how can a model then grow? What are the things that", "tokens": [50968, 583, 550, 550, 456, 311, 264, 1168, 295, 11, 731, 11, 577, 393, 257, 2316, 550, 1852, 30, 708, 366, 264, 721, 300, 51208], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 882, "seek": 491364, "start": 4930.52, "end": 4935.8, "text": " you can add to it or subtract from it? And subtraction is another key element. Because you", "tokens": [51208, 291, 393, 909, 281, 309, 420, 16390, 490, 309, 30, 400, 16390, 313, 307, 1071, 2141, 4478, 13, 1436, 291, 51472], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 883, "seek": 491364, "start": 4935.8, "end": 4939.320000000001, "text": " could take this whole problem from the other direction, and you could say, well, let's start", "tokens": [51472, 727, 747, 341, 1379, 1154, 490, 264, 661, 3513, 11, 293, 291, 727, 584, 11, 731, 11, 718, 311, 722, 51648], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 884, "seek": 491364, "start": 4939.320000000001, "end": 4943.08, "text": " with a model that just has everything in it and take away bits until we've got the model that's", "tokens": [51648, 365, 257, 2316, 300, 445, 575, 1203, 294, 309, 293, 747, 1314, 9239, 1826, 321, 600, 658, 264, 2316, 300, 311, 51836], "temperature": 0.0, "avg_logprob": -0.10768537379022855, "compression_ratio": 1.8471760797342194, "no_speech_prob": 0.0009658921044319868}, {"id": 885, "seek": 494308, "start": 4943.08, "end": 4946.92, "text": " relevant to where we are at the moment. And we know that during development, there's a lot of", "tokens": [50364, 7340, 281, 689, 321, 366, 412, 264, 1623, 13, 400, 321, 458, 300, 1830, 3250, 11, 456, 311, 257, 688, 295, 50556], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 886, "seek": 494308, "start": 4946.92, "end": 4952.2, "text": " synaptic pruning that goes on and removal of synapses that we have when we're much younger", "tokens": [50556, 5451, 2796, 299, 582, 37726, 300, 1709, 322, 293, 17933, 295, 5451, 2382, 279, 300, 321, 362, 562, 321, 434, 709, 7037, 50820], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 887, "seek": 494308, "start": 4953.08, "end": 4959.72, "text": " compared to compared to as you get older. So what can you add on? Well, it depends what your model", "tokens": [50864, 5347, 281, 5347, 281, 382, 291, 483, 4906, 13, 407, 437, 393, 291, 909, 322, 30, 1042, 11, 309, 5946, 437, 428, 2316, 51196], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 888, "seek": 494308, "start": 4959.72, "end": 4963.4, "text": " looks like. So if your model says there's a set of states that can evolve over time, there are", "tokens": [51196, 1542, 411, 13, 407, 498, 428, 2316, 1619, 456, 311, 257, 992, 295, 4368, 300, 393, 16693, 670, 565, 11, 456, 366, 51380], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 889, "seek": 494308, "start": 4963.4, "end": 4966.92, "text": " a set of outcomes that are generated, well, we know what the outcomes are, we know what the", "tokens": [51380, 257, 992, 295, 10070, 300, 366, 10833, 11, 731, 11, 321, 458, 437, 264, 10070, 366, 11, 321, 458, 437, 264, 51556], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 890, "seek": 494308, "start": 4966.92, "end": 4972.68, "text": " data are, because we know what our sensory organs are. So it's the states that are going to change", "tokens": [51556, 1412, 366, 11, 570, 321, 458, 437, 527, 27233, 20659, 366, 13, 407, 309, 311, 264, 4368, 300, 366, 516, 281, 1319, 51844], "temperature": 0.0, "avg_logprob": -0.09174870782428318, "compression_ratio": 1.9419795221843004, "no_speech_prob": 0.0013846696820110083}, {"id": 891, "seek": 497308, "start": 4973.16, "end": 4980.28, "text": " so do we add in more states? Do we allow them to take more alternative values? Do we allow", "tokens": [50368, 370, 360, 321, 909, 294, 544, 4368, 30, 1144, 321, 2089, 552, 281, 747, 544, 8535, 4190, 30, 1144, 321, 2089, 50724], "temperature": 0.0, "avg_logprob": -0.07125847857931386, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0016309517668560147}, {"id": 892, "seek": 497308, "start": 4980.28, "end": 4985.96, "text": " their transitions to change in more than one different way? Which ones can I change? Which", "tokens": [50724, 641, 23767, 281, 1319, 294, 544, 813, 472, 819, 636, 30, 3013, 2306, 393, 286, 1319, 30, 3013, 51008], "temperature": 0.0, "avg_logprob": -0.07125847857931386, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0016309517668560147}, {"id": 893, "seek": 497308, "start": 4985.96, "end": 4991.8, "text": " ones can I not change? And it's really just asking these questions that helps you to grow your model.", "tokens": [51008, 2306, 393, 286, 406, 1319, 30, 400, 309, 311, 534, 445, 3365, 613, 1651, 300, 3665, 291, 281, 1852, 428, 2316, 13, 51300], "temperature": 0.0, "avg_logprob": -0.07125847857931386, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0016309517668560147}, {"id": 894, "seek": 497308, "start": 4991.8, "end": 4996.44, "text": " So you say, well, let's try it. If I allow this state to take additional values, if it's not", "tokens": [51300, 407, 291, 584, 11, 731, 11, 718, 311, 853, 309, 13, 759, 286, 2089, 341, 1785, 281, 747, 4497, 4190, 11, 498, 309, 311, 406, 51532], "temperature": 0.0, "avg_logprob": -0.07125847857931386, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0016309517668560147}, {"id": 895, "seek": 497308, "start": 4996.44, "end": 5002.5199999999995, "text": " providing a sufficiently good explanation for how things are at the moment. And if that improves", "tokens": [51532, 6530, 257, 31868, 665, 10835, 337, 577, 721, 366, 412, 264, 1623, 13, 400, 498, 300, 24771, 51836], "temperature": 0.0, "avg_logprob": -0.07125847857931386, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0016309517668560147}, {"id": 896, "seek": 500252, "start": 5002.52, "end": 5008.040000000001, "text": " your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need", "tokens": [50364, 428, 17630, 11, 300, 311, 665, 293, 291, 1066, 309, 293, 498, 309, 1177, 380, 11, 550, 291, 483, 3973, 295, 309, 13, 1144, 286, 586, 643, 50640], "temperature": 0.0, "avg_logprob": -0.11372966281438278, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0039583779871463776}, {"id": 897, "seek": 500252, "start": 5008.040000000001, "end": 5015.8, "text": " to include additional state factors? So you could either say there is one sort of state of the world", "tokens": [50640, 281, 4090, 4497, 1785, 6771, 30, 407, 291, 727, 2139, 584, 456, 307, 472, 1333, 295, 1785, 295, 264, 1002, 51028], "temperature": 0.0, "avg_logprob": -0.11372966281438278, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0039583779871463776}, {"id": 898, "seek": 500252, "start": 5015.8, "end": 5019.96, "text": " that can take multiple different values, or you could actually this is contextualized by something", "tokens": [51028, 300, 393, 747, 3866, 819, 4190, 11, 420, 291, 727, 767, 341, 307, 35526, 1602, 538, 746, 51236], "temperature": 0.0, "avg_logprob": -0.11372966281438278, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0039583779871463776}, {"id": 899, "seek": 500252, "start": 5019.96, "end": 5024.4400000000005, "text": " completely separate. So where am I along an x coordinate? You also need to know where you are", "tokens": [51236, 2584, 4994, 13, 407, 689, 669, 286, 2051, 364, 2031, 15670, 30, 509, 611, 643, 281, 458, 689, 291, 366, 51460], "temperature": 0.0, "avg_logprob": -0.11372966281438278, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0039583779871463776}, {"id": 900, "seek": 500252, "start": 5024.4400000000005, "end": 5031.320000000001, "text": " along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in", "tokens": [51460, 2051, 288, 15670, 281, 312, 1075, 281, 35526, 1125, 437, 291, 434, 32884, 13, 407, 309, 311, 445, 3365, 437, 307, 294, 51804], "temperature": 0.0, "avg_logprob": -0.11372966281438278, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0039583779871463776}, {"id": 901, "seek": 503132, "start": 5031.32, "end": 5035.5599999999995, "text": " a model? How do you build a model almost gives you the answers to the ways or the directions in", "tokens": [50364, 257, 2316, 30, 1012, 360, 291, 1322, 257, 2316, 1920, 2709, 291, 264, 6338, 281, 264, 2098, 420, 264, 11095, 294, 50576], "temperature": 0.0, "avg_logprob": -0.06086527255543491, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0082784378901124}, {"id": 902, "seek": 503132, "start": 5035.5599999999995, "end": 5041.4, "text": " which you can grow it. The other thing you can then do when you're trying to work out how to grow it", "tokens": [50576, 597, 291, 393, 1852, 309, 13, 440, 661, 551, 291, 393, 550, 360, 562, 291, 434, 1382, 281, 589, 484, 577, 281, 1852, 309, 50868], "temperature": 0.0, "avg_logprob": -0.06086527255543491, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0082784378901124}, {"id": 903, "seek": 503132, "start": 5041.4, "end": 5046.12, "text": " is to say, well, let's treat this as the same sort of problem as exploring my world,", "tokens": [50868, 307, 281, 584, 11, 731, 11, 718, 311, 2387, 341, 382, 264, 912, 1333, 295, 1154, 382, 12736, 452, 1002, 11, 51104], "temperature": 0.0, "avg_logprob": -0.06086527255543491, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0082784378901124}, {"id": 904, "seek": 503132, "start": 5046.12, "end": 5050.04, "text": " selecting actions that will then give me more information about the world. You could say,", "tokens": [51104, 18182, 5909, 300, 486, 550, 976, 385, 544, 1589, 466, 264, 1002, 13, 509, 727, 584, 11, 51300], "temperature": 0.0, "avg_logprob": -0.06086527255543491, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0082784378901124}, {"id": 905, "seek": 503132, "start": 5050.04, "end": 5054.36, "text": " well, actually, now let's treat my exploration of model space as being a similar process of", "tokens": [51300, 731, 11, 767, 11, 586, 718, 311, 2387, 452, 16197, 295, 2316, 1901, 382, 885, 257, 2531, 1399, 295, 51516], "temperature": 0.0, "avg_logprob": -0.06086527255543491, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0082784378901124}, {"id": 906, "seek": 505436, "start": 5054.36, "end": 5063.32, "text": " exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping", "tokens": [50364, 16197, 13, 3013, 295, 613, 1944, 18624, 281, 452, 2316, 1062, 1477, 281, 257, 1570, 39465, 18350, 50812], "temperature": 0.0, "avg_logprob": -0.09076719284057617, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.01066162995994091}, {"id": 907, "seek": 505436, "start": 5063.32, "end": 5068.36, "text": " between what I'm predicting or what's in my world and what I'm currently predicting?", "tokens": [50812, 1296, 437, 286, 478, 32884, 420, 437, 311, 294, 452, 1002, 293, 437, 286, 478, 4362, 32884, 30, 51064], "temperature": 0.0, "avg_logprob": -0.09076719284057617, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.01066162995994091}, {"id": 908, "seek": 505436, "start": 5068.92, "end": 5075.24, "text": " Yes, it rather brings me back to our comments about the space or the manifold that the models", "tokens": [51092, 1079, 11, 309, 2831, 5607, 385, 646, 281, 527, 3053, 466, 264, 1901, 420, 264, 47138, 300, 264, 5245, 51408], "temperature": 0.0, "avg_logprob": -0.09076719284057617, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.01066162995994091}, {"id": 909, "seek": 505436, "start": 5075.24, "end": 5079.96, "text": " sit on, whether they would have a kind of contiguity or whether they would have a gradient.", "tokens": [51408, 1394, 322, 11, 1968, 436, 576, 362, 257, 733, 295, 660, 16397, 507, 420, 1968, 436, 576, 362, 257, 16235, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09076719284057617, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.01066162995994091}, {"id": 910, "seek": 507996, "start": 5080.36, "end": 5085.4800000000005, "text": " I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether", "tokens": [50384, 286, 2041, 286, 478, 27798, 257, 733, 295, 1192, 4383, 1901, 300, 264, 5245, 576, 1394, 322, 13, 286, 500, 380, 458, 1968, 50640], "temperature": 0.0, "avg_logprob": -0.10885682348477638, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004285292234271765}, {"id": 911, "seek": 507996, "start": 5085.4800000000005, "end": 5090.6, "text": " it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,", "tokens": [50640, 309, 311, 3163, 5062, 294, 13, 7580, 11, 291, 434, 257, 28813, 5412, 468, 293, 264, 636, 15442, 589, 11, 50896], "temperature": 0.0, "avg_logprob": -0.10885682348477638, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004285292234271765}, {"id": 912, "seek": 507996, "start": 5091.4, "end": 5098.6, "text": " we must do this. Of course, there's this idea of nativism. Some psychologists think that we have", "tokens": [50936, 321, 1633, 360, 341, 13, 2720, 1164, 11, 456, 311, 341, 1558, 295, 2249, 592, 1434, 13, 2188, 41562, 519, 300, 321, 362, 51296], "temperature": 0.0, "avg_logprob": -0.10885682348477638, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004285292234271765}, {"id": 913, "seek": 507996, "start": 5098.6, "end": 5103.32, "text": " these models built in from birth and then the other school of thought is that we're just a", "tokens": [51296, 613, 5245, 3094, 294, 490, 3965, 293, 550, 264, 661, 1395, 295, 1194, 307, 300, 321, 434, 445, 257, 51532], "temperature": 0.0, "avg_logprob": -0.10885682348477638, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004285292234271765}, {"id": 914, "seek": 507996, "start": 5103.32, "end": 5109.72, "text": " complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical", "tokens": [51532, 3566, 8247, 39118, 13, 759, 291, 1401, 7506, 9325, 10277, 11, 415, 6686, 466, 264, 408, 905, 36143, 382, 341, 12066, 51852], "temperature": 0.0, "avg_logprob": -0.10885682348477638, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.004285292234271765}, {"id": 915, "seek": 510972, "start": 5109.72, "end": 5116.280000000001, "text": " thing that just builds models on the fly. But perhaps one difference at least between brains", "tokens": [50364, 551, 300, 445, 15182, 5245, 322, 264, 3603, 13, 583, 4317, 472, 2649, 412, 1935, 1296, 15442, 50692], "temperature": 0.0, "avg_logprob": -0.1445415892252108, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0020970627665519714}, {"id": 916, "seek": 510972, "start": 5116.280000000001, "end": 5124.12, "text": " and machines is the multi-modality, which is to say we have so many different senses that", "tokens": [50692, 293, 8379, 307, 264, 4825, 12, 8014, 1860, 11, 597, 307, 281, 584, 321, 362, 370, 867, 819, 17057, 300, 51084], "temperature": 0.0, "avg_logprob": -0.1445415892252108, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0020970627665519714}, {"id": 917, "seek": 510972, "start": 5125.16, "end": 5135.08, "text": " creates a gradient or that makes it tractable. Because when a model from a particular sensation", "tokens": [51136, 7829, 257, 16235, 420, 300, 1669, 309, 24207, 712, 13, 1436, 562, 257, 2316, 490, 257, 1729, 20069, 51632], "temperature": 0.0, "avg_logprob": -0.1445415892252108, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0020970627665519714}, {"id": 918, "seek": 510972, "start": 5135.08, "end": 5139.0, "text": " and starts predicting well, we can rapidly optimise and go in the right direction.", "tokens": [51632, 293, 3719, 32884, 731, 11, 321, 393, 12910, 5028, 908, 293, 352, 294, 264, 558, 3513, 13, 51828], "temperature": 0.0, "avg_logprob": -0.1445415892252108, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0020970627665519714}, {"id": 919, "seek": 513900, "start": 5139.08, "end": 5143.56, "text": " Because the problem seems to be that there are so many directions where we can go in,", "tokens": [50368, 1436, 264, 1154, 2544, 281, 312, 300, 456, 366, 370, 867, 11095, 689, 321, 393, 352, 294, 11, 50592], "temperature": 0.0, "avg_logprob": -0.14382964914495294, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0007379217422567308}, {"id": 920, "seek": 513900, "start": 5145.16, "end": 5151.0, "text": " doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the", "tokens": [50672, 884, 512, 733, 295, 1108, 310, 11630, 16235, 5028, 7623, 486, 2049, 1477, 505, 666, 264, 2085, 644, 295, 264, 50964], "temperature": 0.0, "avg_logprob": -0.14382964914495294, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0007379217422567308}, {"id": 921, "seek": 513900, "start": 5151.0, "end": 5158.92, "text": " search space, so we've wasted our time. Yeah, I think that's a really good point,", "tokens": [50964, 3164, 1901, 11, 370, 321, 600, 19496, 527, 565, 13, 865, 11, 286, 519, 300, 311, 257, 534, 665, 935, 11, 51360], "temperature": 0.0, "avg_logprob": -0.14382964914495294, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0007379217422567308}, {"id": 922, "seek": 513900, "start": 5158.92, "end": 5167.16, "text": " absolutely. As soon as you know how one thing works or how vision works, I suppose vision", "tokens": [51360, 3122, 13, 1018, 2321, 382, 291, 458, 577, 472, 551, 1985, 420, 577, 5201, 1985, 11, 286, 7297, 5201, 51772], "temperature": 0.0, "avg_logprob": -0.14382964914495294, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0007379217422567308}, {"id": 923, "seek": 516716, "start": 5167.16, "end": 5173.4, "text": " and proprioception is a good example, isn't it? If I recognise where my hand is and I can", "tokens": [50364, 293, 28203, 7311, 307, 257, 665, 1365, 11, 1943, 380, 309, 30, 759, 286, 23991, 689, 452, 1011, 307, 293, 286, 393, 50676], "temperature": 0.0, "avg_logprob": -0.08833869157639225, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.01107080839574337}, {"id": 924, "seek": 516716, "start": 5173.4, "end": 5177.88, "text": " make a good estimate of that visually, then that helps me tune my joint position sense as to where", "tokens": [50676, 652, 257, 665, 12539, 295, 300, 19622, 11, 550, 300, 3665, 385, 10864, 452, 7225, 2535, 2020, 382, 281, 689, 50900], "temperature": 0.0, "avg_logprob": -0.08833869157639225, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.01107080839574337}, {"id": 925, "seek": 516716, "start": 5177.88, "end": 5184.84, "text": " my arm might be. And it's always fascinating to see situations where that breaks down, so there", "tokens": [50900, 452, 3726, 1062, 312, 13, 400, 309, 311, 1009, 10343, 281, 536, 6851, 689, 300, 9857, 760, 11, 370, 456, 51248], "temperature": 0.0, "avg_logprob": -0.08833869157639225, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.01107080839574337}, {"id": 926, "seek": 516716, "start": 5184.84, "end": 5189.88, "text": " are a number of conditions where if you lose your joint position sense, you're perfectly okay holding", "tokens": [51248, 366, 257, 1230, 295, 4487, 689, 498, 291, 3624, 428, 7225, 2535, 2020, 11, 291, 434, 6239, 1392, 5061, 51500], "temperature": 0.0, "avg_logprob": -0.08833869157639225, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.01107080839574337}, {"id": 927, "seek": 516716, "start": 5189.88, "end": 5194.04, "text": " your arm out like that until you close your eyes, at which point you start getting all these interesting", "tokens": [51500, 428, 3726, 484, 411, 300, 1826, 291, 1998, 428, 2575, 11, 412, 597, 935, 291, 722, 1242, 439, 613, 1880, 51708], "temperature": 0.0, "avg_logprob": -0.08833869157639225, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.01107080839574337}, {"id": 928, "seek": 519404, "start": 5194.04, "end": 5199.88, "text": " twitches and changes. So yes, the multimodality I think probably is a really key thing that really", "tokens": [50364, 34167, 279, 293, 2962, 13, 407, 2086, 11, 264, 32972, 378, 1860, 286, 519, 1391, 307, 257, 534, 2141, 551, 300, 534, 50656], "temperature": 0.0, "avg_logprob": -0.11573303886081861, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0043903859332203865}, {"id": 929, "seek": 519404, "start": 5199.88, "end": 5204.84, "text": " does help constrain the other senses because you're just getting more information about each thing.", "tokens": [50656, 775, 854, 1817, 7146, 264, 661, 17057, 570, 291, 434, 445, 1242, 544, 1589, 466, 1184, 551, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11573303886081861, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0043903859332203865}, {"id": 930, "seek": 519404, "start": 5205.8, "end": 5210.44, "text": " Maybe we should just talk about chapter 10 in general, because that was kind of like the", "tokens": [50952, 2704, 321, 820, 445, 751, 466, 7187, 1266, 294, 2674, 11, 570, 300, 390, 733, 295, 411, 264, 51184], "temperature": 0.0, "avg_logprob": -0.11573303886081861, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0043903859332203865}, {"id": 931, "seek": 519404, "start": 5210.44, "end": 5216.28, "text": " homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch", "tokens": [51184, 1280, 6590, 7187, 11, 498, 291, 411, 1333, 295, 5062, 1214, 512, 295, 264, 3487, 13, 407, 393, 291, 12325, 51476], "temperature": 0.0, "avg_logprob": -0.11573303886081861, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0043903859332203865}, {"id": 932, "seek": 519404, "start": 5216.28, "end": 5223.24, "text": " that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together", "tokens": [51476, 300, 484, 337, 385, 30, 865, 11, 370, 286, 519, 3030, 264, 917, 295, 264, 1446, 11, 264, 1558, 390, 281, 853, 293, 1565, 1214, 51824], "temperature": 0.0, "avg_logprob": -0.11573303886081861, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0043903859332203865}, {"id": 933, "seek": 522324, "start": 5223.24, "end": 5227.32, "text": " a lot of the themes that had been discussed earlier on, but to also make the point that,", "tokens": [50364, 257, 688, 295, 264, 13544, 300, 632, 668, 7152, 3071, 322, 11, 457, 281, 611, 652, 264, 935, 300, 11, 50568], "temperature": 0.0, "avg_logprob": -0.13421437981423368, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0017884087283164263}, {"id": 934, "seek": 522324, "start": 5230.599999999999, "end": 5235.88, "text": " well, I'll come back to one of the things you said earlier was about how it seems we're talking about", "tokens": [50732, 731, 11, 286, 603, 808, 646, 281, 472, 295, 264, 721, 291, 848, 3071, 390, 466, 577, 309, 2544, 321, 434, 1417, 466, 50996], "temperature": 0.0, "avg_logprob": -0.13421437981423368, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0017884087283164263}, {"id": 935, "seek": 522324, "start": 5235.88, "end": 5241.4, "text": " lots of different things from different perspectives, but actually they're really the same thing.", "tokens": [50996, 3195, 295, 819, 721, 490, 819, 16766, 11, 457, 767, 436, 434, 534, 264, 912, 551, 13, 51272], "temperature": 0.0, "avg_logprob": -0.13421437981423368, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0017884087283164263}, {"id": 936, "seek": 522324, "start": 5241.4, "end": 5249.32, "text": " So we talked about how surprise is also a measure of steady state of energies of various sorts of", "tokens": [51272, 407, 321, 2825, 466, 577, 6365, 307, 611, 257, 3481, 295, 13211, 1785, 295, 25737, 295, 3683, 7527, 295, 51668], "temperature": 0.0, "avg_logprob": -0.13421437981423368, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0017884087283164263}, {"id": 937, "seek": 524932, "start": 5250.28, "end": 5256.12, "text": " of statistics and model comparison of homeostatic set points, you know, that all of these things", "tokens": [50412, 295, 12523, 293, 2316, 9660, 295, 1280, 555, 2399, 992, 2793, 11, 291, 458, 11, 300, 439, 295, 613, 721, 50704], "temperature": 0.0, "avg_logprob": -0.09277273746246988, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.001477900892496109}, {"id": 938, "seek": 524932, "start": 5256.12, "end": 5264.599999999999, "text": " can be seen through the same lens. But again, taking one of those inversions, you can invert", "tokens": [50704, 393, 312, 1612, 807, 264, 912, 6765, 13, 583, 797, 11, 1940, 472, 295, 729, 21378, 626, 11, 291, 393, 33966, 51128], "temperature": 0.0, "avg_logprob": -0.09277273746246988, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.001477900892496109}, {"id": 939, "seek": 524932, "start": 5264.599999999999, "end": 5267.96, "text": " that lens and say, well, actually, you can start from the same thing and now project back into", "tokens": [51128, 300, 6765, 293, 584, 11, 731, 11, 767, 11, 291, 393, 722, 490, 264, 912, 551, 293, 586, 1716, 646, 666, 51296], "temperature": 0.0, "avg_logprob": -0.09277273746246988, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.001477900892496109}, {"id": 940, "seek": 524932, "start": 5267.96, "end": 5275.48, "text": " all of these different fields. And I think that's a useful thing to do because I think it helps foster", "tokens": [51296, 439, 295, 613, 819, 7909, 13, 400, 286, 519, 300, 311, 257, 4420, 551, 281, 360, 570, 286, 519, 309, 3665, 17114, 51672], "temperature": 0.0, "avg_logprob": -0.09277273746246988, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.001477900892496109}, {"id": 941, "seek": 527548, "start": 5275.5599999999995, "end": 5280.599999999999, "text": " multidisciplinary work, helps to engage people from different fields and areas,", "tokens": [50368, 2120, 40920, 24560, 589, 11, 3665, 281, 4683, 561, 490, 819, 7909, 293, 3179, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09094177172021958, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.008489474654197693}, {"id": 942, "seek": 527548, "start": 5281.5599999999995, "end": 5288.28, "text": " and helps us know what's happening elsewhere so that you're not just duplicating everything that", "tokens": [50668, 293, 3665, 505, 458, 437, 311, 2737, 14517, 370, 300, 291, 434, 406, 445, 17154, 990, 1203, 300, 51004], "temperature": 0.0, "avg_logprob": -0.09094177172021958, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.008489474654197693}, {"id": 943, "seek": 527548, "start": 5288.28, "end": 5293.639999999999, "text": " people have already done. So I think it's really important to have those connections to different", "tokens": [51004, 561, 362, 1217, 1096, 13, 407, 286, 519, 309, 311, 534, 1021, 281, 362, 729, 9271, 281, 819, 51272], "temperature": 0.0, "avg_logprob": -0.09094177172021958, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.008489474654197693}, {"id": 944, "seek": 527548, "start": 5293.639999999999, "end": 5299.799999999999, "text": " areas. And the chapter 10 from the book was an aim to try and connect to those different areas,", "tokens": [51272, 3179, 13, 400, 264, 7187, 1266, 490, 264, 1446, 390, 364, 5939, 281, 853, 293, 1745, 281, 729, 819, 3179, 11, 51580], "temperature": 0.0, "avg_logprob": -0.09094177172021958, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.008489474654197693}, {"id": 945, "seek": 527548, "start": 5299.799999999999, "end": 5303.24, "text": " whether it be to things you've spoken about, like cybernetics and inactivism,", "tokens": [51580, 1968, 309, 312, 281, 721, 291, 600, 10759, 466, 11, 411, 13411, 7129, 1167, 293, 294, 23397, 1434, 11, 51752], "temperature": 0.0, "avg_logprob": -0.09094177172021958, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.008489474654197693}, {"id": 946, "seek": 530324, "start": 5303.32, "end": 5307.8, "text": " and just to try and understand the relationship between each of them.", "tokens": [50368, 293, 445, 281, 853, 293, 1223, 264, 2480, 1296, 1184, 295, 552, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1594442438196253, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.004697183612734079}, {"id": 947, "seek": 530324, "start": 5307.8, "end": 5312.5199999999995, "text": " Well, I mean, quite a lot of people use this as a model of, you know, just things like", "tokens": [50592, 1042, 11, 286, 914, 11, 1596, 257, 688, 295, 561, 764, 341, 382, 257, 2316, 295, 11, 291, 458, 11, 445, 721, 411, 50828], "temperature": 0.0, "avg_logprob": -0.1594442438196253, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.004697183612734079}, {"id": 948, "seek": 530324, "start": 5312.5199999999995, "end": 5318.44, "text": " sentience and consciousness in general. And I often speak about the strange bedfellows of", "tokens": [50828, 2279, 1182, 293, 10081, 294, 2674, 13, 400, 286, 2049, 1710, 466, 264, 5861, 2901, 69, 898, 1509, 295, 51124], "temperature": 0.0, "avg_logprob": -0.1594442438196253, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.004697183612734079}, {"id": 949, "seek": 530324, "start": 5318.44, "end": 5323.48, "text": " the free energy principle. So, you know, there are, you know, autopoietic and activists and", "tokens": [51124, 264, 1737, 2281, 8665, 13, 407, 11, 291, 458, 11, 456, 366, 11, 291, 458, 11, 31090, 78, 1684, 299, 293, 23042, 293, 51376], "temperature": 0.0, "avg_logprob": -0.1594442438196253, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.004697183612734079}, {"id": 950, "seek": 530324, "start": 5323.48, "end": 5327.639999999999, "text": " phenomenologists and, you know, people talking about sentience and consciousness, you know,", "tokens": [51376, 9388, 12256, 293, 11, 291, 458, 11, 561, 1417, 466, 2279, 1182, 293, 10081, 11, 291, 458, 11, 51584], "temperature": 0.0, "avg_logprob": -0.1594442438196253, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.004697183612734079}, {"id": 951, "seek": 532764, "start": 5327.64, "end": 5333.4800000000005, "text": " obviously you're a clinician, you know, you're working in a hospital. So it's just this", "tokens": [50364, 2745, 291, 434, 257, 45962, 11, 291, 458, 11, 291, 434, 1364, 294, 257, 4530, 13, 407, 309, 311, 445, 341, 50656], "temperature": 0.0, "avg_logprob": -0.10341113697398793, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0060981339775025845}, {"id": 952, "seek": 532764, "start": 5333.4800000000005, "end": 5337.96, "text": " incredible conflation of different people together, and they all bring their own lexicon with them.", "tokens": [50656, 4651, 1497, 24278, 295, 819, 561, 1214, 11, 293, 436, 439, 1565, 641, 1065, 476, 87, 11911, 365, 552, 13, 50880], "temperature": 0.0, "avg_logprob": -0.10341113697398793, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0060981339775025845}, {"id": 953, "seek": 532764, "start": 5337.96, "end": 5342.360000000001, "text": " But maybe we should just get on to this kind of sentience and consciousness thing, because that", "tokens": [50880, 583, 1310, 321, 820, 445, 483, 322, 281, 341, 733, 295, 2279, 1182, 293, 10081, 551, 11, 570, 300, 51100], "temperature": 0.0, "avg_logprob": -0.10341113697398793, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0060981339775025845}, {"id": 954, "seek": 532764, "start": 5342.360000000001, "end": 5348.84, "text": " seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,", "tokens": [51100, 2544, 1596, 13831, 13, 492, 1920, 808, 646, 281, 472, 295, 264, 13544, 321, 600, 10759, 466, 257, 1326, 1413, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10341113697398793, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0060981339775025845}, {"id": 955, "seek": 532764, "start": 5348.84, "end": 5356.12, "text": " which is that the specific words we use for things in the effect that different people,", "tokens": [51424, 597, 307, 300, 264, 2685, 2283, 321, 764, 337, 721, 294, 264, 1802, 300, 819, 561, 11, 51788], "temperature": 0.0, "avg_logprob": -0.10341113697398793, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0060981339775025845}, {"id": 956, "seek": 535612, "start": 5356.12, "end": 5359.72, "text": " that has on different people. So some people, I think, would probably get very angry with the idea", "tokens": [50364, 300, 575, 322, 819, 561, 13, 407, 512, 561, 11, 286, 519, 11, 576, 1391, 483, 588, 6884, 365, 264, 1558, 50544], "temperature": 0.0, "avg_logprob": -0.09286148317398564, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0034378860145807266}, {"id": 957, "seek": 535612, "start": 5359.72, "end": 5369.64, "text": " of using sentience to describe some of the sort of simulations and models that we would develop.", "tokens": [50544, 295, 1228, 2279, 1182, 281, 6786, 512, 295, 264, 1333, 295, 35138, 293, 5245, 300, 321, 576, 1499, 13, 51040], "temperature": 0.0, "avg_logprob": -0.09286148317398564, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0034378860145807266}, {"id": 958, "seek": 535612, "start": 5371.4, "end": 5376.12, "text": " But that comes down to what you mean by sentience. And I think one of the key things for sentience", "tokens": [51128, 583, 300, 1487, 760, 281, 437, 291, 914, 538, 2279, 1182, 13, 400, 286, 519, 472, 295, 264, 2141, 721, 337, 2279, 1182, 51364], "temperature": 0.0, "avg_logprob": -0.09286148317398564, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0034378860145807266}, {"id": 959, "seek": 535612, "start": 5376.12, "end": 5382.36, "text": " is the aboutness we were talking about before. The idea that our brains or any sentient system", "tokens": [51364, 307, 264, 466, 1287, 321, 645, 1417, 466, 949, 13, 440, 1558, 300, 527, 15442, 420, 604, 2279, 1196, 1185, 51676], "temperature": 0.0, "avg_logprob": -0.09286148317398564, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0034378860145807266}, {"id": 960, "seek": 538236, "start": 5382.36, "end": 5388.5199999999995, "text": " really is trying to try not to anthropomorphise too much, but it's almost impossible to do in", "tokens": [50364, 534, 307, 1382, 281, 853, 406, 281, 22727, 32702, 908, 886, 709, 11, 457, 309, 311, 1920, 6243, 281, 360, 294, 50672], "temperature": 0.0, "avg_logprob": -0.10096586268881093, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.006945237051695585}, {"id": 961, "seek": 538236, "start": 5388.5199999999995, "end": 5397.08, "text": " this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system", "tokens": [50672, 341, 3287, 11, 1943, 380, 309, 30, 1726, 1382, 281, 11, 457, 300, 264, 15679, 295, 512, 1185, 19501, 281, 264, 1185, 51100], "temperature": 0.0, "avg_logprob": -0.10096586268881093, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.006945237051695585}, {"id": 962, "seek": 538236, "start": 5397.08, "end": 5401.799999999999, "text": " are reflective of what's going on external to it, and that you can now start to see those dynamics", "tokens": [51100, 366, 28931, 295, 437, 311, 516, 322, 8320, 281, 309, 11, 293, 300, 291, 393, 586, 722, 281, 536, 729, 15679, 51336], "temperature": 0.0, "avg_logprob": -0.10096586268881093, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.006945237051695585}, {"id": 963, "seek": 538236, "start": 5401.799999999999, "end": 5407.0, "text": " as being optimization of beliefs. And those beliefs are about what's happening in the outside world", "tokens": [51336, 382, 885, 19618, 295, 13585, 13, 400, 729, 13585, 366, 466, 437, 311, 2737, 294, 264, 2380, 1002, 51596], "temperature": 0.0, "avg_logprob": -0.10096586268881093, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.006945237051695585}, {"id": 964, "seek": 540700, "start": 5407.08, "end": 5412.68, "text": " and about how I'm affecting the outside world. And I think that probably gets to the root of", "tokens": [50368, 293, 466, 577, 286, 478, 17476, 264, 2380, 1002, 13, 400, 286, 519, 300, 1391, 2170, 281, 264, 5593, 295, 50648], "temperature": 0.0, "avg_logprob": -0.04787720208880545, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.020104585215449333}, {"id": 965, "seek": 540700, "start": 5412.68, "end": 5418.6, "text": " at least a definition of sentience and one that I'd be happy with, which is just the", "tokens": [50648, 412, 1935, 257, 7123, 295, 2279, 1182, 293, 472, 300, 286, 1116, 312, 2055, 365, 11, 597, 307, 445, 264, 50944], "temperature": 0.0, "avg_logprob": -0.04787720208880545, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.020104585215449333}, {"id": 966, "seek": 540700, "start": 5420.04, "end": 5426.12, "text": " dynamics of beliefs about what's external to us and how we want to change it.", "tokens": [51016, 15679, 295, 13585, 466, 437, 311, 8320, 281, 505, 293, 577, 321, 528, 281, 1319, 309, 13, 51320], "temperature": 0.0, "avg_logprob": -0.04787720208880545, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.020104585215449333}, {"id": 967, "seek": 540700, "start": 5427.8, "end": 5431.88, "text": " And there are very few things other than that sort of inferential formalism that give you that.", "tokens": [51404, 400, 456, 366, 588, 1326, 721, 661, 813, 300, 1333, 295, 13596, 2549, 9860, 1434, 300, 976, 291, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.04787720208880545, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.020104585215449333}, {"id": 968, "seek": 543188, "start": 5432.84, "end": 5438.2, "text": " Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,", "tokens": [50412, 1079, 11, 286, 914, 11, 294, 257, 636, 11, 472, 551, 286, 411, 466, 309, 307, 11, 286, 914, 11, 321, 366, 1417, 382, 48716, 11, 50680], "temperature": 0.0, "avg_logprob": -0.10913996445505243, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.021873947232961655}, {"id": 969, "seek": 543188, "start": 5438.2, "end": 5447.64, "text": " so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are", "tokens": [50680, 370, 321, 366, 2527, 1751, 13, 467, 311, 588, 572, 12, 77, 13039, 13, 467, 311, 1596, 2783, 20221, 382, 731, 11, 570, 456, 366, 51152], "temperature": 0.0, "avg_logprob": -0.10913996445505243, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.021873947232961655}, {"id": 970, "seek": 543188, "start": 5447.64, "end": 5453.56, "text": " those who believe that these kind of qualities that we're speaking about, certainly with", "tokens": [51152, 729, 567, 1697, 300, 613, 733, 295, 16477, 300, 321, 434, 4124, 466, 11, 3297, 365, 51448], "temperature": 0.0, "avg_logprob": -0.10913996445505243, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.021873947232961655}, {"id": 971, "seek": 543188, "start": 5453.56, "end": 5459.8, "text": " conscious experience, for example, that it's not reducible to these kind of simple explanations", "tokens": [51448, 6648, 1752, 11, 337, 1365, 11, 300, 309, 311, 406, 2783, 32128, 281, 613, 733, 295, 2199, 28708, 51760], "temperature": 0.0, "avg_logprob": -0.10913996445505243, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.021873947232961655}, {"id": 972, "seek": 545980, "start": 5459.88, "end": 5466.12, "text": " that we're talking about, that it has a different character. David Chalmers talks about a philosophical", "tokens": [50368, 300, 321, 434, 1417, 466, 11, 300, 309, 575, 257, 819, 2517, 13, 4389, 761, 304, 18552, 6686, 466, 257, 25066, 50680], "temperature": 0.0, "avg_logprob": -0.10145686138635394, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.004215025808662176}, {"id": 973, "seek": 545980, "start": 5466.12, "end": 5474.360000000001, "text": " zombie. So for example, you might behave just like a real human being, but you could be divorced of", "tokens": [50680, 20310, 13, 407, 337, 1365, 11, 291, 1062, 15158, 445, 411, 257, 957, 1952, 885, 11, 457, 291, 727, 312, 27670, 295, 51092], "temperature": 0.0, "avg_logprob": -0.10145686138635394, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.004215025808662176}, {"id": 974, "seek": 545980, "start": 5474.360000000001, "end": 5482.12, "text": " conscious experience. So he says that you can think of behavior, dynamics, and function,", "tokens": [51092, 6648, 1752, 13, 407, 415, 1619, 300, 291, 393, 519, 295, 5223, 11, 15679, 11, 293, 2445, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10145686138635394, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.004215025808662176}, {"id": 975, "seek": 545980, "start": 5482.12, "end": 5487.320000000001, "text": " and conscious experience as something entirely different. But as an observer, you would never", "tokens": [51480, 293, 6648, 1752, 382, 746, 7696, 819, 13, 583, 382, 364, 27878, 11, 291, 576, 1128, 51740], "temperature": 0.0, "avg_logprob": -0.10145686138635394, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.004215025808662176}, {"id": 976, "seek": 548732, "start": 5487.32, "end": 5494.44, "text": " know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of", "tokens": [50364, 458, 13, 407, 1338, 11, 309, 3417, 588, 572, 12, 77, 13039, 11, 1177, 380, 309, 30, 583, 300, 2759, 380, 312, 18348, 281, 257, 688, 295, 50720], "temperature": 0.0, "avg_logprob": -0.10061548153559367, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0019353496609255672}, {"id": 977, "seek": 548732, "start": 5494.44, "end": 5503.4, "text": " people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions", "tokens": [50720, 561, 13, 883, 11, 309, 1391, 2759, 380, 13, 509, 434, 558, 13, 865, 11, 293, 4098, 562, 291, 483, 3911, 1651, 51168], "temperature": 0.0, "avg_logprob": -0.10061548153559367, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0019353496609255672}, {"id": 978, "seek": 548732, "start": 5503.4, "end": 5510.679999999999, "text": " like consciousness as well, I mean, I think it does become very, very difficult, because once", "tokens": [51168, 411, 10081, 382, 731, 11, 286, 914, 11, 286, 519, 309, 775, 1813, 588, 11, 588, 2252, 11, 570, 1564, 51532], "temperature": 0.0, "avg_logprob": -0.10061548153559367, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0019353496609255672}, {"id": 979, "seek": 548732, "start": 5510.679999999999, "end": 5516.12, "text": " you're putting forward or advocating a theoretical framework that seems like it's supposed to have", "tokens": [51532, 291, 434, 3372, 2128, 420, 32050, 257, 20864, 8388, 300, 2544, 411, 309, 311, 3442, 281, 362, 51804], "temperature": 0.0, "avg_logprob": -0.10061548153559367, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0019353496609255672}, {"id": 980, "seek": 551612, "start": 5516.12, "end": 5522.2, "text": " all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be", "tokens": [50364, 439, 264, 6338, 13, 286, 914, 11, 294, 4103, 11, 309, 1177, 380, 13, 286, 914, 11, 286, 519, 309, 311, 257, 4420, 8388, 281, 312, 50668], "temperature": 0.0, "avg_logprob": -0.062234810420445034, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006752395536750555}, {"id": 981, "seek": 551612, "start": 5522.2, "end": 5528.68, "text": " able to ask the right questions or to be able to articulate your hypotheses. So if you think that", "tokens": [50668, 1075, 281, 1029, 264, 558, 1651, 420, 281, 312, 1075, 281, 30305, 428, 49969, 13, 407, 498, 291, 519, 300, 50992], "temperature": 0.0, "avg_logprob": -0.062234810420445034, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006752395536750555}, {"id": 982, "seek": 551612, "start": 5528.68, "end": 5535.72, "text": " consciousness is based upon the idea of having some sense of trajectory of temporal extent and", "tokens": [50992, 10081, 307, 2361, 3564, 264, 1558, 295, 1419, 512, 2020, 295, 21512, 295, 30881, 8396, 293, 51344], "temperature": 0.0, "avg_logprob": -0.062234810420445034, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006752395536750555}, {"id": 983, "seek": 551612, "start": 5535.72, "end": 5541.64, "text": " different worlds I can choose between or different futures I can choose between, that might be a key", "tokens": [51344, 819, 13401, 286, 393, 2826, 1296, 420, 819, 26071, 286, 393, 2826, 1296, 11, 300, 1062, 312, 257, 2141, 51640], "temperature": 0.0, "avg_logprob": -0.062234810420445034, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006752395536750555}, {"id": 984, "seek": 554164, "start": 5541.64, "end": 5544.92, "text": " part of it. But for some people, that's not what they mean by consciousness.", "tokens": [50364, 644, 295, 309, 13, 583, 337, 512, 561, 11, 300, 311, 406, 437, 436, 914, 538, 10081, 13, 50528], "temperature": 0.0, "avg_logprob": -0.13186870712831797, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.03485632687807083}, {"id": 985, "seek": 554164, "start": 5547.56, "end": 5556.280000000001, "text": " I found in a particular reading books by people like Anil Seth on this sort of topic, I found one", "tokens": [50660, 286, 1352, 294, 257, 1729, 3760, 3642, 538, 561, 411, 1107, 388, 25353, 322, 341, 1333, 295, 4829, 11, 286, 1352, 472, 51096], "temperature": 0.0, "avg_logprob": -0.13186870712831797, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.03485632687807083}, {"id": 986, "seek": 554164, "start": 5556.280000000001, "end": 5563.4800000000005, "text": " of the interesting comparisons being the questions about consciousness versus questions about life.", "tokens": [51096, 295, 264, 1880, 33157, 885, 264, 1651, 466, 10081, 5717, 1651, 466, 993, 13, 51456], "temperature": 0.0, "avg_logprob": -0.13186870712831797, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.03485632687807083}, {"id": 987, "seek": 554164, "start": 5563.4800000000005, "end": 5568.12, "text": " And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,", "tokens": [51456, 400, 321, 1920, 500, 380, 1029, 437, 993, 307, 3602, 13, 467, 1177, 380, 4725, 1643, 300, 13831, 11, 51688], "temperature": 0.0, "avg_logprob": -0.13186870712831797, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.03485632687807083}, {"id": 988, "seek": 556812, "start": 5568.92, "end": 5574.599999999999, "text": " just because we've had so much of an understanding of the processes involved in life, the dynamics of", "tokens": [50404, 445, 570, 321, 600, 632, 370, 709, 295, 364, 3701, 295, 264, 7555, 3288, 294, 993, 11, 264, 15679, 295, 50688], "temperature": 0.0, "avg_logprob": -0.12188638959612165, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.00196107872761786}, {"id": 989, "seek": 556812, "start": 5574.599999999999, "end": 5581.16, "text": " life and the way biology works, it's still much more to go. But the question of what life is just", "tokens": [50688, 993, 293, 264, 636, 14956, 1985, 11, 309, 311, 920, 709, 544, 281, 352, 13, 583, 264, 1168, 295, 437, 993, 307, 445, 51016], "temperature": 0.0, "avg_logprob": -0.12188638959612165, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.00196107872761786}, {"id": 990, "seek": 556812, "start": 5581.16, "end": 5586.2, "text": " doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions", "tokens": [51016, 1177, 380, 1643, 382, 7340, 965, 382, 286, 9091, 309, 630, 867, 924, 2057, 365, 729, 7527, 295, 1651, 51268], "temperature": 0.0, "avg_logprob": -0.12188638959612165, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.00196107872761786}, {"id": 991, "seek": 556812, "start": 5586.2, "end": 5590.44, "text": " that were being posed. And perhaps we'll see the same thing with questions like consciousness.", "tokens": [51268, 300, 645, 885, 31399, 13, 400, 4317, 321, 603, 536, 264, 912, 551, 365, 1651, 411, 10081, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12188638959612165, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.00196107872761786}, {"id": 992, "seek": 556812, "start": 5591.8, "end": 5597.8, "text": " Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting", "tokens": [51548, 865, 11, 309, 311, 1880, 1673, 577, 24247, 867, 295, 613, 10392, 366, 13, 400, 309, 311, 1596, 364, 1880, 51848], "temperature": 0.0, "avg_logprob": -0.12188638959612165, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.00196107872761786}, {"id": 993, "seek": 559780, "start": 5597.8, "end": 5604.76, "text": " thought experiment just to get someone to explain just an everyday thing, you know, like what happens", "tokens": [50364, 1194, 5120, 445, 281, 483, 1580, 281, 2903, 445, 364, 7429, 551, 11, 291, 458, 11, 411, 437, 2314, 50712], "temperature": 0.0, "avg_logprob": -0.09392717833160072, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0008373463060706854}, {"id": 994, "seek": 559780, "start": 5604.76, "end": 5612.84, "text": " when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and", "tokens": [50712, 562, 291, 3507, 4982, 322, 264, 4123, 13, 400, 445, 1066, 3365, 983, 13, 400, 445, 22107, 577, 834, 78, 511, 317, 293, 51116], "temperature": 0.0, "avg_logprob": -0.09392717833160072, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0008373463060706854}, {"id": 995, "seek": 559780, "start": 5612.84, "end": 5617.24, "text": " incomplete the explanations are. And it's the same thing with life, it's the same thing of", "tokens": [51116, 31709, 264, 28708, 366, 13, 400, 309, 311, 264, 912, 551, 365, 993, 11, 309, 311, 264, 912, 551, 295, 51336], "temperature": 0.0, "avg_logprob": -0.09392717833160072, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0008373463060706854}, {"id": 996, "seek": 559780, "start": 5617.24, "end": 5621.96, "text": " consciousness, it's the same thing of causality, agency, intelligence, all of these different things.", "tokens": [51336, 10081, 11, 309, 311, 264, 912, 551, 295, 3302, 1860, 11, 7934, 11, 7599, 11, 439, 295, 613, 819, 721, 13, 51572], "temperature": 0.0, "avg_logprob": -0.09392717833160072, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0008373463060706854}, {"id": 997, "seek": 562196, "start": 5621.96, "end": 5628.2, "text": " And I guess most people don't spend time digging into their understandings of these things and", "tokens": [50364, 400, 286, 2041, 881, 561, 500, 380, 3496, 565, 17343, 666, 641, 1223, 1109, 295, 613, 721, 293, 50676], "temperature": 0.0, "avg_logprob": -0.08902115232489083, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0007384167402051389}, {"id": 998, "seek": 562196, "start": 5628.2, "end": 5634.36, "text": " realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,", "tokens": [50676, 16734, 577, 834, 78, 511, 317, 293, 31709, 436, 366, 13, 7720, 307, 1596, 364, 1880, 472, 294, 1729, 11, 50984], "temperature": 0.0, "avg_logprob": -0.08902115232489083, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0007384167402051389}, {"id": 999, "seek": 562196, "start": 5634.36, "end": 5642.2, "text": " because I think one of the achievements of active inference is blurring the definition of or the", "tokens": [50984, 570, 286, 519, 472, 295, 264, 21420, 295, 4967, 38253, 307, 14257, 2937, 264, 7123, 295, 420, 264, 51376], "temperature": 0.0, "avg_logprob": -0.08902115232489083, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0007384167402051389}, {"id": 1000, "seek": 562196, "start": 5642.2, "end": 5650.12, "text": " demarcation between things which are and are not alive. For example, the orthopedic anactivists,", "tokens": [51376, 1371, 40088, 399, 1296, 721, 597, 366, 293, 366, 406, 5465, 13, 1171, 1365, 11, 264, 19052, 27277, 299, 364, 23397, 1751, 11, 51772], "temperature": 0.0, "avg_logprob": -0.08902115232489083, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0007384167402051389}, {"id": 1001, "seek": 565012, "start": 5650.12, "end": 5656.04, "text": " they think of biology as being instrumental. And what the, you know, free energy principle", "tokens": [50364, 436, 519, 295, 14956, 382, 885, 17388, 13, 400, 437, 264, 11, 291, 458, 11, 1737, 2281, 8665, 50660], "temperature": 0.0, "avg_logprob": -0.08681617111995302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.003819577395915985}, {"id": 1002, "seek": 565012, "start": 5656.04, "end": 5661.32, "text": " does in my opinion, is it removes the need for this, it almost removes the need for biology", "tokens": [50660, 775, 294, 452, 4800, 11, 307, 309, 30445, 264, 643, 337, 341, 11, 309, 1920, 30445, 264, 643, 337, 14956, 50924], "temperature": 0.0, "avg_logprob": -0.08681617111995302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.003819577395915985}, {"id": 1003, "seek": 565012, "start": 5661.32, "end": 5667.24, "text": " entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that", "tokens": [50924, 7696, 13, 467, 445, 1619, 309, 311, 445, 15679, 11, 309, 311, 445, 10649, 13, 583, 1338, 11, 286, 914, 11, 445, 322, 300, 51220], "temperature": 0.0, "avg_logprob": -0.08681617111995302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.003819577395915985}, {"id": 1004, "seek": 565012, "start": 5667.24, "end": 5672.44, "text": " point, though, I think many of our ideas about the world are quite incoherent.", "tokens": [51220, 935, 11, 1673, 11, 286, 519, 867, 295, 527, 3487, 466, 264, 1002, 366, 1596, 834, 78, 511, 317, 13, 51480], "temperature": 0.0, "avg_logprob": -0.08681617111995302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.003819577395915985}, {"id": 1005, "seek": 565012, "start": 5673.88, "end": 5678.04, "text": " Yeah. And I think it's interesting that, you know, one of the things that you're saying,", "tokens": [51552, 865, 13, 400, 286, 519, 309, 311, 1880, 300, 11, 291, 458, 11, 472, 295, 264, 721, 300, 291, 434, 1566, 11, 51760], "temperature": 0.0, "avg_logprob": -0.08681617111995302, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.003819577395915985}, {"id": 1006, "seek": 567804, "start": 5678.04, "end": 5682.28, "text": " and I would agree with you as one of the big advantages of active inference-based formalisms,", "tokens": [50364, 293, 286, 576, 3986, 365, 291, 382, 472, 295, 264, 955, 14906, 295, 4967, 38253, 12, 6032, 9860, 13539, 11, 50576], "temperature": 0.0, "avg_logprob": -0.11919554492883515, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00396721763536334}, {"id": 1007, "seek": 567804, "start": 5682.28, "end": 5687.48, "text": " you'll probably find some people will say, that's a problem with it, that actually there is a clean", "tokens": [50576, 291, 603, 1391, 915, 512, 561, 486, 584, 11, 300, 311, 257, 1154, 365, 309, 11, 300, 767, 456, 307, 257, 2541, 50836], "temperature": 0.0, "avg_logprob": -0.11919554492883515, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00396721763536334}, {"id": 1008, "seek": 567804, "start": 5687.48, "end": 5693.24, "text": " distinction in their mind between these different things. But then I think the challenge is to work", "tokens": [50836, 16844, 294, 641, 1575, 1296, 613, 819, 721, 13, 583, 550, 286, 519, 264, 3430, 307, 281, 589, 51124], "temperature": 0.0, "avg_logprob": -0.11919554492883515, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00396721763536334}, {"id": 1009, "seek": 567804, "start": 5693.24, "end": 5699.72, "text": " out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't", "tokens": [51124, 484, 437, 300, 16844, 307, 11, 498, 309, 8198, 13, 400, 309, 815, 312, 257, 16844, 294, 641, 1575, 300, 1177, 380, 51448], "temperature": 0.0, "avg_logprob": -0.11919554492883515, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00396721763536334}, {"id": 1010, "seek": 567804, "start": 5699.72, "end": 5707.48, "text": " exist in somebody else's mind. And so getting people to try and or trying to support people to", "tokens": [51448, 2514, 294, 2618, 1646, 311, 1575, 13, 400, 370, 1242, 561, 281, 853, 293, 420, 1382, 281, 1406, 561, 281, 51836], "temperature": 0.0, "avg_logprob": -0.11919554492883515, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00396721763536334}, {"id": 1011, "seek": 570748, "start": 5707.48, "end": 5714.28, "text": " be able to express that in a very precise mathematical hypothesis, I think is quite a", "tokens": [50364, 312, 1075, 281, 5109, 300, 294, 257, 588, 13600, 18894, 17291, 11, 286, 519, 307, 1596, 257, 50704], "temperature": 0.0, "avg_logprob": -0.11271648036623463, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.000966144900303334}, {"id": 1012, "seek": 570748, "start": 5714.28, "end": 5719.0, "text": " useful way of trying to explore those problems. Because clearly, for some people, there is", "tokens": [50704, 4420, 636, 295, 1382, 281, 6839, 729, 2740, 13, 1436, 4448, 11, 337, 512, 561, 11, 456, 307, 50940], "temperature": 0.0, "avg_logprob": -0.11271648036623463, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.000966144900303334}, {"id": 1013, "seek": 570748, "start": 5719.0, "end": 5722.679999999999, "text": " something that's getting at it that is not quite explaining. And it's interesting to try", "tokens": [50940, 746, 300, 311, 1242, 412, 309, 300, 307, 406, 1596, 13468, 13, 400, 309, 311, 1880, 281, 853, 51124], "temperature": 0.0, "avg_logprob": -0.11271648036623463, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.000966144900303334}, {"id": 1014, "seek": 570748, "start": 5722.679999999999, "end": 5728.28, "text": " and explore that and to work out what that thing is. Indeed, indeed. And just final question,", "tokens": [51124, 293, 6839, 300, 293, 281, 589, 484, 437, 300, 551, 307, 13, 15061, 11, 6451, 13, 400, 445, 2572, 1168, 11, 51404], "temperature": 0.0, "avg_logprob": -0.11271648036623463, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.000966144900303334}, {"id": 1015, "seek": 570748, "start": 5728.28, "end": 5732.5199999999995, "text": " what was your experience writing a book? And would you recommend it to other people?", "tokens": [51404, 437, 390, 428, 1752, 3579, 257, 1446, 30, 400, 576, 291, 2748, 309, 281, 661, 561, 30, 51616], "temperature": 0.0, "avg_logprob": -0.11271648036623463, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.000966144900303334}, {"id": 1016, "seek": 573252, "start": 5733.0, "end": 5746.120000000001, "text": " I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of", "tokens": [50388, 286, 4626, 3579, 309, 13, 286, 519, 309, 311, 565, 19867, 293, 393, 841, 411, 309, 311, 516, 322, 5680, 512, 295, 51044], "temperature": 0.0, "avg_logprob": -0.17213521684919084, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0034373414237052202}, {"id": 1017, "seek": 573252, "start": 5746.120000000001, "end": 5751.320000000001, "text": " the time compared to, you know, I think anyone who's had some experience of writing papers will", "tokens": [51044, 264, 565, 5347, 281, 11, 291, 458, 11, 286, 519, 2878, 567, 311, 632, 512, 1752, 295, 3579, 10577, 486, 51304], "temperature": 0.0, "avg_logprob": -0.17213521684919084, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0034373414237052202}, {"id": 1018, "seek": 573252, "start": 5751.320000000001, "end": 5755.160000000001, "text": " often find that at the point where you're ready to submit it, you're just sick of it and want to see", "tokens": [51304, 2049, 915, 300, 412, 264, 935, 689, 291, 434, 1919, 281, 10315, 309, 11, 291, 434, 445, 4998, 295, 309, 293, 528, 281, 536, 51496], "temperature": 0.0, "avg_logprob": -0.17213521684919084, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0034373414237052202}, {"id": 1019, "seek": 573252, "start": 5755.160000000001, "end": 5761.320000000001, "text": " the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments", "tokens": [51496, 264, 646, 295, 309, 13, 400, 550, 309, 311, 32109, 736, 8752, 281, 291, 538, 264, 15108, 45837, 365, 3195, 295, 3053, 51804], "temperature": 0.0, "avg_logprob": -0.17213521684919084, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0034373414237052202}, {"id": 1020, "seek": 576132, "start": 5761.639999999999, "end": 5767.88, "text": " writing a book, it obviously takes you much longer. So you end up being almost more sick of it at", "tokens": [50380, 3579, 257, 1446, 11, 309, 2745, 2516, 291, 709, 2854, 13, 407, 291, 917, 493, 885, 1920, 544, 4998, 295, 309, 412, 50692], "temperature": 0.0, "avg_logprob": -0.0809170286232066, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0009100236929953098}, {"id": 1021, "seek": 576132, "start": 5767.88, "end": 5773.08, "text": " various times. But it's quite fun as a collaborative project. It's quite interesting to get other", "tokens": [50692, 3683, 1413, 13, 583, 309, 311, 1596, 1019, 382, 257, 16555, 1716, 13, 467, 311, 1596, 1880, 281, 483, 661, 50952], "temperature": 0.0, "avg_logprob": -0.0809170286232066, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0009100236929953098}, {"id": 1022, "seek": 576132, "start": 5773.08, "end": 5777.32, "text": " people's perspectives on it. And I was lucky to have great collaborators to write it with.", "tokens": [50952, 561, 311, 16766, 322, 309, 13, 400, 286, 390, 6356, 281, 362, 869, 39789, 281, 2464, 309, 365, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0809170286232066, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0009100236929953098}, {"id": 1023, "seek": 576132, "start": 5778.44, "end": 5783.0, "text": " And I think it really is a good way of organizing your thoughts in a slightly more holistic way", "tokens": [51220, 400, 286, 519, 309, 534, 307, 257, 665, 636, 295, 17608, 428, 4598, 294, 257, 4748, 544, 30334, 636, 51448], "temperature": 0.0, "avg_logprob": -0.0809170286232066, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0009100236929953098}, {"id": 1024, "seek": 576132, "start": 5783.799999999999, "end": 5787.639999999999, "text": " than you would while focusing on a very specific topic in a research paper.", "tokens": [51488, 813, 291, 576, 1339, 8416, 322, 257, 588, 2685, 4829, 294, 257, 2132, 3035, 13, 51680], "temperature": 0.0, "avg_logprob": -0.0809170286232066, "compression_ratio": 1.624113475177305, "no_speech_prob": 0.0009100236929953098}, {"id": 1025, "seek": 578764, "start": 5788.6, "end": 5793.320000000001, "text": " And I've also just enjoyed the response I've had from people who've read it,", "tokens": [50412, 400, 286, 600, 611, 445, 4626, 264, 4134, 286, 600, 632, 490, 561, 567, 600, 1401, 309, 11, 50648], "temperature": 0.0, "avg_logprob": -0.12023619109509039, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.01548830047249794}, {"id": 1026, "seek": 578764, "start": 5794.04, "end": 5801.88, "text": " some of whom have picked out a number of errors, not many. But generally,", "tokens": [50684, 512, 295, 7101, 362, 6183, 484, 257, 1230, 295, 13603, 11, 406, 867, 13, 583, 5101, 11, 51076], "temperature": 0.0, "avg_logprob": -0.12023619109509039, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.01548830047249794}, {"id": 1027, "seek": 578764, "start": 5801.88, "end": 5808.68, "text": " everybody's been very supportive of that and people seem to have responded well to it,", "tokens": [51076, 2201, 311, 668, 588, 14435, 295, 300, 293, 561, 1643, 281, 362, 15806, 731, 281, 309, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12023619109509039, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.01548830047249794}, {"id": 1028, "seek": 578764, "start": 5808.68, "end": 5811.4800000000005, "text": " which I think is always encouraging. And that's what we hope should happen.", "tokens": [51416, 597, 286, 519, 307, 1009, 14580, 13, 400, 300, 311, 437, 321, 1454, 820, 1051, 13, 51556], "temperature": 0.0, "avg_logprob": -0.12023619109509039, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.01548830047249794}, {"id": 1029, "seek": 578764, "start": 5812.04, "end": 5816.360000000001, "text": " Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really", "tokens": [51584, 22768, 13, 1042, 11, 574, 11, 8500, 11, 309, 311, 668, 364, 8236, 5968, 1419, 291, 322, 264, 855, 13, 286, 534, 51800], "temperature": 0.0, "avg_logprob": -0.12023619109509039, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.01548830047249794}, {"id": 1030, "seek": 581636, "start": 5816.36, "end": 5828.92, "text": " appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.", "tokens": [50364, 4449, 291, 1348, 322, 13, 1044, 291, 370, 709, 13, 1042, 11, 1309, 291, 13, 286, 600, 4626, 309, 13, 50992], "temperature": 0.0, "avg_logprob": -0.20766067504882812, "compression_ratio": 1.0263157894736843, "no_speech_prob": 0.01719149760901928}], "language": "en"}