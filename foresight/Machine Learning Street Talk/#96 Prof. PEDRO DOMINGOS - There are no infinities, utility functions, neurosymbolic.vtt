WEBVTT

00:00.000 --> 00:05.720
Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington

00:05.720 --> 00:11.240
and machine learning researcher, probably best known as the author of the master algorithm,

00:11.240 --> 00:14.480
popular science introduction to machine learning.

00:14.480 --> 00:20.680
I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.

00:20.680 --> 00:26.020
The two that I found most interesting and are closest to my own research are Neurosymbolic

00:26.020 --> 00:28.520
AI and symmetry-based learning.

00:28.520 --> 00:33.440
Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.

00:33.440 --> 00:38.840
Pedro is a professor at the University of Washington and give us a quick introduction

00:38.840 --> 00:44.880
to yourself, to your experience here in Europe so far and what's top of mind for you?

00:44.880 --> 00:48.680
I'm a machine learning researcher, I've worked in most of the major areas.

00:48.680 --> 00:54.160
I've also written a popular science book on machine learning called the Master Algorithm.

00:54.160 --> 00:59.200
I'm having a lot of fun here at NeurIPS, listening to various talks like David Chalmers

00:59.200 --> 01:05.320
on Consciousness and Geoff Hinton on Sleep and looking forward to the rest of it.

01:05.320 --> 01:10.480
Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first

01:10.480 --> 01:16.000
thing I wanted to talk about just because it's top of mind is this whole galactica situation.

01:16.000 --> 01:20.360
So first of all, I was speaking with Ian the other day and I think it's a little bit unfair

01:20.360 --> 01:23.720
that Meta really bear the brunt of this.

01:23.720 --> 01:29.440
OpenAI have just released this new chat GPT bot which suffers from similar failure modes

01:29.440 --> 01:34.240
and it just kind of feels that they're not getting anywhere near as much stick as Meta is.

01:34.240 --> 01:42.040
Well I think, I agree with you, I think the brouhaha about galactica is way overblown.

01:42.040 --> 01:44.480
That system is really largely harmless.

01:44.480 --> 01:48.400
It's just another large language model that's designed for actually something that to me

01:48.400 --> 01:50.960
as a scientist is very interesting.

01:50.960 --> 01:55.560
I would love to have a system like that to help me out with certain things and I think

01:55.560 --> 02:00.920
it's a step in the right direction and I think the brouhaha however is an instance of people

02:00.920 --> 02:06.280
jumping the gun on a lot of these AI things in a way that to me is very excessive.

02:06.280 --> 02:11.400
Having said that in a way they set themselves up for it in a way that they need and have,

02:11.400 --> 02:17.560
they kind of over claimed what it did and the problem with these LLMs is that they generate

02:17.560 --> 02:22.560
a lot of stuff that looks good but can be completely wrong and in a way there's no worse

02:22.560 --> 02:25.600
place to do that than in writing scientific articles.

02:25.600 --> 02:30.320
So when they came out with it, they should have been more careful about how they frame

02:30.320 --> 02:31.320
it.

02:31.320 --> 02:34.520
I think they took concerns sort of like this competition and one upping each other on who

02:34.520 --> 02:39.800
comes up with the frilliest demo and that kind of backfired.

02:39.800 --> 02:45.200
So they shouldn't have had to withdraw it.

02:45.200 --> 02:50.840
I think that's all pathetic and hopefully they've learned the lesson that next time

02:50.840 --> 02:52.520
they will do it slightly differently.

02:52.520 --> 02:53.520
Yes.

02:53.520 --> 02:54.520
Okay.

02:54.520 --> 02:55.520
Okay.

02:55.520 --> 02:59.880
Well, so Gary Marcus has been very loud about this on Twitter so he's really pushing

02:59.880 --> 03:05.160
the point about misinformation and the thing is as well I don't want to characterize the

03:05.160 --> 03:09.160
ethics folks as having monolithic views because they don't have monolithic views and I also

03:09.200 --> 03:12.880
think that a lot of the ethics guidelines for large language models are very reasonable.

03:12.880 --> 03:16.840
Like I interviewed the CEO of Coheir the other week, Aidan Gomez.

03:16.840 --> 03:20.800
I went through their terms and conditions and policies all very reasonable.

03:20.800 --> 03:23.360
The only sticking point for me is the misinformation one.

03:23.360 --> 03:31.720
I think the kind of the moral valence of it is in its use and especially with misrepresentation.

03:31.720 --> 03:35.360
I don't like this paternalism telling me what's good for me.

03:35.360 --> 03:38.160
I've just lost out on using a really cool tool basically.

03:38.840 --> 03:39.880
I completely agree with you.

03:39.880 --> 03:41.120
I would take it even further.

03:41.120 --> 03:46.400
I do not want other people deciding for me what is misinformation and what is therefore

03:46.400 --> 03:49.200
allowed to be said because it's misinformation or not.

03:49.200 --> 03:50.200
For a couple of reasons.

03:50.200 --> 03:54.040
One is that these people who claim to be big critics of misinformation, a lot of them

03:54.040 --> 03:56.680
are misinformers themselves.

03:56.680 --> 04:02.120
And the bottom line is that you always have your ideology that informs what you think

04:02.120 --> 04:04.000
is true and false.

04:04.000 --> 04:10.520
And I don't want anybody, every one of us in a democracy should be deciding for themselves

04:10.520 --> 04:13.800
what is true and what is false and what is valid and what isn't.

04:13.800 --> 04:20.320
And I have no fear of attempts to misinform me as long as I have a multiplicity of sources.

04:20.320 --> 04:26.360
The biggest misinformation danger is when you have only one monolithic source of truth,

04:26.360 --> 04:30.760
whatever it is, which is unfortunately what a lot of these anti-misinformation people,

04:30.760 --> 04:33.720
I think consciously or unconsciously want.

04:33.720 --> 04:36.800
Give me 10 things, 9 of which are misinformation.

04:36.800 --> 04:39.960
I can do the job of figuring out which one I think is valid.

04:39.960 --> 04:43.880
Give me only one of those things and chances are 9 in 10 that it is misinformation and

04:43.880 --> 04:45.640
then I have no chance to overcome it.

04:45.640 --> 04:49.880
So this whole attack on things because they're misinformation.

04:49.880 --> 04:55.120
And I mean, I understand the impulse that like, why have all this falsehood flying around?

04:55.120 --> 04:59.080
But the way to overcome that falsehood is not by censoring it.

04:59.080 --> 05:00.880
You should know this, right?

05:00.880 --> 05:05.280
You should be having to refight all of this over again in the context of social media

05:05.280 --> 05:09.200
and large language models and so on.

05:09.200 --> 05:13.000
So you said something really interesting, which is that this notion of a pure truth

05:13.000 --> 05:19.560
or a monolithic truth, and there's this concept of epistemic subjectivity, right?

05:19.560 --> 05:25.240
Or things observe a relative, even complex phenomena like intelligence.

05:25.240 --> 05:26.480
No one understands what it is.

05:26.480 --> 05:28.440
You can't reduce it to one particular thing.

05:28.440 --> 05:29.960
People have different views on it, right?

05:29.960 --> 05:35.440
So this notion that there is a pure monolithic truth of the world, I think is horrifying.

05:35.440 --> 05:39.880
Well, I would put it slightly differently.

05:39.880 --> 05:44.000
So first of all, there's a question, is there one reality or not, right?

05:44.000 --> 05:47.080
Is there truth or is there my truth and your truth, right?

05:47.080 --> 05:51.520
And actually, I understand the impulse to talk about my truth and your truth, but I

05:51.520 --> 05:54.080
think as a...

05:54.080 --> 05:55.200
So what is really true?

05:55.200 --> 05:56.360
We don't know.

05:56.360 --> 05:57.760
But as a...

05:57.760 --> 06:03.880
I think the most useful, including socially useful working hypothesis is that there is

06:03.880 --> 06:08.680
a single reality and the single truth, but it's extremely complex.

06:08.680 --> 06:11.440
So no single one of us can get at it.

06:11.440 --> 06:15.280
So what we need is many different people coming at it from different angles.

06:15.280 --> 06:18.960
But with the premise that we need to try to make these things consistent.

06:18.960 --> 06:23.640
So just saying, oh, we have different truths and there's no reality, that is actually very

06:23.640 --> 06:28.080
counterproductive because it gives everybody a pass to just believe whatever wacky thing

06:28.080 --> 06:29.080
they want.

06:29.080 --> 06:32.840
And then the consequences of that when you have to make the real decisions are very bad.

06:32.840 --> 06:36.640
At the same time, I agree with you.

06:36.640 --> 06:41.080
If I think that I have access to that truth and everybody just needs to, you know, count

06:41.080 --> 06:42.680
out to it, that is very dangerous.

06:42.680 --> 06:46.960
So I think we need to entertain these two ideas that there is a truth, but it's very

06:46.960 --> 06:49.680
complex and no one has a monopoly on it.

06:49.680 --> 06:54.360
And the key is, you know, like objective truth is what different observers can agree on.

06:54.360 --> 06:56.600
And now we can figure out what it is that we agree on.

06:56.600 --> 06:59.280
And that way we make progress in understanding reality.

06:59.280 --> 07:03.800
And we also tend to make more of the right decisions because we're closer to the truth.

07:03.800 --> 07:04.800
Okay.

07:04.800 --> 07:09.600
But do you see it as, I mean, I think the reality thing is interesting, but do you just see

07:09.600 --> 07:12.280
it cynically as gatekeeping?

07:12.280 --> 07:14.520
As in having a clerical class control?

07:14.520 --> 07:15.520
Oh, absolutely.

07:15.520 --> 07:16.520
No, absolutely.

07:16.880 --> 07:22.960
That's precisely the danger that I was referring to is that if you, let me put it this way.

07:22.960 --> 07:27.920
If ever there is, and this is a commonly mooted proposal, right, and not even proposed like,

07:27.920 --> 07:33.280
are we going to have a truth commission of people who decide what is true on whatever,

07:33.280 --> 07:35.160
Twitter or something, right?

07:35.160 --> 07:40.760
That is a really alarming thing because there is no commission that can do that.

07:40.760 --> 07:44.120
What they're going to do is they're going to impose their version of reality on everybody

07:44.120 --> 07:47.640
else, which unfortunately is what a lot of these people want to do.

07:47.640 --> 07:50.600
They convince that they have the truth and they want to impose it on the rest of us.

07:50.600 --> 07:52.040
And that is really alarming.

07:52.040 --> 07:55.200
We know historically what happens when people succeed in doing that.

07:55.200 --> 07:56.200
Yes.

07:56.200 --> 07:57.200
Yes.

07:57.200 --> 08:00.480
But I suppose my point with the gatekeeping is it almost gets you to the actual truth

08:00.480 --> 08:02.120
of the matter is irrelevant.

08:02.120 --> 08:03.760
It's actually about power.

08:03.760 --> 08:07.920
But what's your take on, I don't know whether you think this is putting it too strongly,

08:07.920 --> 08:13.800
but this being a form of industrial kind of gaslighting, kind of, you know, in an Orwellian

08:13.800 --> 08:19.000
sense, trying to shape people's reality through, you know, language, culture and interactions

08:19.000 --> 08:20.520
on the internet.

08:20.520 --> 08:23.600
I think a lot of it is deliberate.

08:23.600 --> 08:29.720
Some of it is, I mean, I'm an optimist about human nature at the end of the day.

08:29.720 --> 08:32.720
Maybe in, you know, maybe with justification, maybe without.

08:32.720 --> 08:38.200
I think so there's this postmodern view that it's all about power and it's certainly partly

08:38.200 --> 08:39.200
about power.

08:39.200 --> 08:44.600
I think a lot of the people doing this, unfortunately, or maybe fortunately, they are, they're not

08:44.600 --> 08:46.880
seeking power for its own sake.

08:46.880 --> 08:49.760
They have a set of beliefs that they think is right.

08:49.760 --> 08:53.120
And then the means, you know, they unjustify the means, right?

08:53.120 --> 08:54.120
That's the problem.

08:54.120 --> 09:01.480
So that gatekeeping, you know, and that gaslighting happen not because, not for their own sake,

09:01.480 --> 09:02.880
they happen for the sake of a cause.

09:02.880 --> 09:07.080
And now there's two problems with this is that these days the causes on behalf of which

09:07.080 --> 09:10.440
this is being done, in my view, are largely wrong, right?

09:10.440 --> 09:12.160
But whether they're right or wrong, right?

09:12.160 --> 09:15.480
The problem is that this is just noxious in its own right.

09:15.480 --> 09:21.840
And also then a lot of sort of like, again, personal desire for power and promotion and

09:21.840 --> 09:22.840
prevailing over others.

09:22.840 --> 09:25.080
Then of course, you know, hitches are right onto this.

09:25.080 --> 09:26.080
Yeah, interesting.

09:26.080 --> 09:30.000
I mean, we'll get into consequentialism in a minute because I think there's quite an

09:30.000 --> 09:31.720
interesting journey we can go there.

09:31.720 --> 09:35.560
But I wanted to cite Francois Chollet, I'm a big fan of him.

09:35.560 --> 09:39.760
He just tweeted saying, I'm not too concerned of whether what I read is right or wrong.

09:39.760 --> 09:42.040
I can figure that part out myself.

09:42.040 --> 09:46.040
I'm interested in things that are useful, thought-provoking, novel.

09:46.040 --> 09:49.200
Sometimes the most creative thinkers have a bias towards wrongness, but they're still

09:49.200 --> 09:50.200
worth reading.

09:50.200 --> 09:51.200
Would you agree with that?

09:51.200 --> 09:52.680
Yes, I largely agree with that.

09:52.680 --> 09:59.080
So as I was saying before, you know, I can tell for myself or I can do that exercise

09:59.080 --> 10:00.840
of figuring out what is right and wrong.

10:00.840 --> 10:06.000
The most important thing that I want is to not miss out on things that I don't want to

10:06.000 --> 10:07.000
miss out on.

10:07.000 --> 10:11.320
Like, you know, the known unknowns and the unknown unknowns, the biggest killer is the

10:11.320 --> 10:12.320
unknown unknowns.

10:12.320 --> 10:17.540
So if anybody trying to learn or understand something, they're for person or organization

10:17.540 --> 10:19.040
or society, right?

10:19.040 --> 10:23.800
If all they do is move the unknown unknowns to known unknowns, they've already gone an

10:23.800 --> 10:25.840
enormous distance, right?

10:25.840 --> 10:30.640
And so I appreciate people who I disagree with, first of all, because that's how you sharpen

10:30.640 --> 10:31.640
ideas.

10:31.640 --> 10:32.640
Yeah.

10:32.640 --> 10:38.280
But also because they may just bring things to my attention that if we were all conforming

10:38.280 --> 10:42.520
to the more majority view, would not come to our attention.

10:42.520 --> 10:45.000
And then those more often than not are the ones that kill you.

10:45.000 --> 10:46.000
Yeah, that's so interesting.

10:46.000 --> 10:49.760
I mean, there's a real analogy here, even this might be tenuous, but between symbolism

10:49.760 --> 10:54.840
and connectionism or, you know, Rich Sutton said we shouldn't be hand-crafting our AI

10:54.840 --> 10:55.840
systems.

10:55.840 --> 10:57.000
We should kind of let them emerge.

10:57.000 --> 11:00.040
And it's a similar thing with our moral framework, but you're kind of saying that it should be

11:00.040 --> 11:05.200
emergent from low level complexity and diversity and interestingness.

11:05.200 --> 11:08.280
And there is another school of thought, which is that we should be top down and we already

11:08.280 --> 11:09.440
have a representation.

11:09.440 --> 11:13.600
I actually think it should be, it needs to be a combination.

11:13.600 --> 11:15.320
We need to have both.

11:15.320 --> 11:18.560
This is one of those debates that in some sense puzzles me because to me the obvious

11:18.560 --> 11:20.880
answer is that we need both.

11:20.880 --> 11:23.160
And then if you read the master argument, this is what I do.

11:23.160 --> 11:26.720
I look at the different paradigms of machine learning and I don't come out in favor of

11:26.720 --> 11:30.480
any of them because I actually think we need ideas from all of them.

11:30.480 --> 11:32.840
And then we need to combine them into something coherent.

11:32.840 --> 11:36.960
And if you look at psychology, like your brain does bottom up and top down processing.

11:36.960 --> 11:40.400
And if it only did one of them, either one, it wouldn't work.

11:40.400 --> 11:43.880
And I think as we try to build a larger intelligence, it's the same thing.

11:43.880 --> 11:48.160
We definitely need the bottom up part and, you know, by volume, the bottom up part is

11:48.160 --> 11:49.160
going to be bigger.

11:49.160 --> 11:52.960
So if you could only have one, that would probably be, you know, the choice.

11:52.960 --> 11:56.200
But the top down part is also very important.

11:56.240 --> 12:00.440
If you go all the way back, the top down part probably started this bottom up and got synthesized

12:00.440 --> 12:01.440
and improved.

12:01.440 --> 12:02.880
But now we need that loop.

12:02.880 --> 12:04.320
The loop is actually very important.

12:04.320 --> 12:05.320
Well, that's interesting.

12:05.320 --> 12:09.600
So in your book, I guess I want to sketch out different types of AI architecture.

12:09.600 --> 12:12.840
So, you know, you get universalists to this kind of deep mind idea that a very simple

12:12.840 --> 12:15.000
underlying algorithm could produce everything.

12:15.000 --> 12:18.000
And then you get, you know, hybrid folks on the other side of the spectrum.

12:18.000 --> 12:20.000
And then there's an integrated approach in the middle.

12:20.000 --> 12:22.680
Like, where would you kind of place yourself on that continuum?

12:22.680 --> 12:28.480
I would place myself very much in the frame of mind, well, let me put it this way.

12:28.480 --> 12:31.320
I don't know, but which is nobody does, right?

12:31.320 --> 12:34.640
If somebody tells you that they know how we're going to get to intelligence, you should be

12:34.640 --> 12:36.200
suspicious right away.

12:36.200 --> 12:40.920
But what do I think is the most promising approach and the one that ideally would be

12:40.920 --> 12:42.600
the best one if we can pull it off?

12:42.600 --> 12:44.520
It's there being a single algorithm.

12:44.520 --> 12:49.240
So at that level, I very much sympathize with what is effectively the deep mind's agenda.

12:49.240 --> 12:55.080
Now, we're at part with a lot of these people is that I don't think the algorithm that we

12:55.080 --> 12:59.240
need is as simple as many of those people think it is.

12:59.240 --> 13:01.760
And I don't think it exists.

13:01.760 --> 13:05.840
It is probably the case that the algorithm that we really need at the end of the day

13:05.840 --> 13:09.040
doesn't even look that much like any of the things that we have now.

13:09.040 --> 13:12.480
So I think hopefully there is such an algorithm, but we're still far from it.

13:12.480 --> 13:13.480
Interesting.

13:13.480 --> 13:17.760
I mean, they would cite the example of evolution as being a very simple underlying algorithm.

13:17.760 --> 13:22.040
Although Ken Stanley would say that people misunderstand evolution.

13:22.040 --> 13:23.720
So I agree with them at that level.

13:23.720 --> 13:27.920
In fact, in the master algorithm, I have a chapter where I go over the objections and

13:27.920 --> 13:31.040
the reasons to believe that there is a master algorithm.

13:31.040 --> 13:35.720
The majority of the people even in the field are skeptical of that notion, even though

13:35.720 --> 13:38.640
I would claim that effectively that's what they're pursuing.

13:38.640 --> 13:41.680
People like Rich Sutton and Jeff Hitton, I asked a bunch of people before I wrote the

13:41.680 --> 13:45.000
book and they do believe in this idea of having a master algorithm.

13:45.000 --> 13:49.920
A lot of people believe that, but intuitively a lot of people believe that no, there is

13:49.920 --> 13:51.560
no such thing.

13:51.560 --> 14:00.920
And I understand that intuition, but I don't think it's a well-founded intuition.

14:00.920 --> 14:01.920
Let me put it that way.

14:01.920 --> 14:05.520
But in a sense, we know there is such a thing, because look at cellular automata, look at

14:05.520 --> 14:07.360
what we've already done with deep learning.

14:07.360 --> 14:11.440
I think the context is, is there such a thing that will produce what we want?

14:12.440 --> 14:21.720
To take your example or DeepMind's example of evolution, in the book I mentioned empirical

14:21.720 --> 14:25.840
evidence that there is a master algorithm and exhibit one is evolution.

14:25.840 --> 14:28.880
If you think of evolution as an algorithm, which by the way is a very old idea, I think

14:28.880 --> 14:35.200
it was George Bull that said, God does not create animals and plants, he creates the

14:35.200 --> 14:38.320
algorithm by which animals and plants come about.

14:38.600 --> 14:41.720
He didn't use the word algorithm, but that's essentially what he said.

14:41.720 --> 14:43.920
This I think is right on.

14:43.920 --> 14:46.520
Another example is your brain.

14:46.520 --> 14:50.480
If the algorithm doesn't have to be something as simple as backprop and you're a materialist

14:50.480 --> 14:54.640
like most of the scientists are, your brain, if the master algorithm is an algorithm that

14:54.640 --> 14:58.200
can learn anything you do, then your brain is that algorithm.

14:58.200 --> 15:02.600
But then there's another one which is even more fundamental, but I think from the point

15:02.600 --> 15:07.160
of view of this debate is very illuminating, which is the laws of physics.

15:07.160 --> 15:08.680
Why stop at evolution?

15:08.680 --> 15:11.480
The laws of physics are the master algorithm.

15:11.480 --> 15:13.080
Evolution is very complicated.

15:13.080 --> 15:17.800
In fact, what I think about evolution in AI currently is that evolution in reality is

15:17.800 --> 15:21.760
much more complex than we give it credit for, which is why a lot of our current generic

15:21.760 --> 15:23.320
algorithms don't work that well.

15:23.320 --> 15:26.560
But the laws of physics at this level are much simpler.

15:26.560 --> 15:31.480
If you think about it, from the laws of physics comes evolution and comes all the intelligence

15:31.480 --> 15:33.480
that we have.

15:33.480 --> 15:37.160
It's very intriguing why that happens and why the laws are such that that happens.

15:37.160 --> 15:40.160
But even just the laws of physics are already a master algorithm.

15:40.160 --> 15:44.680
Now what you could say, and many people immediately say is like, oh, but if you start from there,

15:44.680 --> 15:46.200
you'll never get anywhere, right?

15:46.200 --> 15:52.280
But then you can say evolution is the laws of physics sped up in a certain direction.

15:52.280 --> 15:55.120
And then our reinforcement learning is like evolution.

15:55.120 --> 15:57.920
People have pointed out the same way except it's faster.

15:57.920 --> 16:01.680
And in a way what we're trying to do now in machine learning is the same thing yet again

16:01.720 --> 16:03.920
except only even faster.

16:03.920 --> 16:05.520
But what are the consequences of?

16:05.520 --> 16:08.760
I mean, let's say it is actually a very high resolution algorithm.

16:08.760 --> 16:13.600
So it's something that appears to be completely unintelligible to in respect of the output

16:13.600 --> 16:14.480
phenomena.

16:14.480 --> 16:16.240
Is that is that even a good place to be?

16:16.240 --> 16:20.800
Because, you know, just like with cellular automata, there's no real paradigmatic

16:20.800 --> 16:23.600
relationship between the underlying rules and the emergent phenomena, right?

16:23.600 --> 16:26.400
So is that really even something we want?

16:26.400 --> 16:27.520
No, I think there is.

16:27.520 --> 16:29.600
So we don't know.

16:29.600 --> 16:33.360
But I think people and this is very common among connections is to say this stuff is

16:33.360 --> 16:36.520
also complex that we can't possibly have a handle on it.

16:36.520 --> 16:38.000
We just have to let it happen.

16:38.000 --> 16:41.840
And I think that is not giving enough credit to our human brains, right?

16:41.840 --> 16:46.320
We are incredibly good at making sense of things that in the beginning, I mean, over

16:46.320 --> 16:50.280
and over and over in the history of science and technology, you start out with things

16:50.280 --> 16:52.600
that you don't understand very well at all.

16:52.600 --> 16:57.320
But then over time, we kind of change our representation of the world to make those

16:57.320 --> 16:59.520
things actually be intelligible to us.

16:59.520 --> 17:03.480
And we should not a priori assume that that's not going to be the case here.

17:03.480 --> 17:08.800
So, for example, cellular automata, amazing things come out of whatever the game of life

17:08.800 --> 17:12.840
that seemed completely disconnected from those, but they aren't, right?

17:12.840 --> 17:18.160
And, you know, there's various depths at which you could go into this.

17:18.160 --> 17:22.400
There will probably at the end of the day be some large element of this that we can't

17:22.400 --> 17:26.800
figure out very well, but we can figure out enough that we have a handle on it.

17:26.800 --> 17:31.400
So, this singularity notion that at some point AI is just completely beyond our understanding.

17:31.400 --> 17:33.040
I tend not to buy.

17:33.040 --> 17:35.600
I don't think it will be completely beyond our understanding.

17:35.600 --> 17:40.320
But it's an analog back to our Twitter discussion, like, because we can only understand it through.

17:40.320 --> 17:43.480
It's like having views on a mountain range, you know, the view looks different depending

17:43.480 --> 17:45.080
on where you're standing.

17:45.080 --> 17:48.760
And it's the same thing with the emergent phenomena in a cellular automata.

17:48.760 --> 17:49.760
No, very good.

17:49.760 --> 17:54.400
And, you know, the classic example of this is the blind man and the elephant, right?

17:54.400 --> 17:57.920
And that's actually the metaphor that is in the book, as I say, you know, the different

17:57.920 --> 18:00.640
tribes are like different blind men, right?

18:00.640 --> 18:03.480
But precisely so, AI is one of the blind men.

18:03.480 --> 18:07.800
I can see part of the elephant, but it'd be who's meant to also talk to you who see another

18:07.800 --> 18:08.800
part of the elephant.

18:08.800 --> 18:12.600
And then each of us understands a little bit more of the elephant than we would if we were

18:12.600 --> 18:13.600
on our own.

18:13.600 --> 18:18.800
But most importantly, we collectively, which is what really matters, actually understand

18:18.800 --> 18:23.840
maybe not the elephant completely, but much more of the elephant than either any of us

18:23.960 --> 18:25.560
would alone, right?

18:25.560 --> 18:28.320
And it's certainly a lot better than just giving up and say, like, oh, we're never going to

18:28.320 --> 18:30.560
understand this strange thing that's in front of us.

18:30.560 --> 18:31.560
Interesting.

18:31.560 --> 18:33.080
But that's a great argument to what you were saying before.

18:33.080 --> 18:35.880
So it's beyond our cognitive horizon.

18:35.880 --> 18:38.680
Therefore we need to have diversity of aspect.

18:38.680 --> 18:42.360
There's a, yes, there's a question of whether it's beyond the cognitive ability of a single

18:42.360 --> 18:43.360
human.

18:43.360 --> 18:44.360
Yeah.

18:44.360 --> 18:46.400
And then there's the question of whether it's beyond the cognitive ability of an entire

18:46.400 --> 18:47.920
society of humans.

18:47.920 --> 18:51.840
And obviously, there'll be things that are beyond the cognitive ability of a single human,

18:51.840 --> 18:53.920
but not beyond the cognitive ability of a society.

18:53.920 --> 18:56.280
Also these days, we have computers.

18:56.280 --> 18:58.680
So our cognitive power is augmented by our machine.

18:58.680 --> 19:03.320
So we can understand things or bring things to the point where we understand them to a

19:03.320 --> 19:05.640
degree today that we couldn't a hundred years ago.

19:05.640 --> 19:07.200
Right now, that is a fascinating point.

19:07.200 --> 19:13.080
So it's beyond our cognitive horizon individually, but it might not be beyond the cognitive horizon

19:13.080 --> 19:17.080
of loads and loads of humans on the internet, you know, the wisdom of crowds.

19:17.080 --> 19:21.160
But we don't, I mean, how do we know that the crowd understands?

19:21.160 --> 19:26.320
But we know, well, that's the, in some sense, the beauty of this, right, is that we never,

19:26.320 --> 19:28.040
what is the crowd really understanding, right?

19:28.040 --> 19:31.920
And again, once the crowd is augmented by machines, like machine learning algorithms,

19:31.920 --> 19:36.800
right, we can ask what do we as a society equipped with all of our, you know, large

19:36.800 --> 19:40.120
language models and so on and so forth, what do we really understand, right?

19:40.120 --> 19:44.480
Now, at some level, you can't answer that question individually because you are just

19:44.480 --> 19:48.120
an individual, but right, there's a couple of very important things that we shouldn't

19:48.120 --> 19:49.120
forget.

19:49.240 --> 19:54.080
You could, one thing you can do and that I do do is say, like, do I now actually even

19:54.080 --> 19:59.320
just individually understand things better than I did before when it was just me looking

19:59.320 --> 20:00.320
at it?

20:00.320 --> 20:02.760
And the answer to that is almost invariably yes, right?

20:02.760 --> 20:05.000
So there is a big game to be heard there.

20:05.000 --> 20:11.440
And the second one is that you, ultimately, you tell by the consequences, right?

20:11.440 --> 20:14.120
And like, for example, take a deep network, right?

20:14.120 --> 20:17.920
And you may not know how it works, but if it's doing medical diagnosis, you can tell

20:17.920 --> 20:22.520
whether it, you know, gets the diagnosis right more often than it did before or more

20:22.520 --> 20:23.520
often than another model.

20:23.520 --> 20:28.920
So we as a society may not, you know, we individuals may not very understand very well what we

20:28.920 --> 20:32.880
as a society understand, but we can see the consequences and at some level, that's the

20:32.880 --> 20:33.880
point.

20:33.880 --> 20:34.880
Yeah.

20:34.880 --> 20:37.760
So on that, I mean, that sounds like a bit of an appeal to behaviorism and we're going

20:37.760 --> 20:39.760
to talk about that in respect of charmers as well.

20:39.760 --> 20:43.840
But it also brings us back to, you know, we were talking about empiricism versus rationalism

20:43.840 --> 20:46.600
and nativism and all of these topics.

20:46.600 --> 20:51.200
Would you place yourself in that camp of being a nativist and a rationalist or completely

20:51.200 --> 20:52.200
the other way?

20:52.200 --> 20:53.440
No, absolutely not.

20:53.440 --> 20:59.040
Again, this is one of the points that I, you know, go back to is there are the empiricists

20:59.040 --> 21:03.640
and there are the rationalists and you could see naively machine learning as being the

21:03.640 --> 21:08.000
triumph of the empiricists, but it actually is not there are very fundamental reasons

21:08.000 --> 21:09.000
why it's not.

21:09.000 --> 21:12.720
And I really do think, and this is not just think there's this thing called the no free

21:12.720 --> 21:14.560
lunch theorem, right?

21:14.560 --> 21:18.840
And if you take those things seriously, the solution has to be a combination of empiricism

21:18.840 --> 21:19.840
and rationalism.

21:19.840 --> 21:23.840
I don't think either side alone has or even can have the whole answer.

21:23.840 --> 21:26.200
So very much we need both of those.

21:26.200 --> 21:31.160
And if you're a pure empiricist or a pure rationalist, I'm already suspicious of you.

21:31.160 --> 21:32.160
Wonderful.

21:32.160 --> 21:36.640
Coming back to what Franz Walsh said in his quote, he said, you know, producing things

21:36.640 --> 21:39.360
that are thought provoking novel and all the rest of it.

21:39.360 --> 21:42.360
And I was speaking to some alignment folks yesterday and we'll pivot to that in a minute.

21:42.960 --> 21:47.160
The big thing for me after doing an episode on sales, the Chinese room is, you know, where

21:47.160 --> 21:51.640
does intentionality come from and Chomsky talks about agency, for example, we do things

21:51.640 --> 21:54.200
that are appropriate to the situation, but not caused by them.

21:54.200 --> 21:57.280
So from my perspective, all these generative models, all these large language models and

21:57.280 --> 22:01.120
so on, the creativity, the real spark of genius still comes from us, right?

22:01.120 --> 22:06.680
We've just kind of like, you know, the boring bit of actually doing the task is now delegated

22:06.680 --> 22:08.240
to the algorithm.

22:08.240 --> 22:09.600
I would disagree with that.

22:09.600 --> 22:14.360
I think you are, I mean, your position is very reasonable and actually, I would say

22:14.360 --> 22:19.880
probably the most common, but I think when you do that, you are giving us too much credit

22:19.880 --> 22:22.920
and the large language models too little.

22:22.920 --> 22:27.840
We tend to have this notion that creativity is something magical.

22:27.840 --> 22:32.240
In fact, I remember for many years, so quick parenthesis, in the previous life I was a

22:32.240 --> 22:33.240
musician.

22:33.240 --> 22:37.840
So, you know, I, in some sense know about, and a lot of my job was composing songs, right?

22:37.840 --> 22:41.880
And I was always, at the same time I was already studying AI and I couldn't help but connect

22:41.880 --> 22:42.880
it too, right?

22:42.880 --> 22:48.560
And think about like, what would an AI look that was able to compose music, right?

22:48.560 --> 22:53.480
And talking to late people who are not musicians, they think that composing songs is some kind

22:53.480 --> 22:58.600
of magic thing that comes from, you know, whatever the great beyond, and it's not.

22:58.600 --> 23:02.280
It's a very human enterprise and it can very well be automated.

23:02.280 --> 23:06.600
It's actually now, you know, people, I used to say to people like, people always say like,

23:06.600 --> 23:10.120
well, creativity will be the last thing that we automate because we humans can do it and

23:10.120 --> 23:12.920
there's no machine school and be like, no, it's going to be the opposite.

23:12.920 --> 23:16.920
You automate creativity long before many other things and we're there now, right?

23:16.920 --> 23:23.680
In just the last, so I think when you, let me put it this way, your prompt to the LLM,

23:23.680 --> 23:28.200
let's say, is like the grain of sand to the oyster, right?

23:28.200 --> 23:31.680
You should not give yourself credit for having made the pearl because it put the grain of

23:31.680 --> 23:32.680
sand in there.

23:32.680 --> 23:34.960
That's a, that's a brilliant analogy, right?

23:34.960 --> 23:40.440
So it is still the LLM, we need to, we can critique how creative it is or not and there's

23:40.440 --> 23:44.200
a lot to be said there and a lot of progress to be made, but we need to give it credit

23:44.200 --> 23:45.600
for what it does, right?

23:45.600 --> 23:47.960
It is well or not so well, right?

23:47.960 --> 23:52.680
Maybe it's more of an illusion that we're giving credit for and whatnot, but that text

23:52.680 --> 23:57.760
or that image or whatever, they were created by the AI.

23:57.760 --> 24:01.440
And in many ways, the thing that was created by the AI is no worse than what would have

24:01.440 --> 24:05.240
been created by an artist if I gave them the prompt.

24:05.240 --> 24:06.240
No, okay.

24:06.240 --> 24:07.240
Well, on that, I agree with you.

24:07.240 --> 24:10.560
I mean, Melanie Mitchell had this wonderful anecdote from the Google Plex when she was

24:10.560 --> 24:14.080
with Douglas Hofstadter and he was talking at the time about, you know, how he would

24:14.080 --> 24:18.920
be devastated if an AI could produce a Chopin piece, you know, which was indistinguishable

24:18.920 --> 24:24.520
from one which he actually created and of course, that did happen, but then we get into

24:24.520 --> 24:27.360
this discussion of where does it start, right?

24:27.360 --> 24:29.040
Where does it start?

24:29.040 --> 24:30.920
Computers only do what we tell them to do, right?

24:30.920 --> 24:35.160
They've been trained and actually I was speaking to Sep about this the other day that, you

24:35.160 --> 24:38.560
know, all of the abstractions, all of the things that the computers and the models do,

24:38.560 --> 24:42.680
they are crystallized snapshots of things that humans have previously done and we've

24:42.680 --> 24:44.920
written the computer code.

24:44.920 --> 24:46.720
So where does the creativity start?

24:46.720 --> 24:53.880
Well, but we, by that standard, we humans also only do what we're told to do.

24:53.880 --> 24:55.640
We do what we're told to do by our genes.

24:55.640 --> 24:59.040
Our genes do what they're told to do by evolution, which does what is told to do by the laws

24:59.040 --> 25:00.040
of physics, right?

25:00.160 --> 25:01.160
Right.

25:01.160 --> 25:05.680
And now, again, this gets back to this notion that there's nothing magical about creativity.

25:05.680 --> 25:10.920
Creativity really is, to a large extent, cutting and pasting stuff and satisfying consistency

25:10.920 --> 25:12.320
constraints between them.

25:12.320 --> 25:16.720
And I'm not just saying this in the abstract, like long before the modern era, there's this

25:16.720 --> 25:23.120
guy called David Cope, you know, a composer and professor of music at UC Santa Cruz who

25:23.120 --> 25:28.760
created these programs that exactly they would write, they can write, this was pre-machine

25:28.760 --> 25:29.760
learning.

25:29.760 --> 25:34.520
It was list code that what it did was basically have rules about how music should be.

25:34.520 --> 25:37.200
And then it takes snippets and combines them, right?

25:37.200 --> 25:40.720
You could say it's just parroting those bits, but the truth is at the end of the day and

25:40.720 --> 25:41.720
you can choose.

25:41.720 --> 25:44.080
You say, like, give me something in the style of Mozart.

25:44.080 --> 25:50.040
And it creates something that looks indistinguishable from what Mozart did, but all it's doing

25:50.040 --> 25:52.640
is this kind of recombination of pieces.

25:52.640 --> 25:59.440
So we humans, we have too much respect for appreciation of our own intelligence.

25:59.440 --> 26:01.000
That's also what we're doing.

26:01.000 --> 26:02.760
Yeah, I think I agree with you.

26:02.760 --> 26:06.760
I mean, first of all, intelligence is a receding horizon and there's the McCorduck effect.

26:06.760 --> 26:08.160
I agree with all of that.

26:08.160 --> 26:14.180
But yeah, I think it's a similar thing to how we anthropomorphise large language models

26:14.180 --> 26:17.320
and even, you know, it's tempting to say large language models are slightly conscious

26:17.320 --> 26:21.760
and we'll talk about that in a minute, but maybe like we also anthropomorphise our own

26:21.760 --> 26:22.760
agency, right?

26:22.760 --> 26:26.480
We have like a little bubble around ourselves and we kind of delude ourselves that we exist

26:26.480 --> 26:31.320
as an individual unit with agency disconnected from the rest of the world.

26:31.320 --> 26:37.400
Well, precisely the problem with how we largely take AI today, this has always been the case,

26:37.400 --> 26:42.760
by the way, is that we have a new resistible notion to anthropomorphise anything that behaves

26:42.760 --> 26:44.200
even remotely like us.

26:44.200 --> 26:48.800
We're the only intelligent things that we know, so if something starts behaving intelligently,

26:48.800 --> 26:53.240
then we project onto it all of these other human characteristics.

26:53.240 --> 26:55.400
Same with consciousness, same with creativity.

26:55.400 --> 26:57.600
We don't know anything else that's creative besides us.

26:57.600 --> 27:01.840
So once a machine starts behaving creatively, we cannot help but project a lot of things

27:01.840 --> 27:02.840
onto it.

27:02.840 --> 27:04.840
It's just reasoning by analogy, right?

27:04.840 --> 27:09.320
So it's a kind of analogical, so like you're like me in this respect, so you probably are

27:09.320 --> 27:10.320
in this other spec.

27:10.320 --> 27:13.760
Now, the good news is that we always start out with this kind of very good reasoning

27:13.760 --> 27:17.920
by analogy, but after a while, we actually start to build a model of the real thing.

27:17.920 --> 27:23.320
So AI for the public at large right now is very new, but gradually we'll come to a point

27:23.320 --> 27:29.120
where we zero in on what AI really is rather than just the shallow analogies that we initially

27:29.120 --> 27:30.120
used to try to understand.

27:30.120 --> 27:31.120
Okay.

27:31.120 --> 27:32.560
Well, I'll try it from a slightly different angle.

27:32.560 --> 27:36.040
So we were just saying Seoul makes the argument that it's a biological property and that's

27:36.040 --> 27:40.480
where intentionality and consciousness comes from and it's a requisite, but we'll leave

27:40.480 --> 27:42.160
that for the time being.

27:42.160 --> 27:47.720
Let's go the Fodor and the Gary Marcus and the Chomsky group, and they would argue that

27:47.720 --> 27:53.400
creativity is basically this notion of, or even analogy making by extension, is this

27:53.400 --> 27:58.440
notion of being able to select from a set which has an infinite cardinality.

27:58.440 --> 28:02.840
And as you know, neural networks can't represent infinite sets because they're finite state

28:02.840 --> 28:06.800
automators, therefore they make the move we need to have this compositionality.

28:06.800 --> 28:07.800
What do you say to that?

28:07.800 --> 28:09.920
Well, there's a lot to unpack there.

28:09.920 --> 28:13.040
I think we definitely need compositionality, right?

28:13.040 --> 28:16.840
If somebody asked me, make a list of how there's some things that are actually essential

28:16.840 --> 28:20.400
for intelligence, compositionality would be one of them, right?

28:20.400 --> 28:25.040
And this, of course, is the thing that people like Chomsky and Gary are not really care

28:25.040 --> 28:26.040
about, right?

28:26.040 --> 28:31.560
Having said that, I think first of all, there is no such thing as an infinite set, right?

28:31.560 --> 28:40.320
Like infinite set is a useful but extremely dangerous and confusing mathematical tool,

28:40.320 --> 28:41.320
right?

28:41.960 --> 28:46.520
There is no such thing as an infinite anything and there never will be.

28:46.520 --> 28:50.880
So I would just raise this at, well, yes, creativity and almost anything we can do in

28:50.880 --> 28:56.680
AI is selecting from a very large set, not infinite, but very large, right?

28:56.680 --> 29:00.480
And now, but now we don't just select like one full element at a time.

29:00.480 --> 29:04.280
We compose it out of pieces and that's actually where the intelligence comes in.

29:04.280 --> 29:05.280
Interesting.

29:05.280 --> 29:07.760
I don't want to go too far down the digital physics route, but we did just have Yoshua

29:07.840 --> 29:12.480
Barkon and I mean, just to reclamify on that, would you place yourself in that camp that

29:12.480 --> 29:15.040
the universe is digital and made of information?

29:15.040 --> 29:17.120
Valid question.

29:17.120 --> 29:20.280
I certainly think the universe is finite.

29:20.280 --> 29:27.920
I think, I mean, like Seth Lloyd says, the universe is a computer, right?

29:27.920 --> 29:34.440
And I think that is true or false depending on what you take the word computer to mean,

29:34.440 --> 29:35.440
right?

29:36.440 --> 29:42.920
So if you say that the universe is digital or is a computer as kind of like an analogy

29:42.920 --> 29:46.120
that lets us understand it better, I'm all for that.

29:46.120 --> 29:49.280
I don't think the universe is little, you know, here's the way to put this.

29:49.280 --> 29:51.080
The universe is a computation.

29:51.080 --> 29:55.120
Like, I don't know what the computer is or if there is one.

29:55.120 --> 30:00.160
Now the universe is digital in the sense that deep down at the most basic level, the universe

30:00.160 --> 30:02.120
is made of discrete things.

30:02.360 --> 30:05.880
OK, is this like the it from bit, the John Wheeler type hypothesis?

30:06.960 --> 30:08.440
Yes.

30:08.440 --> 30:13.360
I mean, if you read that paper, it is, I mean, John Wheeler was a brilliant person.

30:14.280 --> 30:19.720
Again, very, you know, to get back to François Chalice's tweet, he was very good at coming

30:19.720 --> 30:21.760
up with his provocative notions, right?

30:21.760 --> 30:25.040
And the it from the thing, of course, is like newly unveiled today.

30:25.440 --> 30:30.760
And I do so at that level, I do agree that looking at the universe is being made of

30:30.760 --> 30:32.960
information is very useful.

30:33.040 --> 30:38.520
And in particular, if you want a grand unified master algorithm, in some sense, the only

30:38.520 --> 30:43.120
way that I at least can see of doing that is by seeing everything as information.

30:43.680 --> 30:48.240
So I think, and if I do something that I am working on that, looking at everything as

30:48.240 --> 30:51.000
information is a very productive thing to do.

30:51.280 --> 30:57.040
Yeah, but but my caution is that information is one aspect of everything.

30:57.680 --> 31:00.960
So I can give you a theory of everything that's based on information.

31:01.400 --> 31:03.560
But it's not truly a theory of everything.

31:03.560 --> 31:05.520
It's a theory of one aspect of everything.

31:06.080 --> 31:07.520
And I think there's a lot to be done there.

31:07.520 --> 31:11.280
But again, we shouldn't forget what we're living out when we focus on that aspect.

31:11.640 --> 31:13.920
Yeah, I mean, we've spoken a lot on the show about, you know,

31:14.480 --> 31:18.720
Penrose's view and obviously Sal's view that arises from from biology.

31:18.720 --> 31:22.680
And I know if Keith was here, he would argue strongly that he believes in in

31:22.680 --> 31:27.840
continuum, and therefore we would need, you know, hyper computation to have this

31:27.840 --> 31:31.160
universe. That would be an interesting discussion to have, because I really don't

31:31.160 --> 31:35.440
see where there is physical or any evidence for continuum of any kind.

31:35.680 --> 31:39.880
The evidence is always that continuum are a useful approximation, but always

31:39.880 --> 31:42.000
underlying the continuum is a discrete reality.

31:42.240 --> 31:44.880
You take a sensor of anything, right?

31:44.880 --> 31:48.080
You know, quantum mechanics is like the quintessential example of this, right?

31:48.520 --> 31:52.240
What do we measure at the end that it's always discrete events?

31:52.680 --> 31:58.960
Right? Like it's the detection of a photon by, you know, by whatever detector, right?

31:58.960 --> 32:01.560
Could be a model of Dobson or a CCD or whatever.

32:01.800 --> 32:03.640
But it's a it's a it's a change of state.

32:03.640 --> 32:04.680
It really is a bit.

32:05.320 --> 32:07.080
Oh, interesting. Well, how would you contrast that?

32:07.080 --> 32:10.960
You know, Stephen Wolfram has got this idea of of digital physics and, you know,

32:11.280 --> 32:15.720
maybe and again, unfortunately, we have to use arguments from behavior, you know,

32:15.720 --> 32:19.440
to kind of say, well, we've we've got potentially a graph cellular automata

32:19.680 --> 32:23.280
and it creates this beautiful emergent structure, which is very much like the universe.

32:24.120 --> 32:28.760
But, you know, Scott Aronson would make the argument that he's discounting quantum mechanics.

32:28.760 --> 32:31.240
I mean, what would you say to that?

32:31.240 --> 32:35.800
So I think Steve Wolfram's theory is very interesting.

32:36.280 --> 32:39.560
And he gets some things right that a lot of other physicists don't,

32:40.440 --> 32:43.600
in particular, that the universe at heart is discrete.

32:44.360 --> 32:47.320
So I'm very much with him on that aspect of his agenda.

32:48.320 --> 32:51.240
And thank God there's someone like him and a number of others,

32:51.960 --> 32:53.640
you know, going that route, right?

32:53.640 --> 32:55.240
They're the minority in physics.

32:55.240 --> 32:58.120
But actually, I think if you look at just what has happened in the last 10 years,

32:58.320 --> 33:00.160
things are very much moving in this direction.

33:00.280 --> 33:02.400
And I think they're going to move more, right?

33:02.720 --> 33:07.680
Now, having said that, his specific theory, I think has a lot of shortcomings.

33:08.520 --> 33:13.840
And I don't think it's the ultimate theory or maybe even the best path to a theory,

33:14.600 --> 33:16.200
you know, to a discrete theory of the universe.

33:16.640 --> 33:22.120
Scott Aaronson's critique in that regard, I think misses the point, right?

33:22.440 --> 33:24.840
It's interesting because it's interesting that you should like pair those two

33:24.840 --> 33:28.800
because Steve Wolfram, in a way, is a physicist to become a computer scientist.

33:28.800 --> 33:30.640
And Scott is the other way around, right?

33:30.840 --> 33:32.920
And I think, you know, like I greatly admire both of them

33:32.920 --> 33:34.400
and I'm friends with both of them.

33:34.400 --> 33:36.400
I've had many discussions with them.

33:36.400 --> 33:40.800
I think, you know, just to very, you know, cruelly caricature things in a way,

33:40.800 --> 33:44.320
the problem with Steve is that he has bought into the computer science

33:44.320 --> 33:45.960
assumptions too much.

33:45.960 --> 33:49.400
And the problem with Scott is that he has bought into the quantum physics

33:49.400 --> 33:50.760
assumptions too much, right?

33:50.760 --> 33:54.200
So if you really think carefully and rigorously about quantum mechanics,

33:54.480 --> 33:59.480
all that continuous mathematics is there just to make discrete predictions.

34:00.280 --> 34:01.760
So the continuity may be useful.

34:01.760 --> 34:02.520
It is useful.

34:02.520 --> 34:05.640
I'm not arguing against the use of continuity and infinity in our mathematics.

34:05.640 --> 34:07.520
In fact, we'd be nowhere if we didn't have it.

34:07.760 --> 34:10.120
We just have to remember that it's an approximation.

34:10.120 --> 34:11.880
It's a useful fiction, right?

34:11.920 --> 34:18.720
So quantum mechanics in no way invalidates Steve Wolf from theories, right?

34:19.280 --> 34:22.000
The problem, however, is that he has not entirely, you know,

34:22.360 --> 34:24.320
ever since cellular automata days, right?

34:24.320 --> 34:26.800
He was always saying like, oh, you know, the laws of physics will come out

34:26.800 --> 34:28.080
of this cellular automata, right?

34:28.080 --> 34:30.440
And people had these objections and, and, you know,

34:30.440 --> 34:32.560
now he and others have partly answered some of them.

34:32.560 --> 34:35.960
But the truth is at the end of the day, he the only way to answer

34:35.960 --> 34:39.680
that objection is to say, look, here is how quantum mechanics arises

34:39.680 --> 34:41.800
from my discrete model of the world.

34:41.800 --> 34:45.040
And I think this will happen, but it hasn't happened yet.

34:45.240 --> 34:47.400
Interesting. OK, we'd love to get Steven on the show.

34:47.400 --> 34:49.720
Actually, he's got a he's got a new book out now,

34:49.720 --> 34:52.200
which is kind of like expanding on his previous work.

34:52.600 --> 34:57.160
But OK, I was having a chat with some alignment folks yesterday.

34:57.200 --> 35:01.000
And it's something that I'm a bit naive to, but as I said, I've just read a book.

35:01.360 --> 35:04.520
I think it's called The Rationalists Guide to the to the Universe.

35:04.840 --> 35:07.640
And it kind of talks all about the the early embryonic stages

35:07.640 --> 35:10.960
of Robin Hansen and Nick Bostrom and Eliezer Yuckowski

35:10.960 --> 35:13.720
and the Les Ron community and, you know, the info hazards.

35:13.720 --> 35:16.800
And, you know, I don't know if you've heard of Rocco's Basilicist

35:16.800 --> 35:18.960
and all of this line of thought, basically.

35:19.320 --> 35:22.080
And yeah, so where to go with this?

35:22.080 --> 35:27.720
Now, I kind of put forward that part of my problem with their conception

35:27.720 --> 35:33.480
is that it relies on this rational agent making trajectories of optimal decisions.

35:33.880 --> 35:37.720
And also, they they tend to be utilitarianism

35:37.720 --> 35:40.720
and utilitarianists and consequentialists.

35:41.120 --> 35:44.720
And yeah, I just wondered, what's your kind of like high level take on this?

35:46.160 --> 35:48.880
Well, there's many aspects to this, right?

35:48.880 --> 35:51.760
I think let me put it this way.

35:51.760 --> 35:57.160
I. A rational agent, right, is an agent that maximizes

35:57.880 --> 35:59.920
maximizes expected utility, right?

35:59.960 --> 36:03.360
The definition of rationality is that it's, you know,

36:03.360 --> 36:05.680
expected utility maximization, right?

36:05.920 --> 36:08.400
And there is a lot of content to this, right?

36:08.400 --> 36:12.400
And, you know, people in many fields like economics and and and, you know, AI, right?

36:12.840 --> 36:14.400
They make good use of it.

36:14.400 --> 36:18.760
It doesn't answer the question of what is the utility that you're maximizing, right?

36:18.760 --> 36:21.640
So if you give me utility function, right?

36:21.640 --> 36:24.400
Now, if you maximize it, you're rational.

36:24.840 --> 36:26.000
You can it can be bound.

36:26.000 --> 36:28.640
You can be boundedly rational because you're and indeed this

36:28.640 --> 36:31.880
this is the interesting and prevalent cases that you can only maximize it

36:31.880 --> 36:34.840
within bounds and you have to make compromises, says this fast and what not.

36:34.840 --> 36:36.760
But still, you're rational.

36:36.760 --> 36:39.160
If you don't do that, you are irrational, right?

36:39.480 --> 36:43.200
So rational is a very, you know, like so many mistakes that we make

36:43.200 --> 36:47.680
as a society as individuals would be avoided if only we were rational in that sense.

36:48.080 --> 36:50.760
So at that level, I sympathize very much with that view of the world.

36:50.760 --> 36:54.680
Having said that, there's a huge gaping hole in the middle of this, which is like,

36:54.880 --> 36:56.600
but what is your utility function?

36:56.640 --> 37:00.120
Right. And and, you know, one attitude is like, oh, that's for you to decide.

37:00.160 --> 37:02.560
You know, you you tell what me or your utility function is.

37:02.560 --> 37:05.320
But but then you you you're you're entitled to sell what like.

37:05.320 --> 37:08.480
But like my whole problem is that I want to figure out what my

37:08.480 --> 37:09.680
utility function should be.

37:09.680 --> 37:13.560
And at that point, this whole theory of rationality just doesn't help you at all.

37:13.960 --> 37:16.040
The utility function is an input, right?

37:16.040 --> 37:18.760
So another question becomes what is your utility function, right?

37:18.920 --> 37:22.080
And then there's a very related, but as Hume said, very different question,

37:22.080 --> 37:25.160
which is like, what should your utility function be like?

37:25.200 --> 37:28.280
Should is a very loaded worth here, right?

37:28.640 --> 37:31.480
And then what what usually happens is things like this is that our

37:31.480 --> 37:35.880
notions of morality and so on are trying to impose a should on you,

37:36.200 --> 37:41.200
a utility function that you should have because it serves the utility of the society.

37:41.960 --> 37:44.840
Now, from the point of view, the society, this is good, right?

37:45.080 --> 37:47.760
But from the point of view, because the society hopefully will live

37:47.760 --> 37:51.680
and prosper if its elements contribute to its utility, not just their own, right?

37:51.880 --> 37:53.640
But it still doesn't answer the question.

37:53.640 --> 37:55.280
So you can you're entitled to ask.

37:55.280 --> 37:56.760
So what does answer the question?

37:56.760 --> 38:02.080
Right. And my view on this is that none of these people and these people

38:02.080 --> 38:05.400
include Kant and Bentham and, you know, Plato and everybody, right?

38:05.920 --> 38:08.120
They you can't do that, right?

38:08.120 --> 38:14.200
The the the so to me, the supreme reality of life, Supreme

38:14.200 --> 38:18.520
maybe is a bad word, but like the overarching reality is evolution, right?

38:19.120 --> 38:21.360
Everything we are is created by evolution.

38:21.360 --> 38:24.360
And as somebody famously said, nothing in biology makes sense,

38:24.360 --> 38:27.960
except in light of evolution, nothing in morality makes sense,

38:27.960 --> 38:30.920
except in light of evolution, not just biological evolution, even though

38:30.920 --> 38:33.560
that's part of it, but also social and cultural evolution.

38:33.560 --> 38:36.800
So at the end of the day, the question that you need to ask yourself is like,

38:36.800 --> 38:39.440
is which utility functions are fitter?

38:39.760 --> 38:41.440
And those are the ones that will prevail.

38:41.440 --> 38:42.920
So so let's let's go there.

38:42.920 --> 38:43.600
That's really interesting.

38:43.600 --> 38:48.400
Now, you're known as a skeptic of collectivist thought, right?

38:48.400 --> 38:51.040
We know, and there's this interesting dichotomy we were talking about

38:51.040 --> 38:55.520
of, you know, monolithic truth, but the utility functions interesting as well.

38:55.520 --> 38:58.440
Because in a sense, I mean, I know these folks are consequentialists,

38:58.440 --> 39:01.120
but in a sense, that's more leaning towards deontology.

39:01.840 --> 39:05.800
I did it again, deontology, you know, which is this idea that

39:05.800 --> 39:09.520
we have kind of like a principled approach to to morality.

39:09.760 --> 39:13.360
And I'm skeptical as well that it's possible to create such a utility

39:13.360 --> 39:15.360
function because it wouldn't really be parsimonious.

39:15.360 --> 39:19.440
But how do you wrestle that, that you have a simple utility function,

39:19.440 --> 39:21.880
even though you believe in diversity of ideas?

39:21.880 --> 39:24.240
Oh, no, I didn't say simple.

39:24.240 --> 39:24.560
Go on.

39:24.560 --> 39:27.920
Crucial point, the utility function could be extremely complex.

39:28.880 --> 39:30.480
And in fact, the utility function.

39:30.480 --> 39:33.600
So first of all, there's there's more than one level to this.

39:34.720 --> 39:38.560
You to let's say you believe in utilitarianism, right, which I don't,

39:38.560 --> 39:41.760
but have, you know, compared to the others, it's probably the least bad.

39:41.760 --> 39:43.040
Right. Yeah.

39:43.040 --> 39:47.440
Believing you tell if you believe in maximizing utility function

39:47.440 --> 39:51.120
that in no way sanctions collectivism.

39:52.320 --> 39:56.560
Collectivism is one particular strength that historically came out of that.

39:56.560 --> 39:57.280
Right.

39:57.280 --> 40:00.640
And, and, and, you know, again, and Bentham is responsible for it.

40:00.640 --> 40:04.560
But it's this notion that you should have a utility function

40:04.560 --> 40:06.240
in which everybody counts equally.

40:07.440 --> 40:09.680
This is now making a choice of utility function,

40:09.680 --> 40:11.280
which is different from having one.

40:11.280 --> 40:13.360
Okay, but I think you're saying something quite interesting as well,

40:13.360 --> 40:14.880
which is that at the moment,

40:15.600 --> 40:17.760
the utility is a function of market value,

40:17.760 --> 40:22.080
which is very much inspired by Adam Smith's hidden hand of the market.

40:22.080 --> 40:25.920
But I think your views against collectivism is very much against

40:25.920 --> 40:27.920
this idea of equality of outcome.

40:27.920 --> 40:29.200
And that's definitely not what you're saying.

40:29.200 --> 40:31.120
No, I mean, that's even going beyond that, right?

40:31.120 --> 40:35.600
Equality of outcome is actually irrational, frankly, to we could go into that.

40:35.600 --> 40:37.280
But, you know, you mentioned the market, right?

40:37.280 --> 40:39.200
And the market is the size utility.

40:39.200 --> 40:42.320
Again, that is a one way to decide.

40:42.320 --> 40:46.480
I mean, that is also a very critical approximation to what you really want.

40:46.480 --> 40:49.920
So actually, all we have with capitalism or carnimism at this point,

40:49.920 --> 40:53.360
in terms of utility function, are very imperfect, right?

40:54.160 --> 40:56.080
And that's even saying it generously.

40:56.080 --> 40:58.640
And really, our job is to try to come up with something better,

40:58.640 --> 41:00.800
which I totally think we can, right?

41:00.800 --> 41:03.600
And by the way, one very salient question here,

41:03.600 --> 41:06.720
which again, for economists, it's very salient,

41:06.720 --> 41:08.560
is this question of like,

41:08.560 --> 41:12.720
should you have one utility function overarching, controlling everything,

41:12.720 --> 41:14.560
even if it's complex, right?

41:14.560 --> 41:16.480
Or should you not, right?

41:16.480 --> 41:19.680
And that I think is a very interesting question, right?

41:19.680 --> 41:22.240
And there are good arguments in both directions, right?

41:22.240 --> 41:24.400
So let me just give you one silly example,

41:24.400 --> 41:26.640
which then I think also generalizes two other things.

41:26.640 --> 41:29.520
Does your brain have a singular utility function?

41:29.520 --> 41:31.440
And I think the answer is no.

41:31.440 --> 41:34.000
Now, you could say from an evolutionary point of view,

41:34.000 --> 41:36.000
the overarching utility is fitness.

41:36.640 --> 41:38.800
But then the way that cashes out in your brain

41:38.800 --> 41:43.760
is that your genes need to control this adaptive machine, right?

41:44.400 --> 41:49.440
In such a way that you give the machine freedom, right?

41:49.440 --> 41:51.520
To do things that the genes by themselves couldn't.

41:51.520 --> 41:53.280
But at the same time, at the end of the day,

41:53.280 --> 41:57.040
that machine has to subserve the propagation of those genes.

41:57.040 --> 41:58.480
And the way you do this, right,

41:58.480 --> 42:00.000
at least the way evolution seems to have done,

42:00.000 --> 42:01.360
and I think it makes a lot of sense,

42:01.360 --> 42:03.600
is that you don't just have one utility.

42:03.600 --> 42:06.880
You have several ones which correspond to your emotions.

42:07.840 --> 42:09.040
And then they fight it out.

42:09.760 --> 42:11.280
So I actually think there's this connection

42:11.280 --> 42:13.520
between rationality and the emotion that people don't make,

42:13.520 --> 42:17.360
which is that your emotions are really your utility functions.

42:17.360 --> 42:21.360
You just have different ones that cater to different things, right?

42:21.360 --> 42:23.840
You know, fear and anger and so on.

42:23.840 --> 42:25.520
And so I think in reality,

42:25.520 --> 42:27.600
we actually have multiple utility functions.

42:27.600 --> 42:30.160
But because, again, it gets to this problem that

42:30.160 --> 42:31.920
what we're trying to do is approximate something

42:31.920 --> 42:34.080
that is very complex and difficult to get at,

42:34.080 --> 42:35.440
maybe it is just one,

42:35.440 --> 42:37.280
but we're better off trying to approximate it

42:37.280 --> 42:38.560
with 10 or 20 different things

42:38.560 --> 42:40.560
than just trying to nail that one thing.

42:41.200 --> 42:42.000
That's really interesting.

42:42.000 --> 42:44.880
And is your view then on having this diversity

42:44.880 --> 42:47.600
of utility functions analogous to your views

42:47.600 --> 42:48.960
on the master algorithm?

42:49.680 --> 42:51.680
Huh. It's analogous,

42:51.680 --> 42:53.760
but you're actually talking about different dimensions, right?

42:53.760 --> 42:55.840
You could make a table where on one side

42:55.840 --> 42:57.840
you have all the different utilities,

42:57.840 --> 42:59.920
and then on the other side you have the algorithms.

42:59.920 --> 43:02.080
And now you can pair off any one of them.

43:02.080 --> 43:04.480
I can say, I'm going to pursue this,

43:04.480 --> 43:08.400
you know, minimize your fear using symbolism or minimum.

43:08.400 --> 43:09.680
So any combination is valid.

43:11.040 --> 43:12.160
Really, really interesting.

43:12.160 --> 43:15.520
Okay. And then let's get into meritocracy, for example.

43:15.520 --> 43:17.520
So at the moment, we do have the market system.

43:17.520 --> 43:19.680
And presumably you think that some people

43:19.680 --> 43:21.760
do genuinely have more market value than others.

43:22.560 --> 43:24.720
For sure. No. And by the way, I think,

43:25.360 --> 43:28.320
I'm definitely a big believer in meritocracy.

43:28.320 --> 43:31.200
I think. But what does it mean to you?

43:31.200 --> 43:34.000
Right. Very good. So let's get that down first, right?

43:34.000 --> 43:38.240
Meritocracy, so our goal is to have the society

43:38.240 --> 43:42.480
that functions best and provides best for everybody, right?

43:43.280 --> 43:45.200
And I mean, we could refine even that,

43:45.200 --> 43:47.680
but let's just take that for now as our assumption, right?

43:47.680 --> 43:49.200
But then if that is the case,

43:49.200 --> 43:52.320
one of our primary goals, maybe even the most important one,

43:52.320 --> 43:56.640
is to get everybody to contribute the most they can, right?

43:56.640 --> 43:58.880
Meritocracy is often seen as like,

43:58.880 --> 44:00.320
I'm going to rank all the people,

44:00.320 --> 44:02.400
and at the top is the greatest genius,

44:02.400 --> 44:04.320
and at the bottom is the most useless person,

44:04.320 --> 44:06.080
and this is wrong, right?

44:06.080 --> 44:09.520
Meritocracy is a many-dimension thing, right?

44:09.520 --> 44:12.160
The goal of meritocracy is to find for everybody

44:12.160 --> 44:14.720
what they're best at doing so that they can do it,

44:15.280 --> 44:18.000
maximize everybody's contribution to society, right?

44:19.120 --> 44:20.960
And this is a very complicated process.

44:20.960 --> 44:23.520
There isn't a single scale of intelligence or anything else.

44:24.080 --> 44:27.040
Having said that, it very much is the case

44:27.040 --> 44:30.720
that some people are better for some things than others, right?

44:30.720 --> 44:31.920
And if you deny that,

44:31.920 --> 44:34.320
you are actually in the process of destroying the society

44:34.320 --> 44:35.440
and making it dysfunctional.

44:35.440 --> 44:41.120
So I find the attacks on meritocracy extremely disturbing, right?

44:41.120 --> 44:42.480
And a lot of them are,

44:42.480 --> 44:45.040
I've talked with many people who have those beliefs, right?

44:45.040 --> 44:46.880
And the number one thing that they say is,

44:46.880 --> 44:48.320
it basically boils down to like,

44:48.320 --> 44:51.120
oh, meritocracy isn't perfect, so we should junk it.

44:51.920 --> 44:54.880
Something not being perfect has never been a reason to junk it.

44:54.880 --> 44:56.000
It's a reason to improve it.

44:56.000 --> 44:59.200
So there's a lot of room to improve in meritocracy,

44:59.200 --> 45:01.680
but if you throw it away, you are destroying society.

45:02.240 --> 45:06.640
Well, I mean, you can trace this back to our argument about utility.

45:06.640 --> 45:11.840
But the thing is though, if we had a value function

45:11.840 --> 45:14.160
which represented actual market contributions

45:14.160 --> 45:16.560
or even societal contributions, that would be one thing.

45:16.560 --> 45:20.480
But would you agree that we have a lot of game playing at the moment?

45:20.480 --> 45:23.360
So utility is based on playing the success game

45:23.360 --> 45:25.120
or the dominance game or the virtue game,

45:25.120 --> 45:26.560
as Will Stor said in his book.

45:26.560 --> 45:28.800
So we've got these kind of emerging games

45:28.800 --> 45:32.960
and it's not really representing utility.

45:33.600 --> 45:34.560
Well, absolutely.

45:34.560 --> 45:38.320
So far we've been talking about utility, right?

45:38.320 --> 45:39.760
But what happens in the real world

45:39.760 --> 45:41.760
is that there are multiple agents,

45:41.760 --> 45:43.360
each with their own different utility.

45:44.160 --> 45:47.120
And at this point, what you have is game theory, right?

45:47.120 --> 45:48.400
Game theory is just what you have

45:48.400 --> 45:50.960
when there's not a single optimization going on,

45:50.960 --> 45:52.400
but multiple optimizations

45:52.400 --> 45:55.120
which are partly contradictory, maybe partly not.

45:55.120 --> 45:57.280
So the best way to understand everything

45:57.280 --> 46:00.400
that we've been talking about, including society and evolution

46:00.400 --> 46:03.520
and even what happens inside your brain is as a big game.

46:04.240 --> 46:06.320
A much bigger and more complex game

46:06.320 --> 46:08.880
than game theorists and economists and so on

46:08.880 --> 46:12.480
and evolutionary biologists, right, prominently,

46:12.480 --> 46:14.960
have been able to handle in the past.

46:14.960 --> 46:18.720
But I think they are very much in the right track

46:18.720 --> 46:21.040
and we can understand a lot of these phenomena

46:21.040 --> 46:24.480
that you're referring to as they are games being played

46:24.480 --> 46:26.880
by people that have certain utilities, right?

46:27.440 --> 46:29.840
And now you are going to impose your, you know, like,

46:29.840 --> 46:31.120
and it's a game, right?

46:31.120 --> 46:32.960
I don't, you don't know who's going to win

46:32.960 --> 46:35.360
until you actually do the linear program

46:35.360 --> 46:37.440
and figure out how this is going to, you know,

46:38.000 --> 46:40.720
and of course games are, you know, in reality,

46:40.720 --> 46:43.040
you know, most games are not single round games, right?

46:43.040 --> 46:44.720
They're continuing games, right?

46:44.960 --> 46:46.880
So things get very, very interesting,

46:46.880 --> 46:50.080
but this I think is the most productive way

46:50.080 --> 46:51.200
to look at all of this.

46:51.200 --> 46:52.160
Okay, good, good.

46:52.160 --> 46:55.600
But then some might say that this is a platonic way

46:55.600 --> 46:57.280
of looking at the world

46:57.280 --> 46:59.600
and the world is actually much more complicated than that.

46:59.600 --> 47:01.920
And again, we're kind of fooled by randomness

47:01.920 --> 47:03.760
because we're anthropomorphizing the world

47:03.760 --> 47:05.520
and we're kind of framing it as a game.

47:05.520 --> 47:07.600
It might be much more complicated than that.

47:07.600 --> 47:09.440
And I've already said this a couple of times,

47:09.440 --> 47:11.440
but you know, the concept of power, for example,

47:11.440 --> 47:13.200
did when Napoleon said,

47:13.200 --> 47:16.000
I want the men to march into this country,

47:16.000 --> 47:18.880
is it just a simple kind of chain of command that goes down?

47:18.880 --> 47:19.440
No, it's not.

47:19.440 --> 47:20.880
It's so much more complicated than that.

47:20.880 --> 47:22.800
Well, yes, but that's, actually,

47:22.800 --> 47:23.920
I'm not even sure what you mean

47:23.920 --> 47:26.480
by when you say it's much more complicated than a game.

47:26.480 --> 47:27.840
Again, when I say a game,

47:28.800 --> 47:30.720
maybe what comes to your mind is something simple,

47:30.720 --> 47:33.920
like in a prisoner's dilemma, two players, two moves.

47:33.920 --> 47:36.800
It's a game with, you know, with a vast number of players,

47:36.800 --> 47:38.400
each with a vast number of moves.

47:39.280 --> 47:41.120
Interesting, but I think this gets to the core

47:41.120 --> 47:42.480
of what the rationalists talk about.

47:42.800 --> 47:44.000
They have these thought experiments.

47:44.000 --> 47:45.600
They talk about prisoners dilemma.

47:45.600 --> 47:47.440
They have that, I forget the name of that game

47:47.440 --> 47:48.640
where there's the two boxes,

47:48.640 --> 47:51.040
and you have to choose the box, I forget that.

47:51.040 --> 47:52.640
But I guess what I'm saying is that

47:53.280 --> 47:55.280
if you do have this rationalist conception of the world,

47:55.280 --> 47:58.160
and think about it in terms of game theory,

47:58.160 --> 47:59.920
just like the symbolists do,

47:59.920 --> 48:02.480
and the people who handcraft cognitive architectures do,

48:02.480 --> 48:04.160
or even with causality, for example,

48:04.160 --> 48:06.960
we create these variables, it's all anthropomorphic.

48:07.600 --> 48:12.160
Well, I would not, so let me put it this way, right?

48:12.960 --> 48:15.120
You can model almost anything,

48:16.080 --> 48:17.760
can is an important word here.

48:17.760 --> 48:20.400
You can model almost anything in the world,

48:20.400 --> 48:22.960
in any domain, from physics to psychology

48:22.960 --> 48:25.920
to sociology, name it, as optimizing a function.

48:26.880 --> 48:29.520
Whether you should is a debatable question,

48:29.520 --> 48:31.360
but you can, right?

48:31.360 --> 48:33.360
But now, what really happens is that

48:33.360 --> 48:35.040
there are many different optimizations

48:35.040 --> 48:36.560
going on at the same time,

48:36.560 --> 48:38.560
all the way from maximizing entropy

48:38.560 --> 48:40.480
to me deciding what I have for lunch today.

48:40.960 --> 48:44.000
And now what you have is all of these interlocking optimizations,

48:44.000 --> 48:46.320
and that's what I'm calling game theory, right?

48:46.320 --> 48:48.320
One of those optimizations is I'm Napoleon,

48:48.320 --> 48:49.600
I want to conquer Russia,

48:49.600 --> 48:52.080
you're the Tsar of Russia, you don't want to be conquered, right?

48:52.080 --> 48:53.680
And then we play a very complicated game,

48:53.680 --> 48:56.160
which includes other agents, like your soldiers,

48:56.160 --> 48:58.960
which maybe, you know, I, a French soldier,

48:58.960 --> 49:00.160
you know, want to conquer Russia,

49:00.160 --> 49:01.600
but I also want to stay alive,

49:01.600 --> 49:03.120
whereas an opponent really couldn't care less

49:03.120 --> 49:05.200
whether I, in particular, stay alive or not,

49:05.200 --> 49:06.720
as long as he conquers Russia in the end.

49:06.720 --> 49:10.080
So this very complex game, I think, is what goes on.

49:10.160 --> 49:12.640
I don't think framing things in this way

49:12.640 --> 49:14.000
is anthropomorphizing them.

49:14.000 --> 49:16.080
In fact, I think this is our best hope

49:16.080 --> 49:17.760
to not anthropomorphize things,

49:17.760 --> 49:19.200
although at the end of the day,

49:19.200 --> 49:21.520
I think you can look at almost anything

49:21.520 --> 49:25.280
and see a ghost of anthropomorphization there.

49:25.280 --> 49:28.000
But if there's a less anthropomorphic way

49:28.000 --> 49:30.000
to look at the universe than through this lens,

49:30.000 --> 49:31.600
I'd be interested to see what it is.

49:31.600 --> 49:33.040
Well, the only reason I'm saying this is,

49:33.040 --> 49:35.360
first of all, I want to play devil's advocate a little bit,

49:35.360 --> 49:37.840
and we even spoke about the blind men

49:37.840 --> 49:39.360
and the elephant a little while ago,

49:39.360 --> 49:42.480
and I'm sure folks on the left, as they did,

49:42.480 --> 49:44.480
they criticized Ayan Ran, for example,

49:44.480 --> 49:46.720
and they said that she had this very transactional way

49:46.720 --> 49:48.720
of viewing the world as this kind of

49:48.720 --> 49:51.680
Nash equilibrium of self-interested actors.

49:51.680 --> 49:53.120
And are we guilty of doing that?

49:53.120 --> 49:55.760
Are we kind of cutting off many aspects of the truth

49:55.760 --> 49:57.040
by doing this? I guess that's what I'm saying.

49:57.920 --> 50:01.600
So we are always cutting off some aspect of the truth

50:01.600 --> 50:03.920
when we look at anything in any way,

50:03.920 --> 50:06.640
which is not a reason to look at nothing in no way.

50:07.280 --> 50:10.880
So I think this is a very productive way to look at things,

50:10.880 --> 50:12.080
but not the only one.

50:12.080 --> 50:13.760
It doesn't exhaust what there is to be said,

50:13.760 --> 50:15.440
but I personally feel like it's the one

50:15.440 --> 50:17.600
where the most progress can come from.

50:17.600 --> 50:18.640
Interesting.

50:18.640 --> 50:22.000
Now, that's sort of like Ayan Randian

50:22.000 --> 50:23.280
simplification of the world.

50:25.200 --> 50:29.280
Looking at things this way does not imply over-simplifying them.

50:29.280 --> 50:32.240
On the contrary, I would actually say it gives us a handle

50:32.240 --> 50:34.960
on how to go into the complexity and not get lost

50:35.440 --> 50:37.440
and not devolve into like platitudes

50:37.440 --> 50:39.120
or over-simplifying ideologies.

50:39.920 --> 50:42.320
The fact that there's a mathematical component to this

50:42.320 --> 50:43.120
is very important.

50:44.080 --> 50:45.600
Mathematics, when you can apply it,

50:45.600 --> 50:47.840
gives you a very solid handle on things.

50:47.840 --> 50:50.880
We are now at the point where we can handle

50:50.880 --> 50:53.840
a lot of things mathematically slash computationally

50:53.840 --> 50:54.800
that we couldn't before.

50:54.800 --> 50:57.760
So when von Neumann invented game theory,

50:57.760 --> 51:00.800
he said, this is the future of the social sciences.

51:00.800 --> 51:02.640
And so far it hasn't been,

51:02.640 --> 51:04.240
but I think we're actually now at the point

51:04.800 --> 51:06.160
partly because we have the data.

51:07.280 --> 51:09.680
We actually can now usefully apply this point of view

51:09.680 --> 51:11.200
in a way that we couldn't before.

51:11.200 --> 51:13.120
How far it takes us, we'll see.

51:13.840 --> 51:15.760
It's not the only possible to look at things,

51:15.760 --> 51:18.720
but I do think it's probably the most productive at this point.

51:18.720 --> 51:19.360
Interesting.

51:19.360 --> 51:20.000
Okay.

51:20.000 --> 51:22.240
So coming back to this rationalist school of thought,

51:22.240 --> 51:25.680
one thing that I'm interested in is morality.

51:25.680 --> 51:27.520
But let's go one step at a time.

51:27.520 --> 51:29.600
So I think Bostrom came up with this idea

51:29.600 --> 51:31.280
of instrumental convergence,

51:31.280 --> 51:33.040
which is this notion that in the pursuit

51:33.040 --> 51:34.480
of doing a particular task,

51:34.480 --> 51:37.760
the intelligence system might actually potentially kill

51:37.760 --> 51:40.080
everyone on the planet or do adjacent.

51:40.080 --> 51:41.840
And this is where the interesting thing comes from.

51:41.840 --> 51:45.200
So one task, but adjacent multitask ability

51:45.200 --> 51:48.240
and potential intelligence and so on.

51:48.240 --> 51:50.240
So there was an example of a cauldron.

51:50.240 --> 51:53.440
So you've got someone filling up a cauldron

51:53.440 --> 51:55.440
and in the pursuit of filling up the cauldron

51:55.440 --> 51:56.320
to just the right level,

51:56.320 --> 51:59.120
they might kill the person who looks after the cauldron room

51:59.120 --> 52:01.840
just so that the agent could do it more efficiently.

52:02.800 --> 52:05.200
Are you cynical about that or what do you think?

52:06.000 --> 52:07.760
No, I'm not cynical about that,

52:07.760 --> 52:09.040
but let me put it this way.

52:09.680 --> 52:13.920
I don't lose any sleep worrying about the paperclip factory

52:13.920 --> 52:15.360
that's going to take over the world.

52:16.400 --> 52:19.600
I think you have to take that as a philosopher's thought experiment.

52:21.120 --> 52:23.200
The philosopher being Nick in this case.

52:24.320 --> 52:29.680
I think there's a real danger that there's putting its finger on,

52:30.400 --> 52:34.880
but it's also mistaking reality for something else.

52:34.880 --> 52:36.160
So let's look at both parts of that.

52:36.880 --> 52:42.000
The real danger is that if you give an AI

52:43.600 --> 52:47.520
an oversimplified, a hugely oversimplified objective function,

52:47.520 --> 52:50.160
and at the same time a very large amount of power,

52:50.160 --> 52:53.040
bad things will happen and we need to worry about that.

52:55.360 --> 52:57.040
By the way, this is already a problem today

52:57.040 --> 52:58.800
in many maybe more modest ways,

52:59.520 --> 53:01.040
but also more relevant, frankly.

53:01.600 --> 53:02.640
So what do you do?

53:02.640 --> 53:06.480
First of all, the utility function needs to be as rich

53:07.280 --> 53:11.680
and as complex and as subtle as the people that it's trying to serve.

53:13.680 --> 53:17.120
As long as what you have to take a really world example today,

53:17.120 --> 53:21.520
social media, who are all designed to just maximize engagement,

53:22.880 --> 53:26.400
you have an enormous amount of AI at the service of maximizing engagement.

53:26.880 --> 53:31.760
I understand why companies do it and partly they have the right to.

53:31.760 --> 53:32.880
We can get into that.

53:32.880 --> 53:36.560
But the point is, it's ignoring too many things.

53:36.560 --> 53:41.360
So one line of defense against is that you have to enrich your utility function

53:41.360 --> 53:44.640
until it's like a bit, and then this is an open-ended problem.

53:45.600 --> 53:47.760
We're never going to have the final utility function.

53:47.760 --> 53:50.240
It's something that the AIs have to be continually,

53:50.240 --> 53:52.960
you know, AIs, I think Stuart Russell said this and I agree,

53:52.960 --> 53:57.440
like they should spend half their time figuring out what the utility function is

53:57.440 --> 53:59.120
and then the other half maximizing it.

53:59.120 --> 54:02.480
Whereas today it's like I wrote down my utility function in one line

54:02.480 --> 54:05.920
and now I spend this enormous amount of power maximizing it.

54:05.920 --> 54:07.120
So that's one line.

54:07.120 --> 54:11.840
The other line or like one other line is you have to put constraints on the machine.

54:11.840 --> 54:12.960
Hard constraints.

54:12.960 --> 54:15.840
You can win the pursuit of this utility function.

54:15.840 --> 54:17.280
You can think of it as like, you know,

54:17.280 --> 54:19.280
terms with infinite weight in the utility function.

54:19.280 --> 54:20.480
You can't go outside this.

54:20.480 --> 54:24.240
And then the other one is the single biggest reason why I sort of like

54:24.240 --> 54:28.000
this paperclip experiment is silly is that, you know,

54:28.000 --> 54:29.920
along with that paperclip factoring the world,

54:29.920 --> 54:32.480
there are going to be a million other AIs, you know,

54:32.480 --> 54:33.920
each of which is doing the same thing.

54:33.920 --> 54:38.400
So none of them is ever going to acquire the power to cause that damage

54:38.400 --> 54:41.360
unless it's doing something very different from just trying to make paperclips.

54:41.360 --> 54:44.960
So at some level that example is extremely unrealistic

54:44.960 --> 54:46.720
and leads us down the wrong track.

54:46.720 --> 54:48.160
Right, loads of places to go there.

54:48.160 --> 54:52.320
But first of all, I think you do believe in AI alignment then

54:52.320 --> 54:54.240
because you're saying exactly the same as what they do,

54:54.240 --> 54:56.800
which is that we need to have the utility function

54:56.800 --> 54:59.920
that represents the richness of the human condition.

54:59.920 --> 55:00.880
So that's the first thing.

55:00.880 --> 55:03.680
So essentially you're all on board of alignment.

55:03.680 --> 55:07.680
Well, I believe in AI alignment in one sense of it.

55:08.320 --> 55:11.360
Many different things get go under that umbrella of AI alignment.

55:11.360 --> 55:12.080
Right.

55:12.080 --> 55:15.200
I think in the near term, thinking of things into,

55:15.200 --> 55:17.600
I mean, if I like, let me put it this way.

55:17.600 --> 55:22.880
If AI alignment is just trying to have a really accurate utility function,

55:22.880 --> 55:23.840
then yes.

55:23.840 --> 55:26.000
And then the machines are optimizing that function.

55:26.000 --> 55:26.640
Absolutely.

55:26.640 --> 55:27.120
Right.

55:27.120 --> 55:27.600
Yeah.

55:27.600 --> 55:31.520
And in the near term, I think talking about AI alignment is a little,

55:31.520 --> 55:35.280
I mean, the problem that I have with the concept of alignment

55:35.280 --> 55:37.200
is that goes far beyond that.

55:37.200 --> 55:41.840
It tends to see AIs as these independent agents

55:42.640 --> 55:46.800
that we have to align their goals to ours.

55:46.800 --> 55:47.280
Right.

55:47.760 --> 55:50.400
And if that just caches out as like,

55:50.400 --> 55:51.840
here's the utility function, that's fine.

55:51.840 --> 55:54.320
But the problem is AIs are not independent agents.

55:54.320 --> 55:55.600
AIs are our tools.

55:56.640 --> 55:58.320
Just to push back on that a little bit,

55:58.320 --> 56:00.880
because I always had that conception of these folks.

56:00.880 --> 56:03.120
I thought I was arguing against people who believed

56:03.120 --> 56:04.960
in a pure monolithic intelligence.

56:04.960 --> 56:07.520
And a lot of them are transhumanists actually,

56:07.520 --> 56:11.680
and they say that they want to ensure human flourishing

56:11.680 --> 56:14.960
through the use of AIs in tandem,

56:14.960 --> 56:18.160
almost as a kind of extended mind from David Chalmers.

56:18.160 --> 56:21.760
But then I really wanted to get into their fears

56:21.760 --> 56:24.400
of recursive self-improving intelligence

56:24.400 --> 56:25.360
and superintelligence.

56:25.360 --> 56:29.360
Because when you do have this kind of heterogeneous approach

56:29.360 --> 56:31.120
to humans and machines,

56:31.120 --> 56:33.040
there are going to be bottlenecks everywhere.

56:33.040 --> 56:35.920
Now, I like to think of it a bit like the market efficiency

56:35.920 --> 56:38.800
hypothesis, which is that you reach an equilibria

56:38.800 --> 56:43.120
where the individual actors in the market become more efficient,

56:43.120 --> 56:44.640
will become more efficient programmers.

56:44.640 --> 56:46.000
Because we're using codecs.

56:46.000 --> 56:47.840
But we will reach a limit, surely.

56:48.720 --> 56:53.040
Well, to touch on transhumanism for just a second,

56:53.040 --> 56:55.760
because I do agree, at least sociologically,

56:55.760 --> 56:57.280
a lot of that crowd is the same.

56:58.720 --> 56:59.840
Let me put it this way.

56:59.840 --> 57:01.920
And I'm sure this is a controversial statement.

57:01.920 --> 57:05.600
But maybe in the long run, the AIs should take over the world.

57:06.480 --> 57:09.920
Why are we so arrogant that we think whatever the AI is,

57:09.920 --> 57:11.440
it should always be there to serve us.

57:12.240 --> 57:13.280
We are a step.

57:13.280 --> 57:16.480
If you take the long view of this, we're a step in evolution.

57:17.280 --> 57:17.920
We're amazing.

57:17.920 --> 57:20.560
Maybe I'm a human chauvinist, but I do think we are amazing.

57:20.560 --> 57:21.920
But we're not the last word.

57:23.680 --> 57:28.160
So the other day, I tweeted something that is maybe provocative,

57:28.160 --> 57:31.600
but it's like, I think in Gemswich, which is,

57:32.240 --> 57:36.000
I said that the killer app of humans is producing AI.

57:37.040 --> 57:40.320
Maybe our role in evolution is that we're going to produce an AI.

57:41.040 --> 57:43.920
That is the next level of whatever you like.

57:43.920 --> 57:46.880
Consciousness, intelligence, et cetera, et cetera.

57:46.880 --> 57:49.520
And so the notion that in the very long term,

57:49.520 --> 57:51.760
the AIs should still be there to serve us,

57:52.320 --> 57:54.240
by this point of view, is actually silly.

57:55.280 --> 57:59.040
Right, but a lot of folks, let's say the ethics folks,

57:59.040 --> 58:00.160
would find it horrifying.

58:00.960 --> 58:03.040
And I was speaking to Irina actually yesterday,

58:03.040 --> 58:05.600
and she said something a little bit tongue in cheek,

58:05.600 --> 58:06.400
which is that which is actually...

58:06.400 --> 58:07.360
Who, sorry?

58:07.360 --> 58:10.240
Irina from Montreal, Mila, and Irina Rich.

58:10.400 --> 58:11.440
Oh yeah, I know her, yeah.

58:11.440 --> 58:13.360
We were classmates at UC Irvine.

58:13.360 --> 58:15.280
Amazing, yeah, I really love her.

58:15.280 --> 58:16.960
But no, she was kind of joking

58:16.960 --> 58:20.400
that we should almost align human values to the AGI values.

58:21.200 --> 58:22.880
Well, that I find alarming.

58:23.440 --> 58:25.840
Well, I think she was saying it tongue in cheek.

58:25.840 --> 58:27.680
I'm not alarmed by a lot of things, but yeah.

58:28.240 --> 58:32.880
But what do you think about this ethical concern

58:32.880 --> 58:35.280
that if it is the case that you believe

58:35.280 --> 58:38.880
that we're just one rung on the ladder and transhumanism

58:38.960 --> 58:40.880
is more AI than it is human?

58:41.520 --> 58:42.800
People would find that horrifying.

58:43.600 --> 58:47.840
Well, I understand why people would find that horrifying.

58:47.840 --> 58:49.600
And I mean, again, we have to distinguish

58:49.600 --> 58:51.760
the short from the meaning from the long term.

58:51.760 --> 58:52.880
When I say something like this,

58:52.880 --> 58:55.200
I'm talking about the very long term, right?

58:55.200 --> 58:58.400
Trying to make humans subservient to AI today

58:58.400 --> 59:00.240
is a horrifying idea, right?

59:00.240 --> 59:01.920
Now, I think the reason a lot of people

59:01.920 --> 59:04.560
are horrified with this idea period, right?

59:04.640 --> 59:08.240
Is natural, but in my view, naive is just,

59:08.880 --> 59:12.320
they are seeing humans as the end goal.

59:14.000 --> 59:15.360
If humans are the end goal,

59:15.360 --> 59:17.200
then the idea that they should be subservient

59:17.200 --> 59:20.720
to developing the next level of AI is horrifying.

59:20.720 --> 59:25.600
If you have a moral system where humans are the be all

59:25.600 --> 59:27.760
and end all, then all of this is horrifying.

59:27.760 --> 59:30.560
But again, if you take the long view of evolution,

59:31.840 --> 59:33.600
humans are not the be all and end all.

59:34.240 --> 59:36.080
Okay, I mean, eventually this might take us

59:36.080 --> 59:37.760
to the effective altruism discussion.

59:37.760 --> 59:39.920
But I think, as we were saying,

59:39.920 --> 59:41.680
Sam Harris recently had a podcast

59:41.680 --> 59:43.120
talking about the FTX disaster.

59:43.120 --> 59:44.480
And he was kind of making the argument

59:44.480 --> 59:45.840
that we're all consequentialists,

59:45.840 --> 59:47.120
even if we don't realize it,

59:47.120 --> 59:49.280
but there are different degrees of consequentialism.

59:49.280 --> 59:52.560
And I think a lot of the ethics folks at the moment,

59:52.560 --> 59:54.480
they really, really don't like what's going on

59:54.480 --> 59:55.840
with long termism.

59:55.840 --> 59:58.320
And it's because there's this slippery slope

59:58.320 --> 01:00:00.880
of the kind of horizon of consequentialism.

01:00:00.880 --> 01:00:02.000
So with Nick Bostrom,

01:00:02.000 --> 01:00:03.600
he came up with this number

01:00:03.600 --> 01:00:05.920
that there could be simulated humans

01:00:06.640 --> 01:00:08.480
living on other planets in the future.

01:00:08.480 --> 01:00:09.440
It's a very big number.

01:00:09.440 --> 01:00:12.160
I think it's got a lot of zeros on it.

01:00:12.160 --> 01:00:15.600
And what's to stop us from just making the argument

01:00:15.600 --> 01:00:18.080
and what's to stop AIs from making the argument

01:00:18.080 --> 01:00:20.720
that those simulated lives have more value than our lives?

01:00:22.160 --> 01:00:23.840
Okay, there's a lot to unpack there.

01:00:24.720 --> 01:00:26.480
So, but let's take this one step at a time.

01:00:27.280 --> 01:00:30.720
I very much by the idea of effective altruism on principle.

01:00:30.720 --> 01:00:33.280
I think that is the way to go about a lot of things.

01:00:33.280 --> 01:00:34.320
I think in some ways,

01:00:35.520 --> 01:00:38.240
if you are not an effective altruist,

01:00:38.240 --> 01:00:41.920
maybe unconsciously, you are being irrational

01:00:41.920 --> 01:00:43.840
or maybe evil, right?

01:00:43.840 --> 01:00:45.120
If you believe in altruism,

01:00:45.120 --> 01:00:47.440
I mean, think about both parts of that, right?

01:00:47.440 --> 01:00:50.800
If altruism is good, then let's say we take that, right?

01:00:50.800 --> 01:00:53.120
And then why should you be in favor

01:00:53.120 --> 01:00:55.520
of ineffective altruism, right?

01:00:55.520 --> 01:00:56.800
If you're an altruist,

01:00:56.800 --> 01:00:58.320
if you want the good of other people,

01:00:58.320 --> 01:01:00.960
you should try to do the best you can, right?

01:01:00.960 --> 01:01:03.600
And so, for example, I very much by the notion that like,

01:01:03.600 --> 01:01:06.640
you want to make the most money you can,

01:01:06.640 --> 01:01:08.320
so then you can give away that money

01:01:08.320 --> 01:01:10.720
as opposed to volunteering at the soup kitchen.

01:01:10.720 --> 01:01:12.400
Volunteering at the soup kitchen for, say,

01:01:12.400 --> 01:01:14.160
someone with a PhD in machine learning

01:01:14.160 --> 01:01:16.480
is an ineffective form of altruism.

01:01:16.480 --> 01:01:18.240
Now, having said that,

01:01:18.240 --> 01:01:22.240
I think the focus on the long term has been in many ways,

01:01:22.240 --> 01:01:25.120
I mean, certainly the long term is important, right?

01:01:25.120 --> 01:01:27.840
But the problem with the whole effective altruism movement

01:01:27.840 --> 01:01:30.240
is that it got overly focused on that,

01:01:30.240 --> 01:01:31.760
and we can talk about why.

01:01:31.760 --> 01:01:34.400
And then even, and then a further mistake

01:01:34.400 --> 01:01:35.760
is that it got overly focused

01:01:35.760 --> 01:01:37.920
on these supposedly existential dangers

01:01:37.920 --> 01:01:41.120
that are much less of a big deal than people think like AI.

01:01:41.120 --> 01:01:44.160
So between effective altruism and fixating on AI

01:01:44.160 --> 01:01:46.720
as an existential danger lies a huge gulf.

01:01:46.720 --> 01:01:49.200
I'm for effective altruism, I think, you know,

01:01:49.200 --> 01:01:53.280
the long term, you know, there's ins and outs there, right?

01:01:53.280 --> 01:01:55.840
And then this focus on like these existential dangers

01:01:56.400 --> 01:01:57.840
is very problematic.

01:01:57.840 --> 01:02:00.080
You know, for example, you know, to get back to the,

01:02:00.080 --> 01:02:02.240
you know, Bostromian notion of like all these minds

01:02:02.240 --> 01:02:04.080
that matter more than us and whatnot,

01:02:04.080 --> 01:02:05.600
there is a basic idea, right,

01:02:05.600 --> 01:02:07.440
that like any economist knows,

01:02:07.440 --> 01:02:09.600
which is that you have to discount the future.

01:02:09.600 --> 01:02:12.560
And the question is what your discount rate is, right?

01:02:12.560 --> 01:02:14.960
And if your discount rate is high, right,

01:02:14.960 --> 01:02:17.200
those minds matter not at all.

01:02:17.200 --> 01:02:19.280
And now why do you have that discount rate?

01:02:19.280 --> 01:02:20.320
The primary reason is that

01:02:20.320 --> 01:02:22.480
there's uncertainty about the future, right?

01:02:22.480 --> 01:02:27.200
I have to weigh the certain benefit of helping you today

01:02:27.200 --> 01:02:29.200
with the increasingly hypothetical benefit

01:02:29.200 --> 01:02:30.000
of helping your mind.

01:02:30.000 --> 01:02:32.160
There's less and less likely to exist in the future.

01:02:32.160 --> 01:02:33.680
So in many of those cases,

01:02:33.680 --> 01:02:36.240
the present and the short-term do win.

01:02:36.240 --> 01:02:38.080
Okay, but a couple of things to contrast that.

01:02:38.080 --> 01:02:40.880
So a lot of effective altruism is this idea

01:02:40.880 --> 01:02:42.800
that we're born with faulty programming, right?

01:02:42.800 --> 01:02:45.200
So we have these views, you know,

01:02:45.200 --> 01:02:48.400
like we have this concept of moral value

01:02:48.400 --> 01:02:51.200
and it gets discounted in space and time, right?

01:02:51.200 --> 01:02:53.440
So we need ways of overcoming our programming.

01:02:53.440 --> 01:02:56.560
But you were saying that we should be thinking about this,

01:02:56.560 --> 01:02:58.880
but contrast that with your, you know,

01:02:58.880 --> 01:03:01.760
with your statement about Ayan Rand earlier, right?

01:03:01.760 --> 01:03:04.480
So Ayan Rand was very, very transactional

01:03:04.480 --> 01:03:07.600
because I think the folks that criticize this movement

01:03:07.600 --> 01:03:10.080
are suspicious that we are actually being

01:03:10.080 --> 01:03:12.240
a bit more like Ayan Rand,

01:03:12.240 --> 01:03:14.960
but with the guise of altruism.

01:03:14.960 --> 01:03:18.720
And I think they think of the FTX disaster

01:03:18.720 --> 01:03:20.640
as being kind of like evidence of that.

01:03:21.920 --> 01:03:23.840
A lot of different points there.

01:03:23.840 --> 01:03:26.400
The FTX disaster actually has nothing whatsoever

01:03:26.400 --> 01:03:28.240
to do with any of this, right?

01:03:28.240 --> 01:03:30.720
Sam Beckman Fried was one guy or is one guy,

01:03:30.720 --> 01:03:32.240
funny that I used the past tense.

01:03:32.240 --> 01:03:35.520
He's one guy who believed in effective altruism,

01:03:35.520 --> 01:03:36.560
good for him, right?

01:03:36.560 --> 01:03:39.360
He was, I mean, the whole FTX thing was also obviously,

01:03:39.360 --> 01:03:40.960
I mean, yeah, we could get into that,

01:03:40.960 --> 01:03:43.360
but the point is you should not,

01:03:44.080 --> 01:03:47.280
I understand why people's image of effective altruism

01:03:47.280 --> 01:03:49.440
would be tainted by what happened with Sam Beckman Fried,

01:03:49.440 --> 01:03:50.880
but really it shouldn't be, right?

01:03:51.360 --> 01:03:54.480
An idea is not responsible for the mistakes

01:03:54.480 --> 01:03:57.280
that its believers make in unrelated domains,

01:03:57.280 --> 01:04:00.160
point one, point two, transactionalism.

01:04:00.160 --> 01:04:02.480
There is nothing in what I've said whatsoever

01:04:02.480 --> 01:04:05.440
that implies transactionalism, in fact, the opposite, right?

01:04:05.440 --> 01:04:09.040
I think relationalism is actually the key concept.

01:04:09.040 --> 01:04:12.640
And part of this is that games are not one shot.

01:04:12.640 --> 01:04:14.400
Your games are played in a repeated way.

01:04:14.400 --> 01:04:15.680
And famously, for example,

01:04:15.680 --> 01:04:17.520
if you play things like Prisoners of the Lemon

01:04:17.520 --> 01:04:19.680
and whatnot repeated, like you're like,

01:04:19.760 --> 01:04:21.600
cooperate, defect and whatnot,

01:04:21.600 --> 01:04:23.680
as soon as you start bringing in these other things,

01:04:23.680 --> 01:04:25.440
like that make things more realistic,

01:04:25.440 --> 01:04:26.800
you actually start to get behavior

01:04:26.800 --> 01:04:29.360
that is much more, what's the way to put it,

01:04:29.360 --> 01:04:32.080
rational in some ways and human and whatnot, right?

01:04:32.080 --> 01:04:34.400
Another one is that traditional economics,

01:04:34.400 --> 01:04:36.560
which I think Ayn Rand was influenced by,

01:04:36.560 --> 01:04:38.640
viewed and still views the world as linear,

01:04:39.360 --> 01:04:40.640
but the world is non-linear.

01:04:41.200 --> 01:04:43.680
Once you start seeing the world as non-linear,

01:04:43.680 --> 01:04:45.600
all of these things really change,

01:04:45.600 --> 01:04:47.600
the face of them changes, right?

01:04:47.680 --> 01:04:49.840
So I think we have to look at all these concepts

01:04:49.840 --> 01:04:51.840
in this view, right?

01:04:51.840 --> 01:04:54.480
And we want to focus on the long, so...

01:04:56.640 --> 01:04:59.600
So to get back to your first point, right?

01:05:00.160 --> 01:05:01.760
We are born with faulty programming.

01:05:02.800 --> 01:05:03.520
Part of our fa...

01:05:03.520 --> 01:05:06.240
And that's what if effective alteration

01:05:06.240 --> 01:05:07.760
is there to overcome, right?

01:05:08.320 --> 01:05:09.840
Part of our faulty programming

01:05:09.840 --> 01:05:12.000
is that our discount rate is too high,

01:05:13.120 --> 01:05:14.640
because we evolved in a world

01:05:14.640 --> 01:05:16.400
where your time horizon was very short.

01:05:17.360 --> 01:05:18.640
The fact that it's too high

01:05:18.640 --> 01:05:20.400
doesn't mean that we should make it zero

01:05:21.600 --> 01:05:23.040
and care only about the future.

01:05:23.600 --> 01:05:24.400
But what would...

01:05:24.400 --> 01:05:26.560
You know, the ethics folks who advocate

01:05:26.560 --> 01:05:28.160
for gatekeeping and paternalism,

01:05:29.120 --> 01:05:30.960
couldn't you just say that they're doing the same thing?

01:05:32.480 --> 01:05:34.160
Well, you should ask them, right?

01:05:34.160 --> 01:05:38.000
Wouldn't they lead by saying our programming is faulty

01:05:38.000 --> 01:05:40.080
and therefore, you know, we need to...

01:05:40.080 --> 01:05:42.800
No, I mean, look, we can...

01:05:42.800 --> 01:05:45.760
So part one, we can debate whether our programming

01:05:45.760 --> 01:05:47.520
is faulty or not and why.

01:05:47.520 --> 01:05:50.560
And so to just start by touching on that,

01:05:50.560 --> 01:05:52.240
our programming is faulty.

01:05:52.240 --> 01:05:54.480
So our programming is not faulty

01:05:54.480 --> 01:05:56.400
in the sense that we evolved

01:05:57.040 --> 01:05:59.680
for a particular set of conditions, right?

01:05:59.680 --> 01:06:01.360
And that evolution may not be complete

01:06:01.360 --> 01:06:02.640
or optimal, et cetera, et cetera.

01:06:02.640 --> 01:06:05.840
But roughly speaking, we are not faulty in that sense.

01:06:05.840 --> 01:06:06.400
The reason...

01:06:06.400 --> 01:06:08.400
Because evolution is doing its job, right?

01:06:08.400 --> 01:06:11.440
We have all those impulses for a reason, right?

01:06:11.440 --> 01:06:16.240
Now, the problem is that we, unlike any other species,

01:06:16.240 --> 01:06:19.120
we actually have actually succeeded in creating a world

01:06:20.240 --> 01:06:21.440
that is better for us.

01:06:22.160 --> 01:06:24.320
But at the same time, and this is the problem,

01:06:24.320 --> 01:06:26.640
we're actually now adapted to a different world

01:06:26.640 --> 01:06:27.760
from the one that we live in.

01:06:28.560 --> 01:06:30.480
So the faulty program just comes from the fact

01:06:30.480 --> 01:06:32.640
that we evolved for one set of conditions.

01:06:32.640 --> 01:06:34.800
For example, among many other examples

01:06:34.800 --> 01:06:36.960
where your time horizon was very short,

01:06:36.960 --> 01:06:38.960
and now we live in a very different world.

01:06:38.960 --> 01:06:41.360
And so our job as rational people,

01:06:41.360 --> 01:06:42.960
that's what our rational minds are for,

01:06:42.960 --> 01:06:45.520
among other things, is to now adapt ourselves

01:06:45.520 --> 01:06:47.360
to the world that we really are in

01:06:47.360 --> 01:06:48.880
so that we do things that are rational

01:06:48.880 --> 01:06:50.000
in the world that we're in, right?

01:06:50.000 --> 01:06:53.120
So now, the fact that our programming is faulty

01:06:53.120 --> 01:06:55.840
does not see anything about what are the faults

01:06:55.840 --> 01:06:56.800
and how you fix them.

01:06:56.800 --> 01:06:58.240
And what these people have, I think,

01:06:58.240 --> 01:07:01.200
is first of all, the wrong notion of what our faults are

01:07:01.200 --> 01:07:02.240
and then on top of that,

01:07:02.240 --> 01:07:03.760
the wrong notion of how to fix them.

01:07:04.800 --> 01:07:08.240
Okay, now, I want to get into the utility function again.

01:07:08.720 --> 01:07:10.880
Again, one of the things that makes me skeptical

01:07:10.880 --> 01:07:14.320
is this notion of immutability, both of what we're doing

01:07:14.320 --> 01:07:16.560
and in the case of what we've been speaking about

01:07:16.560 --> 01:07:19.680
with utilitarianism, what the utility function is.

01:07:19.680 --> 01:07:21.680
Now, you were kind of hinting to something interesting before,

01:07:21.680 --> 01:07:23.280
which is that it might be diverse

01:07:23.280 --> 01:07:25.360
and it might also be self-updating.

01:07:25.360 --> 01:07:27.840
But I'm constantly asking myself the question,

01:07:27.840 --> 01:07:29.520
how does that work and who gets to say?

01:07:30.560 --> 01:07:33.360
Well, so very much, I think it's complex

01:07:33.360 --> 01:07:34.800
and it should be self-updating, right?

01:07:34.800 --> 01:07:36.400
We're never going to final itself.

01:07:37.360 --> 01:07:40.640
If you buy this notion that the ultimate arbiter is evolution,

01:07:41.200 --> 01:07:44.320
then utility functions are subject to evolution.

01:07:45.680 --> 01:07:47.360
Right, so you think about it

01:07:47.360 --> 01:07:48.560
or you can't think about it.

01:07:48.560 --> 01:07:50.880
It's a wrong world.

01:07:50.880 --> 01:07:52.080
You can't think about this

01:07:52.080 --> 01:07:54.320
and it's useful to think about this in the following ways.

01:07:54.960 --> 01:07:56.240
To a first approximation,

01:07:56.240 --> 01:07:59.360
the number one entity that's evolving is utility functions.

01:07:59.360 --> 01:08:01.040
What you have in the world at any point

01:08:01.040 --> 01:08:03.840
is a population of utility functions, right?

01:08:03.920 --> 01:08:06.320
And now they combine, they evolve,

01:08:06.320 --> 01:08:08.080
you have next generation of utility functions.

01:08:08.080 --> 01:08:12.560
And then there's also how the utility function gets optimized.

01:08:12.560 --> 01:08:14.880
That is also subject to evolution, right?

01:08:14.880 --> 01:08:17.440
And now how the utility function is optimized

01:08:17.440 --> 01:08:18.560
changes a lot faster

01:08:18.560 --> 01:08:21.200
and this is a lot more complex than the utility function itself,

01:08:21.200 --> 01:08:22.800
which is the point, right?

01:08:22.800 --> 01:08:24.480
So at a certain time horizon,

01:08:24.480 --> 01:08:26.880
it's reasonable to approximate utilities as being fixed.

01:08:26.880 --> 01:08:30.240
Like for example, the utilities that are encoded in your brain

01:08:30.240 --> 01:08:33.360
are fixed by your genes, right?

01:08:33.360 --> 01:08:35.760
So in the context of our present human moment

01:08:35.760 --> 01:08:37.360
and effective ultramism or not,

01:08:37.360 --> 01:08:39.760
it makes perfect sense to think of utilities fixed.

01:08:39.760 --> 01:08:42.240
But it is evolving and not just on

01:08:42.240 --> 01:08:45.520
eon time scales, but by the generation, right?

01:08:45.520 --> 01:08:46.960
Things evolve by the generation.

01:08:46.960 --> 01:08:48.960
Okay, but it's still relatively glacial

01:08:48.960 --> 01:08:52.000
and I take your point that there's a kind of divergence

01:08:52.000 --> 01:08:52.960
between the world we live in

01:08:52.960 --> 01:08:54.480
and the programming that we've got.

01:08:54.480 --> 01:08:58.240
But then, okay, let's imagine that we create a new population

01:08:58.240 --> 01:09:00.960
and I guess what I'm saying is that

01:09:01.040 --> 01:09:04.800
you think that the utility function should emerge and evolve,

01:09:04.800 --> 01:09:08.560
but I would argue for some kind of morphogenetic engineering

01:09:08.560 --> 01:09:11.280
where it's a kind of hybrid between something which is emergent

01:09:11.280 --> 01:09:13.120
but something which we can nudge.

01:09:13.120 --> 01:09:15.920
Oh, I mean, I'm glad you brought that up.

01:09:16.800 --> 01:09:18.400
Nudging is a form of emergence.

01:09:19.120 --> 01:09:20.400
You yourself are emergent

01:09:20.400 --> 01:09:22.160
and the things that you do are emergent as well.

01:09:22.160 --> 01:09:23.520
Everything is emergent, right?

01:09:23.520 --> 01:09:25.040
Utilities are emergent.

01:09:25.040 --> 01:09:26.800
Maybe the laws of physics aren't emergent.

01:09:26.800 --> 01:09:28.320
Some people will say even those are, right?

01:09:28.320 --> 01:09:30.640
Like, you know, we live in a universe with this constant

01:09:30.720 --> 01:09:31.760
because blah, blah, blah, right?

01:09:31.760 --> 01:09:33.840
So, but to first approximation,

01:09:33.840 --> 01:09:36.640
every single thing that we've been talking about is emergent.

01:09:36.640 --> 01:09:40.400
We make a distinction between emergent and designed

01:09:40.400 --> 01:09:42.720
because that is anthropomorphic, right?

01:09:42.720 --> 01:09:44.880
Is this things that we do are not emergent?

01:09:44.880 --> 01:09:46.480
Actually, no, when you nudge something

01:09:46.480 --> 01:09:48.160
that is an emergent behavior, right?

01:09:48.160 --> 01:09:49.920
We are emergent as well, right?

01:09:49.920 --> 01:09:51.520
So everything that is human, you know,

01:09:51.520 --> 01:09:53.840
so here's a very good way, I think,

01:09:53.840 --> 01:09:54.960
to think about a lot of things

01:09:54.960 --> 01:09:57.360
which I first saw, you know, in Richard Dawkins,

01:09:57.360 --> 01:09:59.600
which is although he really didn't go into this

01:09:59.600 --> 01:10:01.360
and I wish he had like this notion

01:10:01.360 --> 01:10:03.440
of the extended phenotype, right?

01:10:04.080 --> 01:10:06.960
Technology is our extended phenotype.

01:10:06.960 --> 01:10:08.800
So all these things that we do, right?

01:10:08.800 --> 01:10:09.840
All these things that we build,

01:10:09.840 --> 01:10:11.200
including AS and whatnot,

01:10:11.200 --> 01:10:13.440
they are extensions of our phenotype.

01:10:13.440 --> 01:10:15.840
So if you take the long view, all of, you know,

01:10:15.840 --> 01:10:19.360
technology is the continuation of biology by another means.

01:10:19.920 --> 01:10:21.120
So when you make this distinction

01:10:21.120 --> 01:10:22.640
between emergent and not emergent

01:10:22.640 --> 01:10:25.200
and top down and bottom up, it's all emergent.

01:10:25.200 --> 01:10:27.600
Interesting. Well, we recently did a show on emergence

01:10:27.600 --> 01:10:29.840
and it's a topic of interest to me personally

01:10:29.840 --> 01:10:32.400
and there's weak emergence and strong emergence

01:10:32.400 --> 01:10:34.240
and there's, you know, like the view of weak emergence,

01:10:34.240 --> 01:10:37.360
so there's some, you know, surprising macroscopic phenomena,

01:10:37.360 --> 01:10:39.200
maybe something which transiently emerges

01:10:39.200 --> 01:10:41.200
and Wolfram would add in the whole, you know,

01:10:41.200 --> 01:10:43.200
computational irreducibility angle.

01:10:43.200 --> 01:10:44.560
And then with the strong emergence,

01:10:44.560 --> 01:10:45.840
Chalmers would say it's something

01:10:45.840 --> 01:10:47.600
which is paradigmatically surprising.

01:10:47.600 --> 01:10:49.360
It's something which is not deducible

01:10:49.360 --> 01:10:51.440
for many fundamental truths in the lower level domain.

01:10:51.440 --> 01:10:54.720
But I just wondered, like, how do you think about emergence?

01:10:54.720 --> 01:10:56.880
Well, I think that is a very, the distinction

01:10:56.880 --> 01:10:59.520
between weak and strong immersion is a very useful one.

01:10:59.520 --> 01:11:00.320
Right.

01:11:00.320 --> 01:11:03.840
And I would actually phrase it in slightly different terms,

01:11:03.840 --> 01:11:06.320
which is starting from physics, right?

01:11:07.280 --> 01:11:14.000
I think most physicists and scientists believe in weak emergence.

01:11:14.000 --> 01:11:16.160
Well, could I, could I add that Sabine Hossenfelder

01:11:16.160 --> 01:11:18.880
had a paper and she frames it with this idea

01:11:18.880 --> 01:11:21.440
of the resolution of physical theories.

01:11:21.440 --> 01:11:23.360
So like, like a lower resolution theory

01:11:23.360 --> 01:11:25.760
as weakly emergent from a high resolution theory.

01:11:25.760 --> 01:11:26.400
Well, exactly.

01:11:26.400 --> 01:11:28.000
And, you know, like, I like Sabine,

01:11:28.000 --> 01:11:29.680
but this is not her idea, right?

01:11:29.680 --> 01:11:31.760
This far predates all of us here, right?

01:11:31.760 --> 01:11:32.320
Yeah.

01:11:32.320 --> 01:11:34.560
And again, it's, it's a very interesting history

01:11:34.560 --> 01:11:36.000
and a very important concept.

01:11:36.000 --> 01:11:40.960
Now, so my point was that I think few people have a quarrel

01:11:40.960 --> 01:11:42.400
with the notion of weak emergence

01:11:42.400 --> 01:11:43.920
in the sense that, you know,

01:11:43.920 --> 01:11:45.360
I can give you a theory of everything

01:11:45.360 --> 01:11:47.040
in the form of whatever string theory

01:11:47.040 --> 01:11:48.640
let's take a candidate, right?

01:11:48.640 --> 01:11:50.880
But no string theory claims that that's a theory

01:11:50.880 --> 01:11:52.800
of everything in the sense that like now,

01:11:52.800 --> 01:11:54.960
to study biology or psychology or sociology,

01:11:54.960 --> 01:11:56.080
you should just study string theory.

01:11:56.080 --> 01:11:57.600
No one believes that, right?

01:11:57.600 --> 01:11:59.440
There's actually interesting things to be said there,

01:11:59.440 --> 01:12:01.120
but, but let's not, let's look at,

01:12:01.120 --> 01:12:02.880
let's not go there for a second, right?

01:12:02.880 --> 01:12:06.560
There are these levels that emerge weakly

01:12:06.560 --> 01:12:09.760
in the sense that they are determined by the lower levels.

01:12:10.400 --> 01:12:11.840
They're just so much more complex

01:12:11.840 --> 01:12:14.000
that you're better off focusing on the menu.

01:12:14.000 --> 01:12:15.520
Now there's this other notion which to me

01:12:15.520 --> 01:12:17.360
is the really interesting one,

01:12:17.360 --> 01:12:20.480
which is that there is, there are phenomena

01:12:20.480 --> 01:12:22.000
that are at the higher levels

01:12:22.000 --> 01:12:26.160
that are just not reducible to the lower levels, right?

01:12:26.160 --> 01:12:27.840
So the true emergent is in some sense

01:12:27.840 --> 01:12:29.440
is someone who believes the latter.

01:12:29.440 --> 01:12:30.640
And now you can ask the question,

01:12:30.640 --> 01:12:33.040
like do you believe in that or not, right?

01:12:33.040 --> 01:12:37.360
And I think to give the very short answer first

01:12:37.360 --> 01:12:39.920
is that ultimately there's probably no way of knowing.

01:12:41.120 --> 01:12:44.960
But pragmatically, you're actually probably better off

01:12:44.960 --> 01:12:47.600
treating the world as if it has strong emergence.

01:12:47.600 --> 01:12:49.280
And now strong emergence is actually

01:12:49.280 --> 01:12:50.960
a very strong segment to make is to say,

01:12:50.960 --> 01:12:53.200
and by the way, going down to the lowest levels

01:12:53.200 --> 01:12:54.560
to make things very clear,

01:12:54.560 --> 01:12:56.640
you don't need to think about biology or society

01:12:56.640 --> 01:12:58.480
or consciousness or anything.

01:12:58.480 --> 01:13:00.720
Condensed metaphysics, right?

01:13:01.280 --> 01:13:03.360
The particle physicists tend to believe

01:13:03.360 --> 01:13:06.400
that what they do is what everything reduces to.

01:13:06.400 --> 01:13:08.800
You talk to the condensed metaphysicists.

01:13:08.800 --> 01:13:10.240
This was actually an interesting discussion

01:13:10.240 --> 01:13:12.000
that I had with Scott, you know, Aronson,

01:13:12.000 --> 01:13:13.520
because like he was very much on the,

01:13:13.520 --> 01:13:15.040
we're both computer scientists,

01:13:15.040 --> 01:13:17.120
but he was very much on the side of the particle physicists,

01:13:17.120 --> 01:13:20.080
I don't know very much on the side of the condensed metaphysicists.

01:13:20.080 --> 01:13:22.560
What they will tell you over and over again,

01:13:22.560 --> 01:13:26.080
they see is things that you cannot explain

01:13:26.080 --> 01:13:27.680
using quantum mechanics.

01:13:27.680 --> 01:13:29.200
And now people say like,

01:13:29.200 --> 01:13:31.440
oh, but you can always explain things in quantum mechanics.

01:13:31.440 --> 01:13:33.120
You just haven't done the calculations.

01:13:33.120 --> 01:13:36.320
But the point is precisely that you can't do the calculations, right?

01:13:36.320 --> 01:13:37.840
The calculations are chaotic.

01:13:38.640 --> 01:13:41.280
I have a theory, I can come up with 500 theories

01:13:41.280 --> 01:13:43.840
of these phenomena and semiconductors and whatnot.

01:13:43.840 --> 01:13:45.840
And like, I never actually get to test them

01:13:45.840 --> 01:13:48.560
because the computations diverge before I get to test them.

01:13:48.560 --> 01:13:52.640
So for all intents and purposes, it is strong emergence.

01:13:52.640 --> 01:13:55.520
Whether truly that came from below is unanswerable

01:13:55.520 --> 01:13:57.280
because you can't compute the predictions.

01:13:57.280 --> 01:13:58.320
Well, we spoke about that.

01:13:58.320 --> 01:14:01.120
So I think Keith would call that semi-strong emergence,

01:14:01.120 --> 01:14:03.360
which is like, you know, whether it's computationally reachable

01:14:03.360 --> 01:14:06.400
from the lower resolution to the high resolution

01:14:06.400 --> 01:14:07.760
to the lower resolution.

01:14:07.760 --> 01:14:11.120
But no, Sabine in her paper, A Case for Strong Emergence,

01:14:11.120 --> 01:14:12.640
she was talking about singularities

01:14:12.640 --> 01:14:16.080
as being a really good example of what might be strong emergence.

01:14:16.080 --> 01:14:19.440
And the philosopher Mark Badau, I think, said that

01:14:19.440 --> 01:14:21.360
strong emergence is ridiculous.

01:14:21.360 --> 01:14:23.760
It's basically an affront on physicalism.

01:14:24.640 --> 01:14:27.600
Well, certainly, you know, strong emergence and physicalism,

01:14:27.600 --> 01:14:29.440
or let's just call it reductionism, right?

01:14:29.440 --> 01:14:30.160
Reductionism, yeah.

01:14:30.160 --> 01:14:33.040
Strong emergence and reductionism are incompatible.

01:14:33.040 --> 01:14:33.680
Yeah.

01:14:33.680 --> 01:14:37.520
And we scientists tend to be reductionists, right?

01:14:37.520 --> 01:14:41.040
Now, at some level, I'm both a reductionist

01:14:41.040 --> 01:14:43.280
and someone who is willing to believe in strong emergence.

01:14:43.280 --> 01:14:45.840
Again, I don't believe in strong emergence.

01:14:45.840 --> 01:14:49.440
I just don't see a way to disprove it, right?

01:14:49.440 --> 01:14:52.320
And like, you know, if there's an empirical way

01:14:52.320 --> 01:14:54.800
to distinguish semi-strong from strong emergence,

01:14:54.800 --> 01:14:56.800
I'd be very interested to know what it is.

01:14:56.800 --> 01:14:59.040
But now, I think the thing that is very important

01:14:59.040 --> 01:15:01.440
that a lot of people, including a lot of physicists

01:15:01.440 --> 01:15:05.760
and scientists don't see is that we have this hypothesis

01:15:05.760 --> 01:15:08.080
that everything can be reduced to the laws of physics

01:15:08.080 --> 01:15:08.960
as we know it.

01:15:08.960 --> 01:15:12.080
We should not forget that it's just a hypothesis.

01:15:12.080 --> 01:15:13.760
And it's a hypothesis that, again,

01:15:13.840 --> 01:15:15.600
counter to a lot of people's say,

01:15:15.600 --> 01:15:18.720
is very, very, very, very far from established.

01:15:18.720 --> 01:15:20.800
And usually, people say like, oh, but, you know,

01:15:20.800 --> 01:15:24.000
look at all the successes of the laws of physics and blah, blah.

01:15:24.000 --> 01:15:24.960
And then I say, like, you know,

01:15:24.960 --> 01:15:26.560
putting on my machine learning hat,

01:15:26.560 --> 01:15:30.000
the sample that you've used to validate the laws of physics

01:15:30.000 --> 01:15:36.160
is extraordinarily biased in the direction of simple systems.

01:15:36.160 --> 01:15:39.360
OK, so you can't make this claim of if the data was IID,

01:15:39.360 --> 01:15:41.320
I could say with great confidence,

01:15:41.320 --> 01:15:42.800
these laws apply universally.

01:15:42.800 --> 01:15:43.920
But I haven't done it.

01:15:43.920 --> 01:15:47.280
It's more like I've just landed in a new continent

01:15:47.280 --> 01:15:49.200
and I've sealed up all the rivers.

01:15:49.200 --> 01:15:51.760
And I say, I know what this continent looks like.

01:15:51.760 --> 01:15:53.200
You've never climbed the mountains.

01:15:53.200 --> 01:15:54.560
You've never gone in the jungle.

01:15:54.560 --> 01:15:56.640
So like this notion that the laws of physics

01:15:56.640 --> 01:15:58.640
capture everything about daily life,

01:15:58.640 --> 01:16:00.320
we just don't know how exactly.

01:16:00.320 --> 01:16:02.160
Maybe it's true, but it could also

01:16:02.160 --> 01:16:03.600
equally well be completely false.

01:16:04.560 --> 01:16:04.880
Brilliant.

01:16:04.880 --> 01:16:06.880
Well, you gave a bit of a hint to this earlier, actually,

01:16:06.880 --> 01:16:09.200
because you used the word relationism, right?

01:16:09.200 --> 01:16:10.240
Which is basically the...

01:16:10.240 --> 01:16:11.600
Or relationalism.

01:16:11.600 --> 01:16:12.000
Relationalism.

01:16:12.080 --> 01:16:14.000
Maybe it should be shortened to relationalism.

01:16:14.000 --> 01:16:14.800
Relationalism.

01:16:15.680 --> 01:16:18.720
But yeah, I think Rosen is a great advocate of this,

01:16:18.720 --> 01:16:22.240
and he has a whole category theory calculus

01:16:22.240 --> 01:16:24.480
for describing living systems.

01:16:25.040 --> 01:16:26.880
And also we spoke to Bob Koek,

01:16:26.880 --> 01:16:29.760
the quantum physics professor from Cambridge,

01:16:29.760 --> 01:16:32.720
and he was talking about this concept of Cartesian togetherness,

01:16:32.720 --> 01:16:34.400
which is another category or framework.

01:16:34.400 --> 01:16:37.280
But I just wondered, does that inform your view?

01:16:38.160 --> 01:16:41.200
Well, relationalism, at least in one way

01:16:41.200 --> 01:16:44.240
of defining the term, very much informs my view, right?

01:16:44.240 --> 01:16:46.560
And one way to come at this is to say,

01:16:47.200 --> 01:16:49.680
the world is not made of independent entities.

01:16:49.680 --> 01:16:51.040
Actually, let's just start with machine learning,

01:16:51.040 --> 01:16:52.800
which is a very concrete way to look at this.

01:16:53.360 --> 01:16:55.840
A very large part, maybe even the largest part

01:16:55.840 --> 01:16:57.680
of my research in the last 20 years,

01:16:57.680 --> 01:17:01.840
has been to do away with the assumption of IID data, right?

01:17:01.840 --> 01:17:03.840
That the world is made of independent entities,

01:17:03.840 --> 01:17:06.560
in particular, society is made of independent agents,

01:17:06.560 --> 01:17:07.840
et cetera, et cetera, right?

01:17:07.840 --> 01:17:09.600
Now, we make this assumption,

01:17:09.600 --> 01:17:12.400
both as human beings, to some extent,

01:17:12.400 --> 01:17:14.400
and certainly very much so in science,

01:17:14.400 --> 01:17:15.840
because it makes life easier.

01:17:16.720 --> 01:17:19.680
The math is way, way, way easier when you assume independence.

01:17:19.680 --> 01:17:22.880
But it's a blatantly false assumption, right?

01:17:22.880 --> 01:17:24.320
Unfortunately, a lot of, for example,

01:17:24.320 --> 01:17:27.600
economics prominently has embedded in it this notion

01:17:27.600 --> 01:17:29.680
that the world is a bunch of independent agents,

01:17:29.680 --> 01:17:31.520
and it just doesn't work like that.

01:17:31.520 --> 01:17:34.720
And moreover, it's a distinction that is full of consequences.

01:17:34.720 --> 01:17:38.400
A society and economy is a network of agents,

01:17:38.400 --> 01:17:40.960
and almost all the action is in their interactions.

01:17:41.680 --> 01:17:43.760
Until you really start taking that seriously,

01:17:43.760 --> 01:17:45.040
you really don't understand the world.

01:17:45.040 --> 01:17:47.440
Again, I have no quarrel with classic economics

01:17:47.440 --> 01:17:48.640
as a first approximation.

01:17:48.640 --> 01:17:50.160
It's exactly what it should be, right?

01:17:50.720 --> 01:17:51.760
But then, and by the way,

01:17:51.760 --> 01:17:53.440
you should also not just throw it away and say,

01:17:53.440 --> 01:17:55.280
like, oh, this is garbage, like some people say.

01:17:55.280 --> 01:17:56.400
You have to go the next stage.

01:17:56.400 --> 01:17:58.720
It's actually now we have the mathematical

01:17:58.720 --> 01:18:00.320
and computational tools to do,

01:18:00.320 --> 01:18:03.280
and understand it as being a system of interacting agents.

01:18:03.840 --> 01:18:05.840
And all of the questions that we are talking about,

01:18:05.840 --> 01:18:10.080
including in evolution, even in physics, right?

01:18:11.200 --> 01:18:14.720
A piece of condensed matter is a network of interacting,

01:18:14.720 --> 01:18:16.240
spins, et cetera, et cetera, you name it.

01:18:16.240 --> 01:18:18.800
So the relations are at the heart of it,

01:18:18.800 --> 01:18:20.560
and moreover, like as I said,

01:18:20.560 --> 01:18:23.440
a lot of my work is we now have the representations,

01:18:23.440 --> 01:18:25.440
the learning inference algorithms to handle things

01:18:25.440 --> 01:18:27.520
that are big piles of relations,

01:18:27.520 --> 01:18:30.080
and the whole world is better understood in those terms,

01:18:30.080 --> 01:18:32.080
and we just need people to catch up with that.

01:18:32.800 --> 01:18:34.240
You know, once you do that,

01:18:34.240 --> 01:18:36.080
you get into things that can easily be

01:18:36.080 --> 01:18:38.000
computational, intractable, and so on and so forth.

01:18:38.000 --> 01:18:40.160
But there's a lot of things that we can do there

01:18:40.160 --> 01:18:41.440
and a lot more that we'll do.

01:18:41.440 --> 01:18:44.160
So at this level, I think relationalism

01:18:44.160 --> 01:18:46.560
is really should be a cornerstone

01:18:46.560 --> 01:18:48.160
of our understanding of the world

01:18:48.160 --> 01:18:49.920
in a way that it hasn't been in the past.

01:18:50.480 --> 01:18:53.600
Okay, and which existing complexity science brings to mind?

01:18:53.600 --> 01:18:57.440
But I mean, which existing techniques and areas

01:18:57.440 --> 01:19:00.560
can folks look into to take that on board?

01:19:00.640 --> 01:19:02.400
Well, you know, Markov logic,

01:19:02.400 --> 01:19:05.120
which is what I developed for this purpose essentially,

01:19:05.120 --> 01:19:07.280
and I do think, you know,

01:19:07.280 --> 01:19:08.800
this is my talking about my work,

01:19:08.800 --> 01:19:10.320
so you should naturally be suspicious,

01:19:10.320 --> 01:19:12.960
but I think it's the best that we have,

01:19:12.960 --> 01:19:15.120
and I think by a wide measure,

01:19:15.120 --> 01:19:17.120
compared to anything else that we have so far.

01:19:17.120 --> 01:19:18.560
Okay, and can you sketch it out?

01:19:18.560 --> 01:19:22.880
Yeah, so to sketch it out in the simplest terms, right,

01:19:22.880 --> 01:19:26.240
we want to combine all the traditional goodies

01:19:26.240 --> 01:19:28.400
that we have from assuming the world is IID

01:19:28.400 --> 01:19:30.480
with the power to model, you know, relationships,

01:19:30.480 --> 01:19:32.640
there are themselves potentially very complicated.

01:19:32.640 --> 01:19:34.720
The way we do the Markov logic is,

01:19:34.720 --> 01:19:35.840
there's the logical part.

01:19:36.640 --> 01:19:39.840
We actually do not need to solve a new,

01:19:39.840 --> 01:19:41.680
the problem of how to represent

01:19:41.680 --> 01:19:43.120
and do inference with relations.

01:19:43.120 --> 01:19:45.200
We have first order logic for that.

01:19:45.760 --> 01:19:48.240
First order logic is the language of relations.

01:19:48.240 --> 01:19:50.320
That's actually the term that is used, right,

01:19:50.320 --> 01:19:52.080
and how the relations depend predicates.

01:19:52.080 --> 01:19:53.280
Sometimes they're called predicates,

01:19:53.280 --> 01:19:55.440
but let's just call them relations, right?

01:19:55.440 --> 01:19:57.760
We have a formal language to talk about relations.

01:19:57.760 --> 01:20:00.080
And by the way, essentially all of computer science

01:20:00.080 --> 01:20:00.960
can be reduced to that.

01:20:00.960 --> 01:20:03.120
You give me your favorite, you know, whatever,

01:20:03.120 --> 01:20:06.400
knowledge representation, data structure, et cetera, et cetera.

01:20:06.400 --> 01:20:09.680
And I, anybody who knows can immediately say

01:20:09.680 --> 01:20:10.720
how to do that in logic.

01:20:10.720 --> 01:20:11.920
So that's one part.

01:20:11.920 --> 01:20:14.720
The other part is the statistical, you know,

01:20:14.720 --> 01:20:17.280
machine learning probabilistic aspect of the world, right?

01:20:17.280 --> 01:20:20.080
And then again, going all the way back to physics, right?

01:20:20.800 --> 01:20:22.160
All of these things that we deal with

01:20:22.160 --> 01:20:24.160
are essentially special cases of what are

01:20:24.160 --> 01:20:26.000
variously called Markov networks,

01:20:26.000 --> 01:20:28.080
which is where the name Markov comes from.

01:20:28.080 --> 01:20:30.960
Or graphical models, or log linear models,

01:20:30.960 --> 01:20:34.160
Gibbs distributions, Boltzmann machines, right?

01:20:34.160 --> 01:20:36.480
All of these things are essentially the same, right?

01:20:36.480 --> 01:20:39.440
That whole neck of the woods is captured by Markov networks.

01:20:39.440 --> 01:20:40.240
Let's call them that.

01:20:41.200 --> 01:20:43.840
And Markov logic is combining Markov networks

01:20:43.840 --> 01:20:45.920
with first order logic in a single language,

01:20:45.920 --> 01:20:47.840
which you can now do everything with.

01:20:47.840 --> 01:20:48.480
Okay, okay.

01:20:48.480 --> 01:20:50.960
So just to like push back a tiny bit.

01:20:50.960 --> 01:20:53.760
So in the past, we've tried to create,

01:20:54.720 --> 01:20:56.000
let's say things like psych,

01:20:56.000 --> 01:20:58.720
which is a knowledge representation of the world.

01:20:58.720 --> 01:21:01.120
Folks like Montague have tried to do semantics

01:21:01.120 --> 01:21:03.760
using first order logic to set some,

01:21:03.760 --> 01:21:05.760
you know, varying degrees of success.

01:21:05.760 --> 01:21:07.120
And then we have the grounding problem

01:21:07.120 --> 01:21:09.440
we were just talking before about, you know,

01:21:09.440 --> 01:21:10.480
like even Searle said this,

01:21:10.480 --> 01:21:13.840
that you have kind of epistemic objectivity

01:21:13.840 --> 01:21:16.320
and subjectivity and some things are observer relative,

01:21:16.320 --> 01:21:18.560
like even economics is observer relative.

01:21:18.560 --> 01:21:22.080
So with this kind of formalism, how would that work?

01:21:22.080 --> 01:21:23.280
That's so very good.

01:21:23.280 --> 01:21:25.600
The problem with or the main problem

01:21:25.600 --> 01:21:27.520
with a lot of these things that you mentioned,

01:21:27.520 --> 01:21:29.920
like, you know, certain types of semantics and whatnot,

01:21:29.920 --> 01:21:32.240
that are based essentially on first order logic, right?

01:21:32.240 --> 01:21:33.360
Is that they're too brittle.

01:21:34.000 --> 01:21:37.600
In fact, the problem with symbolic AI is that it's too brittle.

01:21:38.160 --> 01:21:41.040
And this is exactly what Markov logic fixes.

01:21:41.040 --> 01:21:43.440
It fixes it by making it statistical.

01:21:43.440 --> 01:21:45.280
When I give you a logical statement now,

01:21:45.280 --> 01:21:47.440
I'm no longer, for example, simple logical statement,

01:21:48.240 --> 01:21:50.960
you know, a smoking causes cancer, right?

01:21:51.680 --> 01:21:53.520
In English, this is a valid statement.

01:21:53.520 --> 01:21:54.880
Smoking does cause cancer.

01:21:54.880 --> 01:21:56.640
But actually, once you translate it to logic

01:21:56.640 --> 01:21:59.280
for every x, smokes of x implies cancer of x,

01:21:59.280 --> 01:22:02.320
it's false because some smokers don't get cancer, right?

01:22:02.320 --> 01:22:04.160
What this really was meant to be all along

01:22:04.160 --> 01:22:06.240
is a statistical statement that says,

01:22:06.240 --> 01:22:08.240
smokers are more likely to get cancer.

01:22:09.120 --> 01:22:11.520
So the way we overcome a lot of those problems

01:22:11.520 --> 01:22:14.560
is precisely that we take all of this logic,

01:22:14.560 --> 01:22:15.920
which again, the language exists,

01:22:15.920 --> 01:22:18.480
we don't have to change it, we can, but we don't have to.

01:22:18.480 --> 01:22:19.920
And we make it statistical.

01:22:20.480 --> 01:22:22.800
As a result of which, it's no longer brittle.

01:22:22.800 --> 01:22:24.240
Or at least now it's only as brittle

01:22:24.240 --> 01:22:26.320
as machine learning and graphical model or not.

01:22:26.320 --> 01:22:29.440
It's not as brittle as, you know, traditional symbolic AI.

01:22:29.440 --> 01:22:31.520
Okay. And so we speak into a lot of Go-Fi people

01:22:31.520 --> 01:22:33.680
and I mean, Wally Subba, for example, he's a rationalist

01:22:33.680 --> 01:22:35.040
and what's interesting about the rationalist

01:22:35.040 --> 01:22:37.920
is they hate any form of uncertainty, right?

01:22:37.920 --> 01:22:39.360
They think in absolute binaries.

01:22:39.360 --> 01:22:40.400
You either know it or you don't.

01:22:40.400 --> 01:22:42.160
No, I mean, let me push back on that.

01:22:42.160 --> 01:22:44.480
There's this, again, you need to distinguish, you know,

01:22:44.480 --> 01:22:47.760
a general field or idea from its subtypes, right?

01:22:47.760 --> 01:22:49.680
There is a type of rationalist.

01:22:49.760 --> 01:22:53.280
That hits uncertainty, big mistake, big, big mistake.

01:22:53.280 --> 01:22:55.200
There's a type of rationalist that, you know,

01:22:55.200 --> 01:22:57.200
uncertainty is what they, you know, like,

01:22:58.720 --> 01:23:01.840
an uncertainty calculus is a type of rationalism.

01:23:01.840 --> 01:23:04.640
And some of the best, you know, AI, philosophy, etc.

01:23:04.640 --> 01:23:05.280
is just that.

01:23:05.280 --> 01:23:07.920
So there is no incompatibility at all

01:23:07.920 --> 01:23:09.520
between rationalism and uncertainty.

01:23:09.520 --> 01:23:13.600
In fact, if rationalism, if being rational is maximizing

01:23:13.600 --> 01:23:17.120
expected utility, notice the expected in there, right?

01:23:17.120 --> 01:23:20.160
You cannot be rational if you ignore the uncertainty.

01:23:20.160 --> 01:23:20.800
Interesting.

01:23:20.800 --> 01:23:23.680
Okay, but then what about the resolution of modeling?

01:23:23.680 --> 01:23:25.680
I mean, smoking is a really good one.

01:23:25.680 --> 01:23:28.320
So us humans, we anthropomorphize things.

01:23:28.320 --> 01:23:30.640
We understand the world in macroscopic terms

01:23:30.640 --> 01:23:33.360
using macroscopic ideas that we understand.

01:23:33.360 --> 01:23:35.760
And that kind of leads to a certain type of modeling.

01:23:35.760 --> 01:23:37.840
And that modeling presumably would be represented

01:23:37.840 --> 01:23:40.320
at that resolution, you know, using this formalism.

01:23:41.360 --> 01:23:42.800
Sure. And what's the question?

01:23:42.800 --> 01:23:46.160
Well, it seemed, again, like I'm intuitively suspicious

01:23:46.160 --> 01:23:48.480
that we were just saying the world is a complex place.

01:23:48.480 --> 01:23:50.960
And with a lot of causal modeling, for example,

01:23:50.960 --> 01:23:53.440
a lot of the art is understanding what is relevant

01:23:53.440 --> 01:23:54.880
and what is not relevant.

01:23:54.880 --> 01:23:58.080
What is relevant might just be kind of, you know, relevant to us.

01:23:58.640 --> 01:24:01.120
No, well, what is relevant is what is relevant

01:24:01.120 --> 01:24:02.800
relative to your utility function.

01:24:03.520 --> 01:24:06.480
Okay, again, it gets back to that precisely.

01:24:07.680 --> 01:24:09.920
The whole problem is that the world is infinitely complex

01:24:09.920 --> 01:24:11.920
and we have only finite computational resources,

01:24:11.920 --> 01:24:14.080
whether it's in our brains or our computers or whatever, right?

01:24:14.080 --> 01:24:15.280
So now what do you do, right?

01:24:15.360 --> 01:24:18.160
You are forced to oversimplify the world,

01:24:18.160 --> 01:24:20.800
not just simplify, but oversimplify, right?

01:24:20.800 --> 01:24:23.200
But now the whole art, that's actually a good word to use,

01:24:23.200 --> 01:24:24.480
even if it's done with computers,

01:24:24.480 --> 01:24:28.960
is how do you not only oversimplify as little as you can,

01:24:29.520 --> 01:24:35.200
but pick out the simplifications that are least harmful to your objective.

01:24:35.200 --> 01:24:37.520
By the way, the art of the physicist,

01:24:37.520 --> 01:24:40.960
physicist would tell you, is precisely doing this, right?

01:24:40.960 --> 01:24:43.840
Physicists are very good at deciding what to simplify.

01:24:43.840 --> 01:24:45.600
And in fact, almost, I think at some level,

01:24:45.600 --> 01:24:47.760
almost any good scientist, this is what they do, right?

01:24:47.760 --> 01:24:51.680
So, and now how do I decide what and how to simplify

01:24:51.680 --> 01:24:54.640
is by relevance to my utility function, right?

01:24:55.280 --> 01:24:57.280
I want to ignore parts of the world

01:24:57.280 --> 01:25:00.800
that do not affect my utility function, number one, right?

01:25:00.800 --> 01:25:03.040
And for example, the notion of conditional independence,

01:25:03.040 --> 01:25:05.440
which is the foundation of graphical models,

01:25:05.440 --> 01:25:06.720
that's what the whole idea is.

01:25:06.720 --> 01:25:08.640
It's like, once I know these things,

01:25:08.640 --> 01:25:10.320
I don't have to know about those others.

01:25:10.320 --> 01:25:12.080
Thank God, right?

01:25:12.080 --> 01:25:14.080
Okay, but if Ken Stanley was here,

01:25:14.080 --> 01:25:16.640
he says that the great thing about evolution is it's divergent,

01:25:16.640 --> 01:25:19.680
it's not convergent, it's discovering new information.

01:25:19.680 --> 01:25:21.600
And my worry is with a system like this,

01:25:21.600 --> 01:25:23.920
with any form of anthropomorphic design,

01:25:23.920 --> 01:25:25.840
would inevitably become convergent.

01:25:25.840 --> 01:25:28.640
And it might look like, oh, those things over there

01:25:28.640 --> 01:25:29.840
that we're ignoring don't matter,

01:25:29.840 --> 01:25:31.600
but actually they might really matter

01:25:31.600 --> 01:25:32.880
if they got introduced into the utility.

01:25:32.880 --> 01:25:38.800
Well, I wouldn't say that maximizing expected utility is anthropomorphic, right?

01:25:38.800 --> 01:25:40.080
In fact, it's one of the least...

01:25:41.040 --> 01:25:43.680
I think maybe there's some degree of anthropomorphism

01:25:43.680 --> 01:25:45.120
is almost anything we do,

01:25:45.120 --> 01:25:49.040
and the progress of science is becoming less and less anthropomorphic,

01:25:49.040 --> 01:25:50.800
and we should keep pushing on that.

01:25:50.800 --> 01:25:53.760
But I would say that maximizing expected utility

01:25:53.760 --> 01:25:56.480
is one of the least anthropomorphic things we can do.

01:25:56.480 --> 01:25:58.240
Well, this is actually a really interesting point,

01:25:58.240 --> 01:26:00.720
because one of the key tenets of the rationalist movement

01:26:00.720 --> 01:26:02.000
and their conception of intelligence,

01:26:02.000 --> 01:26:05.200
because all of the other definitions of intelligence

01:26:05.200 --> 01:26:06.880
are anthropomorphic.

01:26:06.880 --> 01:26:09.600
So, you know, there's based on behavior, capability, AI,

01:26:10.320 --> 01:26:15.280
principal function is a big one, you know, from Norvig.

01:26:15.280 --> 01:26:18.160
And this is the principal based AI,

01:26:18.160 --> 01:26:21.280
which is just making rational moves.

01:26:21.280 --> 01:26:26.160
So why is there such a push to be as, you know,

01:26:26.160 --> 01:26:29.840
to be as the least amount anthropomorphic?

01:26:30.480 --> 01:26:34.000
Oh, the push is not to be, at least in my view,

01:26:34.480 --> 01:26:36.960
being less anthropomorphic is not a goal.

01:26:37.600 --> 01:26:39.040
That's not the goal.

01:26:39.040 --> 01:26:42.720
The goal is to be as accurate and complete as we can

01:26:42.720 --> 01:26:44.880
in modeling the world, right?

01:26:44.880 --> 01:26:47.680
We're just trying to understand the world better, right?

01:26:48.320 --> 01:26:50.400
For whatever purpose, maybe for its own sake,

01:26:50.400 --> 01:26:54.080
maybe for the purpose of the utility and the evolution and so on, right?

01:26:54.080 --> 01:26:54.960
But that's the goal.

01:26:54.960 --> 01:26:57.040
The problem is that,

01:26:57.040 --> 01:26:59.600
and this has been the problem since they won, right?

01:26:59.600 --> 01:27:01.200
They won of humanity,

01:27:01.200 --> 01:27:03.840
is that because we anthropomorphize the world,

01:27:03.840 --> 01:27:07.360
that gets in the way of understanding how it really works, right?

01:27:07.360 --> 01:27:10.560
If I say the wind is some God blowing, right?

01:27:10.560 --> 01:27:11.600
I understand, right?

01:27:11.600 --> 01:27:13.040
That's all they could think of.

01:27:13.040 --> 01:27:15.600
But it's a big obstacle to understanding what the wind really is,

01:27:15.600 --> 01:27:18.560
like there's a pressure difference, et cetera, et cetera, right?

01:27:18.560 --> 01:27:21.440
And we've done away with a lot of anthropomorphism.

01:27:21.440 --> 01:27:23.200
By the way, one of the problems that we're always having

01:27:23.200 --> 01:27:25.760
is that it's always pushing back, right?

01:27:25.760 --> 01:27:27.120
You know, there's always, you know, again,

01:27:27.120 --> 01:27:30.560
intuitively we have a very strong tendency to anthropomorphism,

01:27:31.040 --> 01:27:33.600
as much as science broadly construed as a great victory,

01:27:33.600 --> 01:27:35.360
it's always in danger from this, right?

01:27:35.360 --> 01:27:37.200
But even within science,

01:27:37.200 --> 01:27:42.160
we've gone from doing away with the obvious forms of anthropomorphism

01:27:42.160 --> 01:27:43.600
and anthropomorphism

01:27:43.600 --> 01:27:46.400
to having many things that are still there

01:27:46.400 --> 01:27:50.480
that are less obviously anthropomorphic, but still are, right?

01:27:50.480 --> 01:27:53.120
But if there's something anthropomorphic that's actually is accurate,

01:27:53.120 --> 01:27:54.480
then more power to it.

01:27:54.480 --> 01:27:55.120
Interesting, yeah.

01:27:55.120 --> 01:27:57.920
And then I guess we have so many cognitive priors, right?

01:27:58.160 --> 01:28:00.480
In our brains that give us a cone of attention,

01:28:00.480 --> 01:28:03.280
which is completely anthropocentric.

01:28:03.280 --> 01:28:05.120
Well, very good.

01:28:05.120 --> 01:28:09.040
So those priors, and maybe a better term is heuristics, right?

01:28:09.040 --> 01:28:13.040
Our brains are full of heuristics that evolution put there

01:28:13.040 --> 01:28:16.240
for a good reason, because those heuristics work, right?

01:28:16.240 --> 01:28:17.840
But they are heuristics.

01:28:17.840 --> 01:28:19.760
So they have failure modes, right?

01:28:19.760 --> 01:28:24.160
And we need to understand what is that those heuristics really are getting at

01:28:24.160 --> 01:28:27.760
so that we also, so that we use them when they're good.

01:28:27.760 --> 01:28:29.760
But then when they're not good, we use something else.

01:28:30.640 --> 01:28:31.920
Brilliant, brilliant.

01:28:31.920 --> 01:28:34.000
So Pedro, we're here at NeurIPS this week.

01:28:34.000 --> 01:28:38.080
And could you just like sketch out some of the some of the things you've seen?

01:28:38.080 --> 01:28:42.640
And I also know that you're a huge fan in that there's a Neurosymbolic algorithm

01:28:42.640 --> 01:28:43.680
that you want to tell us about.

01:28:43.680 --> 01:28:45.360
So let's let's hear it.

01:28:45.360 --> 01:28:49.440
So I indeed, I've been enjoying NeurIPS this week.

01:28:49.440 --> 01:28:54.640
One of the big things in AI in the last several years has been Neurosymbolic AI,

01:28:55.600 --> 01:29:00.240
which you probably will not surprise by the fact that I very much believe in.

01:29:00.240 --> 01:29:02.960
So and I believe this since I was a grad student

01:29:02.960 --> 01:29:06.640
and the whole idea of Neurosymbolic AI was something that nobody was interested in, right?

01:29:06.640 --> 01:29:09.360
And now suddenly everybody is, which I think is a good development.

01:29:09.360 --> 01:29:13.280
And this is the idea that if we really want to solve AI by some definite,

01:29:13.280 --> 01:29:18.800
if we want to get to like human level intelligence, etc, etc, we need to have both,

01:29:20.320 --> 01:29:24.160
you know, like, for example, deep learning is not enough, right?

01:29:24.160 --> 01:29:27.920
There are symbolic reasoning capabilities that we have and that are essential.

01:29:28.640 --> 01:29:29.840
And we need to get them.

01:29:29.840 --> 01:29:33.280
And I think, you know, intelligent connection is like, I don't know,

01:29:33.280 --> 01:29:36.960
Yoshua Ben-Joe, you know, Yanle Kunase, they don't disagree with this.

01:29:37.520 --> 01:29:41.120
But one way to look at this is say, we're just going to realize those,

01:29:41.120 --> 01:29:44.880
you know, capabilities using purely connectionist means, right?

01:29:45.520 --> 01:29:48.160
And what I see happening in that direction,

01:29:48.160 --> 01:29:50.480
unfortunately, is a lot of reinventing the wheel.

01:29:50.480 --> 01:29:55.680
So I do think, you know, symbolic AI got wedged for some reasons, including brittleness.

01:29:56.800 --> 01:29:59.920
And, you know, and we have learned from that at the same time,

01:29:59.920 --> 01:30:04.320
they did discover and understand a lot of things that are extremely relevant.

01:30:04.320 --> 01:30:06.880
So it's just not good science to ignore it.

01:30:06.880 --> 01:30:12.960
So I'm working on an approach to combine, you know, symbolic AI with deep learning.

01:30:12.960 --> 01:30:17.440
Again, this is a popular exercise, there are many interesting approaches out there.

01:30:17.520 --> 01:30:18.960
As much as I sympathize with them,

01:30:18.960 --> 01:30:20.880
I think they're all very far from solving the problem.

01:30:20.880 --> 01:30:24.080
They are over complicated and not powerful enough.

01:30:24.080 --> 01:30:26.960
So, you know, I've been working on an approach called TensorFlow logic

01:30:26.960 --> 01:30:33.200
that I do believe is as simple as it can be and as general as it can or needs to be.

01:30:33.760 --> 01:30:38.240
And this, you know, it really is a deep unification of the two things

01:30:38.240 --> 01:30:42.320
in the sense that it's not just that you combine them using, you know,

01:30:42.320 --> 01:30:45.040
a neural model that causes symbolic one or vice versa,

01:30:45.040 --> 01:30:47.520
which is a lot of what these things that you have today do.

01:30:47.520 --> 01:30:51.680
And a lot of the claims that like, oh, this system is neuro symbolic, which it is.

01:30:51.680 --> 01:30:55.600
It's like, you know, AlphaGo is neuro symbolic because some of what it does is symbolic.

01:30:55.600 --> 01:31:00.960
But I'm talking about something much deeper, which is once you start doing AI,

01:31:00.960 --> 01:31:03.440
learning inference representation in TensorFlow logic,

01:31:03.440 --> 01:31:08.720
there's just no distinction between symbolic and neural at all anymore.

01:31:09.840 --> 01:31:10.960
Can you explain that?

01:31:10.960 --> 01:31:16.000
So TensorFlow logic, I'm just inferring from that that the primary representation

01:31:16.000 --> 01:31:18.640
of substrate is a continuous vector space. Is that right?

01:31:18.640 --> 01:31:21.200
Are you encoding discrete information into the vector space?

01:31:21.200 --> 01:31:23.280
So it's a vector space. Yeah.

01:31:23.280 --> 01:31:28.080
Right. In fact, this was the original term that we had for this was vector space logic.

01:31:28.080 --> 01:31:30.880
But then we changed it to TensorFlow logic because it's much more appropriate.

01:31:30.880 --> 01:31:34.960
But it's it's vector space in the abstract algebra sense of vector space,

01:31:34.960 --> 01:31:37.200
not in the traditional, you know, vectors of numbers.

01:31:37.760 --> 01:31:46.000
But anyway, so as the name implies, right, TensorFlow logic is a combination or unification

01:31:46.000 --> 01:31:50.800
of tensor algebra on the one hand and logic programming on the other.

01:31:50.800 --> 01:31:55.200
So is it similar because Bob Koeck had a similar idea using like tensor outer products?

01:31:56.000 --> 01:31:57.040
Is it that kind of?

01:31:57.040 --> 01:31:59.520
It's related, but I think it goes well beyond.

01:31:59.520 --> 01:32:00.160
Okay.

01:32:00.160 --> 01:32:03.120
And the basic idea is actually pretty simple.

01:32:03.120 --> 01:32:08.160
And it's just the following, right, without going into too much, you know, technical detail.

01:32:09.200 --> 01:32:12.320
All of deep learning can be done using tensor algebra.

01:32:12.320 --> 01:32:16.320
Yeah, you know, plus univariate nonlinearities.

01:32:16.320 --> 01:32:18.880
Right. So we've got the tensor algebra to do that.

01:32:18.880 --> 01:32:21.680
All of symbolic AI can be done using logic programming.

01:32:21.680 --> 01:32:23.920
And moreover, it has been done using logic program.

01:32:23.920 --> 01:32:27.120
So if you can unify these two things, this part of the job is done.

01:32:27.120 --> 01:32:32.720
Right. And as it turns out, you can unify them shockingly easily because a tensor

01:32:33.200 --> 01:32:39.120
so tensor algebra is operating on tensors, you know, in that logic, so logic programming,

01:32:39.120 --> 01:32:43.840
and then for learning in that logic program and symbolic AI, they are all operating on relations.

01:32:43.840 --> 01:32:44.160
Yeah.

01:32:44.160 --> 01:32:47.680
Right. So what is the relationship between the tensor and the relation?

01:32:47.680 --> 01:32:52.560
Right. A relation is just this and efficiently represented sparse Boolean tensor.

01:32:53.520 --> 01:32:57.200
So at this point, we actually know that the foundation of these two things is actually the

01:32:57.200 --> 01:33:02.480
same. If your tensor is Boolean and is very sparse, now I'm better off representing it with

01:33:02.480 --> 01:33:06.000
a relation, but at a certain level of abstraction, nothing has changed.

01:33:06.000 --> 01:33:11.280
Right. So by this prism, you can look at logic programming and logic programming is doing tensor

01:33:11.280 --> 01:33:11.760
algebra.

01:33:12.720 --> 01:33:18.000
Okay. Just help me understand this a little bit. So, you know, the main criticism of using a neural

01:33:18.000 --> 01:33:24.480
network as a combined computational and memory substrate is that it's a finite state automator.

01:33:24.480 --> 01:33:29.200
So without having the augmented memory like a Turing machine, you can't represent infinite

01:33:29.200 --> 01:33:32.560
objects. That's the main reason the symbol is, you know, that's the main argument I used.

01:33:32.560 --> 01:33:35.520
So wouldn't that argument still be leveled against you?

01:33:35.520 --> 01:33:40.560
Well, no, because I'm glad you brought that up because there is a very common misconception.

01:33:40.560 --> 01:33:44.800
If you realize that there is no such thing as infinity, right? And in particular, there is no

01:33:44.800 --> 01:33:50.960
such thing as an infinite memory. That problem doesn't arise. So there's the, so the, unfortunately,

01:33:50.960 --> 01:33:56.640
a lot of theorists, including computer theorists, they foster this misconception, right?

01:33:56.640 --> 01:34:01.680
There's the Chomsky hierarchy, right? With finite automata at the bottom and Turing complete,

01:34:01.680 --> 01:34:06.000
you know, Turing machines bubble at the top, right? If your Turing machine has only a finite

01:34:06.000 --> 01:34:11.920
tape, it's a finite automata. So everything is just finite automata. Let's get that out of the way,

01:34:11.920 --> 01:34:16.160
right? A lot of what people do is like completely mistaken because of that. Now,

01:34:16.880 --> 01:34:21.360
the fact that everything is finite automata does not mean that everything is equally good.

01:34:21.360 --> 01:34:27.040
Some representations are far more efficient, compact, etc., etc., for certain purposes than

01:34:27.040 --> 01:34:31.600
others. And the whole game here is that like, I'm not going to solve a finite automata. The question

01:34:31.600 --> 01:34:36.000
is like, what do I need to do? Not because I need to go to a higher level of Chomsky hierarchy,

01:34:36.000 --> 01:34:41.840
because in reality, they don't exist. But because, you know, I mean, if you have infinite resources,

01:34:41.840 --> 01:34:46.000
you could solve a gap with a lookup table. But would you, would you not? I mean, for example,

01:34:46.000 --> 01:34:49.280
there was this DeepMind paper that mapped architectures to different levels of the

01:34:49.280 --> 01:34:55.600
Chomsky hierarchy transformers, I think were, you know, FSAs, RNNs actually were one step higher.

01:34:55.600 --> 01:34:59.200
They could represent regular languages and they got context-free languages. I mean,

01:34:59.200 --> 01:35:02.640
do you think there's any meaningful distinction between those language levels?

01:35:02.640 --> 01:35:07.120
As I said, there is a meaningful distinction, but it's not the distinction that people usually make,

01:35:07.120 --> 01:35:12.160
because once you, I mean, you can debate whether the universe is finite, but certainly computers

01:35:12.160 --> 01:35:16.000
are finite. So as far as anything that you're going to run on a computer, there truly is no

01:35:16.000 --> 01:35:21.120
distinction at this theoretical level between a Turing machine and a finite automata. That does,

01:35:21.120 --> 01:35:25.520
so like, I can reduce and people have, there are papers reducing, you know, any of these things to

01:35:25.520 --> 01:35:30.320
any of the others, right? It's like it's a fairly trivial exercise. So at that level, those distinctions

01:35:30.320 --> 01:35:35.040
are completely meaningless. However, they are meaningful in the sense that for many purposes,

01:35:35.040 --> 01:35:40.320
I am better off having an RNN than having, you know, a transformer. And for many purposes,

01:35:40.320 --> 01:35:44.960
I'm better off. So like, let's take, you know, propositional logic versus first-order logic,

01:35:44.960 --> 01:35:50.080
right? If there's no such thing as infinity, first-order logic is reducible to propositional

01:35:50.080 --> 01:35:55.280
logic. But that does not mean that it's useless because it can represent a lot of things exponentially

01:35:55.280 --> 01:36:00.160
more compactly than propositional logic. If I want to represent the rules of chess in first-order

01:36:00.160 --> 01:36:05.040
logic, it's a page, right? If I want to represent them in propositional logic, it's more pages that

01:36:05.040 --> 01:36:08.480
you can have. Okay, well, I think that that's a very, very good point. But I mean, just, just

01:36:08.480 --> 01:36:11.760
a devil's advocate from the psychologist, you know, do you remember that, that photo,

01:36:11.760 --> 01:36:16.640
Felician Connectionism critique paper, arguing productivity and systematicity? Productivity

01:36:16.640 --> 01:36:20.000
is all about the infinite cardinality of language. I mean, presumably you would agree that language

01:36:20.000 --> 01:36:24.160
has an infinite cardinality. No, well, again, another instance of the same problem. Productivity

01:36:24.160 --> 01:36:29.280
is very important. But the point to just be a little precisely for a second is, is to be able to

01:36:29.280 --> 01:36:36.320
generate a vast number of things beyond the ones that you started with. Vast, not infinite. In fact,

01:36:36.320 --> 01:36:43.360
mathematically, infinity is not a number. Infinity is just a shorthand for something that is so

01:36:43.360 --> 01:36:49.360
large that it doesn't matter how large it is. Okay. I mean, at the end of the day, I'm not a

01:36:49.360 --> 01:36:53.760
mathematician, but surely mathematicians would push back on this because, you know, infinity is,

01:36:53.760 --> 01:37:00.800
is a quantity in mathematics. No, I mean, again, people in every field, mathematicians, physicists,

01:37:00.800 --> 01:37:06.800
computer scientists are all are often guilty of they, they, they, they have this notational shorthand

01:37:06.800 --> 01:37:12.080
or like, you know, this terminological shorthand that serves them well. But then they, and then

01:37:12.080 --> 01:37:16.720
they use that and then the newer generations come along and the, and the public also, right,

01:37:16.720 --> 01:37:21.040
they don't even realize that what's being talked about is a little bit different.

01:37:21.040 --> 01:37:25.920
Infinity is a perfect example. Any serious mathematician will tell you that infinity does

01:37:25.920 --> 01:37:30.080
not have the properties of a number. So for example, if I multiply infinity by 2, I still

01:37:30.080 --> 01:37:36.000
get infinity. There's no number that that happens to, right? So infinity is, is not a number, right?

01:37:36.000 --> 01:37:40.320
When I say infinity is not a number, mathematicians might quibble about the way I'm stating it,

01:37:40.320 --> 01:37:45.440
but this is a, this is a mathematical truth, right? Infinity truly isn't, I'm being colloquial,

01:37:45.440 --> 01:37:48.880
of course, when I say that it's a shorthand for something that is so large that it doesn't

01:37:48.880 --> 01:37:53.680
matter how large it is. When you take limits, you know, in calculus in anything and the limit of

01:37:53.680 --> 01:37:58.320
this blah, blah, as I go to infinity, this is exactly what I'm doing. I'm going to the point

01:37:58.320 --> 01:38:02.160
where I'm saying like, at this point, it doesn't matter how large the number is, the result will

01:38:02.160 --> 01:38:08.800
be the same. And in this way, infinitely is an extraordinarily useful concept. So I'm not here

01:38:08.800 --> 01:38:12.640
to rail against infinity. I'm just saying like, we really need to understand, I mean, like, let me

01:38:12.640 --> 01:38:18.960
give you a very banal example, right? From the point of view of, you know, what to have for lunch,

01:38:18.960 --> 01:38:25.600
right? Because some things cost more than others. Elon Musk is infinitely rich. He does not have

01:38:25.600 --> 01:38:30.480
infinite money. But it makes no difference whatsoever whether he has whatever 100 billion

01:38:30.480 --> 01:38:36.080
or 200 billion to what he's going to have for lunch. You know, like a street person who has

01:38:36.080 --> 01:38:41.360
$5 to them, like their fortune is not infinite, because it very much matters what lunch costs,

01:38:41.360 --> 01:38:45.680
right? So this is the real sense of infinity, which we can and should use, but we shouldn't

01:38:45.680 --> 01:38:51.200
confuse it with like, oh, but then your, you know, like your formalism is incomplete because it

01:38:51.280 --> 01:38:55.760
doesn't encompass infinity. Yeah, it doesn't need to infinity doesn't exist.

01:38:55.760 --> 01:38:59.440
Okay, okay. Well, let's come at it from the other from the composition, you know,

01:38:59.440 --> 01:39:06.000
compositionality and systematicity. So that's all about being able to do, you know, like their main

01:39:06.000 --> 01:39:11.520
argument was when you have a symbolic representation, you can kind of reuse the previous representations

01:39:11.520 --> 01:39:17.040
downstream composition, compositionally. And when you take a discrete symbolic representation,

01:39:17.040 --> 01:39:21.520
and you kind of encode it in the envelope of a vector space, you have a real problem doing that

01:39:21.520 --> 01:39:26.320
because it's now like, it's irreversible that transformation, right? You can't go back to the

01:39:26.960 --> 01:39:32.560
original variables. Well, it is reversible if you realize that all those real numbers are actually

01:39:32.560 --> 01:39:38.800
finite. Right? So notice that real number, there's nothing less real than a real number,

01:39:38.800 --> 01:39:43.280
real numbers are imaginary, right? Real numbers are numbers with infinite precision, which is

01:39:43.280 --> 01:39:48.080
a monstrosity. And many people have said this, including mathematicians and physicists, right?

01:39:48.080 --> 01:39:52.720
The notion of an infinite, of a number with an infinite number of digits is just monstrous.

01:39:52.720 --> 01:39:58.720
And again, in particular on a computer, even if you use, you know, you know, like numbers with

01:39:58.720 --> 01:40:04.560
unlimited floating point precision, right? It's limited by the size of your memory. So this transfer

01:40:04.560 --> 01:40:08.240
from which is actually very important that again, that's what tensor logic largely is about,

01:40:08.320 --> 01:40:13.520
from purely symbolic structures to embeddings in a vector space, right? That vector space

01:40:13.520 --> 01:40:17.200
is still finite. So there's actually nothing irreversible about what happened there.

01:40:17.200 --> 01:40:21.920
Interesting. Okay. So how can people, you know, find out more information about this? And can you

01:40:21.920 --> 01:40:26.160
just sketch out, you know, just, just to bring it home to people where they could actually use it

01:40:26.160 --> 01:40:28.880
and how it would be, you know, better than what they can currently do?

01:40:28.880 --> 01:40:32.400
Right. The answer to the first question, unfortunately, is this is not published yet,

01:40:32.400 --> 01:40:37.920
but hopefully it will be soon. So for the moment, there is no very good place to point people to

01:40:38.000 --> 01:40:45.840
unfortunately, but that hopefully will be fixed soon. The question of where to apply it is,

01:40:46.480 --> 01:40:51.680
our goal for this is that this should become the language or hope, I should say our hope,

01:40:51.680 --> 01:40:56.560
is that this will become the language we're doing just about anything in AI. So for example,

01:40:56.560 --> 01:41:01.120
if what you want to do is actually nothing symbolic, but you just want to build a convent,

01:41:02.000 --> 01:41:07.760
you can express a convent incredibly elegantly in tensor logic. Like if you think of, for example,

01:41:07.760 --> 01:41:13.920
tensor floor or PyTorch versus NumPy, right, they allow that thing to be said much more compactly,

01:41:13.920 --> 01:41:18.880
compared to tensor logic, they are as bad as NumPy is compared to them. Right. Same thing on the

01:41:18.880 --> 01:41:22.880
symbolic side. But of course, the real action comes in all the problems where you have both

01:41:22.880 --> 01:41:28.640
components, the problem with all those problems, which ultimately is every problem in AI, right.

01:41:28.640 --> 01:41:32.000
You're always like, what happens today that is very frustrating. And that's what we're trying to

01:41:32.000 --> 01:41:36.480
overcome is like, you start from one of these sides these days, mainly the connectionist one,

01:41:36.560 --> 01:41:40.640
which you have a good mastery of. And then the other side, for example, the symbolic one,

01:41:40.640 --> 01:41:45.520
the knowledge representation, the reasoning, the composability, you just hack. Yeah. And your

01:41:45.520 --> 01:41:49.760
hack solution is terrible. You're like, you're inventing the wheel, you're making it square,

01:41:49.760 --> 01:41:54.240
you're trying to make it turn, but it's square, right. It's just, you know, it's a disaster.

01:41:54.240 --> 01:41:58.560
And with tensor logic, you can actually have a very well founded, very well understood

01:41:59.200 --> 01:42:03.360
basis on either side. So now you don't have to hack either side. Now there's, of course,

01:42:03.360 --> 01:42:06.560
still things that you're going to have to hack at the end of the day, because at the end of the day,

01:42:06.560 --> 01:42:09.840
you know, AI is intractable and things are heuristic. But this, you know, is,

01:42:10.960 --> 01:42:14.320
you know, you know, this notion of a tradeoff that is very important in engineering. Like,

01:42:14.320 --> 01:42:19.760
people have been exploring different points on this tradeoff curve. The point of tensor logic is

01:42:19.760 --> 01:42:27.120
that whatever your application is, we're moving you to a better tradeoff curve. It's still a

01:42:27.120 --> 01:42:32.480
tradeoff curve, but it dominates the old one. For any given X, you have a better Y and vice

01:42:32.480 --> 01:42:37.360
versa. Okay. And just help me understand, because we'll move over to, you know, the discrete program

01:42:37.360 --> 01:42:41.600
search and some of Josh Tannenbaum's work in a moment. But there are two schools of thought,

01:42:41.600 --> 01:42:45.200
right? There's discrete first and there's continuous first, you're on the continuous

01:42:45.200 --> 01:42:50.160
substrate. But usually the reason for the continuous substrate is stochastic gradient descent,

01:42:50.160 --> 01:42:54.480
learnability, et cetera, et cetera. And like, help me understand. So are you saying we start

01:42:54.480 --> 01:42:58.480
with symbolic representation and then we encode it into the envelope? So where does learning come

01:42:58.480 --> 01:43:03.200
into it? No, very good. So in tensor logic, you can do broadly speaking, two kinds of learning.

01:43:03.200 --> 01:43:07.760
You can learn the structure of these tensor equations, as we call them, using inductive logic

01:43:07.760 --> 01:43:12.080
programming techniques. Again, that whole technology is there. And then once you have that, you can

01:43:12.080 --> 01:43:17.040
learn the numbers by the back prop in particular ways called back propagation through structure,

01:43:17.040 --> 01:43:21.120
because the structure can vary from example to example, but we know what the type parameters

01:43:21.120 --> 01:43:25.680
are. So all of the machinery of inductive logic programming and all the machinery of gradient

01:43:25.760 --> 01:43:30.400
descent and deep learning or not, they're both there available to be used as you traditionally

01:43:30.400 --> 01:43:35.280
have. Okay, what if I made the argument, though, that it's almost like the inductive logic, you

01:43:35.280 --> 01:43:39.440
know, like the program search, that's the hard bit. So if you've already got the program, why do I

01:43:39.440 --> 01:43:42.800
then need to put it into a vector space? No, actually, these are also at the end of the day,

01:43:42.800 --> 01:43:46.880
in machine learning, we're always trying to learn a program of some kind, right? The question is like,

01:43:46.880 --> 01:43:51.600
what is the easiest way to do that? And precisely the problem with ILPS with symbolic logic is,

01:43:51.600 --> 01:43:55.840
that's really a couple of problems. One is that if all that you do, you learn programs that are

01:43:55.840 --> 01:44:01.440
too brittle, and we don't want them to be brittle, right? And the other one is that each type of

01:44:01.440 --> 01:44:08.080
search has its limitations. So in particular, in symbolic AI, including ILP, we tend to use a lot

01:44:08.080 --> 01:44:13.040
of combinatorial optimization types of search, right? What we in AI call search is discrete search.

01:44:13.040 --> 01:44:17.120
And that is good in some ways, but also very limited in others. The same thing is true of gradient

01:44:17.120 --> 01:44:23.840
descent, right? And now to go to that for just a second. Gradient descent is not a continuous

01:44:23.840 --> 01:44:30.400
optimization algorithm. It's not, right? Again, those real numbers are not infinite precision.

01:44:30.400 --> 01:44:34.960
There's actually nothing continuous going on in the computer. Gradient descent truly literally

01:44:34.960 --> 01:44:39.360
rigorously mathematically is a discrete optimization algorithm. It takes discrete steps.

01:44:40.080 --> 01:44:44.960
The assumption that gradient descent depends on, which is that the infinitesimally small updates

01:44:44.960 --> 01:44:50.960
do not hold. And moreover, in machine learning, as a numerical analysis, we are constantly dealing

01:44:50.960 --> 01:44:55.440
with this fact that there's a mismatch between our mathematical conceptual model of the space

01:44:55.440 --> 01:45:00.160
that we're working with as continuous with the reality of the computer that is not continuous.

01:45:00.160 --> 01:45:06.160
So now this is not, but gradient descent still is a different optimization technique

01:45:06.160 --> 01:45:11.440
with some very important advantages, in particular the key, right? The power of gradient descent

01:45:11.440 --> 01:45:16.400
comes from the fact that to move from my current point to a better one, I don't need to try out

01:45:16.400 --> 01:45:20.480
all the neighboring points because that takes order of the time of the neighboring points.

01:45:20.480 --> 01:45:26.160
I have a closed form way to compute what is the best one, right? And then I move there.

01:45:26.160 --> 01:45:29.680
And this is absolutely brilliant, right? Like we don't want to let go of that, right? This is,

01:45:29.680 --> 01:45:34.960
you know, Newton's enlightenment is bright idea, right? The price of that is that in order to do

01:45:34.960 --> 01:45:40.880
that you have to make this approximation, which again, calculus is an approximation. It assumes

01:45:40.880 --> 01:45:46.320
that certain effects are second order and can be ignored. Now, ironically, when you learn a large

01:45:46.320 --> 01:45:50.800
deep network these days, you're actually in a regime where they cannot be ignored, right? Because

01:45:50.800 --> 01:45:56.880
these infinitesimal changes are not that infinitesimal because you take a finite step, right? The gradient

01:45:56.880 --> 01:46:01.200
descent is always taking finite steps, which is why it's a discrete algorithm. And once you take

01:46:01.200 --> 01:46:06.240
that finite step for any reasonable learning rate, the total effect of the approximations that you've

01:46:06.240 --> 01:46:12.560
made typically swamps the step that you're taking. So the assumption of calculus that

01:46:12.560 --> 01:46:17.600
gradient descent is founded on is actually false. Now, in some ways this invalidates a lot of our

01:46:17.600 --> 01:46:23.280
intuitions. In many ways, and again, this remains to be resolved, a lot of why gradient descent works

01:46:23.280 --> 01:46:27.600
better than people expected to is in fact that it's doing something else. It's doing stochastic

01:46:27.600 --> 01:46:31.920
search partly because of the SGD as opposed to being matched partly because of things like this.

01:46:31.920 --> 01:46:35.520
Okay, well, this is really interesting. A couple of places we can go. But first of all, I remember

01:46:35.520 --> 01:46:41.280
you did the paper and that introduced elements of NTK theory as well, which might be an argument

01:46:41.280 --> 01:46:45.760
against the discreteness of the optimization. But also, I wanted to trade off the two types.

01:46:45.760 --> 01:46:47.440
Well, why is there an argument against the discreteness?

01:46:47.440 --> 01:46:51.040
Well, isn't there a, with NTK, isn't there like a closed form solution? Doesn't that kind of like

01:46:51.040 --> 01:46:56.000
erode the discreteness of the optimization? No, I mean, so there's several things here. But like,

01:46:56.560 --> 01:47:01.280
if you have a closed form solution, absolutely brilliantly go for it, right? There's nothing,

01:47:01.920 --> 01:47:06.400
having a closed form solution in no implies that it's continuous or discrete or any other thing,

01:47:06.400 --> 01:47:12.000
right? So, let's say there was a closed form solution and it was like an infinite kernel when

01:47:12.000 --> 01:47:16.880
it represented some neural network, doesn't that erode the argument? Well, so first, okay, so first

01:47:16.880 --> 01:47:20.320
of all, in the work that, so the work that I've done that I think you're referring to is like,

01:47:20.320 --> 01:47:24.960
I have a proof that every model learned by gradient descent is a kernel machine.

01:47:24.960 --> 01:47:28.640
Yeah, right. And it's something called the path kernel, which is the integral

01:47:28.640 --> 01:47:34.320
of the neural tangent kernel over the overgraded descent, right? Yeah. And now the neural tangent

01:47:34.320 --> 01:47:39.920
kernel does not assume that your network is infinite. Most of the theory that people have done

01:47:39.920 --> 01:47:45.600
with it assumes that the network is infinitely wide, but the definition absolutely does not

01:47:45.600 --> 01:47:50.320
require that. And none of what I do, and in fact, that's part of why, you know, of its part is that

01:47:50.320 --> 01:47:56.160
it assumes no infinity of anything. It's for any architecture that you use, and in particular,

01:47:56.160 --> 01:48:00.960
you know, finite architectures. Okay, interesting. Okay, so hence the discreteness, but can we come

01:48:00.960 --> 01:48:05.360
back to this contrasting of the discrete program search and the, you know, stochastic gradient

01:48:05.360 --> 01:48:11.200
descent on a vector space? Now, in the vector space, there are certain characteristics, you know,

01:48:11.200 --> 01:48:14.880
there are certain symmetries, and even though it's a discrete search through the space, I would argue

01:48:14.880 --> 01:48:20.160
that it's still continuous in nature, it has certain characteristics. So contrast those two

01:48:20.160 --> 01:48:25.280
forms of optimization. Precisely so. Exactly. I mean, I think you've put your finger in now.

01:48:25.280 --> 01:48:29.840
The whole point of these continuous spaces, right, is not that they're continuous, because again,

01:48:29.840 --> 01:48:35.200
that's, that's a fiction, is that they have a certain locality structure, yeah, that you can

01:48:35.200 --> 01:48:41.280
exploit to very good effect. And this is exactly what we're going to send us, right? Now, that

01:48:41.280 --> 01:48:46.880
locality structure doesn't have to be infinitesimal, right? You don't need points to be infinitely close

01:48:46.880 --> 01:48:51.040
for all this to apply approximately. And again, they never are, and it's always an approximation.

01:48:51.040 --> 01:48:57.840
Now, the question is, do you want to make these locality assumptions or not, right? Making them

01:48:57.840 --> 01:49:02.960
buys you certain things, right? But it's also potentially unrealistic in some ways, right? Now,

01:49:02.960 --> 01:49:08.560
this actually, to take a very concrete instance of this, think of space, right? We model space in

01:49:08.560 --> 01:49:14.720
science and physics and in anything as a continuous thing, which it is not, right? Which is not to

01:49:14.720 --> 01:49:19.280
say that, and by the way, physicists are coming to this conclusion, right? These days, the prevailing

01:49:19.280 --> 01:49:23.520
views is that it's from big thing, is that like, it's, you know, space arises from entanglement,

01:49:23.520 --> 01:49:28.000
et cetera, et cetera, like space is not the fundamental reality, right? And now, I think

01:49:28.000 --> 01:49:32.400
that where this is inevitably going one way or another is that we realize that space is discrete,

01:49:32.400 --> 01:49:38.880
right? But, and this is key, it has certain properties, including symmetries like translations

01:49:38.880 --> 01:49:43.760
in variance, rotation in variance, et cetera, et cetera, that whole, approximately or exactly,

01:49:43.760 --> 01:49:50.400
but if those hold a whole bunch of things like that, then you have, you know, your latent variable

01:49:50.400 --> 01:49:55.600
structure, right, is very well approximated by our notion of continuous space, in which case,

01:49:55.600 --> 01:50:00.160
it would be foolish to not use it, right? To formulate the laws of physics and to do computer

01:50:00.160 --> 01:50:06.000
vision and so on and so forth. But at the same time, right, if we believe in it too literally,

01:50:06.000 --> 01:50:10.560
we walk ourselves into a blind alley. So concretely, look at computer vision, right?

01:50:10.560 --> 01:50:14.240
People in the universities of computer vision started out trying to do it with differential

01:50:14.240 --> 01:50:19.280
equations and Fourier analysis and all of that could continue with stuff, right? Because that

01:50:19.280 --> 01:50:24.880
was the obvious thing to do, right? And it failed. That doesn't work. That's why we need things like

01:50:24.880 --> 01:50:29.760
deep learning and, you know, Markov random fields that are discrete grids that use, you know,

01:50:29.760 --> 01:50:34.080
to model the images and whatnot, because you are, along with the approximate continuity,

01:50:34.080 --> 01:50:39.360
you also often have large discontinuities. And if you can only model the world continuously,

01:50:39.920 --> 01:50:43.360
you don't know what to do. And the problem precisely is that you have all these phenomena

01:50:43.360 --> 01:50:47.760
that are like this, including, you know, in vision, but also in, in turbulence and condensed

01:50:47.760 --> 01:50:52.800
metaphysics and so on, you've got to realize that there are discontinues and not try to shoehorn

01:50:52.800 --> 01:50:57.360
them into continuity when that's no longer appropriate. Interesting. Okay. Well, can we bring

01:50:57.360 --> 01:51:03.680
in ILP and can you contrast like the kind of function spaces that are learnable in both methods?

01:51:03.680 --> 01:51:09.680
Yeah. So ILP, so let me actually preface this with the following. People in every one of these

01:51:09.680 --> 01:51:16.880
schools of AI tend to have this view that I can represent everything in the world using my approach.

01:51:17.680 --> 01:51:22.640
So I can like, look, prologue is too incomplete. So why do you need neural networks? But I can also

01:51:22.640 --> 01:51:27.600
say neural networks are too incomplete. So why do I need prologue? And in fact, kernel machines have

01:51:27.600 --> 01:51:31.760
a represented theorem that says you can approximate any function, blah, blah, blah, right? So everybody

01:51:31.760 --> 01:51:36.080
has one of these represent their theorems, right? That says, I can represent anything, right? So in

01:51:36.080 --> 01:51:42.160
particular, you can do, right? I mean, look, first, our logic was invented by, by Frege, essentially,

01:51:42.160 --> 01:51:48.080
to, to model the real numbers. So it can almost by definition model real numbers, right? Anything

01:51:48.080 --> 01:51:52.160
you might want to say about real numbers and, and weight and descent and neural networks. And in

01:51:52.160 --> 01:51:58.400
fact, people have even done this. So you can say it all in, in logic programming, right? So why not

01:51:58.400 --> 01:52:02.720
just do that? Well, precisely because certain things are much more easily done in other ways,

01:52:02.720 --> 01:52:06.720
right? So what you have to ask about anything, but then about, you know, not the logic

01:52:06.720 --> 01:52:11.280
program in particular, like, what things are well represented in this way, like compactly

01:52:11.280 --> 01:52:17.440
represented, and then in such a way that learning them and doing inference with them is easy, right?

01:52:17.440 --> 01:52:21.120
And those things are different for logic programming and for things like deep learning,

01:52:21.120 --> 01:52:25.680
which is why we need a unification of both. So what is things like logic programming and

01:52:25.680 --> 01:52:30.480
ILP good for, right? It's precisely, I mean, it's many things, but the key thing is,

01:52:30.480 --> 01:52:36.560
it's precisely for learning pieces of knowledge that can then be reused and composed in arbitrary

01:52:36.560 --> 01:52:44.240
ways. This is the huge power symbolic AI that connectionism does not have, right? It's like,

01:52:44.240 --> 01:52:49.040
I learned the fact here, I learned a rule there. And tomorrow you ask me a question,

01:52:49.040 --> 01:52:53.040
and I combine that fact, actually, several rules by rule changing, right? There's a whole proof

01:52:53.040 --> 01:52:57.360
tree of rules that could have come from very different places. And I do a completely novel

01:52:57.360 --> 01:53:02.240
chain of inference that answers your question. This is spectacular, right? And this is surely

01:53:02.240 --> 01:53:07.760
court-wide intelligence is all about. And the symbolists know how to do it. The connectionists

01:53:07.760 --> 01:53:11.040
don't. But if I was a connectionist, I'd be like, you know, I know if it was a good one,

01:53:11.040 --> 01:53:15.680
and the better ones like Yoshio Benji are doing this, right? It's like, go and try to understand

01:53:15.680 --> 01:53:19.120
what those people understand so that you can then not combine it with those other ideas.

01:53:19.120 --> 01:53:24.560
Yes. Yeah. Yeah, I completely agree. So a huge part of intelligence is this symbolic,

01:53:24.560 --> 01:53:31.600
you know, extrapolation. Yeah. So how do you bring abstraction into this? Because the thing

01:53:31.600 --> 01:53:36.800
that I always get caught on is that the traditional go fi vision was to, you know, handcraft the

01:53:36.800 --> 01:53:41.520
knowledge. And actually, what we need is dynamic knowledge acquisition. And we need the ability

01:53:41.520 --> 01:53:46.160
to create abstractions on the fly rather than just what we do now, which is crystallizing

01:53:46.240 --> 01:53:51.120
existing human abstraction. How could we do that bit? Well, abstraction traditionally was and

01:53:51.120 --> 01:53:57.840
still is a central topic in symbolic AI, right? Like be precise. I mean, I think nobody questions

01:53:57.840 --> 01:54:02.800
that having levels of abstraction, someone is very important. The only question is how. So if you

01:54:02.800 --> 01:54:08.080
look at classic knowledge representation, planning, et cetera, et cetera, abstraction is all over

01:54:08.080 --> 01:54:12.640
the place. If you look at things like reinforcement learning, and I mean, even like, you know, the

01:54:12.640 --> 01:54:17.920
whole idea or hope of a convent is that it captures objects at multiple levels of abstraction,

01:54:17.920 --> 01:54:22.320
at least to some degree. In reality, it doesn't, right? But that's what people are trying to do

01:54:22.320 --> 01:54:27.440
and not quite doing, right? Well, good. Let's touch on that then. So I mean, certainly in

01:54:27.440 --> 01:54:32.560
Jan McCoon's view, I spoke with Jan the other day, he's got this autonomous path, a paper. And,

01:54:33.440 --> 01:54:37.120
you know, his system is learning abstractions, but they're abstractions which are deducible from

01:54:37.120 --> 01:54:42.640
base abstraction priors, like objectness and, you know, basic visual priors. And so there's this

01:54:42.640 --> 01:54:47.280
assumption that everything is deducible from the priors that we put into the model. But I have this

01:54:47.280 --> 01:54:53.360
kind of intuition that abstraction space is much larger than that. Yeah. I mean, so I would even

01:54:53.360 --> 01:54:58.960
say that if you arrive at your abstractions solely by deduction, you have a very impoverished notion

01:54:58.960 --> 01:55:05.120
of abstraction. In fact, most of inductive learning is forming abstractions. And form abstractions at

01:55:05.120 --> 01:55:09.520
the most basic level is something very trivial. It's like, I have an example described by a thousand

01:55:09.520 --> 01:55:15.520
attributes. If from that I induce a rule that uses only 10, I've abstracted the way the other 990,

01:55:16.240 --> 01:55:21.360
right? But if a symbolist was here, they would talk about intention versus extension, and they

01:55:21.360 --> 01:55:24.640
would say that, you know, you're selecting from this infinite set of possible attributes. You

01:55:24.640 --> 01:55:28.400
couldn't possibly represent all of the attributes in this. I mean, just to give you a concrete

01:55:28.400 --> 01:55:34.000
example, you know, you could have a, a, a, a, a, a, a, you know what I mean? You can have like this.

01:55:34.000 --> 01:55:39.120
Again, I hate to bring up infinity again, because that's always what these folks bring up. But

01:55:39.120 --> 01:55:44.240
how could you select from a set that large? Well, I don't need to because it is finite.

01:55:44.240 --> 01:55:48.400
But what I need to do is so, so, but there is actually a good example. And, you know,

01:55:48.400 --> 01:55:54.080
infinity does not bother us at all, at all there, because what it's like, if my training set,

01:55:54.080 --> 01:55:59.280
right, is a set of strings, and those strings are a, a, a, a, a, a, a, right? Going up to

01:55:59.280 --> 01:56:02.720
whatever number you want to pick, like, you know, a million or a quid drill in, you know,

01:56:02.720 --> 01:56:08.480
or a Google, right? Then R is your learning algorithm able to induce that the, the language

01:56:08.480 --> 01:56:12.960
that these rules come out of, right? The grammar is, you know, it's a series of A's, right? You

01:56:12.960 --> 01:56:17.600
and I can do that immediately. You know, most deep networks have no end of trouble doing that,

01:56:17.600 --> 01:56:22.080
even though it's that basic. So it is a very good example of what symbolic learning and

01:56:22.080 --> 01:56:26.000
reasoning can do versus connection is you don't need to go anywhere near infinity to actually

01:56:26.000 --> 01:56:30.240
have that be a very elegant example. Well, let me bring up just one other, we've touched on a

01:56:30.240 --> 01:56:34.000
lot of great things, right? There's one in this space of things that we've been talking about,

01:56:34.000 --> 01:56:38.640
there's one that I think is very important, which I believe you're also a fan of. And I very much

01:56:38.640 --> 01:56:42.400
am. And I think it's going to, you know, maybe you're going back to the question of what I'm

01:56:42.400 --> 01:56:47.040
interested in that's happening at, at new reps right now or not. So new symbolic AI is definitely a

01:56:47.040 --> 01:56:53.280
big one. Another big one. And to my mind, maybe these are the two biggest ones are most interesting

01:56:53.280 --> 01:56:59.040
is, is what I call symmetry based learning. And these days is more popular known by the,

01:56:59.040 --> 01:57:02.880
by the, by the name of like geometric deep learning and things like that. I tend to view

01:57:02.880 --> 01:57:07.680
geometric deep learning as a special case of symmetry based learning. But this idea of,

01:57:08.720 --> 01:57:14.000
I think, let me, you know, to go straight to the punchline, we know that, for example, AI and

01:57:14.320 --> 01:57:18.960
machine learning in particular, have as foundations, things like, you know, logic,

01:57:18.960 --> 01:57:24.000
probability optimization. And I think another foundation is symmetry group theory. In fact,

01:57:24.000 --> 01:57:28.080
I was having, you know, dinner with, with Max Welling just the other day, who, who, of course,

01:57:28.080 --> 01:57:32.320
have also interviewed and is, you know, like a great, you know, person in this area. And we,

01:57:32.320 --> 01:57:37.280
you know, I think we have very similar views on this. Well, Pedro, yesterday, and Taka Kohen

01:57:37.280 --> 01:57:41.760
was sitting where you were sitting. So there you go. Yeah. Again, I remember talking with Taka

01:57:42.000 --> 01:57:45.520
Kohen, some ICML many years ago, where he published one of the first papers on this.

01:57:45.520 --> 01:57:51.040
And I was like, and he seemed a little disheartened by the lack of interest that people had. And I

01:57:51.040 --> 01:57:55.440
said to him, just wait, this is going to be big and we're there now, right? And it's going to be

01:57:55.440 --> 01:57:59.520
even bigger, I think. But also, I think to become bigger and again, to jump straight to the punch

01:57:59.520 --> 01:58:05.360
line, most of the work, including me, that people have done to date has been exploiting known

01:58:05.360 --> 01:58:09.440
symmetries, like, you know, translation invariance is the quintessential example. For example,

01:58:09.440 --> 01:58:13.840
we have something called deep affine networks that generalize coordinates to, you know,

01:58:13.840 --> 01:58:18.400
rotation, you know, scaling, et cetera, et cetera. This is all well and good. But I think

01:58:18.400 --> 01:58:23.440
if this is, and if you look at New York's today, for example, most is in that vein.

01:58:23.440 --> 01:58:27.120
And there's a lot of good work to be done there. But if that's all we ever do, we will always

01:58:27.120 --> 01:58:32.000
remain a niche in AI with certain very good applications, like science applications,

01:58:32.000 --> 01:58:36.240
where we know that certain symmetries hold and whatnot. Max and Taka are doing things like that.

01:58:36.240 --> 01:58:39.680
But I don't want to just do that. I really, you know, I'm trying to make progress towards

01:58:39.680 --> 01:58:44.000
human level AI. And I think the key there is to discover symmetries from data.

01:58:44.560 --> 01:58:49.040
Yeah. And I think most of us agree with this. It's a hard problem, right? But that's what we're

01:58:49.040 --> 01:58:54.000
here for. We want to discover symmetries from data. And, you know, there's an interesting,

01:58:54.000 --> 01:58:57.520
you know, discussion of how to do that, you know, I have a number of ideas and a number of people

01:58:57.520 --> 01:59:02.160
have, then the power of discovering symmetries, right, connecting back to our early conversation

01:59:02.160 --> 01:59:07.200
is that symmetries can, individual symmetries can be very easy to discover because they're

01:59:07.200 --> 01:59:12.880
often very simple. But then, right, by the group axioms, axioms, you can compose them arbitrarily.

01:59:12.880 --> 01:59:17.840
Yeah. Which means I can, for example, by learning 100 different symmetries of a cat

01:59:17.840 --> 01:59:22.640
from 100 different examples, then I can compose them and correctly recognize as a cat

01:59:23.440 --> 01:59:28.000
something that is extremely different from any concrete example of a cat that I saw before.

01:59:28.960 --> 01:59:32.480
Could I push back on a tiny bit? So, I mean, in the geometric deep learning prototype book,

01:59:32.480 --> 01:59:37.200
I mean, they spoke about, you know, the various symmetries of groups like SO3, you know, preserves

01:59:37.200 --> 01:59:42.960
translations and angles, you know, like how primitive and how platonic are these symmetries?

01:59:42.960 --> 01:59:46.640
And aren't they just like obvious in respect of the domain that you're in?

01:59:46.640 --> 01:59:51.680
No, very good. So this is actually a key question. Symmetry group theory is one of them.

01:59:51.680 --> 01:59:56.240
It's a central area in mathematics that it's a very highly developed and it's the foundation

01:59:56.240 --> 02:00:01.440
of modern physics, like the standard model is a bunch of symmetries and so on. But the way,

02:00:01.440 --> 02:00:08.000
and there is an exhaustive listing of what all the possible symmetry groups are, discrete ones,

02:00:08.000 --> 02:00:13.760
you know, continuous ones, you know, so-called lead groups, etc., etc. So at that level, this is

02:00:13.760 --> 02:00:19.600
not naive because people already have a handle on what the space is, right? But crucially for our

02:00:19.600 --> 02:00:26.480
purpose is for AI, that's not enough because precisely because those, again, the analogy

02:00:26.480 --> 02:00:31.440
with logic is actually a very good one here. First of all, the logic is to brittle, right?

02:00:31.440 --> 02:00:35.040
And plain symmetry group theory, the way people have mostly applied so far,

02:00:35.040 --> 02:00:40.080
is also too brilliant for the same reason. So for example, right? Something like, you know,

02:00:40.080 --> 02:00:44.080
people almost always immediately come up with, so like, oh, I understand, you know,

02:00:44.080 --> 02:00:48.560
I like symmetries with the light to recognize, you know, perturbed digits, but a 6 is not a 9.

02:00:49.520 --> 02:00:53.280
So some, like, if you just take naive symmetry group theory and you say, like, well, arbitrary

02:00:53.280 --> 02:00:56.880
composability, as I was just talking about, I was like, well, now you've just said that a 6,

02:00:56.880 --> 02:01:01.680
you've lost the ability to distinguish a 6 from a 9, right? Now, what we need precisely is to

02:01:01.680 --> 02:01:05.520
combine symmetry group theory with the other things like statistics and optimization and

02:01:05.520 --> 02:01:10.720
say something like the following. The space of things that you can compose is unlimited. You

02:01:10.720 --> 02:01:15.600
can have, you know, unlimited compositions, but for example, you pay a cost for composing more

02:01:15.600 --> 02:01:20.240
symmetries. And now when you find the least cost path, and that's how you're going to match things,

02:01:20.240 --> 02:01:25.360
or, you know, your digit becomes less and less probable to be in 6, the more you've rotated

02:01:25.360 --> 02:01:30.000
it, right? So now we know how to do all of that very well. So we know symmetry group theory very

02:01:30.000 --> 02:01:34.240
well. We know how to do all these probabilistic costs, minimizing blah, blah, blah things,

02:01:34.240 --> 02:01:38.880
machine learning very well. We just need to combine it to the same way that we have previously

02:01:38.880 --> 02:01:42.960
combined these things with first order logic. So I'm glad you brought in the cost that that was

02:01:42.960 --> 02:01:46.880
really, really good. So there were trade offs everywhere. I mean, for example, if you want to

02:01:46.880 --> 02:01:52.160
make the models more fair and, you know, prioritize the low frequency attributes on the long tail,

02:01:52.160 --> 02:01:56.080
the headline accuracy goes down. Same thing with robustness. If you robustify a model,

02:01:56.080 --> 02:01:59.760
the headline accuracy goes down. Same thing with symmetry groups. If you introduce other

02:01:59.760 --> 02:02:04.480
symmetry groups, you know, that the headline accuracy goes down. So it all comes back to the

02:02:04.480 --> 02:02:09.440
bias variance trade off at the end of the day. And, you know, where is the limit here? How much

02:02:09.440 --> 02:02:15.200
can we optimize these models and what does good look like? The bias variance trade off is a very

02:02:15.200 --> 02:02:22.720
useful tool, right? But it's not the deepest reality, right? The way to think about bias variance is

02:02:22.720 --> 02:02:27.120
that, again, talking about this notion of a trade off curve, there's a trade off between bias and

02:02:27.120 --> 02:02:31.200
variance, right, which is in some sense unavoidable, right? In machine learning, if you have finite

02:02:31.200 --> 02:02:35.600
data, you're trying to learn powerful models, bias variance is a trade off. And it's a very

02:02:35.600 --> 02:02:39.440
consequential trade off in the sense that, for example, the things that work best with small

02:02:39.440 --> 02:02:42.960
amounts of data tend not to work best with large amounts of data, right? This is something that we

02:02:42.960 --> 02:02:47.040
should all, you know, grow up knowing in machine learning. But so many mistakes have been done

02:02:47.040 --> 02:02:51.200
because of that, because people study things in the easy or historically, that's all they had,

02:02:51.200 --> 02:02:54.880
right? And then they're very surprised when something that seemed not very good, like, say,

02:02:54.880 --> 02:02:58.800
deep learning, right, turns out to be better when you have a large amount of data, or they believe

02:02:58.800 --> 02:03:03.600
in, like, silly things like, you know, Occam's razor version that, you know, accurate, you know,

02:03:03.600 --> 02:03:08.240
simply is more accurate and whatnot. So a lot of mistakes have been made because of lack of

02:03:08.240 --> 02:03:13.840
understanding of this. Having said that, what you really want is to move to a better trade off

02:03:13.840 --> 02:03:19.760
curve between bias and variance, which you can, if you get at what the reality is, right? So the

02:03:19.760 --> 02:03:24.080
real game in machine, once you're evaluating your learner and figuring out, you're like, how much

02:03:24.080 --> 02:03:29.040
to prune and whatnot, or how much to regulate bias variance is very important. But before that,

02:03:29.040 --> 02:03:33.200
the most important question is like, what we're trying to do here is figure out what are the

02:03:33.200 --> 02:03:38.720
inductive biases? What are the regularities that the world really has, at least approximately,

02:03:38.720 --> 02:03:43.520
that we build our algorithms on top of that? And then if you give me a better one than I have now,

02:03:43.520 --> 02:03:47.840
I'll still have a bias variance trade off, but I'll be in a curve where for the same variance,

02:03:47.840 --> 02:03:51.520
I can have less bias and vice versa. And that's where the real action is.

02:03:51.600 --> 02:03:54.640
Oh, interesting. Well, I didn't quite understand that because bias and variance,

02:03:54.640 --> 02:03:59.120
they are mutually exclusive. And I thought at first you were saying, well, if we understand

02:03:59.120 --> 02:04:03.840
what the biases are better, the prototypical symmetries of the world we live in, then we

02:04:03.840 --> 02:04:07.280
can have more bias without having an approximation error, basically.

02:04:07.280 --> 02:04:11.360
The confusion arises because bias is a very unfortunately overloaded term.

02:04:11.360 --> 02:04:15.520
Right. This is not even getting into the psychological notion of bias like in Danny

02:04:15.520 --> 02:04:20.240
Kahneman's work, or even the sociological notion of bias like racial biases, gender biases and

02:04:20.240 --> 02:04:25.600
whatnot. So we need to distinguish. I just used my bad, the word bias into completely

02:04:25.600 --> 02:04:30.080
different senses, completely but not unrelated. That's the thing. One of them is the statistical

02:04:30.080 --> 02:04:35.920
notion of bias. There really is a trade off between the two. There's a sum of squares,

02:04:35.920 --> 02:04:41.600
blah, blah, blah. The machine learning notion of inductive bias, it's the preference that you

02:04:41.600 --> 02:04:46.400
have for certain models of our others, which is really just another way of saying your priors,

02:04:47.360 --> 02:04:53.200
whether they are assumptions or knowledge. Maybe actually instead of bias, they're like,

02:04:53.200 --> 02:04:56.880
what you really want to do is figure out what are the priors? What are the model classes?

02:04:56.880 --> 02:05:02.080
What are the preferences? The bias is a kind of preference that really line up with the world

02:05:02.080 --> 02:05:06.400
in reality or the domain and therefore let you move to a better trade off curve

02:05:06.400 --> 02:05:13.600
among statistical bias and statistical variance. Amazing. Well, Pedro, just tell us a little bit

02:05:13.600 --> 02:05:17.040
about what have you seen at NeurIPS and how's the week been for you?

02:05:18.400 --> 02:05:20.880
We've already touched on some of the interesting things that I saw,

02:05:22.240 --> 02:05:26.560
in particular some of the areas that I'm interested in. The thing about NeurIPS is this,

02:05:26.560 --> 02:05:30.800
of course, is that it's a vast conference. In the early days, I used to at least go through

02:05:30.800 --> 02:05:36.240
the proceedings and look at the title and maybe the abstract of every paper. This is now impossible.

02:05:37.040 --> 02:05:41.200
Now, these days, if all you do is try to walk through the poster sessions,

02:05:41.280 --> 02:05:46.480
you never get to the end. I haven't been to a single poster session in this NeurIPS

02:05:46.480 --> 02:05:50.960
where I actually got through all. I like to go through the poster sessions quickly once

02:05:51.680 --> 02:05:56.080
and then just to see what's there and then go back to the ones that I found really interesting.

02:05:56.080 --> 02:06:01.920
I haven't actually been able to even finish that walk through because they're so vast. You're also

02:06:01.920 --> 02:06:06.880
running to people which is part of the point and talk and whatnot, but when there's 500 posters in

02:06:06.880 --> 02:06:10.640
every session and there's 3,000 papers in the conference, it becomes very hard to find the

02:06:10.640 --> 02:06:15.120
ones that are most relevant. Of course, an easy thing to do is look at what they, I mean,

02:06:16.400 --> 02:06:20.160
something about NeurIPS this year that I honestly thought was absolutely terrible,

02:06:20.160 --> 02:06:27.760
like a really, really terrible idea is that it's a hybrid conference and their idea of

02:06:27.760 --> 02:06:34.080
a hybrid conference is that there are no talks. The talks are all virtual next week. Nips this

02:06:34.080 --> 02:06:40.160
year to a first approximation was one big poster session, which I mean, to me, this is just an

02:06:40.160 --> 02:06:46.080
incredibly bad idea. In that sense, I haven't gotten as much out of Nips by this point of the

02:06:46.080 --> 02:06:51.840
conference as I would have in most years. There's also looking at the papers that were usually

02:06:51.840 --> 02:06:56.880
selected as oral, but this time they call them oral equivalent because there are no oral papers,

02:06:56.880 --> 02:07:03.200
but they still want to have that distinction. The number of those papers these days is 160 or

02:07:03.200 --> 02:07:10.080
something, which is bigger than Nips and ICML some years ago. Usually from those papers,

02:07:10.480 --> 02:07:15.680
some of them kind of like jump out at you as being great and very relevant. I've only looked at

02:07:15.680 --> 02:07:23.440
them briefly, so don't quote me on this, if you will, but none of those have jumped out to me

02:07:25.120 --> 02:07:28.560
as like, oh, yeah, this sounds like something really brilliant and that I want to dig into,

02:07:28.560 --> 02:07:32.640
but there probably are many. I just haven't really had a chance to look at them yet.

02:07:32.640 --> 02:07:37.760
Yeah. I mean, I have a similar reaction. I mean, it feels like we're at the point of saturation

02:07:37.760 --> 02:07:42.640
and there are loads and loads of microvariations on the same idea. It's completely overwhelming,

02:07:42.640 --> 02:07:46.800
but what I find is that it's a very social experience. When I walk through the posters,

02:07:46.800 --> 02:07:51.040
I just immediately become engrossed in conversation and hours go by and I just think, oh my God,

02:07:51.040 --> 02:07:55.680
what have I just been doing for the last year? That's the real point. The posters are very good.

02:07:57.280 --> 02:08:00.880
It's like the grain of sand and the oyster. The poster is the grain of sand. The oyster is the

02:08:00.880 --> 02:08:05.360
conversation that you have with the person at the poster or with other people around there.

02:08:05.360 --> 02:08:08.160
To touch on another point that you made that I think is actually important.

02:08:09.520 --> 02:08:13.680
New Europe's and ICML and so on are bigger today than they've ever been. Actually,

02:08:13.680 --> 02:08:17.360
not strictly true because these recent lips, surprisingly, they tend to have gone down a

02:08:17.360 --> 02:08:26.720
lot. We can and should ask why, but we need to scale. There are bigger conferences,

02:08:26.720 --> 02:08:31.280
like the New Science Conference is one conference and it's 35,000 people every year and they make

02:08:31.280 --> 02:08:37.360
it work. It's good to experiment. I think New Europe's at the scale that it is today can work,

02:08:37.360 --> 02:08:42.000
but it is not working very well. One of the ways in which it's not working very well is that

02:08:42.560 --> 02:08:46.240
we need to think a lot more. I don't understand this is working. It's hard and people have day

02:08:46.240 --> 02:08:51.520
jobs or not, so this is not a criticism in that sense. We need to really work on making it easy

02:08:51.520 --> 02:08:56.640
for people to find the papers that are relevant to them. Number one, number two, and maybe even

02:08:56.640 --> 02:09:01.760
more important, there is more machine learning research today than ever, but in some sense the

02:09:01.760 --> 02:09:06.880
diversity of that research is in some ways lower than ever. Another point that you brought up and

02:09:06.880 --> 02:09:11.280
I think is very important to do with the scaling of New Europe's and the machine learning communities

02:09:11.280 --> 02:09:17.680
that we have in just raw numbers, more machine learning and AI research going on today than ever

02:09:17.680 --> 02:09:22.720
before by an order of magnitude. But in terms of diversity, there's probably less diversity in the

02:09:22.720 --> 02:09:28.160
research now than there was before, which is a tragedy. I understand why people have kind of

02:09:28.160 --> 02:09:32.240
like converged to deep learning. I'm a huge fan of deep learning. I was doing it before it was

02:09:32.240 --> 02:09:37.520
cool as they say and whatnot, but the extent to which 90% of the community, not just in machine

02:09:37.520 --> 02:09:43.280
learning but AI, is not just pursuing and not even deep learning, but a special type of deep

02:09:43.280 --> 02:09:51.120
learning, which you might call applications of backprop, is extremely undesirable. We have

02:09:52.080 --> 02:09:57.120
an infinite number of micro-improvement papers along a particular direction that is almost

02:09:57.120 --> 02:10:02.000
certainly a local optimum, and we're just digging into that local optimum with ever more papers and

02:10:02.000 --> 02:10:08.080
never more, you know, minimal publishable units when this large amount of manpower that has come

02:10:08.080 --> 02:10:14.400
into the field or is moving around, we really need to have a greater diversity of research in

02:10:14.400 --> 02:10:21.680
machine learning, within deep learning, within AI, and so like we are making very poor use of our

02:10:21.680 --> 02:10:26.320
research, you know, manpower right now, and we see that very much at NeurIPS today.

02:10:26.320 --> 02:10:29.200
Yeah, I mean, Sarah Hooker talked about the hardware lottery, you know, being stuck in a

02:10:29.200 --> 02:10:33.600
basin of attraction determined by hardware, but there's also an idea lottery. It might just be

02:10:33.600 --> 02:10:38.320
the case that NeurIPS historically has always been very connectionist anyway. I mean, maybe it

02:10:38.320 --> 02:10:41.520
hasn't, right? That's one of the ironies, but it's something as well. I wasn't aware of that. Okay.

02:10:41.520 --> 02:10:47.920
Oh, absolutely not. I mean, in fact, the joke is, right, that NeurIPS started in the 80s,

02:10:47.920 --> 02:10:52.400
it was called Neural Information Processing Systems, and by the 90s, it should have become

02:10:53.040 --> 02:10:57.600
BIPs for Patient Information Processing Systems, right? There was this study that they did at one

02:10:57.600 --> 02:11:02.320
point of predictors of acceptance and rejection among words in the title, and the biggest predictor

02:11:02.320 --> 02:11:07.120
of rejection was the world neural. Really? And this was very famous in the field, because

02:11:07.200 --> 02:11:12.240
indeed, if you could, you know, 1990 something, you were submitting papers to NIPs with the

02:11:12.240 --> 02:11:16.800
world neural in the title, you didn't know what you were doing. And then in the 2000s, right,

02:11:16.800 --> 02:11:22.240
it became BIPs, or should have become BIPs, sorry, KIPs, Kernal Information Processing Systems.

02:11:22.240 --> 02:11:27.200
And in fact, I remember having lunch with Yoshio Bingo at the ICML in Montreal in 2009,

02:11:27.200 --> 02:11:31.440
and we were talking about this, right? The fact that every day kid, and, you know,

02:11:31.440 --> 02:11:35.680
not a new paradigm, but another one of the same paradigm seems to now be on top, right?

02:11:35.680 --> 02:11:39.520
And, you know, he asked, like, so what is the next decade going to be? And I said,

02:11:39.520 --> 02:11:44.720
it's going to be DIPs, Deep Information Processing Systems. And then we both laughed,

02:11:44.720 --> 02:11:49.280
and I could tell that I believe this, but he, Yoshio Bingo, was actually skeptical of this.

02:11:49.280 --> 02:11:53.920
So, you know, the deep, little did we know, right? If somebody told us that, you know,

02:11:53.920 --> 02:11:56.960
this is going to be on the page of the, on the front page of the New York Times,

02:11:56.960 --> 02:12:01.360
in a couple of years would be like, what are you smoking, right? So the way to which this decade

02:12:01.360 --> 02:12:06.320
has been DIPs is just mind-blowing, but looking forward, right? And to this point of, you know,

02:12:06.320 --> 02:12:11.280
diversity in research approaches, I think if you extrapolate naively from the past,

02:12:11.280 --> 02:12:15.600
the next decade will be about something else. And the trillion-dollar question

02:12:15.600 --> 02:12:21.840
is what, what is that else going to be? Amazing. Okay. You watched Charma's talk, right?

02:12:21.840 --> 02:12:26.400
Yeah. What's your high-level view? I thought it was a nice talk. I thought it was a very

02:12:26.400 --> 02:12:30.720
appropriate talk for an opening talk at the conference. Actually, if New Europe's had,

02:12:30.720 --> 02:12:36.240
like, some conferences, a dinner talk, right? Which is supposed to be interesting, but not as,

02:12:36.240 --> 02:12:39.840
you know, deep or as technical as other. This would have been the perfect dinner talk for

02:12:39.840 --> 02:12:45.520
New Europe's, because the topic is very current, right? Our machine's sentient. And, you know,

02:12:45.520 --> 02:12:50.880
who better to talk about it than Dave Chalmers, right? The world's expert on, on, on, on consciousness,

02:12:50.880 --> 02:12:57.120
right? And by and large, I thought the talk was excellent. In fact, you know, when journalists

02:12:57.120 --> 02:13:01.360
ask me questions, you know, consciousness is like one of their top three, right? Along with

02:13:01.360 --> 02:13:06.080
Terminator and, you know, Unfairness or something like that, right? And I will point them to this

02:13:06.080 --> 02:13:12.880
talk because it kind of like lays out, you know, the, you know, the ground. And, you know, it's good

02:13:12.880 --> 02:13:18.880
for people to at least have those things in mind. At the end of the day, so I think, of course,

02:13:18.880 --> 02:13:24.560
the notion that Lambda was sentient is, you know, ridiculous, as, as most of us do.

02:13:25.280 --> 02:13:30.480
You could ask a slightly more fine-going question was if, if, if, if, if consciousness is on a

02:13:30.480 --> 02:13:35.680
continuum, right? Which I think Dave believes in. And if you believe in like this, you know,

02:13:35.680 --> 02:13:40.000
IT theory and phi and whatnot, you know, like, phi is never zero, right? So there's always some

02:13:40.000 --> 02:13:44.640
consciousness, right? Pensychism and whatnot. I'm not saying I believe in that. We could,

02:13:44.640 --> 02:13:48.480
we could go into the, but like, if you believe in that, then you can ask, well, on that scale,

02:13:48.480 --> 02:13:54.880
you know, where is Lambda? Where are these large language models? And, and, and surely higher than

02:13:54.880 --> 02:14:00.960
previous AI systems, right? But in my view, still very, very, very far. And I think what you want

02:14:00.960 --> 02:14:05.840
to keep in mind is that consciousness does not, does not increase continuously. Precisely,

02:14:05.840 --> 02:14:11.200
there's these transitions where you go, you know, more is different is the, is the famous,

02:14:11.200 --> 02:14:15.520
you know, phrase about emergence, right? Consciousness is very much an emerging,

02:14:15.600 --> 02:14:19.840
you know, phenomenon. And I think what happens is that there are points at which your

02:14:19.840 --> 02:14:24.160
consciousness will leap. Maybe a thermostat does have consciousness, like, you know,

02:14:24.960 --> 02:14:29.360
or, you know, or purpose or whatever, right? Like, like people in, like people like McCarthy,

02:14:29.360 --> 02:14:34.480
for example, had had had that as an example. But the amount of consciousness is minuscule.

02:14:34.480 --> 02:14:40.640
And, and that, and the way I will put that is that these large language models still have not

02:14:40.640 --> 02:14:45.360
passed that first threshold. Interesting. So, so in a similar way to some of the discussion

02:14:45.360 --> 02:14:50.160
about large language models, there are kind of scaling breaks in the levels of consciousness.

02:14:50.160 --> 02:14:53.920
I mean, Chalmers made the comment, though, that rather than it being a pure continuum,

02:14:53.920 --> 02:14:58.160
he said that a bottle was not conscious, but then there was a kind of. No, yes. So very key

02:14:58.160 --> 02:15:04.560
point. Scaling is part of it, but not only. It's not just that. So your cortex to first

02:15:04.560 --> 02:15:10.160
approximation is a monkey brain scaled up, right? There was a module there that evolution

02:15:10.160 --> 02:15:14.960
discovered, and it really paid to keep making more and more of it. And we can easily speculate why.

02:15:14.960 --> 02:15:20.320
But the point is, so let me contrast two things, right? Which is true for consciousness, but also

02:15:20.320 --> 02:15:25.920
for just AI in general. A lot of people are scaling believers and like open AI is the poster child

02:15:25.920 --> 02:15:29.920
of this in a quite conscious ways. Like, we're just going to scale the heck out of things.

02:15:29.920 --> 02:15:33.280
And then a lot of people, like, you know, Gary Marcus being a good example, they just

02:15:33.280 --> 02:15:37.840
completely poo poo that they say, like, oh, no, this is a joke. Right. And I think the truth is that

02:15:38.480 --> 02:15:43.120
scaling is good, right? Again, you know, part of what we are, our intelligence is scaling.

02:15:43.120 --> 02:15:48.240
But the question is, what are you scaling? And the things that we're scaling today,

02:15:48.240 --> 02:15:53.360
it doesn't matter how much we scale them, we never get to human level intelligence or consciousness.

02:15:53.360 --> 02:15:58.080
So I think we need some fundamentally different algorithms, if you want to think at the level

02:15:58.080 --> 02:16:02.320
of algorithms, or fundamentally different architect architectures, if you want to think

02:16:02.320 --> 02:16:07.040
about it in a way, and then scaling those up at some point will give us consciousness.

02:16:07.040 --> 02:16:10.880
If you live that it's possible for a computer to be conscious, but we're not there yet,

02:16:10.880 --> 02:16:15.120
either in terms of the scaling, although actually scaling is actually the easier part of this way,

02:16:15.120 --> 02:16:19.200
we're actually at the point where a computer can have the same amount of computing power that

02:16:19.840 --> 02:16:24.880
your brain does, which was not the case before. But the bigger deeper problem, and the more

02:16:24.880 --> 02:16:29.600
fundamental one is like, we need the architecture to scale. Right. And this is where I sympathize,

02:16:29.600 --> 02:16:34.400
you know, with people like Jeff Hinton, who's just, you know, playing with, you know, ideas

02:16:34.400 --> 02:16:39.280
using Mathematica and very small examples, which in some ways, sounds very underpowered,

02:16:39.280 --> 02:16:44.240
but I think it's people like that, they are going to come up with the things that we then scale.

02:16:44.240 --> 02:16:48.320
As in fact, it was David Roemmerhardt doing that kind of work that invented backprop.

02:16:49.200 --> 02:16:53.120
Right. If he hadn't invented backprop, this whole industry would not exist. So

02:16:53.120 --> 02:16:57.920
what I think is that the real backprop, the real master algorithm is not there yet,

02:16:57.920 --> 02:17:03.600
and we need to discover that first. And then we, and then when we scale that up, which will not

02:17:03.600 --> 02:17:08.960
be trivial, but will be much easier by comparison, then we'll have, you know, human level, intelligence,

02:17:08.960 --> 02:17:14.800
consciousness, et cetera. Interesting. Okay. And so Charmes is a structuralist computationalist.

02:17:14.800 --> 02:17:22.720
So, you know, he thinks information, not biology. And he's also a functionalist, right? So, you

02:17:22.720 --> 02:17:28.720
know, which is very similar to behavior. And, you know, Hillary Putnam made the move that you can

02:17:28.800 --> 02:17:33.600
kind of like represent a computation in any open physical system. And he kind of like used that

02:17:33.600 --> 02:17:38.160
on, you know, if you follow that line of thought, it almost trivializes computationalism because,

02:17:38.160 --> 02:17:42.800
you know, it leads to panpsychism very, very quickly. So, first of all, I mean, what's your

02:17:42.800 --> 02:17:46.800
take on this idea that information could give rise to intelligence and consciousness?

02:17:46.800 --> 02:17:51.440
So I agree, like most scientists, and I think in particular most computer scientists, that

02:17:52.160 --> 02:17:56.880
to a first approximation, the substrate does not matter. And in particular,

02:17:56.880 --> 02:18:01.680
you're not going to convince me that something is not conscious just because it's not biological.

02:18:02.320 --> 02:18:06.320
There is no reason to think that only biological things can have consciousness. Now,

02:18:06.320 --> 02:18:12.000
the deeper problem, and you know, indeed the hard problem, is that so as Dave Chalmers defined it,

02:18:13.040 --> 02:18:16.160
so there's a basic fork here, which you've alluded to, which is,

02:18:17.280 --> 02:18:23.200
if consciousness is subjective experience, then all these questions about consciousness are

02:18:23.200 --> 02:18:29.600
ultimately unresolvable, because only I have my subjective experience. I know that I'm conscious,

02:18:30.160 --> 02:18:35.120
no one can persuade me of the contrary. I don't even know if you are conscious, let alone some machine.

02:18:35.680 --> 02:18:41.120
Right? So if consciousness is an intrinsic property of something that cannot be evaluated

02:18:41.120 --> 02:18:45.920
from the outside, then we're doomed. We're never going to answer this question. And maybe that is

02:18:45.920 --> 02:18:50.320
the case. Right? So I'm not saying that's false, and you need to always keep that in mind. But now,

02:18:50.400 --> 02:18:55.840
if we're going to make any kind of progress, right, we need to look at what are, to generalize a well

02:18:55.840 --> 02:19:01.120
known term, the external correlates of consciousness. Right? One of those which has been well studied by

02:19:01.120 --> 02:19:05.760
people like Christoph Koch and so on, and I think that's a very good direction, is the neural

02:19:05.760 --> 02:19:10.000
correlates of consciousness. Right? What goes on in your brain that correlates with consciousness?

02:19:10.000 --> 02:19:14.080
And we've made a lot of progress with that. You can also talk about what are sort of like the

02:19:14.080 --> 02:19:19.760
informational computational correlates of consciousness. Are there computational structures

02:19:19.760 --> 02:19:24.640
that support consciousness and the ones that don't? I think that is also a useful thing to do.

02:19:24.640 --> 02:19:30.000
Let's develop. It actually interests this panpsychism because it's not like everything is

02:19:30.000 --> 02:19:35.200
consciousness just because it can compute. Some computations after this emergence and these,

02:19:35.200 --> 02:19:40.880
you know, phase transitions may give rise to consciousness. Whereas others, it doesn't matter

02:19:40.880 --> 02:19:44.880
how much of them you have, they will never be conscious. So I think this is also a very useful

02:19:44.880 --> 02:19:51.120
way to make progress on this question and one to which AI versus, you know, a neuroscience or

02:19:51.120 --> 02:19:56.400
psychology is very well suited to. Interesting. So on the functionalism point, and I think

02:19:56.400 --> 02:20:02.080
Chalmers has been very, very consistent. He uses this kind of calculi to reason about

02:20:02.080 --> 02:20:07.760
intelligence as well. So a system is intelligent if it can perform reasoning, if it can perform

02:20:07.760 --> 02:20:12.320
planning, if it has sensing and so on. So we have this collection of functions. And then

02:20:12.400 --> 02:20:16.480
he's kind of like moved this over to the domain of consciousness. So similarly,

02:20:16.480 --> 02:20:23.040
if a system performs these functions and is used in a positive and a negative way. So some

02:20:23.040 --> 02:20:28.320
functions would indicate an absence of consciousness and some functions would, you know, lead to the

02:20:28.320 --> 02:20:33.760
presence of consciousness. And it's kind of like leading towards a, you know, touring test for

02:20:33.760 --> 02:20:38.800
consciousness. I mean, do you kind of support that? That's a very interesting question. In fact,

02:20:38.800 --> 02:20:43.760
you know, I was having dinner with Dave after his talk and I actually brought this up because it

02:20:43.760 --> 02:20:50.160
wasn't clear from his talk. And I said, look, this is the answer that I usually give to journalists

02:20:50.160 --> 02:20:54.080
when they ask me, you know, will machines ever be conscious and whatnot? And asked me a few,

02:20:54.080 --> 02:20:58.800
and asked me if he agreed with it and actually expected him to disagree. But I think again,

02:20:58.800 --> 02:21:03.520
don't want to put words in his mouth, but that he agreed, right? And the answer is the following,

02:21:03.520 --> 02:21:08.480
is that human beings, right? As we've discussed, have an amazing tendency to

02:21:08.480 --> 02:21:13.200
anthropoformize things. It's reasoning by analogy. And what happens, I used to say,

02:21:13.200 --> 02:21:17.280
this is what's going to happen at this point is this is what is already happening is that

02:21:17.280 --> 02:21:24.000
as soon as a machine behaves externally, even vaguely like it's consciousness, we immediately

02:21:24.000 --> 02:21:29.280
start treating it as if it's consciousness. So if you look for 10, 20, 50 years from now,

02:21:29.280 --> 02:21:33.920
we will just treat AI's as if they're consciousness and people won't even ask that question.

02:21:33.920 --> 02:21:38.320
They will assume AI's are conscious in the same way that we assume that each other,

02:21:38.320 --> 02:21:44.160
that we're conscious, right? But then, and so like from that pragmatic external point of view,

02:21:44.160 --> 02:21:49.520
maybe the question is answered, right? But you could be a philosopher or like sort of like a very,

02:21:49.520 --> 02:21:54.960
you know, rigorous, you know, technical person and so like, no, no, no, no, I really want to know

02:21:54.960 --> 02:21:59.760
if things, they may look, you know, conscious from the outside, but are they really, right?

02:22:00.320 --> 02:22:04.720
But that question, as far as I can tell, unfortunately, at the end of the day is probably

02:22:04.720 --> 02:22:09.600
unanswerable. Now, there's a middle ground between these two things that maybe is where

02:22:09.600 --> 02:22:13.920
we'll wind up. And to me, sounds like probably the best thing that we're going to be able to do,

02:22:13.920 --> 02:22:18.800
which is that like, our understanding of the neural informational, et cetera, correlates

02:22:18.800 --> 02:22:24.640
of consciousness evolves to a point where we have the feeling that we do understand consciousness.

02:22:24.960 --> 02:22:28.560
It's not just the late person calls this consciousness even though haha, it's not like

02:22:28.560 --> 02:22:31.680
lambda is not conscious, you know, poor bozo, et cetera, et cetera. It's like,

02:22:32.400 --> 02:22:35.840
you know, there are many analogies to that in the history of science. There used to be a lot of

02:22:35.840 --> 02:22:41.280
things that were like magical, right? And we were like, oh, we're never going to stand like life was

02:22:41.280 --> 02:22:45.920
magical, right? Life did not obey the laws of physics. It's just something else, right? This

02:22:45.920 --> 02:22:50.240
sounds laughable right now, but it wasn't laughable at all then, right? And now, it's not like we've

02:22:50.240 --> 02:22:54.640
understood everything about life very far from it. When you say like, there's DNA and their

02:22:54.640 --> 02:22:59.120
cells and then this is how it all arises, right? And I think we're at the point in consciousness

02:22:59.120 --> 02:23:04.320
where it's to like, oh, consciousness is some so beyond us, right? I think we will get, you know,

02:23:04.320 --> 02:23:09.360
there will be a structure of DNA moment in the history of the study of consciousness.

02:23:09.360 --> 02:23:13.600
And I think, yeah, I think things like Phi and this, you know, IT3 and whatnot,

02:23:13.600 --> 02:23:18.400
they're very brave attempts to make progress in this direction. I think, you know, like Julia

02:23:18.880 --> 02:23:24.400
Tononi in a way is, you know, very deluded in thinking that he has nailed what consciousness

02:23:24.400 --> 02:23:30.400
is, right? I think, you know, Phi maybe is an upper bound on consciousness, but with steps like this,

02:23:30.400 --> 02:23:35.040
hopefully at some point, and very much with the help of AI, right? AI is really useful for this,

02:23:35.040 --> 02:23:40.240
because it's a brain that might be consciousness that we have a lot of control of. And you can do

02:23:40.240 --> 02:23:45.920
experiments that you can't, you know, with people, right? So I think we will make at least some progress

02:23:45.920 --> 02:23:50.800
in that direction for sure. Maybe to the point where we feel that, yes, we do understand what

02:23:50.800 --> 02:23:55.200
consciousness is, we're not asking ourselves that question anymore. And then we can point to things

02:23:55.200 --> 02:23:59.600
and say, this is consciousness, this is that kind of consciousness, that amount of consciousness,

02:23:59.600 --> 02:24:03.440
and so on. Yeah, that's really interesting. I agree, we're making a lot of progress in getting

02:24:03.440 --> 02:24:08.000
a handle on this. And although the biggest game in town is still the computationalism game. And

02:24:08.000 --> 02:24:13.040
as you say, historically, the only alternative was mysterious. And my friend, Professor Mark

02:24:13.120 --> 02:24:17.200
Bishop, that he said that that's one of the reasons why he's become interested in the

02:24:17.200 --> 02:24:21.120
forays in cognitive science, because for the first time, it's given him a kind of robust

02:24:21.120 --> 02:24:25.520
alternative to computationalism. But just coming back quickly, you know, as Charlie's

02:24:25.520 --> 02:24:29.920
reference, Thomas Nagel, you know, which is that it is something it is like to be a bat.

02:24:30.560 --> 02:24:36.000
What do you think about that? So I'm not sure your question is, but let me check.

02:24:36.000 --> 02:24:39.840
Well, what do you mean? Do you agree that there is something it is like to be a bat?

02:24:39.840 --> 02:24:45.760
Oh, absolutely. Right. So there is more and more than that, right? There is something that it's

02:24:45.760 --> 02:24:52.320
like to be a bat. And it's very different from being a human, right? And we grossly underestimate,

02:24:52.320 --> 02:24:56.960
right? Again, we do this thing that again, it's a heuristic, it works very well as like,

02:24:56.960 --> 02:25:01.120
we project ourselves into the bat, because what else could we do, right? But then what you see

02:25:01.120 --> 02:25:06.160
is a bat seen through the mind of a human, right? And in fact, there's this famous, I would say,

02:25:06.160 --> 02:25:11.760
even more famous, you know, you know, notion from, from Wittgenstein, right? That if the

02:25:11.760 --> 02:25:18.720
lion could talk, I would not understand anything that the lion was saying. Because his world is

02:25:18.720 --> 02:25:24.560
so different from mine. Now, I actually think, I think this is a very important position to,

02:25:24.560 --> 02:25:28.880
as a reference point, right? Certainly a defensible one. And, you know, Wittgenstein was a good

02:25:28.880 --> 02:25:34.400
defender of it. But I actually think that this is going too far. I think, ultimately, I mean,

02:25:34.400 --> 02:25:38.960
never be able to completely know what it's like to be a lion. But we can make a lot,

02:25:38.960 --> 02:25:42.640
don't underestimate us either, right? We can make a lot of inwards into understanding what

02:25:42.640 --> 02:25:48.320
it's like to be a lion, much more than we understand today. Same thing for a bat. And,

02:25:48.320 --> 02:25:53.280
you know, you could also ask that for a fruit fly, right? In a way, a fruit fly is more different

02:25:53.280 --> 02:25:58.000
from us than a lion, but it's easy to understand, right? Because at some level, that thing is so

02:25:58.000 --> 02:26:01.520
simple that we can understand what's going on with it, because it's not that deep.

02:26:01.600 --> 02:26:05.680
Yeah, that's a beautiful quote, actually. So, closing this off, do you think that large

02:26:05.680 --> 02:26:08.160
language models are slightly conscious or will be in the near future?

02:26:08.800 --> 02:26:14.160
I think language, I think large language models are not slightly conscious by the reasonable,

02:26:14.160 --> 02:26:18.560
you know, everyday definition of the world slightly, meaning that their consciousness,

02:26:18.560 --> 02:26:25.760
so I think that either their consciousness is just zero, right? If somebody asked me, like, you know,

02:26:25.760 --> 02:26:29.520
how much, you know, consciousness does, you know, lambda half, tell me in one word, and the answer

02:26:29.520 --> 02:26:35.600
would be zero, right? But another answer which is hard to distinguish from the first one is epsilon,

02:26:35.600 --> 02:26:41.360
right? Maybe it has a very tiny amount of consciousness, but it's so tiny that it doesn't

02:26:41.360 --> 02:26:46.320
even qualify as slightly. Again, this gets back to what its architecture is. It actually gets

02:26:46.320 --> 02:26:51.600
too lot of things, but for purposes of this discussion, right, lambda and these large

02:26:51.600 --> 02:26:56.480
language models are not very different from a big lookup table. Any big lookup table is not

02:26:56.480 --> 02:27:00.960
conscious. Now, I mean, there are a lot of interesting distinctions that you can make it well.

02:27:00.960 --> 02:27:05.040
What if what I have is an efficient approximation to a lookup table? Isn't that what your brain is,

02:27:05.040 --> 02:27:09.520
right? And I would say yes, and then people say, well, but then why is your brain conscious

02:27:09.520 --> 02:27:14.400
but not the lookup table, right? And precisely the interesting question is that the consciousness

02:27:14.400 --> 02:27:20.240
comes about from the fact that you have to concentrate all of this information, you know,

02:27:20.240 --> 02:27:25.360
in real time, into something, you know, very compact and that leads to action continuously,

02:27:25.360 --> 02:27:30.960
right? So to put this in another way, maybe God is unconscious because he doesn't need to be,

02:27:30.960 --> 02:27:35.520
right? If you're omnipotent and omniscient, you don't need to be conscious. You are effectively

02:27:35.520 --> 02:27:40.240
just a lookup table. Exactly. And I loved your response earlier about the grain of sand and

02:27:40.240 --> 02:27:43.920
the oyster. I thought that was a beautiful way of looking at it. And having recently studied

02:27:43.920 --> 02:27:49.200
so, I mean, personally, I think it's a lot to do with intentionality and agency, but I remember

02:27:49.200 --> 02:27:56.160
you responded to that. Just final quick question. What's your definition of intelligence?

02:27:57.920 --> 02:28:02.560
So let me start with the technical definition, which is unfortunately not widely known enough

02:28:02.560 --> 02:28:07.920
and not appreciated enough. But I think it's a really important one to have, right? Intelligence

02:28:07.920 --> 02:28:14.880
is solving NP-complete problems using heuristics. This is the real technical definition of AI,

02:28:14.880 --> 02:28:19.440
right? And there's a lot packed into that, right? The fact that it's NP-complete problems and the

02:28:19.440 --> 02:28:24.400
fact that it's using heuristics. If your problem is solvable with a lookup table with polynomial

02:28:24.400 --> 02:28:28.960
algorithms, you don't need intelligence and there's no intelligence there. It's when you start solving

02:28:28.960 --> 02:28:36.320
hard problems using heuristics that you're getting into the realm of intelligence. Moreover, NP-complete

02:28:36.320 --> 02:28:42.160
is not the same as exponential, right? The crucial thing about an NP-complete problem that connects

02:28:42.160 --> 02:28:47.120
very directly to our entire discussion of utility and whatnot is that the solution is easy to check.

02:28:48.000 --> 02:28:53.440
This is the key. If you're working on problems whose solution is impossible to check effectively,

02:28:53.440 --> 02:28:57.920
I can't even tell if you're intelligent or not. The whole thing about intelligence in humans and

02:28:57.920 --> 02:29:02.960
machines is that how you solve the problem requires a lot of intelligence, a lot of computing power

02:29:02.960 --> 02:29:06.720
and whatnot, but then I can easily check the solution. Now, hang on a minute, could that

02:29:06.720 --> 02:29:11.440
say a step away from behavior then if you're saying that, you know, like you have the percepts,

02:29:11.440 --> 02:29:14.720
the state and the action and you're saying the state is also important?

02:29:14.720 --> 02:29:22.240
No, so to answer that head on, intelligence is not behavior, right? Intelligence to give a slightly

02:29:22.240 --> 02:29:26.240
more general definition and then there's several and they all have their merits. Intelligence is the

02:29:26.240 --> 02:29:31.200
ability to solve hard problems. Then more concretely, it's NP-complete problems and using heuristics,

02:29:31.200 --> 02:29:38.000
but like, for example, if you create an AI system that cures cancer, it doesn't behave in the sense

02:29:38.000 --> 02:29:42.720
that a human and a robot behave, but, you know, it's damn intelligence, it's more intelligent

02:29:42.720 --> 02:29:48.080
than we are, right? It would be childish to deny intelligence to that system, no matter how it solves

02:29:48.080 --> 02:29:54.720
cancer. If it finds a ridiculously simple way to solve cancer, then it's even more brilliant,

02:29:54.720 --> 02:29:59.360
right? In fact, the simpler your outcome, the more intelligent you are, right? It takes intelligence

02:29:59.360 --> 02:30:04.800
to produce something simple. Wow. Concretely, in many circumstances, in particular evolution,

02:30:04.800 --> 02:30:09.840
right? Intelligence manifests itself as behavior. There's a sequential decision making problem,

02:30:09.840 --> 02:30:13.840
there's an agent in the world that said a certain stuff, being a stochastic parrot.

02:30:13.840 --> 02:30:18.640
And I think also from, you know, theoretical reasons, by analyzing what a transformer can

02:30:18.640 --> 02:30:23.360
represent and how it can learn, my best guess, which could be wrong again, I don't think anybody

02:30:23.360 --> 02:30:28.880
has the answer to this and it's interesting question is that those transformers, right,

02:30:28.880 --> 02:30:33.680
not LLM scholars, that means more of like a task rather than the, you know, than the architecture.

02:30:34.320 --> 02:30:38.000
Transformers have a certain limited ability to do compositionality,

02:30:38.560 --> 02:30:44.480
very limited to compare to full logic programming, etc., but exponentially better than something like

02:30:44.480 --> 02:30:49.920
an ordinary multilayer perceptron. And if you just, I mean, even a multilayer perceptron or any

02:30:49.920 --> 02:30:55.760
learning algorithm is more than a stochastic parrot, because it's general, the whole point

02:30:55.840 --> 02:31:00.320
of machine learning is to generalize beyond the data. If you generalize correctly beyond

02:31:00.320 --> 02:31:04.880
the data, you're not just a parrot anymore. And, you know, I think it's not an accident that that

02:31:04.880 --> 02:31:09.840
term stochastic parrot came from Emily Bender, my linguistics colleague at UW, who does not

02:31:09.840 --> 02:31:15.360
understand machine learning. She's a classic linguist of the Chomsky and Variety, who does,

02:31:15.360 --> 02:31:19.440
you know, does not fundamentally understand what I think, you know, she might disagree,

02:31:19.440 --> 02:31:23.040
what machine learning is all about. And she would probably look at any learning algorithm and say

02:31:23.040 --> 02:31:27.760
that it's a stochastic parrot, missing the fact that the whole point of machine learning and the

02:31:27.760 --> 02:31:32.800
thing that we focus on from, you know, beginning to end is generalizing. And as soon as you're

02:31:32.800 --> 02:31:37.680
generalizing correctly, even if you have no compositionality, you're already doing something

02:31:37.680 --> 02:31:42.320
that has a little bit of intelligence, and that's beyond what a parrot would do.

02:31:42.320 --> 02:31:46.480
Yeah, I mean, to be fair, it's not a binary. And at the time, I thought they were stochastic

02:31:46.480 --> 02:31:51.200
parents as well. I've updated my view. And you were talking as well about creativity. There's

02:31:51.200 --> 02:31:55.280
a kind of blurred hyperplane of creativity. And we discussed, you know, where that hyperplane

02:31:56.000 --> 02:32:00.240
sits. But, you know, what's really interested me, I've interviewed quite a few people that are

02:32:00.240 --> 02:32:04.800
working on working on in context learning in these language models. And it seems like these

02:32:04.800 --> 02:32:11.120
language models are almost almost like a new type of compiler, you know, you're writing a program

02:32:11.120 --> 02:32:17.120
inside the language prompt. And they seem to work extremely well outside of the training

02:32:17.120 --> 02:32:20.000
range if you're doing like basic multiplication tasks.

02:32:20.000 --> 02:32:23.840
I think it is useful to look at them as a new type of compiler. In fact, I've been saying for a

02:32:23.840 --> 02:32:29.280
long time that, you know, like, there's this continuum from programming an assembly code to

02:32:29.280 --> 02:32:34.880
high level languages to doing AI, right? The point of AI is to continue along that path

02:32:34.880 --> 02:32:40.000
to making the language that computers speak ever closer to ours, so that we can just program them

02:32:40.000 --> 02:32:44.640
by talking to them or writing things at them, right? Having said that, I think that, you know,

02:32:45.600 --> 02:32:51.360
what goes on in the innards of a transformer, right, is actually still

02:32:53.920 --> 02:32:59.920
very primitive, for lack of a better word, right? There's a lot of, so something I tweeted that

02:32:59.920 --> 02:33:04.400
got a lot of follow up from people like Yan and Gary and who the pro because they were all bringing

02:33:04.400 --> 02:33:08.720
in their own angles. So this was like, I said, and I think this is an interesting question. It's

02:33:08.720 --> 02:33:13.520
like the interesting question about transformers is what needs to be added to them to get real

02:33:13.520 --> 02:33:18.640
intelligence. So we should not deny what they have, like the attention mechanism in particular,

02:33:18.640 --> 02:33:23.200
right? And the embeddings and the context. So like, there are two very important things in

02:33:23.200 --> 02:33:28.240
transformers that are beyond what was in neural networks 10 years ago and are key. One of them

02:33:28.240 --> 02:33:34.720
is attention, right? Attention is a real advance. And the other one is context specific embeddings,

02:33:34.720 --> 02:33:38.720
right? Each of these ideas is important in its own right and combining them together is very

02:33:38.720 --> 02:33:43.440
powerful, right? Again, because the context sensitive embeddings get that the similarity

02:33:43.440 --> 02:33:47.840
part of intelligence, the attention combined with the context sensitivity of the embeddings

02:33:47.840 --> 02:33:53.440
gets at the compositionality part. So they do have, so there are a couple of steps forward on

02:33:53.440 --> 02:33:58.400
the road to human level intelligence, but there are many more. And rather than either saying like,

02:33:58.400 --> 02:34:02.640
oh, they're just parrots, they don't do anything, we're saying like, we've almost solved the eye,

02:34:02.640 --> 02:34:07.360
what we really should, we should try to understand better, you know, how the, you know,

02:34:07.360 --> 02:34:12.560
the attention and the context is dependent embeddings work, which we don't. But we also need

02:34:12.560 --> 02:34:17.040
to focus like, now, what are we still missing? Because we definitely are. And that's really

02:34:17.040 --> 02:34:20.960
where most of our focus should be. Yeah, I completely agree. And also just in defense

02:34:20.960 --> 02:34:25.760
of Bender, I mean, I think she's a brilliant linguist. And I personally think having that

02:34:25.760 --> 02:34:32.960
diversity of views from different people is useful. No, I mean, so I very much think that having a

02:34:32.960 --> 02:34:36.560
diversity of views is very important. And I think something that I'm always saying to my

02:34:36.560 --> 02:34:41.200
deep learning friends who can't stand, you know, who hate the guts of Gary Marcus is

02:34:41.920 --> 02:34:48.560
we really, really need informed critics. Yeah. And very typically, your informed critics are not

02:34:48.560 --> 02:34:54.240
people in the field. We are experts, but then we also suffer from the distortion of being experts.

02:34:54.240 --> 02:35:00.080
It's people in adjacent areas. And people like linguists and psychologists are very much those

02:35:00.080 --> 02:35:05.280
people, they're in adjacent areas, enough to have a good critique of AI. So for example,

02:35:05.280 --> 02:35:11.200
something that Jan is always throwing at Gary Marcus, that kind of doesn't sit well with me,

02:35:11.200 --> 02:35:15.040
says like, well, you should try building a real system sometime, and you can criticize this until

02:35:15.040 --> 02:35:19.200
we do. If we take the attitude that only engineers can criticize engineers, we're doomed.

02:35:20.240 --> 02:35:24.560
Having said that, there is a very big distinction between the knowledgeable informed critics like

02:35:24.560 --> 02:35:29.280
Gary Marcus, and the not so knowledgeable, not so well informed ones, which unfortunately,

02:35:29.280 --> 02:35:33.680
Emily is an example. I mean, she's my colleague at UW. And I've talked with her about some of these

02:35:33.680 --> 02:35:38.320
things. And her criticism of machine learning, unfortunately, like a lot of people, comes from

02:35:38.320 --> 02:35:43.840
a place of actually not fundamental understanding it very well. But people do say that Gary isn't

02:35:43.840 --> 02:35:48.480
an expert in deep learning and that he's, you know, attention seeking. What would you say to that?

02:35:48.480 --> 02:35:57.360
No, he's not an expert in deep learning. And so like, I agree with some of his criticisms,

02:35:57.360 --> 02:36:03.040
I disagree with others. Probably on balance, I disagree more with him than I agree. But

02:36:03.680 --> 02:36:07.840
first of all, there is a value to having critics like that, number one. But then number two,

02:36:08.800 --> 02:36:12.880
the reason his criticism, I mean, it would be better if he was also an expert in deep learning

02:36:12.880 --> 02:36:17.440
and made the same criticisms. And then the problem is that often his criticisms are wrong

02:36:17.440 --> 02:36:22.800
because he has a mental model of deep learning that is already outdated, or is oversimplified,

02:36:22.800 --> 02:36:27.440
right? But that to some degree is unavoidable. But the thing that makes his criticism valuable

02:36:27.440 --> 02:36:32.800
is that he's doing it at a level where on a good day, on a bad day, his criticisms miss the mark.

02:36:32.800 --> 02:36:37.200
But on a good day, which is the ones that matter, his criticism is actually useful because it's

02:36:37.200 --> 02:36:41.920
at a level where you don't need to understand the details. It's like, you claim to be producing

02:36:41.920 --> 02:36:47.440
intelligence. I as a psychologist know a lot about intelligence. That's what I study for a living,

02:36:47.440 --> 02:36:52.800
right? He knows more about aspects of intelligence than I do. Yeah. And from that point of view,

02:36:52.800 --> 02:36:57.840
what you're doing is lacking. And that I mean, like, he's written the whole books about, you know,

02:36:57.840 --> 02:37:02.000
again, because this goes back to when he was a PhD student and, you know, and symbolic learning and

02:37:02.000 --> 02:37:08.480
whatnot, there are very, you know, the deep learning folks have repeatedly underestimated

02:37:08.480 --> 02:37:13.200
how well he understands some of these problems. Because as a psychologist in particular interested

02:37:13.200 --> 02:37:18.560
in language learning, he's actually thought very long and hard about them. Oh, I know. So I've

02:37:18.560 --> 02:37:23.520
read his book and we've had him on the show three times. Which book? The algebraic mind.

02:37:23.520 --> 02:37:27.280
Yeah. So that's the most relevant one here. Yeah. As a psychologist, you know, he spent a lot of

02:37:27.280 --> 02:37:31.600
time studying how children learn rules. Right. And he talks very elegantly about a

02:37:31.600 --> 02:37:35.760
compositionality. And we've spoken about this. It's irrefutable. And I agree with him and we've

02:37:35.760 --> 02:37:42.400
supported him. I guess some of the things he argues are based on ethics, politics and virtue.

02:37:42.400 --> 02:37:48.320
And some of the things like compositionality, I think are irrefutable. I mean, I think irrefutable

02:37:48.320 --> 02:37:52.480
is a very strong word. I wouldn't say that they're irrefutable. I would say that they have,

02:37:53.440 --> 02:37:59.120
they have very strong backing, which the connectionists have not been able to effectively

02:37:59.120 --> 02:38:04.880
refute. But some of the criticisms that they have, you know, meaning people like Pinker and Prince

02:38:04.880 --> 02:38:10.320
and whatnot, famously of connectionists in the 80s, some of them are still valid, which is very

02:38:10.320 --> 02:38:15.440
salient. But some of them not really. And again, to go back to the daddy of this whole school of

02:38:15.440 --> 02:38:20.880
thought, who's Chomsky, right? His, you know, he made his name basically panning things like,

02:38:20.960 --> 02:38:25.920
you know, Markov models of language in Graham models, which he could say large language models

02:38:25.920 --> 02:38:31.440
are just a very glorified version of, right? But and at the time, you could, that criticism was

02:38:31.440 --> 02:38:37.120
very apt and, you know, and timely and it was useful, right? But, but, but, and famously, it's

02:38:37.120 --> 02:38:40.880
like, it's like, you can't learn a context free grammar, but context free grammar is what you

02:38:40.880 --> 02:38:46.160
do. Well, actually, now we know formally that you can learn a context free grammar. And, and,

02:38:46.160 --> 02:38:50.080
you know, because you only have to learn it probabilistically, which is what we do. And

02:38:50.160 --> 02:38:55.920
what our systems do. So his criticism was just, you know, mathematically off the mark. But also,

02:38:55.920 --> 02:39:01.360
when you look at systems that do speech language, et cetera, et cetera, it is that statistical

02:39:01.360 --> 02:39:06.320
approach that he made his name panning that has prevailed. And for reasons that we understand

02:39:06.320 --> 02:39:11.040
very well, and large language models are just the latest greatest expression of that. So at that

02:39:11.040 --> 02:39:16.880
level, a whole Chomsky and Pinker, Gary Marcus view of things, not only is it not irrefutable,

02:39:16.880 --> 02:39:23.600
it has been refuted. Okay. Let's just quickly come back to your definition of intelligence. So

02:39:23.600 --> 02:39:29.520
solving NP hard problems, I assume you would zoom out a little bit and, you know, it's more of a

02:39:29.520 --> 02:39:33.760
meta learning algorithm. So the ability to sell to sell different problems.

02:39:36.160 --> 02:39:43.920
Yes. So it's, if very good point, if all you have is the ability to solve one NP complete problem,

02:39:43.920 --> 02:39:48.880
that does not qualify as general intelligence, right? There's like, these days, this is a common

02:39:48.880 --> 02:39:53.920
definition to make this different difference between, you know, narrow intelligence and general

02:39:53.920 --> 02:39:58.720
intelligence and AGI and whatnot, right? And if you only solve one NP complete problem very well,

02:39:58.720 --> 02:40:02.800
you have narrow intelligence is the way I would put it, but you do not have general intelligence.

02:40:02.800 --> 02:40:08.320
General intelligence is precisely the ability to solve a limitless variety of problems, all that

02:40:08.320 --> 02:40:13.600
have this characteristic of they're hard to solve, but the solution is easy to check. Right? I mean,

02:40:13.600 --> 02:40:17.360
if you have the ability to solve problems, whose solution isn't easy to check, then maybe you're

02:40:17.360 --> 02:40:23.360
intelligent, but I can't decide whether intelligent or not. Interesting. Okay. And actually, Gary did,

02:40:23.360 --> 02:40:27.440
he put a paper about 20 years ago talking about how neural networks can't extrapolate. I think it

02:40:27.440 --> 02:40:33.040
was when he encoded numbers with a binary encoding or whatever. And we've been on a bit of a journey

02:40:33.040 --> 02:40:37.360
on this. So we had Randall Bellistrier, I've interviewed him yesterday, he's got this paper

02:40:37.360 --> 02:40:42.640
called the spline theory of neural networks. It basically says that a neural network decomposes

02:40:42.720 --> 02:40:46.960
an input space into these input activated polyhedra. And when we first read that,

02:40:46.960 --> 02:40:52.640
we felt that it kind of indicated Francois Chollet's assertion that neural networks are

02:40:52.640 --> 02:40:57.600
locality sensitive hashing tables, and they only generalize within, you know, these tiny

02:40:57.600 --> 02:41:03.360
polyhedra. And Randall's now updated this view to say in contrast to decision trees, these

02:41:03.360 --> 02:41:08.480
hyperplanes, they actually inform a lot of information in the extrapolative regime outside

02:41:08.480 --> 02:41:13.280
of the training range. So I always thought it was the inductive priors that gave the extrapolative

02:41:13.280 --> 02:41:18.480
performance on neural networks by photocopying the information everywhere. And so like, you know,

02:41:18.480 --> 02:41:23.040
this is a great example of where, you know, Gary might update his views because even basic MLPs

02:41:23.040 --> 02:41:28.720
are far more extrapolative than anyone realized. This is a very interesting question. But the

02:41:28.720 --> 02:41:34.160
way I would put it is that in that regard, in some sense, both of the sides are right.

02:41:34.880 --> 02:41:38.400
And the reason they're both right is that we're in very high dimensional spaces.

02:41:38.400 --> 02:41:42.800
Yeah. And we're in a very high dimensional space. The follow thing can happen, which is,

02:41:42.800 --> 02:41:48.880
you know, you have a data point, and you generalize to a vast region around that data point. And it's

02:41:48.880 --> 02:41:54.000
unfair to characterize these things as saying they just interpolate. In some sense, they really do

02:41:54.000 --> 02:42:00.080
extrapolate. But at the same time, that vast region that they generalize correctly to is an

02:42:00.080 --> 02:42:05.040
infinitesimal fraction of the much, much vaster reason that they have not generalized to but you

02:42:05.040 --> 02:42:11.280
and I can. So you got to keep that distinction in mind. And then in particular, right, I like to

02:42:11.280 --> 02:42:17.760
say that deep learning is nearest neighbor in curved space. And both parts of that are very

02:42:17.760 --> 02:42:25.520
important, right? So, you know, Jan Lacoon was famous, you know, during the glory days of kernel

02:42:25.520 --> 02:42:31.040
machines for saying that kernel machines are just glorified template matches. Right. And of

02:42:31.040 --> 02:42:34.720
course, they didn't earn him any friends, but he was right. They really are just glorified template

02:42:34.720 --> 02:42:39.440
matches. Kernel machine is really a souped up, more mathematically elegant and blah, blah,

02:42:39.440 --> 02:42:43.840
blah version of nearest neighbor. Right. And the nearest neighbor is just a template matcher.

02:42:44.400 --> 02:42:48.400
The beauty in the power of nearest neighbor, though, is that there is a neighborhood within

02:42:48.400 --> 02:42:54.080
which often it generalizes very well. Right. Now, I think what Jan was missing, and I probably

02:42:54.080 --> 02:43:00.960
still is, is that coordinates and deep learning, they are still just a glory. They are also glorified

02:43:00.960 --> 02:43:06.400
nearest neighbor, except more glorified. And the way in which they're more glorified, which is

02:43:06.400 --> 02:43:10.960
very important is that they are doing nearest neighbor in curved space. They are still just

02:43:10.960 --> 02:43:15.760
doing, you know, generalization by similarity, which you could argue is all that machine learning

02:43:15.760 --> 02:43:20.800
does is generalizing by similarity. Another notion of similarity can vary. Right. But the

02:43:20.800 --> 02:43:24.080
important thing that they've done is that nearest neighbor just uses some distance

02:43:24.080 --> 02:43:29.120
measured in the original space, whereas the neural networks are warping the space to make

02:43:29.120 --> 02:43:34.880
the problem easier for the nearest neighbor, you know, essentially dot product based similarity

02:43:36.160 --> 02:43:40.080
computation that they're actually doing. Oh, sure. But you're very much arguing,

02:43:40.080 --> 02:43:43.120
this is the way Francois Chouelet puts it, that, you know, you have all of these

02:43:44.080 --> 02:43:50.080
transformations and you kind of distort the space, you know, to represent the data manifold. And,

02:43:50.080 --> 02:43:54.720
you know, you want it to, you stop SGD at the right time so that you approximate the data

02:43:54.720 --> 02:43:59.600
manifold and you can do this kind of latent space, you know, interpolation on the geodesic of that

02:43:59.600 --> 02:44:04.800
manifold. But, you know, Randall's idea is completely away from that idea of, you know,

02:44:04.800 --> 02:44:12.160
these models learning this curved space. And so if you do slice the space up with these hyperplanes,

02:44:12.160 --> 02:44:16.160
rather than it being a locality prior, which is what you're talking about, these hyperplanes give

02:44:16.160 --> 02:44:21.360
you globally relevant information to things that are, you know, miles away from the training data.

02:44:21.360 --> 02:44:29.040
Yeah, so, but these two perspectives are more similar than you might think, because I can take

02:44:29.680 --> 02:44:35.680
a distorted version of space and decompose it into polyhedron, right? And one or the other might

02:44:35.680 --> 02:44:39.920
approximate what's really going on better. I mean, these neural networks do form curved spaces,

02:44:39.920 --> 02:44:43.840
except they're in practice, they're not curved because they find it, but ignoring that, right?

02:44:44.800 --> 02:44:52.080
When, let me put it this way, an eloquent example of this is if you look back at the original space,

02:44:52.080 --> 02:44:58.000
right? Again, treat this thing as a black box. Where does it generalize to? Does it generalize

02:44:58.000 --> 02:45:03.280
only to things, neural networks as we have them today? Does it generalize correctly only to things

02:45:03.280 --> 02:45:10.640
that are locally near the data point, or you can generalize well to things that are far, right?

02:45:10.640 --> 02:45:15.760
And the thing is that with nearest neighbor, you buy, you know, almost intrinsically, you only

02:45:15.760 --> 02:45:21.840
generalize period at all to things that are local. The beauty of deep learning and of the

02:45:21.840 --> 02:45:26.000
space swapping that's going on is, again, going back to this notion of the path kernel is that

02:45:26.000 --> 02:45:30.160
you're actually doing a nearest neighbor computation, not just in a space that's swapped,

02:45:30.160 --> 02:45:34.240
but you're doing it in the space of gradients, which actually means that you can generalize

02:45:34.240 --> 02:45:39.680
correctly to things that are very far from your examples, except they look similar in gradient

02:45:39.680 --> 02:45:45.600
space. A very simple example of this is a sine wave, right? If I try to learn a sine wave using

02:45:45.600 --> 02:45:50.880
nearest neighbor, I need an infinite number of examples, right? Because, you know, like what

02:45:50.880 --> 02:45:55.840
I've learned over here helps me not at all with the next turn of the sine wave, like that continuous

02:45:55.840 --> 02:46:00.400
extrapolation, right? At some point, there's this disaster where if the last piece of the

02:46:00.400 --> 02:46:05.040
sine was going up, I just keep going up and getting more and more wrong, right? And in fact,

02:46:05.040 --> 02:46:09.520
this kind of thing does happen in neural networks, but they also have the part to say like, and this

02:46:09.600 --> 02:46:15.760
again, this also happens, which is I'm going to transform this space more into a more intelligent

02:46:15.760 --> 02:46:22.320
one, which is the space of the slopes, right? And now if I've seen one cycle of the sine wave

02:46:23.120 --> 02:46:29.680
with some density of examples, by similarity in that transformed space, I generalize correctly

02:46:29.680 --> 02:46:35.760
and trivially to every other turn of the sine wave. So there's a very big fundamental difference

02:46:35.840 --> 02:46:39.600
between the two. Interesting. And you think with an MLP, it would be possible to have that kind

02:46:39.600 --> 02:46:44.240
of extrapolative generalization on a sine wave? Well, so people have studied this in multiple

02:46:44.240 --> 02:46:51.840
ways. And the problem, so the question is, it depends on what are the basis functions that it's

02:46:51.840 --> 02:46:57.520
using. Yes. So something that we didn't allude to at all in this conversation, but analyze all of

02:46:57.520 --> 02:47:01.840
this is like, what is your choice of basis functions, right? And the thing is, an MLP with

02:47:01.920 --> 02:47:06.560
the traditional, say, sigmoid or allude basis functions will not learn this, no matter, for

02:47:06.560 --> 02:47:11.040
obvious reasons, right? And again, you can represent it, right? The representative theorem is there,

02:47:11.040 --> 02:47:15.120
like the sine wave is just one sigmoid and then another one, you know, with a minus sign and

02:47:15.120 --> 02:47:21.120
then another one, but the data doesn't let you learn it. If as a basis function, you have sine

02:47:21.120 --> 02:47:24.960
waves, which is nothing unimaginable, that's what a Fourier transform is then, then you can learn

02:47:24.960 --> 02:47:30.640
it so easily, it's not even funny. So it depends dramatically on the basis function. And the

02:47:30.640 --> 02:47:35.360
question really becomes, what are the basis functions and the architect that let me generalize

02:47:35.360 --> 02:47:40.240
correctly to a lot of things, including this, such that, for example, and this is a very simple test,

02:47:40.240 --> 02:47:46.160
is like, I can nail a sine wave with a small number of examples without it being one of my basis

02:47:46.160 --> 02:47:50.080
functions. Yeah, exactly. And then this all comes back to, you know, we're talking about inductive

02:47:50.080 --> 02:47:54.160
prize and the bias variance trade off and even symmetries, actually. I mean, the Taco Cohen once

02:47:54.160 --> 02:47:59.680
said that, you know, if you encode all of the symmetries into the label function, then you would

02:47:59.680 --> 02:48:04.480
only need one labeled example. So it's always a trade off between how much induction are you doing?

02:48:04.480 --> 02:48:09.680
Well, interesting, you should say that I understand why he says that and it's, and it's not technically

02:48:09.680 --> 02:48:15.760
wrong. But I would say that practically what you need is such a set of symmetries per region of

02:48:15.760 --> 02:48:22.160
the space, per cluster, right? But, you know, in another way, I would actually make an even

02:48:22.160 --> 02:48:28.080
stronger statement, which again, is very perfectly mathematical, sounds same when you say, an object

02:48:28.080 --> 02:48:34.240
is just the sum of its symmetries or a function. If you tell me all the symmetries, every last one

02:48:34.240 --> 02:48:40.320
of an object, you've defined the object. So if I can learn the symmetries at that level, I don't

02:48:40.320 --> 02:48:44.640
need anything else. Of course, as we already discussed, that's not the whole answer. Likewise,

02:48:44.640 --> 02:48:50.000
with any function, if you tell me all the properties of the function, there are there, you know,

02:48:50.560 --> 02:48:54.160
to be more precise, all the symmetries of a function at some point, you've told me the whole

02:48:54.160 --> 02:48:58.640
function. And vice versa, from the function, I can, you know, I can read out all the symmetries

02:48:58.640 --> 02:49:02.960
that it has. In principle, doing that in practice can be, you know, a very difficult and subtle

02:49:02.960 --> 02:49:07.120
thing to do. That's a beautiful thing to say. You give me the symmetries and I'll give you the

02:49:07.120 --> 02:49:11.440
object. Yeah, exactly. Amazing. Professor Pedro Domingos, thank you so much for joining us today.

02:49:11.440 --> 02:49:13.840
It's been an honor. Thanks for having me. Amazing.

