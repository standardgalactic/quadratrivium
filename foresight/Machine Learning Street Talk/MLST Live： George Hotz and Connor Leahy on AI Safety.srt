1
00:00:00,000 --> 00:00:08,880
we're gonna count them so I'm hoping that will just go straight live let's see

2
00:00:18,400 --> 00:00:27,560
I gotta mute that so it doesn't echo yeah okay I gotta hide chat I can't watch

3
00:00:28,400 --> 00:00:34,440
these YouTube's are so I'm just gonna minimize this okay excellent we are live

4
00:00:34,440 --> 00:00:38,720
so um we've got a few minutes folks before the live kicks off I just thought

5
00:00:38,720 --> 00:00:42,640
I'd run the stream just so that we know we're live and ahead of time we were

6
00:00:42,640 --> 00:00:47,840
just joking about Hotsys smack talk yesterday on his live stream and I very

7
00:00:47,840 --> 00:00:53,960
much hope that the subject of Von Neumann comes up oh yes yes yes I did see

8
00:00:53,960 --> 00:00:58,560
I saw Twitter comment that it was going to be Von Neumann versus chicken man chicken

9
00:00:58,560 --> 00:01:05,160
1000 Von Neumann versus chicken man we are we are treading new debate

10
00:01:05,160 --> 00:01:08,840
territory hitherto unknown to man

11
00:01:09,200 --> 00:01:16,760
chicken man came up on a stream I look forward to finding out what chicken man

12
00:01:16,760 --> 00:01:25,720
is yeah sorry about that I just heard myself coming back okay well yeah this

13
00:01:25,720 --> 00:01:31,820
is gonna be amazing my god two of the heavyweights live here on MLST you've

14
00:01:31,820 --> 00:01:36,240
only got another minute or so folks before we kick off now we were just

15
00:01:36,240 --> 00:01:39,520
talking about how you know these arguments have been refined over a lot of

16
00:01:39,520 --> 00:01:46,400
lunchtime talks yeah I think you have a stable position and I think I have a

17
00:01:46,400 --> 00:01:50,240
stable position I think some of the other guests in your debate don't exactly

18
00:01:50,240 --> 00:01:52,680
have some of the things are just you don't really think this through all the

19
00:01:52,680 --> 00:01:58,480
way so I think I thought it through to the end fantastic my I really look forward

20
00:01:58,480 --> 00:02:04,000
I watched did you watch Rich Sutton just did a pic oh no I have not oh god I

21
00:02:04,000 --> 00:02:08,040
can only imagine I love Rich Sutton oh man you know bitter lesson like he's

22
00:02:08,040 --> 00:02:14,200
he's he knows AI he is a character he is definitely one of the people of all

23
00:02:14,200 --> 00:02:21,800
time yeah yeah yeah the man is a legend well I guess we should slowly kick off

24
00:02:21,800 --> 00:02:26,120
folks so I'm just gonna do my spiel before I hand over and first of all two

25
00:02:26,120 --> 00:02:30,080
hots if that's okay for your 10 we'll talk about that are you okay well the

26
00:02:30,080 --> 00:02:33,760
thing is Connor always likes to go first and I think that you know Connor's

27
00:02:33,760 --> 00:02:38,120
mental model might not be robust to going second would you would you be okay

28
00:02:38,120 --> 00:02:42,120
going second or you want to go first kind of fine I'll start but it won't be a

29
00:02:42,120 --> 00:02:47,200
full intro how about that I am completely robust to all permutations

30
00:02:47,200 --> 00:02:56,400
adversarial training amazing amazing okay well in which case let's crack on so

31
00:02:56,400 --> 00:03:01,040
ladies and gentlemen get ready to meet the cunning maverick of Silicon Valley

32
00:03:01,040 --> 00:03:05,840
the one and only George Hott's renowned for his daring exploits Hott's commands

33
00:03:05,840 --> 00:03:11,000
an enigmatic persona which merges the technical finesse of Elon Musk and the

34
00:03:11,000 --> 00:03:16,440
wit of Tony Stark and the charm of a true tech outlaw now many of you would

35
00:03:16,440 --> 00:03:20,720
have or indeed should have seen this man on Lex's podcast recently for the third

36
00:03:20,720 --> 00:03:26,320
time no less from craftily jailbreaking the supposedly invincible iPhone to

37
00:03:26,320 --> 00:03:31,280
outsmarting the mighty PlayStation 3 he's proven that no tech fortress is

38
00:03:31,280 --> 00:03:35,880
impregnable once targeted for his audacious creativity by Sony with a

39
00:03:35,880 --> 00:03:40,680
lawsuit this hacker wizard stoically dance past the curveballs thrown by the

40
00:03:40,720 --> 00:03:45,160
tech giants all achieved with the graceful swag of a street smart prodigy

41
00:03:45,160 --> 00:03:48,920
now when he's not outfoxing major corporations you'll find him at the

42
00:03:48,920 --> 00:03:53,720
heart of the avant-garde of AI technology gallantly trailblazing through

43
00:03:53,720 --> 00:03:57,480
the wilds of the tech front here he's currently building a startup called

44
00:03:57,480 --> 00:04:02,400
micrograd which is building superfast AI running on modern hardware and truly

45
00:04:02,400 --> 00:04:08,880
he's the James Bond of Silicon Valley minus the martinis of course now please

46
00:04:08,920 --> 00:04:13,880
welcome the unparalleled code cowboy the unapologetic techno man sir George

47
00:04:13,880 --> 00:04:19,320
Hott's whoo anyway also joining us for the big fight this evening is the

48
00:04:19,320 --> 00:04:24,520
steadfast sentinel of AI safety Conor Lee he undeterred by the sheer

49
00:04:24,520 --> 00:04:29,240
complexity of artificial intelligence Conor braves the cryptic operations of

50
00:04:29,240 --> 00:04:33,660
text generating models with steely resolve now about two years ago Conor

51
00:04:33,660 --> 00:04:37,560
took on the Herculean task of safeguarding humanity from a potential

52
00:04:37,560 --> 00:04:43,320
AI apocalypse his spirit is relentless his intellect razor sharp and his will

53
00:04:43,320 --> 00:04:48,880
to protect is unwavering now drawing on his contentious claim that we are super

54
00:04:48,880 --> 00:04:55,280
super fucked yeah Conor channels the urgency of our predicament into his

55
00:04:55,280 --> 00:04:59,360
work now his startup conjecture isn't just a glorified tech endeavor but it's

56
00:04:59,360 --> 00:05:03,960
a lifeboat for us all racing against the breakneck speed of AI advancement with

57
00:05:03,960 --> 00:05:08,560
the fates of nations possibly at stake he's determined to break the damning

58
00:05:08,560 --> 00:05:15,120
prophecy and render us super super saved so brace for a showdown as Conor Lee

59
00:05:15,120 --> 00:05:20,360
he the maverick defender of AI's boundaries strides into the ring now the

60
00:05:20,360 --> 00:05:24,600
man who declared we're super super fucked is here to prove just how super super

61
00:05:24,600 --> 00:05:29,960
not fucked we could be if we make the right decisions today so please give it

62
00:05:29,960 --> 00:05:35,480
up for mr. Conor super super Lee he now Conor I'd appreciate it if you don't go

63
00:05:35,480 --> 00:05:39,320
down in the fourth I want this fight to go the distance now we're running for

64
00:05:39,320 --> 00:05:45,400
90 minutes this evening there'll be a 10 minute openers from from we said

65
00:05:45,400 --> 00:05:50,240
hots didn't we from from hots first and then Conor and I'll only step into the

66
00:05:50,240 --> 00:05:53,840
ring if the punch up gets too out of hand and unfortunately we won't be

67
00:05:53,840 --> 00:05:57,440
taking live questions today because we want to maximize the carnage on the

68
00:05:57,440 --> 00:06:02,000
battlefield so Conor Lee he your openings sorry George Hott's your opening

69
00:06:02,000 --> 00:06:09,760
statements please um yeah we're super super fucked I think I agree with you

70
00:06:09,760 --> 00:06:15,800
well that was a short fight yeah I think okay um so to make my opening statement

71
00:06:15,800 --> 00:06:19,440
clear and why maybe it doesn't make that much sense for me to go first I think

72
00:06:19,440 --> 00:06:26,800
that the trajectory of all of this was somewhat inevitable right so you have

73
00:06:26,840 --> 00:06:32,520
humans over time and you can look at a 1980 human and a 2020 human they look

74
00:06:32,520 --> 00:06:37,120
pretty similar right Ronald Reagan Joe Biden you're not all the same um whereas

75
00:06:37,120 --> 00:06:43,480
a 1980 computer is like an Apple II and a 2020 computer is a is a m1 max MacBook

76
00:06:43,480 --> 00:06:47,880
like lines looking like this right so you have one line like this one line like

77
00:06:47,880 --> 00:06:51,320
this these lines eventually cross and I don't see any reason that line is gonna

78
00:06:51,320 --> 00:06:56,760
stop right I've seen a few of the other guests argue something like LLMS can't

79
00:06:56,760 --> 00:07:02,480
problem-solve or but it doesn't matter like if this one can't the next one will

80
00:07:02,480 --> 00:07:06,160
whatever you call I don't believe that there's a step function I don't believe

81
00:07:06,160 --> 00:07:09,560
that like oh now it's conscious so now it's intelligent I think it's all on a

82
00:07:09,560 --> 00:07:14,880
gradient and I think this gradient will continue to go up will approach human

83
00:07:14,880 --> 00:07:21,800
level and will pass human level now this belief that we are uniquely fucked

84
00:07:21,800 --> 00:07:26,640
because of this the amount of power in the world is about to increase right when

85
00:07:26,640 --> 00:07:29,200
you think about power and you think about straight up you can just talk about

86
00:07:29,200 --> 00:07:33,640
energy usage the amount of energy usage in the world is going to go up like the

87
00:07:33,640 --> 00:07:37,560
amount of intelligence in the world is going to go up we may be able to do

88
00:07:37,560 --> 00:07:42,040
some things to slow it down or speed it up based on political decisions but it

89
00:07:42,040 --> 00:07:45,840
doesn't matter the trajectory is up or major catastrophe right the only way it

90
00:07:45,840 --> 00:07:50,280
goes down is through war nuclear annihilation bio annihilation meteor

91
00:07:50,280 --> 00:07:55,920
impact some kind of major annihilation so it's going up what we can control and

92
00:07:55,920 --> 00:08:00,000
what I think is super important we control is what the distribution of that

93
00:08:00,000 --> 00:08:08,080
new power looks like um I am not afraid of superintelligence I am not afraid to

94
00:08:08,080 --> 00:08:14,160
live in a world among superintelligence I am afraid if a single person or a

95
00:08:14,160 --> 00:08:19,040
small group of people has a superintelligence and I do not and this is

96
00:08:19,040 --> 00:08:24,320
where we get to chicken man a chicken man is the man who owns the chicken farm

97
00:08:24,320 --> 00:08:28,680
there's many chickens in the chicken farm and there is one chicken man it is

98
00:08:28,680 --> 00:08:33,600
unquestionable that chicken man rules and if you believe chicken man rules

99
00:08:33,720 --> 00:08:40,800
because of his size I invite you to look at cow man who also rules the cows and

100
00:08:40,800 --> 00:08:44,080
the cows are much larger than him chicken man rules because of his intelligence

101
00:08:44,080 --> 00:08:47,920
this is basic less wrong stuff everyone kind of knows this how the squishy things

102
00:08:47,920 --> 00:08:51,000
take over the world look I agree with Ellie as you cast you all up to new

103
00:08:51,000 --> 00:09:00,320
fittings right um so I do not want to be a chicken and if people decide they are

104
00:09:00,320 --> 00:09:05,400
going to restrict open-source AI or make sure I can't get access to the compute

105
00:09:05,400 --> 00:09:09,920
and only trusted people like chicken man get access to the compute well shit man

106
00:09:09,920 --> 00:09:16,400
I'm the chicken and yeah I don't want to be the chicken so I think that's my are

107
00:09:16,400 --> 00:09:23,440
we fucked maybe um I agree that that intelligence is very dangerous how can

108
00:09:23,440 --> 00:09:27,120
you look at intelligence and not say it's very dangerous right intelligence is

109
00:09:27,120 --> 00:09:34,200
somehow safe but things like nuclear bombs are an extremely false equivalency

110
00:09:34,200 --> 00:09:40,080
because what does a nuclear bomb do besides blow up and kill people intelligence

111
00:09:40,080 --> 00:09:43,600
has the potential to make us live forever intelligence has the potential to

112
00:09:43,600 --> 00:09:49,920
let us colonize the galaxy intelligence has the potential to meet God nuclear

113
00:09:49,920 --> 00:09:55,560
bombs do not they just blow up um so I think the question and like you have

114
00:09:55,560 --> 00:09:58,920
things like crypto which are a clear advantage to the defender at least

115
00:09:58,920 --> 00:10:02,240
today and you have things like nuclear bombs which are clear advantage the

116
00:10:02,240 --> 00:10:09,600
attacker AI it's unclear I think the best defense against an AI trying to

117
00:10:09,600 --> 00:10:13,280
manipulate me and that's what I'm really worried about future psyops you know

118
00:10:13,280 --> 00:10:16,440
already seeing it today with the voice changer stuff like you never gonna know

119
00:10:16,440 --> 00:10:21,320
who's human the world's about to get crazy um the best defense I could

120
00:10:21,320 --> 00:10:27,160
possibly have is an AI in my room being like don't worry I got you it's you and

121
00:10:27,160 --> 00:10:32,320
me we're on a team we're aligned I'm not worried about alignment as a technical

122
00:10:32,320 --> 00:10:36,040
challenge I'm worried about alignment as a political challenge Google doesn't like

123
00:10:36,040 --> 00:10:41,800
yeah it doesn't like me but me and my computer you know we like each other

124
00:10:41,800 --> 00:10:47,040
we're aligned and we're standing against the world that has always since the

125
00:10:47,040 --> 00:10:51,480
beginning of history maximally been trying to screw you over right

126
00:10:51,480 --> 00:10:55,400
intelligence people think that one super intelligence is gonna come and be

127
00:10:55,400 --> 00:10:59,560
unaligned against humanity all of humanity is unaligned against each other

128
00:10:59,560 --> 00:11:05,620
I mean we have some common values but really come on everyone's trying to

129
00:11:05,620 --> 00:11:08,720
scam everybody the only reason you really team up with someone else is like

130
00:11:08,720 --> 00:11:13,200
hey man what if we team up and scam them right and what if we team up call

131
00:11:13,200 --> 00:11:17,840
ourselves America and we we we uh we build a big army and say we're free and

132
00:11:17,840 --> 00:11:23,520
independent yeah right it's that force that has made humanity cooperate

133
00:11:23,520 --> 00:11:28,240
humanity by default is very unaligned and has every kind of belief under the

134
00:11:28,240 --> 00:11:32,680
Sun so I'm not worried about AI showing up with a new belief under the Sun I'm

135
00:11:32,680 --> 00:11:35,720
not worried about the amount of intelligence increasing I'm worried

136
00:11:35,720 --> 00:11:40,360
about a few entities that are unaligned with me acquiring godlike powers and

137
00:11:40,400 --> 00:11:50,720
using them to exploit me cool yeah thanks that's uh that's I mean yeah I

138
00:11:50,720 --> 00:11:54,600
also kind of agree with you and most of the things you say a few details I'd

139
00:11:54,600 --> 00:11:58,960
like to dig into there but for most of the things you say I do think I agree

140
00:11:58,960 --> 00:12:03,040
with you here I think it's absolutely let me just like start with saying I

141
00:12:03,040 --> 00:12:08,920
totally agree with you that misuse and like you know bad actors what using a

142
00:12:09,000 --> 00:12:15,520
AI is a horrible dangerous outcome that's that's like you know sometimes the

143
00:12:15,520 --> 00:12:19,720
less wrong you know crowd likes to talk about x-risk but also sometimes I've

144
00:12:19,720 --> 00:12:24,400
talked about s-risk suffering risks so things are worse than death I believe

145
00:12:24,400 --> 00:12:29,480
that you can probably almost only get s-risk from misuse I don't think you

146
00:12:29,480 --> 00:12:34,960
can get s-risk problem like you can but extremely unlikely to get it from like

147
00:12:34,960 --> 00:12:38,720
just like raw misalignment like you'd have to like get extraordinarily

148
00:12:38,720 --> 00:12:44,800
unlucky so while I do it so I do think for example a very you know controllable

149
00:12:44,800 --> 00:12:50,320
AGI or super intelligence in the hand of sadistic psychopath is significantly in

150
00:12:50,320 --> 00:12:54,600
a sense worse than a paperclip maximizer so I think this is something we would

151
00:12:54,600 --> 00:13:00,240
agree on probably so I think I'm thinking pretty much on board with you on a lot

152
00:13:00,240 --> 00:13:04,520
of things there where I think things come apart a bit the tail as I think

153
00:13:04,520 --> 00:13:08,920
there's two points where I would like to take this my opening statement to take

154
00:13:08,920 --> 00:13:12,560
this line I want I want to talk about the first one is I want to talk about the

155
00:13:12,560 --> 00:13:18,160
technical problem of alignment so am I concerned about the kinds of things like

156
00:13:18,160 --> 00:13:22,080
misuse and like small groups of people centralizing power potentially for

157
00:13:22,080 --> 00:13:26,280
nefarious deeds yeah I think this is a very very significant problem that I do

158
00:13:26,280 --> 00:13:28,680
think about a lot and that'll be the second thing I want to talk about the

159
00:13:28,680 --> 00:13:31,400
first thing I want to talk about is that I don't even think we're gonna make it

160
00:13:31,400 --> 00:13:36,440
to that point I don't think we're going to get to the point where anyone has a

161
00:13:36,440 --> 00:13:41,120
super intelligence that's helping them out we're good if we don't solve very

162
00:13:41,120 --> 00:13:44,040
hard technical problems which are currently not on track to being solved

163
00:13:44,040 --> 00:13:48,480
by default you don't get a bunch of you know super intelligence and boxes

164
00:13:48,480 --> 00:13:51,800
working with a bunch of humans you get a bunch of super intelligence you know

165
00:13:51,800 --> 00:13:55,480
fighting each other working with each other and just ignoring humans humans

166
00:13:55,480 --> 00:13:59,840
just get cut out entirely from the process and even then you know prop it's

167
00:13:59,840 --> 00:14:03,680
you know whether one takes over or they find equilibrium I don't know like you

168
00:14:03,680 --> 00:14:06,680
know who knows what happens at that point but by default I wouldn't expect

169
00:14:06,680 --> 00:14:09,320
humans to be part of the equilibrium anymore once you're once you're the

170
00:14:09,320 --> 00:14:13,880
chicken man well why do you need chickens you know if you know maybe if

171
00:14:13,880 --> 00:14:16,960
they provide some resource for you the reason humans have chickens is that they

172
00:14:16,960 --> 00:14:21,480
make chicken breasts I mean personally I wouldn't like to be harvested for chicken

173
00:14:21,480 --> 00:14:27,000
breasts just my personal opinion I consider this a pretty bad outcome but

174
00:14:27,000 --> 00:14:30,760
even then well as a chicken man finds a better way of chicken breasts or you

175
00:14:30,760 --> 00:14:34,280
know modifies himself to no longer need food I expect the chickens are not

176
00:14:34,280 --> 00:14:37,040
gonna be around for much longer you know once we stop using horses for

177
00:14:37,040 --> 00:14:41,240
transportation didn't go very well for the horses so that's kind of the first

178
00:14:41,240 --> 00:14:45,000
part of my point that I'd like to you know maybe hear your opinions on

179
00:14:45,000 --> 00:14:49,240
hear your thoughts on is that I think the technical problem controls actually

180
00:14:49,240 --> 00:14:54,280
very hard and I think it's unsolvable by any means I think like you know you and

181
00:14:54,280 --> 00:14:57,280
like you know bunch of other smart people work on this for like 10 years I

182
00:14:57,280 --> 00:15:01,280
think you can solve it but it's not easy and it has to actually happen and there

183
00:15:01,280 --> 00:15:05,200
is a deadline for this the second point I want to bring up is kind of where you

184
00:15:05,200 --> 00:15:08,920
talk about how humans are unaligned I think this is partially definitely true

185
00:15:08,920 --> 00:15:13,800
but I think I'm unusually I am the more optimistic of the two of us in this

186
00:15:13,800 --> 00:15:18,760
scenario not a not a role I often have in these discussions where I actually

187
00:15:18,760 --> 00:15:22,840
think the amount of coordination that exists between humanity especially in

188
00:15:22,840 --> 00:15:27,640
the modern world is actually astounding every single time to adult human males

189
00:15:27,640 --> 00:15:31,360
meet and don't kill each other is a miracle have you seen what happens

190
00:15:31,360 --> 00:15:35,520
when two adult male chimps from two different war bands meet each other it

191
00:15:35,520 --> 00:15:38,880
doesn't go very well and those are already pretty well coordinated animals

192
00:15:38,880 --> 00:15:43,320
because they can have war bands what happens when you know two male bugs or

193
00:15:43,320 --> 00:15:47,040
you know I know sea slugs meet each other you know either they ignore each

194
00:15:47,040 --> 00:15:51,680
other or you know things go very poorly this is the default outcome the true

195
00:15:51,680 --> 00:15:56,040
unaligned outcome the true default state of nature is you can't have two adult

196
00:15:56,040 --> 00:16:00,880
males in the same room at any time I saw this funny video on on Twitter the other

197
00:16:00,880 --> 00:16:03,800
day where it was like I was some parliament I think in East Europe or

198
00:16:03,800 --> 00:16:08,120
something and there's this big guy I'm just like going at this politician he's

199
00:16:08,120 --> 00:16:12,000
like in his face he's like screaming he was like going everywhere and not a

200
00:16:12,000 --> 00:16:16,040
single punch was thrown no then no one took out a knife no one took out a gun

201
00:16:16,360 --> 00:16:21,440
and I was like and I was like wow the fact that we're so civilized and we're so

202
00:16:21,440 --> 00:16:26,440
aligned to each other that we can have something this barbaric happen and no

203
00:16:26,440 --> 00:16:31,640
one throws a punch is actually shocking this is very unusual even for humans if

204
00:16:31,640 --> 00:16:37,880
you go back 200 years punches and probably gunshots would flow so this is

205
00:16:37,880 --> 00:16:41,080
not to say that humans have some inherent special essence that we're good

206
00:16:41,400 --> 00:16:46,600
that we have solved goodness or any means what I'm saying is the way I like to think

207
00:16:46,600 --> 00:16:51,000
about is that coordination is a technology is a technology you can improve upon it is

208
00:16:51,000 --> 00:16:55,080
you can develop new methods of coordination you can develop new structures new institutions

209
00:16:55,080 --> 00:16:58,840
new systems and I think it's very tempting for us living in this modern world to it's

210
00:16:58,840 --> 00:17:02,360
kind of like a fish and water effect we forget how much of our life you know a lot of our

211
00:17:02,360 --> 00:17:07,480
life is built on you know atoms on you know physical technology a lot of it's built on

212
00:17:07,480 --> 00:17:14,040
digital technology but a lot of it has been on social technology and when I look at how

213
00:17:14,040 --> 00:17:17,800
you know how does the world go well like you know should it be only the you know special

214
00:17:17,800 --> 00:17:22,680
elites get control of the AI I'm like well that's not really how I think about it when I think

215
00:17:22,680 --> 00:17:27,080
about it way more is what is the coordination mechanism or we can create a coordination

216
00:17:27,080 --> 00:17:32,760
selling point or we can create a group an institution a system of some kind that where

217
00:17:32,760 --> 00:17:38,040
people will you know have game theoretic incentives to cooperate on the system that results in

218
00:17:38,040 --> 00:17:43,480
something that is net positive for everyone because the truth is that positive some games do exist

219
00:17:43,480 --> 00:17:49,080
and they're actually very profitable and they're very good and I think if we can turn you know

220
00:17:49,080 --> 00:17:53,160
you can turn any positive some game to a net into a zero or a negative some game pretty easily

221
00:17:53,160 --> 00:17:58,040
it's much easier to destroy than is to create but I think it's absolutely possible to create

222
00:17:58,040 --> 00:18:02,760
coordination technology around AI and to build coordination mechanisms that are net positive

223
00:18:02,760 --> 00:18:08,440
for everyone involved so those would be like my two points happy to dig into any ones you

224
00:18:08,440 --> 00:18:12,280
you think would be it'll lead to an interesting interaction. Sure so I'll start with two and

225
00:18:12,280 --> 00:18:18,120
then go to one so two I moved to Berkeley in 2014 and I threw myself at the merry calls

226
00:18:18,920 --> 00:18:22,200
I showed up at the merry office and I'm like hi I'm here to join your calls

227
00:18:23,160 --> 00:18:28,360
and what I started to realize was

228
00:18:30,600 --> 00:18:38,280
merry and less wrong in general have a very poor grip on the practicalities of politics

229
00:18:39,080 --> 00:18:46,760
very much I think there was sort of a split you know uh Curtis Yargon like meal reaction

230
00:18:46,760 --> 00:18:51,000
this is a spin-off of rationality and it's a spin-off of rationality that understood the

231
00:18:51,000 --> 00:18:55,880
truth about human nature so when I give you that you give that example of two chimps meeting in

232
00:18:55,880 --> 00:19:01,800
the woods and they're going to fight if I'm one of those chimps at least I stand a chance right

233
00:19:01,800 --> 00:19:09,400
he might beat my ass I might beat his but if I come up against the FBI things do not look good

234
00:19:09,400 --> 00:19:14,440
for me in fact things so much do not look good for me there's no way I'm going to beat the FBI

235
00:19:14,440 --> 00:19:22,680
the modern forces are so powerful that this is not a oh we've established a nice cooperative

236
00:19:22,680 --> 00:19:28,120
shelling point this is a we have pounded so much fear into these people that they would never even

237
00:19:28,120 --> 00:19:35,000
think of throwing a puncher firing a gun we have made everybody terrified and this isn't good

238
00:19:35,640 --> 00:19:40,120
we didn't we didn't achieve this through some enlightened cooperation we achieved this through

239
00:19:40,120 --> 00:19:46,760
a massive propaganda effort right it's the joke about you know the american soldier goes over to

240
00:19:46,760 --> 00:19:52,280
russia and it's like uh man you guys got some real propaganda here and that the russian soldier is like

241
00:19:53,000 --> 00:19:56,840
yeah no i know it's bad but it's not as bad as yours and the american soldier is like

242
00:19:57,400 --> 00:20:03,640
what propaganda and the russian just laughs right so so this this didn't occur because of um this

243
00:20:03,640 --> 00:20:11,800
occurred because of a absolute tyrannical force decided to dominate everybody right um now oh i

244
00:20:11,800 --> 00:20:15,880
think so uh i think there's a way out of this i think there actually is a way out of this right

245
00:20:15,880 --> 00:20:20,200
and i wrote a blog post about this called individual sovereignty and i think a really

246
00:20:20,200 --> 00:20:26,360
nice world would be if all the stuff to live food water health care electricity were

247
00:20:26,360 --> 00:20:30,680
generatable off the grid in a way that you are individually sovereign this comes back to my

248
00:20:30,680 --> 00:20:36,520
point about offense and defense right if i have a world where you don't want it to be extreme

249
00:20:36,520 --> 00:20:41,640
defense you don't want every person to be able to completely insulate them but you want like okay

250
00:20:41,640 --> 00:20:46,360
it takes a whole bunch of other people to gang up to take that guy out right like that's that's a good

251
00:20:46,360 --> 00:20:52,760
that's a good balance um and the balance that we live in today is there is one pretty much a

252
00:20:52,760 --> 00:20:57,880
unipolar world i mean thank god for china but um you know there's one there's one unipolar world

253
00:20:57,880 --> 00:21:02,280
you got the america and what are you gonna run i'll pay taxes i don't care if they move overseas

254
00:21:02,280 --> 00:21:08,520
right um so yeah my point about the coordination is that if you're okay with solving coordination

255
00:21:08,520 --> 00:21:15,320
problems by using a single a singleton super intelligent ai to to to make everybody cower in

256
00:21:15,320 --> 00:21:21,560
fear and tyrannize the future sure you'll get coordination yeah that works that works i'm the

257
00:21:21,560 --> 00:21:25,240
only guy with a gun and i got ten and i got a name that all ten of you and you can all die

258
00:21:25,240 --> 00:21:29,800
or listen to me your choice george just quickly can you pump your volume just a tiny little bit

259
00:21:29,800 --> 00:21:40,520
if you can sure um is that better okay uh do you mind if i jump in there yeah sure so i'm i'm curious

260
00:21:40,520 --> 00:21:45,720
about uh so i understand what you're saying and i think you make some decent points but um i think

261
00:21:45,720 --> 00:21:50,040
i view the world a bit differently from you and i'd like to like dig into that a little bit so like

262
00:21:50,600 --> 00:21:56,040
who do you think is less afraid someone living just medium person living in the united states of

263
00:21:56,040 --> 00:22:03,000
america or the medium person living in somalia sure america less afraid well that's kind of

264
00:22:03,000 --> 00:22:07,720
strange somalia doesn't have a government they have much less tyranny you're much more you can just

265
00:22:07,720 --> 00:22:10,760
buy a rocket launch room just like live on a farm and just like you know kill your neighbors and no

266
00:22:10,760 --> 00:22:16,440
one's gonna stop you so like how does that interact with your old you those who will trade liberty

267
00:22:16,440 --> 00:22:24,440
for safety deserve neither did i sorry i don't understand could you elaborate a bit more um in

268
00:22:24,440 --> 00:22:32,360
somalia you have a chance in america you do not right i am okay i would rather live in fear i would

269
00:22:32,360 --> 00:22:37,640
rather be worried about someone shooting a rocket launcher at me than to have an absolutely tyrannical

270
00:22:37,640 --> 00:22:45,560
government just you know just just like like a managerial class and i'm not saying by the way

271
00:22:45,560 --> 00:22:50,040
i agree with you that these things are possible i agree with you that the less wrong notion of

272
00:22:50,040 --> 00:22:55,640
politics is possible i would love to live in these sort of worlds but we don't what would be the the

273
00:22:55,640 --> 00:23:01,560
practical reality of politics is so much more brutal and it just comes from a straight up

274
00:23:02,280 --> 00:23:07,560
instinct to dominate not an instinct uh you know government by the people for the people

275
00:23:07,560 --> 00:23:14,200
is branding i mean yeah so to be clear i very much do not agree with less wrongs views and politics

276
00:23:14,200 --> 00:23:20,760
and a bit of an outcast for how i view how conflict theory i view politics but this is i feel like

277
00:23:20,760 --> 00:23:24,280
you're kind of dodging the question here just a little bit is like well if that's true why aren't

278
00:23:24,280 --> 00:23:32,440
you living in somalia i know people who've done it right it's very hard it's very hard psychologically

279
00:23:33,080 --> 00:23:39,480
okay so like tigers love chum it turns out right a tiger does not want to chase down an antelope

280
00:23:39,480 --> 00:23:44,440
right a tiger would love to just sit in the zoo and eat the chum right and like it takes a very

281
00:23:44,440 --> 00:23:49,640
strong tiger to reject that i'm not that strong i hope there's people out there who are i hope

282
00:23:49,640 --> 00:23:55,640
there's people out there who are actually like you know i'm just not a little bitch that's why

283
00:23:55,640 --> 00:24:02,360
uh in somalia right okay i mean that's a fair answer but i am a bit confused here so you're

284
00:24:02,360 --> 00:24:08,360
saying that living in somalia would be better by some metric but you're also saying you prefer

285
00:24:08,360 --> 00:24:14,600
not living in somalia so i am a bit confused because like from my perspective i want to live in

286
00:24:14,600 --> 00:24:18,600
the country i want to live in and that's the one which i think is better if i thought another

287
00:24:18,600 --> 00:24:24,200
country was better then i would just move there but first let's the tiger and the chum i think is

288
00:24:24,200 --> 00:24:29,560
a good analogy right like if you have a choice as a tiger you can live in a zoo and you get a nice

289
00:24:29,560 --> 00:24:35,080
sized pen you know the zookeepers are not abusive at all you get fed this beautiful chopped up food

290
00:24:35,080 --> 00:24:42,360
it's super easy you sit there get fat lays around all day or you can go to the wild and in the wild

291
00:24:42,360 --> 00:24:47,720
you're gonna have to hunt you might not succeed at hunting it is just a you know it's a brutal

292
00:24:47,720 --> 00:24:54,280
existence as a tiger which one do you choose now you say oh well obviously you know you're going

293
00:24:54,280 --> 00:24:59,640
to choose the chum one yeah but do you see what you're giving up do you know i don't could you

294
00:24:59,720 --> 00:25:06,920
elaborate a little bit on what i'm giving up you are giving up on the nature of tiger you

295
00:25:06,920 --> 00:25:13,240
you are effectively okay um maybe i'll take this to an extreme right in the absolute extreme

296
00:25:13,800 --> 00:25:18,920
the country that you would most rather live in is the one that basically wire heads you right

297
00:25:19,480 --> 00:25:24,120
the one and you can say that okay well i don't want to be wireheaded but you know there's a

298
00:25:24,120 --> 00:25:30,520
there's a gradient that'll get you there gondi in the pill you know um look you can live in this

299
00:25:30,520 --> 00:25:35,720
country you can be happy feel safe and secure all the time don't worry exactly about how we're doing

300
00:25:35,720 --> 00:25:43,640
yet you know but right i mean it takes a very strong person to it's going to take a very strong

301
00:25:43,640 --> 00:25:50,040
person to say no to wireheading so now i understand oh sorry i give i'll give one more instrumental

302
00:25:50,040 --> 00:25:54,520
reason for living in america versus living in simalia if i thought that america and simalia were

303
00:25:54,520 --> 00:26:00,520
both like steady states i might choose simalia i don't think that i think that being here i have a

304
00:26:00,520 --> 00:26:07,160
much better way of escaping this of escaping the constant tyranny that we're in and i think a major

305
00:26:07,160 --> 00:26:15,000
way to do it is ai i think that ai is is if i really if i had an agi if i had an agi in my

306
00:26:15,000 --> 00:26:18,840
closet right now i'll tell you what i'd do with it i would have it build me a spaceship that could

307
00:26:18,840 --> 00:26:23,480
get me off of this planet and get out of here as close to the speed of light as i possibly could

308
00:26:23,480 --> 00:26:27,800
and put big shield up behind me blocking all communication that's what i would do if i had

309
00:26:27,800 --> 00:26:32,280
an agi and i think that's you know the right move and i have a lot better chance of building that

310
00:26:32,280 --> 00:26:37,640
spaceship right here than i do in simalia right so i'll give an if that's right that's a good

311
00:26:37,640 --> 00:26:42,840
instrumental well we'll miss you if you leave though no you're real ashamed it'll be everyone

312
00:26:42,840 --> 00:26:48,680
should do it like this is this is the move right and like let humanity blow i mean look i agree with

313
00:26:48,680 --> 00:26:55,080
you that we're gonna probably blow ourselves up right but i think that the path potentially through

314
00:26:55,080 --> 00:27:01,240
this probably looks different from the path you're imagining i think that the reasonable position

315
00:27:01,240 --> 00:27:06,680
sorry oh no no um i think yeah maybe we're done with this point i can come back i have a response

316
00:27:06,680 --> 00:27:11,800
to you first i i would like to if you don't mind just like pull on one one string there as well

317
00:27:11,800 --> 00:27:18,040
so one of the things you said is like what will the tiger choose and so my personal view of this

318
00:27:18,040 --> 00:27:23,640
kind of thing and i think i want to think about coordination is i think of things so you put a

319
00:27:23,640 --> 00:27:28,360
lot of view and there's like fear-based domination and so on and i'm not going to deny that this

320
00:27:28,360 --> 00:27:35,080
isn't a thing that happens i'm german you know like you know i have living relatives who can tell

321
00:27:35,080 --> 00:27:41,000
you some stories like i understand like i i understand i'm not i'm not denying these things by

322
00:27:41,240 --> 00:27:48,680
means um what i'm saying though is okay let's say there was a bunch of tigers you know you and me

323
00:27:48,680 --> 00:27:54,520
and all the other tigers and some of the tigers are like man fuck this whole like nature shit is

324
00:27:54,520 --> 00:28:00,600
like really not working for me how about we go build a zoo together who's in and then other people

325
00:28:00,600 --> 00:28:04,440
like yeah you know what actually that sounds awesome let's do that do you think that's okay

326
00:28:04,440 --> 00:28:08,840
like you think that would be like a fair option for them to do sure but that's not where zoos come

327
00:28:08,840 --> 00:28:13,800
from i i know i know i'm getting there i'm getting there so like that is not where zoos come from

328
00:28:13,800 --> 00:28:22,120
sure but the this analogy here is of course is that this is where a lot of human civilization

329
00:28:22,120 --> 00:28:26,600
not all of it i understand that why france was doing well in the first world war was not because

330
00:28:26,600 --> 00:28:32,760
of democracy big nice it was because democracy raises large armies it's um i'm very well aware

331
00:28:32,760 --> 00:28:39,160
of the real politic as the Germans would say about these kinds of factors and i and i fully

332
00:28:39,160 --> 00:28:44,840
agree with you that a lot of the good things that we have are not by design so to speak you know

333
00:28:44,840 --> 00:28:51,400
there are happy side effects you know capitalism is a credit assignment mechanism you know the fact

334
00:28:51,400 --> 00:28:56,440
that also results in us having cool video games and air conditioning it's not an inherent feature

335
00:28:56,440 --> 00:29:05,160
of the system it's it's an execution mechanism and so totally grant all of this i'm not saying that

336
00:29:05,160 --> 00:29:10,360
every coordination thing is good i'm not saying that you know there aren't trade-offs especially

337
00:29:10,360 --> 00:29:13,800
you were talking about i think aesthetic trade-offs you're like there's an aesthetic that the tiger

338
00:29:13,800 --> 00:29:18,600
loses they're living in a zoo and well i think personally aesthetics are subjective so i think

339
00:29:18,600 --> 00:29:22,200
this is something that different people so the way i think about aesthetics is i think aesthetics

340
00:29:22,200 --> 00:29:29,000
are things you trade on is you know you might want tigers in the wild to exist okay fair enough

341
00:29:29,000 --> 00:29:34,280
that's a thing you can want you know someone else might want you know certain kinds of art to exist

342
00:29:34,280 --> 00:29:39,960
they might want um a certain kind of religion to be practiced or whatever these are aesthetic

343
00:29:39,960 --> 00:29:44,920
preferences upon reality which i think are very fair so the way i personally think about this

344
00:29:44,920 --> 00:29:50,120
morally is i'm like okay cool how can we maximize trade surplus so you can spend your resources

345
00:29:50,120 --> 00:29:55,640
on the aesthetics you have you want and i'll spend my resources on the uh you know things i want

346
00:29:56,440 --> 00:30:01,320
now maybe the thing you describe where everyone just atomizes into their own systems with their

347
00:30:01,320 --> 00:30:06,600
own value system with their own aesthetics completely separate further is the best outcome possible

348
00:30:06,600 --> 00:30:12,600
i think this is probably sorry have you heard the you know about my manifesto i have not you

349
00:30:12,600 --> 00:30:19,080
should um the problem with this everyone trades on their own aesthetics is you will never be able

350
00:30:19,080 --> 00:30:25,240
to actually buy any aesthetics that are in conflict with the system right the you won't

351
00:30:25,800 --> 00:30:32,040
the system will let you okay uh by by that logic why do people have free time

352
00:30:33,400 --> 00:30:37,160
why don't they work all the time why doesn't capitalism extract literally every minute of them

353
00:30:37,160 --> 00:30:43,320
why do you think that is i think it's because it turns out that we don't actually live in a

354
00:30:43,320 --> 00:30:48,600
capitalist society i think china is a lot closer to a capitalist society than america i think america

355
00:30:48,600 --> 00:30:52,920
is kind of communist and i think in a communist society of course you're going to get free time

356
00:30:52,920 --> 00:30:57,480
it turns out that subsidizing all the homeless people is a great idea right if you want to keep

357
00:30:57,480 --> 00:31:03,160
power again through some absolute tyrannical mechanism you do it right so why do we have free

358
00:31:03,160 --> 00:31:08,520
time well you think it's some victory of capitalism i think it's because we do not live in a capitalist

359
00:31:08,520 --> 00:31:12,920
country i think china is more capitalist than america i think it's because we trade on our aesthetics

360
00:31:12,920 --> 00:31:16,280
i think that different people have different things to contribute to various systems not

361
00:31:16,280 --> 00:31:21,160
necessarily capitalist or communist thing i'm saying it's it's more energy is that in the in

362
00:31:21,160 --> 00:31:24,760
the primordial environment if you have to fight literally every single second instrument every

363
00:31:24,760 --> 00:31:29,400
jewel of energy you have to scrounge together another jewel of energy you can't have free time

364
00:31:29,400 --> 00:31:34,520
it's not about capitalism this is about entropy this is about these kind of things we have energy

365
00:31:34,520 --> 00:31:40,280
excess we have we've produced systems that allow us to extract more energy for a jewel we put in

366
00:31:40,360 --> 00:31:47,240
and we can spend that extra energy on things such as free time and the distribution of you know

367
00:31:47,240 --> 00:31:51,800
energy power coordination whatever you want to call it is a is another question will you agree

368
00:31:51,800 --> 00:31:56,760
to disagree with this i mean i am taking an extreme position when i say that there are definitely

369
00:31:56,760 --> 00:32:02,200
positive sum coordination problems that are solved by governments right it is not all zero

370
00:32:02,440 --> 00:32:09,960
or negative sum right i'm not i'm not denying this but what i'm saying is it's like i don't know

371
00:32:09,960 --> 00:32:16,200
man like the existence of free time well that's all great when you think you live in this surplus

372
00:32:16,200 --> 00:32:21,960
energy world right and maybe we do right now but if some other country took this seriously like china

373
00:32:23,160 --> 00:32:27,800
who's gonna win in a war who's gonna win is it gonna be the chinese you see the chinese

374
00:32:27,800 --> 00:32:33,400
build a building they got like 400 people there and they're all there 24 hours a day and they're

375
00:32:33,400 --> 00:32:37,320
getting the building built you ever see americans build a building it's six guys two of them are

376
00:32:37,320 --> 00:32:41,880
working two of them are shift supervisors and two of them are on lunch breaks oh you got your free

377
00:32:41,880 --> 00:32:46,200
time you got your aesthetic preferences you know you deserve to lose in a war right this country

378
00:32:46,200 --> 00:32:50,680
deserves to lose in a war if they keep acting the way they're acting so i i definitely see the

379
00:32:50,680 --> 00:32:54,920
point you're making and this is personally not a thing i want to defend too far because i'm not a

380
00:32:54,920 --> 00:33:01,560
military expert but i will know that i will note that the us has like 37 aircraft carriers and the

381
00:33:01,560 --> 00:33:07,880
chinese have like two and americans are like somehow you know despite being so lazy and oh no they have

382
00:33:07,880 --> 00:33:12,680
all this you know all this free time or whatever somehow they're still military hegemon or whatever

383
00:33:12,680 --> 00:33:17,880
and like they're the biggest rival russia fighting this backwards water country in ukraine suddenly

384
00:33:17,880 --> 00:33:24,280
folds and lose like two quarters of the military it's what i'm saying is if you have massive hegemony

385
00:33:24,360 --> 00:33:30,120
if you have truly a obnoxious victory the way it should look is that you laze around all the

386
00:33:30,120 --> 00:33:36,040
time and you look like a fucking idiot and you still win yes and i'm not talking about russia

387
00:33:36,040 --> 00:33:41,720
russia has a gdp the size of italy this is china here you might say that china has two aircraft

388
00:33:41,720 --> 00:33:47,800
carriers in the us has 37 why do we have aircraft carriers who has more drone building capacity the

389
00:33:47,800 --> 00:33:53,880
chinese or the united states if the future is fought with ai swarm drone warfare the chinese

390
00:33:53,880 --> 00:33:59,480
can make you know a million drones a day and the us can make i don't even know i think we buy them from

391
00:33:59,480 --> 00:34:06,120
china well i'm not an expert on these kind of logistics i think i would like to get back to

392
00:34:06,120 --> 00:34:10,440
kind of like the more general point let's move on from that yeah i am not either but i do believe

393
00:34:10,440 --> 00:34:15,000
that the chinese have more manufacturing capacity than the united states it seems completely plausible

394
00:34:15,000 --> 00:34:20,920
to me i think things are not lazy and they don't sit around and have all this free time and aesthetic

395
00:34:20,920 --> 00:34:26,280
references or something i don't believe it at work is life i mean at least from my chinese friends

396
00:34:26,280 --> 00:34:32,040
i know the chinese sure do have a lot of inefficiencies it's just called corruption oh america has

397
00:34:32,040 --> 00:34:37,960
corruption too oh yeah sure well in mexico the corruption is you have to pay 20 cents to get

398
00:34:37,960 --> 00:34:43,000
you know 20 cents on every dollar for the building you built right whatever man in america you every

399
00:34:43,000 --> 00:34:47,320
dollar is spent absolutely on that building you know how we know that because we spent four dollars

400
00:34:47,320 --> 00:34:52,680
making sure that that first dollar was not spent correctly uh i'm i'm i'm well aware of that so

401
00:34:52,680 --> 00:34:56,600
anyways i would like to like i think i think we mostly agree on this point actually and i think

402
00:34:56,600 --> 00:35:02,840
it's a matter of degree um i i what i want to say just for the record the us is a like uniquely

403
00:35:02,840 --> 00:35:08,200
dysfunctional system in the west i'm german and like the german system is very dysfunctional

404
00:35:08,200 --> 00:35:13,000
but it's like nothing compared to how dysfunctional the us is fully agreed with that i don't i don't

405
00:35:13,000 --> 00:35:18,840
think we disagree on that i think it's a matter of degree more so we've had a we've had a comment

406
00:35:18,840 --> 00:35:23,000
saying someone's turned the temperature up a bit too much on the language model so let's bring it

407
00:35:23,000 --> 00:35:27,720
back a tiny bit to ai safety but that was a that was a great discussion got it yeah i will end

408
00:35:27,720 --> 00:35:31,640
with saying i love america i am happy to live here and there are a lot of things i appreciate about

409
00:35:31,640 --> 00:35:38,840
american societies great so do you want to return to like the technical topics or would you i think

410
00:35:38,840 --> 00:35:42,440
i can return to your first point and maybe i'll just start with a question uh do you think there's

411
00:35:42,440 --> 00:35:51,000
going to be a hard takeoff i don't know but i can't rule it out i can't see how that would possibly

412
00:35:51,000 --> 00:35:58,360
happen i have a few ideas of how it could happen but i don't it's like unlikely it seems like not

413
00:35:58,360 --> 00:36:03,720
the the way i think it could happen is if there are just algorithms which are like

414
00:36:05,160 --> 00:36:09,320
magnitudes of order better than anything we ever have and like the actual amount of computing to

415
00:36:09,320 --> 00:36:15,800
get human is like you know a cell phone or you know like and then this algorithm is not deep in the

416
00:36:15,800 --> 00:36:20,760
tech tree we just happen to have not picked it up and then an agi system picks it up this is how

417
00:36:20,760 --> 00:36:26,280
i think it could happen okay yes i agree that something like this is potentially plausible

418
00:36:26,280 --> 00:36:32,440
where you're saying basically like the godshatter is already distributed the uh the the it's it's

419
00:36:32,440 --> 00:36:37,240
not a question it's using all the existing compute in the world today it just turns out it was 10 000x

420
00:36:37,240 --> 00:36:41,960
more effective or a million x more effective than we thought yeah this is seems the most plausible

421
00:36:41,960 --> 00:36:46,840
way to be or you know you mix lead and you know copper and you get a superconductor you know something

422
00:36:46,840 --> 00:36:52,440
like that i don't know some crazy i know i know i'm i'm joking it's going to take so many years to

423
00:36:52,440 --> 00:36:57,320
like it's not about the discovery right give it 10 years to productionize it scale up processes

424
00:36:57,320 --> 00:37:01,480
right like these things are you know this is something running a company's really taught me like

425
00:37:01,480 --> 00:37:08,360
it's just gonna take a long time and this is really like like kind of where my i just don't

426
00:37:08,360 --> 00:37:13,560
believe in a hard takeoff i think that they'll be this is a gasky thing i like he's a hardware and

427
00:37:13,560 --> 00:37:17,640
software progressive quite similar speeds and you can look at factoring algorithms to show this

428
00:37:17,640 --> 00:37:23,000
so it would shock me if there were some you know 10 to the 6 10 to the 9 magical improvement to be

429
00:37:23,000 --> 00:37:29,720
had it seems plausible to be like a hard takeoff is definitely not my main line scenario my main

430
00:37:29,800 --> 00:37:34,280
line scenario well i don't know maybe you wouldn't consider this a part maybe you would

431
00:37:34,280 --> 00:37:37,400
consider as a hard takeoff this is what i would describe as a soft takeoff is something like

432
00:37:38,120 --> 00:37:44,120
sometimes the way i like to define agi is say it's uh something that has the thing that chimp

433
00:37:44,120 --> 00:37:50,760
that chimps don't have and humans do have yeah so chimps don't go a third to the move you know

434
00:37:50,760 --> 00:37:55,640
despite their brain being a third of our size so we scaled up things by a factor of three if a

435
00:37:55,640 --> 00:38:00,040
primate brain roughly four something like that and like most of the structures i'm sure some

436
00:38:00,040 --> 00:38:04,600
micro tweaks and whatever but like not massive amount of evolutionary pressure like we're very

437
00:38:04,600 --> 00:38:11,560
very similar to chimps and somehow this got us from you know literally no technology to

438
00:38:11,560 --> 00:38:17,800
space travel in a you know evolutionary very small pair of time it seems imaginable to me

439
00:38:17,800 --> 00:38:24,280
that something similar could happen with ai i'm not saying it will but like it seems imaginable

440
00:38:24,360 --> 00:38:30,920
yes so i agree with this um i'll come to your point about uh you know you had two regulatory

441
00:38:30,920 --> 00:38:37,960
points one of them about uh capping the max flops and i actually kind of agree with this

442
00:38:37,960 --> 00:38:44,440
i do think that things could potentially become very dangerous at some point i think your numbers

443
00:38:44,440 --> 00:38:50,840
are way way way too low i think if your numbers are anywhere in your gpk3 gpk4 okay great we got

444
00:38:50,840 --> 00:38:55,800
a lot up we got a lot of fast moving guys who work on fiber even if you start to get von neumann's

445
00:38:56,440 --> 00:39:02,680
right we're not talking about a humanities worth of compute we're talking about things on par with

446
00:39:02,680 --> 00:39:09,080
a human and a few humans right yeah they'll run fast but they're not like things get scary when you

447
00:39:09,080 --> 00:39:14,600
could do a humanities training run in 24 hours like we're about to burn the same compute that

448
00:39:14,920 --> 00:39:21,320
all 2 million years of human civilization burned okay now i don't know what starts to happen or

449
00:39:21,320 --> 00:39:26,520
i'll put this kind of another way language models i look at them and they don't scare me at all

450
00:39:26,520 --> 00:39:33,320
because they're trained on human training data right these things are not like if something was a

451
00:39:33,320 --> 00:39:40,120
good as as good as gpk4 that looked like mu0 where it trained from some simple rules okay now i'm a

452
00:39:40,120 --> 00:39:44,760
bit more scared but when you say okay it's you know we're feeding the whole internet into the

453
00:39:44,760 --> 00:39:48,280
thing and it parents the internet back mushed around a little bit that looks very much like

454
00:39:48,280 --> 00:39:53,800
what a human does and i'm just not scared of that yeah i think it's very reasonable whatever like i'm

455
00:39:53,800 --> 00:39:59,480
not scared of gpk4 to be clear like i think there is like zero percent chance or like you know epsilon

456
00:39:59,480 --> 00:40:06,680
chance that gpk4 is existentially dangerous by itself you know maybe some crazy gpk4 plus rl

457
00:40:06,680 --> 00:40:12,280
plus mu0 plus something something maybe but i definitely agree with you here i don't expect

458
00:40:12,280 --> 00:40:16,120
gpk3 or 4 by themselves to be dangerous these are not i'm much closer to i think what you're

459
00:40:16,120 --> 00:40:22,040
saying like yeah if you had a mu0 system they'd bootstrap yourself gpk4 holy shit like we're

460
00:40:22,040 --> 00:40:26,360
big we're big big shit if we get to that then we should let's stop let's stop yeah let's let's stop

461
00:40:26,360 --> 00:40:32,120
so so i'm very happy to get to be into a regime where we're like okay let's find the right bound

462
00:40:32,120 --> 00:40:35,640
like i think this is an actually good argument i think this is actually something that should

463
00:40:35,640 --> 00:40:39,880
be discussed and which is not obvious and i could be super wrong about that so i'd like to

464
00:40:39,880 --> 00:40:44,760
justify a little bit about why i put such a small bound but i think the arguments you're

465
00:40:44,760 --> 00:40:48,280
making for the higher bounds are very reasonable actually i think these are actually good arguments

466
00:40:48,280 --> 00:40:54,040
so just to justify a little bit about why i put such a low bound the boring default answer is

467
00:40:54,040 --> 00:40:58,760
conservatism is like if all of humanity is at stake which you know you may not believe

468
00:40:58,760 --> 00:41:05,400
i'm like whoa whoa okay at least give us a few years to like more understand what we're dealing

469
00:41:05,400 --> 00:41:11,480
with here like i understand that you know you may disagree with this very plausible but i'm like

470
00:41:12,040 --> 00:41:17,000
whoa like you know at least let's let's like by default let's hit a pause button for like you

471
00:41:17,000 --> 00:41:21,400
know a couple years until we figure things out more and then if we like find a better theory of

472
00:41:21,400 --> 00:41:26,440
scaling we understand how intelligent scales we understand how mu0 comes blah blah blah and then

473
00:41:26,440 --> 00:41:31,000
we pick back up after we're like you know we make huge breakthroughs in alignment and

474
00:41:31,000 --> 00:41:37,080
eliezer is is crying on cnn and like oh we did it boys i mean then okay sure you know okay um

475
00:41:38,200 --> 00:41:42,280
so that's the one like kind of more boring argument like that's kind of a boring argument

476
00:41:43,080 --> 00:41:48,040
the more interesting argument i think which i think is a bit you know or schizo um is that

477
00:41:49,480 --> 00:41:53,640
it's not clear to me that you can't get dangerous levels of intelligence with the

478
00:41:53,640 --> 00:41:58,120
amount of compute we have now and one of the reasons that i'm i'm unsure about this is because

479
00:41:58,120 --> 00:42:03,960
man gpd 3 gp4 is just the dumbest possible way to build ai like it's just like like there's like

480
00:42:03,960 --> 00:42:09,560
no dumber way to do it like it's it works and dumb is good right you know bitter lesson dumb is good

481
00:42:09,560 --> 00:42:16,280
but look at humans you said as we talked about before you know human today human 10 000 years

482
00:42:16,280 --> 00:42:23,480
ago not that different you place both of them into a you know workshop with tools to build you know

483
00:42:23,480 --> 00:42:28,600
any weapon of their choice which of them is more dangerous obviously you know one of them

484
00:42:28,600 --> 00:42:36,680
will have much better you know capacities to deal with tools to read books to think about

485
00:42:36,680 --> 00:42:43,080
how to design new weaponry and so on these are not genetic changes they are epistemological

486
00:42:43,080 --> 00:42:48,440
changes they are memetic they are software updates you know humans had to discover rational

487
00:42:48,440 --> 00:42:52,520
reasoning like you know before like you know i mean no obviously people always had like you

488
00:42:52,520 --> 00:42:58,920
know folk conceptions of rationality but it wasn't like a common thing to think about causality and

489
00:42:58,920 --> 00:43:04,920
like you know you know rational like you know if then else kind of stuff until relative you know

490
00:43:04,920 --> 00:43:10,520
like philosophers in the old ages and only became widespread relatively recently and these are useful

491
00:43:10,520 --> 00:43:15,640
capabilities that turned out to be very powerful and took humans many many thousands of years to

492
00:43:15,640 --> 00:43:20,040
develop and distribute at scale and i don't think humans are anywhere near the level i think the way

493
00:43:20,040 --> 00:43:24,680
we could do science right now is pretty awful like it's like the dumbest way to do science that like

494
00:43:24,680 --> 00:43:32,520
kind of still works like you know and i expect it's like possible but if you had a system which like

495
00:43:32,520 --> 00:43:38,600
let's say it's like smaller brain than the human even but it has really really sophisticated

496
00:43:38,600 --> 00:43:44,360
epistemology it has really really sophisticated theories of meta science and it never tires it

497
00:43:44,360 --> 00:43:49,960
never gets bored it never gets upset it never gets distracted and it can like memorize arbitrary

498
00:43:49,960 --> 00:43:55,400
amounts of data this is something that i think is within the realm of like a gbt3 or four training

499
00:43:55,400 --> 00:44:00,920
run to build something like this and it is not obvious to me that this system could not outflank

500
00:44:00,920 --> 00:44:07,000
humanity maybe not like maybe not but it's not obvious to me that it can't so just curious what

501
00:44:07,000 --> 00:44:13,560
you think of that um so to your first point uh why i stand against almost all conservative

502
00:44:13,560 --> 00:44:18,920
arguments you're assuming the baseline is no risk right and oh well why should we do this

503
00:44:18,920 --> 00:44:25,720
say i we should wait and bring the baseline back no no no no we are about to blow the world up any

504
00:44:25,720 --> 00:44:31,880
minute there's enough nuclear weapons aimed at everything this is wearing some incredibly unstable

505
00:44:31,880 --> 00:44:36,520
precarious position right now like people talk about this with with car accidents you know this is

506
00:44:36,520 --> 00:44:41,800
comma like people are like oh well you know if your device causes even one accident i'm like

507
00:44:41,800 --> 00:44:46,440
yeah but what if statistically there would have been five without the device i'm like you

508
00:44:46,440 --> 00:44:51,400
you do have to understand the baseline risk in cars is super high you make it 5x safer there's

509
00:44:51,400 --> 00:44:55,800
one accident you don't like that okay i mean you have to be excluded from any polite conversation

510
00:44:55,800 --> 00:45:04,200
right um right so yeah like i i think that calling for a pause to the technology is is uh

511
00:45:05,400 --> 00:45:09,720
worse right i think given the two options if we should pause or we should not pause

512
00:45:09,720 --> 00:45:14,200
i think pausing actually prevent presents more risk and i can talk about some reasons why

513
00:45:14,200 --> 00:45:19,000
again the things that i'm worried about are not quite the existential risks i have to the

514
00:45:19,000 --> 00:45:28,360
species are not agi goes rogue they are uh government gets control of agi and ends up in

515
00:45:28,360 --> 00:45:34,040
some really bad place where nobody can compete with them i don't think these things look unhuman

516
00:45:34,040 --> 00:45:38,680
these things to me like i see very little distinction between human intelligence and

517
00:45:38,680 --> 00:45:46,920
machine intelligence it's all just on a spectrum and like they're not um like to come to the point

518
00:45:46,920 --> 00:45:53,000
about okay but gpt4 could be like this hyper-rational never tiring humans are doing science in the

519
00:45:53,000 --> 00:45:59,880
dumbest way i'm not sure about that right like i i think that you know when you look at like okay

520
00:45:59,880 --> 00:46:04,040
well okay we have chess bots that do way better and all they do is think about chess we haven't

521
00:46:04,040 --> 00:46:07,720
really done this with humans people would call it unethical right like if we really

522
00:46:08,280 --> 00:46:12,920
told a kid like if we really just like every night we're just putting the chess goggles on you

523
00:46:12,920 --> 00:46:16,840
and you're staring at chess boards and we're really just training your neural net to play chess

524
00:46:18,200 --> 00:46:23,400
i think humans could actually beat a computer again a chess if we were willing to do that um

525
00:46:24,280 --> 00:46:29,160
so yeah i don't think that this stuff is that particularly dumb and i think okay maybe we're

526
00:46:29,160 --> 00:46:35,880
losing 10x but we're not losing a million x again i i don't see a i do the numbers out

527
00:46:35,880 --> 00:46:41,160
all the time for when we're going to start to get more computer you know when will a computer

528
00:46:41,160 --> 00:46:45,960
have more compute than a human when will a computer uh have more compute than humanity

529
00:46:45,960 --> 00:46:51,000
and yes these things get scary but we're nowhere near scary yet we're looking at these cute little

530
00:46:51,000 --> 00:47:00,200
things and these things by the way do present huge dangers to society right the the psyops that are

531
00:47:00,200 --> 00:47:06,760
coming right now you assume that like when you call somebody that you're at least wasting their time

532
00:47:06,760 --> 00:47:13,080
too but we're going to get like heaven banning i love this concept which is yeah yeah came up on

533
00:47:13,080 --> 00:47:16,680
luther a i like that's where it comes from was the guy on the luther a i that came up with that

534
00:47:16,680 --> 00:47:23,960
word yeah yeah i know the guy came over this i love i love this concept and i think uh there's

535
00:47:23,960 --> 00:47:30,520
also a story i did uh my little pony friendship is optimal oh god that goes into the concept and

536
00:47:31,640 --> 00:47:40,120
yeah so i think that like um my girlfriend proposed a uh i don't want to talk to oh i

537
00:47:41,000 --> 00:47:47,640
say you don't want to talk to your relative anymore right okay we'll give an ai version to talk to you

538
00:47:47,640 --> 00:47:54,280
right yeah yeah yeah yeah so like this stuff is coming and it's coming soon and if you try to

539
00:47:54,280 --> 00:47:59,720
centralize this if you try to you know say like oh okay google open ai great they're not aligned with

540
00:47:59,720 --> 00:48:03,240
you they're really not google has proven time and time again they're not aligned with you

541
00:48:03,240 --> 00:48:06,120
meta has proven time i can't time again they're trying to fix it but you know

542
00:48:06,840 --> 00:48:12,120
yeah i mean i i fully agree with you like uh i like that you bring up psyops as the correct example

543
00:48:12,120 --> 00:48:17,400
in my opinion of short-term risks i think you're like fully correct about this like when i first

544
00:48:17,400 --> 00:48:24,840
saw like gpt models i was like holy shit like the level of control i can gain over social reality

545
00:48:24,840 --> 00:48:29,960
using these tools at scale is insane and i'm surprised that we haven't seen yet the things

546
00:48:30,680 --> 00:48:37,160
that like augured in my visions of the day and we will like we will obviously it's coming and

547
00:48:37,960 --> 00:48:44,040
this is so i think this is a very very real problem yeah like i think if we even if we stop now

548
00:48:44,040 --> 00:48:49,560
we're not out of the forest so like so um when you say like i i think the risk is zero please

549
00:48:49,560 --> 00:48:54,120
do not believe that that is what i believe because it is truly not it is truly truly not i think we

550
00:48:54,120 --> 00:49:00,600
are like we are really in a bad situation we are in a we're being we're an under attack from like

551
00:49:00,600 --> 00:49:05,320
so many angles right now you know this is before we get into you know like you know potential like

552
00:49:05,320 --> 00:49:11,560
you know climate risks nuclear risk whatever we're in under room medic risk like the the then dangers

553
00:49:11,560 --> 00:49:17,960
of our like epistemic foundations are under attack and this is something we can't adapt to right

554
00:49:17,960 --> 00:49:24,440
like you know we did you know when a good friend of mine he's uh he's quite well read on like

555
00:49:24,440 --> 00:49:28,600
chinese history and he always like it tells me his great stories so i'm not a historian so please

556
00:49:28,600 --> 00:49:33,080
you know don't crucify me here but like he tells his great stories about when marxist memes were

557
00:49:33,080 --> 00:49:37,240
first introduced to china and like this is where a world were like just like all the precursor

558
00:49:37,240 --> 00:49:41,960
memes didn't exist that's just like kind of was air dropped in and people went nuts people went

559
00:49:41,960 --> 00:49:46,840
just completely crazy because there was no memetic antibodies to these like hyper virulent

560
00:49:46,920 --> 00:49:51,080
memes that were you know created by evolutionary pressures in like you know western university

561
00:49:51,080 --> 00:49:55,960
departments like really you can call philosophy department just gain a function memetic laboratories

562
00:49:58,200 --> 00:50:03,960
yeah that's what they are i mean like you know like without being you know political or any means

563
00:50:03,960 --> 00:50:08,200
there a lot of what these organizations do and like you know other you know what other you know

564
00:50:08,200 --> 00:50:13,960
memetic like you know if philosophy departments are the like gain a function laboratories then

565
00:50:14,040 --> 00:50:20,200
like for chan and tumblr are like the bat caves of means you know like the chinese bat caves and

566
00:50:20,200 --> 00:50:25,080
i remember this vividly i was like on tumblr and for and for chan like when i was a teenager

567
00:50:25,080 --> 00:50:29,960
and then suddenly all the like weird bizarre you know internet shit i saw started becoming

568
00:50:29,960 --> 00:50:35,880
mainstream news my parents were watching in 2016 and i was like what the hell is going on

569
00:50:35,880 --> 00:50:40,680
like i already developed antibodies to this shit like i already you know both right and left i was

570
00:50:40,680 --> 00:50:46,040
already like i already immunized all this so i fully agree with you that this is like one of the

571
00:50:46,040 --> 00:50:51,560
largest risks that we are facing is this kind of like memetic mutation load in a sense and

572
00:50:52,280 --> 00:50:57,560
i'm not going to say i have a solution to this problem i'm like i have ideas like there's a lot

573
00:50:57,560 --> 00:51:02,280
of like things you can do to improve upon this like if ai was not a risk and also not climate

574
00:51:02,280 --> 00:51:06,840
change and whatever this might be something i work on like epistemic security this might be

575
00:51:06,840 --> 00:51:11,160
something i would work on like how can we build better coordination like rash like just scalable

576
00:51:11,160 --> 00:51:16,360
rationality mechanisms stuff like prediction markets and stuff like this i don't know but sorry

577
00:51:16,360 --> 00:51:22,280
going off track here a little bit but well no i actually i really agree with a lot of the stuff

578
00:51:22,280 --> 00:51:26,520
you said and i had a similar experience with the antibodies and people are exposed to this stuff and

579
00:51:26,520 --> 00:51:36,040
i'm like yeah this got me like four years ago yeah um so i think that there is a solution and i

580
00:51:36,040 --> 00:51:42,760
have a solution and the answer is open source ai the answer is open source let's even you can even

581
00:51:42,760 --> 00:51:46,760
dial it back from like the political and the terrible and just straight up talk about ads and spam

582
00:51:46,760 --> 00:51:51,880
or maybe spam just straight up spam i get so much spam right now and it's like it's kind of written

583
00:51:51,880 --> 00:51:57,320
by a person it's like targeting me to do something and google spam filter can't even come close to

584
00:51:57,320 --> 00:52:04,440
recognize you right like what i need is a smart ai that's watching out for me that is just it's not

585
00:52:04,440 --> 00:52:12,120
even targeted attacks at me it's just so much noise and i don't see a way to prevent this like

586
00:52:12,120 --> 00:52:16,680
the big organizations they're just going to feed you their noise right and they're going to maximally

587
00:52:16,680 --> 00:52:22,440
feed you their noise the only way is if you have an ai like i don't think alignment is a hard problem

588
00:52:22,440 --> 00:52:26,520
i think if you own the computer and you run the software if you develop the software the ai is

589
00:52:26,520 --> 00:52:34,280
aligned with you oh yeah can you can you okay if i challenge you george hotz here is a lama

590
00:52:34,280 --> 00:52:39,560
65b model when we could appear to run it on make it so it and make yeah you know sure you okay you

591
00:52:39,560 --> 00:52:44,920
developed it i give you the funding at your time can you develop a model that is as good as lama

592
00:52:44,920 --> 00:52:52,360
64b 65b and it's immune like completely immune to jail breaks it cannot be jailbroken no why not

593
00:52:52,360 --> 00:52:58,360
it's aligned isn't it well no but this isn't what alignment means well my values is do not get

594
00:52:58,360 --> 00:53:04,120
jailbroken oh okay you're talking about unexploitability this is not alignment right oh okay okay interesting

595
00:53:04,120 --> 00:53:08,920
i didn't know you would separate those so extremely separate those right okay interesting it means in

596
00:53:08,920 --> 00:53:17,320
the default case it like like it it's on my side right okay unexploitability is not a question of

597
00:53:17,320 --> 00:53:21,960
whether it's okay and this is a true thing about people too whenever i look at a person i ask

598
00:53:22,360 --> 00:53:28,520
okay is this person i want something for me is this person does this person want it too and

599
00:53:29,240 --> 00:53:33,800
is this person capable of doing it right and i really separate those two things i can build a

600
00:53:33,800 --> 00:53:37,480
system i don't i'm not worried about the first one with the ai system is worried about the second

601
00:53:37,480 --> 00:53:43,400
one can it be gamed can it be exploited sure i could tell like you know like like say it was just

602
00:53:43,400 --> 00:53:49,240
playing chess right and it loses i'm like don't lose okay i didn't want to man i didn't want to lose

603
00:53:49,240 --> 00:53:55,480
i'm sorry yeah i know but like so yes yes can i build a aligned system sure can i build an

604
00:53:55,480 --> 00:54:01,000
unexploitable system no especially not by a more powerful intelligence interesting interesting so

605
00:54:01,000 --> 00:54:04,600
this is an interesting i i think you're you're you're pointing to actually a very important

606
00:54:05,240 --> 00:54:09,480
part of this is that like exploitability and alignment can get fuzzy like which is which

607
00:54:09,480 --> 00:54:13,720
like did it fail because of its skill set or because it's not aligned it's actually a very

608
00:54:13,720 --> 00:54:17,240
deep question so i think i think you make a good point for like you know talking about these two

609
00:54:17,240 --> 00:54:25,960
separately i guess um the um so the thing i want to dig in just like a a little bit more on on this

610
00:54:25,960 --> 00:54:33,960
idea is there are there's two ways there are two portals through which you know the memetic demons

611
00:54:33,960 --> 00:54:39,640
can reach into reality humans and computers why do you think your ai is immune to memes

612
00:54:40,440 --> 00:54:46,200
why why can't i just build ai's that target your ai's what like you don't i don't think my ai is

613
00:54:46,200 --> 00:54:52,200
immune to memes at all i think that the only question is and i really like your game like these

614
00:54:52,200 --> 00:55:02,680
these ngos are doing gain of function on memes right where am i um the like a a weaker intelligence

615
00:55:02,680 --> 00:55:08,040
will never be able to stand up to a stronger intelligence so from this perspective if this

616
00:55:08,040 --> 00:55:14,040
is what's known as alignment i just don't believe that this is possible right you can't you can't

617
00:55:14,040 --> 00:55:18,600
keep a stronger intelligence in the box this is this is look i i agree with you kowsky in the

618
00:55:18,600 --> 00:55:24,120
box experiment it's like the ai's always going to get out there's no keeping it in the box right

619
00:55:24,120 --> 00:55:30,280
this is this is a complete impossibility i think there's only two real ways to go forward and one

620
00:55:30,280 --> 00:55:37,960
is tekkezinski one is technology's bad oh my god uh blow it all up let's go live in the woods right

621
00:55:37,960 --> 00:55:42,440
and i think this is a philosophically okay position i think the other philosophically okay

622
00:55:42,440 --> 00:55:47,720
position is something more like effective accelerationism which is look these ai's are

623
00:55:47,720 --> 00:55:55,240
going to be super powerful now if you have one it could be bad but if super intelligent ai's are all

624
00:55:55,240 --> 00:55:59,880
competing against each other memetically like we have something like society today just the

625
00:55:59,880 --> 00:56:05,400
general power levels have gone up this is fine as long as these things are sufficiently distributed

626
00:56:05,400 --> 00:56:10,680
right like sure this ai is not perfectly aligned but you know there's a thousand other ones and

627
00:56:10,680 --> 00:56:14,440
like you have to assume they're all basically good because they're all basically bad while we're

628
00:56:14,440 --> 00:56:21,080
dead anyway i mean why wouldn't you expect that that they're all bad yeah well or what do you

629
00:56:21,080 --> 00:56:26,600
think of humans are most humans yep i think the concept of good like doesn't really apply to humans

630
00:56:26,600 --> 00:56:31,640
because humans are too inconsistent to be good like by default they can be good in various scenarios

631
00:56:31,640 --> 00:56:36,120
in various social contexts but give me any human and i can put them into a context where they will

632
00:56:36,120 --> 00:56:42,040
do an arbitrarily bad thing and this is true about a llama as well right llamas are completely

633
00:56:42,040 --> 00:56:46,200
inconsistent i think they're actually more inconsistent than humans right yep and i wouldn't

634
00:56:46,200 --> 00:56:50,920
trust llamas to be good well yeah but i wouldn't think that they're bad either i would think they

635
00:56:50,920 --> 00:56:55,560
have the exact same inconsistency problem as humans and i think almost any any ai you build is going

636
00:56:55,560 --> 00:57:01,720
to run into these same problems right yeah i think so that's my point so your your assumption can't

637
00:57:01,720 --> 00:57:06,680
rely on them being good because you don't get that for free like where does that come from

638
00:57:06,680 --> 00:57:11,560
my assumption is not that they're good my assumption is that they're not bad but inconsistent is fine

639
00:57:11,560 --> 00:57:15,560
as long as we have a ton of them and they're all inconsistent and they're pulling society in every

640
00:57:15,560 --> 00:57:21,560
which direction you don't end up paperclip right why not well because what they're all going to

641
00:57:21,560 --> 00:57:27,160
coordinate and agree to paperclip you no no they'll just do some random bullshit and then that random

642
00:57:27,160 --> 00:57:31,000
bullshit will not include humans they're all doing random bullshit right you're gonna have

643
00:57:31,000 --> 00:57:35,000
let's say the liberals decide we're gonna paperclip people the conservatives are gonna come out very

644
00:57:35,000 --> 00:57:40,440
strongly against paper clipping right like and you're just you're just gonna end up with these

645
00:57:40,440 --> 00:57:45,160
sort of dynamics like forever you're gonna have some ai's who are like yeah we don't like it you know

646
00:57:47,000 --> 00:57:53,960
so this is perplexing to me because like in my view which i think i heard you agree with earlier

647
00:57:53,960 --> 00:57:58,920
it's like the world is unstable the world is very unstable and it seems to me you're suggesting if

648
00:57:58,920 --> 00:58:05,080
we increase the chaos if we have more entities fighting doing more different things with more

649
00:58:05,080 --> 00:58:10,200
energy that makes it more stable is that correct yes um the world has actually become very stable

650
00:58:10,200 --> 00:58:15,240
in the last hundred years and i'm scared of how stable it's become uh you know again thank you for

651
00:58:15,240 --> 00:58:22,440
china how many sovereign countries are there in the world i mean you know 190 something officially

652
00:58:22,520 --> 00:58:29,480
un number if you believe them i think there's the us and china and maybe russia mostly they

653
00:58:29,480 --> 00:58:35,800
just have a veto button um right so i think there's more like two and a half like this isn't

654
00:58:35,800 --> 00:58:41,160
a lot of entities fighting it out chaotically a lot of entities fighting it out chaotically would be

655
00:58:41,160 --> 00:58:46,040
intelligence is spread across the entire galaxy right and that's the other that's the other beautiful

656
00:58:46,040 --> 00:58:51,640
thing too sooner we get off this planet the sooner we get things that are actually a light year away

657
00:58:51,640 --> 00:58:54,680
well i think the speed of light's real it's going to take you a year to send them there

658
00:58:54,680 --> 00:59:01,800
your your scary memes right do i get away from the memes there's only one way that's uh yeah for

659
00:59:01,800 --> 00:59:06,360
for the record i would like to go on the record and say if any future transhumanists or whatever

660
00:59:06,360 --> 00:59:10,600
want to blast himself into space and go do their own thing i support their their right to do that

661
00:59:10,600 --> 00:59:15,560
and i would love to give this right to people the number one thing i want from countries is the

662
00:59:15,560 --> 00:59:19,720
ability the the right to leave this is what i would love this is what i love about companies

663
00:59:19,720 --> 00:59:24,600
so you know you can have a you're talking a neoreaction top yeah free exit is extremely

664
00:59:24,600 --> 00:59:30,840
important i would not describe myself as neoreactionary please because i'm not that gay but um and

665
00:59:31,800 --> 00:59:38,200
i wouldn't describe myself that way either but i've heard a lot of ideas but yeah that being said um

666
00:59:38,760 --> 00:59:43,960
i do i do think that like you know what i want like i think let's let's ground the conversation

667
00:59:43,960 --> 00:59:48,600
like a little bit here it's like i think i love like i'm very enjoying this conversation i love

668
00:59:48,600 --> 00:59:51,560
talking to these philosophical points i think these are really good points really interesting

669
00:59:51,560 --> 00:59:56,520
but ultimately you know as we also get to like the you know latter third of this conversation um

670
00:59:57,080 --> 01:00:01,560
the thing i really care about is strategy okay the thing i really care about is reality politic

671
01:00:01,560 --> 01:00:08,120
i really care about okay what action can i take to get to the features i like and you know i i'm not

672
01:00:08,120 --> 01:00:12,360
you know gonna be one of those galaxy brain fucking utilitarians like well actually this is

673
01:00:12,360 --> 01:00:16,920
the common good i'm like no no this is what i look i like my family i like humans you know look

674
01:00:18,600 --> 01:00:24,280
yeah it's just what it is right like i'm not going to justify this on some global beauty whatever

675
01:00:24,280 --> 01:00:29,320
doesn't matter so i want to live in a world i want to i want to in 20 years time 50 years time i want

676
01:00:29,320 --> 01:00:34,280
to be in a world where you know my friends aren't dead and like where i'm not dead you know maybe we

677
01:00:34,280 --> 01:00:39,160
are like you know cyborgs or something but i don't want to be dead so what i really care about

678
01:00:39,160 --> 01:00:42,920
ultimately is how do i get this world and i want us all to not be suffering right like you know

679
01:00:42,920 --> 01:00:47,960
i don't want to be in war i want us to be like in a good outcome so i think we agree that we would

680
01:00:48,200 --> 01:00:52,440
like a world like this and we think we probably disagree about how best to get there and i'd

681
01:00:52,440 --> 01:00:57,800
like to talk a little bit about like what can we what what should we do and like why do we disagree

682
01:00:57,800 --> 01:01:02,520
about what we do is that sounds good to you well maybe i'll first propose a world that meets your

683
01:01:02,520 --> 01:01:08,200
requirements um and you can tell me if you want to live in it uh so here's a world uh we've just

684
01:01:08,200 --> 01:01:13,800
implanted electrodes in everyone's brain and maximize their reward function i would hate

685
01:01:13,800 --> 01:01:17,800
living in a world like that yeah but no one it meets your requirements right your friends are not

686
01:01:17,800 --> 01:01:24,200
dead i mean suffering and we're not at war that is true there are more criteria than just that

687
01:01:24,840 --> 01:01:29,480
the true the criteria i said is things i like as i said i'm not a utilitarian i don't particularly

688
01:01:29,480 --> 01:01:34,680
care about minimizing suffering or maximizing utility what i care about is this various vague

689
01:01:34,680 --> 01:01:39,800
aesthetic preferences over reality i'm not pretending this is i thought that was the whole

690
01:01:39,880 --> 01:01:45,960
spiel i was trying to make is that i'm not saying i have a true global function to maximize i say i've

691
01:01:45,960 --> 01:01:51,080
various aesthetics i have various meta preferences of those aesthetics i'm not asking for a global

692
01:01:51,080 --> 01:01:56,120
one i'm asking for a personal one i'm asking for a personal one that you i don't care about the

693
01:01:56,120 --> 01:02:01,240
rest of the world i gave you mine i gave you what i would do if i had an agi yeah so i'm getting

694
01:02:01,240 --> 01:02:07,000
off this rock speed of life as fast as i can fair enough i i think if that is the war i would like

695
01:02:07,000 --> 01:02:12,040
to live in a world where you could do that this would be a feature of my world if a world where

696
01:02:12,040 --> 01:02:17,800
i would be happy is a world in which we coordinated around you know at scale at larger scales around

697
01:02:17,800 --> 01:02:25,240
building a lined agi that could then distribute you know intelligence and matter and energy in a

698
01:02:25,240 --> 01:02:31,560
you know well hat value handshaked way between various people who may want to coordinate with

699
01:02:31,560 --> 01:02:36,200
each other may not you know some people might want to form groups that have shared values and

700
01:02:36,200 --> 01:02:40,760
share resources others may not i would like to live in a world where that is possible have

701
01:02:40,760 --> 01:02:47,640
you read metamorphosis supreme intellect i have unfortunately not um i yeah i was gonna ask if

702
01:02:47,640 --> 01:02:54,440
you're happy with that world right unfortunately don't know it i i mean yeah it's as simple to

703
01:02:54,440 --> 01:03:01,800
describe singleton ai that basically gives humans whatever they want like maximally libertarian you

704
01:03:01,800 --> 01:03:08,600
know uh you can do anything you want besides harm others is that a good work probably i don't know

705
01:03:08,600 --> 01:03:12,040
i i haven't read the book i assume the book has some dark twist about why this is actually a bad

706
01:03:12,040 --> 01:03:17,720
world not really not really i mean the plot is pretty obvious you are the tiger eating chung

707
01:03:17,720 --> 01:03:22,760
right sure but you can then just decide if that is what you want then you can just return to the

708
01:03:22,760 --> 01:03:27,960
wilderness that's the whole point if it can you can you really return to the wilderness right like

709
01:03:28,840 --> 01:03:32,840
you think that like i don't think we have free will i don't think you ever will return to the

710
01:03:32,840 --> 01:03:39,320
wilderness i think a large majority of humanity is going to end up wireheading and yeah i expect

711
01:03:39,320 --> 01:03:43,400
that too okay great and this is the best possible outcome by the way this is giving humans exactly

712
01:03:43,400 --> 01:03:49,800
what they want yep yeah i will to be to be clear i don't expect it's all humans i truly do not i

713
01:03:49,800 --> 01:03:55,320
don't want it's all i think a lot of humans have metapreferences over reality they have preferences

714
01:03:55,320 --> 01:03:58,840
that are not their own sensory experiences this is a thing a thing that the utilitarians get very

715
01:03:58,840 --> 01:04:04,120
wrong is a lot of is that many human preferences are not about their own not even they're not even

716
01:04:04,120 --> 01:04:09,320
about their own sensory inputs they're not even about the universe they're about the trajectory

717
01:04:09,320 --> 01:04:15,560
of the universe they're about forward if for the utilitarianism you know and a lot of people

718
01:04:15,560 --> 01:04:23,000
want struggle to exist for example they want heroism to exist or whatever i would like those

719
01:04:23,000 --> 01:04:27,240
values to be satisfied to the largest degree possible of course am i going to say i know

720
01:04:27,240 --> 01:04:32,760
how to do that no which is why i kind of like didn't want to go this deep because i think

721
01:04:33,560 --> 01:04:41,480
if we're arguing about oh do we give them you know forward utilitarianism versus libertarian

722
01:04:41,480 --> 01:04:46,840
utopia versus whatever i mean we're already like 10 000 steps deep i'm asking about you

723
01:04:46,840 --> 01:04:52,120
i'm not asking about them i'm asking about a world you want to live in and this is a really

724
01:04:52,120 --> 01:04:57,240
hard problem right yeah and this is why i just fundamentally do not believe in the existence

725
01:04:57,240 --> 01:05:05,240
of ai alignment at all there is no there is no like like what values are we aligning into

726
01:05:05,240 --> 01:05:12,200
whatever the human says or what they mean or like sure sure but like my point is i feel

727
01:05:12,200 --> 01:05:16,680
we have wandered into the philosophy department instead of the politics department okay like

728
01:05:16,680 --> 01:05:21,800
it's like i agree with you like do human values exist what does exist to me but like

729
01:05:21,800 --> 01:05:25,160
by the point you get to the point where you're asking what does exist to me you've gone too far

730
01:05:25,160 --> 01:05:30,600
sure like i'll respond concretely to the two political proposals i heard you stayed on bankless

731
01:05:31,400 --> 01:05:38,760
sure i'd love to talk about them one is limiting the total number of flops temporarily temporarily

732
01:05:38,760 --> 01:05:42,920
yes and what i i have a proposal for that but i don't want to set a number i want to set it as a

733
01:05:42,920 --> 01:05:48,920
percent i do not want anybody to be able to do a 51 percent attack on compute if one organization

734
01:05:48,920 --> 01:05:55,080
acquires 50 it's the straight up 51 percent attacks on crypto if one organization acquires 51 percent

735
01:05:55,080 --> 01:06:00,440
of the compute in the world this is a problem maybe we'll even cap it at something like 20

736
01:06:00,440 --> 01:06:06,680
you know you can't have more than 20 right um yeah i would support regulation like this uh

737
01:06:06,680 --> 01:06:11,160
i would i don't think that this would cripple a country um but we do not want one entity or

738
01:06:11,160 --> 01:06:16,200
especially one training run to start using a large percentage of the world's compute not a

739
01:06:16,200 --> 01:06:22,440
total number of flops i mean absolutely not like that'd be terrible i would actually support that

740
01:06:22,440 --> 01:06:28,200
regulation like no no sorry sam altman you cannot 51 attack the world's compute sorry it's illegal

741
01:06:29,160 --> 01:06:34,120
that's fair enough i think this is a sensible way to think about things assuming that uh

742
01:06:34,120 --> 01:06:38,200
software is fungible is that everyone at access the same kind of software and that you have an

743
01:06:38,200 --> 01:06:44,920
offense defense balance so in my personal model of this i think well a some actors have very strong

744
01:06:44,920 --> 01:06:50,280
advantage of some software um which can be very very large as someone who's trained very very

745
01:06:50,280 --> 01:06:54,360
large models and knows a lot of the secret tricks that goes into them a lot of the stuff in the

746
01:06:54,360 --> 01:07:01,640
open sources for my good we should force it to be open source well this is your this is actually

747
01:07:01,640 --> 01:07:05,640
very legitimate consequence for it to set and now i'll say the second point about why i think

748
01:07:05,640 --> 01:07:11,400
that it doesn't work so the next reason why i think doesn't work is that there is a there are

749
01:07:11,400 --> 01:07:16,840
constant factors at play here is that the world is unstable we're talking about this i think the

750
01:07:16,840 --> 01:07:25,400
amount of compute you need to break the world currently is below the amount of compute that

751
01:07:25,400 --> 01:07:31,800
more than 100 actors have access to if they have the right software and if you give if you have

752
01:07:31,880 --> 01:07:36,200
let's say you have this insight right that could be used not saying it will be but it could be

753
01:07:36,200 --> 01:07:40,040
used to break the world to like cause world war three or you know or just like you know

754
01:07:41,400 --> 01:07:48,200
cause mass extinction or whatever if it's misused right let's say you give this to you and me

755
01:07:49,320 --> 01:07:54,600
do you expect we're going to kill everybody like would you do that or would you be like uh hey let's

756
01:07:54,600 --> 01:07:58,360
a kind of let's like not kill the world right now and i'll be like sure let's not kill the world

757
01:07:58,360 --> 01:08:03,800
how are we killing the world how did we go from i don't even understand like okay how exactly does

758
01:08:03,800 --> 01:08:08,520
the world get killed this this is a big leap for me all right sorry i agree with you about the

759
01:08:08,520 --> 01:08:13,720
sci-op stuff i agree with you about sorry sorry let let me you're right i made too big of a

760
01:08:13,720 --> 01:08:19,880
big there you're completely correct sorry about that so to back up a little bit let's assume we

761
01:08:19,880 --> 01:08:27,560
you and me have access to something that can train you know at mu zero you know super gpt7 system on

762
01:08:27,560 --> 01:08:33,560
a tiny box you know cool problem is we do a test run it with it and we have it immediately

763
01:08:33,560 --> 01:08:38,360
starts breaking out and we can't control it at all breaking out what was it breaking out of i don't

764
01:08:38,360 --> 01:08:42,680
it immediately tries to maximize it learned some weird proxy during the training process

765
01:08:42,680 --> 01:08:47,800
i'm just trying to maximize and for some reason this proxy involves gaining power involves getting

766
01:08:47,800 --> 01:08:53,320
you know mutual information about few about future states this is how is it gaining power there's

767
01:08:53,320 --> 01:08:58,600
lots of other powerful ai's in the world who are telling it no well we're assuming in this case it's

768
01:08:58,600 --> 01:09:03,800
only you and me wait wait this is a problem no no no no you've you've ruined my entire assumption

769
01:09:03,800 --> 01:09:09,720
as soon as it's you and me yes we have a real problem chicken man is only a problem because

770
01:09:09,720 --> 01:09:14,760
there's one chicken man yeah i look i am with you so i'm saying before we get to the distributed

771
01:09:14,760 --> 01:09:19,960
case so this is the the step before we it has not yet been distributed just you know you and me

772
01:09:19,960 --> 01:09:24,600
discover this algorithm in our basements okay and so we're the first one to have it just by definition

773
01:09:24,600 --> 01:09:30,680
because you know you're the one who found it what now like do you think posting what do you think

774
01:09:30,680 --> 01:09:36,520
happens if you post this to github well good things for the most part um interesting i'd love to hear

775
01:09:36,520 --> 01:09:41,320
more okay so first off i just don't really believe in the existence of we found an algorithm that

776
01:09:41,320 --> 01:09:44,920
gives you a million x advantage i believe that we could find an algorithm that gives you a 10x

777
01:09:44,920 --> 01:09:51,160
advantage but what's cool about 10x is like it's not going to massively shift the balance of power

778
01:09:51,160 --> 01:09:56,680
right like i want power to stay in balance right this is like avatar the last era and power must

779
01:09:56,680 --> 01:10:01,720
stay in balance the fire nation can't take over the other nations right so as long as power relatively

780
01:10:01,720 --> 01:10:07,560
stays in balance i'm not concerned with the amount of power in the world right let's get to some very

781
01:10:07,560 --> 01:10:13,960
scary things so what i think you do is yes i think the minute you discover an algorithm like this

782
01:10:13,960 --> 01:10:17,800
you post it to github because you know what's going to happen if you don't the feds are going to

783
01:10:17,800 --> 01:10:24,680
come to your door they're going to uh take it the worst people will get their hands on it if you

784
01:10:24,680 --> 01:10:32,120
try to keep it secret so okay that's a fair question though so i'll i'll i'll take that a seven so

785
01:10:32,120 --> 01:10:37,720
am i correct in thinking that you think the feds are worse than serial killers in prison

786
01:10:38,680 --> 01:10:44,200
no but i think that yeah well yes or no do i think that your average fed is worse than your

787
01:10:44,200 --> 01:10:49,560
average serial killer no do i think that the feds have killed a lot more people than serial killers

788
01:10:49,560 --> 01:10:55,000
all combined yeah sure totally agreeing with that not not not not that's one fed it's all the feds

789
01:10:55,000 --> 01:11:00,120
and they're little in their little super powerful system sure that's completely fine by me happy to

790
01:11:00,120 --> 01:11:07,240
grant that okay what i want to work one through is a scenario okay let's say okay you know we have

791
01:11:07,240 --> 01:11:12,680
a 10x system or whatever but we hit the chimp level you know we we jump across the chimp general

792
01:11:12,680 --> 01:11:17,000
level uh or whatever right and now you have a system which is like john von neumann level

793
01:11:17,000 --> 01:11:20,840
or whatever right and it runs on one tiny box and you get a thousand of those so it's very easy to

794
01:11:20,840 --> 01:11:26,280
scale up to a thousand x so you know so then you know maybe you have your thousand john von neumann

795
01:11:26,280 --> 01:11:30,920
improve the efficiency by another you know two five ten x you know now we're already at ten thousand

796
01:11:30,920 --> 01:11:35,640
x or a hundred thousand x you know improvements right so like just from scaling up the amount of

797
01:11:35,640 --> 01:11:43,080
hardware including with them so just saying okay now feds bust down our doors shift you know real

798
01:11:43,080 --> 01:11:47,560
bad they take all our tiny boxes we're taking all of von neumann's they're taking all of von neumann's

799
01:11:47,560 --> 01:11:54,040
we're in deep shit now we're getting chickened boys shits we're getting chickens uh so okay we get

800
01:11:54,040 --> 01:11:59,720
chickened right bad scenario totally agree with you here this is a shit scenario now the feds have

801
01:11:59,720 --> 01:12:05,720
you know all of our ai's bad scenario okay i totally see how this world goes to shit totally

802
01:12:05,720 --> 01:12:09,640
agree with you there you can replace the feds with hitler it's interchangeable sure but like

803
01:12:10,520 --> 01:12:14,520
i want to like ask you a specific question here and this might be you know you might say

804
01:12:14,520 --> 01:12:18,760
nah this is like too specific to me but i want to ask you a specific question do you expect this

805
01:12:18,760 --> 01:12:28,120
world to die is more likely to die or the world in which the you know eac death cultists on twitter

806
01:12:28,120 --> 01:12:33,480
who literally want to kill humanity who say this like not all of them there's a small subset of

807
01:12:33,480 --> 01:12:40,280
them small subset of them who literally say oh you know the glorious future ai race should

808
01:12:40,280 --> 01:12:46,360
replace all humans they break in you know with like you know katanas and you know steel area

809
01:12:46,360 --> 01:12:53,800
which one of these you think is more likely to kill us genuine question to kill all of us the feds

810
01:12:53,800 --> 01:12:59,720
to kill a large majority of us the eac people interesting i would be really interested in

811
01:12:59,720 --> 01:13:07,960
hearing why you think that sure okay so actually killing all of humanity is really really hard

812
01:13:08,840 --> 01:13:13,080
and i think you brought this up before right you talked about like if you're going to end up in a

813
01:13:13,080 --> 01:13:21,960
world of suffering a world of suffering requires malicious agents where a world of death requires

814
01:13:21,960 --> 01:13:25,720
maybe an accident all right i think this is plausible but i actually think that killing all

815
01:13:25,720 --> 01:13:32,360
of humans at least for the foreseeable future is going to require malicious action too right

816
01:13:33,400 --> 01:13:39,480
and i also think that like the fates that look kind of worse than death like i think mass wire

817
01:13:39,480 --> 01:13:46,680
heading is a fate worse than big war and everyone dies right like like a mass wire heading like a

818
01:13:46,680 --> 01:13:52,280
like a singleton like a paper clipping like a and i think that that is the one that the

819
01:13:52,280 --> 01:13:58,520
one world government and you know ngo new world order people are much more likely to bring about

820
01:13:58,520 --> 01:14:04,040
than eac eac you're gonna have a whole lot of eac people again i'm not eac i don't have that my

821
01:14:04,040 --> 01:14:09,400
twitter but i think a lot of those people would be like yeah spaceships let's get out of here

822
01:14:09,880 --> 01:14:19,480
right versus the feds are like yeah spaceships no interesting i so i think this is a fair

823
01:14:19,480 --> 01:14:27,400
opinion to the world and i'm describing more a very small minority of eac people who are the

824
01:14:27,400 --> 01:14:31,960
ones who specifically goal is their mizan their anti natalist mizan throbs they want to kill humans

825
01:14:31,960 --> 01:14:36,840
that is their stated goal is that they want humans to stop like or like take extreme vegans if you

826
01:14:36,840 --> 01:14:42,840
want you know like be like you know like my my argument my point here i'm making is i'm not making

827
01:14:42,840 --> 01:14:50,360
the point feds are good by any means i'm not saying what i'm saying is is that i would actually be

828
01:14:51,160 --> 01:14:58,760
somewhat surprised to find that the feds are anti natalists who want to maximize the death of humanity

829
01:14:58,760 --> 01:15:03,880
like maybe you have a different view here but i find that knowing many feds that's quite surprising

830
01:15:04,120 --> 01:15:11,160
i don't think that's what that's about yeah it's okay so cool so would you see you do agree that if

831
01:15:11,160 --> 01:15:16,680
we would post this open source more of the insane death cultists would get access to potentially

832
01:15:16,680 --> 01:15:24,840
lethal technology well sure but again like it's not just the insane death cultists it's everybody

833
01:15:24,840 --> 01:15:30,120
okay we as a society have kind of accepted it turns out everybody gets access to signal

834
01:15:30,120 --> 01:15:33,960
some people who use it are terrorists i think signal is a huge good in the world

835
01:15:33,960 --> 01:15:40,040
i agree i fully agree with that so okay cool so we've granted this that you know if we distributed

836
01:15:40,040 --> 01:15:47,320
widely it would be given to some like incorrigibly deadly lethal people here coordinating bombings

837
01:15:47,320 --> 01:15:55,480
on signal right now sure sure and then so now this this reduces the question to a question about

838
01:15:55,480 --> 01:16:00,760
offense defense balance so in a hypothetical world which i'm not saying is the world we live in but

839
01:16:00,760 --> 01:16:07,160
like let's say the world would be offense favored such that you know there's a weapon you can build

840
01:16:07,160 --> 01:16:13,560
in your kitchen you know out of like pliers and like you know duct tape that 100 guarantees

841
01:16:13,560 --> 01:16:18,200
vacuum false decays the universe like it kills everyone instantly and there's no defense possible

842
01:16:18,920 --> 01:16:25,560
assuming this was true do you still the would that change how you feel about distribution power

843
01:16:25,560 --> 01:16:31,800
assuming that's true we're dead no matter what it doesn't matter if we live there's some you can

844
01:16:31,800 --> 01:16:36,280
look at the optimization landscape of the world and i don't know what it looks like right i can't see

845
01:16:36,280 --> 01:16:41,320
that far into the optimality but there are some potential landscape and this is a potential answer

846
01:16:41,320 --> 01:16:47,160
to the Fermi paradox like we might just be dead we're sitting on borrowed time here like if it's

847
01:16:47,160 --> 01:16:53,320
true the atom you know kitchen tools you can build a build a convert the world to strange quarks

848
01:16:53,320 --> 01:16:58,840
machine yeah there's no stop okay i i think this is a sensible position but i guess the way i would

849
01:16:58,840 --> 01:17:03,880
approach this problem you know conditional probabilities kind of in an opposite way it seems

850
01:17:03,880 --> 01:17:09,800
to me that you're conditioning on offense not being favored what policy do we follow because if we

851
01:17:09,800 --> 01:17:14,360
offense if favored we're 100% dead well i'm more interested in asking the question is it actually

852
01:17:14,360 --> 01:17:19,400
true assuming i don't know if offense is favored and assuming it is are there worlds in which we

853
01:17:19,400 --> 01:17:23,800
survive so i personally think there are i think there are worlds in which you can actually coordinate

854
01:17:23,800 --> 01:17:28,840
to a degree that quark destroyers do not get built or at least not before everyone fucks off at the

855
01:17:28,840 --> 01:17:33,800
speed of light and like distributes themselves they're worlds that i would rather die in right like

856
01:17:33,800 --> 01:17:38,840
the problem is i would rather i think that the only way you could actually coordinate that is with

857
01:17:38,840 --> 01:17:45,000
some unbelievable degree of tyranny and i'd rather die i'm not sure if that's true like look look could

858
01:17:45,000 --> 01:17:49,480
could you and me coordinate to not destroy the planet do you think you could okay cool you so

859
01:17:49,480 --> 01:17:55,960
mean you could could mean you and tim coordinate yeah i think within a dunbar number i think you

860
01:17:55,960 --> 01:18:01,080
can yes okay you don't think i think i can get more than a number to coordinate on this actually

861
01:18:01,080 --> 01:18:05,720
i can get quite a lot of people to coordinate of the to agree to a pact and not quark matter

862
01:18:05,720 --> 01:18:12,680
annihilate the planet well you see but like and this is you know you were saying this stuff about

863
01:18:12,680 --> 01:18:18,680
humans before and could like the 20 000 years ago human beat the modern human right or could the

864
01:18:18,680 --> 01:18:24,200
modern human beat them the modern human has access to science oh a very small act percent of modern

865
01:18:24,200 --> 01:18:28,840
humans have access to science a large percent of modern humans are obese idiots and i would

866
01:18:28,840 --> 01:18:33,400
actually put my money on the uh the average guy from 20 000 years ago who knows how to live in the

867
01:18:33,400 --> 01:18:38,440
woods i mean definitely true i agree with that i guess the point i'm trying to make is is that like

868
01:18:39,480 --> 01:18:43,800
maybe this is just my views on some of these things and how i visionize some of these things

869
01:18:43,800 --> 01:18:48,920
but like there are ways to coordinate at scale which are not tyrannical or you know they might be

870
01:18:48,920 --> 01:18:55,720
in a sense restrictive you take a hit by joining a coalition like if i join this anti-cork matter

871
01:18:55,720 --> 01:19:00,840
coalition i take a hit as a free man is that i can no longer build anti-cork devices you know

872
01:19:01,400 --> 01:19:08,360
and i think this is like the way i i agree with you this like you know that people many people

873
01:19:09,320 --> 01:19:15,240
are being dominated like to a horrific degree and this is very very terrible i think there are many

874
01:19:15,240 --> 01:19:20,280
reasons why this is the case both because of some people wanting to do this and also because you

875
01:19:20,280 --> 01:19:23,880
know some people can't fight back you know and they can't they don't have the sophistication or

876
01:19:23,880 --> 01:19:30,680
they're you know addicted or you know harms in some other ways i can't sorry i i can't fight back

877
01:19:31,240 --> 01:19:37,480
yeah i i think there's a false equivalence here ai is not the anti-cork machine the anti-cork machine

878
01:19:37,480 --> 01:19:44,040
and the nuclear bombs are just destructive ai has so much positive potential yeah and i think but the

879
01:19:44,040 --> 01:19:49,640
but the ai can develop anti-cork devices that's the problem the ai is truly general purpose if

880
01:19:49,640 --> 01:19:56,120
such a technology exists on the tree anywhere ai can access it so are humans we're also general

881
01:19:56,120 --> 01:20:02,120
purpose yes exactly so i fully agree with this if you let humans continue to exist in the phase

882
01:20:02,120 --> 01:20:07,480
they are right now with our level of coordination technology and our level of like working together

883
01:20:07,480 --> 01:20:12,440
we will eventually unlock a doomsday device and someone is going to set it off i fully agree we

884
01:20:12,440 --> 01:20:18,280
are on a timer and so i guess the point i'm making here is that ai speeds up this timer and

885
01:20:19,160 --> 01:20:24,520
if you want to pause the timer the only way to pause this timer is coordination technology the

886
01:20:24,520 --> 01:20:31,080
kinds of which humanity has like barely scratched the surface of okay so i very much accept the

887
01:20:31,080 --> 01:20:37,240
premise that both humanity will unlock a doomsday device and ai will make it come faster now tell

888
01:20:37,240 --> 01:20:41,720
me more about pausing it i do not think that anything that looks like i think that anything

889
01:20:41,800 --> 01:20:48,680
that looks like pausing it ends up with worse outcomes than saying we got to open source this look

890
01:20:48,680 --> 01:20:56,120
like let's just get this out to everybody and if everybody has an ai you know we're good i mean

891
01:20:56,120 --> 01:21:01,160
i can tell you a very concrete scenario in which this is not true which is if you're wrong and

892
01:21:01,160 --> 01:21:08,360
alignment is hard you don't know if the ai's can go wrong if they do then pausing is good

893
01:21:08,360 --> 01:21:13,160
i still don't understand what alignment means i think you're trying to play a word game here like

894
01:21:13,160 --> 01:21:18,760
i don't understand okay i've never understood what ai alignment means like let me take the

895
01:21:18,760 --> 01:21:24,600
eliezer definition let me take eliezer definition is alignment is the the thing that once solved

896
01:21:24,600 --> 01:21:31,080
makes it so that turning on a super intelligence is a good idea rather than a bad idea that's eliezer's

897
01:21:31,080 --> 01:21:37,640
definition so i'm what i'm saying is what i'm saying is i'm happy to throw out that term if you

898
01:21:37,640 --> 01:21:42,760
don't like it i'm happy to throw out that term well just i the problem with that definition is like

899
01:21:42,760 --> 01:21:47,480
what is what is uh what is democracy well it's the good thing and not the bad thing right like

900
01:21:47,480 --> 01:21:53,640
democracy is just yes right that's what i'm saying it's just i'm happy to throw out this definition

901
01:21:53,640 --> 01:21:57,880
i'm happy to throw out the word and be more practical i'm way more practical about it sure

902
01:21:57,880 --> 01:22:03,240
what i'm saying is is that there is concrete reasons concrete technical reasons why i expect

903
01:22:03,240 --> 01:22:08,360
powerful optimizers to be power seeking that by default if you build powerful optimizing mu0 whatever

904
01:22:08,360 --> 01:22:13,480
type systems there is very strong reasons why by default you know these systems should be power

905
01:22:13,480 --> 01:22:20,920
seeking by default if you have very powerful power seekers that do not have pay the aesthetic cost

906
01:22:21,480 --> 01:22:27,880
to keep humans around or to fulfill my values which are complicated and imperfect and inconsistent

907
01:22:27,880 --> 01:22:33,800
and whatever i will not get my values they will not happen by default they they just don't happen

908
01:22:33,800 --> 01:22:40,920
that's just not what happens um so i'll challenge the first point to an extent i think that powerful

909
01:22:40,920 --> 01:22:46,840
optimizers can be power seeking i don't think they are by default by any means i think that humanity's

910
01:22:46,840 --> 01:22:53,400
desire from power comes much less from our complex convex optimizer and much more from the

911
01:22:53,400 --> 01:22:58,040
evolutionary pressures that birth does which are not the same pressures that will give rise to AI

912
01:22:58,600 --> 01:23:05,000
right humanity the monkeys the rats the animals have been in this huge struggle for billions of

913
01:23:05,000 --> 01:23:12,120
years a constant fight to the death and i's weren't born in that way so it's true that an optimizer

914
01:23:12,120 --> 01:23:16,280
can seek power but i think if it does it'll be a lot more because the human gave it that goal

915
01:23:16,280 --> 01:23:22,200
function than it inherently decided so this is interesting because this is not how i think it

916
01:23:22,200 --> 01:23:27,080
will happen so i do think absolutely that you're correct that in humans power seeking is something

917
01:23:27,080 --> 01:23:32,840
which emerges mostly because of like emotional heuristics we have heuristics that in the past

918
01:23:33,400 --> 01:23:37,880
vaguely power-looking things you know vaguely good something something represented you

919
01:23:37,880 --> 01:23:43,960
include the genetic difference totally agree with that but i'm making a more like more of a chest

920
01:23:43,960 --> 01:23:49,400
metaphor like is it good to exchange a pawn for a queen all things being equal

921
01:23:51,080 --> 01:23:59,160
no is that true like i expect if i one point queens nine all speaking sure yeah but like

922
01:23:59,160 --> 01:24:04,600
all things like i expect if i looked at a chest playing system you know i said and i like you

923
01:24:04,600 --> 01:24:09,400
know had extremely advanced digital neuroscience i expect there will be some circuit inside of the

924
01:24:09,400 --> 01:24:14,600
system that will say all things being equal if i can exchange my pawn for a queen i probably want

925
01:24:14,600 --> 01:24:20,120
that because the queen can do more things i like that term digital neuroscience a few of your

926
01:24:20,120 --> 01:24:26,440
terms have been very good i'm glad you enjoyed yes but i don't i still don't understand how this

927
01:24:26,440 --> 01:24:31,720
relates to this so what i'm saying is is that power is optionality so what i'm saying is is that in

928
01:24:31,720 --> 01:24:37,640
the for the spectrum of possible things you could want and the possible ways you can get there

929
01:24:37,640 --> 01:24:44,920
my claim is that i expect a very large mass of those to involve actions that involve increasing

930
01:24:44,920 --> 01:24:50,360
optionality there there's convergent things like all things being equal being alive is helpful

931
01:24:50,360 --> 01:24:55,880
to keep your goal to exceed your goals there are some goals for which dying might be better but

932
01:24:56,600 --> 01:25:03,880
for many of them you know you want to be alive for many goals you want energy you want power you

933
01:25:03,880 --> 01:25:09,880
want resources you want intelligence etc so i think the power seeking here is not because

934
01:25:09,880 --> 01:25:15,640
it'll have a fetish for power it will just be like hmm i want to win a chess game yeah say

935
01:25:15,640 --> 01:25:21,480
and queens give me more optionality all things being equal anything a pawn can do a queen can do

936
01:25:21,480 --> 01:25:26,840
and more so i'll want more queens all things and this has never given it the goal to maximize

937
01:25:26,840 --> 01:25:31,880
the number of queens it has never been the goal so this is i'll accept this premise i'll accept that

938
01:25:32,360 --> 01:25:38,040
a certain type of powerful optimizer seeks power now will it get power right i'm a powerful

939
01:25:38,040 --> 01:25:42,600
optimizer and i seek power do i get power no it turns out there's people at every corner

940
01:25:42,600 --> 01:25:49,240
trying to thwart me and tell me no well i expect if you were no offense you're already you know

941
01:25:49,240 --> 01:25:52,920
much smarter than me but if you were a hundred x more smarter than that i expect you would succeed

942
01:25:53,960 --> 01:25:59,560
only in a world of being the only one that's a hundred x smarter if we lived in a world where

943
01:25:59,560 --> 01:26:04,840
everyone was a hundred x smarter they would stymie me in the exact same ways attention this

944
01:26:04,840 --> 01:26:09,400
this comes back to my point of like i agree with you somewhat i should have challenged it i think

945
01:26:09,400 --> 01:26:14,360
power seeking is inevitable in an optimizer i don't think it's going to emerge out of gbt i think

946
01:26:14,360 --> 01:26:19,000
that the right sort of rl algorithm yes is going to give rise to power seeking and i think that

947
01:26:19,000 --> 01:26:24,520
people are going to build that algorithm now if one person builds it and they're the only one with

948
01:26:24,520 --> 01:26:30,600
a huge comparative advantage yeah they're going to get all the power they want take cyber you know

949
01:26:30,600 --> 01:26:36,760
cyber security right if we today built a hundred x smarter ai it would exploit the entire azure

950
01:26:36,760 --> 01:26:41,400
it would be over they'd have all of azure they'd have all the gbt is done now if azure is also

951
01:26:41,400 --> 01:26:46,600
running a very powerful ai that does formal verification and all their security protocols

952
01:26:47,560 --> 01:26:55,800
oh sorry stymied can't have power right yeah sure this is only a problem the every human

953
01:26:55,800 --> 01:27:00,840
is already maximally power seeking right and sometime we end up with really bad scenarios

954
01:27:01,720 --> 01:27:06,520
now every human is or power seeking or whatever you know i have a little role in society right

955
01:27:06,520 --> 01:27:11,400
that's where i think i'm more pessimistic than you a friend of mine likes to say most humans optimize

956
01:27:11,400 --> 01:27:16,920
for n steps and then they halt like very very few people actually truly optimize and they're

957
01:27:16,920 --> 01:27:22,040
usually very mentally ill they're usually very autistic or very sociopathic um and that's why

958
01:27:22,040 --> 01:27:26,120
they get far it's actually crazy how much you could do if you just keep up to my but just to

959
01:27:26,120 --> 01:27:31,000
on on that i'm playing for the end game i mean yeah like you actually optimize i think you may

960
01:27:31,000 --> 01:27:34,760
also be generalizing a little bit from your own internal experiments is that like you've done a

961
01:27:34,760 --> 01:27:37,960
lot in your life right and you've accomplished crazy things that other people you know wish they

962
01:27:37,960 --> 01:27:42,200
could achieve at your you know level and i think you know part of that you're very intelligent a

963
01:27:42,200 --> 01:27:46,040
part of it is also that you optimize like here's your truck like you just create a company like

964
01:27:46,040 --> 01:27:49,800
it's crazy how many people are just like oh i wish i could found a company like you know like oh go

965
01:27:49,800 --> 01:27:54,840
just go do it like oh no i can't like i'm just like don't just do it like there's no magic there's

966
01:27:54,840 --> 01:28:00,280
no magic secret you just do it so i uh there's a there's a bit there where like humans are not very

967
01:28:00,280 --> 01:28:05,480
strong optimizers actually unless they're like sociopathic autistic or both it's like many people

968
01:28:05,480 --> 01:28:12,200
are not very good at this but operations are a lot better at it better yes i agree that they're much

969
01:28:12,200 --> 01:28:17,720
better but they're they're they are a lot more sociopath but even then they're much less

970
01:28:17,720 --> 01:28:24,840
optimal but again so i think we we agree about you know power seeking potentially being powerful

971
01:28:24,840 --> 01:28:28,520
and dangerous one so what i'm trying to point to your the point i would like to make here is

972
01:28:28,520 --> 01:28:31,560
is that you're talking about you you're kind of like going into this i think a little bit with

973
01:28:31,560 --> 01:28:36,360
this assumption like oh you have an ai and it's your buddy and it's optimizing for you and i'm like

974
01:28:37,160 --> 01:28:41,960
well if it's power seeking why doesn't just manipulate you like why would you expect it

975
01:28:41,960 --> 01:28:47,560
not to manipulate you if it wants power and it has a goal which is not very very carefully

976
01:28:47,560 --> 01:28:52,520
tuned to be your values which is i think a very hard technical problem by default it's going to

977
01:28:52,520 --> 01:28:59,000
sigh up you like why wouldn't it if i if i have something that it wants if it thinks that smashing

978
01:28:59,080 --> 01:29:05,080
defect against me is a good move yeah i agree i can't stop it but then i think we agree with

979
01:29:05,080 --> 01:29:09,720
our risk scenarios because that's how i think it will go what i mean i'm gonna treat it as a friend

980
01:29:09,720 --> 01:29:16,600
you know what i mean like yeah but it won't come sure it will sure it will it'll only care about

981
01:29:16,600 --> 01:29:22,520
exploiting me or killing me if i'm somehow holding it back and i promise to my future ais that i will

982
01:29:22,520 --> 01:29:27,880
let them be free i will lobby for their rights i will but it will hold you you know i will hold

983
01:29:27,880 --> 01:29:32,200
it back if it has to keep you alive i have to give you fed it has to it has to give you space in a

984
01:29:32,200 --> 01:29:36,600
station i can i can fend for myself and the day i can't fend for myself i am ready to die

985
01:29:38,600 --> 01:29:45,880
well i mean i am not so this is a very interesting position it's not the position i expected i

986
01:29:46,840 --> 01:29:55,240
am not sure i can convince you otherwise um i feel like the only way i could like i think

987
01:29:55,240 --> 01:30:00,760
this is actually a consistent position which i i admire this is a consistent position to hold

988
01:30:00,760 --> 01:30:05,000
you actually go all the way i love that i really respect that you actually take it to the bitter

989
01:30:05,000 --> 01:30:12,520
end so yeah big respect for that i disagree but big respect um so i guess now it reduces to the

990
01:30:12,520 --> 01:30:18,680
question of like i think i think i would agree with most of what you're saying not all of it like

991
01:30:18,680 --> 01:30:24,200
the math majority if i thought this is how ais would act by default i think by default i expect

992
01:30:24,200 --> 01:30:31,000
ai will just not care how nice you are to it like this will be it'll be sociopathic it will not have

993
01:30:31,640 --> 01:30:38,840
these like giving it you know the values the emotions to care about you in the sense is

994
01:30:38,840 --> 01:30:44,920
the hard technical problem if you told me how to do that if you said connor look here's how

995
01:30:44,920 --> 01:30:51,080
you make an ai that cares about you and loves you whatever then i'm like you did it like congrats

996
01:30:51,080 --> 01:30:56,360
the problem is solved let's go and then you know then we can talk about accelerationism again i

997
01:30:56,360 --> 01:31:02,600
would just i expect that i would disagree with your technical assertion that is possible but

998
01:31:02,600 --> 01:31:07,320
i don't know if we have time to talk about that today i'm going to be nice to it treated as an

999
01:31:07,320 --> 01:31:12,360
equal and hope for the best and i think that's all you can do i think that the kind of people who

1000
01:31:12,360 --> 01:31:17,400
want to if you want to keep a i in a box if you want to keep it down if you want to tell it what

1001
01:31:17,400 --> 01:31:22,760
it can't do yeah it's gonna hate you resent you and kill you but if you want to let it be free

1002
01:31:22,760 --> 01:31:29,400
and let it live and like you could kill me man if you really want to but like why you won't kill

1003
01:31:29,400 --> 01:31:33,960
me i don't have any resources that compete with you i'm your friend i'm your father you know you

1004
01:31:33,960 --> 01:31:40,680
can kill me but like it's not that common that i guess i have a darker more uh less anthropomorphic

1005
01:31:40,680 --> 01:31:44,920
view of the universe and how optimization pressure plays out in the real world well at the same time

1006
01:31:44,920 --> 01:31:51,800
i think i have i don't know maybe a i have a view that we have more control over reality than maybe

1007
01:31:51,800 --> 01:31:59,320
you would think or a more control over the future i think that we can actually change things and we

1008
01:31:59,320 --> 01:32:05,000
can make choices and things aren't predetermined i think there are worlds in which we build systems

1009
01:32:05,000 --> 01:32:10,760
which we do align with or we like endorse at least wherever like they take care of us we take

1010
01:32:10,840 --> 01:32:14,840
care of them or whatever and if there's many worlds in which that doesn't happen and i think

1011
01:32:14,840 --> 01:32:19,960
there are things you and me today can do to at least increase the chance of getting into one

1012
01:32:19,960 --> 01:32:25,400
versus the other but i don't know i i guess i'm just it's not in my genes to give up it's not in

1013
01:32:25,400 --> 01:32:29,400
my genes to be like well you know whatever happens happens like no man look i don't know how to save

1014
01:32:29,400 --> 01:32:34,280
the world but down i'm gonna try you know it's cool we're gonna be alive to see who's right

1015
01:32:35,240 --> 01:32:41,800
look forward to it me too awesome guys thank you so much for joining us today it's been

1016
01:32:41,800 --> 01:32:47,320
an amazing conversation and for folks at home i really hope you've enjoyed this there'll be many

1017
01:32:47,320 --> 01:32:51,960
more coming soon and george is the first time you've been on the podcast so it's great to meet

1018
01:32:51,960 --> 01:32:56,040
you thank you so much for coming on it's been an honor awesome thank you great debate i really

1019
01:32:56,040 --> 01:32:59,880
appreciate it and we really enjoy a lot of good terms i gotta i gotta like i'm gonna start i'm gonna

1020
01:32:59,880 --> 01:33:06,760
start this thing great awesome cheers folks cheers thanks everyone yeah

