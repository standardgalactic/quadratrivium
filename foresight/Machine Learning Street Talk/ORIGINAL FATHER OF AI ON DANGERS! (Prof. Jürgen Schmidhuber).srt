1
00:00:00,000 --> 00:00:08,400
my fondest memory. It's usually when I discover something that I think nobody has seen before,

2
00:00:09,440 --> 00:00:15,760
but that happens very rarely because most of the things you think of somebody else has done before.

3
00:00:16,400 --> 00:00:21,840
This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact

4
00:00:21,840 --> 00:00:28,000
with your skills? Do you love competing against the best minds in the world? Well, introducing

5
00:00:28,000 --> 00:00:34,400
Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.

6
00:00:34,400 --> 00:00:40,320
Numeri combines a competitive data science tournament with powerful, clean stock market data

7
00:00:40,320 --> 00:00:45,200
enabling you to predict the market like never before. Sign up now, become part of the elite

8
00:00:45,200 --> 00:00:49,680
community, taking the stock market by storm and I'll see you on the leaderboard.

9
00:00:49,760 --> 00:00:59,360
Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.

10
00:00:59,360 --> 00:01:05,920
We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,

11
00:01:05,920 --> 00:01:11,360
the researcher responsible for leading the research groups which invented much of the technology

12
00:01:11,360 --> 00:01:16,080
which has powered the deep learning revolution. It's long been a dream to get you on the podcast,

13
00:01:16,160 --> 00:01:20,960
you again. It feels like the day has finally arrived, so welcome to MLST.

14
00:01:20,960 --> 00:01:26,560
Thank you, Tim, for these very kind words and this very generous introduction.

15
00:01:29,840 --> 00:01:34,800
So on that, let's discuss the credit assignment problem in machine learning. Now,

16
00:01:34,800 --> 00:01:40,800
you've dedicated a significant amount of time researching and publishing the actual history

17
00:01:40,800 --> 00:01:45,360
of the field and there's a significant divergence between the public narrative

18
00:01:45,360 --> 00:01:50,960
and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your

19
00:01:50,960 --> 00:01:56,400
accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading

20
00:01:56,400 --> 00:02:00,800
your history of the breakthroughs in machine learning, going back to ancient times and of course even

21
00:02:00,800 --> 00:02:06,080
remarking on the very first computer scientist, Leibniz. And for example, you pointed out the

22
00:02:06,080 --> 00:02:11,440
history of who invented backprop and the CNN. And you explained that there wasn't really

23
00:02:11,440 --> 00:02:16,400
a neural network winter at all in the 1970s. So could you just sketch out a little bit of that

24
00:02:16,400 --> 00:02:30,400
history? So that's a challenge. Actually, computer science history and computing history started

25
00:02:30,400 --> 00:02:40,560
maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.

26
00:02:40,560 --> 00:02:49,760
That was 2000 years ago in the first century basically. And he basically built an automaton

27
00:02:49,760 --> 00:02:58,720
that was programmed through a cable which was wrapped around a rotating cylinder which had

28
00:02:58,720 --> 00:03:06,480
certain knobs and then there was a weight which pulled it down and the whole apparatus

29
00:03:06,480 --> 00:03:14,880
was able to direct the movements of little robots, of little puppets in an automatic theater.

30
00:03:16,000 --> 00:03:24,800
That, as far as I know, was the first program-controlled machine in the history of mankind.

31
00:03:24,800 --> 00:03:31,920
Even before that there were other machines. The ancient Greeks had even earlier the

32
00:03:32,720 --> 00:03:40,880
Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently

33
00:03:43,200 --> 00:03:49,680
we have seen many additional advances and you mentioned Leibniz, of course, who is of

34
00:03:49,680 --> 00:03:56,320
special interest to our field because he not only is called the first computer scientist

35
00:03:56,320 --> 00:04:05,280
because he had the first machine with a memory that was in the 1680s, I think. He not only had the

36
00:04:07,600 --> 00:04:15,200
first machine that could do all the basic arithmetic operations which are addition,

37
00:04:15,200 --> 00:04:25,920
multiplication, division and subtraction, then he not only had these first ideas for a universal

38
00:04:25,920 --> 00:04:32,240
problem solver that would solve all kinds of questions, even philosophical questions,

39
00:04:32,880 --> 00:04:42,000
just through computation. And he not only was the first who had this algebra of thought which

40
00:04:42,000 --> 00:04:51,040
is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,

41
00:04:51,040 --> 00:04:56,240
but especially in our field in deep learning he contributed something essential, which is really

42
00:04:56,240 --> 00:05:04,640
central for this field, which is the chain rule. I think 1676, that's when he published that and

43
00:05:04,640 --> 00:05:15,440
that's what is now being used to train very deep artificial neural networks and also shallow

44
00:05:15,440 --> 00:05:21,920
neural networks and recurrent neural networks. And everything that we are using in modern AI

45
00:05:21,920 --> 00:05:30,000
is really in many ways depending on that early work. But then of course there was so much additional

46
00:05:30,000 --> 00:05:40,640
work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss

47
00:05:40,640 --> 00:05:49,680
and Legendre had the linear neural networks, the linear perceptrons in the sense that they were

48
00:05:49,680 --> 00:05:59,920
linear without having any non-differential aspect to it. So these first neural networks,

49
00:06:00,560 --> 00:06:12,640
back then, were called method of least squares. And the training method was regression and the

50
00:06:12,640 --> 00:06:18,080
error function was exactly the same that we use today. And it was basically just a network with

51
00:06:18,080 --> 00:06:24,560
a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you

52
00:06:24,560 --> 00:06:32,960
could learn to adjust the weights of these connections. So that was the first linear neural

53
00:06:32,960 --> 00:06:42,800
network and many additional later developments led to what we have today. You had this beautiful

54
00:06:42,800 --> 00:06:47,280
statement. You said that machine learning is the science of credit assignment and we should apply

55
00:06:47,920 --> 00:06:53,520
that same science to the field itself. And I guess what I'm really curious about is

56
00:06:54,240 --> 00:06:59,280
first, if you could educate our listeners just a bit on what credit assignment is in the context

57
00:06:59,280 --> 00:07:04,800
of, say, machine learning and why you think it's important that that should apply to the field

58
00:07:04,800 --> 00:07:09,040
in general. You know, why should we care about credit assignment? Why should we study the history

59
00:07:09,040 --> 00:07:16,560
of the developments in the field? Why is it important? I'm interested in credit assignment,

60
00:07:17,200 --> 00:07:22,320
not only in machine learning, but also in the history of machine learning,

61
00:07:23,120 --> 00:07:31,200
because machine learning itself is the science of credit assignment. What does that mean? Suppose

62
00:07:31,200 --> 00:07:39,760
you have a complicated machine, which is influencing the world in a way that leads to the solution

63
00:07:39,760 --> 00:07:45,200
of a problem. And maybe the machine solves the problem. But then the big question is,

64
00:07:45,200 --> 00:07:52,720
which of the components of these many components were responsible? Some of them were active

65
00:07:52,720 --> 00:08:01,760
a long time ago and others later and early actions set the stage for later actions. Now,

66
00:08:02,960 --> 00:08:06,880
if you want to improve the performance of the machine, you should figure out how

67
00:08:07,600 --> 00:08:16,000
did the components contribute to the overall success. And this is what credit assignment is

68
00:08:16,000 --> 00:08:22,000
about. And in machine learning in general, we have a system consisting of many

69
00:08:24,000 --> 00:08:32,640
machine learning engineers and mathematicians and hardware builders and all kinds of people.

70
00:08:32,640 --> 00:08:38,560
And there you also would like to figure out which parts of the system are responsible for later

71
00:08:38,560 --> 00:08:44,800
successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.

72
00:08:44,800 --> 00:08:50,960
And I think the way I think about it is you've got this giant architecture of humanity and in it

73
00:08:50,960 --> 00:08:55,760
are these certain nodes that may be an individual, maybe a research group. And if they come up with

74
00:08:55,760 --> 00:09:01,600
things that are very helpful, right, you want to try and direct more attention, more resources,

75
00:09:02,000 --> 00:09:08,960
at that nodule, at that node, right, because it's likely to come up with additional very

76
00:09:08,960 --> 00:09:14,080
important things. And if we don't get that right, we're just not optimizing the algorithm of science

77
00:09:14,080 --> 00:09:24,160
as a whole. That's right, yes. Machine learning and science in general is based on this principle

78
00:09:24,160 --> 00:09:31,360
of credit assignment where credit usually doesn't come in form of money, sometimes also in form

79
00:09:31,360 --> 00:09:41,440
of money, but in form of reputation. And then the whole system is set up such that you create

80
00:09:41,440 --> 00:09:52,080
an incentive for people who have worked on improving some method to credit those who

81
00:09:52,080 --> 00:09:59,200
maybe came up with the original method and to just have these chains of credit assignment

82
00:09:59,840 --> 00:10:07,280
that make clear who did what, when, because the whole system is based on this incentive.

83
00:10:07,280 --> 00:10:16,480
And yes, those who are then credited with certain valuable contributions, they also can get

84
00:10:16,480 --> 00:10:23,360
reasonable jobs within the economy and so on. But that's more like the secondary

85
00:10:23,760 --> 00:10:31,920
consequence of the basic principle. And that's why all PhD advisors

86
00:10:34,080 --> 00:10:41,120
teach their PhD students to be meticulous when it comes to credit assignment to past work.

87
00:10:42,320 --> 00:10:48,560
So one last question, if I may, I've really enjoyed studying the history of advancement

88
00:10:49,520 --> 00:10:54,320
because I found that when I go back and read original source materials, let's say

89
00:10:54,960 --> 00:11:01,200
Einstein's first paper on diffusion or anything like that, because they're breaking new ground,

90
00:11:01,200 --> 00:11:08,320
they're considering a wider array of possibilities. And then over time, the field becomes more and

91
00:11:08,320 --> 00:11:14,480
more focused on a narrower avenue of that. And you can go back and look at the original work

92
00:11:14,560 --> 00:11:19,120
and actually gain a lot of inspiration for alternative approaches or alternative

93
00:11:19,680 --> 00:11:25,440
considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.

94
00:11:25,440 --> 00:11:30,080
Sometimes we need to go back to go down a different branch of the tree, if you will,

95
00:11:30,080 --> 00:11:35,680
and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.

96
00:11:35,840 --> 00:11:47,440
Yes, science in general is about failure. And 99% of all scientific activity is about

97
00:11:49,200 --> 00:11:52,080
creating failures. But then you learn from these

98
00:11:54,240 --> 00:12:00,080
failures and you do backtracking. And you go back to a previous decision point where you maybe

99
00:12:01,040 --> 00:12:07,680
made the wrong decision and pursued the wrong avenue. But now you have a branching point and

100
00:12:07,680 --> 00:12:17,360
you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very

101
00:12:17,360 --> 00:12:23,440
far usually. You just go back to a recent paper which came out five months ago. And maybe you

102
00:12:23,440 --> 00:12:28,640
have a little improvement there. And then maybe there's yet another little improvement there.

103
00:12:29,200 --> 00:12:33,520
And some parts of our field are at the moment a little bit like that,

104
00:12:33,520 --> 00:12:40,320
where PhD students are moving in, who just look at the most recent papers and then find a way of

105
00:12:40,320 --> 00:12:49,440
improving it a little bit and 2% better results on this particular benchmark. And then the same guys

106
00:12:49,440 --> 00:12:56,320
are also reviewing at major conferences, papers by similar students and so on. And so then sometimes

107
00:12:56,320 --> 00:13:05,680
what happens is that no very deep backtracking is happening, just because the actors aren't really

108
00:13:06,720 --> 00:13:13,280
aware of the entire search tree that has already been explored in the past.

109
00:13:14,240 --> 00:13:22,320
On the other hand, science has this way of healing itself. And since you can gain reputation by

110
00:13:22,720 --> 00:13:32,080
identifying maybe more relevant points, branching points, you have this incentive within the whole

111
00:13:32,080 --> 00:13:41,360
system to improve things as much as you can, sometimes by going back much further.

112
00:13:41,920 --> 00:13:52,080
So there's been a lot of discussion in the discourse around this concept of AI existential

113
00:13:52,080 --> 00:13:57,920
risk. And you again, you've published quite a few pieces about this recently, prominently in

114
00:13:57,920 --> 00:14:04,160
The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept

115
00:14:04,160 --> 00:14:10,800
of recursive self-improvement, because that seems to be one of the plausible explanations that these

116
00:14:10,800 --> 00:14:15,840
folks give. And of course, when it comes to recursive self-improvement, you are an expert in

117
00:14:15,840 --> 00:14:23,200
this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes

118
00:14:23,200 --> 00:14:31,200
and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?

119
00:14:31,200 --> 00:14:44,240
In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,

120
00:14:44,240 --> 00:14:50,480
that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about

121
00:14:50,480 --> 00:14:58,880
machine that learns something in a domain. But not only that, it also learns on top of that to

122
00:14:59,680 --> 00:15:08,400
learn a better learning algorithm based on experience and the lower level domains. And then

123
00:15:08,400 --> 00:15:18,400
also recursively learns to improve the way it improves the way it learns. And then also recursively

124
00:15:18,960 --> 00:15:26,000
learns to improve the way it improves the way it improves the way it learns. And yeah, I called that

125
00:15:26,000 --> 00:15:35,440
meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement

126
00:15:35,440 --> 00:15:43,440
in the recursive way, although it is always limited by the limited time that you run the system like

127
00:15:43,440 --> 00:15:54,080
that. And then, of course, the motivation behind that is that you don't want to have an artificial

128
00:15:54,080 --> 00:16:01,760
system that is stuck always with the same old human-designed learning algorithm. No, you want

129
00:16:01,760 --> 00:16:09,200
something that improves that learning algorithm without any limitations, except for the limitations

130
00:16:09,200 --> 00:16:20,160
of physics and computability. And so much of what I have been doing since then is really about that.

131
00:16:20,240 --> 00:16:27,040
Self-improvement in different settings where you have, on the one hand, reinforcement learning

132
00:16:27,040 --> 00:16:35,440
systems that learn in an environment to better interact and better create ways of learning

133
00:16:35,440 --> 00:16:46,880
from these interactions to learn faster and to learn to improve the way of learning faster,

134
00:16:46,960 --> 00:16:54,320
and so on. And then also gradient-based systems, artificial neural networks, that learn through

135
00:16:55,520 --> 00:17:02,480
gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better

136
00:17:02,480 --> 00:17:10,640
learning algorithm that works better in a given set of environments than the original human-designed

137
00:17:10,640 --> 00:17:19,440
one. And yeah, that started around 1992 neural networks that learned to run their own learning

138
00:17:19,440 --> 00:17:29,040
algorithms on the recurrent network themselves. So you have a network which has standard connections

139
00:17:29,040 --> 00:17:34,640
and input units and output units, but then you have these special output units which are used to

140
00:17:34,640 --> 00:17:43,360
address connections within the system, within this recurrent network, and they can read and

141
00:17:43,360 --> 00:17:50,480
write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose

142
00:17:50,480 --> 00:18:00,720
computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary

143
00:18:00,720 --> 00:18:06,560
learning algorithms that translate incoming signals, not only the input signals, but also the

144
00:18:06,560 --> 00:18:14,640
evaluation signals like reinforcement signals or error signals into weight changes, fast weight

145
00:18:14,640 --> 00:18:24,000
changes, where the weight changes are not dictated any longer through this gradient descent method,

146
00:18:24,080 --> 00:18:31,840
but no, now the network itself is learning to do that. But the initial weight matrix is still

147
00:18:33,200 --> 00:18:37,600
learned through gradient descent, which is propagating through all these self-referential

148
00:18:37,600 --> 00:18:45,520
dynamics in a way that improves the learning algorithm running on the network itself. That

149
00:18:45,520 --> 00:18:52,080
was 1992, and back then, compute was really, really slow, it was a million times more expensive

150
00:18:52,160 --> 00:18:58,080
than today, and you couldn't do much with it. But now, in recent works, all of that is working

151
00:18:58,080 --> 00:19:05,760
out really nicely and has become popular, and we have, just if you look at the past few years,

152
00:19:05,760 --> 00:19:12,880
a whole series of papers just on that. So that's the fast weight programming that you're referring

153
00:19:12,880 --> 00:19:21,680
to? Yes, so it's fast weight programmers where you have a part of the network that

154
00:19:21,680 --> 00:19:30,080
learns to quickly reprogram another part of the network, or the original version of that was

155
00:19:30,080 --> 00:19:35,040
actually two networks, so one is a slow network, and then there's another one, a fast network,

156
00:19:35,040 --> 00:19:42,560
and the slow network learns to generate weight changes for the second network,

157
00:19:43,360 --> 00:19:50,880
and the program of the second network are its weights. So the weight matrix of the second

158
00:19:50,880 --> 00:19:56,080
network, that is the program of the second network, and the first one, what does it do? It

159
00:19:56,800 --> 00:20:04,720
generates outputs, it learns to generate outputs that cause weight changes in the second network,

160
00:20:04,720 --> 00:20:09,840
and these weight changes are being applied to patterns, to input patterns, to queries, for

161
00:20:09,840 --> 00:20:19,200
example, and then the first network essentially learns to program the second network, and essentially

162
00:20:19,200 --> 00:20:25,440
the first network has a learning algorithm for the second network, and the first system of that

163
00:20:25,440 --> 00:20:34,560
kind, 1991, that was really based on on keys and values, so the first network learns to program

164
00:20:34,560 --> 00:20:41,040
the second network by giving it keys and values, and it says now take second network, take this

165
00:20:41,040 --> 00:20:48,960
key and this value, and associate both of them through an outer product, which just means that

166
00:20:48,960 --> 00:20:55,600
those units are strongly active, they get connected through stronger connections, and

167
00:20:56,880 --> 00:21:02,320
the mathematical way of describing that is the outer product between key and value.

168
00:21:03,840 --> 00:21:08,480
So that's how the first network would program the second network, and the important thing was that

169
00:21:08,480 --> 00:21:15,520
the first network had to invent good keys and good values, depending on the context of the input

170
00:21:15,520 --> 00:21:23,360
stream coming in, so it used the context to generate what is today called an attention mapping,

171
00:21:23,360 --> 00:21:32,640
which is then being applied to queries, and this was a first step right before the most general

172
00:21:34,080 --> 00:21:40,400
next step, which is then really about learning a learning algorithm running on the network itself

173
00:21:40,400 --> 00:21:43,600
for the weights of the network itself.

174
00:21:46,880 --> 00:21:53,520
Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?

175
00:21:53,520 --> 00:22:00,240
Now one of the reasons I think why the X-Risk people believe that it will just go on forever

176
00:22:00,240 --> 00:22:06,080
is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in

177
00:22:06,080 --> 00:22:12,400
the real world, and I'm quite amenable to this ecological idea of intelligence that it does,

178
00:22:12,400 --> 00:22:16,400
the world is a computer basically as well as the actual brain that we're building,

179
00:22:17,440 --> 00:22:24,000
so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations

180
00:22:24,000 --> 00:22:35,120
would be? So you are talking about the ongoing acceleration of computing power and limitations

181
00:22:35,200 --> 00:22:41,840
thereof, is that what you have in mind here? Well that's one part of it, so even if you

182
00:22:41,840 --> 00:22:46,720
just scale transformers I think there would be some kind of asymptote, but we're talking here

183
00:22:46,720 --> 00:22:52,560
about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's

184
00:22:52,560 --> 00:22:57,680
similar to this idea of reflection, self-reflection and language models, it actually improves the

185
00:22:57,680 --> 00:23:03,360
performance with successive steps of reflection and then it levels off, it reaches an asymptote.

186
00:23:03,520 --> 00:23:07,920
I just believe that there are asymptotes everywhere and that's the reason why I

187
00:23:07,920 --> 00:23:11,680
don't think recursive self-improvement will go on forever, but I just wondered if you had

188
00:23:11,680 --> 00:23:15,920
any intuitions on what those impressions are. Yeah, you are totally right, there are certain

189
00:23:17,200 --> 00:23:24,320
algorithms that we have discovered in past decades which are already optimal in a way

190
00:23:25,440 --> 00:23:31,840
such that you cannot really improve them any further, and no self-improvement and no fancy

191
00:23:32,080 --> 00:23:39,360
machine will ever be able to further improve them. There are certain sorting algorithms that

192
00:23:39,360 --> 00:23:46,720
under given limitations are optimal and you can further improve them. That's one of the limits.

193
00:23:46,720 --> 00:23:53,920
Then of course there are the fundamental limitations of what's computable, first identified by

194
00:23:53,920 --> 00:24:03,040
Kurt Gödel in 1931, he just showed that there are certain things that no computational process

195
00:24:03,600 --> 00:24:13,600
can ever achieve. No computational theorem prover can prove or disprove certain theorems

196
00:24:13,600 --> 00:24:22,720
in a language, in a symbolic language that is powerful enough to encode

197
00:24:22,720 --> 00:24:29,760
certain simple principles of arithmetic and stuff like that. What he showed was that

198
00:24:31,040 --> 00:24:36,800
there are fundamental limitations to all of computation and therefore there are fundamental

199
00:24:36,800 --> 00:24:47,280
limitations to any AI based on computation. I'm glad you brought that topic up because it's one of

200
00:24:48,240 --> 00:24:54,800
our favorite things to discuss which is do you think the human mind ultimately reduces to just

201
00:24:54,800 --> 00:25:00,080
an effective computation and so subject to those same limits or do you think there's any

202
00:25:01,840 --> 00:25:07,920
known or unknown physics that give us some out in which the brain can do a computation that

203
00:25:07,920 --> 00:25:18,720
amounts to hypercomputation? Since we have no evidence that the brain can compute something

204
00:25:19,280 --> 00:25:27,520
that is not computable in the traditional sense, in Gödel sense and torings and churches sense

205
00:25:27,520 --> 00:25:36,480
and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the

206
00:25:36,560 --> 00:25:44,880
case. As soon as someone shows that people can compute certain things or prove certain theorems

207
00:25:44,880 --> 00:25:55,840
that machines cannot prove given the same initial conditions, we should look more closely but

208
00:25:56,960 --> 00:26:04,080
there are many things that might be possible in fairy tales and we are not really exploring them

209
00:26:04,080 --> 00:26:12,240
because the probability of coming up with interesting results is so low. Fair enough,

210
00:26:12,240 --> 00:26:18,000
so you mentioned so far two asymptotes, one being of the mathematical kind where there's just

211
00:26:18,560 --> 00:26:22,880
mathematical proofs that certain things are optimal, the other one being the limits of

212
00:26:22,880 --> 00:26:29,680
computation itself. What other asymptotes do you see applying to or putting bounds on recursive

213
00:26:29,680 --> 00:26:42,080
self-improvement? The most obvious thing is probably light speed and the limits of physical

214
00:26:42,080 --> 00:26:51,760
computation. We know those for several decades, we have happily enjoyed the fact that every five

215
00:26:51,760 --> 00:27:00,960
years compute is getting 10 times cheaper and this process started long before Moore's law was

216
00:27:02,960 --> 00:27:11,360
defined in the 60s I believe because even in 1941 already when Susie built the first program

217
00:27:11,360 --> 00:27:19,360
controlled computer this law apparently was active so back then he could compute maybe one

218
00:27:19,360 --> 00:27:25,200
instruction per second and since then every 10 years a factor of 100 every 30 years a factor of

219
00:27:25,200 --> 00:27:35,520
a million more or less until today and there's no reason to believe it won't hold for a couple

220
00:27:35,520 --> 00:27:43,440
of additional decades because the physical limits are much further out. The physical limits that we

221
00:27:43,440 --> 00:27:53,120
know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that

222
00:27:53,120 --> 00:27:58,640
one kilogram of matter cannot compute more than 10 to the 51 instructions per second.

223
00:28:00,080 --> 00:28:07,360
So that's a lot of compute but it's limited and to give you an idea of how much compute that is

224
00:28:08,000 --> 00:28:16,240
I also have a kilogram of computer in here and probably it cannot compute 10 to the 20

225
00:28:16,240 --> 00:28:24,240
instructions per second otherwise my head would explode because of the heat problem

226
00:28:25,840 --> 00:28:31,680
but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10

227
00:28:31,680 --> 00:28:39,280
to the 17 something like that although most of my neurons are not active as we speak because again

228
00:28:39,280 --> 00:28:47,760
otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions

229
00:28:47,760 --> 00:28:57,520
per brain then the upper limit of all of humankind would be 10 billion times that individual limit

230
00:28:57,600 --> 00:29:04,480
and that would be 10 to the 30 instructions per second and you see it's still far away from the

231
00:29:04,480 --> 00:29:11,520
10 to the 51 instructions per second that in principle one kilogram of matter could compute

232
00:29:11,520 --> 00:29:20,640
and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some

233
00:29:21,600 --> 00:29:29,120
and so if the current trend continues at some point much of that is going to be used for

234
00:29:29,120 --> 00:29:35,840
computation but then it will have to slow down even if the exponential acceleration

235
00:29:36,880 --> 00:29:44,160
will still be with us for a couple of decades because at some point it is going to be a polynomial

236
00:29:44,720 --> 00:29:52,400
because due to the limits of light speed at some point it will be harder and harder

237
00:29:52,400 --> 00:29:57,840
to acquire additional mass once you have reached the limits of physical computation per kilogram

238
00:29:57,840 --> 00:30:04,560
the only way to expand is to go outwards and you know find additional stars and additional

239
00:30:04,560 --> 00:30:16,480
matter further away from the solar system and then you will get a polynomial acceleration or

240
00:30:16,480 --> 00:30:23,520
a polynomial growth at best so it will be much worse than the current exponential

241
00:30:23,520 --> 00:30:31,440
growth that we are still enjoying. Sure but I would say you know the existential threat

242
00:30:31,520 --> 00:30:37,520
that is more than sufficient to supply an existential threat and let me just put this

243
00:30:37,520 --> 00:30:41,760
a little bit differently which is and I agree with you on this which is you are quoted as

244
00:30:41,760 --> 00:30:46,000
saying that traditional humans won't play a significant role in spreading intelligence

245
00:30:46,000 --> 00:30:50,720
across the universe and I think you are right I think we kind of share a vision of something

246
00:30:50,720 --> 00:30:57,680
like the von Neumann probes that go out into space and form this star spanning civilization of

247
00:30:57,680 --> 00:31:03,120
machines and artificial intelligence that have transcended you know biological limitations

248
00:31:03,120 --> 00:31:09,120
so I guess my question to you is once that space faring star spanning you know civilization

249
00:31:09,120 --> 00:31:16,480
exists if it becomes misaligned with us and decides that we are in the way right isn't that

250
00:31:16,480 --> 00:31:21,280
an existential threat I mean might they just you know repurpose the earth regardless of whether

251
00:31:21,280 --> 00:31:28,240
we're here or not for for their own aims yeah I'm often getting these questions and

252
00:31:29,440 --> 00:31:40,480
and there is no proof that we will be safe forever or something like that on the other hand it's also

253
00:31:41,840 --> 00:31:50,400
very clear as far as I can judge that all of this cannot be stopped and it can be channeled

254
00:31:50,400 --> 00:32:00,800
in a very natural and I think good way in a way that is good for humankind now

255
00:32:02,400 --> 00:32:08,880
first of all at the moment we have a tremendous bias towards good AI

256
00:32:11,200 --> 00:32:18,880
meaning AI that is good for humans why because there is this intense commercial pressure

257
00:32:18,880 --> 00:32:26,000
to create stuff that humans want to buy and they like to buy only stuff they think is good

258
00:32:26,000 --> 00:32:34,000
for them which means that all the companies that are and that are trying to devise AI products

259
00:32:34,000 --> 00:32:42,240
they are maximally incentivized to generate AI products that are good for those guys who are

260
00:32:42,240 --> 00:32:50,640
buying them or at least where the where the customers think it's good for them

261
00:32:52,080 --> 00:32:59,920
so it is still 95 so it may be five percent of all AI researchers really about AI weapons and

262
00:32:59,920 --> 00:33:06,000
one has to be worried about that when all this has to be worried about weapons research but

263
00:33:06,000 --> 00:33:10,800
there's a tremendous bias towards good AI so that is one of the reasons why you can be

264
00:33:11,440 --> 00:33:17,920
a little bit optimistic for the future I'm always trying to point out the two types of

265
00:33:17,920 --> 00:33:31,840
AIs there are those who are just tools of users human human users and the others that invent

266
00:33:31,840 --> 00:33:39,520
their own goals and they pursue their own goals and both of them we have had for a long time

267
00:33:40,160 --> 00:33:46,400
now for the AI tools it's kind of clear there's a human and a human wants to achieve something

268
00:33:46,400 --> 00:33:55,680
and so it uses he uses or she uses that tool to achieve certain ends and and most of those are

269
00:33:56,320 --> 00:34:06,000
of the type let's improve healthcare and let's facilitate translation from one person to another

270
00:34:06,000 --> 00:34:13,360
one in another nation and just make life easier and make human lives longer and healthier

271
00:34:14,960 --> 00:34:22,160
okay so that that's the AI tools but then there are the other AIs which also have existed in my

272
00:34:22,160 --> 00:34:30,240
lab for at least 32 years which invent their own goals and they are a little bit like little

273
00:34:30,240 --> 00:34:39,920
scientists where you have an incentive to explore the environment through actions through

274
00:34:39,920 --> 00:34:45,200
experiments self-invented experiments that tell you more about how the world works such that you

275
00:34:45,200 --> 00:34:49,760
can become a better and better and more and more general problem solver in that world

276
00:34:50,480 --> 00:34:58,080
and so these AIs they have for a long time created their own goals and now of course

277
00:34:58,080 --> 00:35:05,920
the interesting question is these more interesting AIs what are they going to do once they are

278
00:35:08,800 --> 00:35:14,320
once they have been scaled up and can compete or maybe outperform humans and everything

279
00:35:15,440 --> 00:35:23,040
they want to achieve so on the one hand the AI tools and there the greatest worry is

280
00:35:23,280 --> 00:35:31,760
what are the other humans going to do to me with their AI tools so in the extreme case you have

281
00:35:31,760 --> 00:35:39,600
people who are using AI weapons against you and maybe your neighbor is has bought a little drone

282
00:35:39,600 --> 00:35:47,600
for 300 dollars and it has face recognition and it has a little gripper and it flies across the

283
00:35:47,600 --> 00:35:55,760
hedge and puts some poison into your coffee or something like that so then the problem is not

284
00:35:55,760 --> 00:36:03,120
the AI which is trying to enslave humans or something silly like that no it's your neighbor

285
00:36:03,120 --> 00:36:12,080
or the other human and generally speaking you have to be much more afraid of other humans than you

286
00:36:12,080 --> 00:36:22,880
have to be of AIs even those who define or set themselves their own goals because you must mostly

287
00:36:23,520 --> 00:36:31,600
worry about those with whom you share goals so if you share goals then suddenly there is a potential

288
00:36:31,600 --> 00:36:38,960
of conflict because maybe there is only one schnitzel over there and two persons want to

289
00:36:38,960 --> 00:36:45,600
eat the schnitzel and suddenly they have a reason to fight against each other generally speaking

290
00:36:45,600 --> 00:36:56,000
if you share goals then you can do two things you can either collaborate or compete an extreme form

291
00:36:56,000 --> 00:37:06,560
of collaboration would be to maybe marry another person and set up a family and master life together

292
00:37:06,960 --> 00:37:21,200
and an extreme form of competition would be war and and those who share goals they have many more

293
00:37:21,200 --> 00:37:31,360
incentives to interact than those who don't share goals and so humans are mostly interested in other

294
00:37:31,360 --> 00:37:38,400
humans because they share similar goals and because they give them a reason to collaborate or to

295
00:37:38,400 --> 00:37:46,000
compete most CEOs of certain companies are interested in other CEOs of competing companies

296
00:37:46,000 --> 00:37:52,160
and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs

297
00:37:52,160 --> 00:37:57,440
of the future who set themselves their own goals they will be mostly interested in other super

298
00:37:57,440 --> 00:38:05,600
smart AIs of the future who set themselves their own goals generally speaking there is not so much

299
00:38:05,600 --> 00:38:13,840
competition and there are not so many shared goals between biological beings such as humans

300
00:38:13,840 --> 00:38:22,320
and a new type of life that as you mentioned can expand into the universe and can multiply

301
00:38:22,400 --> 00:38:28,880
in a way that is completely infeasible for biological beings so there's a certain

302
00:38:30,000 --> 00:38:35,600
long-term protection at least through lack of interest on the other side

303
00:38:38,720 --> 00:38:44,640
okay brilliant there's a few things I wanted to touch on there we will get on to what it means

304
00:38:44,640 --> 00:38:53,040
for goals to emerge from systems later and you started off by saying that humans will buy

305
00:38:53,040 --> 00:38:58,640
products that make them feel good and Facebook is quite an interesting example to play with

306
00:38:58,640 --> 00:39:04,000
actually because Facebook is a little bit like an AI system which is a collective intelligence

307
00:39:04,000 --> 00:39:09,680
and humans use Facebook but they have some idea that it might cause them harm and the thing with

308
00:39:09,760 --> 00:39:15,600
population ethics is we know that our moral reasoning kind of decays over space and even more

309
00:39:15,600 --> 00:39:20,880
so over time and part of the reason why time is so difficult is because it's predictive we don't

310
00:39:20,880 --> 00:39:26,400
actually know what's going to happen in the future so our kind of reasoning about establishing

311
00:39:26,400 --> 00:39:30,960
what the value of something is is very very faulty and I think that's one of the reasons why

312
00:39:30,960 --> 00:39:36,080
these people would say that we don't really know what's good for us I do completely agree with you

313
00:39:36,080 --> 00:39:44,640
though that the problem I think is humans rather than AIs on their own yes these are good points

314
00:39:54,400 --> 00:40:04,480
feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't

315
00:40:04,480 --> 00:40:16,640
it when you discuss the limitations of what's predictable and um and how people often fail

316
00:40:16,640 --> 00:40:25,120
to see what's good for them well I think maybe so you've already you've already um said that

317
00:40:25,120 --> 00:40:31,920
there's no proof that we'll be safe forever right like I mean there could there could come an

318
00:40:31,920 --> 00:40:37,760
existential risk you know from AI so I think my question to you is do you have sympathy for

319
00:40:37,760 --> 00:40:44,160
the folks who say we need to be putting more resources into researching alignment like we need

320
00:40:44,160 --> 00:40:52,720
to develop the tools um in order to allow it to be easier for people to construct AI that is aligned

321
00:40:52,720 --> 00:40:58,160
for the goals and to make sure that you know that it doesn't that it doesn't have unintended

322
00:40:58,640 --> 00:41:03,120
consequences like in other words there may not be a proof that we can go forever and be

323
00:41:03,120 --> 00:41:08,720
safe for AI but we at least want to develop the basic mechanics that we need to safely

324
00:41:09,520 --> 00:41:17,440
develop and deploy AI don't we yes and I sympathize with those who um are devoting their

325
00:41:17,440 --> 00:41:24,000
lives to alignment issues and trying to build AIs aligned with humans

326
00:41:24,000 --> 00:41:36,320
I view them as part of the evolution of all kinds of other ideas that come up as not only

327
00:41:37,040 --> 00:41:41,520
nations compete with other nations but companies compete with other companies

328
00:41:41,520 --> 00:41:47,040
and shareholders of different companies compete with shareholders of different companies and so on

329
00:41:48,000 --> 00:41:56,240
and so there is such a huge set of different human goals which are not aligned with each other

330
00:41:56,880 --> 00:42:04,800
that makes me doubt that you will come up with a general system that all humans can accept

331
00:42:05,600 --> 00:42:13,440
simply because if you put 10 humans in a room and ask them what is good they will give you

332
00:42:13,440 --> 00:42:22,880
10 different opinions however I sympathize with with this goal and it's good that people are

333
00:42:22,880 --> 00:42:30,560
worried and they spend resources on solving some of these issues in the long run however

334
00:42:31,680 --> 00:42:39,920
I think there is no way of stopping all kinds of AIs from having all kinds of

335
00:42:39,920 --> 00:42:48,320
goals that have very little to do with humans the universe itself is built in a certain way

336
00:42:49,120 --> 00:42:50,080
that apparently

337
00:42:53,680 --> 00:42:59,680
derives it from very simple initial conditions to more and more complexity

338
00:43:00,720 --> 00:43:08,000
and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear

339
00:43:08,000 --> 00:43:13,680
that this cannot be the end of it because the universe is still young it's going to be much

340
00:43:13,680 --> 00:43:24,240
older than it is now now there is this drive built in drive of the cosmos to become more complex

341
00:43:24,240 --> 00:43:29,360
and it seems clear that civilization a civilization like ours is

342
00:43:29,520 --> 00:43:37,680
is a stepping stone on to war it's something that is more complex and

343
00:43:38,480 --> 00:43:43,360
could I touch on a couple of things here the bootloader example is kind of where I want to go

344
00:43:43,360 --> 00:43:50,800
with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a

345
00:43:50,800 --> 00:43:57,440
philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm

346
00:43:57,440 --> 00:44:04,320
a bit of a relativist myself and actually if you trace this tree of complexity and how humans

347
00:44:04,320 --> 00:44:09,200
evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist

348
00:44:09,200 --> 00:44:15,040
future where we become the thing over time that we're so scared of and I think the lens that

349
00:44:15,040 --> 00:44:19,920
we're using here about what's right and what's wrong is kind of like I was saying before it's

350
00:44:19,920 --> 00:44:26,000
a snapshot of humanity now and we kind of think of it as just this monolithic single thing

351
00:44:26,000 --> 00:44:32,240
so does it really work when you project out to how we're going to evolve in the future

352
00:44:34,880 --> 00:44:42,400
but first of all humankind is not a monolithic thing so many of these

353
00:44:43,520 --> 00:44:50,800
arguments go like we should not do that because of that we should not do that because of that

354
00:44:51,600 --> 00:44:57,840
but there is no us there is no we there are only and almost 10 billion different people

355
00:44:57,840 --> 00:45:04,560
and they all have different ideas about what's good for them and so for thousands of years we had

356
00:45:04,560 --> 00:45:14,000
these evolutions of ideas and of devices and philosophies competing partially competing

357
00:45:14,000 --> 00:45:21,520
and partially compatible with each other which in the end led to the current values

358
00:45:21,520 --> 00:45:27,680
that some people agree with and other people over there they agree with different values

359
00:45:27,680 --> 00:45:32,240
nevertheless there are certain values that have become more popular than others more successful

360
00:45:32,880 --> 00:45:39,360
more evolutionary with more success during the evolution of ideas

361
00:45:39,840 --> 00:45:55,760
and so given this entire context of evolution of concepts and accepted ideas of what should be done

362
00:45:55,760 --> 00:46:04,560
or what is worth being supported and what's not worth being supported all of this has changed a lot

363
00:46:05,280 --> 00:46:12,880
if we look back 200 years the average people in the west had different ideas of what's good

364
00:46:12,880 --> 00:46:22,000
than today and and this evolution of ideas is not going to stop any time soon

365
00:46:26,080 --> 00:46:32,320
just a final question on this and there is a very real existential risk right now

366
00:46:32,400 --> 00:46:38,960
of nuclear armageddon a real risk right now and if i were a rational person

367
00:46:39,680 --> 00:46:46,720
i would be devoting all of my effort into that and other risks associated so do you think it's

368
00:46:46,720 --> 00:46:56,080
a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all

369
00:46:56,080 --> 00:47:04,480
these letters coming out warning are the dangers of ai and i think some of the guys who are writing

370
00:47:04,480 --> 00:47:14,880
these letters they are just seeking attention because they know that ai dystopia are attracting

371
00:47:14,880 --> 00:47:23,200
more attention than documentaries about the benefits of ai in healthcare and stuff like that

372
00:47:24,160 --> 00:47:31,120
but generally speaking i am much more worried about nuclear bombs than about ai weapons

373
00:47:34,960 --> 00:47:44,480
a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without

374
00:47:44,480 --> 00:47:52,880
a face recognition just like that without any ai and so in that sense it's much more harmful

375
00:47:52,880 --> 00:48:00,000
than the comparatively harmless ai weapons than that we have today and that we can currently

376
00:48:00,000 --> 00:48:09,680
conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization

377
00:48:09,680 --> 00:48:19,920
within two hours without any ai well i guess um since we're we're not really going to worry about

378
00:48:19,920 --> 00:48:24,800
ai for the moment we can uh we can turn our attention back to discussing with you uh how

379
00:48:24,800 --> 00:48:32,880
we develop ai so um you know i'm really curious with with just the really the the vast you know

380
00:48:32,880 --> 00:48:38,880
breadth and depth of your of your knowledge over the the history of of ai and the state of the art

381
00:48:38,880 --> 00:48:45,280
i'm curious you know which current approaches you're you're most excited about and or what's on the

382
00:48:45,280 --> 00:48:51,280
horizon um that you know for any of our listeners out there are thinking about um going into ai

383
00:48:51,280 --> 00:48:56,560
research machine learning research you know what may be um alternatives that aren't getting enough

384
00:48:56,560 --> 00:49:01,520
attention should they should they look into studying and and perhaps choosing the research

385
00:49:02,080 --> 00:49:09,760
at the moment the limelight is on um language models large language models which pass the

386
00:49:09,760 --> 00:49:17,120
touring tests and do all kinds of things that seemed inconceivable just a couple of years ago

387
00:49:17,120 --> 00:49:25,680
at least to some of those who are now surprised but of course that is just a tiny part of

388
00:49:27,200 --> 00:49:34,480
what's going to be important to develop true ai agi artificial general intelligence

389
00:49:35,440 --> 00:49:42,160
um on the other hand the roots of what we need to develop true ai also

390
00:49:43,200 --> 00:49:48,880
come from the previous millennium they are not new and of course what you need is an

391
00:49:48,880 --> 00:49:58,640
environment to interact with and you need an an agent that can manipulate the environment and you

392
00:49:58,640 --> 00:50:07,280
need a way of learning to improve the rewards that you get from this environment as you are

393
00:50:07,280 --> 00:50:15,680
interacting it with it within a single lifetime so one of the important aspects of reinforcement

394
00:50:15,680 --> 00:50:21,920
learning what we are now talking about is that you have only one single life you don't have

395
00:50:21,920 --> 00:50:27,520
repeatable episodes like in most of traditional reinforcement learning no you have only one

396
00:50:27,520 --> 00:50:35,680
single life and in the beginning you know nothing and then after 30 percent of your life is over

397
00:50:35,680 --> 00:50:42,240
you know something about life and all you know is the data that you collected during these first

398
00:50:42,240 --> 00:50:50,400
30 percent of your life and now there is an infinite almost infinite possibility set of

399
00:50:50,400 --> 00:51:00,640
possibilities of futures and from this little short experience you have to generalize somehow

400
00:51:00,640 --> 00:51:07,040
and try to select action sequences that lead to the most promising futures that you can shape

401
00:51:07,040 --> 00:51:13,680
yourself through your actions now to achieve all of that you need to build a model of the world a

402
00:51:13,680 --> 00:51:20,240
predictive model of the world which means that you have to be able to learn over time and to

403
00:51:20,400 --> 00:51:25,360
predict the consequences of your actions so that you can use this model of the world that you are

404
00:51:25,360 --> 00:51:33,680
acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way

405
00:51:33,680 --> 00:51:40,000
which we had in 1990 which is millisecond by millisecond planning where you say okay now

406
00:51:41,680 --> 00:51:49,680
I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit

407
00:51:49,680 --> 00:51:54,000
and move it a little bit more and move it a little bit more and then get up and so no you want to do

408
00:51:54,000 --> 00:52:03,520
that in a high level way in a hierarchical way in a way that allows you to to focus on the important

409
00:52:04,160 --> 00:52:12,480
abstract concepts for example as you are trying to go from from your home to Beijing you decompose

410
00:52:12,480 --> 00:52:19,440
this whole future into a couple of sub goals you say a first important step is to go to the cap

411
00:52:19,520 --> 00:52:27,520
station and get a taxi to the airport and then in the airport you will find your your plane

412
00:52:27,520 --> 00:52:32,480
and then for nine hours nothing is going to happen and you exit in Beijing and have to find another

413
00:52:32,480 --> 00:52:38,960
cab and so on so you you don't do millisecond by millisecond detailed planning no you have

414
00:52:40,080 --> 00:52:46,640
high level planning to just reduce the computational effort and focus on the essentials of what you

415
00:52:46,640 --> 00:52:53,360
want to do so that is something that most current systems don't do but for a long time we have had

416
00:52:53,360 --> 00:52:59,200
systems like that and they are getting more sophisticated over time important you have a

417
00:52:59,200 --> 00:53:04,560
predictive model of the world that is not just focusing on the pixels and predicting the how

418
00:53:04,560 --> 00:53:11,280
does the video change as I'm moving my hand back and forth the video that I get through my camera

419
00:53:11,360 --> 00:53:21,040
my eyes and so on and no higher level concepts that that reflect islands of predictability many

420
00:53:21,040 --> 00:53:26,000
things are not predictable but certain abstract representations of these things are predictable

421
00:53:26,000 --> 00:53:31,520
and so how can you discover these higher level concepts that you need to efficiently think

422
00:53:31,520 --> 00:53:37,520
about your own future options and select those that are most promising in the single life

423
00:53:37,840 --> 00:53:45,440
yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he

424
00:53:45,440 --> 00:53:52,320
talks about this collective intelligence where you have this multi-agent cybernetic framework

425
00:53:52,320 --> 00:53:58,000
which is causally closed and one of the things we're talking about here really is not the model

426
00:53:58,000 --> 00:54:04,880
itself people talk about chat gpt and it's just a model and people have configured it in arrangements

427
00:54:04,960 --> 00:54:09,840
that have varying degrees of autonomy and in the future we will develop these collective

428
00:54:09,840 --> 00:54:16,000
intelligences and they're not just predicting the actions and behaviors of other agents but even

429
00:54:16,000 --> 00:54:23,040
the world that we're in is a computer to some extent so when you imbue agents with this kind of

430
00:54:23,040 --> 00:54:28,400
creativity and autonomy that's the thing that I don't think people really understand what might

431
00:54:28,400 --> 00:54:33,760
emerge from that it's related to this discussion about what kind of goals might emerge from that

432
00:54:34,400 --> 00:54:39,600
do you have any intuition on what that would look like yeah let me give you just the simplest

433
00:54:39,600 --> 00:54:50,080
example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it

434
00:54:50,080 --> 00:54:57,520
consists of two artificial neural networks and I know that Carl Friston is very interested in that

435
00:54:57,520 --> 00:55:05,840
and only recently for the first time in my life I was on a paper where he was co-author

436
00:55:06,560 --> 00:55:15,760
just a year ago and so back then it was really about a reinforcement learning agent and it

437
00:55:17,120 --> 00:55:23,360
interacts with the world and it generates actions that change the world and then there is

438
00:55:23,360 --> 00:55:35,280
another network which just is trying to predict the consequences of the actions in the environment

439
00:55:35,280 --> 00:55:40,800
so the reactions of the environment to these actions and so that becomes a world model and

440
00:55:40,800 --> 00:55:48,320
then what kind of goal was there which was different from traditional goals well in the

441
00:55:48,320 --> 00:55:53,120
beginning this model of the world this prediction machine which is a model of the world a world

442
00:55:53,120 --> 00:56:01,280
model knows nothing so it has high error as it is trying to predict the next thing as it is trying

443
00:56:01,280 --> 00:56:12,640
to predict the reactions of the environment to the actions of the agent so as the second network

444
00:56:12,640 --> 00:56:18,320
is trying to reduce its prediction error through gradient descent through back propagation essentially

445
00:56:19,040 --> 00:56:28,240
the other one is trying to generate actions outputs that maximize the same error so basically

446
00:56:28,240 --> 00:56:35,120
the goal the self-invented goal if you will of the first network is to generate an action

447
00:56:36,080 --> 00:56:42,240
with whose consequences cannot yet be predicted by the other network by the model of the world

448
00:56:43,040 --> 00:56:48,000
so the first network is generating outputs that surprise the second network

449
00:56:49,920 --> 00:56:56,960
so suddenly you have an incentive where the first network is trying to invent actions

450
00:56:56,960 --> 00:57:06,800
experiments that fool or that surprise the second network and that was called artificial curiosity

451
00:57:07,200 --> 00:57:16,400
so now suddenly you have a little agent which a little bit like a baby doesn't learn by

452
00:57:16,400 --> 00:57:25,120
imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise

453
00:57:25,120 --> 00:57:33,440
itself and have fun by playing with the toys and and observing new unpredictable things which

454
00:57:33,440 --> 00:57:39,440
however become predictable over time and therefore become boring and then it has another incentive

455
00:57:39,440 --> 00:57:46,560
to invent the additional experiments such that it still can surprise its model of the world

456
00:57:46,560 --> 00:57:52,800
which in turn is improving and so on so artificial curiosity does that does that also

457
00:57:53,440 --> 00:57:58,000
have the effect of making the network which is trying to predict does it have the effect of

458
00:57:58,000 --> 00:58:03,760
making it more robust and more generalizable like almost a form of you know regularization

459
00:58:03,760 --> 00:58:11,840
kind of built in in this pairing yeah you can build into that network all kinds of regularizers

460
00:58:12,400 --> 00:58:21,680
an orthogonal concept which is also very important so that was just the first version that was

461
00:58:21,680 --> 00:58:29,280
really in 1990 and then we have had a we had a long string of papers just on improvements of

462
00:58:29,280 --> 00:58:36,320
this original concept of artificial curiosity so this old system is basically what you what you

463
00:58:36,320 --> 00:58:42,320
now know as GANs Generative Adversarial Networks because the first network is generating a probability

464
00:58:42,320 --> 00:58:49,680
distribution over outputs and the second network is then predicting the consequences of these outputs

465
00:58:49,680 --> 00:58:58,320
in the environment and if you if the output is an image then the consequence can be either this

466
00:58:58,320 --> 00:59:04,880
image is of a certain type yes or not no and then that's all that the prediction machine the world

467
00:59:04,880 --> 00:59:10,160
model predicts in that simple case and you minimize the first network minimizes the same

468
00:59:10,960 --> 00:59:16,960
error function that the second one maximizes so then you have basically a GAN but then you

469
00:59:16,960 --> 00:59:24,240
don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is

470
00:59:24,960 --> 00:59:33,600
a model of the world that extracts the regularities in the environment that that

471
00:59:36,320 --> 00:59:43,520
that finds predictable things which are regular in the sense that there's a short explanation there

472
00:59:43,520 --> 00:59:51,920
of for example if you have falling objects in a video then they all fall in the same way they

473
00:59:51,920 --> 00:59:57,040
accelerate in the same way which means it's predictable what these objects do if you see two

474
00:59:57,040 --> 01:00:04,080
of the frames you can predict the third frame pretty well and the law behind that is very simple

475
01:00:04,880 --> 01:00:12,080
this means that you can greatly compress the video that is coming in because you can

476
01:00:13,520 --> 01:00:18,800
instead of storing all the pixels you can compute many of these pixels by just looking at two

477
01:00:18,800 --> 01:00:25,040
successive frames and predicting the third frame or maybe three successive frames and predicting the

478
01:00:25,040 --> 01:00:31,600
fourth frame something like that and you only have to encode the deviations from the prediction so

479
01:00:31,600 --> 01:00:37,680
everything else you don't have to store separately which means you once you understand gravity you

480
01:00:37,760 --> 01:00:44,960
can greatly greatly compress the video so that's what you really want to do and so the more advanced

481
01:00:44,960 --> 01:00:53,280
version of artificial curiosity is about that where you have a motivation to find a disruption

482
01:00:53,840 --> 01:01:02,160
of the data which is coming in of the video of the falling apples for example that is simpler than

483
01:01:02,160 --> 01:01:08,480
the one that you had before so before you had the simple explanation of the data you needed

484
01:01:08,480 --> 01:01:15,920
so many bits so many bits to um to describe the data and afterwards only so many and the

485
01:01:15,920 --> 01:01:22,720
difference between before and after that is the reward that you get so that's the true reward

486
01:01:22,720 --> 01:01:29,040
that the controller the first neural network should get in response to the improvements

487
01:01:30,000 --> 01:01:37,120
of the second network which are now measured in terms of compression progress so first I needed

488
01:01:37,120 --> 01:01:43,520
so many resources to encode the data but then I discovered this regularity gravity and I can

489
01:01:43,520 --> 01:01:49,760
greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly

490
01:01:49,760 --> 01:01:56,800
I'm have a huge insight into the nature of the world and that is my true joy scientific as a

491
01:01:56,960 --> 01:02:03,760
scientist my my true joy as a scientist that I want to encode in a little number which is

492
01:02:03,760 --> 01:02:09,120
given as a reward to the guy who is inventing these experiments that lead to the data to the

493
01:02:09,120 --> 01:02:14,320
data with the falling apples for example right well and of course this is this has been a challenge

494
01:02:14,320 --> 01:02:19,360
in machine learning you know since the beginning which is okay as we add more and more parameters

495
01:02:19,360 --> 01:02:23,920
how do we prevent it from learning spurious information with those parameters and instead

496
01:02:23,920 --> 01:02:29,920
have it focus on parsimonious explanations on regular explanations on things that in this

497
01:02:29,920 --> 01:02:35,360
universe are more likely to generalize you know to unseen examples and so I think my question to

498
01:02:35,360 --> 01:02:41,280
you is does this setup that you describe is it a form of that and or what is the state of the art

499
01:02:41,280 --> 01:02:47,600
you know these days for helping to push or nudge neural networks towards learning parsimonious

500
01:02:48,240 --> 01:02:54,160
models for the world rather than highly detailed spurious susceptible to you know

501
01:02:54,160 --> 01:02:58,720
high frequency anomalies and adversarial examples and all this sort of thing

502
01:03:00,400 --> 01:03:04,640
yes what is the current state of the art in the regularizing

503
01:03:06,320 --> 01:03:12,880
descriptors of the data such as neural networks such that you get simple explanations of the data

504
01:03:13,760 --> 01:03:20,560
such that you get short programs that compute the data in other words such that the description of

505
01:03:20,560 --> 01:03:32,160
the data is a short program that computes the much larger raw data and and how close can we

506
01:03:32,720 --> 01:03:38,640
get to the limits which are given through this concept concept of algorithmic information

507
01:03:38,640 --> 01:03:45,040
or comagor complexity comagor complexity of any data is the length of the shortest program

508
01:03:45,760 --> 01:03:51,440
on some general computer that computes it since in our field the general computers are

509
01:03:51,440 --> 01:03:58,400
recurrent neural networks we want to find a simple recurrent network that computes all this data

510
01:04:01,680 --> 01:04:07,360
and given one computation of the data we want to find an even simpler one so we want to have this

511
01:04:07,360 --> 01:04:15,280
idea of compression progress and here I have to say although we have lots of regularizers

512
01:04:15,280 --> 01:04:22,080
invented throughout the past few decades there's nothing that is really convincing

513
01:04:23,040 --> 01:04:32,080
I think one of the very important missing things is to make that work in a way that is

514
01:04:32,080 --> 01:04:39,200
truly convincing that is as convincing as chat gpt is today in the much more limited domain of

515
01:04:40,400 --> 01:04:48,960
generating text from previously observed texts and stuff a very old idea of I think the 1980s

516
01:04:49,920 --> 01:04:57,760
was to have weight decay in a neural network which basically is the idea that all the weights

517
01:04:57,760 --> 01:05:05,600
should have an incentive to become close to zero such that you can prune them

518
01:05:06,640 --> 01:05:14,560
and so people built in regularizer that just punished weights for being large or being very

519
01:05:14,560 --> 01:05:22,240
negative but that didn't work really well and something better was flat minimum search that was

520
01:05:22,240 --> 01:05:29,840
1998 and first Arthur my brilliant student set book write that back then roughly the same time

521
01:05:29,840 --> 01:05:40,880
when the LSTM paper came out and and there the idea is if you have if you plot the weights of

522
01:05:40,880 --> 01:05:51,680
a neural network on the x-axis and you plot the error on the y-axis then given the weights you

523
01:05:51,680 --> 01:06:00,480
have high or low error and then there is for example a sharp error function which has a sharp

524
01:06:00,480 --> 01:06:08,000
minimum which which goes like that can you see my finger so here here is the x-axis here's the

525
01:06:08,000 --> 01:06:14,480
y-axis here's the error and the error for a certain weight is really really low but then

526
01:06:14,480 --> 01:06:21,920
for a different weight in the environment in the vicinity it's high again which would be very

527
01:06:21,920 --> 01:06:28,800
different from a flat minimum which would be like this so here's the error and it's going down

528
01:06:28,800 --> 01:06:35,280
and for many many ways it is low the error and then it goes up again so if you are a very sharp

529
01:06:35,280 --> 01:06:42,320
well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well

530
01:06:42,320 --> 01:06:48,720
you have to specify the weights with a lot with with high precision so you have to spend many bits

531
01:06:48,720 --> 01:06:55,600
of information on encoding the weights of this network as opposed to a large to a flat minimum

532
01:06:55,600 --> 01:07:03,360
where it doesn't matter if you you know perturb the weights because the error remains low

533
01:07:04,240 --> 01:07:11,040
in this flat minimum so what you really want to find is is a network that has low complexity in

534
01:07:11,040 --> 01:07:17,040
the sense that you can describe the good network so those with low error with very few bits of

535
01:07:17,040 --> 01:07:25,280
information and suddenly if you maximize or if you minimize that flat minimum second order

536
01:07:25,280 --> 01:07:35,440
error function then suddenly you have a preference for networks that that for example do this

537
01:07:35,760 --> 01:07:43,920
you you have a hidden unit and the outgoing weights they have certain values but if you

538
01:07:43,920 --> 01:07:48,640
give a very negative weight to the hidden unit then it doesn't matter what all these outgoing

539
01:07:48,640 --> 01:07:57,760
weights do and flat minimum minimum search likes to find weight matrices like that where one single

540
01:07:57,760 --> 01:08:02,800
weight can eliminate many others which you suddenly don't need any longer such that the

541
01:08:03,600 --> 01:08:10,800
description complexity of the whole thing is much lower than in the beginning when you when you

542
01:08:10,800 --> 01:08:17,840
just had a random initialization of all these weights so that is much more general than weight

543
01:08:17,840 --> 01:08:22,720
decay because weight decay doesn't like these strong weights it wants to remove them but sometimes

544
01:08:22,720 --> 01:08:27,760
it's really good to have a very negative weight coming to a hidden unit which is switched off

545
01:08:27,760 --> 01:08:36,080
through that weight such that all the outgoing connections are meaningless but it's not um what

546
01:08:36,080 --> 01:08:44,000
you what you it's very nice it's a very nice principle but it's not as general as finding the

547
01:08:44,000 --> 01:08:50,400
shortest program on a university computer that computes the weight matrix that is solving your

548
01:08:50,400 --> 01:08:56,160
problem to the extent how do you think we're how do you think we're gonna get to that point

549
01:08:56,160 --> 01:09:00,960
how do you think uh what approaches are going to lead us to finding things that approach

550
01:09:00,960 --> 01:09:07,600
comical of complexity yeah and i think that path has again a lot to do with meta learning and as

551
01:09:07,600 --> 01:09:15,520
um a system is able to run its own learning algorithm on the network itself it can um suddenly

552
01:09:15,600 --> 01:09:17,680
speak about the um

553
01:09:20,080 --> 01:09:28,320
algorithms in form of weight matrices and it can discuss concepts such as the complexity of a

554
01:09:28,320 --> 01:09:38,000
weight matrix and then you can conduct a search um in this space of networks that generates

555
01:09:38,720 --> 01:09:47,360
weight matrices and then you suddenly are in the game so suddenly you are playing the right game

556
01:09:47,360 --> 01:09:55,440
and then it's more a question of how to um choose an initial learning algorithm such as

557
01:09:55,440 --> 01:10:01,120
gradient descent to come up with something that computes the simple solutions which you really

558
01:10:01,120 --> 01:10:11,520
want to see in the end very recent papers on that on on aspects of that came out just a while ago

559
01:10:11,520 --> 01:10:24,160
with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki

560
01:10:24,720 --> 01:10:31,360
and robert joydash also um and also imann olschlag and there the idea is really to

561
01:10:32,080 --> 01:10:39,760
have one network that computes an experiment and the experiment itself is the weight matrix

562
01:10:39,760 --> 01:10:48,000
of a recurrent network so there is a generator of an experiment which can be anything that

563
01:10:48,080 --> 01:10:57,600
describes a computational interaction with an environment so a program so that experiment

564
01:10:57,600 --> 01:11:03,360
is then executed in the real world there's a prediction machine that predicts the outcome

565
01:11:03,360 --> 01:11:13,440
of the experiment before the algorithm is executed and so then there's um just a yes or no question

566
01:11:13,440 --> 01:11:24,800
either the following outcome will occur or not either it will occur or not but now the entire

567
01:11:24,800 --> 01:11:30,160
setup is such that you don't have predictions all the time about every single pixel no you just have

568
01:11:30,160 --> 01:11:35,440
something which is very abstract and which is just about whether a certain unit of the recurrent

569
01:11:35,440 --> 01:11:42,480
network is going to be on or off at the end of the experiment and this internal on and off unit

570
01:11:42,560 --> 01:11:51,120
can represent any computational question any questions that you can ask at all and now the

571
01:11:51,120 --> 01:11:56,560
the task of the experiment generator which is another network which generates a recurrent

572
01:11:57,600 --> 01:12:03,600
network weight matrix which represents the experiment the task of this experiment generator

573
01:12:03,600 --> 01:12:10,480
is to again come up with something that surprises the um the prediction machine which looks at the

574
01:12:10,480 --> 01:12:17,840
experiment and says yeah it's going to work or not and uh and suddenly you are again in this

575
01:12:17,840 --> 01:12:26,400
old game uh except that now you have this world of abstractions where the abstractions can be

576
01:12:26,400 --> 01:12:34,000
anything that is computable interesting really cool really cool could we spend the last 10 minutes

577
01:12:34,000 --> 01:12:40,320
or so just talking about some of the the current ai landscape so in particular the capabilities of

578
01:12:40,480 --> 01:12:48,800
GPT-4 and the moat building thing and and the the power that companies like uh google and open ai

579
01:12:48,800 --> 01:12:55,840
have and um also the potential for open source so maybe we'll just start with the you know the

580
01:12:55,840 --> 01:13:02,080
very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the

581
01:13:02,080 --> 01:13:10,960
sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years

582
01:13:10,960 --> 01:13:18,640
ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing

583
01:13:19,440 --> 01:13:31,440
and it is not really close to um to justifying those fears that some uh researchers sometimes

584
01:13:31,440 --> 01:13:46,320
and now um document and um in letters and public letters and so on so to me it's a little bit

585
01:13:50,000 --> 01:13:58,000
like a visa view because for for many decades i have um had discussions like that and people said

586
01:13:58,960 --> 01:14:03,760
that you are crazy when i said that within my lifetime i want to build something that is smarter

587
01:14:03,760 --> 01:14:11,280
than myself um and now suddenly in recent years um some of the guys who said it's never going to

588
01:14:11,280 --> 01:14:16,480
happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and

589
01:14:17,040 --> 01:14:25,360
whatever uh so i i don't share these um extreme um

590
01:14:31,360 --> 01:14:38,640
i'm less impressed than some of those guys let me say that right uh the open source movement

591
01:14:38,640 --> 01:14:42,800
that you mentioned you you want to ask a specific specific question about that right

592
01:14:42,960 --> 01:14:51,360
well yeah there was that famous google memo that got leaked and when the waits for loma from

593
01:14:51,360 --> 01:14:58,560
facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt

594
01:14:58,560 --> 01:15:04,400
you know with this um laura fine tuning and the open source community has just exploded you know

595
01:15:04,400 --> 01:15:09,760
you can now run it on your laptop and there is some question whether there is a significant

596
01:15:09,760 --> 01:15:14,880
gap between the capability you know is is it just a parlor trick is it really as good potentially

597
01:15:14,880 --> 01:15:19,520
or could it be as good as some of the next best models from open ai but i guess the question is

598
01:15:19,520 --> 01:15:29,120
do you think that we need open ai to to have the best models no of course not um no i'm very

599
01:15:29,120 --> 01:15:37,280
convinced of the open source movement and have um supported that some people say the open source

600
01:15:37,840 --> 01:15:42,640
movement is maybe six or eight months behind the large companies that are now

601
01:15:44,640 --> 01:15:54,720
coming out with these models and i think the best way of making sure that there won't be dominance

602
01:15:54,720 --> 01:16:02,160
through some large company is to support the open source movement because how can a large company

603
01:16:02,160 --> 01:16:08,720
compete against all these brilliant phd students around the world who are so

604
01:16:09,520 --> 01:16:14,640
motivated to you know within a few days create something that is a little bit better than what

605
01:16:14,640 --> 01:16:25,920
the last guy has um um put out there on github and whatever so i'm i'm very convinced that this

606
01:16:25,920 --> 01:16:35,840
open source movement is going to make sure that there won't be a huge mode for a long time

607
01:16:36,880 --> 01:16:40,160
i'm reading between the lines here but i would guess you would be opposed to

608
01:16:41,040 --> 01:16:46,560
legislation like the eu is considering where you know very tight restrictions on

609
01:16:46,560 --> 01:16:52,480
generative models you know onerous onerous kind of uh approval processes and things like that

610
01:16:52,480 --> 01:16:58,480
because that's going to have this chilling effect on on open source innovation and the little guys

611
01:16:58,480 --> 01:17:07,920
wouldn't it yes i have signed letters um which which support the open source movement and whenever

612
01:17:07,920 --> 01:17:18,160
i get a chance to um maybe influence some you um politicians then i'm trying to contribute to

613
01:17:18,160 --> 01:17:24,640
making sure that they don't don't shoot themselves in the foot by by by killing

614
01:17:24,640 --> 01:17:29,440
killing innovation through the open source movement so you certainly want to avoid that

615
01:17:32,240 --> 01:17:39,600
there are lots of different open source movements around the world so if one big entity fails to

616
01:17:40,880 --> 01:17:47,360
support open source or even makes it harder for open source there will still be lots of other

617
01:17:47,360 --> 01:17:55,040
entities which um won't follow follow and so no matter what's going to happen on the political

618
01:17:55,040 --> 01:18:05,520
level i think open source is not going away i guess just in closing you've been in this game

619
01:18:05,520 --> 01:18:11,920
for decades now and what is i know it's a bit of a strange question to ask but what's your fondest

620
01:18:11,920 --> 01:18:17,920
memory in your career my fondest memory oh it's usually when i discover something that i think

621
01:18:19,760 --> 01:18:26,960
nobody has seen before but that is that happens very rarely because most of the things you think

622
01:18:26,960 --> 01:18:32,560
are well somebody else has done before um but yeah so

623
01:18:39,040 --> 01:18:47,440
yeah um what usually happens is um you and and this has happened many times not many times but

624
01:18:48,080 --> 01:18:53,200
quite a few times in my career since the 80s as a scientist who publishes stuff

625
01:18:53,440 --> 01:19:02,960
but suddenly you think oh that is the solution to all these problems and now i really figured out

626
01:19:02,960 --> 01:19:09,600
a way of building this universal system which um learns how to improve itself and learns the way

627
01:19:10,160 --> 01:19:15,440
to improve the way it improves itself and so on and now we are done and now all is that's

628
01:19:15,440 --> 01:19:22,320
necessary is to scale it up and it's going to solve everything and then um you think a little

629
01:19:22,320 --> 01:19:27,280
bit longer about it and maybe you have a couple of publications but then it turns out something

630
01:19:27,280 --> 01:19:33,440
is missing something important is missing and and actually it's not that great and actually

631
01:19:34,160 --> 01:19:41,920
you have to think hard to add something important to it which then for a brief moment looks like the

632
01:19:43,440 --> 01:19:51,120
greatest thing since sliced bread and um and then you get excited again but then suddenly

633
01:19:51,120 --> 01:19:57,520
you realize oh it's still not finished something important is missing and so it goes back and

634
01:19:57,520 --> 01:20:02,800
forth like that i think that's the life of a scientist the greatest joys are those moments

635
01:20:02,800 --> 01:20:09,760
where you have an insight where suddenly things fall into place such that along the lines of what

636
01:20:09,760 --> 01:20:17,920
we discussed before the description length of some solution to a problem suddenly shrinks because

637
01:20:17,920 --> 01:20:27,440
two puzzle pieces they suddenly match and and become one or become one in the sense that

638
01:20:27,440 --> 01:20:32,640
they fit each other such that suddenly you have the shared line between the two

639
01:20:32,640 --> 01:20:37,520
puzzle pieces one is negative and the other one is positive and certainly the whole thing is

640
01:20:38,880 --> 01:20:45,360
much more compressible than the sum of the things separately so these these things that's

641
01:20:45,360 --> 01:20:51,360
what's driving um scientists like myself i guess

642
01:20:54,720 --> 01:20:59,200
wonderful um professor you again schmidhuber it's been an absolute honor thank you so much

643
01:20:59,200 --> 01:21:03,360
for coming on the show today thank you it was such a pleasure talking to you

