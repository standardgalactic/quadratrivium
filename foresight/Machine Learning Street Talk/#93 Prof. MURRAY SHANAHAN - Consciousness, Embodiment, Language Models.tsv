start	end	text
0	5360	Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior
5360	11040	research scientist at DeepMind. He graduated from Imperial College with a first in computer science
11040	18400	in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the
18400	24080	fields of artificial intelligence, robotics and cognitive science. He's published books such as
24080	29440	Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner
29440	34560	Life was a significant influence on the film Ex Machina for which he was a scientific advisor.
35920	40640	Now Professor Shanahan is a renowned researcher on sophisticated cognition
40640	45920	and its implications for artificial intelligence. His work focuses on agents that are coupled to
45920	51600	complex environments through sensory motor loops such as robots and animals. He's also
51600	56240	particularly interested in the relationship between cognition and consciousness and has
56240	61920	developed a strong understanding of the biological brain and cognitive architectures more generally.
61920	67360	In addition Professor Shanahan is interested in the dynamics of the brain including metastability,
67360	72960	dynamical complexity and criticality as well as the application of this understanding to
72960	78240	machine learning. He's also fascinated by the concept of global workspace theory as proposed
78240	83040	by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive
83040	88240	architecture comprising a set of parallel specialist processes and a global workspace.
88240	92640	Professor Shanahan is committed to understanding the long-term implications of artificial
92640	98960	intelligence both its potential and its risks. His research has been published extensively
98960	102800	and he's a member of the External Advisory Board for the Cambridge Centre of the Study of
102800	108560	Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness.
109280	115600	Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica
115600	123040	in 2016 where he invited us to explore the space of possible minds, a concept first proposed by
123040	129760	philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds
129760	135600	which could exist from those of other animals such as chimpanzees to those of life forms that could
135600	141120	have evolved elsewhere in the universe and indeed those of artificial intelligences.
141120	147760	Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity
147760	154480	for consciousness and human likeness of the behavior. According to Shanahan the space of
154480	160080	possible minds must include forms of consciousness that are so alien that we wouldn't even recognize
160080	167120	them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience,
167120	173840	remember we were talking about Nagel's bat on the Charmer's show, insisting instead that
173840	180320	nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that
180320	187120	while no artifacts exist today, which has anything even approaching human-like intelligence,
187200	192160	the potential for variation in artificial intelligences far outstrips the potential
192160	198160	for variation in naturally evolved intelligence. This means that the majority of the space of
198160	205440	possible minds may be occupied by non-natural variants such as the conscious exotica of
205440	211600	which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites
211600	217600	us to consider the possibility for human-like minds but also for those that are radically different
217600	224560	and inscrutable. He concludes that although we may never understand these alien forms of consciousness,
224560	228400	we can still recognize them as part of the same reality as our own.
230160	235760	So Professor Shanahan has just dropped a brand new paper called Talking about large language models
235760	240960	in which he discusses the capabilities and limitations of large language models.
240960	245680	Now in order to properly comprehend the capacities and boundaries of these models,
245680	249360	we must first grasp the relationship between humans and these systems.
249920	255280	Humans have evolved to survive in a common world and have cultivated a mutual understanding
255280	261200	reflected in their ability to converse about convictions and other mental states. Conversely,
261200	266240	AI systems lack this shared comprehension, so attributing beliefs to them should be done
266320	271040	circumspectly. Now prompt engineering is something that we've all become very familiar with,
271040	275520	we've discussed it a lot on this show recently, and it's almost become a fact of the matter when
275520	281600	it comes to these large language models. It involves exploiting prompt prefixes to adjust
281600	286800	the language models to diverse tasks without needing any supplementary training, allowing for
286800	293680	more effective communication between humans and machines. Nevertheless, lacking a more profound
293760	297840	understanding of the system and its relationship to the external world,
297840	302720	it's difficult to be certain whether the arguments produced by a large language model
302720	310080	are genuine reasoning or simply mimicry. Large language models can be integrated into a variety
310080	316240	of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily
316240	322000	mean that these systems possess completely human-like language abilities. Even though the robot in the
322000	328080	SAKAN system is physically embodied and interacts with the real world, its language is still learned
328080	333200	and used in a dramatically different manner than humans. So in summary, although Professor
333200	339280	Shanahan concludes that large language models are formidable and versatile, they're fundamentally
339280	344800	unlike humans and we must be wary of ascribing human-like characteristics to these systems.
344800	350480	We must find a way to communicate the nature of these systems without resorting to simple terms.
350480	355360	This may necessitate an extended period of interaction and experimentation with the technology,
355360	361440	but it's a fundamental step if we are to accurately portray the capabilities and limitations of
361440	367760	large language models. So anyway, without any further delay, I give you Professor Murray Shanahan.
368960	374320	Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your
374320	379680	background. My background? Well, I've been interested in artificial intelligence for as
379680	385120	long as I can remember since I was a child, really, and I was very much drawn to it by
385120	392800	science fiction, by science fiction movies and books. And then I studied computer science
392800	398400	right from when I was a teenager and got very much drawn into programming, was fascinated by
398400	406240	programming. I really did my 10,000 hours of programming experience when I was quite young
406240	411120	and I went on to do computer science at Imperial College London. That was my degree.
411920	416560	And then still fascinated by artificial intelligence, I went on to Cambridge
417520	422880	and did my PhD in AI in Cambridge, very much in the symbolic school then.
424560	430240	And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic
430240	436160	AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of
436160	442960	segued into studying the brain, which was the obvious example of actual general
442960	450960	intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience
450960	457920	and computational neuroscience and that kind of thing. And then deep learning and deep
457920	463280	reinforcement learning happened in the early 2010s and AI started to get interesting again.
463280	471040	And I got very much back into it that way. And I was particularly impressed by DeepMind's
471040	476800	DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic
476800	484000	step forward. And I really kind of went back to my roots and back to AI at that point.
484000	488080	Yeah, and I think we'll talk about DQN when we speak about your article on consciousness.
488080	496560	But so having such a diverse set of experiences in adjacent fields, how have they influenced each
496560	502880	other? Yeah, well, and one thing I didn't mention is that I've also had a long standing
502880	509280	interest in philosophy. And I very often think that what I am is a sort of weird kind of
509280	515440	philosopher, really. And philosophical questions have had a great attraction for me. So I think
516560	522080	there's a sort of three way into relationship between artificial intelligence, neuroscience,
522080	527440	and the other cognitive sciences and philosophy. And I think they all kind of mutually inform
527440	533280	each other, really. Yeah. Fantastic. So you wrote a book called
533280	537040	Embodiment and the Inner Life. What motivated you to write that book?
537040	546720	Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of
546720	554320	long excursion into thinking about consciousness and about brains, which took place after I had
554320	559600	moved away from symbolic AI, really. So I was thinking about the biological brain.
559680	563520	In the back of my mind, I'd always been fascinated by these philosophical questions about
563520	570800	consciousness. And then I went a bit kind of crazy and started thinking about these things
570800	576160	seriously. It became kind of my day job to think about neuroscience, and about consciousness.
576160	582000	And around about that time, the science of consciousness was taking off as a serious
582000	586800	academic discipline with proper experimental paradigms. So that was really fascinating.
587440	594800	And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,
594800	598960	global workspace theory being one of the leading contenders for a scientific
598960	605200	theory of consciousness. And I was very drawn to global workspace theory, and partly because
606000	611520	it was a computational sort of theory. It drew very heavily on computer science
612480	617600	and computer architectures. There was a computer architecture at the center of the theory.
620320	625520	So this kind of collection of interests, along with my philosophical interests, which all came
625520	631520	together, and I wanted to put them all into a book where I expressed my kind of ideas about,
631520	636080	first of all, from the philosophical side, very heavy influence of Wittgenstein about how we
636080	641600	address these problems at all, then lots of global workspace theory and a certain kind of
641600	647280	global workspace architecture, how that might be realized in the brain, drawing also on the
647280	653600	work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea,
654160	657600	and putting all these things together into one big book.
658240	662560	Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper
662640	667520	and your consciousness paper. But two things that did trigger or prick my ears up,
668160	673520	computationalism, which is quite interesting, because some folks in the cognitive science arena,
673520	681440	especially with the fouries, like examples to escape from computationalism. We did a show on
681440	687600	cells, Chinese room argument the other day. He's probably one of the most known people who do
688400	691200	issue computationalism. So what do you think about that?
691200	696080	Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it
698880	706000	comes out of a kind of computational architecture. But in fact, where I took it was very much moving
706000	713200	away from that original presentation, which drew heavily on a kind of quite an old-fashioned
713200	717120	architectural perspective, sort of boxes and how they communicate with each other and so on.
718000	721760	And I was much more interested in taking it in a direction which is very much more
721760	729200	connectionist and drawing much more heavily on the underlying biology and neuroscience,
729200	733600	which in fact is also a direction that Bernie Barnes himself had moved in, because the book
734640	741280	that originally put forward his theory is from 1988. So that was the predominant way of thinking
741280	747840	at the time was this very computational cognitiveist perspective. So by 2010, when my book was
747840	754160	published, I was very much more interested in a kind of more connectionist perspective on things.
755120	759440	So that's the way that it's portrayed in the book, the theory.
760000	767120	Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation,
768080	772880	because people talk about the church-turing hypothesis and this idea that the universe
772880	778080	could be made of information, which is quite interesting. But do you believe that the world
778080	782240	that we live in could be computationally represented and computed?
784240	787600	Well, I'm not sure that I have a belief on that particular one.
789760	795520	So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on
795520	802400	quantum mechanics, and he thinks that quantum effects are important for consciousness. But
802400	808000	I mean, that's very much a minority, a tiny minority view within the people who study
808000	814480	consciousness from a scientific standpoint. And so I don't really subscribe to that
815040	819200	interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke
819200	824080	to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists,
825040	831200	their big argument is about the subject of experience and the limits of our cognitive
831200	837920	horizon and the inability really for us to reduce things into a comprehensible framework of
837920	842800	understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched
842800	850800	right into some really big, difficult topics here, right? So in my book, Embodiment in the
850960	855120	Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of
855120	863040	consciousness. But one of the big sort of outstanding things for me in one of the
863040	867520	outstanding questions that I have not really answered, I felt in that book, is very much
867520	875520	related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the
875520	882320	idea that there's a sort of intuitive idea that maybe there can be very exotic entities,
882320	888560	very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of
888560	894800	consciousness there that we could barely grasp its nature. And this is a sort of natural
894800	899200	intuitive thought. And especially when we look at other animals, like bats, and especially if we
899200	905360	look at an animal that's a bit different from us, then we get hints that there's some
905760	911280	one at home, as it were, and that there's consciousness there. I think we, I'm sure all
911280	918320	of us believe that cats and dogs, and many other animals are conscious and are capable of suffering
918320	924640	and having awareness of the world that's like our awareness and are aware of us and each other.
928000	934080	I mean, I take that as almost axiomatic. That's just the way we treat those creatures.
934080	938640	But then when we think about something like a bat, it's very different from us. So the natural
938640	944320	thing thought is that maybe what it's like is very, very different from what it's like for us,
944320	952800	and it's a natural thought to express. And of course, Nagel takes that thought
954640	963600	to suggest that there are something that is inaccessible to us, which is what is it like to
963600	969280	be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm
969280	977520	very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very
977520	982000	natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says,
982000	987600	for example, you know, nothing is hidden. So he's very, you know, and the whole private language
987600	992480	remarks are all about sort of saying, well, this intuition that we have that there's this
992560	999600	private realm of experience is actually just, it's just a philosophical trick of the mind
999600	1005600	to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective
1005600	1013680	experience in others. And that's his whole thrust of his philosophy or that aspect of it
1013680	1017200	is to try and undermine that. So these two things are intention, right? So there's this
1017280	1022960	natural thought that bats, you know, it must be like something to be a bat, but what is it like
1022960	1026960	and how could we ever know? And then there's the Viconstinian thought, which is actually very
1026960	1031760	difficult to kind of really embrace. But it's that there's a sense in which nothing is really
1031760	1036720	metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't
1036720	1041280	know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined
1041280	1045680	their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden,
1045680	1049280	whereas Nagel's point is that there's something that's deeply, profoundly,
1049280	1053360	philosophically, metaphysically hidden, which is the subjective. Now we can extend that,
1053360	1060320	shall carry on. So I'm rambling now. So now we can extend that thought about bats,
1060320	1064160	now, you know, especially from the perspective of the sort of thing that I'm interested in,
1064160	1068400	to, well, not just bats, but what about the whole space of possible minds to use
1068400	1073600	Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,
1074480	1079200	you know, who surely there is extraterrestrial intelligence out there, it's going to be very,
1079200	1084640	very, very different to us. So, and then what about the things that we build? Maybe we can build
1085520	1091840	things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build
1091840	1095440	something that is also conscious, it's the kind of thing that's depicted in science fiction all
1095440	1099920	the time. In science fiction, it's often depicted as very human-like, but there's no reason why it
1099920	1104880	should be human-like at all. And so we can imagine these very, very exotic entities, and then the
1104880	1109200	question is even bigger, you know, there could be something that we, we won't even be able to recognize
1109200	1113760	that there was even the possibility of consciousness, but maybe it's buried there inside this complex
1113760	1118720	thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this
1118720	1124240	paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian
1124240	1130640	perspective encompass this possibility as well. Yeah, and maybe we should talk about that before
1130640	1136640	the language paper, just because it's, it's what we're talking about now. But there's a few things
1136640	1141440	you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the
1141440	1148320	machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know,
1148320	1154640	introduced this kind of mind-body dualism, you know, which was kind of a move away from
1154640	1158400	the previous desire to have a mechanistic understanding of the world that we live in.
1158400	1164400	Humans want to understand, and actually so many things in the world eludes our understanding.
1164400	1169520	And then that brings us on to David Chalmers' point that the hard problem of consciousness,
1169520	1175280	which I suppose is an extension of the mind-body problem. And it's, as you were saying, this
1175280	1179920	little bit extra, right? So we think about, and I agree with Chalmers that intelligence and
1179920	1184800	consciousness are likely entangled or would co-occur together. But he always said that there's
1184800	1190560	function, dynamics, and behavior. And then there's that little subjective thing on the top. And for
1190560	1195280	Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just
1195280	1199760	something on top. It's not really requisite for anything else. And I believe it might be requisite
1199760	1204880	for intentionality and agency as so did. But what's your take?
1204880	1208480	Well, it's interesting because the whole way that you put that and the whole way that
1208480	1213680	people often talk about this thing is you speak about consciousness. Like, there's this thing,
1213680	1217920	which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe
1217920	1225360	it's this, maybe it's that. But I think that whole way of talking is, which is natural for us
1225360	1229200	in many everyday situations. But when it comes to this kind of conversation, I think that whole
1229200	1235440	way of talking is maybe not quite right, because we're thinking of consciousness as this, you know,
1235440	1239200	we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to
1239200	1244160	take a step back and we have to say, well, when we talk about, when we use that word,
1244160	1248720	conscious or consciousness, so we use it in all kinds of different ways in different contexts.
1248720	1255200	And so when we talk about, you know, we might talk about it in the context of an animal, we might
1255200	1261520	say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see
1262080	1267120	the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see
1267120	1271840	the squirrel, you know, and it can smell more like you'd smell all of these things as well.
1272400	1278080	So we use consciousness, you know, we talk about consciousness in that sense. And we also talk
1278080	1283520	about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts
1283520	1290560	and we talk about our inner life and we use consciousness to encompass that as well.
1291520	1297920	We often use consciousness in the context scientifically of a distinction between
1298720	1302640	conscious and unconscious processes. And that's a very interesting distinction because
1303360	1308640	when we're consciously aware of a stimulus as humans, then a whole lot of things come together.
1308880	1315040	We're able to kind of like deal with novelty better, we're able to report it, we're able to
1315040	1321280	remember things better. So whereas when we perhaps are unconsciously or there's a kind
1321280	1325680	of unconscious processing of the stimulus, then we still can respond to it behaviorally, but
1326560	1330320	and it can have queuing effects and so on, but it doesn't have all those other things.
1330320	1336320	So this and that's kind of, there's a kind of integrative function for consciousness there.
1336320	1342240	And then on top of all of that, there is the capacity for suffering and joy that comes with.
1342240	1349280	So often there's valence to consciousness, you know, so that's another thing.
1349280	1352800	So all of these things, they come as a package in humans, but when we speak about
1353360	1361200	edge cases, then these things come apart and we need to speak about them separately, I think.
1361200	1366240	Fascinating. I mean, there are two kind of minor digressions there. I mean,
1366240	1370000	you were talking about these planes of consciousness, which is also very interesting.
1370000	1374640	And maybe we could get into the integrated information theory or the global workspace
1374640	1377760	theory just for the audience, just to give them some context.
1377760	1380560	Yeah, sure. Or do you want me to say a few words about that?
1380560	1381520	Oh, please, yeah.
1381520	1387920	Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness.
1388720	1392160	And you just mentioned two of the leading ones, which are global workspace theory and
1392160	1397600	integrated information theory. And so global workspace theory. So that's, that's Bernie
1397600	1403360	Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues.
1403360	1408240	So the idea there is it's, it does rest on this sort of architectural idea, which is that,
1410160	1415440	which is that we think of the brain, the biological brain as comprising, you know,
1415440	1419600	a very large number of parallel processes. This is kind of a natural way to think of the brain,
1419600	1425120	a large number of parallel processes. And it, and the global workspace theory posits a particular
1425120	1430480	way in which these, these processes interact and communicate with each other. And that is via
1430480	1436000	this global workspace. And the idea there is that, is that there are sort of two modes of
1436000	1443040	processing that go on. So in one mode of processing, the, these parallel processes just do their,
1443040	1449440	their own thing independently. And in the other mode of processing, they are working via this
1449440	1454640	global workspace theory. So the idea is that they, you might think of them as, as, you know,
1454640	1459680	depositing messages, if you like, in this global workspace, which are then broadcast out to all
1459680	1463440	of the other processes. So, so it's, so there's this kind of, but I think thinking of it in
1463440	1467600	terms of messages is not quite the right way of thinking of it is better to think in terms of
1467600	1471840	kind of signaling and information and so on. But that's a natural way to think of it. But
1471840	1479600	so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence
1479600	1485200	to all the other processes. And that's the global kind of broadcast aspect of it. And that's when
1485200	1490640	consciousness, well, that's when information processing is conscious, according to global
1490640	1494560	workspace theory, as opposed to when it's all just local and the processes are doing their own
1494560	1500480	thing. That's, that's not that that processing is not conscious. So there's a dist, so it's
1500480	1505200	about teasing out this distinction between conscious information processing and unconscious
1505200	1511040	information processing. Now, all of those terms, by the way, are deeply philosophically problematic
1511040	1515360	and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all
1515360	1520320	in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about
1520320	1524800	so the essential idea, though, is to do with broadcast and dissemination of information
1524800	1529120	throughout the brain and going from like local processes and help them having global influence.
1529200	1532240	And that's what consciousness is all about according to global workspace theory.
1533200	1537920	Okay, so integrated information theory. So I think so integrated information theory,
1537920	1548000	which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible
1548000	1552640	in some ways with with global workspace theory. But I don't think that's, that's true. I think
1552640	1556320	I think that there's a lot of synergy between the two theories, in fact.
1556800	1563920	But but that's because they so they come with the same for integrated information theory
1564640	1571040	has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down
1571040	1577120	a property, which is almost like a physical property, which is identical with consciousness.
1577120	1582800	So you can actually speak about the amount of consciousness in any system that you that you
1582800	1588080	look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of
1588080	1593680	how much consciousness is present in the system, like, like part of your brain, your whole brain,
1593680	1599440	or you as a person, or a flock of bats, or whatever, so you can or toaster, you know,
1599440	1604720	so you can give a number to how much consciousness there is, there is there according to his theory.
1604720	1611360	And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's
1611440	1617120	all about trying to see how much information is processed by the individual parts of the system
1617760	1624240	versus how much information is processed by all the parts put together. And it's and it's to do
1624240	1630320	with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how
1630320	1637120	much consciousness there is there. And, and in a way, it actually has some synergies. If you as
1637120	1642160	long as you don't think that it's necessarily measuring, you know, this property of the of
1642160	1646560	the universe, which you can put a number on. But it has some synergies with global workspace theory,
1646560	1654000	because they're both distinguishing between global holistic things versus local things. And the
1654000	1660640	and the consciousness is in the kind of global holistic processing versus the local, you know,
1660640	1664640	local processing in both those theories. So there's a kind of, you know, there's some
1664640	1668640	intuitions that they have in common, I think. Interesting. And it also reminds me a little
1668640	1675120	bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain
1675120	1681520	types of information processing. And the processing must represent causal structures as well. So it
1681520	1687360	can't it's it's not an appeal to panpsychism per se. And although with with all of the things
1687360	1691520	that you've just spoken about, what do they work in another universe? I mean, I guess what I'm
1691520	1697520	saying is, is it just the the physical and the information processing or in a different universe
1697520	1702000	might it not emerge in the same way? Yeah, which depends what you mean by a different universe,
1702000	1705280	I guess. What do you mean by a different universe? Well, if the laws of nature were different.
1705840	1708800	Yeah, okay. So if the laws of physics were different.
1710560	1718800	Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an
1718800	1727440	ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess
1728720	1738160	I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I
1738160	1747520	subscribe to these to these isms. So what I really mean by that is that is that I imagine that a
1747520	1752560	system that is organized in a particular way functionally in terms of its information processing.
1752560	1758720	And if that system is in is embodied in the broadest sense, and, you know, and meets lots of
1758720	1764560	other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word
1764560	1770240	conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature.
1770800	1777200	So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need
1777760	1782400	to give rise to the behavior you need to talk about thing, the thing in a certain way.
1783520	1787120	My question today, because I posed this question to Chalmers last week, because he's also a
1787120	1791600	functionalist. And I agree with the degree of functionalism describing intelligence,
1791600	1795760	but less so with consciousness, you know, there's not a Turing test for consciousness, for example.
1796240	1801440	But the thing is with functionalism, we're at risk of doing what you said people do with
1801440	1805120	large language models, which is anthropomorphizing them, because these functions are intelligible
1805120	1809360	to us. And then our conception of intelligence becomes somewhat observer relative.
1813760	1821840	Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious
1821840	1830240	to you, but not to someone else? Well, so, so, so in all of these cases, I mean,
1830240	1835920	I think it's about the words that we use in our language to talk about the things. So, so,
1835920	1841440	so if there's someone else is someone just like us, right, then we have to and if we want to use
1841440	1846160	the words in different ways. So, so the large language models are a great case in point, right.
1846560	1853280	So, so suddenly we're arriving at a point where somebody can describe something as conscious.
1854240	1858480	And others can say that's rubbish, you know, it's not that's not true at all. And so we,
1858480	1863280	so we've, we've arrived at a point where these philosophically problematic words, which,
1864240	1870400	which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in
1870400	1874960	agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so
1874960	1878960	much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an
1878960	1883520	anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain.
1884880	1891760	Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know,
1891760	1896960	that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then,
1897760	1902880	and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action.
1902880	1907360	So there we're using the terms in ways that we all understand. But now we're getting to a point
1907360	1912640	where suddenly, these words or these concepts are being used, you know, we don't have an way,
1912640	1917440	we don't have agreement about how to use these words, right? Because it's, there are these exotic
1917440	1922400	edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there
1922400	1931840	a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say
1931840	1936480	is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to,
1937120	1943360	I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive
1943360	1947600	at a new consensus about how we use these words. So that might mean that we extend the words,
1947600	1952960	we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the
1952960	1958560	world from self awareness, from integration, cognitive integration from the capacity for
1958560	1963280	suffering, because suddenly we have things that where that they don't all come as a package. And
1963920	1967600	when we need to kind of be a bit more nuanced in the way that we use these words, we need to use
1967600	1972400	them in new ways. But then there's a kind of transition period, because we don't, you know,
1972400	1976640	we're all arguing about how to use these words all of a sudden, because we've got weird edge cases.
1976640	1981280	So there's going to be a time when it'll take a time for language to settle back down again.
1981280	1987280	So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you
1987280	1994800	like, but then, but then there's a kind of consensus needs to emerge, right? But so many
1994800	2001120	things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible.
2001680	2007680	And what platonic? Because what we're talking about here is reductionism and the, I mean,
2007680	2012000	the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,
2012000	2016400	complex phenomenon beyond our cognitive horizon. And as much as we don't want to,
2016400	2020720	we use functions derived from behavior to have some common understanding of this thing.
2020720	2024560	But I wasn't being reductionist, was I? Do you think I was being reductionist?
2024560	2031120	Well, no. So you said that the language game converges. And in some cases, we will arrive on
2031120	2035360	a common definition, but like you can bring in Hofstatter as well. Well, not a common definition,
2035360	2041680	but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement,
2041680	2046880	right? So that's what I, and the reason why I mean, I would, and the reason I would balk at
2046880	2051920	using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well
2051920	2058640	to any kind of ism is because I just think that that may be the way things are organized when
2058640	2064640	you take them apart. So, you know, brains, right, when you examine them on the inside,
2064640	2070800	like animal brains, you might look at how an octopus's brain works. And that might inform
2070800	2075280	whether you think that it suffers, can experience this pain or not. Or we might break apart, you
2075280	2079920	know, an AI of the system of the future, right? You know, and we might break it apart and we may
2079920	2085440	look at its functional organization. And that all is just is grist to the mill of how our language
2085440	2091360	might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this
2091360	2097600	is that it with some big metaphysical capital letters on the is, right? That's really important.
2098320	2103680	So the functional organization of these other things, which when we study and look at it,
2103680	2109120	is all just part becomes part of a conversation that eventually is going to help us to settle on
2109120	2114560	maybe new ways of talking about these things. I think we agree with each other. I think the
2114560	2118880	difference is, so with the parable of the blind men and the elephant, all of the men around the
2118880	2124480	elephant saw something which was part of the truth. And I think that's what we're describing with
2124480	2131440	the function. So we can all agree on what perception means or what action means. The thing is,
2131440	2135440	there will be many other functions that will represent a different slice of that cognitive
2135440	2139520	phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because
2139520	2143200	there's lots of people coming with kind of like new ideas and new theories. I mean,
2144400	2148320	Anil Seth, for example, have you had Anil on your on your not yet being right?
2149200	2154960	So Anil's written this great book called Being You. And Anil brings in a whole kind of new set
2154960	2162080	of ideas. He's particularly interested in the sort of top down effects on perception, top
2162080	2166160	down effects on perception. So then he brings in this kind of top down influence and perception
2166160	2171520	as a big part of things. And then there's Graziano has written this book using this about
2171600	2178800	his attention schema theory of consciousness. And that's, and, you know, there's a whole set
2178800	2182640	of interesting ideas there. And I think you're absolutely right. I think there's, I think there's
2182640	2191120	aspects of all of these things all feed into, you know, all feed into the way, you know,
2192320	2200080	brains and animals work and all of them feed into the, you know, why they behave the way they do
2200080	2204560	and why we use those words when we use those words, you know, conscious and aware and so
2204560	2210560	fascinating. We'll get to your article in a second. But as someone who has such a diverse
2210560	2215120	and interesting background, who is allowed to ask these philosophical questions? So it reminds me
2215120	2220240	and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers
2220240	2225600	about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question.
2225600	2229360	You know, I mean, so why should I have any right to speak about any of these things at all? Because
2229360	2238000	I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to,
2238000	2241600	to, I guess there are two things, right? There's, I guess, I guess there's, there's
2241600	2246640	in that kind of discussion between the neuroscientists and the, and the philosophers. So there you,
2246640	2250160	there you're not talking about, you know, the everyday conversation that we're all having as,
2250160	2255840	as, as humanity or as English speakers, or as Chinese speakers about how we use these, these,
2255920	2260720	these words. So there it's a much more kind of confined to the, to, to different, two different
2260720	2270400	schools or disciplines within academia. So there, I mean, I do think that the people who work in AI
2270400	2276000	and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical
2276000	2279840	debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with,
2279840	2286240	with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101,
2286880	2291440	which people should at least be aware of Descartes arguments and then Chalmers, and the different
2291440	2295920	kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean,
2295920	2300160	you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you
2300160	2305920	hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists
2305920	2312160	need to, you know, you know, they need to kind of have a, a past to enter the conversation,
2312160	2317760	which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with
2317760	2322560	the, with the ethics folks, actually, because, because we have a lot of them fields of expertise,
2323120	2326720	and engineers should learn more about ethics. Yeah, absolutely. But when they do have an
2326720	2331280	opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit
2331280	2337040	naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but
2337040	2341520	that's an interesting and the difficult area in itself. Because of course, you know, you,
2343120	2348240	as a scientist, it's important that you take responsibility as a scientist and the, and that
2348240	2352560	you take, you know, some ethical responsibility. But at the same time, you know, you've only got
2352560	2358240	so much time to become an expert. So, so it's difficult to at the same time, take ethical
2358320	2364880	responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read,
2364880	2371280	you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody
2371280	2377360	again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses
2377360	2382400	teach, you know, ethics, these days, many kind of science and computer science. It's part of,
2382400	2386800	you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting
2386800	2390960	analogy between the functionalism that we were speaking about in consciousness. I mean, even
2390960	2395360	in our own research domain, we have the function of ethics, and we have linguists, and we have
2395360	2399360	all sorts of different people. And that is the blind man and the elephant. And, you know, I
2399360	2405040	tend to believe that even though the views from these diverse folks are inconsistent, diversity
2405040	2410080	is very important. Oh, incredibly important. Intellectual diversity is, you know, every
2410080	2414720	kind of diversity is important. And intellectual diversity is immensely important. And having
2414800	2419680	these conversations, these interdisciplinary conversations is absolutely, you know, essential.
2419680	2424800	So at least if people are talking to each other, that's a really, really positive thing, I think.
2424800	2429840	Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way,
2429840	2435120	Chalmers took a very functionalist approach to talking about consciousness. But I guess,
2435920	2440080	after that tweet, everyone in the community started thinking about and talking about
2440080	2443840	consciousness. So maybe let's just start from that tweet. How did you find it?
2443920	2451280	Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large
2451280	2460240	language models are slightly conscious. And then I replied, tweeted back in the same sense that
2460800	2467200	may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean,
2467200	2471840	I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent
2471840	2477920	out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you
2477920	2488400	know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think
2488400	2496080	about, about what he said at that point. But then, but then I after tweeting my, my flippant
2496160	2502880	response, I then I was violating all my own Twitter rules in in just sending back a flippant
2502880	2509040	response, because I generally don't do that. I would rather kind of, you know, be professional,
2509040	2513360	engage professionally. And so I thought it's very important to follow that on with a, you know,
2513360	2518720	with a little explanation of why, you know, why I thought that it wasn't really appropriate to
2518720	2524160	speak about today's large language models in those terms. Yeah. And for me, the number one thing is
2524160	2532560	to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us
2532560	2538400	being able to use that, that word, use words like consciousness and so on, you know, in the way that
2538400	2544640	we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something
2546160	2551360	that that inhabits our world. And by inhabits, I don't mean just has a physical, you know,
2551360	2554960	like a computer is obviously a physical thing in our world, but inhabits our world means that,
2555680	2562480	you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the
2562480	2570320	same world as us and interacts with it and, and, and, and you know, and interacts with the objects
2570320	2578800	in it and with other, with other creatures like ourselves. So, so that to my mind, that is,
2578800	2584560	that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase
2584560	2589280	trans channeling Wittgenstein that that only in the context of something that
2591040	2597440	that exhibits purposeful behavior, do we speak of consciousness. And the way that that's
2597440	2602960	phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things,
2602960	2607840	because you notice that he's saying that he's making what he's saying is actually he's talking
2607840	2612720	about the way we use the word. So he's not making a metaphysical claim. This is essential. He's
2612720	2618800	saying that this is just this is the circumstances under which we use this word. So we use this word
2618800	2624720	in the context of fellow creatures, basically. And so, so that's the kind of the starting point.
2624720	2629040	So a large, and of course, we, of course, we talk to people on the telephone and over the
2629040	2634400	internet and so on. And we don't, you know, we may not, you know, we can't see them or anything.
2634400	2640880	So we, but, but, but we still, we know that there is, you know, or we assume, we've always been
2640880	2644720	able to assume up to this point that there is a fellow creature at the other end. And that's the
2644720	2651040	kind of grounding for kind of thinking that way and using using that word. Now that is absent
2651040	2655680	in large language models. So large language models do not inhabit the world that we do.
2659280	2664720	Now, I mean, we have to caveat that because, of course, it's possible to embed a large language
2664720	2670800	model in a, you know, in a, we always do embed it in a larger system. It might be very simple
2670800	2675200	embedding. It might be just a chatbot, or it might be much more complicated, like it might
2675200	2680560	be be part of a system that enables a robot to kind of move around and interact with the world
2680560	2687120	and take instructions and so on. So there was a great, some great work from Google with their
2687120	2691760	Palm Seican robot, for example, where there's this embedded large language model. So, so,
2691760	2697120	so there you're kind of moving in a, in a direction where maybe where these, where these words, you
2697120	2704080	know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.
2704080	2708800	Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,
2708800	2714560	right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for,
2714560	2719600	for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,
2719600	2723360	but at least it would, you'd meet the necessary conditions, right? Yes. But the large language
2723360	2730240	models by themselves do not meet even, they're not even candidates. Yes, I agree. And we,
2730240	2734000	there's so many things we can do here, because we can, we can talk about embodiment in general. I
2734000	2739360	mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a
2739360	2744160	representationalist view of artificial intelligence or, or rejecting, rejecting a representation.
2744160	2748080	Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best
2748080	2752880	representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking
2752880	2757600	about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity,
2757600	2761760	and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your
2761760	2766080	paper, you said, although the language model component of SACAN provides natural language
2766080	2770480	descriptions of low level actions, it doesn't take into account what the environment actually
2770480	2775280	affords the robots. And there's this whole affordance thing as well. So, I mean, how do you
2775280	2782800	think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we
2782800	2790720	have as of, you know, the end of 2022 of something that we really can describe as, as, as, as
2790720	2797120	intelligent as generally intelligent is, is the biological brain, biological brains of humans,
2797120	2804960	but also of other animals. And the biological brain, you know, at its very, it's very kind of
2804960	2812320	nature is it's there to help a creature to move around in the world, to move, right? It's there
2812320	2819520	to move, help to guide a creature and help it move in order to help it survive and reproduce.
2819520	2823920	That's what brains are for. So that's what that from an evolutionary point of view, that's that
2823920	2832160	they developed in order to help a creature to move. And they are so they and they are, you know,
2832160	2839360	they're the bit that's comes between the sensory input and the motor output. And as far as you
2839360	2844640	can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their
2844640	2850480	purpose is to intervene in the sensory motor loop in a way that benefits the organism. And
2850480	2858560	everything else is on built on top of that. So, so, so the capacity to recognize objects in our
2858560	2865200	environments and categorize them and the ability to kind of manipulate objects in the environment,
2865200	2872400	pick them up and so on. And all of that is there, you know, initially to help the, the, the organism
2872480	2881840	to survive. And, and, and, you know, and that's what brains brains are there for. And then,
2881840	2889600	then when it comes to, you know, the ability to work out how the world works and to do things
2889600	2895040	like figure out how to gain access to some item of food that's difficult to get hold of, then all
2895040	2901840	kinds of cognitive capabilities might be required to understand how you get inside a, you know,
2902480	2909280	a shell or something to get the fruit inside it or something like that, complex cognitive
2909280	2913760	abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of
2914320	2918640	more and more and more complex cognitive cognition until we get to language and, you know,
2918640	2923360	we need to interact with each other because that that's all very much a part of it, the social,
2923360	2928320	social side of it. And then language is part of that. So as I see it, it's all built on top of
2929280	2934160	this fundamental fact of the embodiment of the animal and the organism. So that's in the
2934160	2941280	biological case. So that's sort of our starting point. So, and so that seems to me to be the,
2941280	2946720	the most natural way to, to understand the very nature of intelligence.
2947520	2950640	Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell
2950640	2954640	recently had a paper out talking about the Four Misconceptions and one of them, of course,
2954640	2959440	citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've
2959440	2964160	basically spoken about. But her fourth one was about embodiment. And she spoke about this in
2964160	2968160	her book as well. She said that one of the misconceptions of AI is that people have this
2968160	2972080	notion of a pure intelligence, you know, something which works in isolation from the
2972080	2976720	environment. And you're talking about social embeddedness and embodiment. And I guess my
2976720	2981920	point with the complexity argument is I'm saying that the brain itself doesn't actually do everything.
2982000	2987440	It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So
2987440	2994400	there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew
2994400	3000960	Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course,
3000960	3005280	that's very much part of it is the, is the idea that, you know, there's the extended mind, we use
3005280	3013600	the environment, you know, as, as, as a kind of memory, for example, we deposit things in the
3013600	3020000	environment, writing, you know, as an example and so on. And then there's, people talk about
3020000	3024720	morphological computation, I'm sure you're familiar with that. Well, so that's the idea
3024720	3031040	that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes,
3031040	3038560	you know, the aspects of intelligence are actually outshort outsourced into the physical shape of,
3038560	3044400	of your body. So where you might think about designing a robot, where you, where you put a lot
3044400	3049040	of work into the control aspect of it, so that it's, so that it can kind of walk in this very
3049040	3053840	carefully engineered ways that it's always permanently stable, or alternatively, you can
3053840	3060160	make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you
3060160	3066080	kind of rely on the combination of the physics of it constantly falling with, with a control system
3066080	3072000	that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very
3072000	3077840	much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this
3077840	3081920	sort of way. So I think that's, I think that's a very natural way of thinking.
3082480	3086080	But in a way that this gets to the, to the complexity argument, because I guess what I'm
3086080	3092480	saying is that life is much more brittle than anyone realises. You were just pointed to some
3092480	3097200	sources of brittleness that most people never would have thought of, which is, which is fascinating.
3097200	3103280	So I think there's a very important relationship between embodiment and language. And this also
3103280	3111840	brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied
3111840	3119360	phenomenon. It's, it's, it's something that is, it's a social practice, something that take that
3119360	3125520	it's a phenomenon that occurs in the context of other language users who inhabit the same world
3125520	3130640	as we do. And where we have kind of like joint activities, we're triangulating on the same world,
3130640	3135520	and we're acting on the same world together. And that's the that's what we're talking about when
3135520	3142400	we use language. So there's this, so that, that's an inherently convincing view of language. And I
3142400	3148640	think it's deeply profoundly correct view of language. That's, that's what, that's what language
3148640	3153680	is there for us is so that we can talk about the same things together so that we can, our collective
3153680	3162640	activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's,
3162640	3168000	so I think that's a really important perspective on language is Wittgenstein perspective. And, and
3168000	3172880	embodiment is at the heart of it, embodiment and inhabiting the same world as our other language
3172880	3178880	users. And, you know, that's the way we learn language, we learn language by being around
3178880	3186480	other language users like our parents and carers and, and, and peers. And, and that's again a very
3186480	3193280	important aspect of the nature of human language. Now large language models, they learn language in
3193280	3198880	a very different way indeed, they do not inhabit the same world as us, they do not kind of sense
3199440	3204640	the world in the way that we do, they don't learn language from, from other language users,
3204640	3209680	from their peers in the way that we do. But rather, you know, will we know how large language
3210800	3215760	models work, there's trained on a very, very large corpus of textual, of textual data.
3215760	3220720	So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you
3220720	3227920	know, by the time they become a proficient language user at a young age. And what they're trained to
3227920	3234720	do is, is not to kind of engage in activities with other language users, but to predict what the
3234720	3239520	next, you know, what the next token is going to be, which is a very, very different sort of thing.
3239520	3244160	So they're very, very different sorts of things. And the, and the role of embodiment is really
3244240	3250480	really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen,
3250480	3253840	he's really, really interested in the grounding problems. I mean, would you mind just speaking
3253840	3258720	about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course,
3258720	3265760	this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998.
3265760	3271200	Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called.
3271200	3280640	And, and, and, you know, he does argue broadly that the symbols in AI systems,
3282160	3285120	the kinds of AI systems he was thinking about at the time were kind of, you know,
3285120	3289040	sort of expert systems say or something like that. And the symbols there are not grounded,
3289040	3293680	they're provided by the human programmers and they're just sort of typed in. Whereas for us,
3293680	3299520	for us, the words we use, those symbols are grounded in, in our activity in the world. So
3299520	3305440	that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with
3305440	3309200	other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And
3309200	3316400	we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all
3316400	3321760	kind of contextualize that. But all of that, that that is kind of grounded in the real world in
3321760	3327360	our and in our perception of the things in question. So that so that's this. So that's what
3327360	3332480	sort of is meant by grounding or that at least that's the original meaning of the word grounding
3332480	3337920	from Stephen Harled's paper. And I think that's a really, really important concept because,
3338880	3344800	because, you know, in an important sense, large language models, the symbols that are used in
3344800	3350480	large language models are not really grounded in that kind of way. Now this, you know, I should
3351120	3356640	be absolutely clear that large language models are immensely powerful and immensely useful and,
3356800	3362240	and so that, you know, so, but it's interesting that to what extent the lack of grounding here
3362960	3369200	that we have in today's large language models, you know, might affect how good they are. So,
3369760	3376560	so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating
3376560	3383040	and, and if you look at multimodal language models that maybe we'll talk about an image that you
3383040	3387840	present to them, then, you know, they, you can have a very interesting conversation, but sometimes
3387840	3392640	they'll go off pieced and start talking about things that are not in the image at all and as
3392640	3399760	if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that
3399760	3405440	so the words are not kind of grounded in the images in, in quite the way that we would like. So
3405440	3409360	that's, it's an important topic of research, I think. Yes, indeed. And although some people do
3409360	3413520	believe there's this magical word called emergence and possibly some emergent symbol
3413520	3417280	grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce
3417280	3423280	your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.
3423280	3430400	And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think
3430400	3436480	in today's large, large language models. So, so, so, you know, even though they're, they're, so
3436560	3442000	they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I
3442000	3445760	should have made this maybe a bit clearer in the paper, but of course, it's not always next word
3445760	3450160	prediction, because there are different models learned to actually, you know, predict what's
3450160	3455120	in the middle of a, of a sequence rather than kind of generally, you know, they're interested in,
3455120	3462080	in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're,
3462080	3466560	so they're trained to just to do next word prediction. Now, the astonishing thing is,
3466560	3473920	as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful,
3474560	3481520	can be used to do all sorts of extraordinary things. Because if you provide, you know,
3481520	3487280	the prompt that describes, you know, describes some kind of complicated thing, you know,
3487280	3495680	situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's
3495680	3500320	20 meters long and three meters. Well, you know, you start to describe this thing, you know, and
3500320	3506880	each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large
3506880	3512720	language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right?
3512720	3518560	Well, this is astonishing, because it was only trained to do next word prediction. And so there's
3518560	3523680	a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing
3523680	3530800	next word prediction, because in the vast and immensely complex distribution of tokens in human
3531360	3539440	text that's out there, then the correct answer is actually the thing that's most likely to come up.
3539520	3544480	And that's, but it's got to discover a mechanism for producing that, right? And so that is where
3544480	3549840	the emergence comes in. And I think that, you know, these capacities are astonishing, the fact
3549840	3554720	that they, that it can discover mechanisms, you know, emergently that will do that sort of thing.
3554720	3558560	Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan
3558560	3563520	of emergence. And, and as you say, the decode is trained to predict the next token or the
3563520	3568640	denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are
3568640	3572480	different ways of thinking about emergence. So there's weak emergence, which might be thought as
3573360	3578560	computational irreducibility, or a surprising macroscopic change, or strong emergence when
3578560	3581520	it's not directly deducible from truths and the lower level to make, you know, lots of things.
3581520	3586960	Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably,
3587600	3592320	it's trained on something quite trivial. So all of this is about convention, right? All of this is
3592320	3597440	about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't
3597440	3602880	think, you know, I'm not making metaphysical claims about, about, about these things. So it's
3602880	3608160	all about what, you know, when is it appropriate to use words, to use certain words? And, and
3608160	3612000	because when, when this becomes problematic is when they're philosophically difficult words,
3612000	3620000	like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we,
3620000	3625760	I do think it's not unreasonable to, to, to use that term to describe what some of the,
3625760	3630640	these models do today. And that's partly because of the content neutrality of,
3631600	3637360	of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the
3637360	3643440	paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well,
3643520	3651840	you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets
3651840	3658560	complicated because we do use the word believes in this intentional stance way to, to talk about
3658560	3665280	ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's
3665280	3670560	British summertime, you know, you know, because we, and then, but then you'd say, then somebody
3670560	3675280	says to you, what you, what you mean, your, your car clock and think, you say, no, obviously,
3675280	3679280	I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to
3679280	3684320	these large language models, we start to use the words like thinks and believes and so on,
3684320	3688800	because they're so powerful, it starts to get ambiguous and yours, and your, and, you know,
3689520	3693360	and some people will say, well, actually, I really didn't mean that it can think or that it believes.
3693360	3699760	So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could,
3699760	3704240	could you tease apart that work? So you resist anthropomorphic language in terms of belief,
3704240	3709920	knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition
3709920	3715440	is that reasoning rather depends on those things that I just said before. Well, I, so I think it
3715440	3722880	doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because,
3722880	3729440	because reason, because logic is content neutral. So if I tell you that every, could you just explain
3729520	3734880	what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense
3734880	3742720	syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and,
3742720	3746720	you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds
3746720	3752000	of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones.
3752000	3760880	Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung
3760880	3766480	forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean,
3766480	3772720	but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have
3772720	3780000	to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way
3780000	3785360	that a theorem prover can do logic, then so can a large language model do logic. So in that sense,
3785360	3790240	I think large, it is reasonable to use the word reasoning in that logical sense in the
3790240	3794080	context of large language models. I don't think that's a problem. Of course, we may think that
3794080	3798480	they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the
3798480	3805280	word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment
3805280	3812880	is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's,
3812880	3818480	it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper,
3819200	3826880	if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not,
3826880	3832560	it does depend upon facts that are out there in the world. And then to, to really have a belief,
3832640	3839680	at least you've got to be able to somehow try and kind of justify those facts, or at least, and you
3839680	3845200	got to be at least built in such a way that you can, you know, interact with the external world
3845200	3850960	and do that sort of thing, right? And, and verify that something is true or false or do an experiment
3850960	3856400	or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside
3856400	3861360	of ourselves and, and in order to establish whether something a belief is true or not. And so,
3862080	3866880	you've got to at least be capable of doing that. Whereas large language models, the bare bones,
3866880	3872560	large language model is not capable of doing that at all, right? Now you can embed it in a larger
3872560	3876080	system. This is a really important distinction that I've tried and make over and again in the
3876080	3881360	paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever
3881360	3886000	a large language model is used, it's not the bare bones, large language model, which just does
3886000	3890560	sequence prediction, but it's embedded in a larger system. When we embed it in a larger system,
3890560	3895280	well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot
3895280	3899040	that goes and investigates the world. So that's a whole other thing. But then you have to look at
3899040	3904800	each case in point and, and, and ask yourself whether it's a, whether, you know, whether we
3904800	3910560	really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just
3910560	3915760	in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of,
3915840	3922880	of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not
3922880	3928560	using the word in the way that we, in the full blown sense that we use it, where we talk about
3928560	3934480	each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful
3934480	3939120	way of thinking about artificial intelligence, allowing us to view computer programs as intelligent
3939120	3942880	agents, even though they may lack the same kind of understanding as a human. And then you cited
3942960	3948160	the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of
3948160	3952560	Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind
3952560	3958320	of like, it's just distinguishing what it, what it means to know, you know, for humans and, and,
3958320	3963600	and for machine. So I think it's, it's useful to think about something like Wikipedia. So,
3964720	3971600	so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?
3971600	3976960	And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia.
3976960	3982960	And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we
3982960	3987280	use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses
3987280	3991840	that word, hey, you know, I don't think you should use the word knows there. And you know, that would,
3991840	3996000	you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think,
3996000	4002400	these kinds of words in this ordinary, every day sense. And we do that all the time. And that's
4002400	4008640	sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls
4008640	4016880	the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs,
4016880	4022080	desires and intentions, because it's a kind of convenient shorthand. And especially if you've
4022080	4025840	got something that's a bit more complicated, like say your car sat down for something or
4025840	4031520	you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense
4031520	4036720	to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them,
4036720	4042080	right? And without getting overly complicated, without knowing the underlying mechanisms. But
4042080	4045280	there's an important sense in which we don't mean it literally. So you know, in the case of
4045280	4048880	Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say,
4048880	4053200	hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and,
4053200	4060000	and, and, and, and all, and all the things that, that go with us as humans actually knowing things.
4060000	4067120	And it's just a turn of phrase. Now, things get sort of interesting with large language models,
4067120	4072560	and with large language model based systems and the kinds of things that we're starting to see
4072560	4077280	in the world, because we're starting to get into this kind of blurry territory where it,
4077760	4083600	we're blurring between the intentional stance and, and, you know, meaning the meaning it literally.
4083600	4087920	And this is where we need to be really, really kind of careful, I think. So at what point does
4087920	4095520	do things shade over into where it's legitimate to use that word, you know, literally, in, in the
4095520	4101680	context of something that we've built, you know, I don't think we're at that point yet. And, and
4101680	4108320	we need to be very careful about, about using the word as if we were using it literally.
4109920	4114720	You know, that's the sort of anthropomorphization, because the problem is that we can then
4115600	4123280	impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there.
4124000	4128560	Yes. And I suppose we could tease apart knowledge. So it justified true belief from
4128560	4134160	knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization.
4134160	4136080	But you had Chomsky, you've had Chomsky on.
4136800	4141520	I can tell you a story about that. I mean, the recording messed up. So when we were interviewing
4141520	4145680	him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to
4145680	4150080	regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated
4150080	4155120	deep learning and how useless it was. And then we used deep learning to rescue his interview.
4155440	4161040	And he gave us his permission to publish it. That is wonderful.
4161040	4165120	So it's quite ironic. But no, he always says it's wonderful for engineering,
4165120	4167840	but not a contribution to science. Yes, sure. Yeah.
4169600	4172880	Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a
4172880	4176560	contribution to science. So who else have you had? I mean, you've had a lot of people on.
4176560	4181120	I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great.
4181120	4184960	So he's great. Andrew is somebody I do work with quite closely.
4185840	4190960	So it was interesting listening to him because Andrew had quite a big influence on this paper,
4190960	4196640	by the way. Oh, okay. But I think I might have had a bit of influence on him as well,
4196640	4202640	to listening to him. I think so. Because that interview was just after he'd read,
4202640	4206640	and he read my paper, gave me lots of comments. And we had a lot of discussion about it.
4207120	4214480	And that interview, looking at the recording date, was sort of just after this. And it's interesting.
4214560	4218800	I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting
4218800	4226240	because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know.
4226240	4231600	I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that,
4231600	4236480	actually, just when you were speaking. But it's funny because we've spent a lot of time arguing
4236480	4242640	with each other about it. And I often feel like we're coming from very different perspectives
4243200	4249840	on this. But in fact, I think there's convergence, really. What are your areas of disagreement?
4253360	4260080	Well, you see, I would have thought that Andrew would have been more on the side of,
4260080	4267280	we can do things without embodiment, and without grounding, or to kind of take grounding in a
4268080	4278960	more liberal sense. Because some people would talk about grounding, so they say that large
4278960	4286160	language models, they are grounded. Prompt Engineering is the process of using prompt
4286160	4292320	prefixes to allow LLMs to understand better. So the context and the purpose of a conversation
4292320	4296800	in order to generate more appropriate responses. What do you think is going on with Prompt
4296800	4302560	Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process
4302560	4307120	of allowing the models to understand better is what you're better. Of course, I don't think
4307120	4312000	guilty as charged. I don't think I don't think that's the right way of characterizing it at all.
4313280	4319760	So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's
4319760	4324560	something that's entered our world as AI researchers, very prominently, just in the
4324560	4330000	last two years. And it's amazing. Of course, we have Prompt Engineering in the context of
4330000	4335760	large language models. We also have Prompt Engineering in the context of the generative
4337360	4344320	image models as well, like Dali and so on. And that's really fascinating as well, how by
4345520	4350640	engineering the Prompt to be just the right sort of thing, you can coax the model into doing
4350640	4356400	something which you might not otherwise do. And it's a great example of how alien these things
4356400	4361040	are. Because if you were giving a human being the same instructions, then you wouldn't necessarily
4361040	4369040	do quite what you do with either an LLM or an image model in order to get it to do the
4369040	4375680	thing that you want it to do. You have to kind of get into the zone with these models and figure
4375680	4380800	out kind of what strange incantations are going to make it do the things that you want it to do.
4380800	4386320	Now, I think an interesting thing is that we may be looking at a moment, a very short moments in
4386320	4392880	the history of the field where Prompt Engineering is relevant. Because if language models become
4392880	4398080	good enough, then we're not going to need to talk to them in this weird way,
4398800	4403680	engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier.
4403920	4407440	Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the
4407440	4416480	case as they get better. But at the moment, you can use a strange incantation like thinking steps
4416480	4421840	and suddenly the large language model will be much more effective on reasoning problems than it
4421840	4425920	was if you didn't use the incantation thinking steps. So that's really fascinating. So what's
4425920	4430960	going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that
4430960	4437680	what the model is really trained to do is next word prediction. But we have to remember that
4437680	4446240	it's doing next word prediction in this unimaginably complex distribution. So we have to remember
4446240	4451760	that it's not just the distribution of what a single human would, the distribution of
4451760	4456480	the sequence of words that a single human will come out with, but of all the sort of text of
4456800	4462640	millions of humans on the internet, plus actually a load of other stuff like code and
4462640	4468160	things which we don't come out with in ordinary everyday language. Well, people do it deep
4468160	4476160	mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's
4476160	4485760	happening with prompt engineering is that you're sort of channeling it into some portion of the
4485760	4494640	distribution. So you're queuing it up with a prompt. And this kind of context is putting it
4494640	4499600	into some portion of this distribution. And that is what's going to enable it to do something
4499600	4504560	different than it would have done if you had a different set of words. And that would have
4504560	4510240	put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably
4510240	4515040	complex distribution. You're finding the bit of it that you want to then concentrate on.
4515120	4519280	Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree
4519280	4522960	with you that they are statistical language models. I'm also a fan of the spline theory of neural
4522960	4528400	networks, which is this idea that you just kind of tessellate the ambient space into these little
4529040	4534400	affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's
4534400	4538800	quite, it's quite a simple way of looking at it, because you were talking about emergence before.
4538800	4543600	And emergence is all about this paradigmatic surprise, a bit like the mind body dualism,
4543600	4547200	if you like, there's something that happens up here, which is paradigmatically
4547200	4550880	completely different to what happens down there. So on the one hand, we're kind of saying, oh,
4550880	4555680	they're just simple interpolators or statistical models. But on the other hand,
4555680	4559600	they really are doing something remarkable up here. So, so which is it?
4560480	4567760	Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand
4568720	4575040	these models in a in a in a more scientific way, which we surely do, you know, even if we're not,
4575040	4579280	even if we're not engineering them in an old fashioned sense of engineering them,
4579280	4584480	but, but rather they kind of, you know, emerge from the, from the learning process,
4584480	4590000	we still want to reverse engineer them to try and get as great as as as comprehensive
4590800	4595520	a scientific understanding of these things as possible. So, so we want to understand it all
4595520	4599360	these levels, right? We, of course, the foundation of that understanding is that we
4599360	4602720	need to understand the actual mechanisms that we've programmed in there, right? So,
4602720	4606320	though, you know, so you've that's essential, you want to, you know, if you want to really
4606320	4611600	understand these things, you've got to understand transformer architectures, the different kinds
4611600	4616320	of transformer architectures that you've got, the, you know, what happens when you use kind of
4616320	4621680	different parameter settings, whether it's sparse or dense, whether it's a decoder only
4621760	4626000	architecture, or how you're doing the tokenization, how you're doing the embedding,
4626000	4629760	when all of these things are essential to understanding, you know, and that's all at the
4629760	4634720	absolutely at the engineering level. So we want to understand all of that. But then we can do a
4634720	4638640	whole load of reverse engineering, you know, at another level, and do the sort of thing that
4638640	4645200	the people at Anthropic AI have done, for example, with, with these induction heads and, and, and,
4645200	4650320	and understanding in terms of transformers in terms of residual streams and induction heads,
4650320	4653920	which I think is fabulous work. So that kind of thing is looking, it's still
4654560	4659520	quite a low level, but it's kind of the next level up, and explaining a little bit about how these
4659520	4665360	things work, and work along those lines, I think is like really essential. And then the more complex
4665360	4670640	these things are, the, you know, the heart, the more we need to kind of ascend these levels of
4670640	4677120	understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one
4677120	4681120	that is the right one. It's, you want to understand that things are all levels.
4681120	4684880	Yeah, different levels of description. You said something before, which really
4684880	4689760	interested me. You said when the language models get good enough, maybe we won't need the prompts
4689760	4694640	anymore. And I'd love to explore that duality, because it's a similar duality to how we talk
4694640	4698720	about embodiment, you know, you can think of the language model being embodied in the prompt in,
4698720	4703600	in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts,
4703600	4708960	I think about them as a new type of program interpreter. And there are some remarkable
4708960	4713920	examples of scratch pad and chain of thought and even algorithmic prompting for getting
4713920	4719200	insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah.
4719200	4723360	And, you know, these, these models are not Turing machines, they're finite state
4723360	4727680	automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt
4727680	4732160	seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is
4732160	4737600	going to go away. But I think that the, and who knows, right? But, but I think that prompt
4737600	4746000	engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people
4746000	4750320	talk about that as being a kind of a whole new job description as prompt engineer. And so that,
4750320	4754640	that as a whole new job description, I'm not quite sure how long exactly that will last because,
4755760	4760400	because prompt prompting may be just, you know, interacting with a thing in a much more natural
4761360	4766080	language way in the way we would with another person, right? So, you know, I don't, I don't,
4766080	4771280	when I, when I, I don't have to kind of think of some peculiar incantation in order to,
4772640	4780080	you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you
4780080	4786960	know, to cook a meal together with somebody, we just, we just use our natural kind of forms
4786960	4791520	of communication. Of course, of course, it does involve, you know, discussion and negotiation,
4791520	4795680	but it's in this, it's just the same as we use with other humans, right? So, so it may be that,
4796240	4801520	rather than it being a peculiar thing in itself with all these funny phrases that just work
4801520	4807600	for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan,
4808480	4812400	thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots
4812400	4814400	of fun. Absolute honor. Absolute honor. Why?
