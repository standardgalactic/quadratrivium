start	end	text
0	5280	Coming up later in today's presentation, I'm wondering at what point we're just developing
6160	11040	complex math models to explain complex math models and we really haven't, you know,
11040	16720	made much progress along the interpretability axis. So you have something you don't understand
16720	21760	and you explain it with something you don't understand? If I have if I have some general
21760	26320	formula, just some very general formula, and then I go in there and I go, you know what,
26320	34240	this formula has five parameters. And if I make this 1.75 and that one one-third and this one two
34240	41520	and that one zero, and I call this the Megatron activation potential, and I go and write a paper
41520	47040	about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a
47040	52160	fancy mathematical passport and you got it published in some journal. And now everybody
52160	57440	has to memorize that as you know, the Megatron potential and kind of learn about it. And that's
57440	68080	a lot of what's going on right now is that it's really just a bunch of hacking.
74960	79760	Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning.
79760	87120	Enjoy. Interpretability has become one of the most important topics in machine learning.
87120	92560	And it's something that every data scientist needs to be familiar with. For hundreds of years,
92560	99280	we've had simple interpretable models like linear regression and rules-based systems.
100320	107360	But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models.
108320	115440	And of course, predictions from these models are not always so easy to explain. So as we start to use
115440	123360	these more powerful, nonlinear models to actually make decisions on real world matters, then it's
123360	129520	inevitable that our attention must now turn to interpretability and explainability. When I
129520	135040	first started learning about machine learning algorithms, I was told they could be dangerous.
135120	142800	They were hard to understand. They were black boxes. But as Christoph lays out, it turns out
142800	147440	there is a whole plethora of techniques out there to explain why a model made a certain
147440	153120	prediction. Some models like low-dimensional linear regression are intrinsically interpretable.
153920	159120	You can just look at the model coefficients and that tells you exactly how the model is working
159120	164880	under the hood. Then there is a whole suite of methods that will actually work with any ML model.
165280	170640	Like training a local surrogate or a global surrogate. There's also Shapley values,
170640	177440	which is a really cool technique that allows you to distribute blame for the prediction amongst
177440	183840	the input features in a really theoretically sound, really principled way. And then there are domain
183840	189840	specific methods. For example, to explain image models, you can try to highlight the most relevant
189840	196640	parts of an input image by making saliency maps. And there's more. You can look at things like
196640	203040	example-based explanations where you try to find the smallest change in the input data that would
203040	209920	cause the output prediction to change. So maybe with this awesome new interpretability toolkit,
209920	217120	we can start to dispel that myth that machine learning models are all just black boxes that
217120	223680	can't be understood and can't be trusted. Christoph Molnar is one of the most important people
223680	229760	in the interpretable machine learning space. In 2018, he released his magnum opus,
229760	235440	interpretable machine learning, a guide for making black box models explainable.
235440	240880	Interpretability is often a deciding factor when a machine learning model is used in a product,
240880	247040	a decision process or research. Interpretability methods can be used to discover knowledge,
247120	254480	to debug or justify a model and its predictions, to control and improve the model, to reason about
254480	260400	potential biases in the model, as well as increase the societal acceptance of models.
261040	267200	But interpretability methods can be quite esoteric. They add an additional layer of complexity
267200	272640	and the potential pitfalls require expert understanding. Machine learning models are
272640	278560	inherently less interpretable than classical statistical models, but typically they have a
278560	283440	better predictive performance and that's because of their ability to handle nonlinear
283440	289760	relationships and also higher order feature interactions automatically. But do we have
289760	296080	to suffer this implicit trade-off between the complexity of a model and the lack of our ability
296080	302160	to understand it? Simplistic model approximations can often mask important information and be
302160	308640	misleading as a result. In classical statistics there's an entire field called model diagnostics
308640	314560	to do exactly this, to check that assumptions and simplifications have not been violated.
314560	318240	This is something that does not yet exist in interpretable machine learning.
318880	324800	Interpretability has exploded and matured in the last few years, in particular since the
324800	331120	deep learning revolution. We now have a better understanding of the weaknesses and strengths
331200	336800	of interpretability methods. A growing number of techniques are available at our fingertips
336800	343680	that can lead to the wrong conclusions if applied incorrectly. Is it even possible to
343680	349600	understand complex models or even humans for that matter in any meaningful way?
350320	353280	That is one of the questions that we're going to be discussing this evening.
354320	360480	Molnar also recently released a couple of papers where he discusses some of the important pitfalls
360480	364800	of interpretable machine learning methods. So some of the things that Christoph Molnar is
364800	370800	really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician.
370800	377680	Also he is exasperated with some of the misguided causal interpretations from some of these IML
377680	382480	methods. He also points out feature dependence or situations where you have shared information
382480	387200	between features. It completely breaks many of the IML methods and this is something that he
387200	393600	focuses on a lot. He also focuses philosophically on the broader impact of interpretability and
394480	400160	what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick
400160	404800	through this paper, interpretable machine learning, a brief history, state of the art and challenges
404800	408880	and as well as pointing out some of the history of IML methods, we'll jump straight into one of
408880	414160	the challenges which is feature dependence. Molnar points out that feature dependence makes
414160	418960	attribution and extrapolation problematic. This is exactly what happens in partial dependency
418960	424720	plots for example. We are basically extrapolating and we are creating fictitious data points that
424720	429840	didn't really exist and these fictitious data points probably exist outside of the data distribution.
430480	437360	So Molnar thinks that the models that we build should reflect the causal structure in the world
437360	442960	but of course that is not really the case most of the time and he points out that statistical
442960	448880	learning is just reflecting surface feature correlations not the true causal structure beneath
448880	454240	the scenes. Causal structures would be more robust if we could actually capture them
454240	459760	and the predicted performance and learning causal factors is a conflicting goal which I think not
459760	464720	many people have thought about. So we need to think about when we can make causal interpretations
464720	469520	and a lot of work is underway in this field but being completely frank this is very nascent. There's
469520	475920	not really much out there at the moment. Molnar also points out this lack of statistical rigor
476560	481120	having been a statistician himself. He was exasperated when he came into the IML field just
481120	486720	to see that most IML methods do not even give you confidence estimates something which is
486720	492080	completely standard in the statistical world. Models and explanations are computed from data
492080	497360	which means they are subject to uncertainty but this is something which is just not captured
497360	502640	using current methods. He says that we need to be making distributional and structural assumptions.
502640	509280	He points out this risk of p-hacking something which is prevalence in the natural sciences.
509280	514000	This is something that could be coming to the world of IML very soon if we don't start thinking
514000	519520	about this more carefully. Molnar also points out that there is no accepted definition of
519520	525920	interpretable machine learning methods so it's not entirely clear how we can compare IML methods to
525920	531520	machine learning models. It's really easy to assess machine learning models because we have
531520	536880	benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well
536880	542800	but we can't really quantify how correct an explanation is and it doesn't really help that
542800	547680	there's a taxonomy of interpretability methods. There are objective methods like sparsity and
547680	554000	interaction strength and there are human-centered evaluations from domain experts or from lay people
554000	558640	and quite often you need to have quite a lot of technical knowledge to even understand
558640	564720	these assessments. He says that the setting of machine learning is too static. It doesn't reflect
564720	569360	how these models are actually used in practice and I really love this idea of thinking about a
569360	574320	process rather than thinking about just the model so he says we need to have a holistic view of the
574320	579520	entire process. He thinks that we need to think about how we explain predictions to folks from
579520	585760	diverse backgrounds, how we have interpretability at the societal level or at the institutional level
585760	589920	thinking much more broadly than we are at the moment. He also thinks that we need to reach out
589920	595280	to other disciplines for example psychologists and social scientists and he thinks that there's
595280	599760	lots of rich knowledge in computer science and statistics that we're just not using yet.
599760	605600	So in July of last year he also released this paper pitfalls to avoid when interpreting machine
605600	609920	learning models. In this paper he points out that there's a growing number of techniques
609920	614800	providing model interpretations but many will lead to the wrong conclusions if used incorrectly
614800	619440	and he goes on to point out many of those pitfalls. For example the first one is
619440	624160	assuming that the model generalizes well so assuming that the model has been fit correctly
624160	629440	if the model is underfit or overfit then the interpretation method will perform badly as well
629440	635680	and interpretation can only be as good as the model underlying it. So the next pitfall he points
635680	641440	out is the unnecessary use of complex models which is to say the use of opaque or complex
641440	646960	machine learning models when an interpretable model would have sufficed which is to say when
646960	651600	the performance of an interpretable model is only negligibly worse than one of these black box
651600	656640	models and to be honest this is something I see all the time I think the gratuitous use of complex
656640	660880	machine learning models is something which is really serious. One of the things I don't like
660880	667120	about machine learning is the laziness. I think we should always seek to understand and simplify
667120	671120	problems wherever we can it's the same thing in software engineering. We should always be trying
671120	677680	to create the most elegant and simple and maintainable solution. We shouldn't be trying to over
677680	682800	complicate things and I think that's a very you know the kiss principle is very generalizable here.
683440	688560	So he recommends to start with simple interpretable models like generalized linear models or lasso
688560	694160	models or additive models decision trees or decision rules and gradually ratcheting up the
694160	700480	complexity as required. So he also points out that ignoring feature dependence is super important
700480	705200	right and this is a problem that many of the IML methods have so he gives an example of
705200	710640	partial dependency plots where they extrapolate in areas where the model has little training data
710640	715840	and it can cause misleading interpretations. So these perturbations produce artificial data
715840	720800	points that are used for model predictions which in turn are aggregated to produce global
720800	725760	interpretation so that's a big problem. Another thing he points out is confusing correlation
725760	730720	with dependence so he gives an example here features with a Pearson correlation coefficient
730720	735280	close to zero can still be dependent and cause misleading model interpretations
735280	740160	while independence between two features implies that the Pearson correlation coefficient is zero
740160	745600	the converse is generally false. So there's a pretty cool example here this is a couple of features
745600	751200	that absolutely have a dependence on each other you can see it visualized here but you wouldn't
751200	755360	know that if you looked at the Pearson correlation it would have said that it wasn't significant.
755360	760960	Another one misleading effect due to interactions so there's a couple of things here there's the
760960	766320	partial dependency plots on a couple of dependent features and then he's used a simulation to kind
766400	772320	of trace all of these different features to see what the predicted label was and according to
772320	778560	these IML methods there is actually no clear dependency between these features and the predicted
778560	782640	outcome whereas you can see that that's just blatantly false. So something I've been meaning
782640	788240	to do for more than a year now is to go through Molnar's interpretability book and to make some
788240	792960	bite-sized videos on every single approach well Connor and I are actually going to do that over
792960	798080	on Machine Learning Dojo with the first one next week on Shapley Values so make sure you
798080	803680	subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your
803680	811680	comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube
811680	818080	channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society
818160	826320	and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision
826320	832960	and these days interpretable machine learning. We have an exemplar German on the show Christoph
832960	840640	Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable
840640	846160	machine learning a guide for making black box models explainable. If a machine learning model
846160	851840	performs well why don't we just trust the model and ignore why it made a certain decision? Well
851840	857120	the problem is that a single metric such as classification accuracy is an incomplete description
857120	863680	of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he
863680	868960	introduces the importance of interpretability and reports an incredibly detailed taxonomy
868960	874000	of interpretability methods and his style of writing is at times entertaining and entirely
874000	880320	absent of hype and nonsense. He runs the gamut of interpretability models so for example model
880320	885520	agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual
885520	890560	examples and adversarial examples he motivates the importance of interpretability methods
890560	896080	but he's also extremely transparent about its current weaknesses and pitfalls. He's currently
896080	901920	finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich
901920	906560	after getting a stats master's from the same institution. He's recently written several
906560	911040	very interesting papers on interpretable machine learning for example pitfalls to avoid when
911040	916880	interpreting machine learning models in July of 2020 where Christoph detailed several problematic
916880	921760	model interpretations for example ignoring estimation uncertainty feature interactions
921760	926640	or confusing correlations with dependence. More recently he published a paper called
926640	931600	interpretable machine learning a brief history state of the art and challenges while he acknowledged
931600	936480	that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods
936480	941200	such as the lack of statistical uncertainty, shared information between features, lack of a
941200	946160	clear definition of interpretability and the need for a more holistic view. Christoph Mulner
946160	951120	it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here.
951760	956000	You know Christoph I have to say I really enjoyed your book. I read this actually
956000	961120	some months back in preparation for a completely different show. I loved how scientific it was
961680	966880	you know it was it was very much laying out essentially a survey of the facts a lay of the
966880	972800	land very objective evaluation. It had both the pros and cons you know of different approaches
972800	977920	examples to make them you know more understandable so kudos to you. I thought it was a great book
977920	983040	very enjoyable and very informative. I also loved how it lays out the beginning you know what the
984080	988560	goals that we're trying to achieve with with interpretability are especially kind of the
988560	995360	human goals right like what does it mean for an explanation to be good for people what kind of
995360	1000400	explanations do people like and sometimes there can be conflicting conflicting goals there and
1000400	1006080	I think one thing that that I realized from reading your book is that that actually explanations can
1006080	1013840	be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have
1013840	1021200	to look for contrastive explanations or counterfactual explanations like and principle it seems good
1021200	1028560	it's kind of like you know I'm sorry we can't give you this loan you know well why not like why
1028560	1033120	can't you give this loan well well we've detected really that you're a that you're a deadbeat what
1033120	1037600	do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look
1037600	1042160	through this and we find a decision tree here and and some big decision tree and we get to this one
1042160	1048800	little point what says here you know you didn't pay this furniture bill back in 2018 you know if
1048800	1053600	if only you'd have paid that furniture bill like we'd be able to give you the loan right but the
1053600	1058320	truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree
1058320	1064320	right with so many contributing points yeah I think I like this chapter that you referenced
1064320	1069520	was about like kind of from the social view or the human view what what what people like or
1069520	1075360	prefer is explanations and the whole chapter is based on I forgot the title of the paper it's
1075360	1081040	like from Miller about like what we can learn from the social sciences about what a good
1081040	1087760	explanation is and was like a paper where I learned a lot and it was super interesting also to see
1087760	1091840	how like what people think are good explanation as you mentioned they should be contrastive
1091840	1096560	they should be short but they should also confirm to some prior knowledge that the people have
1097440	1102320	and I mean like objectively a lot of those things might not like you wouldn't say these are good
1102320	1109680	explanations in some sense like maybe maybe it's not good to give an explanation that fits with
1109680	1116800	a prior knowledge because it's not the correct one maybe so it was quite interesting to learn
1116800	1121520	and to think about like what's the human side of it that's a very cool part of your book I thought
1121520	1126000	the fact that it's actually quite interesting thinking about what we really want out of an
1126000	1130640	explanation I remember first of all looking at you know sharp values that are very fair and
1130640	1135760	will distribute the blame equally amongst all the different relevant features and then you turn
1135760	1141360	to something else like you know selective interpretations that in a way are way less
1141360	1146320	good because they're kind of arbitrary they'll just select a few a few a subset of the features
1146320	1150000	and give them all the blame but then it turns out that apparently that's what people actually
1150000	1156160	want as a useful intuptable explanation yeah so as I see there's like many many dimensions of
1156160	1163680	explainability or like what what can be a good explanation and one of these dimensions is maybe
1163680	1168400	sparsity that you have a short explanation with just a few facts that's something that people
1168400	1174800	prefer maybe but this might be in conflict with other dimensions of a good explanation
1174880	1180000	which should be maybe that all causes should be addressed by the explanation that plays some
1180000	1186240	role at least but this is of course in conflict with sparsity if you want this full attribution
1186240	1192880	like you get with shepley values for example so I that's why I also think there's not like just
1192880	1198320	one correct explanation but there's like many attributes or many dimensions on which you can
1198320	1203520	judge explanations yeah I think this is one of the problems because even machine learning is really
1203520	1208640	difficult right because we use benchmarks and benchmarks are just something that people have
1208640	1213440	come up with but you say in your you know you talk about one of the problems being that there's no
1213440	1218880	definition of IML methods to start with but at least in machine learning methods we have ground
1218880	1226160	truth which is which is significantly better in a way but if we if we can't quantify how good
1226160	1231200	an explanation is then where are we really because you talk about a kind of taxonomy of
1231200	1235760	interpretability methods right you say that there are objective evaluations like sparsity and
1235760	1241520	interaction strength and fidelity and humans human-centered evaluations you know which might
1241520	1247200	come from domain experts or lay people so I suppose you're just hitting straight on the
1247200	1253520	fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this
1253520	1259040	big criticisms okay this is not scientific or not well defined at least what interpretability is
1259040	1265680	how can we even do research in this area but I have a bit more relaxed view I mean otherwise I
1265680	1270560	should have stopped writing the book before I really started so I kind of see like this
1271680	1277200	endeavor of giving interpretability or bringing interpretability to machine learning it's more
1277200	1283520	like a first of all it's just a keyword so it's it kind of bundles all the methods together
1283520	1291120	that kind of aim to reduce this high-dimensional function to something well mostly it's something
1291120	1296080	in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine
1296080	1302240	and it's I think part of science to find out like or to yeah some part of analysis to find out what
1302240	1309840	part gets lost so when you for example look at just some feature importance values for example
1309840	1318800	of course it's a summary of your model and a lot of information gets lost but I still think
1319600	1323680	it's useful to have obviously so many people use it but it's useful to have these
1324400	1331440	tools and we just have to understand what they do and how to interpret the results so how do
1331440	1336640	you interpret when like the feature importance is zero of a feature could that be quite dangerous
1336640	1341360	though because this you gave the example of random forests when you have a lot of shared
1341360	1347360	information between the features it would actually tell you that these correlated features have a
1347360	1353040	higher feature importance than you might otherwise expect so does this imply that we need to have
1353040	1357680	very detailed knowledge of how we should how we should use this information that we get from
1357680	1363200	IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls
1364000	1369280	to avoid and stuff like this so I think so these are just tools so they do something with the
1369280	1374640	model a kind of distill some knowledge so for example for feature importance you kind of
1376160	1380400	measure how well does your model perform and then you measure again after you
1380400	1385440	shuffle one of the features and and then then you get something out of it so then we can ask
1385440	1391440	questions is this interpretable or not and it's kind of well not so relevant the question because
1391440	1396080	you just have to understand what what happens when you shuffle feature and one is for example
1396720	1404800	you kind of break the association between the feature and the prediction because now it doesn't
1404800	1409120	carry the information about the target anymore because you're shuffled in randomly in your model
1409840	1415360	so you kind of this this feature importance now measures how much performance you lose because
1415440	1421680	of this break of information but then you also when you think about this method and want to use
1421680	1426320	it you also have to understand that this shuffling for example breaks also association with your
1426320	1431120	other data feature like the features in your data so this is a limitation of the method and
1432000	1438480	what I think is needed is that we understand in which way these methods break or which
1438480	1443600	scenarios we're allowed to use them or how we are allowed to interpret them and I think the
1443600	1449760	situation is kind of similar to statistics where you have these models and and then you
1449760	1455360	interpret like the coefficients of your models you still like have to learn how you do the
1455360	1459520	interpretation what are the assumptions that have to be met that you're allowed to do this
1459520	1465920	interpretation and I think it's we're in a similar situation here with interpretability of machine
1465920	1471440	learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality
1471520	1477200	in the thread because you make a very good point in the book which is look even these so-called
1477200	1482720	intrinsically interpretable models are only interpretable up to a certain dimensionality
1482720	1488000	and you know I have I have tons of experience with with multi-linear regression right and and I can
1488000	1494160	guarantee that beyond a very small number of dimensions those coefficients are not interpretable
1494160	1499040	because it starts to play a bunch of games where it's inflating one coefficient and another because
1499040	1503440	their difference is important and you know whatever else is happening a lot of correlated
1503440	1508560	structures are all essentially getting compressed into the small number of small number of weights
1508560	1514080	right and so as the dimensionality goes up I would say like no model is is intrinsically
1514080	1518960	interpretable same can be said of decision trees like anybody who's looked at a decision tree that's
1518960	1523520	come from real data you're going to find out it's not interpretable it's like oh look at this you
1523520	1528480	know market capitalization matters oh and it matters over here too and down here and and
1528480	1533520	actually I have to go through five checks on market capitalization before I get down to this
1533520	1539120	decision and maybe the features aren't that intuitive either and then you have to kind of
1539120	1546400	like mentally stack up like until you get to the decision like five decisions and then we have a
1546400	1556560	very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean
1556640	1560800	I have I have this distinction the book like interpretable models and not so interpretable
1560800	1567680	models um but it's as you say it's like a gray like it's a scale really people could definitely
1567680	1573120	overhype how interpretable these white box models are right whether it's linear models
1573120	1579200	as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use
1579200	1584880	models like a decision tree because it's possible in theory to write down on a piece of paper exactly
1584880	1590240	how the decision is made right yeah you can trace every decision but that's never actually useful
1590240	1595440	in practice is it since when have you ever looked at a decision tree fitted to data there's any
1595440	1600240	complexity and the fact that in theory it's possible to go and examine how it works yeah it's
1600240	1604560	completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision
1604560	1609760	tree sometimes it but in practice it will not like give you probably the best predictions
1610720	1614640	um but it might be useful sometimes to shorten it artificially so even like
1614640	1619680	you're throwing away some some predictive accuracy um but you shorten it so you understand
1619680	1626240	that somehow it's manageable you can have a look at it and and see what's going on well there's also
1626240	1630800	you know the other issue is that as I was looking through a lot of the methods that you describe
1630800	1636720	and you survey in your book you know some of them um are not simple I mean if you start looking at
1636720	1642560	partial dependency plots and trying to explain what those are I mean you know you have to almost
1642560	1647360	have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering
1647360	1654560	at what point we're just developing complex math models to explain complex math models and we really
1654560	1660720	haven't you know made much progress along the interpretability axis yeah yeah it's true it's
1660720	1665440	also like the criticism too like um so you have something you don't understand and you explain it
1665440	1672640	with something you don't understand um I think some methods are complex um but for some at least
1672640	1678800	there's some intuition how they work for partial dependence plot is kind of your this um intuition
1678800	1683760	that you do some intervention on your more or intervention on your data so you replace all your
1684320	1690080	like for one feature you replace all the values with one fixed value and kind of look at the
1690080	1694800	average prediction that you get afterwards and do this for a lot of points and then you connect
1694800	1700880	the points and you have this curve so kind of gives gives you the expected change over the feature
1700880	1707120	range maybe there was already a bit complex I don't know um maybe I'm too deep into the method
1707120	1714560	already um but yeah of course it's it's something additional people have to learn or if they agree
1714560	1721440	to use it of course could I get a quick take from you on saliency maps as an example because you
1721440	1726560	said in one of your youtube videos that saliency maps are glorified edge detectors they are not
1726560	1732320	good explanation at all and I've noticed now that many machine learning platform providers are building
1732320	1737760	these kind of um saliency maps into their models you know into their platforms and then it becomes a
1737760	1742320	kind of box ticking exercise where you can say okay well yeah we've done interoperability now
1742320	1747360	that's all you need to know and that really is quite a false sense of security isn't it
1747360	1752880	it's funny you mentioned the saliency maps because I'm writing a book chapter about it and
1752880	1759280	actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has
1759280	1763840	been a long time in the making and it was very very frustrating like by far the most frustrating
1763840	1770160	chapter to write um number one reason is because there's so many methods out there uh reason number
1770160	1779040	two is I I can't judge really or if they work and it seems like they mostly don't or it's it's
1779040	1784800	still unclear like how you say you would judge that they work so they're like dozens of these
1784800	1791360	like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance
1791360	1799040	propagation in 10 variants um so and then I mean you in the end you when you apply these methods on
1799040	1803440	they are also like for image classification and you get these nice-looking images and some areas
1803440	1808960	are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if
1809840	1815600	if it kind of makes sense then you maybe would be inclined to trust the method um but then there's
1815600	1821440	this uh paper which is called um sanity checks for saliency maps and they kind of found out that
1821440	1828720	they the most of the methods are very similar to edge detectors meaning that they are kind of
1828720	1834960	insensitive to the model and the data which is very bad of course well if you change the model um
1834960	1841360	the explanation should obviously change um could could you expand on that a little bit so you said
1841360	1847040	it wasn't really reflection of the model or the data but what what would a perfect saliency map
1847040	1854240	look like well I don't know myself actually so I mean the the ideas that you saw the basic idea
1854240	1859360	of most of these methods is that you you have your class prediction or your class score and you
1859360	1866560	want to back propagate it not you want to back propagate it to the original image so you look
1866560	1873120	at the gradient um with respect to your input pixels and that's there's no not not one way to
1873120	1878480	do this but there are many different ways so that's also why we have so many different methods
1878480	1886000	and then they highlight which pixels were relevant for the classification um but yeah
1886640	1891680	they these these methods they have like a lot of like issues for example there's the issue of
1891680	1897440	saturation for example because of the real unit where you have flat parts of the gradient so if
1897440	1903280	you pass the gradient through that then um your method would say that some some neuron might not
1903280	1909680	be uh important at all and there's a lot of these little issues that these methods have um
1910800	1915920	yeah so but but back to your question like I think that's also the issue that I don't
1915920	1920480	wouldn't know how to answer I mean obviously it should be some area that should be highlighted on
1920480	1925600	this aliens method was important for the for the neural network um but then again I don't know how
1925600	1931760	the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean
1931760	1936880	I could highlight the part where I think the network should look but then again I mean there are
1936880	1942240	lots of papers like the clever hands paper which saw like the reveal that there are some
1943360	1949520	sometimes it would look at watermarks on on the photo um so these are like these things that we
1949520	1955600	just don't know uh what the neural network basis this on have if I could take a stab at that answer
1956240	1962640	for one I think just the idea of quote a saliency map is a problem like there isn't one
1963520	1967760	map of of the importance of the pixels it's like they're they're operating on multiple
1968480	1972720	multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you
1972720	1978320	know why is this image a dog you know well for one thing it's it's the overall shape you know it has
1978320	1984720	four legs and you know two ears sticking out over here that's one saliency another is that
1984720	1989040	it's it's got a certain color you know and it and it's coat and that's a that's a different
1989040	1994000	concept of what's salient and another is that there's a frisbee flying at it and its mouth is open
1994000	1999200	and it's about to catch it and I know dogs do that so they're kind of you know when your mind
1999200	2004640	analyzes an image it breaks it down into these many large scale kind of structural features
2004640	2009200	and I think that gets completely lost and most of the approach is the saliency maps this is
2010000	2016480	really important point actually because if you're just looking at the pixels on this kind of 2d
2016480	2022240	planar manifold that's only a very it is quite literally a surface view and I think Christoph you
2022240	2028880	said that there are all sorts of causal structures and even in the model itself right there are
2029520	2034560	these entangled neurons and surely that's giving me more insight into what's actually happening
2034560	2038160	just seeing a bunch of pixels and the other thing is that these models that they are completely
2038160	2042400	lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has
2042400	2049520	just got completely broken right yeah so um but in in that vein some of these feature visualization
2049520	2054960	techniques you know like the deep dream type stuff maybe maybe that's a better way of of
2054960	2059920	interpreting these models yeah um so like for the one point you mentioned about the adversarial
2059920	2067360	examples so there's also a paper I forgot to title again um which that manipulated neural networks
2067360	2072480	so they would give the same prediction for all the images but different explanation like different
2072480	2079200	saliency maps so this is perfectly possible to create different explanations um for these saliency
2079200	2086320	maps um but but keeping the model like at least for the predictions the same there's another criticism
2086320	2090320	you can throw at saliency maps where they they can be quite deceiving you think they're useful and
2090320	2094320	they turn out not to be useful yeah there's a classic example of looking at you know comparing
2094320	2099040	a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful
2099040	2103520	sometimes it highlights the animal and you think okay I understand it's looking at the face that's
2103520	2107920	why it thinks it's a dog because it's in the face and then you look at the predicted class for something
2107920	2113120	else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah
2113120	2117120	so the saliency map for all these different classes looks the same and when you realize that
2117120	2122640	you realize this this saliency map hasn't actually told you anything about why it's gone for one class
2122640	2126480	versus the other all it said is that it's just highlighted the thing in the middle of the picture
2126480	2131680	yeah I think that's especially also when when you look at images you know like we're very good
2132640	2136720	with images yeah like we were very quick to see what's happening on a scene and such
2136720	2141360	so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense
2141360	2146400	it's more difficult to interpret like if you have like a graph and there's like things going on inside
2146400	2151760	you have to like now understand what the method does and stuff like this but for an image like a
2151760	2157200	heatmap IR this area is highlighted makes sense case closed I like the method
2159120	2165680	yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining
2165680	2170640	complex things with complex things we don't understand you know so I think a lot of people
2170640	2174800	if they looked at it and again one of the points of this interpretability is really the social
2174800	2181200	aspects of it right like being able to convince people to be at ease with machine learning models
2181280	2187920	or to accept the results of of a machine learned you know decision process and I think if somebody
2187920	2192320	looks at an image of a dog you know they have no problem understanding that but if you showed them a
2192320	2199200	bunch of salience maps or or any of the other sort of you know feature projections if you will
2199200	2204880	like you said it takes a lot of deep understanding to understand those whereas the image is kind of
2204880	2210560	immediately obvious I think two of the main themes that you touch on is we'll get to the
2211360	2216640	to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue
2216640	2222560	that you point out is feature dependence okay and and you say that when you have feature dependence
2222560	2227840	it makes attribution and extrapolation problematic so a dependence just means that you got correlated
2227840	2232560	or shared information between your features right so you say that in feature permutation methods
2233520	2237200	these things basically break everything when you have the shared information
2237200	2242400	and the extrapolated data points are no longer in the distribution and you say that there are
2242400	2246960	conditional permutation schemes you know that try and and maintain that joint distribution but
2246960	2251040	those things sometimes make it even worse right so do you think that's one of the most important
2251040	2257120	things that people should think about when using iml methods yeah at least so that's at least like a
2257120	2265440	very um deep issue I would say which is inherent in in most of the model agnostic methods where you
2265440	2271280	manipulate your data see what how the model prediction changes and then create your explanations
2271280	2276560	out of this sort of select the shadowy value line partial dependence plot feature importance
2276560	2283600	they all work with this mechanism of manipulation of the data prediction and then kind of aggregating
2283600	2292880	the results and most manipulations happen in isolation so that you for example when you
2294640	2299840	for feature importance you can meet one of the features as I like said before and then
2301120	2304240	well you break the association of target but also a few other features
2304960	2310800	but similar things happen if you use lines or you kind of replace parts of your image
2310800	2315440	but then again you also have to replace it with something like which is I think in line the
2315440	2321120	defaults with just a gray image and then of course it's not like it's outside of your data
2321120	2326640	distribution subtly because your network was not confronted with like these patchy images before
2326640	2332080	they had like just normal photographs usually and depends on your neural network but I mean you
2332080	2337680	certainly didn't train it on on images where parts were grayed out so it's pretty likely what the
2337680	2343760	model should predict and what will predict at this point but but you use these images to
2344480	2348960	create your data set like you send it through the neural network you get predictions and
2348960	2355600	you kind of aggregate from this your explanation but you left your data distribution and your model
2355600	2361920	can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know
2362640	2368800	like it like a simple example from the medical field would be that you know height and weight
2368800	2375920	are highly correlated right and and on the other hand sort of the ratio or some relationship between
2375920	2381520	your your weight to your height that actually has very important medical consequences right that's
2381520	2387120	that's the measure of of health and so if I were to sit there and just permute say the height index
2387120	2390800	and create a whole bunch of people that had all these bizarre combinations of
2391360	2395920	of height and weight you know first of all those don't even probably exist in the data set and the
2395920	2402800	ones that do exist in the data set probably had some medical issues right yeah well you actually
2402800	2406560	gave a similar example I think you gave the example Christoph of a baby that earns a hundred
2406560	2411040	thousand dollars a year which is which is insane but when you talk about something like lime
2411040	2416400	maybe that's different because the cnn the what you know it shines a flashlight over the input
2416400	2421440	space in it and it's a kind of local method so in some sense you could argue that it doesn't
2421440	2425280	matter that you've grayed out all this other stuff because if the model was sufficiently well
2425280	2429440	trained in the first place it should hopefully learn to ignore the background or is that just
2429440	2433920	wishful thinking that's an interesting thought I haven't thought about it because like the property
2433920	2441120	of like that you have these filters that trust a wander over the image yeah maybe it would make
2441120	2447200	it more robust for these kind of interventions that we do when we create these images with lime
2447200	2454720	and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these
2454720	2460400	ways in which interpretability methods can go wrong right how the model might not be a realistic
2460400	2464880	the interpretability model might not be a good approximation to the actual ml model
2464880	2469280	so some people a bit controversially perhaps take the idea and remember that and say you're just
2469280	2474080	working up completely the wrong tree and you should give up using interpretability
2474080	2479520	interpretability methods to explain these black boxes this dish them instead just use an interpretable
2479520	2484960	model to begin with use a white box model I think there's an example um from compass in the US which
2484960	2490880	is that model to predict reoffending and I think quite famously there was a investigative journalists
2490880	2496320	that tried to interpret this model it was a black box model because it's proprietary right it's a
2496320	2502320	trade secret and they they fit it a proxy model a kind of a linear model and they made a report
2502320	2507120	saying okay we think your model is racist because it looks like it's it's taking race as a factor
2507680	2511680	and then some further work was done and they came back and said well actually you've just used a
2511680	2516240	uh interpretability model that doesn't really fit our model very well you've made some assumptions
2516240	2520320	that don't hold if you use a different interpretability model you get a completely different answer
2520320	2525040	that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong
2525040	2530640	assumption by using a bad interpretability model and I think they were saying that this model
2530640	2535680	instead you could get just just as good a model of reoffending with like three FL statements you
2535680	2542160	know ditching this massive complex 100 and something features and just use three FL statements
2542160	2547600	based on I think age and reoffending so is that was that what we should do should we just drop
2547600	2552560	these methods and start using white boxes instead so I mean like one one thing to mention here is
2552560	2558320	that a white box is very soon also like a gray or black box if you add interactions if you have
2558320	2564160	many features and so on but putting that aside I would agree if you're in first place like that you
2564160	2569840	say you should start with like a white box so if you start modeling then then you then you should
2571040	2575360	consider these first like maybe they already solved your problem then it's perfect and you have a model
2575360	2584880	that is quite I mean stable it's interpretable I think that would be great but then I think the
2584880	2589520	next step would be to see like what like a black box or a machine learning model would give you in
2589520	2595440	terms of performance and then maybe if you see the gap is really big then maybe you can try
2595440	2600800	some feature engineering and close the gap maybe from the interpretative model to the machine learning
2600800	2607200	model but then you're probably already infusing some features that are not so interpretable
2607200	2612720	or maybe if you're using a linear regression model you're maybe using then splines and
2612720	2618800	interactions so you're already moving towards more complex models usually but then if you still have
2618800	2626240	a gap then I think you have to decide is the gap and predictive performance like worth changing to
2626240	2632480	a black box model so I think that's your decision will be different in many cases
2633760	2636720	as you relates back to the point you made on your paper about criticisms of using
2637360	2641200	interpretable machine learning models that some people leap straight away at using an overly
2641200	2646640	complex model and sometimes depending on the situation sometimes you know a linear model can
2646640	2651360	do just as well and have all these advantages it's so much easier to explain do you have a
2651360	2657760	philosophy from a high level here right because if it if it were a human if it were an airplane
2657760	2664880	pilot we don't really understand how the brain works right we would just test the pilot you've
2664880	2671680	got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't
2671680	2675520	really seek to understand how his or her brain works but with machine learning models there's
2675520	2680720	this continuum right so if you use these complex black box models the predictive performance is
2680720	2686160	usually better but you're trading off understandability and assuming those things are completely
2686160	2690640	mutually exclusive what kind of decision process do you go through when you select these models
2690640	2695600	but by the way with machine learning right the reason why we use machine learning is because
2695600	2702640	we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say
2702880	2708960	um when when it ate us like so high-dimensional so complex many interactions and so on
2710240	2716000	that your simple models don't cover the complex cannot cover the complexity I think then you
2716000	2722560	need machine learning would you rather understand exactly how the plane worked or would you
2722560	2726480	rather I mean if I was saying to you you can go and fly in a plane would you rather that you
2726480	2731200	understood how the plane worked or would you rather that the plane was tested why not both
2732000	2740160	so um I think we can do both it's to some degree so um of course with black box model we don't
2740160	2746000	exactly understand how they work um but in comparison to a pilot we can test them for
2746000	2752720	three more or less um so because I mean maybe it's not as good as an interpretable model
2753520	2759520	but we still can use a lot of methods to at least approximate and and try to understand a few
2759520	2765840	properties of this model so I think we are even in a situation where we don't have like these
2765840	2773280	complete like A or B decisions but we can have so if if the machine learning works much better
2773280	2779280	and it's like really robustly tested with lots of different data I would prefer machine learning
2779280	2788480	model I guess um but then I would also want to like people to to apply all these methods that
2788480	2792720	are available even if they are not perfect but still they give you something they give you some
2792720	2799920	insights so yeah and I think so Tim one answer to your question is that a lot of people's response
2800640	2804960	here and kind of demanding interpretability and having concerns about machine learning
2804960	2809600	it all comes down to generalizability and we've seen through using machine learning
2810160	2817520	that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser
2817520	2824320	you know is really great at dispensing it dispensing soap you know 87 percent of time but
2824320	2830640	it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to
2830640	2835200	people with a certain skin color like we've kind of decided is a society that are that there are
2835200	2841360	certain generalizations or certain dimensions along which our models just have to perform
2841360	2846720	and also because a lot of these these things that break machine learning models are things
2846720	2852640	that happen quite quite regularly in the real world it's like a pilot you know a human being
2852640	2858160	pilot flying around if he looks down at the ground and sees a hot air balloon with a big
2858160	2863120	smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot
2863120	2868320	about the uh the hot air balloon contest that's going on today where's a machine learning model
2868320	2873440	if it looks out a camera and sees something with a particular shape of lightning bolt you know it
2873440	2877840	might just decide it's time to like dive for the ground right and crash the plane like that's
2877840	2883040	sort of what these adversarial examples kind of show and I think that's why people are really
2883040	2889280	hungering for human understandable explanations because still to this day the human brain
2890000	2896960	is the only AGI really that that we have around yeah but deep learning models that they they
2896960	2902960	essentially memorize lots and lots of things and they have this sparse coding so in a way it's just
2902960	2907360	like the white box model even if we use interpretability methods we could enumerate all of the
2907360	2913040	things that they are learning and one of those things might be a sensitivity or lack of sensitivity
2913040	2918160	to hot air balloons or smiley faces on but even if we could enumerate all the things that they are
2918160	2923440	learning we wouldn't understand that either in the same way we don't understand how a real human's
2923440	2928080	brain works and I'm not sure whether we should view a human brain as a computer program and whether
2928080	2934320	that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have
2934320	2939600	to accept that we're not going to understand a totally it's actually it's a good comparison
2939600	2944320	I think with humans you know if you're interviewing and you want to hire let's say a software developer
2945040	2949200	you tend to set them a coding interview you wouldn't think of taking them into surgery
2949200	2952880	opening up their brain and trying to find the neuron that predicts what the next you know
2952880	2957200	but if code is going to be and understanding how that works it's just why would you do it that way
2957200	2962000	instead you learn to trust humans by working with them giving them a test seeing how they
2962000	2967120	perform in the real world and I know maybe we're asking too much of a machine learning model if
2967120	2972720	you want to be able to understand these complex things in terms of like a bottom-up white box
2973280	2979280	set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can
2980400	2985200	cut off the like cut open the brain of a machine learning model without breaking it
2985200	2994160	and without hurting it hopefully and we can do all these try out all these interpretation methods
2994160	2999280	see how it behaves under certain situations and I also would make a distinction between
2999280	3004800	we understand what's going on inside and doing like this kind of sensitivity analysis
3004800	3011360	where we just try out what happens in certain scenarios so it always do that that we can like
3012320	3017680	check like how it behaves so I mean feature importance is basically like a way to see
3017680	3021760	like how does it behave if we break some features and then we rank the features by this
3021760	3028000	as an importance we can do it and that's also the big difference between humans because we
3028000	3033520	can't test in the same way and yeah shouldn't probably. I think there's one other difference
3033520	3039200	though which is to do with the substrate of how neural networks work I think if I'm giving someone
3039200	3044960	a job interview or something I mean of course it's a very valuable process but I'm looking at
3044960	3049360	their values and I'm looking to try and understand how they would behave in different situations
3049360	3053920	and I'm coming up with lots of illustrative examples but the difference is with humans
3053920	3061120	we have that level of generalization we have a kind of guiding taxonomy of behaviors which means
3061120	3067040	if I know if I have guiding examples of what a human will do in certain situations I expect
3067040	3072720	that to generalize whereas my hypothesis is is that a deep neural network model is almost like
3072720	3077440	an infinite number of rules and there's absolutely no carryover between the rules
3077440	3083360	so knowing even some or even most of the rules doesn't really tell me about those edge cases.
3085920	3092000	Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know
3092000	3097280	that they exist like with adversarial examples so even if we don't know like exactly what they
3097280	3102800	would look like or there's even an infinite amount of uh I mean there's an infinite amount of like
3102800	3110480	how you can change the image to make it like have a different class so we know so I think it's important
3110480	3116480	that we know that these exist at least. Yeah I love the point you made though that we have we
3116480	3122640	have the luxury to do analysis on these on these boxes because we can open them up and that that's
3122640	3126880	another point I'm pretty sure that you make this in your book as well which is that part of this is
3126880	3134240	just scientific inquiry it's it's like understanding better how to interpret and explain machine
3134240	3140160	learning models will probably actually contribute to us being able to construct even better machine
3140160	3145520	learning models isn't that true? So so you're basically saying that um also interpretability
3145520	3151120	might help to to be better at like create better machine learning models themselves?
3152240	3157200	Yeah like as we as we develop these interpretability methods because in a sense like you point out
3157200	3163040	earlier there are statistical projections of kind of the behavior of the model and so like a saliency
3163040	3168480	map you know if we can if we can kind of use that to learn the way in which the the neural networks
3168480	3174720	are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I
3174720	3181920	also have seen approaches where they try like kind of fuse also these two worlds like interpretative
3181920	3186880	models or white box models and black box models so that you try to to generate features out of
3186880	3194960	the black box model which you then use in your more understandable white box model so um I think
3195040	3201440	this and and this also like by using similar techniques um to which you would use for interpretability
3201440	3208080	like detecting interactions for examples for example so um yeah these can be used also to
3208080	3214400	to build better models and also to build better interpretable models. The other and I'll make
3214400	3220160	one last point here which is another social good that can come out of interpretability is
3220160	3225440	imagine we've got you know an ML model that's not trying to make any decisions but it's just
3225440	3232080	trying to figure out what leads to happiness and success in life you know and so we analyze a whole
3232080	3237360	bunch of data and we find out well it's really important if you graduate from high school and
3237360	3241680	it's really important if you you know don't have children before you're married and you know all
3241680	3247440	these other factors if we can dive in and kind of isolate those factors it actually allows people
3247440	3252320	to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of
3252320	3257920	data and it actually has some some understandable recommendations for how to lead a healthier
3257920	3264000	life or a better life or whatever. Yeah I think that this very good example were the fact that
3264000	3271760	you have a prediction model doesn't solve your problem so actually it's just a means to to some
3271840	3278640	other goal in this case understanding like what are the factors for happiness and one example I am
3278640	3282800	from a friend who worked at a telecom company and they built like a churn prediction model
3284080	3291920	to see like who will quit the telecom contract and then they started like the ones with the
3291920	3296640	highest likelihood they started sending out emails say maybe offering them a better deal
3297440	3304480	but actually the outcome was that well they they when they once they wrote to the customers they
3304480	3311920	well left and quit their contract so it's kind of had like so this is a case where the prediction
3311920	3316240	model actually works but then they people leave in this case probably because they
3316240	3323920	realized ah shit I have this contract still going on time to quit now so if you knew the
3323920	3330800	reasons why they are likely to churn then you could like better select like when you write some
3330800	3337040	email or maybe some other campaign or when maybe not to write anything at all yeah right now um
3337040	3342080	Christoph you have a background in stats which means you I mean you like Connor as well we take
3342080	3347360	an incredibly dim view of machine learning and you wonder how how is it possible for us to be
3347360	3351920	stabbing in the dark like this but you know you said that we need to be more rigorous and
3352000	3357520	there's no quantification of uncertainty with the current IML methods and I suspect you might
3357520	3361840	be working on some methods behind the the scenes on this but you know when you have models and
3361840	3367280	explanations which are computed from data they are subject to uncertainty and that's just not
3367280	3371760	modeled at all at the moment right so we need to be making some distributional and structural
3371760	3375920	assumptions that we're not making now and you point out that there's this phenomenon of p-hacking
3375920	3380320	which is a huge problem in the natural sciences which hasn't quite made its way to IML methods
3380320	3387520	yet but probably will do yeah so yeah the I think and in statistics we're really good at
3387520	3392400	quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on
3392400	3398240	but I still would say it's better to have um not only just one number or one explanation
3398240	3405440	but also have the distribution to do of this explanation or this number and to quantify what
3405440	3410800	uncertainties behind computing this number so when you have a linear model then you get
3412160	3416960	some coefficient which you interpret in the end but usually you don't just interpret the
3416960	3422560	coefficient but you look at the confidence intervals but we don't do it at the moment for
3422560	3428400	interpretability so you maybe get the saliency maps but how certain are you about maybe it's
3428400	3433920	a bad example because we don't so much on it but if you have like a feature importance value and
3433920	3439040	you get some result how like what's the range actually like how much variance is behind it
3439040	3444480	if I were to use slightly different data or refit my model again how similar would the number be
3445600	3450400	and I think that's something that will or should come to interpretability as well
3451840	3456080	it's funny how when we come to machine learning it's almost like open season and forgetting
3456080	3459680	everything you know about maths and stats you throw it all out the window okay so excited
3459680	3465360	about these algorithms right like one example is if you take a if you're fitting a model to predict
3465360	3470240	something that was unlikely I don't know maybe it was like a covid test for example and then
3470240	3473760	if you know the prevalence of covid you get it back you kind of know what the false positive
3473760	3478640	rate is going to be and so you notice that you think it's the multiple comparison issue right
3478640	3481920	you know that you're expecting a certain level of false positives when it comes to doing something
3481920	3488080	like feature importance or looking at interpretability from a thousand features and then five come
3488160	3492800	through is really really important you mentioned sometimes we just forget that multiple comparison
3492800	3496800	issue forget the fact that probably these five are going to be completely false positives and
3496800	3501520	probably completely meaningless yeah I agree especially if you have like these high-dimensionality
3501520	3506880	features and for the record I have to say I mean there are already approaches so especially for
3506880	3513200	feature importance because there's like a huge community in random forests for example and they
3513200	3517760	thought a lot about these issues and their tests for this and stuff like it but for the rest of
3517760	3522560	interpretability I think it could gain a lot thinking more about I mean this is very simple
3522560	3528480	stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians
3528480	3535520	think like a long time already about it and I mean if you even if you leave the area of
3535520	3540640	interpretability and look at the benchmarks so even like if you have like accuracy like a table
3540640	3545440	and you see accuracies in it but there's no variance attached to it then it should be like
3545440	3550880	suspicious of it but because if you just retrain your neural network with a different seed you might
3550880	3555440	end up with a different accuracy in the end so and if you want to say a method is better than
3555440	3562720	another method you want to quantify how larger ranges of uncertainty do you I mean there are a
3562720	3567200	lot of things like the choice of data choice of splitting points and training and test data
3567360	3574800	weight initialization and so on so I think a lot of this rigor from statistics could help
3575360	3578720	the machine learning community and and machine learning science to become better
3579840	3584320	yeah but let's let's never forget this uh quite quite well known saying which is there are three
3584320	3591520	kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right
3591520	3597280	is is you know fundamentally whenever we go measure data and we have a model what we're
3597280	3603120	actually able to extract from that data and and the model is inherently probabilistic
3603120	3608480	it's a probability distribution right at the end of the day and we get into trouble anytime we try
3608480	3614480	to take that probability distribution and project it to numbers i.e. statistics like as soon as we
3614480	3620640	start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval
3620640	3624960	or whatever the fact is we're throwing away information the totality of the information
3624960	3631440	sitting there in that weird multimodal you know spread out distribution right and then we we find
3631440	3636320	some way to simplify it and project it down to a set of numbers we've got a problem like that's
3636320	3641200	and if you forget that that's happening if you forget that you're throwing away all this information
3641200	3646560	I think that you know the tendency to do that isn't just simplification it's that oftentimes we
3646560	3651680	have to use these probabilistic things to reach a decision and as soon as we get to the point where
3651680	3656800	look i've got to choose either to go left or right give the loan or not give the loan as soon as we
3656800	3662080	get down to some point where we have to make a concrete decision we're forced you know we're
3662080	3668240	forced to project it right but along the way it's important not to lose sight of the the fact that
3668240	3674320	we're throwing away information fantastic well and by the way you also said something interesting
3674400	3678800	a minute ago Christoph which is about at least in most machine learning algorithms if you change
3678800	3682960	the random seed you know that there's enough stability there that it still gives you roughly
3682960	3688800	the the same model every time but in reinforcement learning if you change the random seed the entire
3688800	3694720	thing is completely broken but but yeah what Keith was saying about this this this information
3694720	3699280	and structure in models I think that's really interesting because people have said with reinforcement
3699280	3703680	learning you can actually learn causal factors right but that's not really true you're interacting
3703680	3707840	with the system but what you're learning is is a surface representation of causal factors so you
3707840	3714320	might learn that there's a causal factor between like a hose putting out fire but it wouldn't
3714320	3718960	actually learn that it was the water that put out you know that there was a causal relationship
3718960	3723040	between the water and the fire and this is the case with so many of our models as we were saying
3723040	3726960	earlier that there's there's just a a surface representation which doesn't actually represent
3726960	3731680	the reality of our world at all but this brings me on to the next point because you have a real
3731680	3737520	problem with causal interpretations of some of these iml metrics right and you you say that models
3737520	3741600	well the the goal of models is that they should reflect the causal structure right this is what
3741600	3746480	we want to do in science but most statistical learning just reflects these surface feature
3746480	3750960	correlations they they don't even scratch the surface of what we want so what are we going
3750960	3755760	to do right are you doing some work in this field to help us out here and and why are people making
3755760	3762800	these fallacious interpretations yeah so so i'm not not working on anything causality related at the
3762800	3770480	moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and
3770480	3778160	master and zero i mean the only time we were talked about causality was when i heard the
3778160	3785120	sentence correlation does not imply causality and it was really about it so um i think it's
3785200	3791040	like really um yeah should be taught a lot more like how to think about causality like
3791040	3797680	just super simple things like you should include confounders or um like what types of features
3797680	3803520	if you include them in the model um like destroy your causal interpretation of another features
3803520	3808960	these these are not super difficult things so um then you don't have to like learn any like
3808960	3815680	difficult frameworks to work with or read like causality books on it that's like super simple
3817200	3823520	yeah rules of thumb for your features even um yeah and i think you also have to decide
3824080	3830800	or distinguish between like what's the goal of your model do you want a causal interpretation
3832640	3837200	or do you want to like because in a sense and you have to also distinguish between
3837200	3841760	two levels you have the real world level and the model level i mean once you use features for the
3841760	3846240	model they are causal for the model prediction of course because you designed it that way
3846240	3850000	and the question is when you are you allowed to go to the real world level where you say
3850000	3854400	okay this um the feature importance that i see here or also the feature dependence
3855120	3863600	plot that i see is also causal um or and may interpret it as a cause and or as a causal effect
3863600	3871040	also for for the real world and i think that also depends like if you need this interpretation
3871760	3878480	if you do scientific modeling for example then you probably want it um but that there can also
3878480	3882960	be good reasons to include non-causal features into your model if your goal is really just
3882960	3888640	prediction and and some feature might help you with the with a good prediction um but it might
3888720	3893920	not be causal at all yes but the problem is when we're using these deep learning models
3894560	3900720	they they will learn a structure which probably has no relationship to the real world whatsoever
3900720	3907600	but um i think causal um factors do generalize much better there's the example of um i don't know
3907600	3913360	with car crashes right male testosterone levels is a causal factor so that will probably generalize
3913360	3918240	to other locations where you didn't train your data on but unfortunately models don't really do that
3918960	3924000	well but so just real quickly on that tim like the only reason that we know testosterone
3924560	3930400	is a causal factor is is not from that data set it's from a bunch of mechanistic you know
3930400	3937440	scientific research and biology and and elsewhere um so you know i i'm kind of wondering how
3938320	3942800	it would be nice if at least machine learning methods could indicate that there may be the
3942800	3949600	possibility of a causal structure so just looking for underlying hidden structures um that that you
3949600	3955440	know they're more generalizable that could explain large pieces of the of the data and give kind of
3955440	3961680	a list of hey there might be a causal factor here like go investigate it but on on that there's a
3961680	3966560	difference between a causal factor and a causal structure i think that the challenge is that we
3966560	3971120	don't have enough fidelity in the structure that benjo by the way is doing some interesting work on
3971120	3977680	this using data driven approaches to um you know learn causal factors but it's the structure of the
3977680	3981360	factor graph which i think is the important thing i mean this is one of the most interesting parts
3981360	3985840	i think of machine learning actually trying to learn causation from a data set which you can do
3985840	3990320	right things like beta networks where you specify all your variables you connect these nodes with
3990320	3996400	edges and you can try to learn the optimal structure like the simplest structure and that
3996400	4002240	sometimes turns out to be the real life causal effect if you do it well but the difference is
4002240	4009200	you as a human you you know the causal structure and you've you've created that that graph so it's
4009200	4013760	not learned you've created it you can learn these things from data right you can actually you can
4013760	4019200	search over the set of all possible graphs all the possible edges and you have a bit of a loss
4019200	4023360	function you try to find a graph that fits the data well so it's got enough edges but it's not too
4023360	4028160	complex you're not relating everything to everything and so just from data without any human input
4028160	4033680	with structure learning you can sometimes get a model that kind of out of nothing will give you
4033680	4038560	the cause of relationships sometimes there is redundancy right because a graph that says a
4038560	4045600	implied near causes b equals a c that's identical to c causes b causes a right but even then with
4045600	4051280	structure learning you you've got this adjacency matrix and all of those nodes you've already come
4051280	4056800	up with a priori so what you want to learn is what the nodes are themselves right yeah i think
4056800	4061200	like what kona mentioned there's lots of moda setting where you have well defined features
4061920	4067280	and i think what tim referred to was more like the you don't even know what the features are like
4067280	4073200	if you have a convolutional neural network and and like what's an object um what is a feature that
4073200	4079280	like is disentangled also from other objects um so i think also there is the big issue that you
4079280	4085120	have this entanglement between concepts that i don't know that the frisbee is always with uh on
4085120	4092960	the same image as a as a dog so um maybe the the neural network can't even separate these two things
4092960	4099600	then because they are too entangled in the data set to even discover the structure that that is
4099600	4104880	really underlying the the real world in this case that's a fascinating point actually because one of
4104880	4109600	the reasons why there's no easy solution to adversarial examples is because you you learn these
4109600	4115120	these non robust features and you might just think to yourself well um fur is a low magnitude
4115120	4119120	feature it's really easy just to kind of create fur on anything and for the neural network into
4119120	4123440	thinking it's a cat and you just say well this is obvious right you just create some rules to say
4123440	4129760	well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled
4129760	4134480	in this complex neural network so you can't do that but i wanted to move the discussion on a bit
4134480	4139920	so you said that there are some really interesting challenges ahead in in iml and what's fascinating
4139920	4143760	is you start talking about the process so you say that the setting of machine learning is too static
4143760	4148400	it doesn't reflect how these models are used in reality and models are embedded in a process or a
4148400	4153280	product or even complex people interactions and i love this right because i talk about ml devops
4153280	4157680	and machine learning models in isolation are irrelevant it's the people in the process that's
4157680	4162560	where the complexity is even with intelligence itself it's a process right you know intelligence
4162560	4167680	is the interaction between a brain a body and an environment and you know within the context of
4167680	4171760	this process you know we've got all of this rich information that we could be bringing in from other
4171760	4176480	disciplines and you're saying we should bring in compsci and stats folks and we should be bringing
4176480	4181440	in psychologists and social scientists and we need to also have interpretability at a higher level
4181440	4186480	at the institutional level right or at the society level so when you kind of broaden the discussion
4186480	4193280	out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist
4193280	4199280	it's so convenient to just have this fixed model a fixed data set and then you just geek out and
4199280	4205600	invent all these methods and so on but reality is that that you use the method some place and then
4205600	4211600	it interacts with the institution it's built with the developers it's built by with the people it
4211600	4218720	affects and my favorite example there is when you have this closed loop where your model makes
4218720	4225680	predictions and these predictions generate the next generation's data so for the next generation
4225680	4231840	of the model it produces the data so there's this example of the rent index where you have this model
4231840	4238560	that tells you how much rent you should like pay for a certain kind of apartment and so on
4239040	4247040	and this is actually like um legally binding so if you're a land lord you have to accept kind of
4247040	4252800	the range that is outputted by the model which also means that the data that is produced so the
4252800	4259760	new flats that are rented out in new apartments they all have to fit the model kind of and then
4259760	4265040	but then you use this data again to train your model so you have this very weird feedback loop
4265760	4271680	and I think it's also difficult to wrap your head around it and understand implications of it
4273600	4281040	that that same thing a very similar feedback loop was a fear in the you know in our algo
4281040	4287360	shambles video about the uk testing since they couldn't conduct the the uh what was it the
4287360	4292320	a-level test right Tim they they built some some models around that and so it would do things like
4292320	4297520	well you know if this school historically never had anyone in this grade bucket then we're not
4297520	4303040	going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating
4303600	4310880	you know feedback loop we were reading through a lot of your work and you I mean I'm just going
4310880	4316640	to hit this point head on um you don't really talk that much about AI ethics and you know there's the
4316640	4321600	f word which is the fairness word and and I don't I don't recall you ever using that word
4321600	4329040	and is that something that you've deliberately shied away from um yeah I just like um
4330320	4336400	define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness
4336400	4341920	metrics or so on because I think there's a really big field on its own and I just don't know as much
4341920	4346160	like about all these things so I know a little bit like about the fairness metrics that they're
4346160	4354080	out there um I also think I mean they're kind of like research wise a little bit overlapping but
4354080	4359040	more or less separate fields I think interpretability and fairness um but of course they have some
4359040	4366160	commonalities that I mean when you kind of to for fairness you have to look it's not necessarily
4366160	4372960	inside the model but you have to study how the model behaves um and that's kind of the connection
4373040	4378800	to interpretability I would say yeah well where yeah where I see the connection is work
4378800	4386240	like yours is helping to build the tool set that will allow people to apply human you know intuition
4386240	4392560	and and ethics and evaluations to machine learning because at the end of the day a lot of these are
4392560	4398640	human moral judgments or ethical judgments and it's important that people be happy with them
4398640	4404240	because you know we have to have the population as a whole understand and accept and be able to
4404240	4410000	move forward with the increasing role that machine learning is having in our lives and building that
4410000	4414800	tool set is necessary so it's like you said very early in this talk you know what do we do just
4414800	4419760	stick our heads in the sand and ignore it and just accept machine learning models are going to do
4419760	4425120	whatever they do as long as they fly the plane or you know don't kill too many people we're okay
4425120	4429520	like I don't think that's going to work like we have to build the tool set that you're talking
4429520	4436080	about and continue this process of exploring how to better explain and interpret ML models so that
4436080	4441200	human beings can have that oversight because it's the only thing that's going to give us
4441200	4446960	comfort really as a society I suppose the reason I segue to this is we were just talking about
4446960	4452800	the process and you you mentioned some of these feedback loops because we can have a very superficial
4452800	4458720	discussion and you could say well we need to be able to represent reality better than we do and
4458720	4464560	we have a whole tool set here to identify sources of bias or you know lack of robustness etc in
4464560	4469920	models but it's so much more complex than that because these models are used in a very complex
4469920	4475760	process and you get these very very complex dynamics emerging as a result of that and I think
4475760	4481520	we're only really just scratching the surface of understanding those dynamics yeah I think so too
4482000	4489360	as I said I think in science it's always very easy to to study things in isolation like study one
4489360	4496640	type of model study one type of adjustment for a deep neural network and hopefully we will see more
4496640	4502720	work emerge on this I think I've never read the paper like I mean of course discussed implications
4502720	4509440	but really like analyze like what happens in terms of the data and the model when we have like
4509440	4514640	multiple generations for example of a model and how it changes over time but this thing I mean to
4514640	4521280	study those things also means that you have to wait for a long time until you have these dynamics
4521280	4527120	and I think in many cases it's just starting that we use these models more extensively in our daily
4527120	4534640	life I have a question is is anyone because look interpretability metrics whatever they are say
4534720	4539680	saliency maps and you know could be partial dependency plots whatever you could actually
4539680	4545360	build in some requirements of those into the objective functions when you go to train models
4545360	4549520	so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at
4549520	4555920	all but somebody could say look I want all my saliency maps to be you know sets of of a
4555920	4559360	bezier curves or something like that like they have to have a certain smoothness
4560000	4564880	property and you could actually put that as a constraint in the objective function has anybody
4564880	4570160	tried anything like that yeah there are approaches so I saw one paper they added some
4572080	4578000	some parts to their objective function so that when you create line explanations with line that
4578000	4585200	they were most more stable and there are a lot of things like for neural networks you have
4585200	4593280	disentanglement that you try that the feature maps or that the nodes learn disentangled concepts
4594640	4600320	there are ways to introduce like monotonicity so that a feature can always go into the effect of
4600320	4607600	a feature can always be in one direction not to like zigzag around so there are approaches to do
4607600	4617200	this to like have okay like interpretability constraints in your modeling here yeah because
4617200	4620960	I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we
4620960	4627280	just create white box models maybe we can use if the definition to a human being of white box is
4627280	4632000	that it's interpretable and understandable if we can build into the objective functions when we're
4632000	4637200	actually training the network that it has these properties then we'll actually be helping to create
4637600	4643440	more white box you know models even if they are complex that's definitely an option but I think the
4644480	4649200	issue remains the same that you with it's similar to a white box model I mean you'll make some trade
4649200	4655840	offs in the end you have to make the judgment whether so when you put more constraints I mean
4655840	4660960	you can actually also help the model of course that if you I mean if you have some inductive biases
4660960	4667760	also which which you infuse into the model which help with predicting or be more more stable
4669280	4674960	but sometimes you might maybe also trade off with accuracy and you just have to like in the end you
4674960	4683120	have this yeah this set of models where some are more accurate some better than this one
4683120	4689440	interpretability dimension the other is cheaper to deploy and then you have this so this kind of
4689440	4694160	going into direction of like automatic machine learning and you don't get like just the best
4694800	4700000	performing one but you have this parater set like well so we have multiple objectives that you want
4700000	4704880	to hit and then there's not one model that works best but you have a set of models that
4704880	4710160	that have different trade-offs between these objectives and then you have to decide what
4710160	4716960	is the trade-off that you want to do that you want to have I guess there's no getting away from it is
4717040	4721680	the interpretability it is going to get more important and more important I think you mentioned
4721680	4726240	Christoph that you know we've had linear models for hundreds of years and then there's been this
4726240	4732800	big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability
4732800	4736720	is really kicked off I what do you think do you think where's it going are we just going to get
4736720	4742960	more and more attention paid to this area I don't know if you can get more than than this I don't
4742960	4750560	know yeah but I think it's at least here to stay and I think it's important I mean it has been
4750560	4758960	important before but of course with like the push from deep learning especially and that it just
4758960	4763600	became more clear to a lot of people that we need interpretability in some sense at least
4764960	4771120	yeah of course people have attempted it before and worked on it before it's just more urgent now
4771360	4777760	I really like the bit in your book talking about what's changed recently how interpretability is
4777760	4781920	coming together as a field you know with this a unification so you know in physics we love a
4781920	4785920	big unification when you take all these different things in the past and say oh they're all just
4785920	4791120	part of this one big framework and it was chap that chap paper was amazing wasn't it saying things
4791120	4797520	like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of
4797520	4802480	these additive feature attribution methods and we can prove that this is the only one that's
4802480	4807440	theoretically valid because it has these properties of symmetry it's got the stummy property so
4808160	4811040	you know everything that's been done before in interpretability well they're all in our
4811040	4816080	framework now and shapely values are the way forward yeah they're quite uh quite famous to
4816080	4821120	shapely values yeah would you believe them then I mean I guess in their paper they're kind of
4821120	4824960	they kind of disagree with lime a bit don't they they say well lime is a count of this but
4825040	4829760	they're going to be breaking our properties of efficiency and symmetry so lime is using the
4829760	4834240	wrong weights right they should be using this yeah kernel shape weights rather than the line weights
4834240	4840320	I think that's just a different approach also to think about it I mean you don't maybe you don't
4840320	4848320	I think I think the properties are quite attractive or meaningful at least um but also also the
4848480	4855840	line approach I'm very critical about lime because it um I think it's difficult to have the correct
4856560	4865600	to know like how to parameterize your your local models um so I think I'm a bit more of a fan
4865600	4871440	of shapely values because of the theoretical properties it comes with are you talking about
4871440	4875840	that distance measure in lime where you have to be able to quantify how far away is the permutation
4875840	4883600	yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and
4883600	4888960	I mean it's it's a very difficult question it goes to the heart of like what's local um
4889840	4893840	because like I mean you have this this kernel that decides like how much you weight
4893840	4899920	all the data points around the point you want to explain and and like how how big is this area
4899920	4905200	I think this is very dependent on your model and your data and there's no answer no easy answer to
4905200	4911200	how to set it yeah or even more generally than that this this whole notion of what does it mean
4911200	4916720	to have a local interpretation method in you know in text or vision so in in vision there's this
4916720	4921120	super pixel concept which is something that seems to make intuitive sense but but does it you know
4921120	4926800	when you create all of these different uh maskings of different parts of the input space but um
4926800	4932000	with shapely values as well that they they are a beautiful a beautiful technique especially
4932080	4938000	because the the values are are quite meaningful but if you have shared information between the
4938000	4942240	features I mean Connor and I were talking about this for example you if you had the same model
4942240	4948080	where you were predicting someone's income and you put their I don't know let's say you had salary
4948080	4954960	in the model twice then the shapely value would be divided between the two duplicate fields right
4954960	4961520	so there just seems to be so much esoterica in these IML methods right are we expected to know
4961520	4966960	all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things
4966960	4971520	that you have to know all the these these uh disadvantages of the methods where I try to
4971520	4978640	be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools
4978640	4985520	that we usually have um also with statistics and so on you you have to know like um these like as
4985520	4989840	you mentioned this if you have salary twice then it will just I mean depends also on what your model
4989840	4996160	does if it just picks one of the salary features or if it itself uses both so this also something
4996160	5002640	that will define how the shapely value will look like later on um but you have to know these things
5002640	5007520	if you want to use shapely values and interpret interpret them correctly yeah because I think
5007520	5012800	philosophically we've got we've got the real behavior and then we use these interpretability
5012800	5017760	methods and then we've got the kind of perceived behavior so we've got these these levels of
5018400	5024720	modeling or or do you know what I mean simplification and it's all well and true if you are dealing
5024720	5031520	with data scientists who understand how these you know methods work that's fine but invariably
5031520	5037760	data scientists need to present this information to lay people and they are not going to understand
5038480	5042480	all of the various different trade-offs and how information is being compressed and and lost and
5042480	5048880	so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any
5048880	5057920	number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh
5057920	5063040	outcomes of statistical models that uh well everyone can understand well of course not because
5063040	5068080	you need training to understand how to interpret a linear model or any regression model yeah there's
5068160	5074080	difficulty but I don't think it's new in any sense um because there's always I mean any number that
5074080	5080800	you read anywhere has a very complex process um so I don't know if you have like COVID testing
5080800	5086640	numbers it's very complex like how the number was generated maybe like how it was aggregated over many
5086640	5092400	states and like what cases it includes and which it doesn't and so the number looks very innocent
5092400	5098000	and simple but there's a very long process behind it to produce it um maybe this process is a bit
5098560	5104240	uh a bit more black box or a bit more difficult if it comes out if there's some machine learning
5104240	5110160	in between machine learning model in between to generate a number um but yeah I think this
5111040	5119120	problem is well old well let me let me challenge a little bit here on something which is okay if
5119120	5124960	I have if I have some general formula just some very general formula and then I go in there and I go
5124960	5131200	you know what this formula has five parameters and if I make this one point seven five and that one
5131200	5139840	one third and this one two and that one zero and I call this the megatron you know uh activation
5139840	5145760	potential and I go and write a paper about it that's really just an arbitrary you know kind of
5145760	5151120	selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it
5151200	5157120	published in some journal and now everybody has to memorize that as you know the megatron potential
5157120	5161920	and kind of learn about it and that's a lot of what's going on right now is that it's really just
5161920	5166560	a bunch of hacking like it's people just they don't really know a general solution and they don't
5166560	5171520	know how to solve like in general the problem they're trying to solve and so they just hack around and
5171520	5177120	then the ones that are kind of famous or demonstrate some success in a particular combination you know
5177200	5181760	competition over in this corner or something it now becomes something that's part of the lexicon
5181760	5186400	that we all have to learn and I think like I look back on this like imagine what physics was like
5186400	5191840	before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole
5191840	5196960	bunch of little purpose built kind of formulas and then along comes a general framework which
5196960	5202240	now we can just learn calculus and derive the special circumstances as needed. You're onto
5202240	5208080	something really interesting there which is that with IML methods we are we are kind of
5208080	5213440	compressing information down into a representation you know and then that that is a transport that
5213440	5218160	can be understood by different people but there's a trade-off right because as you said you can learn
5218160	5223040	calculus and that's a compact framework for doing lots of stuff but it's all about the amount of
5223040	5230240	common knowledge that is required so it's possible to compress something down just to one symbol
5230240	5233840	and that symbol could represent all of that knowledge but it doesn't help you because I still
5233840	5240640	need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple
5240640	5246160	framework that I could learn and then once I learned that simple thing I could go and solve all
5246160	5250320	kinds of problems with it that before I would have to memorize specific solutions or like the
5250320	5255840	quadratic formula for example is a student I didn't actually memorize the quadratic formula I just
5255920	5260240	learned how to complete the square and then I would just do complete the square and if somebody
5260240	5264720	asked me what the quadratic formula was I would just quickly derive it right because it was
5264720	5270080	easier to memorize the rule and then apply the rule to any situation rather than to memorize
5270080	5275840	all these little one-off you know kinds of hacks that we come up with. You're not normal Keith
5275840	5280640	right so most people won't be able to go and understand this because I think it's well no
5280640	5287520	these IML methods are brilliant for data scientists who can it's a framework right it's a
5287520	5292000	reference of understanding so assuming that people can understand how Shapley values work
5292000	5298320	then this is a beautiful representation to reason about the behavior of models. Sure but when I
5298320	5303840	first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis
5303840	5309520	to marginalization you know all we're really doing here is computing the expected marginal you know
5309520	5315280	contribution to this value it's not a probability but it's still the same procedure being done right
5315280	5320000	and I think I'm going to throw myself in with the lay people to a degree because the reason I'm
5320000	5326320	always striving for simplifications is because I don't have the capacity to memorize all these little
5326320	5332080	arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I
5332080	5337360	said you know previously in some other videos statistics made no sense to me until I learned
5337360	5344080	the Bayesian framework because that was based on very simple rules that I could then reapply as needed.
5345600	5352720	I think that what you refer to Keith maybe the worst situation is with the saliency maps because
5352720	5359360	you have so many methods and they all like back propagate the gradient and to do input pixels
5359920	5367120	and to um now do you have to like learn like how dozens of these framework works or like
5368240	5373280	how to interpret do interpretation with all of these and they're all kind of variants of each
5373280	5381680	other so mostly because they just there's some ambiguity how you how you back propagate the
5381680	5388800	gradient because because of the non-linear units and stuff and a little bit differences how you
5388880	5398320	can define this and so you have this huge like a sea of many different methods I think it would be
5398320	5403280	nice therefore as you said to have some like simplification where you say okay this is like
5404480	5409280	all these methods work under this one principle basically and we have these two parameters
5410320	5416640	and that's how they differ I think that's also that some I think I wrote something in a chapter
5416720	5425840	that the police stop inventing new methods for saliency maps so I think it's enough and we
5425840	5433040	should focus more on like doing this consolidation to like understand the limitations of the methods
5433040	5438720	and consolidate them to see like what's the commonalities in which ways do they differ and so on
5438720	5442960	that's probably actually my first impression actually when I first opened the interpretable ML
5442960	5447680	book I was amazed how many different things there are I've you know heard people say ah
5447680	5452080	you can't use machine learning it's just a black box so many times it'd almost been drilled into my
5452080	5458240	head then seeing all the things you know from white box models ways of training salient models
5458240	5462400	counterfactual explanations it's what a wonderful recipe right there are so many different things
5462400	5467840	that you can do I feel like now I trust ML models more than other kinds of things because I have
5467840	5473520	this amazing toolbox of ways to understand them the thing that strikes me though is
5474400	5478800	most of these methods as we were just saying they require interpretation by a human and a human who
5478800	5484480	understands how the method works I love this concept of turning machine learning into an
5484480	5492400	engineering discipline and being able to do a lot of these tests non-interactively and I think
5493120	5497600	Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping
5497600	5503360	and what excites me about these methods is they seem like methods that we could actually run
5503360	5508400	as part of an automated process we still have to set thresholds maybe we could set a threshold
5508400	5513840	that said if this if this counterfactual example flips the switch on more than one percent of
5513840	5519200	examples then fail the build that seems reasonable but a saliency method I mean how the hell do you
5519200	5523840	say well if there's lots of red pixels over here then break the build I mean it's just ridiculous
5524720	5532640	yeah yeah so I've seen interesting approaches to like using interpretability also more
5532640	5541680	automatically like when you do model monitoring you can do things like create interpretations
5541680	5547920	and see if they significantly change over time for example so then have thresholds that warn you
5547920	5554880	that hey something's going on with your model so I think that's also interesting approaches there
5555520	5561760	yeah you know Tim to your point of making this an engineering field and even making interpretability
5561760	5566640	and and understandability an engineering field I mean I think that maybe that's why I like your
5566640	5571520	book so much Christoph as I think it's it's a step towards that direction it's like let's
5571520	5579280	survey everything and more importantly let's create a finite and hopefully smallish set of simple
5579280	5585760	concepts that we can all agree on and understand that we can use to catalog you know what's out there
5586560	5591680	so please keep up the good work you know I'm interested to see where this goes so final
5591680	5595440	question for you Christoph then I wonder what's what's next for interpretability like are we going
5595440	5599920	to the point where it's going to be almost a box ticking exercise where we can say yes our process
5599920	5605840	we've done the standard interpretability step I mean is it is computing power going to change it
5605840	5611680	I remember when Shaq the library came out it made that approach possible whereas previously you know
5611680	5616320	it was very hard very computation infusible my friend Angem who's a wonderful data scientist
5616320	5621520	sent me n-video rapids they've got that running on GPUs way faster than of course before is it
5621520	5626400	just going to become a standard step do you think or is it going to be something where you need
5626400	5631840	decent subject matter expertise and some real thought to do to really understand how a model
5631840	5638640	works so well predictions about the future are always hard so maybe more like what I wish or
5638640	5644320	what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot
5644320	5650320	of implementations of these methods so they're kind of getting a commonality a rook one can use it
5650320	5658720	very easily there's a lot of libraries out there in python r but also in like this machine learning
5659680	5666960	cloud tools they also have a lot of interpretation methods available now so in that sense I think
5667600	5674800	and it's maturing a lot I still believe that we need some expertise to understand them or at
5674800	5679200	least some good references and there will also be hopefully more than my book maybe have some
5680800	5686480	documentation when for these tools and people answering on stack overflow questions and whatnot
5688240	5694240	so I think um yeah it's it's getting we're getting that everyone can use it easily
5695840	5701840	I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics
5702720	5706400	you know governance process or something the last thing you want is for it just to be an
5706400	5711120	automatic response so I've just you know yeah I've thought about AI ethics um it needs to be
5711120	5716400	something that that we really engage with I think we need to abstract away a lot of the complexity
5716400	5722080	at the moment I think it's possible to come up with an interface to standardize the way that we
5722080	5728800	do interpretability and we can reduce down what we have now to certain primitives which means that
5728800	5733280	it can plug into an engineering process and it also means that we can abstract away some of the
5733280	5738880	complexity I think that's possible yeah I also would agree that it shouldn't be like just box
5738880	5744400	ticking but you can like for the initial um when you start interpreting a model that you just have
5744400	5749200	like with a click you have a report and then it shows you the most basic things but then you still
5749200	5753360	like should ask the like the question like does it really make sense that this feature is the most
5753360	5757680	important one or what's happening there with these weird interactions between the two features
5757680	5764720	let's dig a bit deeper here and see what's going on so I think um there's this one portion that is
5764720	5772160	just like this automated reporting thing um but this should then be like the starting point for
5772160	5779200	more critical uh questioning of the model and and and checking what's going on um for some specific
5780160	5786480	problems maybe so it's going to be you click on the molnar report and it gives you the the report
5786480	5791840	from the book right yeah there will be convenience well um christoph molnar thank you very much for
5791840	5795600	joining us today it's an absolute honor to have you on the show thanks for having me hey folks this
5795600	5798960	is tim in post script there's just a couple of thoughts that didn't come to my mind during the
5798960	5803600	interview that I think I'd like to quickly cover now the first thing is on the lack of fairness
5803600	5810160	the reason why I raised that is most folks who talk about AI ethics and fairness they use the
5810160	5815600	toolkit of interpretability methods quite often you know to apply their trade there are tools out
5815600	5822000	there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this
5822000	5830400	what we really need is an operating model or a set of guidelines on how to implement these tools
5831120	5837760	how do I identify sources of problematic correlations we need to have a database of problematic
5837760	5843840	correlations having a tool that allows me to identify and mitigate bias frankly is useless
5844560	5850160	what do I do with that as we mentioned on the show many of the machine learning cloud providers
5850160	5856560	whether it's data iq or azure ml and sage maker they all have these interpretability methods built
5856560	5862640	in now including saliency maps and it's just a box ticking exercise frankly it's completely useless
5862640	5869680	there is no accepted guidance on how these tools should be used right so if I'm a large company
5869680	5875600	and I'm building an operating model around how to implement fairness techniques just having the
5875600	5881120	technology is irrelevant it's about the people and the process and the the kind of operating
5881120	5887280	model of how we implement it and there is basically no useful information out there to help us do that
5887280	5892960	the other thing is we spoke about this becoming an engineering discipline which is to say what if we
5892960	5898720	could create an interface to abstract away some of the vagaries and esoteric of interpretability
5898720	5904400	methods we might come up with some primitives or some common language and then we can hide the
5904400	5909440	complexity behind the interface this is kind of what we do with mo dev ops already we automate
5909440	5914960	as much as we can then we templatize and remove friction out of the process we even create building
5914960	5922640	blocks using domain specific languages or yaml files and pipelines and and so on so what we do
5922640	5928800	is is we create a level of abstraction where people can compose together pipelines remember
5928800	5933600	when Conor made the comment that this might just become a box ticking exercise and this is something
5933600	5940640	we see in security and AI ethics already we can't really trust people to self report that the model
5940640	5947040	is behaving correctly or that the project has no concerns from an AI ethics point of view the whole
5947040	5952960	point here is process if we want to create an operating model and ensure best practices are
5952960	5959360	followed or any kind of standardization in a large organization we have to design a process
5959360	5967760	and many eyes make shallow holes so the process would mandate that a certain number of stakeholders
5967760	5974320	were involved in assessing the particular iml technique and validating it essentially and
5974320	5979760	then we would need to record that assessment so who said what when and then if the company ever
5979760	5984960	became audited or if god forbid there was some kind of a problem where the iml model did something
5984960	5989920	wrong and it caused the company lots of damage or it harmed the environment or society or something
5989920	5995040	like that we would then be able to rewind the clock and say okay well joe blogs said it was okay
5995040	6002960	because of xyz so that is an operating model it's a process and how to design such a process again
6002960	6009600	is completely absent speaking as a chief data scientist myself that's the kind of thing that
6009600	6014560	i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the
6014560	6019680	episode today we've had so much fun making it remember to like comment and subscribe and we'll
6019680	6025520	see you back next week
