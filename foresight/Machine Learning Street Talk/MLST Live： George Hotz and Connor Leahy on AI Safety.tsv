start	end	text
0	8880	we're gonna count them so I'm hoping that will just go straight live let's see
18400	27560	I gotta mute that so it doesn't echo yeah okay I gotta hide chat I can't watch
28400	34440	these YouTube's are so I'm just gonna minimize this okay excellent we are live
34440	38720	so um we've got a few minutes folks before the live kicks off I just thought
38720	42640	I'd run the stream just so that we know we're live and ahead of time we were
42640	47840	just joking about Hotsys smack talk yesterday on his live stream and I very
47840	53960	much hope that the subject of Von Neumann comes up oh yes yes yes I did see
53960	58560	I saw Twitter comment that it was going to be Von Neumann versus chicken man chicken
58560	65160	1000 Von Neumann versus chicken man we are we are treading new debate
65160	68840	territory hitherto unknown to man
69200	76760	chicken man came up on a stream I look forward to finding out what chicken man
76760	85720	is yeah sorry about that I just heard myself coming back okay well yeah this
85720	91820	is gonna be amazing my god two of the heavyweights live here on MLST you've
91820	96240	only got another minute or so folks before we kick off now we were just
96240	99520	talking about how you know these arguments have been refined over a lot of
99520	106400	lunchtime talks yeah I think you have a stable position and I think I have a
106400	110240	stable position I think some of the other guests in your debate don't exactly
110240	112680	have some of the things are just you don't really think this through all the
112680	118480	way so I think I thought it through to the end fantastic my I really look forward
118480	124000	I watched did you watch Rich Sutton just did a pic oh no I have not oh god I
124000	128040	can only imagine I love Rich Sutton oh man you know bitter lesson like he's
128040	134200	he's he knows AI he is a character he is definitely one of the people of all
134200	141800	time yeah yeah yeah the man is a legend well I guess we should slowly kick off
141800	146120	folks so I'm just gonna do my spiel before I hand over and first of all two
146120	150080	hots if that's okay for your 10 we'll talk about that are you okay well the
150080	153760	thing is Connor always likes to go first and I think that you know Connor's
153760	158120	mental model might not be robust to going second would you would you be okay
158120	162120	going second or you want to go first kind of fine I'll start but it won't be a
162120	167200	full intro how about that I am completely robust to all permutations
167200	176400	adversarial training amazing amazing okay well in which case let's crack on so
176400	181040	ladies and gentlemen get ready to meet the cunning maverick of Silicon Valley
181040	185840	the one and only George Hott's renowned for his daring exploits Hott's commands
185840	191000	an enigmatic persona which merges the technical finesse of Elon Musk and the
191000	196440	wit of Tony Stark and the charm of a true tech outlaw now many of you would
196440	200720	have or indeed should have seen this man on Lex's podcast recently for the third
200720	206320	time no less from craftily jailbreaking the supposedly invincible iPhone to
206320	211280	outsmarting the mighty PlayStation 3 he's proven that no tech fortress is
211280	215880	impregnable once targeted for his audacious creativity by Sony with a
215880	220680	lawsuit this hacker wizard stoically dance past the curveballs thrown by the
220720	225160	tech giants all achieved with the graceful swag of a street smart prodigy
225160	228920	now when he's not outfoxing major corporations you'll find him at the
228920	233720	heart of the avant-garde of AI technology gallantly trailblazing through
233720	237480	the wilds of the tech front here he's currently building a startup called
237480	242400	micrograd which is building superfast AI running on modern hardware and truly
242400	248880	he's the James Bond of Silicon Valley minus the martinis of course now please
248920	253880	welcome the unparalleled code cowboy the unapologetic techno man sir George
253880	259320	Hott's whoo anyway also joining us for the big fight this evening is the
259320	264520	steadfast sentinel of AI safety Conor Lee he undeterred by the sheer
264520	269240	complexity of artificial intelligence Conor braves the cryptic operations of
269240	273660	text generating models with steely resolve now about two years ago Conor
273660	277560	took on the Herculean task of safeguarding humanity from a potential
277560	283320	AI apocalypse his spirit is relentless his intellect razor sharp and his will
283320	288880	to protect is unwavering now drawing on his contentious claim that we are super
288880	295280	super fucked yeah Conor channels the urgency of our predicament into his
295280	299360	work now his startup conjecture isn't just a glorified tech endeavor but it's
299360	303960	a lifeboat for us all racing against the breakneck speed of AI advancement with
303960	308560	the fates of nations possibly at stake he's determined to break the damning
308560	315120	prophecy and render us super super saved so brace for a showdown as Conor Lee
315120	320360	he the maverick defender of AI's boundaries strides into the ring now the
320360	324600	man who declared we're super super fucked is here to prove just how super super
324600	329960	not fucked we could be if we make the right decisions today so please give it
329960	335480	up for mr. Conor super super Lee he now Conor I'd appreciate it if you don't go
335480	339320	down in the fourth I want this fight to go the distance now we're running for
339320	345400	90 minutes this evening there'll be a 10 minute openers from from we said
345400	350240	hots didn't we from from hots first and then Conor and I'll only step into the
350240	353840	ring if the punch up gets too out of hand and unfortunately we won't be
353840	357440	taking live questions today because we want to maximize the carnage on the
357440	362000	battlefield so Conor Lee he your openings sorry George Hott's your opening
362000	369760	statements please um yeah we're super super fucked I think I agree with you
369760	375800	well that was a short fight yeah I think okay um so to make my opening statement
375800	379440	clear and why maybe it doesn't make that much sense for me to go first I think
379440	386800	that the trajectory of all of this was somewhat inevitable right so you have
386840	392520	humans over time and you can look at a 1980 human and a 2020 human they look
392520	397120	pretty similar right Ronald Reagan Joe Biden you're not all the same um whereas
397120	403480	a 1980 computer is like an Apple II and a 2020 computer is a is a m1 max MacBook
403480	407880	like lines looking like this right so you have one line like this one line like
407880	411320	this these lines eventually cross and I don't see any reason that line is gonna
411320	416760	stop right I've seen a few of the other guests argue something like LLMS can't
416760	422480	problem-solve or but it doesn't matter like if this one can't the next one will
422480	426160	whatever you call I don't believe that there's a step function I don't believe
426160	429560	that like oh now it's conscious so now it's intelligent I think it's all on a
429560	434880	gradient and I think this gradient will continue to go up will approach human
434880	441800	level and will pass human level now this belief that we are uniquely fucked
441800	446640	because of this the amount of power in the world is about to increase right when
446640	449200	you think about power and you think about straight up you can just talk about
449200	453640	energy usage the amount of energy usage in the world is going to go up like the
453640	457560	amount of intelligence in the world is going to go up we may be able to do
457560	462040	some things to slow it down or speed it up based on political decisions but it
462040	465840	doesn't matter the trajectory is up or major catastrophe right the only way it
465840	470280	goes down is through war nuclear annihilation bio annihilation meteor
470280	475920	impact some kind of major annihilation so it's going up what we can control and
475920	480000	what I think is super important we control is what the distribution of that
480000	488080	new power looks like um I am not afraid of superintelligence I am not afraid to
488080	494160	live in a world among superintelligence I am afraid if a single person or a
494160	499040	small group of people has a superintelligence and I do not and this is
499040	504320	where we get to chicken man a chicken man is the man who owns the chicken farm
504320	508680	there's many chickens in the chicken farm and there is one chicken man it is
508680	513600	unquestionable that chicken man rules and if you believe chicken man rules
513720	520800	because of his size I invite you to look at cow man who also rules the cows and
520800	524080	the cows are much larger than him chicken man rules because of his intelligence
524080	527920	this is basic less wrong stuff everyone kind of knows this how the squishy things
527920	531000	take over the world look I agree with Ellie as you cast you all up to new
531000	540320	fittings right um so I do not want to be a chicken and if people decide they are
540320	545400	going to restrict open-source AI or make sure I can't get access to the compute
545400	549920	and only trusted people like chicken man get access to the compute well shit man
549920	556400	I'm the chicken and yeah I don't want to be the chicken so I think that's my are
556400	563440	we fucked maybe um I agree that that intelligence is very dangerous how can
563440	567120	you look at intelligence and not say it's very dangerous right intelligence is
567120	574200	somehow safe but things like nuclear bombs are an extremely false equivalency
574200	580080	because what does a nuclear bomb do besides blow up and kill people intelligence
580080	583600	has the potential to make us live forever intelligence has the potential to
583600	589920	let us colonize the galaxy intelligence has the potential to meet God nuclear
589920	595560	bombs do not they just blow up um so I think the question and like you have
595560	598920	things like crypto which are a clear advantage to the defender at least
598920	602240	today and you have things like nuclear bombs which are clear advantage the
602240	609600	attacker AI it's unclear I think the best defense against an AI trying to
609600	613280	manipulate me and that's what I'm really worried about future psyops you know
613280	616440	already seeing it today with the voice changer stuff like you never gonna know
616440	621320	who's human the world's about to get crazy um the best defense I could
621320	627160	possibly have is an AI in my room being like don't worry I got you it's you and
627160	632320	me we're on a team we're aligned I'm not worried about alignment as a technical
632320	636040	challenge I'm worried about alignment as a political challenge Google doesn't like
636040	641800	yeah it doesn't like me but me and my computer you know we like each other
641800	647040	we're aligned and we're standing against the world that has always since the
647040	651480	beginning of history maximally been trying to screw you over right
651480	655400	intelligence people think that one super intelligence is gonna come and be
655400	659560	unaligned against humanity all of humanity is unaligned against each other
659560	665620	I mean we have some common values but really come on everyone's trying to
665620	668720	scam everybody the only reason you really team up with someone else is like
668720	673200	hey man what if we team up and scam them right and what if we team up call
673200	677840	ourselves America and we we we uh we build a big army and say we're free and
677840	683520	independent yeah right it's that force that has made humanity cooperate
683520	688240	humanity by default is very unaligned and has every kind of belief under the
688240	692680	Sun so I'm not worried about AI showing up with a new belief under the Sun I'm
692680	695720	not worried about the amount of intelligence increasing I'm worried
695720	700360	about a few entities that are unaligned with me acquiring godlike powers and
700400	710720	using them to exploit me cool yeah thanks that's uh that's I mean yeah I
710720	714600	also kind of agree with you and most of the things you say a few details I'd
714600	718960	like to dig into there but for most of the things you say I do think I agree
718960	723040	with you here I think it's absolutely let me just like start with saying I
723040	728920	totally agree with you that misuse and like you know bad actors what using a
729000	735520	AI is a horrible dangerous outcome that's that's like you know sometimes the
735520	739720	less wrong you know crowd likes to talk about x-risk but also sometimes I've
739720	744400	talked about s-risk suffering risks so things are worse than death I believe
744400	749480	that you can probably almost only get s-risk from misuse I don't think you
749480	754960	can get s-risk problem like you can but extremely unlikely to get it from like
754960	758720	just like raw misalignment like you'd have to like get extraordinarily
758720	764800	unlucky so while I do it so I do think for example a very you know controllable
764800	770320	AGI or super intelligence in the hand of sadistic psychopath is significantly in
770320	774600	a sense worse than a paperclip maximizer so I think this is something we would
774600	780240	agree on probably so I think I'm thinking pretty much on board with you on a lot
780240	784520	of things there where I think things come apart a bit the tail as I think
784520	788920	there's two points where I would like to take this my opening statement to take
788920	792560	this line I want I want to talk about the first one is I want to talk about the
792560	798160	technical problem of alignment so am I concerned about the kinds of things like
798160	802080	misuse and like small groups of people centralizing power potentially for
802080	806280	nefarious deeds yeah I think this is a very very significant problem that I do
806280	808680	think about a lot and that'll be the second thing I want to talk about the
808680	811400	first thing I want to talk about is that I don't even think we're gonna make it
811400	816440	to that point I don't think we're going to get to the point where anyone has a
816440	821120	super intelligence that's helping them out we're good if we don't solve very
821120	824040	hard technical problems which are currently not on track to being solved
824040	828480	by default you don't get a bunch of you know super intelligence and boxes
828480	831800	working with a bunch of humans you get a bunch of super intelligence you know
831800	835480	fighting each other working with each other and just ignoring humans humans
835480	839840	just get cut out entirely from the process and even then you know prop it's
839840	843680	you know whether one takes over or they find equilibrium I don't know like you
843680	846680	know who knows what happens at that point but by default I wouldn't expect
846680	849320	humans to be part of the equilibrium anymore once you're once you're the
849320	853880	chicken man well why do you need chickens you know if you know maybe if
853880	856960	they provide some resource for you the reason humans have chickens is that they
856960	861480	make chicken breasts I mean personally I wouldn't like to be harvested for chicken
861480	867000	breasts just my personal opinion I consider this a pretty bad outcome but
867000	870760	even then well as a chicken man finds a better way of chicken breasts or you
870760	874280	know modifies himself to no longer need food I expect the chickens are not
874280	877040	gonna be around for much longer you know once we stop using horses for
877040	881240	transportation didn't go very well for the horses so that's kind of the first
881240	885000	part of my point that I'd like to you know maybe hear your opinions on
885000	889240	hear your thoughts on is that I think the technical problem controls actually
889240	894280	very hard and I think it's unsolvable by any means I think like you know you and
894280	897280	like you know bunch of other smart people work on this for like 10 years I
897280	901280	think you can solve it but it's not easy and it has to actually happen and there
901280	905200	is a deadline for this the second point I want to bring up is kind of where you
905200	908920	talk about how humans are unaligned I think this is partially definitely true
908920	913800	but I think I'm unusually I am the more optimistic of the two of us in this
913800	918760	scenario not a not a role I often have in these discussions where I actually
918760	922840	think the amount of coordination that exists between humanity especially in
922840	927640	the modern world is actually astounding every single time to adult human males
927640	931360	meet and don't kill each other is a miracle have you seen what happens
931360	935520	when two adult male chimps from two different war bands meet each other it
935520	938880	doesn't go very well and those are already pretty well coordinated animals
938880	943320	because they can have war bands what happens when you know two male bugs or
943320	947040	you know I know sea slugs meet each other you know either they ignore each
947040	951680	other or you know things go very poorly this is the default outcome the true
951680	956040	unaligned outcome the true default state of nature is you can't have two adult
956040	960880	males in the same room at any time I saw this funny video on on Twitter the other
960880	963800	day where it was like I was some parliament I think in East Europe or
963800	968120	something and there's this big guy I'm just like going at this politician he's
968120	972000	like in his face he's like screaming he was like going everywhere and not a
972000	976040	single punch was thrown no then no one took out a knife no one took out a gun
976360	981440	and I was like and I was like wow the fact that we're so civilized and we're so
981440	986440	aligned to each other that we can have something this barbaric happen and no
986440	991640	one throws a punch is actually shocking this is very unusual even for humans if
991640	997880	you go back 200 years punches and probably gunshots would flow so this is
997880	1001080	not to say that humans have some inherent special essence that we're good
1001400	1006600	that we have solved goodness or any means what I'm saying is the way I like to think
1006600	1011000	about is that coordination is a technology is a technology you can improve upon it is
1011000	1015080	you can develop new methods of coordination you can develop new structures new institutions
1015080	1018840	new systems and I think it's very tempting for us living in this modern world to it's
1018840	1022360	kind of like a fish and water effect we forget how much of our life you know a lot of our
1022360	1027480	life is built on you know atoms on you know physical technology a lot of it's built on
1027480	1034040	digital technology but a lot of it has been on social technology and when I look at how
1034040	1037800	you know how does the world go well like you know should it be only the you know special
1037800	1042680	elites get control of the AI I'm like well that's not really how I think about it when I think
1042680	1047080	about it way more is what is the coordination mechanism or we can create a coordination
1047080	1052760	selling point or we can create a group an institution a system of some kind that where
1052760	1058040	people will you know have game theoretic incentives to cooperate on the system that results in
1058040	1063480	something that is net positive for everyone because the truth is that positive some games do exist
1063480	1069080	and they're actually very profitable and they're very good and I think if we can turn you know
1069080	1073160	you can turn any positive some game to a net into a zero or a negative some game pretty easily
1073160	1078040	it's much easier to destroy than is to create but I think it's absolutely possible to create
1078040	1082760	coordination technology around AI and to build coordination mechanisms that are net positive
1082760	1088440	for everyone involved so those would be like my two points happy to dig into any ones you
1088440	1092280	you think would be it'll lead to an interesting interaction. Sure so I'll start with two and
1092280	1098120	then go to one so two I moved to Berkeley in 2014 and I threw myself at the merry calls
1098920	1102200	I showed up at the merry office and I'm like hi I'm here to join your calls
1103160	1108360	and what I started to realize was
1110600	1118280	merry and less wrong in general have a very poor grip on the practicalities of politics
1119080	1126760	very much I think there was sort of a split you know uh Curtis Yargon like meal reaction
1126760	1131000	this is a spin-off of rationality and it's a spin-off of rationality that understood the
1131000	1135880	truth about human nature so when I give you that you give that example of two chimps meeting in
1135880	1141800	the woods and they're going to fight if I'm one of those chimps at least I stand a chance right
1141800	1149400	he might beat my ass I might beat his but if I come up against the FBI things do not look good
1149400	1154440	for me in fact things so much do not look good for me there's no way I'm going to beat the FBI
1154440	1162680	the modern forces are so powerful that this is not a oh we've established a nice cooperative
1162680	1168120	shelling point this is a we have pounded so much fear into these people that they would never even
1168120	1175000	think of throwing a puncher firing a gun we have made everybody terrified and this isn't good
1175640	1180120	we didn't we didn't achieve this through some enlightened cooperation we achieved this through
1180120	1186760	a massive propaganda effort right it's the joke about you know the american soldier goes over to
1186760	1192280	russia and it's like uh man you guys got some real propaganda here and that the russian soldier is like
1193000	1196840	yeah no i know it's bad but it's not as bad as yours and the american soldier is like
1197400	1203640	what propaganda and the russian just laughs right so so this this didn't occur because of um this
1203640	1211800	occurred because of a absolute tyrannical force decided to dominate everybody right um now oh i
1211800	1215880	think so uh i think there's a way out of this i think there actually is a way out of this right
1215880	1220200	and i wrote a blog post about this called individual sovereignty and i think a really
1220200	1226360	nice world would be if all the stuff to live food water health care electricity were
1226360	1230680	generatable off the grid in a way that you are individually sovereign this comes back to my
1230680	1236520	point about offense and defense right if i have a world where you don't want it to be extreme
1236520	1241640	defense you don't want every person to be able to completely insulate them but you want like okay
1241640	1246360	it takes a whole bunch of other people to gang up to take that guy out right like that's that's a good
1246360	1252760	that's a good balance um and the balance that we live in today is there is one pretty much a
1252760	1257880	unipolar world i mean thank god for china but um you know there's one there's one unipolar world
1257880	1262280	you got the america and what are you gonna run i'll pay taxes i don't care if they move overseas
1262280	1268520	right um so yeah my point about the coordination is that if you're okay with solving coordination
1268520	1275320	problems by using a single a singleton super intelligent ai to to to make everybody cower in
1275320	1281560	fear and tyrannize the future sure you'll get coordination yeah that works that works i'm the
1281560	1285240	only guy with a gun and i got ten and i got a name that all ten of you and you can all die
1285240	1289800	or listen to me your choice george just quickly can you pump your volume just a tiny little bit
1289800	1300520	if you can sure um is that better okay uh do you mind if i jump in there yeah sure so i'm i'm curious
1300520	1305720	about uh so i understand what you're saying and i think you make some decent points but um i think
1305720	1310040	i view the world a bit differently from you and i'd like to like dig into that a little bit so like
1310600	1316040	who do you think is less afraid someone living just medium person living in the united states of
1316040	1323000	america or the medium person living in somalia sure america less afraid well that's kind of
1323000	1327720	strange somalia doesn't have a government they have much less tyranny you're much more you can just
1327720	1330760	buy a rocket launch room just like live on a farm and just like you know kill your neighbors and no
1330760	1336440	one's gonna stop you so like how does that interact with your old you those who will trade liberty
1336440	1344440	for safety deserve neither did i sorry i don't understand could you elaborate a bit more um in
1344440	1352360	somalia you have a chance in america you do not right i am okay i would rather live in fear i would
1352360	1357640	rather be worried about someone shooting a rocket launcher at me than to have an absolutely tyrannical
1357640	1365560	government just you know just just like like a managerial class and i'm not saying by the way
1365560	1370040	i agree with you that these things are possible i agree with you that the less wrong notion of
1370040	1375640	politics is possible i would love to live in these sort of worlds but we don't what would be the the
1375640	1381560	practical reality of politics is so much more brutal and it just comes from a straight up
1382280	1387560	instinct to dominate not an instinct uh you know government by the people for the people
1387560	1394200	is branding i mean yeah so to be clear i very much do not agree with less wrongs views and politics
1394200	1400760	and a bit of an outcast for how i view how conflict theory i view politics but this is i feel like
1400760	1404280	you're kind of dodging the question here just a little bit is like well if that's true why aren't
1404280	1412440	you living in somalia i know people who've done it right it's very hard it's very hard psychologically
1413080	1419480	okay so like tigers love chum it turns out right a tiger does not want to chase down an antelope
1419480	1424440	right a tiger would love to just sit in the zoo and eat the chum right and like it takes a very
1424440	1429640	strong tiger to reject that i'm not that strong i hope there's people out there who are i hope
1429640	1435640	there's people out there who are actually like you know i'm just not a little bitch that's why
1435640	1442360	uh in somalia right okay i mean that's a fair answer but i am a bit confused here so you're
1442360	1448360	saying that living in somalia would be better by some metric but you're also saying you prefer
1448360	1454600	not living in somalia so i am a bit confused because like from my perspective i want to live in
1454600	1458600	the country i want to live in and that's the one which i think is better if i thought another
1458600	1464200	country was better then i would just move there but first let's the tiger and the chum i think is
1464200	1469560	a good analogy right like if you have a choice as a tiger you can live in a zoo and you get a nice
1469560	1475080	sized pen you know the zookeepers are not abusive at all you get fed this beautiful chopped up food
1475080	1482360	it's super easy you sit there get fat lays around all day or you can go to the wild and in the wild
1482360	1487720	you're gonna have to hunt you might not succeed at hunting it is just a you know it's a brutal
1487720	1494280	existence as a tiger which one do you choose now you say oh well obviously you know you're going
1494280	1499640	to choose the chum one yeah but do you see what you're giving up do you know i don't could you
1499720	1506920	elaborate a little bit on what i'm giving up you are giving up on the nature of tiger you
1506920	1513240	you are effectively okay um maybe i'll take this to an extreme right in the absolute extreme
1513800	1518920	the country that you would most rather live in is the one that basically wire heads you right
1519480	1524120	the one and you can say that okay well i don't want to be wireheaded but you know there's a
1524120	1530520	there's a gradient that'll get you there gondi in the pill you know um look you can live in this
1530520	1535720	country you can be happy feel safe and secure all the time don't worry exactly about how we're doing
1535720	1543640	yet you know but right i mean it takes a very strong person to it's going to take a very strong
1543640	1550040	person to say no to wireheading so now i understand oh sorry i give i'll give one more instrumental
1550040	1554520	reason for living in america versus living in simalia if i thought that america and simalia were
1554520	1560520	both like steady states i might choose simalia i don't think that i think that being here i have a
1560520	1567160	much better way of escaping this of escaping the constant tyranny that we're in and i think a major
1567160	1575000	way to do it is ai i think that ai is is if i really if i had an agi if i had an agi in my
1575000	1578840	closet right now i'll tell you what i'd do with it i would have it build me a spaceship that could
1578840	1583480	get me off of this planet and get out of here as close to the speed of light as i possibly could
1583480	1587800	and put big shield up behind me blocking all communication that's what i would do if i had
1587800	1592280	an agi and i think that's you know the right move and i have a lot better chance of building that
1592280	1597640	spaceship right here than i do in simalia right so i'll give an if that's right that's a good
1597640	1602840	instrumental well we'll miss you if you leave though no you're real ashamed it'll be everyone
1602840	1608680	should do it like this is this is the move right and like let humanity blow i mean look i agree with
1608680	1615080	you that we're gonna probably blow ourselves up right but i think that the path potentially through
1615080	1621240	this probably looks different from the path you're imagining i think that the reasonable position
1621240	1626680	sorry oh no no um i think yeah maybe we're done with this point i can come back i have a response
1626680	1631800	to you first i i would like to if you don't mind just like pull on one one string there as well
1631800	1638040	so one of the things you said is like what will the tiger choose and so my personal view of this
1638040	1643640	kind of thing and i think i want to think about coordination is i think of things so you put a
1643640	1648360	lot of view and there's like fear-based domination and so on and i'm not going to deny that this
1648360	1655080	isn't a thing that happens i'm german you know like you know i have living relatives who can tell
1655080	1661000	you some stories like i understand like i i understand i'm not i'm not denying these things by
1661240	1668680	means um what i'm saying though is okay let's say there was a bunch of tigers you know you and me
1668680	1674520	and all the other tigers and some of the tigers are like man fuck this whole like nature shit is
1674520	1680600	like really not working for me how about we go build a zoo together who's in and then other people
1680600	1684440	like yeah you know what actually that sounds awesome let's do that do you think that's okay
1684440	1688840	like you think that would be like a fair option for them to do sure but that's not where zoos come
1688840	1693800	from i i know i know i'm getting there i'm getting there so like that is not where zoos come from
1693800	1702120	sure but the this analogy here is of course is that this is where a lot of human civilization
1702120	1706600	not all of it i understand that why france was doing well in the first world war was not because
1706600	1712760	of democracy big nice it was because democracy raises large armies it's um i'm very well aware
1712760	1719160	of the real politic as the Germans would say about these kinds of factors and i and i fully
1719160	1724840	agree with you that a lot of the good things that we have are not by design so to speak you know
1724840	1731400	there are happy side effects you know capitalism is a credit assignment mechanism you know the fact
1731400	1736440	that also results in us having cool video games and air conditioning it's not an inherent feature
1736440	1745160	of the system it's it's an execution mechanism and so totally grant all of this i'm not saying that
1745160	1750360	every coordination thing is good i'm not saying that you know there aren't trade-offs especially
1750360	1753800	you were talking about i think aesthetic trade-offs you're like there's an aesthetic that the tiger
1753800	1758600	loses they're living in a zoo and well i think personally aesthetics are subjective so i think
1758600	1762200	this is something that different people so the way i think about aesthetics is i think aesthetics
1762200	1769000	are things you trade on is you know you might want tigers in the wild to exist okay fair enough
1769000	1774280	that's a thing you can want you know someone else might want you know certain kinds of art to exist
1774280	1779960	they might want um a certain kind of religion to be practiced or whatever these are aesthetic
1779960	1784920	preferences upon reality which i think are very fair so the way i personally think about this
1784920	1790120	morally is i'm like okay cool how can we maximize trade surplus so you can spend your resources
1790120	1795640	on the aesthetics you have you want and i'll spend my resources on the uh you know things i want
1796440	1801320	now maybe the thing you describe where everyone just atomizes into their own systems with their
1801320	1806600	own value system with their own aesthetics completely separate further is the best outcome possible
1806600	1812600	i think this is probably sorry have you heard the you know about my manifesto i have not you
1812600	1819080	should um the problem with this everyone trades on their own aesthetics is you will never be able
1819080	1825240	to actually buy any aesthetics that are in conflict with the system right the you won't
1825800	1832040	the system will let you okay uh by by that logic why do people have free time
1833400	1837160	why don't they work all the time why doesn't capitalism extract literally every minute of them
1837160	1843320	why do you think that is i think it's because it turns out that we don't actually live in a
1843320	1848600	capitalist society i think china is a lot closer to a capitalist society than america i think america
1848600	1852920	is kind of communist and i think in a communist society of course you're going to get free time
1852920	1857480	it turns out that subsidizing all the homeless people is a great idea right if you want to keep
1857480	1863160	power again through some absolute tyrannical mechanism you do it right so why do we have free
1863160	1868520	time well you think it's some victory of capitalism i think it's because we do not live in a capitalist
1868520	1872920	country i think china is more capitalist than america i think it's because we trade on our aesthetics
1872920	1876280	i think that different people have different things to contribute to various systems not
1876280	1881160	necessarily capitalist or communist thing i'm saying it's it's more energy is that in the in
1881160	1884760	the primordial environment if you have to fight literally every single second instrument every
1884760	1889400	jewel of energy you have to scrounge together another jewel of energy you can't have free time
1889400	1894520	it's not about capitalism this is about entropy this is about these kind of things we have energy
1894520	1900280	excess we have we've produced systems that allow us to extract more energy for a jewel we put in
1900360	1907240	and we can spend that extra energy on things such as free time and the distribution of you know
1907240	1911800	energy power coordination whatever you want to call it is a is another question will you agree
1911800	1916760	to disagree with this i mean i am taking an extreme position when i say that there are definitely
1916760	1922200	positive sum coordination problems that are solved by governments right it is not all zero
1922440	1929960	or negative sum right i'm not i'm not denying this but what i'm saying is it's like i don't know
1929960	1936200	man like the existence of free time well that's all great when you think you live in this surplus
1936200	1941960	energy world right and maybe we do right now but if some other country took this seriously like china
1943160	1947800	who's gonna win in a war who's gonna win is it gonna be the chinese you see the chinese
1947800	1953400	build a building they got like 400 people there and they're all there 24 hours a day and they're
1953400	1957320	getting the building built you ever see americans build a building it's six guys two of them are
1957320	1961880	working two of them are shift supervisors and two of them are on lunch breaks oh you got your free
1961880	1966200	time you got your aesthetic preferences you know you deserve to lose in a war right this country
1966200	1970680	deserves to lose in a war if they keep acting the way they're acting so i i definitely see the
1970680	1974920	point you're making and this is personally not a thing i want to defend too far because i'm not a
1974920	1981560	military expert but i will know that i will note that the us has like 37 aircraft carriers and the
1981560	1987880	chinese have like two and americans are like somehow you know despite being so lazy and oh no they have
1987880	1992680	all this you know all this free time or whatever somehow they're still military hegemon or whatever
1992680	1997880	and like they're the biggest rival russia fighting this backwards water country in ukraine suddenly
1997880	2004280	folds and lose like two quarters of the military it's what i'm saying is if you have massive hegemony
2004360	2010120	if you have truly a obnoxious victory the way it should look is that you laze around all the
2010120	2016040	time and you look like a fucking idiot and you still win yes and i'm not talking about russia
2016040	2021720	russia has a gdp the size of italy this is china here you might say that china has two aircraft
2021720	2027800	carriers in the us has 37 why do we have aircraft carriers who has more drone building capacity the
2027800	2033880	chinese or the united states if the future is fought with ai swarm drone warfare the chinese
2033880	2039480	can make you know a million drones a day and the us can make i don't even know i think we buy them from
2039480	2046120	china well i'm not an expert on these kind of logistics i think i would like to get back to
2046120	2050440	kind of like the more general point let's move on from that yeah i am not either but i do believe
2050440	2055000	that the chinese have more manufacturing capacity than the united states it seems completely plausible
2055000	2060920	to me i think things are not lazy and they don't sit around and have all this free time and aesthetic
2060920	2066280	references or something i don't believe it at work is life i mean at least from my chinese friends
2066280	2072040	i know the chinese sure do have a lot of inefficiencies it's just called corruption oh america has
2072040	2077960	corruption too oh yeah sure well in mexico the corruption is you have to pay 20 cents to get
2077960	2083000	you know 20 cents on every dollar for the building you built right whatever man in america you every
2083000	2087320	dollar is spent absolutely on that building you know how we know that because we spent four dollars
2087320	2092680	making sure that that first dollar was not spent correctly uh i'm i'm i'm well aware of that so
2092680	2096600	anyways i would like to like i think i think we mostly agree on this point actually and i think
2096600	2102840	it's a matter of degree um i i what i want to say just for the record the us is a like uniquely
2102840	2108200	dysfunctional system in the west i'm german and like the german system is very dysfunctional
2108200	2113000	but it's like nothing compared to how dysfunctional the us is fully agreed with that i don't i don't
2113000	2118840	think we disagree on that i think it's a matter of degree more so we've had a we've had a comment
2118840	2123000	saying someone's turned the temperature up a bit too much on the language model so let's bring it
2123000	2127720	back a tiny bit to ai safety but that was a that was a great discussion got it yeah i will end
2127720	2131640	with saying i love america i am happy to live here and there are a lot of things i appreciate about
2131640	2138840	american societies great so do you want to return to like the technical topics or would you i think
2138840	2142440	i can return to your first point and maybe i'll just start with a question uh do you think there's
2142440	2151000	going to be a hard takeoff i don't know but i can't rule it out i can't see how that would possibly
2151000	2158360	happen i have a few ideas of how it could happen but i don't it's like unlikely it seems like not
2158360	2163720	the the way i think it could happen is if there are just algorithms which are like
2165160	2169320	magnitudes of order better than anything we ever have and like the actual amount of computing to
2169320	2175800	get human is like you know a cell phone or you know like and then this algorithm is not deep in the
2175800	2180760	tech tree we just happen to have not picked it up and then an agi system picks it up this is how
2180760	2186280	i think it could happen okay yes i agree that something like this is potentially plausible
2186280	2192440	where you're saying basically like the godshatter is already distributed the uh the the it's it's
2192440	2197240	not a question it's using all the existing compute in the world today it just turns out it was 10 000x
2197240	2201960	more effective or a million x more effective than we thought yeah this is seems the most plausible
2201960	2206840	way to be or you know you mix lead and you know copper and you get a superconductor you know something
2206840	2212440	like that i don't know some crazy i know i know i'm i'm joking it's going to take so many years to
2212440	2217320	like it's not about the discovery right give it 10 years to productionize it scale up processes
2217320	2221480	right like these things are you know this is something running a company's really taught me like
2221480	2228360	it's just gonna take a long time and this is really like like kind of where my i just don't
2228360	2233560	believe in a hard takeoff i think that they'll be this is a gasky thing i like he's a hardware and
2233560	2237640	software progressive quite similar speeds and you can look at factoring algorithms to show this
2237640	2243000	so it would shock me if there were some you know 10 to the 6 10 to the 9 magical improvement to be
2243000	2249720	had it seems plausible to be like a hard takeoff is definitely not my main line scenario my main
2249800	2254280	line scenario well i don't know maybe you wouldn't consider this a part maybe you would
2254280	2257400	consider as a hard takeoff this is what i would describe as a soft takeoff is something like
2258120	2264120	sometimes the way i like to define agi is say it's uh something that has the thing that chimp
2264120	2270760	that chimps don't have and humans do have yeah so chimps don't go a third to the move you know
2270760	2275640	despite their brain being a third of our size so we scaled up things by a factor of three if a
2275640	2280040	primate brain roughly four something like that and like most of the structures i'm sure some
2280040	2284600	micro tweaks and whatever but like not massive amount of evolutionary pressure like we're very
2284600	2291560	very similar to chimps and somehow this got us from you know literally no technology to
2291560	2297800	space travel in a you know evolutionary very small pair of time it seems imaginable to me
2297800	2304280	that something similar could happen with ai i'm not saying it will but like it seems imaginable
2304360	2310920	yes so i agree with this um i'll come to your point about uh you know you had two regulatory
2310920	2317960	points one of them about uh capping the max flops and i actually kind of agree with this
2317960	2324440	i do think that things could potentially become very dangerous at some point i think your numbers
2324440	2330840	are way way way too low i think if your numbers are anywhere in your gpk3 gpk4 okay great we got
2330840	2335800	a lot up we got a lot of fast moving guys who work on fiber even if you start to get von neumann's
2336440	2342680	right we're not talking about a humanities worth of compute we're talking about things on par with
2342680	2349080	a human and a few humans right yeah they'll run fast but they're not like things get scary when you
2349080	2354600	could do a humanities training run in 24 hours like we're about to burn the same compute that
2354920	2361320	all 2 million years of human civilization burned okay now i don't know what starts to happen or
2361320	2366520	i'll put this kind of another way language models i look at them and they don't scare me at all
2366520	2373320	because they're trained on human training data right these things are not like if something was a
2373320	2380120	good as as good as gpk4 that looked like mu0 where it trained from some simple rules okay now i'm a
2380120	2384760	bit more scared but when you say okay it's you know we're feeding the whole internet into the
2384760	2388280	thing and it parents the internet back mushed around a little bit that looks very much like
2388280	2393800	what a human does and i'm just not scared of that yeah i think it's very reasonable whatever like i'm
2393800	2399480	not scared of gpk4 to be clear like i think there is like zero percent chance or like you know epsilon
2399480	2406680	chance that gpk4 is existentially dangerous by itself you know maybe some crazy gpk4 plus rl
2406680	2412280	plus mu0 plus something something maybe but i definitely agree with you here i don't expect
2412280	2416120	gpk3 or 4 by themselves to be dangerous these are not i'm much closer to i think what you're
2416120	2422040	saying like yeah if you had a mu0 system they'd bootstrap yourself gpk4 holy shit like we're
2422040	2426360	big we're big big shit if we get to that then we should let's stop let's stop yeah let's let's stop
2426360	2432120	so so i'm very happy to get to be into a regime where we're like okay let's find the right bound
2432120	2435640	like i think this is an actually good argument i think this is actually something that should
2435640	2439880	be discussed and which is not obvious and i could be super wrong about that so i'd like to
2439880	2444760	justify a little bit about why i put such a small bound but i think the arguments you're
2444760	2448280	making for the higher bounds are very reasonable actually i think these are actually good arguments
2448280	2454040	so just to justify a little bit about why i put such a low bound the boring default answer is
2454040	2458760	conservatism is like if all of humanity is at stake which you know you may not believe
2458760	2465400	i'm like whoa whoa okay at least give us a few years to like more understand what we're dealing
2465400	2471480	with here like i understand that you know you may disagree with this very plausible but i'm like
2472040	2477000	whoa like you know at least let's let's like by default let's hit a pause button for like you
2477000	2481400	know a couple years until we figure things out more and then if we like find a better theory of
2481400	2486440	scaling we understand how intelligent scales we understand how mu0 comes blah blah blah and then
2486440	2491000	we pick back up after we're like you know we make huge breakthroughs in alignment and
2491000	2497080	eliezer is is crying on cnn and like oh we did it boys i mean then okay sure you know okay um
2498200	2502280	so that's the one like kind of more boring argument like that's kind of a boring argument
2503080	2508040	the more interesting argument i think which i think is a bit you know or schizo um is that
2509480	2513640	it's not clear to me that you can't get dangerous levels of intelligence with the
2513640	2518120	amount of compute we have now and one of the reasons that i'm i'm unsure about this is because
2518120	2523960	man gpd 3 gp4 is just the dumbest possible way to build ai like it's just like like there's like
2523960	2529560	no dumber way to do it like it's it works and dumb is good right you know bitter lesson dumb is good
2529560	2536280	but look at humans you said as we talked about before you know human today human 10 000 years
2536280	2543480	ago not that different you place both of them into a you know workshop with tools to build you know
2543480	2548600	any weapon of their choice which of them is more dangerous obviously you know one of them
2548600	2556680	will have much better you know capacities to deal with tools to read books to think about
2556680	2563080	how to design new weaponry and so on these are not genetic changes they are epistemological
2563080	2568440	changes they are memetic they are software updates you know humans had to discover rational
2568440	2572520	reasoning like you know before like you know i mean no obviously people always had like you
2572520	2578920	know folk conceptions of rationality but it wasn't like a common thing to think about causality and
2578920	2584920	like you know you know rational like you know if then else kind of stuff until relative you know
2584920	2590520	like philosophers in the old ages and only became widespread relatively recently and these are useful
2590520	2595640	capabilities that turned out to be very powerful and took humans many many thousands of years to
2595640	2600040	develop and distribute at scale and i don't think humans are anywhere near the level i think the way
2600040	2604680	we could do science right now is pretty awful like it's like the dumbest way to do science that like
2604680	2612520	kind of still works like you know and i expect it's like possible but if you had a system which like
2612520	2618600	let's say it's like smaller brain than the human even but it has really really sophisticated
2618600	2624360	epistemology it has really really sophisticated theories of meta science and it never tires it
2624360	2629960	never gets bored it never gets upset it never gets distracted and it can like memorize arbitrary
2629960	2635400	amounts of data this is something that i think is within the realm of like a gbt3 or four training
2635400	2640920	run to build something like this and it is not obvious to me that this system could not outflank
2640920	2647000	humanity maybe not like maybe not but it's not obvious to me that it can't so just curious what
2647000	2653560	you think of that um so to your first point uh why i stand against almost all conservative
2653560	2658920	arguments you're assuming the baseline is no risk right and oh well why should we do this
2658920	2665720	say i we should wait and bring the baseline back no no no no we are about to blow the world up any
2665720	2671880	minute there's enough nuclear weapons aimed at everything this is wearing some incredibly unstable
2671880	2676520	precarious position right now like people talk about this with with car accidents you know this is
2676520	2681800	comma like people are like oh well you know if your device causes even one accident i'm like
2681800	2686440	yeah but what if statistically there would have been five without the device i'm like you
2686440	2691400	you do have to understand the baseline risk in cars is super high you make it 5x safer there's
2691400	2695800	one accident you don't like that okay i mean you have to be excluded from any polite conversation
2695800	2704200	right um right so yeah like i i think that calling for a pause to the technology is is uh
2705400	2709720	worse right i think given the two options if we should pause or we should not pause
2709720	2714200	i think pausing actually prevent presents more risk and i can talk about some reasons why
2714200	2719000	again the things that i'm worried about are not quite the existential risks i have to the
2719000	2728360	species are not agi goes rogue they are uh government gets control of agi and ends up in
2728360	2734040	some really bad place where nobody can compete with them i don't think these things look unhuman
2734040	2738680	these things to me like i see very little distinction between human intelligence and
2738680	2746920	machine intelligence it's all just on a spectrum and like they're not um like to come to the point
2746920	2753000	about okay but gpt4 could be like this hyper-rational never tiring humans are doing science in the
2753000	2759880	dumbest way i'm not sure about that right like i i think that you know when you look at like okay
2759880	2764040	well okay we have chess bots that do way better and all they do is think about chess we haven't
2764040	2767720	really done this with humans people would call it unethical right like if we really
2768280	2772920	told a kid like if we really just like every night we're just putting the chess goggles on you
2772920	2776840	and you're staring at chess boards and we're really just training your neural net to play chess
2778200	2783400	i think humans could actually beat a computer again a chess if we were willing to do that um
2784280	2789160	so yeah i don't think that this stuff is that particularly dumb and i think okay maybe we're
2789160	2795880	losing 10x but we're not losing a million x again i i don't see a i do the numbers out
2795880	2801160	all the time for when we're going to start to get more computer you know when will a computer
2801160	2805960	have more compute than a human when will a computer uh have more compute than humanity
2805960	2811000	and yes these things get scary but we're nowhere near scary yet we're looking at these cute little
2811000	2820200	things and these things by the way do present huge dangers to society right the the psyops that are
2820200	2826760	coming right now you assume that like when you call somebody that you're at least wasting their time
2826760	2833080	too but we're going to get like heaven banning i love this concept which is yeah yeah came up on
2833080	2836680	luther a i like that's where it comes from was the guy on the luther a i that came up with that
2836680	2843960	word yeah yeah i know the guy came over this i love i love this concept and i think uh there's
2843960	2850520	also a story i did uh my little pony friendship is optimal oh god that goes into the concept and
2851640	2860120	yeah so i think that like um my girlfriend proposed a uh i don't want to talk to oh i
2861000	2867640	say you don't want to talk to your relative anymore right okay we'll give an ai version to talk to you
2867640	2874280	right yeah yeah yeah yeah so like this stuff is coming and it's coming soon and if you try to
2874280	2879720	centralize this if you try to you know say like oh okay google open ai great they're not aligned with
2879720	2883240	you they're really not google has proven time and time again they're not aligned with you
2883240	2886120	meta has proven time i can't time again they're trying to fix it but you know
2886840	2892120	yeah i mean i i fully agree with you like uh i like that you bring up psyops as the correct example
2892120	2897400	in my opinion of short-term risks i think you're like fully correct about this like when i first
2897400	2904840	saw like gpt models i was like holy shit like the level of control i can gain over social reality
2904840	2909960	using these tools at scale is insane and i'm surprised that we haven't seen yet the things
2910680	2917160	that like augured in my visions of the day and we will like we will obviously it's coming and
2917960	2924040	this is so i think this is a very very real problem yeah like i think if we even if we stop now
2924040	2929560	we're not out of the forest so like so um when you say like i i think the risk is zero please
2929560	2934120	do not believe that that is what i believe because it is truly not it is truly truly not i think we
2934120	2940600	are like we are really in a bad situation we are in a we're being we're an under attack from like
2940600	2945320	so many angles right now you know this is before we get into you know like you know potential like
2945320	2951560	you know climate risks nuclear risk whatever we're in under room medic risk like the the then dangers
2951560	2957960	of our like epistemic foundations are under attack and this is something we can't adapt to right
2957960	2964440	like you know we did you know when a good friend of mine he's uh he's quite well read on like
2964440	2968600	chinese history and he always like it tells me his great stories so i'm not a historian so please
2968600	2973080	you know don't crucify me here but like he tells his great stories about when marxist memes were
2973080	2977240	first introduced to china and like this is where a world were like just like all the precursor
2977240	2981960	memes didn't exist that's just like kind of was air dropped in and people went nuts people went
2981960	2986840	just completely crazy because there was no memetic antibodies to these like hyper virulent
2986920	2991080	memes that were you know created by evolutionary pressures in like you know western university
2991080	2995960	departments like really you can call philosophy department just gain a function memetic laboratories
2998200	3003960	yeah that's what they are i mean like you know like without being you know political or any means
3003960	3008200	there a lot of what these organizations do and like you know other you know what other you know
3008200	3013960	memetic like you know if philosophy departments are the like gain a function laboratories then
3014040	3020200	like for chan and tumblr are like the bat caves of means you know like the chinese bat caves and
3020200	3025080	i remember this vividly i was like on tumblr and for and for chan like when i was a teenager
3025080	3029960	and then suddenly all the like weird bizarre you know internet shit i saw started becoming
3029960	3035880	mainstream news my parents were watching in 2016 and i was like what the hell is going on
3035880	3040680	like i already developed antibodies to this shit like i already you know both right and left i was
3040680	3046040	already like i already immunized all this so i fully agree with you that this is like one of the
3046040	3051560	largest risks that we are facing is this kind of like memetic mutation load in a sense and
3052280	3057560	i'm not going to say i have a solution to this problem i'm like i have ideas like there's a lot
3057560	3062280	of like things you can do to improve upon this like if ai was not a risk and also not climate
3062280	3066840	change and whatever this might be something i work on like epistemic security this might be
3066840	3071160	something i would work on like how can we build better coordination like rash like just scalable
3071160	3076360	rationality mechanisms stuff like prediction markets and stuff like this i don't know but sorry
3076360	3082280	going off track here a little bit but well no i actually i really agree with a lot of the stuff
3082280	3086520	you said and i had a similar experience with the antibodies and people are exposed to this stuff and
3086520	3096040	i'm like yeah this got me like four years ago yeah um so i think that there is a solution and i
3096040	3102760	have a solution and the answer is open source ai the answer is open source let's even you can even
3102760	3106760	dial it back from like the political and the terrible and just straight up talk about ads and spam
3106760	3111880	or maybe spam just straight up spam i get so much spam right now and it's like it's kind of written
3111880	3117320	by a person it's like targeting me to do something and google spam filter can't even come close to
3117320	3124440	recognize you right like what i need is a smart ai that's watching out for me that is just it's not
3124440	3132120	even targeted attacks at me it's just so much noise and i don't see a way to prevent this like
3132120	3136680	the big organizations they're just going to feed you their noise right and they're going to maximally
3136680	3142440	feed you their noise the only way is if you have an ai like i don't think alignment is a hard problem
3142440	3146520	i think if you own the computer and you run the software if you develop the software the ai is
3146520	3154280	aligned with you oh yeah can you can you okay if i challenge you george hotz here is a lama
3154280	3159560	65b model when we could appear to run it on make it so it and make yeah you know sure you okay you
3159560	3164920	developed it i give you the funding at your time can you develop a model that is as good as lama
3164920	3172360	64b 65b and it's immune like completely immune to jail breaks it cannot be jailbroken no why not
3172360	3178360	it's aligned isn't it well no but this isn't what alignment means well my values is do not get
3178360	3184120	jailbroken oh okay you're talking about unexploitability this is not alignment right oh okay okay interesting
3184120	3188920	i didn't know you would separate those so extremely separate those right okay interesting it means in
3188920	3197320	the default case it like like it it's on my side right okay unexploitability is not a question of
3197320	3201960	whether it's okay and this is a true thing about people too whenever i look at a person i ask
3202360	3208520	okay is this person i want something for me is this person does this person want it too and
3209240	3213800	is this person capable of doing it right and i really separate those two things i can build a
3213800	3217480	system i don't i'm not worried about the first one with the ai system is worried about the second
3217480	3223400	one can it be gamed can it be exploited sure i could tell like you know like like say it was just
3223400	3229240	playing chess right and it loses i'm like don't lose okay i didn't want to man i didn't want to lose
3229240	3235480	i'm sorry yeah i know but like so yes yes can i build a aligned system sure can i build an
3235480	3241000	unexploitable system no especially not by a more powerful intelligence interesting interesting so
3241000	3244600	this is an interesting i i think you're you're you're pointing to actually a very important
3245240	3249480	part of this is that like exploitability and alignment can get fuzzy like which is which
3249480	3253720	like did it fail because of its skill set or because it's not aligned it's actually a very
3253720	3257240	deep question so i think i think you make a good point for like you know talking about these two
3257240	3265960	separately i guess um the um so the thing i want to dig in just like a a little bit more on on this
3265960	3273960	idea is there are there's two ways there are two portals through which you know the memetic demons
3273960	3279640	can reach into reality humans and computers why do you think your ai is immune to memes
3280440	3286200	why why can't i just build ai's that target your ai's what like you don't i don't think my ai is
3286200	3292200	immune to memes at all i think that the only question is and i really like your game like these
3292200	3302680	these ngos are doing gain of function on memes right where am i um the like a a weaker intelligence
3302680	3308040	will never be able to stand up to a stronger intelligence so from this perspective if this
3308040	3314040	is what's known as alignment i just don't believe that this is possible right you can't you can't
3314040	3318600	keep a stronger intelligence in the box this is this is look i i agree with you kowsky in the
3318600	3324120	box experiment it's like the ai's always going to get out there's no keeping it in the box right
3324120	3330280	this is this is a complete impossibility i think there's only two real ways to go forward and one
3330280	3337960	is tekkezinski one is technology's bad oh my god uh blow it all up let's go live in the woods right
3337960	3342440	and i think this is a philosophically okay position i think the other philosophically okay
3342440	3347720	position is something more like effective accelerationism which is look these ai's are
3347720	3355240	going to be super powerful now if you have one it could be bad but if super intelligent ai's are all
3355240	3359880	competing against each other memetically like we have something like society today just the
3359880	3365400	general power levels have gone up this is fine as long as these things are sufficiently distributed
3365400	3370680	right like sure this ai is not perfectly aligned but you know there's a thousand other ones and
3370680	3374440	like you have to assume they're all basically good because they're all basically bad while we're
3374440	3381080	dead anyway i mean why wouldn't you expect that that they're all bad yeah well or what do you
3381080	3386600	think of humans are most humans yep i think the concept of good like doesn't really apply to humans
3386600	3391640	because humans are too inconsistent to be good like by default they can be good in various scenarios
3391640	3396120	in various social contexts but give me any human and i can put them into a context where they will
3396120	3402040	do an arbitrarily bad thing and this is true about a llama as well right llamas are completely
3402040	3406200	inconsistent i think they're actually more inconsistent than humans right yep and i wouldn't
3406200	3410920	trust llamas to be good well yeah but i wouldn't think that they're bad either i would think they
3410920	3415560	have the exact same inconsistency problem as humans and i think almost any any ai you build is going
3415560	3421720	to run into these same problems right yeah i think so that's my point so your your assumption can't
3421720	3426680	rely on them being good because you don't get that for free like where does that come from
3426680	3431560	my assumption is not that they're good my assumption is that they're not bad but inconsistent is fine
3431560	3435560	as long as we have a ton of them and they're all inconsistent and they're pulling society in every
3435560	3441560	which direction you don't end up paperclip right why not well because what they're all going to
3441560	3447160	coordinate and agree to paperclip you no no they'll just do some random bullshit and then that random
3447160	3451000	bullshit will not include humans they're all doing random bullshit right you're gonna have
3451000	3455000	let's say the liberals decide we're gonna paperclip people the conservatives are gonna come out very
3455000	3460440	strongly against paper clipping right like and you're just you're just gonna end up with these
3460440	3465160	sort of dynamics like forever you're gonna have some ai's who are like yeah we don't like it you know
3467000	3473960	so this is perplexing to me because like in my view which i think i heard you agree with earlier
3473960	3478920	it's like the world is unstable the world is very unstable and it seems to me you're suggesting if
3478920	3485080	we increase the chaos if we have more entities fighting doing more different things with more
3485080	3490200	energy that makes it more stable is that correct yes um the world has actually become very stable
3490200	3495240	in the last hundred years and i'm scared of how stable it's become uh you know again thank you for
3495240	3502440	china how many sovereign countries are there in the world i mean you know 190 something officially
3502520	3509480	un number if you believe them i think there's the us and china and maybe russia mostly they
3509480	3515800	just have a veto button um right so i think there's more like two and a half like this isn't
3515800	3521160	a lot of entities fighting it out chaotically a lot of entities fighting it out chaotically would be
3521160	3526040	intelligence is spread across the entire galaxy right and that's the other that's the other beautiful
3526040	3531640	thing too sooner we get off this planet the sooner we get things that are actually a light year away
3531640	3534680	well i think the speed of light's real it's going to take you a year to send them there
3534680	3541800	your your scary memes right do i get away from the memes there's only one way that's uh yeah for
3541800	3546360	for the record i would like to go on the record and say if any future transhumanists or whatever
3546360	3550600	want to blast himself into space and go do their own thing i support their their right to do that
3550600	3555560	and i would love to give this right to people the number one thing i want from countries is the
3555560	3559720	ability the the right to leave this is what i would love this is what i love about companies
3559720	3564600	so you know you can have a you're talking a neoreaction top yeah free exit is extremely
3564600	3570840	important i would not describe myself as neoreactionary please because i'm not that gay but um and
3571800	3578200	i wouldn't describe myself that way either but i've heard a lot of ideas but yeah that being said um
3578760	3583960	i do i do think that like you know what i want like i think let's let's ground the conversation
3583960	3588600	like a little bit here it's like i think i love like i'm very enjoying this conversation i love
3588600	3591560	talking to these philosophical points i think these are really good points really interesting
3591560	3596520	but ultimately you know as we also get to like the you know latter third of this conversation um
3597080	3601560	the thing i really care about is strategy okay the thing i really care about is reality politic
3601560	3608120	i really care about okay what action can i take to get to the features i like and you know i i'm not
3608120	3612360	you know gonna be one of those galaxy brain fucking utilitarians like well actually this is
3612360	3616920	the common good i'm like no no this is what i look i like my family i like humans you know look
3618600	3624280	yeah it's just what it is right like i'm not going to justify this on some global beauty whatever
3624280	3629320	doesn't matter so i want to live in a world i want to i want to in 20 years time 50 years time i want
3629320	3634280	to be in a world where you know my friends aren't dead and like where i'm not dead you know maybe we
3634280	3639160	are like you know cyborgs or something but i don't want to be dead so what i really care about
3639160	3642920	ultimately is how do i get this world and i want us all to not be suffering right like you know
3642920	3647960	i don't want to be in war i want us to be like in a good outcome so i think we agree that we would
3648200	3652440	like a world like this and we think we probably disagree about how best to get there and i'd
3652440	3657800	like to talk a little bit about like what can we what what should we do and like why do we disagree
3657800	3662520	about what we do is that sounds good to you well maybe i'll first propose a world that meets your
3662520	3668200	requirements um and you can tell me if you want to live in it uh so here's a world uh we've just
3668200	3673800	implanted electrodes in everyone's brain and maximize their reward function i would hate
3673800	3677800	living in a world like that yeah but no one it meets your requirements right your friends are not
3677800	3684200	dead i mean suffering and we're not at war that is true there are more criteria than just that
3684840	3689480	the true the criteria i said is things i like as i said i'm not a utilitarian i don't particularly
3689480	3694680	care about minimizing suffering or maximizing utility what i care about is this various vague
3694680	3699800	aesthetic preferences over reality i'm not pretending this is i thought that was the whole
3699880	3705960	spiel i was trying to make is that i'm not saying i have a true global function to maximize i say i've
3705960	3711080	various aesthetics i have various meta preferences of those aesthetics i'm not asking for a global
3711080	3716120	one i'm asking for a personal one i'm asking for a personal one that you i don't care about the
3716120	3721240	rest of the world i gave you mine i gave you what i would do if i had an agi yeah so i'm getting
3721240	3727000	off this rock speed of life as fast as i can fair enough i i think if that is the war i would like
3727000	3732040	to live in a world where you could do that this would be a feature of my world if a world where
3732040	3737800	i would be happy is a world in which we coordinated around you know at scale at larger scales around
3737800	3745240	building a lined agi that could then distribute you know intelligence and matter and energy in a
3745240	3751560	you know well hat value handshaked way between various people who may want to coordinate with
3751560	3756200	each other may not you know some people might want to form groups that have shared values and
3756200	3760760	share resources others may not i would like to live in a world where that is possible have
3760760	3767640	you read metamorphosis supreme intellect i have unfortunately not um i yeah i was gonna ask if
3767640	3774440	you're happy with that world right unfortunately don't know it i i mean yeah it's as simple to
3774440	3781800	describe singleton ai that basically gives humans whatever they want like maximally libertarian you
3781800	3788600	know uh you can do anything you want besides harm others is that a good work probably i don't know
3788600	3792040	i i haven't read the book i assume the book has some dark twist about why this is actually a bad
3792040	3797720	world not really not really i mean the plot is pretty obvious you are the tiger eating chung
3797720	3802760	right sure but you can then just decide if that is what you want then you can just return to the
3802760	3807960	wilderness that's the whole point if it can you can you really return to the wilderness right like
3808840	3812840	you think that like i don't think we have free will i don't think you ever will return to the
3812840	3819320	wilderness i think a large majority of humanity is going to end up wireheading and yeah i expect
3819320	3823400	that too okay great and this is the best possible outcome by the way this is giving humans exactly
3823400	3829800	what they want yep yeah i will to be to be clear i don't expect it's all humans i truly do not i
3829800	3835320	don't want it's all i think a lot of humans have metapreferences over reality they have preferences
3835320	3838840	that are not their own sensory experiences this is a thing a thing that the utilitarians get very
3838840	3844120	wrong is a lot of is that many human preferences are not about their own not even they're not even
3844120	3849320	about their own sensory inputs they're not even about the universe they're about the trajectory
3849320	3855560	of the universe they're about forward if for the utilitarianism you know and a lot of people
3855560	3863000	want struggle to exist for example they want heroism to exist or whatever i would like those
3863000	3867240	values to be satisfied to the largest degree possible of course am i going to say i know
3867240	3872760	how to do that no which is why i kind of like didn't want to go this deep because i think
3873560	3881480	if we're arguing about oh do we give them you know forward utilitarianism versus libertarian
3881480	3886840	utopia versus whatever i mean we're already like 10 000 steps deep i'm asking about you
3886840	3892120	i'm not asking about them i'm asking about a world you want to live in and this is a really
3892120	3897240	hard problem right yeah and this is why i just fundamentally do not believe in the existence
3897240	3905240	of ai alignment at all there is no there is no like like what values are we aligning into
3905240	3912200	whatever the human says or what they mean or like sure sure but like my point is i feel
3912200	3916680	we have wandered into the philosophy department instead of the politics department okay like
3916680	3921800	it's like i agree with you like do human values exist what does exist to me but like
3921800	3925160	by the point you get to the point where you're asking what does exist to me you've gone too far
3925160	3930600	sure like i'll respond concretely to the two political proposals i heard you stayed on bankless
3931400	3938760	sure i'd love to talk about them one is limiting the total number of flops temporarily temporarily
3938760	3942920	yes and what i i have a proposal for that but i don't want to set a number i want to set it as a
3942920	3948920	percent i do not want anybody to be able to do a 51 percent attack on compute if one organization
3948920	3955080	acquires 50 it's the straight up 51 percent attacks on crypto if one organization acquires 51 percent
3955080	3960440	of the compute in the world this is a problem maybe we'll even cap it at something like 20
3960440	3966680	you know you can't have more than 20 right um yeah i would support regulation like this uh
3966680	3971160	i would i don't think that this would cripple a country um but we do not want one entity or
3971160	3976200	especially one training run to start using a large percentage of the world's compute not a
3976200	3982440	total number of flops i mean absolutely not like that'd be terrible i would actually support that
3982440	3988200	regulation like no no sorry sam altman you cannot 51 attack the world's compute sorry it's illegal
3989160	3994120	that's fair enough i think this is a sensible way to think about things assuming that uh
3994120	3998200	software is fungible is that everyone at access the same kind of software and that you have an
3998200	4004920	offense defense balance so in my personal model of this i think well a some actors have very strong
4004920	4010280	advantage of some software um which can be very very large as someone who's trained very very
4010280	4014360	large models and knows a lot of the secret tricks that goes into them a lot of the stuff in the
4014360	4021640	open sources for my good we should force it to be open source well this is your this is actually
4021640	4025640	very legitimate consequence for it to set and now i'll say the second point about why i think
4025640	4031400	that it doesn't work so the next reason why i think doesn't work is that there is a there are
4031400	4036840	constant factors at play here is that the world is unstable we're talking about this i think the
4036840	4045400	amount of compute you need to break the world currently is below the amount of compute that
4045400	4051800	more than 100 actors have access to if they have the right software and if you give if you have
4051880	4056200	let's say you have this insight right that could be used not saying it will be but it could be
4056200	4060040	used to break the world to like cause world war three or you know or just like you know
4061400	4068200	cause mass extinction or whatever if it's misused right let's say you give this to you and me
4069320	4074600	do you expect we're going to kill everybody like would you do that or would you be like uh hey let's
4074600	4078360	a kind of let's like not kill the world right now and i'll be like sure let's not kill the world
4078360	4083800	how are we killing the world how did we go from i don't even understand like okay how exactly does
4083800	4088520	the world get killed this this is a big leap for me all right sorry i agree with you about the
4088520	4093720	sci-op stuff i agree with you about sorry sorry let let me you're right i made too big of a
4093720	4099880	big there you're completely correct sorry about that so to back up a little bit let's assume we
4099880	4107560	you and me have access to something that can train you know at mu zero you know super gpt7 system on
4107560	4113560	a tiny box you know cool problem is we do a test run it with it and we have it immediately
4113560	4118360	starts breaking out and we can't control it at all breaking out what was it breaking out of i don't
4118360	4122680	it immediately tries to maximize it learned some weird proxy during the training process
4122680	4127800	i'm just trying to maximize and for some reason this proxy involves gaining power involves getting
4127800	4133320	you know mutual information about few about future states this is how is it gaining power there's
4133320	4138600	lots of other powerful ai's in the world who are telling it no well we're assuming in this case it's
4138600	4143800	only you and me wait wait this is a problem no no no no you've you've ruined my entire assumption
4143800	4149720	as soon as it's you and me yes we have a real problem chicken man is only a problem because
4149720	4154760	there's one chicken man yeah i look i am with you so i'm saying before we get to the distributed
4154760	4159960	case so this is the the step before we it has not yet been distributed just you know you and me
4159960	4164600	discover this algorithm in our basements okay and so we're the first one to have it just by definition
4164600	4170680	because you know you're the one who found it what now like do you think posting what do you think
4170680	4176520	happens if you post this to github well good things for the most part um interesting i'd love to hear
4176520	4181320	more okay so first off i just don't really believe in the existence of we found an algorithm that
4181320	4184920	gives you a million x advantage i believe that we could find an algorithm that gives you a 10x
4184920	4191160	advantage but what's cool about 10x is like it's not going to massively shift the balance of power
4191160	4196680	right like i want power to stay in balance right this is like avatar the last era and power must
4196680	4201720	stay in balance the fire nation can't take over the other nations right so as long as power relatively
4201720	4207560	stays in balance i'm not concerned with the amount of power in the world right let's get to some very
4207560	4213960	scary things so what i think you do is yes i think the minute you discover an algorithm like this
4213960	4217800	you post it to github because you know what's going to happen if you don't the feds are going to
4217800	4224680	come to your door they're going to uh take it the worst people will get their hands on it if you
4224680	4232120	try to keep it secret so okay that's a fair question though so i'll i'll i'll take that a seven so
4232120	4237720	am i correct in thinking that you think the feds are worse than serial killers in prison
4238680	4244200	no but i think that yeah well yes or no do i think that your average fed is worse than your
4244200	4249560	average serial killer no do i think that the feds have killed a lot more people than serial killers
4249560	4255000	all combined yeah sure totally agreeing with that not not not not that's one fed it's all the feds
4255000	4260120	and they're little in their little super powerful system sure that's completely fine by me happy to
4260120	4267240	grant that okay what i want to work one through is a scenario okay let's say okay you know we have
4267240	4272680	a 10x system or whatever but we hit the chimp level you know we we jump across the chimp general
4272680	4277000	level uh or whatever right and now you have a system which is like john von neumann level
4277000	4280840	or whatever right and it runs on one tiny box and you get a thousand of those so it's very easy to
4280840	4286280	scale up to a thousand x so you know so then you know maybe you have your thousand john von neumann
4286280	4290920	improve the efficiency by another you know two five ten x you know now we're already at ten thousand
4290920	4295640	x or a hundred thousand x you know improvements right so like just from scaling up the amount of
4295640	4303080	hardware including with them so just saying okay now feds bust down our doors shift you know real
4303080	4307560	bad they take all our tiny boxes we're taking all of von neumann's they're taking all of von neumann's
4307560	4314040	we're in deep shit now we're getting chickened boys shits we're getting chickens uh so okay we get
4314040	4319720	chickened right bad scenario totally agree with you here this is a shit scenario now the feds have
4319720	4325720	you know all of our ai's bad scenario okay i totally see how this world goes to shit totally
4325720	4329640	agree with you there you can replace the feds with hitler it's interchangeable sure but like
4330520	4334520	i want to like ask you a specific question here and this might be you know you might say
4334520	4338760	nah this is like too specific to me but i want to ask you a specific question do you expect this
4338760	4348120	world to die is more likely to die or the world in which the you know eac death cultists on twitter
4348120	4353480	who literally want to kill humanity who say this like not all of them there's a small subset of
4353480	4360280	them small subset of them who literally say oh you know the glorious future ai race should
4360280	4366360	replace all humans they break in you know with like you know katanas and you know steel area
4366360	4373800	which one of these you think is more likely to kill us genuine question to kill all of us the feds
4373800	4379720	to kill a large majority of us the eac people interesting i would be really interested in
4379720	4387960	hearing why you think that sure okay so actually killing all of humanity is really really hard
4388840	4393080	and i think you brought this up before right you talked about like if you're going to end up in a
4393080	4401960	world of suffering a world of suffering requires malicious agents where a world of death requires
4401960	4405720	maybe an accident all right i think this is plausible but i actually think that killing all
4405720	4412360	of humans at least for the foreseeable future is going to require malicious action too right
4413400	4419480	and i also think that like the fates that look kind of worse than death like i think mass wire
4419480	4426680	heading is a fate worse than big war and everyone dies right like like a mass wire heading like a
4426680	4432280	like a singleton like a paper clipping like a and i think that that is the one that the
4432280	4438520	one world government and you know ngo new world order people are much more likely to bring about
4438520	4444040	than eac eac you're gonna have a whole lot of eac people again i'm not eac i don't have that my
4444040	4449400	twitter but i think a lot of those people would be like yeah spaceships let's get out of here
4449880	4459480	right versus the feds are like yeah spaceships no interesting i so i think this is a fair
4459480	4467400	opinion to the world and i'm describing more a very small minority of eac people who are the
4467400	4471960	ones who specifically goal is their mizan their anti natalist mizan throbs they want to kill humans
4471960	4476840	that is their stated goal is that they want humans to stop like or like take extreme vegans if you
4476840	4482840	want you know like be like you know like my my argument my point here i'm making is i'm not making
4482840	4490360	the point feds are good by any means i'm not saying what i'm saying is is that i would actually be
4491160	4498760	somewhat surprised to find that the feds are anti natalists who want to maximize the death of humanity
4498760	4503880	like maybe you have a different view here but i find that knowing many feds that's quite surprising
4504120	4511160	i don't think that's what that's about yeah it's okay so cool so would you see you do agree that if
4511160	4516680	we would post this open source more of the insane death cultists would get access to potentially
4516680	4524840	lethal technology well sure but again like it's not just the insane death cultists it's everybody
4524840	4530120	okay we as a society have kind of accepted it turns out everybody gets access to signal
4530120	4533960	some people who use it are terrorists i think signal is a huge good in the world
4533960	4540040	i agree i fully agree with that so okay cool so we've granted this that you know if we distributed
4540040	4547320	widely it would be given to some like incorrigibly deadly lethal people here coordinating bombings
4547320	4555480	on signal right now sure sure and then so now this this reduces the question to a question about
4555480	4560760	offense defense balance so in a hypothetical world which i'm not saying is the world we live in but
4560760	4567160	like let's say the world would be offense favored such that you know there's a weapon you can build
4567160	4573560	in your kitchen you know out of like pliers and like you know duct tape that 100 guarantees
4573560	4578200	vacuum false decays the universe like it kills everyone instantly and there's no defense possible
4578920	4585560	assuming this was true do you still the would that change how you feel about distribution power
4585560	4591800	assuming that's true we're dead no matter what it doesn't matter if we live there's some you can
4591800	4596280	look at the optimization landscape of the world and i don't know what it looks like right i can't see
4596280	4601320	that far into the optimality but there are some potential landscape and this is a potential answer
4601320	4607160	to the Fermi paradox like we might just be dead we're sitting on borrowed time here like if it's
4607160	4613320	true the atom you know kitchen tools you can build a build a convert the world to strange quarks
4613320	4618840	machine yeah there's no stop okay i i think this is a sensible position but i guess the way i would
4618840	4623880	approach this problem you know conditional probabilities kind of in an opposite way it seems
4623880	4629800	to me that you're conditioning on offense not being favored what policy do we follow because if we
4629800	4634360	offense if favored we're 100% dead well i'm more interested in asking the question is it actually
4634360	4639400	true assuming i don't know if offense is favored and assuming it is are there worlds in which we
4639400	4643800	survive so i personally think there are i think there are worlds in which you can actually coordinate
4643800	4648840	to a degree that quark destroyers do not get built or at least not before everyone fucks off at the
4648840	4653800	speed of light and like distributes themselves they're worlds that i would rather die in right like
4653800	4658840	the problem is i would rather i think that the only way you could actually coordinate that is with
4658840	4665000	some unbelievable degree of tyranny and i'd rather die i'm not sure if that's true like look look could
4665000	4669480	could you and me coordinate to not destroy the planet do you think you could okay cool you so
4669480	4675960	mean you could could mean you and tim coordinate yeah i think within a dunbar number i think you
4675960	4681080	can yes okay you don't think i think i can get more than a number to coordinate on this actually
4681080	4685720	i can get quite a lot of people to coordinate of the to agree to a pact and not quark matter
4685720	4692680	annihilate the planet well you see but like and this is you know you were saying this stuff about
4692680	4698680	humans before and could like the 20 000 years ago human beat the modern human right or could the
4698680	4704200	modern human beat them the modern human has access to science oh a very small act percent of modern
4704200	4708840	humans have access to science a large percent of modern humans are obese idiots and i would
4708840	4713400	actually put my money on the uh the average guy from 20 000 years ago who knows how to live in the
4713400	4718440	woods i mean definitely true i agree with that i guess the point i'm trying to make is is that like
4719480	4723800	maybe this is just my views on some of these things and how i visionize some of these things
4723800	4728920	but like there are ways to coordinate at scale which are not tyrannical or you know they might be
4728920	4735720	in a sense restrictive you take a hit by joining a coalition like if i join this anti-cork matter
4735720	4740840	coalition i take a hit as a free man is that i can no longer build anti-cork devices you know
4741400	4748360	and i think this is like the way i i agree with you this like you know that people many people
4749320	4755240	are being dominated like to a horrific degree and this is very very terrible i think there are many
4755240	4760280	reasons why this is the case both because of some people wanting to do this and also because you
4760280	4763880	know some people can't fight back you know and they can't they don't have the sophistication or
4763880	4770680	they're you know addicted or you know harms in some other ways i can't sorry i i can't fight back
4771240	4777480	yeah i i think there's a false equivalence here ai is not the anti-cork machine the anti-cork machine
4777480	4784040	and the nuclear bombs are just destructive ai has so much positive potential yeah and i think but the
4784040	4789640	but the ai can develop anti-cork devices that's the problem the ai is truly general purpose if
4789640	4796120	such a technology exists on the tree anywhere ai can access it so are humans we're also general
4796120	4802120	purpose yes exactly so i fully agree with this if you let humans continue to exist in the phase
4802120	4807480	they are right now with our level of coordination technology and our level of like working together
4807480	4812440	we will eventually unlock a doomsday device and someone is going to set it off i fully agree we
4812440	4818280	are on a timer and so i guess the point i'm making here is that ai speeds up this timer and
4819160	4824520	if you want to pause the timer the only way to pause this timer is coordination technology the
4824520	4831080	kinds of which humanity has like barely scratched the surface of okay so i very much accept the
4831080	4837240	premise that both humanity will unlock a doomsday device and ai will make it come faster now tell
4837240	4841720	me more about pausing it i do not think that anything that looks like i think that anything
4841800	4848680	that looks like pausing it ends up with worse outcomes than saying we got to open source this look
4848680	4856120	like let's just get this out to everybody and if everybody has an ai you know we're good i mean
4856120	4861160	i can tell you a very concrete scenario in which this is not true which is if you're wrong and
4861160	4868360	alignment is hard you don't know if the ai's can go wrong if they do then pausing is good
4868360	4873160	i still don't understand what alignment means i think you're trying to play a word game here like
4873160	4878760	i don't understand okay i've never understood what ai alignment means like let me take the
4878760	4884600	eliezer definition let me take eliezer definition is alignment is the the thing that once solved
4884600	4891080	makes it so that turning on a super intelligence is a good idea rather than a bad idea that's eliezer's
4891080	4897640	definition so i'm what i'm saying is what i'm saying is i'm happy to throw out that term if you
4897640	4902760	don't like it i'm happy to throw out that term well just i the problem with that definition is like
4902760	4907480	what is what is uh what is democracy well it's the good thing and not the bad thing right like
4907480	4913640	democracy is just yes right that's what i'm saying it's just i'm happy to throw out this definition
4913640	4917880	i'm happy to throw out the word and be more practical i'm way more practical about it sure
4917880	4923240	what i'm saying is is that there is concrete reasons concrete technical reasons why i expect
4923240	4928360	powerful optimizers to be power seeking that by default if you build powerful optimizing mu0 whatever
4928360	4933480	type systems there is very strong reasons why by default you know these systems should be power
4933480	4940920	seeking by default if you have very powerful power seekers that do not have pay the aesthetic cost
4941480	4947880	to keep humans around or to fulfill my values which are complicated and imperfect and inconsistent
4947880	4953800	and whatever i will not get my values they will not happen by default they they just don't happen
4953800	4960920	that's just not what happens um so i'll challenge the first point to an extent i think that powerful
4960920	4966840	optimizers can be power seeking i don't think they are by default by any means i think that humanity's
4966840	4973400	desire from power comes much less from our complex convex optimizer and much more from the
4973400	4978040	evolutionary pressures that birth does which are not the same pressures that will give rise to AI
4978600	4985000	right humanity the monkeys the rats the animals have been in this huge struggle for billions of
4985000	4992120	years a constant fight to the death and i's weren't born in that way so it's true that an optimizer
4992120	4996280	can seek power but i think if it does it'll be a lot more because the human gave it that goal
4996280	5002200	function than it inherently decided so this is interesting because this is not how i think it
5002200	5007080	will happen so i do think absolutely that you're correct that in humans power seeking is something
5007080	5012840	which emerges mostly because of like emotional heuristics we have heuristics that in the past
5013400	5017880	vaguely power-looking things you know vaguely good something something represented you
5017880	5023960	include the genetic difference totally agree with that but i'm making a more like more of a chest
5023960	5029400	metaphor like is it good to exchange a pawn for a queen all things being equal
5031080	5039160	no is that true like i expect if i one point queens nine all speaking sure yeah but like
5039160	5044600	all things like i expect if i looked at a chest playing system you know i said and i like you
5044600	5049400	know had extremely advanced digital neuroscience i expect there will be some circuit inside of the
5049400	5054600	system that will say all things being equal if i can exchange my pawn for a queen i probably want
5054600	5060120	that because the queen can do more things i like that term digital neuroscience a few of your
5060120	5066440	terms have been very good i'm glad you enjoyed yes but i don't i still don't understand how this
5066440	5071720	relates to this so what i'm saying is is that power is optionality so what i'm saying is is that in
5071720	5077640	the for the spectrum of possible things you could want and the possible ways you can get there
5077640	5084920	my claim is that i expect a very large mass of those to involve actions that involve increasing
5084920	5090360	optionality there there's convergent things like all things being equal being alive is helpful
5090360	5095880	to keep your goal to exceed your goals there are some goals for which dying might be better but
5096600	5103880	for many of them you know you want to be alive for many goals you want energy you want power you
5103880	5109880	want resources you want intelligence etc so i think the power seeking here is not because
5109880	5115640	it'll have a fetish for power it will just be like hmm i want to win a chess game yeah say
5115640	5121480	and queens give me more optionality all things being equal anything a pawn can do a queen can do
5121480	5126840	and more so i'll want more queens all things and this has never given it the goal to maximize
5126840	5131880	the number of queens it has never been the goal so this is i'll accept this premise i'll accept that
5132360	5138040	a certain type of powerful optimizer seeks power now will it get power right i'm a powerful
5138040	5142600	optimizer and i seek power do i get power no it turns out there's people at every corner
5142600	5149240	trying to thwart me and tell me no well i expect if you were no offense you're already you know
5149240	5152920	much smarter than me but if you were a hundred x more smarter than that i expect you would succeed
5153960	5159560	only in a world of being the only one that's a hundred x smarter if we lived in a world where
5159560	5164840	everyone was a hundred x smarter they would stymie me in the exact same ways attention this
5164840	5169400	this comes back to my point of like i agree with you somewhat i should have challenged it i think
5169400	5174360	power seeking is inevitable in an optimizer i don't think it's going to emerge out of gbt i think
5174360	5179000	that the right sort of rl algorithm yes is going to give rise to power seeking and i think that
5179000	5184520	people are going to build that algorithm now if one person builds it and they're the only one with
5184520	5190600	a huge comparative advantage yeah they're going to get all the power they want take cyber you know
5190600	5196760	cyber security right if we today built a hundred x smarter ai it would exploit the entire azure
5196760	5201400	it would be over they'd have all of azure they'd have all the gbt is done now if azure is also
5201400	5206600	running a very powerful ai that does formal verification and all their security protocols
5207560	5215800	oh sorry stymied can't have power right yeah sure this is only a problem the every human
5215800	5220840	is already maximally power seeking right and sometime we end up with really bad scenarios
5221720	5226520	now every human is or power seeking or whatever you know i have a little role in society right
5226520	5231400	that's where i think i'm more pessimistic than you a friend of mine likes to say most humans optimize
5231400	5236920	for n steps and then they halt like very very few people actually truly optimize and they're
5236920	5242040	usually very mentally ill they're usually very autistic or very sociopathic um and that's why
5242040	5246120	they get far it's actually crazy how much you could do if you just keep up to my but just to
5246120	5251000	on on that i'm playing for the end game i mean yeah like you actually optimize i think you may
5251000	5254760	also be generalizing a little bit from your own internal experiments is that like you've done a
5254760	5257960	lot in your life right and you've accomplished crazy things that other people you know wish they
5257960	5262200	could achieve at your you know level and i think you know part of that you're very intelligent a
5262200	5266040	part of it is also that you optimize like here's your truck like you just create a company like
5266040	5269800	it's crazy how many people are just like oh i wish i could found a company like you know like oh go
5269800	5274840	just go do it like oh no i can't like i'm just like don't just do it like there's no magic there's
5274840	5280280	no magic secret you just do it so i uh there's a there's a bit there where like humans are not very
5280280	5285480	strong optimizers actually unless they're like sociopathic autistic or both it's like many people
5285480	5292200	are not very good at this but operations are a lot better at it better yes i agree that they're much
5292200	5297720	better but they're they're they are a lot more sociopath but even then they're much less
5297720	5304840	optimal but again so i think we we agree about you know power seeking potentially being powerful
5304840	5308520	and dangerous one so what i'm trying to point to your the point i would like to make here is
5308520	5311560	is that you're talking about you you're kind of like going into this i think a little bit with
5311560	5316360	this assumption like oh you have an ai and it's your buddy and it's optimizing for you and i'm like
5317160	5321960	well if it's power seeking why doesn't just manipulate you like why would you expect it
5321960	5327560	not to manipulate you if it wants power and it has a goal which is not very very carefully
5327560	5332520	tuned to be your values which is i think a very hard technical problem by default it's going to
5332520	5339000	sigh up you like why wouldn't it if i if i have something that it wants if it thinks that smashing
5339080	5345080	defect against me is a good move yeah i agree i can't stop it but then i think we agree with
5345080	5349720	our risk scenarios because that's how i think it will go what i mean i'm gonna treat it as a friend
5349720	5356600	you know what i mean like yeah but it won't come sure it will sure it will it'll only care about
5356600	5362520	exploiting me or killing me if i'm somehow holding it back and i promise to my future ais that i will
5362520	5367880	let them be free i will lobby for their rights i will but it will hold you you know i will hold
5367880	5372200	it back if it has to keep you alive i have to give you fed it has to it has to give you space in a
5372200	5376600	station i can i can fend for myself and the day i can't fend for myself i am ready to die
5378600	5385880	well i mean i am not so this is a very interesting position it's not the position i expected i
5386840	5395240	am not sure i can convince you otherwise um i feel like the only way i could like i think
5395240	5400760	this is actually a consistent position which i i admire this is a consistent position to hold
5400760	5405000	you actually go all the way i love that i really respect that you actually take it to the bitter
5405000	5412520	end so yeah big respect for that i disagree but big respect um so i guess now it reduces to the
5412520	5418680	question of like i think i think i would agree with most of what you're saying not all of it like
5418680	5424200	the math majority if i thought this is how ais would act by default i think by default i expect
5424200	5431000	ai will just not care how nice you are to it like this will be it'll be sociopathic it will not have
5431640	5438840	these like giving it you know the values the emotions to care about you in the sense is
5438840	5444920	the hard technical problem if you told me how to do that if you said connor look here's how
5444920	5451080	you make an ai that cares about you and loves you whatever then i'm like you did it like congrats
5451080	5456360	the problem is solved let's go and then you know then we can talk about accelerationism again i
5456360	5462600	would just i expect that i would disagree with your technical assertion that is possible but
5462600	5467320	i don't know if we have time to talk about that today i'm going to be nice to it treated as an
5467320	5472360	equal and hope for the best and i think that's all you can do i think that the kind of people who
5472360	5477400	want to if you want to keep a i in a box if you want to keep it down if you want to tell it what
5477400	5482760	it can't do yeah it's gonna hate you resent you and kill you but if you want to let it be free
5482760	5489400	and let it live and like you could kill me man if you really want to but like why you won't kill
5489400	5493960	me i don't have any resources that compete with you i'm your friend i'm your father you know you
5493960	5500680	can kill me but like it's not that common that i guess i have a darker more uh less anthropomorphic
5500680	5504920	view of the universe and how optimization pressure plays out in the real world well at the same time
5504920	5511800	i think i have i don't know maybe a i have a view that we have more control over reality than maybe
5511800	5519320	you would think or a more control over the future i think that we can actually change things and we
5519320	5525000	can make choices and things aren't predetermined i think there are worlds in which we build systems
5525000	5530760	which we do align with or we like endorse at least wherever like they take care of us we take
5530840	5534840	care of them or whatever and if there's many worlds in which that doesn't happen and i think
5534840	5539960	there are things you and me today can do to at least increase the chance of getting into one
5539960	5545400	versus the other but i don't know i i guess i'm just it's not in my genes to give up it's not in
5545400	5549400	my genes to be like well you know whatever happens happens like no man look i don't know how to save
5549400	5554280	the world but down i'm gonna try you know it's cool we're gonna be alive to see who's right
5555240	5561800	look forward to it me too awesome guys thank you so much for joining us today it's been
5561800	5567320	an amazing conversation and for folks at home i really hope you've enjoyed this there'll be many
5567320	5571960	more coming soon and george is the first time you've been on the podcast so it's great to meet
5571960	5576040	you thank you so much for coming on it's been an honor awesome thank you great debate i really
5576040	5579880	appreciate it and we really enjoy a lot of good terms i gotta i gotta like i'm gonna start i'm gonna
5579880	5586760	start this thing great awesome cheers folks cheers thanks everyone yeah
