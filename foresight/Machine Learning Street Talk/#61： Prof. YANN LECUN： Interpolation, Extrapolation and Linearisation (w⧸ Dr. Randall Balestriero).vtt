WEBVTT

00:00.000 --> 00:02.480
Coming up later in Machine Learning Street Talk,

00:02.480 --> 00:07.560
Professor Jan Le Koon, the godfather of deep learning.

00:07.560 --> 00:12.840
There's been a lot of people who have been sort of saying

00:12.840 --> 00:15.480
there's a limitation to deep learning, let's say,

00:15.480 --> 00:17.880
or machine learning more generally.

00:17.880 --> 00:20.240
Because it's obvious that those things basically

00:20.240 --> 00:22.640
do curve fitting, and that only works for interpolation

00:22.640 --> 00:24.400
and not for extrapolation.

00:24.480 --> 00:30.680
And that kind of dismissal always sounded wrong to me.

00:30.680 --> 00:35.120
Would this qualitative difference be in the form of fundamentally

00:35.120 --> 00:37.960
different things from deep learning, things that

00:37.960 --> 00:40.280
are like discrete symbolic reasoning

00:40.280 --> 00:42.080
or things of that type?

00:42.080 --> 00:44.280
And to that, my answer is clearly no.

00:44.280 --> 00:46.360
I do not believe that's the case.

00:46.360 --> 00:48.280
That's a limitation of supervised learning.

00:48.280 --> 00:52.000
It has absolutely nothing to do with deep learning.

00:52.000 --> 00:53.160
Computing on things.

00:53.160 --> 00:55.480
OK, and I put a stop right there.

00:55.480 --> 00:57.760
We also had a fascinating conversation

00:57.760 --> 01:02.280
with Randall Bellisterio from Meta AI Research.

01:02.280 --> 01:04.160
So I think it's two different things

01:04.160 --> 01:07.600
to be able to interpolate or move on your manifold

01:07.600 --> 01:11.240
and being in the interpolation regime from your training set.

01:11.240 --> 01:14.040
So is this similar to interpolation?

01:14.040 --> 01:15.440
Well, I mean, all of machine learning

01:15.440 --> 01:18.240
is similar to interpolation if you want, right?

01:18.240 --> 01:21.560
When you train a linear regression on scalar values,

01:21.560 --> 01:22.960
you're training a model, right?

01:22.960 --> 01:25.600
You're giving a bunch of pairs X and Y.

01:25.600 --> 01:29.560
You're asking what are the best values of A and B for Y equals

01:29.560 --> 01:33.640
AX plus B that minimizes the squared error of the prediction

01:33.640 --> 01:35.520
of a line to all of the points, right?

01:35.520 --> 01:36.880
That's linear regression.

01:36.880 --> 01:38.320
That's interpolation.

01:38.320 --> 01:39.960
All of machine learning is interpolation.

01:39.960 --> 01:41.720
In a high-dimensional space, there

01:41.720 --> 01:44.040
is essentially no such thing as interpolation.

01:44.040 --> 01:45.840
Everything is extrapolation.

01:45.840 --> 01:48.960
So imagine you are in a space of images, right?

01:48.960 --> 01:51.680
So you have a car images 256 by 256.

01:51.720 --> 01:54.440
So it's 200,000 dimensional input space.

01:54.440 --> 01:56.000
Even if you have a million samples,

01:56.000 --> 02:00.240
you're only covering a tiny portion of the dimensions

02:00.240 --> 02:01.120
of that space, right?

02:01.120 --> 02:05.960
Those images are in a tiny sliver of surface

02:05.960 --> 02:09.200
among the space of all possible combinations

02:09.200 --> 02:10.680
of values of pixels.

02:10.680 --> 02:13.560
So when you show the system a new image,

02:13.560 --> 02:16.240
it's very unlikely that this image is a linear combination

02:16.240 --> 02:17.200
of previous images.

02:17.200 --> 02:20.440
What you're doing is extra-appalation, not interpolation.

02:20.480 --> 02:22.480
And in high-dimension, all of machine learning

02:22.480 --> 02:24.640
is extrapolation, which is why it's high.

02:24.640 --> 02:26.800
Now, that was a clip that went out on the Chalet show.

02:26.800 --> 02:29.480
And I think I understand now what Lacune meant.

02:29.480 --> 02:31.640
He's saying that our intuition of interpolation

02:31.640 --> 02:35.200
is only correct in a very low number of dimensions.

02:35.200 --> 02:36.880
It's a mathematical impossibility

02:36.880 --> 02:39.640
to have statistical generalization in high dimensions.

02:39.640 --> 02:42.080
So he's not saying that neural networks are really

02:42.080 --> 02:44.880
extrapolating in the sense of continuing the pattern.

02:44.880 --> 02:47.240
He's saying our definition of extrapolation,

02:47.240 --> 02:49.760
the convex whole membership, is broken.

02:49.800 --> 02:52.080
Machine learning does not and will never

02:52.080 --> 02:53.440
work in high dimensions.

02:53.440 --> 02:55.520
That's why we've invented so many tricks

02:55.520 --> 02:58.880
to reduce the statistical and approximation complexity

02:58.880 --> 03:01.120
of problems, just like we do in computer science

03:01.120 --> 03:03.560
of the computational complexity of algorithms.

03:03.560 --> 03:08.080
Lacune is not saying that deep learning models are clairvoyant.

03:08.080 --> 03:11.240
Jan Lacune thinks that it's specious to say

03:11.240 --> 03:13.840
that neural network models are interpolating

03:13.840 --> 03:18.840
because in high dimensions, everything is extrapolation.

03:20.080 --> 03:21.600
Oh boy.

03:21.600 --> 03:24.680
We started thinking about making this show many months ago

03:24.680 --> 03:27.200
when Randall Belastriro and Jerome Pesente

03:27.200 --> 03:30.120
and Jan Lacune first released their paper,

03:30.120 --> 03:34.280
Learning in High Dimensions Always Amounts to Extrapolation.

03:34.280 --> 03:36.960
And it feels like a lot has happened since then.

03:36.960 --> 03:39.200
I mean, they do say the mark of an educated mind

03:39.200 --> 03:41.960
is being able to change your opinion

03:41.960 --> 03:44.120
in light of new evidence.

03:44.120 --> 03:46.560
Where we are now compared to where we were then,

03:46.560 --> 03:48.400
I mean, let's just say, I feel like I'm standing

03:48.400 --> 03:50.360
on the surface of Pluto.

03:51.800 --> 03:54.960
I was initially quite skeptical of this paper

03:54.960 --> 03:56.640
and I was trying to pick it apart.

03:56.640 --> 04:00.000
And now it feels like we've done 180 degrees,

04:00.000 --> 04:02.760
not only on the way we think about the paper,

04:02.760 --> 04:05.200
but how we think about neural networks in general.

04:05.200 --> 04:07.440
And we really wanna try and impart some of that knowledge

04:07.440 --> 04:08.960
with you today in the introduction,

04:08.960 --> 04:10.680
which might be quite ambitious.

04:10.680 --> 04:11.680
So please bear with us.

04:11.680 --> 04:14.320
But generally speaking, the structure of the show today,

04:14.320 --> 04:15.520
it's gonna be a big show.

04:15.520 --> 04:16.720
It's gonna be a big show.

04:16.720 --> 04:20.440
Use the table of contents, skip around a bit.

04:20.440 --> 04:22.400
But there's an intro, there's a, you know,

04:22.400 --> 04:24.280
where we talk about this blind view

04:24.280 --> 04:26.640
or this boundary view of neural networks,

04:26.640 --> 04:28.120
which I think is very interesting.

04:28.120 --> 04:30.160
There's a conversation with Yann LeCun,

04:30.160 --> 04:31.720
the godfather of deep learning.

04:31.720 --> 04:35.120
And we also speak with Randall Belastriro,

04:35.120 --> 04:38.000
who released this beautiful paper

04:38.000 --> 04:40.760
about thinking about neural networks in this new way.

04:40.760 --> 04:42.000
And we have a debrief as well.

04:42.000 --> 04:43.520
So I've not edited the show yet,

04:43.520 --> 04:45.640
but it's gonna be about three hours.

04:45.640 --> 04:47.520
Anyway, the show must go on.

04:47.520 --> 04:51.880
So as I was saying, particularly speaking with Randall,

04:51.880 --> 04:53.160
was a revelation.

04:53.160 --> 04:56.040
I mean, I originally thought that the authors were saying

04:56.040 --> 04:58.040
that people shouldn't think of neural networks

04:58.040 --> 05:01.600
as interpolators because they thought neural networks

05:01.600 --> 05:03.520
were doing something even more sophisticated.

05:03.520 --> 05:06.480
But what I came to realize, at least in Randall's case,

05:06.480 --> 05:08.360
is he's even more cynical about the behavior

05:08.360 --> 05:10.440
of neural networks than Gary Marcus.

05:10.440 --> 05:13.360
He doesn't think that they're fitting curves at all.

05:13.400 --> 05:15.200
Reading Randall's recent spline theory

05:15.200 --> 05:18.080
of deep learning paper has completely changed the way

05:18.080 --> 05:20.760
that I think about their behavior.

05:20.760 --> 05:24.920
So his view, basically, is that neural networks

05:24.920 --> 05:27.160
recursively chop up the input space

05:28.040 --> 05:32.040
into these little convex cells, or polyhedra,

05:32.040 --> 05:34.600
conceptually similar to decision trees,

05:34.600 --> 05:37.880
but one where the regions in the layer can share information,

05:37.880 --> 05:40.880
which means that different faces of these polyhedra

05:40.880 --> 05:43.480
will correspond to different hyperplanes

05:43.480 --> 05:46.640
set down by different neurons in the neural network,

05:46.640 --> 05:48.800
of course, ones which are topologically addressable

05:48.800 --> 05:50.080
from that location.

05:50.080 --> 05:54.920
So to me, this ends the notion that these neural networks

05:54.920 --> 05:56.640
are learning smooth data manifolds

05:56.640 --> 05:58.920
or performing smooth geometric transformations

05:58.920 --> 06:01.080
in the generative setting, which I used to think of

06:01.080 --> 06:03.400
as being a kind of diffeomorphism.

06:03.400 --> 06:04.960
It also gave me the realization that,

06:04.960 --> 06:07.720
at least in inference, each input prediction

06:07.720 --> 06:12.080
is representable with a single linear affine transformation,

06:12.080 --> 06:14.960
which leads directly to the next realization,

06:14.960 --> 06:17.520
which is that the latent space is not homogenous.

06:17.520 --> 06:19.160
It's potentially a different space

06:19.160 --> 06:21.200
for every single input example.

06:21.200 --> 06:22.960
I just didn't realize that before.

06:22.960 --> 06:24.400
You know, suddenly a lot of the magic

06:24.400 --> 06:25.600
of deep learning has vanished,

06:25.600 --> 06:28.320
and I see them in the same light as things like decision trees

06:28.320 --> 06:30.200
and SVMs in classical machine learning.

06:30.200 --> 06:32.320
I mean, probably because I understood

06:32.320 --> 06:33.720
those things very deeply.

06:34.640 --> 06:36.200
So yeah, neural networks have lost

06:36.200 --> 06:37.360
a little bit of their mystery,

06:37.360 --> 06:39.640
but I think it's a good thing.

06:39.640 --> 06:41.680
Reminds me a little bit of this parable

06:41.680 --> 06:44.240
of the blind men and the elephant,

06:44.240 --> 06:46.400
where, let's say you've got four blind men

06:46.400 --> 06:47.680
around an elephant.

06:47.680 --> 06:49.720
One's got his arm around the trunk.

06:49.720 --> 06:51.520
Oh, it feels like a snake.

06:51.520 --> 06:54.560
One's feeling the tail, or it looks like a rope,

06:54.560 --> 06:56.080
or it feels like a rope.

06:56.080 --> 06:58.720
One's on the side, or it feels like a wall.

06:58.720 --> 07:01.400
And it's very similar situation with neural networks

07:01.400 --> 07:04.240
that, you know, the experience of these blind men,

07:04.240 --> 07:07.320
it's all the truth, but it's not the complete truth.

07:07.320 --> 07:09.560
We're all just trying to understand this thing

07:09.560 --> 07:11.120
from different angles.

07:11.120 --> 07:14.960
Now anyway, the two key challenges in machine learning

07:14.960 --> 07:17.760
are one, the curse of dimensionality.

07:17.760 --> 07:19.200
And there's a corollary of that,

07:19.200 --> 07:21.080
knowing what to ignore in the input space,

07:21.080 --> 07:24.040
because it blows up exponentially with the dimensions.

07:24.040 --> 07:27.240
And two, the extrapolation problem.

07:27.240 --> 07:29.280
What happens when you extrapolate

07:29.280 --> 07:31.000
outside of the training data?

07:31.000 --> 07:35.400
And why does this notion of extrapolation even matter?

07:35.400 --> 07:38.640
Well put simply, it matters because of the implications

07:38.640 --> 07:42.240
it has for generalization in deep learning.

07:42.240 --> 07:45.240
Now, why do deep learning models work at all?

07:45.240 --> 07:47.600
Well, I think there's an incredible amount of engineering

07:47.600 --> 07:51.220
which goes into the state of the art machine learning models,

07:51.220 --> 07:54.880
which makes them work very well on specific tasks.

07:54.880 --> 07:58.680
But it's easy to deceive yourself with neural networks.

07:58.680 --> 08:01.320
Literally, everything about your training process

08:01.320 --> 08:03.760
and predictive architecture could be leaking

08:03.760 --> 08:06.360
the main specific information into your model.

08:06.360 --> 08:09.000
Neural networks are not really blank slate models

08:09.000 --> 08:10.480
like we've been led to believe.

08:10.480 --> 08:12.880
A lot of human crafted domain knowledge

08:12.880 --> 08:15.840
goes into these models.

08:15.840 --> 08:17.560
By the way, this is why Francois Schollet,

08:17.560 --> 08:19.480
I mean, he talks about this notion

08:19.480 --> 08:21.240
of developer aware generalization,

08:21.240 --> 08:24.320
which is being able to generalize to tasks

08:24.320 --> 08:26.280
which the developer of the system

08:26.280 --> 08:28.080
didn't know about at the time.

08:29.060 --> 08:32.200
Now, weights and biases is the developer first

08:32.240 --> 08:35.640
of all, MLops platform build better models faster

08:35.640 --> 08:38.560
with experiment tracking, dataset versioning

08:38.560 --> 08:40.160
and model management.

08:40.160 --> 08:42.480
It provides a platform and a set of tools

08:42.480 --> 08:44.960
which you can use for the entire life cycle

08:44.960 --> 08:46.840
of your machine learning process.

08:46.840 --> 08:49.600
Every team has a reliable system of record

08:49.600 --> 08:50.880
for their engineering system, right?

08:50.880 --> 08:53.360
Whether it's Azure DevOps for software engineering

08:53.360 --> 08:55.680
or Confluence for team wikis.

08:55.680 --> 08:58.560
Weights and biases is your system of record

08:58.560 --> 08:59.880
for machine learning.

08:59.880 --> 09:02.600
Now, for your quick link and to help out the podcast,

09:02.600 --> 09:06.280
head over to 1db.me forward slash MLST.

09:06.280 --> 09:08.880
I'm extremely proud actually that we've been sponsored

09:08.880 --> 09:09.800
by weights and biases.

09:09.800 --> 09:12.080
I've been a huge fan of what they've been doing

09:12.080 --> 09:13.240
since the very beginning.

09:13.240 --> 09:15.800
And also I've been a part of their community forum.

09:15.800 --> 09:17.800
They are the one company who in my opinion

09:17.800 --> 09:19.880
has totally changed the game

09:19.880 --> 09:22.640
around what is the highly nuanced process

09:22.640 --> 09:25.240
and the kind of multidisciplinary team actions

09:25.240 --> 09:28.040
that are required to get machine learning models

09:28.040 --> 09:29.600
safely into production.

09:29.600 --> 09:32.280
I've been a chief data scientist for a major corporation

09:32.280 --> 09:33.440
for the last 18 months or so.

09:33.440 --> 09:35.720
And I've always been a big believer

09:35.720 --> 09:37.560
in introducing engineering rigor

09:37.560 --> 09:39.400
to machine learning and data science.

09:39.400 --> 09:41.560
Engineering fundamentals are so important

09:41.560 --> 09:44.080
when you bring machine learning systems to production

09:44.080 --> 09:47.800
as is helping data scientists to become first class citizens

09:47.800 --> 09:51.660
in the entire life cycle of model development and deployment.

09:51.660 --> 09:53.720
It's almost a shame that MLST has become

09:53.720 --> 09:55.200
quite science orientated

09:55.200 --> 09:58.000
because in my day job, I'm an engineer first.

09:58.000 --> 10:00.640
I would love to make more content about this stuff.

10:00.640 --> 10:02.880
Anyway, weights and biases helps you go faster.

10:02.880 --> 10:04.400
It makes your team more resilient

10:04.400 --> 10:06.100
by increasing your knowledge sharing.

10:06.100 --> 10:07.120
It helps you build models

10:07.120 --> 10:09.640
which not only have better predictive performance

10:09.640 --> 10:12.000
but are more safe and secure.

10:12.000 --> 10:14.720
Weights and biases gives you better management information

10:14.720 --> 10:16.980
allowing your company to make better decisions

10:16.980 --> 10:19.840
and build data products with greater transparency

10:19.840 --> 10:21.160
than ever before.

10:21.160 --> 10:23.440
And critically, because weights and biases

10:23.440 --> 10:26.080
orchestrates the entire machine learning process,

10:26.080 --> 10:27.680
your models are reproducible

10:27.680 --> 10:30.240
and important decisions are immortalized.

10:30.240 --> 10:32.680
This fine level of control lets you set guardrails

10:32.680 --> 10:36.480
where you need to and reduce friction wherever possible.

10:36.480 --> 10:39.400
I've designed many ML DevOps systems over the years,

10:39.400 --> 10:41.360
including when I worked at Microsoft.

10:41.360 --> 10:43.360
The technology has really come of age now

10:43.360 --> 10:45.200
and I think you should be using a platform

10:45.200 --> 10:46.520
like weights and biases

10:46.520 --> 10:49.120
to help you train your machine learning models

10:49.120 --> 10:51.080
and to get them into production.

10:51.960 --> 10:54.840
Reproducibility has always been one of the biggest problems

10:54.840 --> 10:55.880
in machine learning.

10:55.880 --> 10:57.840
And the reason for that is that these systems

10:57.840 --> 10:59.600
are quite brittle, frankly.

10:59.600 --> 11:01.920
Their runtime characteristics depend strongly

11:01.920 --> 11:04.000
on the vagaries of the training regime,

11:04.000 --> 11:05.720
the choice of hyperparameters,

11:05.720 --> 11:08.280
even the hardware that they were trained on.

11:08.280 --> 11:09.800
You know, you always get into this syndrome

11:09.800 --> 11:11.800
where it works on my machine

11:11.800 --> 11:13.200
but I can't put it into production

11:13.200 --> 11:15.040
because I've got no idea

11:15.040 --> 11:17.560
how to recreate this thing somewhere else.

11:17.560 --> 11:19.680
I work in a highly regulated industry

11:19.680 --> 11:21.800
and we often need to wind the clock back

11:21.800 --> 11:24.560
to try and understand how a model was created

11:24.600 --> 11:26.200
which decisions were made

11:26.200 --> 11:27.480
and possibly go back later

11:27.480 --> 11:29.440
to reason about a model's behavior.

11:29.440 --> 11:32.600
This is precisely what weights and biases does.

11:32.600 --> 11:34.560
Now, it's completely free for academics

11:34.560 --> 11:35.880
and it's simple to get started.

11:35.880 --> 11:37.880
You just have to add a few lines of code.

11:37.880 --> 11:39.400
So what are you waiting for?

11:39.400 --> 11:40.960
Remember to click on our special link

11:40.960 --> 11:42.240
because it helps us out.

11:42.240 --> 11:44.920
That's 1db.me forward slash MLST.

11:44.920 --> 11:46.720
We are so grateful for the sponsorship

11:46.720 --> 11:47.880
from weights and biases.

11:47.880 --> 11:49.640
And also feel free to get in touch with us

11:49.640 --> 11:51.600
if you're interested in sponsoring the show.

11:51.600 --> 11:53.720
It'll help us scale up the operation a little bit

11:53.720 --> 11:55.640
because at the moment I'm doing all the editing

11:55.640 --> 11:56.680
and it takes a lot of time.

11:56.680 --> 11:58.480
Anyway, thank you very much.

11:58.480 --> 12:01.160
Randall's other recent work demonstrates

12:01.160 --> 12:03.720
that a large class of neural networks

12:03.720 --> 12:08.040
including CNNs, ResNets, RNNs and beyond,

12:08.040 --> 12:11.480
those that use piecewise linear activation functions

12:11.480 --> 12:15.160
like RELU can be entirely rewritten

12:15.160 --> 12:17.760
as compositions of linear functions

12:17.760 --> 12:22.040
arranged in polyhedra or cells in the input space.

12:22.960 --> 12:27.880
It's a high resolution slice and dice into linear forms

12:27.880 --> 12:31.800
like a fruit ninja set loose on a marching cubes algorithm

12:31.800 --> 12:35.160
such as K-means clustering, matched filter banks

12:35.160 --> 12:37.240
and vector quantitization,

12:37.240 --> 12:40.640
which in plain English means locality sensitive clustering.

12:41.520 --> 12:43.320
It also provides new insight

12:43.320 --> 12:47.000
into the functioning of neural networks themselves.

12:47.000 --> 12:49.120
For example, with this view,

12:49.120 --> 12:50.800
what a neural network is doing

12:50.800 --> 12:53.920
is performing an input sensitive lookup

12:53.920 --> 12:55.720
for a matching filter

12:55.720 --> 12:58.240
and then computing a simple inner product

12:58.240 --> 13:00.760
between that filter and the input signal.

13:01.800 --> 13:03.120
If that doesn't shed light

13:03.120 --> 13:06.240
on Francois Chalet's characterization of neural networks

13:06.240 --> 13:08.880
as locally sensitive hash tables,

13:08.880 --> 13:11.120
then I don't know what will.

13:11.120 --> 13:13.400
Another cool thing to come out of Randall's work

13:13.400 --> 13:15.520
was a geometrically principled way

13:15.520 --> 13:18.920
of devising regularization penalty terms

13:19.000 --> 13:21.160
which can improve neural network performance

13:21.160 --> 13:24.200
by orthogonalizing the placement

13:24.200 --> 13:26.720
of those latent hyperplane boundaries

13:26.720 --> 13:30.200
to increase their representational power.

13:30.200 --> 13:32.520
In short, looking at neural networks

13:32.520 --> 13:36.520
through this piecewise linear kaleidoscope, if you will,

13:36.520 --> 13:39.480
is opening new avenues of technical understanding

13:39.480 --> 13:40.680
and exploration.

13:40.760 --> 13:43.760
Now, we spoke with Yannick about this as well,

13:43.760 --> 13:46.600
and he commented that he had looked into these polyhedra,

13:46.600 --> 13:50.400
which he called relu cells in his own research.

13:50.400 --> 13:52.600
While he agreed that the boundaries between them

13:52.600 --> 13:54.960
are not technically smooth,

13:54.960 --> 13:56.800
there are so many of them,

13:56.800 --> 13:59.920
combinatorially many possible polyhedra, in fact,

13:59.920 --> 14:03.280
since each can be defined by any combination

14:03.280 --> 14:05.800
of topologically addressable hyperblogics

14:05.800 --> 14:08.640
that can be found in a neural network.

14:08.760 --> 14:11.880
So, if you look at some of the topologically addressable

14:11.880 --> 14:14.920
hyperplane boundaries, there are so many,

14:14.920 --> 14:16.920
Yannick thinks that neural networks

14:16.920 --> 14:20.320
can make them effectively smooth by arranging them

14:20.320 --> 14:24.400
so that they change very little from neighbor to neighbor.

14:24.400 --> 14:28.200
This insight came from his work in adversarial examples,

14:28.200 --> 14:31.200
where the name of the game is to perturbed the input

14:31.200 --> 14:34.440
as little as possible to the nearest polyhedron

14:34.440 --> 14:36.240
with a different class.

14:36.240 --> 14:39.720
He, Randall's work has transformed the way I think

14:39.720 --> 14:41.720
about neural networks.

14:41.720 --> 14:45.760
I know what they're doing at a much deeper level now.

14:45.760 --> 14:47.760
Each layer of a neural network

14:47.760 --> 14:50.680
contributes a new set of hyperplanes,

14:50.680 --> 14:53.720
and the relu's act to toggle the hyperplanes

14:53.720 --> 14:56.160
in an input sensitive way.

14:56.160 --> 15:00.040
Every layer, indeed every neuron in the network,

15:00.040 --> 15:02.080
not just the final layer,

15:02.080 --> 15:05.640
participates in placing flat decision boundaries,

15:05.640 --> 15:09.400
defining a honeycomb of affine cells.

15:10.560 --> 15:13.880
Each input signal will fall into one of these cells

15:13.880 --> 15:18.280
and be transformed by the combined affine transformation

15:18.280 --> 15:20.400
of every neuron it activated.

15:22.040 --> 15:26.320
I also used to think of a single unified latent space

15:26.320 --> 15:27.840
at each layer.

15:27.840 --> 15:31.360
However, it seems obvious that any particular input

15:31.360 --> 15:33.960
will toggle in an input specific way

15:33.960 --> 15:35.440
a set of hyperplanes.

15:35.440 --> 15:39.560
By virtue of the relu's, it does or does not activate.

15:39.560 --> 15:42.880
Therefore, different populations of input samples

15:42.880 --> 15:47.640
will reside in different latent spaces at any one layer

15:47.640 --> 15:51.960
defined by the activated set of hyperplanes.

15:51.960 --> 15:53.840
So it seems to me that instead of having

15:53.840 --> 15:56.080
a unified latent space,

15:56.080 --> 16:01.080
we have input specific latent spaces plural at each layer.

16:02.080 --> 16:04.440
There's also the matter of whether classifiers

16:04.440 --> 16:07.600
are even learning the data manifold at all.

16:07.600 --> 16:09.360
I have my doubts.

16:09.360 --> 16:12.440
They may be learning predictably useful aspects

16:12.440 --> 16:14.080
of the manifold.

16:14.080 --> 16:17.160
However, neural network classifiers are optimized

16:17.160 --> 16:19.480
to find class boundaries,

16:19.480 --> 16:23.120
and any structure between boundaries can be ignored.

16:24.240 --> 16:26.800
The idea that learning boundary manifolds

16:26.800 --> 16:31.040
necessarily means learning intrinsic connection manifolds

16:31.360 --> 16:33.560
is where I'm really struggling now.

16:33.560 --> 16:38.000
I can't convince myself that optimizing for separation

16:38.000 --> 16:41.800
will give the same outcome as optimizing for connection.

16:42.720 --> 16:45.360
For example, training the same neural network

16:45.360 --> 16:48.880
in two different ways, one for classification

16:48.880 --> 16:52.000
and once for decoding or generation,

16:52.000 --> 16:54.880
will result in very different network weights

16:54.880 --> 16:56.800
and latent structure.

16:56.800 --> 16:58.880
It's even more difficult to accept now

16:58.880 --> 17:02.920
that I'm thinking of these manifolds as piecewise linear,

17:02.920 --> 17:04.720
where what the neural network has learned

17:04.720 --> 17:07.800
is combinations of separating hyperplanes

17:07.800 --> 17:12.400
that chop the space into single class polyhedra.

17:12.400 --> 17:16.080
After all, there is nothing in the objective function

17:16.080 --> 17:19.480
which cares about the structure within a polyhedra.

17:19.480 --> 17:22.920
As long as the class is correct, we're good.

17:22.920 --> 17:25.760
Nor is there anything necessarily optimizing

17:25.760 --> 17:28.560
for global manifold structure

17:28.560 --> 17:31.880
beyond just efficient and parsimonious use

17:31.880 --> 17:34.080
of the available parameter space

17:34.080 --> 17:37.480
and whatever prior structure was hard-coded

17:37.480 --> 17:41.120
by humans and the network constraints and topology itself.

17:42.720 --> 17:44.520
Randall commented in the interview

17:44.520 --> 17:46.560
that in high-dimensional spaces,

17:46.560 --> 17:49.240
you can easily separate everything

17:49.240 --> 17:52.320
but your generalization performance might be bad.

17:52.320 --> 17:56.520
So you want to trade off separability with dimensionality.

17:56.560 --> 17:59.880
He thinks the merit of deep neural networks

17:59.880 --> 18:03.320
is being able to find a nonlinear transformation

18:03.320 --> 18:06.520
which retains separating hyperplanes

18:06.520 --> 18:09.240
while reducing dimensionality enough

18:09.240 --> 18:12.200
to confer generalization power.

18:12.200 --> 18:13.760
I mean, I couldn't help but notice

18:13.760 --> 18:17.120
that everything in the machine learning world is linear.

18:17.120 --> 18:19.720
All the popular algorithms are linear

18:19.720 --> 18:22.000
and any nonlinearity is a trick.

18:22.000 --> 18:24.400
I mean, we just apply some nonlinear transformation

18:24.400 --> 18:27.080
to the data before running it through our algorithms

18:27.080 --> 18:29.600
just like how it is in support vector machines

18:29.600 --> 18:32.760
or kernel ridge regression in Gaussian processors.

18:32.760 --> 18:34.480
Deep learning models are no different.

18:34.480 --> 18:37.520
We're just placing these relus all over the input space

18:37.520 --> 18:39.120
to slice it and dice it.

18:41.040 --> 18:41.880
Let's think about this.

18:41.880 --> 18:44.560
What is the simplest mathematical model?

18:44.560 --> 18:46.160
Linear, right?

18:46.160 --> 18:49.120
What's the next most simple one?

18:49.120 --> 18:50.880
Piecewise linear.

18:50.880 --> 18:53.320
So machine learning hasn't even evolved yet

18:53.320 --> 18:54.240
to the second order.

18:54.280 --> 18:56.840
Neural networks in their contemporary usage

18:56.840 --> 18:59.560
only include the minimum possible nonlinearity

18:59.560 --> 19:01.720
of piecewise linear.

19:01.720 --> 19:03.800
If they didn't, an entire neural network model

19:03.800 --> 19:07.280
could be described as one monolithic linear function

19:07.280 --> 19:09.680
with a single transformation matrix.

19:09.680 --> 19:12.200
This view, I think, should give you a cleaner

19:12.200 --> 19:15.240
and analytical view of neural networks.

19:15.240 --> 19:17.480
Now, there are some reasons, in my opinion,

19:17.480 --> 19:19.000
for the tendency towards linear.

19:19.000 --> 19:21.200
I mean, computability is one of the reasons,

19:21.200 --> 19:25.720
but almost every part of mathematics favours linearization,

19:25.720 --> 19:27.920
whether it's Newton's method or calculus

19:27.920 --> 19:31.440
or linear algebra solving PDEs,

19:31.440 --> 19:32.520
plenty of other examples.

19:32.520 --> 19:35.720
So it shouldn't come as a huge surprise.

19:35.720 --> 19:39.200
Not only that, the function space of nonlinear functions

19:39.200 --> 19:40.880
is exponentially larger.

19:40.880 --> 19:42.360
And remember, in machine learning,

19:42.360 --> 19:44.440
the big challenge is to reduce the size

19:44.440 --> 19:47.000
of the approximation class.

19:47.000 --> 19:48.680
So what about boundaries?

19:48.680 --> 19:50.400
Now, the purpose of every value cell

19:50.400 --> 19:53.480
is to define a boundary in the ambient space

19:53.480 --> 19:56.360
to chop off what is no longer required.

19:56.360 --> 19:58.920
All you have is a linear separating hyperplane.

19:58.920 --> 20:01.840
And downstream, you work out the distance

20:01.840 --> 20:05.040
from the hyperplanes that you're on the right side of,

20:05.040 --> 20:06.640
you know, the ones that didn't get chopped off.

20:06.640 --> 20:08.600
So the magic of neural networks

20:08.600 --> 20:12.920
is actually learning what to ignore in the ambient space.

20:12.920 --> 20:15.960
Now, I think a problem with my previous intuition

20:15.960 --> 20:19.280
is that, like most people, I imagine to neural network,

20:19.280 --> 20:22.080
latent space is a bit like a UMAP or a Disney projection

20:22.080 --> 20:25.360
plot, and this leads us to misunderstand their behavior.

20:25.360 --> 20:28.400
The latent space that these examples get projected into

20:28.400 --> 20:29.680
is not homogeneous.

20:29.680 --> 20:31.480
Depending on which cell you fell into

20:31.480 --> 20:33.920
in the input space or the ambient space,

20:33.920 --> 20:36.480
a different affine transformation will be applied,

20:36.480 --> 20:39.160
sending you to a different region of the latent space.

20:39.160 --> 20:42.520
So the latent space is kind of stitched together

20:42.520 --> 20:46.440
like bits of a cosmic jigsaw puzzle in the ambient space.

20:46.440 --> 20:48.760
And then when you run UMAP on the latent,

20:48.760 --> 20:50.480
you see all of the clusters, but you'd

20:50.480 --> 20:52.960
be forgiven for thinking that it was performing

20:52.960 --> 20:56.360
some kind of smooth, diffeomorphic transformation

20:56.360 --> 20:59.000
of the entire input space and successive layers

20:59.000 --> 21:01.520
in the neural network, or even learning the topology

21:01.520 --> 21:02.800
of the data manifold.

21:02.800 --> 21:04.880
I think it's much better to think of neural networks

21:04.880 --> 21:08.080
as quantizing the input space, much like a vector search

21:08.080 --> 21:11.280
engine does using locality-sensitive hashing.

21:11.280 --> 21:14.520
Now, imagine a classification problem on the Cartesian plane

21:14.520 --> 21:17.800
where the upper right and lower left quadrants are blue,

21:17.800 --> 21:21.400
and the upper left and lower right quadrants are orange.

21:21.400 --> 21:24.480
Now, as a minimal example, if we trained a single layer

21:24.480 --> 21:27.840
for neuron-relu neural network to fit this,

21:27.840 --> 21:30.640
it will fit four diagonal hyperplanes,

21:30.640 --> 21:34.400
two of which are reflected versions of the same hyperplane.

21:34.400 --> 21:36.120
Now, by the way, you can play with this

21:36.120 --> 21:38.560
on the TensorFlow playground, which we think is probably

21:38.560 --> 21:41.520
one of the best tools for building strong intuition

21:41.520 --> 21:43.680
on how neural networks work.

21:43.680 --> 21:46.720
Now, what it does is it shows you how the ambient space

21:46.720 --> 21:49.000
is being effectively subdivided by all of these

21:49.000 --> 21:50.760
addressable hyperplanes.

21:50.760 --> 21:53.400
But this quadrant example is unintuitive in the tool

21:53.400 --> 21:56.800
because getting your head around how the relus act together

21:56.800 --> 22:00.360
to combine these hyperplanes to carve up the ambient space

22:00.360 --> 22:02.200
isn't immediately obvious.

22:02.200 --> 22:05.040
Now, when you run the tool on a more complex example,

22:05.040 --> 22:07.200
so for example, a spiral manifold,

22:07.200 --> 22:11.080
you'll now see the artifacts of these hyperplanes everywhere.

22:11.080 --> 22:13.840
When these piecewise linear chops are composed together

22:13.840 --> 22:16.320
in the second layer, we get a decision surface

22:16.320 --> 22:19.080
in the ambient space, which can appear smooth given

22:19.080 --> 22:21.600
enough pieces, but it's actually a composition

22:21.600 --> 22:23.720
of piecewise linear functions.

22:23.720 --> 22:26.040
Now, as you can see, it's just chopping up the input space

22:26.040 --> 22:28.280
in every neuron in the first layer,

22:28.280 --> 22:31.080
each one with a different angle and translation.

22:31.080 --> 22:32.440
And when you get to the second layer,

22:32.440 --> 22:35.280
it's chopping up the space in a more sophisticated way

22:35.280 --> 22:38.400
by composing together the chops from the previous layer.

22:38.400 --> 22:40.160
Now, the first thing to realize here

22:40.160 --> 22:42.520
is that for each test input example,

22:42.520 --> 22:44.440
and every example in its vicinity,

22:44.440 --> 22:46.600
its class projection is definable

22:46.600 --> 22:48.560
with a single affine projection.

22:48.560 --> 22:50.400
There's no continuous morphing going on here.

22:50.400 --> 22:53.800
It's more like a jigsaw puzzle being pieced together

22:53.800 --> 22:57.520
to form many parts of a bigger ambient space.

22:57.520 --> 23:00.240
Now, oftentimes we see negative weights,

23:00.240 --> 23:03.200
which actually allows the hyperplanes to combine

23:03.200 --> 23:05.440
in interesting ways to kind of partially cancel

23:05.440 --> 23:09.200
each other out to allow more complex decision boundaries.

23:09.200 --> 23:12.920
Any hyperplane can be reused by multiple other neurons,

23:12.920 --> 23:14.680
whichever neurons it can reach

23:14.680 --> 23:17.480
via some topologically addressable pathway.

23:17.480 --> 23:19.120
Every hyperplane can be the face

23:19.120 --> 23:21.520
of multiple polyhedra in the ambient space.

23:21.520 --> 23:23.080
This is how they share information.

23:23.080 --> 23:24.400
And it should be obvious by now

23:24.400 --> 23:27.080
that there are two to the N addressable polyhedra

23:27.080 --> 23:30.040
in the ambient space where N is the number of neurons.

23:30.040 --> 23:31.800
Now, we also observed that the neural network

23:31.800 --> 23:34.360
is only capable of slicing and dicing the input space

23:34.360 --> 23:35.720
with flat planes.

23:35.720 --> 23:38.280
The only smooth nonlinearities enter the picture here

23:38.280 --> 23:41.440
where we as data scientists linearize the data

23:41.440 --> 23:44.200
by performing some smooth nonlinear transformation

23:44.200 --> 23:46.560
before it even enters the neural network,

23:46.560 --> 23:47.880
much like is the case

23:47.880 --> 23:49.960
with other machine learning algorithms.

23:49.960 --> 23:52.640
Now, even on the spiral data set example,

23:52.640 --> 23:55.600
the quickest way to make the neural network fit the data

23:55.600 --> 23:57.640
is to effectively linearize the data

23:57.640 --> 24:00.000
by applying a nonlinear transformation

24:00.000 --> 24:01.800
before it even gets in.

24:01.800 --> 24:02.880
Now, we also played around

24:02.880 --> 24:05.600
with this kind of contrived circular manifold data set

24:05.600 --> 24:08.240
where there's a ball of data encompassed

24:08.240 --> 24:09.320
by a circle around it.

24:09.320 --> 24:11.400
And it's possible to do a better job on that

24:11.400 --> 24:13.400
with one relu neuron, right?

24:13.400 --> 24:15.160
But with two nonlinear transformations

24:15.160 --> 24:16.920
of the input data set,

24:16.920 --> 24:19.040
rather than fit this huge bunch

24:19.040 --> 24:21.280
of piecewise linear relus on the ambient data.

24:21.280 --> 24:22.560
I mean, it should be obvious, right?

24:22.560 --> 24:25.240
But this idea that you don't need feature engineering

24:25.240 --> 24:27.920
in neural networks, I think is nonsense.

24:27.920 --> 24:30.280
Now, given how obvious it is that neural networks

24:30.280 --> 24:31.840
are just chopping up the input space

24:31.840 --> 24:33.560
with these piecewise linear functions,

24:33.560 --> 24:34.680
it begs the question,

24:34.680 --> 24:37.560
how could they possibly extrapolate in any meaningful way,

24:37.560 --> 24:39.520
right, especially in the general setting?

24:39.520 --> 24:42.840
And by the way, when we say extrapolation,

24:42.840 --> 24:45.320
we're talking about this more general sense

24:45.320 --> 24:46.680
of continuing the pattern,

24:46.680 --> 24:51.240
not the convex whole notion in the given in the paper.

24:51.240 --> 24:53.920
Regarding these piecewise linear functions,

24:53.920 --> 24:56.320
I remember an impactful moment

24:56.320 --> 24:58.680
for more than 20 years ago in grad school,

24:59.520 --> 25:02.400
a visiting professor who had multiple science

25:02.400 --> 25:03.920
and nature publications,

25:03.920 --> 25:06.960
you know, a pinnacle of academic achievement,

25:06.960 --> 25:09.760
was sharing his insights on how we students

25:09.760 --> 25:13.760
might also extract such papers ourselves from nature.

25:13.760 --> 25:15.720
And he was showing figures from his paper

25:15.720 --> 25:18.520
and they all had two things in common.

25:18.520 --> 25:22.320
First, they depicted very simple relationships

25:22.320 --> 25:25.120
that were previously undiscovered, okay?

25:25.120 --> 25:29.680
And second, all the fitted models were piecewise linear.

25:30.680 --> 25:33.120
He even explicitly commented along the lines

25:33.120 --> 25:36.040
that of course there's some underlying

25:36.040 --> 25:39.040
smooth nonlinear relationship,

25:39.040 --> 25:42.480
but absent a solid theoretical model of that,

25:42.480 --> 25:45.920
which is often going to be the case for new discoveries,

25:45.920 --> 25:50.840
you're better off with simple piecewise linear models.

25:50.840 --> 25:52.200
I guess when in doubt,

25:52.200 --> 25:55.480
Occam's razor always makes straight cuts.

25:56.600 --> 25:58.280
And now it's clear to me

25:58.280 --> 26:01.240
that the successful deep networks of today

26:01.240 --> 26:03.720
are following that advice.

26:03.720 --> 26:07.280
If you've ever wondered why piecewise linear activation

26:07.280 --> 26:10.160
functions are dominating the field,

26:10.160 --> 26:13.840
maybe it's because they've abandoned all pretense

26:13.840 --> 26:16.920
at finding smooth nonlinear models

26:16.920 --> 26:21.840
and are keeping it simple by fitting piecewise linear models,

26:21.840 --> 26:25.800
albeit at machine scale with billions of parameters.

26:26.800 --> 26:31.160
Randall's spline work makes that bit of philosophical insight

26:31.240 --> 26:34.080
brutally clear in my opinion.

26:34.080 --> 26:36.280
It conjures a scene of Darth Vader,

26:36.280 --> 26:39.640
that cybernetic warlord towering over

26:39.640 --> 26:41.880
a growing neural network saying,

26:41.880 --> 26:44.780
embrace your hyperplanes.

26:45.920 --> 26:48.040
Now, you know, I've always been skeptical

26:48.040 --> 26:50.240
and often remind us of the limitations

26:50.240 --> 26:52.320
of today's machine learning.

26:52.320 --> 26:55.720
I'd say things like ML isn't magic learning,

26:56.600 --> 26:57.680
but for a moment,

26:57.680 --> 27:01.560
I too allowed myself to become deluded into thinking

27:01.560 --> 27:04.760
that they are creating some kind of nonlinearity

27:04.760 --> 27:07.880
beyond piecewise linearity.

27:07.880 --> 27:12.840
But relu is by far the dominant activation function

27:12.840 --> 27:16.440
because it stops pretending at anything other

27:16.440 --> 27:18.840
than piecewise linear.

27:18.840 --> 27:22.880
Just stick in a flat boundary threshold and the line.

27:22.880 --> 27:24.880
A neuron puts in a hyperplane

27:24.880 --> 27:27.120
and then lets the rest of the network

27:27.120 --> 27:29.300
chop more as needed.

27:29.300 --> 27:31.800
All the success of neural networks

27:31.800 --> 27:36.380
seems explained by piecewise linear functions.

27:37.600 --> 27:40.760
I also find it intriguing that our own brains,

27:40.760 --> 27:43.440
our own wet neural networks,

27:43.440 --> 27:45.520
have somehow gained access

27:45.520 --> 27:48.940
to smooth nonlinear imagination.

27:49.920 --> 27:53.220
Just look at the laws we've defined in physics.

27:54.180 --> 27:59.060
Many exhibit nonlinear but smooth structure.

27:59.060 --> 28:00.420
On the other hand,

28:00.420 --> 28:02.820
neural networks chop up the input space

28:02.820 --> 28:07.240
with flat boundaries and sharp edges.

28:07.240 --> 28:11.700
How can we possibly expect them to learn or discover

28:11.700 --> 28:14.200
the kinds of smooth relationships

28:14.200 --> 28:18.140
that seem fundamental to science and reality?

28:18.140 --> 28:20.200
All feature engineering and representation learning

28:20.200 --> 28:22.380
and machine learning is about finding these

28:22.420 --> 28:24.620
interpolative representations.

28:24.620 --> 28:26.300
Now, I saw a really interesting example of this

28:26.300 --> 28:28.620
when I was reading Francois Chouelet's book recently

28:28.620 --> 28:32.920
and he spoke of pixel grids of watch faces, right?

28:32.920 --> 28:36.780
Now, they're not interpolative in the ambient space.

28:36.780 --> 28:38.780
If you take the average of two of these images,

28:38.780 --> 28:42.860
you'll just get four faded clock hands

28:42.860 --> 28:44.180
on top of each other, right?

28:44.180 --> 28:46.020
Superposed, not very helpful.

28:46.020 --> 28:48.340
But however, if you represent the hands

28:48.340 --> 28:49.980
with Euclidean coordinates,

28:49.980 --> 28:52.540
then the problem becomes more interpolated

28:52.540 --> 28:55.300
but you're still not all the way there.

28:55.300 --> 28:57.100
The average of two points of interest

28:57.100 --> 28:59.540
doesn't fall on this intended circle manifold

28:59.540 --> 29:01.580
that we're interested in.

29:01.580 --> 29:02.900
You know, it would be interpolated

29:02.900 --> 29:05.700
if we use some relevant distance function

29:05.700 --> 29:08.400
which was some geodesic interpolation.

29:09.460 --> 29:10.580
But you know, anyway,

29:10.580 --> 29:13.180
if we encoded the problem using polar coordinates,

29:13.180 --> 29:15.420
then it becomes linearly interpolated, right?

29:15.420 --> 29:16.580
The problem is solved

29:16.580 --> 29:19.400
and you can generalize perfectly to any new clock face.

29:19.400 --> 29:21.000
But I mean, in that case,

29:21.000 --> 29:22.480
you wouldn't even need a neural network

29:22.480 --> 29:25.040
and the kicker is that we as data scientists

29:25.040 --> 29:27.880
would have to figure this out ourselves, right?

29:27.880 --> 29:29.800
The neural network wouldn't be particularly efficient

29:29.800 --> 29:33.200
at doing so, especially if we, the data scientists,

29:33.200 --> 29:35.900
didn't feed in the relevant nonlinear transformation

29:35.900 --> 29:39.000
before it went into the model.

29:39.000 --> 29:40.720
So anyway, if there exists

29:40.720 --> 29:42.640
a nonlinear transformation of this problem,

29:42.640 --> 29:45.080
then, you know, which makes it interpolated,

29:45.080 --> 29:46.420
then we can say in some sense

29:46.420 --> 29:48.840
that it's got a lower intrinsic dimension.

29:48.840 --> 29:50.280
So crucially, deep learning models,

29:50.280 --> 29:53.720
they can be understood as unconstrained surfaces.

29:53.720 --> 29:55.180
They do have some structure

29:55.180 --> 29:56.860
coming from their architectural priors,

29:56.860 --> 30:00.520
but that structure only serves to restrict the search space

30:00.520 --> 30:03.200
to a smaller space of unconstrained surfaces.

30:03.200 --> 30:06.060
It doesn't provide the kind of inductive prior

30:06.060 --> 30:09.080
that would enable stronger generalization.

30:09.080 --> 30:12.160
A model architecture does not contain a model of the domain

30:12.160 --> 30:14.780
except in the extremely restricted sense

30:14.780 --> 30:17.360
of injecting priors such as, I don't know,

30:17.360 --> 30:20.520
translational equivalents in convolutional neural networks.

30:20.520 --> 30:23.960
If you want to generalize in a more systematic fashion,

30:23.960 --> 30:26.600
you either need a model with a strong inductive prior

30:26.600 --> 30:28.160
or a model which doesn't operate

30:28.160 --> 30:31.800
by empirically slicing up the ambient space,

30:31.800 --> 30:34.280
yeah, like a deep learning model does.

30:34.280 --> 30:36.240
A discrete symbolic program

30:36.240 --> 30:37.600
would be an interesting alternative.

30:37.600 --> 30:41.480
I mean, just imagine the program Y equals X squared.

30:41.480 --> 30:44.000
It generalizes to any arbitrary number, right?

30:44.000 --> 30:45.320
Because it's highly structured.

30:45.320 --> 30:47.560
You can fit it with three training points.

30:47.560 --> 30:49.800
And once fit, it'll generalize to anything,

30:49.800 --> 30:52.680
even things that are outside of the training range.

30:52.680 --> 30:54.000
But the kicker, of course,

30:54.000 --> 30:55.880
is that you wouldn't know which curve to use

30:55.880 --> 30:58.800
if you only saw three points in the first place.

30:58.800 --> 31:00.680
Now, let's think of another example.

31:00.680 --> 31:04.340
And imagine a discrete staircase space.

31:04.340 --> 31:06.000
So in this particular example,

31:06.000 --> 31:09.680
an unconstrained curve gives you complete garbage,

31:09.680 --> 31:12.920
but a structured model with the correct priors,

31:12.920 --> 31:14.560
you know, it can still be a curve,

31:14.560 --> 31:17.320
will generalize in the extrapolation regime.

31:17.320 --> 31:19.800
And this prior basically means that you need to encode it

31:19.800 --> 31:20.640
or linearize it

31:20.640 --> 31:23.160
before it even gets into the neural network.

31:23.160 --> 31:25.520
In both cases, you're making the neural network

31:25.520 --> 31:27.800
work extremely well for a specific thing,

31:27.800 --> 31:31.440
but not very well for generalizing between new tasks.

31:31.440 --> 31:32.760
So anyway, when we say

31:32.760 --> 31:35.480
that deep learning models generalize via interpolation,

31:35.480 --> 31:37.560
what we're really saying is that these models

31:37.560 --> 31:38.400
that we're using today,

31:38.400 --> 31:39.640
that they work well with data

31:39.640 --> 31:42.440
that's within the training distribution of the problem,

31:42.440 --> 31:46.840
right, well, a problem that's intrinsically interpolative,

31:46.840 --> 31:50.160
but they won't generalize systematically to anything else.

31:50.160 --> 31:52.360
You won't generalize when looking at problems

31:52.360 --> 31:54.440
that are not interpolative in nature

31:54.440 --> 31:58.400
and problems that are outside the training distribution.

31:58.400 --> 31:59.280
So what do we talk about

31:59.280 --> 32:01.520
this binary notion of extrapolation?

32:01.520 --> 32:03.760
Well, the problem with the binary convex

32:03.760 --> 32:05.800
whole notion of extrapolation is that

32:05.800 --> 32:07.480
we're promoting this idea

32:07.480 --> 32:11.240
that the moment an example falls epsilon outside the hole,

32:11.240 --> 32:13.640
that the prediction qualitatively changes.

32:13.640 --> 32:17.040
Instead, I think it would be more of a distance question,

32:17.040 --> 32:19.560
which is to say, the further away you get

32:19.560 --> 32:22.760
from this convex hole, the greater the uncertainty.

32:22.760 --> 32:24.320
I mean, it's not like you fall off a cliff

32:24.320 --> 32:26.400
when you step outside the convex hole.

32:26.400 --> 32:28.360
Your approximation of the latent manifold

32:28.360 --> 32:30.080
remains valid for a little while,

32:30.080 --> 32:31.880
although it will quickly degrade

32:31.880 --> 32:33.480
as you move further outside,

32:33.480 --> 32:36.040
unless your model has some strong structural prior

32:36.040 --> 32:38.760
that matches the regularity of that latent manifold.

32:38.760 --> 32:40.720
So this brings up an important point, right?

32:40.800 --> 32:43.120
The relevant question is not whether you're inside

32:43.120 --> 32:44.640
or outside the convex hole.

32:44.640 --> 32:48.080
It's how far you are from the nearest training data points.

32:48.080 --> 32:50.160
If you're outside the convex hole,

32:50.160 --> 32:52.440
but you remain close to the training data,

32:52.440 --> 32:55.160
then your model remains an accurate approximation

32:55.160 --> 32:56.520
of the latent manifold.

32:56.520 --> 32:58.920
And even if you're inside the convex hole,

32:58.920 --> 33:01.200
you're in a region that wasn't densely sampled

33:01.200 --> 33:02.720
at training time, right?

33:02.720 --> 33:05.200
Where there's no nearby training points.

33:05.200 --> 33:06.560
Your unconstrained model

33:06.560 --> 33:08.960
may not accurately approximate the latent manifold.

33:08.960 --> 33:12.960
So inside or outside doesn't really tell you very much.

33:12.960 --> 33:14.800
It's all about the proximity.

33:14.800 --> 33:17.520
You're performing local generalization

33:17.520 --> 33:19.880
and the quality of your ability to generalize

33:19.880 --> 33:23.040
to new situations depends entirely

33:23.040 --> 33:25.960
on their proximity to known situations.

33:25.960 --> 33:27.920
However, if you're dealing with training data

33:27.920 --> 33:29.200
that is densely sampled

33:29.200 --> 33:31.360
within a certain region of the problem space,

33:31.360 --> 33:34.480
then the question, can I generalize here

33:34.480 --> 33:37.200
becomes approximately equivalent to,

33:37.200 --> 33:39.920
is this inside the convex hole or outside?

33:39.920 --> 33:41.520
Now, the whole point of feature engineering

33:41.520 --> 33:44.480
is to make data sets interpolative.

33:44.480 --> 33:47.400
Either us data scientists design the features by hand

33:47.400 --> 33:51.000
or neural networks learn them as part of the training process

33:51.000 --> 33:53.800
in big air quotes for the podcast listeners.

33:53.800 --> 33:56.280
So, I think when folks make the argument

33:56.280 --> 33:57.920
that machine learning models generalize

33:57.920 --> 34:01.360
via interpolation or that computer vision data sets,

34:01.360 --> 34:02.840
that they're interpolative,

34:02.840 --> 34:05.200
what they mean is that there's an encoding space

34:05.200 --> 34:08.000
for which the problem becomes interpolative

34:08.000 --> 34:11.600
or there exists a non-Euclidean pairwise distance function

34:11.600 --> 34:15.080
between the instances that makes the problem interpolative,

34:15.080 --> 34:16.920
which is basically the same thing.

34:16.920 --> 34:19.120
So, if it's possible to do this,

34:19.120 --> 34:22.320
then you can say that the problem is intrinsically interpolative

34:22.320 --> 34:25.560
but your current representation of the problem is not.

34:25.560 --> 34:27.480
So the key endeavor in machine learning

34:27.480 --> 34:29.720
is to find better representations,

34:29.720 --> 34:33.120
representations which reveal the interpolative nature

34:33.120 --> 34:33.960
of the problem.

34:34.880 --> 34:37.880
Now, you might just cynically write off this paper

34:37.880 --> 34:39.360
as being a trivial finding, right?

34:39.360 --> 34:42.480
You know, it seems trivial that the pixel space

34:42.480 --> 34:44.720
is not linearly interpolatable.

34:44.720 --> 34:46.320
Something that we've known about for decades.

34:46.320 --> 34:49.560
I mean, that's why computer vision engineers in the 1980s

34:49.560 --> 34:51.440
would write feature detectors

34:51.440 --> 34:53.420
to create an interpolative space

34:53.420 --> 34:56.160
for machine learning algorithms to work on, right?

34:56.160 --> 34:57.560
If you take any two images

34:57.560 --> 35:00.120
and you interpolate between them, what do you get?

35:00.120 --> 35:02.240
You just get a faded copy of both.

35:02.280 --> 35:05.720
Everyone in the field has known about this for decades, right?

35:05.720 --> 35:09.280
That examples are not interpolative

35:09.280 --> 35:12.160
in their original encoding space or their pixel space.

35:12.160 --> 35:13.440
I think the most important thing though

35:13.440 --> 35:15.400
is that this paper also shows

35:15.400 --> 35:17.760
that the models are not interpolative

35:17.760 --> 35:20.220
in their latent space either.

35:20.220 --> 35:21.140
That's a shocker.

35:22.040 --> 35:23.920
Now, what does Randall say about all of this?

35:23.920 --> 35:27.440
Well, Randall Bellisterio thinks that the steelman

35:27.440 --> 35:29.760
for this interpolation argument would be,

35:29.760 --> 35:32.620
well, what if you had very low dimensional,

35:32.620 --> 35:34.880
kind of like approximation of your data

35:34.880 --> 35:36.520
with very few factors of variation,

35:36.520 --> 35:39.600
which could be easily linearized in the latent space,

35:39.600 --> 35:42.440
then it might be interpolative.

35:42.440 --> 35:45.960
Randall thinks that the very concept of interpolation,

35:45.960 --> 35:48.080
I mean, it was defined about 50 years ago

35:48.080 --> 35:50.680
to describe these very small models

35:50.680 --> 35:53.640
with very few factors of variation.

35:53.640 --> 35:56.800
And it's just not relevant today, right?

35:56.800 --> 35:57.800
They showed in their paper

35:57.920 --> 36:00.140
that even in the relatively small latent space

36:00.140 --> 36:03.280
on a popular neural network classifier,

36:03.280 --> 36:05.320
interpolation does not occur.

36:05.320 --> 36:07.960
He thinks that most people think of interpolation

36:07.960 --> 36:10.160
by kind of conceptualizing their data

36:10.160 --> 36:12.560
into a few important latent factors.

36:12.560 --> 36:14.160
You might have dogs, for example,

36:14.160 --> 36:16.200
and they might exhibit a latent color,

36:16.200 --> 36:18.800
and a new color might be an interpolation

36:18.800 --> 36:20.440
between observed colors.

36:20.440 --> 36:22.640
But these guys think that we need to have

36:22.640 --> 36:25.640
an entirely new definition of interpolation.

36:25.680 --> 36:27.920
I think the main purpose behind their paper,

36:27.920 --> 36:30.480
according to them, is to show that even though

36:30.480 --> 36:32.800
the intuition that people have of interpolation

36:32.800 --> 36:34.600
works well in low dimensions,

36:34.600 --> 36:36.880
it falls down completely flat on its face

36:36.880 --> 36:38.120
in higher dimensions.

36:38.120 --> 36:40.720
Actually, the probability of your test data

36:40.720 --> 36:43.520
being in the convex hull of your training data

36:43.520 --> 36:46.720
is near zero past a certain number of dimensions.

36:46.720 --> 36:48.680
So the current definition of interpolation,

36:48.680 --> 36:50.400
which is to say like this notion

36:50.400 --> 36:51.880
of convex hull membership,

36:51.880 --> 36:55.120
it's too rigid to think about how interpolation works

36:55.120 --> 36:56.600
in neural networks.

36:56.600 --> 37:00.360
So Randall thinks that any new definition of extrapolation

37:00.360 --> 37:04.040
should be directly linked to generalization itself.

37:04.040 --> 37:05.760
This is what they're trying to get at.

37:05.760 --> 37:08.960
This is a clip from our show with Professor Max Welling.

37:08.960 --> 37:09.920
The first thing I wanna say,

37:09.920 --> 37:12.240
there is no machine learning without assumptions.

37:12.240 --> 37:16.040
It just basically, you have to interpolate between the dots,

37:16.040 --> 37:18.520
and to interpolate means that you have to make assumptions

37:18.520 --> 37:20.080
on smoothness or something like that.

37:20.080 --> 37:23.000
So the machine learning doesn't exist without assumptions.

37:23.000 --> 37:24.080
I think that's very clear.

37:24.080 --> 37:26.960
But clearly it's a dial, right?

37:26.960 --> 37:29.160
So you can have, on the one end,

37:29.160 --> 37:31.480
you can have problems with a huge amount of data.

37:31.480 --> 37:33.720
It has to be available clearly.

37:33.720 --> 37:37.000
And there you can dial down your inductive biases.

37:37.000 --> 37:40.600
You can basically say that the data do most of the work

37:40.600 --> 37:41.920
in some sense.

37:41.920 --> 37:44.960
Now, I think it's a good thing to build intuition

37:44.960 --> 37:46.960
about machine learning principally

37:46.960 --> 37:49.120
as an interpolation problem, right?

37:49.120 --> 37:50.840
So we're given the training data,

37:50.840 --> 37:53.080
and we need to cleverly interpolate

37:53.080 --> 37:55.400
between the training examples to reason

37:55.400 --> 37:58.600
about the statistical context of the test examples.

37:58.600 --> 38:00.840
This is what machine learning is in a nutshell.

38:00.840 --> 38:04.200
And actually, a researcher friend of mine

38:04.200 --> 38:06.120
from MetaAI Research in Silicon Valley,

38:06.120 --> 38:08.160
a guy called Dr. Thomas Lukes,

38:08.160 --> 38:09.840
he published a paper a couple of years ago

38:09.840 --> 38:13.000
called Interpolation of Sparse High-Dimensional Data.

38:13.000 --> 38:15.320
And he showed that it was indeed possible

38:15.320 --> 38:18.440
to perform competitively with multi-layer perceptrons

38:18.440 --> 38:22.040
for a regression problem using pure play interpolation methods

38:22.080 --> 38:26.080
like Voronoi and Dolornoi triangulation and spline methods.

38:26.080 --> 38:28.920
I mean, think of creating simplexes

38:28.920 --> 38:31.320
of the nearest neighbors around a training data

38:31.320 --> 38:33.480
and then averaging the results together.

38:33.480 --> 38:36.000
This simple method works remarkably well

38:36.000 --> 38:37.880
up to about 30 dimensions.

38:37.880 --> 38:39.880
I mean, obviously eventually it gets deranged

38:39.880 --> 38:41.360
by the curse of dimensionality,

38:41.360 --> 38:44.120
but as we'll discover slightly later,

38:44.120 --> 38:46.800
neural networks might have a slight advantage

38:46.800 --> 38:49.280
over pure play simplex interpolation

38:49.280 --> 38:50.520
just because they work principally

38:50.520 --> 38:54.120
by figuring out boundaries of the input space to exclude.

38:54.120 --> 38:57.160
So, you know, if you're on the zero side of the ReLU, that is.

38:57.160 --> 38:59.240
So that helps a lot in high dimensions

38:59.240 --> 39:00.960
when you have a poor sampling density

39:00.960 --> 39:03.040
in a particular region of the input space.

39:03.040 --> 39:07.120
But anyway, if you do principally think of machine learning

39:07.120 --> 39:08.960
as being an interpolation method,

39:08.960 --> 39:11.080
it raises some interesting ideas.

39:11.080 --> 39:13.760
You know, Thomas said to me that the crux of the problem

39:13.760 --> 39:15.840
is that we're trying to approximate a function

39:15.840 --> 39:17.760
that has spatially unique behavior

39:17.760 --> 39:19.720
in more than about 20 or so dimensions.

39:19.720 --> 39:22.240
Like in that case, it's hopeless.

39:22.240 --> 39:23.720
There's no question about it.

39:23.720 --> 39:26.680
He says that you can increase the data density,

39:26.680 --> 39:28.720
you know, like in 100 dimensions,

39:28.720 --> 39:32.120
but to get a grid with 10 points on a side, you know,

39:32.120 --> 39:34.200
which is to say 10 to the power of 100,

39:34.200 --> 39:36.240
there aren't enough protons in the universe

39:36.240 --> 39:37.880
that could convexly cover it.

39:37.880 --> 39:39.760
So, and Thomas concluded by saying,

39:39.760 --> 39:42.600
what this means is that everything that we do successfully

39:42.600 --> 39:45.280
approximate now with millions or billions

39:45.280 --> 39:47.120
or even trillions of data points,

39:47.160 --> 39:49.160
which accounts for a lot of the successes

39:49.160 --> 39:52.080
in machine learning, these data sets

39:52.080 --> 39:56.760
only have spatially novel behavior in very few dimensions,

39:56.760 --> 39:59.040
right, or a varying gradient in very few dimensions,

39:59.040 --> 40:00.240
16 or fewer.

40:00.240 --> 40:03.120
If we suppose that the true function has a varying gradient

40:03.120 --> 40:05.360
in more than that many dimensions,

40:05.360 --> 40:08.800
there's simply not enough data in the world to approximate it.

40:08.800 --> 40:11.880
The mathematics here is unequivocal.

40:13.120 --> 40:16.520
So, what actually happens when the test examples

40:16.560 --> 40:19.040
outside the convex hull of the training data?

40:19.040 --> 40:22.520
What happens when we're in an extrapolative regime?

40:22.520 --> 40:24.920
Well, it's increasingly impossible in high dimensions

40:24.920 --> 40:28.000
for the training set to be statistically representative

40:28.000 --> 40:29.120
of the test set.

40:29.120 --> 40:32.000
Test set instances will often distinctly differ

40:32.000 --> 40:34.120
from anything that we've seen during training.

40:34.120 --> 40:34.960
In machine learning,

40:34.960 --> 40:37.760
the test set will never fully characterize

40:37.760 --> 40:39.560
the problem that we're interested in.

40:39.560 --> 40:41.840
So, in high dimensional settings,

40:41.840 --> 40:44.200
features are often linearly correlated as well,

40:44.200 --> 40:47.160
which makes it challenging to know what information to use

40:47.160 --> 40:48.320
and what to discard.

40:48.320 --> 40:51.040
This is another fundamental problem of machine learning.

40:51.040 --> 40:54.440
Many machine learning approaches depend on modeling

40:54.440 --> 40:56.280
these local statistical relationships

40:56.280 --> 40:57.720
between the training samples,

40:57.720 --> 40:58.800
but in high dimensions,

40:58.800 --> 41:01.200
the probability of a new test example

41:01.200 --> 41:03.320
being in the convex hull of the training data

41:03.320 --> 41:06.000
goes to zero extremely quickly.

41:06.000 --> 41:08.360
So, making machine learning models work

41:08.360 --> 41:11.760
in an extrapolative regime necessitates the introduction

41:11.840 --> 41:13.480
of inductive biases,

41:13.480 --> 41:16.880
which are a way of adding a kind of enforced smoothness

41:16.880 --> 41:18.320
to the model predictions.

41:18.320 --> 41:21.520
So, if you hit the right bias,

41:21.520 --> 41:23.400
then it can be beneficial.

41:23.400 --> 41:26.440
If you impose the wrong bias,

41:26.440 --> 41:27.840
then it's gonna hurt you.

41:27.840 --> 41:29.280
And this is a well-known trade-off.

41:29.280 --> 41:32.840
So, of course, the whole endeavor of machine learning

41:32.840 --> 41:36.240
is defining the right inductive biases

41:36.240 --> 41:39.720
and leaving whatever you don't know to the data,

41:39.720 --> 41:42.680
and then basically learning to focus your models

41:42.680 --> 41:45.000
on the data that you're actually seeing.

41:45.000 --> 41:46.760
This is how we stop models

41:46.760 --> 41:48.600
from fitting the noise in the data

41:48.600 --> 41:50.440
or going completely haywire

41:50.440 --> 41:53.000
outside the convex hull of the training data.

41:53.000 --> 41:56.480
Extrapolation requires that sensible answers

41:56.480 --> 41:59.240
are given for all of the elements of the input space,

41:59.240 --> 42:02.360
even in regions not seen during training.

42:02.360 --> 42:05.040
And in particular, those within the convex hull

42:05.040 --> 42:07.160
of, let's say, some hypothetical,

42:07.200 --> 42:10.080
infinitely-sampled training data set.

42:10.080 --> 42:12.240
Now, what we do in neural networks

42:12.240 --> 42:14.200
and in many other machine learning algorithms,

42:14.200 --> 42:16.200
for that matter, is we just take a bunch

42:16.200 --> 42:19.720
of parameterized basis functions and we stack them

42:19.720 --> 42:21.880
and we fit them to our training data

42:21.880 --> 42:23.560
until it's well-described.

42:23.560 --> 42:26.400
The most important considerations for interpolation

42:26.400 --> 42:29.800
are the smoothness and robustness of these models.

42:29.800 --> 42:32.520
We want our models to produce gradual changes

42:32.520 --> 42:34.080
between the training points.

42:34.080 --> 42:35.600
Points outside the training set

42:35.600 --> 42:37.600
should be handled with great care,

42:37.600 --> 42:41.400
but what exactly that means depends greatly on the problem.

42:41.400 --> 42:44.200
In order to even know that we're in an extrapolative regime,

42:44.200 --> 42:45.800
the basis functions must be based

42:45.800 --> 42:48.120
near to the training examples.

42:48.120 --> 42:50.600
Typically, neural networks do not behave sensibly

42:50.600 --> 42:53.200
in regions where there's no training examples, right,

42:53.200 --> 42:54.960
because their learned basis functions

42:54.960 --> 42:57.520
have been localized around the training data

42:57.520 --> 42:59.440
in the ambient space.

42:59.440 --> 43:01.200
It's also worth noting that extrapolation

43:01.200 --> 43:03.920
and interpolation are two completely different regimes.

43:03.920 --> 43:06.240
Optimizing for one, typically,

43:06.240 --> 43:08.800
means being worse at the other.

43:08.800 --> 43:10.560
Now Randall said that the boundary view

43:10.560 --> 43:12.240
of neural networks is very clear

43:12.240 --> 43:13.840
in the discriminative setting.

43:13.840 --> 43:16.200
You only need interpolation to try

43:16.200 --> 43:18.160
and understand how neural networks work

43:18.160 --> 43:19.840
in the generative setting.

43:19.840 --> 43:21.120
This is a key point.

43:21.120 --> 43:24.440
In the generative setting, when you interpolate the latent,

43:24.440 --> 43:26.400
we see at first glance what looks

43:26.400 --> 43:28.240
like geometric morphing, right?

43:28.240 --> 43:30.480
Looking at this, you'd be forgiven for thinking

43:30.480 --> 43:33.120
that we're traversing some smooth latent manifold,

43:33.120 --> 43:36.280
but on closer inspection, it's not geometric

43:36.280 --> 43:37.560
or diffeomorphic at all.

43:37.560 --> 43:40.720
It's more of a fuzzy, high-resolution fade-in.

43:40.720 --> 43:43.280
Now, one commonly used visual argument

43:43.280 --> 43:44.680
for the manifold hypothesis

43:44.680 --> 43:48.560
is the MNIST digit interpolation in a generative model.

43:48.560 --> 43:50.800
Our own investigation shows that there's a degree

43:50.800 --> 43:53.080
of cherry picking in some of the visual examples

43:53.080 --> 43:54.480
used to demonstrate this.

43:54.480 --> 43:57.240
It's not actually hard to stumble across cases

43:57.240 --> 44:00.840
where purported manifold interpolation is no better

44:00.880 --> 44:03.280
than ambient-space linear interpolation.

44:03.280 --> 44:05.720
There are many examples where latent-space interpolation

44:05.720 --> 44:08.640
gives superposed and fuzzy intermediate representations

44:08.640 --> 44:12.200
and even crisp images, which are unrecognizable.

44:12.200 --> 44:14.440
The blur and cutting and gluing,

44:14.440 --> 44:16.360
which is apparent on these images,

44:16.360 --> 44:19.120
show that it's definitely not a diffeomorphism

44:19.120 --> 44:20.440
in the ambient space.

44:20.440 --> 44:22.120
And even though the interpolation path

44:22.120 --> 44:24.120
is continuous in the latent space,

44:24.120 --> 44:26.840
it's questionable whether that path is semantically relevant

44:26.840 --> 44:29.000
to the classification task at hand.

44:29.000 --> 44:31.120
I mean, just think of the mean value theorem.

44:31.120 --> 44:32.840
It tells you that if you take the value

44:32.840 --> 44:36.240
of any continuous function at two points, right,

44:36.240 --> 44:38.360
you can also find a point between them

44:38.360 --> 44:40.560
where the function hits the average of those values.

44:40.560 --> 44:42.000
So in other words,

44:42.000 --> 44:44.720
any continuous function produces a manifold,

44:44.720 --> 44:46.720
but that doesn't tell you anything interesting

44:46.720 --> 44:49.640
about the function beyond what we already know, right,

44:49.640 --> 44:50.840
that it's continuous.

44:51.840 --> 44:54.800
Now, I'm not so sure that these manifolds are even smooth.

44:54.800 --> 44:56.320
When we're talking about smoothness,

44:56.320 --> 44:58.400
we're only talking about local smoothness

44:58.440 --> 45:00.840
and inside the polyhedrus convex hole.

45:00.840 --> 45:02.320
It's linear, right?

45:02.320 --> 45:04.360
But there's no smooth surfaces or manifolds

45:04.360 --> 45:05.880
anywhere to be seen.

45:05.880 --> 45:07.840
For any particular node in a neural network,

45:07.840 --> 45:10.640
it's actually a linear function of a subset

45:10.640 --> 45:13.080
of the upstream linear functions, right?

45:13.080 --> 45:14.560
Very much like a decision tree,

45:14.560 --> 45:16.520
but with information sharing.

45:16.520 --> 45:18.640
Now, another often used visual aid

45:18.640 --> 45:20.280
is this idea that a neural network

45:20.280 --> 45:24.240
is effectively uncrumpling and smoothing the ambient space,

45:24.240 --> 45:26.600
much like you would do with a sheet of paper

45:26.600 --> 45:28.280
with successive transformations

45:28.280 --> 45:30.120
in the layers of the neural network.

45:30.120 --> 45:32.640
Keith and I previously agreed with this view of neural networks

45:32.640 --> 45:35.320
as progressively kind of flattening out the paper,

45:35.320 --> 45:37.520
but it now seems to us like they may instead

45:37.520 --> 45:40.480
be progressively inserting planes

45:40.480 --> 45:43.000
aligned to the facets of the paper ball

45:43.000 --> 45:46.680
to chop out locally affine polyhedra or cells

45:46.680 --> 45:49.320
to cover the paper's polyhedra.

45:49.320 --> 45:52.000
So for us, this is an entirely new way

45:52.000 --> 45:54.280
to think about neural networks, okay?

45:54.280 --> 45:57.320
So in a way, this is a new parlor trick, right?

45:57.320 --> 45:59.200
Do you remember that thing, the game of life,

45:59.200 --> 46:00.640
Conway's game of life?

46:00.640 --> 46:03.240
It looks like the shapes are smoothly morphing,

46:03.240 --> 46:06.480
but they're actually toggling pixels with discrete rules.

46:06.480 --> 46:09.240
Gary Marcus talks about the parlor trick of intelligence,

46:09.240 --> 46:11.680
but isn't it ironic that there are more parlor tricks

46:11.680 --> 46:13.520
going on than most people realize?

46:13.520 --> 46:16.520
Namely, that rather than doing smooth geometric morphing

46:16.520 --> 46:19.400
via interpolation, these networks are actually chopping up

46:19.400 --> 46:22.280
and composing linear polyhedra.

46:22.280 --> 46:25.120
Now, if you interpolate between two latent classes,

46:25.120 --> 46:28.760
it might traverse several polyhedra in the intermediate space.

46:28.760 --> 46:31.320
Along the way, it would pick up characteristics

46:31.320 --> 46:33.040
from all of those polyhedra.

46:33.040 --> 46:36.400
Some of the black regions are impossible regions,

46:36.400 --> 46:39.840
so it's not possible to get there from the latent space,

46:39.840 --> 46:41.520
which is very, very interesting.

46:41.520 --> 46:44.120
You only see the illusion of continuous morphing,

46:44.120 --> 46:47.920
where the neighboring cells are very small and very similar.

46:47.920 --> 46:50.560
This is Professor Michael Bronstein.

46:50.560 --> 46:52.720
I like to think of geometric deep learning

46:52.720 --> 46:56.560
as not a single method or architecture, but as a mindset.

46:56.560 --> 46:59.440
It's a way of looking at machine learning problems

46:59.440 --> 47:03.480
from the first principles of symmetry and invariance.

47:03.480 --> 47:07.600
And symmetry is a key idea that underpins our physical world

47:07.600 --> 47:10.920
and the data that is created by physical processes.

47:10.920 --> 47:13.560
And accounting for this structure

47:13.560 --> 47:15.800
allows us to beat the course of dimensionality

47:15.800 --> 47:17.840
in machine learning problems.

47:17.840 --> 47:19.440
So what is the manifold hypothesis?

47:19.440 --> 47:22.760
Well, natural data falls on smooth manifolds.

47:22.760 --> 47:25.160
The manifold hypothesis states that real-world

47:25.160 --> 47:29.000
high-dimensional data lie on low-dimensional manifolds,

47:29.000 --> 47:31.600
embedded in the high-dimensional space.

47:31.600 --> 47:33.360
When people invoke this hypothesis

47:33.360 --> 47:34.840
in a machine learning context,

47:34.840 --> 47:37.200
they're generally suggesting that neural networks

47:37.200 --> 47:39.560
are actually learning this data manifold,

47:39.560 --> 47:42.080
which we now find quite hard to believe, frankly.

47:42.080 --> 47:43.520
At best, we think they're learning

47:43.520 --> 47:45.400
some approximate aspects of it.

47:50.400 --> 47:52.320
So essentially, all machine learning problems

47:52.320 --> 47:53.840
that we need to deal with nowadays

47:53.840 --> 47:56.120
are extremely highly dimensional.

47:56.120 --> 47:58.800
Even basic image problems live in thousands

47:58.800 --> 48:01.040
or even millions of dimensions.

48:01.040 --> 48:03.080
Now, I think most people have this intuition

48:03.080 --> 48:06.080
of convex-hole membership, which is to say, in two dimensions.

48:06.080 --> 48:08.240
As you sample more and more training data,

48:08.240 --> 48:11.640
the convex-hole eventually fills the entire space.

48:11.640 --> 48:13.920
But the kicker is that in higher dimensions,

48:13.920 --> 48:17.240
the space is so vast, this will never happen.

48:17.760 --> 48:19.840
High-dimensional learning is impossible

48:19.840 --> 48:22.000
due to the curse of dimensionality.

48:22.000 --> 48:25.280
It only works if we make some very strong assumptions

48:25.280 --> 48:27.560
about the regularities in the space of functions

48:27.560 --> 48:29.120
that we need to search through.

48:29.120 --> 48:31.080
The classical assumptions that we make

48:31.080 --> 48:33.720
in machine learning are no longer appropriate.

48:33.720 --> 48:36.560
So in general, learning in high dimensions is intractable.

48:36.560 --> 48:38.600
The number of samples grows exponentially

48:38.600 --> 48:39.920
with the number of dimensions.

48:39.920 --> 48:41.640
And the curse of dimensionality refers

48:41.640 --> 48:43.680
to the various phenomena that arise

48:43.680 --> 48:46.040
when analyzing and organizing data

48:46.080 --> 48:49.080
in high-dimensional spaces that do not occur

48:49.080 --> 48:50.480
in low-dimensional settings,

48:50.480 --> 48:54.080
such as the three-dimensional physical reality

48:54.080 --> 48:56.240
of everyday experience.

48:56.240 --> 48:57.920
Now, the common theme of these problems

48:57.920 --> 49:00.800
is that when the dimensionality increases,

49:00.800 --> 49:03.720
the volume of the space increases so fast

49:03.720 --> 49:06.280
that the available data becomes sparse.

49:06.280 --> 49:09.200
And this sparsity is problematic for any method

49:09.200 --> 49:11.640
that requires statistical significance.

49:11.640 --> 49:14.440
In order to obtain a statistically sound

49:14.440 --> 49:16.840
and reliable result, the amount of data needed

49:16.840 --> 49:19.520
to support the result grows exponentially

49:19.520 --> 49:20.960
with the dimensionality.

49:20.960 --> 49:23.800
So the curse of dimensionality in a nutshell

49:23.800 --> 49:25.960
is that the probability of a new data point

49:25.960 --> 49:28.640
being inside the convex hull of your training data

49:28.640 --> 49:31.680
decreases exponentially with the number of dimensions.

49:31.680 --> 49:34.960
As an example, to estimate a standard normal density

49:34.960 --> 49:37.920
in 10 dimensions with a relative mean square error

49:37.920 --> 49:41.120
of 0.1 using an efficient non-parametric technique

49:41.120 --> 49:45.440
would require more than 800,000 samples.

49:45.440 --> 49:47.160
Now, in the last show,

49:47.160 --> 49:49.760
we discussed geometric deep learning in great detail

49:49.760 --> 49:52.360
and in some sense extrapolation

49:52.360 --> 49:54.600
in the curse of dimensionality are analogous.

49:54.600 --> 49:55.960
The reason that these folks wanted

49:55.960 --> 49:58.120
to combat the curse of dimensionality

49:58.120 --> 49:59.720
was they wanted to build models

49:59.720 --> 50:01.960
which could extrapolate outside the training range

50:01.960 --> 50:04.000
using geometrical priors.

50:04.000 --> 50:06.840
I think it's possible to build understanding

50:06.840 --> 50:08.640
of the curse of dimensionality

50:08.640 --> 50:12.880
through visual analogy to familiar lower dimensional shapes

50:12.880 --> 50:16.200
such as circles, squares, balls and cubes.

50:17.200 --> 50:20.560
Imagine perfectly sampling an entire space

50:20.560 --> 50:22.440
with a regular grid.

50:22.440 --> 50:26.040
This would partition space into squares or cubes

50:26.040 --> 50:30.440
or hyper cubes with a sample in the center of each.

50:31.480 --> 50:34.520
Now imagine around each sample,

50:34.520 --> 50:37.760
a disc or ball or hyper ball

50:37.760 --> 50:42.320
representing that point's region of nearness or influence.

50:42.320 --> 50:45.520
And let's ask how much of the total volume

50:45.520 --> 50:49.180
of that point's grid cell is actually near the sample?

50:50.240 --> 50:51.960
The answer is a fraction

50:51.960 --> 50:55.200
which diminishes faster than exponentially

50:55.200 --> 50:56.640
with increasing dimension.

50:57.640 --> 50:59.400
First think of the 2D case,

50:59.400 --> 51:02.520
a disc or two ball with diameter one

51:02.520 --> 51:06.560
inscribed in a square or two cube with sides of length one.

51:07.480 --> 51:11.040
In each corner we of course have a dart shaped chunk

51:11.040 --> 51:13.640
that isn't covered by the disc.

51:13.640 --> 51:15.040
A trick to think about this

51:15.040 --> 51:17.800
which I think will help in higher dimensions

51:17.800 --> 51:20.720
is to imagine scanning a line segment

51:20.720 --> 51:23.680
along one dimension from side to side.

51:24.520 --> 51:26.080
At the edge of the square,

51:26.080 --> 51:29.200
none of the segment is covered by the disc.

51:29.200 --> 51:30.440
At the very center,

51:30.440 --> 51:33.640
the entire segment is covered by the disc.

51:33.640 --> 51:37.080
And then as it scans towards the other edge of the square,

51:37.080 --> 51:39.240
the covered section begins shrinking

51:39.240 --> 51:41.520
and then rapidly falls to zero.

51:42.440 --> 51:47.400
The total coverage then is just the sum over that scan.

51:47.400 --> 51:48.480
In two dimensions,

51:48.480 --> 51:52.280
that is just the area of a disc with diameter one

51:52.280 --> 51:54.400
and is about 79%.

51:55.520 --> 51:57.400
Extending to three dimensions,

51:57.400 --> 51:59.960
imagine a ball in a cube

51:59.960 --> 52:03.760
and we scan a square from one face through the ball

52:03.760 --> 52:05.960
to the opposite face.

52:05.960 --> 52:07.920
As it passes the center,

52:07.920 --> 52:12.160
we have our familiar inscribed disc in a square.

52:12.160 --> 52:14.880
This is already missing the corner darts

52:14.880 --> 52:17.320
and as we scan towards the edge of the cube,

52:17.320 --> 52:20.320
the darts expand and surround the disc

52:20.320 --> 52:22.760
as it shrinks to zero.

52:22.760 --> 52:24.720
So by adding a third dimension,

52:24.720 --> 52:27.280
we've lost even more coverage

52:27.320 --> 52:32.320
and the volume of a diameter one ball is only 52%.

52:33.800 --> 52:36.880
In fact, if we continue on to higher dimension,

52:36.880 --> 52:39.680
the volume of a diameter one hyperball

52:39.680 --> 52:42.720
decays faster than exponential,

52:42.720 --> 52:45.880
factorially fast in fact.

52:45.880 --> 52:48.760
The volume decays so fast

52:48.760 --> 52:51.040
that even if we allowed for sampling

52:51.040 --> 52:55.560
with the best possible densely packed balls,

52:55.600 --> 52:58.800
theoretical work on hypersphere packing

52:58.800 --> 53:01.240
tells us that the volume occupied

53:01.240 --> 53:04.080
by optimally packed hyperballs

53:04.080 --> 53:07.760
would still decay at least exponentially

53:07.760 --> 53:10.160
with increasing dimensions.

53:10.160 --> 53:12.360
This is the curse.

53:13.280 --> 53:15.400
As dimensionality grows,

53:15.400 --> 53:18.680
space expands exponentially,

53:18.680 --> 53:21.240
points grow further apart

53:21.240 --> 53:24.820
and the volume near each point vanishes.

53:26.520 --> 53:29.040
In this episode, our guests argue this curse

53:29.040 --> 53:32.680
dunes traditional concepts of interpolation,

53:32.680 --> 53:35.560
even if we allow for the high dimensional

53:35.560 --> 53:39.360
transformative power of deep neural networks.

53:39.360 --> 53:41.680
Yeah, so the course of dimensionality,

53:41.680 --> 53:46.680
it refers generally to the inability of algorithms

53:50.160 --> 53:53.080
to keep certifying certain performance

53:53.080 --> 53:54.840
as the data becomes more complex

53:54.840 --> 53:56.360
and data becoming more complex here

53:56.360 --> 53:58.960
means that you have more and more dimensions,

53:58.960 --> 54:00.440
more and more pixels.

54:00.440 --> 54:05.440
And so this inability of like scaling,

54:05.720 --> 54:07.240
basically it's like it really says

54:07.240 --> 54:09.880
that if I scale up the input,

54:09.880 --> 54:11.560
my algorithm is gonna have more and more trouble

54:11.560 --> 54:12.760
to keep the pace.

54:12.760 --> 54:16.960
And so this curse can take different flavors, right?

54:16.960 --> 54:20.640
So this curse might have like a statistical reason

54:20.680 --> 54:25.480
in the sense that as I make my input space bigger,

54:26.640 --> 54:29.720
there would be many, many, many much exponentially more

54:30.800 --> 54:33.040
functions, real functions out there

54:33.040 --> 54:34.520
that would explain the training set

54:34.520 --> 54:37.520
that would basically pass through the training points.

54:37.520 --> 54:40.120
And so the more dimensions I add,

54:40.120 --> 54:43.480
the more uncertainty I have about the true function, right?

54:43.480 --> 54:45.680
So I would need more and more training samples

54:45.680 --> 54:47.040
to keep the pace.

54:47.040 --> 54:50.240
This curse can also be from the approximation side, right?

54:50.240 --> 54:52.280
So in the sense that the number of neurons

54:52.280 --> 54:55.440
that I'm considering to approximate my target function,

54:55.440 --> 54:57.840
I might need to keep adding more and more neurons

54:57.840 --> 55:00.840
at the rate that is exponentially in dimension.

55:00.840 --> 55:04.160
And the curse can also be from the computational side, right?

55:04.160 --> 55:07.040
The sense that if I keep adding parameters

55:07.040 --> 55:09.520
and parameters to my training model,

55:09.520 --> 55:13.880
I might have to optimize to solve an optimization problem

55:13.880 --> 55:16.280
that becomes exponentially harder.

55:16.280 --> 55:19.200
And so you can see that you are basically bombarded

55:19.200 --> 55:21.480
by three different, by all angles.

55:21.480 --> 55:25.160
And so an algorithm like here in the context

55:25.160 --> 55:28.280
of statistical learning or learning theory, if you want,

55:28.280 --> 55:30.960
having a kind of a theorem that would say,

55:30.960 --> 55:33.760
yes, I can promise you that you can learn,

55:33.760 --> 55:36.320
you need to actually solve these three problems at once, right?

55:36.320 --> 55:38.680
You need to be able to say that in their condition

55:38.680 --> 55:40.760
that you're studying, you have an algorithm

55:40.760 --> 55:42.760
that it does not suffer from approximation

55:42.760 --> 55:45.000
nor statistical nor computational crisis.

55:46.000 --> 55:48.160
So as you can imagine, it's very hard, right?

55:50.120 --> 55:53.200
Today is a big day here at Machine Learning Street Talk.

55:53.200 --> 55:55.200
We have invited one of the Godfathers

55:55.200 --> 55:56.920
of deep learning on the show,

55:56.920 --> 55:59.920
none other than Professor Yan Le-Kun.

55:59.920 --> 56:03.720
Machine learning royalty, he has met the chiefs AI scientists

56:03.720 --> 56:05.200
and a Turing Award winner,

56:05.200 --> 56:08.120
which I don't have to tell you is a big deal.

56:08.120 --> 56:11.720
Wiredly recognized as the Nobel Prize of Computing.

56:11.720 --> 56:16.720
Yan Le-Kun was born in the suburbs of Gay Pari in the 1960s.

56:17.640 --> 56:19.920
He received his PhD in computer science

56:19.920 --> 56:24.000
from the modern day Sir Vaughan University in 1987,

56:24.000 --> 56:27.760
during which he proposed an early form of back propagation,

56:27.760 --> 56:29.280
which is of course the backbone

56:29.280 --> 56:32.000
for training all neural networks.

56:32.000 --> 56:33.440
Whilst he was at Bell Labs,

56:33.440 --> 56:36.080
he invented convolutional neural networks,

56:36.080 --> 56:37.400
which again are the backbone

56:37.400 --> 56:41.080
of most major deep learning architectures in production today.

56:42.440 --> 56:45.520
He has been at New York University since 2003,

56:45.520 --> 56:47.920
where he is still the professor of computer science

56:47.920 --> 56:49.360
and neural science.

56:49.360 --> 56:52.160
Apart from being perhaps the most known researcher

56:52.160 --> 56:54.000
and main ambassador for deep learning,

56:54.000 --> 56:56.280
he has championed self-supervised learning

56:56.280 --> 56:58.200
and energy-based models.

56:58.200 --> 57:03.040
In 2013, he created the hugely prestigious ICLR conference

57:03.040 --> 57:04.720
with Yoshua Ben-Gio.

57:07.240 --> 57:09.920
A lover of the green screen IC.

57:09.920 --> 57:10.920
Well, normally,

57:11.680 --> 57:17.200
when I use Zoom, I put a substituted background.

57:17.200 --> 57:20.960
Well, Tim could do it for you and post, if you want.

57:20.960 --> 57:21.920
If you want.

57:21.920 --> 57:23.480
Well, it's never a very good cut-out.

57:23.480 --> 57:25.960
I did hack together some Python code to do it,

57:25.960 --> 57:28.480
but there's no substitute for the real thing.

57:29.360 --> 57:31.240
Yeah, I attempted to do this.

57:31.240 --> 57:33.760
I'm running Linux, so I attempted to do this

57:33.760 --> 57:37.240
by making a fake video driver,

57:37.240 --> 57:39.400
but introduces a little bit of delay, so.

57:41.920 --> 57:44.360
Professor Lacune, it's an absolute honour

57:44.360 --> 57:45.520
to have you on the show.

57:45.520 --> 57:49.920
When I first started MLST with Yanakin and Keith and Connor,

57:49.920 --> 57:51.920
we were discussing how long it might take

57:51.920 --> 57:55.280
to finally get the main man himself on the show.

57:55.280 --> 57:58.480
You've been a huge inspiration to all of us here on MLST

57:58.480 --> 58:01.120
and also you've inspired millions around the world

58:01.120 --> 58:04.760
to embark on successful careers in data science

58:04.760 --> 58:06.920
and to dream about what might be possible

58:06.920 --> 58:08.840
with artificial intelligence.

58:09.000 --> 58:11.080
Now, we've read your recent paper,

58:11.080 --> 58:14.480
Learning in High Dimensions Always Amounts to Extrapolation,

58:14.480 --> 58:16.440
which you co-authored with Randall,

58:16.440 --> 58:18.960
Belastriro and Jerome Pesente.

58:18.960 --> 58:20.760
Let's get straight into it.

58:20.760 --> 58:23.480
So, we were wondering,

58:23.480 --> 58:25.440
why did you write this paper, basically?

58:27.640 --> 58:31.680
We were thinking whether interpolation and extrapolation

58:31.680 --> 58:32.880
is a useful dichotomy.

58:32.880 --> 58:34.320
I mean, at the end of the day,

58:34.320 --> 58:36.600
we measure performance of learning methods

58:36.600 --> 58:39.000
with accepted metrics of predicted performance,

58:39.000 --> 58:40.160
such as accuracy.

58:40.160 --> 58:42.880
So, suppose everyone adopts your linear convention,

58:42.880 --> 58:45.280
which is a convex whole membership,

58:45.280 --> 58:47.400
and concludes that high-dimensional learning

58:47.400 --> 58:49.360
is always extrapolation.

58:49.360 --> 58:50.200
How is that useful?

58:50.200 --> 58:51.520
What do we do with this knowledge?

58:51.520 --> 58:54.200
And vice versa, suppose that everyone adopts

58:54.200 --> 58:55.720
a different definition

58:55.720 --> 58:58.840
and concludes that learning is always interpolation.

58:58.840 --> 59:00.640
What difference would that make

59:00.640 --> 59:02.760
in machine learning research and practice?

59:02.760 --> 59:05.880
So, in sum, why is it important to distinguish

59:05.920 --> 59:07.840
whether we're predicting by interpolation or extrapolation?

59:07.840 --> 59:10.960
Okay, very interesting question.

59:10.960 --> 59:12.320
So, first of all,

59:12.320 --> 59:16.360
this may be the first time you have me as an interviewee,

59:16.360 --> 59:17.920
but I've watched a bunch of your videos.

59:17.920 --> 59:20.000
So, you've had me as an audience, at least.

59:21.480 --> 59:23.520
Which I find really interesting, actually.

59:23.520 --> 59:26.760
So, the answer to your question is,

59:26.760 --> 59:28.240
the whole point of the paper is to show

59:28.240 --> 59:31.560
that this notion of interpolation versus extrapolation

59:31.560 --> 59:34.360
is not useful, essentially.

59:35.240 --> 59:37.960
That, you know, there's been a lot of people

59:39.000 --> 59:42.440
who've been sort of saying there's a limitation

59:42.440 --> 59:44.200
to deep learning, let's say,

59:44.200 --> 59:46.600
or machine learning more generally,

59:46.600 --> 59:48.520
because it's obvious that those things

59:48.520 --> 59:49.720
basically do curve fitting,

59:49.720 --> 59:51.320
and that only works for interpolation

59:51.320 --> 59:53.160
and not for extrapolation.

59:53.160 --> 59:58.160
And that kind of dismissal always sounded wrong to me

59:59.280 --> 01:00:01.200
because in high-dimension,

01:00:01.200 --> 01:00:02.920
your geometry in high-dimension is very different

01:00:02.920 --> 01:00:06.480
from the intuition that we form

01:00:06.480 --> 01:00:09.520
with curve fitting and low-dimension, right?

01:00:09.520 --> 01:00:12.040
So, part of the motivation for this paper

01:00:12.040 --> 01:00:14.360
is it was to kind of, you know,

01:00:14.360 --> 01:00:18.640
perhaps help some people gain some intuition

01:00:18.640 --> 01:00:21.520
about what really is taking place

01:00:21.520 --> 01:00:24.040
in machine learning and high-dimension.

01:00:24.040 --> 01:00:26.400
And also kind of, you know,

01:00:26.400 --> 01:00:28.280
dispel the myth, essentially,

01:00:28.280 --> 01:00:31.520
that machine learning and deep learning, in particular,

01:00:31.520 --> 01:00:33.120
only does interpolation.

01:00:33.120 --> 01:00:34.120
Of course, it depends a lot

01:00:34.120 --> 01:00:36.160
on your definition of interpolation.

01:00:36.160 --> 01:00:37.800
So here we adopted definition,

01:00:37.800 --> 01:00:41.920
which is, you know, an obvious and simple generalization

01:00:41.920 --> 01:00:45.640
of interpolation in low-dimension,

01:00:45.640 --> 01:00:46.760
which is that, you know,

01:00:46.760 --> 01:00:49.800
you interpolate when a point is in between the points

01:00:49.800 --> 01:00:50.640
you already know.

01:00:52.120 --> 01:00:55.320
And the generalization of this in high-dimension

01:00:55.320 --> 01:00:58.400
is, you know, you interpolate when a new point

01:00:58.400 --> 01:01:00.120
is inside the convex hull of the points

01:01:00.120 --> 01:01:03.440
that you already know.

01:01:03.440 --> 01:01:07.040
Now, what you have to realize, of course,

01:01:07.040 --> 01:01:08.000
is that in high-dimension,

01:01:08.000 --> 01:01:10.520
the volume of that space is actually tiny

01:01:10.520 --> 01:01:13.520
compared to the overall volume of space

01:01:13.520 --> 01:01:15.960
that would be filled, you know, in high-dimension.

01:01:15.960 --> 01:01:19.720
And so that's kind of the intuition behind this paper

01:01:19.720 --> 01:01:23.200
that, you know, any new point,

01:01:23.200 --> 01:01:27.160
regardless of how you sample it to some extent,

01:01:27.240 --> 01:01:29.760
any kind of reasonable ways to sample points,

01:01:31.560 --> 01:01:32.520
you know, in high-dimension,

01:01:32.520 --> 01:01:35.080
new points are always going to be outside the convex hull,

01:01:35.080 --> 01:01:36.920
almost always going to be outside the convex hull

01:01:36.920 --> 01:01:40.480
of existing points with that definition.

01:01:40.480 --> 01:01:41.320
Interesting.

01:01:41.320 --> 01:01:42.480
Now, there's a second part to your question.

01:01:42.480 --> 01:01:43.320
It's that, you know,

01:01:43.320 --> 01:01:45.280
is there a more sensible definition

01:01:45.280 --> 01:01:48.200
of interpolation and extrapolation?

01:01:48.200 --> 01:01:49.880
And the answer is probably yes, you know,

01:01:49.880 --> 01:01:51.360
the paper doesn't address it,

01:01:51.360 --> 01:01:56.360
but there are other definitions of hulls,

01:01:56.720 --> 01:01:58.600
if you want, they're not necessarily convex hulls,

01:01:58.600 --> 01:02:01.600
or they're not necessarily the usual type of convex hull.

01:02:01.600 --> 01:02:05.040
So for example, there is a definition of a hull

01:02:05.040 --> 01:02:06.280
for a cloud of points

01:02:06.280 --> 01:02:09.880
that would be the smallest hyperboloid,

01:02:10.640 --> 01:02:14.040
paraboloid, or ellipsoid, I should say, actually,

01:02:15.320 --> 01:02:17.880
the smallest ellipsoid that contains all the points, right?

01:02:17.880 --> 01:02:21.000
So some points are going to be on the surface

01:02:21.000 --> 01:02:23.880
of the ellipsoid, but most of them are going to be inside.

01:02:23.880 --> 01:02:26.480
And for this type of,

01:02:26.480 --> 01:02:29.600
and now your definition of interpolation is that,

01:02:29.600 --> 01:02:31.880
is a new point likely to be inside the ellipsoid

01:02:31.880 --> 01:02:33.520
of a previous point or outside?

01:02:34.720 --> 01:02:38.000
And the answer to that is probably very different

01:02:38.000 --> 01:02:39.880
from the one in the paper,

01:02:39.880 --> 01:02:41.880
in the sense that it's very likely

01:02:41.880 --> 01:02:43.600
for a lot of natural data,

01:02:43.600 --> 01:02:46.320
new points are likely to be inside

01:02:46.320 --> 01:02:49.000
the containing ellipsoid.

01:02:49.000 --> 01:02:51.920
So it very much depends on what you mean,

01:02:51.920 --> 01:02:53.920
but it's just that the notion of interpolation

01:02:53.920 --> 01:02:56.920
in high dimensional space or intuition

01:02:56.920 --> 01:02:59.680
are kind of biased toward low dimension

01:02:59.680 --> 01:03:02.280
and we have to be very careful what we say.

01:03:03.240 --> 01:03:05.440
So that was the main thing.

01:03:05.440 --> 01:03:10.440
And we have a bunch of tons of mathematicians

01:03:10.880 --> 01:03:12.960
that worked on these questions for many years

01:03:12.960 --> 01:03:15.520
and there's a whole bunch of theorems about this

01:03:15.520 --> 01:03:17.800
that we survey in the paper.

01:03:18.800 --> 01:03:20.160
Very interesting.

01:03:20.160 --> 01:03:22.640
Well, we'll dig more into that in a second,

01:03:22.640 --> 01:03:25.560
but folks like Gary Marcus make the case

01:03:25.560 --> 01:03:29.520
that deep learning models don't reason and only interpolate.

01:03:29.520 --> 01:03:31.800
Now, I know you disagree vehemently,

01:03:31.800 --> 01:03:35.000
but my intuition is that reasoning and extrapolation

01:03:35.000 --> 01:03:37.240
are somewhat synonymous.

01:03:37.240 --> 01:03:40.800
Is it, by arguing that deep learning models extrapolate,

01:03:40.800 --> 01:03:44.160
are you kind of making the argument that they reason as well?

01:03:44.160 --> 01:03:45.720
Because that would make a strong case

01:03:45.720 --> 01:03:47.280
that deep learning models could scout

01:03:47.280 --> 01:03:48.400
artificial general intelligence.

01:03:48.400 --> 01:03:50.880
Okay, so no, the short answer is no.

01:03:51.960 --> 01:03:56.960
But there are important questions in there.

01:03:57.960 --> 01:04:00.560
First of all, what's our definition of reasoning?

01:04:01.960 --> 01:04:05.280
What is the process by which we elaborate models

01:04:05.280 --> 01:04:07.600
and is there a qualitative difference

01:04:07.600 --> 01:04:12.520
between a models that merely performs curve feeding

01:04:12.520 --> 01:04:14.920
as we normally know it

01:04:14.920 --> 01:04:17.720
and a model that has a,

01:04:17.720 --> 01:04:21.680
let's say to adopt a terminology that others have proposed

01:04:22.840 --> 01:04:25.840
that models that establish sort of a causal model

01:04:25.840 --> 01:04:27.520
of the data you're observing,

01:04:28.960 --> 01:04:31.000
which can be the basis for reasoning

01:04:31.000 --> 01:04:32.840
and things like that, right?

01:04:34.480 --> 01:04:36.840
And the answer to this is probably no.

01:04:36.840 --> 01:04:38.560
There is a difference, of course,

01:04:38.560 --> 01:04:41.080
but is it an essential qualitative difference?

01:04:41.080 --> 01:04:42.840
I'm not entirely sure.

01:04:42.880 --> 01:04:45.440
And then there is the argument,

01:04:45.440 --> 01:04:46.920
if there is a qualitative difference,

01:04:46.920 --> 01:04:48.120
which I'm not sure about,

01:04:49.320 --> 01:04:53.200
would this qualitative difference be in the form of

01:04:53.200 --> 01:04:55.640
fundamentally different things from deep learning,

01:04:55.640 --> 01:04:58.920
things that are like discrete symbolic reasoning

01:04:58.920 --> 01:05:00.720
or things of that type?

01:05:00.720 --> 01:05:02.920
And to that, my answer is clearly no.

01:05:02.920 --> 01:05:04.680
I do not believe that's the case.

01:05:05.760 --> 01:05:08.320
So I think reasoning is certainly,

01:05:08.320 --> 01:05:09.800
I always list in my talks,

01:05:10.800 --> 01:05:13.800
the ability, giving the ability to learning machines

01:05:13.800 --> 01:05:16.600
to reason is one of the main challenges

01:05:16.600 --> 01:05:21.600
of the next decade or perhaps couple of decades in AI.

01:05:21.880 --> 01:05:24.920
So I'm clearly aware of the fact

01:05:24.920 --> 01:05:26.840
that they don't do this very well at the moment.

01:05:26.840 --> 01:05:28.360
The big question I think is,

01:05:28.360 --> 01:05:31.920
how do we get machines to reason

01:05:31.920 --> 01:05:34.240
in ways that are compatible with deep learning?

01:05:34.240 --> 01:05:36.320
Because most of the criticism

01:05:36.320 --> 01:05:39.480
that I've heard from Gary Marcus and several others

01:05:39.520 --> 01:05:41.400
towards deep learning is not a criticism

01:05:41.400 --> 01:05:42.440
towards deep learning.

01:05:42.440 --> 01:05:44.840
It's a criticism towards supervised learning.

01:05:44.840 --> 01:05:46.680
And I agree with them.

01:05:46.680 --> 01:05:48.880
Supervised learning sucks.

01:05:48.880 --> 01:05:51.440
I mean, it's very limited in the sense that

01:05:51.440 --> 01:05:54.720
you can train machines to do very specific tasks.

01:05:54.720 --> 01:05:57.200
And because they're trying to do very specific tasks,

01:05:57.200 --> 01:05:59.160
they're going to use all the biases that are in the data

01:05:59.160 --> 01:06:01.280
to do that task.

01:06:01.280 --> 01:06:04.080
And if you try to get outside of that task,

01:06:04.080 --> 01:06:05.520
they're not gonna perform very well.

01:06:05.520 --> 01:06:07.480
That's the limitation of supervised learning.

01:06:07.480 --> 01:06:10.640
It has absolutely nothing to do with deep learning.

01:06:11.560 --> 01:06:15.200
So regardless of which learning techniques you're gonna use,

01:06:15.200 --> 01:06:16.640
you're gonna have that problem.

01:06:16.640 --> 01:06:18.600
It's a problem with supervised learning.

01:06:18.600 --> 01:06:21.920
So I take exception with the confusion

01:06:21.920 --> 01:06:23.400
between deep learning and supervised learning.

01:06:23.400 --> 01:06:25.040
Now, of course today, most of supervised learning

01:06:25.040 --> 01:06:26.200
is deep learning,

01:06:26.200 --> 01:06:29.000
but it's the limitation of supervised learning.

01:06:29.000 --> 01:06:32.040
And as you probably know,

01:06:32.040 --> 01:06:36.920
I've been a very strong advocate of self-supervised learning,

01:06:36.920 --> 01:06:41.160
sort of moving away from task-specific supervised learning

01:06:41.160 --> 01:06:43.080
towards more kind of generic learning

01:06:43.080 --> 01:06:45.920
followed by specialization,

01:06:45.920 --> 01:06:48.760
using supervised or reinforcement learning.

01:06:48.760 --> 01:06:53.680
And in that, I've kind of followed the path of Jeff Hinton,

01:06:53.680 --> 01:06:58.680
who's been basically advocating for this for 40 years now.

01:07:00.520 --> 01:07:03.120
And for me, it's less time.

01:07:03.120 --> 01:07:06.040
I disagreed with him originally and changed my mind

01:07:06.720 --> 01:07:08.120
about 20 years ago.

01:07:08.120 --> 01:07:13.120
So that's the story, really.

01:07:13.480 --> 01:07:16.400
It's awesome that you have the ability to change your mind

01:07:16.400 --> 01:07:18.720
and admit that this is a good thing.

01:07:18.720 --> 01:07:20.840
We run into so many people that are just,

01:07:20.840 --> 01:07:23.040
they don't wanna change their mind no matter what.

01:07:23.040 --> 01:07:26.440
It's seen as a bad thing, which it isn't at all.

01:07:26.440 --> 01:07:30.040
No, I mean, I think the essence of being a scientist

01:07:30.040 --> 01:07:35.040
is to be able to change your mind in the face of evidence.

01:07:35.680 --> 01:07:39.760
You cannot be a scientist if you have preconceived ideas.

01:07:39.760 --> 01:07:42.600
On the other hand, I've also been known

01:07:43.640 --> 01:07:47.120
to hold very tight to ideas that I thought were true

01:07:47.120 --> 01:07:51.600
in the face of considerable differing opinion

01:07:51.600 --> 01:07:52.840
from my dear colleague.

01:07:52.840 --> 01:07:57.840
So it also helps to have deeply held convictions sometimes.

01:08:00.040 --> 01:08:04.280
So to this notion of interpolation,

01:08:04.320 --> 01:08:07.440
I think we've also, and you mentioned this, right?

01:08:07.440 --> 01:08:08.920
It depends on your definition.

01:08:08.920 --> 01:08:12.440
And we've also had a little bit of the feeling

01:08:12.440 --> 01:08:15.520
that people might be talking past one another

01:08:15.520 --> 01:08:17.920
when they criticize, can it interpolate?

01:08:17.920 --> 01:08:19.680
Can't it interpolate?

01:08:19.680 --> 01:08:21.240
And we've come up with this example

01:08:21.240 --> 01:08:24.440
of I give a classifier a dog.

01:08:24.440 --> 01:08:27.520
And that dog is like the most doggy dog I've ever seen.

01:08:27.520 --> 01:08:29.440
Like it's like such a dog.

01:08:29.440 --> 01:08:32.880
It's more dog than any dog I had in the training dataset.

01:08:32.920 --> 01:08:37.480
So clearly that dog is like outside of the training distribution,

01:08:37.480 --> 01:08:40.200
outside of the convex hull in any space,

01:08:40.200 --> 01:08:44.840
like be that the original space, be that the latent space.

01:08:44.840 --> 01:08:48.760
It's like, all that matters is that it's at the correct side

01:08:48.760 --> 01:08:51.360
of the classifying hyperplane.

01:08:51.360 --> 01:08:55.440
So that would not be contained on sort of in the convex hull.

01:08:55.440 --> 01:08:58.760
It would not be contained in the smallest ellipsoid

01:08:58.760 --> 01:08:59.760
and so on.

01:08:59.760 --> 01:09:04.040
So do you think when people talk about interpolation,

01:09:04.040 --> 01:09:09.160
they might be talking about something maybe different?

01:09:09.160 --> 01:09:10.720
Because you can also make the example

01:09:10.720 --> 01:09:13.600
if I take the convex hull of all the training data points

01:09:13.600 --> 01:09:15.800
and I take a point in the middle of it,

01:09:15.800 --> 01:09:18.560
it's the neural network going to be pretty bad at it, right?

01:09:18.560 --> 01:09:22.120
That's going to be like a messy blur of pixels

01:09:22.120 --> 01:09:25.280
and it's not going to be very, very good at it.

01:09:25.280 --> 01:09:32.240
So could you maybe make the strongest argument you could

01:09:32.240 --> 01:09:37.240
for people saying neural networks just interpolate,

01:09:37.240 --> 01:09:41.400
but what notion of interpolation would you substitute?

01:09:41.400 --> 01:09:45.080
Would it be like, oh, they just nearest neighbor classify

01:09:45.080 --> 01:09:51.360
or if you had to give the best shot at sort of doing the,

01:09:51.360 --> 01:09:54.640
oh, neural networks just do something, what would it be?

01:09:55.520 --> 01:09:57.160
I wouldn't give any answer of this type

01:09:57.160 --> 01:10:02.640
because the answer would very heavily depend on the architecture.

01:10:02.640 --> 01:10:09.800
So for example, if most of the layers in a neural net,

01:10:09.800 --> 01:10:12.040
I mean, you can use weighted sums and sigmoids

01:10:12.040 --> 01:10:13.480
or weighted sums and values, right?

01:10:13.480 --> 01:10:19.040
And that effectively performs a classification separating

01:10:19.040 --> 01:10:24.680
the input into kind of half, two halves with the hyperplanes.

01:10:24.680 --> 01:10:26.680
But you could also use a Euclidean distance unit.

01:10:26.680 --> 01:10:28.600
So Euclidean distance unit computes the distance

01:10:28.600 --> 01:10:31.640
of the input vector to a weight vector,

01:10:31.640 --> 01:10:33.680
which is not a weight vector anymore,

01:10:33.680 --> 01:10:35.840
and then passes it through some sort of decreasing function

01:10:35.840 --> 01:10:37.640
like an exponential.

01:10:37.640 --> 01:10:41.040
And what that gives you is a Gaussian bump in input space

01:10:41.040 --> 01:10:43.840
where the activity will be high

01:10:43.840 --> 01:10:47.640
if the input is close to the vector and low if it's not.

01:10:47.640 --> 01:10:50.520
When you have an attention layer,

01:10:50.520 --> 01:10:54.640
where you have a whole bunch of those kind of vector comparison

01:10:54.640 --> 01:10:56.960
where the vectors are normalized and then you do a softmax,

01:10:56.960 --> 01:11:00.640
you're basically doing sort of a multinomial version of this.

01:11:00.640 --> 01:11:03.640
And this is using transformers, right?

01:11:03.640 --> 01:11:05.880
And in all kinds of architectures these days.

01:11:05.880 --> 01:11:09.760
So those things that compare vectors with each other

01:11:09.760 --> 01:11:13.200
and only react to the two vectors nearby

01:11:13.200 --> 01:11:16.600
do a sort of glorified nearest neighbor

01:11:16.600 --> 01:11:17.960
with some interpolation.

01:11:17.960 --> 01:11:25.520
By the way, most kernel-based methods do this, right?

01:11:25.520 --> 01:11:30.120
Kernel methods basically are one layer of such PRS comparisons.

01:11:30.120 --> 01:11:33.240
I mean comparisons of the input with the training samples.

01:11:33.240 --> 01:11:35.120
And then you pass that through some function,

01:11:35.120 --> 01:11:36.960
some response function.

01:11:36.960 --> 01:11:40.480
So Gaussian SVMs, for example, are the perfect example of this.

01:11:40.480 --> 01:11:42.920
And then you take those cores and you compute a linear combination.

01:11:42.920 --> 01:11:46.800
That's glorified interpolating nearest neighbor.

01:11:46.800 --> 01:11:48.840
To some extent, transformers do this as well.

01:11:48.840 --> 01:11:52.880
Transformers are the basic module of a transformer.

01:11:52.880 --> 01:11:57.800
It's basically an associative memory that compares the incoming vector

01:11:57.800 --> 01:12:00.880
to a bunch of keys and then gives you an answer

01:12:00.880 --> 01:12:03.360
that is some linear combination of values, right?

01:12:03.360 --> 01:12:07.640
So that is a good idea.

01:12:07.640 --> 01:12:09.480
It learns fast.

01:12:09.480 --> 01:12:12.640
There used to be a whole series of models that were now forgotten,

01:12:12.640 --> 01:12:15.960
that are now forgotten in the 90s called RBF networks.

01:12:15.960 --> 01:12:18.480
So an RBF network was basically a two-layer neural net.

01:12:18.480 --> 01:12:20.480
Well, the first layer was very much like an SVM.

01:12:20.480 --> 01:12:23.520
This was before SVMs that, again,

01:12:23.520 --> 01:12:26.320
had riddle-based functions, responses, right?

01:12:26.320 --> 01:12:30.000
Comparing an input to vectors and passing it to an exponential

01:12:30.000 --> 01:12:31.960
or something like this.

01:12:31.960 --> 01:12:33.840
And then you would initialize the first layer.

01:12:33.840 --> 01:12:34.960
You could train it with backprop,

01:12:34.960 --> 01:12:36.360
but it would get stuck in local minima.

01:12:36.360 --> 01:12:38.160
So that wasn't a good idea.

01:12:38.160 --> 01:12:42.600
You had to initialize the first layer with something like k-means

01:12:42.600 --> 01:12:45.720
or mixer or Gaussian or something like that.

01:12:45.720 --> 01:12:48.680
And then you could either just train the second layer

01:12:48.680 --> 01:12:52.040
or fine-tune the entire thing with backprop.

01:12:52.040 --> 01:12:53.040
And that worked pretty well.

01:12:53.040 --> 01:12:57.280
It was actually a fairly fast learner.

01:12:57.280 --> 01:12:59.840
They were faster than the neural nets.

01:12:59.840 --> 01:13:01.880
So those things, for those things,

01:13:01.880 --> 01:13:04.280
the answer to your question is of one type.

01:13:04.280 --> 01:13:08.200
They're basically doing interpolation with kernels.

01:13:08.200 --> 01:13:13.280
And it's very much like a smooth version of nearest neighbors.

01:13:13.280 --> 01:13:15.240
But then for classical neural nets,

01:13:15.240 --> 01:13:17.240
where you have either a hyperboleic tantrum,

01:13:17.240 --> 01:13:19.800
nonlinearity, or a value, or something of that type,

01:13:19.800 --> 01:13:23.080
something with a kink in it, or two kinks,

01:13:23.080 --> 01:13:24.480
the answer is different.

01:13:24.480 --> 01:13:27.800
There it's a whole cone of response

01:13:27.800 --> 01:13:32.600
that will produce a positive response versus not

01:13:32.600 --> 01:13:34.720
if you take a combination of units, right?

01:13:34.720 --> 01:13:39.520
So does it make sense to talk about interpolation

01:13:39.520 --> 01:13:40.520
in that kind of geometry?

01:13:40.520 --> 01:13:42.000
I'm not sure.

01:13:42.000 --> 01:13:44.640
I think people should just not use

01:13:44.640 --> 01:13:48.680
the word interpolation for that situation.

01:13:48.680 --> 01:13:54.720
But the fact that kind of things in a neural net,

01:13:54.720 --> 01:13:56.120
the response of a neural net actually

01:13:56.120 --> 01:14:01.320
are kind of half-spaced, extend beyond the training points,

01:14:01.320 --> 01:14:03.680
perhaps has something to do with the fact

01:14:03.680 --> 01:14:06.120
that they do extrapolate in certain ways

01:14:06.120 --> 01:14:08.400
that may or may not be relevant for the problem,

01:14:08.400 --> 01:14:11.640
but they do extrapolate.

01:14:11.640 --> 01:14:14.520
Well, so we've taught some about the interpolation

01:14:14.520 --> 01:14:16.760
versus extrapolation versus what was in the paper,

01:14:16.760 --> 01:14:19.680
because the paper is somewhat a rigid definition.

01:14:19.680 --> 01:14:21.720
It's like there's this convex hull,

01:14:21.720 --> 01:14:24.040
and if you're inside, it's interpolation.

01:14:24.040 --> 01:14:26.840
If you're outside, it's extrapolation.

01:14:26.840 --> 01:14:29.920
And you talked about maybe we could use a ball and ellipsoid

01:14:29.920 --> 01:14:32.000
instead, but there's kind of a key thing there,

01:14:32.000 --> 01:14:35.200
which is that it's going across all dimensions.

01:14:35.200 --> 01:14:39.240
So you're inside the convex hull.

01:14:39.240 --> 01:14:43.120
It's a necessary condition that on every single dimension,

01:14:43.120 --> 01:14:46.200
your sample data point falls within the range

01:14:46.200 --> 01:14:47.560
of the training data.

01:14:47.560 --> 01:14:49.680
We could kind of go the opposite extreme

01:14:49.680 --> 01:14:53.800
and say that you're interpolating if any dimension falls

01:14:53.800 --> 01:14:55.840
within the training domain, or rather,

01:14:55.840 --> 01:15:00.120
you're extrapolating only if every single dimension falls

01:15:00.120 --> 01:15:01.360
outside the training range.

01:15:01.360 --> 01:15:04.600
And both of those are kind of these exponential extremes.

01:15:04.600 --> 01:15:07.120
And it would seem like the truth is maybe somewhere

01:15:07.120 --> 01:15:09.520
in between, like there's a subset of dimensions

01:15:09.520 --> 01:15:13.400
that might be salient for any particular data point.

01:15:13.400 --> 01:15:16.120
So why are we kind of using this very exponential extreme

01:15:16.120 --> 01:15:17.560
definition?

01:15:17.560 --> 01:15:18.880
It was still exponential.

01:15:18.880 --> 01:15:21.080
Even if you, you know, you can try

01:15:21.080 --> 01:15:23.720
to divide the set of dimensions into the ones that are useful

01:15:23.720 --> 01:15:26.560
and the ones that are just going to use parameters that

01:15:26.560 --> 01:15:31.480
are useless, but that are not relevant for the task at hand.

01:15:31.480 --> 01:15:33.400
But first of all, that task is very difficult.

01:15:33.400 --> 01:15:35.880
I mean, basically, the entire machine learning problem

01:15:35.920 --> 01:15:38.320
is exactly that, trying to figure out

01:15:38.320 --> 01:15:41.040
what information in the input is relevant for the task

01:15:41.040 --> 01:15:44.000
and what part should basically be considered

01:15:44.000 --> 01:15:46.920
as noise or useless parameters.

01:15:46.920 --> 01:15:49.160
So solving that problem is solving the machine learning

01:15:49.160 --> 01:15:50.360
problem, first of all.

01:15:50.360 --> 01:15:54.000
Second of all, what we show in the paper is that regardless,

01:15:54.000 --> 01:15:58.120
so the experiment that Randall did, as a matter of fact,

01:15:58.120 --> 01:16:04.200
is train a ResNet 18 or 50 or whatever on a ResNet

01:16:04.200 --> 01:16:05.680
or CIFAR or MNIST.

01:16:05.680 --> 01:16:08.240
And then take the output representation

01:16:08.240 --> 01:16:16.120
and see whether this sort of exponential growth of number

01:16:16.120 --> 01:16:19.440
of data points to stay within the interpolation regime

01:16:19.440 --> 01:16:20.040
still exists.

01:16:20.040 --> 01:16:21.800
And it still exists.

01:16:21.800 --> 01:16:24.400
It's just that the dimension now, instead of being

01:16:24.400 --> 01:16:28.120
the input dimension of the data, which may be very large,

01:16:28.120 --> 01:16:30.240
is the dimension of the embedding.

01:16:30.240 --> 01:16:33.320
But as soon as the dimension of the embedding

01:16:33.320 --> 01:16:37.400
is larger than 20 or so, the number of training samples

01:16:37.400 --> 01:16:41.680
you would need to stay within the interpolation regime

01:16:41.680 --> 01:16:44.520
is already going to be very large.

01:16:44.520 --> 01:16:47.400
2 to the 20 is a large number.

01:16:47.400 --> 01:16:50.760
So there is another experiment that

01:16:50.760 --> 01:16:52.720
shows that if your entire data set is

01:16:52.720 --> 01:16:55.440
contained within a linear subspace,

01:16:55.440 --> 01:16:59.520
so the ambient space may be of dimension 100,

01:16:59.520 --> 01:17:03.040
but the entire data set is within a linear subspace

01:17:03.040 --> 01:17:05.360
of dimension 4, then what matters

01:17:05.360 --> 01:17:09.120
is the dimension 4, not the dimension 100.

01:17:09.120 --> 01:17:16.920
So automatically, that convex hull process, what

01:17:16.920 --> 01:17:19.000
matters to it is not the dimension of the input space,

01:17:19.000 --> 01:17:22.080
it's the dimension of the linear subspace that

01:17:22.080 --> 01:17:24.760
contains all the points.

01:17:24.760 --> 01:17:26.800
Right, but I think what I'm saying though

01:17:26.800 --> 01:17:31.000
is that you can invert the definition so that whether or not

01:17:31.040 --> 01:17:33.840
you're extrapolating just becomes 1 minus,

01:17:33.840 --> 01:17:36.160
whether or not you're interpolating, or vice versa.

01:17:36.160 --> 01:17:38.920
And so you can wind up with an equally extreme definition

01:17:38.920 --> 01:17:42.760
that concludes everything is interpolation.

01:17:42.760 --> 01:17:46.160
So I'm just wondering, it seems like there should be a balance.

01:17:46.160 --> 01:17:48.000
And this kind of gets to what you said earlier.

01:17:48.000 --> 01:17:53.120
Is there a more useful definition of interpolation

01:17:53.120 --> 01:17:56.800
versus extrapolation that could have some utility

01:17:56.800 --> 01:17:57.840
for machine learning?

01:17:57.840 --> 01:18:00.880
I mean, it's not clear you would get much out of it.

01:18:01.320 --> 01:18:03.240
I give you a potential candidate,

01:18:03.240 --> 01:18:07.760
which is whether the points are contained in a ellipsoid that

01:18:07.760 --> 01:18:10.080
contains all the points, you could make it a sphere.

01:18:10.080 --> 01:18:13.360
A sphere would be, of course, bigger volume

01:18:13.360 --> 01:18:15.360
because the data doesn't necessarily

01:18:15.360 --> 01:18:18.280
have the same radius in all dimensions.

01:18:18.280 --> 01:18:22.920
But I think the result would be fairly similar for both,

01:18:22.920 --> 01:18:28.560
for both definitions that in that case,

01:18:28.560 --> 01:18:30.120
things would be mostly interpolation.

01:18:30.120 --> 01:18:32.800
But that would be kind of a weird definition

01:18:32.800 --> 01:18:37.280
of interpolation in the sense that it

01:18:37.280 --> 01:18:40.480
would rely on a false intuition about high dimensional

01:18:40.480 --> 01:18:42.200
geometry.

01:18:42.200 --> 01:18:46.640
OK, calling this interpolation basically

01:18:46.640 --> 01:18:49.200
would mean that you have a completely wrong idea about how

01:18:49.200 --> 01:18:50.840
things behave in high dimension, right,

01:18:50.840 --> 01:18:54.200
about geometry in high dimension.

01:18:54.200 --> 01:18:55.760
I'm trying to get some intuition about this

01:18:55.760 --> 01:18:59.360
because we spoke to Professor Bronstein and his friends,

01:18:59.360 --> 01:19:01.920
including Joanne Bruner, and they're

01:19:01.920 --> 01:19:06.120
talking about geometric learning and their approach

01:19:06.120 --> 01:19:07.920
to defeating the curse of dimensionality

01:19:07.920 --> 01:19:11.320
is finding geometric priors to reduce the hypothesis space,

01:19:11.320 --> 01:19:12.400
which is quite interesting.

01:19:12.400 --> 01:19:16.000
A lot of this is about our intuitions of interpolation.

01:19:16.000 --> 01:19:18.640
Because if I take an autoencoder and I train it on MNIST

01:19:18.640 --> 01:19:21.360
and I start interpolating between the train

01:19:21.360 --> 01:19:24.360
or the test examples, it is learning this continuous

01:19:24.360 --> 01:19:25.560
geometric morphing.

01:19:25.560 --> 01:19:28.840
And that is what people's intuition is about interpolation.

01:19:28.840 --> 01:19:32.280
I know you would say that's extrapolation, right?

01:19:32.280 --> 01:19:35.080
Yeah, no, I think it would be a type of interpolation,

01:19:35.080 --> 01:19:37.240
but it would be some sort of geodesic interpolation,

01:19:37.240 --> 01:19:40.120
interpolation on a manifold, right?

01:19:40.120 --> 01:19:42.000
I mean, so certainly if you have some idea

01:19:42.000 --> 01:19:44.720
about the structure of the data manifold,

01:19:44.720 --> 01:19:47.080
you can interpolate within that manifold

01:19:47.080 --> 01:19:50.480
without making big mistakes.

01:19:50.480 --> 01:19:53.280
But then you're back to the original problem

01:19:53.280 --> 01:19:53.800
of machine learning.

01:19:53.800 --> 01:19:54.720
What is that manifold?

01:19:54.720 --> 01:19:55.880
How do you learn that manifold?

01:19:55.880 --> 01:19:59.320
That's one of the essential problems of machine learning.

01:19:59.320 --> 01:20:01.600
And learning the structure of a data manifold

01:20:01.600 --> 01:20:06.240
is a much more complex problem than learning a task

01:20:06.240 --> 01:20:08.920
of classifying objects on that manifold.

01:20:08.920 --> 01:20:11.800
For example, classifying points on that manifold.

01:20:11.800 --> 01:20:17.200
So there is this old adage by which

01:20:17.200 --> 01:20:22.320
I used to be a big fan of, which is why I was

01:20:22.320 --> 01:20:24.200
disagreeing with Jeff Hinton about the usefulness

01:20:24.200 --> 01:20:26.480
of unsupervised learning.

01:20:26.480 --> 01:20:32.200
Because if you have a task at hand for which you have data

01:20:32.200 --> 01:20:35.200
that you can use to train a supervised system,

01:20:35.200 --> 01:20:41.920
why would you go to the trouble of pre-training a system

01:20:41.920 --> 01:20:46.200
in unsupervised mode knowing that the unsupervised learning

01:20:46.200 --> 01:20:49.840
problem is considerably more complicated both from every

01:20:49.840 --> 01:20:52.920
aspect you can think of, certainly

01:20:52.920 --> 01:20:55.320
from the theoretical point of view.

01:20:55.320 --> 01:21:00.040
Vladimir Vapnik actually has kind of a similar opinion.

01:21:00.040 --> 01:21:03.080
One of the few things that he and I agree on,

01:21:03.080 --> 01:21:05.840
or agreed on, at least, which is why would you

01:21:05.840 --> 01:21:08.280
want to solve a more complex problem than you have to?

01:21:08.280 --> 01:21:10.520
But of course, that forgets the fact

01:21:10.520 --> 01:21:12.680
that you don't want to solve a single problem.

01:21:12.680 --> 01:21:16.880
You want to take advantage of multiple problems

01:21:16.880 --> 01:21:23.920
and whatever data you have available at your disposal

01:21:23.920 --> 01:21:26.800
to prepare for learning a task.

01:21:26.800 --> 01:21:34.520
And we have access to considerably more unlabeled data

01:21:34.520 --> 01:21:37.440
than we have access to labeled data.

01:21:37.440 --> 01:21:42.320
And therefore, why not use unlabeled data in large quantity

01:21:42.320 --> 01:21:44.240
to pre-train very large neural nets

01:21:44.240 --> 01:21:46.120
so that we can function them for the tasks

01:21:46.520 --> 01:21:47.720
that we are interested in?

01:21:47.720 --> 01:21:49.480
So that's the whole idea of self-supervised learning.

01:21:49.480 --> 01:21:52.720
And of course, we all know that that kind of strategy has

01:21:52.720 --> 01:21:56.160
been unbelievably successful in natural language

01:21:56.160 --> 01:22:00.760
processing with denoising autoencoder or master

01:22:00.760 --> 01:22:04.960
autoencoder or bird-style training of transformers

01:22:04.960 --> 01:22:08.200
followed by a supervised phase.

01:22:08.200 --> 01:22:10.440
That success has not yet translated

01:22:10.440 --> 01:22:14.880
in the domain of vision, although I'm sort of predicting

01:22:14.880 --> 01:22:17.280
that it will happen very soon.

01:22:17.280 --> 01:22:22.200
But I mean, there's a lot of interesting avenues there

01:22:22.200 --> 01:22:25.120
and recent progress.

01:22:25.120 --> 01:22:27.680
And then there is the cake analogy, right?

01:22:27.680 --> 01:22:32.720
The fact that any sample in sort of a self-supervised context

01:22:32.720 --> 01:22:36.400
give you way more information than a supervised sample,

01:22:36.400 --> 01:22:38.400
the label from supervised learning,

01:22:38.400 --> 01:22:42.600
a fortiori reinforcement for the context of reinforcement.

01:22:42.600 --> 01:22:44.760
Absolutely.

01:22:45.080 --> 01:22:46.440
We interviewed Ishan.

01:22:46.440 --> 01:22:48.640
So we've done a show on self-supervised learning.

01:22:48.640 --> 01:22:50.840
We're huge fans of self-supervised learning.

01:22:50.840 --> 01:22:54.360
And I know your vision is to get to these latent predictive

01:22:54.360 --> 01:22:56.200
models to solve that problem.

01:22:56.200 --> 01:22:59.800
That's something else I changed my mind on the last two years.

01:22:59.800 --> 01:23:02.000
This may be outside the scope of this particular interview.

01:23:02.000 --> 01:23:05.640
But yeah, there's basically two major architectures, right,

01:23:05.640 --> 01:23:07.200
for self-supervised learning, particularly

01:23:07.200 --> 01:23:09.040
in the context of vision.

01:23:09.040 --> 01:23:12.840
And the main characteristic of both of them

01:23:12.840 --> 01:23:16.920
is the fact that it can handle multimodal prediction.

01:23:16.920 --> 01:23:18.600
So if you have a system, let's say you

01:23:18.600 --> 01:23:20.720
want to do video prediction or something like that.

01:23:20.720 --> 01:23:22.600
So you have a piece of a video clip.

01:23:22.600 --> 01:23:25.640
You want to predict the next video clip.

01:23:25.640 --> 01:23:27.080
Or you just want a machine that tells you

01:23:27.080 --> 01:23:30.240
whether a proposed video clip continuation clip is

01:23:30.240 --> 01:23:31.920
a good continuation of the previous one.

01:23:31.920 --> 01:23:33.240
You don't want it to predict.

01:23:33.240 --> 01:23:36.080
You just want it to tell you whether it's a good one.

01:23:36.080 --> 01:23:37.120
So you have two architectures.

01:23:37.120 --> 01:23:40.000
The first one is a latent variable predictive architecture

01:23:40.000 --> 01:23:42.120
that predicts the next video clip.

01:23:42.120 --> 01:23:44.080
And of course, you have to parameterize the prediction

01:23:44.080 --> 01:23:46.560
with a latent variable, because there are multiple predictions

01:23:46.560 --> 01:23:48.040
that are plausible.

01:23:48.040 --> 01:23:49.840
So I used to be a big fan of that.

01:23:49.840 --> 01:23:51.640
And about two years ago, I changed my mind.

01:23:51.640 --> 01:23:54.680
Maybe a year and a half.

01:23:54.680 --> 01:23:57.880
The other approach is something I played with in the early 90s,

01:23:57.880 --> 01:23:59.800
came up with some of the early models for this.

01:23:59.800 --> 01:24:01.880
And it's called a joint embedding architecture,

01:24:01.880 --> 01:24:05.440
where you have the first and the second video clip both going

01:24:05.440 --> 01:24:07.600
through an neural net.

01:24:07.600 --> 01:24:09.200
And then what you're doing is you're

01:24:09.200 --> 01:24:11.640
training the system so that the representation of the second

01:24:11.640 --> 01:24:14.720
video clip is easily predictable from the representation

01:24:14.720 --> 01:24:16.760
of the first one.

01:24:16.760 --> 01:24:18.240
So there you're not predicting pixels.

01:24:18.240 --> 01:24:20.680
You're predicting representations.

01:24:20.680 --> 01:24:22.480
But you still have a system that can tell you

01:24:22.480 --> 01:24:23.840
here is a video clip and the second one

01:24:23.840 --> 01:24:25.520
tell me if they are compatible, if one

01:24:25.520 --> 01:24:27.960
is a good continuation of the other.

01:24:27.960 --> 01:24:30.720
The main reason why I stayed away from those architectures,

01:24:30.720 --> 01:24:34.120
because I knew that you had to use, in the past,

01:24:34.120 --> 01:24:35.680
you had to use contrastive learning.

01:24:35.680 --> 01:24:38.400
You had to have pairs of things that are compatible,

01:24:38.400 --> 01:24:40.480
as well as pairs of things that are incompatible.

01:24:40.480 --> 01:24:42.280
And in high dimension, there's just too many ways

01:24:42.280 --> 01:24:44.120
two things can be incompatible.

01:24:44.120 --> 01:24:46.000
And so that was going to do to failure.

01:24:46.000 --> 01:24:49.040
And I played with this back in the suit

01:24:49.040 --> 01:24:53.520
to be called Siamese networks at a paper in 1992, 1993,

01:24:53.520 --> 01:24:56.360
on doing signature verification using those techniques.

01:24:56.360 --> 01:24:59.560
Jeff Hinton had a slightly different method

01:24:59.560 --> 01:25:02.680
based on maximizing mutual information with his former

01:25:02.680 --> 01:25:04.720
student, Sue Becker.

01:25:04.720 --> 01:25:06.240
But then in the last two years, we've

01:25:06.240 --> 01:25:09.400
had methods that are non-contrastive

01:25:09.800 --> 01:25:13.560
that allows us to train those joint embedding

01:25:13.560 --> 01:25:14.120
architectures.

01:25:14.120 --> 01:25:16.440
So I've become a big fan of them now.

01:25:16.440 --> 01:25:20.240
And it's because of algorithms like BYUL,

01:25:20.240 --> 01:25:26.040
from our friends at DeepBind, like Barlow Twins, whose idea came

01:25:26.040 --> 01:25:30.080
from Stephane Denis, who's doing a postdoc with me at FAIR.

01:25:30.080 --> 01:25:36.360
He's now a professor at the University of Alto in Finland.

01:25:37.320 --> 01:25:38.640
And then more recently, Vic Craig,

01:25:38.640 --> 01:25:43.920
which is a kind of improvement, if you want,

01:25:43.920 --> 01:25:45.880
on Barlow Twins, that is also based

01:25:45.880 --> 01:25:47.800
on the same idea that Jeff Hinton and Sue Becker

01:25:47.800 --> 01:25:51.040
had of maximizing a measure of mutual information between the

01:25:51.040 --> 01:25:54.600
outputs to networks, but in a slightly different way.

01:25:54.600 --> 01:25:56.600
So I'm really excited about those things.

01:25:56.600 --> 01:25:59.240
And again, I change my mind all the time,

01:25:59.240 --> 01:26:06.120
whenever a good idea seems to be overcome by a better one.

01:26:06.160 --> 01:26:11.320
So in this whole space of maybe interpolation, extrapolation,

01:26:11.320 --> 01:26:13.480
but also data manifolds, and so on,

01:26:13.480 --> 01:26:16.840
what's your view on things like data augmentation,

01:26:16.840 --> 01:26:21.360
training and simulation, and domain adaptation, and so on?

01:26:21.360 --> 01:26:25.000
Because it could be argued that these things increase

01:26:25.000 --> 01:26:28.480
the convex hull of the training data.

01:26:28.480 --> 01:26:31.800
They sort of make the distribution broader.

01:26:31.800 --> 01:26:35.040
Or is that just also out of question?

01:26:35.040 --> 01:26:35.560
Not much.

01:26:35.560 --> 01:26:37.040
I think data augmentation does not

01:26:37.040 --> 01:26:42.720
increase the volume of the data point cloud,

01:26:42.720 --> 01:26:44.520
if you want, very much.

01:26:44.520 --> 01:26:48.280
Because those augmentations are generally fairly local,

01:26:48.280 --> 01:26:50.480
in dimensions that are already explored.

01:26:50.480 --> 01:26:53.480
So it may increase it a little bit,

01:26:53.480 --> 01:26:56.960
but not significantly, I think.

01:26:56.960 --> 01:26:58.800
Obviously, data augmentation is very useful.

01:26:58.800 --> 01:27:01.880
I mean, we've used this for decades,

01:27:01.880 --> 01:27:04.560
so it's not a new phenomenon either.

01:27:04.560 --> 01:27:07.120
There's a lot of ideas along those lines, right?

01:27:07.120 --> 01:27:12.200
So it's the idea where you give two examples,

01:27:12.200 --> 01:27:15.080
and you have two examples in your training set,

01:27:15.080 --> 01:27:18.200
and you actually do an interpolation in input space

01:27:18.200 --> 01:27:20.480
to generate a fake example that's in between the two,

01:27:20.480 --> 01:27:22.600
and you try to produce the intermediate target

01:27:22.600 --> 01:27:24.840
between the two original examples.

01:27:24.840 --> 01:27:25.560
Mix up.

01:27:25.560 --> 01:27:26.480
Mix up.

01:27:26.480 --> 01:27:27.000
Mix up.

01:27:27.000 --> 01:27:28.080
Yeah.

01:27:28.080 --> 01:27:28.600
There is that.

01:27:28.600 --> 01:27:29.720
There is distillation.

01:27:29.720 --> 01:27:32.040
There is various techniques like this

01:27:32.040 --> 01:27:35.200
are basically implicitly kind of ways

01:27:35.200 --> 01:27:37.400
to fill in the space between samples, right,

01:27:37.400 --> 01:27:41.160
with other kind of fake samples, or virtual samples,

01:27:41.160 --> 01:27:42.360
if you want.

01:27:42.360 --> 01:27:46.600
There was a paper by Patrick Simard and me,

01:27:46.600 --> 01:27:50.400
and John Denker many years ago, when we were all at Bell Labs,

01:27:50.400 --> 01:27:51.760
on something called tangent prop.

01:27:51.760 --> 01:27:55.720
So the idea of tangent prop was kind of somewhat similar.

01:27:55.720 --> 01:27:58.640
The idea was you take a training sample,

01:27:58.640 --> 01:28:01.080
and you're going to be able to distort that training

01:28:01.080 --> 01:28:02.760
sample in several ways.

01:28:02.760 --> 01:28:05.800
You could generate points by data augmentation.

01:28:05.800 --> 01:28:08.360
But the other thing you can do is just figure out

01:28:08.360 --> 01:28:15.840
the plane in which those augmentations live, OK?

01:28:15.840 --> 01:28:18.560
And that plane is going to be a tangent plane

01:28:18.560 --> 01:28:21.280
to the data manifold.

01:28:21.280 --> 01:28:25.800
What you want is your, for a given class, for example, right?

01:28:25.800 --> 01:28:28.520
So what you want is your input output function

01:28:28.520 --> 01:28:30.280
that the neural net learns to be invariant

01:28:30.280 --> 01:28:33.040
to those little distortions.

01:28:33.040 --> 01:28:35.280
And you can do this by just augmenting the data,

01:28:35.280 --> 01:28:37.640
or you can do this by explicitly having a regularization

01:28:37.640 --> 01:28:40.920
term that says the overall derivative of the function

01:28:40.920 --> 01:28:45.560
in the direction of the spending vectors of that plane

01:28:45.560 --> 01:28:48.440
should be 0, or should be whatever it is that you want it

01:28:48.440 --> 01:28:51.280
to be, but 0 is a good target for this.

01:28:51.280 --> 01:28:53.240
So that's called tangent prop.

01:28:53.240 --> 01:28:57.560
And you can think of it as a regularizer that says,

01:28:57.560 --> 01:28:59.920
I don't just want my input output function

01:28:59.920 --> 01:29:01.200
to have this value at this point.

01:29:01.200 --> 01:29:03.320
I also want its derivative to be 0

01:29:03.320 --> 01:29:06.400
when I change the input in those directions.

01:29:06.400 --> 01:29:10.400
That's another indirect way of doing data augmentation

01:29:10.400 --> 01:29:13.800
without doing data augmentation, essentially.

01:29:13.800 --> 01:29:16.040
And I think there's a lot of those ideas

01:29:16.040 --> 01:29:19.960
that are very useful, certainly.

01:29:19.960 --> 01:29:25.680
So I think there seems to be a spectrum coming back

01:29:25.680 --> 01:29:27.320
to this interpolation, extrapolation.

01:29:27.320 --> 01:29:29.520
You said yourself you don't believe neural networks

01:29:29.520 --> 01:29:32.440
can do something like discrete, abstractive reasoning,

01:29:32.440 --> 01:29:33.120
and so on.

01:29:33.120 --> 01:29:34.720
No, no, no, I didn't say that.

01:29:34.720 --> 01:29:41.080
Or I said, we need to do work for them to be able to do that.

01:29:41.080 --> 01:29:44.560
But I have no doubt that eventually they will.

01:29:44.560 --> 01:29:46.480
I mean, there's a lot of work in this area already.

01:29:46.480 --> 01:29:49.680
And I'll give you some more specific examples if you want.

01:29:49.680 --> 01:29:53.120
I mean, that was actually kind of the nature of my question.

01:29:53.120 --> 01:29:56.400
Where do you think there is this spectrum of what

01:29:56.400 --> 01:29:59.560
people think neural networks are doing now

01:29:59.560 --> 01:30:01.480
or will be able to do later?

01:30:01.480 --> 01:30:04.880
Where do you think the actual biggest disagreement

01:30:04.880 --> 01:30:08.320
between the community right now lies?

01:30:08.320 --> 01:30:11.920
And what can we do to, what experiments, what evidence

01:30:11.920 --> 01:30:15.000
can we gather to solve these disagreements?

01:30:15.000 --> 01:30:15.320
Right.

01:30:15.320 --> 01:30:20.600
So I think we shouldn't talk too much about neural networks

01:30:20.600 --> 01:30:25.200
because people have a relatively narrow picture of what

01:30:25.200 --> 01:30:26.640
a neural network is, right?

01:30:26.640 --> 01:30:28.600
It's a bunch of layers of neurons

01:30:28.600 --> 01:30:30.960
that perform wetted sums and pass a result

01:30:30.960 --> 01:30:32.400
through a nonlinear function.

01:30:32.400 --> 01:30:33.920
And there's a fixed number of layers.

01:30:33.920 --> 01:30:38.640
Maybe they can be recurrent, and you produce an output.

01:30:38.640 --> 01:30:41.760
That's a very restricted view of what deep learning systems

01:30:41.760 --> 01:30:42.920
can do.

01:30:42.920 --> 01:30:49.720
So let me take an example of what reasoning might mean.

01:30:49.720 --> 01:30:52.840
Reasoning might be seen at least one particular type of reasoning

01:30:52.840 --> 01:30:58.000
might be seen as a minimization of an energy function,

01:30:58.000 --> 01:31:00.240
not with respect to parameters in a neural net,

01:31:00.240 --> 01:31:02.680
but with respect to latent variables.

01:31:02.680 --> 01:31:05.520
And a lot of systems that are in use today do that.

01:31:05.520 --> 01:31:08.240
So a lot of speech recognition systems, for example,

01:31:08.240 --> 01:31:12.720
have a decoder on the output, which essentially given

01:31:12.720 --> 01:31:19.920
a list of scores for what a particular segment of speech

01:31:19.920 --> 01:31:21.800
could be in terms of what sound it could be,

01:31:21.920 --> 01:31:25.280
or what syllable, or whatever, the decoder basically

01:31:25.280 --> 01:31:29.080
figures out what is the best interpretation of that sequence

01:31:29.080 --> 01:31:35.000
of sound that makes it a legal sentence in the dictionary.

01:31:35.000 --> 01:31:39.320
Techniques like this have been used for 25 years now

01:31:39.320 --> 01:31:42.160
in the context of speech recognition and handwriting

01:31:42.160 --> 01:31:45.040
recognition, even before neural nets were used for those things.

01:31:45.040 --> 01:31:51.360
So back in the old days of hidden Markov models.

01:31:51.400 --> 01:31:55.640
And those techniques have been really developed.

01:31:55.640 --> 01:31:58.600
Now, if you think about what those techniques do,

01:31:58.600 --> 01:32:01.000
first of all, all the operations that are done in those systems

01:32:01.000 --> 01:32:01.640
are differentiable.

01:32:01.640 --> 01:32:04.000
You can backpropagate gradient through an operation

01:32:04.000 --> 01:32:07.520
that will figure out the shortest path in a graph

01:32:07.520 --> 01:32:09.360
using dynamic programming.

01:32:09.360 --> 01:32:11.040
The first paper on this was in 1991

01:32:11.040 --> 01:32:13.040
by my friend Leon Boutou and Xavier de Leoncourt.

01:32:13.040 --> 01:32:17.360
This is not recent stuff that we're trying to do speech recognition.

01:32:17.360 --> 01:32:20.280
You can backpropagate gradient through a lot of different modules.

01:32:20.320 --> 01:32:25.040
What those things do is that they infer a latent variable

01:32:25.040 --> 01:32:27.120
by basically doing energy minimization.

01:32:27.120 --> 01:32:28.520
Finding the shortest path in a graph

01:32:28.520 --> 01:32:32.440
is a form of energy minimization.

01:32:32.440 --> 01:32:34.800
And so you can have, in a deep learning system,

01:32:34.800 --> 01:32:36.920
you can have a module that has a latent variable.

01:32:36.920 --> 01:32:39.160
And what this module will do is figure out

01:32:39.160 --> 01:32:41.840
a value of the latent variable that minimizes some energy

01:32:41.840 --> 01:32:42.480
function.

01:32:42.480 --> 01:32:43.720
It could be a prediction error.

01:32:43.720 --> 01:32:45.160
It could be something else.

01:32:45.160 --> 01:32:48.520
It could be regularizers in it, et cetera.

01:32:48.520 --> 01:32:51.080
But it basically performs a minimization.

01:32:51.080 --> 01:32:57.240
And that type of operation basically

01:32:57.240 --> 01:33:02.400
can be used to, like, you can formulate

01:33:02.400 --> 01:33:06.720
most types of reasoning in that form.

01:33:06.720 --> 01:33:10.080
Now, it's not necessarily in a continuous differentiable space,

01:33:10.080 --> 01:33:12.640
but almost all reasoning, even in classical AI,

01:33:12.640 --> 01:33:16.200
can be formulated in terms of optimizing some sort of function.

01:33:16.200 --> 01:33:17.640
Like, you won't do a SAT problem, right?

01:33:17.640 --> 01:33:20.360
If you have a collection of Boolean formula,

01:33:20.360 --> 01:33:22.680
you want to find a combination of variables

01:33:22.680 --> 01:33:23.920
that satisfy this formula.

01:33:23.920 --> 01:33:25.040
It's an optimization problem.

01:33:25.040 --> 01:33:26.280
It's combinatorial optimization,

01:33:26.280 --> 01:33:28.040
but it's an optimization problem.

01:33:28.040 --> 01:33:28.920
You want to do planning.

01:33:28.920 --> 01:33:31.560
So planning is the best example.

01:33:31.560 --> 01:33:36.760
So you can do a planning where the thing you're controlling

01:33:36.760 --> 01:33:39.440
as discrete actions and discrete states,

01:33:39.440 --> 01:33:41.240
and you have to use dynamic programming.

01:33:41.240 --> 01:33:44.560
You can back propagate gradient through a system that

01:33:44.560 --> 01:33:45.680
does that.

01:33:45.680 --> 01:33:49.040
But more classical planning is in continuous space.

01:33:49.040 --> 01:33:52.080
So planning for, like, planning the trajectory of an arm

01:33:52.080 --> 01:33:55.600
for a robot, planning the trajectory of a rocket,

01:33:55.600 --> 01:33:58.760
what you have is a differentiable model,

01:33:58.760 --> 01:34:01.800
dynamical model of what is the state of the system

01:34:01.800 --> 01:34:04.000
you're trying to control at time t plus 1

01:34:04.000 --> 01:34:06.280
as a function of the state at time t

01:34:06.280 --> 01:34:07.720
and as a function of the action you take.

01:34:07.720 --> 01:34:09.920
And perhaps as a function of some random variable,

01:34:09.920 --> 01:34:13.040
you can't measure from the environment

01:34:13.040 --> 01:34:16.040
to something like that to make it non-deterministic.

01:34:16.040 --> 01:34:18.880
Now, you can enroll that model.

01:34:18.880 --> 01:34:23.360
So start with time equals 0, and then make a hypothesis

01:34:23.360 --> 01:34:25.560
about a sequence of action, and then

01:34:25.560 --> 01:34:29.760
apply your predictive model of the next state of the system.

01:34:29.760 --> 01:34:31.920
And then at the end, you can measure some cost function.

01:34:31.920 --> 01:34:36.520
Is my rocket docking with a space station or it's far away?

01:34:36.520 --> 01:34:38.360
And how much fuel have I consumed?

01:34:38.360 --> 01:34:40.280
Or whatever, right?

01:34:40.280 --> 01:34:44.400
Or have I reached the goal of the arm,

01:34:44.400 --> 01:34:46.920
avoiding all the obstacles, right?

01:34:46.920 --> 01:34:50.000
So what you can do now is an inference process

01:34:50.000 --> 01:34:52.200
which consists in figuring out what is the sequence of action

01:34:52.200 --> 01:34:55.080
I should take to minimize this cost function according

01:34:55.080 --> 01:34:56.960
to my dynamical model.

01:34:56.960 --> 01:34:58.640
You can do this by gradient descent.

01:34:58.640 --> 01:35:02.320
And basically, that's what the Kitty-Bison algorithm,

01:35:02.320 --> 01:35:05.400
in optimal control, that goes back to 1962.

01:35:05.400 --> 01:35:07.480
And it consists in basically doing backprop through time.

01:35:07.480 --> 01:35:08.680
It's as simple as that, right?

01:35:08.720 --> 01:35:10.240
So you do backprop through time.

01:35:10.240 --> 01:35:14.120
So in effect, optimal control theorist invented backprop

01:35:14.120 --> 01:35:15.280
back in the early 60s.

01:35:15.280 --> 01:35:17.560
But nobody realized you could use this for machine

01:35:17.560 --> 01:35:20.040
learning until the V-80s, essentially.

01:35:20.040 --> 01:35:22.240
Or the 70s.

01:35:22.240 --> 01:35:24.600
Just to jump in there, because there's a little caveat there,

01:35:24.600 --> 01:35:26.840
which is that you can do that at backprop

01:35:26.840 --> 01:35:29.560
if you have a fixed number of steps.

01:35:29.560 --> 01:35:31.960
Like what backprop can't handle is the case

01:35:31.960 --> 01:35:34.120
where I want some expandable number of steps.

01:35:34.120 --> 01:35:37.520
We have no algorithms that can currently optimize.

01:35:37.520 --> 01:35:43.120
For example, neural networks with a variable number of layers.

01:35:43.120 --> 01:35:44.720
Like we just can't train those, right?

01:35:44.720 --> 01:35:45.560
No, it's not true.

01:35:45.560 --> 01:35:46.080
It's not true.

01:35:46.080 --> 01:35:47.680
That's what recurrent nets are.

01:35:47.680 --> 01:35:50.640
You can have a varying number of iterations in your recurrent

01:35:50.640 --> 01:35:50.960
net.

01:35:50.960 --> 01:35:53.560
And what I'm describing here is an unfolded recurrent net.

01:35:53.560 --> 01:35:55.960
It's the same model that you apply every time step, right?

01:35:55.960 --> 01:35:57.920
So it's very much like a recurrent net.

01:35:57.920 --> 01:36:01.960
And you can very well have an unknown number of steps.

01:36:01.960 --> 01:36:03.560
You don't know a priori how long it's

01:36:03.560 --> 01:36:06.400
going to take for your rocket to get to the space station, right?

01:36:06.600 --> 01:36:10.160
So you may have kind of a large potential number of steps.

01:36:10.160 --> 01:36:13.280
And you can have a cost function which will count,

01:36:13.280 --> 01:36:15.560
like how much time it's going to take to get there.

01:36:15.560 --> 01:36:17.920
And this will be part of the optimization, for example.

01:36:17.920 --> 01:36:19.480
And this is classic optimal control.

01:36:19.480 --> 01:36:22.440
I'm not telling you anything that I came up with.

01:36:22.440 --> 01:36:26.240
This is from at least the 60s and 70s.

01:36:26.240 --> 01:36:29.920
No, but the backprop, that variable number of layers

01:36:29.920 --> 01:36:33.080
has to be done in kind of a classic iterative,

01:36:33.080 --> 01:36:35.440
try out the variable number of layers

01:36:35.480 --> 01:36:37.360
and see what the backprop comes up with.

01:36:37.360 --> 01:36:40.320
Or you start with a very large number of layers

01:36:40.320 --> 01:36:42.200
and then let backprop try and find.

01:36:42.200 --> 01:36:45.520
But still, what I'm saying is there is fundamentally

01:36:45.520 --> 01:36:48.560
two different kinds of computation that are at play here.

01:36:48.560 --> 01:36:51.600
One is like the finite fixed thing,

01:36:51.600 --> 01:36:54.640
and then you do some differentiable optimization on it.

01:36:54.640 --> 01:36:57.600
Another kind of computation is this discrete symbolic

01:36:57.600 --> 01:36:59.280
that has like an expandable memory

01:36:59.280 --> 01:37:01.320
and an unbounded amount of time

01:37:01.320 --> 01:37:03.120
to sit there kind of computing on things.

01:37:03.160 --> 01:37:05.080
Okay, I put a stop right there.

01:37:06.080 --> 01:37:07.640
What I'm describing has nothing to do

01:37:07.640 --> 01:37:09.560
with symbolic, discrete, or anything.

01:37:09.560 --> 01:37:11.640
I understand that, I understand that.

01:37:11.640 --> 01:37:14.520
But you seem to be equating a variable number

01:37:14.520 --> 01:37:17.800
with discrete symbolic, this is two different things.

01:37:17.800 --> 01:37:19.960
Okay, let me pose it in this form,

01:37:19.960 --> 01:37:22.080
which is that I can very easily write down

01:37:22.080 --> 01:37:26.760
a symbolic program that can output the arbitrary digit of pi,

01:37:26.760 --> 01:37:28.760
like the nth digit of pi.

01:37:28.760 --> 01:37:31.400
Nobody can train a neural network that can do that.

01:37:32.360 --> 01:37:35.080
So what's the difference between these two types

01:37:35.080 --> 01:37:38.360
of computation and where in the future might we go

01:37:38.360 --> 01:37:41.480
with neural networks or by augmenting neural networks,

01:37:41.480 --> 01:37:44.840
whether it's differentiable Turing machines or whatever,

01:37:44.840 --> 01:37:46.840
or neural Turing machines,

01:37:46.840 --> 01:37:49.240
to try and bridge that gap and capability?

01:37:49.240 --> 01:37:51.040
Okay, before we bridge that gap,

01:37:51.040 --> 01:37:54.800
the algorithm to compute the digit of pi,

01:37:54.800 --> 01:37:57.120
there's only a tiny number of humans,

01:37:57.120 --> 01:38:01.080
only in the last few centuries that I figured this one out.

01:38:01.560 --> 01:38:04.960
I'm interested in how is it that a cat can plan

01:38:04.960 --> 01:38:08.800
to jump on the table and not fall or even open a door

01:38:08.800 --> 01:38:10.520
or do things like that, right?

01:38:10.520 --> 01:38:11.880
Once we figure this one out,

01:38:11.880 --> 01:38:13.920
maybe we can think about kind of more complex stuff,

01:38:13.920 --> 01:38:16.840
like designing algorithms that involve

01:38:16.840 --> 01:38:18.480
complex mathematical concepts.

01:38:18.480 --> 01:38:22.920
But I think we're way, we're not there, right?

01:38:22.920 --> 01:38:26.880
We're faced with much more fundamental issues

01:38:26.880 --> 01:38:30.560
of how do we learn predictive models to the world

01:38:30.680 --> 01:38:33.000
by observing it and things of that type,

01:38:33.000 --> 01:38:36.440
which are much more basic that most animals can do

01:38:36.440 --> 01:38:41.440
and digit of pi, it's some kind of a line.

01:38:43.560 --> 01:38:45.640
So, because you're talking about model predictive control,

01:38:45.640 --> 01:38:49.640
is it possible that there are different shapes of problem?

01:38:49.640 --> 01:38:52.600
So, for example, some problems are interpolative

01:38:52.600 --> 01:38:56.720
and are solvable using differentiable models.

01:38:56.720 --> 01:38:58.840
Do you think there exists problems

01:38:59.560 --> 01:39:01.400
that are quite discreet in nature

01:39:01.400 --> 01:39:04.000
and a different type of approach would be required?

01:39:04.000 --> 01:39:04.840
Of course, yeah.

01:39:04.840 --> 01:39:07.320
I mean, there is certainly a lot of situations

01:39:07.320 --> 01:39:11.920
where the mapping from action to result

01:39:11.920 --> 01:39:15.440
is very kind of discontinuous, if you want, right?

01:39:15.440 --> 01:39:16.920
This is qualitatively different.

01:39:17.800 --> 01:39:22.800
And so there are many situations where

01:39:23.000 --> 01:39:25.240
or it's somewhat continuous

01:39:25.240 --> 01:39:30.240
and situations where you change your action a little bit

01:39:30.280 --> 01:39:33.520
and it results in a completely different outcome.

01:39:33.520 --> 01:39:35.240
And so the big question, I think,

01:39:35.240 --> 01:39:40.240
is how you handle that kind of uncertainty in the search.

01:39:40.360 --> 01:39:42.680
And you can think of sort of two extremes.

01:39:42.680 --> 01:39:45.000
So at one extreme in the continuous case

01:39:45.000 --> 01:39:46.680
is the case where you're planning the trajectory

01:39:46.680 --> 01:39:51.680
of a rocket or that's pretty continuous

01:39:52.280 --> 01:39:54.080
and differentiable and everything you want.

01:39:54.080 --> 01:39:55.320
And you don't even need to learn the model.

01:39:55.320 --> 01:39:57.200
You basically write it down, right?

01:39:57.200 --> 01:39:59.480
But there are situations there where you're flying a drone

01:39:59.480 --> 01:40:01.520
or something where you might need to learn the model

01:40:01.520 --> 01:40:04.080
because there's so many kind of nonlinear effects

01:40:04.080 --> 01:40:05.720
that it's probably better to learn it

01:40:05.720 --> 01:40:06.760
and people are working on this.

01:40:06.760 --> 01:40:09.480
Same for like working robots and stuff like that.

01:40:09.480 --> 01:40:11.000
Then there are things that are a little intermediate

01:40:11.000 --> 01:40:14.640
where there are sort of hard constraints on what you can do.

01:40:14.640 --> 01:40:17.000
So you want to grab an object with a robot arm,

01:40:17.000 --> 01:40:18.400
but there is obstacles in between

01:40:18.400 --> 01:40:21.040
and you don't want to bump into them and things like this.

01:40:21.040 --> 01:40:23.120
So people tend to put like penalty functions

01:40:23.120 --> 01:40:25.120
to make this sort of more continuous,

01:40:25.120 --> 01:40:27.360
but there's sort of qualitative difference

01:40:27.360 --> 01:40:30.160
between using your left arm or your right arm, for example,

01:40:30.160 --> 01:40:35.000
or going, scratching your left ear with your left hand

01:40:35.000 --> 01:40:37.560
or with your right ear going to the back of your head, right?

01:40:37.560 --> 01:40:40.440
Those are qualitatively different solutions.

01:40:40.440 --> 01:40:42.840
And then all the way to the other side,

01:40:42.840 --> 01:40:46.000
there is intrinsically discrete problems

01:40:46.000 --> 01:40:49.400
with that may be fully observable

01:40:49.400 --> 01:40:53.040
with some uncertainty like chess and go-playing, okay?

01:40:53.960 --> 01:40:57.680
And those we can handle to some extent

01:40:57.680 --> 01:41:00.680
because the number of actions is finite.

01:41:00.680 --> 01:41:02.200
It goes exponentially, but it's finite.

01:41:02.200 --> 01:41:07.200
And so using kind of ways to direct the search,

01:41:07.440 --> 01:41:10.600
despite the fact that the search space is exponential,

01:41:10.600 --> 01:41:13.080
using neural nets as basically evaluation functions

01:41:13.080 --> 01:41:14.640
to direct the search in the right way

01:41:14.640 --> 01:41:18.800
and doing multi-color tree search and blah, blah, blah,

01:41:18.800 --> 01:41:20.720
will work, okay?

01:41:20.720 --> 01:41:22.280
With sufficiently many trials,

01:41:23.760 --> 01:41:25.320
in the completely deterministic,

01:41:25.320 --> 01:41:28.280
fully observable, differentiable case,

01:41:28.280 --> 01:41:31.520
that's classical model predictive control, that's fine.

01:41:31.520 --> 01:41:35.120
Then there is some stuff that we really don't know how to do

01:41:35.120 --> 01:41:36.160
and it's twofold.

01:41:36.160 --> 01:41:39.520
One is the model is not given to us

01:41:39.520 --> 01:41:43.560
by equations derived from first principles, right?

01:41:43.560 --> 01:41:46.840
So the stuff we're trying to do is in the real world

01:41:46.840 --> 01:41:48.160
and it's got complicated dynamics

01:41:48.160 --> 01:41:49.960
that we can't just model from first principle.

01:41:49.960 --> 01:41:51.360
So we have to learn the model.

01:41:52.520 --> 01:41:54.600
That's the first issue.

01:41:54.600 --> 01:41:57.160
Second issue, the model lives in the world

01:41:57.160 --> 01:41:59.560
and the world is not completely predictable.

01:41:59.560 --> 01:42:01.320
It may be deterministic,

01:42:01.320 --> 01:42:04.640
but you don't have full observation.

01:42:04.640 --> 01:42:07.440
So you cannot predict exactly what's gonna happen

01:42:07.440 --> 01:42:08.960
in the world because the world is being the world

01:42:08.960 --> 01:42:10.800
or as a consequence of your actions.

01:42:12.240 --> 01:42:13.680
So how do you deal with the certainty?

01:42:13.680 --> 01:42:14.920
And for that, you need predictive models

01:42:14.920 --> 01:42:16.120
that can represent uncertainty.

01:42:16.120 --> 01:42:19.800
And we are back to the issue I was telling you about earlier.

01:42:19.800 --> 01:42:21.080
Do you need latent variable models

01:42:21.080 --> 01:42:22.680
or joint embedding architectures, right?

01:42:22.680 --> 01:42:26.240
That's, and I've changed my mind about this as I told you.

01:42:26.240 --> 01:42:31.240
So then there is the third obstacle,

01:42:31.920 --> 01:42:35.480
which is, is the problem we're trying to solve

01:42:35.480 --> 01:42:38.640
of the so continuous differentiable nature

01:42:38.640 --> 01:42:41.680
or of the kind of completely discreet,

01:42:41.680 --> 01:42:42.800
you know, qualitatively discreet

01:42:42.800 --> 01:42:45.160
depending on what action you take nature.

01:42:45.160 --> 01:42:48.640
And I think most problems are some combination of the two.

01:42:48.680 --> 01:42:52.600
So, you know, you're trying to build a box

01:42:52.600 --> 01:42:54.120
at a wood or something like that, right?

01:42:54.120 --> 01:42:56.640
You can make the box bigger or smaller.

01:42:56.640 --> 01:42:58.760
You know, you can hit the nail in this way or that way.

01:42:58.760 --> 01:43:02.040
And that may be sort of continuous and differentiable.

01:43:02.040 --> 01:43:03.040
But then there is, you know,

01:43:03.040 --> 01:43:05.120
you put glue or screws or nails,

01:43:05.120 --> 01:43:09.360
do you, or use kind of more classical carpentry or whatever.

01:43:09.360 --> 01:43:11.960
And those are kind of discreet choices.

01:43:11.960 --> 01:43:13.960
What type of wood are you using, you know, things like that.

01:43:13.960 --> 01:43:16.360
So I think the, you know,

01:43:16.360 --> 01:43:18.760
human mind is able to deal with all of those situations,

01:43:18.760 --> 01:43:22.200
have, you know, know to use differentiable continuous stuff

01:43:22.200 --> 01:43:26.280
when they have to and use the sort of discreet exploration

01:43:26.280 --> 01:43:28.080
when we have to as well.

01:43:28.080 --> 01:43:31.360
But we have to realize that humans are really, really bad

01:43:31.360 --> 01:43:33.680
at the discreet exploration stuff.

01:43:33.680 --> 01:43:34.760
We totally suck at it.

01:43:36.360 --> 01:43:38.560
If we didn't, then we would be better than computers

01:43:38.560 --> 01:43:40.920
at playing chess and go, but we're not.

01:43:40.920 --> 01:43:42.560
We're actually really, really bad.

01:43:43.560 --> 01:43:45.560
So...

01:43:45.560 --> 01:43:47.560
That's fascinating.

01:43:47.560 --> 01:43:51.560
So you seem to be saying in a way that you are a fan of hybrid models.

01:43:51.560 --> 01:43:52.560
No.

01:43:52.560 --> 01:43:55.560
And something, well, something like AlphaGo, for example.

01:43:55.560 --> 01:43:58.560
I mean, that's basically, there's an interpolative space

01:43:58.560 --> 01:44:00.560
which, you know, guides a discreet search.

01:44:00.560 --> 01:44:02.560
Let's say it like that.

01:44:02.560 --> 01:44:04.560
Do you think that's, is that a good thing?

01:44:04.560 --> 01:44:06.560
Or do you think that's...

01:44:06.560 --> 01:44:08.560
So you like that kind of model?

01:44:08.560 --> 01:44:12.560
Okay, there is something very interesting about, you know,

01:44:12.560 --> 01:44:14.560
in the context of reinforcement running about this,

01:44:14.560 --> 01:44:16.560
which is actual critic models, right?

01:44:16.560 --> 01:44:20.560
And you could think of all of the stuff that actually works

01:44:20.560 --> 01:44:22.560
in reinforcement running.

01:44:22.560 --> 01:44:24.560
I mean, they don't work in the real world, right?

01:44:24.560 --> 01:44:27.560
But they work in games and stuff and simulated environment.

01:44:27.560 --> 01:44:32.560
They very often use actual critic type architectures.

01:44:32.560 --> 01:44:34.560
And the idea of a critic, you know,

01:44:34.560 --> 01:44:37.560
goes back to early papers by Saturn and Bartow.

01:44:37.560 --> 01:44:42.560
And the basic idea of a critic is to have a differentiable

01:44:42.560 --> 01:44:45.560
approximation of your value function, right?

01:44:45.560 --> 01:44:50.560
So you train a small neural net essentially to compute,

01:44:50.560 --> 01:44:55.560
to estimate, to predict the value function from your state.

01:44:55.560 --> 01:44:57.560
And the reason you need this is now you can propagate

01:44:57.560 --> 01:44:59.560
gradient through it, right?

01:44:59.560 --> 01:45:03.560
So that's kind of the first step into sort of making

01:45:03.560 --> 01:45:04.560
the world differentiable.

01:45:04.560 --> 01:45:06.560
You're just making the value function differentiable.

01:45:06.560 --> 01:45:10.560
Now, inside of the world, there's two parts in my opinion.

01:45:10.560 --> 01:45:13.560
And this is the source of the idea behind model-based reinforcement

01:45:13.560 --> 01:45:19.560
running is that you have the world, right?

01:45:19.560 --> 01:45:23.560
The world is going from state to state because it wants to

01:45:23.560 --> 01:45:25.560
or because you're taking an action.

01:45:25.560 --> 01:45:29.560
And then there is a value function or, you know, a cost function.

01:45:29.560 --> 01:45:32.560
I prefer to talk in terms of cost function that takes the state

01:45:32.560 --> 01:45:38.560
of the world and gives you kind of estimate of that cost, right?

01:45:38.560 --> 01:45:40.560
Or gives you the cost.

01:45:40.560 --> 01:45:46.560
Now, the world itself and the value function that takes the state

01:45:46.560 --> 01:45:51.560
of the world and gives you pain or pleasure, right, is unknown.

01:45:51.560 --> 01:45:55.560
But what you can do is build a differentiable model of that, right?

01:45:55.560 --> 01:45:58.560
So a differentiable model of that is what NPC is all about.

01:45:58.560 --> 01:46:01.560
You build a model of the world that predicts the new state

01:46:01.560 --> 01:46:03.560
of the world as a function of the previous state.

01:46:03.560 --> 01:46:05.560
It doesn't have to be a complete state.

01:46:05.560 --> 01:46:09.560
It has to be a state that's complete enough to contain the relevant information

01:46:09.560 --> 01:46:12.560
about the world, you know, relative to your task, right?

01:46:12.560 --> 01:46:16.560
And then you have a differentiable function that you learn

01:46:16.560 --> 01:46:21.560
that learns to predict the reward or the cost from your estimate

01:46:21.560 --> 01:46:23.560
of the state of the world.

01:46:23.560 --> 01:46:27.560
So what you have now is, you know, a neural net inside your agent

01:46:27.560 --> 01:46:32.560
basically can simulate the world and simulate the cost

01:46:32.560 --> 01:46:36.560
that is going to result from the state of the world in a differentiable way.

01:46:36.560 --> 01:46:41.560
So now you can use gradient descent or gradient based methods for two things.

01:46:41.560 --> 01:46:45.560
One for inferring a sequence of action that will minimize a particular cost.

01:46:45.560 --> 01:46:48.560
Okay, there's no learning there.

01:46:48.560 --> 01:46:54.560
Two, to learn a policy that will learn to produce the right action

01:46:54.560 --> 01:46:58.560
even the state without having to do model predictive control,

01:46:58.560 --> 01:47:04.560
without having to do this inference by energy minimization, if you want.

01:47:04.560 --> 01:47:09.560
And that, in my opinion, explains the process that we observe in humans

01:47:09.560 --> 01:47:13.560
by which when you learn a new task, you go from the, you know,

01:47:13.560 --> 01:47:17.560
Daniel Kahneman system two to Daniel Kahneman system one, right?

01:47:17.560 --> 01:47:19.560
So, you know, you learn to drive, you're learning to drive,

01:47:19.560 --> 01:47:22.560
you're using, of course, your entire model of the world that you've learned

01:47:22.560 --> 01:47:27.560
in the last 18 years, if you are 18, to predict that, you know,

01:47:27.560 --> 01:47:30.560
when you turn the wheel of the car to the right, the car will go to the right

01:47:30.560 --> 01:47:33.560
and if there's a cliff next to you, the car is going to fall off the cliff

01:47:33.560 --> 01:47:35.560
and, you know, you're going to die, right?

01:47:35.560 --> 01:47:38.560
You don't have to try this to know that this is going to happen.

01:47:38.560 --> 01:47:44.560
You can rely on your internal model to, you know, avoid yourself a lot of pain, right?

01:47:44.560 --> 01:47:51.560
And so, but you pay attention to the situation.

01:47:51.560 --> 01:47:53.560
You pay a lot of attention to the situation.

01:47:53.560 --> 01:47:56.560
You're completely deliberate about it.

01:47:56.560 --> 01:48:00.560
You imagine all kinds of scenarios and you drive slowly

01:48:00.560 --> 01:48:04.560
so you leave yourself enough time to actually do this kind of reasoning.

01:48:04.560 --> 01:48:09.560
And then after maybe 20, 50 hours of practice,

01:48:09.560 --> 01:48:13.560
it becomes subconscious and automatic, you know?

01:48:13.560 --> 01:48:15.560
That's even true for chess players.

01:48:15.560 --> 01:48:18.560
So, that's an interesting thing about chess, right?

01:48:18.560 --> 01:48:21.560
I played once. I'm a terrible chess player, by the way.

01:48:21.560 --> 01:48:25.560
And I played once a simultaneous game against a grandmaster.

01:48:25.560 --> 01:48:30.560
So, he was, you know, he was playing against like 50 other people.

01:48:30.560 --> 01:48:33.560
And so, I had plenty of time to think about my move, right?

01:48:33.560 --> 01:48:38.560
Because he had to kind of play with the 49 other players before getting to me.

01:48:38.560 --> 01:48:41.560
And so, I wait for him to come and make one move.

01:48:41.560 --> 01:48:45.560
And then, you know, in one second, or first of all, he does, like, you know,

01:48:45.560 --> 01:48:47.560
play something stupid, which I did.

01:48:47.560 --> 01:48:49.560
And he moves within one second.

01:48:49.560 --> 01:48:50.560
He doesn't have to think about it, right?

01:48:50.560 --> 01:48:52.560
It's completely subconscious to him.

01:48:52.560 --> 01:48:54.560
You know, it's just pattern recognition, right?

01:48:54.560 --> 01:48:58.560
He's got this, you know, covenants predicting the next move,

01:48:58.560 --> 01:49:00.560
you know, completely instinctively.

01:49:00.560 --> 01:49:03.560
He doesn't have to think because I'm not, you know,

01:49:03.560 --> 01:49:08.560
I'm not good enough for him to really kind of cause his system to kick in.

01:49:08.560 --> 01:49:13.560
And of course, you know, he beat me in 10, you know, in 10 plays, right?

01:49:13.560 --> 01:49:16.560
I mean, as I told you, I'm terrible.

01:49:16.560 --> 01:49:19.560
You know, I learned to drive a long time ago.

01:49:19.560 --> 01:49:28.560
But as it turned out, very recently, I went to drive a sports car on a raceway.

01:49:28.560 --> 01:49:33.560
And, you know, again, the first few times, you were very deliberate about it, you know?

01:49:33.560 --> 01:49:38.560
You explain a few things, and then you basically have to integrate all of that by yourself.

01:49:38.560 --> 01:49:41.560
Over the course of a day, you get better and better, like much better,

01:49:41.560 --> 01:49:47.560
just by, you know, basically compiling what at first is deliberate

01:49:47.560 --> 01:49:51.560
into something that becomes automatic and subconscious.

01:49:51.560 --> 01:49:55.560
Indeed. And also, abstractly reusing knowledge that you've gleaned elsewhere

01:49:55.560 --> 01:49:56.560
and applying it in a new situation.

01:49:56.560 --> 01:50:00.560
But anyway, Professor Yanlacun, thank you so much for joining us today.

01:50:00.560 --> 01:50:01.560
Thank you. It's been an absolute honor.

01:50:01.560 --> 01:50:02.560
Well, it's been a pleasure.

01:50:02.560 --> 01:50:04.560
You guys are doing great work.

01:50:04.560 --> 01:50:06.560
So, you know, keep going.

01:50:06.560 --> 01:50:08.560
Thank you for being a fan, too.

01:50:08.560 --> 01:50:10.560
Thank you so much for watching some episodes.

01:50:10.560 --> 01:50:11.560
Thank you so much.

01:50:11.560 --> 01:50:12.560
Wonderful.

01:50:12.560 --> 01:50:14.560
Rando, introduce yourself.

01:50:14.560 --> 01:50:21.560
Okay, so I joined FAIR, or should I say Meta AI Research, I guess.

01:50:21.560 --> 01:50:28.560
Last June, for a postdoc position, I was doing my PhD at Rice University

01:50:28.560 --> 01:50:31.560
under the supervision of Professor Richard Baronyuk.

01:50:31.560 --> 01:50:36.560
And during my PhD, I focused on basically trying to understand deep networks

01:50:36.560 --> 01:50:39.560
from a geometrical point of view through spline theory.

01:50:39.560 --> 01:50:43.560
And now I'm trying to, let's say, expand my horizons

01:50:43.560 --> 01:50:49.560
and do more diversified research topics with Yanlacun.

01:50:49.560 --> 01:50:56.560
And for this paper, basically, the main goal was to try to understand

01:50:56.560 --> 01:51:03.560
through a lot of empirical experiments, what do we understand by interpolation?

01:51:03.560 --> 01:51:05.560
Does it occur in practice?

01:51:05.560 --> 01:51:10.560
And does it make sense to use interpolation as we know it,

01:51:10.560 --> 01:51:15.560
as, let's say, a measure of generalization performance for current networks?

01:51:15.560 --> 01:51:20.560
And the main point is really to say that the current definition of interpolation,

01:51:20.560 --> 01:51:28.560
which uses this convex hull, might be too rigid to really provide any meaningful intuition.

01:51:28.560 --> 01:51:36.560
And so we either need to adapt this definition or just entirely think about this in a different angle.

01:51:36.560 --> 01:51:43.560
But yeah, the current definition is not good enough for the current data regimes that we are in right now.

01:51:43.560 --> 01:51:46.560
I don't know if it's precise enough.

01:51:46.560 --> 01:51:47.560
Perfect.

01:51:47.560 --> 01:51:48.560
No, it's wonderful.

01:51:48.560 --> 01:51:49.560
Cool.

01:51:49.560 --> 01:51:52.560
Sorry, I need to get into the mood again.

01:51:52.560 --> 01:51:53.560
Yeah.

01:51:53.560 --> 01:51:55.560
Hi, Randall.

01:51:55.560 --> 01:51:58.560
It's really cool to have you here.

01:51:58.560 --> 01:52:00.560
We've enjoyed reading the paper.

01:52:00.560 --> 01:52:04.560
It's quite a short and concise paper, I have to say.

01:52:04.560 --> 01:52:08.560
And the experiments are quite, I find them to be really on point,

01:52:08.560 --> 01:52:14.560
especially where you look at the latent space experiments.

01:52:14.560 --> 01:52:19.560
Because a lot of people would say, of course we're not interpolating in data space.

01:52:19.560 --> 01:52:22.560
We're interpolating in the latent space.

01:52:22.560 --> 01:52:29.560
Yet, even in the latent space, you know, and we've talked a little bit about the notion of interpolation and extrapolation.

01:52:29.560 --> 01:52:35.560
Is it fair to say that the paper is just sort of a negative argument?

01:52:35.560 --> 01:52:40.560
Is it fair to say that the main point of the paper is arguing,

01:52:40.560 --> 01:52:48.560
look, interpolation is the wrong concept you're looking for when you criticize these models?

01:52:48.560 --> 01:52:49.560
Yes.

01:52:49.560 --> 01:52:51.560
I don't think it's negative per se.

01:52:51.560 --> 01:52:59.560
It's more, let's say, a call to change the definition for the current usage of machine learning models.

01:52:59.560 --> 01:53:03.560
Because now we're not in low dimension regimes anymore.

01:53:03.560 --> 01:53:10.560
And so using those concepts that have been defined 50 years ago when we were looking at univariate models,

01:53:10.560 --> 01:53:12.560
does not really make sense.

01:53:12.560 --> 01:53:16.560
So I think the intuition behind what we try to mean by interpolation is right.

01:53:16.560 --> 01:53:17.560
We should not change that.

01:53:17.560 --> 01:53:22.560
We should change the mathematical definition of it.

01:53:22.560 --> 01:53:28.560
And yeah, like you say, people could argue, yes, but you have interpolation in the latent space and so on.

01:53:28.560 --> 01:53:32.560
But we showed that even in a classifier setting, it does not happen.

01:53:32.560 --> 01:53:39.560
And you can also show that in a generative network setting, it will not happen again or because of the dimensionality.

01:53:39.560 --> 01:53:43.560
So it's really about going to the high dimensional setting.

01:53:43.560 --> 01:53:48.560
Then things start to break and we have to adapt to that basically.

01:53:48.560 --> 01:53:51.560
And we've already asked Jan this question.

01:53:51.560 --> 01:54:00.560
But if you had to give your best shot at making the argument that these people want to make when they say,

01:54:00.560 --> 01:54:02.560
oh, it's just interpolating.

01:54:02.560 --> 01:54:09.560
If you had to give your best shot at making that argument successfully, you know, what would you change?

01:54:09.560 --> 01:54:16.560
How would you change the notion of interpolation or what argument would you make for those people?

01:54:16.560 --> 01:54:22.560
So I think what most people try to say is they try first to conceptualize the data that they have.

01:54:22.560 --> 01:54:24.560
So for example, you have an apple, right?

01:54:24.560 --> 01:54:25.560
You have different colors.

01:54:25.560 --> 01:54:33.560
And if you think of this color as being your latent space, then you can say, okay, between green and red, you have a new color.

01:54:33.560 --> 01:54:34.560
But it's in between the two.

01:54:34.560 --> 01:54:36.560
You are in an interpolation regime, right?

01:54:36.560 --> 01:54:39.560
So all of this point of view is from a generative perspective.

01:54:39.560 --> 01:54:43.560
And this is because you only think of a few factors of variations like this.

01:54:43.560 --> 01:54:47.560
And then you think, okay, everything should be in interpolation regime.

01:54:47.560 --> 01:54:54.560
But even with that definition, if you start having a lot of factors of variation, then the course of dimensionality will kick again.

01:54:54.560 --> 01:54:58.560
And you will never be in interpolation regime, even in a generative setting.

01:54:58.560 --> 01:55:11.560
So for them, the best argument would be if I don't consider the real data set, but very low dimensional approximation, very rough, which can be explained only with a very few factors of variation.

01:55:11.560 --> 01:55:15.560
And I can somehow linearize those factors in my latent space.

01:55:15.560 --> 01:55:19.560
Then I will have more chance at being in an interpolation regime.

01:55:19.560 --> 01:55:24.560
So we'll have to have a sort of lossy compression of your data, if you will.

01:55:24.560 --> 01:55:27.560
And then you can try to reach there with more chances.

01:55:27.560 --> 01:55:31.560
So I think you just still manned the argument for interpolation.

01:55:31.560 --> 01:55:37.560
So I think that's precisely what folks do argue is happening in a deep neural network.

01:55:37.560 --> 01:55:39.560
Do you not believe that?

01:55:39.560 --> 01:55:46.560
Well, as soon as you have high dimensionality settings, then it does not happen almost surely.

01:55:46.560 --> 01:55:52.560
And I mean, you could argue, so for example, let's say you take a gun or any big generative network, right?

01:55:52.560 --> 01:55:59.560
In the latent space, you have hundreds of dimensions and you sample those guys from a Gaussian distribution.

01:55:59.560 --> 01:56:07.560
So even if you were saying, OK, your training set is sample from a generative model and your test set is sample from the same generative model.

01:56:07.560 --> 01:56:10.560
And in that latent space, you have interpolation.

01:56:10.560 --> 01:56:23.560
Well, it's wrong from the beginning because in that latent space alone, you will never have a Gaussian sample that lies in an interpolation regime as soon as the dimensionality is greater than, let's say, 15 or 20.

01:56:23.560 --> 01:56:29.560
So it does not happen because of the dimensionality, unless you have very degenerate things.

01:56:29.560 --> 01:56:36.560
Of course, if your generative network just speeds out a constant, then in pixel space, you will have interpolation.

01:56:36.560 --> 01:56:39.560
But this is degenerate by definition.

01:56:39.560 --> 01:56:44.560
I'd like to pick up on that, if you don't mind.

01:56:44.560 --> 01:56:55.560
Yeah, so what I want to pick up on is, again, all of this hinges on definition one, which is in the paper, which is membership within this convex hull.

01:56:55.560 --> 01:57:02.560
And there's a sense in which that's an extremely rigid definition of interpolation, right?

01:57:03.560 --> 01:57:18.560
And I think I heard you earlier say that what we need to do is redefine what we mean by interpolation in higher dimension because the intuition of interpolation is still correct, but we need to redefine what we mean by interpolation.

01:57:18.560 --> 01:57:28.560
So if you took a shot at redefining interpolation, how would you define it? What do you think is a better definition of interpolation?

01:57:28.560 --> 01:57:36.560
So I think it's very task dependent. So let's say you want to look at a task which is pure object classification.

01:57:36.560 --> 01:57:48.560
In that case, I think going back to what we were saying earlier on first trying to compress some of your data so that you can explain it with as little factors of variation as possible.

01:57:48.560 --> 01:57:53.560
And then you can use the current definition just on a compression of your data.

01:57:53.560 --> 01:58:04.560
Then it could make sense because you don't really mind about the finicky details of your objects if you just want to differentiate between different classes of objects.

01:58:04.560 --> 01:58:18.560
But in another setting where you might be trying, I don't know, to denoise samples or things like that, then you might want to have a very more specific definition based on what you try to denoise in your data and so on.

01:58:18.560 --> 01:58:33.560
So I don't think there will be a general definition that works across the board. It will be really dependent on the type of task you are looking down downstream. It's really the key.

01:58:33.560 --> 01:58:49.560
Let me throw something out there because I've been thinking about this and I tried to ask Professor Lacune about this, but I didn't communicate it clearly, which is that for something to be in the convex hull, so in dimensions,

01:58:49.560 --> 01:59:02.560
for something to be within a convex hull, it's a necessary condition that on every single dimension, the sample data point lies within the range that was sampled.

01:59:02.560 --> 01:59:10.560
So that's a necessary but not sufficient condition because the convex hull is actually even smaller than that space.

01:59:10.560 --> 01:59:19.560
But it's necessary that it be within that axis-aligned bounding box. It has to be inside there on every dimension.

01:59:19.560 --> 01:59:33.560
And so what if I were to invert this and say that, well, instead I'm going to define extrapolation as on every single dimension, it has to be outside the sample range.

01:59:33.560 --> 01:59:46.560
So in other words, the point has to be outside the axis-aligned bounding box. That would actually come to the complete inverse conclusion because then exponentially so all machine learning would be interpolation

01:59:46.560 --> 01:59:51.560
because it's very, very unlikely that you're outside every single dimension.

01:59:51.560 --> 02:00:08.560
So I'm thinking somewhere in between there, there's almost like a fractional concept of interpolation versus extrapolation where we can say it's almost like on average, how many of the dimensions do you hit inside the sample point?

02:00:08.560 --> 02:00:16.560
So maybe I'm inside 20% of the dimensions on average. Could that be like a route to any type of improved definition?

02:00:16.560 --> 02:00:23.560
So that's actually a very good point. So basically what you are saying is instead looking at, let's say, the closest, enclosing hypercube.

02:00:23.560 --> 02:00:29.560
And then you say, okay, if you're in that hypercube or not, you're in interpolation or extrapolation regime.

02:00:29.560 --> 02:00:38.560
And that's actually something we're looking at right now, which is not the smallest hypercube but the smallest ellipsoid that encloses your data.

02:00:38.560 --> 02:00:43.560
And it's somewhere in between the convex hull and the hypercube that you mentioned.

02:00:43.560 --> 02:00:57.560
So for sure, there is some ways in between those two extremes where you will have a meaningful interpolation and extrapolation regime that does not collapse all one way or the other way.

02:00:57.560 --> 02:01:05.560
So this is for sure one interesting route. And this could potentially be, let's say, task agnostic.

02:01:05.560 --> 02:01:14.560
But then again, would it be precise enough to re-quantify generation performances per se? I don't know yet.

02:01:14.560 --> 02:01:17.560
But yeah, that's something we are looking into right now.

02:01:18.560 --> 02:01:38.560
And the reason that we kind of came to that thought process is because there is an intuition that machine learning is kind of very good at taking all these dimensions that we sample and saying, you know, it's really only a subset of those dimensions that matter, at least in some transform way.

02:01:38.560 --> 02:01:54.560
Like for example, suppose I was doing, suppose I only had 16 dimensions of ambient space and everybody agreed, yeah, given the amount of data we have, we are interpolating, like almost all the time we're in this rigid definition of a convex hull.

02:01:54.560 --> 02:02:02.560
And then somebody comes along and says, oh, by the way, we upgraded all our sensors now and we added 240 dimensions.

02:02:02.560 --> 02:02:12.560
And if all those dimensions happen to not be useful for the problem that we're trying to solve or classification problem or whatever, it would be weird to say now we're all of a sudden extrapolating.

02:02:12.560 --> 02:02:24.560
Because if the neural network is doing what it should be doing, it will ignore all those irrelevant definitions and continue just calculating exactly what it was calculating before.

02:02:25.560 --> 02:02:28.560
Yes, that's a very good point.

02:02:28.560 --> 02:02:39.560
But I think for this to actually work, you will need to assume that you have enough actual samples so that when you train your network understands that basically those dimensions are pure noise.

02:02:39.560 --> 02:02:43.560
It does not need it to solve the task at hand and it disregards it.

02:02:43.560 --> 02:02:50.560
So you have some all regularization terms that kicks in and it does not try to use them to minimize the training loss.

02:02:50.560 --> 02:02:53.560
But I think in practice, that's not really what we see.

02:02:53.560 --> 02:03:11.560
If you take like image task classifications, even especially now with self-supervised learning methods, we see that actually most of the information that we could think is irrelevant for the task is actually kept because it can of course help in reducing the training loss.

02:03:11.560 --> 02:03:18.560
So I think for what you said to work, you really need to be sure that either you have the perfect regularizer,

02:03:18.560 --> 02:03:23.560
or you have the perfect model and regime of training samples and so on.

02:03:23.560 --> 02:03:29.560
But in a general setting, I think it will be tricky to claim it like this.

02:03:29.560 --> 02:03:31.560
That's a very interesting point.

02:03:31.560 --> 02:03:38.560
So you're saying when you add these extra dimensions, even to learn to ignore them, you have to have lots of data.

02:03:38.560 --> 02:03:44.560
Yes, yes, or regularization or some mechanisms that exactly.

02:03:44.560 --> 02:03:46.560
Yeah, very interesting.

02:03:46.560 --> 02:03:48.560
That's one thing.

02:03:48.560 --> 02:03:54.560
That's one reason why, let's say, add up parameterization by end is sometimes useful.

02:03:54.560 --> 02:04:03.560
Because if you know which things are useless for your task, you can by end remove them from the data that you feed to your deep net or whatever model.

02:04:03.560 --> 02:04:06.560
And then doing this will improve generalization.

02:04:06.560 --> 02:04:13.560
So that's why in some cases it's still useful to do those preprocessing steps.

02:04:13.560 --> 02:04:14.560
Right.

02:04:14.560 --> 02:04:16.560
Could you give me a bit of intuition here?

02:04:16.560 --> 02:04:22.560
So we've been listening to quite a few folks that make the argument about the interpolative nature of deep learning.

02:04:22.560 --> 02:04:28.560
And you could argue that any kind of processing and machine learning, not just feature transformations,

02:04:28.560 --> 02:04:31.560
but even things like regularization and domain randomization,

02:04:31.560 --> 02:04:37.560
that there are all ways of making the problem more interpolative and it would be appropriate for an interpolated problem.

02:04:37.560 --> 02:04:42.560
So I'm thinking to myself, I mean, in this example, you take pictures of clock faces,

02:04:42.560 --> 02:04:47.560
and it's not interpolative in the ambient space, you perform some kind of feature transformation.

02:04:47.560 --> 02:04:51.560
There exists some nonlinear transformation that transforms it into an interpolative space.

02:04:51.560 --> 02:04:52.560
That's what these people say.

02:04:52.560 --> 02:04:57.560
But I'm wondering, you know, I'm interested in the curse of dimensionality.

02:04:57.560 --> 02:05:00.560
You know, why does deep learning work at all?

02:05:00.560 --> 02:05:07.560
And I've spoken to folks that talk about creating various priors to combat the curse of dimensionality.

02:05:07.560 --> 02:05:11.560
But why do you think that deep learning works at all in these high dimensional spaces?

02:05:12.560 --> 02:05:22.560
Well, so first, I might say something a bit speculative or not agreed upon by everyone,

02:05:22.560 --> 02:05:23.560
but I don't think you can...

02:05:23.560 --> 02:05:24.560
We love it. We love it. Please do.

02:05:24.560 --> 02:05:25.560
Exactly.

02:05:25.560 --> 02:05:29.560
I don't think you can state so generally that deep learning works right.

02:05:29.560 --> 02:05:31.560
You need the right architecture.

02:05:31.560 --> 02:05:33.560
You need the right loss function.

02:05:33.560 --> 02:05:36.560
You need everything right for it to work.

02:05:36.560 --> 02:05:39.560
That's why it works now because we spend, I don't know,

02:05:39.560 --> 02:05:43.560
much amount of money and manpower to get to where we are now.

02:05:43.560 --> 02:05:46.560
But if you just take a plain MLP, you apply it on ImageNet,

02:05:46.560 --> 02:05:49.560
I don't think you will say it's working right.

02:05:49.560 --> 02:05:51.560
So I think right now it's working.

02:05:51.560 --> 02:05:55.560
I mean, what we have is working because we basically, by end,

02:05:55.560 --> 02:06:01.560
did all the cross-validation such that the network that we are using is regularized enough

02:06:02.560 --> 02:06:06.560
to only learn the necessary or meaningful information,

02:06:06.560 --> 02:06:11.560
or at least as best as we can to provide good generalization performances.

02:06:11.560 --> 02:06:19.560
So, yeah, it's a bit too general of a statement to say that deep learning works out of the box for everything.

02:06:19.560 --> 02:06:20.560
In fact, many cases...

02:06:20.560 --> 02:06:21.560
I love that point.

02:06:21.560 --> 02:06:22.560
Yes, yes.

02:06:22.560 --> 02:06:28.560
Many, if you go into not just image classification, but let's say audio classification

02:06:28.560 --> 02:06:32.560
and some maybe trickier data set where you don't have a lot of samples,

02:06:32.560 --> 02:06:36.560
then everything starts to break quite rapidly, right?

02:06:36.560 --> 02:06:40.560
You have always people trying to go into the medical field as well,

02:06:40.560 --> 02:06:44.560
but since it's very hard to generalize between patients,

02:06:44.560 --> 02:06:48.560
between recording devices and so on, things get very, very messy

02:06:48.560 --> 02:06:53.560
and everything is ad hoc and optimization, basically.

02:06:54.560 --> 02:06:57.560
I don't think we're at this point yet where you can say,

02:06:57.560 --> 02:06:59.560
that's it, deep learning works out.

02:06:59.560 --> 02:07:01.560
Yeah, can I jump in real quickly here, Tim?

02:07:01.560 --> 02:07:03.560
Because what I want to say is, brilliant point.

02:07:03.560 --> 02:07:07.560
I mean, and when you think about it, let's take Professor LeCun,

02:07:07.560 --> 02:07:12.560
you know, one of his great invention, right, the CNN, Convolutional Neural Network.

02:07:12.560 --> 02:07:16.560
That was discovered by a human being, like a machine,

02:07:16.560 --> 02:07:19.560
and no neural network learned to do convolution.

02:07:19.560 --> 02:07:24.560
LeCun taught machines how to do convolution.

02:07:24.560 --> 02:07:27.560
And so we oftentimes discount this and say,

02:07:27.560 --> 02:07:31.560
look how great machine learning works, we discount all the human engineering

02:07:31.560 --> 02:07:34.560
that has gone into actually making machine learning work

02:07:34.560 --> 02:07:36.560
through specific architectures.

02:07:36.560 --> 02:07:38.560
Yes, exactly, exactly.

02:07:38.560 --> 02:07:42.560
Basically, we are guiding where to go,

02:07:42.560 --> 02:07:46.560
and then of course, once we guide enough, the machine knows what to do.

02:07:46.560 --> 02:07:51.560
I mean, this is not a reduction of what we can do with deep learning, right?

02:07:51.560 --> 02:07:54.560
But he's just saying that a generalized statement

02:07:54.560 --> 02:07:59.560
out of the box, deep learning works even in a new task that we never tried before

02:07:59.560 --> 02:08:01.560
is an overstatement.

02:08:01.560 --> 02:08:04.560
But I still want to linger on this point of why you said,

02:08:04.560 --> 02:08:06.560
or here's just a random projection of the data,

02:08:06.560 --> 02:08:09.560
and then here's a ResNet projection, here's a trained ResNet projection.

02:08:09.560 --> 02:08:14.560
What I took from that is irrespective, it's all in an extrapolative regime.

02:08:14.560 --> 02:08:17.560
And there was a guy who posted an article

02:08:17.560 --> 02:08:20.560
who is disparagingly saying that neural networks are just interpolators,

02:08:20.560 --> 02:08:23.560
analogous to a Taylor series approximation

02:08:23.560 --> 02:08:26.560
within a small part of the domain of a function,

02:08:26.560 --> 02:08:28.560
and it goes haywire outside of the domain.

02:08:28.560 --> 02:08:31.560
And there are a lot of people that would make that case,

02:08:31.560 --> 02:08:34.560
that neural networks cannot extrapolate outside of the training range,

02:08:34.560 --> 02:08:37.560
but they do seem to work remarkably well,

02:08:37.560 --> 02:08:38.560
given the curse of dimensionality.

02:08:38.560 --> 02:08:40.560
So why is that exactly?

02:08:40.560 --> 02:08:45.560
So let's say for simplification that you have a binary classification problem.

02:08:45.560 --> 02:08:48.560
So once you go into your Latin space,

02:08:48.560 --> 02:08:50.560
or if you just do linear classification,

02:08:50.560 --> 02:08:52.560
if you stay in your ambient space,

02:08:52.560 --> 02:08:55.560
basically you say you have class 0 or 1

02:08:55.560 --> 02:08:58.560
based on which side of an hyperplane you lie in.

02:08:58.560 --> 02:09:01.560
Now, if you are on the good side,

02:09:01.560 --> 02:09:03.560
you can go to the infinity, right?

02:09:03.560 --> 02:09:06.560
You can go to extrapolation regime,

02:09:06.560 --> 02:09:08.560
as far as you want from the training set,

02:09:08.560 --> 02:09:11.560
as long as you are in the correct side of that hyperplane,

02:09:11.560 --> 02:09:13.560
you will have good generalization performance.

02:09:13.560 --> 02:09:16.560
So it's not exactly correlated.

02:09:16.560 --> 02:09:18.560
The only thing you need,

02:09:18.560 --> 02:09:21.560
some O is to have the orientation of the axis

02:09:21.560 --> 02:09:23.560
where you will extrapolate

02:09:23.560 --> 02:09:27.560
to be somewhat aligned in the orthogonal space of that hyperplane.

02:09:27.560 --> 02:09:29.560
Once you have that, then you will generalize,

02:09:29.560 --> 02:09:31.560
even though you can be in extrapolation regime

02:09:31.560 --> 02:09:34.560
in ambient or feature space.

02:09:34.560 --> 02:09:39.560
So that's one thing that is maybe proper to linear classification.

02:09:39.560 --> 02:09:43.560
If you were doing maybe some other type of classifier on top of it,

02:09:43.560 --> 02:09:45.560
it might change.

02:09:45.560 --> 02:09:47.560
But given the current settings,

02:09:47.560 --> 02:09:50.560
I don't think people should expect bad performance

02:09:50.560 --> 02:09:53.560
because you are not in an interpolation regime.

02:09:53.560 --> 02:09:55.560
This is quite of a shortcut,

02:09:55.560 --> 02:09:58.560
maybe that it guided by our intuition, right?

02:09:58.560 --> 02:10:01.560
We think it's much easier to classify something

02:10:01.560 --> 02:10:03.560
if it's in an interpolation regime.

02:10:03.560 --> 02:10:06.560
But if you just look at the plane in our classifier,

02:10:06.560 --> 02:10:09.560
that's not the case.

02:10:09.560 --> 02:10:11.560
Well, so just to steal man a bit

02:10:11.560 --> 02:10:15.560
what say others on the interpolation side,

02:10:15.560 --> 02:10:18.560
again, that's only if you're buying into a very rigid definition

02:10:18.560 --> 02:10:21.560
of interpolation, which the way the paper defines it

02:10:21.560 --> 02:10:23.560
is in a linear way.

02:10:23.560 --> 02:10:27.560
I mean, it's defining interpolation by this linear convex hull.

02:10:27.560 --> 02:10:30.560
And of course, neural networks, for example,

02:10:30.560 --> 02:10:32.560
are highly nonlinear systems.

02:10:32.560 --> 02:10:36.560
And what you just said, sort of which side of a separating hyperplane you're on,

02:10:36.560 --> 02:10:38.560
that itself is a nonlinear function.

02:10:38.560 --> 02:10:42.560
And so I think the point you made early on was very good,

02:10:42.560 --> 02:10:45.560
which is we have this intuition of what interpolation is.

02:10:45.560 --> 02:10:49.560
We haven't yet got a good definition for interpolation

02:10:49.560 --> 02:10:54.560
in multi or high-dimensionality nonlinear cases.

02:10:54.560 --> 02:10:59.560
But obviously the linear definition doesn't really apply.

02:10:59.560 --> 02:11:01.560
Yeah, exactly.

02:11:01.560 --> 02:11:06.560
It's really because, I mean, it's linear,

02:11:06.560 --> 02:11:10.560
but again, you can consider your data as being nonlinearly transformed.

02:11:10.560 --> 02:11:14.560
The problem is still that if you have high-dimensionality,

02:11:14.560 --> 02:11:18.560
then to be within a convex hull becomes exponentially hard

02:11:18.560 --> 02:11:21.560
using the convex hull of the training side.

02:11:21.560 --> 02:11:24.560
If you define something else, like you said before the hypercube,

02:11:24.560 --> 02:11:26.560
it could be the opposite.

02:11:26.560 --> 02:11:29.560
It's always in interpolation regime.

02:11:29.560 --> 02:11:34.560
And the key is basically to find a meaningful set

02:11:34.560 --> 02:11:38.560
such that you don't go all the way in one direction or the other.

02:11:38.560 --> 02:11:42.560
And because I think the main point also is that this should give intuition

02:11:42.560 --> 02:11:45.560
into the generalization performance of a model, right?

02:11:45.560 --> 02:11:50.560
Because if you can detect a sample is in an extrapolation regime,

02:11:50.560 --> 02:11:53.560
but your model still performs very good on it.

02:11:53.560 --> 02:11:56.560
I don't know if it has really a lot of practical use.

02:11:56.560 --> 02:12:00.560
So what will be good as well is to have the basically

02:12:00.560 --> 02:12:06.560
generalization performance of a model correlate with your definition of extrapolation.

02:12:06.560 --> 02:12:09.560
And that's really what we are trying to get.

02:12:09.560 --> 02:12:14.560
And that's why I was saying that probably you might have a task-dependent definition,

02:12:14.560 --> 02:12:20.560
because that might be where you get the best precise definition to reach that.

02:12:21.560 --> 02:12:26.560
I was wondering to what extent, if any of this invalidates the so-called manifold hypothesis.

02:12:26.560 --> 02:12:30.560
So I think when most people speak of interpolation and deep learning,

02:12:30.560 --> 02:12:32.560
their intuition is something along the lines of,

02:12:32.560 --> 02:12:36.560
well, the model learns some manifold in the latent space,

02:12:36.560 --> 02:12:42.560
and interpolation means I'm just kind of like traversing the geodesic on that manifold.

02:12:42.560 --> 02:12:45.560
And when you visualize the results on an autoencoder or something,

02:12:45.560 --> 02:12:48.560
you can see this kind of continuous geometric morphing

02:12:48.560 --> 02:12:51.560
of, let's say, one image into another image.

02:12:51.560 --> 02:12:55.560
And that manifold, I think you said in your paper that the actual data manifold,

02:12:55.560 --> 02:12:59.560
it's not possible to approximate that well, but it's doing something interesting.

02:12:59.560 --> 02:13:03.560
And a prediction on that manifold in the intermediate space,

02:13:03.560 --> 02:13:05.560
it's not massively deranged, is it?

02:13:05.560 --> 02:13:07.560
It's still doing something very useful, statistically.

02:13:07.560 --> 02:13:08.560
Yes, yes, yes, totally.

02:13:08.560 --> 02:13:11.560
So I think it does not at all contradict it.

02:13:11.560 --> 02:13:16.560
And in fact, think of a very, very simple example where you say your data

02:13:16.560 --> 02:13:20.560
is a linear manifold of dimension 100, let's say,

02:13:20.560 --> 02:13:25.560
which is a relatively meaningful number of dimensions even for images.

02:13:25.560 --> 02:13:28.560
But now suppose it's completely linear.

02:13:28.560 --> 02:13:32.560
Well, even in that case, it will be hard to be in interpolation regime

02:13:32.560 --> 02:13:34.560
just because you have 100 dimensions

02:13:34.560 --> 02:13:40.560
and picking a new sample that lies in the convex of your training set is exponentially higher.

02:13:40.560 --> 02:13:44.560
So you can have a manifold for your data.

02:13:44.560 --> 02:13:46.560
It can even be linear.

02:13:46.560 --> 02:13:48.560
So the simplest manifold you wish for,

02:13:48.560 --> 02:13:51.560
the only problem is the dimensionality of that manifold.

02:13:51.560 --> 02:13:55.560
Now that being said, again, it does not mean that you cannot do

02:13:55.560 --> 02:13:59.560
basically moving on the geodesic of your manifold

02:13:59.560 --> 02:14:01.560
or you cannot do some sort of interpolation

02:14:01.560 --> 02:14:06.560
because that's what happens in most of the current state-of-the-art generative networks, right?

02:14:06.560 --> 02:14:10.560
It's just that you don't need to be in interpolation regime

02:14:10.560 --> 02:14:14.560
to have a correct generation process, basically.

02:14:14.560 --> 02:14:18.560
And that's, again, one thing that is very important.

02:14:18.560 --> 02:14:21.560
And you mentioned, you have those examples.

02:14:21.560 --> 02:14:24.560
That's true that the interpolation is extremely nice,

02:14:24.560 --> 02:14:28.560
but again, it does not say that the points you start with

02:14:28.560 --> 02:14:32.560
and the point you end up with are in interpolation regime of your training set.

02:14:32.560 --> 02:14:37.560
So I think it's two different things to be able to interpolate or move on your manifold

02:14:37.560 --> 02:14:41.560
and being in the interpolation regime from your training set.

02:14:41.560 --> 02:14:46.560
So what's intriguing me about, and I want to clarify something earlier,

02:14:46.560 --> 02:14:52.560
which is that though a hyperplane is a linear construct,

02:14:52.560 --> 02:14:58.560
the function that says you're on one side or the other of a hyperplane is a nonlinear function.

02:14:58.560 --> 02:15:03.560
And in that case, it's a digital function.

02:15:03.560 --> 02:15:06.560
You're either on this side or that side, it's zero or one.

02:15:06.560 --> 02:15:12.560
What's kind of interesting is you gave that as an explanation for why machine learning works at all,

02:15:12.560 --> 02:15:18.560
which is that for data sets that we care about, or problem spaces that we care about,

02:15:18.560 --> 02:15:25.560
it seems to often be the case that we can do enough sequences of linear and nonlinear transformations

02:15:25.560 --> 02:15:32.560
to arrange the data such that it falls on one side or another of a separating plane.

02:15:32.560 --> 02:15:39.560
In a sense, it seems like real-world data is often separable if you do enough transformations on it.

02:15:39.560 --> 02:15:47.560
And since that separability is a very digital concept, is there anything interesting in that?

02:15:47.560 --> 02:15:49.560
Any intuition?

02:15:49.560 --> 02:15:52.560
Yes, yes. So for this, you still have to be careful, right?

02:15:52.560 --> 02:15:59.560
Because if you go in high-dimensional spaces, you can basically separate everything very easily.

02:15:59.560 --> 02:16:02.560
But your generalization performance might be bad.

02:16:02.560 --> 02:16:09.560
So basically, what you want is really to have, let's say, a good ratio of how much you expand dimension,

02:16:09.560 --> 02:16:14.560
how much you nonlinearly transform your data, to then reach a good generalization performance.

02:16:14.560 --> 02:16:18.560
Because otherwise, you can just fall back into a kernel regime, let's say,

02:16:18.560 --> 02:16:23.560
and you expand so much your space that sure you can separate everything on your training set,

02:16:23.560 --> 02:16:26.560
but then the generalization is going to be very poor.

02:16:26.560 --> 02:16:32.560
And that's one strength of deep networks, right, is that you have so much of those nonlinear transformations

02:16:32.560 --> 02:16:38.560
that you can somehow not expand the dimension of your space too much or even contract it,

02:16:38.560 --> 02:16:47.560
and still have a separating hyperplane in the end where generalization is much higher than in other class of models.

02:16:47.560 --> 02:16:53.560
So I think the key is really to have those meaningful nonlinear transformations

02:16:53.560 --> 02:16:56.560
such that you don't have to increase your space too much.

02:16:56.560 --> 02:16:59.560
You just shape it around, if you will.

02:16:59.560 --> 02:17:02.560
And what you said earlier is exactly true.

02:17:02.560 --> 02:17:06.560
If you think of which side you are in, basically, you are binarizing your data, right?

02:17:06.560 --> 02:17:15.560
And if you have good classification, it means all the classes are assigned to the same labels, which is 1 or 0.

02:17:15.560 --> 02:17:22.560
And if you think of it still in interpolation regime, then suddenly you are in interpolation regime, right?

02:17:22.560 --> 02:17:30.560
You are a 1. The new sample is a 1 after binarization, and you become interpolation regime, and you have a good performance.

02:17:30.560 --> 02:17:36.560
But this comes after this compression step, if you will, or discretization step.

02:17:36.560 --> 02:17:41.560
And that might be another direction to explore as well.

02:17:41.560 --> 02:17:47.560
If you start to quantize things or to compress things, as we were saying a bit at the beginning,

02:17:47.560 --> 02:17:53.560
then you can reach the interpolation much more easily as well.

02:17:53.560 --> 02:17:55.560
I have a couple of questions.

02:17:55.560 --> 02:18:01.560
So again, in Table 1, I mean, your paper is making the argument that everything is extrapolation,

02:18:01.560 --> 02:18:04.560
given this convex notion in high-dimensional space.

02:18:04.560 --> 02:18:12.560
But if we zoom in a little bit, though, so you are using a pre-trained ResNet classifier, pre-trained on ImageNet,

02:18:12.560 --> 02:18:18.560
and how do all of these things change the structure of the latent space in a meaningful way?

02:18:18.560 --> 02:18:21.560
And the embedding space is also highly distributed.

02:18:21.560 --> 02:18:25.560
And we were wondering, can you give us some intuition here?

02:18:25.560 --> 02:18:30.560
So is the information likely to be quite evenly distributed over the latent,

02:18:30.560 --> 02:18:36.560
or do you think it's actually quite bunched up and sparsely encoded in few of the features?

02:18:37.560 --> 02:18:41.560
So I think this will depend on which training setting you use, right?

02:18:41.560 --> 02:18:44.560
So for example, if you start using dropout and things like that,

02:18:44.560 --> 02:18:52.560
you will try to have a more evenly distribution of your information to have a more stable loss when you drop those units.

02:18:52.560 --> 02:18:54.560
So I don't think you have a general answer for that.

02:18:54.560 --> 02:19:00.560
It will depend on the type of regularization you have or training is done, etc.

02:19:00.560 --> 02:19:06.560
But you have to keep in mind that what you try to do with gradient descent is just minimize your loss, right?

02:19:06.560 --> 02:19:11.560
But then with cross-ontropolis, let's say, your gradient starts to vanish as you become really good

02:19:11.560 --> 02:19:14.560
and you stop learning where you are, basically.

02:19:14.560 --> 02:19:21.560
So given that, depending on your initialization, you will still try to make the best of what you get.

02:19:21.560 --> 02:19:29.560
And even if it means learning redundant dimensions, if this can reduce your loss further at a more rapid rate,

02:19:29.560 --> 02:19:30.560
that's what you will do.

02:19:30.560 --> 02:19:39.560
So if you don't impose any regularization or anything, there is no clear reason to assume that everything is well organized and so on.

02:19:39.560 --> 02:19:41.560
And that's what we see even in generative networks.

02:19:41.560 --> 02:19:48.560
You have to start putting so much regularization to try to have disentanglement and to try to make sense of those latent spaces.

02:19:48.560 --> 02:19:57.560
Because otherwise, you just try to learn what minimizes your loss with the most short-term view of your loss landscape.

02:19:57.560 --> 02:20:03.560
So basically, that could be built if you have some specific regularizer.

02:20:03.560 --> 02:20:07.560
But otherwise, it will not occur naturally.

02:20:07.560 --> 02:20:14.560
Of course, and again, if you wanted to only retain the minimal information to solve the task at hand,

02:20:14.560 --> 02:20:19.560
then you will see much more interpolation regime in that latent space.

02:20:19.560 --> 02:20:28.560
If you think of MNIST, for example, you will disregard the translation of your digits, the rotation of the digits, all those things.

02:20:28.560 --> 02:20:31.560
All those things will be disregarded when you reach the latent space.

02:20:31.560 --> 02:20:36.560
And then you will basically be in interpolation regime most of the time.

02:20:36.560 --> 02:20:45.560
But since you keep as much information as possible to try to minimize your loss as best as you can, then you basically occupy as much as you can.

02:20:45.560 --> 02:20:50.560
Unless, again, you have some degeneracies because of the whole architecture tricks.

02:20:50.560 --> 02:20:57.560
For example, if you have a bottleneck layer, you will limit the dimensionality of the manifold you span in the latent space.

02:20:57.560 --> 02:21:01.560
So you can have all those parameters that can play a big role.

02:21:01.560 --> 02:21:06.560
So it will be in the general setting. I don't think you could assume anything.

02:21:07.560 --> 02:21:17.560
And I think there's also a relationship, too, between the dimensionality of the latent space and, let's say, some intrinsic dimensionality of the problem.

02:21:17.560 --> 02:21:27.560
So if the intrinsic dimensionality of the problem only takes five dimensions to solve, and yet I give a latent space of 256 dimensions,

02:21:28.560 --> 02:21:36.560
I think what I hear you saying is that, of course, gradient descent is going to make some use of those other 251 dimensions,

02:21:36.560 --> 02:21:43.560
but they're going to have maybe a very minuscule or diminishing effect on the latent space.

02:21:43.560 --> 02:21:50.560
Whereas on the other hand, if I then took that same network and increased the complexity of the problem,

02:21:50.560 --> 02:21:55.560
we could end up with, for example, it's sparsifying for any particular class.

02:21:55.560 --> 02:22:04.560
So if we're doing some multi-class problem, we may find that it sort of arranges these seven dimensions to solve the dog versus hot dog problem,

02:22:04.560 --> 02:22:09.560
and these 12 dimensions to dissolve the car versus motorcycle problem.

02:22:09.560 --> 02:22:16.560
It might be forced to make more compact use of that latent information space per class. Is that fair?

02:22:16.560 --> 02:22:18.560
Yes, so that's a very good point.

02:22:18.560 --> 02:22:27.560
So first of the first things that you said, one thing to keep in mind, so let's say your data is even linear manifold of dimension one,

02:22:27.560 --> 02:22:33.560
and then you go through a deep net, and then somehow it's already linearly separable.

02:22:33.560 --> 02:22:37.560
Then you only need to learn the identity mapping with your deep net to solve the task.

02:22:37.560 --> 02:22:41.560
But if you start from a random initialization, it's extremely hard to do that.

02:22:41.560 --> 02:22:44.560
That's why people came up with ResNet and all those things.

02:22:44.560 --> 02:22:54.560
So already from this point of view, it means that almost surely your linear one-dimensional manifold will become highly non-linear,

02:22:54.560 --> 02:23:00.560
still one-dimensional, but very highly non-linear in the latent space of your classifier.

02:23:00.560 --> 02:23:06.560
And as we showed in the figure one of the paper, even if the intrinsic dimension is one,

02:23:06.560 --> 02:23:11.560
if you are highly non-linear in your space, then you will basically never be in interpolation regime.

02:23:12.560 --> 02:23:17.560
So you have those sort of artifacts that come just from the fact that learning simple mapping,

02:23:17.560 --> 02:23:22.560
let's say, with a non-linear deep network is not always simple.

02:23:22.560 --> 02:23:32.560
And those artifacts will be introduced right away because it's so hard in your parameter space to reach that point where you have identity mapping.

02:23:32.560 --> 02:23:42.560
So this is another effect that kicks in and that can somehow remove those assumptions that even if your data is in a low-dimensional regime

02:23:42.560 --> 02:23:49.560
and almost linear, it will be preserved in the output space of your network.

02:23:49.560 --> 02:23:54.560
So this is something really important to keep in mind as well.

02:23:54.560 --> 02:24:03.560
So, Randall, I also noticed in about 2018, you were the first author on some really interesting work and it was called a spline theory of deep learning.

02:24:03.560 --> 02:24:06.560
And then I think the next year it got into Neuribs.

02:24:06.560 --> 02:24:09.560
So I'm just reading a bit from the abstract.

02:24:09.560 --> 02:24:16.560
So you said you built a rigorous bridge between deep networks and approximation theory via spline functions and operators.

02:24:16.560 --> 02:24:25.560
And you actually think that this can explain a lot about deep learning in the sense of then being a composition of these max-safine spline operators.

02:24:25.560 --> 02:24:28.560
Has any of this work informed your view of deep learning now?

02:24:28.560 --> 02:24:30.560
Yes, a lot actually.

02:24:30.560 --> 02:24:38.560
So first of all, I want to be precise that of course a lot of people knew about the fact that if you use, for example,

02:24:38.560 --> 02:24:46.560
volume activations, absolute value and so on, or max-pulling, then the whole network is continuous piecewise linear mapping.

02:24:46.560 --> 02:24:55.560
So what we did mostly is to make this a bit more rigorous and try to understand what the partition of the input space looks like,

02:24:55.560 --> 02:25:02.560
what the perigen mapping looks like, and how can we use that to gain some more intuitions into what's happening.

02:25:02.560 --> 02:25:08.560
And the nice thing with this is that if you think about it, there is nothing simpler than piecewise linear mappings, right?

02:25:08.560 --> 02:25:18.560
Basically, it says that if you are in a region of your space, a region from the partition of your mapping, then the input-output mapping will stay linear.

02:25:18.560 --> 02:25:24.560
And this allows to do a lot of analysis, for example, to adversarial perturbation or all these type of things.

02:25:24.560 --> 02:25:28.560
And it can also open the door to other lines of research.

02:25:28.560 --> 02:25:38.560
For example, one thing we did, I think it was NeurIPS last year or two years ago, was to use that to derive the exact expectation maximization algorithm for the generative networks,

02:25:38.560 --> 02:25:44.560
because now you have a cleaner, let's say, or simpler analytical form of your network.

02:25:44.560 --> 02:25:52.560
And this really opens to me the door to derive some more interesting theoretical results.

02:25:52.560 --> 02:26:08.560
It does not really help intuitively, because I think everyone had this intuition from before already, but it's mostly a mathematical tool that allows to derive with small is some interesting results.

02:26:08.560 --> 02:26:24.560
And I think one important thing as well is that you have really this dichotomy right between the, let's say, old school signal processing, template matching type of academia researchers,

02:26:24.560 --> 02:26:28.560
and the new school with deep learning, everything has to be trained and so on.

02:26:28.560 --> 02:26:32.560
And what is interesting in this paper is that we somehow bridge the two.

02:26:32.560 --> 02:26:46.560
We say that basically a deep net is a very smart way to build an adaptive spline, which will learn automatically its partition of the input space and the perigen affine mapping, such that it works in high dimension.

02:26:46.560 --> 02:26:49.560
And this was not known before by anyone in the spline theory.

02:26:49.560 --> 02:27:05.560
So I think to speak about adaptive spline in high dimension, no one has no idea what to do except dimension two or three, maybe, because a lot of PDE work uses, but outside of dimension three, no one thinks about splines.

02:27:05.560 --> 02:27:11.560
So I think there's a very strong result is to bridge those two different fields.

02:27:11.560 --> 02:27:21.560
That's fascinating. I think one of the issues, because I'm speaking to Juan Bruno about this, and he was saying there was a big tradition in harmonic analysis of trying to reason about the behavior of these models.

02:27:21.560 --> 02:27:31.560
And we did have the universal function approximation theorem, which in a sense is talking about stacking basis functions, you know, to approximate an arbitrary function.

02:27:31.560 --> 02:27:42.560
But you say in your abstract here that the spline partition of the input signal opens up in a new, you know, opens up a new geometric avenue to study how deep neural networks organize signals in a hierarchical fashion.

02:27:42.560 --> 02:27:49.560
I don't think people really have much of an intuition on how these models behave and how to reason about them.

02:27:49.560 --> 02:27:51.560
Yes. Yeah, that's a very good point.

02:27:51.560 --> 02:27:59.560
So one thing that we did, for example, is to study all the partition of the mapping evolves as you go through the layers.

02:27:59.560 --> 02:28:05.560
And what we show is that at each layer, you keep subdividing your current partition.

02:28:05.560 --> 02:28:18.560
And this is very interesting because once you know that if you look at a binary classification, for example, you get that the decision boundary is basically linear in each region of the partition.

02:28:18.560 --> 02:28:28.560
And so what this tells you is that as you add layer, what you have to do is to refine the regions that still contain more than one class within them.

02:28:28.560 --> 02:28:39.560
So this kind really brings insights and maybe opens the door to building new learning techniques to how many layers you should stack, which regions should they subdivide and so on.

02:28:39.560 --> 02:28:44.560
And this is really akin to decision trees and how they build their partition as well.

02:28:44.560 --> 02:28:51.560
So there is a lot of work that could be done also to maybe bridge the two or use one to understand the other.

02:28:51.560 --> 02:29:04.560
And it's really geometrical because it's in the whole field of computational geometry and so on because we have those hyperplane arrangements, those half-spaces, intersection of them, hyperplane desalation.

02:29:04.560 --> 02:29:07.560
It's also a known system.

02:29:07.560 --> 02:29:14.560
So it's really geometrical and it can have a lot of interesting insights to understand what's happening.

02:29:14.560 --> 02:29:18.560
It also provides nice visualization tools.

02:29:18.560 --> 02:29:21.560
It's really fascinating.

02:29:21.560 --> 02:29:27.560
Would you be interested in coming back on the show for a future episode just dedicated to this topic?

02:29:27.560 --> 02:29:30.560
That'd be amazing.

02:29:30.560 --> 02:29:35.560
Randall also actually released a paper called Neural Decision Trees.

02:29:35.560 --> 02:29:42.560
And in the abstract he said they propose a synergistic melting of neural networks and decision trees.

02:29:42.560 --> 02:29:47.560
So this is something that you've been thinking about actually from many angles.

02:29:47.560 --> 02:29:49.560
Yes, yes, yes.

02:29:49.560 --> 02:29:55.560
And that's very, very synergic to think about one from the point of view of the other as well, right?

02:29:55.560 --> 02:30:08.560
Because if you think of a decision tree, the real limitation of it is that all you subdivide one region by adding a node does not really tell you how to subdivide another region in another part of the space.

02:30:08.560 --> 02:30:14.560
You don't have this, let's say, communication or friendly help between the region subdivision.

02:30:14.560 --> 02:30:23.560
But in a deep net, what you do actually is that if you know how to subdivide one region, then it will automatically enforce or you subdivide nearby regions.

02:30:23.560 --> 02:30:31.560
And through this, suddenly you have a mechanism that appears which is that you don't need samples in each region to know to subdivide them.

02:30:31.560 --> 02:30:38.560
You just need samples in some of the regions and then it will guide you on how to subdivide regions of the space without samples.

02:30:38.560 --> 02:30:47.560
And that's something that is extremely strong when you go to high dimensional settings because you cannot have samples in all parts of the space by definition.

02:30:47.560 --> 02:30:56.560
So I think this is extremely nice to have both point of view because then you can try to use the strengths of one to maybe improve the other.

02:30:56.560 --> 02:30:59.560
So that's really interesting to me.

02:30:59.560 --> 02:31:14.560
I want to ask you this question that we ask a lot of our guests because it's just at least something that's kind of profound interest to me is that there is this apparent dichotomy between continuous and discreet.

02:31:14.560 --> 02:31:31.560
It's like the human brain is at the most lowest possible level and analog kind of continuous system and yet it evolved all these sort of discreet almost computations on top of it like pulses that either fire or don't fire like that type of thing.

02:31:31.560 --> 02:31:47.560
And in the in the learning world or let's say really in the computation world we have the fact that you know I can write a very short piece of symbolic code discreet code that that can go and calculate the nth digit of Pi or something like that.

02:31:47.560 --> 02:31:55.560
But it's almost impossible to train or it is currently impossible to train any neural network of fixed depth to do the same thing.

02:31:55.560 --> 02:32:06.560
And so we have this this weird you know different regimes we have the discreet kind of logic reasoning type world and then we have the continuous differentiable type of world.

02:32:06.560 --> 02:32:28.560
Do you view those as I mean are they fundamentally different regimes and we're always going to have hybrid systems that kind of combine both types of reasoning or is it possible to just say project that discreet computation fully into an analog type space if you just have enough you know parameters or something.

02:32:28.560 --> 02:32:50.560
Yeah I think so I think it really depends on the resources that you have right because to me at least it seems that the hybrid system might be the most efficient where you can easily let's say cluster different settings into groups and then and for this you can just have a discreet settings and then within each group.

02:32:51.560 --> 02:32:56.560
Discreetization is not good enough anymore and you need to go into the continuous regime.

02:32:56.560 --> 02:33:07.560
So I think it will depend on the application you have at hand or or efficient you want to be doing it either energy wise or anything else.

02:33:07.560 --> 02:33:28.560
So I think it's not not clear if one should dominate those are necessarily like for example in the if you go back to the spline setting of a deep net you have discreet setting which is basically your partition which which which region you are in and then within that region.

02:33:28.560 --> 02:33:37.560
You have a linear transformation of the mapping which is basically continuous and both interact if you adapt one the other one change and so on.

02:33:37.560 --> 02:33:51.560
And I think having this type of hybrid systems and where somehow learning through the continuous part adapts the discreet part is what is extremely powerful.

02:33:51.560 --> 02:34:06.560
And that's I think one extremely beautiful property of current networks is that they do automatically this adaptive training of their discreet path through training of their continuous parameters.

02:34:06.560 --> 02:34:09.560
And that's why they are so efficient.

02:34:09.560 --> 02:34:18.560
If you think of pure approximation theory and you have an adaptive spline in one day that's why you have the best convergence rate basically.

02:34:18.560 --> 02:34:22.560
So I think you really need both systems to interact.

02:34:22.560 --> 02:34:29.560
If they don't interact then I think it's really easy to become suboptimal and interesting.

02:34:29.560 --> 02:34:37.560
Because we put a similar question to Lacune and he was kind of saying that in an ideal world we would have a discreet system as well.

02:34:37.560 --> 02:34:42.560
Humans are really bad at playing chess because we don't have that discreet system built in.

02:34:42.560 --> 02:34:51.560
But the problem I think people like Lacune have with these discrete systems is typically they're symbolic and they're statically coded.

02:34:51.560 --> 02:34:59.560
You could start talking about getting into a discreet program search and you could even guide that program search based on some deep learning model.

02:34:59.560 --> 02:35:10.560
But I don't think to Keith's point I don't think it's really possible to do that well inside the continuous domain because if the problem even was learnable with stochastic gradient descent.

02:35:10.560 --> 02:35:15.560
The representation would be glitchy. It just wouldn't work.

02:35:15.560 --> 02:35:22.560
I think it depends a lot too on what are you trying to achieve with a model you build.

02:35:22.560 --> 02:35:33.560
If you just try to be as close as possible to let's say what the human brain is doing then you might impose yourself to have some restriction on do you want it to be discreet or not.

02:35:33.560 --> 02:35:39.560
Or if you just want to have a model that you can deploy on a task and it can solve the task as best as you want.

02:35:39.560 --> 02:35:50.560
So I think depending on what is your goal and what are you trying to imitate with the model would change or you answer that as well.

02:35:50.560 --> 02:35:54.560
But what if the goal was task acquisition efficiency?

02:35:54.560 --> 02:35:59.560
So it's like I don't know what the task is yet.

02:35:59.560 --> 02:36:12.560
Yeah I think that's like again to me a hybrid system where you have interaction between both parts intuitively would be the most efficient.

02:36:12.560 --> 02:36:18.560
But yeah it might not be true for all settings.

02:36:19.560 --> 02:36:30.560
When I was looking at figure three both Tim and I were interested in the fact that if you look at the MNIST data which to a human being is kind of a simpler data set.

02:36:30.560 --> 02:36:34.560
One, two, three we know how to do that type of thing.

02:36:35.560 --> 02:36:49.560
That as you increase dimensionality it much more rapidly becomes extrapolation versus image net which seems to kind of more slowly transition from interpolation to image extrapolation.

02:36:49.560 --> 02:37:01.560
And what I'm wondering is the intuition I got from that and I wonder if this is completely wrong or it's correct is that for machine learning there's a sense in which MNIST is actually a harder problem.

02:37:01.560 --> 02:37:12.560
Because it has to look at kind of global relationships like it has to try and say well there's a circle over here that's kind of oriented with respect to a line that's kind of further away.

02:37:12.560 --> 02:37:26.560
And so it's harder for it to do that whereas we know that with like image net very frequently ML sort of devolves to looking at these micro texture like you know well everything that has this shade of yellow is a school bus you know type of thing.

02:37:26.560 --> 02:37:33.560
Is that an intuition one can take from that plot or what does it mean that MNIST decreases so much more quickly.

02:37:33.560 --> 02:37:39.560
Yes, so that's actually some things that we tried to clarify with a figure that comes after this one.

02:37:39.560 --> 02:37:48.560
Basically the thing that you really need to be careful about and keep in mind is that if you look at for example a 16 by 16 patch for MNIST.

02:37:48.560 --> 02:37:56.560
You have basically maybe most of the information you need to solve the task and you have a lot of texture about your DG etc.

02:37:56.560 --> 02:38:08.560
If you look at image net and you take a 16 by 16 patch you have basically no information about the class it's extremely small patch it's almost constant across the spatial dimensions right.

02:38:08.560 --> 02:38:13.560
It's basically a very small percentage of your full image.

02:38:13.560 --> 02:38:22.560
So that's why MNIST goes much more quickly to extrapolation regime for a fixed dimensionality in pixel space.

02:38:22.560 --> 02:38:28.560
Because the information you have in that amount of dimension is greater than for the image net case.

02:38:28.560 --> 02:38:33.560
And this is only because it's already much more done sample right.

02:38:33.560 --> 02:38:46.560
If we were looking at MNIST but with a 224 by 224 spatial dimension and we look at a fixed dimensionality then you will not have this difference anymore and it might even reverse.

02:38:46.560 --> 02:38:58.560
So the really important thing to keep in mind is that even though it's the same dimension for both it does not represent the same proportion of image that is present within that patch.

02:38:58.560 --> 02:39:05.560
And that's why you have those differences in that's mostly why you have those differences in those curves.

02:39:05.560 --> 02:39:20.560
It's funny because we had the opposite intuition and so in the following figure 4 you're showing then on MNIST that more of the variance is explained with fewer of the principal components on MNIST.

02:39:20.560 --> 02:39:26.560
And as you say that that does just because it's those pixels on MNIST are more salient for the problem.

02:39:26.560 --> 02:39:29.560
I mean just so I understand because we were debating this a little bit.

02:39:29.560 --> 02:39:32.560
Could you just give us your articulation of figure 4?

02:39:32.560 --> 02:39:47.560
Yes sure. So basically what we are looking for the increasing dimensionality for the three data set is we pick a number of dimension in the spatial space.

02:39:47.560 --> 02:39:50.560
So we do this by extracting a patch.

02:39:50.560 --> 02:39:56.560
And then what we do is that we extract of course the same patch for all the samples.

02:39:56.560 --> 02:40:04.560
And then we are looking at the proportion of the test set patches that are in interpolation regime and we report this.

02:40:04.560 --> 02:40:16.560
Now for the PCA plot what we do is basically we look at once you extract this patch how many principal components you need to explain to perfectly reconstruct those patches.

02:40:16.560 --> 02:40:20.560
Or you could say to explain the variance in those patches.

02:40:20.560 --> 02:40:27.560
And this gives direct relationship because it shows how concentrated you are on lower dimensional manifolds.

02:40:27.560 --> 02:40:31.560
A fine one of course that learns through the principal components.

02:40:31.560 --> 02:40:41.560
And it means that if you can encode much more information with much less principal components then you lie on a lower dimensional affine manifold.

02:40:41.560 --> 02:40:48.560
And this coupled with figure 1 shows that basically it's much easier to be in interpolation regime.

02:40:48.560 --> 02:41:01.560
So the whole point of using this PCA plot was to show how good low dimensional affine manifold represents the current extracted patches.

02:41:01.560 --> 02:41:07.560
And then to use this as a way to justify the extrapolation regime curve that we see.

02:41:07.560 --> 02:41:11.560
So because again in the PCA regime it's linear manifold.

02:41:11.560 --> 02:41:21.560
So if only two components for example perfectly describe all your patches then you will need very few training samples to be in interpolation regime.

02:41:21.560 --> 02:41:23.560
Okay that makes sense.

02:41:23.560 --> 02:41:29.560
And then the staircase effect on the smooth subsample row is a function of the size of the Gaussian filter used to smooth it?

02:41:29.560 --> 02:41:38.560
Yeah so the staircase occurs basically whenever the number of dimensions increase and we get a new bigger patch to get it.

02:41:38.560 --> 02:41:42.560
Because there is different ways to get it right.

02:41:42.560 --> 02:41:52.560
One would be to always extract the center patch and remove some dimensions if you don't have exact number of dimensions that you can represent with a square patch.

02:41:52.560 --> 02:41:58.560
Another thing you could do is to first smooth subsample and then remove the dimensions.

02:41:58.560 --> 02:42:02.560
So there is different variants on how to extract those patches.

02:42:02.560 --> 02:42:09.560
We try to show two different ones to show that the results do not really depend on how you do this process.

02:42:09.560 --> 02:42:14.560
But yeah you will have some little different artifacts like this.

02:42:14.560 --> 02:42:21.560
It does not change the overall trend but yeah it can change the small trends when you change from one dimension to another.

02:42:21.560 --> 02:42:25.560
Well let me ask a question here about figure four.

02:42:25.560 --> 02:42:31.560
Again about the intuition that we had on leaving aside interpolation and extrapolation for the moment.

02:42:31.560 --> 02:42:43.560
It seems that MNIST for a given amount of variance explained and for a given dimensionality MNIST requires more principal components.

02:42:43.560 --> 02:42:46.560
For a given amount of dimension yeah.

02:42:46.560 --> 02:42:54.560
If we fix the dimension and we fix the percent variance explained MNIST requires more principal components than ImageNet.

02:42:54.560 --> 02:42:58.560
That seems to me to tell me that MNIST is a more difficult problem.

02:42:58.560 --> 02:43:00.560
Is that not true?

02:43:00.560 --> 02:43:05.560
I think it's a bit so for a specific number of dimensions you could argue that.

02:43:05.560 --> 02:43:12.560
But the thing you have to recall is that because MNIST images are much smaller in spatial dimensions.

02:43:12.560 --> 02:43:18.560
If you have a 16 by 16 patch you have basically the whole MNIST dataset let's say.

02:43:18.560 --> 02:43:24.560
And so just having a few principal components is not enough to really reconstruct the whole MNIST dataset.

02:43:24.560 --> 02:43:30.560
Now on ImageNet a 16 by 16 patch is almost a constant texture right.

02:43:30.560 --> 02:43:33.560
You have a few different colors but you don't have a lot of variation.

02:43:33.560 --> 02:43:38.560
It's basically through the spatial dimensions it's basically constant.

02:43:38.560 --> 02:43:45.560
And what this means is that with only three or four principal components basically one for each color channel.

02:43:45.560 --> 02:43:49.560
You can perfectly reconstruct all those 16 by 16 patches.

02:43:49.560 --> 02:43:58.560
So to really get to the conclusions you are saying what you will have to do first is to either done sample ImageNet to be 28 by 28

02:43:58.560 --> 02:44:02.560
or up sample MNIST to be 224 by 224.

02:44:02.560 --> 02:44:10.560
And if you do that then basically I think you will have the roughly same evolution of the interpolation regime.

02:44:10.560 --> 02:44:20.560
Because the 16 by 16 patch on this extra up sample MNIST image will be either completely black or completely white.

02:44:20.560 --> 02:44:24.560
And in this case you will still need a few principal components.

02:44:24.560 --> 02:44:29.560
So this is also something very interesting right because it shows that maybe for the task at hand

02:44:29.560 --> 02:44:34.560
you might not need to have such a high resolution image you might done sample.

02:44:34.560 --> 02:44:41.560
But because when you done sample you basically keep those in crucial information

02:44:41.560 --> 02:44:44.560
you don't necessarily go faster to interpolation regime.

02:44:44.560 --> 02:44:47.560
So this is another point.

02:44:47.560 --> 02:44:55.560
It's really tricky you should think of it as how much of the image do I uncut given that number of dimensions.

02:44:55.560 --> 02:45:01.560
And then given that this plot might be easier to understand.

02:45:01.560 --> 02:45:04.560
16 by 16 by each.

02:45:04.560 --> 02:45:09.560
Yeah exactly so 16 by 16 on MNIST is maybe around 60-70% of the image.

02:45:09.560 --> 02:45:14.560
While 16 by 16 on ImageNet is maybe around maybe 5% of the image.

02:45:14.560 --> 02:45:18.560
And that's why you have those different regimes that appear.

02:45:18.560 --> 02:45:20.560
Incredible.

02:45:20.560 --> 02:45:23.560
Randall thank you so much for joining us this evening.

02:45:23.560 --> 02:45:25.560
Sure sure thanks.

02:45:25.560 --> 02:45:28.560
So we just spoke with Randall what was your take on that Keith?

02:45:28.560 --> 02:45:32.560
Absolutely brilliant it was a true pleasure to speak with him.

02:45:32.560 --> 02:45:38.560
For me it cleared up a lot of thoughts and issues I was having with this paper.

02:45:38.560 --> 02:45:51.560
So for example right up front what he says is my purpose behind this paper is to show that even though the intuition that people have of interpolation

02:45:51.560 --> 02:45:56.560
like the intuition that we have of interpolation is good.

02:45:56.560 --> 02:46:02.560
The mathematical definition that we have of interpolation is not useful in high dimension.

02:46:02.560 --> 02:46:08.560
What I thought was interesting too is when we asked about well what about the manifold concept.

02:46:08.560 --> 02:46:12.560
You know why isn't that sort of the definition of interpolation.

02:46:12.560 --> 02:46:20.560
And he brought up a really strong argument there as he said well let's just take the simple case of suppose the problem you're trying to solve

02:46:20.560 --> 02:46:28.560
just is linear you know like it just everything is a linear data set linear problem we're trying to solve like even in that case

02:46:28.560 --> 02:46:34.560
this linear case in high enough dimension interpolation doesn't work.

02:46:34.560 --> 02:46:42.560
And there your manifold is just literally a convex hull you know and so sure you can have kind of a nonlinear transformation

02:46:42.560 --> 02:46:47.560
and a nonlinear shape and whatever you're still hit by this curse of dimensionality.

02:46:47.560 --> 02:46:58.560
And he you know he brought up the point that like you know of course if your problem compresses down enough to where only a small number of

02:46:58.560 --> 02:47:06.560
of transformed dimensions right latent space dimensions matter and everything then you can be said that you're you're interpolating

02:47:06.560 --> 02:47:12.560
you know because we're not really hit by that curse of dimensionality because we stripped away all the dimensionality down to these dimensions

02:47:12.560 --> 02:47:19.560
we've gotten lucky our problem space or data samples etc allowed us to do that okay but that's not going to be all problems

02:47:19.560 --> 02:47:27.560
like some problems may just intrinsically have high dimensionality of interpolation that's not useful.

02:47:27.560 --> 02:47:37.560
So we need to do something better we need to we need to come up with a definition of interpolation that maintains the intuitive notion

02:47:37.560 --> 02:47:42.560
that we have of interpolation but that continues to work in high dimension.

02:47:42.560 --> 02:47:49.560
And you know and he made a very you know very interesting kind of end goal here which is like if we can get a definition of interpolation

02:47:49.560 --> 02:47:57.560
that ends the approaching this concept of generalization you know that's that's what we're trying to achieve really.

02:47:57.560 --> 02:48:01.560
And what do you think his take on generalization is then.

02:48:01.560 --> 02:48:07.560
Well as he made it you made a comment that you know on the one hand you could just ridiculously specify the space and make it

02:48:07.560 --> 02:48:10.560
trivially separable but then you lose generalization.

02:48:10.560 --> 02:48:18.560
Well yeah and I think I mean because because we did ask or you asked you know put the question what if what you're what if the problem

02:48:18.560 --> 02:48:25.560
you're trying to solve is the ability to solve novel problems like you know what happens in of course that was in the context of our

02:48:25.560 --> 02:48:34.560
discussion about hybrid systems where you're combining the continuous with with discreet and they're able to ping pong and kind of modify each other.

02:48:34.560 --> 02:48:42.560
And I think what we got from him there and consistently really throughout throughout our discussion.

02:48:42.560 --> 02:48:50.560
And what's interesting is this this totally aligns actually with Francois Chalet as well which is that this all these questions like a lot of these

02:48:50.560 --> 02:48:56.560
very difficult questions that we're asking are problem specific and there is no.

02:48:56.560 --> 02:49:06.560
We don't currently have some one size fits all set of concepts that fits well for it for every every problem space you know it's very task specific

02:49:06.560 --> 02:49:10.560
very problem specific you know data specific.

02:49:10.560 --> 02:49:18.560
So I think that's an area where we got dive deeper with him but I didn't hear anything definitive you know today.

02:49:18.560 --> 02:49:22.560
It's a wrap we just interviewed the Godfather of deep learning.

02:49:22.560 --> 02:49:23.560
How's that possible.

02:49:23.560 --> 02:49:27.560
I think we can just quit now we might as well just shut the channel down.

02:49:27.560 --> 02:49:29.560
Yeah.

02:49:29.560 --> 02:49:33.560
I mean obviously after we've published it.

02:49:33.560 --> 02:49:35.560
That's the singularity.

02:49:35.560 --> 02:49:37.560
So what's your take.

02:49:37.560 --> 02:49:43.560
I would have I would I think I think this but we also interrupted him a little bit.

02:49:43.560 --> 02:49:49.560
Maybe the question was I would have loved for him to be more a bit unlike you know where's the actual disagreement.

02:49:49.560 --> 02:49:50.560
Right.

02:49:50.560 --> 02:49:59.560
Because a lot of times you know when we when we put put you know questions to him like OK other people say this he was like oh yeah I agree right.

02:49:59.560 --> 02:50:08.560
And this seems it seems to be a general sentiment that's also when we talk to other people about you know perceived disagreements they're always very

02:50:08.560 --> 02:50:20.560
you know being being being good being academics being also friends probably with a lot of people you always like yeah you know they have they have good point you know we essentially agree on all the

02:50:20.560 --> 02:50:40.560
on all the things right but then I sort of want to know where actually do people disagree right if it's if it's you know in and that that that is a little bit I mean I obviously have a feeling but it's still a little bit elusive and

02:50:40.560 --> 02:50:50.560
and if people disagree on the actual technical nature or more on the philosophical end of what do what does something mean.

02:50:50.560 --> 02:50:58.560
I definitely think there is from hearing now and I think from his answer to that question.

02:50:59.560 --> 02:51:07.560
I could hear a little bit in that he seems to be more optimistic on what these learning systems can do.

02:51:07.560 --> 02:51:20.560
Then maybe other people are right because some people seem to have really kind of a hard stop on like this will never be possible with like a learning system.

02:51:20.560 --> 02:51:30.560
Whereas it seemed it seemed it seemed that he had sort of a more optimistic outlook and said of course they can't do it now right.

02:51:30.560 --> 02:51:43.560
However you know we work on it we modify them you know we're we're figuring out how do we need to build these systems such that you know a learning system can conceivably.

02:51:43.560 --> 02:51:56.560
You know do many of the things that people would call a kind of reasoning and I think that's why he went into you know let me give you an example of reasoning to sort of show look here you know.

02:51:56.560 --> 02:52:09.560
Here is here is an example of a reasoning that neural networks already do and that means that something like reasoning in general isn't too far away.

02:52:09.560 --> 02:52:14.560
And that's ultimately where the disagreement might be with other people.

02:52:14.560 --> 02:52:20.560
I mean I think that's that was my take on his answer to our first question like the you know why'd you write this paper.

02:52:20.560 --> 02:52:28.560
Is I think and I may go back and watch this but I think essentially what he was saying is you know sometimes people.

02:52:28.560 --> 02:52:34.560
They have some definition in their mind whatever it is of interpolation and extrapolation.

02:52:34.560 --> 02:52:46.560
And they come up with kind of an argument to say see the this this machine learning is doing interpolation and in a binary sense it can't do extrapolation.

02:52:46.560 --> 02:52:58.560
And so therefore there's this entire class of things you know that it's not capable of doing and I think that's what his objection was which is look like this is not a useful distinction to say between.

02:52:59.560 --> 02:53:14.560
You know it's not a useful distinction to draw between quote unquote interpolation and extrapolation like here's an example of a definition right that's a relatively standard definition of interpolation and if we apply this interpolation to.

02:53:14.560 --> 02:53:20.560
Theoretically what machine learning is doing and empirically to what machine learning is doing it's always extrapolating.

02:53:20.560 --> 02:53:27.560
So I think that's what he's objecting to is just like look don't come to some strong conclusion about what.

02:53:27.560 --> 02:53:38.560
Machine learning neural networks I'm not sure what the right way to phrase things is now what it's capable of doing like we just need to do more work to expand its capabilities.

02:53:38.560 --> 02:53:48.560
Folks like Gary Marcus making this case of we need discrete models because they can abstract broadly they give a couple of examples you know binary encoding for numbers.

02:53:48.560 --> 02:53:56.560
And there's one example with can you have a model that reverses the bits or I think there was another example in his algebraic mind which was about can you.

02:53:56.560 --> 02:54:06.560
Generalize from the even numbers to the odd numbers and you can't do that and the reason you can't do it is the new examples are completely outside of the training space of the input data.

02:54:06.560 --> 02:54:11.560
And probably that problem is not interpolative or it's not differentiable.

02:54:11.560 --> 02:54:21.560
I mean what lead sub is the same thing he says in language processing in particular can like language understanding in order to have broad generalization you need to have abstract rules.

02:54:21.560 --> 02:54:30.560
So for example being able to generalize from Mary loves John to Mary loves Jane they make the argument that with a statistical approach that just wouldn't be possible.

02:54:30.560 --> 02:54:49.560
Well I think so I think and this is just my opinion but I think his answer to my digits a pie question kind of shed some light on what his position is on this which is that it's too early to theoretically and scientifically for us to make that that determination

02:54:49.560 --> 02:55:00.560
because he said look okay people only been able to calculate digits of pie you know like within the last hundred years or whatever I'm not talking like we're not there yet I'm not talking about that class of problems I'm talking about these.

02:55:00.560 --> 02:55:01.560
Try numbers as well.

02:55:01.560 --> 02:55:11.560
Right so I'm talking about these things like a cat jumping up here and so some of these examples that Gary Marcus may bring up may fall into that even though they seem simple okay they seem deceptively simple.

02:55:11.560 --> 02:55:28.560
They may still fall in the bucket of these kind of much more towards the discrete spectrum of capabilities which we can't currently do with with our methods and machine learning but we're going to continue and continue to get closer to that that into the spectrum.

02:55:29.560 --> 02:55:39.560
And so it's and so it's premature to say that like machine learning and again whatever you can see that to be neural networks whatever will never be able to get there.

02:55:39.560 --> 02:56:03.560
Like he and he and Jan is optimistic that we'll be able to get there I'm not as optimistic I think there is a very there's just a qualitative difference between you know structural topological discrete reasoning and continuous differentiable reasoning and I don't know how we're ever going to get that gap bridge to brought up some some methods to think about but.

02:56:04.560 --> 02:56:08.560
Right but the key question is whether it's possible at all.

02:56:08.560 --> 02:56:18.560
And there was an article by a guy called Andrew Yee I think and it really annoyed you and the Coon and he said look people are still making the argument that neural networks interpolate and in that article.

02:56:18.560 --> 02:56:29.560
He was basically saying well neural networks are a little bit like a Taylor series approximation you take a function and you just kind of approximate it you know inside a certain range and then it goes haywire outside of the range.

02:56:29.560 --> 02:56:35.560
And Francoise Relay came on the podcast and he said look you know it's a little bit like in Fourier analysis when you try and you kind of like.

02:56:35.560 --> 02:56:44.560
You take these little signs and cosines and you fit it to a discrete function and it's glitchy right and actually if you look at harmonic analysis this is what.

02:56:45.560 --> 02:57:00.560
As you and Bruno said on the show on the on the geometric show he said that what a neural network even the universal function approximation theorem it's all about stacking these basis functions together right to kind of approximate some target function well if that's all neural

02:57:00.560 --> 02:57:02.560
networks do how could they possibly generalize.

02:57:03.560 --> 02:57:10.560
Look I don't want to use the word like generalized I don't know that what I would say is this which is that.

02:57:10.560 --> 02:57:22.560
No matter how discreet in appearance the human brain is you know all the signals that the neurons generate receive whatever you know at the end of the day.

02:57:22.560 --> 02:57:28.560
Are continuous functions I mean they're you know a charge that has a continuous function.

02:57:28.560 --> 02:57:36.560
Now somehow or another like the brain in its structure does take that continuous analog.

02:57:36.560 --> 02:57:52.560
I should have said analog you know probably there were but it takes that analog computational environment and produces digital reasoning so I don't think it's beyond that like I just don't see how anyone could reach the conclusion that it will be impossible like to do.

02:57:53.560 --> 02:58:07.560
Discreet reasoning with with neural networks I think it's just a question of like for me I think is more of a question is that the efficient way to do it like you know if you fast forward 2000 years from now when people have figured out all these problems and we have like you know

02:58:08.560 --> 02:58:27.560
just walking around killing us or working for us whatever the thing may be you know are they going to be using a neural network for everything or is it going to be a neural network with you know some classic digital compute components with some other stuff like a hybrid kind of structural system that does things I think is more what I'm asking.

02:58:28.560 --> 02:58:45.560
Like Lacan hint and they of course that that's what I was going to say they have like nature on their side in that in no matter how much we cut open the human brain we don't find like a discrete computer in there like of course the individual neural spikes are discrete like there's

02:58:45.560 --> 02:59:06.560
sort of charge or no charge but then there is like continuous release of neurotransmitter so I agree like the brain is like a very continuous distributed machine and there is no no there's no discrete thing in there there's no part of the brain where it's like look one one brain brain

02:59:07.560 --> 02:59:32.560
he said that and there is a level at which that's true which is there's a spike train and you know we can kind of recognize a spike because it's a maximum in this analog dimension but my point is that still an underlying analog dimension and so I don't see why in principle you couldn't build a neural network that you know has like these kind of continuous

02:59:32.560 --> 02:59:44.560
values but still ends up with something that that's that synthesizes a a discrete you know decision sure but wouldn't it be glitchy and it still wouldn't extrapolate.

02:59:45.560 --> 02:59:54.560
I have no idea like what you know I just don't know I think I think this kind of point is we're too early on to reach these conclusions that it cannot be done.

02:59:54.560 --> 03:00:09.560
I do sort of feel that it's just not the efficient way to do it that you know we're going to end up with a case where we're going to have systems that have analog continuous chunks that are neural networks or whatever and we're also going to overlay that with

03:00:09.560 --> 03:00:26.560
with digital computation that's implemented by the typical kind of digital computer that's going to be these hybrid systems working together just like you brought up with hey look you know alpha alpha zero alpha go all those things or it can be viewed in the same kind of hybrid way right.

03:00:26.560 --> 03:00:36.560
But that's exactly I mean you said this is what you said and also what when he when he said like so first when he said you know we're far away from that.

03:00:37.560 --> 03:00:45.560
With the digits of pie and so on but also when he said you know humans are actually pretty bad at chess or at discreet exploration in general and that is.

03:00:46.560 --> 03:00:51.560
That's how humans do it right humans build discreet reasoning.

03:00:52.560 --> 03:01:05.560
On top of this sort of neural continuous function and it's actually really hard like to do discreet reasoning in your head is you can do it slowly and you have to do it deliberately with attention.

03:01:05.560 --> 03:01:12.560
If you do like multiplication of five digit number in your head right this is not a this is not a oh my god feeling.

03:01:13.560 --> 03:01:16.560
This is like you like you sit there and you have to like keep all the stuff.

03:01:17.560 --> 03:01:31.560
It's not a hash table look up like the multiplying numbers of the thing and we can like we can build children's toys like that for like 20 cents that multiply 10 digit numbers easily right so I like yeah I think this is.

03:01:31.560 --> 03:01:41.560
Yeah I think this is we teach children point that way is we teach them hash table look up like memorize you know yeah all the numbers from one to a hundred multiply or something and then we teach them an algorithm.

03:01:42.560 --> 03:02:00.560
You know we teach them this exact thing and we teach them hash table look up and and this is built on so if you and I think that if you build if you build this discreet reasoning you can probably build it on top of these models right but yes is it the most efficient way to do it.

03:02:01.560 --> 03:02:05.560
Maybe maybe not right but then again if you do it.

03:02:06.560 --> 03:02:21.560
If you manage to do it train it you seem at least from at least from from the perspective of humans you seem to have something insanely powerful right because you know if you just have a discrete algorithm you have the algorithm but if you have a.

03:02:21.560 --> 03:02:39.560
Discrete if you've actually managed to train a discrete algorithm on top of this continuous function it will be able to sort of learn in the same continuous way that you know a neural network learns but you know it will be sort of be able to self modify its discreet algorithm and that.

03:02:40.560 --> 03:02:49.560
Yeah I think you know I agree I'd also be in the maybe longer term future positive that might be possible.

03:02:49.560 --> 03:02:57.560
I think that's because I said to him and do you think alpha goes a good thing do you think is a good thing to have a kind of.

03:02:58.560 --> 03:03:11.560
Discreet search which is guided by a neural network and he didn't like it and I assume he didn't like it for the reason you just said you and it which is that the discrete part of it is hard coded and it's been told to do a specific task it's it's not learning.

03:03:11.560 --> 03:03:17.560
I think look who would rather have something which is entirely differentiable right so because.

03:03:18.560 --> 03:03:26.560
Look who wants learning end to end in a in a in a neural network right yeah and and also it's almost like he was saying before well.

03:03:27.560 --> 03:03:41.560
Humans can't do discrete reasoning so it would actually be a more sophisticated form of intelligence if we had discrete reasoning but do we necessarily even want that in our models or is it just the fact that he wants it in the models but he wants it to be differentiable and learnable as part of the main model.

03:03:42.560 --> 03:03:50.560
I think so because that's where he got with the you know the critic actor critic type models I think if people nowadays talk about differentiable they always.

03:03:51.560 --> 03:04:04.560
Talk about sort of they always talk about like single shot feet forward like you know I input something into the machine and outcomes of prediction whereas he also makes a lot of arguments for you know this.

03:04:05.560 --> 03:04:17.560
Energy minimization which which means that essentially at test time at inference time I do a minimization algorithm and you know when he when he talked about reasoning he was mentioning that.

03:04:18.560 --> 03:04:31.560
As an example as essentially saying look I have my trained neural network and now at inference time I actually still perform an algorithm on top of that right to minimize some energy function given my trained model of the world let's say so.

03:04:31.560 --> 03:04:47.560
I think you know we might be able to make that more learned by also learning the algorithm that we do at inference time but I don't necessarily I don't necessarily think that he's talking about you know we need.

03:04:48.560 --> 03:04:58.560
Like we need to end to end learn like a machine where you simply input something and then out pops through forward prop out pops and all of these things are just reasoning by the back door.

03:04:59.560 --> 03:05:05.560
It's even with a critic as I understand that that's just the way of hacking that the value the advantage function right you know with some.

03:05:06.560 --> 03:05:10.560
I'm not an expert in reinforcement learning but I think that's what it is the same thing with the energy minimization.

03:05:11.560 --> 03:05:12.560
Yeah I kind of agree with you there.

03:05:13.560 --> 03:05:21.560
Yeah but the energy minimization has this particular thing of this idea which I think today is underexplored of to actually do something at inference time.

03:05:22.560 --> 03:05:29.560
Like to not just forward prop and that I feel it's I feel it's under underexplored nowadays.

03:05:30.560 --> 03:05:48.560
I agree with you there too but I think what kind of sometimes I feel like this is feature creep in a sense it's like you know we have neural networks and we know what deep learning is and some people now want that to be redefined it's just a general purpose research paradigm that includes all possible things that we can do with.

03:05:48.560 --> 03:06:06.560
You know machines or chemicals it's like what use is that like what use is it defining you know all types of computation is as differentiable computation like we lose some ability to talk about these and sort of the same way that was kind of like what I was saying you know nobody I mean look.

03:06:07.560 --> 03:06:23.560
Allowing for a variable number of layers in a neural network that's discrete computation right that's not differentiable training nobody knows how to train in like an arbitrarily unbounded number of layers in some differentiable way right.

03:06:24.560 --> 03:06:26.560
But there's the neural ODE stuff.

03:06:27.560 --> 03:06:43.560
Well all those types of things run into big training problems like things become really hard to train when you have these types of yeah of essentially discrete you know transitions right or combinatorial kinds of kinds of transitions.

03:06:44.560 --> 03:06:59.560
And this is classic stuff I mean this is really classic stuff it's like even if you try to do mixed integer optimization right so you have a problem that has some combination of integer variables discrete variables and some combination of continuous variables.

03:06:59.560 --> 03:07:03.560
There's all kinds of hacks to try to do that in differentiable ways.

03:07:04.560 --> 03:07:18.560
They don't in general arrive at the optimal discrete solution like you do some continuous stuff and then at the end you kind of discretize it in one way or another and you wind up with a solution that's kind of approximately correct.

03:07:18.560 --> 03:07:27.560
But you're not guaranteed that you're going to find the one that's actually correct if you discreetly combinatorially examine that that space right.

03:07:28.560 --> 03:07:36.560
Can we finish by talking about a couple of things so first of all there's table one and the structure of the latent table one is the biggest.

03:07:37.560 --> 03:07:51.560
The biggest argument against the most prevalent argument against this paper sorry for the triple negative but you know when when you know when the paper starts out by saying you know we build a convex hull of the training data.

03:07:51.560 --> 03:08:04.560
If your point is not in the convex hull you know not interpolating and then people go wait wait wait wait wait but you're talking about the input space of data you're talking about the pixel space and you know for sure we're not talking about the pixel space.

03:08:04.560 --> 03:08:17.560
Because you know the neural network is you know the data manifold and who but we're talking about you know if you go to the last layer to the latent space before the classification it's just a linear class.

03:08:17.560 --> 03:08:26.560
You know there in that space we're talking about like interpolation right there is where the neural networks interpolate in this experiment clearly shows like.

03:08:26.560 --> 03:08:41.560
Okay might be a rigid definition of interpolation but it clearly shows like no even in that space on a trained Resnet there is no like no interpolation going on as soon as you pick like 30 dimensions or more.

03:08:41.560 --> 03:08:54.560
It's all outside of the convex hull of training right but but this this this gets to the punchline right because Keith was going to ask a really cool question which is like well imagine it was a very small latent and it was interpolating.

03:08:54.560 --> 03:09:03.560
And then what if you up projected it like to one or two four dimensions and that you know with with that suddenly now be extrapolating right so that we almost need to have an information theoretical.

03:09:03.560 --> 03:09:16.560
Theoretic way of looking at this but anyway but my intuition is that deep learning models encode the most high frequency information into the latent space right and you know this information would be encoded in a minimally distributed way.

03:09:16.560 --> 03:09:21.560
To denoise the predictive task which is to say right you know there's a few dimensions of the latent.

03:09:22.560 --> 03:09:37.560
You know which should be encoding the actual things that that you trained it on so my intuition is actually most of the dimensions of the latent are kind of just encoding low frequency information so you can discard them right and I know you said the other day well

03:09:37.560 --> 03:09:45.560
Well maybe the whole point of neural networks is there a distributed representation so maybe they are distributed overall of the layers overall of the dimensions in the latent.

03:09:45.560 --> 03:10:03.560
I mean I would I would actually argue not that neural networks are putting the data in a minimally distributed way but in like a maximally like why I think just just my intuition is that a neural if if I were a neural network and I had all of these dimensions at my disposal.

03:10:03.560 --> 03:10:09.560
I would encode lots of information redundantly if it like if it were too much space.

03:10:09.560 --> 03:10:24.560
I would like encode the same information pretty much redundantly in many of the dimensions that I could noisy right so you're taking a soft max and if you're noisily aggregating overall of those features that you don't want to do that.

03:10:25.560 --> 03:10:36.560
Yeah but still if I have backprop right and the backprop path goes through each of the dimensions so for each of the dimensions I'm asking myself how can I change this one to make my prediction even better right.

03:10:36.560 --> 03:10:51.560
It doesn't matter if over there one of the dimensions is already doing the classification for me right the backprop the nature of backprop means that you know I'm going to every single dimension independently and deciding how can I change this one.

03:10:52.560 --> 03:11:07.560
To make it even better so that's why I think lots of information if the latent is too big lots of information will be encoded right there is if the latent is too big and so this is where all the pruning literature comes into which is that.

03:11:07.560 --> 03:11:13.560
The majority of the time people are running neural networks and situations where the latent space is too big.

03:11:13.560 --> 03:11:33.560
Like they're just you know we just flat out have far too many you know far too large of a latent space and so even though it may be encoded you know densely there like it may be putting all kind of little bits of extra information it's probably only adding little plus or minus you know 0.1% type things to the accuracy.

03:11:33.560 --> 03:11:39.560
Whereas and so since it is only adding these little kind of very tiny values there right.

03:11:39.560 --> 03:11:53.560
The only meaningful way to talk about interpolation or extrapolation because you've only got little bits of stuff you're using on that entire dimension that you've added there is really more just are you interpolating on the most salient dimensions.

03:11:53.560 --> 03:12:05.560
You know which is again back to my question about why are we you know why are we concerned with whether or not every single dimension falls within within the sample range.

03:12:05.560 --> 03:12:15.560
Yeah exactly but there's a few things that I mean it's really good that you bring up sparsity I think that's fantastic that you brought that in because as you say most of that information is redundant.

03:12:15.560 --> 03:12:26.560
But then what Yannick was saying was interesting does distributing all that information rather than my intuition is it increases noise but actually I think Yannick saying it increases precision.

03:12:26.560 --> 03:12:36.560
But in a very tiny tiny like it's a tiny tiny contribution so let's suppose the latent space is not too large right like if the latent space is not too large.

03:12:36.560 --> 03:12:51.560
In other words it's let's say it's just barely big enough to classify your images and let's suppose you're doing multi class so we've got 10 10 10 classes you know and we just barely got enough latent space for it.

03:12:51.560 --> 03:13:06.560
My intuition would tell me that if those classes are somewhat different from one another like it's not we're classifying brown dogs from white dogs from like every other you know simple kind of dog but they're different from each other and it's not easy to determine.

03:13:06.560 --> 03:13:24.560
You know which is which that what would and just just to guess is it probably what would happen is you would find that you know these five bits are kind of the ones that tell you whether something is a dog and these seven bits tell you if it's a duck and these four bits tell you if it's a gun and these five

03:13:24.560 --> 03:13:39.560
bits tell you if it's an image of the sky like I would guess that it would end up encoding it that way since they don't have a lot in common you can't really you can't really use entanglement like too much except for a few dimensions there might be some overlaps but.

03:13:39.560 --> 03:13:57.560
Yeah I think I think what what people what people mean a little bit is when they when they start going down the interpolation road is that you know we've played with GPT three as well right and then you you do something with it you enter something and in some places you're like.

03:13:58.560 --> 03:14:13.560
I see how you did that right you like you just you just took that that newspaper article and you just kind of replace some stuff in it right like you you sort of where whereas if you were to to talk to like a human you you sort of.

03:14:14.560 --> 03:14:27.560
Like that stuff wouldn't happen as much even and then of course there's there's the argument I think people that say well it's just interpolating or it might also be you know it's just sort of repeating the stuff in the training data.

03:14:28.560 --> 03:14:44.560
I think what they what I'd like to see is more like the pattern that these models extract aren't sufficiently high level for for them right and then I think the entire discussion is can we get to.

03:14:45.560 --> 03:15:09.560
Arbitrary high abstract levels of pattern recognition with such models if we engineer then train them in the correct way or is there is there I guess some fundamental limitation to that and yes as we said that the answer the answer might be might be quite far off as as for the.

03:15:10.560 --> 03:15:16.560
Number of you know latent dimensions and so on I I mean I agree with with Keith I think.

03:15:17.560 --> 03:15:27.560
Having a big latent space and also having big weights and so on is might be more of a of a necessity for training.

03:15:28.560 --> 03:15:45.560
Then it really is for for encoding like it appears to help if we have lots of weights to train such that sort of we get to combinatorically try out combinations of weights only few of which might ultimately end up being important right.

03:15:45.560 --> 03:15:54.560
That's kind of a lottery hypothesis sort of way of going down so yeah I agree you know most of most of the information ultimately might be.

03:15:55.560 --> 03:16:13.560
Only contributing a little bit right but my my my intuition would meet be that you know this is kind of because it's kind of redundant information right because it's like you know I'm encoding this over here I'm also encoding it in like a tiny little different way over here.

03:16:14.560 --> 03:16:21.560
Or or some add on or some uncertainty or some one training samples a bit different so I'm going to put that right here.

03:16:22.560 --> 03:16:41.560
Yeah that's fascinating what you said about Frank whose work that I never really heard you articulate it like that but it is actually kind of like a search problem rather than a learning problem what you're doing is you're giving it all of the possible you know you give a random initialization on a densely connected network and you're saying you know just go and find the ones that work rather than create it from first principle.

03:16:42.560 --> 03:16:59.560
Yeah that's why initializations are so important right is is because you sort of you sort of try like your initialization essentially is is your your your buffet for SGD to like choose a good one from to then go ahead and refine.

03:17:00.560 --> 03:17:04.560
But how much refining is it is it more is it more finding things that already work versus refining.

03:17:05.560 --> 03:17:26.560
Yeah it's it's it's well the same but what it is not is sort of learning from scratch that that's what like people like we cannot you cannot initialize neural network from zeros and then have it have it learn well at least not today maybe that's going to come in in some form but initialization is actually pretty

03:17:26.560 --> 03:17:48.560
important pretty crazy and that's crazy and that's yeah that's like one hint that you know we're we're not essentially we're never essentially training from scratch where we're we're sort of training we're sort of giving the choice of many combinations of good weights or of semi good weights and it has to pick sort of the good ones to continue exactly it's already pre trained you just don't know which.

03:17:49.560 --> 03:18:09.560
Yeah I mean that the argument that a little bit of the argument against that is in in sort of the evolutionary approach where you say you know you can make the argument you know humans have sort of develop these abilities to reason to recognize super high level patterns while only having a continuous brain.

03:18:10.560 --> 03:18:28.560
But then the other side of this is yeah but it's not like a single human that has achieved that right it's not like one single learning system that has achieved that but it's actually like this evolutionary system which is in essence a massively distributed combinatorical.

03:18:29.560 --> 03:18:40.560
Trial and error search right and that is that is not a learning system so to say as we imagine it today it at the end you end up with a learning result but.

03:18:41.560 --> 03:18:57.560
The evolutionary algorithm is way different than we imagine learning it's it's not even all it's not even all humans that have learned it it's it's all you know individuals of all tens of millions of species that have ever lived on earth that have that have learned.

03:18:58.560 --> 03:19:11.560
Which is pretty much like like the lottery ticket hypothesis let's just let's just randomly train crap tons of you know weights and then slowly prune them and see what happens.

03:19:12.560 --> 03:19:20.560
Right brilliant well gentlemen it's been an absolute pleasure I guess I guess we'll make this a Christmas special I mean it is pretty special.

03:19:21.560 --> 03:19:26.560
Oh yeah let's be honest so yeah this is gonna warrant a hat we're gonna sign off.

03:19:28.560 --> 03:19:29.560
Nice meeting you.

03:19:30.560 --> 03:19:38.560
Indeed but thanks for bearing with us folks we have had a couple of months off I've had a bit of a break and you know the guys have had a bit of a break so yeah we're back.

03:19:39.560 --> 03:19:40.560
Good to see you all again.

03:19:41.560 --> 03:19:42.560
Peace out.

