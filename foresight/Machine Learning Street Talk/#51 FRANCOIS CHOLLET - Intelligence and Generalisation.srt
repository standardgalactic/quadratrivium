1
00:00:00,000 --> 00:00:20,640
Past a certain level of complexity, every system starts looking like a living organism.

2
00:00:20,640 --> 00:00:27,280
In order to build a general intelligence, you need to be optimising for generality itself.

3
00:00:30,240 --> 00:00:37,920
We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness

4
00:00:37,920 --> 00:00:45,280
of patterns from a tiny little bit of information. Generalisation is the ability to mine previous

5
00:00:45,280 --> 00:00:52,640
experience to make sense of future novel situations. Generalisation describes a knowledge

6
00:00:52,640 --> 00:01:00,160
differential. It characterises the ratio between known information and the space of possible future

7
00:01:00,160 --> 00:01:07,440
situations. To what extent can we analyse the knowledge that we already have into simulacrums

8
00:01:07,440 --> 00:01:14,880
that apply widely across experienced space? So intelligence, which is to say generalisation

9
00:01:14,880 --> 00:01:22,880
power, is literally sensitivity to abstract analysis, and that's in fact all there is to it.

10
00:01:22,880 --> 00:01:30,080
In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years.

11
00:01:30,080 --> 00:01:36,320
I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy

12
00:01:36,320 --> 00:01:41,520
Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops

13
00:01:41,520 --> 00:01:48,080
and hackathons. I used to travel around the world and I always had a copy of Francois's book

14
00:01:48,080 --> 00:01:54,720
Under My Arm. It never left my side. I used to force everyone to read the first four chapters

15
00:01:54,720 --> 00:01:59,680
of that book and of course the chapter on the limitations of deep learning before we did anything.

16
00:02:00,320 --> 00:02:07,760
Francois has a clarity of thought, which is unparalleled I think in any other human being

17
00:02:07,760 --> 00:02:14,320
on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has

18
00:02:15,120 --> 00:02:20,720
no trouble at all finding holes in some of our guests' work, had this to say while prepping

19
00:02:20,720 --> 00:02:26,320
for the show. I'm working on it. It turned out to be a little bit more difficult than I thought.

20
00:02:26,960 --> 00:02:32,400
Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me

21
00:02:32,400 --> 00:02:37,120
doing that. But anyway Chalet is extremely controversial to some people actually, but

22
00:02:37,120 --> 00:02:43,360
he's not controversial to us. Our discussion today lies at the intersection of machine learning

23
00:02:43,360 --> 00:02:49,040
and reasoning. Now Chalet has made his vision completely clear about what he thinks the future

24
00:02:49,040 --> 00:02:54,320
of machine learning is. Make no mistake, what you should take from today's episode is that the

25
00:02:54,320 --> 00:03:00,640
future of artificial intelligence is going to be discrete as well as continuous. Actually the two

26
00:03:00,640 --> 00:03:08,640
are going to be enmeshed. The future of AI will almost certainly involve a large degree of program

27
00:03:08,640 --> 00:03:14,880
synthesis. Deep learning has its limits. You can use deep learning for continuous problems

28
00:03:14,880 --> 00:03:20,800
where the data is interpolative and has a learnable manifold and where you have a dense

29
00:03:20,800 --> 00:03:27,200
sampling across the entire surface of the manifold between which you need to make predictions.

30
00:03:27,760 --> 00:03:35,200
For Chalet, generalization itself is by far the most important feature of intelligence

31
00:03:35,200 --> 00:03:42,240
and of developing strong AI. He describes a spectrum of generalization starting with,

32
00:03:42,240 --> 00:03:49,360
for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed.

33
00:03:50,080 --> 00:03:57,200
The machine learning we have today confers some adaptation within a known domain of tasks. For

34
00:03:57,200 --> 00:04:04,400
example, being able to recognize dogs or cats within a variety of different poses and lighting

35
00:04:04,400 --> 00:04:12,400
conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to

36
00:04:12,400 --> 00:04:18,960
unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the

37
00:04:18,960 --> 00:04:25,280
door of this now with GPT-3, where the subtask, if you like, is given at test time. Although

38
00:04:25,280 --> 00:04:30,960
Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3

39
00:04:30,960 --> 00:04:36,320
knows was learned on the vast amounts of training data that we trained it on, the poet algorithm

40
00:04:36,320 --> 00:04:41,840
from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training

41
00:04:41,840 --> 00:04:46,400
process, which is very, very interesting. It's creating new problems and new solutions as part

42
00:04:46,400 --> 00:04:51,200
of the training process. But broadly speaking in the machine learning space at the moment,

43
00:04:51,200 --> 00:04:57,120
the task that we are doing is fixed and not generalizable. The other thing is that the real

44
00:04:57,120 --> 00:05:03,680
world does not have a static distribution. We need systems that can adapt dynamically.

45
00:05:03,680 --> 00:05:09,360
Intelligence requires that you adapt to novelty without the help of the engineer who helped you

46
00:05:09,360 --> 00:05:14,880
write the system. Chalet has come up with a formalism of intelligence that balances the task

47
00:05:14,880 --> 00:05:21,840
skill, the difficulty, the knowledge, and experience to effectively quantify and normalise

48
00:05:21,840 --> 00:05:28,560
an algorithmic information conversion ratio. It's the ability to convert experience into future skill

49
00:05:28,560 --> 00:05:33,040
that is Chalet's measure of intelligence. At the end of his measure of intelligence paper,

50
00:05:33,040 --> 00:05:38,560
Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a

51
00:05:38,640 --> 00:05:44,640
massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware

52
00:05:44,640 --> 00:05:50,240
generalisation. Any model that we have needs to generalise to tasks that the developer was

53
00:05:50,240 --> 00:05:56,240
unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or

54
00:05:56,240 --> 00:06:02,480
anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to

55
00:06:02,480 --> 00:06:07,360
represent the kind of priors that us humans have. Now machine learning algorithms are completely

56
00:06:07,360 --> 00:06:12,480
ineffective against the ARC challenge because it's so challenging to generalise from a few

57
00:06:12,480 --> 00:06:18,640
examples. The only solutions that were effective in the ARC challenge were programme synthesis.

58
00:06:18,640 --> 00:06:23,280
The manifold hypothesis is that natural data forms lower-dimensional manifolds

59
00:06:23,280 --> 00:06:29,920
in its embedding space. There are both theoretical and experimental reasons to believe this is true.

60
00:06:29,920 --> 00:06:34,640
If you believe this, then the task of a classification algorithm is fundamentally

61
00:06:34,640 --> 00:06:40,480
to separate a bunch of tangled manifolds. The only way deep learning models can generalise

62
00:06:40,480 --> 00:06:46,960
is via interpolation. Most perception problems in particular, according to Francois, are

63
00:06:46,960 --> 00:06:53,120
interpolative. Neural networks not only have to represent the manifold of the data that they're

64
00:06:53,120 --> 00:06:58,640
learning, the manifold also needs to be learnable. And that's an even tougher constraint.

65
00:06:58,640 --> 00:07:03,760
Gradient descent will not learn data that has challenging discontinuities in its manifold.

66
00:07:03,760 --> 00:07:10,080
It'll just resort to memorising the data. Deep learning allows you to represent complex programmes

67
00:07:10,080 --> 00:07:15,920
that you couldn't write by hand, but on the other side of the coin it also fails to represent

68
00:07:15,920 --> 00:07:22,000
very simple programmes that you could write by hand. Discrete programmes. So there are some

69
00:07:22,000 --> 00:07:26,400
problems where deep learning is a great fit and there are other problems where deep learning

70
00:07:26,400 --> 00:07:31,760
is a disaster. And the reason for that is that they are not interpolative in nature. These tend

71
00:07:31,760 --> 00:07:37,680
to be algorithmic reasoning problems. Francois thinks that 99% of software written today,

72
00:07:37,680 --> 00:07:43,120
with code, is not interpolative in nature and therefore it's a bad fit for deep learning.

73
00:07:43,120 --> 00:07:48,160
The only answer to these problems is discrete programme search. To use deep learning for these

74
00:07:48,160 --> 00:07:53,840
problems requires a lot of data. It's hard to train and the representation will be glitchy.

75
00:07:53,840 --> 00:07:59,840
It'll be brittle. Neural networks cannot even extrapolate the scalar identity function,

76
00:07:59,920 --> 00:08:07,200
f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent

77
00:08:07,200 --> 00:08:14,000
space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.

78
00:08:14,560 --> 00:08:19,440
So is this similar to interpolation? Well, I mean, all of machine learning is similar to

79
00:08:19,440 --> 00:08:24,880
interpolation if you want, right? When you train a linear regression on scalar values,

80
00:08:24,880 --> 00:08:30,000
you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the

81
00:08:30,000 --> 00:08:36,960
best values of A and B for y equals A x plus B that minimizes the square error of the prediction

82
00:08:36,960 --> 00:08:41,040
of a line to all of the points, right? That's linear regression. That's interpolation.

83
00:08:41,600 --> 00:08:46,320
All of machine learning is interpolation. In a high dimensional space, there is essentially no such

84
00:08:46,320 --> 00:08:52,240
thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right?

85
00:08:52,240 --> 00:08:58,240
So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have

86
00:08:58,240 --> 00:09:04,560
a million samples, you're only covering a tiny portion of the dimensions of that space, right?

87
00:09:04,560 --> 00:09:12,480
Those images are in a tiny sliver of surface among the space of all possible combinations

88
00:09:12,480 --> 00:09:18,400
of values of pixels. So when you show the system a new image, it's very unlikely that this image

89
00:09:18,400 --> 00:09:23,360
is a linear combination of previous images. What you're doing is extrapolation, not interpolation,

90
00:09:23,360 --> 00:09:27,840
okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard.

91
00:09:27,840 --> 00:09:33,760
I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out.

92
00:09:33,760 --> 00:09:40,480
It's certainly true that interpolation on the native data domain is useless, right? We need to

93
00:09:40,480 --> 00:09:45,680
pull some useful information out of the data and the model architecture and training method matter

94
00:09:45,680 --> 00:09:52,320
a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation

95
00:09:52,320 --> 00:09:58,000
in the original space of the data, right? Chalet is quite clear that neural networks only

96
00:09:58,880 --> 00:10:05,040
generalize through interpolation. You might argue that you can go a tiny step outside of the convex

97
00:10:05,040 --> 00:10:11,280
hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue

98
00:10:11,280 --> 00:10:16,240
that if the manifold doesn't give you any useful information outside of the training range, then

99
00:10:16,240 --> 00:10:20,880
it wouldn't be any better than finding your nearest training example and just adding a bit of random

100
00:10:20,880 --> 00:10:28,320
noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly,

101
00:10:28,320 --> 00:10:34,880
you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on

102
00:10:34,880 --> 00:10:40,800
might be shaped like a football or a sphere, which means if you go outside of the training range,

103
00:10:40,800 --> 00:10:46,720
you actually have some information about those data points. The scalar identity function might seem

104
00:10:46,720 --> 00:10:51,280
like a contrived example, but it's a really interesting one. When you go outside of the

105
00:10:51,280 --> 00:10:57,760
training range, nothing about the manifold is known, right? Think about the manifold. It's just a

106
00:10:57,760 --> 00:11:03,920
string that goes on forever. We don't know anything about that manifold outside of the training range.

107
00:11:03,920 --> 00:11:08,800
This is not true for most perceptual problems in deep learning. And this is why image models,

108
00:11:08,880 --> 00:11:14,000
for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you

109
00:11:14,000 --> 00:11:19,120
let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous

110
00:11:19,120 --> 00:11:24,080
problems versus discrete problems that we're going to be exploring in the show today. It's very

111
00:11:24,080 --> 00:11:29,200
interesting that brittleness works both ways, depending on the discreteness of the problem.

112
00:11:29,200 --> 00:11:36,320
Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,

113
00:11:36,320 --> 00:11:42,560
and deep learning would be extremely brittle predicting the digits of pi or prime numbers

114
00:11:42,560 --> 00:11:48,480
or sorting a list. So brittleness here means the overall fit of your model or your program,

115
00:11:48,480 --> 00:11:55,200
so accuracy and robustness. Imagine if every single bug you experienced with computer software

116
00:11:55,200 --> 00:12:00,720
was entirely unique to you and the development team wouldn't even be able to reproduce it.

117
00:12:00,720 --> 00:12:04,880
This is what would happen if software was written entirely with neural networks.

118
00:12:04,880 --> 00:12:10,560
It would be more, not less brittle. Sholey thinks that motivated thinking is the primary

119
00:12:10,560 --> 00:12:15,840
obstacle to getting people to wake up to the fact that neural networks are poorly suited

120
00:12:15,840 --> 00:12:21,600
to discrete problems. The people who are good enough at deep learning to realize its limitations

121
00:12:21,600 --> 00:12:27,600
are too invested in its success to say so. Sholey fundamentally thinks that there are two types of

122
00:12:27,600 --> 00:12:34,880
thinking, type one and type two. He thinks that every single thought in our minds is not simply

123
00:12:34,880 --> 00:12:42,240
one or the other, rather it's a combination of both types. Type one and type two, they are

124
00:12:42,240 --> 00:12:50,560
enmeshed together in everything you think and in everything you do. Even our reasoning is guided

125
00:12:50,560 --> 00:12:58,560
by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization

126
00:12:58,560 --> 00:13:05,920
and the way we perform abstraction is different in continuous versus discrete space. We need to

127
00:13:05,920 --> 00:13:11,680
find analogies and those analogies will be found differently in both of those different spaces.

128
00:13:11,680 --> 00:13:17,840
Program search allows us to generalize broadly from just a few examples. It marks a significant

129
00:13:17,920 --> 00:13:22,640
deviation from traditional machine learning. Rather than trying to interpolate between the

130
00:13:22,640 --> 00:13:28,720
examples you have, you're constructing an entire search space from scratch and testing

131
00:13:28,720 --> 00:13:34,640
if it fits our training data. It all started with the flash fill feature in Microsoft Excel.

132
00:13:34,640 --> 00:13:39,600
Do you remember that? You give a few examples of some transformation that you want to perform

133
00:13:39,600 --> 00:13:44,400
and it will generate a piece of programming code for you, which means it can generalize

134
00:13:44,400 --> 00:13:50,000
that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around

135
00:13:50,000 --> 00:13:55,680
for about 20 years actually, but what's really making it work now is the idea of using neural

136
00:13:55,680 --> 00:14:03,040
networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks

137
00:14:03,040 --> 00:14:09,360
that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new

138
00:14:09,360 --> 00:14:15,520
algorithms on the fly. It's already learned continuous and often glitchy representations

139
00:14:15,520 --> 00:14:20,800
of existing tasks during its training. It's completely ineffective against his arc challenge

140
00:14:20,800 --> 00:14:29,360
tasks. People often claim that neural networks are turing complete. No, they're not. A model has

141
00:14:29,360 --> 00:14:35,520
a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded

142
00:14:35,520 --> 00:14:42,400
space or unbounded time. For example, could you train a neural network to predict the nth digit of

143
00:14:42,400 --> 00:14:48,000
pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural

144
00:14:48,000 --> 00:14:53,200
network to do it. A simple turing machine program can do just that and that is because a turing machine

145
00:14:53,200 --> 00:14:58,960
can access unbounded memory and time. The best thing that neural networks can do is approximate

146
00:14:58,960 --> 00:15:05,200
unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural

147
00:15:05,200 --> 00:15:10,640
network to approximately multiply integers together. Yet, even when learning to multiply

148
00:15:10,640 --> 00:15:15,920
fixed-width integers, practically-sized neural networks introduce errors occasionally,

149
00:15:15,920 --> 00:15:21,760
and for a fixed-sized neural network, these errors grow more common as the size of the input grows.

150
00:15:21,760 --> 00:15:27,280
That said, neural networks are finite state machines, and just as finite state machines

151
00:15:27,280 --> 00:15:32,640
can be augmented with unbounded memory and iteration to yield a turing machine, neural

152
00:15:32,640 --> 00:15:38,400
networks can also be automated in the same way to produce a turing-complete computational model.

153
00:15:38,400 --> 00:15:43,280
If you want to see a concrete example of the kind of discrete program search that

154
00:15:43,280 --> 00:15:49,360
Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video

155
00:15:49,360 --> 00:15:54,480
about it. So yeah, it feels like today is the culmination of a year of really hard work and

156
00:15:54,480 --> 00:15:59,840
passion from the MLST team. We've worked with so many fascinating people. We've had so many

157
00:15:59,840 --> 00:16:05,600
amazing guests on. It really means a lot to us. Today is a very, very special episode. It was

158
00:16:05,600 --> 00:16:10,560
my dream from the beginning to get Chalet on the show. I know that Chalet is going to say

159
00:16:10,560 --> 00:16:15,840
lots of interesting things that will trigger some people and inspire others, and please take to the

160
00:16:15,840 --> 00:16:22,480
comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out.

161
00:16:22,560 --> 00:16:26,320
Welcome back to the Machine Learning Street Talk YouTube channel and podcast

162
00:16:26,320 --> 00:16:32,960
with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.

163
00:16:33,600 --> 00:16:39,920
Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in

164
00:16:39,920 --> 00:16:44,800
the machine learning space who's caused a massive stir in my thinking, the only other notable one

165
00:16:44,800 --> 00:16:48,880
actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was

166
00:16:48,880 --> 00:16:53,680
always to get Francois on the show, and I can't believe that it's actually happened. We actually

167
00:16:53,680 --> 00:16:57,920
have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show,

168
00:16:57,920 --> 00:17:04,160
but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually

169
00:17:04,160 --> 00:17:08,160
than anyone else by a wide margin, and it's because his work is very thought-provoking

170
00:17:08,160 --> 00:17:13,360
and disruptive. I spent many weeks actually studying his measure of intelligence paper last year,

171
00:17:13,360 --> 00:17:18,000
and of course his recent New York's workshop was fascinating as well. Almost every single word in

172
00:17:18,000 --> 00:17:22,400
my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that.

173
00:17:23,440 --> 00:17:28,960
Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's

174
00:17:28,960 --> 00:17:33,680
skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general

175
00:17:33,680 --> 00:17:40,080
intelligence. All intelligence is specialized. Critically, he thinks that generalization,

176
00:17:40,080 --> 00:17:46,960
the ability to deal with novelty and uncertainty is the most important concept in intelligence.

177
00:17:46,960 --> 00:17:52,800
He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep

178
00:17:52,800 --> 00:17:57,920
learning only works for problems where the manifold hypothesis applies. For example,

179
00:17:57,920 --> 00:18:03,520
problems which are interpolative in nature and when a sufficiently dense sampling of your

180
00:18:03,520 --> 00:18:11,040
distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize,

181
00:18:11,040 --> 00:18:16,400
but it cannot always generalize. And in his recent New York's presentation, he introduced the concept

182
00:18:16,400 --> 00:18:21,120
of program-centric and value-centric generalization, which we'll get into in the show today.

183
00:18:21,120 --> 00:18:26,800
But I wanted to move straight on to this concept of deep learning being a hash table,

184
00:18:26,800 --> 00:18:33,200
because this is what Francois thinks. He says that a deep learning model is a high-dimensional

185
00:18:33,200 --> 00:18:39,280
curve with some constraints on its structure given by inductive priors, and that curve has

186
00:18:39,280 --> 00:18:44,000
enough parameters that it could fit almost anything. If you train your model for long enough,

187
00:18:44,000 --> 00:18:50,080
it'll simply memorize your data. And because of SGD, your manifold fit is found progressively,

188
00:18:50,080 --> 00:18:54,560
and at some point, the manifold will approximate the natural manifold between underfitting and

189
00:18:54,560 --> 00:19:00,640
overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating

190
00:19:00,640 --> 00:19:05,680
on that manifold. So the power of the model to generalize is actually a consequence of the

191
00:19:05,680 --> 00:19:11,680
structure of the data and the gradual process of SGD, according to Francois, rather than any property

192
00:19:11,680 --> 00:19:17,600
of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a

193
00:19:17,600 --> 00:19:21,680
rather different view, because one school of thought is that deep learning models are kind of like

194
00:19:22,400 --> 00:19:28,000
searching for a space of possible programs, and advocates of GPT-3 make this argument quite

195
00:19:28,000 --> 00:19:32,560
strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is

196
00:19:33,680 --> 00:19:38,960
interpolating between mathematical conjectures, assuming that interpolation space would actually

197
00:19:38,960 --> 00:19:44,480
give us new information about mathematics, if he thought that that space wasn't interpolatable.

198
00:19:44,480 --> 00:19:51,440
What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation

199
00:19:51,440 --> 00:19:57,600
is the origin of generalization in deep learning models, and that's very much by construction,

200
00:19:57,600 --> 00:20:03,360
by nature, right? Like a deep learning model is a very large, differentiable,

201
00:20:03,360 --> 00:20:09,120
parametric model, trained with gradient descent. And so the only way it's ever going to be

202
00:20:09,120 --> 00:20:15,040
generalizing is your interpolation. This is literally, this is what it is, this is what it does.

203
00:20:15,040 --> 00:20:20,240
So I think the question, you know, are all deep learning models, interpolators or not,

204
00:20:20,240 --> 00:20:24,160
is not a super interesting question, because it's not an open question. We know they are.

205
00:20:24,160 --> 00:20:28,880
But the more interesting question, I think, is what can you actually achieve with the

206
00:20:28,880 --> 00:20:33,680
stored of interpolation on this very complex, very high-dimensional manifold,

207
00:20:33,680 --> 00:20:38,000
that they're deep learning models and implementing. We're telling you the properties of this generalization,

208
00:20:39,120 --> 00:20:42,720
the tasks for which it will perform well, the tasks for which it will not perform well.

209
00:20:42,720 --> 00:20:48,720
I guess one example I could give you is encoding data with the Fourier transform,

210
00:20:48,720 --> 00:20:53,120
like you know about the Fourier transform. And maybe, you know, some people will play around

211
00:20:53,120 --> 00:20:57,840
with it and they will be like, hey, you know, actually the Fourier transform can draw much

212
00:20:57,840 --> 00:21:02,160
more than curves. Look, I made a square with it, right? And then you would have to point out that,

213
00:21:02,160 --> 00:21:06,640
no, actually the square, you've made it by supposing lots of tiny curves. And it's not,

214
00:21:06,640 --> 00:21:10,880
in fact, a perfect square, right? Because it is made of this, with the other supposition of lots

215
00:21:10,880 --> 00:21:16,320
of tiny curves. And that's really, this is true by nature, by construction. This is where the

216
00:21:16,320 --> 00:21:21,600
Fourier transform starts, right? And the more interesting question is, you know, what sort of

217
00:21:21,600 --> 00:21:26,880
data is a good fit for encoding the Fourier transform? And what sort of data is not a good

218
00:21:26,880 --> 00:21:31,760
fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have

219
00:21:31,760 --> 00:21:37,360
a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves,

220
00:21:37,360 --> 00:21:42,000
then it's going to be a very, very efficient encoding at a good idea. And deep learning is

221
00:21:42,000 --> 00:21:46,160
very much like that. We should ask, you know, what are its strong points, what are its weak points?

222
00:21:46,160 --> 00:21:50,880
Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there,

223
00:21:50,880 --> 00:21:55,840
I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table

224
00:21:55,840 --> 00:22:00,960
with some amount of generalization power, because they have some notion of distance

225
00:22:00,960 --> 00:22:06,160
between parts. They're capable of comparing points by measuring the distance between them, right?

226
00:22:06,160 --> 00:22:11,120
And this, this is what would enable this kind of hash table to actually generalize, as opposed to

227
00:22:11,120 --> 00:22:14,560
the classic kind of hash table, which is just memorizing the data.

228
00:22:14,560 --> 00:22:19,760
It's very interesting that you allude to the fact that, you know, what kind of data is the model

229
00:22:19,760 --> 00:22:26,720
good for, and so on. And now, deep learning models being essentially like really, as Tim said,

230
00:22:26,720 --> 00:22:31,040
like big interpolators of arbitrary manifolds, do you think there is something

231
00:22:31,680 --> 00:22:38,480
common across the types of data we choose deep learning for? Or, you know, could we in fact

232
00:22:38,480 --> 00:22:45,280
use deep learning for most kinds of manifold dish data? Or do you think there is some kind of

233
00:22:45,360 --> 00:22:50,720
specialness about natural signals that makes deep learning very attuned to them?

234
00:22:51,440 --> 00:22:57,040
So I think most things are to some extent interpolative, which is why you can actually do

235
00:22:57,040 --> 00:23:00,960
lots of things with deep learning models. Doesn't necessarily mean it's always a good idea,

236
00:23:00,960 --> 00:23:05,200
but it's going to kind of work, right? You know, when people hear the word interpolation,

237
00:23:05,200 --> 00:23:10,720
they tend to think about linear interpolation, that's what pops up in their mind. That's not

238
00:23:10,880 --> 00:23:15,440
always what deep learning models are doing, right? They're interpolating on this very complex,

239
00:23:15,440 --> 00:23:21,680
very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior.

240
00:23:22,400 --> 00:23:28,960
And in practice, it's always possible to an arbitrary discrete algorithm in a continuous

241
00:23:28,960 --> 00:23:34,080
manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory.

242
00:23:34,080 --> 00:23:39,680
So for any program, you can imagine, you can ask, you know, is there a deep learning model that

243
00:23:39,680 --> 00:23:44,480
will encode some kind of approximation of it? And the answer is always yes, right?

244
00:23:45,120 --> 00:23:51,040
Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But

245
00:23:51,040 --> 00:23:55,120
there are, if you try to do that, actually, there are some issues with that. There are very much,

246
00:23:55,120 --> 00:23:59,520
you know, some problems for which deep learning is good fit, some problems for which deep learning

247
00:23:59,520 --> 00:24:06,400
is not a good fit. In the limit, the extreme point is a space that is not interpolative at all,

248
00:24:06,400 --> 00:24:10,400
which is quite right, actually. You know, most spaces, even very discrete kind of spaces,

249
00:24:10,400 --> 00:24:16,960
do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance,

250
00:24:16,960 --> 00:24:24,400
trying to train a deep learning model to predict the next prime number, right? Or to tell whether

251
00:24:24,400 --> 00:24:30,480
a number is a prime number. As you cannot actually do that, the best you can do is memorize the

252
00:24:30,480 --> 00:24:36,800
train data point, because the space of prime numbers is not interpreted at all. So your deep

253
00:24:36,800 --> 00:24:43,200
learning model will always have zero generalization power. But that's actually quite rare. This is

254
00:24:43,200 --> 00:24:49,680
kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic

255
00:24:49,680 --> 00:24:54,240
problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily

256
00:24:54,240 --> 00:24:59,120
mean that it's a good idea to try to solve, you know, such problems with deep learning models

257
00:24:59,120 --> 00:25:04,000
for deep learning to be a good idea. You need a very, you need very much the manifold

258
00:25:04,000 --> 00:25:09,360
level as it is to apply. So it works best for perception problems. Any problem that humans

259
00:25:09,360 --> 00:25:14,320
can solve via pure intuition or perception is probably a good fit for deep learning. But any

260
00:25:14,320 --> 00:25:19,600
problem where you need, you know, high level explicit step by step reasoning is probably

261
00:25:19,600 --> 00:25:24,240
a bad fit for deep learning. And, you know, 99% of what today software engineers solve,

262
00:25:24,960 --> 00:25:30,000
the writing code is going to be a bad fit for deep learning. That doesn't mean that there

263
00:25:30,000 --> 00:25:35,440
wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm

264
00:25:35,440 --> 00:25:41,040
in a smooth manifold. This is always possible to some extent, right? But there are very significant

265
00:25:41,040 --> 00:25:46,080
issues with attempting to do this. I like just because something is theoretically possible doesn't

266
00:25:46,080 --> 00:25:52,240
mean you should actually do it. I think we might be not being careful enough when we say what we

267
00:25:52,240 --> 00:25:59,200
mean by program. Because, for example, if I take program to be the universal sense like a program

268
00:25:59,200 --> 00:26:04,240
is something that can run on a Turing machine, for example, because of the fact that that type

269
00:26:04,240 --> 00:26:11,200
of program actually has access to unbounded time and memory computation. It's impossible in the

270
00:26:11,200 --> 00:26:17,280
general sense to encode that in any finite neural network, like I can write a very short piece of

271
00:26:17,280 --> 00:26:22,960
code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do

272
00:26:22,960 --> 00:26:27,600
that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I

273
00:26:27,600 --> 00:26:31,360
think that's like a big source of confusion often time with these statements that like, you know,

274
00:26:31,920 --> 00:26:36,480
oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural

275
00:26:36,480 --> 00:26:42,480
Turing machine, which is a neural network that's the finite state machine piece of a Turing machine,

276
00:26:43,040 --> 00:26:47,600
that can be Turing complete. But in the general case, you know, finite neural networks, which is

277
00:26:47,600 --> 00:26:52,320
what everyone means by neural networks, are not Turing complete. And it actually has practical

278
00:26:52,320 --> 00:26:57,840
effects, right? This is why we see this sort of explosion and the number of parameters to kind of,

279
00:26:57,840 --> 00:27:03,520
you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only

280
00:27:03,520 --> 00:27:07,360
interested in realistic programs, like the sort of programs that start to engineer with right,

281
00:27:07,360 --> 00:27:11,360
for instance. And we're only interested in realistic neural networks. And by the way,

282
00:27:11,440 --> 00:27:15,360
the constraints that we have on neural networks are actually much stronger than asking,

283
00:27:15,360 --> 00:27:21,280
given this program that I have, is there a neural network that could embed it in a continuous

284
00:27:21,280 --> 00:27:26,240
manifold? The constraint is actually, is there a neural network that could not only represent it,

285
00:27:26,240 --> 00:27:32,480
but that could learn this embedding of the program from there. And this is a several orders of

286
00:27:32,480 --> 00:27:39,360
magnitude harder, right? Learnability is a big problem because you're fitting your manifold via

287
00:27:39,360 --> 00:27:47,360
gradient descent, right? And if the structure you're trying to fit is too discrete, with too

288
00:27:47,360 --> 00:27:53,840
big discontinuities, gradient descent will not work at all. And the best you can do is, again,

289
00:27:53,840 --> 00:28:00,960
just memorize the train data. So I can maybe give you a concrete example to kind of ground

290
00:28:00,960 --> 00:28:08,480
our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do

291
00:28:08,480 --> 00:28:14,160
something pretty cool, which actually became a cool example on the Keras website. He used a LSTM

292
00:28:14,160 --> 00:28:22,160
model to multiply numbers, but not like numbers multiplied by value, but the input of the model

293
00:28:22,160 --> 00:28:27,360
would be strings, like two strings, strings of digits. And the LSTM will actually learn the

294
00:28:27,360 --> 00:28:32,240
multiplication algorithm for like multiplying three digits and three digits numbers, kind of the

295
00:28:32,240 --> 00:28:38,000
sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably,

296
00:28:38,080 --> 00:28:42,800
that worked, right? It works just fine. So you can train a deep learning model to learn this

297
00:28:42,800 --> 00:28:46,880
algorithm. And you could, of course, train a transformer model to do the same. It will actually

298
00:28:46,880 --> 00:28:52,880
be probably significantly more efficient. So that works. That comes with a number of downsides.

299
00:28:52,880 --> 00:28:56,880
So first, in order to train that algorithm, which is very simple, you're going to need

300
00:28:56,880 --> 00:29:03,280
thousands and thousands of examples of different strategic numbers. And once you've trained your

301
00:29:03,280 --> 00:29:10,400
algorithm, because the actual algorithm was embedded in the neural network, it does generalize to

302
00:29:10,400 --> 00:29:15,600
never see before digits, right? So it's actually learning the algorithm. It's not just learning,

303
00:29:15,600 --> 00:29:21,520
I'm just not memorizing the data. But the thing is, because the embedding of an algorithm,

304
00:29:21,520 --> 00:29:25,840
the embedding of a discrete structure in the continuous space, is not the same thing as the

305
00:29:25,840 --> 00:29:31,120
original discrete object. There are glitches in your deep learning network, unless that's something

306
00:29:31,120 --> 00:29:35,520
you could have found via program synthesis, for instance, it's not going to be correct 100% of

307
00:29:35,520 --> 00:29:41,520
the time, it's going to be correct 95% of the time. In much the same way that if you try to

308
00:29:41,520 --> 00:29:46,560
encode a very discrete object via the Fourier transform, it's not going to be correct. 100%

309
00:29:46,560 --> 00:29:50,720
of the time is going to be an approximation and around sharp angles, it's actually going to be

310
00:29:50,720 --> 00:29:56,000
wrong. And very importantly, and this is really like the algorithm that you've painstakingly

311
00:29:56,080 --> 00:30:02,800
embedded into your deep learning model via exposure to data, does only, it does not generalize very

312
00:30:02,800 --> 00:30:08,000
well, it only does local generalization, meaning that if you train it with three, to multiply

313
00:30:08,000 --> 00:30:12,480
three digit numbers, and then you send it a five digit number, is it going to work? No,

314
00:30:12,480 --> 00:30:16,400
absolutely not. And not only is it not going to work, but you could not in fact,

315
00:30:18,240 --> 00:30:23,760
few shots fine tune your algorithm to learn to handle five digits, seven digits and so on.

316
00:30:23,760 --> 00:30:27,760
If you want to fine tune your algorithm, you're going to need thousands, maybe millions

317
00:30:28,640 --> 00:30:35,200
of examples, right? So it's all local generalization. And lastly, it's super inefficient,

318
00:30:35,200 --> 00:30:40,080
like I think we can all agree with this, that multiplication is not like it's not

319
00:30:41,440 --> 00:30:46,400
a clever use of an LSTM, it's you're burning tons of resources for something that's actually

320
00:30:46,400 --> 00:30:51,920
super easy. And you can compare that, like since we are talking about pros and cons of deep learning,

321
00:30:51,920 --> 00:30:56,480
you can compare that to what you could get with a program synthesis engine. Like I don't want to

322
00:30:56,480 --> 00:31:00,480
compare to what you could get with a human written algorithm, because kind of the point of deep

323
00:31:00,480 --> 00:31:05,920
learning is that it enables you to develop programs that you could not otherwise write by hand.

324
00:31:05,920 --> 00:31:10,400
So the right point of comparison is actually what you could do with deep learning versus what you

325
00:31:10,400 --> 00:31:17,280
could do with discrete program synthesis based on discrete search and the DSL. And if you were to

326
00:31:17,280 --> 00:31:23,760
use a program synthesis to solve the multiplication problem, so you would find a solution, even a

327
00:31:23,760 --> 00:31:30,240
very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to

328
00:31:30,240 --> 00:31:35,120
find it, it can find it with a handful of examples, you're not going to need thousands of examples,

329
00:31:35,120 --> 00:31:39,280
like in the deep learning case, you're going to need maybe five. And the program you get out of it

330
00:31:39,280 --> 00:31:45,040
is going to be exact, because it is the exact discrete algorithm, it is not a continuous

331
00:31:45,040 --> 00:31:49,760
embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight,

332
00:31:49,760 --> 00:31:53,680
so it will be very efficient, you know, and like the LCM or transformer model,

333
00:31:53,680 --> 00:31:59,280
and crucially, it's going to generalize. So if you develop it only from three digit numbers,

334
00:31:59,280 --> 00:32:03,040
maybe there will be something inside it that will hardcore the assumption that they're dealing with

335
00:32:03,040 --> 00:32:08,240
three digit numbers. But even if that's the case, you can take it and automatically learn

336
00:32:09,200 --> 00:32:13,840
a generalized form of it if you just start giving it seven digit numbers. Very easy because it's

337
00:32:13,840 --> 00:32:18,880
just modifying probably a couple lines of code. So it is capable of strong generalization. So here

338
00:32:18,880 --> 00:32:23,840
you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning

339
00:32:23,840 --> 00:32:30,160
problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with

340
00:32:30,160 --> 00:32:36,320
extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting

341
00:32:36,320 --> 00:32:40,640
embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on

342
00:32:40,640 --> 00:32:44,640
a person at a time, it's going to be pretty long. It's only going to be capable of hardcore

343
00:32:44,640 --> 00:32:53,520
generalization, right? Because again, there is a huge difference in representational flexibility

344
00:32:53,520 --> 00:32:59,040
between your very simple, discrete algorithm and some kind of very complex, high dimensional

345
00:32:59,040 --> 00:33:04,400
continuous embedding. And then there's also the efficiency consideration. So clearly for

346
00:33:04,400 --> 00:33:09,600
if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's

347
00:33:10,160 --> 00:33:16,800
perception problem, where you have data points that fit on a nice and smooth manifold, then

348
00:33:16,800 --> 00:33:24,640
deep learning is actually the right answer. And if you tried to train a discrete program to develop

349
00:33:24,640 --> 00:33:29,360
your program synthesis, an actual algorithm to classify MNIST digits, for instance.

350
00:33:30,800 --> 00:33:36,240
Everything I just said would be true, but in reverse, your program would be brittle. The

351
00:33:36,240 --> 00:33:40,960
deep learning model would be robust, and so on. So there are really problems where deep learning

352
00:33:40,960 --> 00:33:45,120
is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list

353
00:33:45,120 --> 00:33:50,640
with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying.

354
00:33:51,200 --> 00:33:56,720
It is possible to sort a list of deep learning with some hacky inductive priors and probably

355
00:33:56,720 --> 00:34:01,440
memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots

356
00:34:01,440 --> 00:34:06,880
of problems that fall in the middle, where there is a semi continuous structure and some

357
00:34:06,880 --> 00:34:12,000
regularity, but it's still a discrete problem. And you're saying in that situation, we should

358
00:34:12,000 --> 00:34:16,400
still use program search, but maybe we can use deep learning, maybe something about the shape

359
00:34:16,400 --> 00:34:20,720
of the manifold, even though it's semi continuous, could actually tell us about how to do that

360
00:34:20,720 --> 00:34:26,320
program search more efficiently. But it seems to me that if there are problems out there,

361
00:34:26,320 --> 00:34:32,320
let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here,

362
00:34:32,320 --> 00:34:38,880
it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily

363
00:34:38,880 --> 00:34:44,000
paying a lot of attention to the nature of deep learning, why it works, why it doesn't work.

364
00:34:44,000 --> 00:34:49,520
I also think the people, they are basically two categories of people. They are like laypeople,

365
00:34:49,520 --> 00:34:55,200
and they are people with deep expertise. And the big problem we have here is that the people with

366
00:34:55,200 --> 00:35:01,760
a lot of expertise are going to be a lot of the time driven by motivated thinking. Because

367
00:35:03,680 --> 00:35:07,360
like I do, they work in the field of deep learning, and so they're going to have this vested

368
00:35:07,360 --> 00:35:12,960
interest in deep learning being potentially more powerful, more general at the nature is. I think

369
00:35:12,960 --> 00:35:19,440
if you want to think clearly, the primary obstacle is motivated thinking. It's fighting

370
00:35:19,520 --> 00:35:26,160
against what you want to be true. So I tend to have super boring opinions in that sense,

371
00:35:26,160 --> 00:35:31,440
because I do my best to try to forget kind of what I would like the world to be in my

372
00:35:31,440 --> 00:35:36,240
best interest and try to look at it as it really is. And that will tend to actually diminish

373
00:35:36,240 --> 00:35:41,840
the importance of my own work. So yeah, but you know, I've been doing deep learning for

374
00:35:41,840 --> 00:35:47,360
almost a decade. Of course, I would want it to be like this incredible world changing thing that

375
00:35:47,360 --> 00:35:51,040
leads to human level intelligence, right off the bat, that would be that would be awesome,

376
00:35:51,040 --> 00:35:56,240
that would be amazing, and that would be right in the middle of it. But that's not that's not

377
00:35:56,240 --> 00:36:02,960
actually what's going on. You said you tend to be what was the word, not not controversial ideas

378
00:36:02,960 --> 00:36:07,600
or something because you try to stick to the way the world is rather than the way you want the

379
00:36:07,600 --> 00:36:13,200
world to be. But we just had Yannick produce an interesting video about how if you think that

380
00:36:13,280 --> 00:36:17,360
machine learning models essentially attempt to do the same thing, right? I mean, they're not human

381
00:36:17,360 --> 00:36:22,160
beings, they don't really have wants per se, they're just modeling reality as it is. It turns out

382
00:36:22,160 --> 00:36:28,640
reality itself really annoys a lot of people, like they just don't like reality, and they don't like

383
00:36:28,640 --> 00:36:33,280
the way the world is, and they wish it was something different. And that infects like every mode of

384
00:36:33,280 --> 00:36:37,840
their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for

385
00:36:37,840 --> 00:36:45,600
me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend.

386
00:36:45,600 --> 00:36:51,280
But I have no exception. Most people have opinions not because they've seen evidence in

387
00:36:51,280 --> 00:36:56,400
support of their opinion, but because it's in their interest for this opinion to be true,

388
00:36:56,400 --> 00:37:01,360
or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3

389
00:37:01,360 --> 00:37:07,120
and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim

390
00:37:07,120 --> 00:37:11,680
that the pre-trained language model could perform few short generalizations. I thought that's

391
00:37:11,680 --> 00:37:17,600
super fascinating. I'm always super excited if I hear about something that's really challenging

392
00:37:18,320 --> 00:37:25,120
my initial kind of mental model of how the world works, you know, it's like a few years back,

393
00:37:25,120 --> 00:37:29,920
and there was this claim that a neutrino was measured going faster than speed of flight.

394
00:37:29,920 --> 00:37:34,960
I mean, that's exciting, right? That's like new physics, you want it to be true, at least you

395
00:37:34,960 --> 00:37:39,280
want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's

396
00:37:39,280 --> 00:37:44,240
disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something,

397
00:37:44,240 --> 00:37:48,400
something novel, and that would really challenge what they thought to be true by deep learning

398
00:37:48,400 --> 00:37:54,960
models. And I regret to say that everything I've seen close has actually confirmed. In my view,

399
00:37:54,960 --> 00:38:00,720
that basically deep learning models, they can learn to embed algorithms given sufficient exposure

400
00:38:00,720 --> 00:38:08,080
to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern

401
00:38:08,080 --> 00:38:13,120
they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC,

402
00:38:13,120 --> 00:38:20,960
for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually

403
00:38:20,960 --> 00:38:28,240
learned anything from it. It hasn't expanded my view of the world, which is too bad. Like,

404
00:38:28,240 --> 00:38:34,560
I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model

405
00:38:34,560 --> 00:38:40,560
is exposed to many patterns. You could call them algorithms, for instance, in many different contexts.

406
00:38:40,560 --> 00:38:45,440
And so it has memorized these patterns. And now it's able to take these patterns and apply them

407
00:38:45,440 --> 00:38:49,680
to new data in much the same way that the multiplication algorithm we are talking about.

408
00:38:49,680 --> 00:38:54,800
Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits

409
00:38:54,800 --> 00:39:00,560
in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms

410
00:39:00,560 --> 00:39:06,560
like that. But the model is not synthesizing these algorithms on the fly. They are in the model already.

411
00:39:07,280 --> 00:39:12,880
And if you try to apply GPT-3 to something for which a new algorithm would need to be produced,

412
00:39:12,880 --> 00:39:16,000
like an ARC task, for instance, it has just completed anything.

413
00:39:16,000 --> 00:39:21,280
It seems to all build up what you're saying, because there is this strong generalization

414
00:39:21,280 --> 00:39:27,760
versus local generalization. And then you make a case that in order to do strong generalization,

415
00:39:27,760 --> 00:39:33,360
we need maybe something like program synthesis approach. So deep learning can't necessarily

416
00:39:33,360 --> 00:39:39,680
get us there in most problems. And you make an interesting case that something like graph

417
00:39:39,680 --> 00:39:47,440
isomorphism search could play a core role in that. Could you briefly connect all of these

418
00:39:47,520 --> 00:39:51,680
terms together of the case you're making there? Because it's super interesting.

419
00:39:51,680 --> 00:40:00,240
So going back to it, Tim was saying it's rarely the case that you have problems that are fully

420
00:40:00,240 --> 00:40:04,560
interpretive or fully discrete. There are definitely such problems. In fact, most perception

421
00:40:04,560 --> 00:40:10,160
problems are almost entirely interpretive. And most programs, the kind of programs that you

422
00:40:10,720 --> 00:40:16,400
write there, they're largely discrete, not interpretive. But most tasks actually are best

423
00:40:16,400 --> 00:40:22,960
solved via a combination of both. And I actually believe that's true for the way humans think.

424
00:40:22,960 --> 00:40:28,560
You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every

425
00:40:28,560 --> 00:40:36,560
thought you have and everything you do with your mind is not one or the other. It's a combination

426
00:40:36,800 --> 00:40:42,240
of both. That type 1 and type 2 are really unmatched into each other in everything you

427
00:40:42,240 --> 00:40:48,720
think and everything you do. Like, for instance, perception. That looks like something very

428
00:40:48,720 --> 00:40:53,840
instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot

429
00:40:53,840 --> 00:40:58,960
of reasoning that's embedded into perception. And the reverse is true, for instance. If you

430
00:40:58,960 --> 00:41:03,760
look at a mathematician, for instance, proving a theorem, where they're writing down on the

431
00:41:03,760 --> 00:41:09,600
sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided

432
00:41:09,600 --> 00:41:14,800
by high-level intuition, which is very much interpreted. They know where they're going,

433
00:41:14,800 --> 00:41:22,240
without having to derive the exact sequence of steps to get there. So they have this high-level

434
00:41:22,240 --> 00:41:26,960
kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions

435
00:41:26,960 --> 00:41:33,280
because you are driving on network frauds. But if you have a bird, a GPS, for instance,

436
00:41:33,280 --> 00:41:37,760
you can kind of see the direction in which you are going, which is interpolated. If you're talking

437
00:41:37,760 --> 00:41:42,080
about direction, you're talking about distances, you're talking about geometric spaces. And

438
00:41:42,080 --> 00:41:49,040
everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same

439
00:41:49,040 --> 00:41:55,920
time. If you go back to first principles, intelligence is about abstraction. So intelligence

440
00:41:55,920 --> 00:42:05,360
is about the ability to face the future, given things you've seen in the past. And the way you do

441
00:42:05,360 --> 00:42:12,240
that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template,

442
00:42:12,240 --> 00:42:17,120
maybe it's an algorithm that will actually be effective in terms of explaining the future. And

443
00:42:17,120 --> 00:42:23,760
that's why it makes it abstract, is that it can handle multiple instances of some kind of thing,

444
00:42:24,000 --> 00:42:28,960
that thing is an abstraction. And if it's abstract enough, it can actually handle instances

445
00:42:28,960 --> 00:42:36,080
you've never seen before, right? It does generalization power. And all abstraction is worn

446
00:42:36,080 --> 00:42:43,200
from analogy. Abstraction starts when you make an analogy between two things. Like you say,

447
00:42:43,200 --> 00:42:47,520
time is like a river, if you want to get philosophical or something. But in general,

448
00:42:47,520 --> 00:42:51,520
you can just say this apple looks similar to this other apple. So there is such a thing as

449
00:42:51,520 --> 00:42:57,440
the concept of an apple, for instance. And the part that is shared between the two things that

450
00:42:57,440 --> 00:43:02,960
you're relating to each other, the subject of the analogy that that's the part that can be said to

451
00:43:02,960 --> 00:43:07,760
be abstract, that is the part that will help you make sense of the future, like you encounter a

452
00:43:07,760 --> 00:43:12,400
third apple in the future, you know, it's an apple. Because you don't even need to relate this to

453
00:43:12,400 --> 00:43:16,640
the apple should have memorized, you just need to, you just need to relate it to the template,

454
00:43:16,640 --> 00:43:21,120
the abstract template of an apple that you've formed by from exposure to different kinds of

455
00:43:21,120 --> 00:43:26,800
apples in the past. And if you think about what's what's an analogy, really, like how do you find

456
00:43:26,800 --> 00:43:33,440
an analogy, it's a way to compare two things to each other. And there are only really two ways

457
00:43:33,440 --> 00:43:43,120
to compare things. You can, you can basically ask how similar are they in terms of distance,

458
00:43:43,120 --> 00:43:48,640
like you can say implicitly, there's you're looking at the space of points, there's a distance

459
00:43:48,640 --> 00:43:54,000
between any two points. That's, that's the type one, a subject analogy that leads to type one

460
00:43:54,000 --> 00:44:00,880
abstractions, which leads to a type one thinking, right? So a type one analogy is like your things,

461
00:44:00,880 --> 00:44:06,400
you say to what degree they're similar to each other. So you read them by distance, you, so

462
00:44:06,400 --> 00:44:13,120
implicitly, it means you put your things on in a geometric space, right? And type one abstraction

463
00:44:13,120 --> 00:44:17,760
is going to be a template. It's like you're going to have clusters of things, you can take the average

464
00:44:17,760 --> 00:44:23,360
and say everything that is within a certain distance of that template belongs to this category.

465
00:44:23,360 --> 00:44:29,120
That's that's type one. It's very much the way deep learning models work. And then you and then

466
00:44:29,120 --> 00:44:33,680
you start adding perception and intuition on top of that, which is very much the type one thing.

467
00:44:33,680 --> 00:44:40,080
And the other way you can compare two things is the discrete way, right? You can say these two

468
00:44:40,080 --> 00:44:45,920
things are exactly the same. They have exactly the same structure. Or maybe the structure of this

469
00:44:45,920 --> 00:44:51,280
thing is a subset of the structure of this bigger thing. So this creates topology grounded

470
00:44:51,280 --> 00:44:56,880
comparisons. So you have the geometry grounded comparison. It's all about distances and templates.

471
00:44:56,880 --> 00:45:02,720
And then you have the topology grounded way of comparing things. That's all about exact comparison

472
00:45:02,720 --> 00:45:09,760
or finding a sub graph isomorphism. So in the first case, your objects are very much

473
00:45:09,760 --> 00:45:14,720
points in geometric spaces. So they are vectors. And deep learning is always a great fit for this

474
00:45:14,720 --> 00:45:19,440
sort of stuff. And in the second case, your objects are going to be graphs, right? And you're

475
00:45:19,440 --> 00:45:22,640
and you're really looking at the structure of these graphs and substructure and so on.

476
00:45:22,640 --> 00:45:30,160
And you're doing always you're doing exact comparisons. And in practice, most thinking is

477
00:45:30,160 --> 00:45:35,440
actually kind of some some combination of these two atoms, right? Of these two poles.

478
00:45:36,240 --> 00:45:42,320
You're very rarely just going to say, yeah, this airport is exactly this close to my template

479
00:45:42,320 --> 00:45:46,640
of an airport. So it's an airport. You're going to have basically layers upon layers of thinking.

480
00:45:46,640 --> 00:45:50,880
And some of them are going to be intuitive. Some of them are going to be more about, you know,

481
00:45:50,880 --> 00:45:55,440
comparing structures and so on. What you're saying is really interesting, right? Because you invoke

482
00:45:55,440 --> 00:46:02,160
the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information,

483
00:46:02,160 --> 00:46:10,160
just like in a kaleidoscope, could be represented widely across experience space. So you say that

484
00:46:10,160 --> 00:46:16,000
intelligence is literally having some kind of sensitivity to abstract analogies.

485
00:46:16,000 --> 00:46:23,280
So the intelligence is about being able to face the future unknown future, given your past experience.

486
00:46:23,280 --> 00:46:29,200
And that's fundamentally requires the future to share some commonalities with the past. And

487
00:46:29,200 --> 00:46:34,800
that's that's the idea of the kaleidoscope hypothesis that the universe and our lives

488
00:46:34,880 --> 00:46:41,280
are made of lots of repeated atoms of structure. And in fact, if you look at the source,

489
00:46:41,280 --> 00:46:44,640
there are very few things that are that are unique that are kind of like

490
00:46:44,640 --> 00:46:49,360
the grains of sand that are at the origin of all the different kinds of moving patterns you

491
00:46:49,360 --> 00:46:54,080
can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe

492
00:46:54,080 --> 00:47:01,920
is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see

493
00:47:01,920 --> 00:47:07,360
two things in the universe that look similar to each other or that share some commonalities,

494
00:47:07,360 --> 00:47:13,360
a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is

495
00:47:13,360 --> 00:47:17,280
going to be is going to be an abstraction. We'll be one of these grains of sand in your

496
00:47:17,280 --> 00:47:24,800
in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering

497
00:47:24,800 --> 00:47:30,400
the universe to get back to this source of intrinsic complexity in the universe to get

498
00:47:30,400 --> 00:47:35,360
back to these abstractions. I think the heart of this conversation goes back thousands of years

499
00:47:35,360 --> 00:47:39,120
because what we're talking about right now is a lot of say, Platonism, right? Which is that there

500
00:47:39,120 --> 00:47:44,400
are these ideal abstract structures. And of course, they they really thought of them as actually

501
00:47:44,400 --> 00:47:50,720
existing in some universe. But you know, even if they don't exist in some reality, they at least

502
00:47:50,720 --> 00:47:56,160
exist in concept. And it strikes at the heart of this duality that's always been a very

503
00:47:56,880 --> 00:48:00,960
that's been one of the central mystery, really, of a lot of human thinking, which is

504
00:48:01,600 --> 00:48:07,120
particle versus wave, you know, discrete versus continuous abstract versus the real versus the

505
00:48:07,120 --> 00:48:12,160
messy. And you know, I think you pointed out, you definitely pointed this out in this call. But

506
00:48:12,160 --> 00:48:17,040
I think also in some of your papers that in your view, you know, let's say the ultimate solution

507
00:48:17,040 --> 00:48:23,120
or whatever of creating artificial intelligence or synthetic intelligence or whatever is a

508
00:48:23,200 --> 00:48:28,400
hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers.

509
00:48:29,040 --> 00:48:34,880
And, you know, I'm kind of curious, where is the state of the art now with actually implementing

510
00:48:35,680 --> 00:48:39,760
hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the

511
00:48:39,760 --> 00:48:45,840
topological neural networks that we talked about? Where where lies the direction of some type of a

512
00:48:45,840 --> 00:48:53,600
hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you

513
00:48:53,600 --> 00:48:59,280
will? Yeah, that's a great question. So I think this is definitely an active field of research,

514
00:48:59,280 --> 00:49:05,360
but I think the most promising direction right now is going to be discrete search very much. So

515
00:49:05,360 --> 00:49:09,760
a system that is discrete search centric that has a DSA and so on. And that's one of the

516
00:49:09,760 --> 00:49:15,360
it's basically just problems in this engine. But it is getting lots of help from deep learning

517
00:49:15,360 --> 00:49:21,680
models. And there are two ways in which you can incorporate this type one sort of thinking into

518
00:49:21,680 --> 00:49:30,080
a phenomenally type two centric system. So one way is so basically, you want to apply deep learning

519
00:49:30,080 --> 00:49:36,960
to any sorts of data sets where you have an abundance of data, and your data is interpreted.

520
00:49:36,960 --> 00:49:41,840
One example would be being able to easily play models to generate a sort of like perception

521
00:49:41,840 --> 00:49:50,080
DSL that your discrete search process can build upon. So look at art, art tasks, for instance,

522
00:49:50,080 --> 00:49:54,160
a human that is looking at art tasks, the very first layer through which they're approaching

523
00:49:54,160 --> 00:49:59,600
the art task is by applying basically perception primitives to the grid they're looking at. They

524
00:49:59,600 --> 00:50:05,040
are not actually analyzing the grid in a in a discrete way like cell by cell, object by object,

525
00:50:05,280 --> 00:50:10,320
they're approaching it holistically, like what do they see? And these outputs can be discrete

526
00:50:10,320 --> 00:50:14,560
concepts. And then you can start you can start applying the script reasoning to them. So generating

527
00:50:14,560 --> 00:50:20,160
the DSL. And by the way, the reason it's possible is because humans have access to tons of visual

528
00:50:20,160 --> 00:50:25,840
data and these different frames share lots of commonalities, right? So it is an interpolative

529
00:50:25,840 --> 00:50:29,680
space where deep learning is relevant, where intuition and perception are relevant. And the

530
00:50:29,680 --> 00:50:35,920
other way, which is is is much more difficult and much, much more subtle thing is basically being

531
00:50:35,920 --> 00:50:43,040
able to provide guidance to the discrete search process, basically, because even though one single

532
00:50:43,040 --> 00:50:47,600
program, so learning one single problem, for instance, for an art task is not a good fit

533
00:50:47,600 --> 00:50:52,560
for deep learning model at all, because you only have a handful of examples to learn from.

534
00:50:53,360 --> 00:50:59,920
And the program is super discrete. It's not really easily embeddable in this movement.

535
00:50:59,920 --> 00:51:04,800
However, here's the thing, the space of all possible programs, for instance, the space of

536
00:51:04,800 --> 00:51:09,120
all possible art tasks and all possible programs that solve art tasks is actually

537
00:51:09,120 --> 00:51:14,800
very likely going to be interpolative, at least to some extent. And so you can imagine a deep

538
00:51:14,800 --> 00:51:21,040
learning model that has enough experience with with these problems and the algorithmic solution

539
00:51:21,040 --> 00:51:26,240
that it can it can start providing directions to the search to the discrete system. So

540
00:51:27,600 --> 00:51:32,880
basically, you're in a kind of like you have, yeah, you have like layers of

541
00:51:34,000 --> 00:51:39,120
of learning the lowest layer is going to be perceptive. It's going to be learned across many

542
00:51:39,120 --> 00:51:44,080
different tasks and many different environments. It's going to be type type one, then you're going

543
00:51:44,080 --> 00:51:50,480
to have the context specific on the fly problem solving system that's going to be type two.

544
00:51:51,200 --> 00:51:55,280
And the reason is going to be possible and efficient is because it's going to be guided

545
00:51:55,280 --> 00:51:59,840
by this upper layer, which is going to be type one, which is also going to be trained

546
00:51:59,840 --> 00:52:04,960
from a very, very long experience across many different problems and tasks. And it is able

547
00:52:04,960 --> 00:52:12,320
to do interpolation between different tasks. So can I challenge you a little bit maybe because

548
00:52:12,320 --> 00:52:17,760
you say maybe, you know, all of these problems and what humans do is a bit of an interpolate

549
00:52:17,760 --> 00:52:25,200
like an interpolation between the interpolative systems and the discrete systems. And I see that

550
00:52:25,200 --> 00:52:32,000
going for, you know, something like an arc task or or if you really write code. But if you really

551
00:52:32,000 --> 00:52:38,000
come to let's say, let's say the highest levels of human intelligence, which to me seems to be

552
00:52:38,000 --> 00:52:46,880
navigating social situations, which is is is ultimately is super complex. And I can imagine

553
00:52:46,960 --> 00:52:53,840
something like the graph structure you're referring to be that being, let's say I come into a room

554
00:52:53,840 --> 00:53:01,440
and I see the graphs as, you know, what kind of social dynamics exist in this room, you know,

555
00:53:01,440 --> 00:53:07,200
this is the father of this person, and that person's kind of angry at me. And so I need to,

556
00:53:07,200 --> 00:53:15,360
you know, do something. And my question is, how often is that really a disk like how often can

557
00:53:15,360 --> 00:53:22,000
you really map this in a discrete way to another graph? Isn't isn't every situation going to be

558
00:53:22,000 --> 00:53:29,120
a little bit different, even in terms of its graph structure? And, you know, even if in an arc task,

559
00:53:29,120 --> 00:53:36,640
a line is just like a little bit squiggled, any program synthesis approach would have a hard

560
00:53:36,640 --> 00:53:42,160
time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like

561
00:53:42,640 --> 00:53:49,840
how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction

562
00:53:49,840 --> 00:53:56,880
is to erase the irrelevant differences between different instances of the thing and focus on

563
00:53:57,920 --> 00:54:03,760
the commonalities that matter. So like if the squiggled in your line is not relevant,

564
00:54:03,760 --> 00:54:08,240
then the proper abstraction for a line should abstract it away. I was going to pick up on that

565
00:54:08,240 --> 00:54:13,360
because your main point basically is that program based abstraction is more powerful

566
00:54:13,360 --> 00:54:16,880
than geometric based abstraction, because topology is robust to small perturbations,

567
00:54:16,880 --> 00:54:21,840
but it's more than that. It comes back to these analogies, right? So we actually have functions

568
00:54:21,840 --> 00:54:27,440
and abstractions in our mind that as you say, will take away all of the relevant differences,

569
00:54:27,440 --> 00:54:34,320
but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you

570
00:54:34,320 --> 00:54:41,040
think the type one and type two reasoning are really different or is there also a continuum

571
00:54:41,040 --> 00:54:46,400
between them? Like you say we need we need hybrid systems, but is there something,

572
00:54:47,680 --> 00:54:51,280
right? Because they're both they're both in the brain, they're both on the same neurons,

573
00:54:51,280 --> 00:54:57,360
like is there a continuum? So right, so yes and no, I do believe they are they are very

574
00:54:57,360 --> 00:55:02,240
qualitatively different. These are the two poles of cognition, but there are there are, you know,

575
00:55:02,240 --> 00:55:06,800
most most things we do with our mind are a combination of both. That doesn't mean it's

576
00:55:06,800 --> 00:55:10,720
it lies somewhere in between. It means it's a direct combination of one pole with the other,

577
00:55:10,720 --> 00:55:16,160
kind of like what I described with with the arc solver with three layers, with two layers of

578
00:55:16,160 --> 00:55:21,920
that type one and one layer in the middle of type two. But in very much the same way that you can

579
00:55:22,560 --> 00:55:29,360
embed discrete programs in a smooth manifold, you can also do the reverse. And when you're

580
00:55:29,360 --> 00:55:35,360
meaning you can basically encode an approximation of a geometric space using discrete constructs. In

581
00:55:35,360 --> 00:55:39,760
fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing,

582
00:55:39,760 --> 00:55:44,800
you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors

583
00:55:44,800 --> 00:55:49,440
of seemingly constant new numbers, you can compute a distance between two vectors and so

584
00:55:49,440 --> 00:55:54,640
all of this is an approximation that's actually grounded in discrete programs. So you can you

585
00:55:54,640 --> 00:55:59,280
can actually kind of merge the two together. It's not necessarily always a good idea. In

586
00:55:59,280 --> 00:56:05,120
particular, I think it's often not a good idea to try to embed an overly complex or overly

587
00:56:05,120 --> 00:56:12,400
discreet program in a constant new space. As I was mentioning earlier, the reverse is actually

588
00:56:12,400 --> 00:56:17,920
usually way more tractable. And by the way, my I think this is something that came up before

589
00:56:17,920 --> 00:56:23,920
in our conversation, but my kind of subjective totally not backed by any evidence opinion of

590
00:56:24,000 --> 00:56:30,240
how the brain works is that fundamentally it's doing type one on type two, using a discrete

591
00:56:30,240 --> 00:56:35,920
system, because it's actually much easier to do to type one via an approximation of a geometric

592
00:56:35,920 --> 00:56:39,840
space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can,

593
00:56:40,400 --> 00:56:44,640
if I just for the benefit of the reader, the listeners, if I can give some other examples,

594
00:56:44,640 --> 00:56:49,600
you know, for example, and mixed integer optimization, it's often the case that you

595
00:56:49,600 --> 00:56:54,480
take that problem. And instead of having these discrete values, you project it into a continuous

596
00:56:54,480 --> 00:57:00,160
space, do a continuous optimization. And then as you get sort of close to a good optimization,

597
00:57:00,160 --> 00:57:05,520
you discretize it back over into the, the discrete variables, you know, to, to kind of,

598
00:57:05,520 --> 00:57:10,960
you know, flesh out the most optimal path within that discrete space, or an example to is the

599
00:57:10,960 --> 00:57:15,920
gamma function, you know, which is a continuous generalization of the factorial, right? And

600
00:57:15,920 --> 00:57:21,760
it kind of provides some cool and interesting behavior in between those, those poles that

601
00:57:21,760 --> 00:57:26,400
show up very clearly on the graph as these discrete points. And this is this bizarre

602
00:57:27,040 --> 00:57:31,440
duality between the continuous and the discrete that we see like throughout the universe. And

603
00:57:31,440 --> 00:57:36,960
it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what

604
00:57:36,960 --> 00:57:40,880
some of the transformers folks must be saying now, because Max Welling, we had him on and

605
00:57:41,600 --> 00:57:46,640
folks have done topological applications using transformers or using graph neural

606
00:57:46,640 --> 00:57:52,960
networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,

607
00:57:52,960 --> 00:57:58,880
right? It was looking at different types of equivariance in topological space. Is it a naive

608
00:57:58,880 --> 00:58:05,440
thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess,

609
00:58:05,520 --> 00:58:12,080
I guess the question is, is there like one approach that's going to end up being universal? And it's,

610
00:58:12,080 --> 00:58:17,200
it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind

611
00:58:17,200 --> 00:58:23,360
of, it's kind of the question. And the answer is no, actually, like, by, by construction, do,

612
00:58:23,360 --> 00:58:29,600
due to the very nature of what deep learning is, it's like parametric continuous parametric models

613
00:58:29,840 --> 00:58:35,200
in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never

614
00:58:35,200 --> 00:58:41,760
actually going to be a good fit for most discrete programs. So, and, and the reverse is true as

615
00:58:41,760 --> 00:58:46,320
well. I don't think, so you have basically two engines that you can use to learn problems. You

616
00:58:46,320 --> 00:58:51,680
have quite undecent and you have discrete search. And I think the reverse is also true that discrete

617
00:58:51,680 --> 00:58:57,200
search is not going to be this universal approach that's going to beat everything. I truly believe

618
00:58:57,280 --> 00:59:02,640
that the AIs of the future will be truly hybrid in the sense that they will have these two engines

619
00:59:02,640 --> 00:59:06,720
inside them, they will be able to do this, they will be able to do this quick search.

620
00:59:06,720 --> 00:59:10,560
Right. And then, and then, and they will set, you, is that appropriate? You said, by the way,

621
00:59:10,560 --> 00:59:14,640
in your measure of intelligence paper that there are three types of priors, right? Low level,

622
00:59:14,640 --> 00:59:18,960
sensory motor priors and meta learning priors. That's the interesting one. I think that's got

623
00:59:18,960 --> 00:59:23,840
intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you

624
00:59:23,840 --> 00:59:28,720
said in your presentation last year, the two winning folks on that Kaggle challenge, one was

625
00:59:28,720 --> 00:59:33,200
doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program

626
00:59:33,200 --> 00:59:39,120
search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that

627
00:59:39,120 --> 00:59:45,200
was just doing a brute force, you know, selecting combinations of, of operations on this DSL.

628
00:59:46,000 --> 00:59:51,200
So this absolutely fascinates me. So at the moment, that seems like a horrific solution,

629
00:59:51,200 --> 00:59:55,520
but clearly no one could do it using deep learning. So, but, but this is what you're

630
00:59:55,520 --> 00:59:59,840
advocating for. So you're saying for these discrete problems, get, get a DSL. Now,

631
00:59:59,840 --> 01:00:03,280
all the stuff you're talking about, presumably they haven't done yet, you're saying, well,

632
01:00:03,280 --> 01:00:07,840
software engineering, the beauty of software engineering is being able to modularize things

633
01:00:07,840 --> 01:00:12,560
into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But

634
01:00:12,560 --> 01:00:17,280
he said, the reason why software engineering is so good is if I ask you, how long will it take

635
01:00:17,280 --> 01:00:22,960
you to build the game of Tetris? You will say not long at all. And if you look at the number of

636
01:00:22,960 --> 01:00:28,640
state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple

637
01:00:28,640 --> 01:00:34,000
of weeks is because you know that you can modularize it into, into blocks, you can't say the same for

638
01:00:34,000 --> 01:00:38,160
deep learning, right? But they don't appear to have done that on the arc challenge yet.

639
01:00:38,880 --> 01:00:43,440
Yeah, so the, the solutions we've seen on the accident so far have been incredibly,

640
01:00:43,440 --> 01:00:48,320
incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%.

641
01:00:49,360 --> 01:00:54,640
It's very primitive solutions. I think you can, even with today's technology, you can go much

642
01:00:54,640 --> 01:01:00,560
further. Like the, what I was describing before about learning a DSL that is perceptive and then

643
01:01:00,560 --> 01:01:05,840
guiding discrete program search. Yeah, intuition about program space. This is already something

644
01:01:05,840 --> 01:01:10,640
that you can try today. So there's one approach that I was very excited about. And that I thought

645
01:01:10,960 --> 01:01:16,880
was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks.

646
01:01:17,920 --> 01:01:21,760
So check it out if you, if you have incidents, it's very good. I think that they're trying to

647
01:01:21,760 --> 01:01:27,680
play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into

648
01:01:27,680 --> 01:01:33,440
this engine? And I think that's really to me, that that is the sort of direction that is the most

649
01:01:33,440 --> 01:01:39,360
promising to that. So you have a paper that's fairly long on, it's called on the measure of

650
01:01:39,360 --> 01:01:45,120
intelligence. And you make the case that intelligence is something like the efficiency

651
01:01:45,120 --> 01:01:51,440
with which we transform prior information and experience into task solutions, as, as you have

652
01:01:51,440 --> 01:01:58,240
said before. And in that same paper, the arc challenge is presented. So, you know, a naive

653
01:01:58,240 --> 01:02:04,160
reader like me assumes there is some connection between, you know, what you say about intelligence

654
01:02:04,240 --> 01:02:11,280
and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and

655
01:02:11,280 --> 01:02:17,440
gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge.

656
01:02:18,160 --> 01:02:25,840
Is it immediately intelligent? Or what would you ask of that system for, for you to say,

657
01:02:25,840 --> 01:02:30,800
yes, that's intelligent, or it's, it's intelligent is, is high or something like this.

658
01:02:30,880 --> 01:02:38,000
So you, you, you would be able to make that, that conclusion, if and only if arc was a,

659
01:02:38,000 --> 01:02:43,280
was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc,

660
01:02:43,280 --> 01:02:50,080
are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the

661
01:02:50,080 --> 01:02:56,560
thing. So the thing you need to really understand about arc is that it's not kind of the end state

662
01:02:56,560 --> 01:03:02,240
of the intelligence benchmark. It is very much a work in progress. And there will be new iterations,

663
01:03:02,240 --> 01:03:07,680
especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle

664
01:03:07,680 --> 01:03:13,200
challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches

665
01:03:13,200 --> 01:03:18,160
although there were some cool stuff we still learned about and so on. But mostly we learned about

666
01:03:18,160 --> 01:03:23,200
the flows of arc. So there will be future additions and so on. So I will tell you this,

667
01:03:24,080 --> 01:03:29,600
if you solve the specific test set of arc as it exists today, you're not necessarily intelligent

668
01:03:29,600 --> 01:03:36,000
because it is not perfect because it has its laws. But if more generally speaking, you give me a system

669
01:03:36,000 --> 01:03:43,280
that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow,

670
01:03:43,280 --> 01:03:47,520
for instance, I give them to your system. If it's always solving them, I will say,

671
01:03:47,520 --> 01:03:51,280
yeah, it's looking like you've got a system that's, that's got, you know, pretty close to

672
01:03:51,280 --> 01:03:57,760
human level fluid intelligence. This is one of the things that, look, and I like the paper a lot,

673
01:03:57,760 --> 01:04:03,520
I think, I think it serves as a really good, you know, foundation for us to think differently

674
01:04:03,520 --> 01:04:08,000
about how to build intelligence. But, but I have some, some issues with it too as well. And one

675
01:04:08,000 --> 01:04:14,080
of them is this sort of necessity that it requires kind of white box analysis of things in order to

676
01:04:14,080 --> 01:04:17,920
figure out whether or not they're intelligent. Because for example, suppose time travel is

677
01:04:18,000 --> 01:04:23,920
actually possible. And you know, somebody like 100 years from now looks back on your arc thing and

678
01:04:23,920 --> 01:04:28,480
writes an algorithm that, that solves all, all them in there because it actually knows about them

679
01:04:28,480 --> 01:04:33,040
already and then ships it back into the past and we enter it into the competition. And no matter

680
01:04:33,040 --> 01:04:37,200
what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know,

681
01:04:37,200 --> 01:04:42,160
this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper,

682
01:04:42,160 --> 01:04:46,240
it's actually just encoded, you know, prior knowledge from the future. So we have to,

683
01:04:46,240 --> 01:04:50,160
we always have to kind of be able to look into the box, right, in order to evaluate

684
01:04:50,720 --> 01:04:54,400
intelligence in the way that you define in the paper. And so my question is one,

685
01:04:54,960 --> 01:05:01,200
isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box

686
01:05:01,200 --> 01:05:06,160
measure of intelligence? So basically, the fundamental issue is that if intelligence

687
01:05:06,160 --> 01:05:13,040
is this conversion ratio, then computing it requires knowing where you start from. And

688
01:05:13,040 --> 01:05:17,600
you don't really have a way around it. So the thing to keep in mind is that the

689
01:05:17,600 --> 01:05:23,280
under measure of intelligence stuff is not so much meant to provide like a sort of like

690
01:05:23,840 --> 01:05:28,480
golden measure of tape to measure anyone's intelligence or anything's intelligence.

691
01:05:28,480 --> 01:05:36,080
It is more meant as a sort of cognitive device to help you think about what the actual challenges are

692
01:05:36,800 --> 01:05:41,680
to help you kind of kind of reframe AI because they think they have been pretty deep and

693
01:05:41,680 --> 01:05:46,320
longstanding conceptual misunderstandings. So that is really being, that's being holding the

694
01:05:46,320 --> 01:05:54,400
feedback. So it's very much meant as a cognitive device. If you take a step back and you ask,

695
01:05:54,400 --> 01:05:59,200
why are we even trying to define intelligence and measure intelligence in the first place,

696
01:05:59,200 --> 01:06:05,280
why is it useful at all? I think it's useful to the extent that it is actionable, right,

697
01:06:05,280 --> 01:06:10,800
a good definition and a good measure should be actionable. So meaning it should help you

698
01:06:11,600 --> 01:06:17,280
think, it should help you find solutions and it should help you make progress. In particular,

699
01:06:17,280 --> 01:06:22,640
a good definition is a definition that will highlight the key challenges and help you think

700
01:06:22,640 --> 01:06:27,200
about it. And I think that's what the paper does. And a good measure is a measure that gives you an

701
01:06:27,200 --> 01:06:33,760
actionable feedback signal towards building the right kind of system, right in the sense that

702
01:06:33,760 --> 01:06:39,840
it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying

703
01:06:39,840 --> 01:06:48,720
to achieve. And the way it's trying to control for priors and experience is by assuming a fixed

704
01:06:48,720 --> 01:06:53,680
set of priors. And you're going to see, you know, every test taker can have such priors.

705
01:06:53,680 --> 01:06:58,800
This is the core knowledge priors. And then it controls for experience by only giving you a very

706
01:06:58,800 --> 01:07:05,360
small number of input examples. And also by making sure the tasks are sufficiently novel and

707
01:07:05,360 --> 01:07:11,440
surprising that you're unlikely to have seen a very similar instance before. So now, of course,

708
01:07:11,440 --> 01:07:16,240
it's super flawed. So this is not 100% true, of course, but this is kind of like the the

709
01:07:16,240 --> 01:07:20,880
planning ideal that we're trying to get to. So that for the record, that's a fascinating point to

710
01:07:20,880 --> 01:07:25,200
me is that you view this more as a cognitive device to help guide us to produce better,

711
01:07:26,000 --> 01:07:32,880
better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence

712
01:07:32,880 --> 01:07:39,200
and all we need to do is solve ARC. This is not at all the point. It's like it's one.

713
01:07:39,200 --> 01:07:43,280
Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would

714
01:07:43,280 --> 01:07:47,280
mean I was intelligent. But another interesting point, because Keith and I were looking at the

715
01:07:47,280 --> 01:07:51,840
paper again yesterday, because it's been, I haven't properly studied it since last year. But

716
01:07:52,880 --> 01:07:56,320
we were starting to talk about an alien that comes in from outer space. And, you know,

717
01:07:56,960 --> 01:08:02,640
we don't know the priors and the experience. And then I was thinking in a way, it might be a

718
01:08:02,640 --> 01:08:08,240
kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat

719
01:08:08,240 --> 01:08:12,720
someone with a higher elo than me, then only really tells me that I'm better, you know, as

720
01:08:12,720 --> 01:08:18,320
good as that person that I just beat. And similarly, this measure of intelligence, it only gives you

721
01:08:18,320 --> 01:08:23,600
a reading in the situation when you know what the conversion was. So if they are not converting

722
01:08:23,600 --> 01:08:29,120
anything, then you don't know. And another interesting byproduct of this is the more

723
01:08:29,120 --> 01:08:36,480
experienced you get, the less intelligent you get. So I would push back against that last claim

724
01:08:36,480 --> 01:08:41,520
that the measure of intelligence as I define it is dependent on how much experience you have.

725
01:08:43,360 --> 01:08:47,120
Because the amount of initial experience you have does not actually change at the conversion

726
01:08:47,120 --> 01:08:54,240
ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks,

727
01:08:54,240 --> 01:08:59,520
then yes, it does affect it. But if you're able to renew your set of tasks and come up with

728
01:08:59,520 --> 01:09:04,000
styles that are orthogonal to the experience that you have, then it's not the actual effect,

729
01:09:04,000 --> 01:09:11,280
the definition. So, but yeah, you're definitely right that if you take a pure black box approach,

730
01:09:11,280 --> 01:09:17,600
and all you're looking at, the only thing you can really measure is the behavior of a system.

731
01:09:17,600 --> 01:09:23,520
And unless you know how that behavior is achieved, you can't really tell immediately

732
01:09:23,520 --> 01:09:29,040
how much intelligence was involved in producing this behavior. If you look at an insect,

733
01:09:29,040 --> 01:09:33,920
they're capable of super complex behavior. Are they crazy intelligent? Well, actually,

734
01:09:33,920 --> 01:09:39,360
you know, probably not. And the way you can really tell is by putting these systems out of

735
01:09:39,360 --> 01:09:44,800
their comfort zone, getting them to face novel situations and see how they adapt. And that's

736
01:09:44,800 --> 01:09:52,160
the measure of intelligence. It's adaptability, the ability to deal with novel and unknown

737
01:09:52,160 --> 01:09:58,960
situations. But in order to give your system a novel and unknown situation, you need to have

738
01:09:58,960 --> 01:10:05,280
this white box understanding of what it already knows about. And that's not really something

739
01:10:05,280 --> 01:10:12,400
you can work on. So can I ask about the generalization difficulty? Because I sort of had

740
01:10:12,400 --> 01:10:17,760
some difficulty intuitively with some of, let's say, it's limiting cases. So for example,

741
01:10:18,720 --> 01:10:23,760
you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems

742
01:10:23,760 --> 01:10:30,320
tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic

743
01:10:30,320 --> 01:10:36,080
complexity will be greatest when that's just a random mapping, like I just assigned zero and one

744
01:10:36,080 --> 01:10:41,280
randomly to every single integer. And if I go to look at that generalization difficulty,

745
01:10:41,280 --> 01:10:46,320
it's going to be super high, because the length of the program for any set is basically going to

746
01:10:46,720 --> 01:10:52,320
be, you'd have to encode the entire set as a hash table, right? So how does like this measure

747
01:10:52,320 --> 01:10:59,120
account for or help us avoid problems where we're confusing generalization difficulty with just

748
01:10:59,120 --> 01:11:04,880
increasing random, you know, randomness? Well, I mean, increasing randomness is a part of

749
01:11:04,880 --> 01:11:10,800
the realization difficulty, right? Generalization is really the ability to deal with the stuff you

750
01:11:10,800 --> 01:11:15,840
don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is

751
01:11:15,920 --> 01:11:20,400
a part of it. But you're right that if you just add randomness to a system, you're increasing

752
01:11:20,400 --> 01:11:25,680
the generalization difficulty, but you're not increasing it in a very interesting way, right?

753
01:11:25,680 --> 01:11:31,440
Because you're increasing it in a way that's kind of orthogonal to an integration system's ability

754
01:11:31,440 --> 01:11:37,520
to deal with it, right? The best you can do is modify the system to be more robust to very much

755
01:11:37,520 --> 01:11:43,120
randomness. But that's not super interesting. What's really interesting is to test the system's

756
01:11:43,120 --> 01:11:50,320
sensitivity to subtle analogies, is to make the system face novel and unexpected situations that

757
01:11:50,320 --> 01:11:56,160
are actually derived from the past, but in interesting ways, right? Not just random ways.

758
01:11:56,160 --> 01:12:04,720
You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and

759
01:12:04,720 --> 01:12:12,000
so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable,

760
01:12:12,000 --> 01:12:18,640
like playing them against each other and so on. And also, we know that something like markets can

761
01:12:18,640 --> 01:12:29,200
be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks

762
01:12:29,200 --> 01:12:35,040
and other agents solving ARC tasks, and they're going some kind of money around and so on. And

763
01:12:35,040 --> 01:12:41,120
this could be kind of a powerful engine for research teams to research anything like this.

764
01:12:41,120 --> 01:12:47,600
And, you know, given that you have, I don't know how much, but you do have the backing of Google

765
01:12:47,600 --> 01:12:58,160
with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is

766
01:12:58,160 --> 01:13:08,880
it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google

767
01:13:08,880 --> 01:13:15,440
around this kind of project. But, yeah, so it would be super interesting to have this kind of

768
01:13:15,440 --> 01:13:20,640
two-part system where one part is generating the task and one part is learning to solve them.

769
01:13:20,640 --> 01:13:26,720
And you could get them to do some kind of curriculum optimization, like the task generator network

770
01:13:26,720 --> 01:13:33,040
would not just be trying to generate tasks that look like ARC tasks. It would be trying to

771
01:13:33,040 --> 01:13:39,120
generate tasks that correspond to level of generalization, difficulty and complexity that is

772
01:13:40,080 --> 01:13:45,440
right below the limits of the student system that's trying to solve them. Kind of like, you know,

773
01:13:45,440 --> 01:13:52,720
the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be.

774
01:13:52,720 --> 01:13:56,720
They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because

775
01:13:56,720 --> 01:14:01,760
that's how you get the most growth. So it's actually a system that's described at the very end

776
01:14:02,480 --> 01:14:07,680
of the paper on the measure of contagions. And I think one thing I point out in the paper is

777
01:14:07,680 --> 01:14:13,360
kind of like the pitfall you should avoid falling into is that this system is circular,

778
01:14:14,000 --> 01:14:20,000
right? And the complexity you're going to see in your task, it needs to come from somewhere, right?

779
01:14:21,200 --> 01:14:26,720
It's like conservation of complexity. So the system, this two-part system needs to have

780
01:14:27,680 --> 01:14:33,840
a source of intrinsic complexity. It needs to be grounded in the real world.

781
01:14:33,840 --> 01:14:39,360
And one way we can achieve that grounding, and I've been thinking about it, is I think we should,

782
01:14:39,360 --> 01:14:45,120
you know, like ARC tasks, as they are today, they're made by me and this is not a good setup

783
01:14:45,120 --> 01:14:50,400
because it's going to be biased. It's going to be very bottlenecked as well. I think we should

784
01:14:50,400 --> 01:14:55,920
start crowdsourcing our task. There should definitely be, you know, a filtering system so

785
01:14:55,920 --> 01:15:00,000
that we make sure that we're only keeping our tasks that are interesting, that are not too easy,

786
01:15:00,000 --> 01:15:06,480
that are not difficult, and that are only grounded in core knowledge priors. But if we have, like,

787
01:15:06,480 --> 01:15:12,000
this stream of novel ARC tasks that contain intrinsic complexity and novel information,

788
01:15:12,000 --> 01:15:17,040
because they come from the real world, they come from human brains, that have experienced the

789
01:15:17,040 --> 01:15:23,120
real world, and you use that as a way to ground your task generator, then you're starting to get

790
01:15:23,120 --> 01:15:29,440
a very interesting three-part system, right? So I would love to actually get that started,

791
01:15:29,440 --> 01:15:36,240
to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks

792
01:15:36,240 --> 01:15:41,440
that will be crowdsourced, and maybe something that will take the form of a continuous challenge

793
01:15:41,440 --> 01:15:46,240
where you have an API where you can draw a new ARC task, and every time you draw a task, it's

794
01:15:46,240 --> 01:15:52,400
actually a different one because you have so many of them. Gamify it, that'll make a fun game

795
01:15:52,480 --> 01:15:57,440
on a mobile app. There are actually a few people who have created, because ARC is open source,

796
01:15:57,440 --> 01:16:01,120
and they're totally free licensed, there are a few people who have created mobile apps where

797
01:16:01,120 --> 01:16:05,120
users sort of ARC tasks, and apparently it's popular. So there's also the other angle you

798
01:16:05,120 --> 01:16:08,960
mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right

799
01:16:08,960 --> 01:16:17,440
now, which is that, okay, let's start thinking about how to map ARC performance to psychometric,

800
01:16:17,440 --> 01:16:21,680
you know, classic kind of psychometric tests. Are there any efforts that you're aware of

801
01:16:21,680 --> 01:16:30,000
underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a

802
01:16:30,000 --> 01:16:35,600
workshop at AAAI the other day, and there were two presentations about efforts that teams of people,

803
01:16:35,600 --> 01:16:41,280
so there are people who do neuropsychology, and they're using ARC in very interesting ways. So

804
01:16:41,280 --> 01:16:46,960
there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology

805
01:16:46,960 --> 01:16:53,280
experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of

806
01:16:53,280 --> 01:16:58,640
course, you know, other than the measurement of intelligence, you are also famous for a small

807
01:16:58,640 --> 01:17:07,280
library you wrote once in a while called Keras. And I wish I wrote it, and then that was that.

808
01:17:08,640 --> 01:17:12,560
No, I yeah, it's been very much an ongoing project for the past six years.

809
01:17:13,520 --> 01:17:19,040
It was because I remember, you know, the days of TensorFlow one and and Theano,

810
01:17:19,840 --> 01:17:26,320
and things like this. And Keras was just, I think, so helpful to a lot of people, because it just

811
01:17:26,960 --> 01:17:33,360
simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to

812
01:17:33,360 --> 01:17:40,240
so many people. And now with the development of, you know, things like PyTorch and TensorFlow two,

813
01:17:40,320 --> 01:17:47,760
it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras.

814
01:17:47,760 --> 01:17:54,240
And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see

815
01:17:55,280 --> 01:18:00,720
Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going?

816
01:18:01,360 --> 01:18:06,880
Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we

817
01:18:06,880 --> 01:18:11,760
are still growing very nicely, both inside Google, like one more teams that Google are moving away

818
01:18:11,760 --> 01:18:17,040
from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there,

819
01:18:17,040 --> 01:18:23,760
and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly

820
01:18:23,760 --> 01:18:30,080
evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at

821
01:18:30,080 --> 01:18:36,240
Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very

822
01:18:36,240 --> 01:18:42,560
different and much, much bigger set of features and things you can do it. So evolving, definitely.

823
01:18:42,560 --> 01:18:49,440
And there are so several, so you, I think you asked, you know, about, yeah, like,

824
01:18:49,440 --> 01:18:53,520
Keras is getting kind of merged into TensorFlow, does it mean it's like failing away?

825
01:18:53,520 --> 01:18:59,840
So definitely not. So merging with TensorFlow was a good idea because it starts enabling

826
01:18:59,920 --> 01:19:06,480
a spectrum of workflows from the very high level, like scikit-learn like, to the very low level,

827
01:19:06,480 --> 01:19:13,760
numpy like, and everything in between. In the early days, because Keras had to interact with

828
01:19:13,760 --> 01:19:20,080
multiple backends via backend interface, it means you had this kind of like a barrier where as long

829
01:19:20,080 --> 01:19:25,520
as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy,

830
01:19:25,520 --> 01:19:31,520
very proactive, very fast. But if you wanted more customization, at some point, you would hit

831
01:19:31,520 --> 01:19:37,440
that backend barrier. And you had to reverse to TensorFlow base or piano base workflow,

832
01:19:37,440 --> 01:19:42,640
that was low level, but when, where you couldn't really leverage Keras effectively,

833
01:19:42,640 --> 01:19:47,920
by removing the backend thing and just saying the flow together in one spectrum,

834
01:19:47,920 --> 01:19:53,680
then you get really this progressive disclosure of complexity when you can start out with the

835
01:19:53,840 --> 01:19:58,880
very high level thing, but then you need to customize your training step. You have an API for

836
01:19:58,880 --> 01:20:05,520
that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high

837
01:20:05,520 --> 01:20:10,000
level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the

838
01:20:10,000 --> 01:20:15,680
level of abstraction that you want. Very, very easy high level or very, very low level full

839
01:20:15,680 --> 01:20:21,520
flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting

840
01:20:21,520 --> 01:20:27,840
type one with type two reason. Yeah, why not? I was just about to do that. At least Francois

841
01:20:27,840 --> 01:20:32,880
has great form for this, because not only does he talk about having powerful and useful interfaces

842
01:20:32,880 --> 01:20:38,160
and abstractions in deep learning, he's been playing this game in the library world for quite

843
01:20:38,160 --> 01:20:42,960
some time. But I wanted to touch on this quickly. We had a couple of people in our community asking

844
01:20:42,960 --> 01:20:50,560
you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned

845
01:20:50,560 --> 01:20:55,120
with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end

846
01:20:55,120 --> 01:21:00,000
and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end?

847
01:21:00,000 --> 01:21:03,200
We've also had a couple of questions about PyTorch as well. Is there anything on the

848
01:21:03,200 --> 01:21:07,920
roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the

849
01:21:07,920 --> 01:21:12,880
developers have really done a very, very interesting and very good job with it. And lots of people,

850
01:21:12,880 --> 01:21:18,240
I like Jax actually. So that said, adoption is not super high. I think Google is probably the

851
01:21:18,240 --> 01:21:22,880
company where it's the most adopted, where you will find the most users. And even then,

852
01:21:22,880 --> 01:21:28,640
it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project,

853
01:21:28,640 --> 01:21:35,040
it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add

854
01:21:35,040 --> 01:21:41,200
Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away

855
01:21:41,200 --> 01:21:49,680
from this like interface back end kind of model. So precisely for the reason I was describing,

856
01:21:49,680 --> 01:21:53,680
because you want to achieve this spectrum of workflows, with that, I think this cliff where

857
01:21:53,680 --> 01:21:59,280
you go, you fall from the high level down to the low level. We don't want the cliffs. We don't

858
01:21:59,280 --> 01:22:03,520
because cliffs create silos of users where you have the high level users. You want a gradient.

859
01:22:03,520 --> 01:22:09,040
Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a

860
01:22:09,040 --> 01:22:15,040
sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening

861
01:22:15,600 --> 01:22:19,920
and that will still follow the Keras API spec. It would still be the same thing,

862
01:22:21,600 --> 01:22:25,840
but on top of Jax. That said, so I would love to see something like this. This is also a very

863
01:22:25,840 --> 01:22:31,120
low priority for us because we have the actual current Keras, which I wish we need to work on,

864
01:22:31,120 --> 01:22:35,840
which has lots of users. So we don't really have time to do this. But in theory, would it be cool?

865
01:22:35,840 --> 01:22:40,400
Yeah, sure. I would love to see something like this. So if I had tons of free time, I would

866
01:22:40,400 --> 01:22:43,760
probably build it, but in practice, I don't. Fantastic. Well, we've got another question

867
01:22:43,760 --> 01:22:48,400
from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth

868
01:22:48,400 --> 01:22:52,640
of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives

869
01:22:52,640 --> 01:23:00,400
and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book.

870
01:23:01,120 --> 01:23:06,560
Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me.

871
01:23:06,560 --> 01:23:14,560
And he talks a lot about objectives leading to deception. So sometimes following an objective

872
01:23:14,560 --> 01:23:19,520
monotonically sends you in the wrong direction. And his solution to that is either quality,

873
01:23:19,520 --> 01:23:24,000
diversity, or more recently, open-endedness, which is that if you have an infinitude of

874
01:23:24,000 --> 01:23:29,280
objectives, in a sense, the system has no objective. And you can also with diversity,

875
01:23:29,280 --> 01:23:33,520
preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the

876
01:23:33,520 --> 01:23:38,480
poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes

877
01:23:38,480 --> 01:23:44,720
to your description of the problem's objectives, I completely agree that one thing I mentioned

878
01:23:44,720 --> 01:23:51,280
in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one

879
01:23:51,280 --> 01:23:55,680
objective, you're going to achieve it. But the thing is, you're going to take every shortcut

880
01:23:55,680 --> 01:23:59,920
along the way for things that we are not actually incorporated in your objective.

881
01:23:59,920 --> 01:24:04,560
And this leads to systems that are not actually doing what you wanted them to do. Like for instance,

882
01:24:04,560 --> 01:24:10,880
we built chess playing systems, because we hoped that a system that could play chess would have to

883
01:24:10,880 --> 01:24:16,640
be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays

884
01:24:16,640 --> 01:24:23,200
chess. That's what it does. The same is true with challenges and Kaggle. The winning systems,

885
01:24:23,200 --> 01:24:28,160
they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the

886
01:24:28,160 --> 01:24:34,320
expense of everything else that you might care about the system. Like, is the code base readable?

887
01:24:34,320 --> 01:24:39,520
No. Is it computationally efficient? No, it's actually terrible. You could never put it in

888
01:24:39,520 --> 01:24:44,080
production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for

889
01:24:44,080 --> 01:24:49,280
something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth

890
01:24:49,280 --> 01:24:53,440
says as well. I love what you said about shortcuts. You said in your New York's presentation that if

891
01:24:53,440 --> 01:24:58,320
you optimize for a specific metric, then you'll take shortcuts on every other dimension, not

892
01:24:58,320 --> 01:25:02,800
captured by your metric. And you said in a machine learning context, it's similar to overfitting,

893
01:25:02,800 --> 01:25:07,760
right? Because on task specific skills, you actually lose generalization if you get good at

894
01:25:07,760 --> 01:25:12,080
a particular task. So it's completely orthogonal to what you want. I know you're very well known

895
01:25:12,080 --> 01:25:16,640
for your skepticism of the intelligence explosion. And what I love about your conception of

896
01:25:16,640 --> 01:25:21,840
intelligence is that you think of it as a system or as a process, you say that intelligence is

897
01:25:21,840 --> 01:25:28,240
embodied, right? So you have a brain in a body acting in an environment. And in that context,

898
01:25:28,240 --> 01:25:32,880
it makes sense that you would think that there are environmental kind of rate limiting steps to

899
01:25:32,880 --> 01:25:37,920
any kind of super intelligence, right? But I spoke to someone the other day who is of the other

900
01:25:37,920 --> 01:25:42,880
persuasion, shall we say, and this person was saying, Well, what if you had a super, super

901
01:25:42,880 --> 01:25:49,120
smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a

902
01:25:49,120 --> 01:25:55,600
scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out

903
01:25:55,600 --> 01:26:01,040
that IQ only helps up to about 125. And then it stops helping you. But these people would say,

904
01:26:01,040 --> 01:26:05,280
Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is

905
01:26:05,280 --> 01:26:09,360
just making better decisions, they would consistently make better decisions and science

906
01:26:09,360 --> 01:26:14,720
would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a

907
01:26:14,720 --> 01:26:18,480
super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into

908
01:26:18,480 --> 01:26:22,960
the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you

909
01:26:22,960 --> 01:26:27,440
know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent

910
01:26:27,440 --> 01:26:32,400
in human terms, that would in fact accelerate science. But it would not really like accelerate

911
01:26:32,400 --> 01:26:39,200
science in a linear fashion and very much not in an exponential fashion. So I guess the main

912
01:26:39,360 --> 01:26:46,080
conceptual differences I have with these folks is that they tend to credit everything humans can do

913
01:26:46,080 --> 01:26:52,000
to the human brain. And they have this vision of intelligence as you know, a brain in a jar

914
01:26:52,000 --> 01:26:55,920
kind of thing. And if you tweak the brain, it gets more intelligent and intelligence

915
01:26:56,560 --> 01:27:02,080
is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can

916
01:27:02,080 --> 01:27:07,040
do more things, you can solve more problems and so on. And in particular, you can build a better

917
01:27:07,120 --> 01:27:12,560
brain. And by the way, there is not really any practical evidence that this is true. But

918
01:27:12,560 --> 01:27:18,160
I view intelligence here more as this holistic thing that okay, you have the brain, but actually

919
01:27:18,160 --> 01:27:24,640
the brain is in a body which gives it access to a certain set of actions it can do and set

920
01:27:24,640 --> 01:27:30,320
up a perception primitives. And this body is an environment which gives it access to a set of

921
01:27:30,320 --> 01:27:38,160
experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just,

922
01:27:38,160 --> 01:27:44,400
it's not so much a problem solving algorithm, like a problem center descending, as it is a

923
01:27:44,400 --> 01:27:50,080
big sport. And you put it in an environment to absorb experiences from that environment. And

924
01:27:51,280 --> 01:27:56,000
one thing that's super important to understand if you're on issue, if you really think deeply about

925
01:27:56,080 --> 01:28:02,880
intelligence, is that most of our expressed intelligence does not come from here, it is

926
01:28:02,880 --> 01:28:06,800
externalized intelligence. So externalized intelligence can be can be many things.

927
01:28:08,480 --> 01:28:14,400
If I look up something online, that's externalized intelligence, Google is part of my brain. If I

928
01:28:14,400 --> 01:28:20,240
write a Python script to test some idea, that's externalized intelligence, my laptop is part of

929
01:28:20,240 --> 01:28:27,600
my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition

930
01:28:27,600 --> 01:28:35,360
is crystallized, the crystallized output of someone, someone else's thinking. And the process

931
01:28:35,360 --> 01:28:40,880
through which we get access to all these accumulated outputs of people's thinking is civilization,

932
01:28:40,880 --> 01:28:50,720
right? And like 99% of the things you think are the behaviors you act, the behaviors you execute,

933
01:28:50,720 --> 01:28:57,840
you did not invent them. You did not solve the underlying problem yourself. You're just copying

934
01:28:58,880 --> 01:29:04,400
a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands

935
01:29:04,400 --> 01:29:09,840
after you went outside. And that's a very smart behavior. But did you invent it? Did you come

936
01:29:09,840 --> 01:29:15,680
up with that? No, actually, other people came up with that. You did not also come up with the

937
01:29:15,680 --> 01:29:20,480
infrastructure that enables you to do it in the first place. And so, and this is true, you know,

938
01:29:20,480 --> 01:29:27,920
for even the most intimate of your thoughts, you're thinking with words that you did not invent,

939
01:29:27,920 --> 01:29:34,400
you're thinking with concepts that you did not invent or that you did not derive from your own

940
01:29:34,480 --> 01:29:41,520
experience. They really come from other people, from this accumulation of past generations.

941
01:29:41,520 --> 01:29:49,040
And if you want to enhance the expressed intelligence of people, then this is actually the

942
01:29:49,040 --> 01:29:54,880
system you need to tweak and improve, not the human brain, but civilization, right?

943
01:29:54,880 --> 01:30:00,880
In a way, that seems like a contradiction, because you're talking about the externalization of knowledge,

944
01:30:01,440 --> 01:30:07,760
not intelligence. So by your own definition, isn't that the opposite of intelligence?

945
01:30:08,400 --> 01:30:14,000
That's a great point. So I'm relating expressed intelligence. So I was specifically saying

946
01:30:14,000 --> 01:30:18,000
expressed intelligence as opposed to fluid intelligence. And what expressed intelligence

947
01:30:18,720 --> 01:30:22,720
means in this context is something very different from what we talk about in the measure of

948
01:30:22,720 --> 01:30:28,640
intelligence. It means intelligence behavior. And in particular, I think the ability to solve

949
01:30:28,720 --> 01:30:33,120
problems that you encounter as an individual. Typically, when you solve a problem as an

950
01:30:33,120 --> 01:30:38,640
individual, you're actually using a solution you found somewhere else. There are not that many

951
01:30:38,640 --> 01:30:45,040
problems that as an individual, you solve from scratch in your own lifetime. But here's the

952
01:30:45,040 --> 01:30:50,400
thing is that if you're able to actually solve something novel yourself, you have the ability

953
01:30:50,400 --> 01:30:55,120
to write about it, you have the ability to communicate it, and then the next generation can

954
01:30:55,120 --> 01:31:02,560
benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're

955
01:31:02,560 --> 01:31:08,160
reading a novel about, I don't know, a kind of planet of the apes or something, which was a

956
01:31:08,160 --> 01:31:16,240
planet that had a life form similar to ours, but with a significantly lower IQ. And a human being

957
01:31:16,240 --> 01:31:21,360
shows up there one day, and these things start writing about this, hey, this weird alien just

958
01:31:21,360 --> 01:31:26,480
showed up here, and we captured it, we ran some tests on it, and we figured out it's really

959
01:31:26,480 --> 01:31:32,720
intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen

960
01:31:32,720 --> 01:31:39,040
when 100 of them show up instead of just this initial explorer. And some other of these guys

961
01:31:39,040 --> 01:31:44,320
were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what

962
01:31:44,320 --> 01:31:50,480
they are is kind of outside of their brain. So I'm not really worried about it. We would be

963
01:31:50,480 --> 01:31:56,640
reading that with trepidation, right, because we know that when this more intelligent species

964
01:31:56,640 --> 01:32:01,920
with more fluid intelligence, more externalized intelligence, better technology, all this kind

965
01:32:01,920 --> 01:32:07,040
of stuff shows up, those guys are going to get wiped out. And it's actually happened like many

966
01:32:07,040 --> 01:32:12,640
times throughout human history, not that humans were more fluid intelligence showed up and killed

967
01:32:12,640 --> 01:32:18,560
off, you know, other people, but humans that had more externalized intelligence or more, you know,

968
01:32:18,640 --> 01:32:22,000
represented intelligence and technology certainly showed up and dominated.

969
01:32:22,000 --> 01:32:27,120
Absolutely. You're saying it yourself that when it has happened in history, it was not

970
01:32:27,120 --> 01:32:34,160
fundamentally about one people having smarter brains, but one people having higher technology.

971
01:32:34,160 --> 01:32:38,800
But that that is not something that is attributable to intelligence itself, right?

972
01:32:38,800 --> 01:32:42,320
There's a connection there. If you did have a group of species or whatever,

973
01:32:42,320 --> 01:32:47,440
that was much more intelligent, they will have advanced technologically much faster and further

974
01:32:47,520 --> 01:32:50,720
in any given amount of time, all else being equal, right?

975
01:32:51,520 --> 01:32:56,960
It depends on many factors. And that's kind of my point is that is your brain a factor? Yes,

976
01:32:57,600 --> 01:33:02,480
absolutely, it is. But there are other factors like we are just talking about the development

977
01:33:02,480 --> 01:33:08,880
of technology. So in that case, the critical factor was not the brain, but the superstructure

978
01:33:08,880 --> 01:33:13,520
in particular communication and environmental constraints around it. The direction in which

979
01:33:13,840 --> 01:33:20,720
civilization develops is a direct function of the specific challenges it encounters that

980
01:33:20,720 --> 01:33:25,280
come from its environment, that comes from its surrounding enemies, and so on. And

981
01:33:25,920 --> 01:33:32,720
technological development advances the fastest when you have a civilization that are dealing with

982
01:33:32,720 --> 01:33:37,040
very harsh challenges, but that are not quite fortunate to work them out.

983
01:33:38,000 --> 01:33:44,400
Because that's what forces them to develop as fast as it can survive. So this is actually a

984
01:33:44,400 --> 01:33:49,680
very good example where the critical factor was the superstructure that guided the development

985
01:33:49,680 --> 01:33:54,000
civilization was not actually the brain. But of course, yeah, if a one is smaller,

986
01:33:54,000 --> 01:34:01,520
then civilization will advance faster. But my point is that there are many factors and that

987
01:34:01,600 --> 01:34:07,200
by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately

988
01:34:07,200 --> 01:34:14,240
some other factor will be the bottleneck. There are civilizations that have not actually advanced

989
01:34:14,240 --> 01:34:20,560
very much at all because they simply did not face any changes. And did they have worse brains? No,

990
01:34:20,560 --> 01:34:25,680
actually, they had exactly the same brain. But somehow the outcome was different because

991
01:34:25,680 --> 01:34:31,920
something else, then the brain turned out to be the bottleneck like lack of environmental change.

992
01:34:32,960 --> 01:34:38,080
I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and

993
01:34:38,080 --> 01:34:42,880
when you have role fragmentation and lots of different businesses and lots of different

994
01:34:42,880 --> 01:34:48,720
organization or structures, some people might decide to structure themselves based on data

995
01:34:48,720 --> 01:34:54,480
domain or based on organization or based on something else. And you can think of it topologically.

996
01:34:54,480 --> 01:35:00,160
And I think human society is very similar to this. And I'm not sure whether evolution

997
01:35:00,160 --> 01:35:05,200
would lead itself to one particular topology. But the environmental structures and the ways

998
01:35:05,200 --> 01:35:11,680
that we organize ourselves can create incredible bottlenecks. And that seems to be where the real

999
01:35:11,680 --> 01:35:17,120
interesting stuff goes on rather than the individuals. And I think you would agree with that,

1000
01:35:17,120 --> 01:35:22,800
Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ

1001
01:35:22,800 --> 01:35:28,320
is like 15 points higher, but it has a terrible organizational structure and terrible incentives

1002
01:35:28,320 --> 01:35:34,160
and the promo process is super broken or something. And that company is actually going to perform worse

1003
01:35:34,160 --> 01:35:39,360
than the more progressive innovation encouraging company that has a very nice organizational

1004
01:35:39,360 --> 01:35:45,280
structure and where people are actually more mediocre. Maybe they have on average 15 points

1005
01:35:45,280 --> 01:35:50,000
less in IQ, but they're actually going to do a better job because they have the better superstructure.

1006
01:35:50,640 --> 01:35:55,680
Yeah, it's fascinating that the problem is in most corporations, you can't actually design the

1007
01:35:55,680 --> 01:36:01,600
information architecture to be more efficient, because everything is so decentralized and

1008
01:36:01,600 --> 01:36:06,720
fractionated, you can only do it in pockets. And if you try and fix something in one part

1009
01:36:06,720 --> 01:36:10,160
of the organization, everyone else will say, well, my requirements are different. I'm not going to

1010
01:36:10,160 --> 01:36:14,240
wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing

1011
01:36:14,240 --> 01:36:20,640
to do well. To sum up the whole like intelligence explosion thing, the point is really that it's

1012
01:36:20,640 --> 01:36:26,000
a system you have to look at holistically to get it holistically. And just by tweaking one factor,

1013
01:36:26,000 --> 01:36:31,280
which is the intelligence of an individual human brain, then what this means is this factor starts

1014
01:36:31,280 --> 01:36:35,680
being the bottleneck. But that means some other factor in the system, because there's an infinite

1015
01:36:35,680 --> 01:36:40,480
factor that will become the bottleneck. And by just focusing on one factor, you're not going to

1016
01:36:40,480 --> 01:36:46,000
actually lift all the votes. Yeah, and I actually agree with you. However,

1017
01:36:46,960 --> 01:36:51,040
I do want to say, I think we just don't know. I think both sides of the intelligence,

1018
01:36:51,040 --> 01:36:57,360
quote unquote, explosion really can't say for certain that it will or will not pose a mortal

1019
01:36:57,360 --> 01:37:02,160
threat to humanity. I think we have to accept that it's at least a risk factor. And we have

1020
01:37:02,160 --> 01:37:09,280
to be very careful about, in the future, when we start embodying, if we find general intelligence,

1021
01:37:09,280 --> 01:37:13,760
we need to be cautious. If we come up with something that looks like general intelligence,

1022
01:37:13,760 --> 01:37:19,600
there is absolutely some risk potential around it. However, I've never seen anything coming

1023
01:37:19,600 --> 01:37:25,520
anywhere close to that. In fact, the systems that we have today, they fit your almost no

1024
01:37:25,520 --> 01:37:29,920
intelligence whatsoever. So I think it's a bit early to start banning them.

1025
01:37:29,920 --> 01:37:33,840
And even if we get into that conversation, I think Francois would say that intelligence

1026
01:37:33,840 --> 01:37:37,360
must be specialized, right, because of the no free lunch theorems.

1027
01:37:37,360 --> 01:37:42,560
If you define intelligence as your ability to solve problems, then yeah, it's going to be

1028
01:37:42,560 --> 01:37:49,680
specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch

1029
01:37:49,680 --> 01:37:55,200
theorem is saying is basically, if you want to learn something from data, you have to make assumptions

1030
01:37:55,200 --> 01:38:00,000
about it. Which is why you know a convent, for instance, is a great fit for image data. It's

1031
01:38:00,000 --> 01:38:04,880
not really a great fit for natural language processing. And because it makes different

1032
01:38:04,880 --> 01:38:09,440
assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly

1033
01:38:09,440 --> 01:38:14,560
certain that whatever the first AGI that gets created, it's going to be highly specialized

1034
01:38:14,560 --> 01:38:19,920
for killing other people, because it's going to be a military, you know, secret project,

1035
01:38:19,920 --> 01:38:26,480
probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't

1036
01:38:26,480 --> 01:38:34,640
have anything coming close to AGI. It's probably going to be actually a system that just displays

1037
01:38:34,640 --> 01:38:41,200
you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the

1038
01:38:41,200 --> 01:38:46,960
first AGI is probably just going to like write, not only display, but write the perfect ad for

1039
01:38:46,960 --> 01:38:52,800
you on the fly. You know, it knows what you ate and you know, I know you're joking with

1040
01:38:52,960 --> 01:38:58,240
actually think on the, on the more serious, I think that's highly unlikely because of the

1041
01:38:58,240 --> 01:39:03,920
short code of the story because of the short patrol. I don't think a general intelligence is

1042
01:39:03,920 --> 01:39:08,720
going to be created by the military is not going to be created by a system that's trying to show

1043
01:39:08,720 --> 01:39:14,640
you ads because these are specific goals. And so if you try to optimize those specific goals,

1044
01:39:14,640 --> 01:39:19,920
you're going to end up with a very specialized system in order to build a general intelligence,

1045
01:39:20,000 --> 01:39:27,360
you need to be optimizing for generality itself. So it's going to come from, if it comes from the

1046
01:39:27,360 --> 01:39:31,360
applied, either it's going to come from the academic side, where you have researchers who are

1047
01:39:31,360 --> 01:39:35,920
actually optimizing for generality itself, who said generality as they are going. Or if it's

1048
01:39:35,920 --> 01:39:40,240
come from the applied side, it's going to come from people who have problems where they have to

1049
01:39:40,240 --> 01:39:45,760
deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads,

1050
01:39:45,760 --> 01:39:48,800
it's not going to be the military. I don't know where this is going to be.

1051
01:39:48,880 --> 01:39:54,240
One of the things that interested me about Kenneth Stanley was that he says the reason we

1052
01:39:54,240 --> 01:39:59,280
can't monotonically optimize on objectives is because of deception, which means sometimes you

1053
01:39:59,280 --> 01:40:04,160
need to get a lot worse before you get better. His original conception was quality diversity,

1054
01:40:04,160 --> 01:40:08,960
which basically means if you optimize for novelty, that's something that you can optimize on

1055
01:40:08,960 --> 01:40:14,480
monotonically. And also, if you look at evolution, where there is a cacophony of problems and

1056
01:40:14,480 --> 01:40:21,120
solutions divergently being generated, then as an information accumulator, you can optimize

1057
01:40:21,120 --> 01:40:26,320
on that monotonically. And your conception of intelligence is generality. And that also appears

1058
01:40:26,320 --> 01:40:31,360
to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite

1059
01:40:31,360 --> 01:40:36,320
interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show.

1060
01:40:36,320 --> 01:40:40,160
Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you.

1061
01:40:40,240 --> 01:40:44,960
Thanks for having me on the podcast. It's really my pleasure. This was super fun.

1062
01:40:44,960 --> 01:40:48,640
Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful.

1063
01:40:48,640 --> 01:40:52,080
We're going to jump straight into the post-show analysis.

1064
01:40:52,080 --> 01:40:56,000
Okay, well, I'm going to mention you did really well, Tim, that trickle sweat

1065
01:40:56,000 --> 01:41:01,200
that this was running down your face the whole time. Not very noticeable. So I think you can

1066
01:41:01,200 --> 01:41:07,120
relax. That was fun. I think it went pretty well. Yeah, it was a dream come true.

1067
01:41:07,680 --> 01:41:13,760
I was actually I was very pleasantly kind of interested in how he he framed, you know,

1068
01:41:13,760 --> 01:41:17,840
the measure of intelligence paper like, look, it's not really about the measure per se. It's just

1069
01:41:17,840 --> 01:41:25,440
that this is this is a cognitive framework, a cognitive tool for thinking about where to go

1070
01:41:25,440 --> 01:41:31,840
and a guidepost for building more generalizable or more general intelligences say like that,

1071
01:41:31,840 --> 01:41:36,800
I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like,

1072
01:41:36,800 --> 01:41:40,320
here's a framework to help us think more in the direction we need to be thinking.

1073
01:41:40,880 --> 01:41:48,160
Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because

1074
01:41:48,880 --> 01:41:55,360
you know, he self admits that it's flawed, right? Because he like, he makes the tasks.

1075
01:41:55,360 --> 01:42:00,960
And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks

1076
01:42:01,040 --> 01:42:07,600
he makes, you know, in the public set, you would think that not someone will come up with an

1077
01:42:07,600 --> 01:42:13,520
intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that

1078
01:42:13,520 --> 01:42:20,320
sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many

1079
01:42:20,320 --> 01:42:29,200
people investigating it. Or whether it's really actually a hard problem. And if it is a problem,

1080
01:42:29,200 --> 01:42:34,720
you know, well, it's fascinating too, because if he if he achieves what he wanted, which was

1081
01:42:34,720 --> 01:42:38,720
getting it more outsourced, right, like getting all the intelligent people all around the world

1082
01:42:38,720 --> 01:42:45,440
contributing to arc problems and refining them over time, I think actually that community project

1083
01:42:45,440 --> 01:42:50,720
would help the core knowledge people in that line of research and figuring out, okay, what,

1084
01:42:50,720 --> 01:42:55,840
what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these

1085
01:42:55,840 --> 01:43:00,480
prime thoughts, because we would, we would play these brain teasers all the time. And we realized

1086
01:43:00,480 --> 01:43:05,840
that there were patterns, right? Like, well, this brain teaser requires the concept of coloring,

1087
01:43:05,840 --> 01:43:09,600
like with a red black tree, where you add an additional variable that kind of lets you

1088
01:43:10,240 --> 01:43:15,600
solve the problem. And if we could really have a nice catalog of, here's all the core knowledge,

1089
01:43:15,600 --> 01:43:20,560
here's all the like problem solving techniques, I think that would be really powerful. I mean,

1090
01:43:20,560 --> 01:43:25,440
well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about

1091
01:43:25,440 --> 01:43:30,240
six core knowledge systems, right? And that and the arc challenge uses four of them. So

1092
01:43:30,240 --> 01:43:36,000
objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three,

1093
01:43:36,000 --> 01:43:40,800
numbers, counting, quantitative comparisons. So the two that weren't in there are places

1094
01:43:40,800 --> 01:43:44,640
and social partners. Now, the thing is, I think we may discover new ones.

1095
01:43:45,200 --> 01:43:50,480
Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it,

1096
01:43:50,720 --> 01:43:56,000
imagine if you just guessed the classification on ImageNet when you've got 1000 classes,

1097
01:43:56,000 --> 01:44:01,920
20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc,

1098
01:44:01,920 --> 01:44:07,440
right? And what's interesting as well is that all of those different tasks that have been created

1099
01:44:07,440 --> 01:44:13,760
by Francois, they all tie back to just four priors, right? Which means, I don't know whether

1100
01:44:13,760 --> 01:44:20,400
it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM.

1101
01:44:20,400 --> 01:44:24,880
Yeah, there's, there's two things. So first, I would have thought that if someone,

1102
01:44:24,880 --> 01:44:29,840
if someone came up with something that solves more than 5%, it's going to be like immediately at

1103
01:44:29,840 --> 01:44:35,600
95%. Like just because they've sort of cracked the problem. And then, you know, there might be

1104
01:44:35,600 --> 01:44:40,880
a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct

1105
01:44:40,880 --> 01:44:46,320
solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising.

1106
01:44:46,320 --> 01:44:53,760
And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I

1107
01:44:53,760 --> 01:45:00,320
do think is that the space of these priors is still way too large. Like, so if you just think

1108
01:45:00,320 --> 01:45:08,720
about something like object, because in, in these arc tasks, there are, I feel so many more priors

1109
01:45:08,800 --> 01:45:15,280
than just the core knowledge things. Because so one of them is like, you have the, you have like

1110
01:45:16,240 --> 01:45:20,960
this thing, and then you have this thing. And the solution is like, it goes, right? It

1111
01:45:20,960 --> 01:45:26,160
could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize

1112
01:45:26,160 --> 01:45:32,000
like this is a wall or something, but there is no, there's no, no prior to says like a wall

1113
01:45:32,000 --> 01:45:38,480
needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact

1114
01:45:38,480 --> 01:45:44,960
that this is much more core knowledge, right? Like in, you know, we build stuff out of straight

1115
01:45:44,960 --> 01:45:49,360
walls. And I think, I think I agree with you, which is I think, I think what you're getting at,

1116
01:45:49,360 --> 01:45:53,360
correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified

1117
01:45:53,360 --> 01:45:59,040
right now is vague, right? There's a vagueness to it. And I think if we actually start to try and

1118
01:45:59,040 --> 01:46:04,720
codify that more and some type of a mathematical language, Tim, I think it's going to expand

1119
01:46:04,720 --> 01:46:09,520
like the scope of that, we're going to end up with more core knowledge concepts really than,

1120
01:46:09,520 --> 01:46:12,880
than just six, we'll need to make them finer grained. And I'm really excited,

1121
01:46:13,440 --> 01:46:18,320
you know, to see that develop because this has been for me a long wonder, right, which is

1122
01:46:18,960 --> 01:46:25,760
what are the in, in a rigorously defined way? What are these core concepts, these core bits

1123
01:46:25,760 --> 01:46:30,880
of knowledge that make human cognition so powerful? Yeah. And there's also,

1124
01:46:31,440 --> 01:46:36,400
because Yannick made the point about brittleness, right, even in topological space, you still have

1125
01:46:36,400 --> 01:46:42,880
brittleness, but, but the solution was to create powerful abstractions, right? But how would that

1126
01:46:42,880 --> 01:46:48,480
work with the priors? Because if you think about it, you can recombine many of the priors to come

1127
01:46:48,480 --> 01:46:52,720
up with powerful abstractions. And you might find that it doesn't actually filter down to, to that

1128
01:46:52,720 --> 01:46:57,440
many. But the question is, how many things are there? Remember when we spoke to Walid Saber,

1129
01:46:57,440 --> 01:47:01,760
and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them

1130
01:47:01,760 --> 01:47:06,800
to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained

1131
01:47:06,800 --> 01:47:10,720
concepts are more important. I think probably stems from a lot of the computer science

1132
01:47:11,840 --> 01:47:17,040
education that I had where, where when we were devising algorithms to do one thing or another,

1133
01:47:17,040 --> 01:47:23,280
you get these little hints that kind of like clever bits of core knowledge that was used to

1134
01:47:23,280 --> 01:47:27,440
solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going

1135
01:47:27,440 --> 01:47:31,840
to randomly choose an element. Well, random selection is kind of a bit of core knowledge.

1136
01:47:31,840 --> 01:47:35,840
And then I'm just going to partition by that, and then repeat, you know, or things like,

1137
01:47:35,840 --> 01:47:40,000
I don't know how to balance this tree the way it is. But if I color stuff, like add in red,

1138
01:47:40,000 --> 01:47:45,040
black nodes, I can now overlay a computation that, you know, so there's all these little bits,

1139
01:47:45,040 --> 01:47:49,600
you know, that's what's fascinating about computer programming is it, is it really strikes at the

1140
01:47:49,600 --> 01:47:54,640
heart of this cognition and this core knowledge and how to read, and you have to do it rigorously,

1141
01:47:54,640 --> 01:47:59,120
right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to

1142
01:47:59,120 --> 01:48:04,560
define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a

1143
01:48:04,560 --> 01:48:11,600
bit amazed by people who have just kind of sort of learned programming, because it's, it's almost

1144
01:48:11,600 --> 01:48:17,520
like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve

1145
01:48:17,520 --> 01:48:23,680
this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not

1146
01:48:23,680 --> 01:48:29,120
fully. Yeah. But then on the other side of the coin to that. So when I was working in,

1147
01:48:30,800 --> 01:48:35,520
you know, quantitative trading, right, we had these these massive globally integrated,

1148
01:48:35,520 --> 01:48:41,440
automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks,

1149
01:48:41,440 --> 01:48:48,640
but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually

1150
01:48:48,640 --> 01:48:53,280
work in reality. You know, you sit there and you look at them and go, when I first went in there,

1151
01:48:53,280 --> 01:48:57,200
as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to

1152
01:48:57,200 --> 01:49:01,920
figure out some continuous equation that, you know, fits this piecewise linear thing, and it's

1153
01:49:01,920 --> 01:49:06,960
going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better.

1154
01:49:06,960 --> 01:49:12,240
It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're

1155
01:49:12,240 --> 01:49:16,800
like, Oh, that's, well, there should be some continuous like thing in there. Like all these

1156
01:49:16,800 --> 01:49:22,880
weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete

1157
01:49:23,440 --> 01:49:27,520
work. And, and that's weird. It was weird to me and still weird to me.

1158
01:49:28,080 --> 01:49:35,040
Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay.

1159
01:49:35,040 --> 01:49:42,480
So one, intelligence is generalization. I think that's super powerful to his idea that deep learning

1160
01:49:42,480 --> 01:49:49,200
is really good for value centric abstraction. And because of the manifold hypothesis, lots of

1161
01:49:49,200 --> 01:49:55,200
natural data has some kind of manifold, which you can interpolate on, but lots of discrete

1162
01:49:55,200 --> 01:50:00,880
problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just

1163
01:50:00,880 --> 01:50:05,520
use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But

1164
01:50:05,520 --> 01:50:09,360
he's saying that it doesn't exist for discrete problems. The manifold might be there or it might

1165
01:50:09,360 --> 01:50:13,840
only be there in parts. So that was interesting. And then the third thing that fascinated me about

1166
01:50:13,840 --> 01:50:18,880
Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be

1167
01:50:18,880 --> 01:50:24,160
thinking about individual brains, we should be thinking about the externalization of knowledge.

1168
01:50:24,800 --> 01:50:31,760
Yeah. And the way he described this, what he thinks like a hybrid system should look like,

1169
01:50:31,760 --> 01:50:39,040
which is sort of you have a perception layer and then a discrete search layer. And then on top of

1170
01:50:39,040 --> 01:50:46,400
that kind of another fuzzy layer that guides the search that can be deep learning again. And I

1171
01:50:46,400 --> 01:50:52,960
think we're like halfway there on the top with the top very much looks like alpha zero, right,

1172
01:50:52,960 --> 01:50:58,720
which is kind of a discrete search that is guided by a neural networks. And the bottom

1173
01:50:58,720 --> 01:51:04,960
layer we have to because that's just our, you know, regular neural networks. I think we have

1174
01:51:04,960 --> 01:51:11,760
big trouble in how to connect the two in a in a single unified way such that we can learn them,

1175
01:51:11,760 --> 01:51:18,640
right? Because the best we can do right now is is right, we can, we can plug a pre train network

1176
01:51:19,600 --> 01:51:24,640
onto alpha zero or something like this, but we can't really, we don't really have it figured

1177
01:51:24,640 --> 01:51:29,360
out yet how to connect the all the stuff. A good example of that is the neural Turing machines,

1178
01:51:29,360 --> 01:51:34,720
like how it's so hard to to optimize them, right? And I think not only do we need these kind of

1179
01:51:34,720 --> 01:51:40,720
three components that that nicely integrate and are optimal, we have to be able to modularize

1180
01:51:40,720 --> 01:51:47,360
and componentize and connect multiple instances of those things together. And some, you know,

1181
01:51:47,360 --> 01:51:52,320
weird topological network to really achieve like kind of the capsule network kind of vision

1182
01:51:52,320 --> 01:51:57,840
where each of the capsules is maybe one of these units. And then they're part of it's like a fractal,

1183
01:51:57,840 --> 01:52:02,640
you know, kind of these fractal layers of those pieces. I don't know whether I was

1184
01:52:02,640 --> 01:52:08,880
misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has

1185
01:52:08,880 --> 01:52:13,840
been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning

1186
01:52:13,840 --> 01:52:18,880
models and the way you search is quite opinionated. What you're always talking about is have a very

1187
01:52:18,880 --> 01:52:26,080
basic DSL and in that topological space, you just search and you start to modularize and you start

1188
01:52:26,080 --> 01:52:29,920
to create functions and abstractions. And you have from a software engineering point of view,

1189
01:52:29,920 --> 01:52:35,760
you start to build a library of functions that have been written in code that do certain things,

1190
01:52:35,760 --> 01:52:39,040
right? And that's that's different, isn't it to alpha zero?

1191
01:52:40,000 --> 01:52:47,200
Well, the alpha zero is made specifically to search over actions in some kind of RL space.

1192
01:52:48,000 --> 01:52:55,360
Yeah, I mean, what he describes is certainly much more abstract in that you search over applications

1193
01:52:55,360 --> 01:53:04,160
of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by

1194
01:53:04,160 --> 01:53:10,080
these lower level neural networks. But I mean, in S, I just, that just came to my mind when he

1195
01:53:10,080 --> 01:53:15,840
described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because

1196
01:53:15,840 --> 01:53:22,480
that's essentially neural network guided search is something we, we already, already do though.

1197
01:53:23,280 --> 01:53:30,880
Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because

1198
01:53:31,600 --> 01:53:37,520
what you do as a, as a human, there's also some part of hierarchical system to it,

1199
01:53:37,520 --> 01:53:43,920
in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like,

1200
01:53:43,920 --> 01:53:50,000
okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the

1201
01:53:50,000 --> 01:53:57,760
search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub

1202
01:53:57,760 --> 01:54:04,800
problem. And there is also, you can do it at will too, by the way, like you can, you can scan an

1203
01:54:04,800 --> 01:54:09,600
image, and you get this type one that sort of finds a bunch of objects, and then you do this type two

1204
01:54:09,600 --> 01:54:13,920
thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let

1205
01:54:13,920 --> 01:54:19,200
me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark

1206
01:54:19,200 --> 01:54:25,120
is objects and bugs and reason about so you have this ability to transcend the process and tune it

1207
01:54:25,120 --> 01:54:31,200
and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's

1208
01:54:31,200 --> 01:54:37,280
even like, apart from intelligence, you have the ability to, to introspect the whole thing.

1209
01:54:38,000 --> 01:54:44,160
And that probably is a big part of intelligence. I mean, I guess you could have intelligence

1210
01:54:44,160 --> 01:54:49,360
without consciousness, but you know, there is an argument to be made that the fact that you can

1211
01:54:49,360 --> 01:54:55,040
introspect your own processes contributes in big part to the furthering of intelligence.

1212
01:54:56,400 --> 01:55:02,800
Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on

1213
01:55:02,800 --> 01:55:07,520
his newest presentation was when he said intelligence is literally sensitivity to abstract

1214
01:55:07,520 --> 01:55:13,200
analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that

1215
01:55:13,200 --> 01:55:19,440
there is so much repetition in the universe. Right, but it's repetition in this funny way

1216
01:55:19,440 --> 01:55:25,920
where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies,

1217
01:55:25,920 --> 01:55:31,440
kind of resembles, you know, but, but then there are these little weird differences, these asymmetries,

1218
01:55:31,440 --> 01:55:36,800
and you know, like the universe is a fascinating place. And I don't know, something, yeah.

1219
01:55:36,800 --> 01:55:41,840
Right, that's not what when you say you have to make analogies, which is I can, I can absolutely

1220
01:55:41,840 --> 01:55:46,320
see, you know, this and me, I think my question was formulated a bit dumb where I said, you know,

1221
01:55:46,320 --> 01:55:53,440
if the line is squiggly, what I more meant is that, you know, in that case, it's not a line,

1222
01:55:53,440 --> 01:55:58,640
it's a squiggly line. And the same with the social situations, you know, that is like, okay,

1223
01:55:58,640 --> 01:56:03,280
that that person over there kind of doesn't like me. But then in the next social situation, it's

1224
01:56:03,280 --> 01:56:09,520
kind of a person that doesn't like you and has a gun, or something like this. I almost feel like

1225
01:56:09,600 --> 01:56:15,360
or a group of people that you don't consider as a single single sure they are similar in some way,

1226
01:56:15,360 --> 01:56:20,960
but it's never the exact same thing. So this reasoning by analogy does work, but you always

1227
01:56:20,960 --> 01:56:26,560
do your little modifications on top specific to the situation. And I'm sure there there's a place

1228
01:56:26,560 --> 01:56:33,280
in his framework for this, but it's it's just, again, it's it's like a lot more complex than

1229
01:56:33,280 --> 01:56:38,240
yeah. I think that's what he call I think that's abstraction, at least that, you know, that with

1230
01:56:38,240 --> 01:56:43,280
prior to today, my my concept of abstraction was similar to that, which it's removing the

1231
01:56:43,280 --> 01:56:48,640
insignificant details. So you're able you're you're able to take whatever, you know, some,

1232
01:56:48,640 --> 01:56:53,680
you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't

1233
01:56:53,680 --> 01:56:58,800
matter for whatever your purpose is, that's abstraction. And, you know, I think one of

1234
01:56:58,800 --> 01:57:03,920
the weird things is that, and this is kind of the unreasonable effectiveness of mathematics,

1235
01:57:03,920 --> 01:57:11,040
right, is that abstracting actually produces things that are useful, you know, that abstraction,

1236
01:57:11,040 --> 01:57:16,240
I think the fact that abstraction helps with generalization is a very not well understood

1237
01:57:16,240 --> 01:57:21,920
kind of mystery in a sense, like, why should abstraction help generalize, but it does, like

1238
01:57:21,920 --> 01:57:28,880
in the real world, that's what happens. Though the yet abstraction in though abstraction has to

1239
01:57:28,880 --> 01:57:35,120
be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only

1240
01:57:35,120 --> 01:57:40,880
if, you know, you're looking for food or non food, but when it comes to this fear, if you want to

1241
01:57:40,880 --> 01:57:46,560
shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by

1242
01:57:46,560 --> 01:57:51,440
ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in

1243
01:57:51,440 --> 01:57:57,600
common with this orange, right, so that even the way how you abstract, it's not like, it's not like

1244
01:57:57,600 --> 01:58:02,880
we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's

1245
01:58:02,880 --> 01:58:09,840
our abstraction, but the how you abstract is also incredibly specific to what you want to do.

1246
01:58:10,960 --> 01:58:15,600
Yeah, and that's what, and I agree with Saba that this is an empirical question, right,

1247
01:58:16,400 --> 01:58:20,080
you know, like he's kind of like these concepts or whatever, it's an empirical question, and

1248
01:58:20,080 --> 01:58:25,920
Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give

1249
01:58:25,920 --> 01:58:30,880
us lots of data to start thinking about this empirically, and it's going to be really fascinating.

1250
01:58:30,880 --> 01:58:37,680
I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can,

1251
01:58:37,680 --> 01:58:43,840
you can probably, like you can probably even zero, you can zero knowledge prove that you can solve

1252
01:58:44,880 --> 01:58:50,320
a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't

1253
01:58:50,320 --> 01:58:56,320
even have to show your solution, and if they're, you know, people would put up arc problems,

1254
01:58:56,320 --> 01:59:02,400
and they, you know, if you want to try them, you will have to put up some money, and if you can

1255
01:59:02,400 --> 01:59:08,240
solve it, you know, the creator of the challenge gives you some money or something like this,

1256
01:59:08,240 --> 01:59:13,920
like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic,

1257
01:59:13,920 --> 01:59:18,160
like arc, right, or like you don't even, you somehow, like you're saying, you can just prove

1258
01:59:18,160 --> 01:59:22,480
you can solve the problem without ever you having seen the problem, but just an encryption of it.

1259
01:59:23,360 --> 01:59:27,920
Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence.

1260
01:59:30,800 --> 01:59:35,920
And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks?

1261
01:59:35,920 --> 01:59:41,520
So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin,

1262
01:59:41,520 --> 01:59:46,080
can't we just say people should have to pay through Bitcoin, if they, if somebody wins

1263
01:59:46,080 --> 01:59:51,760
the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that

1264
01:59:51,760 --> 01:59:59,760
domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer

1265
01:59:59,760 --> 02:00:05,520
aware generalization, which means the developer could not have conceived of the task. But if all

1266
02:00:05,520 --> 02:00:12,720
of the tasks are representing four human priors, then how is that developer aware of generalization?

1267
02:00:12,800 --> 02:00:15,200
Because the developer would be aware of all of those priors.

1268
02:00:16,000 --> 02:00:22,320
Of the priors, right? But not, not of the task, right? That's the control is the control, like

1269
02:00:22,320 --> 02:00:27,760
that's what he said, you have to know the start of where your, your white box analyzing from. And

1270
02:00:27,760 --> 02:00:33,600
the start is not clean slate, but the start here is these four priors. So it's, it's kind of the

1271
02:00:33,600 --> 02:00:39,840
diff between you give the developer those four priors, what can the developer come up with,

1272
02:00:39,840 --> 02:00:45,600
just from that, right? Yeah, because I think, I think there's a lot of information leakage there.

1273
02:00:45,600 --> 02:00:51,520
And you implicitly said the same thing, because you said, once you solve it, you know, once you

1274
02:00:51,520 --> 02:00:56,480
solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's,

1275
02:00:56,480 --> 02:01:01,200
but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it?

1276
02:01:02,160 --> 02:01:08,880
I guess. We need to figure out something cooler.

1277
02:01:08,880 --> 02:01:14,240
Like, no art coin. Okay. I don't care enough to grab it.

1278
02:01:15,440 --> 02:01:18,240
Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah,

1279
02:01:18,240 --> 02:01:23,040
thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional.

1280
02:01:23,040 --> 02:01:27,440
We've recently reached 10k subscribers actually. So yeah, thank you very much.

1281
02:01:27,440 --> 02:01:30,800
We're still going to continue the show now that we've had Shaleo.

1282
02:01:32,320 --> 02:01:35,920
Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean,

1283
02:01:35,920 --> 02:01:43,760
to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye.

1284
02:01:43,760 --> 02:01:48,640
I really hope you've enjoyed the episode today. Remember to like, comment and subscribe.

1285
02:01:48,640 --> 02:01:52,960
We love reading your comments and we'll see you back next week.

