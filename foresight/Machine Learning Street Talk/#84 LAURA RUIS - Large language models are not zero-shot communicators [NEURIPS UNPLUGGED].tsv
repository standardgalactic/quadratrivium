start	end	text
0	4080	Hello, folks. I was in New Orleans last week and I had the pleasure of interviewing Laura Ruiz,
4080	6160	the primary author on this paper,
6160	10080	large language models are not zero-shot communicators.
10080	14880	Now, this is exploring the ability of language models to perform Implicituk,
14880	18160	which I guess from a machine learning audience point of view,
18160	23440	you might think of as being some kind of extrapolation or even abstractive reasoning.
24080	25920	There's an example of this that we can try.
26880	29680	Esther asked, can you come to my party on Friday?
29680	34480	And Zhuang responded, I've got to work, which means no.
35520	39840	Yeah, part of the reason I wanted to do this quick intro is since this interview,
39840	45200	OpenAI has released a chat GPT, which is pretty impressive, actually.
45200	50320	So we can come in here and we can say something along the lines of
51840	54000	Esther asked, can you come to the party on Friday?
54080	55920	Zhuang responded, I have to work.
57360	64720	Does Zhuang, can Zhuang come to party?
69520	72960	It looks like it has failed. It says it's not possible to say
72960	76480	whether Zhuang can come to the party or not because we don't have enough information.
77840	82080	Zhuang may or may not be able to come to the party depending on his work schedule and other factors.
84560	88400	Yeah, so this is an example of failed implicature.
88400	91360	But anyway, if we come over to Laura's Twitter,
92400	96640	she posted a little thread the other day saying that loads of people have been sending her
96640	99920	implicatures, which they used as examples in the paper.
100560	105200	And apparently chat GPT does understand some of them, which she's very happy about,
105200	107600	but she wanted to write a short thread about it.
107600	112160	So she said before they started writing the paper, she would try lots of implicatures
112160	116880	that she came up with on Da Vinci 2 in different wordings with moderate success.
116880	121120	Some always solved and some half of the time depending on the wording,
121120	125840	meaning random performance since the test is a binary, which is to say a yes or a no.
126720	132320	That's why they decided to do a systematic test to figure out how good it actually was
132320	136000	and how much it depended on the wording of the prompt.
136560	140080	And a few months later, they had the answer that it was okay,
140080	141520	but not close to humans.
142080	149440	And okay means that on Da Vinci 2 and 3, the performance of zero shot implicature is roughly
149440	154880	70%. Most of the other models fail, even with few shot in context prompting.
155760	161280	So anyway, she said that she gets the people are excited that chat GPT is doing pragmatic
161280	164960	inferences, but she felt the same with Da Vinci 2.
164960	166800	It's all anecdotal, she says.
166800	171520	But a more systematic test shows a significant gap with humans nonetheless,
171520	176480	and it's the same for Da Vinci 3 and presumably the same for chat GPT.
176480	181040	She says that once this implicature dataset gets solved, and she has no doubt that it will get
181040	185200	solved relatively soon, since fine tuning with human feedback helps a lot,
185200	188960	they might have some baseline pragmatics in their models.
188960	191200	And that's when it will get really exciting.
191200	195200	She says that she's personally blown away by chat GPT's capabilities.
195200	200880	It's absolutely incredible at explaining things, compositional generalization of concepts,
200880	202240	simulating a VM.
202880	208240	I'm not sure what VM means, coherence, creativity, writing essays, poems and more.
209200	214480	She said that the pragmatic language that they studied is part of a type of casual language
214480	218320	that we're using conversation that might emerge from social interactions.
218320	222320	She's personally thinking about why human feedback helps so much,
222320	226720	and whether interactivity and social pressures might help even more.
226720	228480	Anyway, enjoy the interview.
228480	229040	Hi.
229040	230080	It's lovely to meet you.
230080	231120	Nice to meet you too.
231120	236160	So, I was speaking with Andrew Lampanam yesterday, and he really highly recommended your paper.
236160	237360	I looked it up, it's called,
237360	240960	Large Language Models Are Not Zero Shot Communicators,
240960	244880	and I also recognize Stella Biderman and Sarah Hooker, of course.
244880	246560	Sarah's an absolute legend.
246560	249360	Now, you led in the paper by saying,
249360	254240	humans interpret language using beliefs and prior knowledge about the world.
254240	256960	For example, we intuitively understand the response,
256960	261440	I wore gloves to the question, did you leave fingerprints as meaning no?
262080	268800	So, you call this implicature, but I suppose I would think of it as some kind of
269920	274560	extrapolation, being able to reuse knowledge that we have about the world
274560	275920	in a different situation.
275920	277920	But could you talk us through that paper?
278640	279440	So, yeah, thank you.
279440	281040	That was a great introduction to the paper.
281760	287360	Indeed, implicature is kind of the technical term that we use for this example that you gave.
288720	292800	And indeed, extrapolation is a sensible way to describe this.
292800	298160	What we do in this paper is kind of show that large language models are not really good at this
298160	302880	aspect of communication, and we think it's a very important aspect of communication.
302880	306880	So, the title says, Large Language Models Are Not Zero Shots Communicators, right?
307440	313600	So, what we mean by that is to be a communicator, you have to infer the meaning of utterances,
314400	320000	not only by their semantics, so not only by how words combine into some kind of meaning,
320000	324480	but by interpreting the shared knowledge, our shared experience of the world.
325600	327920	And that's what we look at in this paper.
327920	332880	And what we find is that large language models are pretty bad at this.
333840	337600	Specifically, we group them into different groups.
337600	341760	So, we have base large language models like OPT and BLOOM that are just trained on
341760	343280	next-word prediction.
343280	352400	And we also have instructable models like Flonty5, T0, or DaVinci Instructable Models by OpenAI.
353360	358160	And all models perform really bad, closer to random than to humans.
358800	364880	But OpenAI's instructable models have much more promise.
364880	365760	They're much better at it.
366480	366960	Interesting.
366960	370560	Okay, so now the zero shot thing is very interesting.
370560	374000	So, we take these models, and it's kind of like self-supervised learning.
374000	376560	We train them on loads of data on the Internet.
376560	381600	And you're saying that zero shot is when we don't really give much information in the prompt.
381600	386320	So, there's a relationship between how big the model is and how much in-context learning
386400	388240	that we give to the model in the prompt.
388240	389600	Yeah, that's true.
389600	396000	Yeah, the zero shot case that we tested is we give the model a short instruction saying like,
397600	403280	in the following exchange, someone gives a response that has some meaning beyond the utterances.
404400	407600	It had the meaning is yes or no, can you resolve this?
407600	409440	And then we give an example.
410720	415280	And then we evaluate it on ways based on whether it can choose yes or no.
416400	419840	So, that's the zero shot case, and humans, we don't give any instructions at all.
419840	424080	We just said, resolve this to a yes or no.
424640	430480	Okay, so, is it the case that large language models then zero shot almost irrespective of their size
430480	433200	and irrespective of this human feedback alignment?
433200	435440	They just don't perform very well at this implicature at all.
436160	441120	The instructable models by OpenAI gets a non-trivial performance.
441840	446400	I think the models like OPT and Bloom, those kind of base models,
447200	449680	they really conduce this style very well at all.
449680	457520	They get 10% above random, but OpenAI's models are around 70% at zero shots.
457520	458000	Interesting.
458000	462640	So, did you do some work looking at, okay, well, let's try some in-context learning.
462640	464320	Does that improve the implicature?
464960	465840	Definitely, yeah.
466400	468800	Like, it's unclear, right?
468800	473600	Whether zero shots is a fair comparison to humans for these models.
473600	476240	Humans are primed in different ways.
476240	480000	So, we also wanted to try view shots in context learning.
480640	485520	And personally, I thought in this case, in context learning wouldn't help much because
485520	489120	each implicature requires a completely novel type of inference.
489760	494560	But in fact, we show that OpenAI's models is the only group of models that really
495280	497040	benefits from this a lot.
497040	501200	And they can get to up to 80% performance with roughly five examples,
501760	504880	which, and afterwards, more than five examples, it's kind of plateaus.
506240	510160	But they're still like a significant gap with humans.
510160	512720	But it's a great improvement.
512720	513520	Yeah, that's fascinating.
513520	517360	So, can you give me an example of, if we were doing some in-context learning,
517360	521040	let's say with DaVinci 2, what would that prompt look like?
522000	527760	So, if we, I don't exactly remember the wording of the prompts,
527760	531520	but there would be something like the following are examples of the task.
531520	535920	And then you get a bunch of implicatures that are already resolved to a yes or no.
536560	541040	And then you get the original instruct prompt that says
542160	545040	resolve the following sentence to a yes or no.
545040	547440	And then you get the actual example.
547440	550560	And these in-context examples are all taken from a development set.
551120	554720	Okay, so, can you tell us a little bit about
554720	558480	how this reinforcement learning for human preferences works on language models?
559200	562800	So, reinforcement learning for human preferences is a method to fine-tune
563760	564800	based on our language models.
564800	568880	So, the based on our language models are OPT and BLU, for example,
568880	570240	that's part of the group.
570240	572400	And they are just trying some next-word prediction, right?
572400	574080	But they are not really aligned.
574080	578000	They're sort of this alignment problem where they're trained on next-word prediction
578000	580560	and that's not really what we are asking them to do.
581680	585440	And then with reinforcement learning from human feedback,
585440	590080	what we further do, I mean, not we, unfortunately, other people do,
590080	595040	is they take some kind of human preferences from somewhere.
595040	599200	Like, for example, humans are shown prompts and completions by models
599200	601360	and they say, this one is better than that one.
601360	604960	This completion for the text for this prompt is better than that one.
605520	608800	By that, we get a sort of ranking by preference
608800	611440	and we can learn a reward model on those preferences
612640	615520	with an interesting trick that was published in 2017.
616640	619760	And through this rewards model, we have sort of,
619760	626000	we can bootstrap the preferences from humans into the based archangels model
626000	631440	by fine-tuning them with regular RL on this reward model.
631440	632720	Yeah, it's really interesting.
632720	637520	I was speaking with Srijan Kumar who won one of the outstanding
637520	641360	paper awards in Europe and he's got this work on kind of,
642400	645360	we want the models to be more anthropomorphic
645360	648320	and we have these priors to help us understand the world.
648320	653360	And he came up with a framework of kind of like importing these priors
653360	657920	from language encodings into, let's say, a discrete program synthesis model.
657920	660960	But I guess what I'm saying is that there's something really interesting
660960	665280	going on within context learning and it's almost like we're giving the model
665280	671040	the priors to extrapolate or to do something useful in this particular situation.
671040	672800	Yeah, yeah, that's really interesting.
672800	674480	I don't know the paper, I should check it out.
674480	679360	But the way I view it, and my thinking has been shaped this week also
679360	683120	by Andrew Lampinen who wrote an interesting paper on comparing models in humans,
683680	688880	is that it seems that in context learning for this specific task, implicatures,
688880	692800	it's not really that they learn how to use their shared experience
693600	696640	from the in-concept samples, they're primed for the task
696640	698880	with a few shot examples in the context.
699440	702160	And I think that's actually what's happening here.
702160	706240	Like if you test the model zero shots, there's no,
706240	709840	why would we expect it to do this task properly?
709840	711840	There's no motivation or anything like that.
711840	716480	But if you prime it with in-concept samples, it does better.
716480	722560	And that would also explain why it doesn't help to add more than five examples
722560	726480	because it's not using the inherent information in the examples,
726480	728880	it's just being primed for this specific task.
728880	730000	Yeah, that's really interesting.
730000	733840	Sarah has done lots of work on interpretability in machine learning models.
733840	739280	And one thing that I wrestle with a lot is whether we should try and get models
739280	741040	to think the way humans do.
741040	745360	And you can come at it from an intelligibility point of view,
745360	748080	but you could also come at it from a generalization point of view.
748080	751440	Like maybe we do symbolic generalization over cognitive priors,
751440	752960	and that's how we understand the world.
752960	756320	But there are people who just say large language models,
756320	757920	they're just a different mode of understanding,
757920	759280	and we shouldn't try and make them like us.
759280	760080	Like what do you think?
762240	763360	It's a good question.
763360	766480	I am really a non-expert on interpretability.
766480	771120	I'm like, I always come at it from a very anthropocentric view.
771120	773040	Like I would love them to be more like humans
773040	776400	because that would make them interesting subjects to study also
776400	778960	and better to communicate with.
778960	781680	But at the same time, you can take this opposite view.
781680	786000	And I think Stella, the co-author on this paper often says,
786880	789920	you're making a category area, you're attributing something to these models
789920	793440	that they don't have knowledge, those kind of things.
793440	798000	So it might also very well be that we're trying to look for pragmatics
798000	799600	or semantic understanding in these models,
799600	802000	but that's just not how you should think about it.
802640	804400	And I completely forgot to ask you.
804640	808560	So again, some of the audience don't know about
808560	810480	natural language understanding in linguistics and so on.
810480	812720	So what is pragmatics?
812720	814000	Yeah, that's a good question.
814000	821840	So pragmatics is an aspect of language, the way we study language,
821840	826160	that doesn't really look at the syntax or semantics,
827520	832000	which look at, for example, those kind of aspects of linguistics,
832000	835920	look at what a word means and how you combine them into novel meanings.
835920	839120	So those kind of areas of linguistics really look at
839760	843840	when someone understands the term, the utterance as John loves Mary,
843840	846080	they also understand utterance Mary loves John.
846640	848720	Pragmatics goes beyond that.
848720	855520	It looks at how context and our shared experience really influences meaning.
855520	859360	So usually the meaning determined by pragmatic inference
859360	863040	is not really directly part of the context window.
863040	868080	You really have to tap into your prior knowledge.
869280	871600	Yeah, so I'm a fan of Montague as well.
871600	876240	So it's almost like we have the semantic potential and then we have pragmatics,
876240	879760	and that's bringing some additional context.
879760	880880	Yeah, yeah, exactly.
882880	887840	Okay, and because that's a really great example from that extrapolative example
887840	889440	from Montague about Mary loves John,
890640	892880	how could a large language model realistically,
892880	896160	because I think of that as being a symbolic generalization.
896160	898640	So how could a language model do that kind of generalization?
899200	900400	Symbolic generalization?
900400	901200	Yes.
901200	902160	Oh, that's a big one.
905360	906160	I don't know.
906160	907120	I really, really don't know.
908400	911200	In my research journey, I can kind of
912000	914080	come from studying compositionality in language,
914080	917200	which is really more this type of thing that we're talking about now.
917840	922960	And looking at more sort of neurosymbolic approaches or stronger inductive biases.
922960	926720	And now these large language models really showed us that
927280	931360	there is an insane amount of compositional generalization going on
931360	933280	without any inductive bias for that.
933840	936720	Chat GPT kind of shows us that with all these examples on Twitter,
936720	939280	right, you give it two novel concepts,
939280	943040	and it combines it beautifully into some kind of story.
944240	946560	But yeah, to go back to your question, how can they do it?
946560	950640	I don't know. Maybe Skill will get us there to the sense that
951280	953840	humans are also imperfect symbolic reasoners.
953840	955280	Again, to mention Andrew Lampinen,
955280	958640	he did a great paper on symbolic behavior,
958640	960880	where it's not really a discrete,
960880	964560	I can do symbolic processing versus I cannot do symbolic processing,
964560	966000	but it's more a scale.
967360	968800	That's kind of shaped my thinking as well.
968800	970160	I think it's a scale.
970160	972880	Large language models are pretty far on that scale.
972880	975760	They can do very interesting compositional generalization.
976720	978800	And sort of symbolic behavior,
978800	982000	but they fail in catastrophic ways as well.
982880	986000	Again, an example that I think comes from Gary Marcus is
986000	987760	when you ask Chat GPT,
990960	993280	how do horses ride cowboys?
993280	997120	And it just writes a whole story about how a horse rides a cowboy,
997120	999600	even though it doesn't work.
1000320	1001520	Yeah, it's so interesting,
1001520	1005120	because I think it's easy to think of large language models in the binary.
1005840	1007360	Marcus says they're bloviators,
1008800	1010720	and Bender says they're stochastic parrots.
1011280	1014880	And then you have the folks who think that it's emergent reasoning
1014880	1016560	and symbolic generalization.
1016560	1020240	And I was a skeptic, and I just can't ignore the evidence.
1020240	1023760	They really are doing amazing things now.
1024720	1026480	And you were just speaking to Lampinen,
1026480	1030800	and it's a similar thing with this idea of symbolic generalization.
1030800	1032080	It might not be a binary, right?
1033120	1033840	Yeah, exactly.
1034480	1036400	It might be a very great as competency.
1037440	1039760	And humans also fail in certain cases.
1040720	1042880	So on this in-context learning,
1042880	1044960	because that's something that has interested me.
1044960	1048000	So the first version of GPT-3 is zero-shot.
1048000	1049600	We didn't really know how to prompt it.
1049600	1051280	It looked like a bloviator.
1051280	1053040	We then went on this intellectual journey,
1053040	1056160	and we discovered front engineering, scratch pad,
1056160	1057360	chain of thought reasoning.
1058000	1059760	I spoke with Hattie Zoe the other day,
1059760	1063040	and she's got this kind of algorithmic front in-context learning.
1063040	1065680	And it's just remarkable what's going on there.
1066720	1067920	Do you have any intuition?
1068960	1072960	Is it like the prompt is some kind of a program interpreter or something?
1075760	1078640	My intuition is rather that the prompt kind of...
1079360	1081920	I don't know how to formalize this intuition,
1081920	1083600	but I guess that's why it's an intuition.
1084640	1088080	That the prompt kind of primes the model
1088080	1091440	and puts it into a sort of area of its weight space,
1091440	1097680	where it can better answer the actual question
1097680	1101440	that is asked in the actual question that's asked.
1102000	1105120	And I think certain things that point towards this
1105120	1108160	is that there is also some research coming out
1108160	1113520	where they permute the labels in the in-context examples,
1113520	1115520	and they show that the performance is similarly good,
1116160	1119520	even though at the same or like they do completely random labels
1119600	1123280	in the in-context examples, and the performance is still pretty good.
1123280	1127040	But there's also other work that shows that doesn't always work.
1127040	1128800	Sometimes you do need actual labels.
1130560	1133520	So yeah, again, the answer is basically, I don't know,
1133520	1136800	but my intuition is rather that the model is really primed for...
1138480	1141040	Or there's also another great way of viewing this,
1142400	1144880	and I read that unless wrong at some point.
1144880	1147120	I don't know how to attribute the author,
1147840	1150480	because I forgot their name, but it's about
1151840	1154080	that these models are good at simulating anything.
1154080	1156880	So you have to sort of prime them to let them know
1156880	1158080	what they're simulating right now.
1159440	1160240	Yes, weird, isn't it?
1160240	1165040	Because we have an anthropocentric view of the world,
1165040	1169440	and we're agentive with individual agents.
1169440	1172400	And a language model is everything at once.
1172400	1176480	So it's almost like you need to give it a trajectory just to get it
1176480	1177840	to go somewhere interesting.
1177840	1182160	So with this in mind, we really want to make progress
1182160	1183920	in natural language understanding.
1183920	1186720	And what do you think are the steps we need to make
1186720	1190000	to robustify these language models?
1190000	1191280	Yeah, that's a good question.
1192000	1194960	Personally, from this pragmatics paper,
1194960	1199360	I think pragmatics is one area where they can make huge strides too.
1199360	1201920	I think even though they have semantic failure modes,
1201920	1203680	they're really impressive at that.
1203680	1206400	They're really impressive at compositional generalization.
1208080	1209600	But pragmatics might be something
1209600	1213840	that they're simply not trained for currently.
1213840	1218160	And one very low-hanging fruit is the RLHF that we talked about.
1218960	1221920	I think that clearly really improves that.
1221920	1226960	And intuitively, it seems like in the instructivity paper,
1226960	1228880	you see that they ask the human laborers
1228880	1231760	to really infer the human intent and the problems and write on.
1231760	1234480	And that's very reminiscent of implicatures.
1235680	1237520	But then on a more sort of broader scale,
1237520	1241040	I think some kind of embodiment or interactivity
1241040	1242800	might be really important.
1242800	1246160	Like pragmatic inference is really a social skill that we have.
1247120	1251360	There's a lot of pragmatic pressures that you encounter
1251360	1254960	while just acting in the world and navigating communication
1254960	1256960	and navigating a lot of things.
1257520	1260000	So I think I'm currently trying to look at
1261680	1264000	a setup in reinforcement learning
1264000	1266960	where we're trying to make a pragmatic task
1266960	1269600	and see when pragmatic reasoning would emerge there.
1272560	1274720	I don't know how to consolidate that fully
1274720	1275920	with large language models yet,
1275920	1282320	but I think interactivity and social navigation is important.
1283280	1287040	I'm really fascinated by the embedded tradition
1287040	1288400	in cognitive science.
1288400	1290080	And I think there's a lot of interesting work there.
1290080	1292720	But I suspect you do as well a little bit.
1293360	1295600	How do you contrast what is essentially
1295600	1296960	the representation of this view
1296960	1299200	where everything's in this big monolithic model
1299200	1301200	to some kind of relational view
1301200	1304000	where we're using essentially the world
1304000	1305520	as its own representation?
1305520	1307600	Yeah. Again, I don't know.
1307600	1310640	To what extent it's also possible to express everything
1310720	1312880	in just the representation in this view
1312880	1314560	where you have an internal world model.
1314560	1316640	And I don't know to what extent
1316640	1318720	you really need an external world to learn,
1318720	1322160	but intuitively it seems like that might be very important.
1322160	1328240	And intuitively it also seems like the behaviors
1328240	1332320	that can emerge are really limited by the world's models
1332320	1335360	acting in a large language model only see sex.
1336320	1340400	And there's basic things that just simply cannot learn,
1340400	1343120	even though it has surprised us a lot.
1344080	1345600	So I think, I don't know,
1345600	1348000	it's easy to think about that it's really important
1348000	1350240	to have some kind of external world to interact with.
1351440	1355200	But I'm happy people are working on scaling
1355200	1361040	and I'm not saying some type of AGI, whatever that means,
1361040	1363920	might not emerge from simply scaling up
1363920	1365280	basically what we're doing right now.
1366320	1369920	Amazing. And are there any other parts of your paper
1369920	1371120	that we haven't spoken about?
1372960	1376000	Yeah, one thing that we found pretty interesting
1376000	1379200	is that even though all these open AIs models
1379200	1382560	can get really high performance, close to humans,
1382560	1385760	6% roughly, that won't tell you much without the details.
1385760	1388480	But it's a significant gap still, but it's really, really close.
1388480	1392640	I don't know if a human might figure out
1392640	1396320	whether this model is a model or a human in that case.
1396960	1401920	But when we sort of drill down and make a taxonomy
1401920	1404080	of the examples that are in this data set,
1404080	1407040	we find that they are mostly benefiting
1407040	1410720	from the simple examples where not a lot of context is needing.
1410720	1416080	So one example is an implicator is if you ask me
1416640	1418240	how many people came to your party,
1418240	1419920	and then I say some people came.
1420560	1422960	It's really the conventional meaning kind of of the word
1422960	1425280	some that I meant not all people came.
1425840	1428800	But it's still an implicator, but it's a very common one.
1428800	1431120	So those kind of examples, if we isolate those
1431120	1433680	and we look at specifically examples
1433680	1435680	that are really context-dependent like
1438400	1441440	are you coming to the open AI party tonight?
1441440	1443040	I have food poisoning.
1443040	1446080	Those need much more context to be resolved.
1446080	1447920	And then the performance decreases again,
1447920	1453360	and there is roughly a 9% gap, which like the best model,
1453440	1457280	but all other based models and instructable models
1457280	1458560	like Flancy V and stuff,
1458560	1461920	they then again completely fail on those kind of examples.
1461920	1462560	Fascinating.
1463280	1465200	I'm really interested by this idea that
1465200	1467680	understanding is a complex phenomenon,
1467680	1471200	and it's like the parable of the blind man and the elephant.
1471200	1473200	So we create all of these metrics,
1473840	1476320	and the metrics exclude most of the truth.
1476320	1480240	And the metrics for pragmatics presumably are in some sense
1480240	1481840	even more complicated than the metrics
1481840	1483920	that we already use in natural language understanding.
1483920	1488960	And it just feels like is that going to be a serious problem
1488960	1492560	for us to kind of encapsulate how well the model understands?
1493360	1497440	Do you mean that we're sort of giving it a test
1497440	1498560	that is couldn't really solve?
1499280	1501520	Well, I suppose one way of looking at it is,
1501520	1504240	in this particular test, we've come up with lots of examples
1504240	1506960	of pragmatic inference, if you like.
1506960	1509760	And what we're doing is we're taking a very complex phenomenon
1509760	1512320	and we're putting pins through it,
1512320	1515360	so we're putting like little slices through it in different angles.
1515360	1518000	And then we've got this shortcut problem
1518000	1520720	that if we optimize on any one of those slices,
1520720	1524320	we might be kind of like excluding everything else that's important.
1524320	1525280	So it feels like we've got,
1526800	1530240	is this like a general problem in natural language understanding?
1530240	1534240	It seems like you're getting at evaluation to some extent, right?
1534480	1539680	I think evaluation is the single most difficult thing in NLP.
1541280	1543760	This is just a benchmark to give us some intuition
1543760	1546800	as to what these current failure models,
1547360	1549120	failure modes of these models might be.
1549680	1553520	And I think if this benchmark is at some point passed
1553520	1557280	by a model that's in and of itself
1557280	1559600	without trying to diminish my own paper, it doesn't tell us much.
1559600	1560640	There's much more to be done.
1560640	1562560	We need more different benchmarks.
1562560	1565360	We need like human testing in a sort of Turing style maybe.
1566800	1572000	And yeah, I think this is the most interesting problem in NLP,
1572000	1573440	like how the property evaluates.
1574080	1574880	Interesting.
1574880	1578320	And do you take an interest in fairness and bias in the models as well?
1578880	1582480	I'm very interested in it, but from a sort of spectator view as well.
1582480	1584560	Like I haven't worked on it at all.
1584560	1587600	Okay, yeah, because that's presumably a massive challenge.
1587600	1590320	Yeah, yeah, definitely, yeah.
1590400	1591280	Amazing.
1591280	1597440	So in final question, what are you most excited about in your research trajectory
1597440	1598240	over the next year or so?
1600080	1604000	Well, definitely, I just feel super excited to be working on stuff like this
1604000	1607520	currently given the capabilities these models show.
1607520	1609520	Like they're absolutely amazing.
1609520	1612240	And I love seeing how people interact with them.
1612240	1616160	Like the creativity of people is really needed to get some kind of interesting
1616160	1617680	response out of these models, right?
1617680	1621760	And also the creativity of people is needed to find the failure modes.
1621760	1628320	And yeah, so what I'm most interested, excited about now personally for my own
1628320	1633920	research journey is really trying to look at, you know, an interactive setup
1633920	1640480	and see when pragmatic inferences might emerge in what kind of environments,
1640480	1645040	what kind of pressures do we need, and how can we translate that back to getting to be,
1645520	1650080	to getting like language models be zero shells communicators.
1650080	1650480	Amazing.
1650480	1652000	And where can people find out more about you?
1653680	1658240	They can follow me on Twitter is first name, last name, and I have a website,
1658240	1659840	also firstname, lastname.com.
1660560	1661280	Amazing.
1661280	1662240	Laura, thank you so much.
1662240	1663280	It's a pleasure to meet you.
1663280	1664400	Thank you for having me.
1664400	1664880	Amazing.
1664880	1665120	Amazing.
1665120	1665360	Right.
1665360	1665760	Cool.
