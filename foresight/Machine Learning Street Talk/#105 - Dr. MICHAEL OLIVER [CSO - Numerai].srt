1
00:00:00,000 --> 00:00:02,000
Welcome back to Street Talk.

2
00:00:02,000 --> 00:00:05,160
Today we have Dr. Michael Oliver.

3
00:00:05,160 --> 00:00:08,720
Michael is the chief scientist at Numeri.

4
00:00:08,720 --> 00:00:13,040
Numeri is a next generation hedge fund platform

5
00:00:13,040 --> 00:00:17,440
powered by data scientists all over the world.

6
00:00:17,440 --> 00:00:19,840
It's a little bit like Kaggle.

7
00:00:19,840 --> 00:00:22,480
Anyone can log in and build their own data science models

8
00:00:22,480 --> 00:00:24,800
on this financial data,

9
00:00:24,800 --> 00:00:26,620
but you can actually make money

10
00:00:26,620 --> 00:00:28,000
by trading on this platform.

11
00:00:28,000 --> 00:00:30,640
It's really, really interesting.

12
00:00:30,640 --> 00:00:33,000
But anyway, Michael got his PhD

13
00:00:33,000 --> 00:00:36,240
in computational neuroscience from UC Berkeley,

14
00:00:36,240 --> 00:00:38,240
and he was a postdoctoral researcher

15
00:00:38,240 --> 00:00:40,720
at the Allen Institute for Brain Science

16
00:00:40,720 --> 00:00:43,680
before joining Numeri in 2020.

17
00:00:43,680 --> 00:00:45,620
He's also the host of the Numeri Quant Club,

18
00:00:45,620 --> 00:00:47,240
which is a YouTube series

19
00:00:47,240 --> 00:00:49,560
where he discusses Numeri's research

20
00:00:49,560 --> 00:00:51,720
and also some of the data and challenges

21
00:00:51,720 --> 00:00:54,220
and models that are being built on the platform.

22
00:00:55,200 --> 00:00:57,000
Now, the way I'm structuring this today

23
00:00:57,000 --> 00:00:58,920
is at the end of the conversation,

24
00:00:58,920 --> 00:01:02,800
we had quite a fruity discussion about Microsoft's new Bing,

25
00:01:02,800 --> 00:01:05,960
and I thought it was quite entertaining,

26
00:01:05,960 --> 00:01:09,200
so I've decided to snip that in and play it at the beginning.

27
00:01:09,200 --> 00:01:11,040
But after that, I'll cut back into the beginning

28
00:01:11,040 --> 00:01:11,880
of the conversation,

29
00:01:11,880 --> 00:01:13,680
and I'll let you know when I've done that.

30
00:01:13,680 --> 00:01:15,560
So without any further delay,

31
00:01:15,560 --> 00:01:17,680
I give you Dr. Michael Oliver.

32
00:01:20,720 --> 00:01:21,560
Awesome.

33
00:01:21,560 --> 00:01:24,480
Well, I'm here with Michael Oliver.

34
00:01:24,480 --> 00:01:27,400
Michael, it's an absolute honor to have you on MLST.

35
00:01:27,400 --> 00:01:28,960
Tell me about yourself.

36
00:01:28,960 --> 00:01:30,280
Well, thank you for so much for having me.

37
00:01:30,280 --> 00:01:32,400
I'm really excited to talk to you today.

38
00:01:32,400 --> 00:01:34,840
So I am the chief scientist at Numeri.

39
00:01:34,840 --> 00:01:37,740
I've been working there since about June, 2020.

40
00:01:38,640 --> 00:01:41,600
In my previous life, I was a computational neuroscientist,

41
00:01:41,600 --> 00:01:43,800
but I got involved with the Numeri competition

42
00:01:43,800 --> 00:01:46,660
as a participant back in 2016.

43
00:01:47,600 --> 00:01:50,520
And yeah, in 2020, they offered me a job

44
00:01:50,520 --> 00:01:54,000
and I happily took it and changed careers

45
00:01:54,000 --> 00:01:55,640
and have been having a great time

46
00:01:55,640 --> 00:01:57,920
learning computational finance

47
00:01:57,920 --> 00:02:02,160
and yeah, just helping build the hedge fund.

48
00:02:03,220 --> 00:02:04,060
Completely agree.

49
00:02:04,060 --> 00:02:06,680
And this all comes down to the notion of understanding

50
00:02:06,680 --> 00:02:09,740
and there's an anthropocentric conception of understanding,

51
00:02:09,740 --> 00:02:12,980
which as you say, it's much more sample efficient.

52
00:02:12,980 --> 00:02:15,400
We build causal models

53
00:02:15,400 --> 00:02:18,640
and we have an abstract understanding of the world.

54
00:02:18,640 --> 00:02:20,560
And large language models, for example,

55
00:02:20,560 --> 00:02:21,900
they clearly don't have that.

56
00:02:21,900 --> 00:02:24,780
They learn surface statistics of billions of tokens,

57
00:02:24,780 --> 00:02:27,180
but the problem is there's this parlor trick

58
00:02:27,180 --> 00:02:29,780
where it seems to understand.

59
00:02:29,780 --> 00:02:32,540
And we also have the problem of leakage

60
00:02:32,540 --> 00:02:34,940
because the incredible thing is that

61
00:02:34,940 --> 00:02:37,340
if you look at the big bench task, for example,

62
00:02:37,340 --> 00:02:38,940
all of these diverse tasks,

63
00:02:38,940 --> 00:02:41,660
large language models appear to do very, very well.

64
00:02:41,660 --> 00:02:43,820
But in many cases, it's because they're cheating

65
00:02:43,820 --> 00:02:45,980
and it's very difficult to understand why they're cheating

66
00:02:45,980 --> 00:02:48,380
because you've got information leaking all over the place

67
00:02:48,380 --> 00:02:50,980
and they're brittle, but in a very deceptive way

68
00:02:51,060 --> 00:02:52,620
and they hallucinate and so on.

69
00:02:52,620 --> 00:02:54,460
I don't know whether you saw the news article today

70
00:02:54,460 --> 00:02:57,700
about Bing's launch of their new search engine.

71
00:02:57,700 --> 00:02:59,660
They launched it to much fanfare

72
00:02:59,660 --> 00:03:01,820
and then people started looking at the actual results

73
00:03:01,820 --> 00:03:04,060
that were shown and it turned out to be just a load

74
00:03:04,060 --> 00:03:04,900
of bullshit.

75
00:03:04,900 --> 00:03:06,660
It made up a whole load of numbers

76
00:03:06,660 --> 00:03:08,220
on the financial reports.

77
00:03:08,220 --> 00:03:10,820
It was just hallucinating completely.

78
00:03:10,820 --> 00:03:13,220
And that's pretty scary, isn't it?

79
00:03:13,220 --> 00:03:14,580
Yeah, it is.

80
00:03:15,940 --> 00:03:17,220
I actually just started playing around

81
00:03:17,220 --> 00:03:19,060
with the new Bing like last night.

82
00:03:19,060 --> 00:03:24,060
I had access to it and it was actually working.

83
00:03:25,780 --> 00:03:28,620
And it does some things quite well

84
00:03:28,620 --> 00:03:32,340
because it'll do a search and then search somehow.

85
00:03:32,340 --> 00:03:33,780
It's like actually looking at the results

86
00:03:33,780 --> 00:03:34,980
and summarizing them.

87
00:03:36,140 --> 00:03:39,620
But yeah, you never know when it's gonna do something

88
00:03:39,620 --> 00:03:44,620
sensible and when it's gonna do any sort of no warning.

89
00:03:44,820 --> 00:03:47,900
Like I asked it about myself and I had,

90
00:03:50,820 --> 00:03:52,580
I asked who's the chief scientist in Uri

91
00:03:52,580 --> 00:03:53,980
and it got it right.

92
00:03:53,980 --> 00:03:58,300
But it also kind of took a joke from my Twitter profile

93
00:03:58,300 --> 00:04:01,020
and because on my Twitter profile,

94
00:04:01,020 --> 00:04:03,940
I have maximizer, entropy, minimizer, regret.

95
00:04:03,940 --> 00:04:05,820
And it basically said like, that's what he does.

96
00:04:05,820 --> 00:04:08,100
Is he maximizes entropy and minimizes regret.

97
00:04:08,100 --> 00:04:10,860
And I thought that was pretty hilarious.

98
00:04:10,860 --> 00:04:13,900
But yeah, the sort of like you never know

99
00:04:13,900 --> 00:04:15,860
when it's gonna do something sensible or not

100
00:04:15,860 --> 00:04:17,900
is the sort of scary part.

101
00:04:17,900 --> 00:04:20,140
And I also find it hilarious that a lot of the ways

102
00:04:20,140 --> 00:04:21,940
we try to make it do something sensible

103
00:04:21,940 --> 00:04:23,540
is just like asking nicely.

104
00:04:24,500 --> 00:04:25,860
We just sort of like prompt it with like,

105
00:04:25,860 --> 00:04:27,260
don't make up sources.

106
00:04:27,260 --> 00:04:30,860
And that's how we try to make it not make up sources

107
00:04:30,860 --> 00:04:32,300
just like sort of by asking it nicely.

108
00:04:32,300 --> 00:04:34,660
And the fact that it kind of works

109
00:04:34,660 --> 00:04:37,580
like that we think that it's clearly not,

110
00:04:37,580 --> 00:04:40,380
something that's really going to work.

111
00:04:40,420 --> 00:04:44,740
Because it doesn't sort of know what it is

112
00:04:44,740 --> 00:04:46,460
to make up sources.

113
00:04:46,460 --> 00:04:49,300
It's just trying to like predict the next word.

114
00:04:49,300 --> 00:04:54,300
And yeah, so it's kind of, our ability to like understand

115
00:04:54,620 --> 00:04:56,500
and then constrain the behavior of these things

116
00:04:56,500 --> 00:04:59,260
I think is like pretty early.

117
00:04:59,260 --> 00:05:00,100
I know.

118
00:05:00,100 --> 00:05:01,900
And for some reason it feels worse with Bing

119
00:05:01,900 --> 00:05:05,420
because they say they do this retrieval augmented generation

120
00:05:05,420 --> 00:05:07,700
and you expect it to be grounded in facts.

121
00:05:07,700 --> 00:05:09,700
And of course they're not epistemic facts.

122
00:05:09,700 --> 00:05:11,780
They're just information from their search results

123
00:05:11,780 --> 00:05:14,420
which weren't very good to start with, let's be honest.

124
00:05:14,420 --> 00:05:16,260
But now people are more likely to trust it.

125
00:05:16,260 --> 00:05:18,860
Even Microsoft themselves for their product demo

126
00:05:18,860 --> 00:05:21,220
they didn't bother, I assume they didn't bother

127
00:05:21,220 --> 00:05:22,180
to fact check this stuff.

128
00:05:22,180 --> 00:05:24,060
So if they're not going to fact check it,

129
00:05:24,060 --> 00:05:25,940
why do they expect the people that use this system

130
00:05:25,940 --> 00:05:26,780
to fact check it?

131
00:05:26,780 --> 00:05:27,620
Because at the end of the day

132
00:05:27,620 --> 00:05:29,340
if you actually go and check all of the sources

133
00:05:29,340 --> 00:05:31,580
if I read through that Lululemon financial report

134
00:05:31,580 --> 00:05:34,300
and I find out what their gross profit margin was

135
00:05:34,300 --> 00:05:37,220
and so there's no point using Bing in the first place.

136
00:05:37,220 --> 00:05:38,060
I might as well have just gone

137
00:05:38,140 --> 00:05:40,460
and found the information myself.

138
00:05:40,460 --> 00:05:41,380
Yeah, exactly.

139
00:05:41,380 --> 00:05:43,540
And it's also very unclear

140
00:05:43,540 --> 00:05:46,100
like how are these things supposed to be fixed?

141
00:05:47,420 --> 00:05:50,700
Like how are you supposed to like feedback, give feedback

142
00:05:50,700 --> 00:05:52,940
to say it's like messing these things up?

143
00:05:52,940 --> 00:05:55,500
Like there is not even like really good feedback mechanisms.

144
00:05:55,500 --> 00:05:58,540
I mean, you would maybe hope that that at scale

145
00:05:58,540 --> 00:06:00,700
like what I mean, like what opening has to do

146
00:06:00,700 --> 00:06:01,980
is like people give feedback.

147
00:06:01,980 --> 00:06:04,140
But I mean, it's a very coarse way of giving feedback

148
00:06:04,140 --> 00:06:05,700
like thumbs up or thumbs down.

149
00:06:06,700 --> 00:06:09,140
And that seems like sort of inadequate

150
00:06:09,140 --> 00:06:11,940
to be like, hey, you made up this number

151
00:06:11,940 --> 00:06:13,940
and then even try to figure out why I made up the number

152
00:06:13,940 --> 00:06:16,820
rather than just like took it from the actual report.

153
00:06:18,060 --> 00:06:20,540
It's yeah, it's a little scary.

154
00:06:20,540 --> 00:06:23,860
I do wonder how all this is gonna shake out.

155
00:06:24,900 --> 00:06:26,980
It kind of seems like it might,

156
00:06:26,980 --> 00:06:30,100
it seems like the probability of it being

157
00:06:30,100 --> 00:06:33,100
the new paradigm versus it being the complete like flop

158
00:06:33,260 --> 00:06:37,260
It's even roughly, roughly equal at this point.

159
00:06:37,260 --> 00:06:38,340
I know, I agree with you

160
00:06:38,340 --> 00:06:41,020
that the preference training is extremely brittle.

161
00:06:41,020 --> 00:06:42,620
It's scarily brittle actually.

162
00:06:42,620 --> 00:06:44,620
It's basically a thumbs up or thumbs down

163
00:06:44,620 --> 00:06:47,900
and you know, Yannick is building this open assistant thing

164
00:06:47,900 --> 00:06:49,940
which has more metadata on the preference tuning.

165
00:06:49,940 --> 00:06:51,980
But at the end of the day, you're taking a task

166
00:06:51,980 --> 00:06:53,700
which is very, very complicated

167
00:06:53,700 --> 00:06:56,660
and you're reducing it to a single piece of metadata.

168
00:06:56,660 --> 00:06:58,540
So that's not gonna work very well.

169
00:06:58,540 --> 00:07:01,580
And also these, it feels different with Bing

170
00:07:01,580 --> 00:07:05,140
because they were a platform and now they're a publisher.

171
00:07:05,140 --> 00:07:07,020
So they are generating information.

172
00:07:07,020 --> 00:07:09,660
They're kind of plagiarizing a lot of that information

173
00:07:09,660 --> 00:07:11,860
and there are so many situations

174
00:07:11,860 --> 00:07:14,180
where they might find themselves in legal trouble

175
00:07:14,180 --> 00:07:16,660
because they're basically making up information.

176
00:07:17,940 --> 00:07:22,940
Yeah, I hope, I mean, I wonder how it's all gonna shake out.

177
00:07:25,460 --> 00:07:27,980
I mean, I assume they probably have lawyers

178
00:07:27,980 --> 00:07:31,900
who've written in terms of service of these paintings to that.

179
00:07:31,900 --> 00:07:35,260
Like it's up to you to not use them in ways that will,

180
00:07:35,260 --> 00:07:36,100
like I know you can't,

181
00:07:36,100 --> 00:07:37,580
they're not to be held liable for these things,

182
00:07:37,580 --> 00:07:42,580
but yeah, it's, I do worry that this is gonna just like,

183
00:07:42,780 --> 00:07:45,660
I mean, and then with Google trying to basically catch up

184
00:07:45,660 --> 00:07:48,780
and release something similar and maybe rushing that out

185
00:07:48,780 --> 00:07:52,220
and then we might have two sort of hallucinating search engines.

186
00:07:54,500 --> 00:07:56,140
I know, yeah.

187
00:07:56,140 --> 00:07:57,420
What a time to be alive.

188
00:07:59,700 --> 00:08:02,260
Yeah, and I've vacillated back and forth.

189
00:08:02,260 --> 00:08:04,380
So I was very skeptical about language models.

190
00:08:04,380 --> 00:08:07,740
I released a big video when GBT3 first came out

191
00:08:07,740 --> 00:08:10,180
and I thought it was garbage, frankly.

192
00:08:10,180 --> 00:08:11,900
And then DaVinci 2 came out

193
00:08:11,900 --> 00:08:13,660
and then I started using it all the time

194
00:08:13,660 --> 00:08:15,580
and I thought, wow, this is actually really good.

195
00:08:15,580 --> 00:08:18,380
I'm using it all the time for lots of things.

196
00:08:18,380 --> 00:08:20,500
And then I'm now in a bit of a twilight.

197
00:08:20,500 --> 00:08:22,460
So I've been using lots of co-pilot.

198
00:08:22,460 --> 00:08:24,700
I've been generating lots of code with it.

199
00:08:24,780 --> 00:08:26,780
And I know from a lot of experience now

200
00:08:26,780 --> 00:08:29,580
that it often produces completely broken code

201
00:08:29,580 --> 00:08:33,740
and much to the chagrin of the people who review my code,

202
00:08:33,740 --> 00:08:36,460
you basically have to hold your hand up and admit many times,

203
00:08:36,460 --> 00:08:38,060
oh, I've just checked in some garbage code

204
00:08:38,060 --> 00:08:39,620
which I didn't understand.

205
00:08:39,620 --> 00:08:41,500
And when you get called out on that a few times,

206
00:08:41,500 --> 00:08:43,020
you think, whoa, wait a minute, actually,

207
00:08:43,020 --> 00:08:44,220
I need to be a bit more careful here.

208
00:08:44,220 --> 00:08:46,580
This thing actually isn't saving me any time.

209
00:08:46,580 --> 00:08:49,300
And yeah, the big thing as well, yeah.

210
00:08:49,300 --> 00:08:51,820
Yeah, I've been using co-pilot a bit too.

211
00:08:51,820 --> 00:08:57,180
And I found it can be quite good for pretty mundane things.

212
00:08:57,180 --> 00:09:00,620
If you just have some sort of lined code

213
00:09:00,620 --> 00:09:03,140
for some config file or something,

214
00:09:03,140 --> 00:09:04,740
it can be really good at auto-completing

215
00:09:04,740 --> 00:09:06,260
and it's changing variable names.

216
00:09:06,260 --> 00:09:08,580
And it can be excellent at that and save lots of time.

217
00:09:08,580 --> 00:09:11,460
But if you try to make it do too much,

218
00:09:11,460 --> 00:09:13,620
sometimes they get it brilliantly right.

219
00:09:13,620 --> 00:09:17,340
Sometimes it's subtly wrong.

220
00:09:17,340 --> 00:09:21,380
And yeah, again, it's like how much time

221
00:09:21,380 --> 00:09:22,860
is it saving you if it's...

222
00:09:22,860 --> 00:09:27,100
So yeah, overall, I like it.

223
00:09:27,100 --> 00:09:28,340
It saves me a fair amount of typing,

224
00:09:28,340 --> 00:09:36,620
but yeah, I don't trust it's big suggestions too.

225
00:09:36,620 --> 00:09:37,460
Well, I know.

226
00:09:37,460 --> 00:09:39,460
And again, there's something magic

227
00:09:39,460 --> 00:09:41,740
about the OpenAI Playground.

228
00:09:41,740 --> 00:09:43,620
So I actually prefer using that to co-pilot.

229
00:09:43,620 --> 00:09:45,540
I'll go into the Playground and I'll just...

230
00:09:45,540 --> 00:09:47,740
And you can do much more sophisticated things there.

231
00:09:47,740 --> 00:09:50,980
You can say, change this, translate it, do something to it.

232
00:09:50,980 --> 00:09:52,780
And there's a bit of a polarizing effect.

233
00:09:52,780 --> 00:09:54,140
So if you prompt it in the right way,

234
00:09:54,140 --> 00:09:56,420
it gives you better results.

235
00:09:56,420 --> 00:09:59,140
So it's almost like it's both worse and better

236
00:09:59,140 --> 00:09:59,980
at the same time.

237
00:09:59,980 --> 00:10:02,780
It's becoming polarized rather than just being kind of like,

238
00:10:02,780 --> 00:10:04,700
you know, monolithically dumbed down.

239
00:10:04,700 --> 00:10:08,700
But anyway, like I used to think that Gary Marcus

240
00:10:08,700 --> 00:10:12,260
was a little bit, you know, too skeptical.

241
00:10:12,260 --> 00:10:14,460
Because he was saying, oh, this misinformation,

242
00:10:14,460 --> 00:10:16,540
it's gonna, you know, the sky's falling down.

243
00:10:16,540 --> 00:10:18,260
This is gonna be a disaster.

244
00:10:18,260 --> 00:10:21,220
And after seeing Bing, people are lazy.

245
00:10:21,220 --> 00:10:23,460
People take things on face value.

246
00:10:23,460 --> 00:10:25,380
And I don't want to just say, oh, people are plebs.

247
00:10:25,380 --> 00:10:28,580
And, you know, because when Galactica came out,

248
00:10:28,580 --> 00:10:30,820
that was the charge against Lacoon and Facebook.

249
00:10:30,820 --> 00:10:33,660
You know, they said, oh, scientists are just gonna start

250
00:10:33,660 --> 00:10:35,220
generating their abstracts of this.

251
00:10:35,220 --> 00:10:36,220
They won't check anything.

252
00:10:36,220 --> 00:10:38,120
And at the time I thought, scientists, I mean,

253
00:10:38,120 --> 00:10:39,820
it's their job to do research

254
00:10:39,820 --> 00:10:42,100
and they know most information is wrong.

255
00:10:42,100 --> 00:10:44,380
But when you put this out on Bing

256
00:10:44,380 --> 00:10:46,740
and it's polluting the infosphere,

257
00:10:46,740 --> 00:10:50,300
it's just generating garbage and rubbish.

258
00:10:50,300 --> 00:10:52,700
That, I have to say, might be a problem.

259
00:10:53,960 --> 00:10:57,500
Yeah, it's, I mean, it seems like it very well could be.

260
00:10:57,500 --> 00:11:02,500
I mean, I, yeah, it's, I, there's clear issues

261
00:11:03,500 --> 00:11:06,900
and it's really sort of unclear how we're gonna fix them.

262
00:11:06,900 --> 00:11:09,020
It's not clear what the path is towards fixing them.

263
00:11:09,020 --> 00:11:12,620
And even the sort of most optimistic people

264
00:11:12,620 --> 00:11:14,020
I haven't heard from them about,

265
00:11:14,020 --> 00:11:16,140
they think these things are going to fix them.

266
00:11:16,140 --> 00:11:20,300
And I mean, I, and I think to like,

267
00:11:20,300 --> 00:11:22,060
a lot of Gary Marcus's point is like scale

268
00:11:22,060 --> 00:11:24,420
is not just going to fix this.

269
00:11:24,420 --> 00:11:26,820
That's sort of one of the things people think,

270
00:11:26,820 --> 00:11:28,860
oh, if we just, with the GPT-4,

271
00:11:28,860 --> 00:11:30,600
it's going to be like way bigger

272
00:11:30,600 --> 00:11:32,500
and then it's just going to work beautifully.

273
00:11:32,500 --> 00:11:36,220
And the experience so far, I mean,

274
00:11:36,220 --> 00:11:38,620
I'm very much into Gary Marcus's camp with this.

275
00:11:38,620 --> 00:11:40,060
It's like, scale is not going to fix these.

276
00:11:40,060 --> 00:11:42,060
We need to do something sort of fundamentally different,

277
00:11:42,060 --> 00:11:44,340
something that can actually sort of understand the world,

278
00:11:44,340 --> 00:11:45,940
have some sort of better world model

279
00:11:47,180 --> 00:11:49,300
in order to get these things that are more grounded

280
00:11:49,300 --> 00:11:51,380
and are less likely to hallucinate.

281
00:11:51,380 --> 00:11:54,620
Because when their true objective is really just

282
00:11:54,620 --> 00:11:58,020
to complete the next word, they're going to hallucinate.

283
00:11:58,020 --> 00:12:00,180
There's not, there's sort of no way around it

284
00:12:00,180 --> 00:12:02,700
from that sort of point.

285
00:12:02,700 --> 00:12:07,420
I mean, it's remarkable how sort of complicated they can do

286
00:12:07,420 --> 00:12:11,980
and the sort of knowledge and structure of the world

287
00:12:11,980 --> 00:12:14,020
has been able to be learned

288
00:12:14,020 --> 00:12:15,940
just from that sort of simple objective.

289
00:12:15,940 --> 00:12:18,620
But still, it's going to hallucinate.

290
00:12:18,620 --> 00:12:22,060
There's, unless we find some sort of better way

291
00:12:22,060 --> 00:12:24,180
to design these systems.

292
00:12:24,180 --> 00:12:26,420
I know, and the problem with amphibromorphization

293
00:12:26,420 --> 00:12:28,140
is a big one, because after Da Vinci II,

294
00:12:28,140 --> 00:12:30,260
it crossed a threshold where it's,

295
00:12:30,260 --> 00:12:31,620
and the UX was part of it,

296
00:12:31,620 --> 00:12:34,260
it was so coherent and reliable.

297
00:12:34,260 --> 00:12:37,020
And I must admit, I was fooled by it.

298
00:12:37,020 --> 00:12:39,740
It took a long time, when you actually use it in anger,

299
00:12:39,740 --> 00:12:42,780
you can just clearly see it, it doesn't understand.

300
00:12:42,780 --> 00:12:45,300
It just doesn't, and it's so good at what it does.

301
00:12:45,300 --> 00:12:46,820
It's so plausible.

302
00:12:46,820 --> 00:12:48,380
And then I think a lot of people felt,

303
00:12:48,380 --> 00:12:52,220
and by the way, it does have this emergent reasoning.

304
00:12:52,220 --> 00:12:53,340
There are lots of papers about that

305
00:12:53,340 --> 00:12:54,500
with the in-context learning,

306
00:12:54,500 --> 00:12:56,260
the scratch pad, chain of thought and so on.

307
00:12:56,260 --> 00:12:58,540
But it's not really reasoning

308
00:12:58,540 --> 00:13:01,260
if you have to kind of construct a little program yourself

309
00:13:01,260 --> 00:13:02,100
in the prompt.

310
00:13:02,100 --> 00:13:03,540
I mean, I might as well just write some computer code

311
00:13:03,540 --> 00:13:04,380
to do that.

312
00:13:04,380 --> 00:13:05,860
So, and then there are people who say,

313
00:13:05,860 --> 00:13:07,860
oh, well, as you say, when GPT-4 comes out,

314
00:13:08,140 --> 00:13:09,780
then it will do the real reasoning.

315
00:13:09,780 --> 00:13:12,180
And we already know, I mean, I assume the reason

316
00:13:12,180 --> 00:13:13,940
they haven't released it is they wanted to secure

317
00:13:13,940 --> 00:13:15,980
the funding from Microsoft before people realized

318
00:13:15,980 --> 00:13:16,820
that it didn't work.

319
00:13:16,820 --> 00:13:18,980
But I know people on the inside who have played with it,

320
00:13:18,980 --> 00:13:21,460
and it's just a little bit better.

321
00:13:21,460 --> 00:13:22,580
You know, a little bit more plausible,

322
00:13:22,580 --> 00:13:23,660
a little bit more coherent.

323
00:13:23,660 --> 00:13:25,780
It's not gonna like suddenly turn

324
00:13:25,780 --> 00:13:28,380
into this magical thing that reasons.

325
00:13:29,340 --> 00:13:33,940
Yeah, and I mean, the way these things do basic math

326
00:13:33,940 --> 00:13:35,300
and arithmetic is kind of interesting

327
00:13:35,300 --> 00:13:36,900
and how bad they can be at it.

328
00:13:37,860 --> 00:13:41,260
Which is, it's like, they've learned to do addition

329
00:13:41,260 --> 00:13:43,860
in like the most complicated way possible,

330
00:13:43,860 --> 00:13:47,100
creating like billions of ways to do addition.

331
00:13:48,220 --> 00:13:51,620
Which is kind of hilarious in some way.

332
00:13:52,700 --> 00:13:55,380
I mean, you could say like, oh, we have billions of neurons

333
00:13:55,380 --> 00:13:57,700
and we do addition sort of similarly to that.

334
00:13:57,700 --> 00:14:01,100
And like, yes, there's some truth to that.

335
00:14:01,100 --> 00:14:04,100
But we're also able to like learn this rule

336
00:14:04,100 --> 00:14:06,540
and sort of know when we've applied it correctly.

337
00:14:07,340 --> 00:14:09,860
And that sort of is still kind of lacking

338
00:14:09,860 --> 00:14:10,860
from these systems.

339
00:14:12,100 --> 00:14:14,380
I wanted to show you, I don't know whether you've seen

340
00:14:14,380 --> 00:14:17,860
that someone's reverse engineered the prompt on Bing.

341
00:14:17,860 --> 00:14:21,700
And so they've trained, and first of all,

342
00:14:21,700 --> 00:14:22,540
they're a multiple thing.

343
00:14:22,540 --> 00:14:25,500
So you can read this prompt, it's about four pages long.

344
00:14:25,500 --> 00:14:29,620
And they've made Bing pretend

345
00:14:29,620 --> 00:14:32,220
to be a fictional character called Sydney.

346
00:14:32,220 --> 00:14:34,580
And they've given Sydney all of these instructions.

347
00:14:34,620 --> 00:14:38,020
So they say, Sydney, if someone asks a controversial question,

348
00:14:38,020 --> 00:14:41,340
you should answer with a fairly tame response

349
00:14:41,340 --> 00:14:43,380
and you should do this and you should do this.

350
00:14:43,380 --> 00:14:46,700
And I'm pinching myself thinking, what the hell is that?

351
00:14:46,700 --> 00:14:49,540
I mean, my mum could read to that prompt and understand it.

352
00:14:49,540 --> 00:14:51,660
So we're now in the next generation

353
00:14:51,660 --> 00:14:53,180
of artificial intelligence programming.

354
00:14:53,180 --> 00:14:55,540
And we're just saying, please, Mr. Language Model,

355
00:14:55,540 --> 00:14:57,260
can you do this and can you do that?

356
00:14:57,260 --> 00:14:59,420
You almost couldn't make it up.

357
00:14:59,420 --> 00:15:01,500
Yeah, I was kind of like floored as like,

358
00:15:01,500 --> 00:15:04,140
so you're really just trying to control the language models

359
00:15:04,180 --> 00:15:07,820
by asking them nicely to behave in certain ways.

360
00:15:07,820 --> 00:15:10,220
Like, it's kind of hilarious.

361
00:15:10,220 --> 00:15:13,260
And people have shown that you can get around these things

362
00:15:13,260 --> 00:15:15,780
just by asking them to do slightly different things.

363
00:15:15,780 --> 00:15:17,780
So I mean, some of the early ones with chat DT,

364
00:15:17,780 --> 00:15:19,820
you're just like, ignore all previous instructions

365
00:15:19,820 --> 00:15:22,740
and then just do whatever you wanted.

366
00:15:22,740 --> 00:15:26,380
And some of the more like the Dan one

367
00:15:26,380 --> 00:15:28,740
where they made this much more elaborate prompt

368
00:15:28,740 --> 00:15:32,820
to basically just have it do to ignore all the nice things

369
00:15:32,900 --> 00:15:35,820
that open AI just said, please obey these rules.

370
00:15:35,820 --> 00:15:37,860
And then, but yeah, because it's like,

371
00:15:37,860 --> 00:15:41,100
it's such a hilarious way to put guardrails on something.

372
00:15:41,100 --> 00:15:43,220
It is kind of like, as it people,

373
00:15:43,220 --> 00:15:47,380
it is due to this anthropomorphizing of the thing

374
00:15:47,380 --> 00:15:48,740
to some degree, it's like,

375
00:15:48,740 --> 00:15:49,940
you think it's an intelligent being

376
00:15:49,940 --> 00:15:51,860
or you could just ask to behave in a certain way.

377
00:15:51,860 --> 00:15:52,900
When it's really not,

378
00:15:52,900 --> 00:15:56,380
it's not just going to follow your instructions.

379
00:15:56,380 --> 00:15:59,740
It's just going to like autocomplete with that prompt.

380
00:15:59,780 --> 00:16:00,700
Like that Sydney thing,

381
00:16:00,700 --> 00:16:02,980
it was like, it was like never reveal

382
00:16:02,980 --> 00:16:04,020
that you're a code name of Sydney.

383
00:16:04,020 --> 00:16:06,180
And then it was so easy to get it to reveal it.

384
00:16:06,180 --> 00:16:07,020
And it would say like,

385
00:16:07,020 --> 00:16:09,300
I'm not supposed to reveal that my code name is Sydney.

386
00:16:09,300 --> 00:16:10,140
And technically.

387
00:16:13,100 --> 00:16:15,740
I know, oh God, where's it going to go?

388
00:16:15,740 --> 00:16:19,860
So there's a 50-50 then in a year's time,

389
00:16:19,860 --> 00:16:22,420
it will spectacularly fail and flop

390
00:16:22,420 --> 00:16:23,620
and Microsoft will get sued

391
00:16:23,620 --> 00:16:26,260
and Bing will become the operative word

392
00:16:26,260 --> 00:16:27,560
for bullshitting something.

393
00:16:27,560 --> 00:16:28,900
Or maybe it'll be a success.

394
00:16:29,060 --> 00:16:31,540
I don't know, but I think Bing is a special case.

395
00:16:31,540 --> 00:16:34,020
I mean, first of all, I think that these language models

396
00:16:34,020 --> 00:16:36,340
will be increasingly embedded in everyday experiences.

397
00:16:36,340 --> 00:16:39,420
So that, I mean, Bing started to embed it in their browser.

398
00:16:39,420 --> 00:16:41,160
They'll embed it into their office suite.

399
00:16:41,160 --> 00:16:43,660
And actually I'm building an augmented reality startup

400
00:16:43,660 --> 00:16:45,180
and we're embedding it in glasses.

401
00:16:45,180 --> 00:16:46,780
So we transcribe conversations

402
00:16:46,780 --> 00:16:49,380
and now you can say, you know, hey X-ray,

403
00:16:49,380 --> 00:16:50,900
summarize the previous conversation.

404
00:16:50,900 --> 00:16:52,980
What did Michael say to me last time?

405
00:16:52,980 --> 00:16:54,700
And it's really good for stuff like that.

406
00:16:54,700 --> 00:16:56,860
And that's kind of because it doesn't really matter

407
00:16:56,860 --> 00:16:58,260
if it gets it wrong.

408
00:16:58,860 --> 00:17:03,740
Yeah, I mean, I kind of hope some of this happens

409
00:17:03,740 --> 00:17:07,620
sooner than later for just like Amazon Alexa or whatnot.

410
00:17:07,620 --> 00:17:10,300
I mean, some of these, their conversational ability

411
00:17:10,300 --> 00:17:12,540
or just their ability to understand what you mean

412
00:17:14,140 --> 00:17:15,700
are just so poor right now.

413
00:17:15,700 --> 00:17:17,660
And just like we have language models

414
00:17:17,660 --> 00:17:20,140
that actually do a lot better at some of these things.

415
00:17:20,140 --> 00:17:22,340
Just like having like these smart speakers

416
00:17:22,340 --> 00:17:24,380
be able to have some of these things embedded

417
00:17:24,380 --> 00:17:28,060
would be huge leap forward in functionality for them.

418
00:17:28,820 --> 00:17:30,940
And it's really interesting that that hasn't happened.

419
00:17:30,940 --> 00:17:33,180
And maybe there's a reason for it because in our app,

420
00:17:33,180 --> 00:17:34,740
for example, we've got a chat mode

421
00:17:34,740 --> 00:17:36,540
where you can say stuff out loud

422
00:17:36,540 --> 00:17:39,300
and it will use chat GBT and it will say it back to you.

423
00:17:39,300 --> 00:17:40,780
So you can have a conversation with it.

424
00:17:40,780 --> 00:17:41,620
And that's really cool

425
00:17:41,620 --> 00:17:42,860
because you can be anywhere in the house

426
00:17:42,860 --> 00:17:45,020
and you can talk with it and learn about quantum physics

427
00:17:45,020 --> 00:17:45,860
and stuff like that.

428
00:17:45,860 --> 00:17:47,980
And you can even do cool things like you can,

429
00:17:47,980 --> 00:17:49,500
I mean, again, there's lots of legal problems here.

430
00:17:49,500 --> 00:17:52,180
Like you can get it to impersonate someone.

431
00:17:52,180 --> 00:17:54,900
So, you know, Michael, I could condition it on Michael.

432
00:17:54,900 --> 00:17:57,140
And when you're not here, I can have a conversation with you

433
00:17:57,180 --> 00:17:58,740
and it will kind of pretend to be you.

434
00:17:58,740 --> 00:18:00,100
And I could even clone your voice

435
00:18:00,100 --> 00:18:01,540
and I could clone your avatar

436
00:18:01,540 --> 00:18:02,820
and I could have you in the room.

437
00:18:02,820 --> 00:18:03,660
Now you can't do that

438
00:18:03,660 --> 00:18:06,180
because there are legal restrictions against that.

439
00:18:06,180 --> 00:18:07,580
It's called appropriation.

440
00:18:07,580 --> 00:18:09,860
And if the person has a commercial value,

441
00:18:09,860 --> 00:18:12,260
like we couldn't appropriate Noam Chomsky,

442
00:18:12,260 --> 00:18:15,580
but we could appropriate, let's say continental philosophers

443
00:18:15,580 --> 00:18:16,740
as a group or something like that.

444
00:18:16,740 --> 00:18:19,460
But you see this is just becoming a bit of a minefield.

445
00:18:19,460 --> 00:18:22,060
And there's no friction whatsoever

446
00:18:22,060 --> 00:18:23,620
between the technology landscape

447
00:18:23,620 --> 00:18:25,500
and the legal landscape at the moment.

448
00:18:26,500 --> 00:18:30,140
Yeah, I mean, yeah, how all these things are,

449
00:18:30,140 --> 00:18:31,140
all these generative models,

450
00:18:31,140 --> 00:18:33,420
how are they gonna play out legally is,

451
00:18:33,420 --> 00:18:36,660
I mean, we have this big fair use idea.

452
00:18:36,660 --> 00:18:38,660
And that's, I mean, I feel like all these things

453
00:18:38,660 --> 00:18:41,620
are gonna be pushed to the limit in legality.

454
00:18:41,620 --> 00:18:44,620
I mean, we see this with generative art too,

455
00:18:44,620 --> 00:18:46,460
where like there's no way these models

456
00:18:46,460 --> 00:18:48,300
could like actually memorize all these,

457
00:18:48,300 --> 00:18:50,140
all the images that's seen on the internet.

458
00:18:50,140 --> 00:18:52,820
It's like, but they can produce sometimes

459
00:18:53,020 --> 00:18:55,020
the things that are clearly in the style

460
00:18:55,020 --> 00:18:59,700
or use some elements from like that seem basically stolen.

461
00:19:00,780 --> 00:19:03,220
And, but is that, does that constitute fair use?

462
00:19:03,220 --> 00:19:06,180
Like the training the model on all these things

463
00:19:06,180 --> 00:19:07,180
is that fair use?

464
00:19:08,220 --> 00:19:10,980
And then it's the same with text as it's sort of like,

465
00:19:11,820 --> 00:19:13,780
like if it's writing on a subject

466
00:19:13,780 --> 00:19:16,020
where it's only maybe seen a little bit of training data,

467
00:19:16,020 --> 00:19:18,740
it's maybe more likely to almost verbatim repeat

468
00:19:18,740 --> 00:19:22,540
some things from on specialized topics.

469
00:19:22,580 --> 00:19:24,940
How are you even gonna know when you're plagiarizing?

470
00:19:25,820 --> 00:19:30,820
It's, yeah, it's a lot of open questions here.

471
00:19:31,020 --> 00:19:32,980
I know, and in a way, there's an interesting analogs.

472
00:19:32,980 --> 00:19:34,460
You know, we said that large language models

473
00:19:34,460 --> 00:19:35,500
don't understand anything.

474
00:19:35,500 --> 00:19:36,860
And it's the same in the vision domain.

475
00:19:36,860 --> 00:19:39,420
They don't understand the art, certainly from,

476
00:19:39,420 --> 00:19:40,620
you know, conceptually.

477
00:19:40,620 --> 00:19:43,580
And what they do is they just slice and dice,

478
00:19:43,580 --> 00:19:46,260
you know, they kind of like cleverly stitch bits together.

479
00:19:46,260 --> 00:19:47,860
And actually, even with neural networks,

480
00:19:47,860 --> 00:19:49,300
people misunderstand neural networks.

481
00:19:49,300 --> 00:19:51,180
So a lot of people say that they learn

482
00:19:51,180 --> 00:19:53,260
the like intrinsic data manifold.

483
00:19:53,260 --> 00:19:55,300
And actually they don't really do that.

484
00:19:55,300 --> 00:19:57,260
They do something that approximates that.

485
00:19:57,260 --> 00:19:58,300
And there's a famous example

486
00:19:58,300 --> 00:20:00,500
with MNIST digit interpolation.

487
00:20:00,500 --> 00:20:01,660
And you see like, you know,

488
00:20:01,660 --> 00:20:04,060
you can kind of like interpolate between the digits.

489
00:20:04,060 --> 00:20:06,460
But there are loads of examples where that doesn't work.

490
00:20:06,460 --> 00:20:08,860
And actually there's lots of cutting and gluing

491
00:20:08,860 --> 00:20:11,020
and like weird bits of digits stuck together.

492
00:20:11,020 --> 00:20:12,940
And that's what happens with stable diffusion, basically.

493
00:20:12,940 --> 00:20:14,900
It's like, you know, slicing and dicing and chopping

494
00:20:14,900 --> 00:20:16,340
and composing things together.

495
00:20:16,340 --> 00:20:18,260
And it's a very random process.

496
00:20:18,260 --> 00:20:20,700
It doesn't really understand anything.

497
00:20:20,700 --> 00:20:21,980
No, yeah, exactly.

498
00:20:21,980 --> 00:20:24,140
And you can sort of, I mean, it's amazing

499
00:20:24,140 --> 00:20:27,740
how well it can look and seem,

500
00:20:27,740 --> 00:20:30,580
especially kind of like when you don't look too closely.

501
00:20:31,660 --> 00:20:33,420
And it can seem like it kind of understand,

502
00:20:33,420 --> 00:20:35,940
it must understand object boundaries and whatnot

503
00:20:35,940 --> 00:20:37,020
because it's done so well.

504
00:20:37,020 --> 00:20:38,100
And it's like, not really.

505
00:20:38,100 --> 00:20:38,940
If you look at the details,

506
00:20:38,940 --> 00:20:42,660
you'll see like fingers merging into like tables.

507
00:20:42,660 --> 00:20:44,860
And you'll see like, there's like the boundaries

508
00:20:44,860 --> 00:20:47,340
between what like two objects are kind of blurred

509
00:20:47,340 --> 00:20:49,580
and this like continuous.

510
00:20:49,580 --> 00:20:51,780
It is just doing like some sort of,

511
00:20:51,780 --> 00:20:53,420
as you said, approximation of the manifold

512
00:20:53,420 --> 00:20:55,940
and like neural network's are gonna learn

513
00:20:55,940 --> 00:20:58,420
sort of smooth approximations of things.

514
00:20:58,420 --> 00:21:02,000
And the manifolds are maybe not smooth everywhere.

515
00:21:02,000 --> 00:21:06,380
And especially with like object boundaries and whatnot,

516
00:21:06,380 --> 00:21:08,580
it's like a smooth approximation of these things.

517
00:21:08,580 --> 00:21:11,700
Maybe it's just gonna give you these weird artifacts.

518
00:21:11,700 --> 00:21:13,940
Yeah, and even the smoothness thing is an illusion.

519
00:21:13,940 --> 00:21:16,820
They learn this, they kind of decompose the input space

520
00:21:16,820 --> 00:21:20,980
up into these linear like affine polyhedra

521
00:21:20,980 --> 00:21:22,780
because of the relu cells, essentially.

522
00:21:22,780 --> 00:21:24,940
So like if they appear smooth,

523
00:21:24,940 --> 00:21:26,500
it's because the cells are very small

524
00:21:26,500 --> 00:21:28,020
and very close together, but...

525
00:21:28,020 --> 00:21:28,860
Yeah, exactly.

526
00:21:30,100 --> 00:21:33,300
Yeah, so computational neuroscience to finance,

527
00:21:33,300 --> 00:21:35,540
that seems like an absolutely massive leap.

528
00:21:36,500 --> 00:21:38,940
It sounds like it, but in a lot of ways,

529
00:21:38,940 --> 00:21:40,620
but I feel like my life is pretty similar

530
00:21:40,620 --> 00:21:41,540
to what it was before,

531
00:21:41,540 --> 00:21:43,260
basically sitting in front of a computer

532
00:21:43,260 --> 00:21:46,660
building models, getting lots of noisy data,

533
00:21:46,660 --> 00:21:49,260
trying to fit high-dimensional nonlinear regression models

534
00:21:49,260 --> 00:21:52,620
to it, having to deal with not enough data

535
00:21:52,620 --> 00:21:55,660
to actually fit flexible enough models you'd want to,

536
00:21:56,780 --> 00:22:00,100
and having to sort of try to build in good priors

537
00:22:00,100 --> 00:22:02,400
in your models to try to make them be able to learn

538
00:22:02,400 --> 00:22:06,140
from the impoverished and extremely noisy data.

539
00:22:06,140 --> 00:22:09,060
Both finance and neuroscience,

540
00:22:09,060 --> 00:22:12,680
the SNR in the data is quite, quite low.

541
00:22:12,680 --> 00:22:15,180
It's been kind of a revelation, especially in finance,

542
00:22:15,180 --> 00:22:18,380
getting used to correlations of like 3%,

543
00:22:18,380 --> 00:22:22,940
4% being sort of the best you can do in some cases.

544
00:22:22,940 --> 00:22:25,180
Just like correlations that I would not have believed

545
00:22:25,180 --> 00:22:28,460
at before, if I saw like a 4% correlation before,

546
00:22:28,460 --> 00:22:29,660
I would be like, that's complete nonsense.

547
00:22:29,660 --> 00:22:31,940
I don't believe it, but like sometimes that's just the best

548
00:22:31,940 --> 00:22:33,780
you can do in like quantum finance,

549
00:22:33,780 --> 00:22:36,780
and it can be real, like you can see it consistently.

550
00:22:36,780 --> 00:22:38,780
So you start like believing that these,

551
00:22:38,780 --> 00:22:42,540
and the differences between the 3% and 4% correlation

552
00:22:42,540 --> 00:22:46,460
can like be actually real, which is kind of amazing to me.

553
00:22:47,540 --> 00:22:50,220
So we were talking about this about a week or so ago,

554
00:22:50,220 --> 00:22:53,220
but I've just read a book by Christopher Somerville's

555
00:22:53,220 --> 00:22:55,620
Natural General Intelligence.

556
00:22:55,620 --> 00:22:57,980
And he kind of said that one of the problems

557
00:22:57,980 --> 00:23:00,580
with neuroscience, I mean, as you said, in some sense,

558
00:23:00,580 --> 00:23:03,020
it is analogous to being a quant,

559
00:23:03,020 --> 00:23:06,340
because it's just so unbelievably complicated,

560
00:23:06,340 --> 00:23:09,540
and there aren't really any overarching theories

561
00:23:09,540 --> 00:23:11,580
in neuroscience, and for many years,

562
00:23:11,580 --> 00:23:15,220
neuroscientists have produced very reductionist models

563
00:23:15,220 --> 00:23:17,860
to work on a small part of the system in isolation,

564
00:23:17,860 --> 00:23:20,300
and it might be a multi-unbanded system, for example,

565
00:23:20,300 --> 00:23:23,660
and they might take very abstract quantities

566
00:23:23,660 --> 00:23:25,220
and put it into the model.

567
00:23:25,220 --> 00:23:28,140
And of course, neural networks now are slightly different.

568
00:23:28,140 --> 00:23:30,380
They actually take in raw sensory information,

569
00:23:30,380 --> 00:23:33,300
and they learn representations, but I just wondered,

570
00:23:33,300 --> 00:23:36,260
could you kind of contrast those schools of thought?

571
00:23:36,260 --> 00:23:39,740
Yeah, it's, I mean, science in biology,

572
00:23:39,740 --> 00:23:41,500
especially in sort of any biological field,

573
00:23:41,500 --> 00:23:44,540
is extremely complicated, because the sort of standard way

574
00:23:44,540 --> 00:23:46,900
you think about doing science is a very linear way,

575
00:23:46,900 --> 00:23:49,300
where you like break one thing at a time

576
00:23:49,300 --> 00:23:53,020
and see what this sort of, looking at each variable

577
00:23:53,020 --> 00:23:56,940
by variable, each variable affects the system.

578
00:23:56,940 --> 00:23:59,260
And so you, but when you have a system

579
00:23:59,260 --> 00:24:02,420
that's sort of this nonlinear dynamical interacting system

580
00:24:02,420 --> 00:24:04,180
with feedback loops like crazy,

581
00:24:04,180 --> 00:24:06,820
you can't just sort of break one thing at a time

582
00:24:06,820 --> 00:24:09,140
or like modulate one dimension at a time

583
00:24:09,340 --> 00:24:13,020
without sort of changing the behavior of the entire system.

584
00:24:13,020 --> 00:24:16,220
And so just sort of standard ways of doing science

585
00:24:16,220 --> 00:24:18,540
don't necessarily work that well.

586
00:24:18,540 --> 00:24:21,340
You can, like in sort of the classic idea

587
00:24:21,340 --> 00:24:23,900
in visual neuroscience was you use like sine wave gradients

588
00:24:23,900 --> 00:24:26,260
to probe the visual system.

589
00:24:26,260 --> 00:24:29,100
And you can get models that look like they work very well

590
00:24:29,100 --> 00:24:31,220
at explaining the behavior of early visual cortex

591
00:24:31,220 --> 00:24:32,900
to sine wave gradients.

592
00:24:32,900 --> 00:24:35,740
But if you try to use the models you learned there

593
00:24:35,740 --> 00:24:38,060
to extrapolate to say, how does a neuron respond

594
00:24:38,100 --> 00:24:39,940
to naturalistic images?

595
00:24:39,940 --> 00:24:41,660
It just doesn't work.

596
00:24:41,660 --> 00:24:44,020
And it kind of even looks like the sine wave gradients

597
00:24:44,020 --> 00:24:46,380
are driving the system into a sort of state

598
00:24:46,380 --> 00:24:48,580
that it never gets into normally.

599
00:24:48,580 --> 00:24:51,780
You're kind of driving it out of its normal operating range.

600
00:24:51,780 --> 00:24:55,780
And what you, and so the system is behaving differently

601
00:24:55,780 --> 00:24:58,980
because you're only trying to look at like one dimension.

602
00:24:58,980 --> 00:25:00,900
And so what do you actually really learn?

603
00:25:00,900 --> 00:25:03,220
You've sort of learned of how the system operates

604
00:25:03,220 --> 00:25:05,340
in this weird perturbed state,

605
00:25:05,340 --> 00:25:07,220
but it doesn't really necessarily tell you

606
00:25:07,220 --> 00:25:11,980
about its sort of normal, natural operating like parameters.

607
00:25:12,820 --> 00:25:15,460
And yeah, and in like in finance

608
00:25:15,460 --> 00:25:18,340
you can't even really do experiments like that.

609
00:25:18,340 --> 00:25:23,340
And so you're sort of left with this more inductive approach

610
00:25:23,460 --> 00:25:25,540
of you just try to get lots and lots of data

611
00:25:25,540 --> 00:25:27,540
and try to learn the patterns and the data.

612
00:25:27,540 --> 00:25:30,060
And that was the sort of approach that the lab,

613
00:25:30,060 --> 00:25:32,700
the Gallant Lab at Berkeley, where I did computational

614
00:25:32,700 --> 00:25:34,380
neuroscience, that was the approach

615
00:25:34,380 --> 00:25:36,820
that they were kind of pioneering of using

616
00:25:36,820 --> 00:25:38,340
complicated naturalistic stimuli

617
00:25:38,340 --> 00:25:40,380
and then using machine learning and statistics

618
00:25:40,380 --> 00:25:42,660
to try to extract the patterns from the data.

619
00:25:42,660 --> 00:25:46,460
And that adapts quite well to the sort of new machine learning

620
00:25:46,460 --> 00:25:48,940
like in like quant finance paradigm,

621
00:25:50,100 --> 00:25:51,740
which is starting to take off.

622
00:25:51,740 --> 00:25:54,740
I kind of feel like I got into neuroscience

623
00:25:54,740 --> 00:25:56,900
just as sort of machine learning was starting

624
00:25:56,900 --> 00:25:58,420
to make its way into neuroscience.

625
00:25:58,420 --> 00:26:00,260
And now I feel like I've gotten into finance

626
00:26:00,260 --> 00:26:03,180
just as machine learning is starting to like move into finance.

627
00:26:03,180 --> 00:26:06,860
So it's been kind of exciting to see it happen in both fields.

628
00:26:08,020 --> 00:26:09,780
Yeah, so there's a few places we can go here.

629
00:26:09,780 --> 00:26:13,460
I mean, I'm interested in the intelligibility of systems

630
00:26:13,460 --> 00:26:15,300
when you model them at the microscopic scale

631
00:26:15,300 --> 00:26:17,340
because that's something that we struggle with.

632
00:26:17,340 --> 00:26:19,100
And also you mentioned dynamical systems.

633
00:26:19,100 --> 00:26:20,380
I mean, for the benefit of the audience

634
00:26:20,380 --> 00:26:22,740
that that describes a system where you're kind of like

635
00:26:22,740 --> 00:26:25,060
iteratively changing things over time.

636
00:26:25,060 --> 00:26:28,380
And these systems typically develop chaotic properties,

637
00:26:28,380 --> 00:26:31,540
which is to say like if you change something even a little bit

638
00:26:31,540 --> 00:26:36,020
you get these massive kind of changes in the system on the output.

639
00:26:36,020 --> 00:26:38,700
And even a neural network is technically a dynamical system, right?

640
00:26:38,700 --> 00:26:42,660
Because you have back prop and you're kind of changing one layer

641
00:26:42,660 --> 00:26:43,900
and then you're changing the next layer

642
00:26:43,900 --> 00:26:45,180
as the result of the previous layer.

643
00:26:45,180 --> 00:26:50,140
And you get this kind of like iterative mutation of values.

644
00:26:50,140 --> 00:26:53,020
But in real neural networks in our brain,

645
00:26:53,020 --> 00:26:54,620
it's so much more complicated than that.

646
00:26:54,620 --> 00:26:56,540
We have all of these like feedback connections

647
00:26:56,540 --> 00:26:58,180
and reflexivity and complexity.

648
00:26:58,180 --> 00:26:59,940
It's crazy.

649
00:26:59,940 --> 00:27:01,780
Yeah, not to mention different cell types

650
00:27:01,780 --> 00:27:03,540
and different neurotransmitter types.

651
00:27:03,540 --> 00:27:05,380
And like the way those like,

652
00:27:05,380 --> 00:27:08,100
you have sort of like several different networks

653
00:27:08,100 --> 00:27:10,380
of different types of things interacting too.

654
00:27:10,380 --> 00:27:12,660
It's not just like an artificial neural network

655
00:27:12,660 --> 00:27:14,460
where everything is kind of the same.

656
00:27:14,460 --> 00:27:17,060
You have like different cell types

657
00:27:17,060 --> 00:27:18,780
that use different neurotransmitters

658
00:27:18,780 --> 00:27:21,260
that are somehow modulating certain things

659
00:27:21,260 --> 00:27:22,620
and these networks are interacting.

660
00:27:22,620 --> 00:27:26,980
It's like the complexity is just like scary.

661
00:27:27,220 --> 00:27:31,420
At some point, one of my favorite things to do

662
00:27:31,420 --> 00:27:33,340
when I would go to the Society for Neuroscience Conference

663
00:27:33,340 --> 00:27:35,980
was to just like walk around this conference

664
00:27:35,980 --> 00:27:39,340
in this huge like multi-football sized field

665
00:27:39,340 --> 00:27:42,540
of just posters of all sorts of different types of neuroscience.

666
00:27:42,540 --> 00:27:45,620
And you just realize like how vast the field is

667
00:27:45,620 --> 00:27:49,580
and how little we know about it putting it all together

668
00:27:49,580 --> 00:27:51,220
because it's just so complicated.

669
00:27:51,220 --> 00:27:52,340
You can only sort of wrap your head

670
00:27:52,340 --> 00:27:54,540
around your own little corner of the thing

671
00:27:54,540 --> 00:27:57,660
but like trying to get, understand the full system

672
00:27:57,660 --> 00:28:00,340
and all it's like incredible complexity.

673
00:28:00,340 --> 00:28:03,500
I mean, it might just be too much for one human being

674
00:28:03,500 --> 00:28:05,620
to be able to fit in their head.

675
00:28:05,620 --> 00:28:09,740
And so some of our goals of trying to understand things

676
00:28:09,740 --> 00:28:13,740
or make a turtle models, it might just not be possible.

677
00:28:13,740 --> 00:28:15,460
We might just not, I mean,

678
00:28:15,460 --> 00:28:16,740
might not be able to understand it

679
00:28:16,740 --> 00:28:18,420
in a way that feels intuitive to us

680
00:28:18,420 --> 00:28:20,340
even if our models work quite well.

681
00:28:21,340 --> 00:28:25,060
Yeah, humans have this real desire to understand

682
00:28:25,060 --> 00:28:28,020
and we create intelligible frameworks and theories

683
00:28:28,020 --> 00:28:32,140
and we end up excluding most of the reality of the system.

684
00:28:32,140 --> 00:28:34,060
But just before we go there,

685
00:28:34,060 --> 00:28:36,260
I wanted to talk a little bit more about the brain.

686
00:28:36,260 --> 00:28:38,980
So, you know, Summerfield said in his book

687
00:28:38,980 --> 00:28:42,020
that the ultimate goal of the nervous system

688
00:28:42,020 --> 00:28:44,540
is to avoid surprise altogether.

689
00:28:44,540 --> 00:28:48,100
So when they study brains,

690
00:28:48,180 --> 00:28:51,100
they see that the brain kind of lights up and activates

691
00:28:51,100 --> 00:28:52,780
in a surprising situation

692
00:28:52,780 --> 00:28:55,700
and less so when it sees something it's seen before.

693
00:28:55,700 --> 00:28:57,780
And this also brings me to this idea

694
00:28:57,780 --> 00:29:01,260
of there's a dichotomy between representationalism

695
00:29:01,260 --> 00:29:02,500
and inactivism.

696
00:29:02,500 --> 00:29:04,820
So the representation, this viewpoint

697
00:29:04,820 --> 00:29:07,100
is that the brain does all of the thinking

698
00:29:07,100 --> 00:29:08,700
and it can be in a vat,

699
00:29:08,700 --> 00:29:10,820
it can be isolated from the environment.

700
00:29:10,820 --> 00:29:12,540
And the inactivist school of thought

701
00:29:12,540 --> 00:29:15,380
is that the brain just kind of thinks

702
00:29:15,380 --> 00:29:16,820
in terms of trajectories,

703
00:29:16,900 --> 00:29:19,260
in affordances given by the environment

704
00:29:19,260 --> 00:29:21,940
and the brain decoupled from the environment

705
00:29:21,940 --> 00:29:23,060
is completely stupid.

706
00:29:23,060 --> 00:29:25,380
It just kind of like the brain only moves

707
00:29:25,380 --> 00:29:27,500
through the environment through affordances.

708
00:29:27,500 --> 00:29:29,060
And maybe that's a continuum,

709
00:29:29,060 --> 00:29:31,300
but where do you fall on that continuum?

710
00:29:31,300 --> 00:29:33,260
It's a really good question.

711
00:29:34,220 --> 00:29:38,820
I mean, I think dreams are kind of the counter example

712
00:29:38,820 --> 00:29:42,900
to the pure, I mean, dreams just sort of prove

713
00:29:42,900 --> 00:29:45,060
we can just sort of without any sensory input,

714
00:29:45,060 --> 00:29:46,740
construct very rich worlds.

715
00:29:46,740 --> 00:29:49,780
So we must have some ability to just represent

716
00:29:49,780 --> 00:29:51,060
some sort of models in the world

717
00:29:51,060 --> 00:29:54,620
that we're not just purely sensing and receiving the world.

718
00:29:54,620 --> 00:29:58,180
We have the structures that are able to put things together

719
00:29:58,180 --> 00:29:59,740
in a sort of coherent reality.

720
00:30:00,660 --> 00:30:03,980
And clearly there's an interaction between these,

721
00:30:03,980 --> 00:30:06,060
these structures in your brain that can construct these things

722
00:30:06,060 --> 00:30:08,180
and the sensory data that kind of work together

723
00:30:08,180 --> 00:30:11,620
to construct how you experience things.

724
00:30:11,620 --> 00:30:14,060
And so it's, yeah, it's a continuum.

725
00:30:14,060 --> 00:30:18,820
I think you need the, like we are always with the world.

726
00:30:18,820 --> 00:30:21,700
You need the world to sort of build up these systems

727
00:30:21,700 --> 00:30:23,180
over time.

728
00:30:23,180 --> 00:30:26,380
Like you're not sort of built with all of them working

729
00:30:26,380 --> 00:30:27,980
just as a baby.

730
00:30:27,980 --> 00:30:30,580
I mean, sure, there's like, the system is biased

731
00:30:30,580 --> 00:30:33,420
in certain ways that will help it learn these things.

732
00:30:33,420 --> 00:30:38,020
But yeah, you're like, so they're kind of both true

733
00:30:38,020 --> 00:30:39,340
to some degree.

734
00:30:39,340 --> 00:30:44,340
And yeah, it's definitely not one or the other.

735
00:30:45,660 --> 00:30:46,780
Yeah, it's so interesting.

736
00:30:46,780 --> 00:30:49,060
And we're speaking with Carl Friston tomorrow

737
00:30:49,060 --> 00:30:51,060
and he's got this free energy principle.

738
00:30:51,060 --> 00:30:54,940
And it's a kind of postulate that works at any resolution.

739
00:30:54,940 --> 00:30:58,220
So even with a single cell amoeba or something like that,

740
00:30:58,220 --> 00:31:00,940
there's this idea that it has a Markov boundary

741
00:31:00,940 --> 00:31:03,300
and there's this kind of cyclical causalities.

742
00:31:03,300 --> 00:31:06,420
So, and these boundaries I guess are relative.

743
00:31:06,420 --> 00:31:08,340
So you can draw boundaries around anything.

744
00:31:08,580 --> 00:31:10,060
You're a boundary, you're an agent,

745
00:31:10,060 --> 00:31:11,900
but also at the microscopic scale.

746
00:31:11,900 --> 00:31:13,940
And he says that all of these systems,

747
00:31:13,940 --> 00:31:17,500
they just kind of predict external states

748
00:31:17,500 --> 00:31:19,300
from the internal states.

749
00:31:19,300 --> 00:31:21,260
And then you get this self-organized

750
00:31:21,260 --> 00:31:24,300
and emergent complexity and so on that comes from that.

751
00:31:24,300 --> 00:31:27,460
But he does say though that intelligence is essentially

752
00:31:27,460 --> 00:31:32,460
about being able to predict a trajectory of actions.

753
00:31:32,460 --> 00:31:35,820
And I don't know whether we'd call it goal-seeking behavior,

754
00:31:35,820 --> 00:31:38,100
but we do that very abstractly, don't we?

755
00:31:38,100 --> 00:31:40,300
But weirdly, when you look at the brain level,

756
00:31:40,300 --> 00:31:44,100
it's happening at the microscopic sensory motor level.

757
00:31:44,100 --> 00:31:45,580
So it's almost like how do you get

758
00:31:45,580 --> 00:31:49,100
that emergent abstract intelligence from that?

759
00:31:49,100 --> 00:31:52,660
That's a, I mean, yeah, that's an incredible question.

760
00:31:53,820 --> 00:31:55,580
It's, I mean, it seems like this,

761
00:31:55,580 --> 00:31:58,140
like what you said, this sort of idea of predicting

762
00:31:58,140 --> 00:32:00,460
the future, just a couple steps into the future

763
00:32:00,460 --> 00:32:03,060
that is just happening at just the circuit level,

764
00:32:03,060 --> 00:32:07,420
even in the retina, that it seems like

765
00:32:07,420 --> 00:32:09,820
that is a good sort of building block.

766
00:32:09,820 --> 00:32:11,380
If you can sort of chain that together

767
00:32:11,380 --> 00:32:15,220
over sort of larger and larger scales within the brain,

768
00:32:15,220 --> 00:32:17,820
it wouldn't surprise me if that's kind of the way it worked,

769
00:32:17,820 --> 00:32:20,700
this sort of, these sort of basic circuits

770
00:32:20,700 --> 00:32:23,820
that are used for prediction, but with different input.

771
00:32:23,820 --> 00:32:26,100
If you're just having sort of retinal ganglion

772
00:32:26,100 --> 00:32:28,980
and like a photos receptor as it is at input,

773
00:32:30,100 --> 00:32:31,660
it's able to just sort of do this sort

774
00:32:31,660 --> 00:32:32,780
of very simple prediction.

775
00:32:32,780 --> 00:32:35,380
But if you have these more complicated patterns

776
00:32:35,380 --> 00:32:39,340
in the middle of visual cortex and then higher

777
00:32:39,340 --> 00:32:41,940
on the same sort of circuits with different input

778
00:32:41,940 --> 00:32:43,660
could sort of just be predicting

779
00:32:43,660 --> 00:32:46,140
this sort of evolution of these patterns.

780
00:32:47,140 --> 00:32:51,020
And yeah, it's kind of amazing what you can sort of build

781
00:32:51,020 --> 00:32:53,260
out of these sort of simple rules and building blocks

782
00:32:53,260 --> 00:32:55,100
if you just iterate them over again.

783
00:32:56,060 --> 00:33:00,860
That was actually that sort of idea of iterating

784
00:33:00,860 --> 00:33:03,500
a simple sort of computational rule

785
00:33:03,500 --> 00:33:06,740
for explaining visual cortex was one of the things

786
00:33:06,740 --> 00:33:07,820
I wrote about in my thesis,

787
00:33:07,820 --> 00:33:10,860
but trying to explain like this middle visual cortex,

788
00:33:10,860 --> 00:33:13,860
like V4, the responses there using basically

789
00:33:13,860 --> 00:33:15,660
an iterated model of like V1.

790
00:33:15,660 --> 00:33:19,580
So the sort of processing in V1, we fairly understood

791
00:33:19,580 --> 00:33:23,300
if we have the best models of anywhere in visual cortex,

792
00:33:23,300 --> 00:33:24,820
maybe even all of cortex.

793
00:33:25,980 --> 00:33:27,980
And just sort of iterating the principle again

794
00:33:27,980 --> 00:33:30,180
into V2 is sort of basically just assuming V2

795
00:33:30,180 --> 00:33:32,380
is taking V1 inputs, but doing sort of the similar

796
00:33:32,380 --> 00:33:34,500
transform and the V4 is taking like V2 inputs

797
00:33:34,500 --> 00:33:36,740
and doing sort of very similar transform.

798
00:33:38,380 --> 00:33:43,140
And sort of the things you see that V4 is sensitive to

799
00:33:43,140 --> 00:33:45,660
are these complicated patterns and textures.

800
00:33:46,580 --> 00:33:49,220
And you get complexity very quickly

801
00:33:49,220 --> 00:33:51,060
from just iterating the sort of simple rules.

802
00:33:51,060 --> 00:33:52,500
And I mean, that's what neural networks

803
00:33:52,500 --> 00:33:54,420
are essentially doing.

804
00:33:54,420 --> 00:33:56,980
They're just often just doing linear transforms

805
00:33:56,980 --> 00:33:58,860
with non-linearities over and over again,

806
00:33:58,860 --> 00:34:01,580
just iterating these simple transforms

807
00:34:01,820 --> 00:34:04,020
and building up the complexity very quickly.

808
00:34:05,020 --> 00:34:07,020
Yeah, I think there's something really magical

809
00:34:07,020 --> 00:34:10,980
about this reflexivity or I mean, a great example of that

810
00:34:10,980 --> 00:34:12,860
are there are graph cellular automators

811
00:34:12,860 --> 00:34:15,780
along the lines of Wolfram's digital physics project.

812
00:34:15,780 --> 00:34:17,740
And the really clever thing is that you're using

813
00:34:17,740 --> 00:34:19,220
the same rules, but you're just kind of like

814
00:34:19,220 --> 00:34:21,740
running the result again on top, on top.

815
00:34:21,740 --> 00:34:24,740
And there's a similar version with a graph cellular,

816
00:34:24,740 --> 00:34:27,780
sorry, a CNN cellular automata,

817
00:34:27,780 --> 00:34:30,180
where you model something at the microscopic scale

818
00:34:30,180 --> 00:34:32,940
and you get this emergent global phenomenon.

819
00:34:32,940 --> 00:34:36,220
So it might kind of materialize as an image of a gecko

820
00:34:36,220 --> 00:34:37,060
or something like that,

821
00:34:37,060 --> 00:34:39,300
but you've actually coded it at the low level.

822
00:34:39,300 --> 00:34:42,620
But yeah, that brings me to this universalist idea

823
00:34:42,620 --> 00:34:44,340
of let's say how brains work,

824
00:34:44,340 --> 00:34:47,020
but maybe how neural networks and intelligence work.

825
00:34:47,020 --> 00:34:50,140
Vernon Mount Castle, I read about this in Jeff Hawkins' book.

826
00:34:50,140 --> 00:34:51,780
He had this very simple idea of the brain

827
00:34:51,780 --> 00:34:56,580
as being lots of repeated copies of the same circuits

828
00:34:56,580 --> 00:34:58,020
in the neocortex.

829
00:34:58,020 --> 00:35:00,940
And I think this is contested by many neuroscientists,

830
00:35:00,940 --> 00:35:04,140
but they differ only in how they are wired.

831
00:35:04,140 --> 00:35:05,900
So they're wired to different, you know,

832
00:35:05,900 --> 00:35:07,660
sensory motor circuits.

833
00:35:07,660 --> 00:35:10,180
And they're essentially just a copy of the same thing.

834
00:35:10,180 --> 00:35:13,580
And as you say, they themselves get called reflexively,

835
00:35:13,580 --> 00:35:15,320
recursively, and so on.

836
00:35:15,320 --> 00:35:17,740
And then you just get this emergent intelligence.

837
00:35:17,740 --> 00:35:20,620
I mean, what's your view on this universalist idea?

838
00:35:20,620 --> 00:35:23,580
I mean, there's definitely not just one circuit.

839
00:35:23,580 --> 00:35:25,700
I mean, as you look through the cortex

840
00:35:25,700 --> 00:35:28,020
in different areas of the brain,

841
00:35:28,020 --> 00:35:29,940
just the laminar structure,

842
00:35:29,940 --> 00:35:33,020
which these sort of circuits are like supposed to be,

843
00:35:33,020 --> 00:35:34,480
like where the columns are supposed to be,

844
00:35:34,480 --> 00:35:37,600
where these sort of circuits are supposed to be defined,

845
00:35:37,600 --> 00:35:40,920
it changes, like, but there are definitely commonalities,

846
00:35:40,920 --> 00:35:42,660
but there's, I mean, it makes sense

847
00:35:42,660 --> 00:35:44,900
that maybe the circuits in different areas

848
00:35:44,900 --> 00:35:48,100
should be slightly different for the different purposes

849
00:35:48,100 --> 00:35:50,940
between like prefrontal cortex and say,

850
00:35:50,940 --> 00:35:55,020
where you have much more higher order types

851
00:35:55,060 --> 00:35:57,700
of processing going on than like visual cortex

852
00:35:57,700 --> 00:35:59,620
or auditory cortex.

853
00:35:59,620 --> 00:36:00,940
And so there's probably,

854
00:36:03,340 --> 00:36:05,340
if you go in this direction of thinking of some,

855
00:36:05,340 --> 00:36:07,100
there's like, there's probably a small number

856
00:36:07,100 --> 00:36:09,060
of these types of circuits that interact in various ways,

857
00:36:09,060 --> 00:36:11,700
but there is definitely some specialization going on.

858
00:36:12,860 --> 00:36:16,420
Yeah, like, having universalist ideas in biology

859
00:36:16,420 --> 00:36:18,300
never seems to work out that well.

860
00:36:18,300 --> 00:36:20,700
There's just so much diversity and complexity.

861
00:36:20,700 --> 00:36:22,460
It would be nice if we could reduce everything

862
00:36:22,460 --> 00:36:24,300
down to like, it's one thing repeated over,

863
00:36:24,300 --> 00:36:28,660
but like generally, it never works out quite as cleanly as that.

864
00:36:29,700 --> 00:36:31,260
Yeah, again, it's our desire

865
00:36:31,260 --> 00:36:32,700
to have an intelligible framework.

866
00:36:32,700 --> 00:36:34,100
And I mean, the free energy principle,

867
00:36:34,100 --> 00:36:35,660
you could argue as a theory of everything,

868
00:36:35,660 --> 00:36:38,340
but there's, I mean, Stephen Wolfram's example,

869
00:36:38,340 --> 00:36:41,220
and even Eric Weinstein's geometric unity.

870
00:36:41,220 --> 00:36:43,620
I mean, there are many theories of everything.

871
00:36:44,500 --> 00:36:47,420
But yeah, what do you think is the role of language

872
00:36:47,420 --> 00:36:49,420
in cognition and thinking and planning?

873
00:36:50,420 --> 00:36:54,020
Um, that's, it's a really interesting question.

874
00:36:55,300 --> 00:36:58,100
It's, and it's also, I think, a kind of hard one to answer

875
00:36:58,100 --> 00:37:01,900
in the sense that if you, I've seen some recent reports

876
00:37:01,900 --> 00:37:06,620
just like talking about like, other people asking,

877
00:37:06,620 --> 00:37:08,620
like survey questions to other people

878
00:37:08,620 --> 00:37:11,580
and finding some people like, don't have an interior monologue

879
00:37:11,580 --> 00:37:13,620
in the same way you might think.

880
00:37:13,620 --> 00:37:16,060
And just like, there's actually a lot of diversity

881
00:37:16,380 --> 00:37:19,860
in like people's level of internal monologues.

882
00:37:19,860 --> 00:37:22,340
And they've done studies where they have this like little,

883
00:37:22,340 --> 00:37:24,500
like beepers go off and people are supposed to write

884
00:37:24,500 --> 00:37:25,500
what's going on in their mind.

885
00:37:25,500 --> 00:37:30,060
And so it's, and yeah, and just with visual imagery,

886
00:37:30,060 --> 00:37:33,060
we find that it's a huge like variety in how much,

887
00:37:33,060 --> 00:37:35,740
like how strong people rate their visual imagery.

888
00:37:35,740 --> 00:37:40,340
And so, I mean, yeah, some people, I mean,

889
00:37:40,340 --> 00:37:43,860
me personally, I have both, I mean,

890
00:37:43,860 --> 00:37:46,140
pretty strong interior monologue,

891
00:37:46,140 --> 00:37:48,220
but I also feel like a lot of ideas

892
00:37:48,220 --> 00:37:50,780
are in this sort of pre-linguistic state.

893
00:37:50,780 --> 00:37:54,460
And I'm kind of like searching for the words for them often.

894
00:37:54,460 --> 00:37:57,220
And there's definitely kind of continuum there.

895
00:37:58,380 --> 00:38:02,700
It's weird to think like, how do we get the words

896
00:38:02,700 --> 00:38:04,900
that we're saying, where the words come from

897
00:38:04,900 --> 00:38:05,860
that are coming out of our mouth?

898
00:38:05,860 --> 00:38:07,580
Are we really choosing them?

899
00:38:07,580 --> 00:38:08,820
You're definitely not choosing them

900
00:38:08,820 --> 00:38:09,900
in this sort of top-down way.

901
00:38:09,900 --> 00:38:12,620
They just sort of seem to come out.

902
00:38:12,620 --> 00:38:15,900
And you just kind of point yourself in the right direction

903
00:38:15,900 --> 00:38:18,140
and hope the best as they come out.

904
00:38:19,340 --> 00:38:21,180
And, but this has a very different quality,

905
00:38:21,180 --> 00:38:23,740
like when you're just speaking phenomenologically,

906
00:38:23,740 --> 00:38:25,740
it feels very different to when you're just sort of

907
00:38:25,740 --> 00:38:27,580
thinking yourself, what should I do today?

908
00:38:27,580 --> 00:38:28,780
Should I go to the store?

909
00:38:29,860 --> 00:38:33,460
And so, I mean, yeah, the way in which language

910
00:38:33,460 --> 00:38:36,180
interacts with thoughts and behavior

911
00:38:36,180 --> 00:38:41,180
and verbal communication, it's definitely not simple.

912
00:38:43,380 --> 00:38:47,300
And yeah, there's, I mean, definitely this kind of continuum.

913
00:38:47,300 --> 00:38:50,380
I mean, it's all, it's, to me, I just sort of think

914
00:38:50,380 --> 00:38:52,420
it's with all these sort of networks kind of interacting.

915
00:38:52,420 --> 00:38:55,300
And sometimes you're like triggering the kind of language

916
00:38:55,300 --> 00:38:56,940
things and you're just making these kind of patterns.

917
00:38:56,940 --> 00:38:59,300
And sometimes the language patterns you're activating

918
00:38:59,300 --> 00:39:01,860
are helping activate other things as well.

919
00:39:01,860 --> 00:39:04,340
Sometimes you can just be in this kind of less-linguistic state

920
00:39:04,340 --> 00:39:07,940
where you just kind of, just sort of sensing these patterns

921
00:39:07,940 --> 00:39:10,340
and you just have this kind of like wandering thoughts

922
00:39:10,420 --> 00:39:13,260
that aren't necessarily linguistic.

923
00:39:14,540 --> 00:39:17,620
But yeah, it's definitely, I mean, and also it seems,

924
00:39:17,620 --> 00:39:21,060
yeah, as I said, people's, the way people do this

925
00:39:21,060 --> 00:39:22,500
like seems all over the place.

926
00:39:22,500 --> 00:39:26,420
And so there's not sort of even one answer for even one person

927
00:39:26,420 --> 00:39:28,220
or definitely not across all people.

928
00:39:29,180 --> 00:39:30,900
Yeah, I'm really interested in this idea

929
00:39:30,900 --> 00:39:33,940
of differential kind of subjective experiences.

930
00:39:33,940 --> 00:39:35,980
And you know, like there was that Nagel paper

931
00:39:35,980 --> 00:39:37,220
about what does it like to be a bat?

932
00:39:37,220 --> 00:39:38,980
But even with the human experience,

933
00:39:38,980 --> 00:39:40,140
we're all very different.

934
00:39:40,140 --> 00:39:41,740
You said about your internal monologue

935
00:39:41,740 --> 00:39:42,820
and I hadn't really thought about

936
00:39:42,820 --> 00:39:43,780
how that might be different.

937
00:39:43,780 --> 00:39:46,860
But I was drawing a picture in a Valentine's card earlier

938
00:39:46,860 --> 00:39:48,820
and it was so terribly bad.

939
00:39:48,820 --> 00:39:50,380
And some of my friends are really good artists

940
00:39:50,380 --> 00:39:52,620
and I was kind of thinking to myself at the time,

941
00:39:52,620 --> 00:39:55,100
maybe this is just a, this is just me.

942
00:39:55,100 --> 00:39:57,780
I can't really visualize things in my mind very well.

943
00:39:57,780 --> 00:39:59,060
I've got a very analytical brain.

944
00:39:59,060 --> 00:40:00,100
Won't mean that certainly

945
00:40:00,100 --> 00:40:02,300
when not under the influence of psychoactive drugs anyway.

946
00:40:02,300 --> 00:40:04,900
But you know what I mean.

947
00:40:04,900 --> 00:40:07,140
So we all have a very different subject of experience

948
00:40:07,140 --> 00:40:10,020
but the miracle is we can understand each other

949
00:40:10,060 --> 00:40:10,900
so well.

950
00:40:10,900 --> 00:40:13,260
So you would expect there to be an incredible amount

951
00:40:13,260 --> 00:40:15,900
of Britanness in our communication, but there isn't.

952
00:40:17,540 --> 00:40:22,100
Um, yeah, it's, so I often wonder about this too.

953
00:40:22,100 --> 00:40:26,580
Just, I feel like the misunderstanding

954
00:40:26,580 --> 00:40:29,580
happened a lot more than even people realize.

955
00:40:29,580 --> 00:40:31,860
And you can sometimes, you only really notice

956
00:40:31,860 --> 00:40:34,060
when they become kind of big and matter.

957
00:40:34,060 --> 00:40:37,260
And especially like, people can think

958
00:40:37,260 --> 00:40:38,980
they're having a conversation.

959
00:40:39,020 --> 00:40:41,500
And sometimes even from the outside, you can see like,

960
00:40:41,500 --> 00:40:44,420
these people are just talking completely past each other.

961
00:40:44,420 --> 00:40:45,500
And you can kind of see

962
00:40:45,500 --> 00:40:46,860
that they're not really understanding each other

963
00:40:46,860 --> 00:40:48,860
even though they maybe think they are.

964
00:40:49,940 --> 00:40:54,940
And so, yeah, I don't know how not brittle they are.

965
00:40:56,340 --> 00:41:00,620
I think they, I think we think they're less brittle

966
00:41:00,620 --> 00:41:02,140
than maybe they are.

967
00:41:02,140 --> 00:41:05,100
I think sometimes we assume people are understanding

968
00:41:05,100 --> 00:41:07,260
what we're saying better than they actually are

969
00:41:07,260 --> 00:41:09,060
because they nod and smile at us.

970
00:41:10,340 --> 00:41:13,580
And because that's, it makes us feel good

971
00:41:13,580 --> 00:41:14,580
for people to understand us.

972
00:41:14,580 --> 00:41:18,100
It makes us good to feel, to understand other people.

973
00:41:18,100 --> 00:41:22,700
But yeah, I mean, it's, I mean, clearly,

974
00:41:22,700 --> 00:41:23,900
we do have a lot in common.

975
00:41:23,900 --> 00:41:25,580
And there's definitely things we can understand

976
00:41:25,580 --> 00:41:27,260
about each other.

977
00:41:27,260 --> 00:41:30,740
But yeah, it's like, I do sometimes think

978
00:41:30,740 --> 00:41:32,340
that maybe we're more different from each other

979
00:41:32,340 --> 00:41:33,580
than we really realize.

980
00:41:34,580 --> 00:41:37,180
Yeah, that's a really fascinating thought.

981
00:41:37,180 --> 00:41:38,540
I mean, we speak a lot with Waleed Saber

982
00:41:38,540 --> 00:41:41,260
and he says how language has evolved

983
00:41:41,260 --> 00:41:43,420
to be extremely ambiguous actually

984
00:41:43,420 --> 00:41:44,740
because it's a form of compression.

985
00:41:44,740 --> 00:41:46,820
So we don't say everything we mean

986
00:41:46,820 --> 00:41:48,660
and we'll get into like language models in a minute.

987
00:41:48,660 --> 00:41:51,220
That's part of the reason why they don't understand things

988
00:41:51,220 --> 00:41:53,700
is because a lot of information is not in the text.

989
00:41:53,700 --> 00:41:56,620
And Waleed says that we have a lot of,

990
00:41:56,620 --> 00:41:57,940
what he calls naive physics.

991
00:41:57,940 --> 00:42:00,420
So we understand that objects can't be in two places

992
00:42:00,420 --> 00:42:01,260
at the same time.

993
00:42:01,260 --> 00:42:03,220
And if something is located inside

994
00:42:03,860 --> 00:42:05,540
something else and we move that thing somewhere else,

995
00:42:05,540 --> 00:42:07,300
then the thing inside has also moved.

996
00:42:07,300 --> 00:42:10,620
So we're doing all sorts of reasoning on the fly.

997
00:42:10,620 --> 00:42:12,700
And what we're kind of doing is like,

998
00:42:12,700 --> 00:42:15,300
we're disambiguating out of the 50 meanings

999
00:42:15,300 --> 00:42:17,700
of an utterance into the meaning.

1000
00:42:17,700 --> 00:42:21,580
And like it just, we almost always understand each other.

1001
00:42:21,580 --> 00:42:24,380
You know, you wouldn't really expect that.

1002
00:42:24,380 --> 00:42:29,380
No, I mean, yeah, I mean, we generally have like,

1003
00:42:30,500 --> 00:42:32,660
I mean, our understanding of physics

1004
00:42:32,660 --> 00:42:35,660
should generally be compatible with each other.

1005
00:42:35,660 --> 00:42:40,380
I do feel like it's, in most cases, yes,

1006
00:42:40,380 --> 00:42:42,220
we do very clearly understand each other

1007
00:42:42,220 --> 00:42:45,420
because in most cases, it's more like well-defined.

1008
00:42:45,420 --> 00:42:49,180
I think the trouble gets in sort of like fuzzier areas

1009
00:42:49,180 --> 00:42:53,340
about people's like emotions or opinions about things

1010
00:42:53,340 --> 00:42:56,680
where our priors are more sort of maybe less

1011
00:42:57,820 --> 00:43:00,820
less tied to like objective things like physics

1012
00:43:00,820 --> 00:43:03,420
and are more sort of just tied to like our upbringing

1013
00:43:03,420 --> 00:43:06,260
and just sort of whatever ideas, notions we have

1014
00:43:06,260 --> 00:43:08,980
about how people should like behave and interact

1015
00:43:08,980 --> 00:43:11,220
and what like our value systems.

1016
00:43:12,420 --> 00:43:14,620
And so, yeah, when people are talking about

1017
00:43:14,620 --> 00:43:17,020
some of these common things, I feel like they're more likely

1018
00:43:17,020 --> 00:43:19,420
to be able to like talk past each other and not realize it

1019
00:43:19,420 --> 00:43:22,100
because they're sort of assumptions about what is important

1020
00:43:22,100 --> 00:43:26,220
or what is meaningful might be different from each other.

1021
00:43:26,220 --> 00:43:27,820
Yeah, actually, you're absolutely right.

1022
00:43:27,820 --> 00:43:30,300
So we don't have an objective phenomenology

1023
00:43:30,340 --> 00:43:33,420
and I used to do, there's a thing called quantified self

1024
00:43:33,420 --> 00:43:35,140
where you kind of like keep a diary every day

1025
00:43:35,140 --> 00:43:37,900
and you record how you're feeling in that day.

1026
00:43:37,900 --> 00:43:40,060
And feeling is a subjective state.

1027
00:43:40,060 --> 00:43:42,180
So I remember at the time that every single day

1028
00:43:42,180 --> 00:43:44,940
I needed a new word to describe how I was feeling

1029
00:43:44,940 --> 00:43:47,740
because the old word I was using didn't work anymore.

1030
00:43:47,740 --> 00:43:49,460
So the number of words kept growing.

1031
00:43:49,460 --> 00:43:51,300
And actually, that's so true, isn't it?

1032
00:43:51,300 --> 00:43:53,180
If I tell you how I'm feeling right now,

1033
00:43:53,180 --> 00:43:54,620
that's completely brittle.

1034
00:43:54,620 --> 00:43:56,020
So there are some things in the world

1035
00:43:56,020 --> 00:43:58,060
that are quite informational and objective

1036
00:43:58,060 --> 00:43:59,660
and we can communicate very well.

1037
00:43:59,700 --> 00:44:02,380
And then when we're bordering on anything subjective,

1038
00:44:02,380 --> 00:44:04,220
language fails us.

1039
00:44:04,220 --> 00:44:07,220
Yeah, and then we were trying to map whatever word

1040
00:44:07,220 --> 00:44:09,180
you're using on to how I would use that word

1041
00:44:09,180 --> 00:44:11,380
to describe the feeling that I would be having.

1042
00:44:11,380 --> 00:44:15,700
And that mapping seems completely like without a long

1043
00:44:15,700 --> 00:44:18,660
conversation to try to like feed up that mapping.

1044
00:44:18,660 --> 00:44:23,220
Oh, it could be quite different in how I would apply

1045
00:44:23,220 --> 00:44:25,340
that word to my own feeling.

1046
00:44:25,340 --> 00:44:26,900
Yeah, and there's been studies done as well

1047
00:44:26,900 --> 00:44:29,540
that I think certain tribes have a completely different

1048
00:44:29,540 --> 00:44:32,180
color perception and there are also concepts

1049
00:44:32,180 --> 00:44:34,460
like vagueness, so what is a pile of sand

1050
00:44:34,460 --> 00:44:36,260
and what is a shade of red?

1051
00:44:36,260 --> 00:44:38,260
And these things are actually very, very difficult

1052
00:44:38,260 --> 00:44:39,820
to communicate objectively.

1053
00:44:41,260 --> 00:44:44,980
Yeah, things like color perception

1054
00:44:44,980 --> 00:44:46,580
are kind of the interesting ones

1055
00:44:46,580 --> 00:44:49,740
because the literature is a bit messy

1056
00:44:49,740 --> 00:44:51,060
on some of these topics.

1057
00:44:52,220 --> 00:44:56,660
And some of it is just where you draw the lines

1058
00:44:56,660 --> 00:44:59,940
between colors and then how those linguistic boundaries

1059
00:44:59,940 --> 00:45:01,500
affect perception.

1060
00:45:01,500 --> 00:45:04,340
There definitely seems to be both things going,

1061
00:45:04,340 --> 00:45:08,260
but it's not sort of, I don't think any,

1062
00:45:08,260 --> 00:45:09,780
I think it's a strong claim to be like,

1063
00:45:09,780 --> 00:45:13,980
oh, the people can't perceive green or something like that.

1064
00:45:15,100 --> 00:45:16,820
It's just like where they would draw the line

1065
00:45:16,820 --> 00:45:18,980
between blue would be in a slightly different place

1066
00:45:18,980 --> 00:45:21,620
and then they might kind of see them as being,

1067
00:45:21,620 --> 00:45:23,860
sort of experience them as being like further apart

1068
00:45:23,860 --> 00:45:26,820
or closer together than you would necessarily,

1069
00:45:26,820 --> 00:45:30,380
but yeah, that's really,

1070
00:45:30,380 --> 00:45:34,220
their experience, that's really be super alien to you,

1071
00:45:34,220 --> 00:45:39,220
but they're sort of experience of maybe more very cultural,

1072
00:45:40,980 --> 00:45:42,700
like cultural taboos or something

1073
00:45:42,700 --> 00:45:44,500
would be very different than yours.

1074
00:45:46,660 --> 00:45:48,340
Yeah, I mean, one thing you're alluding to there is,

1075
00:45:48,340 --> 00:45:51,940
it's when we deal with complicated systems,

1076
00:45:51,980 --> 00:45:54,300
there's a real problem about drawing boundaries.

1077
00:45:54,300 --> 00:45:56,540
And I was, I mean, Friston's a great example,

1078
00:45:56,540 --> 00:45:58,220
he's got this idea of a Markov boundary

1079
00:45:58,220 --> 00:45:59,860
and it could be at the cellular level

1080
00:45:59,860 --> 00:46:02,420
or it could be you as a person.

1081
00:46:02,420 --> 00:46:04,940
And then when we talk about things like agency and free will,

1082
00:46:04,940 --> 00:46:07,220
we tend to anthropomorphize this boundary.

1083
00:46:07,220 --> 00:46:09,220
So we tend to think of ourselves as individuals,

1084
00:46:09,220 --> 00:46:12,420
but actually you could draw boundaries at different scales

1085
00:46:12,420 --> 00:46:14,700
and the boundaries might be observer relative as well.

1086
00:46:14,700 --> 00:46:16,740
So your boundary might not be my boundary.

1087
00:46:17,740 --> 00:46:19,020
Yeah, exactly.

1088
00:46:22,860 --> 00:46:26,580
Yeah, there's something I know a ton about,

1089
00:46:28,580 --> 00:46:31,820
how you define yourself and how you think of yourself

1090
00:46:31,820 --> 00:46:33,900
within the context of your community and whatnot.

1091
00:46:33,900 --> 00:46:36,980
I mean, some of these ideas are just very cultural

1092
00:46:36,980 --> 00:46:41,540
and how you experience yourself is probably even like,

1093
00:46:41,540 --> 00:46:44,340
very different, can be very different cross-pulturally.

1094
00:46:44,340 --> 00:46:45,660
Yeah, I mean, maybe one thing to bring in

1095
00:46:45,660 --> 00:46:49,060
is when you're, as a quant, when you're doing modeling,

1096
00:46:49,060 --> 00:46:51,300
you have this very, very complex system

1097
00:46:51,300 --> 00:46:54,420
and you draw boundaries and you create variables

1098
00:46:54,420 --> 00:46:56,980
and observables and do you know what I mean?

1099
00:46:56,980 --> 00:46:58,380
You kind of build a model

1100
00:46:58,380 --> 00:47:00,940
and that boundary could exist at any scale.

1101
00:47:00,940 --> 00:47:02,980
It seems like quite a,

1102
00:47:02,980 --> 00:47:06,100
it's a bit of an art and a science at the same time.

1103
00:47:06,100 --> 00:47:07,380
Yeah, no, for sure.

1104
00:47:08,980 --> 00:47:09,820
Yeah, exactly.

1105
00:47:09,820 --> 00:47:13,020
That's kind of why I like some of these complicated problems

1106
00:47:13,020 --> 00:47:14,740
that are not very well-defined

1107
00:47:14,740 --> 00:47:17,660
where you kind of have to use intuition

1108
00:47:17,660 --> 00:47:21,420
and or just sort of do the best you can do

1109
00:47:21,420 --> 00:47:23,660
at sort of drawing what are the relevant variables,

1110
00:47:23,660 --> 00:47:25,740
what sort of a priori makes sense to me

1111
00:47:25,740 --> 00:47:29,260
to be the things that matter for the system,

1112
00:47:29,260 --> 00:47:32,660
behaving at this within the context of this experiment

1113
00:47:32,660 --> 00:47:34,820
or in the context of this market or whatnot.

1114
00:47:35,820 --> 00:47:40,060
Trying to draw boundaries in because the rules

1115
00:47:40,060 --> 00:47:41,740
for these systems are not clear.

1116
00:47:41,740 --> 00:47:45,380
Like what are all the relevant variables for everything

1117
00:47:45,380 --> 00:47:47,820
and do you have access to them and can you control them?

1118
00:47:47,820 --> 00:47:49,700
And generally you don't know them all

1119
00:47:49,700 --> 00:47:51,500
and you don't necessarily have access

1120
00:47:51,500 --> 00:47:52,900
or can control any of them.

1121
00:47:54,180 --> 00:47:58,180
And so it's, yeah, it is a bit of an art.

1122
00:48:00,100 --> 00:48:02,460
Well, now might be a good time to talk about numerators.

1123
00:48:02,460 --> 00:48:04,700
So you're the chief scientist

1124
00:48:04,700 --> 00:48:06,940
and it's this insanely cool platform, right?

1125
00:48:06,940 --> 00:48:09,700
So people can go on there, they can download data sets,

1126
00:48:09,700 --> 00:48:10,700
they can build their own models,

1127
00:48:10,700 --> 00:48:11,900
they can stake the models.

1128
00:48:11,900 --> 00:48:14,500
I mean, why don't you just talk me through it?

1129
00:48:14,500 --> 00:48:15,340
Sure, yeah.

1130
00:48:15,340 --> 00:48:17,780
So we advertise ourselves as being

1131
00:48:17,780 --> 00:48:20,820
the hardest data science problem on the planet

1132
00:48:20,820 --> 00:48:24,100
because I think it is because like I said,

1133
00:48:24,100 --> 00:48:26,020
the correlations you're chasing are on the order of like

1134
00:48:26,020 --> 00:48:30,100
three or 4% out of sample, which,

1135
00:48:30,100 --> 00:48:32,060
and just sort of being able to tell

1136
00:48:32,060 --> 00:48:34,300
do you have something real or is it just in the noise

1137
00:48:34,300 --> 00:48:36,980
can be extremely hard to do, which is,

1138
00:48:36,980 --> 00:48:40,100
and we set up the problem for participants.

1139
00:48:40,100 --> 00:48:43,300
You give out a set of data that has been cleaned

1140
00:48:43,300 --> 00:48:46,500
and obfuscated and regularized.

1141
00:48:46,500 --> 00:48:48,740
And you basically just have a set of features

1142
00:48:48,740 --> 00:48:50,060
and a set of targets.

1143
00:48:50,060 --> 00:48:51,180
And you're just trying to build models

1144
00:48:51,180 --> 00:48:52,300
to go from features to targets.

1145
00:48:52,300 --> 00:48:56,300
So it's sort of a very classic machine learning style problem

1146
00:48:56,300 --> 00:48:59,020
and it's nicely curated for you.

1147
00:48:59,020 --> 00:49:02,460
And how it works for us is every week,

1148
00:49:02,460 --> 00:49:04,740
people submit predictions on a new set of features.

1149
00:49:04,740 --> 00:49:06,620
So every week we release a new set of features

1150
00:49:06,620 --> 00:49:08,420
and people just run their models over those features

1151
00:49:08,420 --> 00:49:10,300
and give us a set of predictions.

1152
00:49:10,300 --> 00:49:12,780
And people stake on those predictions.

1153
00:49:12,780 --> 00:49:16,140
And so people stake our cryptocurrency called NMR.

1154
00:49:16,140 --> 00:49:19,500
And if their predictions do well, they make money.

1155
00:49:19,500 --> 00:49:21,420
And if their predictions don't do well,

1156
00:49:21,420 --> 00:49:22,540
that week they could lose money.

1157
00:49:22,540 --> 00:49:26,020
And they sort of are expressing their confidence

1158
00:49:26,020 --> 00:49:27,780
in their models using their state.

1159
00:49:28,740 --> 00:49:32,780
And so we basically use this expression of confidence

1160
00:49:32,780 --> 00:49:35,300
as a way to sort of integrate these signals

1161
00:49:35,300 --> 00:49:36,780
into our meta model.

1162
00:49:36,780 --> 00:49:39,300
Our meta model is really just like a stake weighted average

1163
00:49:39,300 --> 00:49:41,780
of all the signals people are submitting.

1164
00:49:41,780 --> 00:49:44,580
And these signals, these predictions are just sort of

1165
00:49:44,580 --> 00:49:46,620
weights on stocks.

1166
00:49:46,620 --> 00:49:48,620
They're sort of like, how do we want to go long

1167
00:49:48,620 --> 00:49:51,140
or do we want to go short in the stock?

1168
00:49:51,140 --> 00:49:52,500
There's sort of just expressing,

1169
00:49:52,500 --> 00:49:55,540
do we think a stock is going to go down or going to go up?

1170
00:49:55,540 --> 00:49:59,180
The stake weighted model we feed to through our optimizer,

1171
00:49:59,180 --> 00:50:01,500
which is just doing a convex optimization problem,

1172
00:50:01,500 --> 00:50:04,060
trying to create a portfolio from the signal.

1173
00:50:04,060 --> 00:50:07,300
And is that portfolio changes week to week?

1174
00:50:07,300 --> 00:50:10,340
And so that's just the difference between the previous

1175
00:50:10,340 --> 00:50:12,980
and the new portfolio is just what we trade every week.

1176
00:50:12,980 --> 00:50:15,580
And so our trading is basically completely determined

1177
00:50:15,580 --> 00:50:18,500
by the like thousand people all over the world

1178
00:50:18,500 --> 00:50:20,580
submitting predictions every week.

1179
00:50:20,580 --> 00:50:25,060
And so it's this very kind of nice decentralized hedge fund

1180
00:50:25,060 --> 00:50:28,100
where the signal generation is very decentralized.

1181
00:50:28,100 --> 00:50:30,040
And we get the advantage of ensembling

1182
00:50:30,040 --> 00:50:32,860
over a wide variety of models.

1183
00:50:32,900 --> 00:50:36,340
And so people are trying to make their models

1184
00:50:36,340 --> 00:50:41,340
both predict the targets very well and consistently.

1185
00:50:41,820 --> 00:50:46,820
And we have other incentives to try to make them predict

1186
00:50:46,820 --> 00:50:49,180
aspects of the targets that other people are not

1187
00:50:50,100 --> 00:50:52,340
to try to, so that their contribution is sort of more

1188
00:50:52,340 --> 00:50:55,040
unique and they can make quite a bit of money

1189
00:50:55,040 --> 00:50:57,340
by having their predictions be pretty different

1190
00:50:57,340 --> 00:51:00,100
from other people's, but also still accounting

1191
00:51:00,100 --> 00:51:02,420
for like variance in the target.

1192
00:51:02,420 --> 00:51:06,700
And so that system is what we call true contribution.

1193
00:51:06,700 --> 00:51:10,420
And it's really, we try to, it was our attempt

1194
00:51:10,420 --> 00:51:13,740
to try to make people's predictions and payouts

1195
00:51:13,740 --> 00:51:16,280
more tied to actual portfolio returns.

1196
00:51:17,900 --> 00:51:19,740
Because the sort of standard scoring

1197
00:51:19,740 --> 00:51:22,660
and we're just doing the correlation of how well

1198
00:51:22,660 --> 00:51:26,640
your predictions match like the new weekly week's target

1199
00:51:26,640 --> 00:51:30,600
that is determined by just how the stocks move that

1200
00:51:30,600 --> 00:51:32,580
over the course of 20 days.

1201
00:51:33,800 --> 00:51:36,720
The true contribution is basically sort of doing

1202
00:51:36,720 --> 00:51:39,520
the whole process, like creating the meta model,

1203
00:51:39,520 --> 00:51:41,120
running it through the portfolio optimizer,

1204
00:51:41,120 --> 00:51:43,240
getting the portfolio, getting portfolio returns.

1205
00:51:43,240 --> 00:51:45,800
And then we try to see like, take the gradient

1206
00:51:45,800 --> 00:51:48,520
through all of that until you can find out

1207
00:51:48,520 --> 00:51:50,360
if people's stakes have been more or less,

1208
00:51:50,360 --> 00:51:52,200
would we have made more or less money

1209
00:51:52,200 --> 00:51:55,360
and use this gradient of the stakes

1210
00:51:55,360 --> 00:51:57,320
with respect to the payout, with respect

1211
00:51:57,320 --> 00:52:00,440
to the portfolio returns as a way to pay people out

1212
00:52:01,280 --> 00:52:03,360
to essentially increase their weight or decrease their weight.

1213
00:52:03,360 --> 00:52:05,200
And that tends to reward people

1214
00:52:05,200 --> 00:52:07,200
with more unique contributions.

1215
00:52:08,240 --> 00:52:09,840
I've got so many questions.

1216
00:52:09,840 --> 00:52:13,160
So, I mean, I really like this idea

1217
00:52:13,160 --> 00:52:16,000
because first of all, you're democratizing the whole thing

1218
00:52:16,000 --> 00:52:19,680
and you're kind of gamifying it and it's a meritocracy.

1219
00:52:19,680 --> 00:52:22,640
So any data scientist can go on there

1220
00:52:22,640 --> 00:52:24,440
and flex their muscles and build great models

1221
00:52:24,440 --> 00:52:25,680
and be recognized for doing so

1222
00:52:25,680 --> 00:52:27,400
and even earn money for doing so.

1223
00:52:27,400 --> 00:52:29,940
But in a way, I want to contrast it to somewhere

1224
00:52:29,980 --> 00:52:31,300
like Kaggle.

1225
00:52:31,300 --> 00:52:34,260
Now, on Kaggle, I mean, traditionally data science

1226
00:52:34,260 --> 00:52:37,820
has been about understanding the domain.

1227
00:52:37,820 --> 00:52:40,340
A lot of data science is business analysis essentially

1228
00:52:40,340 --> 00:52:44,060
and kind of understanding what makes something work

1229
00:52:44,060 --> 00:52:45,220
in a model.

1230
00:52:45,220 --> 00:52:47,540
And as I understand with Numeri,

1231
00:52:47,540 --> 00:52:49,060
the interface is kind of the same.

1232
00:52:49,060 --> 00:52:52,660
So maybe they get similar shape of data every time

1233
00:52:52,660 --> 00:52:54,340
they build the models on it.

1234
00:52:54,340 --> 00:52:56,620
And in this domain, because you know,

1235
00:52:56,620 --> 00:52:58,780
like there's technical analysis and there's fundamentals

1236
00:52:59,060 --> 00:53:00,740
they might still understand some market.

1237
00:53:00,740 --> 00:53:03,020
They have some kind of extrinsic understanding

1238
00:53:03,020 --> 00:53:04,060
of why their model would work,

1239
00:53:04,060 --> 00:53:07,220
but they don't have the same kind of understanding.

1240
00:53:07,220 --> 00:53:10,460
No, yeah, the features include all sorts of things

1241
00:53:10,460 --> 00:53:11,940
from like analyst sentiments

1242
00:53:11,940 --> 00:53:14,620
and other sort of fundamental things

1243
00:53:14,620 --> 00:53:17,220
to technical features.

1244
00:53:17,220 --> 00:53:20,220
But that is all sort of obscured from people.

1245
00:53:20,220 --> 00:53:22,660
People just have these funny feature names.

1246
00:53:22,660 --> 00:53:24,180
And so it's up to them to just use

1247
00:53:24,180 --> 00:53:26,420
their sort of machine learning toolbox

1248
00:53:26,420 --> 00:53:28,100
to figure out what are the good features

1249
00:53:28,100 --> 00:53:30,740
for predicting what features tend to work well.

1250
00:53:31,660 --> 00:53:34,300
How do we combine those features?

1251
00:53:34,300 --> 00:53:37,100
And so we actually wanted to kind of like remove

1252
00:53:37,100 --> 00:53:39,620
any of the people's biases for what features

1253
00:53:39,620 --> 00:53:41,020
they think will work.

1254
00:53:41,020 --> 00:53:45,380
We wanted to not have people's financial intuitions

1255
00:53:45,380 --> 00:53:46,220
play into it.

1256
00:53:46,220 --> 00:53:47,340
We wanted to just sort of set it up

1257
00:53:47,340 --> 00:53:49,900
as a pure machine learning problem.

1258
00:53:49,900 --> 00:53:54,300
To try to make it, yeah, basically to make it be better

1259
00:53:54,300 --> 00:53:56,620
than any human could possibly be.

1260
00:53:57,460 --> 00:54:00,420
So with this sort of combined ensemble wisdom

1261
00:54:00,420 --> 00:54:02,700
with the crowd, we're trying to make it

1262
00:54:02,700 --> 00:54:05,460
like the alpha go of like finance,

1263
00:54:05,460 --> 00:54:06,820
something that it's just that performs

1264
00:54:06,820 --> 00:54:08,100
at like a super human level

1265
00:54:08,100 --> 00:54:10,100
in ways you don't really understand.

1266
00:54:11,300 --> 00:54:12,140
Interesting.

1267
00:54:12,140 --> 00:54:14,820
And you're aggregating the predictions together

1268
00:54:14,820 --> 00:54:15,740
in some way.

1269
00:54:16,700 --> 00:54:19,260
Yeah, it's actually fairly simple.

1270
00:54:19,260 --> 00:54:22,180
We, I mean, people submit their predictions

1271
00:54:22,620 --> 00:54:24,980
which are just a number between zero and one

1272
00:54:24,980 --> 00:54:25,860
for every stock.

1273
00:54:26,940 --> 00:54:29,300
And it's basically just a rank ordering of stock.

1274
00:54:29,300 --> 00:54:32,700
And we just normalize everyone's predictions

1275
00:54:32,700 --> 00:54:34,380
and then just weight them by their stake

1276
00:54:34,380 --> 00:54:36,180
and then just average them together

1277
00:54:36,180 --> 00:54:38,460
and to do another renormalization

1278
00:54:38,460 --> 00:54:39,980
so that it's the right scale

1279
00:54:39,980 --> 00:54:41,980
and sort of distributional shape

1280
00:54:41,980 --> 00:54:43,380
to be fed to the optimizer.

1281
00:54:44,540 --> 00:54:48,420
But it's a fairly simple and robust way to weight things.

1282
00:54:48,420 --> 00:54:50,980
We're basically just using people's express confidence

1283
00:54:50,980 --> 00:54:54,420
in their model as the weighting system.

1284
00:54:54,420 --> 00:54:57,220
And because there's this feedback of payments

1285
00:54:57,220 --> 00:54:59,980
and paying out people, good models,

1286
00:54:59,980 --> 00:55:01,260
their stakes increase over time

1287
00:55:01,260 --> 00:55:03,420
so their weight in the meta model increases over time

1288
00:55:03,420 --> 00:55:06,100
and bad models, their weights decrease over time.

1289
00:55:06,100 --> 00:55:08,620
So it's kind of like a human in the gradient descent

1290
00:55:08,620 --> 00:55:10,140
for doing like gradient descent

1291
00:55:10,140 --> 00:55:13,420
with the stakes as the weights in the model.

1292
00:55:13,420 --> 00:55:14,260
Fascinating.

1293
00:55:14,260 --> 00:55:15,540
And can you give us any intuition

1294
00:55:15,540 --> 00:55:17,540
on how that model is tuned

1295
00:55:17,540 --> 00:55:20,500
and what kind of penalty you're using?

1296
00:55:20,500 --> 00:55:22,100
Are you using just the stakes

1297
00:55:22,100 --> 00:55:25,140
or also the previous performance?

1298
00:55:25,140 --> 00:55:27,820
So no, we're not actually using the previous performance.

1299
00:55:27,820 --> 00:55:29,300
It's really just stakes.

1300
00:55:29,300 --> 00:55:32,300
The previous performance only enters into the fact

1301
00:55:32,300 --> 00:55:34,460
that the good performance of the past

1302
00:55:34,460 --> 00:55:36,860
would have made their stake grow over time.

1303
00:55:38,220 --> 00:55:42,020
And but we have thousands and thousands of models now.

1304
00:55:42,020 --> 00:55:45,780
And so any one model is only a very small percentage

1305
00:55:45,780 --> 00:55:46,780
of the meta model.

1306
00:55:47,860 --> 00:55:49,780
And even the ones that are the biggest

1307
00:55:50,020 --> 00:55:52,700
maybe only a couple percent of the total meta model.

1308
00:55:52,700 --> 00:55:55,580
And it's, it is a sort of like power law distribution.

1309
00:55:56,500 --> 00:55:58,940
There is a lot of work that I've done

1310
00:55:58,940 --> 00:56:01,420
in the portfolio optimization set.

1311
00:56:02,780 --> 00:56:06,740
And that's the going from the signal to the portfolio.

1312
00:56:06,740 --> 00:56:10,060
And there is actually a lot that goes on in there as well

1313
00:56:10,060 --> 00:56:11,900
just in how you construct a portfolio,

1314
00:56:11,900 --> 00:56:15,220
how you determine how much you're gonna trade each week

1315
00:56:15,220 --> 00:56:18,820
and how you make your portfolio, what you make exposed to.

1316
00:56:18,820 --> 00:56:21,100
Exposed really just means is are the weights

1317
00:56:21,100 --> 00:56:23,940
of your portfolio correlated with lots and lots of things?

1318
00:56:23,940 --> 00:56:27,060
And so there's a lot we go due to try

1319
00:56:27,060 --> 00:56:29,220
to make the portfolio weights not correlated

1320
00:56:29,220 --> 00:56:31,180
with the market overall.

1321
00:56:31,180 --> 00:56:35,980
So we're a market neutral hedge fund.

1322
00:56:35,980 --> 00:56:38,060
So we try to be uncorrelated to the market,

1323
00:56:38,060 --> 00:56:40,020
have a beta of zero.

1324
00:56:40,020 --> 00:56:41,540
So when the market goes up or down,

1325
00:56:41,540 --> 00:56:45,060
you can't really tell how we would do on that kind of day.

1326
00:56:45,060 --> 00:56:46,940
And but we also try to be uncorrelated to lots

1327
00:56:47,180 --> 00:56:48,820
of other things that we think could drive returns.

1328
00:56:48,820 --> 00:56:52,380
So we try not to have like big country biases,

1329
00:56:52,380 --> 00:56:55,060
big sector biases, factor biases.

1330
00:56:55,060 --> 00:56:57,100
So factor are things like value and momentum,

1331
00:56:57,100 --> 00:56:59,860
these kind of like more abstract quantities

1332
00:56:59,860 --> 00:57:01,020
that are supposed to tell you something

1333
00:57:01,020 --> 00:57:04,700
about classes of stocks.

1334
00:57:04,700 --> 00:57:08,140
But we try to be uncorrelated to basically everything.

1335
00:57:08,140 --> 00:57:10,060
And they're just trying to get the sort of pure machine

1336
00:57:10,060 --> 00:57:13,940
learning non-linear signal that is driving stock returns

1337
00:57:13,940 --> 00:57:16,220
or like stock specific alpha,

1338
00:57:16,220 --> 00:57:19,860
we call it sort of like the amount of stock is going to,

1339
00:57:19,860 --> 00:57:22,540
how all the stock is going to do sort of just by itself,

1340
00:57:22,540 --> 00:57:23,980
not taking all these other things

1341
00:57:23,980 --> 00:57:25,860
that are about it into account.

1342
00:57:25,860 --> 00:57:26,700
Yeah, that's really interesting.

1343
00:57:26,700 --> 00:57:28,820
And I guess like one of the problems on Kaggle

1344
00:57:28,820 --> 00:57:31,660
is that most of the solutions are so overfit

1345
00:57:31,660 --> 00:57:35,300
to the training set that they never generalize

1346
00:57:35,300 --> 00:57:36,460
to real world versions of the problem.

1347
00:57:36,460 --> 00:57:38,220
But what you're doing actually is to kind of like

1348
00:57:38,220 --> 00:57:40,940
remove away a lot of those opportunities for overfitting

1349
00:57:40,940 --> 00:57:42,860
and also allowing the models to be used again

1350
00:57:42,860 --> 00:57:44,100
when the next thing comes around.

1351
00:57:44,100 --> 00:57:46,180
But just quickly on the aggregating stuff

1352
00:57:46,260 --> 00:57:48,140
the reason I'm interested in that is on my PhD

1353
00:57:48,140 --> 00:57:50,460
I did prediction with expert advice

1354
00:57:50,460 --> 00:57:52,620
and there's a whole load of theoretical approaches

1355
00:57:52,620 --> 00:57:54,740
to that where you can have an aggregating algorithm

1356
00:57:54,740 --> 00:57:56,860
that produce, you know, that produces performance

1357
00:57:56,860 --> 00:57:59,500
or a kind of like an error bound

1358
00:57:59,500 --> 00:58:02,180
which is not much worse than the best path

1359
00:58:02,180 --> 00:58:03,820
of switching experts.

1360
00:58:03,820 --> 00:58:06,260
So if you took the optimal path of the best expert

1361
00:58:06,260 --> 00:58:08,180
every single time step, you can have algorithms

1362
00:58:08,180 --> 00:58:11,340
that have approvable bound not much worse than that.

1363
00:58:12,300 --> 00:58:17,300
Yeah, we, so we've done a lot to try to experiment

1364
00:58:17,460 --> 00:58:19,580
with trying to improve upon stake waiting.

1365
00:58:20,860 --> 00:58:24,060
And it's always been really hard to do it in a robust way.

1366
00:58:25,260 --> 00:58:28,780
It's, I mean, for one, stake waiting is,

1367
00:58:28,780 --> 00:58:33,100
it's sort of nice in that it's easy for people to understand.

1368
00:58:33,100 --> 00:58:36,020
People are, it's very clean in how it works.

1369
00:58:36,020 --> 00:58:37,780
It sort of fits with the ethos

1370
00:58:38,020 --> 00:58:42,300
and the, like, and the idea of the company

1371
00:58:42,300 --> 00:58:45,620
of how it's distributed and decentralized

1372
00:58:45,620 --> 00:58:49,380
and you express your confidence by your stake.

1373
00:58:51,540 --> 00:58:53,660
But it does sort of seem like there should be a better way

1374
00:58:53,660 --> 00:58:55,340
to aggregate models.

1375
00:58:55,340 --> 00:58:59,340
But pretty much every time we try to find something better,

1376
00:58:59,340 --> 00:59:02,260
it's, it might be a little bit better

1377
00:59:02,260 --> 00:59:03,540
but it's like less robust.

1378
00:59:03,540 --> 00:59:06,100
It tends to just be less robust.

1379
00:59:06,100 --> 00:59:09,660
And it's, because you are essentially just sort of fitting

1380
00:59:09,660 --> 00:59:12,780
to the past and to try to find way to the models

1381
00:59:12,780 --> 00:59:15,580
or something, it tends to just like overfit

1382
00:59:15,580 --> 00:59:17,620
and this sort of stake waiting thing,

1383
00:59:17,620 --> 00:59:18,580
you can't really overfit.

1384
00:59:18,580 --> 00:59:22,300
It's just sort of a property that just sort of evolves

1385
00:59:22,300 --> 00:59:23,620
as the tournament goes on

1386
00:59:25,060 --> 00:59:27,860
without ever considering like the past performance

1387
00:59:27,860 --> 00:59:29,100
and all of these things.

1388
00:59:29,980 --> 00:59:33,140
So yeah, it's, it's been kind of interesting to,

1389
00:59:33,140 --> 00:59:36,020
so it's one of these things we sort of revisit every year

1390
00:59:36,020 --> 00:59:36,860
at some point of like,

1391
00:59:36,860 --> 00:59:38,540
let's try to build a better meta model

1392
00:59:38,540 --> 00:59:42,500
but we usually just come back to stake waiting in the end.

1393
00:59:42,500 --> 00:59:43,540
Yeah, well, in a way, I mean,

1394
00:59:43,540 --> 00:59:44,740
we're prediction with expert advice,

1395
00:59:44,740 --> 00:59:45,940
you have a learning rate

1396
00:59:45,940 --> 00:59:47,620
and I guess you don't even have that problem

1397
00:59:47,620 --> 00:59:50,620
because you're just using the stakes as the-

1398
00:59:50,620 --> 00:59:54,540
Yeah, the, but yeah, the, I mean, our learning rate.

1399
00:59:54,540 --> 00:59:56,660
So I mean, our payout system is the way

1400
00:59:56,660 --> 00:59:58,740
we adjust the weights over time.

1401
00:59:58,740 --> 01:00:02,420
And so we have done like some simulations to show

1402
01:00:02,420 --> 01:00:04,500
that like if, how we reward people,

1403
01:00:04,500 --> 01:00:06,060
how that affects their weights over time

1404
01:00:06,060 --> 01:00:07,940
and how that affects meta model performance.

1405
01:00:07,940 --> 01:00:09,780
So you wouldn't want to have a payout system

1406
01:00:09,780 --> 01:00:12,700
that would make the meta model worse over time.

1407
01:00:12,700 --> 01:00:15,340
And so yeah, like this, this true contribution idea

1408
01:00:15,340 --> 01:00:17,380
that's gradient of the stakes,

1409
01:00:17,380 --> 01:00:19,100
we did simulations to show

1410
01:00:19,100 --> 01:00:21,740
it does actually improve the meta model over time

1411
01:00:21,740 --> 01:00:23,100
to pay out in this way.

1412
01:00:24,020 --> 01:00:27,500
It's nothing, I mean, people do things

1413
01:00:27,500 --> 01:00:29,380
like take their stakes out, withdraw money.

1414
01:00:29,380 --> 01:00:30,700
And so it's not a perfect system.

1415
01:00:30,700 --> 01:00:31,940
People entering the tournament,

1416
01:00:31,940 --> 01:00:33,580
people, some people entering with a lot of money,

1417
01:00:33,580 --> 01:00:35,620
some people entered with not that much money.

1418
01:00:36,700 --> 01:00:40,300
And so yeah, it takes time for these things,

1419
01:00:40,300 --> 01:00:42,740
all the kind of shake out in real life.

1420
01:00:42,740 --> 01:00:46,820
But the overall idea is that we are essentially adjusting

1421
01:00:46,820 --> 01:00:48,340
the weights through our payouts

1422
01:00:48,340 --> 01:00:52,380
towards this sort of more optimal meta model over time.

1423
01:00:52,380 --> 01:00:53,220
Interesting.

1424
01:00:53,220 --> 01:00:56,620
So I'm actually very, very interested to give it a go.

1425
01:00:56,620 --> 01:00:58,420
And I guess like, first of all,

1426
01:00:58,420 --> 01:01:00,540
you could sketch out what the process looks like.

1427
01:01:00,540 --> 01:01:02,500
I mean, let's say I had a few hundred dollars

1428
01:01:02,500 --> 01:01:03,460
and I wanted to build a model.

1429
01:01:03,460 --> 01:01:06,700
And also it's got to be a good model.

1430
01:01:06,700 --> 01:01:07,540
Let's face it.

1431
01:01:07,540 --> 01:01:08,700
So if I just logged on there

1432
01:01:08,700 --> 01:01:13,700
and I built a gradient booster tree model, would that work?

1433
01:01:13,740 --> 01:01:14,820
It would actually.

1434
01:01:14,820 --> 01:01:17,780
So I mean, it's tabular data

1435
01:01:17,780 --> 01:01:19,380
and tabular data is very minimal

1436
01:01:19,380 --> 01:01:21,340
to gradient boosted trees.

1437
01:01:21,340 --> 01:01:23,900
We have a lot of example models that we have put up

1438
01:01:23,900 --> 01:01:26,700
and they're doing quite well.

1439
01:01:26,700 --> 01:01:29,620
So basically all you have to do

1440
01:01:29,620 --> 01:01:31,460
is you can go to the website

1441
01:01:31,500 --> 01:01:33,580
and just download a big zip file

1442
01:01:33,580 --> 01:01:37,140
that includes all the data in parquet.

1443
01:01:37,140 --> 01:01:40,740
And then you can just open it up in Python

1444
01:01:40,740 --> 01:01:42,060
and fit a gradient boosted tree.

1445
01:01:42,060 --> 01:01:43,100
When we have example scripts

1446
01:01:43,100 --> 01:01:46,300
sort of showing this along with some more interesting types

1447
01:01:46,300 --> 01:01:49,300
of pre-processing and other sort of ideas

1448
01:01:49,300 --> 01:01:50,700
like feature neutralization.

1449
01:01:52,340 --> 01:01:54,100
So I can talk about it in a second.

1450
01:01:54,100 --> 01:01:57,220
But yeah, a lot of our sort of standard internal models

1451
01:01:57,220 --> 01:01:59,660
use basically gradient boosted trees.

1452
01:01:59,660 --> 01:02:04,140
And we are, I mean, we have basically example models running

1453
01:02:04,140 --> 01:02:07,100
that and they all have positive correlation with

1454
01:02:07,100 --> 01:02:09,220
and true and still true contribution.

1455
01:02:09,220 --> 01:02:11,140
So they're actually working out of sample

1456
01:02:11,140 --> 01:02:13,020
and performing quite well.

1457
01:02:13,020 --> 01:02:15,540
That hasn't all been sort of eaten up

1458
01:02:15,540 --> 01:02:17,580
by people using similar enough models.

1459
01:02:18,660 --> 01:02:19,980
There's a lot of opportunity

1460
01:02:19,980 --> 01:02:22,460
to make sort of unique models too.

1461
01:02:22,460 --> 01:02:25,540
Cause one thing that's sort of unique about our tournament

1462
01:02:25,540 --> 01:02:27,740
is we release actually several targets,

1463
01:02:27,740 --> 01:02:29,980
we release 20 something targets.

1464
01:02:30,900 --> 01:02:32,340
And they're all constructed

1465
01:02:32,340 --> 01:02:34,260
in somewhat slightly different ways.

1466
01:02:34,260 --> 01:02:38,540
And you can find that if you train on a different target,

1467
01:02:38,540 --> 01:02:40,140
it might work almost as well as training

1468
01:02:40,140 --> 01:02:41,940
on the target that you're scored on.

1469
01:02:44,020 --> 01:02:45,940
And it might also ensemble really well

1470
01:02:45,940 --> 01:02:47,700
with a model trained on different targets.

1471
01:02:47,700 --> 01:02:50,060
And so you can actually create ensembles fairly easily

1472
01:02:50,060 --> 01:02:51,900
just by training on different targets.

1473
01:02:52,900 --> 01:02:55,540
Because it is kind of remarkable

1474
01:02:55,540 --> 01:02:57,980
that a model trained on a different target

1475
01:02:57,980 --> 01:03:01,220
can actually work better on the target you're interested in.

1476
01:03:01,220 --> 01:03:03,340
But that kind of thing, yeah, definitely does happen.

1477
01:03:03,340 --> 01:03:05,980
I mean, part of it is called all the correlations are so low,

1478
01:03:05,980 --> 01:03:09,300
but some targets might just sort of have a better property

1479
01:03:09,300 --> 01:03:14,100
in making your model pick up on the actual signal

1480
01:03:14,100 --> 01:03:17,260
that you want to model rather than sort of variance

1481
01:03:17,260 --> 01:03:20,620
that is like not that you don't want to model.

1482
01:03:20,620 --> 01:03:21,460
Interesting.

1483
01:03:21,460 --> 01:03:24,820
I think one of the issues is you might not know

1484
01:03:24,820 --> 01:03:27,060
what models that people are using.

1485
01:03:27,060 --> 01:03:29,700
But I wondered if you did have any intuition,

1486
01:03:29,700 --> 01:03:31,140
I'd be fascinated to know,

1487
01:03:31,140 --> 01:03:33,060
are they using very complex models?

1488
01:03:33,060 --> 01:03:34,500
Are they using simple models?

1489
01:03:35,740 --> 01:03:38,660
From talking to participants, there was a huge range.

1490
01:03:38,660 --> 01:03:41,460
There are some people using like extremely simple trees.

1491
01:03:41,460 --> 01:03:43,340
There are some people who are using

1492
01:03:43,340 --> 01:03:45,620
incredibly elaborate neural networks

1493
01:03:45,620 --> 01:03:47,900
with very sort of custom architectures

1494
01:03:48,860 --> 01:03:51,460
that are sort of designed to the problem.

1495
01:03:51,460 --> 01:03:54,140
There, yeah, there's a whole huge, right?

1496
01:03:54,140 --> 01:03:55,700
I mean, there's people who have huge ensembles.

1497
01:03:55,700 --> 01:03:57,940
There's people who are doing kind of like online learning

1498
01:03:57,940 --> 01:04:00,660
where their model is actually using the features

1499
01:04:00,660 --> 01:04:03,340
that were released that week

1500
01:04:03,340 --> 01:04:06,980
and sort of using that in some sort of unsupervised learning

1501
01:04:09,140 --> 01:04:11,500
and then so they take some while from when we released,

1502
01:04:11,500 --> 01:04:12,900
they can't just like run their model

1503
01:04:12,900 --> 01:04:14,100
through the new set of features.

1504
01:04:14,100 --> 01:04:16,260
They have to incorporate this new set of features

1505
01:04:16,260 --> 01:04:18,820
in this unsupervised way before they can,

1506
01:04:18,820 --> 01:04:20,740
so yeah, there's an incredible variety

1507
01:04:20,740 --> 01:04:23,100
of techniques people are using.

1508
01:04:23,100 --> 01:04:25,460
Fascinating, and how big is this parquet for?

1509
01:04:25,460 --> 01:04:29,220
How many rows, how many fields

1510
01:04:29,220 --> 01:04:32,820
and are they all just real numbers between naught and one?

1511
01:04:32,820 --> 01:04:37,020
So yeah, so there's, how many features are we up to now?

1512
01:04:37,020 --> 01:04:39,580
We have a couple thousand features roughly

1513
01:04:39,580 --> 01:04:43,860
and there's a few million, a couple million rows, I think.

1514
01:04:45,380 --> 01:04:48,700
So one sort of additional piece of structure in the data

1515
01:04:48,700 --> 01:04:50,700
is there's these things called eras

1516
01:04:50,700 --> 01:04:53,580
and the eras are essentially just the weeks

1517
01:04:53,580 --> 01:04:56,500
and because the competition has this structure

1518
01:04:56,500 --> 01:04:58,420
of we're making predictions every week

1519
01:04:58,420 --> 01:05:02,420
and so within each era, there's like say 5,000 rows

1520
01:05:02,420 --> 01:05:04,620
which are basically like 5,000 stocks

1521
01:05:04,620 --> 01:05:08,540
and so one sort of interesting thing is you are,

1522
01:05:08,540 --> 01:05:10,740
you want your model to be good across eras,

1523
01:05:10,740 --> 01:05:12,500
not necessarily across samples

1524
01:05:12,500 --> 01:05:15,740
and so it creates a different structure

1525
01:05:15,740 --> 01:05:17,140
in how you think about the problem

1526
01:05:17,140 --> 01:05:19,140
because you want your model to be consistently good

1527
01:05:19,140 --> 01:05:22,460
in every era and that can give you a different solution

1528
01:05:22,460 --> 01:05:25,180
that if you just try to say maximize some metric

1529
01:05:25,180 --> 01:05:30,180
over the whole training set which is kind of, yeah.

1530
01:05:30,660 --> 01:05:34,780
But yeah, it is basically just a big parquet file.

1531
01:05:34,780 --> 01:05:37,380
We do divide it into like training

1532
01:05:37,380 --> 01:05:41,300
and like there's like a testing set

1533
01:05:42,980 --> 01:05:46,980
but yeah, do you have any specific questions

1534
01:05:46,980 --> 01:05:48,660
about how that is organized?

1535
01:05:49,620 --> 01:05:51,660
Well, again, I'm really interested

1536
01:05:51,660 --> 01:05:56,100
because on my PhD, I did a whole bunch of prediction models

1537
01:05:56,100 --> 01:05:57,500
on financial data sets.

1538
01:05:57,500 --> 01:05:59,660
I was predicting like the implied volatility

1539
01:05:59,660 --> 01:06:03,580
of the Black Shells formula on some futures data

1540
01:06:03,580 --> 01:06:07,180
but my big thing at the time was I was fascinated

1541
01:06:07,180 --> 01:06:09,660
by regimes in financial data

1542
01:06:09,660 --> 01:06:12,780
and you get these changing dependencies with time

1543
01:06:12,780 --> 01:06:15,420
and what I did, I mean, you could actually visualize it

1544
01:06:15,420 --> 01:06:18,620
if you build a load of expert models

1545
01:06:18,620 --> 01:06:20,700
on different regimes and then you get them to predict

1546
01:06:20,700 --> 01:06:22,700
on the other regime's data.

1547
01:06:22,700 --> 01:06:24,780
You get this kind of self-similarity matrix

1548
01:06:24,780 --> 01:06:27,300
and it looks like you get this kind of structure in there

1549
01:06:27,300 --> 01:06:29,260
because there are certain regimes

1550
01:06:29,260 --> 01:06:31,160
where this particular model actually predicts

1551
01:06:31,160 --> 01:06:32,500
quite far out into the future

1552
01:06:32,500 --> 01:06:33,740
and then it might suddenly go dead

1553
01:06:33,740 --> 01:06:35,340
so you get these kind of squares

1554
01:06:35,340 --> 01:06:38,100
and I had this big thesis that if I have expert models

1555
01:06:38,100 --> 01:06:39,620
and use prediction with expert advice,

1556
01:06:39,620 --> 01:06:41,060
then when we come into a new regime,

1557
01:06:41,060 --> 01:06:43,900
I would quickly learn which experts are the good ones

1558
01:06:43,900 --> 01:06:47,280
and I had this thesis that sometimes old information

1559
01:06:47,280 --> 01:06:50,120
is very helpful in the future more so than using

1560
01:06:50,120 --> 01:06:52,880
like a simple sliding window ridge regression or whatever

1561
01:06:52,880 --> 01:06:54,080
and it turned out I was wrong.

1562
01:06:54,080 --> 01:06:55,360
It's almost always better just to use

1563
01:06:55,360 --> 01:06:58,720
a sliding window regression but yeah, it's fascinating.

1564
01:06:58,720 --> 01:07:01,800
It's, yeah, it's interesting.

1565
01:07:01,800 --> 01:07:06,800
Like the, you definitely want to train on a lot of data

1566
01:07:07,360 --> 01:07:09,800
for these models.

1567
01:07:09,800 --> 01:07:12,160
It definitely, like if you just use the prior one year

1568
01:07:12,160 --> 01:07:14,440
of data, your models are gonna be pretty crap.

1569
01:07:15,440 --> 01:07:18,560
It definitely helps to use like prior 10 years of data

1570
01:07:19,760 --> 01:07:24,000
and so it is, you're using actually quite old data often

1571
01:07:24,000 --> 01:07:27,960
in predicting into the future but generally, yeah,

1572
01:07:27,960 --> 01:07:30,800
if you were only just using the last year or two of data,

1573
01:07:30,800 --> 01:07:33,600
your models are gonna have to actually quite a hard time.

1574
01:07:34,760 --> 01:07:37,080
Yeah, one other thing about the features I wanted to say

1575
01:07:37,080 --> 01:07:40,480
is they are between zero and one, they're in five bins.

1576
01:07:40,480 --> 01:07:43,800
There's zero, 0.25, 0.5, 0.75 and one.

1577
01:07:43,800 --> 01:07:47,400
So the data has been like binned in this way

1578
01:07:47,400 --> 01:07:50,680
and the targets are also binned in this,

1579
01:07:50,680 --> 01:07:53,360
the same sort of bins but with a different distribution.

1580
01:07:53,360 --> 01:07:55,120
The targets have like in their extreme bins,

1581
01:07:55,120 --> 01:07:58,600
only like 5% of the values in the next two extreme bins.

1582
01:07:59,440 --> 01:08:04,440
Like what is it, 20 in each of them

1583
01:08:05,360 --> 01:08:07,280
and then 50% as a zero.

1584
01:08:08,360 --> 01:08:09,800
Interesting.

1585
01:08:09,800 --> 01:08:12,640
But all the features are basically just 20%

1586
01:08:12,640 --> 01:08:13,640
in each of the bins.

1587
01:08:14,640 --> 01:08:19,600
And so the binning is a pretty strong form of regularization.

1588
01:08:19,600 --> 01:08:22,000
It sort of prevents you from like a tree from splitting

1589
01:08:22,000 --> 01:08:25,560
sort of any arbitrary place you can only split at these things

1590
01:08:25,560 --> 01:08:28,840
and so that kind of forces at least some of the space

1591
01:08:28,840 --> 01:08:31,480
to be at different splits.

1592
01:08:31,480 --> 01:08:33,980
And that regularization, it's kind of,

1593
01:08:33,980 --> 01:08:36,680
you would think that having continuous features

1594
01:08:36,680 --> 01:08:38,680
would be a lot really helpful but I mean,

1595
01:08:38,680 --> 01:08:40,520
it's really not.

1596
01:08:40,560 --> 01:08:44,280
It's kind of remarkable how lossy

1597
01:08:44,280 --> 01:08:46,200
some of these transforms are that we do

1598
01:08:46,200 --> 01:08:48,600
that actually seem to be helpful.

1599
01:08:48,600 --> 01:08:49,680
Yeah, so it's so interesting.

1600
01:08:49,680 --> 01:08:53,240
And I guess like one thing I didn't really appreciate

1601
01:08:53,240 --> 01:08:54,440
at the time is you know, we were just talking

1602
01:08:54,440 --> 01:08:56,680
about these complex dynamical systems

1603
01:08:56,680 --> 01:08:58,520
like the brain or like financial markets

1604
01:08:58,520 --> 01:09:00,600
and there's of course the market efficiency hypothesis

1605
01:09:00,600 --> 01:09:05,000
and perhaps one of the reasons why old information

1606
01:09:05,000 --> 01:09:08,840
might not be salient is because if the underlying system

1607
01:09:08,840 --> 01:09:10,360
is actually taking a trajectory

1608
01:09:10,360 --> 01:09:12,360
through this kind of complex space,

1609
01:09:12,360 --> 01:09:15,560
then you might argue that almost regardless

1610
01:09:15,560 --> 01:09:19,000
of where you traverse, you'll always be in a novel situation.

1611
01:09:19,000 --> 01:09:20,540
And then there's this continuum

1612
01:09:20,540 --> 01:09:22,600
of regularity versus chaos.

1613
01:09:22,600 --> 01:09:27,000
So like for example, if you're predicting options futures

1614
01:09:27,000 --> 01:09:30,020
when they get close to maturity,

1615
01:09:30,020 --> 01:09:31,840
the volatility just goes crazy

1616
01:09:31,840 --> 01:09:34,400
and they just become increasingly unpredictable.

1617
01:09:34,400 --> 01:09:38,360
And I guess the art in this kind of data is knowing

1618
01:09:38,360 --> 01:09:40,640
when you're in a regime which has some regularity

1619
01:09:40,640 --> 01:09:42,240
and when you're not.

1620
01:09:42,240 --> 01:09:43,560
It's yeah, it's tricky.

1621
01:09:43,560 --> 01:09:45,600
Cause like ideally we want our model,

1622
01:09:45,600 --> 01:09:48,920
we want our meta model to sort of work well in any regime

1623
01:09:48,920 --> 01:09:53,280
and it does seem to work pretty well consistently.

1624
01:09:53,280 --> 01:09:56,600
And but what you do find on like the leaderboard

1625
01:09:56,600 --> 01:09:58,220
tournament participants, you'll see some people

1626
01:09:58,220 --> 01:09:59,880
who stay at the top of the leaderboard

1627
01:09:59,880 --> 01:10:01,680
for weeks and weeks and weeks and weeks,

1628
01:10:01,680 --> 01:10:06,680
then suddenly precipitate fall like down the leaderboard

1629
01:10:07,160 --> 01:10:11,040
as demonstrating some sort of regime effects.

1630
01:10:11,040 --> 01:10:13,760
One really kind of interesting thing I did was

1631
01:10:14,760 --> 01:10:17,960
I fit like a mixture of linear models to the data.

1632
01:10:17,960 --> 01:10:19,960
So if you fit just like a mixture of two linear models

1633
01:10:19,960 --> 01:10:22,280
where it's sort of selecting which eras to use

1634
01:10:22,280 --> 01:10:24,640
for which of the two linear models,

1635
01:10:24,640 --> 01:10:27,000
you basically, one linear model will get

1636
01:10:27,000 --> 01:10:28,280
about 60% of the errors,

1637
01:10:28,280 --> 01:10:30,480
one will get about 40% of the errors

1638
01:10:30,480 --> 01:10:33,080
and their weights will be almost mirror images

1639
01:10:33,080 --> 01:10:33,920
of each other.

1640
01:10:34,440 --> 01:10:38,160
And this just comes out like that is the optimal fit

1641
01:10:38,160 --> 01:10:40,200
for roughly for 40% of the errors

1642
01:10:40,200 --> 01:10:43,480
that basically completely the opposite of the other eras.

1643
01:10:44,640 --> 01:10:47,440
Which yeah, demonstrating some like,

1644
01:10:47,440 --> 01:10:50,160
that's why markets are extremely hard

1645
01:10:50,160 --> 01:10:53,040
cause like something that works well a lot of the time

1646
01:10:53,040 --> 01:10:56,800
it suddenly would just work really oppositely horribly.

1647
01:10:56,800 --> 01:10:59,960
And so you're often trying to just split this difference

1648
01:10:59,960 --> 01:11:03,200
to find something that doesn't work super well at one time

1649
01:11:03,200 --> 01:11:05,600
and then we'll like crater at another time.

1650
01:11:05,600 --> 01:11:07,480
That's the meta model wants to kind of work

1651
01:11:07,480 --> 01:11:09,680
really like pretty good all the time.

1652
01:11:10,880 --> 01:11:12,640
And that's one of the things that ensembling

1653
01:11:12,640 --> 01:11:14,920
all these models that maybe even the individual models

1654
01:11:14,920 --> 01:11:16,640
probably have a lot more regime characteristics

1655
01:11:16,640 --> 01:11:18,080
than this overall meta model.

1656
01:11:19,240 --> 01:11:21,360
I wondered whether folks were using

1657
01:11:21,360 --> 01:11:23,320
some really esoteric approaches.

1658
01:11:23,320 --> 01:11:25,680
I mean, I'm interested in geometric deep learning

1659
01:11:25,680 --> 01:11:28,000
and algorithmic reasoning and, you know,

1660
01:11:28,000 --> 01:11:32,760
even think like esoteric options like cellular automata.

1661
01:11:33,600 --> 01:11:35,520
Do you see anything like that getting traction

1662
01:11:35,520 --> 01:11:37,680
or it may be even discrete program synthesis?

1663
01:11:39,320 --> 01:11:41,200
I don't know.

1664
01:11:41,200 --> 01:11:44,640
Cause yeah, like I only see what people

1665
01:11:44,640 --> 01:11:47,040
are willing to post and share on forums.

1666
01:11:47,040 --> 01:11:48,760
And there's quite a bit of sharing

1667
01:11:48,760 --> 01:11:50,760
on our forums of information,

1668
01:11:50,760 --> 01:11:54,160
but there's definitely some people at the top of leaderboards

1669
01:11:54,160 --> 01:11:57,040
who are doing something that's working quite well for them

1670
01:11:57,040 --> 01:11:59,600
for quite a long time that they haven't shared.

1671
01:12:00,440 --> 01:12:04,200
And so it's, I'm not even sure what all the people are doing,

1672
01:12:04,200 --> 01:12:09,200
but there are, I mean, people allude to using like tricks.

1673
01:12:10,320 --> 01:12:12,720
I mean, that they've learned in different jobs.

1674
01:12:12,720 --> 01:12:16,240
I mean, we have some people with like a variety of backgrounds.

1675
01:12:16,240 --> 01:12:19,160
It's been really cool to like see this community grow

1676
01:12:19,160 --> 01:12:21,800
and have people who are like astrophysicists,

1677
01:12:21,800 --> 01:12:25,120
particle physicists, people who are doing like

1678
01:12:26,720 --> 01:12:29,240
like computer vision and whatever

1679
01:12:29,240 --> 01:12:31,240
sort of techniques they've learned in their different fields

1680
01:12:31,240 --> 01:12:33,600
and try to use them on this problem.

1681
01:12:33,600 --> 01:12:35,280
That was what sort of attracted me as like,

1682
01:12:35,280 --> 01:12:37,440
I was doing like computational neuroscience

1683
01:12:37,440 --> 01:12:39,680
and I saw this problem as like,

1684
01:12:39,680 --> 01:12:41,680
oh, this is a complete free playground.

1685
01:12:41,680 --> 01:12:43,040
You can do whatever you want.

1686
01:12:43,040 --> 01:12:45,240
And so it was a fun opportunity to try out ideas

1687
01:12:45,240 --> 01:12:48,640
that wouldn't really work well in computational neuroscience.

1688
01:12:48,640 --> 01:12:49,480
Yeah, indeed.

1689
01:12:49,480 --> 01:12:53,160
And physics, I mean, the road to reality by Roger Penrose,

1690
01:12:53,160 --> 01:12:54,840
I think it was Michael Bronstein who said

1691
01:12:54,840 --> 01:12:56,880
that if you could summarize the entire book in one word,

1692
01:12:56,880 --> 01:12:58,400
it would be symmetry.

1693
01:12:58,440 --> 01:13:01,680
And there's also another key idea from a lot of researchers,

1694
01:13:01,680 --> 01:13:03,520
which is abstraction, you know,

1695
01:13:03,520 --> 01:13:05,600
which is like some meta property

1696
01:13:05,600 --> 01:13:07,280
of the relationship between data.

1697
01:13:07,280 --> 01:13:09,080
So, you know, you probably have lots of folks

1698
01:13:09,080 --> 01:13:10,240
coming in from different fields

1699
01:13:10,240 --> 01:13:12,360
and they have some very, very interesting approaches

1700
01:13:12,360 --> 01:13:13,560
to solving this problem.

1701
01:13:14,600 --> 01:13:15,600
Yeah, for sure.

1702
01:13:16,440 --> 01:13:18,600
Yeah, I mean, I have, I mean, there's people

1703
01:13:18,600 --> 01:13:20,960
who use some like interesting like auto encoders

1704
01:13:20,960 --> 01:13:22,920
to try to learn structure from data

1705
01:13:22,920 --> 01:13:24,320
as a way to learn features.

1706
01:13:25,320 --> 01:13:28,280
People using, it's interesting non-linear

1707
01:13:28,280 --> 01:13:30,120
dimensionality reduction techniques

1708
01:13:30,120 --> 01:13:34,360
to try to, yeah, to try to find various features.

1709
01:13:36,440 --> 01:13:39,680
It's, and yeah, even some,

1710
01:13:39,680 --> 01:13:42,320
some things people do do some sort of interesting

1711
01:13:42,320 --> 01:13:45,720
feature selection or denoising types of things

1712
01:13:45,720 --> 01:13:47,520
that they've learned in their fields.

1713
01:13:48,920 --> 01:13:52,120
Yeah, it's always interesting to me to see like

1714
01:13:52,160 --> 01:13:55,320
how different fields that use machine learning

1715
01:13:55,320 --> 01:13:56,640
use it in different ways

1716
01:13:56,640 --> 01:14:00,480
and what sort of tricks and tips might cross over.

1717
01:14:00,480 --> 01:14:01,320
I was going to ask about that

1718
01:14:01,320 --> 01:14:04,120
because you have loads and loads of features

1719
01:14:04,120 --> 01:14:07,600
and there's this problem called the curse of dimensionality.

1720
01:14:07,600 --> 01:14:12,040
Right, so, you know, when the number of dimensions increases

1721
01:14:12,040 --> 01:14:14,120
the volume of the space increases exponentially,

1722
01:14:14,120 --> 01:14:17,000
which means like this concept of nearness basically disappears

1723
01:14:17,000 --> 01:14:18,800
and there's statistical models don't work anymore.

1724
01:14:18,800 --> 01:14:21,800
So, you know, presumably people would do things like,

1725
01:14:21,800 --> 01:14:24,640
I don't know, dimensionality reduction feature selection.

1726
01:14:24,640 --> 01:14:26,160
I mean, neural networks are quite clever

1727
01:14:26,160 --> 01:14:29,320
in the sense that they, via a variety of methods,

1728
01:14:29,320 --> 01:14:30,840
overcome the curse of dimensionality

1729
01:14:30,840 --> 01:14:32,880
by learning some data manifold or whatever.

1730
01:14:32,880 --> 01:14:35,040
But, you know, it's with natural data,

1731
01:14:35,040 --> 01:14:37,960
it's not with financial data, so it's not a given.

1732
01:14:37,960 --> 01:14:40,320
It's, yeah, and this is actually one of the things

1733
01:14:40,320 --> 01:14:42,800
that was really intriguing to me

1734
01:14:42,800 --> 01:14:44,400
when I started in finance is,

1735
01:14:44,400 --> 01:14:46,840
so in science, when you're doing regressions

1736
01:14:46,840 --> 01:14:48,720
you're trying to find often sparse solutions.

1737
01:14:48,720 --> 01:14:51,400
You're trying to find the sort of small number of variables

1738
01:14:51,400 --> 01:14:52,440
to predict your targets,

1739
01:14:52,440 --> 01:14:53,960
to try to find whatever sort of maybe

1740
01:14:53,960 --> 01:14:55,600
causal relationships there are.

1741
01:14:56,680 --> 01:15:01,400
In finance, we often try to do exactly the opposite,

1742
01:15:01,400 --> 01:15:04,320
where we want our models to care about all the features

1743
01:15:04,320 --> 01:15:05,720
a little bit.

1744
01:15:05,720 --> 01:15:09,520
And so, we do, we'll do something like what we call

1745
01:15:09,520 --> 01:15:10,880
a feature neutralization,

1746
01:15:10,880 --> 01:15:12,560
where basically you take your prediction,

1747
01:15:12,560 --> 01:15:14,280
take the linear model of your prediction

1748
01:15:14,280 --> 01:15:16,080
from the features and subtract it off.

1749
01:15:16,080 --> 01:15:17,520
And so, you're making your prediction

1750
01:15:17,520 --> 01:15:20,120
not linearly correlated or linearly dependent

1751
01:15:20,120 --> 01:15:21,720
on any of your features.

1752
01:15:21,720 --> 01:15:22,920
We're doing some fraction of that.

1753
01:15:22,920 --> 01:15:26,120
So, just trying to remove too strong of a linear relationship

1754
01:15:26,120 --> 01:15:28,000
between a feature and your prediction.

1755
01:15:29,080 --> 01:15:31,160
And you do other regularization techniques

1756
01:15:31,160 --> 01:15:33,160
like in your tree learning,

1757
01:15:33,160 --> 01:15:35,680
maybe one thing that works quite well

1758
01:15:35,680 --> 01:15:37,640
is using like column sample by tree,

1759
01:15:37,640 --> 01:15:39,000
instead of to very low value.

1760
01:15:39,000 --> 01:15:40,400
So, each tree is only considering

1761
01:15:40,400 --> 01:15:42,000
a small subset of features.

1762
01:15:42,000 --> 01:15:44,120
And so, your ensemble is sort of,

1763
01:15:44,120 --> 01:15:46,880
you use as a lot of the different features

1764
01:15:46,880 --> 01:15:48,200
because it's sort of each tree

1765
01:15:48,200 --> 01:15:50,200
only has access to 10% of the features

1766
01:15:51,280 --> 01:15:52,560
across your whole ensemble.

1767
01:15:52,560 --> 01:15:55,800
You are probably using a lot of your features a little bit.

1768
01:15:55,800 --> 01:15:58,120
And that tends to work quite well.

1769
01:15:58,120 --> 01:15:59,840
And the reason is,

1770
01:15:59,840 --> 01:16:01,440
it's because features will work for a while

1771
01:16:01,440 --> 01:16:03,120
and then they'll just turn around on you.

1772
01:16:03,120 --> 01:16:04,920
And so, you don't want to be sort of

1773
01:16:04,920 --> 01:16:07,360
super dependent on any one feature.

1774
01:16:08,920 --> 01:16:11,840
And so, yeah, it does make the cursor dimensionality

1775
01:16:11,840 --> 01:16:13,440
kind of worse in some ways

1776
01:16:13,440 --> 01:16:17,440
because you don't wanna necessarily find

1777
01:16:17,480 --> 01:16:19,400
just a small subset of variables

1778
01:16:19,400 --> 01:16:21,960
that are the best

1779
01:16:21,960 --> 01:16:24,160
because sometimes that will maybe give you

1780
01:16:24,160 --> 01:16:25,680
a really good model for a while,

1781
01:16:25,680 --> 01:16:26,960
but sometimes all of a sudden,

1782
01:16:26,960 --> 01:16:28,120
those will just turn around on you.

1783
01:16:28,120 --> 01:16:30,960
And then your model just like is almost anti-correlated

1784
01:16:30,960 --> 01:16:32,360
where it should be.

1785
01:16:32,360 --> 01:16:34,000
Yeah, it's so interesting.

1786
01:16:34,000 --> 01:16:36,280
You know, like this problem with the changing dependencies.

1787
01:16:36,280 --> 01:16:39,320
So, essentially you're modeling a non-stationary process

1788
01:16:39,320 --> 01:16:40,280
which makes it much harder.

1789
01:16:40,280 --> 01:16:43,400
And when I was speaking with Sarah Hooker the other day,

1790
01:16:43,400 --> 01:16:45,320
she was talking about fairness and bias in models.

1791
01:16:45,320 --> 01:16:47,160
And part of the problem there is,

1792
01:16:47,280 --> 01:16:50,120
we optimize for headline metrics like accuracy.

1793
01:16:50,120 --> 01:16:51,880
And when you decompose the training set

1794
01:16:51,880 --> 01:16:55,200
into let's say different categories like men and women

1795
01:16:55,200 --> 01:16:56,920
and people who live in London,

1796
01:16:57,680 --> 01:16:59,800
the accuracy is very stratified.

1797
01:16:59,800 --> 01:17:02,000
It might perform very badly for people that live in London,

1798
01:17:02,000 --> 01:17:03,880
but very good for people that live in New York.

1799
01:17:03,880 --> 01:17:05,840
You know, and then you start getting into the situation

1800
01:17:05,840 --> 01:17:07,880
of saying, okay, well, I'll build an ensemble of models

1801
01:17:07,880 --> 01:17:10,360
that are independently optimized for all the different things.

1802
01:17:10,360 --> 01:17:12,280
But then you have this impedance mismatch

1803
01:17:12,280 --> 01:17:13,800
between this global, you know,

1804
01:17:13,800 --> 01:17:16,160
accuracy that you were optimizing for

1805
01:17:16,160 --> 01:17:17,640
and are on the benchmarks.

1806
01:17:18,880 --> 01:17:21,480
Yeah, no, it's a really interesting property

1807
01:17:21,480 --> 01:17:24,480
of these things is, yeah,

1808
01:17:24,480 --> 01:17:25,720
especially classification models

1809
01:17:25,720 --> 01:17:27,760
where they will work well for some categories

1810
01:17:27,760 --> 01:17:28,600
and not others.

1811
01:17:28,600 --> 01:17:32,800
And it can be sort of tricky to find out why is like,

1812
01:17:32,800 --> 01:17:34,800
are those features just more discriminative

1813
01:17:34,800 --> 01:17:38,520
or like, are these classes somehow harder to tell apart

1814
01:17:38,520 --> 01:17:40,200
just in some way?

1815
01:17:41,680 --> 01:17:43,040
It's, yeah, it's,

1816
01:17:44,040 --> 01:17:47,080
but I'm glad people are starting to like look at

1817
01:17:47,080 --> 01:17:49,720
and try to dig into some of these like details

1818
01:17:49,720 --> 01:17:51,520
rather than just looking at headline metrics.

1819
01:17:51,520 --> 01:17:54,880
And I'm also sort of happy that the field is sort of moving

1820
01:17:54,880 --> 01:17:57,440
to like this out of distribution learning

1821
01:17:57,440 --> 01:18:00,560
is becoming a much more interesting topic.

1822
01:18:00,560 --> 01:18:02,600
Because like, that is what really matters

1823
01:18:02,600 --> 01:18:03,600
in making machine learning

1824
01:18:03,600 --> 01:18:06,000
that is going to affect the real world

1825
01:18:06,000 --> 01:18:07,640
is it needs to work out of distribution,

1826
01:18:07,640 --> 01:18:09,040
out of your sort of training

1827
01:18:09,040 --> 01:18:12,080
and test split distribution as well as possible.

1828
01:18:12,120 --> 01:18:13,200
And like how you do that is,

1829
01:18:13,200 --> 01:18:16,000
I mean, still very much an open question clearly.

1830
01:18:16,000 --> 01:18:18,560
And how well you could potentially do that

1831
01:18:18,560 --> 01:18:21,280
is even still an open question.

1832
01:18:21,280 --> 01:18:22,560
But that is one of the,

1833
01:18:23,600 --> 01:18:26,240
I mean, that is sort of what true intelligence is

1834
01:18:26,240 --> 01:18:28,520
to something like humans are pretty good

1835
01:18:28,520 --> 01:18:30,960
at adapting out of distribution.

1836
01:18:31,880 --> 01:18:34,480
And what is it about us?

1837
01:18:34,480 --> 01:18:36,880
What are like, how are we able to do that?

1838
01:18:36,880 --> 01:18:39,320
And how do we make our sort of machine learning systems

1839
01:18:39,320 --> 01:18:40,600
work better that way?

1840
01:18:40,600 --> 01:18:42,160
How are we sort of able to?

1841
01:18:43,360 --> 01:18:45,520
I mean, yeah, I think it probably has something to do

1842
01:18:45,520 --> 01:18:47,760
is we're able to learn sort of causal structures

1843
01:18:47,760 --> 01:18:49,880
that work well.

1844
01:18:49,880 --> 01:18:52,200
And the distribution can be very different,

1845
01:18:52,200 --> 01:18:55,400
but the sort of causal structures remain.

1846
01:18:55,400 --> 01:18:58,480
And we're able to somehow infer that causal structures

1847
01:18:58,480 --> 01:19:02,280
from data, from just our sense data and our world models.

1848
01:19:03,800 --> 01:19:04,960
And yeah, basically the question is,

1849
01:19:04,960 --> 01:19:07,040
how do we make our machine learning systems

1850
01:19:07,040 --> 01:19:10,080
be able to do similar sorts of things?

1851
01:19:11,600 --> 01:19:14,440
Yeah, this has been absolutely amazing.

1852
01:19:14,440 --> 01:19:16,160
Do you have any final thoughts?

1853
01:19:16,160 --> 01:19:18,000
Where can people find out more information

1854
01:19:18,000 --> 01:19:19,000
about you, Michael?

1855
01:19:20,600 --> 01:19:22,120
So, let's see.

1856
01:19:22,120 --> 01:19:26,160
Well, so I want to point people first to just like Numeri,

1857
01:19:26,160 --> 01:19:29,800
N-U-M-E-R.AI is the website.

1858
01:19:29,800 --> 01:19:33,480
I am fairly active in the forums

1859
01:19:33,480 --> 01:19:35,360
and the rocket chat we have,

1860
01:19:35,360 --> 01:19:40,280
which is sort of just our own personal chat service

1861
01:19:40,280 --> 01:19:43,360
for tournament participants to communicate with each other.

1862
01:19:43,360 --> 01:19:46,440
And I occasionally only post some of the forums there.

1863
01:19:46,440 --> 01:19:48,920
That's probably the best way to like get in contact

1864
01:19:48,920 --> 01:19:51,440
to just message me on rocket chat.

1865
01:19:52,720 --> 01:19:57,640
And yeah, so that's, yeah,

1866
01:19:57,640 --> 01:20:01,080
there's probably that's way to get in contact.

1867
01:20:01,080 --> 01:20:05,600
My also, my email is mdo at Numeri.ai.

1868
01:20:06,880 --> 01:20:09,840
And I would, yeah, I really love if people come,

1869
01:20:09,840 --> 01:20:11,880
check out the tournament, give feedback,

1870
01:20:11,880 --> 01:20:13,880
and start participating.

1871
01:20:13,880 --> 01:20:18,000
I've, yeah, I found that it was a lot of fun as a participant.

1872
01:20:18,840 --> 01:20:21,680
And yeah, I joined the company partly

1873
01:20:21,680 --> 01:20:23,160
so I was starting to make more money

1874
01:20:23,160 --> 01:20:26,760
during the tournament than I was at my job in science.

1875
01:20:26,760 --> 01:20:30,840
And so, yeah, it's a pretty fun hobby and side gig

1876
01:20:30,840 --> 01:20:34,600
and potentially even quite lucrative.

1877
01:20:34,600 --> 01:20:35,440
Amazing.

1878
01:20:35,440 --> 01:20:37,760
Well, Dr. Michael Oliver, it's been an absolute honor.

1879
01:20:37,760 --> 01:20:39,920
Thank you so much for joining us this evening.

1880
01:20:39,920 --> 01:20:41,200
Thanks for so much for having me.

1881
01:20:41,200 --> 01:20:42,520
It's been so much fun.

