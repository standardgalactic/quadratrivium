WEBVTT

00:00.000 --> 00:03.680
Previous generations of the model have been weak reasoners,

00:03.680 --> 00:04.560
but they do reason.

00:04.560 --> 00:06.280
In the same way that hallucination

00:06.280 --> 00:08.840
was an existential threat to this technology,

00:08.840 --> 00:11.480
no, we'll never be able to trust this stuff.

00:11.480 --> 00:13.120
There are hundreds of millions of people

00:13.120 --> 00:14.800
using this techno, and they trust it.

00:14.800 --> 00:16.000
It's actually useful for them.

00:16.000 --> 00:18.360
We're making very good progress on the hallucination problem.

00:18.360 --> 00:20.480
I think we'll make very good progress

00:20.480 --> 00:22.360
this year and next on Reasoning.

00:22.360 --> 00:25.360
Here we are again, another episode of MLST.

00:25.360 --> 00:28.600
Today with Aidan Gomez, the CEO of Coheir.

00:28.600 --> 00:31.360
Now, I interviewed Aidan in London a couple of weeks ago

00:31.360 --> 00:32.840
just after their build event

00:32.840 --> 00:35.440
and after Aidan did his presentation,

00:35.440 --> 00:38.840
I sat him down for an hour and I gave him a grilling.

00:38.840 --> 00:40.360
And he was such a good sport

00:40.360 --> 00:42.960
for being so transparent and authentic.

00:42.960 --> 00:45.000
This is the difference with Coheir.

00:45.000 --> 00:46.840
They say it like it is.

00:46.840 --> 00:48.160
There's no bullshit.

00:48.160 --> 00:49.720
There's no digital gods.

00:49.720 --> 00:51.520
There's no super intelligence.

00:51.520 --> 00:53.360
They're just a bunch of folks

00:53.360 --> 00:55.560
solving real world business problems.

00:55.560 --> 00:57.160
Their models are incredibly good

00:57.160 --> 00:59.800
for a true vlogmented generation, for multilingual.

00:59.800 --> 01:02.080
But they also have some serious challenges

01:02.080 --> 01:03.400
that they need to overcome.

01:03.400 --> 01:05.960
We spoke about the AI risk discussion,

01:05.960 --> 01:08.400
where the language models take away our agency,

01:08.400 --> 01:10.120
how he's dealing with policy, right?

01:10.120 --> 01:12.840
So on the one hand, he's talking with governments,

01:12.840 --> 01:15.760
trying to get them to allow startups

01:15.760 --> 01:18.520
to be more competitive, to innovate.

01:18.520 --> 01:21.040
But at the same token, he's also very concerned

01:21.040 --> 01:23.440
about some of the societal risks of AI.

01:23.440 --> 01:25.000
So that's quite an interesting burden

01:25.000 --> 01:26.880
and juxtaposition that he has to hold

01:27.080 --> 01:28.280
in his mind.

01:28.280 --> 01:30.600
We spoke a lot about the company culture at Coheir.

01:30.600 --> 01:32.040
Now, I'm very impressed with Coheir.

01:32.040 --> 01:34.480
All of the people I've spoken to have been very smart,

01:34.480 --> 01:36.280
just really nice people.

01:36.280 --> 01:39.440
And how has he cultivated that culture?

01:39.440 --> 01:41.080
He was very frank and transparent

01:41.080 --> 01:44.360
about some of the mistakes he made early on as a CEO.

01:44.360 --> 01:46.480
So yeah, plenty to get your teeth into.

01:46.480 --> 01:48.280
I hope you enjoy the conversation.

01:49.440 --> 01:51.200
For Coheir, I think we're a little bit different

01:51.200 --> 01:53.160
than some of the other companies in the space,

01:53.160 --> 01:56.520
in the sense that we're not really here to build AGI.

01:56.520 --> 01:59.280
What we're here to do is create value for the world.

01:59.280 --> 02:00.840
And the way that we think we can do that

02:00.840 --> 02:05.000
is by putting this tech into the hands of enterprises

02:05.000 --> 02:07.240
so that they can integrate it into their products,

02:07.240 --> 02:10.640
they can augment their workforce with it.

02:10.640 --> 02:13.440
And so it's all about driving value

02:13.440 --> 02:15.600
and really putting this technology

02:15.600 --> 02:17.880
into the hands of more people

02:17.880 --> 02:21.000
and driving productivity for humanity.

02:21.840 --> 02:23.160
Aiden, welcome to MLSD.

02:23.160 --> 02:25.160
It's an absolute honor to have you on.

02:25.200 --> 02:27.200
Thank you so much. Appreciate it.

02:27.200 --> 02:28.640
There's a bit of a last mile problem

02:28.640 --> 02:29.800
with large language models,

02:29.800 --> 02:33.680
so you folks have created this incredible general technology.

02:33.680 --> 02:36.080
But when enterprises implement it,

02:36.080 --> 02:39.560
they have a whole bunch of legislative constraints,

02:39.560 --> 02:41.280
security constraints.

02:41.280 --> 02:45.600
Yeah, yeah, I think there's loads of barriers to access.

02:45.600 --> 02:50.440
Privacy to policy to just the familiarity of the teams

02:50.440 --> 02:52.560
with the tech, it's brand new.

02:52.560 --> 02:54.920
And so they haven't built with this technology before

02:54.920 --> 02:56.960
and they're still getting up to speed

02:56.960 --> 03:00.800
with the opportunity space, what they can do with it.

03:00.800 --> 03:05.240
That being said, people are so excited about AI

03:05.240 --> 03:08.240
and its opportunity that the motivation and the will

03:08.240 --> 03:10.760
is there to overcome a lot of these hurdles.

03:10.760 --> 03:13.520
So we're trying to help with that as much as we can,

03:13.520 --> 03:17.320
whether it's like our LMU education course

03:17.320 --> 03:19.720
to help general developers get up to speed

03:19.720 --> 03:21.280
on how to build with this stuff

03:21.280 --> 03:23.480
or us engaging at the policy level

03:23.480 --> 03:26.120
to make sure that we have sensible policy

03:26.120 --> 03:28.880
and not over-regulation or regulation

03:28.880 --> 03:33.880
that hurts startups or encumbers industry in adopting it.

03:34.040 --> 03:35.560
So we're trying to pull the levers that we can

03:35.560 --> 03:37.280
to help accelerate the adoption

03:37.280 --> 03:40.560
and make sure that it gets adopted in the right way.

03:40.560 --> 03:42.960
But yeah, no, it's definitely the past two years

03:42.960 --> 03:46.840
have been a push.

03:46.840 --> 03:49.520
There's a lot of stuff slowing down adoption

03:49.520 --> 03:51.360
that I would love to see eased.

03:51.400 --> 03:53.720
I think the tools need to get better, easier to use,

03:53.720 --> 03:55.720
more intuitive, more robust.

03:55.720 --> 03:57.960
Prompt engineering is still a thing.

03:57.960 --> 03:59.520
It shouldn't be right.

03:59.520 --> 04:02.200
It shouldn't matter how you phrase something specifically.

04:02.200 --> 04:04.560
It should be, the model should be smart enough

04:04.560 --> 04:06.680
to generally understand your intent

04:06.680 --> 04:09.520
and take action on your behalf reliably.

04:09.520 --> 04:11.680
So even at the technological layer,

04:11.680 --> 04:14.320
I think us as model builders, we have a lot to do

04:14.320 --> 04:16.480
to bring the barriers down.

04:16.480 --> 04:20.680
But I'm optimistic, like the pace of progress is fantastic.

04:20.680 --> 04:21.760
Yes.

04:21.760 --> 04:25.000
On that kind of prompt Britonist thing,

04:25.000 --> 04:28.080
I wonder whether you think that we are on a path to have it.

04:28.080 --> 04:30.000
I mean, in an ideal world,

04:30.000 --> 04:33.240
the model and the application would be completely decoupled.

04:33.240 --> 04:34.600
So you could swap the model out

04:34.600 --> 04:36.080
or when you folks bring out a new model,

04:36.080 --> 04:38.600
we can just swap it out and nothing breaks.

04:38.600 --> 04:40.360
But at the moment, that's not the case.

04:40.360 --> 04:43.640
But as the models become increasingly better,

04:43.640 --> 04:45.960
do you think they will be robust in that way?

04:45.960 --> 04:47.160
They should be, right?

04:48.160 --> 04:50.840
There will always be quirks to different models

04:50.840 --> 04:52.800
because we're all training on, you know,

04:52.800 --> 04:56.680
we hope different data that focuses on different aspects

04:56.680 --> 05:00.680
or elicits different behavior in the model.

05:00.680 --> 05:05.440
So there will always be quirks to the behavior of models,

05:05.440 --> 05:07.440
the personalities of models,

05:07.440 --> 05:09.200
what they're good at and bad at.

05:09.200 --> 05:13.440
But in general, in terms of like following an instruction,

05:13.440 --> 05:16.680
we should be quite robust to that universally.

05:18.160 --> 05:19.680
And so the ideal is that, yeah,

05:19.680 --> 05:22.680
you can just take a prompt and drop it into any system

05:22.680 --> 05:24.200
and see which one performs best

05:24.200 --> 05:26.320
and then move forward with that one.

05:26.320 --> 05:29.080
In reality, the status quo is a prompt

05:29.080 --> 05:30.160
that works on one system,

05:30.160 --> 05:32.920
fundamentally does not work on another.

05:32.920 --> 05:37.920
And so there's this rift or these walls in between systems

05:38.240 --> 05:40.320
that make them very, very different.

05:40.320 --> 05:42.840
Hopefully that'll start to lift.

05:42.840 --> 05:46.280
There's a lot of effort going into data augmentation

05:46.280 --> 05:47.800
that makes these models more robust

05:47.800 --> 05:50.320
to changes in prompt space.

05:50.320 --> 05:51.720
We're doing a lot of work on that.

05:51.720 --> 05:55.120
It's driven a lot by synthetic data

05:55.120 --> 05:59.560
and finding, doing search to basically find the prompts

05:59.560 --> 06:02.320
or the augmentations that changes to prompts

06:02.320 --> 06:06.920
that break the model and then training to fix that break.

06:07.880 --> 06:09.960
So I'm optimistic that sort of brittleness

06:09.960 --> 06:10.800
is gonna go away.

06:10.800 --> 06:11.800
Interesting.

06:11.800 --> 06:14.400
So kind of finding problems with the model

06:14.400 --> 06:17.400
and then robustifying and robustifying.

06:17.400 --> 06:19.240
In doing so, how does that change

06:19.240 --> 06:21.280
the characteristics of the model?

06:21.280 --> 06:24.480
Does it make it less creative or less capable in some sense?

06:24.480 --> 06:26.440
I mean, what do you ever feel for the trade-offs there?

06:26.440 --> 06:29.240
Yeah, I mean, I don't think so.

06:29.240 --> 06:32.680
I think that that's orthogonal.

06:32.680 --> 06:34.280
The process of making it more robust

06:34.280 --> 06:35.720
is orthogonal to creativity.

06:35.720 --> 06:37.840
There are aspects of the post-training procedure

06:37.840 --> 06:41.920
that do reduce creativity or, you know,

06:41.960 --> 06:44.720
some people like to say like lobotomize the model.

06:46.000 --> 06:47.680
So it's definitely a problem.

06:47.680 --> 06:50.320
It's something that we watch for

06:50.320 --> 06:54.040
and we're trying to prevent.

06:54.040 --> 06:59.040
I would say that one of the most disappointing aspects

06:59.040 --> 07:00.960
of the current regime of building these models

07:00.960 --> 07:04.440
is that a lot of people train on synthetic data

07:04.440 --> 07:05.760
from one source, right?

07:05.760 --> 07:07.720
Like just from the GPT models.

07:07.720 --> 07:09.680
And so all of them, all of the models

07:09.680 --> 07:11.760
that are being created,

07:11.760 --> 07:13.400
they sort of speak the same.

07:13.400 --> 07:15.680
They kind of have the same personality

07:15.680 --> 07:18.560
and it leads to this collapse

07:18.560 --> 07:21.160
into a lot of different models

07:21.160 --> 07:22.640
looking and feeling the same.

07:25.160 --> 07:26.200
And that makes them boring

07:27.280 --> 07:31.800
because like you have the same shortcomings across models

07:31.800 --> 07:34.520
rather than if you have a diverse set of models

07:34.520 --> 07:36.840
that have different failures in different places,

07:36.840 --> 07:39.760
you can much better address, you know,

07:39.760 --> 07:42.800
the preferences of much more people.

07:44.480 --> 07:48.320
I've noticed due to synthetic data taking off,

07:49.280 --> 07:51.600
just a total collapse in terms of

07:52.600 --> 07:57.400
the different types of behavior models exhibit

07:57.400 --> 08:01.280
and that cohere because our customers are enterprises.

08:01.280 --> 08:02.360
Like that's who we sell to.

08:02.360 --> 08:04.080
It's not consumers, it's not, you know,

08:04.080 --> 08:06.240
anything other than enterprises

08:06.240 --> 08:08.600
who want to adopt this tech.

08:08.600 --> 08:10.040
And they're very, very sensitive

08:10.040 --> 08:11.840
to what data went into the model.

08:11.840 --> 08:14.800
And so we exclude other model providers,

08:14.800 --> 08:18.040
data, you know, very aggressively.

08:18.040 --> 08:19.320
Of course, some will slip in

08:19.320 --> 08:21.120
as we're scraping the web, et cetera.

08:21.120 --> 08:23.000
But we make a very concerted effort

08:23.000 --> 08:26.280
to avoid other model outputs.

08:26.280 --> 08:28.320
And so if you talk to our model,

08:28.320 --> 08:30.280
when we release command R and R plus,

08:30.280 --> 08:33.280
one of the things I kept reading on Reddit and Twitter

08:33.280 --> 08:34.600
was it feels different.

08:34.600 --> 08:37.680
Like something feels special about this model.

08:37.680 --> 08:40.360
I don't think that's any like magic at cohere

08:40.360 --> 08:41.800
other than the fact that we didn't do

08:41.800 --> 08:43.240
what the other guys are doing,

08:43.240 --> 08:46.360
which is training on the model outputs of OpenAI.

08:46.360 --> 08:47.200
I agree with you.

08:47.200 --> 08:50.040
So when people, you know,

08:50.040 --> 08:52.000
see chat GPT or whatever for the first time,

08:52.000 --> 08:53.760
that they're blown away by it.

08:53.760 --> 08:57.200
But there are motifs that come up again and again

08:57.200 --> 09:00.040
and again, unraveling the mysteries, you know,

09:00.040 --> 09:02.640
delving into the intricate complexities

09:02.640 --> 09:04.000
or blah, blah, blah, blah, blah.

09:04.000 --> 09:06.320
And when you start to see these patterns

09:06.320 --> 09:09.080
and these constructions, you just start to think,

09:09.080 --> 09:10.720
oh, I don't like this very much

09:10.720 --> 09:12.080
because you start to see through it.

09:12.080 --> 09:13.160
It's a little bit like, you know,

09:13.160 --> 09:14.800
when you start to see through someone,

09:14.800 --> 09:16.520
they're not interesting anymore.

09:16.520 --> 09:18.720
And I haven't seen that with cohere,

09:18.720 --> 09:21.520
but I have seen it with many of the other models.

09:21.520 --> 09:24.040
Now, my intuition was always,

09:24.040 --> 09:25.840
I don't, I haven't really formed this very well,

09:25.840 --> 09:28.400
but I thought that maybe it could come from

09:28.400 --> 09:30.280
just the kind of data sets that we're using,

09:30.280 --> 09:34.040
or maybe it could come from the preference fine tuning.

09:34.400 --> 09:36.960
Are you saying that that monolithic effect

09:36.960 --> 09:40.080
is because they're kind of eating each other's poop?

09:40.080 --> 09:41.640
Yeah, no, yeah, yeah.

09:41.640 --> 09:44.440
It's some sort of like human centipede effect.

09:44.440 --> 09:48.960
I think, yeah, they're training on the outputs

09:48.960 --> 09:50.360
of a single model.

09:50.360 --> 09:52.960
And so it's all collapsing into that model's

09:52.960 --> 09:54.000
output distribution.

09:54.000 --> 09:56.080
And so if that output distribution has quirks

09:56.080 --> 09:59.440
like saying the word delve a lot,

09:59.440 --> 10:01.400
then it's gonna just pop up all over the place.

10:01.400 --> 10:03.880
And people will take it for granted that,

10:03.880 --> 10:06.280
oh, I guess LLMs just behave like this,

10:06.280 --> 10:07.640
but they don't have to.

10:07.640 --> 10:08.480
They don't have to.

10:08.480 --> 10:10.760
It's interesting how subjective creativity is as well,

10:10.760 --> 10:11.840
because a lot of people thought

10:11.840 --> 10:13.880
that it was creative a couple of years ago.

10:13.880 --> 10:14.840
And then when you see it everywhere,

10:14.840 --> 10:15.800
it's not creative anymore.

10:15.800 --> 10:18.280
So it needs to be novel to be creative.

10:18.280 --> 10:21.160
But I mean, you folks have just released

10:21.160 --> 10:22.720
the command R series of models

10:22.720 --> 10:25.480
and you've blown everyone away.

10:25.480 --> 10:26.360
Tell me about them.

10:26.360 --> 10:28.080
But if you wouldn't mind also,

10:28.080 --> 10:30.400
why did it take you a while to catch up

10:30.400 --> 10:31.800
and get state of the art performance?

10:31.840 --> 10:36.280
Yeah, we spent a lot of 2023 lagging.

10:36.280 --> 10:38.520
I think that that is accurate to say.

10:39.400 --> 10:43.880
What we were doing was sort of reorganizing internally.

10:43.880 --> 10:45.200
We were rebuilding the company,

10:45.200 --> 10:48.480
rebuilding the modeling team, the tech strategy

10:48.480 --> 10:52.800
and preparing for the runs that led to command R.

10:52.800 --> 10:54.120
It was clear to us that the process

10:54.120 --> 10:56.280
that we had used to build the first command

10:56.280 --> 10:58.960
and generations before that, it wasn't working.

10:58.960 --> 11:00.280
It wasn't gonna scale.

11:00.280 --> 11:03.520
And so we just rethought the entire pipeline.

11:03.520 --> 11:06.520
And it took us a while to rebuild things,

11:07.840 --> 11:10.360
run the experiments that we needed to run

11:10.360 --> 11:12.440
in order to make decisions on what the design

11:12.440 --> 11:17.160
of this new model building engine would look like.

11:19.040 --> 11:21.040
And then it takes time to do the runs.

11:21.040 --> 11:23.080
We spent a lot of last year doing that.

11:23.080 --> 11:25.880
But I think the results speak for themselves.

11:25.880 --> 11:28.120
And also with command R and R plus,

11:28.120 --> 11:31.880
it's just the first step in a series of new models

11:31.880 --> 11:33.200
that we wanna produce.

11:33.200 --> 11:37.320
We're very excited to lean into specific capabilities.

11:37.320 --> 11:41.720
And so while the general language model improvements,

11:41.720 --> 11:42.760
they're super important.

11:42.760 --> 11:43.600
They're crucial, right?

11:43.600 --> 11:44.720
Like the models have to get smarter.

11:44.720 --> 11:47.120
They have to get more capable.

11:47.120 --> 11:49.820
And we'll continue to press on that direction.

11:50.760 --> 11:54.080
We care about narrowing our focus a bit more.

11:54.120 --> 11:58.360
And so for 2024, even with the command R series,

11:58.360 --> 12:00.680
we focused in on rag and tool use.

12:00.680 --> 12:03.080
I think you're gonna see a continuation

12:03.080 --> 12:05.520
and extension of that focus

12:05.520 --> 12:07.600
really making these models robust

12:07.600 --> 12:10.600
at the key capabilities that enterprise cares about,

12:10.600 --> 12:12.000
that will drive productivity,

12:12.000 --> 12:14.720
that will automate really sophisticated processes

12:14.720 --> 12:19.160
that today, as humanity knows it,

12:19.160 --> 12:21.400
is only the domain of humans.

12:21.400 --> 12:22.920
We really wanna go after that.

12:23.920 --> 12:28.920
And give our models the ability to help in those spaces.

12:29.120 --> 12:30.680
Is it fair to say that,

12:30.680 --> 12:31.840
I don't know whether you feel

12:31.840 --> 12:35.000
that the general large language models are saturating.

12:35.000 --> 12:36.240
Maybe you could comment on that first,

12:36.240 --> 12:38.000
but if you do think that,

12:39.000 --> 12:40.440
does that give me a bit of a read

12:40.440 --> 12:43.800
that there's a move towards specialization of the models?

12:43.800 --> 12:45.560
I don't think they're saturating.

12:45.560 --> 12:46.960
I think they're getting so good

12:46.960 --> 12:49.720
that it's hard to see the incremental improvement.

12:50.720 --> 12:55.480
But that incremental improvement is extremely important.

12:55.480 --> 12:58.880
So once the models are smarter than you,

12:58.880 --> 13:02.120
it's really hard to, in a domain, in medicine,

13:02.120 --> 13:04.200
like Coher's model,

13:04.200 --> 13:07.960
it knows more than I do about medicine, for sure.

13:07.960 --> 13:09.160
Just absolutely.

13:09.160 --> 13:11.280
And so I can't really effectively assess

13:11.280 --> 13:14.000
whether we're improving in that dimension.

13:14.000 --> 13:15.240
I can't tell anyone.

13:15.240 --> 13:16.640
It's smarter than me.

13:16.640 --> 13:18.600
I trust it more than I trust myself

13:18.600 --> 13:23.440
to diagnose symptoms or process medical data.

13:23.440 --> 13:24.720
And so I'm not equipped to do that.

13:24.720 --> 13:27.040
Instead, what we need to do is create data sets

13:27.040 --> 13:30.000
or go out and find people who are still better

13:30.000 --> 13:32.560
than the model at those domains.

13:32.560 --> 13:35.560
And they can tell me whether it's improving.

13:35.560 --> 13:38.480
But for us, like the general population,

13:39.600 --> 13:43.920
at some point, we kind of stop seeing improvement

13:43.920 --> 13:45.040
between model versions.

13:45.040 --> 13:46.640
It's harder to feel.

13:46.960 --> 13:49.400
And you need to really zoom in

13:49.400 --> 13:51.080
to a place that you're an expert

13:51.080 --> 13:53.760
and that you know previous generations failed

13:53.760 --> 13:55.160
to see the progress.

13:56.200 --> 13:58.000
I think about it sometimes as like

13:59.360 --> 14:03.040
painting in a canvas of knowledge.

14:03.040 --> 14:06.560
And at some point, the holes in the canvas become so small.

14:06.560 --> 14:08.000
You have to take out a microscope

14:08.000 --> 14:10.200
to actually see it and paint it in.

14:10.200 --> 14:13.640
We're sort of in that part of the space for these models.

14:13.640 --> 14:15.560
And so improvement becomes much harder

14:15.560 --> 14:17.440
for us, the model builders,

14:17.440 --> 14:20.520
but it's much harder to feel and see for users

14:20.520 --> 14:25.520
who aren't diving in very close to analyze performance.

14:26.080 --> 14:27.160
I don't think it's saturating.

14:27.160 --> 14:29.040
I think it's still making,

14:29.040 --> 14:32.000
we're still making very significant progress.

14:32.000 --> 14:35.040
I do think the past 18 months,

14:36.280 --> 14:37.960
maybe a little bit less than 18 months.

14:37.960 --> 14:40.760
Yeah, the past 18 months, 12 months,

14:40.760 --> 14:43.920
we've been compressing.

14:44.920 --> 14:47.360
So we built these massive, giant,

14:47.360 --> 14:49.680
multi-trillion parameter models,

14:49.680 --> 14:53.200
which were just extraordinary artifacts

14:53.200 --> 14:56.240
of intelligence and capability.

14:56.240 --> 14:59.960
And we realized it's impractical.

14:59.960 --> 15:02.440
You can't actually put this thing into production, right?

15:02.440 --> 15:05.520
Like it takes 60, a 100s to certainly,

15:05.520 --> 15:08.880
it just, we could not productionize this.

15:08.880 --> 15:10.760
The economics don't work out.

15:10.760 --> 15:13.280
And so then we spent the year compressing

15:13.280 --> 15:16.800
those massive models down into much smaller form factors.

15:18.680 --> 15:23.680
There's very likely going to be a series of re-expansion

15:23.960 --> 15:28.960
and scale, both on the model scale

15:30.320 --> 15:34.800
in terms of parameters, but also data scale and data quality.

15:34.800 --> 15:37.960
And that's being supported by much better

15:37.960 --> 15:40.680
synthetic data methods that find much more useful

15:40.680 --> 15:44.520
synthetic data that are quite compelling at search

15:44.520 --> 15:48.560
to discover, to automatically discover weak points of models

15:48.560 --> 15:50.680
and then close those gaps.

15:51.760 --> 15:54.320
So I think we've, over the past year,

15:54.320 --> 15:58.320
gotten very good at making models more efficient.

15:58.320 --> 16:03.080
And we've created new methods that let us

16:03.080 --> 16:06.320
sort of just like plug in compute and data

16:06.320 --> 16:09.280
and have the model continuously improve.

16:09.280 --> 16:13.240
You've used words like smart and capabilities.

16:13.240 --> 16:16.760
And if you think of smart as knowledge,

16:16.760 --> 16:17.720
I completely agree with you.

16:17.720 --> 16:20.640
I think knowledge is a living thing.

16:20.640 --> 16:23.480
We're all improvising and we are generating

16:23.480 --> 16:24.760
new knowledge all the time.

16:24.760 --> 16:27.400
And it just increases exponentially.

16:27.400 --> 16:29.440
And there's no reason why language models

16:29.440 --> 16:31.200
can't become more and more and more knowledgeable

16:31.200 --> 16:33.960
because we just acquire more and more data.

16:33.960 --> 16:37.520
And in that sense, a medical doctor is smarter than me

16:37.520 --> 16:39.240
in that domain because they have the knowledge

16:39.440 --> 16:40.680
that I don't have.

16:40.680 --> 16:42.920
But some people could say that intelligence

16:42.920 --> 16:45.640
is something a little bit more abstract than that.

16:45.640 --> 16:48.600
It might be the ability to build models.

16:49.640 --> 16:51.200
It might be the ability to reason.

16:51.200 --> 16:53.240
It might be the ability to plan.

16:53.240 --> 16:56.040
And this is when we get into the kind of the age your own

16:56.040 --> 16:57.400
thing.

16:57.400 --> 17:00.240
So how do you demarcate those things?

17:00.240 --> 17:02.480
Are you saying that the models are becoming more knowledgeable

17:02.480 --> 17:04.600
but they're not necessarily becoming more intelligent

17:04.600 --> 17:05.520
like we are?

17:05.520 --> 17:10.520
I think reasoning is crucial to intelligence.

17:13.120 --> 17:16.200
I think that these models can reason

17:16.200 --> 17:18.080
and that's a controversial claim.

17:18.080 --> 17:20.160
I think a lot of people would debate that.

17:20.160 --> 17:21.320
A lot of people would make the claim

17:21.320 --> 17:22.960
that the architectures we're using

17:22.960 --> 17:25.240
or the methods we're using don't support

17:26.600 --> 17:27.920
that sort of behavior.

17:27.920 --> 17:31.400
I think that previous generations of the model

17:31.400 --> 17:34.320
have been weak reasoners, but they do reason.

17:35.560 --> 17:38.280
And it's not a discreet,

17:38.280 --> 17:40.040
does it have this capability or not?

17:40.040 --> 17:42.840
It's a continuum of how robust the reasoning engine

17:42.840 --> 17:44.480
inside these models is.

17:45.440 --> 17:47.240
We're getting much better methods

17:47.240 --> 17:50.320
for improving reasoning generally.

17:50.320 --> 17:51.640
We're getting much better methods

17:51.640 --> 17:54.120
of eliciting that behavior from the models

17:54.120 --> 17:55.920
and teaching them how to do it

17:55.920 --> 17:57.880
and apply it to many different domains,

17:57.880 --> 18:01.960
whether it's math, whether it's decision-making tasks,

18:01.960 --> 18:04.860
breaking down tasks, planning how to execute them.

18:06.440 --> 18:09.720
Those were key missing capabilities

18:09.720 --> 18:13.080
that were quite weak in previous generations of models,

18:13.080 --> 18:15.560
which are now starting to emerge

18:16.880 --> 18:19.960
in a significantly more robust fashion.

18:19.960 --> 18:22.760
And so in the same way that hallucination

18:22.760 --> 18:26.280
was it used to be an existential threat to this technology,

18:26.280 --> 18:28.520
no, we'll never be able to trust this stuff.

18:29.520 --> 18:32.160
There are hundreds of millions of people using this techno

18:32.160 --> 18:33.000
and they trust it.

18:33.000 --> 18:33.960
It's actually useful for them.

18:33.960 --> 18:36.280
They use it because it's useful to their job.

18:36.280 --> 18:38.680
We're making very good progress on the hallucination problem.

18:38.680 --> 18:40.840
I think we'll make very good progress

18:40.840 --> 18:43.160
this year and next on reasoning.

18:43.160 --> 18:46.640
I think it's just a capability,

18:46.640 --> 18:51.640
a skill that the model needs to be taught.

18:51.800 --> 18:54.560
And we're building the methods and data

18:54.560 --> 18:59.560
and techniques to support teaching, teaching these models.

18:59.600 --> 19:01.440
Yeah, it is interesting how you can kind of break

19:01.440 --> 19:03.240
intelligence down to all of these things

19:03.240 --> 19:05.240
and some you might argue are missing now,

19:05.240 --> 19:09.680
like planning, creativity is an interesting one.

19:09.680 --> 19:11.720
Agency is quite an interesting one.

19:11.720 --> 19:15.000
And presumably as a thing has more understanding

19:15.000 --> 19:16.280
and it has more autonomy,

19:16.280 --> 19:19.000
it could in principle develop agency

19:19.000 --> 19:19.960
at some point in the future.

19:19.960 --> 19:22.440
But you think of these things as skills.

19:22.440 --> 19:25.600
Could you give any hints to how you've moved the needle

19:25.600 --> 19:26.440
on this?

19:26.440 --> 19:27.880
So the knowledge thing, it seems to me

19:27.880 --> 19:29.840
that you would just get more data

19:29.840 --> 19:31.440
and curate and refine the data.

19:31.440 --> 19:32.880
But could you give any hints

19:32.880 --> 19:35.080
on how you've made it better at reasoning, for example?

19:35.080 --> 19:35.920
Yeah, with knowledge,

19:35.920 --> 19:38.720
I think it's about augmentation with RAG

19:38.720 --> 19:42.040
and better modeling techniques, cleaner data sets

19:42.040 --> 19:44.720
so that you remember the right stuff

19:44.720 --> 19:48.200
and don't retain the less relevant stuff.

19:49.560 --> 19:52.400
Those are the techniques that move the needle there.

19:52.400 --> 19:55.360
With reasoning, there are like circuits

19:55.360 --> 19:57.640
that you really need to bake in to the model.

19:57.640 --> 19:59.560
You need to show it and demonstrate it,

19:59.600 --> 20:02.160
how to break down tasks at a very low level,

20:03.120 --> 20:04.680
think through them.

20:04.680 --> 20:06.760
And that's stuff that's not actually

20:06.760 --> 20:08.880
that abundant on the internet.

20:08.880 --> 20:12.160
So it doesn't come for free using our previous techniques

20:12.160 --> 20:15.040
of just scrape the web and train the model and scale up.

20:15.040 --> 20:18.520
People don't usually write out their inner monologue, right?

20:18.520 --> 20:20.720
They usually write the results of that inner monologue.

20:20.720 --> 20:23.520
And so it's something that the model

20:23.520 --> 20:26.560
has been missing a view into.

20:26.560 --> 20:28.800
I think synthetic data will go a long way

20:28.800 --> 20:31.760
in closing that gap and supporting building

20:32.760 --> 20:36.400
multi-trillion token data sets

20:36.400 --> 20:40.280
that actually demonstrate how to have an inner monologue,

20:40.280 --> 20:41.480
how to reason through things,

20:41.480 --> 20:44.440
how to think through problems, make mistakes,

20:44.440 --> 20:47.680
identify mistakes, correct them and retry.

20:47.680 --> 20:50.440
That sort of long thought process data

20:50.440 --> 20:53.680
is actually extremely scarce.

20:53.680 --> 20:54.520
It's very rare.

20:54.520 --> 20:55.360
It's very rare.

20:55.360 --> 20:56.200
It's really hard to find.

20:56.200 --> 20:57.040
You can find it on the internet, of course.

20:57.240 --> 21:02.240
Stuff like forums where people help each other

21:02.280 --> 21:03.880
with homework and sort of break down.

21:03.880 --> 21:06.560
This is how I arrived at this answer.

21:06.560 --> 21:09.120
But when you look at the internet in totality,

21:09.120 --> 21:12.560
those are like pinpricks on the surface of this thing.

21:12.560 --> 21:15.040
And so pulling that forward, emphasizing it,

21:15.040 --> 21:18.240
augmenting it, producing more of that data

21:18.240 --> 21:22.240
should be a key priority if you're gonna actually

21:22.240 --> 21:24.920
teach these models to exhibit that behavior.

21:24.920 --> 21:26.560
Is there a trade-off between,

21:26.600 --> 21:29.800
I mean, for example, we could use the Unreal Engine

21:29.800 --> 21:32.320
to generate lots of visual training data

21:32.320 --> 21:34.680
for a Vision Foundation model.

21:34.680 --> 21:36.680
Or an alternative would be we could have

21:36.680 --> 21:39.400
like some kind of hybrid prediction architecture

21:39.400 --> 21:42.520
where we somehow encode naive physics

21:42.520 --> 21:43.960
into the architecture itself,

21:43.960 --> 21:47.320
which means rather than memorizing lots of generated data,

21:47.320 --> 21:50.920
we just kind of build a hybrid architecture.

21:50.920 --> 21:53.760
Is that a trade-off that you're kind of thinking about?

21:53.760 --> 21:56.280
Like specifically with the video side of things

21:56.320 --> 21:58.560
where physics is relevant, I think that's

22:00.040 --> 22:01.600
a totally fine strategy.

22:01.600 --> 22:05.080
I think that, yeah, a lot of the physics engines

22:05.080 --> 22:09.440
that people have built are, they're flawed, right?

22:09.440 --> 22:12.600
Like video games still don't look like reality.

22:12.600 --> 22:14.640
They still don't behave like reality.

22:14.640 --> 22:17.040
And so training off of that data,

22:17.040 --> 22:21.840
I think will leave you in a really unsatisfying place.

22:21.880 --> 22:26.480
Like there's just still some Uncanny Valley weirdness to it.

22:28.080 --> 22:30.680
I think like we have tons of actual video data

22:30.680 --> 22:34.120
of the real world where physics is definitely implemented

22:34.120 --> 22:37.000
and being represented completely accurately.

22:37.000 --> 22:40.480
And so that should be our go-to source.

22:40.480 --> 22:45.480
I think trying to use simulators at this stage

22:45.920 --> 22:46.760
is the wrong approach.

22:46.760 --> 22:48.680
I think you should take as much data

22:48.680 --> 22:50.000
from the real world as you can

22:50.000 --> 22:51.600
and use that as a bootstrap

22:51.600 --> 22:53.720
to then build synthetic data engines

22:53.720 --> 22:56.320
that help you iteratively improve.

22:56.320 --> 22:58.120
It's what happened in language as well, right?

22:58.120 --> 23:01.640
Like we didn't go to synthetic language,

23:01.640 --> 23:04.160
rules-based synthetic language generators

23:04.160 --> 23:05.480
to teach our language models,

23:05.480 --> 23:07.520
the basic principles of language

23:07.520 --> 23:11.360
using our linguistic models that we've built.

23:11.360 --> 23:13.200
No, we threw all that away.

23:13.200 --> 23:17.360
We took actual language data from humans, trained on that,

23:17.360 --> 23:20.280
and then used the models that were the output of that

23:20.320 --> 23:25.320
to improve iteratively and via experimentation.

23:27.040 --> 23:29.600
I think the same will be true in vision.

23:29.600 --> 23:32.160
That's a really interesting point, actually.

23:32.160 --> 23:34.400
Because with the SORA model from OpenAI,

23:34.400 --> 23:35.720
it does look a bit weird.

23:35.720 --> 23:37.160
It looks like it's always flying

23:37.160 --> 23:39.760
and it looks very game engine-like.

23:39.760 --> 23:42.320
And the language example is beautiful.

23:42.320 --> 23:44.960
But what about something like mathematics?

23:44.960 --> 23:48.560
Are there examples where rather than kind of, you know,

23:48.560 --> 23:51.360
perturbing or mutating what already exists,

23:51.360 --> 23:54.280
you might just start from first principles and rules?

23:54.280 --> 23:55.280
Totally, yeah.

23:55.280 --> 24:00.280
Like mathematics is so explicitly rule-driven

24:01.040 --> 24:04.000
and so explicitly verifiable.

24:04.000 --> 24:08.600
It's like the perfect example of synthetic data generation.

24:08.600 --> 24:13.600
I think it's definitely one of the domains

24:13.760 --> 24:15.240
that will crack first.

24:15.440 --> 24:19.240
And on top of that, code, right?

24:19.240 --> 24:22.280
You can completely, synthetically generate code, verify it.

24:22.280 --> 24:23.120
Does it run?

24:23.120 --> 24:26.560
Does it produce the outputs that you want on a test set?

24:27.960 --> 24:31.240
So when it is that explicit and verifiable,

24:31.240 --> 24:33.680
it's perfect for synthetic data generation,

24:33.680 --> 24:34.880
like just ideal.

24:34.880 --> 24:37.840
Yeah, but I guess this is kind of what I'm thinking about.

24:37.840 --> 24:40.680
That with code, you can actually constrain it way more

24:40.680 --> 24:41.880
than language.

24:41.880 --> 24:45.520
So you could, rather than using an existing self-attention

24:45.520 --> 24:47.680
transformer, you know, you might want to have something

24:47.680 --> 24:50.080
that only works on trees or whatever.

24:50.080 --> 24:52.960
And maybe that would work better for that particular thing.

24:52.960 --> 24:55.240
But then I guess we'd have to have some kind of mixture

24:55.240 --> 24:58.640
of experts and not have a single model.

24:58.640 --> 25:02.640
Yeah, I think that's behind the scenes actually,

25:02.640 --> 25:04.040
a lot of the strategy.

25:04.040 --> 25:07.440
You'll likely have an MOE where one component,

25:07.440 --> 25:10.000
one of those experts is gonna be an expert in code,

25:10.000 --> 25:11.800
very highly specialized to that.

25:11.840 --> 25:14.600
Heavily upsampled on synthetic code data

25:14.600 --> 25:17.480
and real code data, math, et cetera.

25:18.760 --> 25:22.120
And that expert will act as a general reasoning engine

25:22.120 --> 25:23.680
and will be very good at logic

25:23.680 --> 25:25.760
and those sorts of components.

25:25.760 --> 25:28.320
You might have a medical expert,

25:28.320 --> 25:31.960
which has dramatic upsampling along that axis.

25:34.200 --> 25:39.200
Yeah, I think that's a very effective path towards

25:39.440 --> 25:41.580
even more efficient models.

25:41.580 --> 25:43.740
So if you're in the medical domain

25:43.740 --> 25:45.940
or the math or code domain,

25:45.940 --> 25:47.980
you can then pull out that expert

25:47.980 --> 25:49.420
and use it independently.

25:49.420 --> 25:54.420
You don't need to keep around this huge monolithic model.

25:54.660 --> 25:57.460
You can just take out a sub-component and deploy that.

25:59.380 --> 26:02.180
Yeah, I think that architecture already exists.

26:02.180 --> 26:03.820
Yeah, I wonder if you can talk to that a little bit

26:03.820 --> 26:05.180
because I'm very excited about that

26:05.180 --> 26:08.060
because it now seems that maybe we could call

26:08.060 --> 26:12.420
what you just described an agentic distributed AI system

26:12.420 --> 26:15.820
where the agents can pass messages to each other

26:15.820 --> 26:18.500
and one of them might be an expert in mathematics,

26:18.500 --> 26:20.860
one of them might be an expert in coding or so on.

26:20.860 --> 26:21.860
But then you've got this problem

26:21.860 --> 26:24.300
that you kind of send a message into the nexus

26:24.300 --> 26:26.860
and all of the models are kind of passing messages

26:26.860 --> 26:30.460
to each other and it's kind of unbounded in runtime

26:30.460 --> 26:31.860
as opposed to one of the great things

26:31.860 --> 26:33.340
with the language model is

26:33.340 --> 26:35.940
it just does a fixed amount of compute per iteration.

26:36.020 --> 26:38.020
You just put some prompt in

26:38.020 --> 26:40.180
and you get the answer straight back out.

26:40.180 --> 26:44.020
So does that kind of unboundedness introduce problems?

26:45.060 --> 26:47.940
I mean, a language model could just generate infinitely

26:47.940 --> 26:49.580
and not produce a stop token

26:49.580 --> 26:51.580
and you would go on forever.

26:51.580 --> 26:53.660
So I think the problem already exists

26:53.660 --> 26:56.020
and models are quite well-behaved in terms of,

26:58.700 --> 27:02.020
if you train them to give up

27:02.020 --> 27:05.220
and to say, I need to respond, they will.

27:05.940 --> 27:06.940
They tend to.

27:07.820 --> 27:09.900
So I'm not too concerned about like,

27:09.900 --> 27:13.180
runaway processes that would just not be useful

27:13.180 --> 27:16.180
as well, hugely computationally expensive.

27:16.180 --> 27:19.060
And yeah, it seems like models can produce stop tokens.

27:19.060 --> 27:22.700
And I think that even in a multi-agent scenario,

27:24.340 --> 27:27.380
discourse between agents will conclude itself

27:27.380 --> 27:28.540
in a reasonable amount of time.

27:28.540 --> 27:29.380
Yeah, interesting.

27:29.380 --> 27:32.340
And I think even now with your multi-step tool use,

27:32.340 --> 27:33.340
that's basically what you've done.

27:33.340 --> 27:35.420
You could in principle do that recursively

27:35.420 --> 27:37.700
and you could constrain the computation graph

27:37.700 --> 27:39.060
so that there's no cycles

27:39.060 --> 27:41.340
and it comes back in a fixed amount of time.

27:41.340 --> 27:44.060
Yeah, we terminate execution

27:44.060 --> 27:46.620
after some number of failed attempts.

27:46.620 --> 27:48.740
So it's easy to solve that way.

27:48.740 --> 27:51.060
It's a little bit unsatisfying.

27:51.060 --> 27:53.620
I think our multi-optimal use right now,

27:53.620 --> 27:55.060
it's our very first pass.

27:55.060 --> 27:57.180
It's like the negative one.

27:57.180 --> 28:00.860
And so it's not that good at catching

28:00.860 --> 28:02.060
when it's made mistakes.

28:02.060 --> 28:04.200
It's not that good at correcting its mistakes,

28:04.200 --> 28:06.320
even if it's caught that it fucked up.

28:07.360 --> 28:10.720
And so I think we're still very early there,

28:10.720 --> 28:12.520
but those systems are gonna start to become

28:12.520 --> 28:16.040
extremely robust and reliable.

28:16.040 --> 28:18.160
And I'm very excited for that.

28:18.160 --> 28:19.000
Amazing.

28:19.000 --> 28:24.000
So I'm interested to know from, in your own words,

28:24.000 --> 28:27.320
how, I mean, we're just talking to you folks

28:27.320 --> 28:29.680
who've got this forward engineering team,

28:29.680 --> 28:31.360
your enterprise focused,

28:31.360 --> 28:33.560
you're helping bridge the last mile problem

28:33.560 --> 28:35.920
and really embedding yourselves into large enterprise,

28:35.920 --> 28:38.280
which is an amazing differentiator.

28:38.280 --> 28:40.680
But other than that, there's always the question,

28:40.680 --> 28:44.200
lots of people say these models are just kind of interchangeable

28:44.200 --> 28:48.000
and you're just kind of playing the token game at some point.

28:48.000 --> 28:50.760
And I just wondered like, what's your plan there?

28:50.760 --> 28:52.760
I agree with that sentiment.

28:52.760 --> 28:54.880
Models are way too similar.

28:54.880 --> 28:57.720
I think there's going to start to be differentiation

28:57.720 --> 28:58.560
between models.

28:58.560 --> 29:02.720
Like I was talking about before with command R and R plus,

29:03.920 --> 29:08.480
we're going to start really focusing in on key capabilities.

29:08.480 --> 29:12.040
The general language model game is,

29:13.000 --> 29:16.680
there's a lot of players and it's pretty saturated.

29:18.880 --> 29:21.480
I think people are gonna start to have to branch out.

29:21.480 --> 29:24.240
I think that consumer language models

29:24.240 --> 29:27.000
are going to separate away from enterprise language models.

29:27.000 --> 29:29.320
Within enterprise, there's gonna be a lot of specialization

29:29.320 --> 29:31.100
into specific domains.

29:31.660 --> 29:35.500
And so for Cohere, what I want to see us do

29:35.500 --> 29:40.500
is in product space, push into more tailored capabilities

29:41.340 --> 29:42.740
for particular problems.

29:44.740 --> 29:46.980
We want to drive value for enterprise

29:46.980 --> 29:49.340
and different enterprises operated in different spaces

29:49.340 --> 29:50.340
and they have different needs.

29:50.340 --> 29:52.260
The tools that their models might need to use

29:52.260 --> 29:54.460
look very different from one another.

29:54.460 --> 29:56.180
And we want to make sure that we're serving

29:56.180 --> 30:00.140
each of those niches particularly well or uniquely well.

30:00.140 --> 30:01.820
And that will be our value proposition

30:01.820 --> 30:03.380
differentiated from others.

30:05.100 --> 30:09.180
So that notion of specialization

30:09.180 --> 30:12.420
or enhanced capability in particular domains

30:12.420 --> 30:14.660
is something that we definitely want to explore

30:14.660 --> 30:17.460
at the product level and start to offer.

30:17.460 --> 30:20.580
Because like you say, the dollar per token space,

30:20.580 --> 30:23.100
it's super, we're not gonna stop that.

30:23.100 --> 30:24.820
It's important for the community, right?

30:24.820 --> 30:26.620
Like folks need to build on top of this.

30:26.620 --> 30:29.740
They need access to good models at fair prices.

30:29.980 --> 30:33.340
So we're gonna continue to give that to the world.

30:34.780 --> 30:36.940
But we want to create differentiated value.

30:36.940 --> 30:39.220
And I think that's gonna come from focusing on

30:39.220 --> 30:42.220
the actual problems that enterprises want to tackle

30:42.220 --> 30:45.140
and getting extremely, extremely good at them.

30:45.140 --> 30:45.980
Interesting.

30:45.980 --> 30:50.900
On that, are you planning kind of horizontal products

30:50.900 --> 30:52.340
or vertical products?

30:52.340 --> 30:56.820
And the reason I say horizontal is I know a few startups now

30:56.860 --> 31:00.060
that are building kind of low code,

31:00.060 --> 31:02.420
app dev platforms with large language models

31:02.420 --> 31:04.580
and they're making it incredibly easy in the enterprise

31:04.580 --> 31:06.740
to compose together different models

31:06.740 --> 31:08.900
and to deploy applications on phones.

31:08.900 --> 31:12.660
And it's really democratized because it's so much easier

31:12.660 --> 31:14.580
now for people to do artificial intelligence.

31:14.580 --> 31:17.820
That would be a good example of like a horizontal one, I guess.

31:17.820 --> 31:20.820
So I think our product right now is super horizontal, right?

31:20.820 --> 31:22.820
It's like general language models, embedding models,

31:22.820 --> 31:23.660
re-rank models.

31:23.660 --> 31:26.020
It's a platform that's deployable privately

31:26.020 --> 31:27.980
on every single cloud.

31:27.980 --> 31:31.460
You can deploy the model against any sort of data,

31:31.460 --> 31:35.540
whether it's medical, finance, legal, it doesn't matter.

31:35.540 --> 31:40.540
It's the most horizontal product and platform you can build.

31:41.660 --> 31:44.020
What we're gonna start to do is more

31:44.020 --> 31:47.820
towards verticalization and so specializing models

31:47.820 --> 31:51.060
at particular problems or objectives

31:51.060 --> 31:55.100
that exist in the world and offering a product

31:55.100 --> 31:57.060
that solves that for the enterprise.

31:57.060 --> 31:59.220
Would you ever go beyond the model

31:59.220 --> 32:01.580
and kind of plug a little bit deeper

32:01.580 --> 32:03.180
into the platform in the enterprise?

32:03.180 --> 32:06.580
So for example, building operating models

32:06.580 --> 32:10.780
or one approach would be to just fine-tune the model

32:10.780 --> 32:12.620
on lots of data from a particular domain,

32:12.620 --> 32:14.260
but it's still a language model.

32:14.260 --> 32:16.180
The interface with Coheir is the same

32:16.180 --> 32:19.900
or another one would be to let's say build

32:19.900 --> 32:22.460
something a little bit like Databricks or Snowflake

32:22.500 --> 32:25.820
or something like an enterprise-wide suite

32:25.820 --> 32:29.900
that allows you to deploy, discover, create, share

32:29.900 --> 32:32.060
artificial intelligence in an enterprise.

32:32.060 --> 32:35.780
The only reason I say that is as you're an AWS,

32:35.780 --> 32:37.780
they give you free credits.

32:37.780 --> 32:39.900
They want you to get on their platform

32:39.900 --> 32:41.780
because they know you're never leaving

32:41.780 --> 32:44.180
because you've got something there

32:44.180 --> 32:46.100
which is not easily replaceable.

32:46.100 --> 32:48.540
People learn how to use it, they love it.

32:48.540 --> 32:49.980
Would that be a potential future?

32:49.980 --> 32:54.460
Yeah, it's definitely still going to be a platform,

32:54.460 --> 32:57.060
customizable, something that the user,

32:57.060 --> 32:59.220
which for us as an enterprise can adopt

32:59.220 --> 33:01.460
and sort of bring into their environment,

33:01.460 --> 33:04.660
hook in their data, their tools,

33:04.660 --> 33:07.820
their whatever they want to plug in.

33:07.820 --> 33:10.900
The verticalization is gonna come from

33:10.900 --> 33:12.780
investing in the model to be good

33:12.780 --> 33:14.300
within a particular domain.

33:14.300 --> 33:16.900
That might mean fine-tuning on data within that domain.

33:16.900 --> 33:20.500
That might mean making sure the model is very good

33:20.500 --> 33:23.700
at using the tools that employees operating

33:23.700 --> 33:25.140
in that domain would use.

33:25.140 --> 33:25.980
But that's our focus.

33:25.980 --> 33:27.660
It's starting to get more specific

33:27.660 --> 33:30.420
and focused on the actual use cases

33:30.420 --> 33:31.780
that enterprises care about

33:31.780 --> 33:34.420
and not just doing, you know,

33:34.420 --> 33:39.420
version 345 of the same general model.

33:42.060 --> 33:45.220
So I saw you tweeted about Nick Bostrom's

33:45.260 --> 33:48.340
Future of Humanity Institute shutting down.

33:48.340 --> 33:50.060
Do you have any thoughts on that?

33:51.500 --> 33:54.340
Yeah, I think it sucks to see

33:54.340 --> 33:59.340
any sort of academic institute collapse.

34:00.780 --> 34:03.340
To be honest, I know nothing more than what's public there.

34:03.340 --> 34:06.220
So I don't know if there were some internal issues

34:06.220 --> 34:10.180
that caused the philosophy department to pull funding.

34:11.180 --> 34:16.180
But I've been a pretty vocal critic of ex-risk

34:17.820 --> 34:22.180
and the idea that language models are going to

34:22.180 --> 34:24.260
take over the world and kill everyone.

34:25.700 --> 34:28.820
But despite that, I still want people thinking about that.

34:28.820 --> 34:30.460
I still want academics thinking about that.

34:30.460 --> 34:34.100
I don't think that regulators and policy folks

34:34.100 --> 34:35.500
should be thinking about it yet

34:35.500 --> 34:37.420
because it's so far away and remote

34:38.300 --> 34:40.180
and potentially completely irrelevant.

34:40.180 --> 34:42.540
But that's the domain of academia,

34:42.540 --> 34:46.060
is to pursue those long horizon, high risk projects

34:46.060 --> 34:47.060
and make progress on them.

34:47.060 --> 34:50.220
And so I certainly don't want to see

34:50.220 --> 34:55.220
the academic front of that effort get defunded.

34:57.180 --> 34:59.740
That being said, I think that those organizations

34:59.740 --> 35:03.820
have really been trying to get their hands into policy

35:03.820 --> 35:07.100
and impact private sector, public sector

35:07.100 --> 35:10.740
in a way that is threatening to progress,

35:11.900 --> 35:16.900
misleading and so I think that we're starting

35:19.260 --> 35:20.860
to have within our community,

35:20.860 --> 35:23.300
like the AI machine learning community,

35:23.300 --> 35:24.540
a bit of a correction.

35:25.460 --> 35:27.580
Those people were kind of given a lot of power,

35:27.580 --> 35:28.860
were listened to a lot

35:30.780 --> 35:34.140
and developed what I think we all

35:34.140 --> 35:36.180
recognize as too much influence.

35:37.180 --> 35:40.740
And it started to produce bills

35:40.740 --> 35:45.740
and talks about policy that would totally collapse progress

35:46.460 --> 35:49.180
in the space, very, very prematurely

35:49.180 --> 35:54.180
about theoretical long-term risks that might be an issue.

35:54.540 --> 35:55.380
And so fortunately,

35:55.380 --> 35:57.380
I think there's a cultural correction happening

35:57.380 --> 35:59.500
where even the legislators and policymakers

35:59.500 --> 36:02.620
are starting to say, you know, this is not,

36:03.620 --> 36:07.300
it's not appropriate the level of influence

36:07.300 --> 36:09.580
that this one group is having

36:09.580 --> 36:11.620
and we should listen to a much more broad

36:11.620 --> 36:14.860
and diverse set of opinions.

36:14.860 --> 36:16.260
So I'm still concerned about that

36:16.260 --> 36:19.100
and I'll continue to speak out against that when I see it,

36:19.100 --> 36:21.420
but at the academic level,

36:21.420 --> 36:26.420
I don't want to see, you know, professors lose their funding.

36:26.540 --> 36:30.060
I think that they should continue to pursue those ideas.

36:30.060 --> 36:31.580
Yeah, I think Bostrom blogged that.

36:31.580 --> 36:34.180
He was trying to resist the entropic forces

36:34.180 --> 36:36.180
of the philosophy department for several years

36:36.180 --> 36:38.180
and eventually he lost.

36:38.180 --> 36:39.980
But I'm in two minds as well.

36:39.980 --> 36:42.020
So as you know, I've hosted many debates

36:42.020 --> 36:43.260
with, you know, like Connolly,

36:43.260 --> 36:45.020
he for example, and Beth Jezos

36:45.020 --> 36:46.700
and a bunch of different people.

36:46.700 --> 36:49.500
And one thing that strikes me is how ideological it is.

36:49.500 --> 36:51.180
I really thought as a podcaster,

36:51.180 --> 36:54.140
I could have an honest and open conversation

36:54.140 --> 36:56.260
and it's never gone well.

36:56.260 --> 36:57.780
And I've put a lot of thought

36:57.780 --> 37:00.060
into trying to understand why that is.

37:00.060 --> 37:02.500
And I think philosophically you can trace it back

37:02.500 --> 37:05.860
to things like paternalism and safetyism

37:05.860 --> 37:08.620
and utilitarianism and consequentialism

37:08.620 --> 37:10.620
and long-termism and, you know,

37:10.620 --> 37:15.060
these are ideologies that make one believe

37:15.060 --> 37:17.820
that even though it's just a subjective probability

37:17.820 --> 37:20.020
that I know better than you,

37:20.020 --> 37:22.180
I can predict the future better than you.

37:22.180 --> 37:24.980
And they've become much more pragmatic in recent years.

37:24.980 --> 37:26.700
So rather than talking about

37:26.700 --> 37:28.860
the old school Bostrom superintelligence,

37:28.860 --> 37:30.780
we're now talking about, you know,

37:30.780 --> 37:33.380
memetic risks and bio risks and things

37:33.380 --> 37:36.460
that I think are designed to get more people on board with it.

37:36.460 --> 37:39.620
And I agree with you that they've had a lot of influence.

37:39.620 --> 37:41.580
But why is it so difficult

37:41.580 --> 37:43.340
to have a rational conversation?

37:43.340 --> 37:44.780
Yeah, no, I think it's what you say.

37:44.780 --> 37:46.980
It's very ideological.

37:46.980 --> 37:49.140
There are camps and positions

37:49.140 --> 37:52.500
and it's, for some reason,

37:52.500 --> 37:55.420
it's become very cult-like on both sides.

37:56.420 --> 37:58.820
Obviously, there's the EA movement,

37:58.820 --> 38:02.220
which formed a cult-like environment

38:02.220 --> 38:05.100
of adherence to those principles

38:05.100 --> 38:09.820
and their recommended behaviors and actions,

38:09.820 --> 38:13.100
you know, what you should work on in your life.

38:13.100 --> 38:14.860
They have dating apps for you.

38:14.860 --> 38:16.540
Like, it's very insular.

38:16.540 --> 38:21.140
And then there was an ironic, I think,

38:21.140 --> 38:23.820
although it's increasingly not clear,

38:23.940 --> 38:26.540
an ironic counter-movement, which was EAAC.

38:26.540 --> 38:28.260
Yeah.

38:28.260 --> 38:29.700
And that has spun out into something

38:29.700 --> 38:31.300
that is very not ironic.

38:31.300 --> 38:35.620
It's very libertarian, accelerationist,

38:35.620 --> 38:38.380
which are ideals that I don't hold either.

38:38.380 --> 38:42.980
And so both of these camps I find really unappealing.

38:42.980 --> 38:44.700
I don't want to be associated with either of them.

38:44.700 --> 38:49.420
Yeah, I found, you know, EA was very dominant

38:49.420 --> 38:50.500
for a long time.

38:50.540 --> 38:54.300
And so when EAAC came out, it was, like, refreshing.

38:54.300 --> 38:57.540
Finally, someone's, like, calling them on their bullshit.

38:57.540 --> 38:59.420
But at this point, it's just mind-numbing

38:59.420 --> 39:03.700
and, like, completely not of interest to me.

39:03.700 --> 39:04.980
Yeah, we've had Beth on the show.

39:04.980 --> 39:06.420
He's a really nice guy, actually.

39:06.420 --> 39:08.660
I invested in Guillaume's company.

39:08.660 --> 39:09.700
Oh, did he? Yeah, yeah, yeah.

39:09.700 --> 39:10.780
Oh, it's not that...

39:10.780 --> 39:14.420
He's brilliant. Like, he's a really, really nice person.

39:15.620 --> 39:18.700
I'm proud to see Canadians doing great things.

39:19.700 --> 39:21.580
The best thing I found super funny.

39:21.580 --> 39:24.260
And I think EAAC was necessary.

39:24.260 --> 39:28.180
I now believe that both EAAC and EAAC need to be dissolved.

39:28.180 --> 39:33.380
We've seen them through to their logical conclusion,

39:33.380 --> 39:35.100
and now we're starting to get into territory

39:35.100 --> 39:38.060
that's very strange.

39:38.060 --> 39:39.420
From a philosophical perspective,

39:39.420 --> 39:43.020
how do you kind of see the role of AI in society?

39:43.020 --> 39:47.060
And I'm quite interested in how it's affecting our reality,

39:47.060 --> 39:49.980
and how we interface with technology

39:49.980 --> 39:51.740
is really dramatically changing over time.

39:51.740 --> 39:52.900
I mean, what do you think about that?

39:52.900 --> 39:54.940
Yeah, completely true.

39:54.940 --> 40:00.020
I think I view it in the same way I view the computer

40:00.020 --> 40:01.660
or the CPU.

40:01.660 --> 40:02.620
It's a tool.

40:02.620 --> 40:04.020
It's something that we're going to leverage,

40:04.020 --> 40:06.580
that we're going to build on top of

40:06.580 --> 40:08.660
and use to make our lives better,

40:08.660 --> 40:11.060
to make us more productive,

40:11.060 --> 40:13.700
to make things cheaper, more accessible.

40:14.700 --> 40:18.180
I think all the good that came from the computer

40:18.180 --> 40:21.700
and the internet is going to be dwarfed by this,

40:21.700 --> 40:25.700
the democratization of intelligence

40:25.700 --> 40:29.180
and having that always at your disposal at any time.

40:29.180 --> 40:32.700
That's something that, you know, 50 years ago,

40:32.700 --> 40:35.980
you couldn't even dream of it, right?

40:35.980 --> 40:38.900
It's surreal, the amount of progress

40:38.900 --> 40:40.580
that's been made in half a century.

40:40.580 --> 40:41.940
And so I'm really excited for that.

40:42.380 --> 40:47.380
I think it will do a lot of good and alleviate a lot of ills.

40:49.140 --> 40:52.420
I think the human experience, our lives,

40:52.420 --> 40:55.900
will be dramatically improved by having access

40:55.900 --> 40:59.300
to much more intelligence in our lives.

40:59.300 --> 41:00.380
I'm really excited as well,

41:00.380 --> 41:04.420
but are there particular things that you are concerned about?

41:04.420 --> 41:07.940
I mean, for example, people say that language models

41:07.940 --> 41:09.580
might enfee us,

41:09.580 --> 41:13.020
they might lead to mass manipulation and persuasion.

41:13.020 --> 41:15.220
I mean, Jan Lacoon tweeted the other day,

41:15.220 --> 41:18.140
he said, where's the mass manipulation?

41:18.140 --> 41:19.180
Where's the persuasion?

41:19.180 --> 41:21.580
There might be something that just happens gradually

41:21.580 --> 41:23.140
over time, but are there things

41:23.140 --> 41:25.260
that you do worry about?

41:25.260 --> 41:26.580
Of course, yeah, of course.

41:26.580 --> 41:28.180
It's a general technology,

41:28.180 --> 41:31.340
and so it can be used in a lot of different ways,

41:31.340 --> 41:33.500
many of which are, I think,

41:33.500 --> 41:35.620
abhorrent and ones that we should avoid

41:35.620 --> 41:37.340
and make very difficult to do.

41:38.940 --> 41:42.700
I'm much more of an optimist than I am a pessimist,

41:42.700 --> 41:46.660
but on the side of things that are risky,

41:46.660 --> 41:49.980
I think that misinformation is high up on the list.

41:49.980 --> 41:52.180
I think that we're already seeing

41:53.540 --> 41:56.700
social media platforms start to build in the mitigations.

41:56.700 --> 41:58.500
I think things like human verification

41:58.500 --> 41:59.780
are gonna become crucial.

42:00.740 --> 42:04.020
If I'm reading a poster talking about

42:04.940 --> 42:08.020
whatever Canadian elections or politicians,

42:08.020 --> 42:11.620
I wanna know that that's a voting Canadian citizen.

42:11.620 --> 42:14.100
I wanna know, because I want to know

42:14.100 --> 42:17.060
what my compatriots think, right?

42:17.060 --> 42:19.820
Even if they're on the opposite side of the fence to me,

42:19.820 --> 42:20.860
like that's fine.

42:20.860 --> 42:24.060
I wanna hear what they think,

42:25.140 --> 42:28.300
but I don't wanna hear what some foreign adversary

42:28.300 --> 42:32.500
has spun up a bot to push into the discourse.

42:32.500 --> 42:36.460
And so human verification, I think, is crucial.

42:36.460 --> 42:39.060
That's the one that's top of mind for me.

42:39.060 --> 42:41.140
I think some of the more remote risks,

42:41.140 --> 42:46.060
like bio weapons and this sort of stuff,

42:46.060 --> 42:47.500
I'm less concerned about.

42:47.500 --> 42:51.100
In feeblement and becoming dependent on the technology,

42:51.100 --> 42:53.340
I think folks said that about calculators

42:53.340 --> 42:55.940
and we wouldn't learn how to do basic math.

42:55.940 --> 42:59.660
Humans are intrinsically curious.

42:59.660 --> 43:00.740
We want to know things,

43:00.740 --> 43:04.100
and we can't ask the right questions of machines

43:04.100 --> 43:05.620
without knowing things.

43:05.620 --> 43:08.980
And so we'll continue to be really well educated,

43:08.980 --> 43:12.140
better educated, more knowledgeable than we were before

43:12.140 --> 43:13.260
without that technology.

43:13.260 --> 43:15.820
Yeah, because if you look at the enfeeblement pie chart,

43:15.820 --> 43:17.500
a calculator is a very small part

43:17.500 --> 43:19.940
and a general AI is quite a large part,

43:19.940 --> 43:21.940
which is a little bit concerning, I guess,

43:21.940 --> 43:26.940
but I agree with you that maybe the jobs one has spoken about.

43:26.940 --> 43:28.500
I've not seen a lot of evidence of that yet,

43:28.500 --> 43:31.460
but it's so pernicious, it might happen slowly over time.

43:31.460 --> 43:33.220
Daniel Dennett wrote an interesting article,

43:33.220 --> 43:35.180
and rest in peace, by the way,

43:35.180 --> 43:37.740
Daniel Dennett called Counterfeit People,

43:37.740 --> 43:39.020
which he published in The Atlantic,

43:39.020 --> 43:41.460
and he was kind of saying that when we have all of these bots

43:41.460 --> 43:44.780
and generative video models and so on,

43:44.780 --> 43:47.860
at some point they'll become indistinguishable from reality,

43:47.860 --> 43:50.060
and that will lead to a kind of acquiescence

43:50.060 --> 43:52.260
where we don't trust anything we see.

43:52.260 --> 43:55.020
And I think that's quite interesting,

43:55.020 --> 43:57.300
and I also think that these models

43:57.300 --> 44:00.940
might kind of affect our agency in quite a weird way,

44:00.940 --> 44:02.980
but it's so difficult to understand now

44:02.980 --> 44:05.780
how that's going to affect society.

44:05.780 --> 44:08.700
Yeah, I think even now,

44:08.700 --> 44:11.420
people have been taught to be extremely skeptical

44:11.420 --> 44:14.220
of what they read and see.

44:14.220 --> 44:17.100
There's a very strong prior inside of us,

44:17.100 --> 44:18.380
for any media that we consume,

44:18.380 --> 44:21.100
that it's been skewed or manipulated

44:21.100 --> 44:24.420
or produced to propagate an idea,

44:24.420 --> 44:26.780
and I think it's good to have a skeptical populace.

44:26.780 --> 44:28.500
I think it's good to be skeptical

44:28.500 --> 44:32.020
about what you read, regardless of the medium.

44:32.020 --> 44:34.020
And I think people will do what they've always done,

44:34.020 --> 44:36.260
which is filter towards sources

44:36.260 --> 44:38.140
they find trustworthy and objective.

44:40.220 --> 44:43.820
That'll happen even with ML and the loop,

44:43.820 --> 44:45.940
disinformation and misinformation campaigns,

44:45.940 --> 44:47.260
manipulation campaigns,

44:47.260 --> 44:50.500
they existed well before models existed.

44:51.780 --> 44:54.700
And so it's not like a novel concept,

44:54.980 --> 44:56.460
and it's always been a risk.

44:57.660 --> 45:00.860
And the question is how much more prevalent

45:00.860 --> 45:03.900
does the technology make that risk?

45:05.620 --> 45:09.260
I'm optimistic that we're quite robust

45:09.260 --> 45:13.100
and that we'll find ways to make it very hard

45:13.100 --> 45:15.220
for bad actors to exploit the technology.

45:15.220 --> 45:18.740
My rough take is that the more agency the AI has,

45:18.740 --> 45:20.020
the more of a risk it is,

45:20.020 --> 45:22.780
because if it is just doing supervised things,

45:22.820 --> 45:25.340
then every step of the process,

45:25.340 --> 45:29.060
it's being aligned and constrained and steered by humans.

45:29.060 --> 45:31.420
If we ever did create a gentile AI,

45:31.420 --> 45:34.260
then there's this kind of weird divergence

45:34.260 --> 45:36.780
and all sorts of scary things might happen.

45:36.780 --> 45:38.340
But I wanted to talk a little bit

45:38.340 --> 45:39.900
about policy and regulation.

45:39.900 --> 45:41.100
So you spoke to that earlier,

45:41.100 --> 45:43.300
you said that potentially there are some

45:43.300 --> 45:46.700
quite damaging policy changes being considered.

45:46.700 --> 45:47.540
Could you speak to that?

45:47.540 --> 45:50.180
Yeah, I've seen ideas floated.

45:50.180 --> 45:53.380
I don't think any seriously damaging policy

45:53.380 --> 45:55.860
has actually passed, fortunately.

45:55.860 --> 45:57.940
But within what's being considered,

45:57.940 --> 46:02.300
there are ideas that they will destroy innovation,

46:02.300 --> 46:03.620
they will destroy startups.

46:05.020 --> 46:06.860
And so you'll just entrench power

46:06.860 --> 46:08.580
with the existing incumbents.

46:09.500 --> 46:13.340
Some of those examples might be fines,

46:14.340 --> 46:17.220
which if they're a $100 million fine,

46:17.260 --> 46:20.620
that's gonna wipe out and stamp out a startup.

46:20.620 --> 46:24.700
But for a large, you know, big tech company,

46:24.700 --> 46:27.140
it's like 10 minutes of revenue.

46:27.140 --> 46:28.140
It just doesn't matter.

46:28.140 --> 46:29.780
It fundamentally is irrelevant.

46:29.780 --> 46:32.020
And certainly a cost they're willing to take

46:32.020 --> 46:33.180
to capture a market.

46:34.820 --> 46:37.820
And so very disproportionate consequences

46:37.820 --> 46:41.420
for the same punishment.

46:41.420 --> 46:45.180
Over-regulation in that way that it's thoughtless

46:45.180 --> 46:47.020
will have the exact opposite effect

46:47.020 --> 46:49.860
of what I think all of us in the public

46:49.860 --> 46:51.340
and in government want.

46:51.340 --> 46:52.980
We want competitive markets.

46:52.980 --> 46:54.900
We don't want oligopolies.

46:55.740 --> 46:58.580
And we're starting to see oligopolies emerge.

46:58.580 --> 47:02.540
And so there needs to be fairly strong action

47:02.540 --> 47:05.500
pushing against the entrenchment of those oligopolies.

47:05.500 --> 47:09.860
And we need to preserve the ability to self-disrupt.

47:09.860 --> 47:12.100
Because if you can't, if you have an oligopoly

47:12.100 --> 47:13.900
and you have entrenched incumbents,

47:14.620 --> 47:18.500
the likelihood of self-disrupting,

47:18.500 --> 47:21.900
of the new winner emerging within your market,

47:21.900 --> 47:24.480
being one of your players, goes down.

47:24.480 --> 47:27.300
And so you're gonna be disrupted from outside.

47:27.300 --> 47:28.380
That's a huge risk.

47:28.380 --> 47:31.820
And so you need competitive, self-disrupting markets.

47:31.820 --> 47:35.620
And it seems like some of the policy folks

47:35.620 --> 47:37.540
are just acting non-strategically

47:37.540 --> 47:40.140
and not considering that.

47:41.100 --> 47:45.980
But fortunately, what has been passed seems sensible.

47:45.980 --> 47:47.740
Can you comment in particular

47:47.740 --> 47:52.740
on the EU AI legislation and the Canadian?

47:52.820 --> 47:55.180
I probably can't say anything too specific.

47:55.180 --> 48:00.100
I think the Canadian legislation hasn't gone through yet.

48:00.100 --> 48:03.740
The EU AI Act has, but fortunately,

48:03.740 --> 48:06.420
it was reigned quite far back

48:06.420 --> 48:09.260
from its initial position.

48:09.260 --> 48:11.300
I think all of those regulators were in conversation

48:11.300 --> 48:15.020
with all of them when you talk to the folks,

48:15.020 --> 48:16.740
they wanna do the right thing.

48:16.740 --> 48:19.060
They're under a lot of pressure from different parties

48:19.060 --> 48:22.340
with conflicting interests,

48:22.340 --> 48:23.300
but they're trying to do the right thing.

48:23.300 --> 48:24.540
They're trying to make sure this technology

48:24.540 --> 48:26.140
gets out into the world in a safe way,

48:26.140 --> 48:28.140
that there's oversight,

48:28.140 --> 48:31.300
that we don't entrench the incumbents.

48:31.300 --> 48:36.300
And we ideally actually sort of bias towards disruption

48:36.460 --> 48:41.460
and the creation of new value and innovation, new players.

48:45.060 --> 48:48.740
So I think they all want that, but it's a tightrope.

48:48.740 --> 48:52.660
It's a very difficult line to walk.

48:52.660 --> 48:54.860
I think one of the issues is that

48:54.860 --> 48:57.500
not a lot of people certainly in the government

48:57.500 --> 48:59.340
understand how this technology works.

48:59.340 --> 49:00.860
It seems like magic.

49:00.860 --> 49:04.660
And many people, I mean, even in the AI space,

49:04.700 --> 49:06.860
I mean, Lacoon and Hinton, for example,

49:06.860 --> 49:08.980
people have very different opinions about it,

49:08.980 --> 49:10.860
but I'm also interested in your views

49:10.860 --> 49:12.940
on the kind of health of the startup scene.

49:12.940 --> 49:14.860
So we're in a bit of a downturn at the moment.

49:14.860 --> 49:17.580
It doesn't seem to have affected the LLM space,

49:17.580 --> 49:19.700
but even in the LLM space, I've noticed a trend

49:19.700 --> 49:23.060
that many people started kind of wrapper companies

49:23.060 --> 49:26.420
where they did an LLM, but it didn't really do anything

49:26.420 --> 49:28.460
that couldn't easily be replicated.

49:28.460 --> 49:30.180
And what are your thoughts there?

49:30.180 --> 49:33.020
Do you think we're gonna see a trend towards startups

49:33.060 --> 49:35.740
doing something that is very differentiated?

49:35.740 --> 49:38.700
Yeah, I mean, I think we're in a moment of churn.

49:38.700 --> 49:43.700
So I think there are gonna be some players

49:43.700 --> 49:48.700
who started a while ago who fold or go into other companies,

49:48.700 --> 49:51.180
get acquired, that type of thing,

49:51.180 --> 49:53.100
but there's a whole new generation emerging.

49:53.100 --> 49:57.660
I know a bunch of people starting up, Ivan and I,

49:57.660 --> 50:01.100
we invest in startups and we're seeing an uptick

50:01.100 --> 50:03.500
in the number of AI startups that are coming out.

50:04.980 --> 50:06.820
It's sort of like a reformatting.

50:06.820 --> 50:09.500
There was a bunch of folks building at one layer,

50:09.500 --> 50:13.240
like the LLM layer or the one layer above that,

50:14.500 --> 50:15.820
tooling, et cetera.

50:16.740 --> 50:19.140
The players have kind of been set in that space,

50:19.140 --> 50:20.980
it seems, yeah, of course,

50:20.980 --> 50:23.240
I'd be happy to see new players emerge.

50:24.500 --> 50:29.500
But we now need a set of ideas and products and companies

50:30.100 --> 50:31.900
building up the stack.

50:31.900 --> 50:36.900
So more abstract concepts, stuff like end user products

50:37.300 --> 50:40.140
and agent companies, they're all starting to pop up

50:40.140 --> 50:43.940
and create really interesting new ideas.

50:43.940 --> 50:45.500
And then that will sort of settle

50:45.500 --> 50:47.820
and we'll have our players at that layer.

50:47.820 --> 50:50.980
So it's a continuous cycle.

50:52.820 --> 50:55.220
Yeah, I'm really excited about the AI startup space.

50:55.220 --> 50:57.900
It feels like we're finally starting to get our feet

50:57.900 --> 50:58.740
on the ground a little bit.

50:59.020 --> 51:01.460
For example, with the tool use, with the RAG,

51:01.460 --> 51:03.740
it's starting to look a lot more like traditional

51:03.740 --> 51:04.820
software engineering.

51:04.820 --> 51:06.980
So what we're seeing now is people kind of rolling up

51:06.980 --> 51:08.380
their sleeves and actually building out

51:08.380 --> 51:10.660
these very sophisticated software architectures

51:10.660 --> 51:13.180
that compose LLMs in interesting ways.

51:13.180 --> 51:15.460
And they're not just kind of, you know,

51:15.460 --> 51:18.020
just building a simple LLM with a prompt on the top.

51:19.020 --> 51:21.100
Yeah, it's definitely getting more sophisticated.

51:21.100 --> 51:25.140
And as the tools get more robust and reliable,

51:25.140 --> 51:28.980
it's unlocking totally new applications.

51:28.980 --> 51:32.060
And the utility is starting to be seen and felt

51:32.060 --> 51:33.100
in the real world.

51:33.100 --> 51:35.420
I think last year was very much like the year

51:35.420 --> 51:38.260
the world woke up to the technology

51:38.260 --> 51:40.820
and got their footing with it.

51:40.820 --> 51:42.380
So it got familiarity.

51:44.260 --> 51:49.260
This year is when things are gonna start hitting production.

51:49.260 --> 51:51.220
They're gonna actually start to hit our hands

51:51.220 --> 51:54.260
and we're gonna be able to use this as part of our work,

51:54.260 --> 51:58.340
part of our play, the products that we use.

51:59.380 --> 52:02.260
It's gonna become a much more fundamental part

52:02.260 --> 52:03.420
of our daily life.

52:05.820 --> 52:06.780
So it's very gratifying.

52:06.780 --> 52:11.020
Like we've been building Cohere for four and a half years now.

52:11.020 --> 52:12.080
We're in our fifth year.

52:15.580 --> 52:17.660
And I think for a long time,

52:17.660 --> 52:19.660
we were out there sort of preaching,

52:19.660 --> 52:22.100
this is really cool, please care about this.

52:22.420 --> 52:25.460
Like this is gonna be an important thing.

52:25.460 --> 52:26.780
And folks would pat us on the back

52:26.780 --> 52:29.180
and say, nice science project.

52:32.180 --> 52:35.260
But finally, we're actually starting to see

52:35.260 --> 52:37.820
the fruits of all that labor.

52:37.820 --> 52:41.540
And so it's really gratifying to see real world impact.

52:41.540 --> 52:43.180
And I think that's what we exist for

52:43.180 --> 52:45.100
is really trying to accelerate that

52:45.100 --> 52:48.420
and make more of it happen faster

52:48.420 --> 52:52.060
and in the best way possible.

52:52.060 --> 52:53.660
And what was your biggest mistake?

52:53.660 --> 52:56.540
I mean, do you have any advice for other startup founders?

52:56.540 --> 52:59.860
What did you do that perhaps they should avoid?

53:01.100 --> 53:06.100
I fucked up constantly at every stage of the company.

53:09.220 --> 53:12.980
I think, I guess just like admitting that you've messed up

53:14.060 --> 53:18.420
and trying not to be in denial about it

53:18.420 --> 53:20.260
and fixing it as quickly as possible

53:20.300 --> 53:23.620
has been the most important thing

53:23.620 --> 53:27.740
to continue to thrive and exist.

53:29.020 --> 53:32.340
But yeah, this is the first company I started.

53:32.340 --> 53:33.700
Same for Nick and Ivan.

53:33.700 --> 53:37.260
And so the whole founding team, we were fresh into it

53:37.260 --> 53:40.040
and we made potentially every mistake

53:40.040 --> 53:41.400
you could possibly make.

53:43.300 --> 53:48.060
Fortunately, we were good at listening to others

53:48.100 --> 53:49.580
who had done it before

53:49.580 --> 53:52.180
and seen a lot more than we'd seen.

53:52.180 --> 53:54.740
And so I'm sure we've dodged some mistakes,

53:54.740 --> 53:57.940
but it feels like we've made them all.

53:57.940 --> 53:59.500
Yeah, if you don't make mistakes you're not learning,

53:59.500 --> 54:01.060
I guess, but just final question.

54:01.060 --> 54:04.700
How do you, because it's such a large organization now

54:04.700 --> 54:06.980
and there's this problem of vertical information flow.

54:06.980 --> 54:08.140
So there might be a problem

54:08.140 --> 54:09.860
that some of your folks have discovered now

54:09.860 --> 54:12.580
and it takes a while to filter through to you.

54:12.580 --> 54:14.700
But obviously it needs to be scalable

54:14.700 --> 54:15.540
so you need to delegate.

54:15.540 --> 54:16.860
How do you deal with that?

54:16.860 --> 54:21.580
Yeah, I'm very close to like the ICs.

54:21.580 --> 54:22.420
I'm very close.

54:22.420 --> 54:26.580
Like I'm not someone who works through their reports

54:26.580 --> 54:28.420
or follows the chain of command.

54:28.420 --> 54:32.540
I just talk to the people who are actually doing the work.

54:34.020 --> 54:37.540
And so information flows quite freely.

54:39.140 --> 54:42.020
I'm sometimes, I think I'm mostly annoying people

54:42.020 --> 54:43.700
at this point because I'm like pinging them every day.

54:43.700 --> 54:44.580
How's that run going?

54:44.580 --> 54:46.660
You know, have we tried this experiment?

54:46.660 --> 54:48.740
I'm very deeply involved in stuff.

54:49.700 --> 54:51.380
So we haven't had too much.

54:52.740 --> 54:57.740
I don't feel like there's an information flow issue

54:58.100 --> 55:03.100
echo here with scaling, collaboration between teams,

55:03.380 --> 55:05.120
especially when you're a global company

55:05.120 --> 55:08.120
and you're not sitting in the same office as a person.

55:09.860 --> 55:10.980
That's very difficult.

55:10.980 --> 55:13.220
I think remote work is really, really hard.

55:13.220 --> 55:14.380
It's not easy.

55:14.380 --> 55:15.220
It's not easy.

55:15.220 --> 55:20.220
And I think that concentrating teams to geographical areas

55:20.820 --> 55:24.900
or at least time zones is very important

55:24.900 --> 55:28.780
and leads to a lot more productivity and effectiveness.

55:28.780 --> 55:31.780
It's part of the reason I moved here to London

55:31.780 --> 55:34.220
is to be closer to a good chunk

55:34.220 --> 55:35.460
of our machine learning team.

55:35.460 --> 55:38.180
Phil's here, Patrick is here.

55:40.660 --> 55:43.620
Like I want to be present and involved in the ML

55:43.620 --> 55:46.180
component of the company as much as possible.

55:49.340 --> 55:51.140
Yeah, I think as we've scaled,

55:51.140 --> 55:54.140
there's been systems that we've used

55:54.140 --> 55:56.100
that have broken down at each phase,

55:56.100 --> 55:58.940
stuff that worked for the first 10 of us,

55:58.940 --> 56:01.220
didn't work for the next 20,

56:02.220 --> 56:04.180
didn't work for the next 100.

56:04.180 --> 56:09.180
Now we're pushing 350, I think.

56:09.740 --> 56:12.260
And there are people at the company who I don't know,

56:12.260 --> 56:13.380
which is insane.

56:14.620 --> 56:18.780
A super weird experience.

56:21.060 --> 56:23.260
But we've hired really fantastic people

56:23.260 --> 56:24.740
and we continue to do so.

56:24.740 --> 56:28.780
And I think you just trust that people will still

56:28.780 --> 56:30.620
make the right decisions going forward

56:30.620 --> 56:33.460
and that you've set standards high enough

56:35.740 --> 56:38.140
that you don't need to approve every single hire.

56:38.140 --> 56:39.740
You don't need to know what every single person

56:39.740 --> 56:40.820
is working on.

56:41.820 --> 56:45.180
And you just trust your colleagues.

56:45.180 --> 56:47.900
Yeah, I can attest that you hire very well.

56:47.900 --> 56:49.820
It's probably the best culture I've ever seen

56:49.820 --> 56:51.700
in any company, actually.

56:51.700 --> 56:53.020
Thank you so much.

56:53.020 --> 56:55.020
Final question, I mean, just out of interest,

56:55.020 --> 56:57.540
do you get like microcosms in the different offices?

56:57.540 --> 57:01.220
I mean, do you see like different mini cultures?

57:01.220 --> 57:02.940
Oh yeah, totally, 100%.

57:02.940 --> 57:06.140
Like the London office compared to the Toronto office

57:06.140 --> 57:08.980
compared to SF, New York.

57:10.900 --> 57:13.300
The vibes are very, very different.

57:13.300 --> 57:14.620
Like so different.

57:16.180 --> 57:18.180
London is so nice.

57:18.180 --> 57:21.020
It feels really tight-knit.

57:21.020 --> 57:22.820
Like it still feels like a startup,

57:24.420 --> 57:25.340
which we are a startup,

57:25.340 --> 57:27.900
but it still feels like a tiny 30-person startup

57:27.900 --> 57:31.420
where you go out for beers with your colleagues

57:31.420 --> 57:34.700
after work at the pub like regularly.

57:34.700 --> 57:36.620
Everyone knows what everyone else is working on

57:36.620 --> 57:40.700
inside the office, calls on each other for help.

57:40.740 --> 57:41.980
London is super tight-knit.

57:41.980 --> 57:43.620
I think the culture here is like,

57:45.380 --> 57:46.900
I can't pick favorites,

57:46.900 --> 57:48.580
but I really like the culture here.

57:50.060 --> 57:52.380
In Toronto, it's our biggest office,

57:52.380 --> 57:54.940
and so it's much broader,

57:56.780 --> 57:58.020
but the culture there is amazing too.

57:58.020 --> 58:00.700
Super hard-working, stay late,

58:00.700 --> 58:03.220
and like grind very passionate.

58:03.220 --> 58:04.820
There's like different groups

58:04.820 --> 58:06.620
that are close with each other there.

58:07.620 --> 58:10.900
New York is new, but that city is just so much fun.

58:10.900 --> 58:13.500
It's just like such an incredible city,

58:13.500 --> 58:16.140
so much energy, always awake,

58:16.140 --> 58:17.780
work hard, play hard.

58:19.580 --> 58:21.340
SF, I spend the least time in.

58:21.340 --> 58:25.420
I'm not a huge SF fan, to be completely honest.

58:25.420 --> 58:26.860
It's our second HQ,

58:27.740 --> 58:31.820
but I just haven't gotten into the city,

58:31.820 --> 58:33.980
I think, in the way that others have.

58:34.820 --> 58:38.620
I think SF compared to like New York, Toronto, London,

58:39.540 --> 58:41.580
I feel like New York, Toronto, and London

58:41.580 --> 58:44.220
are real cities.

58:45.220 --> 58:48.580
There are artists, there are, you know,

58:48.580 --> 58:52.780
people just doing a very diverse set of things,

58:52.780 --> 58:56.380
and across all the different fields

58:56.380 --> 58:58.060
that are going on there,

58:58.060 --> 59:00.220
you have some of the best people in the world.

59:01.220 --> 59:04.100
SF is much more, it feels more homogenous to me.

59:04.100 --> 59:05.540
It's a lot of people doing the same stuff.

59:05.540 --> 59:07.500
There's sort of one topic of conversation.

59:07.500 --> 59:09.860
You don't bump into someone

59:09.860 --> 59:14.060
with a categorically different worldview than you,

59:14.060 --> 59:16.060
or perspective, or experience.

59:18.060 --> 59:19.500
And so I like visiting,

59:19.500 --> 59:23.500
because I meet like brilliant people in our field, in tech.

59:25.460 --> 59:28.460
But to live there would be really cool.

59:28.460 --> 59:32.420
To live there would be really difficult for me.

59:32.420 --> 59:37.420
I would feel like I'm sacrificing whole pieces of my life.

59:41.340 --> 59:42.420
But I love visiting.

59:42.420 --> 59:43.260
It's a great place.

59:43.260 --> 59:44.900
And I love the folks who are there,

59:44.900 --> 59:47.460
and most of our investors are there.

59:47.460 --> 59:51.300
And so it's a really cool environment,

59:51.300 --> 59:53.700
very intense and like competitive,

59:53.700 --> 59:55.940
and those are really good things.

59:55.940 --> 59:58.260
It's very motivating to be there.

59:58.260 --> 01:00:00.260
But I think I can get that just by visiting.

01:00:00.260 --> 01:00:03.140
I don't need to commit myself full time.

01:00:03.980 --> 01:00:06.300
Aidan Gomez, it's been an honor and a pleasure.

01:00:06.300 --> 01:00:07.580
Thank you so much for joining us.

01:00:07.580 --> 01:00:09.220
Thank you so much for having me.

